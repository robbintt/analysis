---
ver: rpa2
title: Pre-trained Models Perform the Best When Token Distributions Follow Zipf's
  Law
arxiv_id: '2507.22543'
source_url: https://arxiv.org/abs/2507.22543
tags:
- vocabulary
- size
- performance
- tasks
- zipf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of determining the optimal vocabulary
  size for tokenizers in pre-trained models across diverse domains. The authors propose
  a principled method that leverages Zipf's law, positing that downstream task performance
  improves when token frequency distributions closely follow a power-law pattern.
---

# Pre-trained Models Perform the Best When Token Distributions Follow Zipf's Law

## Quick Facts
- **arXiv ID**: 2507.22543
- **Source URL**: https://arxiv.org/abs/2507.22543
- **Reference count**: 40
- **Primary result**: Optimal tokenizer vocabulary sizes are determined by measuring Zipfian alignment (R² fit to power-law on log-log rank-frequency plots)

## Executive Summary
This paper establishes that pre-trained models achieve peak downstream performance when their token frequency distributions closely follow Zipf's law. The authors propose using the coefficient of determination (R²) from log-log rank-frequency plots as a principled stopping criterion for vocabulary expansion during tokenizer construction. Through extensive experiments across natural language processing, genomics, and chemistry domains, they demonstrate that models consistently perform best when token distributions exhibit power-law behavior. The work provides both theoretical justification and practical methodology for vocabulary size selection that improves model efficiency and effectiveness.

## Method Summary
The method leverages Zipf's law to determine optimal tokenizer vocabulary sizes through a principled stopping criterion. The process iteratively expands BPE vocabulary and computes R² scores measuring how closely token frequency distributions follow a power-law. The algorithm terminates when R² improvements stagnate over N consecutive steps beyond a threshold ε. The approach is validated across three domains: NLP (BERT pre-training on OpenWebText+BookCorpus, fine-tuning on GLUE), genomics (BERT on DNA sequences, fine-tuning on GUE), and chemistry (mBART on SMILES strings, fine-tuning on MoleculeNet). Performance metrics include GLUE scores, accuracy, and ROC-AUC, demonstrating consistent correlation between Zipfian alignment and downstream task success.

## Key Results
- NLP models peak at vocabulary size ~30K with GLUE performance of 81.38 and R² of 0.94
- Genomics models peak at vocabulary size ~4K with accuracy of 84.16% and R² of 0.9727
- Chemistry models peak at vocabulary size ~3K with ROC-AUC of 0.9741 and R² of 0.9741
- Zipfian alignment scores consistently correlate with downstream task performance across all domains
- Stagnation-based stopping criterion successfully identifies optimal vocabulary sizes

## Why This Works (Mechanism)

### Mechanism 1: Vocabulary Expansion Induces Zipfian Self-Organization
Increasing vocabulary size causes token rank-frequency distributions to converge toward a linear power-law form on log-log plots. BPE iteratively merges frequent adjacent pairs, progressively capturing recurring substructures at multiple granularities. As vocabulary grows, the statistical distribution of token usage naturally reorganizes toward Zipfian scaling because merge operations concentrate frequency among high-utility tokens while the long tail captures rarer structures. This mechanism relies on underlying data containing reusable substructures at multiple scales that, when properly merged, produce power-law distribution of usage frequencies.

### Mechanism 2: Zipfian Alignment Correlates with Downstream Performance
Downstream task performance peaks when token distribution's R² reaches its highest stable value. When tokens follow power-law distribution, models allocate embedding capacity efficiently—frequent tokens receive sufficient representation while the long tail preserves coverage of rare but meaningful units. This balance reduces over-fragmentation and under-segmentation, improving both semantic representation and sequence-level efficiency. The mechanism assumes that Zipfian token distribution reflects optimal trade-off between token granularity and vocabulary coverage for representation learning.

### Mechanism 3: Stagnation-Based Early Stopping for Vocabulary Selection
Monitoring R² improvement stagnation provides data-driven stopping criterion for vocabulary expansion. The algorithm iteratively expands vocabulary and tracks Zipfian fit (R² at step t). If R² fails to exceed maximum by threshold ε after N consecutive steps, the process terminates, avoiding redundant merges that inflate vocabulary without improving statistical alignment. The mechanism assumes that diminishing R² improvements indicate additional merges capture noise rather than meaningful substructures.

## Foundational Learning

- **Concept: Power-law distributions and Zipf's law**
  - Why needed here: Understanding why frequency ∝ rank⁻ᵏ produces straight line on log-log plots is essential for interpreting R² as fit metric and recognizing when distributions deviate
  - Quick check question: On a log-log plot, what shape indicates perfect adherence to Zipf's law?

- **Concept: Byte Pair Encoding (BPE) tokenization**
  - Why needed here: BPE is iterative merge algorithm used to construct vocabularies; its mechanics explain how vocabulary expansion creates new tokens by combining frequent pairs
  - Quick check question: In BPE, how is the next token added to the vocabulary?

- **Concept: Coefficient of determination (R²) as goodness-of-fit**
  - Why needed here: R² quantifies linear fit on log-log plots; higher values indicate closer Zipfian alignment, which paper links to performance
  - Quick check question: What does R² = 0.95 vs. R² = 0.70 indicate about fit quality?

## Architecture Onboarding

- **Component map**: Tokenizer trainer (BPE implementation) -> Frequency analyzer (compute token frequencies) -> Zipf fit calculator (log-log linear regression, return R²) -> Stagnation monitor (track R² history, compare against max + ε over N steps) -> Stopping controller (terminate expansion when condition met)

- **Critical path**: 1) Initialize vocabulary with character set 2) Run BPE merge step → updated vocabulary 3) Re-tokenize corpus with new vocabulary 4) Compute frequency distribution → log-log plot → R² 5) Check stagnation; if triggered, output optimal vocabulary size; else, return to step 2

- **Design tradeoffs**: Smaller ε → more sensitive stopping, risk of under-segmentation; Larger N → more robust stopping, higher computational cost; Choice of regression method (full-spectrum vs. segmented fitting): Full-spectrum R² is simpler but may obscure multi-regime distributions

- **Failure signatures**: R² never stabilizes: May indicate corpus with insufficient substructure or extremely small dataset; Performance degrades despite high R²: May indicate over-merged tokens violating domain semantics; Large gap between R² peak and performance peak: May indicate task-specific tokenization requirements not captured by Zipfian fit alone

- **First 3 experiments**: 1) Replicate Figure 1 on new corpus: Train BPE tokenizers at vocab sizes 2K-50K, plot log-log rank-frequency curves, verify linearization trend, compare R² values against paper's benchmarks 2) Validate R²-performance correlation on held-out domain: Train BERT-based models with vocab sizes 2K-10K on new genomics dataset, measure R² and downstream accuracy, check for alignment 3) Stress-test stagnation thresholds: Vary ε (0.001, 0.005, 0.01) and N (3, 5, 10 steps) on chemistry dataset, record resulting vocabulary sizes and performance, compare against fixed-vocab baselines to assess robustness

## Open Questions the Paper Calls Out

**Scalability to Larger Models**: Due to hardware limitations, the authors only conducted pre-training experiments on relatively small models (72M-124M parameter BERT and 177M-320M mBART). Further experiments on larger models are necessary to solidify conclusions and validate scalability of the approach.

**Extension to Additional Modalities**: Future work should extend evaluation to additional modalities such as vision and audio, as well as diverse model architectures beyond BERT and mBART.

## Limitations

- The method's applicability to non-BPE tokenization algorithms (WordPiece, SentencePiece, Unigram) remains untested
- Optimal vocabulary sizes appear corpus-dependent and may shift with different data compositions or preprocessing approaches
- The causal relationship between Zipfian alignment and improved performance is not definitively established—correlation may reflect other underlying factors
- Computational cost of iterative vocabulary expansion and R² calculation may be prohibitive for very large corpora

## Confidence

**High Confidence**: The observation that R² increases with vocabulary size and plateaus at specific points is well-supported by empirical evidence across all three domains. The correlation between R² plateaus and downstream performance peaks is consistently observed.

**Medium Confidence**: The proposed stagnation-based stopping criterion is theoretically sound but lacks extensive validation across diverse stopping parameter settings. The choice of ε and N thresholds is not systematically explored.

**Low Confidence**: The claim that Zipfian alignment is a fundamental principle underlying optimal vocabulary selection across all domains and tokenization methods is extrapolative. While supported within the paper's scope, broader validation across different architectures, objectives, and tokenization algorithms is needed.

## Next Checks

1. **Cross-architecture validation**: Replicate the R² vs. performance correlation using different model architectures (RoBERTa, ELECTRA) and tokenization methods (WordPiece, SentencePiece unigram) on the same corpora to test if Zipfian alignment remains predictive.

2. **Corpus composition sensitivity**: Systematically vary corpus size and domain diversity for a fixed domain, measuring how R² plateaus and optimal vocabulary sizes shift, to establish bounds on the method's applicability.

3. **Alternative stopping criteria comparison**: Implement and compare the stagnation-based criterion against alternative vocabulary selection methods (held-out likelihood, entropy-based metrics) across all three domains to quantify relative performance and robustness.