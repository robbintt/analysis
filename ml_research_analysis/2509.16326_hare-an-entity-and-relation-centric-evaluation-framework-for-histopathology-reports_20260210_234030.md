---
ver: rpa2
title: 'HARE: an entity and relation centric evaluation framework for histopathology
  reports'
arxiv_id: '2509.16326'
source_url: https://arxiv.org/abs/2509.16326
tags:
- reports
- hare
- histopathology
- report
- diagnosis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces HARE, the first domain-specific evaluation
  framework for histopathology report generation, addressing the lack of specialized
  metrics in this area. The framework includes a benchmark dataset of 1,465 annotated
  histopathology reports, a NER model (HARE-NER) and a RE model (HARE-RE) trained
  on these annotations, and a novel metric that computes alignment between reference
  and generated reports based on entity and relation embeddings.
---

# HARE: an entity and relation centric evaluation framework for histopathology reports

## Quick Facts
- arXiv ID: 2509.16326
- Source URL: https://arxiv.org/abs/2509.16326
- Reference count: 20
- Primary result: HARE metric achieves r=0.606 correlation with expert evaluations, outperforming existing radiology metrics for histopathology report quality assessment

## Executive Summary
HARE introduces the first domain-specific evaluation framework for histopathology report generation, addressing the lack of specialized metrics in this area. The framework includes a benchmark dataset of 1,465 annotated histopathology reports, NER and RE models trained on these annotations, and a novel metric that computes alignment between reference and generated reports based on entity and relation embeddings. GatorTronS-based models achieved an F1-score of 0.915 on the benchmark. The HARE metric demonstrated superior correlation with expert evaluations compared to traditional and radiology-specific metrics, showing the highest correlation (r = 0.606) and regression performance (R² = 0.368, RMSE = 0.134). HARE effectively captures histopathology-specific clinical content, providing a robust tool for assessing and advancing automated histopathology report generation.

## Method Summary
HARE comprises three main components: a benchmark dataset of 1,465 annotated histopathology reports with 5 entity types and 2 relation types, a GatorTronS-based NER model (HARE-NER) and RE model (HARE-RE), and a novel metric computing entity/relation alignment using semantic similarity. The framework extracts entities (Anatomical Site, IHC Markers, Pathological Diagnosis, Diagnosis Descriptor, IHC Modifier) and relations (IHC Markers-IHC Modifier, Diagnosis-Diagnosis Descriptor), computes embeddings via SapBERT-UMLS, and measures alignment using cosine similarity with a 0.7 confidence threshold. The final score combines NER F1 and RE F1, validated against expert evaluations using Pearson r, Spearman ρ, Kendall τ, R², and RMSE metrics.

## Key Results
- HARE-NER and HARE-RE models achieved an overall F1-score of 0.915 on the benchmark dataset
- HARE metric achieved Pearson correlation r=0.606 and R²=0.368 with expert evaluations
- HARE outperformed existing metrics including GREEN (second-best) with higher correlation and regression metrics
- Threshold ablation confirmed 0.7 confidence threshold optimal, with HARE_ERROR (inverted threshold) showing r=0.026 correlation

## Why This Works (Mechanism)

### Mechanism 1: Entity-Relation Alignment via Semantic Similarity
HARE aligns clinically-relevant entities and relations between candidate and reference reports using semantic similarity rather than lexical overlap. The framework extracts entities and relations, computes entity embeddings via SapBERT-UMLS, and measures alignment through cosine similarity. The final score combines NER F1 and RE F1, capturing report quality by prioritizing clinically relevant content. Core assumption: semantic similarity of entity embeddings correlates with clinical correctness. Evidence shows strong correlation with expert evaluations, though corpus evidence is limited to radiology report evaluation work. Break condition: significant NER F1 drops make alignment scores noisy, as shown in ablation studies.

### Mechanism 2: Domain-Adaptive Pre-training (GatorTronS Backbone)
Clinical-domain pre-training improves entity/relation extraction in histopathology text through GatorTronS, pre-trained on large-scale clinical corpora and fine-tuned on 2,181 NER samples and 5,014 RE samples. Achieved 0.915 overall F1, outperforming BERT-large, DeBERTa, and PathologyBERT. Core assumption: clinical text patterns transfer to histopathology despite domain shift. Evidence anchors show GatorTronS superiority, supported by corpus work on LLM efficacy in pathology IE tasks. Break condition: significant vocabulary divergence from pre-training clinical corpus degrades entity boundary detection, though paper notes limited pathology-specific pre-trained models exist.

### Mechanism 3: Threshold-Gated Extraction for Robustness
Filtering low-confidence predictions improves metric-to-expert correlation through a 0.7 confidence threshold applied to NER and RE outputs, excluding uncertain predictions before alignment computation. Core assumption: model confidence correlates with correctness, and noisy predictions hurt alignment quality. Evidence from ablation studies shows HARE_0.7_Threshold (r=0.606) significantly outperforms HARE_No_Threshold (r=0.567) and HARE_ERROR (r=0.026). No direct corpus evidence for this specific thresholding mechanism. Break condition: too high threshold drops valid entities (lower recall), too low threshold increases noise; optimal threshold likely dataset-dependent.

## Foundational Learning

- **Named Entity Recognition (NER):** HARE-NER extracts histopathology-specific entities; understanding NER token classification is essential for debugging extraction failures. Quick check: Given "ER – weak positive", can you identify which tokens belong to which entity class (IHC Marker vs. IHC Modifier)?

- **Relation Extraction (RE) with Entity Markers:** HARE-RE uses E1/E2 markers for sequence classification; understanding this architecture is critical for extending relation types. Quick check: How would you construct training pairs for a new relation type (e.g., "Tumor Grade – Anatomical Site")?

- **Semantic Similarity with Medical Embeddings:** Cosine similarity over SapBERT-UMLS embeddings is the alignment core; synonym handling (e.g., "lymphovascular invasion" ≈ "vascular invasion") depends on this. Quick check: If two entities have cosine similarity 0.65, should they be considered a match? What threshold would you choose?

## Architecture Onboarding

- **Component map:** Report text → HARE-NER (entity extraction) → embedding → cosine similarity → alignment score. NER errors propagate directly; RE is secondary but contributes to final score.

- **Critical path:** Input Layer (Reference + Candidate reports) → HARE-NER (GatorTronS-based token classifier → entity spans + confidence scores) → HARE-RE (GatorTronS with E1/E2 markers → relation triples) → Embedding Layer (SapBERT-UMLS fine-tuned embeddings) → Alignment Module (Cosine similarity matrix → max-similarity per entity) → Scoring Module (Precision/Recall/F1 for entities + F1 for relations → HARE Score)

- **Design tradeoffs:** Threshold (0.7) balances precision vs. recall; ablation validates choice but may need tuning for new datasets. Entity schema (5 types) is focused but limited; paper notes missing negation/uncertainty handling and rare entity types. Single backbone (GatorTronS) ensures consistent embeddings but larger model (345M) vs. efficiency.

- **Failure signatures:** Low correlation with expert scores indicates vocabulary shift or NER issues; high HARE score but incorrect diagnosis shows entity-level alignment can miss logical errors; empty entity extraction suggests confidence threshold too high or improper fine-tuning.

- **First 3 experiments:** 1) Baseline validation: Run HARE on provided test set (243 NER samples, 1,068 RE samples); verify reported F1 scores (0.854 NER, 0.977 RE). 2) Threshold sweep: Test confidence thresholds [0.5, 0.6, 0.7, 0.8, 0.9] on held-out validation set; measure correlation with expert scores to validate 0.7 choice. 3) Error analysis on edge cases: Extract reports where HARE and expert scores diverge >0.3; inspect entity/relation errors; categorize failure modes (missing entities, wrong relations, synonym mismatches).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating explicit negation and uncertainty handling into the HARE metric significantly improve its correlation with expert evaluations?
- Basis in paper: The authors state in the Limitations section that the current framework "does not yet explicitly handle negation or uncertainty," which are critical for clinical interpretation.
- Why unresolved: The current model ignores these linguistic modifiers, potentially treating "no evidence of malignancy" similarly to "evidence of malignancy" regarding entity presence.
- What evidence would resolve it: A comparative study evaluating a negation-aware version of HARE against the current version using the expert-annotated score dataset.

### Open Question 2
- Question: How does HARE's performance compare when evaluating specialized histopathology generative models versus the general-purpose GPT-4o models used in validation?
- Basis in paper: The authors acknowledge they "have not tested a comprehensive list of models trained for histopathology reports such as HistGen and WsiCaption."
- Why unresolved: The metric's robustness was primarily validated against general LLMs (GPT-4o/mini) which performed poorly, leaving its ability to discern nuances in specialized models unproven.
- What evidence would resolve it: Benchmarking HARE against expert scores using reports generated by domain-specific architectures like HistGen.

### Open Question 3
- Question: Can expanding the entity and relation taxonomy to include higher-order clinical relationships improve the framework's representation of complex diagnostic reasoning?
- Basis in paper: The paper notes that "nuanced or higher-order clinical relationships, as well as rare or emerging entity types, remain underrepresented."
- Why unresolved: The current schema focuses on simple binary relations, potentially missing the complex interdependencies required for accurate diagnosis in difficult cases.
- What evidence would resolve it: An ablation study using an expanded annotation schema on complex, rare-case reports to see if metric correlation with experts improves.

## Limitations

- The 0.7 confidence threshold may not generalize across different clinical settings and histopathology datasets
- Entity schema excludes critical elements like negation and uncertainty markers that significantly impact clinical interpretation
- Framework relies on GatorTronS embeddings (345M parameters), creating dependency on a single large language model that may not be accessible to all clinical institutions

## Confidence

**High Confidence:** Benchmark dataset construction and basic NER/RE model training methodology are well-documented and reproducible. Correlation results with expert evaluations (r=0.606, R²=0.368) are robust within controlled conditions.

**Medium Confidence:** Generalizability of the 0.7 confidence threshold across different clinical settings and framework's ability to handle diverse histopathology report styles from various institutions.

**Low Confidence:** Long-term clinical utility in real-world deployment, particularly regarding handling of rare entity types, complex negation patterns, and subtle clinical distinctions not captured by semantic similarity alone.

## Next Checks

1. **Cross-Institutional Validation:** Test HARE on histopathology reports from multiple institutions with different reporting styles and diagnostic practices. Measure correlation with local expert panels to assess generalizability beyond original dataset.

2. **Clinical Accuracy Correlation:** Conduct blinded study where HARE scores are compared against clinical outcomes (e.g., treatment decisions, patient follow-up results) rather than expert evaluations alone, to validate framework's clinical relevance.

3. **Threshold Sensitivity Analysis:** Systematically test confidence thresholds from 0.5 to 0.9 on validation set with known ground truth, measuring precision-recall tradeoffs and identifying optimal thresholds for different clinical scenarios (routine vs. complex cases).