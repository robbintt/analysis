---
ver: rpa2
title: 'From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document
  Analysis'
arxiv_id: '2508.10311'
source_url: https://arxiv.org/abs/2508.10311
tags:
- table
- semantic
- document
- text
- tables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DOTABLER is the first table-centric semantic parsing framework
  for documents, addressing the gap in deep semantic understanding between tables
  and their contextual text. It introduces a complete pipeline combining document
  structure preprocessing, relation-aware annotation, and a Table-Text Association
  Model (TTAM) trained on a custom dataset to capture semantic associations.
---

# From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis

## Quick Facts
- arXiv ID: 2508.10311
- Source URL: https://arxiv.org/abs/2508.10311
- Reference count: 40
- Primary result: DOTABLER achieves over 90% precision and F1 scores in linking tables to relevant paragraphs while being over 100× faster than GPT-4o, Gemini-2.0, and Claude-3.5.

## Executive Summary
DOTABLER introduces the first table-centric semantic parsing framework for documents, addressing the gap in deep semantic understanding between tables and their contextual text. It combines document structure preprocessing, relation-aware annotation, and a Table-Text Association Model (TTAM) trained on a custom dataset to capture semantic associations. The framework leverages RoBERTa to achieve over 90% precision and F1 scores in linking tables to relevant paragraphs. Evaluated on nearly 4,000 pages with over 1,000 tables, DOTABLER significantly outperforms GPT-4o, Gemini-2.0, and Claude-3.5 in semantic parsing accuracy while delivering over 100× faster execution. It also enables domain-specific table retrieval with high recall, demonstrating practical utility in complex, real-world document analysis.

## Method Summary
DOTABLER employs a modular pipeline approach to table-centric semantic parsing. The system first preprocesses PDF documents using pdf2image to render them as images, then applies Faster R-CNN (fine-tuned on PubLayNet) for layout detection and Tesseract for OCR text extraction. The core component, TTAM, is a Table-Text Association Model that treats table-paragraph relationships as a binary sentence-pair classification task. TTAM fine-tunes a RoBERTa-base encoder to classify whether a text block semantically describes a table block, using annotated pairs from a custom dataset of 227 PDFs (3,248 pairs). The framework constructs positive pairs from annotated links and negative pairs from random text blocks within the same document, achieving over 90% precision and F1 scores while operating more than 100× faster than competing LLM approaches.

## Key Results
- Achieves over 90% precision and F1 scores in linking tables to relevant paragraphs
- Outperforms GPT-4o, Gemini-2.0, and Claude-3.5 in semantic parsing accuracy
- Delivers over 100× faster execution compared to LLM baselines
- Demonstrates high recall in domain-specific table retrieval applications

## Why This Works (Mechanism)

### Mechanism 1: Binary Sentence-Pair Classification
Converting the implicit relationship between tables and paragraphs into a binary sentence-pair classification task enables high-precision semantic linking. The Table-Text Association Model (TTAM) concatenates OCR-extracted table content and text content as a single input sequence, processing it through a RoBERTa encoder to generate contextualized embeddings, which a classifier layer maps to "Related" or "Unrelated." The core assumption is that semantic relationships can be captured via textual content alone without requiring explicit spatial coordinates. Evidence shows RoBERTa-based TTAM achieving 90.01% F1, significantly higher than LLM baselines. The break condition occurs when OCR extraction is poor or text relies heavily on visual cues without naming headers.

### Mechanism 2: Tables as Structural Anchors
Using tables as fixed structural anchors reduces document parsing complexity to a localized "neighbor-judgment" problem rather than a global segmentation problem. Instead of parsing the entire document hierarchy simultaneously, the system identifies all Table blocks first, then iteratively evaluates the semantic relevance of every Text/List block relative to each anchor. The core assumption is that relevant context can appear anywhere in the document, requiring global search per table. Evidence shows the parsing is formalized as iterating over table blocks to find associated sets. The break condition occurs in documents with extremely high table density, where computational cost of all-pairs comparison may become a bottleneck.

### Mechanism 3: Fine-tuned Encoder vs. Generative LLMs
Fine-tuning a discriminative encoder (RoBERTa) outperforms generative LLMs (GPT-4o) for this specific task by mitigating "conservative" hallucination and high false-negative rates. Generative models often struggle with specific binary constraints without extensive prompt engineering, tending to miss implicit links. A fine-tuned bidirectional encoder captures nuanced cross-attention between table and text tokens more effectively for classification. The core assumption is that the training distribution of the custom dataset generalizes well to target domain documents. Evidence shows LLMs like Claude-3.5 produced 190 false negatives vs TTAM's 51. The break condition occurs if the document domain differs drastically from scientific/technical papers.

## Foundational Learning

- **Sentence-Pair Classification (NLI)**: The core logic of TTAM treats (Table, Paragraph) as a premise-hypothesis pair. Understanding how attention mechanisms overlap between two distinct text blocks is essential. *Quick check*: Can you explain why a [CLS] token embedding represents the semantic relationship of the entire sequence in a Transformer?

- **Document Layout Analysis (DLA)**: DOTABLER relies on a pre-processing stage (Faster R-CNN) to isolate tables from noise. If this step fails, the semantic parsing receives garbage input. *Quick check*: What is the IoU (Intersection over Union) metric, and why does bounding box accuracy matter before semantic analysis?

- **Negative Sampling Strategies**: The paper notes training uses equal parts positive and negative samples. *Quick check*: Why would training only on "adjacent" paragraphs create a brittle model for this task?

## Architecture Onboarding

- **Component map**: PDF binary → pdf2image (Renderer) → Faster R-CNN + PubLayNet (Layout) → Tesseract (OCR) → TTAM (RoBERTa-base encoder + Classification Head) → Cross-Encoder scoring function using RoBERTa weights
- **Critical path**: The OCR quality is the primary dependency. If Tesseract returns "Image" or garbled text for a table, the RoBERTa encoder cannot compute semantic similarity.
- **Design tradeoffs**: The authors chose a smaller, faster encoder (RoBERTa) over massive LLMs, gaining ~100× speed and higher Recall but losing generative ability to "summarize" or "explain" the link. This is a modular pipeline (Detection → OCR → NLP), easier to debug but suffering from error propagation compared to theoretical end-to-end models.
- **Failure signatures**: "Generic Text" Failure occurs when paragraphs describe trends generally without citing specific table headers, causing misclassification as unrelated. Cross-Page fragmentation occurs if a table spans pages and the pre-processor treats it as two separate blocks, splitting semantic context.
- **First 3 experiments**: 1) Run the Faster R-CNN + OCR pipeline on target domain samples to visually verify Table Block IoU and OCR text readability. Do not proceed to TTAM if OCR is <90% accurate. 2) Train a local version of TTAM using "hard negatives" vs. random negatives to test robustness. 3) Measure inference time of the RoBERTa-based TTAM on a single document with 20+ tables to verify claimed <0.01s overhead scales with table density.

## Open Questions the Paper Calls Out

- **Implicit Semantic Links**: How can the model's capability be enhanced to infer implicit semantic links when contextual descriptions refer to general trends without explicitly citing specific table headers or values? The current TTAM struggles with "overly generic text descriptions" that lack specific references to table elements, a problem identified in the error analysis where the model incorrectly classifies pairs as unrelated.

- **Irregular Document Structures**: How can the framework's robustness be improved to handle documents with irregular structures, non-standard templates, or scanned documents with complex embedded tables? The current pipeline relies on standard tools like Faster R-CNN and Tesseract, which can fail on non-standard formats, creating a bottleneck that prevents accurate semantic parsing regardless of the TTAM's theoretical capability.

- **Error Propagation Quantification**: To what extent does error propagation from the document structure preprocessing stage (layout analysis and OCR) quantitatively impact the final accuracy of the Table-Text Association Model? The evaluation attributes failures primarily to semantic ambiguity but does not distinguish between semantic misunderstanding by TTAM and failures caused by incorrect text extraction or block misclassification in the preprocessing step.

## Limitations

- **Dataset Bias**: The custom dataset drawn from arXiv and PubMed may not generalize to non-scientific documents like legal, financial, or creative documents where table-text semantics differ significantly.

- **Error Propagation**: The modular pipeline approach means errors compound through each stage (Layout Detection → OCR → NLP), and the paper does not quantify this impact on final TTAM performance.

- **Scalability**: While TTAM is fast per pair, the all-pairs comparison approach is O(N²) in document size, creating potential bottlenecks for documents with high table density.

## Confidence

- **Speed and Efficiency Claims**: High Confidence - The 100× speedup over LLMs is well-supported by concrete timing comparisons.
- **Accuracy Claims**: Medium Confidence - TTAM achieves >90% F1 on the test set, but this is measured only on the custom dataset.
- **Semantic Parsing Claims**: Low Confidence - The claim of "deep semantic parsing" is not rigorously defined or compared to established semantic parsing benchmarks.

## Next Checks

1. **Cross-Domain Validation**: Evaluate DOTABLER on a diverse set of document types (legal contracts, financial reports, annual reports) to test generalization beyond scientific papers. Measure performance degradation and identify failure patterns.

2. **Error Attribution Analysis**: Instrument the pipeline to measure the contribution of each stage (Layout Detection, OCR, TTAM) to final errors. Use techniques like ablation studies or error propagation modeling to identify the primary bottleneck.

3. **Scalability Benchmarking**: Test DOTABLER on documents with varying table densities (5, 10, 20, 50+ tables) to empirically measure the O(N²) scaling behavior. Compare against approximate methods (e.g., locality-based filtering) to identify practical limits.