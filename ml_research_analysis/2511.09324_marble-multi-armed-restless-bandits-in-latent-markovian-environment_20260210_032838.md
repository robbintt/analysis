---
ver: rpa2
title: 'MARBLE: Multi-Armed Restless Bandits in Latent Markovian Environment'
arxiv_id: '2511.09324'
source_url: https://arxiv.org/abs/2511.09324
tags:
- whittle
- state
- index
- restless
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MARBLE, a restless multi-armed bandit framework
  with a latent Markovian environment that induces nonstationary behavior through
  abrupt regime changes. The authors address the challenge of learning optimal policies
  when dynamics are driven by unobserved environmental states, relaxing the classical
  stationarity assumption.
---

# MARBLE: Multi-Armed Restless Bandits in Latent Markovian Environment
## Quick Facts
- arXiv ID: 2511.09324
- Source URL: https://arxiv.org/abs/2511.09324
- Reference count: 40
- Primary result: MARBLE learns optimal policies in restless multi-armed bandits with latent Markovian environment changes using two-timescale Q-learning with Whittle indices.

## Executive Summary
This paper introduces MARBLE, a restless multi-armed bandit framework with a latent Markovian environment that induces nonstationary behavior through abrupt regime changes. The authors address the challenge of learning optimal policies when dynamics are driven by unobserved environmental states, relaxing the classical stationarity assumption. They propose the Markov-Averaged Indexability (MAI) criterion, which requires only that the average environment be indexable rather than every environment. Under this assumption, they prove that synchronous Q-learning with Whittle Indices (QWI) converges almost surely to the optimal Q-function and Whittle indices despite unobserved regime switches. The method is validated on a calibrated recommender system simulator (digital twin), where QWI consistently adapts to the shifting latent state and converges to an optimal policy, achieving near-identical average rewards compared to the oracle Whittle index policy over 500,000 iterations.

## Method Summary
The MARBLE framework learns optimal policies in restless multi-armed bandits with latent Markovian environment changes. The algorithm maintains Q-tables for relaxed single-arm problems and Whittle index estimates, updating them via two-timescale stochastic approximation. The fast timescale updates Q-values using current Whittle indices, while the slow timescale updates the indices themselves. A calibrated simulator (digital twin) provides environment-averaged transitions, enabling the algorithm to learn without directly observing the latent environment. The Markov-Averaged Indexability (MAI) criterion ensures that the environment-averaged problem is indexable, making the Whittle index approach valid despite unobserved regime switches.

## Key Results
- QWI converges almost surely to optimal Q-function and Whittle indices despite unobserved regime switches
- MARBLE achieves near-identical average rewards compared to oracle Whittle index policy over 500,000 iterations
- The MAI criterion provides a relaxed indexability assumption sufficient for Whittle index optimality
- Two-timescale stochastic approximation enables simultaneous Q-learning and Whittle index convergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-timescale stochastic approximation enables simultaneous Q-learning and Whittle index convergence despite unobserved environment switches.
- Mechanism: The algorithm maintains separate timescales—Q-values update on the fast timescale (α_k), while Whittle indices update on the slow timescale (β_k where β_k/α_k → 0). This separation allows Q-estimates to quasi-statically track the current index, reducing the coupled system to tractable ODEs with provable attractors.
- Core assumption: Step-size sequences satisfy Σα_k = Σβ_k = ∞, Σα²_k < ∞, and α_k = o(β_k) (Assumption II.2).
- Evidence anchors:
  - [abstract] "synchronous Q-learning with Whittle Indices (QWI) converges almost surely to the optimal Q-function and the corresponding Whittle indices"
  - [section IV, Theorem IV.2] Invokes Borkar's two-timescale SA theorem; proves fast-ODE global stability via γ-contraction, slow-ODE stability via Lyapunov function V(y) = ½(y - λ̅*_i(z))²
  - [corpus] Related work on non-stationary RMABs (arXiv:2506.18186, 2508.10804) addresses nonstationarity via windowing/change detection rather than two-timescale SA
- Break condition: If α_k and β_k decay at comparable rates, or if Σβ²_k diverges, tracking fails and convergence guarantees are void.

### Mechanism 2
- Claim: Ergodicity of the latent environment chain ensures algorithm convergence to environment-averaged optimal policies without requiring environment state estimation.
- Mechanism: The latent chain {E_k} is irreducible and aperiodic (Assumption III.1), yielding unique stationary distribution μ_E. Martingale-difference noise M^{i,z}_{k+1} has conditional expectation zero because E[r^j_k | F_k] = Σ_e μ_E(e)r^i_e(s,j) = r^i_E(s,j). The noise averages out over time.
- Core assumption: The latent Markov chain has unique stationary distribution and bounded rewards (Assumptions III.1, III.3).
- Evidence anchors:
  - [abstract] "despite unobserved regime switches, under the MAI criterion, synchronous QWI converges almost surely"
  - [section IV, proof] "E[M^{i,z}_{k+1}(s,a)] = 0" derived from μ_E averaging; bounded second moments shown via |U_k| ≤ 2R_max
  - [corpus] Prior work (arXiv:2503.18607 by same authors) addresses RL in switching MDPs; corpus lacks direct comparison on hidden-state ergodicity requirements
- Break condition: If the latent chain is periodic or reducible (no unique μ_E), or if rewards are unbounded, martingale-difference properties fail.

### Mechanism 3
- Claim: Markov-Averaged Indexability (MAI) provides a relaxed structural condition sufficient for Whittle index optimality.
- Mechanism: Standard indexability requires every environment-dependent single-arm problem to be indexable. MAI requires only that the environment-averaged problem (with transition p^i_E(s'|s,a) = Σ_e μ_E(e)p^i_e(s'|s,a)) be indexable. The action gap f_λ(z) = Q̅^{i,z}_λ(z,1) - Q̅^{i,z}_λ(z,0) is decreasing in λ, yielding unique root λ̅*_i(z).
- Core assumption: The passive region P̅_i(λ) is increasing in λ for the averaged environment (Assumption III.4/MAI).
- Evidence anchors:
  - [abstract] "We further introduce the Markov-Averaged Indexability (MAI) criterion as a relaxed indexability assumption"
  - [section III, Definition III.2 & Assumption III.4] Formalizes MAI; "instead of assuming every environment is indexable, it requires only that the average environment be indexable"
  - [corpus] Related work on indexability verification (Akbarzadeh & Mahajan 2022, cited as [7]) addresses standard indexability but not MAI specifically
- Break condition: If the averaged environment is non-indexable (passive regions non-monotonic in λ), the Whittle index is undefined and the entire framework collapses.

## Foundational Learning
- Concept: **Restless Multi-Armed Bandits (RMABs)**
  - Why needed here: MARBLE extends RMABs; understanding Whittle relaxation and indexability is prerequisite.
  - Quick check question: Can you explain why Whittle's relaxation converts a PSPACE-hard problem to a tractable index policy?
- Concept: **Two-Timescale Stochastic Approximation**
  - Why needed here: Core proof technique; QWI convergence relies on Borkar's theorem.
  - Quick check question: In coupled recursions x_{k+1} = x_k + a_k h(x_k, y_k) and y_{k+1} = y_k + b_k g(x_k, y_k), why must b_k/a_k → 0?
- Concept: **Whittle Index and Indexability**
  - Why needed here: MAI is a relaxation of standard indexability; the algorithm outputs Whittle indices.
  - Quick check question: Define the Whittle index λ*_i(z) as the subsidy making activation and passivity indifferent at state z.

## Architecture Onboarding
- Component map: Q-tables -> Index table -> Calibrated simulator (digital twin) -> Step-size schedulers
- Critical path:
  1. For each arm i, sample (s,a,s') ~ G_i(θ) from simulator
  2. Update Q-table via Eq. (6) using current λ_k and reward r^i_{E_k}(s,a)
  3. Update index λ^{i}_{k+1}(z) via Eq. (7) driving Q-gap to zero
  4. For online execution: select top M arms by current indices (ε-greedy)
- Design tradeoffs:
  - **Synchronous vs asynchronous**: Paper assumes synchronous (full sweep over state-actions); asynchronous extension is future work
  - **Tabular vs neural**: Current implementation is tabular; scalability to large state spaces requires function approximation (cf. [17], [20])
  - **Calibrated simulator requirement**: Assumes known θ and generative model access; real deployments may have simulator misspecification
- Failure signatures:
  - Diverging Q-values or indices: Check step-size conditions (Σα²_k < ∞)
  - Non-converging indices: Verify MAI holds for your problem (test monotonicity of passive regions numerically)
  - Poor online performance despite convergence: Simulator may be miscalibrated; environment distribution μ_E may differ from reality
- First 3 experiments:
  1. **Sanity check**: Replicate Figure 1(b) homogeneous case with N=100, M=10, γ=0.8; verify index convergence to oracle values
  2. **MAI necessity test**: Construct a problem where individual environments are indexable but the averaged environment is not; confirm algorithm divergence
  3. **Robustness to simulator error**: Introduce systematic bias in G_i(θ) transitions (e.g., perturb μ_E); measure performance degradation vs oracle

## Open Questions the Paper Calls Out
- Question: Does synchronous Q-learning with Whittle Indices (QWI) maintain convergence guarantees when the calibrated simulator (digital twin) is erroneously specified?
  - Basis in paper: [explicit] The conclusion states that future work aims to "target synchronous QWI with an erroneously calibrated simulator."
  - Why unresolved: The theoretical proof (Theorem IV.1) explicitly assumes access to a calibrated simulator $G_i(\theta)$ with known parameters $\theta$.
  - What evidence would resolve it: Theoretical bounds relating simulator calibration errors to the bias or variance of the converged Whittle indices, or empirical robustness analysis under perturbed model parameters.

- Question: Can the convergence properties of QWI be extended to the asynchronous (online) setting within the MARBLE framework?
  - Basis in paper: [explicit] The conclusion identifies "asynchronous QWI" as a target for future work.
  - Why unresolved: The paper specifically analyzes "synchronous" updates using a generative model (simulator) that allows full state-action sweeps; online learning involves non-i.i.d. trajectories and partial observations.
  - What evidence would resolve it: A convergence proof for QWI under trajectory-based (asynchronous) updates or a demonstration of stability in a purely online deployment without a digital twin.

## Limitations
- Strong structural assumptions (MAI, ergodicity, known simulator) may not hold in practice
- Empirical validation limited to a single calibrated simulator without real-world data testing
- Complexity of two-timescale convergence proof makes verification challenging

## Confidence
- Confidence in core claims: Medium
- Confidence in mechanism description: High
- Confidence in real-world applicability: Low

## Next Checks
1. **MAI Verification**: Test the Markov-Averaged Indexability criterion on benchmark RMAB problems where standard indexability is known, confirming that MAI is both necessary and sufficient for convergence.
2. **Simulator Sensitivity**: Evaluate MARBLE's performance under varying degrees of simulator misspecification (systematic bias in transition probabilities) to assess robustness to the calibrated simulator assumption.
3. **Scalability Test**: Extend the tabular implementation to moderate state spaces (e.g., 10 states) using function approximation, measuring the impact on convergence speed and policy quality compared to the tabular case.