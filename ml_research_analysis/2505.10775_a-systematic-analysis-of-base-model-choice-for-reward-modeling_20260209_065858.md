---
ver: rpa2
title: A Systematic Analysis of Base Model Choice for Reward Modeling
arxiv_id: '2505.10775'
source_url: https://arxiv.org/abs/2505.10775
tags:
- arxiv
- benchmarks
- language
- reward
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how the choice of base model affects reward
  modeling performance. Experiments on 40 models show that selecting the best model
  within each size group can improve performance by up to 14% compared to the commonly
  used Llama-3.x.
---

# A Systematic Analysis of Base Model Choice for Reward Modeling

## Quick Facts
- arXiv ID: 2505.10775
- Source URL: https://arxiv.org/abs/2505.10775
- Reference count: 32
- One-line primary result: Selecting the best base model within each size group can improve reward modeling performance by up to 14% compared to Llama-3.x.

## Executive Summary
This paper systematically analyzes how base model choice affects reward modeling (RM) performance. Through experiments on 40 models, the authors demonstrate that model selection is a critical hyperparameter, with the best choice within each size group (small, medium, large) outperforming Llama-3.x by up to 14%. The study reveals that five specific benchmarks—coding (MBPP+, HumanEval+), safety (ToxiGen), general (IFEval), and model size—can predict top-performing RMs with 18% better accuracy than single benchmarks. The analysis also shows that post-training alignment steps (DPO/RLVR) degrade RM performance by 3-5% compared to SFT checkpoints, while SFT itself improves performance by 15.5%.

## Method Summary
The study evaluates 40 base models (494M–10.3B parameters) using two reward modeling approaches: Bradley-Terry (binary classification) and Regression (multi-attribute scoring). Models are trained on HelpSteer2-Preference (BT) and HelpSteer2 (Regression) datasets, then evaluated on RewardBench across four subcategories. The authors implement a five-feature Elastic Net regression combining MBPP+, HumanEval+, ToxiGen, IFEval, and parameter count to predict RM performance. Training uses AdamW optimizer with learning rate grid search, constant learning rate, and 20 warmup steps. Models are grouped by size (<3B, 3–6B, ≥6B) and compared against Llama-3.x baselines.

## Key Results
- Base model selection within each size group can improve RM performance by up to 14% compared to Llama-3.x.
- A regression model combining five benchmarks predicts top-performing RMs 18% more accurately than single benchmarks.
- SFT checkpoints outperform final DPO/RLVR checkpoints as RM bases by 15.5% vs. 3-5% degradation.
- Estimated pre-training data distributions provide an additional 1.5% improvement to regression model accuracy.

## Why This Works (Mechanism)

### Mechanism 1
Base model selection functions as a critical hyperparameter where latent skills from pre-training data distribution dictate downstream RM capacity. Models with better foundational capabilities in coding and safety provide superior initialization for the reward head. The estimated pre-training data distributions correlate with specific reasoning patterns required for preference classification. This selection proxy may fail if the RM task requires capabilities orthogonal to standard benchmarks.

### Mechanism 2
A regression model combining five benchmarks predicts top-performing RMs more accurately than single-metric selection because RM performance is multi-factorial. Single benchmarks show high correlation but low coverage in top-k selection. By combining coding, safety, general, and model size features, the regression captures the low-dimensional subspace of capabilities required for RM tasks, correcting for individual metric limitations. This approach may not generalize if the "best" RM is defined by criteria outside the RewardBench distribution.

### Mechanism 3
Supervised Fine-Tuning (SFT) enhances RM performance (+15.5%) by adapting the base model to the instruction-following distribution, whereas subsequent alignment steps degrade it (-3-5%) via "alignment tax." SFT improves the utility of the final hidden state for reward prediction, while DPO/RLVR optimize for a specific preference policy that may narrow the model's distribution and harm general representation quality. Future alignment methods preserving general capabilities may eliminate this degradation pattern.

## Foundational Learning

- **Concept: Bradley-Terry (BT) vs. Regression Modeling**
  - Why needed here: The paper evaluates two distinct architectures for Reward Models. BT models (binary classification) suffered from instability/overfitting, while Regression models performed better for stronger models.
  - Quick check question: Can you explain why a Regression model might be more robust to base model choice than a Bradley-Terry classification head? (Hint: Loss function stability).

- **Concept: Coverage vs. Correlation**
  - Why needed here: High statistical correlation does not guarantee practical utility in model selection. "Coverage" measures the overlap of top-k models between the proxy and target.
  - Quick check question: If Benchmark A has 0.9 correlation with RM performance but only 30% coverage in the top-10, why is it dangerous to use for selecting a base model?

- **Concept: Alignment Tax (Capability Forgetting)**
  - Why needed here: Understanding that optimizing a model for specific alignment goals can reduce its general representational power or reasoning diversity.
  - Quick check question: Why would a model fine-tuned to generate preferred answers be worse at judging answers than an SFT model?

## Architecture Onboarding

- **Component map:** 40 Base Models -> HelpSteer2 / HelpSteer2-Preference -> Bradley-Terry (Classification) OR Regression (MSE Loss) -> RewardBench Evaluation -> 5-feature Elastic Net Regression
- **Critical path:** Use the SFT checkpoint of a model (not the final DPO/Instruct checkpoint) from the Qwen2.5 or Gemma-2 family. Train using Regression if data permits, as it showed better performance and stability than BT for stronger models.
- **Design tradeoffs:** Regression vs. BT requires multi-attribute labeled data but yields higher performance (+10-15% absolute) and less overfitting. Performance vs. Similarity: Qwen2.5-7B offers high performance but may have different failure modes than Llama-3.1-8B.
- **Failure signatures:** Reasoning Drop: Using a post-DPO/RLVR model as the RM base often results in a 10-20% relative drop in "Reasoning" sub-scores. Low-Coverage Selection: Selecting models based solely on MMLU leads to suboptimal RM performance.
- **First 3 experiments:** 1. Replicate Regression Validation: Train the Elastic Net regression model to validate the "coverage" improvement over single benchmarks. 2. SFT vs. DPO Base: Train two identical RMs—one on the SFT checkpoint, one on the final Instruct checkpoint—to reproduce the 3-5% alignment degradation. 3. Family Swap: Replace a default Llama-3.x RM base with a size-matched Qwen2.5 or Gemma-2 model in your pipeline to verify the 3-14% improvement claim.

## Open Questions the Paper Calls Out

### Open Question 1
What mechanisms cause post-SFT alignment steps (DPO, RLVR) to degrade reasoning performance in reward models, and is this linked to the specific composition of math versus coding data? The authors observe the drop and hypothesize about the 31% math vs. 69% coding split in RewardBench but do not isolate the root cause.

### Open Question 2
Does the variability in reward modeling performance based on base model choice persist when training on datasets significantly larger than HelpSteer2? The study was computationally constrained to a single, relatively small dataset; it is unknown if "compute-optimal" base model choices change with data scaling.

### Open Question 3
Can advanced pre-training data estimation techniques improve the predictive power of regression models for reward modeling beyond the modest 1.5% gain observed using length-normalized probability? The current method showed "untapped potential" but was limited by the noise and simplicity of the truncated probability metric.

## Limitations
- Estimated pre-training data distributions are not verified against actual training corpora, introducing potential bias.
- Individual benchmarks have "low coverage" in top-k selection, suggesting potential overfitting to the RewardBench distribution.
- The mechanism of 3-5% performance degradation from alignment steps remains speculative without ablation studies.

## Confidence

- **High:** Base model selection significantly impacts RM performance (14% gain observed) and SFT generally outperforms DPO/RLVR as a base (15.5% vs 3-5% degradation).
- **Medium:** The five-benchmark regression model improves selection accuracy by 18% over single metrics, though this depends on RewardBench's specific preference distribution.
- **Low:** Estimated pre-training data distributions reliably predict RM performance across domains outside the tested model families.

## Next Checks

1. **Distribution Validation:** Compare estimated pre-training data distributions against actual training corpora for a subset of models to verify proxy accuracy.
2. **Domain Transfer Test:** Apply the five-benchmark regression model to select base models for a non-RewardBench preference task (e.g., code review preferences) to test generalization.
3. **Alignment Tax Isolation:** Train RMs on DPO/RLVR checkpoints with modified KL penalties or preference sharpening to isolate whether performance degradation stems from optimization or representation changes.