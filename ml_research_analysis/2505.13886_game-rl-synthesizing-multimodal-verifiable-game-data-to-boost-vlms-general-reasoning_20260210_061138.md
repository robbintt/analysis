---
ver: rpa2
title: 'Game-RL: Synthesizing Multimodal Verifiable Game Data to Boost VLMs'' General
  Reasoning'
arxiv_id: '2505.13886'
source_url: https://arxiv.org/abs/2505.13886
tags:
- game
- reasoning
- player
- position
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Game-RL proposes using video games as training resources for vision-language
  reinforcement learning. It introduces Code2Logic, which adapts game code to synthesize
  multimodal, verifiable reasoning tasks, creating the GameQA dataset of 30 games
  and 158 tasks with controllable difficulty.
---

# Game-RL: Synthesizing Multimodal Verifiable Game Data to Boost VLMs' General Reasoning

## Quick Facts
- arXiv ID: 2505.13886
- Source URL: https://arxiv.org/abs/2505.13886
- Reference count: 40
- Primary result: Game-RL improves VLM performance on 7 vision-language benchmarks using game-based RL training

## Executive Summary
Game-RL introduces a novel approach to enhance Vision-Language Models' (VLMs) general reasoning capabilities by training on synthesized game data. The method uses Code2Logic to transform game code into verifiable reasoning tasks, creating the GameQA dataset of 30 games and 158 tasks. Through GRPO reinforcement learning on this dataset, multiple VLMs achieve significant improvements across diverse vision-language benchmarks, with Qwen2.5-VL-7B showing a 2.33% improvement. The approach demonstrates that video games are valuable resources for developing transferable reasoning skills in VLMs.

## Method Summary
Game-RL uses Code2Logic to adapt game code logic into verifiable question-answer pairs, creating the GameQA dataset through a data engine that simulates game states and records transitions. The method employs GRPO reinforcement learning with outcome-based binary rewards determined by an LLM judge (Qwen2.5-32B-AWQ). Training uses a 5K sample subset from 20 in-domain games, with hyperparameters including LR=2e-7, 1 epoch, batch size 3, and KL beta=0.04. The approach demonstrates that RL training on diverse game tasks leads to improved performance on 7 general vision-language benchmarks.

## Key Results
- Qwen2.5-VL-7B achieves 2.33% improvement across 7 diverse vision-language benchmarks
- GRPO RL training outperforms SFT, which causes catastrophic forgetting on general benchmarks
- Game diversity more effective for generalization than sample count per game
- 13.57% of cases show improved visual perception after GRPO training

## Why This Works (Mechanism)

### Mechanism 1: Code-Driven Synthesis of Verifiable Reasoning Chains
Mapping game code logic directly to question-answer templates creates datasets where reasoning steps are objectively verifiable. The Code2Logic approach reuses core game functions to automatically generate step-by-step solutions guaranteed correct by construction. The data engine simulates game states and records transitions, filling templates with accurate state trajectories.

### Mechanism 2: Game Diversity Induces Generalizable Reasoning Patterns
Training on diverse, multi-domain game tasks forces models to learn transferable reasoning skills rather than game-specific heuristics. The 30 games span four cognitive categories. RL optimization on this varied set encourages emergence of general-purpose reasoning capabilities that transfer to non-game benchmarks.

### Mechanism 3: RL with Outcome-Based Rewards Enhances Visual Perception
Reinforcement learning with binary outcome rewards sharpens visual perception as a side effect of optimizing reasoning accuracy. GRPO uses reward based solely on final answer correctness. To maximize reward, the model must learn precise visual extraction because incorrect perception guarantees wrong answers.

## Foundational Learning

- **Reinforcement Learning (RL)**
  - Why needed here: Game-RL uses GRPO algorithm to train VLMs. Understanding policy optimization is critical.
  - Quick check question: Can you explain how a policy network is updated using a reward signal?

- **Vision-Language Models (VLMs)**
  - Why needed here: The method trains and evaluates on VLMs (e.g., Qwen2.5-VL, InternVL). Knowing their architecture is essential.
  - Quick check question: What are the two main components of a typical VLM, and how do they interact?

- **Domain Transfer / Generalization**
  - Why needed here: The central claim is that training on games improves non-game benchmark performance.
  - Quick check question: If a model improves on benchmark A after training on data B, what are three possible reasons?

## Architecture Onboarding

- **Component map:** Game Code Generator -> Task Template Designer -> Data Engine -> GameQA Dataset -> GRPO Training Pipeline -> Evaluation

- **Critical path:** Code Quality -> Template-Code Alignment -> RL Reward Design

- **Design tradeoffs:** GRPO yields better generalization than SFT; Game diversity more effective than sample count; Verifiability vs. Open-endedness

- **Failure signatures:** High in-domain, low out-of-domain (overfitting); Stagnant RL loss (unreliable evaluator); Visual hallucination (poor grounding); Reasoning chain breaks (weak multi-step reasoning)

- **First 3 experiments:**
  1. Data Engine Validation: Manually verify 50 TicTacToe QA pairs with correct answers/states
  2. SFT vs. GRPO Ablation: Train 3B model on 5K subset using both methods, compare generalization
  3. Diversity Scaling: Train on 2-game vs. 10-game subsets (same total samples), evaluate on general benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning based on process supervision effectively utilize the reasoning traces in GameQA to achieve better out-of-domain generalization than the current outcome-supervised GRPO approach?
- Basis in paper: [explicit] Appendix A notes that SFT with reasoning processes failed to generalize and suggests "employing reinforcement learning based on process supervision" as a specific direction for future work.

### Open Question 2
- Question: How can the GameQA framework be adapted for multi-turn game interactions, and does training on sequential gameplay data further enhance VLMs' planning capabilities?
- Basis in paper: [explicit] Appendix A explicitly lists "single-turn game question answering" as a current limitation and identifies developing methods for "multi-turn interactions in gaming scenarios" as a goal for future work.

### Open Question 3
- Question: Is the Code2Logic synthesis approach scalable to complex, non-discrete game environments (e.g., continuous 3D worlds), or is it limited to structured logic puzzle games?
- Basis in paper: [inferred] The selected 30 games (e.g., Sokoban, Sudoku, Minesweeper) are predominantly grid-based, deterministic, and rely on discrete state logic.

## Limitations
- Code quality dependency: Bug-free game code is paramount as source of truth
- Template-code alignment: QA templates must accurately reflect game logic
- Benchmark contamination: Evaluation benchmarks may have inherent overlap with game reasoning patterns

## Confidence
- **High Confidence:** The Code2Logic mechanism for generating verifiable game data is well-specified and experimental results showing in-domain performance improvements are robust
- **Medium Confidence:** The claim that game diversity induces generalizable reasoning patterns is supported by correlation analysis but lacks ablation studies isolating diversity effects
- **Medium Confidence:** The visual perception enhancement through RL is based on qualitative analysis of failure cases, representing a limited sample

## Next Checks
1. **Template Fidelity Audit:** Manually verify 100 randomly sampled GameQA QA pairs across different games to confirm synthesized questions accurately reflect game code logic

2. **Benchmark Independence Analysis:** Conduct systematic overlap analysis between game task categories and general benchmark tasks to quantify potential contamination

3. **Judge Reliability Assessment:** Evaluate the LLM judge's consistency by testing agreement rate on semantically equivalent answers across different phrasings