---
ver: rpa2
title: Augmenting Question Answering with A Hybrid RAG Approach
arxiv_id: '2601.12658'
source_url: https://arxiv.org/abs/2601.12658
tags:
- retrieval
- query
- ssrag
- graph
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of retrieving contextually relevant
  information in retrieval-augmented generation (RAG) systems, which often struggle
  to produce complete or accurate answers in question-answering tasks. To solve this,
  the authors propose Structured-Semantic RAG (SSRAG), a hybrid architecture that
  combines vector-based and graph-based retrieval methods, along with query augmentation
  and agentic query routing.
---

# Augmenting Question Answering with A Hybrid RAG Approach

## Quick Facts
- arXiv ID: 2601.12658
- Source URL: https://arxiv.org/abs/2601.12658
- Reference count: 40
- Key outcome: SSRAG achieves 87% factual accuracy on TruthfulQA for GPT-4 and reduces hallucinations to 0.18 SelfCheckGPT score

## Executive Summary
This paper introduces Structured-Semantic RAG (SSRAG), a hybrid retrieval architecture that combines vector-based and graph-based methods to address the challenge of retrieving contextually relevant information in retrieval-augmented generation systems. SSRAG integrates structured knowledge from knowledge graphs with semantic understanding from vector embeddings through a unified context representation, enabling more accurate and complete question-answering responses. The approach incorporates query augmentation and agentic query routing to further refine retrieval processes and reduce hallucinations.

The system was evaluated across three popular QA datasets (TruthfulQA, SQuAD, WikiQA) using five different large language models, demonstrating consistent performance improvements over standard RAG implementations. SSRAG achieved significant gains in factual accuracy, reduced hallucination rates, and enhanced context precision, establishing it as a scalable and reliable framework for improving the trustworthiness of AI-generated responses in question-answering tasks.

## Method Summary
SSRAG combines vector-based and graph-based retrieval methods in a hybrid architecture, creating a unified context representation that leverages both structured knowledge from knowledge graphs and semantic understanding from vector embeddings. The system incorporates query augmentation to enrich search queries and employs agentic query routing to dynamically direct queries to the most appropriate retrieval method. This approach addresses the limitations of standard RAG systems, which often struggle to retrieve contextually relevant information for complete and accurate question-answering responses.

The hybrid retrieval pipeline processes queries through multiple retrieval pathways, with the agentic router determining the optimal routing strategy based on query characteristics. Retrieved contexts from both vector and graph sources are then merged and fed into the language model for answer generation. The architecture was evaluated across multiple datasets and LLM configurations, with comprehensive metrics tracking factual accuracy, hallucination rates, and context precision to demonstrate consistent improvements over baseline RAG implementations.

## Key Results
- Achieved 87% factual accuracy on TruthfulQA dataset using GPT-4
- Reduced hallucination rates to 0.18 SelfCheckGPT score
- Demonstrated consistent improvements across three datasets (TruthfulQA, SQuAD, WikiQA) and five different LLMs

## Why This Works (Mechanism)
SSRAG's effectiveness stems from its ability to leverage complementary strengths of vector-based and graph-based retrieval methods. Vector retrieval excels at capturing semantic similarity and handling open-ended queries, while graph-based retrieval provides structured, factual knowledge that enhances precision and reduces ambiguity. By combining these approaches through agentic query routing, SSRAG can dynamically select the most appropriate retrieval strategy based on query characteristics, ensuring optimal context retrieval for different types of questions.

The query augmentation component further enhances retrieval effectiveness by expanding and enriching search queries with relevant terms and concepts, improving the likelihood of retrieving comprehensive and relevant contexts. The unified context representation that merges structured and semantic information provides the language model with richer, more complete information, enabling more accurate and comprehensive answers while reducing the likelihood of hallucination or factual errors.

## Foundational Learning

**Vector-based retrieval**: Uses embeddings to capture semantic similarity between queries and documents. Needed because semantic understanding is crucial for handling natural language queries that may not match exact keywords. Quick check: Verify embeddings capture relevant semantic relationships through nearest neighbor analysis.

**Graph-based retrieval**: Leverages structured knowledge from knowledge graphs to provide factual, precise information. Needed because structured data offers unambiguous relationships and facts that improve retrieval accuracy. Quick check: Validate graph traversal returns relevant nodes for test queries.

**Agentic query routing**: Dynamically directs queries to appropriate retrieval methods based on query characteristics. Needed because different query types benefit from different retrieval approaches. Quick check: Analyze routing decisions across diverse query types to ensure appropriate method selection.

**Query augmentation**: Expands and enriches search queries with relevant terms and concepts. Needed because original queries may lack sufficient context for comprehensive retrieval. Quick check: Measure retrieval performance improvement with and without augmentation.

## Architecture Onboarding

**Component map**: User query -> Query Augmentation -> Agentic Router -> Vector Retrieval / Graph Retrieval -> Context Merger -> LLM -> Answer

**Critical path**: Query → Augmentation → Routing → Retrieval (vector/graph) → Merging → Generation

**Design tradeoffs**: The hybrid approach increases computational overhead compared to single-method retrieval but provides superior accuracy and reduced hallucinations. The agentic router adds complexity but enables adaptive retrieval strategies. Context merging requires careful handling to avoid redundancy and conflicting information.

**Failure signatures**: 
- Routing failures: Agentic router consistently selects suboptimal retrieval method
- Augmentation issues: Query expansion introduces noise or irrelevant terms
- Merging problems: Conflicting information from vector and graph sources
- Scalability concerns: Increased latency due to hybrid pipeline

**First experiments**:
1. Test agentic routing accuracy by comparing selected methods against optimal choices for diverse query types
2. Evaluate individual retrieval methods (vector vs graph) to establish baseline performance
3. Measure context merging quality by analyzing information overlap and conflict resolution

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas warrant further investigation: the scalability of the hybrid retrieval pipeline in production environments, the interpretability of agentic routing decisions, and the generalization performance across domains beyond the tested QA datasets.

## Limitations

- Performance evaluation based on only three QA datasets may not generalize to broader domains
- Computational overhead and latency implications of the hybrid retrieval pipeline were not addressed
- The agentic query routing mechanism's decision-making process lacks transparency and interpretability

## Confidence

**High**: SSRAG outperforms standard RAG implementations on tested datasets (TruthfulQA, SQuAD, WikiQA)
**Medium**: SSRAG reduces hallucinations and improves factual accuracy across multiple LLMs
**Medium**: The hybrid retrieval approach provides consistent benefits across different evaluation metrics

## Next Checks

1. Evaluate SSRAG on additional QA datasets from diverse domains (e.g., medical, technical, multilingual) to test generalizability beyond the current three datasets.
2. Conduct ablation studies to isolate the contribution of each SSRAG component (vector retrieval, graph retrieval, query augmentation, agentic routing) to overall performance improvements.
3. Measure computational overhead and inference latency of the full SSRAG pipeline compared to standard RAG implementations to assess real-world deployment feasibility.