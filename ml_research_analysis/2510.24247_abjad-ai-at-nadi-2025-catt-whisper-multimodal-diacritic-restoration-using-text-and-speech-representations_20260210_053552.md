---
ver: rpa2
title: 'Abjad AI at NADI 2025: CATT-Whisper: Multimodal Diacritic Restoration Using
  Text and Speech Representations'
arxiv_id: '2510.24247'
source_url: https://arxiv.org/abs/2510.24247
tags:
- speech
- text
- arabic
- fusion
- catt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents CATT-Whisper, a multimodal model for Arabic\
  \ diacritic restoration that combines a pre-trained text encoder (CATT) with a speech\
  \ encoder (Whisper). The approach integrates text and speech through two fusion\
  \ strategies\u2014early fusion and cross-attention\u2014with early fusion selected\
  \ for its efficiency."
---

# Abjad AI at NADI 2025: CATT-Whisper: Multimodal Diacritic Restoration Using Text and Speech Representations

## Quick Facts
- arXiv ID: 2510.24247
- Source URL: https://arxiv.org/abs/2510.24247
- Reference count: 10
- Primary result: CATT-Whisper achieves WER 0.25/CER 0.09 on dev and WER 0.55/CER 0.13 on test for Arabic diacritic restoration

## Executive Summary
CATT-Whisper is a multimodal model for Arabic diacritic restoration that combines text and speech representations. The approach integrates a pre-trained CATT text encoder with a Whisper speech encoder through early fusion and cross-attention strategies, ultimately selecting early fusion for efficiency. The model was trained with a modality-robust strategy allowing it to perform well even without speech input. Experiments on the NADI 2025 dataset show the model outperforms other submissions and the baseline by leveraging speech cues to disambiguate diacritic placement in dialectal Arabic.

## Method Summary
The CATT-Whisper model combines text and speech modalities for Arabic diacritic restoration. It uses a pre-trained CATT encoder for text processing and a Whisper base encoder for speech features. The speech features undergo frame averaging compression to match text sequence length. Two fusion strategies were explored: early fusion (concatenation of text and speech representations) and cross-attention fusion. The model was trained with a modality-robust strategy that allows it to handle missing speech input during inference. The system was evaluated on the NADI 2025 dataset, achieving state-of-the-art performance on both development and test sets.

## Key Results
- Achieves word error rate of 0.25 and character error rate of 0.09 on development set
- Achieves word error rate of 0.55 and character error rate of 0.13 on test set
- Outperforms other submissions and the baseline on NADI 2025 dataset

## Why This Works (Mechanism)
The model works by integrating complementary information from text and speech modalities. Text representations capture lexical and syntactic patterns while speech representations provide prosodic and phonological cues that help disambiguate diacritic placement, particularly for words with multiple valid diacritizations. The modality-robust training strategy ensures the model can generalize across different input conditions, while the frame averaging compression enables efficient fusion of speech and text sequences.

## Foundational Learning
- **Arabic diacritic restoration**: The task of adding diacritical marks to Arabic text to indicate vowel sounds and pronunciation, needed because modern Arabic text typically omits these marks, making many words ambiguous
- **Multimodal fusion**: Combining information from multiple modalities (text and speech) to improve model performance, needed to leverage complementary information sources
- **Frame averaging compression**: Reducing speech feature dimensionality by averaging consecutive frames, needed to match speech and text sequence lengths for fusion
- **Modality-robust training**: Training strategies that ensure model performance across different input conditions, needed to handle cases where speech input may be missing

## Architecture Onboarding

**Component map**: Text input -> CATT encoder -> Text representations -> Early fusion -> Transformer decoder -> Diacritized output
                                                                 Speech input -> Whisper encoder -> Speech representations -> Early fusion -> Transformer decoder -> Diacritized output

**Critical path**: Text and speech inputs → respective encoders → fusion layer → decoder → output

**Design tradeoffs**: Early fusion was chosen over cross-attention for efficiency despite comparable performance, accepting potential loss of fine-grained alignment between modalities

**Failure signatures**: Model struggles with challenging test cases involving variable pronunciations and heteronyms, suggesting limitations in handling acoustic ambiguity

**First experiments**:
1. Compare early fusion vs cross-attention fusion on development set
2. Evaluate modality-robustness by testing without speech input
3. Analyze performance on heteronym-specific test cases

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Would replacing the Whisper encoder with CTC-based phoneme-level encoders (e.g., Conformer-CTC or Squeezeformer) significantly improve the resolution of ambiguous sequences?
- Basis in paper: The conclusion explicitly states the need for "stronger phoneme-level encoders" and suggests exploring "CTC-based models such as Conformer-CTC, Squeezeformer" to address sequences that remain challenging.
- Why unresolved: The current system uses the Whisper base encoder which provides high-level speech representations, but the authors acknowledge that ambiguous sequences are not fully solved, hypothesizing that explicit phoneme-level modeling might be required.
- What evidence would resolve it: A comparative study on the NADI 2025 dataset measuring WER/CER specifically on the "challenging test cases" subset using CTC-based encoders versus the Whisper encoder.

### Open Question 2
- Question: Does the frame averaging compression strategy (reducing 1500 frames to 150 tokens) result in the loss of critical prosodic information required for disambiguation?
- Basis in paper: Section 3.2.1 describes a drastic downsampling of speech features by averaging 10 consecutive frames to match text sequence length. While necessary for concatenation, this averaging acts as a low-pass filter that could smooth over subtle acoustic cues needed for dialectal nuances.
- Why unresolved: The paper does not ablate this specific compression ratio or compare it against alternative alignment strategies (e.g., adaptive pooling or attention-based downsampling) that might preserve more acoustic detail.
- What evidence would resolve it: An ablation study comparing the fixed 10-frame averaging approach against a method that preserves higher temporal resolution (and handles the length mismatch differently) to see if CER increases on short, ambiguous words.

### Open Question 3
- Question: In what specific error modes does cross-attention fusion outperform early fusion, despite their comparable aggregate performance?
- Basis in paper: Section 3.2.3 states that Early Fusion and Cross-Attention yielded comparable results, but Early Fusion was chosen for efficiency. However, Section 5.3 highlights that "challenging test cases" with variable pronunciations remain difficult.
- Why unresolved: Aggregate WER/CER scores may hide specific strengths of the Cross-Attention mechanism, which might handle text-speech misalignment better than the concatenation-based Early Fusion approach.
- What evidence would resolve it: A comparative error analysis on the "variable pronunciation" test cases mentioned in Section 5.3, contrasting the performance of the two fusion strategies to see if one handles heteronyms better than the other.

## Limitations
- Lacks ablation studies to quantify the specific contribution of speech features versus text-only processing
- Training procedure description is incomplete, missing details about data preprocessing and hyperparameter tuning
- Evaluation focuses only on development and test set metrics without cross-validation or analysis of model generalization across dialects

## Confidence
- **High**: The reported numerical performance metrics (WER 0.25 dev, 0.55 test; CER 0.09 dev, 0.13 test) are likely accurate as they represent direct measurements from the NADI 2025 evaluation
- **Medium**: The claim that speech features improve diacritic restoration accuracy is supported by the results but lacks rigorous ablation evidence to confirm causality
- **Low**: The assertion that early fusion is definitively superior to cross-attention is based on efficiency considerations rather than comparative performance measurements on the target task

## Next Checks
1. Conduct ablation studies comparing CATT-Whisper against a text-only baseline using identical architecture except for speech features to isolate the multimodal contribution
2. Test model robustness across different noise levels and speaker conditions using the NADI dataset's provided audio variations to assess real-world applicability
3. Implement the described modality-robust training strategy on held-out data to verify that the model maintains performance when speech input is absent, as claimed in the paper