---
ver: rpa2
title: 'HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource
  African Language'
arxiv_id: '2509.16256'
source_url: https://arxiv.org/abs/2509.16256
tags:
- sentiment
- dataset
- learning
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces HausaMovieReview, a benchmark dataset of
  5,000 YouTube comments for sentiment analysis in Hausa. The dataset was annotated
  by three independent annotators, achieving a Fleiss' Kappa score of 0.85.
---

# HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource African Language

## Quick Facts
- **arXiv ID**: 2509.16256
- **Source URL**: https://arxiv.org/abs/2509.16256
- **Reference count**: 26
- **Primary result**: Decision Tree with TF-IDF achieves 89.72% accuracy, outperforming transformers (BERT 79.7%, RoBERTa 76.6%) on 5,000 Hausa YouTube comments

## Executive Summary
This paper introduces HausaMovieReview, a benchmark dataset of 5,000 annotated YouTube comments from the Kannywood movie series "LABARINA" for sentiment analysis in Hausa. The dataset was annotated by three native Hausa speakers with majority-vote consensus, achieving a Fleiss' Kappa inter-annotator agreement score of 0.85. The study compares classical machine learning models (Logistic Regression, Decision Tree, KNN) with transformer models (BERT, RoBERTa) on this low-resource dataset. Surprisingly, the Decision Tree classifier with TF-IDF features significantly outperformed all other models, achieving 89.72% accuracy and 89.60% F1-score, while transformer models achieved lower performance (BERT: 79.7% accuracy, RoBERTa: 76.6% accuracy). The authors hypothesize that classical models perform better due to the small dataset size and domain-specificity of the text.

## Method Summary
The study created a 5,000-comment dataset from YouTube comments on the Kannywood movie series "LABARINA." Three native Hausa speakers independently annotated comments as Positive, Neutral, or Negative, with majority-vote consensus determining final labels. Classical models (Logistic Regression, Decision Tree, KNN) used TF-IDF vectorization and 10-fold cross-validation, while transformers (BERT-base-uncased, RoBERTa-twitter-sentiment) were fine-tuned with learning rate 2×10⁻⁵, batch size 16, and 3 epochs using AdamW optimizer. The dataset shows class distribution of 56.8% Positive, 23.3% Negative, and 19.9% Neutral comments.

## Key Results
- Decision Tree classifier with TF-IDF achieved 89.72% accuracy and 89.60% F1-score, significantly outperforming all other models
- Transformer models (BERT: 79.7% accuracy, RoBERTa: 76.6% accuracy) underperformed classical models despite fine-tuning
- Dataset annotation achieved high reliability with Fleiss' Kappa score of 0.85 across three independent annotators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Native-speaker majority-vote annotation produces reliable sentiment labels for low-resource languages.
- Mechanism: Three independent native Hausa speakers annotated 5,000 comments; final labels assigned via majority vote, with consensus discussion for three-way disagreements.
- Core assumption: Annotators share cultural and linguistic context necessary to interpret code-switched, informal text.
- Evidence anchors:
  - [abstract] "The dataset was meticulously annotated by three independent annotators... Fleiss' Kappa score of 0.85"
  - [section 3.2] Details the three-phase annotation: label definition, independent annotation, majority vote
  - [corpus] Neighbor papers on Hausa NLP (e.g., HausaNLP challenges, SemEval emotion detection) similarly rely on native annotators, suggesting community validation of this approach
- Break condition: If annotators lack domain familiarity (Kannywood context) or if label definitions are ambiguous, inter-annotator agreement would drop below reliable thresholds.

### Mechanism 2
- Claim: Classical ML with TF-IDF can outperform fine-tuned transformers on small, domain-specific datasets.
- Mechanism: Decision Tree with TF-IDF achieved 89.71% accuracy vs. BERT (79.7%) and RoBERTa (76.6%); TF-IDF captures domain-specific keywords without requiring large-scale pretraining adaptation.
- Core assumption: The dataset's predictive features are captured by term frequency patterns rather than complex contextual relationships.
- Evidence anchors:
  - [abstract] "Decision Tree classifier, with an accuracy and F1-score 89.72% and 89.60% respectively, significantly outperformed the deep learning models"
  - [section 4.1, Table 3] Shows Decision Tree achieving highest accuracy and F1 among all tested models
  - [corpus] Afro-XLMR-Large and AfriBERTa papers suggest transformer gains emerge with language-adaptive pretraining—absent here, classical models may have advantage
- Break condition: If dataset size increases substantially (>50K examples) or if Hausa-specific transformer pretraining is applied, classical model advantage may disappear.

### Mechanism 3
- Claim: Transformer overfitting risk increases in low-data, code-switched settings.
- Mechanism: BERT and RoBERTa validation loss fluctuated after Step 40 during fine-tuning, indicating overfitting; limited data prevents effective adaptation of pre-trained weights.
- Core assumption: The pre-trained transformers' English-centric representations do not transfer efficiently to Hausa-English code-switched text with only 5,000 examples.
- Evidence anchors:
  - [section 4.1, Figure 5] "validation loss... indicating potential points of overfitting"
  - [section 5] "Transformer models... require vast amounts of data... In a limited data environment, these models may be prone to overfitting"
  - [corpus] Neighbor paper on language-adaptive fine-tuning (AfriBERTa) explicitly addresses this gap by pretraining on African languages first
- Break condition: If language-adaptive pretraining (e.g., AfriBERTa, AfriBERTa-large) is used before fine-tuning, overfitting should reduce and transformer performance may improve.

## Foundational Learning

- **Concept: TF-IDF Vectorization**
  - Why needed here: The paper uses TF-IDF as the sole feature extraction method for classical models; understanding how term importance is weighted is essential to interpreting why Decision Tree succeeded.
  - Quick check question: How does TF-IDF handle a word that appears frequently in one document but rarely across the corpus?

- **Concept: Inter-Annotator Agreement (Fleiss' Kappa)**
  - Why needed here: Dataset quality is validated through Fleiss' Kappa = 0.85; understanding this metric is necessary to assess annotation reliability.
  - Quick check question: What does a Fleiss' Kappa of 0.85 indicate about the consistency among three annotators?

- **Concept: Fine-Tuning vs. Pre-Training in Transformers**
  - Why needed here: The paper fine-tunes BERT/RoBERTa without Hausa-specific pretraining; understanding this distinction explains the performance gap.
  - Quick check question: Why might fine-tuning an English-pretrained model on 5,000 Hausa-English code-switched examples lead to suboptimal performance?

## Architecture Onboarding

- **Component map:**
  - YouTube scraping -> 5,000 random sample -> 3-annotator labeling -> majority vote -> TF-IDF vectorization
  - Classical Track: TF-IDF -> Logistic Regression / Decision Tree / KNN -> 10-fold cross-validation
  - Transformer Track: Raw text -> BERT-base-uncased / RoBERTa-twitter-sentiment -> Fine-tune (lr=2e-5, batch=16, epochs=3) -> Evaluation

- **Critical path:** Annotation quality (Fleiss' Kappa ≥ 0.85) -> Feature extraction choice (TF-IDF for classical, raw text for transformers) -> Model selection based on dataset size and language specificity.

- **Design tradeoffs:**
  - Classical models: Faster, lower compute, effective on small datasets with TF-IDF; limited contextual understanding.
  - Transformers: Higher capacity, contextual embeddings; require more data and language-adaptive pretraining for low-resource languages.
  - 10-fold CV vs. hold-out: CV reduces overfitting risk but increases training time.

- **Failure signatures:**
  - Low inter-annotator agreement (Kappa < 0.6): Indicates ambiguous label definitions or annotator unfamiliarity.
  - Transformer validation loss increasing after early epochs: Overfitting signal; reduce epochs or increase data augmentation.
  - Large accuracy gap between CV folds: Dataset may contain domain-specific clusters; consider stratified sampling.

- **First 3 experiments:**
  1. Replicate the Decision Tree + TF-IDF baseline on the open-source dataset (GitHub link provided) to validate 10-fold CV accuracy ~89%.
  2. Fine-tune AfriBERTa or Afro-XLMR-Large (language-adaptive transformers) on the same data; compare against BERT/RoBERTa results to test whether pretraining language matters.
  3. Expand annotation to 10,000+ comments and re-run transformer fine-tuning; observe whether the classical-transformer performance gap narrows as data increases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the Decision Tree classifier with TF-IDF features significantly outperform transformer models (BERT/RoBERTa) on the HausaMovieReview dataset?
- Basis in paper: [explicit] The authors state in Future Work: "Conduct further analysis to understand why classical models, particularly Decision Tree with TF-IDF, performed exceptionally well. This could involve feature importance analysis, examining decision boundaries, or comparing with other classical ML techniques."
- Why unresolved: The paper documents the counterintuitive result (89.71% vs 79.7%/76.6%) but only hypothesizes causes—small dataset size and domain-specificity—without empirical validation through feature importance or decision boundary analysis.
- What evidence would resolve it: Feature importance rankings from the Decision Tree, ablation studies comparing models across varying dataset sizes, and analysis of which linguistic patterns each model captures successfully.

### Open Question 2
- Question: Would fine-tuning larger, Africa-focused multilingual transformer models (e.g., Afro-XLMR-Large, AfriBERTa-large, mDeBERTaV3) outperform classical models on this dataset?
- Basis in paper: [explicit] The authors propose in Future Work: "Fine-tuning larger, multilingual pre-trained models (e.g., Afro-XLMR-Large, mDeBERTaV3, AfriBERTa-large) on the KannySenti dataset and conducting a more in-depth comparison with the current BERT and RoBERTa results is a key next step."
- Why unresolved: The study only tested bert-base-uncased and cardiffnlp/twitter-roberta-base-sentiment-latest, neither specifically trained on African languages; Afro-XLMR and AfriBERTa have different pre-training corpora including African languages.
- What evidence would resolve it: Comparative experiments fine-tuning Afro-XLMR-Large, AfriBERTa-large, and mDeBERTaV3 on HausaMovieReview using identical evaluation protocols.

### Open Question 3
- Question: Can the HausaMovieReview dataset and models generalize to sentiment analysis tasks beyond Kannywood movie comments?
- Basis in paper: [inferred] The authors acknowledge: "the dataset is restricted to Kannywood movie comments and may not be generalizable to other domains or languages. The models' performance on a broader range of text would likely differ."
- Why unresolved: All 5,000 comments come from a single YouTube series (LABARINA); no cross-domain evaluation was conducted to assess whether learned sentiment patterns transfer to news, social issues, or product reviews in Hausa.
- What evidence would resolve it: Zero-shot or few-shot evaluation of trained models on Hausa sentiment data from different domains (news comments, product reviews, political discourse), or annotation of a multi-domain Hausa sentiment benchmark.

## Limitations
- Small dataset size (5,000 examples) constrains transformer model performance and may not represent full Hausa language diversity
- Domain-specific bias from YouTube comments on single Kannywood series may not generalize to other Hausa text sources
- Lack of specification for TF-IDF hyperparameters and exact train/test splits for transformers affects reproducibility

## Confidence

- **High Confidence**: The Fleiss' Kappa annotation reliability (0.85) and Decision Tree's superior performance over transformers are well-supported by the presented evidence.
- **Medium Confidence**: The generalization claim that classical models outperform transformers in low-resource contexts is supported but would benefit from testing across multiple low-resource languages and dataset sizes.
- **Medium Confidence**: The overfitting explanation for transformer underperformance is plausible but requires additional experiments with language-adaptive pretraining to confirm.

## Next Checks

1. Replicate the experiment using AfriBERTa or Afro-XLMR-Large (language-adaptive transformers) to determine if pretraining on African languages eliminates the classical model advantage.
2. Test the dataset on additional low-resource African languages to validate whether the classical model advantage extends beyond Hausa.
3. Conduct a domain expansion experiment by evaluating model performance on Hausa text from non-Kannywood sources (news, social media, literature) to assess generalization beyond the specific domain.