---
ver: rpa2
title: 'ACPBench Hard: Unrestrained Reasoning about Action, Change, and Planning'
arxiv_id: '2503.24378'
source_url: https://arxiv.org/abs/2503.24378
tags:
- action
- tasks
- task
- planning
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ACPBench Hard introduces a generative version of the ACPBench dataset
  for testing reasoning abilities about action, change, and planning. It presents
  open-ended questions for 8 tasks, including applicability, progression, reachability,
  action reachability, validation, justification, landmarks, and next action.
---

# ACPBench Hard: Unrestrained Reasoning about Action, Change, and Planning

## Quick Facts
- arXiv ID: 2503.24378
- Source URL: https://arxiv.org/abs/2503.24378
- Authors: Harsha Kokel; Michael Katz; Kavitha Srinivas; Shirin Sohrabi
- Reference count: 6
- Key outcome: Generative version of ACPBench dataset tests reasoning about action, change, and planning; even largest models score below 65% on average across 8 atomic tasks

## Executive Summary
ACPBench Hard introduces a generative benchmark for evaluating reasoning capabilities about action, change, and planning tasks. The dataset transforms 13 PDDL domains into 8 open-ended tasks including applicability, progression, reachability, and landmarks. Each task has an associated evaluator to score model responses. Testing across 14 language and reasoning models revealed that even the largest models struggle with most tasks, with average scores below 65%, highlighting the gap between current models and true planning competence.

## Method Summary
The benchmark generates natural language questions from PDDL domains, presenting models with current state descriptions and task-specific prompts. Models receive 2-shot examples and generate responses that are parsed via a grammar-based parser. Task-specific validators score responses: simple set comparisons for applicability/progression/validation, and PSPACE-complete planner calls for reachability/landmarks/next-action. The dataset and evaluation pipeline are available at https://ibm.github.io/ACPBench, with experiments conducted on 14 models using 2 A100 80GB GPUs.

## Key Results
- Generative reasoning tasks are systematically harder than boolean/multiple-choice formats (GPT-4o error rates significantly higher across all tasks except validation)
- Action reachability proves particularly difficult, requiring reasoning about multi-atom preconditions simultaneously
- Performance varies dramatically across domains, with depot/swap/visitall showing higher scores than alfworld/floortile/grippers
- Top models like o1-preview score 44% on action applicability, jumping to 57% with Jaccard similarity relaxation
- No model achieves strong performance across all 8 tasks, with average scores below 65%

## Why This Works (Mechanism)

### Mechanism 1: Generative Questions Reveal True Planning Competence
- Claim: Open-ended generative questions more accurately measure the reasoning capabilities required for actual planning than boolean or multiple-choice formats.
- Mechanism: Generative tasks require models to produce answers from scratch, eliminating shortcuts from option elimination and forcing complete reasoning chains.
- Core assumption: Planning competence requires generative capability; recognition-based performance does not transfer to planning components.
- Evidence: Figure 5 shows GPT-4o error rates significantly higher on generative format compared to boolean/multiple-choice across all tasks except validation.

### Mechanism 2: Task Difficulty Scales with Computational Complexity and Reasoning Depth
- Claim: Tasks requiring multi-step reasoning about action preconditions and state reachability are systematically harder than single-step tasks.
- Mechanism: Action reachability requires reasoning about atom reachability plus additional reasoning about precondition conjunctions holding simultaneously in a reachable state.
- Core assumption: The performance gap reflects genuine reasoning difficulty rather than training data distribution bias.
- Evidence: Observations note that action reachability is much harder than progression despite their relationship, and action reachability is PSPACE-hard.

### Mechanism 3: Atomic Task Decomposition Enables Targeted Planner Integration
- Claim: Breaking planning into 8 atomic reasoning tasks allows models to be evaluated and potentially integrated as modular planner components.
- Mechanism: Each task corresponds to a decision symbolic planners make. Models performing well on specific tasks could replace or augment those components.
- Core assumption: Atomic task performance correlates with end-to-end planning utility; models can be combined without compounding errors.
- Evidence: The dataset aims to distill complex plan generation into separate atomic reasoning tasks that could be integrated into planners or used directly as policies.

## Foundational Learning

- Concept: STRIPS Planning Formalism
  - Why needed here: All ACPBench Hard tasks are defined in STRIPS notation (propositions F, actions A with pre/add/del effects, states s ⊆ F). Understanding state transitions s⟦a⟧ = (s \ del(a)) ∪ add(a) is prerequisite for all 8 tasks.
  - Quick check question: Given state s = {at(truck, loc1), carrying(robot, box)} and action unload with pre={carrying(robot, box), at(robot, loc1)}, add={at(box, loc1)}, del={carrying(robot, box)}, what is s⟦unload⟧ if the action is applicable?

- Concept: PDDL (Planning Domain Definition Language)
  - Why needed here: The benchmark generates questions from 13 PDDL domains. Understanding domain/problem separation, predicates, and action schemas enables interpreting the natural language question contexts.
  - Quick check question: In PDDL, what distinguishes a domain definition from a problem instance?

- Concept: Reachability and Landmarks
  - Why needed here: Reachability determines if a fact can become true via any action sequence; landmarks are facts that must become true in every valid plan. Both are PSPACE-hard but underpin planning heuristics.
  - Quick check question: If proposition P is unreachable from state s, can P be a landmark for any goal containing P?

## Architecture Onboarding

- Component map: PDDL → natural language context + task-specific question → Model receives 2-shot examples + question → generates response → Parser extracts structured answer per task grammar → Validator scores → Aggregate accuracy per task/domain

- Critical path: 1) PDDL → natural language context + task-specific question; 2) Model receives 2-shot examples + question → generates response; 3) Parser extracts structured answer per task grammar; 4) Validator scores: direct comparison (app, prog, val) or planner invocation (reach, areach, land, nexta); 5) Aggregate accuracy per task/domain

- Design tradeoffs:
  - Strict vs. lenient validation: Current design uses exact-match for applicability. Jaccard similarity would reward partial soundness (o1-preview: 44% → 57%) but loses completeness guarantees
  - Cached vs. on-demand computation: Pre-computed landmarks/reachability accelerate evaluation but may miss edge cases the approximations skip
  - 2-shot vs. zero-shot: Examples help format adherence but may bias reasoning patterns

- Failure signatures:
  - Action applicability failure: Model invents non-existent objects (Llama 3.1 8B) or generates infinite action loops (context exhaustion)
  - Action reachability failure: Models default to "None" without verification; only OpenAI models correctly identify "all actions reachable"
  - Validation failure: Off-by-one errors common (o1-preview: 86% of errors within ±1 index)
  - Landmark failure: Models output trivial landmarks (s₀ ∪ s★) which score 0

- First 3 experiments:
  1. Baseline task profiling: Run your model across all 8 tasks with 2-shot prompting. Identify which of the three difficulty tiers (progression/easy, justification-nexta/medium, app-areach-land-reach/hard) your model falls into per task.
  2. Format ablation: Compare boolean, multiple-choice, and generative performance on the same task (e.g., progression) to quantify the generative gap. Use Figure 5 methodology.
  3. Domain generalization test: Evaluate on high-performing domains (depot, swap, visitall) vs. low-performing (alfworld, floortile, grippers) to diagnose whether failures are domain-specific or task-inherent.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning on generative data with Chain-of-Thought (CoT) annotations significantly improve model performance on ACPBench Hard tasks compared to models trained on boolean or multiple-choice data?
- Basis in paper: The conclusion states that "Creating training data for generative questions, possibly with a chain of thought, would therefore be a promising area of future research," noting that models trained on the original ACPBench failed to adapt to the new format.
- Why unresolved: Preliminary experiments with models fine-tuned on the previous multiple-choice dataset performed poorly, often defaulting to "yes/no" or "A/B/C/D" answers unsuitable for generative tasks.
- What evidence would resolve it: A comparative evaluation showing accuracy improvements on ACPBench Hard for models specifically fine-tuned on generative Chain-of-Thought data versus the baselines established in the paper.

### Open Question 2
- Question: Can advanced prompting strategies (e.g., Tree of Thoughts or ReAct) overcome the performance deficits observed with standard few-shot prompting, which was found to be detrimental?
- Basis in paper: The conclusion suggests that "advanced prompting techniques, beyond multiple shots," should be investigated because "multiple shots were often found detrimental in our experiments."
- Why unresolved: The standard approach of providing examples (2-shot) unexpectedly lowered performance or failed to guide models effectively on these open-ended reasoning tasks.
- What evidence would resolve it: An ablation study testing various complex prompting architectures against the zero-shot and 2-shot baselines to identify a method that yields consistent gains across the 8 tasks.

### Open Question 3
- Question: Can the strict "Applicability" task evaluation be relaxed to allow for sound but incomplete action generation without compromising the viability of the model as a planner component?
- Basis in paper: In the "Observations" section, the authors note that if the evaluation allowed for subsets of actions (soundness) rather than exact matches (completeness), the accuracy of top models like o1-preview would jump significantly (e.g., from 44% to 57%).
- Why unresolved: The paper currently penalizes models for missing actions (incompleteness), but it remains unclear if a sound-but-incomplete action set is sufficient for integration into a working planner.
- What evidence would resolve it: A simulation where LLMs are used as heuristics or action pruners in a planner, demonstrating whether high soundness (making no errors) with lower completeness (missing some actions) still results in valid plan generation.

### Open Question 4
- Question: How do state-of-the-art models perform on extended atomic reasoning tasks, such as object counting within a state description?
- Basis in paper: The conclusion explicitly lists "extending the benchmark to additional new tasks such as object counting in a current state" as a specific avenue for future research.
- Why unresolved: The current benchmark focuses on action-oriented reasoning (progression, reachability) and state validation, but does not test the model's ability to perform quantitative state estimation.
- What evidence would resolve it: The creation of a "Counting" task subset within the benchmark and the subsequent evaluation of the model suite to see if they can accurately quantify objects in a given state.

## Limitations

- Prompt template specificity: The study uses "2-shot examples" but does not fully specify the prompt templates or example selection process, which could significantly affect model performance across tasks.
- Domain selection bias: The 13 PDDL domains include some (depot, swap, visitall) where models perform relatively well, but others (alfworld, floortile, grippers) where performance is consistently poor, potentially not representing general planning challenges.
- Parser leniency vs strictness trade-off: The grammar-based parser is designed to extract structured answers, but its strictness level significantly impacts scores, with Jaccard similarity potentially improving scores by 13 percentage points.

## Confidence

- High confidence: The core observation that generative reasoning tasks are systematically harder than boolean/multiple-choice formats is well-supported by direct experimental evidence across 14 models and 8 tasks.
- Medium confidence: The claim that action reachability is significantly harder than progression due to multi-atom precondition reasoning is plausible but could be confounded by training data distribution or model-specific weaknesses.
- Medium confidence: The assertion that atomic task performance could enable modular planner integration assumes error propagation between components won't compound, which requires empirical validation.

## Next Checks

1. Template ablation study: Systematically vary the 2-shot prompt templates (example selection, format, context length) to determine how much performance variance stems from prompt engineering versus fundamental reasoning capabilities.

2. End-to-end planner integration test: Take the highest-performing models on individual atomic tasks and chain them together in a modular planner architecture. Measure whether strong atomic performance translates to competent overall planning or whether error propagation degrades results.

3. Domain generalization benchmark: Create a held-out PDDL domain not in the original 13 and evaluate whether performance patterns (easy/medium/hard task clusters) persist, or whether failures are primarily domain-specific rather than task-inherent.