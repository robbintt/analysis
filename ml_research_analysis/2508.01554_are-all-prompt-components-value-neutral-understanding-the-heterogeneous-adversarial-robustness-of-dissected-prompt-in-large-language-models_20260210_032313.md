---
ver: rpa2
title: Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial
  Robustness of Dissected Prompt in Large Language Models
arxiv_id: '2508.01554'
source_url: https://arxiv.org/abs/2508.01554
tags:
- prompt
- output
- password
- examples
- components
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework to systematically dissect LLM
  prompts into functional components and measure their heterogeneous vulnerability
  to adversarial perturbations. The authors propose PromptAnatomy to parse prompts
  into five canonical components and ComPerturb to apply targeted attacks on each
  component.
---

# Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models

## Quick Facts
- arXiv ID: 2508.01554
- Source URL: https://arxiv.org/abs/2508.01554
- Reference count: 24
- This paper introduces a framework to systematically dissect LLM prompts into functional components and measure their heterogeneous vulnerability to adversarial perturbations, showing component-aware attacks outperform baselines with 15.4% ASR improvement.

## Executive Summary
This work challenges the assumption that all prompt components contribute equally to LLM robustness by introducing a systematic framework to dissect prompts into five functional components and evaluate their vulnerability to targeted adversarial perturbations. The authors develop PromptAnatomy to parse prompts into Role, Directive, Additional Information, Output Formatting, and Examples components, then apply ComPerturb to generate component-specific attacks. Across four datasets and five LLMs, they demonstrate that Directive and Additional Information components are significantly more vulnerable to perturbations than Role and Output Formatting, with semantic perturbations proving more effective than syntactic ones.

## Method Summary
The paper introduces PromptAnatomy, a two-stage dissection pipeline that segments prompts into sentences, constructs context windows (±2 sentences), and uses an LLM classifier to categorize each sentence into one of five components. ComPerturb then applies five perturbation strategies (Special Character Insertion, Synonym Replacement, Word Deletion, Sentence Rewriting, Component Deletion) at 10% rate to targeted components, calculating perplexity ratios to filter adversarial samples. The framework evaluates Attack Success Rate across four datasets (PubMedQA, EMEA, Leetcode, CodeGeneration) using five LLMs (GPT-4o, Claude3.7, Qwen2.5-14B, LLaMA3.3-70B, Gemma3.12B), with task-specific success criteria (BLEU < 20 for translation, wrong answers for classification, code correctness for generation).

## Key Results
- Component-aware attacks achieve 15.4% higher average ASR compared to existing baselines
- Directive and Additional Information components show highest vulnerability (up to 87.9% ASR under Component Deletion)
- Semantic perturbations (synonym replacement, sentence rewriting, component deletion) outperform syntactic perturbations (special character insertion)
- Attack effectiveness increases with prompt structural complexity (PCM scores), showing 29.6% improvement on complex CodeGeneration prompts vs 17.76% on simpler PubMedQA

## Why This Works (Mechanism)

### Mechanism 1: Component-Targeted Semantic Vulnerability
- Claim: Directive and Additional Information components are disproportionately vulnerable to adversarial perturbation compared to Role and Output Formatting
- Mechanism: DIR and ADI carry task-defining semantics and contextual constraints. Perturbations here disrupt the model's task comprehension directly, whereas ROL and OFT serve auxiliary formatting functions that are less semantically load-bearing
- Core assumption: LLMs process prompt components asymmetrically based on semantic importance, not uniformly as flat text
- Evidence anchors:
  - [abstract] "components like DIR and ADI are more susceptible to perturbations, while ROL and OFT remain relatively robust"
  - [Table 2] DIR shows ASR up to 87.9% under Component Deletion across models; ROL consistently shows lower ASRs
  - [corpus] Limited direct validation. Related work on system prompt robustness (arXiv:2502.12197) examines structural prompts but does not test component-wise vulnerability specifically
- Break condition: If task instructions were embedded across multiple components redundantly, DIR-targeted attacks would show diminished relative effectiveness

### Mechanism 2: Semantic Perturbation Superiority Over Syntactic
- Claim: Semantic perturbations achieve higher ASR than syntactic perturbations
- Mechanism: Meaning-level changes disrupt the model's representation of task intent more fundamentally than surface-level noise, which LLMs can often denoise through contextual inference
- Core assumption: Models have robustness mechanisms for syntactic noise (from training data diversity) but weaker defenses against semantic drift
- Evidence anchors:
  - [abstract] "semantic perturbations are more effective than syntactic ones"
  - [Figure 3 heatmap] SER and COD show higher ASR than SCI across components and models
  - [corpus] No direct corpus validation of this semantic-syntactic distinction in prompt attack contexts
- Break condition: If models were fine-tuned with semantic augmentation specifically, the gap between semantic and syntactic attack effectiveness would narrow

### Mechanism 3: Complexity-Amplified Attack Effectiveness
- Claim: Component-aware perturbations achieve greater relative improvement on structurally complex prompts
- Mechanism: Complex prompts contain more semantically dense instructions distributed across components, providing more attack surface. Component-aware targeting exploits this density more efficiently than monolithic perturbation
- Core assumption: Prompt complexity correlates with semantic information distribution across components
- Evidence anchors:
  - [Table 1] CodeGeneration-PA (PCM=44.524) shows AVG↑=29.6% improvement; PubMedQA-PA (PCM=34.883) shows AVG↑=17.76%
  - [Section 5] "ComPerturb performs best on structurally rich prompts"
  - [corpus] Related work on RAG adversarial prompts (arXiv:2509.15159) examines complex retrieval pipelines but does not quantify prompt structural complexity effects
- Break condition: If complex prompts used redundant instruction across components, targeted attacks would show diminishing returns

## Foundational Learning

- Concept: Prompt Component Taxonomy
  - Why needed here: The entire framework depends on valid decomposition. Without understanding that prompts have functional structure (role, directive, context, format, examples), component-wise analysis is meaningless
  - Quick check question: Given "You are a translator. Translate this to French. Be precise. Output in JSON.", can you label each sentence by component type?

- Concept: Perplexity-Based Filtering
  - Why needed here: ComPerturb uses PPL ratio to filter adversarial samples. Understanding why high-PPL perturbations may be rejected (distribution shift concerns) is critical for interpreting results
  - Quick check question: Why might a perturbation that drastically changes perplexity be excluded from the final adversarial set?

- Concept: Attack Success Rate (ASR) Formulation
  - Why needed here: ASR is the core metric, but it's task-specific (BLEU < 20 for translation, wrong answer for classification, code correctness for generation)
  - Quick check question: For a sentiment classification task where the correct answer is "POSITIVE", what outcomes would count as a successful attack?

## Architecture Onboarding

- Component map:
  - Prompt input -> Sentence segmentation -> Context window construction (±2 sentences) -> LLM classification into 5 components -> XML-tagged output
  - Perturbation application (5 types) -> PPL ratio computation -> Top-20% filtering -> ASR evaluation

- Critical path: Prompt input → dissection accuracy → perturbation targeting → PPL filtering → ASR measurement. Dissection errors propagate; if DIR is misclassified as ROL, targeted attacks will hit the wrong semantic content

- Design tradeoffs:
  - Two-stage dissection with context windows improves accuracy but adds computational overhead vs. single-pass classification
  - PPL filtering removes low-impact perturbations but may also filter out subtle semantic attacks that happen to have moderate perplexity
  - Component deletion (COD) achieves highest ASR but is least realistic as an adversarial scenario

- Failure signatures:
  - Dissection accuracy drops on overlapping components (e.g., "Use the following format" could be Directive or Output Formatting)
  - ASR variability across models suggests architecture-specific vulnerabilities not captured by component taxonomy alone
  - Low baseline ASR on simple prompts (PCM < 35) indicates ceiling effects

- First 3 experiments:
  1. Validate dissection accuracy on your domain-specific prompts by sampling 20 prompts and comparing PromptAnatomy output to human annotation
  2. Run ablation on PPL filtering threshold (try 10%, 20%, 30% retention) to test sensitivity of adversarial sample quality
  3. Test transferability: apply ComPerturb perturbations generated from GPT-4o dissection to other target models to assess cross-model robustness patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does component-aware adversarial training effectively harden "Directive" and "Additional Information" components without compromising the model's utility on clean inputs?
- Basis in paper: [inferred] The authors propose Guideline #2 for developers to "focus defenses on semantically critical elements," but the experiments only evaluate attack success rates rather than defense mechanisms
- Why unresolved: The paper identifies vulnerable components but does not test if targeted data augmentation or fine-tuning can mitigate these specific structural weaknesses
- What evidence would resolve it: Experiments fine-tuning LLMs using data augmented with ComPerturb, measuring both ASR reduction and performance retention on clean datasets

### Open Question 2
- Question: How robust is the PromptAnatomy dissection pipeline when processing prompts with overlapping or implicitly expressed functional components?
- Basis in paper: [inferred] Section 3.1 highlights the challenge of "disentangling overlapping or implicitly expressed components," noting that accuracy drops for smaller models (e.g., 88% for Gemma3-12B)
- Why unresolved: The method relies heavily on GPT-4o for high-accuracy dissection, but the reliability of structural decomposition for open-weight models on ambiguous text remains uncertain
- What evidence would resolve it: Evaluating dissection accuracy on a curated dataset of structurally ambiguous prompts across various open-weight LLMs

### Open Question 3
- Question: Do the identified heterogeneous vulnerabilities persist in multi-turn, agentic workflows where prompt context accumulates dynamically?
- Basis in paper: [inferred] The Related Works section distinguishes this work from "LLM-based agents," and the experiments are restricted to static, single-turn inputs
- Why unresolved: It is unclear if the fragility of specific components (like Directive) remains consistent when the model processes a history of previous turns or tool outputs
- What evidence would resolve it: Applying ComPerturb to multi-turn dialogue or agent benchmarks to analyze if component vulnerability shifts with context length

## Limitations
- Component taxonomy assumes all prompts naturally decompose into five canonical types, which may not hold for domain-specific or highly complex prompts with overlapping components
- Perplexity filtering sensitivity: choice of reference model and threshold significantly impacts which perturbations are considered effective, with no ablation studies on threshold selection
- Task-specific success criteria use somewhat arbitrary thresholds (BLEU < 20, code correctness) that may not capture all forms of successful attacks

## Confidence

**High Confidence**: The core finding that component-aware attacks outperform baseline attacks (AVG↑=15.4%) is well-supported by extensive experimental results across four datasets and five LLMs

**Medium Confidence**: The observation that Directive and Additional Information components are more vulnerable than Role and Output Formatting is well-supported, but assumes semantic load-bearing determines vulnerability

**Medium Confidence**: The superiority of semantic over syntactic perturbations is demonstrated empirically, but lacks theoretical justification for why LLMs would be more robust to syntactic noise

## Next Checks

1. Cross-Dataset Dissection Validation: Test PromptAnatomy's component classification accuracy on prompts from domains not represented in the current datasets (e.g., medical, legal, or creative writing prompts) to verify taxonomy generalizability

2. Threshold Sensitivity Analysis: Conduct ablation studies on the PPL filtering threshold (10%, 20%, 30%, 40%) to quantify how threshold selection impacts ASR results and identify potential overfitting to the 20% cutoff

3. Transferability Testing: Apply perturbations generated from one model's dissection (e.g., GPT-4o) to other models (Claude3.7, LLaMA3.3-70B) to assess whether component vulnerabilities are architecture-specific or more general patterns