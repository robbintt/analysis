---
ver: rpa2
title: A State-of-the-Art SQL Reasoning Model using RLVR
arxiv_id: '2509.21459'
source_url: https://arxiv.org/abs/2509.21459
tags:
- rlvr
- data
- bird
- arxiv
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work demonstrates how reinforcement learning with verifiable
  rewards (RLVR) can produce state-of-the-art reasoning models for text-to-SQL tasks
  without proprietary models or extra data. The method uses a two-stage fine-tuning
  pipeline: an offline warm-up with TAO followed by online RLVR, both using simple
  execution-based rewards.'
---

# A State-of-the-Art SQL Reasoning Model using RLVR

## Quick Facts
- **arXiv ID**: 2509.21459
- **Source URL**: https://arxiv.org/abs/2509.21459
- **Reference count**: 40
- **Primary result**: 73.56% exact-match accuracy without self-consistency, 75.68% with self-consistency on BIRD benchmark

## Executive Summary
This work demonstrates how reinforcement learning with verifiable rewards (RLVR) can produce state-of-the-art reasoning models for text-to-SQL tasks without proprietary models or extra data. The method uses a two-stage fine-tuning pipeline: an offline warm-up with TAO followed by online RLVR, both using simple execution-based rewards. Applied to the BIRD benchmark, the resulting model achieved 73.56% exact-match accuracy without self-consistency and 75.68% with self-consistency, outperforming existing methods while requiring fewer generations. The approach generalizes well and can be broadly applied to enterprise reasoning tasks.

## Method Summary
The approach employs a two-stage fine-tuning pipeline on the Qwen 2.5 32B Coder Instruct model. First, TAO (offline RL) generates multiple responses per training example, computes execution rewards, and performs offline optimization to establish strong inductive bias. Second, online RLVR refines the policy through live exploration using a modified GRPO approach with lower learning rates and minimal KL penalty. The method uses simple binary execution rewards with syntax penalties, avoiding complex reward shaping. Inference employs self-consistency with 7 generations and parameter-free weighted majority voting.

## Key Results
- Achieved 73.56% exact-match accuracy without self-consistency on BIRD benchmark
- Reached 75.68% exact-match accuracy with self-consistency using only 7 generations
- Outperformed existing methods while requiring fewer generations (7 vs 8-32)
- Open-source models (Qwen 2.5 Coder) outperformed proprietary models (GPT-4o, O3, Claude Sonnet) on this task

## Why This Works (Mechanism)

### Mechanism 1
A two-stage offline-to-online RL curriculum provides better optimization than direct online RL for SQL reasoning tasks. TAO first establishes strong inductive bias before online RLVR refines the policy through live exploration. Limiting TAO iterations prevents over-training that would reduce gains from the online stage.

### Mechanism 2
Simple binary execution rewards with syntax penalties are sufficient for SQL reasoning without complex reward shaping. The 0-1 execution match directly measures task success; the -1 syntax penalty provides negative feedback for invalid SQL. Reward shaping experiments yielded no significant improvements.

### Mechanism 3
Code-specialized open models can outperform larger proprietary models on SQL reasoning when properly fine-tuned. Domain-specific pre-training provides stronger priors for code generation than general-purpose capabilities, and RLVR further specializes this knowledge.

## Foundational Learning

- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Why needed - Core paradigm distinguishing this from RLHF; rewards come from objective execution, not trained preference models. Quick check - Explain why SQL execution accuracy is "verifiable" but essay quality assessment is not.

- **Offline vs. Online Reinforcement Learning**: Why needed - Understanding why TAO precedes online RLVR determines how you schedule training stages. Quick check - What's the difference between learning from a fixed collected dataset (TAO) vs. learning from live model outputs (online RLVR)?

- **Test-Time Compute / Self-Consistency**: Why needed - Final accuracy boost comes from generating multiple candidates and voting; understanding when this helps vs. adds cost. Quick check - Why would generating 7 responses and selecting the most consistent one improve accuracy over a single generation?

## Architecture Onboarding

- **Component map**: Base model (Qwen 2.5 32B Coder Instruct) -> Modified OmniSQL prompt -> TAO warm-up -> Online RLVR -> Self-consistency inference

- **Critical path**: Prompt/model selection (dev set) -> limited TAO iterations -> online RLVR -> inference with self-consistency

- **Design tradeoffs**: TAO duration (more TAO reduces online RL gains) vs. KL penalty (removing it didn't hurt performance) vs. self-consistency count (7 generations sufficient vs. competitors' 8-32)

- **Failure signatures**: TAO over-training (stagnant or degraded online RL improvements) vs. missing reasoning traces (required explicit prompt instructions) vs. wrong model selection (proprietary models underperformed on this task)

- **First 3 experiments**:
  1. Baseline evaluation: Run Qwen 2.5 32B Coder on BIRD dev set with your prompt to establish starting accuracy.
  2. TAO-only ablation: Apply limited TAO training, measure dev set improvement, confirm warm-up benefit.
  3. Full pipeline validation: Complete TAO + online RLVR, compare single-call (73.56% target) vs. self-consistency with 7 generations (75.68% target).

## Open Questions the Paper Calls Out

### Open Question 1
Why do open-source models (Qwen, Llama) outperform proprietary models (GPT-4o, O3, Claude Sonnet) on the text-to-SQL task? The authors identify this phenomenon but don't investigate underlying causes such as training data composition, code-specific pre-training, or prompt compatibility.

### Open Question 2
What is the optimal trade-off between TAO warm-up duration and subsequent online RLVR training effectiveness? The paper reports using "limited" TAO training without characterizing the curve of diminishing returns or identifying the optimal transition point.

### Open Question 3
Can more sophisticated reward shaping beyond the simple 0-1 execution metric with syntax penalty significantly improve RLVR performance for text-to-SQL? The negative finding is reported briefly without details on what shaping approaches were tried.

## Limitations

- TAO algorithm references a Databricks blog rather than a published paper, making exact reproduction difficult
- Modified OmniSQL prompt and specific RLVR improvements over GRPO are described but not provided
- Findings on proprietary model underperformance are domain-specific and may not generalize to other reasoning tasks
- Reliance on execution-based rewards assumes clean, unambiguous evaluation

## Confidence

**High Confidence (95%+):** Two-stage offline-to-online RL curriculum improves SQL reasoning performance; self-consistency mechanism with 7 generations provides measurable accuracy gains; simple binary execution rewards with syntax penalties are sufficient.

**Medium Confidence (70-85%):** Domain-specific pre-training (Qwen 2.5 Coder) outperforms larger proprietary models on SQL reasoning; specific RLVR improvements over GRPO meaningfully improve training dynamics; 73.56% and 75.68% accuracy targets are achievable.

**Low Confidence (50-65%):** Approach generalizes well to other enterprise reasoning tasks beyond SQL; finding that proprietary models underperform extends beyond BIRD benchmark; specific stopping criteria and hyperparameters are optimal.

## Next Checks

1. **Prompt Reconstruction Validation**: Reconstruct the modified OmniSQL prompt from described changes and validate whether it enables reasoning traces in model outputs on BIRD dev set.

2. **TAO Algorithm Specification**: Implement TAO using standard offline RL techniques as a proxy, then systematically vary iterations to confirm over-training reduces subsequent online RLVR gains.

3. **Cross-Domain Generalization Test**: Apply the exact pipeline to a different verifiable reasoning task (e.g., mathematical problem solving or code generation) to test whether the approach generalizes beyond SQL reasoning.