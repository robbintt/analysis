---
ver: rpa2
title: 'COSTAR-A: A prompting framework for enhancing Large Language Model performance
  on Point-of-View questions'
arxiv_id: '2510.12637'
source_url: https://arxiv.org/abs/2510.12637
tags:
- prompt
- costar
- questions
- llms
- outputs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COSTAR-A, an enhanced prompting framework
  that adds an explicit "Answer" directive to the original COSTAR method for guiding
  large language model outputs. The study addresses the challenge of generating consistent,
  decisive responses from smaller, locally deployed language models when constrained
  by short token limits.
---

# COSTAR-A: A prompting framework for enhancing Large Language Model performance on Point-of-View questions

## Quick Facts
- **arXiv ID**: 2510.12637
- **Source URL**: https://arxiv.org/abs/2510.12637
- **Reference count**: 3
- **Primary result**: COSTAR-A significantly improves decisiveness for specific small models (Yi-Coder-1.5B achieved 100% single-selection accuracy in POV tasks) but shows model-specific variability, with some models (Qwen2.5-3B) degrading when switching from COSTAR to COSTAR-A.

## Executive Summary
This paper introduces COSTAR-A, an enhanced prompting framework that adds an explicit "Answer" directive to the original COSTAR method for guiding large language model outputs. The study addresses the challenge of generating consistent, decisive responses from smaller, locally deployed language models when constrained by short token limits. Using five small models (1.3B–3.8B parameters), the authors compare open-ended and point-of-view (POV) tasks under Context-Aware Prompt (CAP), COSTAR, and COSTAR-A frameworks. Results show that while COSTAR improves structure and self-omission, it sometimes fails to produce decisive POV outputs. COSTAR-A significantly increased decisiveness for the Llama 3.1-8B and Yi-Coder-1.5B models, with Yi-Coder-1.5B achieving 100% single-selection accuracy in POV tasks. However, improvements were model-specific, highlighting the need for tailored prompt strategies in resource-constrained environments.

## Method Summary
The authors conducted inference-only experiments comparing three prompting frameworks (CAP, COSTAR, COSTAR-A) across five small language models (1.3B–8B parameters) on two task types: open-ended questions (factual, contextual, opinion, numerical, summarization) and point-of-view demographic questions. Models were constrained to 50 new tokens per generation with deterministic sampling. Part A evaluated correctness, completion time, and self-omission across frameworks. Part B focused on decisiveness (% single-option selection) for POV tasks, introducing COSTAR-A's explicit "Answer" directive for models that failed to produce coherent outputs with standard COSTAR.

## Key Results
- COSTAR-A achieved 100% decisiveness for Yi-Coder-1.5B on POV tasks, compared to 0% with standard COSTAR
- Llama 3.1-8B showed significant improvement in decisiveness with COSTAR-A versus COSTAR
- Qwen2.5-3B's decisiveness dropped from 85.7% with COSTAR to 28.6% with COSTAR-A, demonstrating model-specific variability
- Self-omission improved with COSTAR across models, though token budget constraints sometimes limited answer completeness

## Why This Works (Mechanism)

### Mechanism 1: Forced Commitment via Explicit Termination
Adding an explicit "Answer" directive to a structured prompt increases the probability of a decisive, single-choice output in POV tasks for specific small models that require strong termination signals. The "Answer" component acts as a syntactic anchor or "stop token" logic guide, forcing the model to exit its reasoning loop and output a concrete value, reducing null outputs or hedging. This mechanism fails if the model interprets the "Answer" header as context continuation rather than a command to generate.

### Mechanism 2: Token-Budget Interference
Highly structured prompting frameworks can degrade performance in resource-constrained environments by consuming the output token budget. Small models operating under strict token limits (e.g., 50 tokens) may struggle if the prompt structure is verbose, allocating processing power to parsing the structure and leaving insufficient capacity to generate a full response. This mechanism is irrelevant if token limits are expanded significantly or if models prioritize output generation regardless of length.

### Mechanism 3: Inverted Instruction Sensitivity
Increasing prompt structure/complexity can negatively impact decisiveness for models fine-tuned on specific instruction formats. Not all small models benefit from added directives; some may be over-parameterized for their fine-tuning data. Adding an extra "Answer" component might confuse models that expect the "Response" component to be the final instruction, or trigger verbose explanation styles in models trained to be "helpful" rather than "decisive."

## Foundational Learning

- **Concept: Context-Aware Prompting (CAP) vs. Structured Prompting (COSTAR)**
  - Why needed here: The paper positions COSTAR-A as an evolution of CAP. Understanding the baseline (CAP: Question + Context + Answer) is necessary to see why adding structure helps "self-omission" but hurts token efficiency.
  - Quick check question: If a model fails to omit its AI identity using CAP, which component of COSTAR is most likely addressing this behavior? (Answer: Style/Tone constraints).

- **Concept: Self-Omission in Synthetic Data**
  - Why needed here: A primary evaluation metric is "self-omission" (not revealing AI identity). This is critical for the paper's use case (simulating human respondents).
  - Quick check question: Does higher correctness always correlate with better self-omission? (Answer: No, Table 7 shows CAP had better correctness for Deepseek, but COSTAR had better self-omission).

- **Concept: Deterministic Sampling**
  - Why needed here: The authors used deterministic sampling (temperature ~0) to minimize randomness. Understanding this is key to reproducing the "decisiveness" results.
  - Quick check question: Why would a researcher choose deterministic sampling when testing POV questions? (Answer: To ensure the model's choice is a deterministic function of the prompt persona, not random chance).

## Architecture Onboarding

- **Component map:**
  - CAP: [Question] + [Context] + [Answer]
  - COSTAR: [Context] + [Objective] + [Style] + [Tone] + [Audience] + [Response]
  - COSTAR-A: [COSTAR Components] + [Answer] (Explicit directive to output the final value)

- **Critical path:**
  1. Select a small model (1.5B–8B parameters)
  2. Define a persona (e.g., "Jake, 20-year-old student")
  3. Construct COSTAR context components
  4. **Critical Step:** Append the "Answer" directive specifically for models prone to null outputs (like Llama 3.1 or Yi-Coder)
  5. Constrain output to 50 tokens

- **Design tradeoffs:**
  - Structure vs. Latency: COSTAR improves self-omission but increases completion time (e.g., Qwen took 173s for COSTAR vs 54s for CAP)
  - Clarity vs. Decisiveness: COSTAR provides clarity but may reduce decisiveness compared to COSTAR-A for specific models, or reduce it for others (Qwen)

- **Failure signatures:**
  - The "Parrot" Failure: Model repeats the prompt verbatim (observed in Llama 3.1 with COSTAR without streamer)
  - The "Hedging" Failure: Model lists multiple options or refuses to choose (observed in Yi-Coder with standard COSTAR)
  - The "Verbose" Failure: Model explains the answer rather than stating it (observed in Qwen with COSTAR-A)

- **First 3 experiments:**
  1. **Baseline Validation (Part A):** Run 5 different small models on 5 open-ended questions using CAP vs. COSTAR. Measure "Self-Omission" scores to identify which models can sustain a persona.
  2. **Null Output Test (Part B):** Take the top 2 models + Llama 3.1. Run POV questions with standard COSTAR. Flag any model that returns empty strings or repeats the prompt (indicating a need for COSTAR-A).
  3. **A/B Decisiveness Test:** For models flagged in Experiment 2, switch to COSTAR-A. Verify if the "Answer" tag fixes the null output (Yi-Coder/Llama) or causes degradation (Qwen).

## Open Questions the Paper Calls Out

- **Open Question 1:** What specific model architecture characteristics predict whether COSTAR-A will improve or degrade performance compared to the original COSTAR framework?
  - Basis: The authors note COSTAR-A improved Yi-Coder-1.5B and Llama3.1-8B but degraded Qwen2.5-3B's decisiveness (85.7% → 28.6%), concluding "improvements were model-specific."
  - Why unresolved: The study tested only five models without analyzing architectural features that might explain why some models respond positively to the explicit "Answer" directive while others do not.
  - What evidence would resolve it: Systematic evaluation across a larger, diverse set of localized LLMs with regression analysis correlating architectural attributes with COSTAR-A effectiveness.

- **Open Question 2:** How does COSTAR-A perform on more complex or dynamic task environments beyond the limited question types tested?
  - Basis: The authors state results may not fully capture behavior in more complex or dynamic task environments.
  - Why unresolved: Part A used only five open-ended questions and Part B used only seven multiple-choice demographic questions—insufficient for generalizing to multi-turn conversations or real-world deployment scenarios.
  - What evidence would resolve it: Evaluation across diverse task types (multi-turn dialogue, code generation, domain-specific QA, agentic workflows) with statistically significant sample sizes.

- **Open Question 3:** Does GPU optimization or relaxed token constraints change the relative effectiveness of COSTAR versus COSTAR-A?
  - Basis: The authors acknowledge that token limits and lack of GPU optimization may have influenced model responsiveness.
  - Why unresolved: The 50-token output limit and CPU-only inference may have artificially constrained some models, making it unclear whether COSTAR-A's benefits persist when models have more generous generation budgets or hardware acceleration.
  - What evidence would resolve it: Controlled experiments varying token limits (50, 128, 256, 512) and comparing CPU versus GPU inference.

- **Open Question 4:** How does COSTAR-A compare to other established prompting techniques such as Chain-of-Thought, few-shot prompting, or constrained decoding methods for POV tasks?
  - Basis: The paper only compares COSTAR-A against COSTAR and CAP, despite acknowledging advanced methods like Chain-of-Thought and Active Prompting.
  - Why unresolved: The relative efficacy of COSTAR-A versus these other techniques remains unknown, particularly for decisiveness in POV tasks where constrained decoding or logit filtering might achieve similar results.
  - What evidence would resolve it: Head-to-head comparison of COSTAR-A against CoT, few-shot, and constrained decoding methods on the same POV task benchmark using identical models and evaluation metrics.

## Limitations

- Model-specific variability remains the dominant limitation, with effectiveness varying dramatically based on model architecture and fine-tuning strategy
- Token budget constraints create artificial conditions that may not reflect realistic deployment scenarios
- Evaluation metrics lack nuance, using binary correctness and self-omission that may oversimplify complex language generation behaviors

## Confidence

- **High Confidence:** The core finding that COSTAR-A improves decisiveness for specific models (Yi-Coder-1.5B, Llama 3.1-8B) is well-supported by the data
- **Medium Confidence:** The claim that structured prompting can degrade performance due to token-budget interference is plausible but requires additional validation across different token limits and model families
- **Low Confidence:** The assertion that COSTAR-A's benefits are universally applicable to resource-constrained environments is not supported by the data due to model-specific failures

## Next Checks

1. **Cross-token-limit validation:** Replicate experiments with varying token budgets (25, 50, 100, 200 tokens) to determine whether observed improvements are artifacts of the strict 50-token constraint or persist under more realistic conditions.

2. **Fine-tuning intervention test:** Fine-tune a subset of models (Yi-Coder and Qwen) on instruction-following datasets that include explicit "Answer" termination signals, then re-run the POV tasks to test whether observed model-specific differences are due to training data gaps rather than fundamental architectural differences.

3. **Prompt ablation study:** Systematically remove individual components from COSTAR-A (Context, Objective, Style, Tone, Audience, Response, Answer) and measure their individual contributions to correctness, self-omission, and decisiveness to identify which structural elements are truly necessary versus which create unnecessary complexity or token overhead.