---
ver: rpa2
title: On the Mathematical Impossibility of Safe Universal Approximators
arxiv_id: '2507.03031'
source_url: https://arxiv.org/abs/2507.03031
tags:
- universal
- catastrophic
- network
- networks
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes fundamental mathematical limits on the controllability
  of universal approximators by proving that catastrophic failures are an inescapable
  feature of any useful computational system. The authors demonstrate that for any
  universal approximator complex enough to be useful, perfect reliable control is
  mathematically impossible due to dense sets of instabilities that are inextricably
  linked to the system's expressive power.
---

# On the Mathematical Impossibility of Safe Universal Approximators

## Quick Facts
- arXiv ID: 2507.03031
- Source URL: https://arxiv.org/abs/2507.03031
- Authors: Jasper Yao
- Reference count: 20
- Key outcome: Proves that perfect reliable control of universal approximators is mathematically impossible due to dense sets of instabilities inextricably linked to expressive power.

## Executive Summary
This paper establishes fundamental mathematical limits on the controllability of universal approximators by proving that catastrophic failures are an inescapable feature of any useful computational system. The authors demonstrate that for any universal approximator complex enough to be useful, perfect reliable control is mathematically impossible due to dense sets of instabilities that are inextricably linked to the system's expressive power. The core argument proceeds through three complementary levels: combinatorial necessity, topological necessity, and empirical necessity, each showing that catastrophe density is directly proportional to expressive power.

## Method Summary
The paper employs mathematical proofs across three pillars: (1) combinatorial necessity showing ReLU networks have catastrophe density scaling with neuron count through piecewise boundaries, (2) topological necessity using Whitney's singularity theorem to prove generic functions require catastrophic singularities, and (3) empirical necessity linking Fisher Information Matrix pathology to task complexity. Quantitative analysis compares maximum safe complexity thresholds (C₀) against minimum useful complexity (C_min), showing modern systems exceed safe bounds by factors of 10^13 to 10^17.

## Key Results
- For ReLU networks, catastrophe density ρ(δ) scales exponentially with complexity: ρ(δ) ≥ 1 − exp(−αC/δ^d)
- Modern systems like GPT-4 with ~10^12 parameters have catastrophe density indistinguishable from 1
- Safe complexity threshold C₀ ≈ 10^-5 is exceeded by useful complexity C_min by 10^13 to 10^17 factors
- Perfect alignment is mathematically impossible, reframing safety as operating within irreducible uncontrollability

## Why This Works (Mechanism)

### Mechanism 1: Combinatorial Catastrophe Density in Piecewise-Linear Networks
- Claim: For ReLU networks, catastrophe density scales directly with expressive power.
- Mechanism: Each ReLU neuron creates a hyperplane boundary in input space where the function's derivative discontinuously changes. With N neurons in d-dimensional space, the number of linear regions—and associated catastrophic boundaries—grows combinatorially. The probability of any δ-ball avoiding all boundaries decays exponentially with complexity.
- Core assumption: Catastrophic failures are topologically concentrated at these piecewise boundaries, not distributed uniformly.
- Evidence anchors: Abstract states catastrophe density is directly proportional to network's expressive power; Theorem 5.1 provides explicit bound showing exponential convergence to density 1.

### Mechanism 2: Topological Genericity of Catastrophic Functions
- Claim: Almost all smooth functions (measure 1 in function space) contain dense singularities; universal approximators must replicate this.
- Mechanism: Whitney's singularity theorem establishes that generic smooth functions have dense sets of points where derivatives behave pathologically. Since UAT requires approximating arbitrary continuous functions—and generic functions dominate function space—any universal approximator must be capable of implementing catastrophe-dense behaviors to achieve usefulness on real tasks.
- Core assumption: Real-world tasks require approximating functions from the "generic" (catastrophe-dense) class, not from the measure-zero subset of simple functions.
- Evidence anchors: Abstract states ability to approximate generic functions requires ability to implement dense catastrophic singularities; Theorem 3.1 applies Whitney's theorem to neural function spaces.

### Mechanism 3: Task-Induced Fisher Information Matrix Pathology
- Claim: Complex real-world tasks force trained networks into FIM configurations that mathematically guarantee behavioral instability.
- Mechanism: High mutual information I(X; Y) in complex tasks requires some parameters to become extremely sensitive (large FIM eigenvalues) while others become redundant (near-zero eigenvalues). This creates extreme condition numbers (10^6–10^8), making the natural gradient explosive and guaranteeing that small perturbations cause large behavioral changes.
- Core assumption: The FIM spectral pathology is caused by task complexity, not training artifacts, and directly causes observable catastrophic behavior.
- Evidence anchors: Abstract states universal existence of adversarial examples proves real-world tasks are catastrophic; Theorem 3.5 establishes causal chain from task complexity to FIM pathology to catastrophic instabilities.

## Foundational Learning

- Concept: Universal Approximation Theorem (UAT)
  - Why needed here: The entire argument hinges on what UAT guarantees (capability to represent any continuous function) and what it doesn't guarantee (safety, stability, or control).
  - Quick check question: Can you explain why UAT says nothing about the *density* or *distribution* of problematic behaviors in the approximator?

- Concept: Singularity Theory / Whitney's Theorem
  - Why needed here: The topological necessity argument relies on understanding that "generic" functions (measure 1 in function space) have dense singularities, making smooth/safe functions the exception, not the rule.
  - Quick check question: What does it mean for a property to be "generic" in a function space, and why does this matter for safety arguments?

- Concept: Fisher Information Matrix (FIM) and Eigenvalue Spectra
  - Why needed here: The empirical mechanism links task complexity to behavioral instability through FIM pathology; understanding eigenvalue ratios as sensitivity indicators is essential.
  - Quick check question: If a network has FIM eigenvalues spanning 10^8 decades, what does this imply about uniform sensitivity to parameter perturbations?

## Architecture Onboarding

- Component map: Three Pillars (Combinatorial → Topological → Empirical) form cumulative constraints; Impossibility Sandwich (C_min > C_0) establishes fundamental limit; Cascade of Corollaries shows impossibility inherits to verification, interpretability, etc.

- Critical path: Understanding requires accepting UAT as double-edged sword → recognizing expressive power and catastrophe density are coupled → seeing task complexity forces systems into dangerous regime → concluding no architectural escape exists.

- Design tradeoffs: The paper argues there are *no* favorable tradeoffs within UAT systems—only the strategic trilemma: limit capability, accept dense failures, or abandon provable safety guarantees entirely.

- Failure signatures: Claims of "provably safe" universal approximators without restricting capability below C_0; proposals for verification systems that don't acknowledge they inherit same impossibilities; arguments that smooth activations escape combinatorial proof (topological proof still applies).

- First 3 experiments:
  1. Catastrophe density measurement: For ReLU network of varying sizes, estimate ρ(δ) empirically by sampling input neighborhoods and measuring behavioral discontinuities. Compare to theoretical bounds.
  2. FIM spectral analysis: Train networks on tasks of varying complexity and measure eigenvalue spectra. Test whether condition number correlates with adversarial vulnerability rate.
  3. Safe complexity threshold estimation: Attempt to identify C₀ empirically by finding largest network that can maintain ρ < threshold on given task—verify whether below useful complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can regularization methods be developed that meaningfully reduce FIM pathology without sacrificing universal approximation capability?
- Basis in paper: New methods for regularizing the FIM and avoiding its singularities during training could lead to more stable and robust models.
- Why unresolved: The paper proves FIM pathology is causally linked to task complexity itself, suggesting any regularization that reduces pathology may inherently limit capability.
- What evidence would resolve it: Demonstration of training method that achieves bounded eigenvalue ratios (λmax/λmin < 10^3) while maintaining competitive performance on complex benchmarks.

### Open Question 2
- Question: What concrete safety paradigms can successfully operate under the assumption of irreducible uncontrollability?
- Basis in paper: The development of new safety paradigms that do not rely on the assumption of perfect alignment is a critical area for future work.
- Why unresolved: The paper proves impossibility of perfect alignment but offers only high-level strategic options without operationalized frameworks.
- What evidence would resolve it: Formalization and empirical validation of monitoring, containment, or fail-safe mechanisms that provide statistical safety guarantees without requiring (ε,δ)-safety.

### Open Question 3
- Question: Do the quantitative catastrophe density bounds (α, K constants derived from geometric analysis) match empirical measurements in trained networks?
- Basis in paper: The paper states "conservative estimate is α ≈ log(m)" and "K is related to the volume of the input space, typically normalized to K = 1," suggesting these values are approximations requiring empirical validation.
- Why unresolved: The theoretical bounds may be loose or tight—determining their accuracy is essential for assessing whether real systems actually exceed safety thresholds by 10^13-10^17 factors.
- What evidence would resolve it: Systematic empirical measurement of catastrophe density in trained networks of varying complexity, comparing observed rates to theoretical predictions.

## Limitations

- The central claim depends on whether real-world tasks truly require approximating functions from the "generic" (catastrophe-dense) class; if practical tasks are restricted to a non-generic subclass with sparse singularities, the topological necessity collapses.
- The formal definition of "(ε,δ)-catastrophic behavior" is critical but not fully specified in the provided text, making precise verification difficult.
- The claim that GPT-4 has ρ indistinguishable from 1 requires empirical verification through massive adversarial search, which may be computationally infeasible.

## Confidence

- High confidence in combinatorial catastrophe density - follows directly from ReLU network geometry and is independently verifiable
- Medium confidence in topological necessity - Whitney's theorem is established, but its application to neural networks and the assumption about task function class requires scrutiny
- Medium confidence in empirical necessity - FIM pathology is documented, but the causal link to observable catastrophic behavior needs empirical validation

## Next Checks

1. **Concrete calculation verification**: Reproduce the bound ρ(δ) ≥ 1 - exp(-αC/δ^d) and confirm C₀ ≈ 10^-5 for the stated parameters to ensure the impossibility sandwich is correctly formulated
2. **Task complexity classification**: Empirically measure the mutual information I(X; Y) for diverse real-world tasks to determine if they indeed require the high-complexity regime where catastrophes dominate
3. **Safe subnetwork exploration**: Systematically search for provably safe subnetworks within larger universal approximators to test whether the "dense" catastrophe claim holds on the data manifold rather than input space