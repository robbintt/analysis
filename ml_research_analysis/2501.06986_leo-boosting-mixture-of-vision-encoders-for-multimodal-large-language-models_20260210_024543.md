---
ver: rpa2
title: 'LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language Models'
arxiv_id: '2501.06986'
source_url: https://arxiv.org/abs/2501.06986
tags:
- vision
- visual
- arxiv
- fusion
- encoders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of integrating multiple vision
  encoders in multimodal large language models (MLLMs) to improve visual understanding
  capabilities. The core method introduces LEO, a novel MLLM with a dual-branch vision
  encoder framework that employs a tile-level post-adaptation fusion strategy.
---

# LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2501.06986
- Source URL: https://arxiv.org/abs/2501.06986
- Reference count: 40
- Outperforms state-of-the-art open-source MLLMs on 13 vision-language benchmarks

## Executive Summary
This paper addresses the challenge of integrating multiple vision encoders in multimodal large language models (MLLMs) to improve visual understanding capabilities. LEO introduces a dual-branch vision encoder framework with a novel tile-level post-adaptation fusion strategy. By processing high-resolution images through independent projectors for each encoder and then interleaving visual tokens, LEO achieves state-of-the-art performance across diverse vision-language tasks while maintaining computational efficiency through pixel unshuffling and dynamic tiling.

## Method Summary
LEO processes high-resolution images by dividing them into 448×448 tiles (up to 6 per image), which are independently processed by two specialized vision encoders (InternViT-300M-448px and SAM-L). Pixel unshuffling reduces visual tokens per tile, and the model uses separate MLP projectors for each encoder branch, sequentially interleaving visual tokens before combining them with text tokens for LLM processing. The model is trained in two stages: Stage 1 aligns the vision encoders and projectors while freezing the LLM, and Stage 2 fine-tunes the projectors and LLM while keeping vision encoders frozen.

## Key Results
- Achieves 68.8% on ChartQA, 80.1% on DocVQA, 78.5% on ScienceQA, and 88.0% on POPE
- Outperforms state-of-the-art open-source MLLMs on the majority of 13 vision-language benchmarks
- Shows competitive performance in autonomous driving domain with 61.0% on LingoQA
- Post-adaptation fusion strategy (separate projectors per encoder) consistently outperforms pre-adaptation fusion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-adaptation fusion (separate projectors per encoder) outperforms pre-adaptation fusion (shared projector) for combining diverse vision encoders.
- Mechanism: Each vision encoder receives a dedicated MLP projector that independently maps its features to the LLM embedding space. Visual tokens are then sequentially interleaved at the tile level after adaptation, preserving encoder-specific representations during alignment rather than forcing early feature mixing.
- Core assumption: Different vision encoders learn incompatible feature distributions that benefit from independent alignment pathways before fusion.
- Evidence anchors:
  - [abstract] "LEO...incorporates a post-adaptation fusion strategy...each segmented tile of the input images, LEO sequentially interleaves the visual tokens from its two vision encoders"
  - [section 3.2] "each vision encoder maintains its own dedicated projector module, allowing for independent processing of visual tokens before they are combined. We find this to be a more flexible and effective fusion strategy"
  - [corpus] Related work Mixpert (arXiv:2505.24541) identifies "multimodal learning conflicts" in single-encoder approaches, supporting the hypothesis that encoder diversity requires careful fusion strategies

### Mechanism 2
- Claim: Combining InternViT (vision-language pretrained) with SAM (segmentation-pretrained) provides complementary visual capabilities that exceed either encoder alone.
- Mechanism: InternViT contributes strong OCR and text understanding from large-scale vision-language pretraining. SAM contributes fine-grained spatial and segmentation capabilities. Their complementary strengths emerge through late fusion—InternViT handles text recognition while SAM enhances spatial reasoning.
- Core assumption: Task-specific pretraining creates encoder specializations that persist through projection and combine additively at the LLM level.
- Evidence anchors:
  - [section 4.4, Table 5] InternViT alone achieves 57.0 on TextVQA vs SAM alone at 45.2; SAM alone struggles on text recognition but combined model achieves 68.8
  - [section 4.4] "InternViT alone performs better than SAM alone across all benchmarks, with SAM struggling on tasks like text recognition...when combined with InternViT, SAM enhances performance on this type of task"
  - [corpus] Insufficient direct corpus evidence for this specific encoder combination; related work focuses on CLIP/DINOv2 combinations rather than InternViT/SAM

### Mechanism 3
- Claim: Pixel unshuffling with tile-level processing maintains visual detail while keeping sequence lengths tractable.
- Mechanism: High-resolution images are divided into 448×448 tiles (up to 6 per image). Pixel unshuffling (factors 2 and 4 for respective encoders) reduces tokens per tile to 256 per encoder by spatially rearranging pixels into channel dimensions. This preserves spatial information while reducing the token burden on the LLM.
- Core assumption: Local tile-level processing captures fine details that would be lost in global downsampling, and LLMs can integrate information across interleaved tile sequences.
- Evidence anchors:
  - [section 3.2] "we apply downscaling factors of 2 and 4 for our first and second vision encoders, respectively, reducing the number of visual tokens to 256 per tile and encoder, resulting in a total of 512 visual tokens per tile"
  - [section 4.4, Table 7] Tiling improves Lingo-Judge by 3.4% (61.00 vs 59.02) on autonomous driving tasks
  - [corpus] LEO-MINI (arXiv:2504.04653) extends this work with conditional token reduction, suggesting token reduction is an active research direction with validation

## Foundational Learning

- Concept: **Vision Encoder Pretraining Objectives**
  - Why needed here: Understanding why InternViT excels at OCR while SAM excels at segmentation requires knowing their pretraining tasks—InternViT on vision-language alignment, SAM on promptable segmentation.
  - Quick check question: Can you explain why a segmentation-pretrained encoder might struggle with text recognition compared to a vision-language pretrained encoder?

- Concept: **Pixel Unshuffling vs. Pooling for Token Reduction**
  - Why needed here: The paper uses pixel unshuffling (rearranging spatial pixels to channels) rather than pooling. Understanding this distinction is critical for implementing the token reduction correctly.
  - Quick check question: Given input shape [b, c, w, h] and factor r, what is the output shape after pixel unshuffling, and why might this preserve more information than average pooling?

- Concept: **Sequence Concatenation vs. Channel Concatenation Fusion**
  - Why needed here: Table 6 shows sequence concatenation outperforms channel concatenation for post-adaptation fusion. Understanding the difference helps debug fusion implementations.
  - Quick check question: In sequence concatenation, how are tokens from two encoders combined, and what effect does this have on the attention mechanism compared to channel concatenation?

## Architecture Onboarding

- Component map:
  Input Pipeline -> Dual Vision Encoders -> Token Reduction -> Projectors -> Fusion Block -> LLM

- Critical path:
  1. Image → tile segmentation (max 6 tiles + 1 global thumbnail)
  2. Each tile → both vision encoders in parallel
  3. Each encoder output → pixel unshuffling → 256 tokens
  4. Each token set → dedicated MLP projector
  5. Projected tokens → sequential interleaving per tile
  6. Interleaved visual tokens + text tokens → LLM

- Design tradeoffs:
  - Post- vs. Pre-adaptation fusion: Post-adaptation requires 2× projector parameters but enables encoder-specific alignment; pre-adaptation is cheaper but forces single alignment pathway
  - Tile count vs. context length: More tiles capture more detail but consume context window (6 tiles × 512 tokens = 3072 visual tokens before text)
  - Frozen vs. unfrozen vision encoders: Table 5 shows freezing improves performance; unfreezing SAM reduced SEED by 6.62%

- Failure signatures:
  - Degraded OCR performance: Likely SAM projector not trained properly in phase 1; verify projector learning rate (4×10⁻⁴ for alignment stage)
  - Context overflow errors: Exceeded 8196 tokens; check tile count or reduce unshuffling factor
  - Poor multi-image reasoning: Sequential frames may exceed tile limits; verify max 6 tiles per image constraint
  - Channel concatenation underperformance: If using channel fusion instead of sequence, expect ~1-4% drop on most benchmarks (Table 6)

- First 3 experiments:
  1. Single encoder baselines: Run InternViT-only and SAM-only configurations to verify each encoder's contribution matches Table 5 (should see InternViT >> SAM on TextVQA)
  2. Ablate fusion strategy: Compare sequence concatenation vs. channel concatenation on 3 benchmarks (TextVQA, GQA, MMBench) to validate Table 6 results
  3. Tile count sensitivity: Test with 2, 4, 6 tiles on a high-resolution benchmark (DocVQA or ChartQA) to understand the resolution-performance tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the constraint of the LLM context window be overcome to support higher-resolution images or a larger number of multi-image inputs beyond the current six-patch limit?
- Basis in paper: [explicit] Section 4.6 states the processing capacity is limited to a maximum of six patches due to LLM context length constraints, preventing support for higher-resolution images or more multi-image inputs.
- Why unresolved: The current architecture relies on a fixed context length (8196 tokens), creating a hard ceiling on visual token throughput.
- What evidence would resolve it: Implementing sparse attention mechanisms or extended context windows (e.g., 32k+ tokens) and evaluating performance on high-resolution or multi-image benchmarks.

### Open Question 2
- Question: Does the tile-level post-adaptation fusion strategy maintain its efficiency and performance advantages when scaling to a mixture of more than two vision encoders?
- Basis in paper: [inferred] The paper compares LEO (2 encoders) against Brave (5 encoders) and suggests LEO as a "foundation for advancing hybrid multimodal models," leaving the scalability of the specific post-adaptation architecture an open area.
- Why unresolved: The study only validates a dual-branch setup (InternViT and SAM), leaving the interaction of $N>2$ projectors with sequential interleaving untested.
- What evidence would resolve it: Extending LEO to integrate 3-5 vision experts and comparing performance and latency against pre-adaptation baselines.

### Open Question 3
- Question: How does LEO's performance in autonomous driving tasks improve when expanding temporal reasoning beyond the currently limited two-frame input?
- Basis in paper: [explicit] Section 4.3 notes that "Due to computational limitations, we use only two frames during training," despite the benchmark utilizing sequences of five frames.
- Why unresolved: The full capacity for temporal video reasoning in the autonomous driving domain was restricted by resource constraints during training.
- What evidence would resolve it: Training the model with the full 5-frame sequence from LingoQA and comparing action justification and temporal reasoning scores against the 2-frame baseline.

## Limitations
- Context window constraint limits support for higher-resolution images or multi-image inputs beyond six patches
- Single training run per configuration without variance reporting makes result robustness difficult to assess
- Limited evaluation of computational efficiency beyond noting hardware requirements (8xA100-80GB)

## Confidence

**High Confidence**: The core architectural design (dual-branch vision encoders with post-adaptation fusion) and its implementation details are well-specified and reproducible. The superiority of post-adaptation fusion over pre-adaptation fusion is supported by direct comparison in Table 6 with consistent improvements across multiple benchmarks.

**Medium Confidence**: The complementary strengths of InternViT and SAM encoders are demonstrated through ablation studies, but the evidence relies on a single combination of specific encoder architectures. The claim that pixel unshuffling preserves more information than pooling is plausible but not directly validated against alternative token reduction methods.

**Low Confidence**: The generalization of LEO's performance gains to other LLM backbones, autonomous driving scenarios beyond LingoQA, and real-world deployment conditions remains unproven. The absence of efficiency metrics and variance analysis across runs limits confidence in practical applicability.

## Next Checks

1. **Encoder Diversity Validation**: Implement and evaluate LEO with alternative vision encoder pairs (e.g., CLIP-ViT with DINOv2, or two differently pretrained InternViT variants) to test whether post-adaptation fusion consistently outperforms pre-adaptation when encoder feature distributions differ. Measure performance degradation when using redundant encoders with similar pretraining objectives.

2. **Token Reduction Method Comparison**: Replace pixel unshuffling with alternative token reduction techniques including average pooling, max pooling, and learned token reduction (following LEO-MINI's conditional token reduction). Systematically compare preservation of fine-grained visual details on high-resolution benchmarks like DocVQA and ChartQA while measuring LLM sequence length constraints.

3. **Cross-LLM Generalization Study**: Port the LEO architecture to alternative LLM backbones including both open-source (Llama-3, Qwen-72B) and closed-source (GPT-4V, Gemini) models. Evaluate performance consistency across the 13 benchmarks and measure whether the dual-encoder advantage persists when the underlying attention mechanisms and context handling differ significantly from InternLM2-7B-Chat.