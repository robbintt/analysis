---
ver: rpa2
title: Deep reinforcement learning for optimal trading with partial information
arxiv_id: '2511.00190'
source_url: https://arxiv.org/abs/2511.00190
tags:
- trading
- signal
- agent
- network
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes three deep reinforcement learning algorithms
  for optimal trading under partial information. The trading signal follows an Ornstein-Uhlenbeck
  process with regime-switching parameters.
---

# Deep reinforcement learning for optimal trading with partial information

## Quick Facts
- arXiv ID: 2511.00190
- Source URL: https://arxiv.org/abs/2511.00190
- Reference count: 40
- Three deep RL algorithms for optimal trading under partial information achieve superior performance when posterior regime probabilities are explicitly provided to the agent

## Executive Summary
This paper proposes three deep reinforcement learning algorithms for optimal trading under partial information where the trading signal follows an Ornstein-Uhlenbeck process with regime-switching parameters. The methods integrate GRU networks with DDPG to extract temporal dependencies and latent regime information. A one-step approach (hid-DDPG) encodes GRU hidden states, while two-step approaches either estimate posterior regime probabilities (prob-DDPG) or forecast next signal values (reg-DDPG). Across increasingly complex simulation scenarios and real equity pair trading data, prob-DDPG achieves the highest cumulative rewards and most interpretable trading strategies.

## Method Summary
The paper develops three deep RL frameworks for optimal trading of a mean-reverting signal with latent regime parameters. All methods use DDPG with a GRU encoder to process historical signal data. hid-DDPG trains GRU and DDPG jointly with auxiliary prediction loss, feeding hidden states to the actor. prob-DDPG pre-trains a GRU classifier to estimate posterior regime probabilities, then freezes it while training DDPG on these explicit features. reg-DDPG pre-trains a GRU regressor to forecast next signal values, using these point predictions as features for DDPG. The methods are tested on synthetic OU processes with Markov-switching parameters and real equity pair data.

## Key Results
- prob-DDPG achieves superior cumulative rewards (average 25.65) compared to hid-DDPG (15.70) and reg-DDPG (8.69) in complex simulation scenarios
- prob-DDPG exhibits more interpretable trading strategies with clearer relationships between regime probabilities and position sizing
- reg-DDPG provides limited benefits due to point forecasts failing to capture regime uncertainty needed for optimal positioning

## Why This Works (Mechanism)

### Mechanism 1
Embedding posterior probability estimates of latent regimes substantially improves RL trading performance over direct temporal encoding or point forecasting. The two-step prob-DDPG separates filtering from optimization, allowing the policy to condition actions on structured uncertainty about the latent state rather than compressed hidden representations. Core assumption: regime probabilities capture sufficient information for optimal trading. Evidence: prob-DDPG outperforms other approaches with average 25.65 vs 15.70 and 8.69 cumulative rewards. Break condition: if regime classification accuracy falls below chance or if Markov chain transitions faster than GRU's receptive window.

### Mechanism 2
GRU hidden states encode useful temporal structure of regime-switching signals, providing intermediate performance without explicit filtering. The hid-DDPG trains GRU jointly with DDPG by backpropagating through signal prediction task. Core assumption: GRU can jointly learn to compress regime information and provide useful features for policy optimization. Evidence: hid-DDPG offers intermediate performance with less interpretable strategies. Break condition: if prediction task and policy optimization objectives conflict, limiting performance.

### Mechanism 3
Direct next-signal forecasting provides limited trading benefit because point predictions fail to capture regime uncertainty needed for positioning. The reg-DDPG trains GRU regressor to predict next value, but without regime awareness cannot optimally size positions based on mean-reversion confidence. Core assumption: next-signal forecast contains sufficient information for profitable positions. Evidence: reg-DDPG provides limited benefits with significantly lower rewards. Break condition: when volatility regimes switch or κt varies, next-value MSE loss provides noisy supervision causing mechanism to collapse toward near-zero rewards.

## Foundational Learning

- **Ornstein-Uhlenbeck (OU) process with regime switching**: The trading signal follows dSt = κt(θt - St)dt + σt dWt where parameters follow independent Markov chains. Understanding mean reversion and regime dynamics is essential to interpret what GRU and agent are learning. Quick check: If θt switches between {0.9, 1.0, 1.1} with equal probability every 10 steps, how should optimal inventory change when St = 0.85 vs St = 1.15?

- **Deep Deterministic Policy Gradient (DDPG) and actor-critic architecture**: The agent uses DDPG with separate actor (policy π) and critic (Q-value) networks. Understanding loss functions L1 (critic) and L2 (actor) is required to debug training and diagnose divergence. Quick check: Why does critic use target network Qtgt with soft updates, and what happens if target network is updated too frequently?

- **Gated Recurrent Units (GRU) and sequence modeling**: The GRU encodes temporal dependencies via reset (pk) and update (zk) gates. Understanding how hidden states evolve across lookback window W explains what information is available to policy. Quick check: If W=10 and regime typically persists for 50+ steps, what risk does GRU face in learning regime transitions?

## Architecture Onboarding

- **Component map**: Signal Generator -> GRU Encoder -> DDPG Actor-Critic -> Target Networks
- **Critical path**: (1) For two-step approaches, pre-train GRU classifier/regressor on signal histories with ground-truth regimes (synthetic) or Hamilton-filtered regimes (real data); (2) Freeze GRU weights, train DDPG actor-critic using filtered features plus St and It; (3) For hid-DDPG, train GRU and DDPG jointly with prediction loss and policy gradient interleaved
- **Design tradeoffs**: prob-DDPG vs hid-DDPG: Interpretability vs simplicity. Lookback window W: Longer W improves regime detection but increases latency. Batch size b vs training episodes N: Larger batches stabilize gradients but require more memory
- **Failure signatures**: reg-DDPG near-zero rewards (policy becomes overly conservative); hid-DDPG clustered actions (policy collapses to few inventory levels); Critic divergence (Q-values explode or oscillate)
- **First 3 experiments**: (1) Replicate θt-only MC case with prob-DDPG: verify average cumulative reward ≈25±3 on 500 test episodes; (2) Ablate GRU depth: compare dℓ=1 vs dℓ=5 in prob-DDPG to test if deeper GRU captures complex dynamics; (3) Stress test regime transition speed: increase Markov transition rate from 0.05 to 0.3 and observe performance degradation

## Open Questions the Paper Calls Out

- **Multi-agent extension**: How does the quality and structure of information provided to agents affect learning dynamics and emerging equilibria in a multi-agent trading environment? The current study is limited to single-agent setup where trading signal is exogenous. Extending to competitive multi-agent environment would reveal if prob-DDPG retains advantage in equilibrium strategies.

- **Unsupervised regime discovery**: Can superior performance of prob-DDPG be maintained using unsupervised or end-to-end latent state discovery, removing dependency on external filtering models? The method currently depends on pre-defined statistical models or labeled synthetic data. A comparison against VAE or unsupervised clustering to generate posterior probabilities without supervision would resolve this.

- **Alternative architectures**: Do attention-based architectures or LSTMs outperform GRUs in capturing temporal dependencies required for optimal trading with partial information? While GRUs are computationally efficient, Transformers often excel at longer-range dependencies critical for slow-switching regime detection. Ablation studies replacing GRU with LSTM and Transformer encoders would compare cumulative rewards and training stability.

## Limitations

- The prob-DDPG performance gains critically depend on access to true regime labels during pre-training, which is rarely available in real-world trading
- The simulation framework assumes known transition matrices and independent Markov chains for each parameter, creating an idealized environment
- The GRU architectures are fixed without hyperparameter optimization across methods, potentially biasing results

## Confidence

- **High confidence**: hid-DDPG outperforms reg-DDPG across all scenarios (consistent evidence from multiple experiments with large reward gaps)
- **Medium confidence**: prob-DDPG outperforms hid-DDPG in cumulative rewards and interpretability (supported by simulation results but real-data performance depends on regime estimation quality)
- **Low confidence**: The specific GRU architecture choices (depth, hidden size) are optimal for regime detection (no ablation studies or architectural search reported)

## Next Checks

1. **Real-data regime robustness test**: Implement Hamilton filtering on INTC/SMH spread and evaluate prob-DDPG performance with estimated vs. true regimes to quantify cost of unsupervised regime detection
2. **Transition rate stress test**: Systematically vary Markov transition probability from 0.01 to 0.5 in θt, κt, σt simulation and measure performance degradation curves for all three methods to identify break points
3. **Architecture ablation study**: Fix prob-DDPG architecture and perform grid search over GRU depth (1-5 layers) and hidden size (10-50 nodes) to determine if performance gap is due to architectural advantages rather than probabilistic feature encoding