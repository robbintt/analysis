---
ver: rpa2
title: Table as a Modality for Large Language Models
arxiv_id: '2512.00947'
source_url: https://arxiv.org/abs/2512.00947
tags:
- table
- llms
- tamo
- data
- tables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TAMO addresses the challenge of LLMs\u2019 limited ability to\
  \ understand table structures by treating tables as an independent modality, using\
  \ a hypergraph-based encoder to capture global structural information and aligning\
  \ it with LLM embeddings. This allows LLMs to perceive table structure globally\
  \ before processing text, improving robustness to row/column permutations and enhancing\
  \ table reasoning performance."
---

# Table as a Modality for Large Language Models

## Quick Facts
- arXiv ID: 2512.00947
- Source URL: https://arxiv.org/abs/2512.00947
- Reference count: 40
- Primary result: TAMO achieves up to 42.65% average relative gain over text-only baselines on table reasoning tasks

## Executive Summary
TAMO addresses LLMs' structural understanding limitations by treating tables as an independent modality. It uses a hypergraph-based encoder to capture global table structure and aligns it with LLM embeddings, allowing global perception of table structure before text processing. The framework achieves strong performance on five benchmarks (StructQA, HiTab, WikiTQ, WikiSQL, FeTaQA), showing up to 42.65% average relative gain over text-only baselines and robust performance against row/column permutations.

## Method Summary
TAMO treats tables as an independent modality using hypergraph-based structural encoding. Tables are modeled as hypergraphs where nodes are leaf cells and hyperedges are rows/columns, preserving permutation invariance. A HyperTrans encoder processes this structure, which is then aligned to LLM embedding space via an MLP projector and injected as soft prompts. The serialized table text provides semantic content. TAMO operates in frozen LLM or LoRA settings, with the encoder and projector trained while the LLM backbone remains frozen or minimally adapted.

## Key Results
- Achieves 42.65% average relative gain over text-only baselines across five benchmarks
- Shows 53.73% robustness vs 39.67% for text-only baselines on permuted tables
- Outperforms GPT-3.5 and GPT-4 on most tasks while maintaining strong generalization across datasets

## Why This Works (Mechanism)

### Mechanism 1: Hypergraph-Based Permutation Invariant Encoding
- **Claim:** Hypergraph representation preserves table structural semantics that text serialization destroys
- **Mechanism:** Tables as hypergraphs where nodes = leaf cells, hyperedges = rows/columns. Swapping rows/columns preserves graph structure. HyperTrans uses multiset aggregation functions that are inherently order-agnostic
- **Core assumption:** Tables possess hierarchical structure with permutation invariance; modeling relationships as set-based captures this property
- **Evidence anchors:** Section 2.2 states graph structure remains consistent under permutation; TAMO achieves 53.73% vs 39.67% robustness over text-only LoRA on permuted tables

### Mechanism 2: Dual-Stream Complementary Information
- **Claim:** Structure embeddings provide global relational context ("where"); serialized text provides fine-grained semantics ("what")
- **Mechanism:** Xst (structure embeddings) and Xtt (text embeddings) concatenated as input. Structure helps LLMs locate relevant cells; text provides content
- **Core assumption:** Neither modality is redundant; structure guides attention, text provides content
- **Evidence anchors:** Appendix C.4 shows Graph-only achieves 7.00% accuracy; Text-only 37.80%; TAMO (both) 59.07%

### Mechanism 3: Soft Prompt Structure Injection
- **Claim:** Injecting structure embeddings as soft prompts enables frozen LLMs to perceive table structure globally before text processing
- **Mechanism:** Structure embeddings Xst prepended via alignment projector (MLP + pooling). LLM receives structure tokens before serialized table text
- **Core assumption:** Prepending structure tokens influences attention distribution over subsequent text tokens
- **Evidence anchors:** Attention visualization in Figure 5 shows [table_structure_token] influences importance distribution toward correct answer tokens

## Foundational Learning

- **Hypergraphs vs. Graphs**
  - Why needed here: Standard graphs model pairwise relationships; hyperedges connect arbitrary node subsets, matching table structure where a row/column contains multiple cells
  - Quick check question: Can you explain why a standard GCN would fail to capture "all cells in row 3" as a single relationship?

- **Permutation Invariance**
  - Why needed here: Tables are semantically unchanged by row/column reordering. Neural architectures must explicitly encode this inductive bias rather than learning it from data
  - Quick check question: If you shuffle all rows in a table, should a model's prediction change? Why do LLMs fail this test?

- **Soft Prompts vs. Hard Prompts**
  - Why needed here: Soft prompts are learnable continuous embeddings; they can encode information that natural language cannot express
  - Quick check question: Why can't you just write "The table has 5 rows and 3 columns" as a text prompt to convey structure?

## Architecture Onboarding

- **Component map:** Table → Hypergraph construction → HyperTrans encoder → Alignment MLP → Prepend to text tokens → LLM → Answer

- **Critical path:** Table → Hypergraph construction → HyperTrans encoder → Alignment MLP → Prepend to text tokens → LLM → Answer

- **Design tradeoffs:**
  - Frozen LLM vs. LoRA/SFT: Frozen is fastest (0.35 hours/epoch) but lower ceiling; SFT achieves best performance (71.60% on StructQA) but requires 2 GPUs
  - Token count: 2+ structure tokens sufficient; more tokens don't improve performance
  - Encoder dimension: 768 hidden dim balances expressiveness and efficiency

- **Failure signatures:**
  - Graph-only mode fails on generative tasks (<8% accuracy): structure alone lacks semantic content
  - Cross-dataset generalization limited: encoder trained on StructQA transfers poorly to WikiTQ (18.74% vs 37.06% when trained on-target)
  - Multi-table scenarios: marginal gains under LoRA setting (+0.58% F1)

- **First 3 experiments:**
  1. Reproduce permutation robustness test: Run Llama2-7B baseline and TAMO on StructQA with shuffled rows/columns; verify robustness gap (~14 percentage points)
  2. Ablate modality streams: Compare Graph-only, Text-only, TAMO (both) on StructQA; expect 7% → 38% → 59% progression
  3. Test token count sensitivity: Vary structure tokens {1,2,3,5,7,9} on WikiTQ subset; confirm performance plateaus at 2+ tokens

## Open Questions the Paper Calls Out

- **Open Question 1:** How do different text serialization templates (e.g., Markdown vs. SQL-based) interact with TAMO's hypergraph-based structural modality?
  - Basis in paper: Section 5 states that a systematic study of how different text serialization templates interact with the structural modality "remains an important direction for future work"
  - Why unresolved: The authors utilized a consistent serialization format for all experiments, leaving the impact of textual formatting variance on the alignment of structural embeddings untested
  - What evidence would resolve it: Evaluations comparing model performance when identical tables are serialized into Markdown, CSV, or SQL formats

- **Open Question 2:** Can the TAMO framework be extended to support dynamic multi-step reasoning and multi-turn dialogue over tables?
  - Basis in paper: Section 5 notes that extending the model to support dynamic multi-step reasoning, table editing, and multi-turn dialogue "remains an open challenge"
  - Why unresolved: The current framework focuses on static table understanding in a single-turn setting, lacking mechanisms to maintain state or context across iterative reasoning steps
  - What evidence would resolve it: Adapting the framework to multi-turn conversational QA benchmarks and measuring consistency over sequential questions

- **Open Question 3:** To what extent can a dedicated task-agnostic pretraining phase for the table encoder improve cross-dataset generalization?
  - Basis in paper: Appendix C.6 suggests that "a dedicated pretraining phase... focusing purely on table-related structural information" is an important direction
  - Why unresolved: Current training ties structural representations to specific QA objectives, causing embeddings to be influenced by instruction types and limiting transferability
  - What evidence would resolve it: Pre-training the hypergraph encoder on a large corpus of tables using structural objectives only, then testing zero-shot performance across heterogeneous datasets

## Limitations
- Hypergraph construction details lack explicit algorithmic rules for handling merged cells and nested structures
- Serialization format specification incomplete, affecting reproducibility of text-only baseline
- Cross-dataset generalization degrades significantly (StructQA→WikiTQ drops from 59.07% to 18.74%)

## Confidence

- **High Confidence:** Permutation invariance mechanism, dual-stream architecture benefits, and overall benchmark performance improvements
- **Medium Confidence:** Claims about TAMO being "plug-and-play" across diverse LLMs, as most results focus on Llama2-7B with limited validation on other models
- **Low Confidence:** Claims about TAMO's effectiveness on complex hierarchical tables, given the vague description of the hypergraph construction algorithm

## Next Checks
1. Implement and test the exact hypergraph construction algorithm on tables with merged cells and nested headers. Compare TAMO's performance on these tables versus standard flat tables
2. Create two versions of the text-only baseline—one using the exact serialization format described in TAMO, and another using a more standard markdown format. Measure the performance gap
3. Train the hypergraph encoder on StructQA, then fine-tune it on WikiTQ and HiTab separately. Measure whether targeted fine-tuning improves cross-dataset generalization, and analyze which structural features transfer poorly