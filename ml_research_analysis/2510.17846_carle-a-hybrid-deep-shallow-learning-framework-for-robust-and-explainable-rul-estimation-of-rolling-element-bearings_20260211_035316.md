---
ver: rpa2
title: 'CARLE: A Hybrid Deep-Shallow Learning Framework for Robust and Explainable
  RUL Estimation of Rolling Element Bearings'
arxiv_id: '2510.17846'
source_url: https://arxiv.org/abs/2510.17846
tags:
- score
- carle
- bearing
- data
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CARLE, a hybrid deep-shallow learning framework
  for robust and explainable RUL estimation of rolling element bearings. The framework
  integrates Res-CNN and Res-LSTM blocks with multi-head attention and residual connections
  to capture spatial and temporal degradation patterns, coupled with a Random Forest
  Regressor (RFR) for stable, accurate RUL prediction.
---

# CARLE: A Hybrid Deep-Shallow Learning Framework for Robust and Explainable RUL Estimation of Rolling Element Bearings

## Quick Facts
- arXiv ID: 2510.17846
- Source URL: https://arxiv.org/abs/2510.17846
- Reference count: 40
- Primary result: Hybrid deep-shallow learning framework for robust and explainable RUL estimation

## Executive Summary
This paper introduces CARLE, a hybrid deep-shallow learning framework designed for robust and explainable Remaining Useful Life (RUL) estimation of rolling element bearings. The framework combines deep learning blocks (Res-CNN and Res-LSTM) with multi-head attention mechanisms and residual connections to capture both spatial and temporal degradation patterns, integrated with a Random Forest Regressor for stable predictions. The method demonstrates superior performance on XJTU-SY and PRONOSTIA bearing datasets, particularly under dynamic operating conditions, while providing interpretability through LIME and SHAP analyses.

## Method Summary
CARLE employs a two-stage architecture where the first stage uses Res-CNN and Res-LSTM blocks with multi-head attention and residual connections to extract spatial and temporal features from bearing vibration data. This is coupled with a Random Forest Regressor in the second stage for RUL prediction. The preprocessing pipeline applies Gaussian filtering for noise reduction and Continuous Wavelet Transform for time-frequency feature extraction. The framework is evaluated on two benchmark datasets, showing significant improvements over state-of-the-art methods, with particular emphasis on cross-domain validation and interpretability analysis.

## Key Results
- Achieves MSE as low as 0.0022 and MAE as low as 0.04087 on XJTU-SY dataset
- Outperforms state-of-the-art methods, especially under dynamic operating conditions
- Demonstrates superior cross-domain validation performance compared to baseline methods

## Why This Works (Mechanism)
The hybrid architecture leverages the complementary strengths of deep learning for feature extraction and shallow learning for stable prediction. The Res-CNN blocks capture spatial patterns in vibration data, while Res-LSTM blocks extract temporal degradation trends. Multi-head attention mechanisms help the model focus on the most relevant time steps and frequency components for RUL estimation. The residual connections facilitate gradient flow and enable deeper architectures without vanishing gradients. The Random Forest Regressor provides robustness to noise and overfitting while maintaining interpretability.

## Foundational Learning
1. **Continuous Wavelet Transform (CWT)**: Time-frequency analysis technique needed for capturing transient features in bearing vibration signals; quick check: verify proper mother wavelet selection and scale parameters
2. **Residual Connections**: Architectural pattern that allows training of deeper networks by addressing vanishing gradient problems; quick check: confirm skip connections preserve spatial/temporal dimensions
3. **Multi-head Attention**: Mechanism for capturing relationships across different parts of the input sequence; quick check: validate attention weights correlate with known degradation patterns
4. **Random Forest Regression**: Ensemble learning method providing robust predictions and feature importance; quick check: ensure proper hyperparameter tuning (number of trees, depth)
5. **SHAP Values**: Game-theoretic approach for feature attribution and model interpretability; quick check: validate Shapley values sum to prediction difference from baseline
6. **LIME (Local Interpretable Model-agnostic Explanations)**: Technique for explaining individual predictions through local linear approximations; quick check: confirm neighborhood sampling captures relevant input variations

## Architecture Onboarding

**Component Map**: Vibration Data -> Gaussian Filter -> CWT -> Res-CNN -> Res-LSTM -> Multi-head Attention -> Feature Fusion -> Random Forest Regressor -> RUL Prediction

**Critical Path**: Data Preprocessing (Gaussian + CWT) -> Feature Extraction (Res-CNN + Res-LSTM + Attention) -> Prediction (Random Forest)

**Design Tradeoffs**: Deep learning components provide powerful feature extraction but require substantial data and are prone to overfitting, while the Random Forest Regressor offers stability and interpretability but may miss complex nonlinear relationships. The hybrid approach balances these competing objectives.

**Failure Signatures**: Performance degradation under insufficient training data, poor generalization to unseen operating conditions, or when degradation patterns deviate significantly from training distribution. Attention mechanisms may fail to identify relevant features if degradation signatures are subtle or masked by noise.

**First Experiments**:
1. Train and evaluate each component (Res-CNN, Res-LSTM, attention) individually to establish baseline performance
2. Perform ablation studies removing one component at a time to quantify individual contributions
3. Test model performance across different noise levels to assess robustness of preprocessing pipeline

## Open Questions the Paper Calls Out
None

## Limitations
- Reported MSE and MAE values appear exceptionally low, raising questions about normalization or evaluation methodology
- Limited validation scope focused only on rolling element bearings without testing on other mechanical systems
- Lack of detailed implementation specifications including CWT parameters, random forest hyperparameters, and attention mechanism configurations
- Post-hoc interpretability methods (LIME, SHAP) rather than built-in model transparency

## Confidence
- **High confidence**: The hybrid deep-shallow architecture design is technically sound and aligns with established approaches in bearing prognosis literature
- **Medium confidence**: Cross-domain validation results are promising but limited in scope and dataset diversity
- **Medium confidence**: Performance superiority claims are supported by comparative analysis but lack statistical significance testing

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of Res-CNN, Res-LSTM, attention mechanisms, and Random Forest components to overall performance
2. Test the framework on additional bearing datasets (e.g., IEEE PHM 2012 challenge data) and other mechanical systems to assess generalizability
3. Implement statistical significance testing (t-tests or ANOVA) across all comparative experiments to validate performance claims against baseline methods