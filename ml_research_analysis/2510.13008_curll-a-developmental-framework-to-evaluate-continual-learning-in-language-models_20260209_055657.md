---
ver: rpa2
title: 'CurLL: A Developmental Framework to Evaluate Continual Learning in Language
  Models'
arxiv_id: '2510.13008'
source_url: https://arxiv.org/abs/2510.13008
tags:
- stage
- skill
- stages
- learning
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CurLL introduces a continual learning framework grounded in human
  developmental curricula for ages 5-10, featuring a skill graph with 1,300+ indicators
  and a 23.4B-token synthetic dataset. The dataset controls vocabulary complexity,
  skill dependencies, and format diversity across five developmental stages.
---

# CurLL: A Developmental Framework to Evaluate Continual Learning in Language Models

## Quick Facts
- **arXiv ID:** 2510.13008
- **Source URL:** https://arxiv.org/abs/2510.13008
- **Reference count:** 40
- **Primary result:** Sequential training improves generalization to later stages but causes forgetting of earlier skills, with forgetting patterns correlating to skill graph structure

## Executive Summary
CurLL introduces a continual learning framework grounded in human developmental curricula for ages 5-10, featuring a skill graph with 1,300+ indicators and a 23.4B-token synthetic dataset. The dataset controls vocabulary complexity, skill dependencies, and format diversity across five developmental stages. Experiments with a 135M-parameter transformer show that sequential training improves generalization to later stages but causes forgetting of earlier skills, with forgetting patterns correlating to skill graph structure. CurLL enables fine-grained evaluation of transfer, forgetting, and sample efficiency at skill, sub-skill, and indicator levels, advancing continual learning assessment for language models.

## Method Summary
CurLL constructs a developmental curriculum spanning ages 5-10 (five stages) using a skill graph with 1,300+ indicators derived from early childhood education standards. The framework generates a 23.4B-token synthetic dataset through LLM inference, controlling vocabulary complexity, skill dependencies, and format diversity across stages. Three training configurations are evaluated: independent (single stage), joint (mixed stages), and continual (sequential stages). Evaluation uses LLM-based grading (1-5 scale) for IR/CQA/CSQA formats with per-indicator granularity, enabling skill-level analysis of transfer, forgetting, and sample efficiency.

## Key Results
- Sequential training improves generalization to later stages compared to random/shuffled training
- Forgetting patterns correlate with skill graph structure—skills with fewer prerequisite connections experience greater degradation
- Fine-grained skill metadata enables precise diagnosis of continual learning dynamics at skill, sub-skill, and indicator levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forgetting in sequential training correlates with skill graph structure—skills with fewer prerequisite connections to later stages experience greater degradation.
- Mechanism: When later-stage training occurs, parameter updates primarily affect networks involved in current tasks. Skills with low out-degree in the dependency graph (fewer connections to future skills) receive less reinforcement during later training, making them vulnerable to interference.
- Core assumption: Parameter sharing between related skills means training on dependent skills partially maintains prerequisite representations; isolation leads to decay.
- Evidence anchors:
  - [abstract] "forgetting patterns correlating to skill graph structure"
  - [section 4.1] "Perceptual, Motor, and Physical Development" and "Digital Literacy" showed highest forgetting; these skills have "least number of outgoing edges"
  - [corpus] Limited direct corpus evidence on skill graph forgetting correlations in LMs; related work on brain-inspired temporal development (arxiv:2504.05621) suggests cross-regional development mechanisms may influence retention patterns
- Break condition: If skills are encoded in largely non-overlapping parameter regions, dependency structure may not predict forgetting patterns.

### Mechanism 2
- Claim: Sequential developmental curriculum improves generalization to later stages compared to random/shuffled training.
- Mechanism: Progressive skill introduction builds foundational representations that later skills can reuse. Skills are trained when prerequisite knowledge is already encoded, reducing the learning burden.
- Core assumption: Skills in the curriculum have genuine prerequisite relationships that parameter learning can exploit.
- Evidence anchors:
  - [abstract] "sequential training improves generalization to later stages"
  - [section 4.1] "changing the order of the data, i.e. by arranging the data in a progressive fashion, leads to better generalization"
  - [corpus] Hierarchical continual imitation learning work (arxiv:2504.15561) shows skill-based prompts enable better forward transfer in robot manipulation, suggesting skill ordering matters across domains
- Break condition: If prerequisite relationships are spurious or if later skills don't actually depend on earlier ones, curriculum ordering provides no benefit.

### Mechanism 3
- Claim: Fine-grained skill metadata enables more precise diagnosis of continual learning dynamics than aggregate task-level metrics.
- Mechanism: By tagging each training instance with skill/subskill/goal/indicator metadata, evaluation can attribute performance changes to specific capabilities rather than treating stages as monolithic tasks.
- Core assumption: The skill decomposition captures meaningful cognitive distinctions that manifest in model behavior.
- Evidence anchors:
  - [abstract] "fine-grained evaluation of transfer, forgetting, and sample efficiency at skill, sub-skill, and indicator levels"
  - [section 3.1] Four-component skill-tuple provides "specific, observable behaviors that demonstrate mastery"
  - [corpus] Skill-based training frameworks (arxiv:2307.14430 Skill-it) show skill-level analysis reveals patterns hidden by aggregate metrics
- Break condition: If skills are too fine-grained (noisy) or too coarse (conflating distinct abilities), granularity provides no diagnostic advantage.

## Foundational Learning

- Concept: **Catastrophic forgetting**
  - Why needed here: CurLL's primary finding is that sequential training causes forgetting of earlier skills; understanding why forgetting occurs is essential for interpreting results and designing mitigations.
  - Quick check question: Can you explain why updating parameters on new tasks degrades performance on old tasks, and why this is particularly problematic for language models?

- Concept: **Stability-plasticity dilemma**
  - Why needed here: The paper positions CurLL as addressing this fundamental trade-off; sequential training shows plasticity (learning new stages) but reduced stability (forgetting earlier ones).
  - Quick check question: How would you determine whether a model has achieved good stability-plasticity balance on CurLL's five-stage curriculum?

- Concept: **Skill/dependency graphs**
  - Why needed here: CurLL's skill graph with weighted prerequisite edges is central to interpreting forgetting patterns and transfer effects.
  - Quick check question: Given a skill A with high out-degree and skill B with low out-degree, which would you expect to be more vulnerable to forgetting during later-stage training, and why?

## Architecture Onboarding

- Component map:
  - **Skill graph**: Directed, weighted graph with 1,300+ indicator nodes; edges represent prerequisite relationships (weights 1-5); generated via LLM inference over skill tuples
  - **Data generation pipeline**: Seeds (skill-tuple + vocabulary word + template) → LLM generates instances → filtering/scoring for test set quality
  - **Training configurations**: Independent (single stage), Joint (mixed stages), Continual (sequential stages); each with same hyperparameters
  - **Evaluation**: LLM-based grading (1-5 scale) for IR/CQA/CSQA formats; per-indicator granularity enables skill-level analysis

- Critical path:
  1. Understand skill graph structure (download and inspect edge distributions across stages)
  2. Examine one stage's data instances (context + questions + metadata)
  3. Replicate single independent training run (M0 or M1)
  4. Compare joint vs. continual training on same cumulative data
  5. Analyze forgetting at skill level using graph structure

- Design tradeoffs:
  - **Synthetic vs. real data**: Synthetic enables precise skill control and dependency tracking; may not reflect real-world distribution shifts
  - **LLM-based graph construction**: Scalable but introduces noise (section 5.2 notes erroneous edges between stages)
  - **Single model scale (135M)**: Enables controlled experiments; dynamics may differ at larger scales (section 6 limitation)
  - **One epoch training**: Standardizes comparison; may not reflect typical pretraining regimes

- Failure signatures:
  - Training loss decreases but test scores don't improve → check template/format alignment
  - Forgetting is uniform across skills → skill graph may not capture real dependencies
  - Joint and continual show identical performance → data order not affecting learning (check shuffling)
  - Low inter-annotator agreement in evaluation → rubric needs refinement for stage-appropriate expectations

- First 3 experiments:
  1. **Baseline replication**: Train M0 independently; verify scores match paper's IR/CQA/CSQA results (establishes pipeline correctness)
  2. **Forgetting diagnosis**: Train M0-1-2-3-4 sequentially; for each skill in stage 0, compute performance drop and correlate with out-degree; verify "Perceptual, Motor, and Physical Development" shows highest forgetting
  3. **Transfer analysis**: Train M0-2 (skipping M1); compare performance on stage 2 skills with/without M1 prerequisite training to isolate transfer effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the skill transfer and catastrophic forgetting dynamics observed in the 135M-parameter model persist in large-scale foundation models (e.g., 1B+ parameters)?
- Basis in paper: [explicit] Section 6 (Limitations) states that experiments were limited to a 135M-parameter model and that "dynamics of catastrophic forgetting and knowledge transfer may differ significantly at scale."
- Why unresolved: The authors explicitly note that the conclusions drawn from the smaller proof-of-concept model may not fully translate to the billion-parameter models typically used in practice.
- What evidence would resolve it: Replicating the CurLL sequential training experiments on larger architectures (e.g., 7B or 70B parameter models) and comparing the correlation between skill graph structure and forgetting.

### Open Question 2
- Question: How do the controlled skill dependencies and forgetting patterns in CurLL’s synthetic dataset correlate with real-world continual learning scenarios?
- Basis in paper: [explicit] Section 6 (Limitations) highlights that the exclusive use of synthetic data, while ensuring control, "might not reflect the real world scenarios of continual learning."
- Why unresolved: Synthetic data generation pipelines inherently lack the noise, ambiguity, and distributional shifts present in natural text, potentially limiting the ecological validity of the benchmark.
- What evidence would resolve it: A comparative study evaluating models on both CurLL and a natural longitudinal corpus of child educational materials to validate the transferability of the findings.

### Open Question 3
- Question: How does the model's ability to acquire skills sequentially change when moving from static dataset training to an interactive, agentic learning environment?
- Basis in paper: [explicit] Section 6 (Limitations) argues that a setup reflecting human-like learning "would involve, instead of a static dataset, an environment in which the agent learns by interactions."
- Why unresolved: The current framework optimizes for next-token prediction on static text, whereas true developmental learning involves active feedback loops and environmental interaction which are not modeled here.
- What evidence would resolve it: Integrating the CurLL skill graph into a reinforcement learning environment where agents must actively use skills to solve tasks and measuring retention rates.

### Open Question 4
- Question: Can the explicit prerequisite edges in the CurLL skill graph be leveraged to mitigate the forgetting of low-outdegree skills?
- Basis in paper: [inferred] Section 4.1 identifies that skills with fewer outgoing edges (low-outdegree), such as "Perceptual, Motor, and Physical Development," are more vulnerable to forgetting, while Section 5 suggests the graph serves as a diagnostic tool.
- Why unresolved: The paper identifies the correlation between graph topology and forgetting but does not propose or test an algorithm that explicitly uses the graph structure to prevent this specific type of forgetting.
- What evidence would resolve it: Developing a continual learning algorithm (e.g., a graph-aware replay method) that prioritizes the retention of skills identified as prerequisites for future stages and evaluating it on the benchmark.

## Limitations
- **Synthetic data validity**: The exclusive use of synthetic data may not reflect real-world continual learning scenarios and distributional shifts
- **Model scale**: Experiments limited to 135M-parameter model; dynamics may differ significantly at larger scales
- **LLM-based graph quality**: Skill graph construction via LLM inference introduces potential noise and erroneous prerequisite edges

## Confidence
- **High confidence**: Sequential training improves generalization to later stages (directly observed in experiments with clear metrics)
- **Medium confidence**: Forgetting patterns correlate with skill graph structure (statistically observed but depends on synthetic graph quality)
- **Medium confidence**: Fine-grained skill metadata enables better continual learning diagnosis (mechanistically sound but requires real-world validation)

## Next Checks
1. **Scale validation**: Replicate key experiments with a 1B-2B parameter transformer to assess whether forgetting patterns and transfer effects scale consistently with model size
2. **Real data verification**: Evaluate models trained on CurLL data on external developmental benchmarks (e.g., early childhood education assessments) to test skill transfer beyond synthetic domains
3. **Graph structure ablation**: Systematically perturb the skill graph (add/remove edges) and retrain to quantify how dependency structure quality affects forgetting patterns and curriculum effectiveness