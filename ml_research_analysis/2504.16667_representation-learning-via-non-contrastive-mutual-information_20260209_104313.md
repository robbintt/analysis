---
ver: rpa2
title: Representation Learning via Non-Contrastive Mutual Information
arxiv_id: '2504.16667'
source_url: https://arxiv.org/abs/2504.16667
tags:
- contrastive
- learning
- non-contrastive
- representation
- byol
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MINC, a non-contrastive mutual information
  objective that combines the theoretical foundation of contrastive methods with the
  low variance of non-contrastive approaches. Starting from the Spectral Contrastive
  Loss, the authors reformulate it using power iteration and the Generalized Hebbian
  Algorithm to remove pairwise comparisons while maintaining mutual information maximization.
---

# Representation Learning via Non-Contrastive Mutual Information

## Quick Facts
- arXiv ID: 2504.16667
- Source URL: https://arxiv.org/abs/2504.16667
- Reference count: 7
- Primary result: MINC achieves 0.711 top-1 accuracy vs 0.701 for Spectral Contrastive baseline on ImageNet with batch size 4096

## Executive Summary
This paper introduces MINC, a non-contrastive mutual information objective that converts the Spectral Contrastive Loss into an iterative form using power iteration and the Generalized Hebbian Algorithm. By reformulating the contrastive objective into a non-contrastive form, MINC removes pairwise comparisons while maintaining mutual information maximization, resulting in lower variance and avoiding representation collapse. Tested on ImageNet with ResNet-50, MINC consistently outperforms the Spectral Contrastive Loss baseline across multiple batch sizes while matching the performance of Linear BYOL.

## Method Summary
MINC reformulates the Spectral Contrastive Loss through power iteration, converting it from a contrastive objective requiring pairwise comparisons into an iterative non-contrastive form. The method maintains an auxiliary matrix Λ via exponential moving average that summarizes second-order statistics of embeddings, replacing the need for direct pairwise computations. A Generalized Hebbian Algorithm with lower-triangular transformation enforces asymptotic orthogonality to prevent representation collapse. The objective combines a scaled similarity term with a Mahalanobis distance-like regularization, allowing gradient computation without contrastive pairs while preserving the mutual information maximization property.

## Key Results
- MINC achieves 0.711 top-1 accuracy vs 0.701 for Spectral Contrastive baseline on ImageNet with batch size 4096
- MINC consistently outperforms Spectral Contrastive Loss across all tested batch sizes (512, 1408, 4096)
- MINC with linear predictor matches Linear BYOL performance (~71.5% top-1) but doesn't reach full BYOL with MLP predictor (74.3%)
- Auxiliary EMA parameter β=0.8 is optimal; values outside [0.7, 0.9] degrade performance
- GHA transform is essential for preventing representation collapse, confirmed by ablation studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting contrastive loss to power iteration form removes quadratic sample dependence while preserving the mutual information objective.
- Mechanism: The Spectral Contrastive Loss optimizes an eigen-decomposition problem. By recognizing that eigen-decomposition can be solved iteratively via power iteration, the authors reformulate the contrastive objective into iterative fixed-point updates that compare each sample to a summary statistic rather than all other samples.
- Core assumption: The embedding space satisfies the eigen-function fixed point equation (Eq. 9), and power iteration converges to the same solution as direct eigen-decomposition.
- Evidence anchors:
  - [abstract]: "we convert it into a more general non-contrastive form; this removes the pairwise comparisons resulting in lower variance, but keeps the mutual information formulation"
  - [Section 3.3]: "power iteration... is a simple yet effective algorithm for eigen-decomposition... It has been proved the convergence rate of subspace iteration is exponential"
  - [corpus]: Corpus provides limited direct validation; related work "Dual Perspectives on Non-Contrastive Self-Supervised Learning" discusses stop-gradient and EMA procedures but does not address power iteration specifically.
- Break condition: If the fixed point equation assumptions fail (e.g., embedding space is non-stationary or non-convex), power iteration may not converge to the same solution as contrastive methods.

### Mechanism 2
- Claim: The auxiliary matrix Λ estimated via EMA captures sufficient second-order statistics of embeddings to replace pairwise comparisons.
- Mechanism: Rather than computing φ(x)φ(x')^T for all pairs, MINC maintains an exponential moving average Λ_t = βΛ_{t-1} + (1-β)E[φ(x)φ(x)^T]. Each sample is then compared against this summary via φ(x')^T Λ φ(x'), reducing variance from O(N²) to O(N).
- Core assumption: The EMA estimate of Λ tracks the true second-moment matrix fast enough relative to embedding drift; bias-variance tradeoff is favorable at β ≈ 0.8.
- Evidence anchors:
  - [Section 3.1]: "Λ has become a summary of the statistics of φ(x), allowing φ(x') to be compared with this summary matrix instead of every other point"
  - [Section 4.2, Figure 4]: "An auxiliary EMA of 0 means that the auxiliary Λ is only computed from the current mini-batch... But increasing the auxiliary EMA further improves the performance, with 0.8 being the best"
  - [corpus]: Weak corpus evidence; neighbor papers do not analyze EMA-based summary statistics in this context.
- Break condition: If embeddings drift rapidly or batch size is too small, Λ becomes a biased estimator and the approximation degrades.

### Mechanism 3
- Claim: The Generalized Hebbian Algorithm (GHA) with lower-triangular transformation enforces asymptotic orthogonality, preventing representation collapse without explicit regularization.
- Mechanism: GHA updates embeddings with a modified gradient that subtracts correlations with previously learned components. The LT[·] operation zeros upper-triangular elements of Λ, implementing sequential orthogonalization akin to Oja's rule extended to multiple components.
- Core assumption: Asymptotic orthogonality is sufficient to prevent collapse; strict orthogonality at each step is unnecessary.
- Evidence anchors:
  - [Section 3.3]: "the GHA update rule... ensures the orthogonality in an asymptotic sense"
  - [Section 4.2, Figure 3]: "Without GHA, the representation collapses... GHA is successfully preventing the collapse"
  - [corpus]: Corpus papers on non-contrastive SSL (e.g., "Dual Perspectives") discuss collapse but do not evaluate GHA specifically.
- Break condition: If learning rates are too high or batch statistics are highly non-stationary, GHA's asymptotic guarantees may not hold during practical training.

## Foundational Learning

- Concept: **Power Iteration for Eigen-decomposition**
  - Why needed here: MINC derives its update rules from power iteration, requiring understanding of how iteratively multiplying by a matrix and normalizing converges to principal eigenvectors.
  - Quick check question: Can you explain why power iteration converges to the largest eigenvalue/eigenvector pair?

- Concept: **f-Divergences and Mutual Information**
  - Why needed here: MINC generalizes Spectral Contrastive Loss through α-divergences; selecting α=2 (χ²-divergence) requires understanding how different f-divergences relate to MI bounds.
  - Quick check question: What property of χ²-divergence makes it yield a squared dot product term in the objective?

- Concept: **Exponential Moving Average (EMA) for Online Statistics**
  - Why needed here: Both the auxiliary matrix Λ and target network φ_target are updated via EMA; understanding bias-variance tradeoffs is critical for tuning β and γ.
  - Quick check question: How does the decay rate β affect bias vs. variance in the Λ estimate when embeddings are non-stationary?

## Architecture Onboarding

- Component map:
  - Encoder (φ) -> Projector MLP -> Target network (φ_target) -> Auxiliary matrix (Λ) -> GHA transform

- Critical path:
  1. Forward pass: x, x' → augmentations → φ(x), φ(x') (normalized to unit norm)
  2. Target branch: φ_target(x) computed with stop-gradient
  3. Loss computation: t_α(s · φ_target(x)^T φ(x')) - 0.5 · s² · φ(x')^T LT[Λ] φ(x')
  4. Λ update: Λ ← βΛ + (1-β) · batch_mean[φ_target(x)φ_target(x)^T]
  5. Target network update: φ_target ← γφ_target + (1-γ)φ

- Design tradeoffs:
  - **α selection**: α=2 (χ²) is stable and performs best; α>2 causes instability from pushing embeddings too far apart; α<2 degrades performance
  - **β (auxiliary EMA)**: 0.8 is optimal; lower β increases variance, higher β increases bias from stale embeddings
  - **With vs. without target network**: Target network improves stability under large learning rates but adds memory/maintenance overhead
  - **Linear vs. non-linear predictor**: MINC with linear predictor matches Linear BYOL (~71.5% top-1) but doesn't reach full BYOL with MLP predictor (74.3%)

- Failure signatures:
  - **Collapse**: Representations converge to constant vectors — check if GHA transform (LT[·]) is correctly applied; ablation shows collapse occurs without GHA even with EMA
  - **Training instability with α>2**: Divergence in embeddings — reduce α or lower learning rate
  - **Degraded performance with β too high**: Stale Λ causes bias — reduce β to 0.7-0.8
  - **No improvement over baseline**: Verify normalization to unit norm and inner scale s are correctly set

- First 3 experiments:
  1. **Baseline sanity check**: Reproduce MINC vs. Spectral Contrastive at batch size 1408 on ImageNet train split (300 epochs); expect ~0.710 vs. 0.701 top-1
  2. **GHA ablation**: Remove LT[·] transform while keeping EMA and target network; confirm collapse (Figure 3 shows near-zero accuracy)
  3. **β sensitivity**: Sweep β ∈ {0.0, 0.7, 0.8, 0.9, 0.95} at fixed batch size 1408; expect peak at 0.8 with degradation at extremes (Figure 4 pattern)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a stronger contrastive loss foundation enable MINC-style conversion to match or surpass non-linear BYOL performance?
- Basis in paper: [explicit] "Perhaps a better contrastive loss is needed as a foundation to connect to the non-linear BYOL." (Introduction) and "This leaves the door open for better contrastive methods to be used and subsequently converted to improved non-contrastive objectives."
- Why unresolved: MINC matches Linear BYOL but gaps with non-linear BYOL (0.711 vs 0.743 top-1), suggesting the Spectral Contrastive foundation may limit representational capacity.
- What evidence would resolve it: Applying the MINC conversion procedure to alternative contrastive losses (e.g., InfoNCE variants) and demonstrating performance competitive with non-linear BYOL.

### Open Question 2
- Question: Why does α = 2 (χ²-divergence) outperform other α-divergences in MINC when f-MICL shows smaller differences?
- Basis in paper: [explicit] "This result is surprising at first, due to the results for f-MICL (Lu et al., 2024), which showed much smaller differences between f-divergences." The paper observes dramatic performance drops for α ≠ 2 but lacks theoretical explanation.
- Why unresolved: The scalar transformation tα fundamentally changes MINC's behavior vs. f-MICL, but the theoretical implications of this difference remain unexplored.
- What evidence would resolve it: Theoretical analysis of how tα interacts with the squared dot product reformulation, or empirical sweep across more α values with controlled experiments isolating the transformation effect.

### Open Question 3
- Question: How does MINC generalize to domains beyond ImageNet classification (e.g., video, NLP, RL)?
- Basis in paper: [inferred] The paper evaluates only on ImageNet with ResNet-50, despite claiming the method is broadly applicable to "unlabeled data." The related work section discusses RL applications of spectral methods, suggesting cross-domain potential.
- Why unresolved: No experiments validate whether the power iteration formulation and GHA-based orthogonality transfer to modalities with different structural properties (temporal dependencies, discrete tokens).
- What evidence would resolve it: Benchmarking MINC on standard SSL benchmarks in other domains (e.g., video with ViT, text with transformers, or RL representation learning) and comparing to domain-specific baselines.

## Limitations
- The exact formulation of the spectral contrastive loss t_α(·) for α=2 is not explicitly given, potentially affecting exact reproduction
- The inner scale s (inverse temperature) is mentioned as a hyperparameter but not specified numerically
- Evaluation details (linear probe training schedule, optimizer) are omitted, potentially affecting final accuracy comparisons
- MINC matches Linear BYOL but doesn't reach full BYOL with MLP predictor, suggesting representational capacity limitations

## Confidence
- **High confidence**: MINC converts contrastive losses to non-contrastive form via power iteration and GHA successfully prevents collapse; this is directly validated in controlled ablations
- **Medium confidence**: MINC consistently outperforms Spectral Contrastive Loss across batch sizes (0.711 vs 0.701 at B=4096); ablation studies support β=0.8 as optimal, though exact gain depends on unreported s parameter
- **Low confidence**: Claims about bridging contrastive/non-contrastive methods are suggestive but not rigorously compared to full BYOL with predictor; gap to 74.3% BYOL accuracy remains unexplained

## Next Checks
1. Reproduce MINC vs Spectral Contrastive baseline on ImageNet at batch size 1408 (300 epochs) to verify the 0.710 vs. 0.701 accuracy gain
2. Implement GHA ablation: remove LT[·] transform while keeping EMA/target network to confirm collapse to constant representations
3. Conduct β sensitivity sweep (0.0 to 0.95) to verify peak performance at β=0.8 with degradation at extremes