---
ver: rpa2
title: 'FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts
  for General Continual Learning'
arxiv_id: '2602.01976'
source_url: https://arxiv.org/abs/2602.01976
tags:
- uni00000013
- uni00000011
- uni00000015
- uni00000018
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FlyPrompt addresses the challenge of general continual learning
  (GCL), where models must learn from single-pass, non-stationary data streams without
  clear task boundaries. Inspired by the fruit fly's hierarchical memory system, it
  decomposes GCL into expert routing and expert competence improvement.
---

# FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning

## Quick Facts
- **arXiv ID:** 2602.01976
- **Source URL:** https://arxiv.org/abs/2602.01976
- **Reference count:** 40
- **Primary result:** Up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200 respectively in general continual learning

## Executive Summary
FlyPrompt addresses the challenge of general continual learning (GCL) where models must learn from single-pass, non-stationary data streams without clear task boundaries. Inspired by the fruit fly's hierarchical memory system, it decomposes GCL into expert routing and expert competence improvement. The method introduces a randomly expanded analytic router (REAR) for instance-level expert activation and a temporal ensemble of output heads (TE2) to dynamically adapt decision boundaries over time. Theoretically, it shows that robust and forward-only routing can be achieved by employing sufficiently rich random expansions and moderate regularization. Empirically, FlyPrompt achieves significant gains over state-of-the-art baselines across multiple datasets and pretrained model paradigms.

## Method Summary
FlyPrompt is a general continual learning framework that combines a randomly expanded analytic router (REAR) with a temporal ensemble of experts (TE2). The REAR projects backbone embeddings into a high-dimensional sparse space using a fixed random matrix and solves a closed-form ridge regression solution online, avoiding gradient-based routing updates that suffer from drift in GCL. The TE2 maintains multiple output heads with varying temporal windows (via EMA decay rates) to adapt to non-stationary drift while retaining long-term memory. Each expert maintains an online head (updated via SGD) and multiple EMA heads with distinct decay rates. Inference aggregates these via SoftMax + element-wise maximum. The method also employs an average-prompt warm start to accelerate convergence in the single-pass GCL setting.

## Key Results
- Achieves up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200 respectively
- Consistently outperforms existing methods across datasets and pretrained model paradigms
- Demonstrates effectiveness in resolving GCL challenges through ablation studies and oracle router checks

## Why This Works (Mechanism)

### Mechanism 1: Random Expanded Analytic Router (REAR)
- **Claim:** REAR enables robust instance-level routing in a single forward pass without gradient-based training, theoretically ensuring low misrouting probability given sufficient expansion dimension.
- **Mechanism:** Projects backbone embeddings into high-dimensional sparse space using fixed random matrix and ReLU activation. Accumulates second-order statistics online and solves closed-form ridge regression solution $\hat{U}^\top = (G+\lambda I)^{-1}Q$.
- **Core assumption:** Random expansion preserves sufficient structure for linear separability of experts in expanded space, with regularization $\lambda$ stabilizing the solution against noise.
- **Evidence anchors:** [Section 3.1] "calculation of $\hat{U}$ is only needed once upon evaluation... requires no gradient updates"; [Theorem 1] "Robust and forward-only routing can be achieved by employing sufficiently rich random expansions and moderate regularization"; Contrast: PASs-MoE notes jointly updating routers and experts causes "misaligned co-drift"; REAR avoids this by decoupling routing updates from gradient flow.
- **Break condition:** Performance degrades if expansion dimension $M$ is too small (increasing approximation error) or regularization $\lambda$ is too large (increasing bias).

### Mechanism 2: Temporal Ensemble of Experts (TE2)
- **Claim:** Maintaining a bank of output heads with varying temporal windows allows the model to adapt to non-stationary drift while retaining long-term memory, resolving the bias-variance trade-off inherent in single-window approaches.
- **Mechanism:** Each expert maintains an "online" head (updated via SGD) and multiple EMA heads with distinct decay rates $\alpha \in \{0.9, 0.99\}$. Online head captures rapid adaptation while EMA heads provide stability.
- **Core assumption:** At any given time $t$, the input stream exhibits a specific drift rate that aligns well with at least one of the pre-defined EMA decay windows.
- **Evidence anchors:** [Section 3.2] "Different EMA heads can align better with different segments... leading to more adaptive predictions"; [Theorem 2] "A geometric bank of EMA windows ensures that, at any time, one head is near the optimal trade-off"; [Section 4.2] "Two EMA heads... achieve the best trade-off... aligning with neurobiological findings".
- **Break condition:** Fails if data drift is too fast for the fastest head (lag) or if stream is so stable that noisy online head dominates performance.

### Mechanism 3: Average-Prompt Warm Start
- **Claim:** Initializing a new expert's prompt as the average of previous prompts accelerates convergence in single-pass GCL setting by providing informed starting point.
- **Mechanism:** When new task $t$ begins, its prompt $p_t$ is initialized as $p_t = \frac{1}{t-1} \sum_{i=1}^{t-1} p_i$ rather than randomly.
- **Core assumption:** Features learned in previous tasks share semantic relevance with new tasks, valid given strong priors of pretrained backbones.
- **Evidence anchors:** [Section 3.2] "This average-prompt warm start provides a more informed initialization... and empirically accelerates convergence"; Contrast: LLaVA-CMoE notes "naive model expansion leads to rapid... overfitting"; FlyPrompt mitigates this via inheritance.
- **Break condition:** Performance may degrade if tasks are highly dissimilar, causing "informed" initialization to act as strong negative bias.

## Foundational Learning

### Concept: Ridge Regression / Analytic Learning
- **Why needed here:** REAR relies on solving linear system $(G+\lambda I)^{-1}Q$ rather than gradient descent. Must understand bias-variance trade-off introduced by regularization term $\lambda I$.
- **Quick check question:** If Gram matrix $G$ is singular, what does the $\lambda I$ term ensure regarding solution existence?

### Concept: Mixture of Experts (MoE) Routing
- **Why needed here:** FlyPrompt is modular system where inputs are routed to specific sub-networks (experts).
- **Quick check question:** Why does FlyPrompt compute routing scores in fixed random feature space rather than learning router network (like learned MLP)?

### Concept: Exponential Moving Average (EMA)
- **Why needed here:** TE2 component relies on EMA heads with varying decay rates to simulate biological multi-timescale memory.
- **Quick check question:** If EMA head has decay rate $\alpha=0.99$, how many steps of history does it roughly average over? (Approximation: $1/(1-\alpha)$).

## Architecture Onboarding

- **Component map:** Input -> Backbone (Frozen ViT) -> REAR (Random Projection + Ridge Router) -> Select Expert -> Apply Prompt -> TE2 Heads (Online + EMA) -> Max-Ensemble
- **Critical path:**
  1. **Inference:** Input → Backbone → REAR Project → Ridge Router → Select Expert → Apply Prompt → TE2 Heads → Max-Ensemble
  2. **Training (Online):** Input → Select Expert (via identity or oracle during session) → Update Prompt & Online Head (SGD) → Update EMA Heads → Accumulate REAR Stats (G, Q)
  3. **Post-Training:** Solve Ridge Regression $\hat{U}^\top = (G+\lambda I)^{-1}Q$ (Done once per session or end of stream)
- **Design tradeoffs:**
  - **Dimension $M$ (REAR):** Higher $M$ improves separability (Theorem 1) but costs $O(M^2)$ memory for $G$
  - **# of EMA Heads:** More heads cover more drift speeds but increase parameter count (though lightweight)
  - **Ridge $\lambda$:** Too small → noise amplification; Too large → underfitting/bias
- **Failure signatures:**
  - **Router Collapse:** If $M$ is too low or $\lambda$ is wrong, routing scores may flatten, causing model to revert to single dominant expert
  - **EMA Lag:** If stream changes drastically (high drift) and fastest decay head is still too slow, accuracy will drop immediately after boundary transition
  - **Catastrophic Forgetting:** If prompts are not adequately isolated or router fails to select correct expert for old data
- **First 3 experiments:**
  1. **Ablation on $M$ & $\lambda$:** Run REAR with $M \in \{1000, 5000, 10000\}$ and $\lambda \in \{10^4, 10^6\}$ to verify Theorem 1's sensitivity on validation split (Ref: Fig 5)
  2. **Oracle Router Check:** Use "Oracle" setting (forcing correct expert selection) to measure upper bound of expert competence. If this is low, issue is experts (TE2), not router (Ref: Fig 2c)
  3. **TE2 Ensemble Analysis:** Test different aggregation strategies (Mean vs. SoftMax+Max) to validate choice of element-wise maximum for handling distribution shifts

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the fixed composition of EMA decay rates in the Temporal Ensemble (TE2) be adapted dynamically to changing data drift rates to further enhance robustness?
- **Basis in paper:** [explicit] Conclusion states current method relies on fixed decay rates and suggests "adapting these dynamically to data drift could enhance robustness"
- **Why unresolved:** Current implementation uses static geometric bank of windows (e.g., $\alpha \in \{0.9, 0.99\}$), assuming fixed trade-off but not adapting to specific non-stationary dynamics of current stream segment
- **What evidence would resolve it:** Experiments incorporating drift-detection mechanism that adjusts $\alpha$ values in real-time, comparing performance against fixed grid on high-variance GCL streams

### Open Question 2
- **Question:** How does FlyPrompt perform under extreme long-tailed class distributions, and does the Random Expanded Analytic Router (REAR) maintain routing accuracy for tail classes?
- **Basis in paper:** [explicit] Conclusion lists "performance under extreme long-tailed distributions" as limitation warranting further study
- **Why unresolved:** Analysis and experiments focus on standard Si-Blurry datasets which may not capture severity of distribution skew found in real-world long-tailed data, potentially biasing ridge regression router towards head classes
- **What evidence would resolve it:** Empirical evaluation on GCL benchmarks specifically designed with high class imbalance (e.g., ImageNet-LT converted to stream) to analyze per-class accuracy drops and router confusion

### Open Question 3
- **Question:** Is the fixed random projection matrix $R$ in REAR theoretically optimal for routing, or would learnable or dynamically adjusted projection layer provide better separation for high-dimensional, overlapping class streams?
- **Basis in paper:** [inferred] Method uses fixed Gaussian matrix $R$ to mimic biological sparse expansion, and Theorem 1 relies on approximation error bounds for finite random features without training them
- **Why unresolved:** While fixed projections are computationally efficient (forward-only), they may lack representational specificity of trained projections, potentially increasing "misrouting probability" in complex semantic spaces where classes overlap significantly
- **What evidence would resolve it:** Comparative study where projection $R$ is fine-tuned (e.g., via gradient descent on small subset of replay data) versus fixed version, measuring impact on population excess risk $R(\hat{U})$

## Limitations
- The method's robustness to extreme non-stationarity (e.g., concept drift faster than fastest EMA head) or highly dissimilar task sequences is not thoroughly tested
- The average-prompt warm start assumes task similarity, which may not hold in all GCL scenarios
- The theoretical analysis relies on idealized assumptions about random feature separability and drift alignment

## Confidence
- **High:** The empirical results on CIFAR-100, ImageNet-R, and CUB-200 are robust, with consistent gains over state-of-the-art baselines. The ablation studies (Fig 5) and oracle router check (Fig 2c) provide strong evidence for the method's effectiveness.
- **Medium:** The theoretical guarantees (Theorems 1-2) are sound under their assumptions, but the practical impact of hyperparameters like $M$ and $\lambda$ on real-world data streams requires further validation.
- **Low:** The generalizability to extreme non-stationarity or highly dissimilar tasks is not thoroughly tested, leaving potential failure modes unexplored.

## Next Checks
1. **Hyperparameter Sensitivity:** Conduct grid search on $M$ and $\lambda$ across all datasets to validate Theorem 1's sensitivity and identify optimal ranges for different data distributions
2. **Extreme Drift Robustness:** Test FlyPrompt on synthetic data stream with rapidly changing concepts (e.g., alternating between CIFAR-10 and ImageNet-R every few batches) to assess performance under extreme non-stationarity
3. **Task Dissimilarity Stress Test:** Evaluate FlyPrompt on sequence of highly dissimilar tasks (e.g., natural images, medical images, and text embeddings) to test limits of average-prompt warm start assumption