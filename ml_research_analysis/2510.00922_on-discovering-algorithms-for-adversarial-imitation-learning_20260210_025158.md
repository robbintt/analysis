---
ver: rpa2
title: On Discovering Algorithms for Adversarial Imitation Learning
arxiv_id: '2510.00922'
source_url: https://arxiv.org/abs/2510.00922
tags:
- reward
- learning
- policy
- functions
- dail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the instability problem in Adversarial Imitation
  Learning (AIL) by focusing on the often-overlooked reward assignment (RA) function,
  which maps discriminator outputs to rewards for policy optimization. The authors
  propose discovering RA functions via LLM-guided evolutionary search, resulting in
  Discovered Adversarial Imitation Learning (DAIL), the first meta-learned AIL algorithm.
---

# On Discovering Algorithms for Adversarial Imitation Learning

## Quick Facts
- arXiv ID: 2510.00922
- Source URL: https://arxiv.org/abs/2510.00922
- Reference count: 40
- Primary result: First meta-learned AIL algorithm via LLM-guided evolutionary discovery of reward assignment functions

## Executive Summary
This paper addresses the persistent instability in Adversarial Imitation Learning (AIL) by focusing on the reward assignment (RA) function - the component that maps discriminator outputs to rewards for policy optimization. The authors propose DAIL (Discovered Adversarial Imitation Learning), which uses LLM-guided evolutionary search to discover RA functions optimized for a specific environment (Minatar SpaceInvaders). DAIL achieves significant performance improvements over human-designed baselines (GAIL, AIRL, FAIRL, GAIL-heuristic) across unseen Brax and Minatar environments, reducing Wasserstein distance by 20% and improving normalized returns by 12.5%. The discovered bounded S-shaped RA function provides more informative learning signals, leading to sharper policy entropy and improved training stability.

## Method Summary
DAIL employs evolutionary search with LLM guidance to discover optimal RA functions. The process iterates through populations of candidate RA functions, where each function is represented as executable code and evaluated via meta-training on a target environment. The LLM performs crossover operations between parent functions using in-context learning, guided by their fitness scores. Selection favors functions that minimize discriminator loss and maximize normalized return. After discovery on Minatar SpaceInvaders, the resulting RA function is fixed and applied across unseen environments. This approach relaxes the convexity constraint traditionally required for f-divergence-based RA functions, enabling exploration of non-standard reward mappings that empirically outperform classical designs.

## Key Results
- DAIL reduces Wasserstein distance by 20% and improves normalized returns by 12.5% compared to GAIL baseline
- The discovered RA function is a bounded S-shaped function that outperforms human-designed alternatives across all tested environments
- DAIL generalizes to unseen policy optimization algorithms (A2C), demonstrating cross-algorithm applicability
- Ablation studies confirm the critical role of RA functions in AIL stability and performance

## Why This Works (Mechanism)
AIL instability stems from the adversarial training dynamics between the discriminator and policy. Traditional RA functions (GAIL, AIRL, FAIRL) map discriminator outputs linearly or through heuristic transformations, but these human-designed mappings may not provide optimal learning signals. DAIL's discovered bounded S-shaped function creates a more informative reward landscape by saturating at appropriate thresholds, preventing extreme gradients while maintaining sensitivity to meaningful discriminator outputs. This design balances exploration and exploitation during training, reducing the oscillation and divergence commonly observed in standard AIL. The evolutionary search explores a broader space of reward mappings than human intuition alone, finding solutions that better align discriminator confidence with policy improvement signals.

## Foundational Learning

**Adversarial Imitation Learning (AIL)** - Framework where a discriminator distinguishes expert from policy-generated trajectories while the policy tries to fool it. Needed to understand the adversarial dynamics; quick check: verify discriminator loss decreases while policy loss increases during training.

**Reward Assignment (RA) Functions** - Functions mapping discriminator outputs to rewards for policy optimization. Critical because they bridge discriminator learning with policy improvement; quick check: examine how different RA functions affect reward magnitude and sign.

**f-divergences and Convexity Constraints** - Mathematical framework for measuring distribution similarity in AIL. Traditionally requires convex RA functions for theoretical guarantees; quick check: verify RA function satisfies convexity if theoretical convergence is desired.

**Evolutionary Search with LLM Guidance** - Optimization paradigm combining population-based search with large language model assistance. Enables efficient exploration of complex function spaces; quick check: monitor population diversity and fitness improvement across generations.

**Meta-learning in AIL** - Learning to learn approach where components (RA functions) are optimized for generalization. Shifts focus from hand-design to automated discovery; quick check: test discovered components on held-out environments.

## Architecture Onboarding

**Component Map**: Expert Demonstrations -> Policy -> Policy Rollouts -> Discriminator -> RA Function -> Rewards -> Policy Update

**Critical Path**: Expert Demonstrations → Policy → Discriminator → RA Function → Rewards → Policy Update → Better Policy

**Design Tradeoffs**: Evolutionary search provides broad exploration but is computationally expensive; relaxing convexity constraints enables better empirical performance but sacrifices theoretical guarantees; LLM guidance accelerates search but adds complexity.

**Failure Signatures**: Discriminator collapse (outputs all ones or zeros), policy collapse (produces random actions), unstable training (oscillating losses), poor generalization (overfitting to training environment).

**First Experiments**: 1) Verify DAIL outperforms GAIL on Minatar SpaceInvaders training environment, 2) Test DAIL's RA function on unseen Minatar games, 3) Evaluate DAIL with A2C policy optimization algorithm.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can time-aware reward assignment functions that condition on training state (e.g., number of updates remaining, current loss, observed log-ratios) outperform static RA functions in AIL?
- Basis in paper: [explicit] "the RA function of DAIL remains static throughout training and does not adapt to the training state... Exploring time-aware RA functions—those that condition on the training state—could yield richer, more informative learning signals"
- Why unresolved: The current DAIL RA function is fixed throughout training; adaptive functions may better handle non-stationary dynamics in adversarial training.
- What evidence would resolve it: Comparing DAIL against variants with RA functions conditioned on training progress metrics across multiple environments.

### Open Question 2
- Question: Can theoretical convergence guarantees be established for discovered RA functions that do not correspond to valid f-divergences?
- Basis in paper: [explicit] "the discovered RA function r_disc does not correspond to a valid f-divergence and therefore lacks theoretical guarantees"
- Why unresolved: The relaxation of convexity constraints enabled exploration beyond classical divergence theory, sacrificing theoretical foundations for empirical performance.
- What evidence would resolve it: Deriving convergence bounds for bounded S-shaped RA functions, or identifying sufficient conditions under which such functions preserve AIL convergence properties.

### Open Question 3
- Question: Does incorporating environment information and training state into the LLM's context during evolutionary search produce more effective RA functions?
- Basis in paper: [explicit] "including more information into the LLM's context such as environment information and training state may facilitate more effective crossovers"
- Why unresolved: Current crossover generation relies only on parent function code and fitness scores, without contextual information about the domain or learning dynamics.
- What evidence would resolve it: Comparing RA functions discovered with enriched LLM context against the current approach across diverse environment suites.

### Open Question 4
- Question: Will RA functions discovered on a single environment generalize to significantly different domains such as robotic manipulation or autonomous driving?
- Basis in paper: [inferred] Discovery was conducted only on Minatar SpaceInvaders; generalization was tested on Brax/MuJoCo and other Minatar games, but not real-world or high-dimensional continuous control domains.
- Why unresolved: The meta-learning objective optimizes for performance on one environment; transfer to qualitatively different domains remains unverified.
- What evidence would resolve it: Evaluating DAIL's discovered RA function on diverse real-world IL benchmarks (e.g., physical robot demonstrations, autonomous driving datasets).

## Limitations

- Computational overhead of evolutionary search scales poorly with search space complexity
- Evaluation scope limited to specific domain types without extensive testing on real-world robotics applications
- Performance advantage appears context-dependent and may not generalize to all environment types
- Discovered RA function lacks theoretical guarantees about optimality or universal applicability

## Confidence

- **High Confidence**: DAIL's superior empirical performance over human-designed baselines in tested environments
- **Medium Confidence**: Claims about RA function's critical role in AIL stability (supported by ablation studies but requiring broader validation)
- **Medium Confidence**: Generalization to unseen policy optimization algorithms (A2C results are promising but limited)

## Next Checks

1. Test DAIL's performance on high-dimensional continuous control benchmarks (e.g., MuJoCo locomotion tasks) to assess scalability
2. Conduct ablation studies varying the evolutionary search budget to quantify sensitivity to computational resources
3. Evaluate DAIL's robustness when trained with suboptimal or noisy expert demonstrations to assess practical applicability