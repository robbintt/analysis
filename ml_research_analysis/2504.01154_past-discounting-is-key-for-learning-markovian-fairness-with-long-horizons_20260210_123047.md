---
ver: rpa2
title: Past-Discounting is Key for Learning Markovian Fairness with Long Horizons
arxiv_id: '2504.01154'
source_url: https://arxiv.org/abs/2504.01154
tags:
- fairness
- time
- learning
- past
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical scalability issue in long-horizon
  fairness learning: perfect-recall methods, which track complete historical utility,
  suffer from unbounded state-space growth that prevents learning convergence. To
  address this, the authors introduce past-discounting, a framework that applies geometrically
  decaying weights to historical utilities, inspired by behavioral economics showing
  humans discount distant past events.'
---

# Past-Discounting is Key for Learning Markovian Fairness with Long Horizons

## Quick Facts
- arXiv ID: 2504.01154
- Source URL: https://arxiv.org/abs/2504.01154
- Authors: Ashwin Kumar; William Yeoh
- Reference count: 33
- Key outcome: Past-discounting enables scalable long-horizon fairness learning by bounding state space and maintaining non-vanishing gradients

## Executive Summary
This paper identifies a critical scalability issue in long-horizon fairness learning: perfect-recall methods, which track complete historical utility, suffer from unbounded state-space growth that prevents learning convergence. To address this, the authors introduce past-discounting, a framework that applies geometrically decaying weights to historical utilities, inspired by behavioral economics showing humans discount distant past events. The key theoretical contribution proves that past-discounting guarantees a bounded, horizon-independent state space, while perfect-recall lacks this property. Experiments using PPO agents in a dynamic resource allocation task demonstrate that agents with past-discounting successfully learn fair policies across horizons up to 10,000 steps, while perfect-recall agents fail to converge as horizon increases.

## Method Summary
The method introduces past-discounting to address scalability issues in long-horizon fairness learning. It maintains a discounted memory Z^t = γp·Z^{t-1} + u^t of historical utilities u^t, where γp < 1 ensures bounded state space. This provides a Markovian sufficient statistic without requiring unbounded auxiliary variables. The framework integrates with PPO agents in a dynamic resource allocation task, using potential-based shaping rewards r^t = (1-λ)·Σu^t + λ·[W(Z^t) - W(Z^{t-1})] where W is a welfare function (Egalitarian or Log-Nash). The method provides a principled trade-off between instantaneous and perfect-recall fairness, enabling tractable long-horizon fairness learning.

## Key Results
- Past-discounting guarantees a bounded, horizon-independent state space, while perfect-recall methods lack this property
- Agents with past-discounting successfully learn fair policies across horizons up to 10,000 steps
- Perfect-recall agents fail to converge as horizon increases due to unbounded state-space growth
- The method provides a principled trade-off between instantaneous and perfect-recall fairness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Past-discounting with γp < 1 guarantees a horizon-independent bounded state space, enabling tractable long-horizon learning.
- Mechanism: The recursive update Z^t = γp·Z^{t-1} + u^t creates a convergent geometric series bounded by umax/(1-γp), preventing the linear growth (t+1)·umax that afflicts perfect-recall methods.
- Core assumption: Utilities are bounded in [0, umax] and γp is strictly less than 1.
- Evidence anchors: [abstract] "showing past-discounting guarantees a bounded, horizon-independent state space, a property that we prove perfect-recall methods lack"; [Theorem 4, page 5] Formal proof that 0 ≤ Z^t_i ≤ umax/(1-γp) for all t ≥ 0.

### Mechanism 2
- Claim: Past-discounted memory provides a sufficient Markovian statistic without requiring unbounded auxiliary variables.
- Mechanism: Unlike perfect-recall averaging (which requires tracking time index t), discounted averaging maintains bounded denominator d^t ≤ 1/(1-γp) that updates from prior state alone.
- Core assumption: Approximate fairness from discounted utilities is acceptable; exact historical records are not required.
- Evidence anchors: [Theorem 6, page 5-6] Proves (Z^t, d^t) is Markov-sufficient with both coordinates bounded; [Theorem 5, page 5] Shows perfect-recall averaging fails Markov property without unbounded t augmentation.

### Mechanism 3
- Claim: Past-discounting maintains non-vanishing gradient signals throughout long episodes.
- Mechanism: Marginal weight of current utility ∂Z^t/∂u^t → (1-γp) as t→∞, whereas perfect-recall averaging has weight 1/t → 0, causing vanishing responsiveness.
- Core assumption: Reward signals based on welfare function changes must remain detectable for learning.
- Evidence anchors: [Theorem 6(iii), page 6] Formal proof of non-vanishing responsiveness limit; [page 11, appendix C] Discusses "vanishing responsiveness" as failure mode for perfect-recall averaging.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The paper frames fairness as a non-Markovian problem requiring state augmentation; understanding why Markovian structure enables tractable RL is essential.
  - Quick check question: Why does non-Markovian structure increase sample complexity?

- Concept: Geometric Discounting
  - Why needed here: The method applies future-style discounting to past utilities; understanding standard γ for future rewards contextualizes γp.
  - Quick check question: What is Σγ^k from k=0 to ∞ when γ = 0.9?

- Concept: Social Welfare Functions (Nash, Egalitarian)
  - Why needed here: Different welfare functions produce different reward densities; Egalitarian yields sparse signals (only when worst-off agent changes), Nash is denser.
  - Quick check question: Which welfare function requires longer memory to learn effectively—sparse or dense signals?

## Architecture Onboarding

- Component map:
  - Fairness Memory Module -> Welfare Calculator -> Reward Composer -> Policy Network
  - Maintains Z^t (n-dimensional), updates via Z^t = γp·Z^{t-1} + u^t
  - Computes W(Z^t) using selected function (Egalitarian/Nash)
  - r^t = (1-λ)·Σu^t + λ·[W(Z^t) - W(Z^{t-1})]
  - PPO receiving concatenated [needs_vector, Z^{t-1}]

- Critical path:
  1. Initialize Z^0 = 0
  2. Each step: observe needs → select allocation → receive utilities → update Z → compute potential-based reward
  3. Z update must precede reward calculation for valid potential-based shaping

- Design tradeoffs:
  - γp = 0.99 (W_eff = 100): Robust default across welfare functions
  - Higher γp: Longer memory but slower adaptation; lower γp: Faster learning but may miss long-term patterns
  - λ = 0.9: Strong fairness emphasis; increase to 0.999 for sparse Egalitarian signals

- Failure signatures:
  - Performance degrades as horizon increases → Check if γp = 1.0 (perfect recall)
  - High Gini despite convergence → γp too low or λ too small
  - Zero fairness gradients in long episodes → Using averaged perfect-recall (vanishing responsiveness)

- First 3 experiments:
  1. Baseline: γp ∈ {0.0, 0.9, 0.99, 0.999, 1.0} at T=1000; verify γp=1.0 matches γp=0.999 at this horizon
  2. Horizon scaling: γp=0.99, T ∈ {100, 500, 1000, 5000, 10000}; confirm stable Gini across all horizons
  3. Welfare sensitivity: Compare Nash vs. Egalitarian with γp=0.9 vs. 0.99 at T=5000; validate sparser signals need longer memory

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the discount factor γp be automatically or adaptively selected based on environment characteristics and the fairness welfare function being optimized?
- Basis in paper: [explicit] The authors state "selecting an appropriate discount factor involves balancing the complexity of the environment with the nature of the fairness signal. Denser signals may allow for shorter effective memories, while sparser signals necessitate longer ones" but provide no systematic method for this selection.
- Why unresolved: The paper empirically shows optimal γp differs between Nash Welfare (γp=0.9 sufficient) and Egalitarian Welfare (γp=0.99 required), but offers only heuristics for practitioners.
- What evidence would resolve it: A theoretical or learned relationship between welfare function properties, environment dynamics, and optimal γp; or an adaptive mechanism that adjusts γp online.

### Open Question 2
- Question: What memory structures beyond scalar past-discounting can satisfy the Schur-stability condition (ρ(A)<1) while capturing richer inter-agent fairness relationships?
- Basis in paper: [explicit] Section 5.4 states the LTI framework "outlines a design rule for richer memory paradigms that allow cross-agent mixing or modeling memory across multiple time scales as long as A is Schur-stable. This provides a rigorous foundation for extending our work."
- Why unresolved: The paper only implements A=γpI (scalar discounting) but theoretically permits any matrix A with spectral radius <1, enabling potentially more expressive memory mechanisms.
- What evidence would resolve it: Construction and empirical evaluation of non-diagonal memory transition matrices that encode cross-agent dependencies or multi-scale temporal patterns.

### Open Question 3
- Question: How does past-discounted fairness learning scale to decentralized multi-agent settings where no central allocator has access to all agents' utilities?
- Basis in paper: [inferred] All experiments use a centralized PPO agent with full observability of all agents' needs and memory states. The framework's applicability to decentralized MARL (where each agent observes only local information) is not addressed.
- Why unresolved: Decentralization introduces partial observability and coordination challenges that may require different memory-sharing or estimation mechanisms.
- What evidence would resolve it: Experiments in decentralized settings where each agent maintains local fairness memory and coordinates through communication or emergent conventions.

### Open Question 4
- Question: Does past-discounting interact favorably with function approximation in deep RL, particularly regarding credit assignment across the effective memory window?
- Basis in paper: [inferred] The paper uses tabular-like PPO with small state vectors (20 dimensions). While the bounded state space is theoretically tractable, the interaction between past-discounting and neural network function approximation—especially regarding temporal credit assignment—remains unexplored.
- Why unresolved: Neural networks may implicitly learn different effective discount factors than specified, and the relationship between explicit memory decay and implicit network memory is unclear.
- What evidence would resolve it: Analysis of learned representations in deep networks trained with past-discounted fairness objectives, measuring alignment between explicit and implicit temporal weighting.

## Limitations
- Limited to single synthetic environment; generalization to other domains requires validation
- Theoretical claims about gradient preservation and behavioral economics motivation lack empirical or corpus support
- Only tested with centralized PPO agent; scalability to decentralized multi-agent settings is unexplored

## Confidence

- **High confidence**: The bounded state-space proof and Markovian sufficiency claims (Mechanisms 1-2) are rigorously proven and mathematically sound.
- **Medium confidence**: The experimental demonstration of scalability across horizons is compelling but limited to one domain, requiring broader validation.
- **Low confidence**: The gradient preservation mechanism (Mechanism 3) and behavioral economics motivation lack empirical or corpus support.

## Next Checks

1. **Cross-domain validation**: Test past-discounting on a different multi-agent fairness task (e.g., traffic routing, load balancing) to verify the bounded-state and convergence benefits generalize beyond resource allocation.

2. **Mechanism isolation**: Run controlled experiments comparing past-discounting against averaged perfect-recall with explicit time augmentation to definitively demonstrate the gradient preservation advantage.

3. **Parameter sensitivity analysis**: Systematically vary γp and λ across a wider range (including γp > 0.99) to map the full performance landscape and identify optimal settings for different welfare functions.