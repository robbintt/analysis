---
ver: rpa2
title: Can Language Models Understand Social Behavior in Clinical Conversations?
arxiv_id: '2505.04152'
source_url: https://arxiv.org/abs/2505.04152
tags:
- social
- patient
- signals
- provider
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores whether large language models (LLMs) can track
  social signals in clinical conversations without explicit training. The authors
  introduce a prompting-based pipeline called SocialLM that uses role specification,
  task description, and structured output formatting to guide models in detecting
  20 different social signals from patient-provider transcripts.
---

# Can Language Models Understand Social Behavior in Clinical Conversations?

## Quick Facts
- arXiv ID: 2505.04152
- Source URL: https://arxiv.org/abs/2505.04152
- Reference count: 40
- Key outcome: LLMs can detect social signals in clinical conversations without fine-tuning using prompting, with performance varying by model, signal type, and transcript segment.

## Executive Summary
This paper investigates whether large language models can track social behaviors in clinical conversations without explicit training. The authors introduce SocialLM, a prompting-based pipeline that uses role specification, task description, and structured output formatting to guide models in detecting 20 different social signals from patient-provider transcripts. Evaluating three models (FLAN-T5, Gemma2, and LLaMA) across multiple prompting strategies, they find that while smaller models like FLAN-T5 can perform competitively, LLaMA generally outperforms others, especially with Few-Shot and Chain-of-Thought configurations. The study demonstrates that LLMs can capture social behaviors in clinical contexts, but effectiveness depends on model choice, prompt design, and conversation stage.

## Method Summary
The study evaluates whether LLMs can classify social signals in clinical conversations without fine-tuning using a prompting-based approach. The Establishing Focus (EF) corpus provides 91 patient-provider visits, segmented into 3-minute slices (508 total segments), with social behaviors annotated using the RIAS framework. Three LLMs (FLAN-T5-base 250M, Gemma2-2B, LLaMA3.1-405B with 4-bit quantization) are evaluated across four prompting strategies (Zero-Shot, Few-Shot, Chain-of-Thought, FS-CoT). The task involves binary classification of 20 social signals (e.g., warmth, dominance, empathy) from transcribed segments, with performance measured using balanced accuracy to account for class imbalance.

## Key Results
- LLaMA3.1-405B with Few-Shot + Chain-of-Thought achieves the highest balanced accuracy (0.588) across all models and configurations.
- FLAN-T5-Base performs competitively in some signals despite being significantly smaller, particularly in Zero-Shot configurations.
- Model performance varies significantly by social signal type, with Type-II signals (e.g., irritation, sadness) being more difficult to detect than Type-I signals.
- Transcript segment location affects performance, with middle segments showing higher accuracy due to better conversational context and turn-taking ratios.

## Why This Works (Mechanism)

### Mechanism 1: In-Context Knowledge Activation via Role Specification
Large Language Models can map clinical text to abstract social constructs by leveraging pre-trained priors when explicitly framed as a "behavior analyst." The model retrieves existing associations between linguistic markers and social labels from its pre-training corpus rather than learning new patterns from the dataset. This role specification constrains the output space to behavioral analysis, filtering out clinical diagnostic reasoning.

### Mechanism 2: Affective Linguistic Density as a Signal-to-Noise Filter
LLM performance is causally linked to the density of affective and cognitive linguistic markers. When transcripts contain low emotional tone words and high functional words, the signal-to-noise ratio for social behavior detection drops below the model's threshold. The models act as statistical pattern matchers for emotional tone rather than performing deep pragmatic inference.

### Mechanism 3: Architecture-Dependent Prompt Sensitivity
The efficacy of prompt augmentations depends on model architecture. Instruction-tuned models (FLAN-T5) thrive in Zero-shot due to strong task compliance priors, while general decoders (LLaMA) require Few-Shot + CoT to align reasoning. Larger parameter count alone is insufficient for niche social tasks without specific instruction tuning or in-context examples.

## Foundational Learning

- **Concept: RIAS Global Affect Ratings (Thin Slicing)**
  - Why needed: The entire system targets the RIAS framework, where social signals are complex aggregates rated on a 1-6 scale and binarized for prediction.
  - Quick check: Can you explain why the study converts the 1-6 RIAS scale into a binary classification (Neutral vs. High) for Type-I signals?

- **Concept: Label Imbalance & Balanced Accuracy**
  - Why needed: Clinical datasets are heavily imbalanced with most interactions being "normal" and extreme behaviors rare, making standard accuracy misleading.
  - Quick check: Why is "Balanced Accuracy" used instead of F1-score or standard accuracy to evaluate the LLM predictions in this paper?

- **Concept: Aleatoric vs. Epistemic Uncertainty in Text**
  - Why needed: The paper suggests using linguistic features (LIWC) to estimate difficulty (aleatoric) versus model configuration (epistemic) to gauge trust.
  - Quick check: According to the paper, does a transcript with high "function word" usage indicate an "easy" or "difficult" prediction case for the LLM?

## Architecture Onboarding

- **Component map:** Whisper large-v3 + Pyannote diarization -> 3-minute transcript slice -> Prompt Engine (Role + Task + Examples) -> Inference Core (FLAN-T5/Gemma2/LLaMA) -> Output Parser (extracts 0/1/Yes/No + reasoning)
- **Critical path:** The Prompt Design. Without the role specification ("You are a behavior analyst..."), the model defaults to generic medical reasoning and fails to detect nuanced social signals.
- **Design tradeoffs:** Local execution (privacy) limits model choice to quantized versions; Few-Shot improves LLaMA but hurts FLAN-T5 (distraction); Zero-shot is more scalable but less accurate for complex signals.
- **Failure signatures:** "Action Parallel to Voice" transcripts fail classification; FLAN-T5 frequently fails to output CoT reasoning; LLaMA shows performance variance between White and Non-White patient cohorts.
- **First 3 experiments:**
  1. Baseline Sanity Check: Run FLAN-T5 (Zero-Shot) vs. LLaMA (Zero-Shot) on a single transcript slice to verify baseline performance differences.
  2. Linguistic Stress Test: Take a "High Warmth" transcript and systematically remove positive tone words (LIWC category) to confirm LLM confidence drops.
  3. Segmentation Analysis: Feed the model "Start," "Middle," and "End" segments of a visit to validate performance variation across conversation stages.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do SocialLM-style pipelines perform when implemented in applied healthcare settings like medical training, telehealth triage, or patient support services? The current study uses a legacy dataset and offline metrics rather than measuring real-time utility or outcomes in live clinical environments.

- **Open Question 2:** How do the emotions of one speaker influence the social behaviors of the other in a clinical dyad, and can this mapping be modeled computationally? The current framework predicts social signals for each speaker independently without modeling the interaction effect where one party's emotional state drives the other's behavioral response.

- **Open Question 3:** Can segment-aware prompting or segment-specific calibration improve the interpretability and accuracy of LLM predictions given the variation in social signals across visit stages? The authors found significant variation between start, middle, and end segments but used a uniform prompting strategy.

## Limitations

- The study relies on a single annotated dataset (EF corpus) collected between 2002-2006, which may not reflect current clinical communication patterns or demographic diversity.
- Local execution constraints necessitated using quantized LLaMA and smaller models, potentially limiting performance compared to cloud-based full-parameter models.
- The prompting strategy shows signal-specific performance variation, with Type-II signals being particularly difficult to detect due to sparse positive examples and reliance on explicit lexical cues.

## Confidence

- **High Confidence:** The finding that prompting-based approaches can achieve social signal detection without fine-tuning is well-supported by balanced accuracy results across multiple models and configurations.
- **Medium Confidence:** The interpretation that LLMs rely on affective linguistic cues rather than deep pragmatic inference is supported by correlations with LIWC features, but the causal mechanism remains partially speculative.
- **Low Confidence:** The claim that larger models require specific prompt augmentations while smaller instruction-tuned models do not is based on observed patterns but lacks theoretical grounding about why architecture should influence prompt sensitivity.

## Next Checks

1. **Cross-Dataset Generalization:** Evaluate the SocialLM pipeline on a contemporary clinical conversation dataset with different demographics and communication patterns to verify performance is not dataset-specific.

2. **Pragmatic Inference Stress Test:** Design transcripts containing subtle social signals that lack explicit affective markers (e.g., sarcasm, indirect requests) to test whether the model relies on deep pragmatic inference or surface-level cues.

3. **Demographic Bias Audit:** Conduct a systematic analysis of model performance across age, gender, race, and socioeconomic status to quantify and understand the sources of observed disparities, particularly for larger models like LLaMA.