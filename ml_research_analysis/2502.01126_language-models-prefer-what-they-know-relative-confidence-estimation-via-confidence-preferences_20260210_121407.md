---
ver: rpa2
title: 'Language Models Prefer What They Know: Relative Confidence Estimation via
  Confidence Preferences'
arxiv_id: '2502.01126'
source_url: https://arxiv.org/abs/2502.01126
tags:
- confidence
- relative
- estimation
- question
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces relative confidence estimation for language
  models, where instead of directly rating confidence for each question, models compare
  pairs of questions and indicate which they're more confident about. This approach
  addresses the issue that models struggle with absolute confidence judgments, often
  producing coarse-grained scores that lack discriminative power.
---

# Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences

## Quick Facts
- arXiv ID: 2502.01126
- Source URL: https://arxiv.org/abs/2502.01126
- Authors: Vaishnavi Shrivastava; Ananya Kumar; Percy Liang
- Reference count: 16
- Primary result: Relative confidence estimation outperforms absolute methods by 3.5% in selective classification AUC

## Executive Summary
Language models struggle with absolute confidence judgments, often producing coarse-grained scores that lack discriminative power. This paper introduces relative confidence estimation, where instead of directly rating confidence for each question, models compare pairs of questions and indicate which they're more confident about. The approach treats questions as "players" in a series of matchups, using rank aggregation techniques like Elo rating, Bradley-Terry, and TrueSkill to convert relative preferences into confidence scores.

Evaluated on five state-of-the-art models (GPT-4, GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet, Llama 3.1 405B) across 14 challenging question-answering tasks, relative confidence estimation consistently outperforms absolute confidence estimation methods. The improvements are most significant for Llama 3.1 405B (6.1% gain) and GPT-4 (4.1% gain), demonstrating that relative comparisons lead to more reliable confidence estimates for selective prediction tasks.

## Method Summary
The paper proposes a novel approach to confidence estimation that leverages pairwise comparisons rather than absolute ratings. The method works by presenting the language model with pairs of questions and asking it to identify which question it's more confident about answering correctly. These pairwise preferences are then aggregated using established rank aggregation algorithms (Elo rating, Bradley-Terry, TrueSkill) to produce confidence scores for individual questions. This framework addresses the fundamental challenge that language models struggle with absolute confidence judgments but can more reliably make relative comparisons. The approach is evaluated through selective classification tasks where models choose which questions to answer based on their estimated confidence, measuring performance via area under the ROC curve (AUC).

## Key Results
- Relative confidence estimation achieves 3.5% average gains in selective classification AUC over direct absolute confidence estimation
- The method outperforms self-consistency approaches by 1.7% on average
- Llama 3.1 405B shows the largest improvement at 6.1% gain, followed by GPT-4 at 4.1% gain
- Across 14 diverse question-answering tasks, relative confidence estimation consistently outperforms absolute methods for all five tested models

## Why This Works (Mechanism)
Language models struggle with absolute confidence judgments because they lack calibrated internal representations of uncertainty and tend to produce coarse-grained scores. However, when comparing two questions side-by-side, models can more reliably identify which they understand better or which has clearer answer paths. This cognitive asymmetry - difficulty with absolute ratings but competence with relative comparisons - forms the foundation of the approach. By converting confidence estimation into a ranking problem and using established rank aggregation algorithms, the method leverages the model's comparative reasoning abilities while avoiding the pitfalls of direct confidence scoring.

## Foundational Learning
**Rank Aggregation**: The process of combining multiple pairwise preferences into a global ranking. Needed because pairwise comparisons alone don't directly provide confidence scores for individual items. Quick check: Does the aggregation method preserve the transitive properties of the pairwise preferences?

**Selective Classification**: A framework where models choose whether to answer questions based on confidence estimates, answering only those above a threshold. Needed to evaluate the practical utility of confidence estimation methods. Quick check: What threshold maximizes the F1 score on the validation set?

**Elo Rating System**: Originally developed for chess, this algorithm updates player ratings based on pairwise match outcomes. Needed as one of the rank aggregation methods for converting preferences to confidence scores. Quick check: Does the rating converge within a reasonable number of iterations?

**Bradley-Terry Model**: A probabilistic model for pairwise comparison data that estimates the probability of one item being preferred over another. Needed to provide a statistical foundation for rank aggregation. Quick check: Are the estimated parameters well-calibrated and do they sum to reasonable values?

**TrueSkill**: A Bayesian ranking algorithm that models both skill and uncertainty. Needed as an alternative rank aggregation method that can handle uncertainty in pairwise comparisons. Quick check: How sensitive are the results to the prior distributions specified in the model?

## Architecture Onboarding

**Component Map**: Question pairs -> LLM pairwise comparison -> Preference matrix -> Rank aggregation (Elo/Bradley-Terry/TrueSkill) -> Confidence scores -> Selective classification

**Critical Path**: The LLM generates pairwise preferences, which are aggregated into confidence scores used for selective classification decisions

**Design Tradeoffs**: Relative confidence requires more inference calls (for pairwise comparisons) but produces more reliable estimates. The choice of rank aggregation method involves balancing accuracy, computational efficiency, and convergence properties.

**Failure Signatures**: Poor performance may indicate that pairwise comparisons are inconsistent, rank aggregation fails to converge, or the model struggles with distinguishing between similar difficulty questions. The approach may be less effective when questions are uniformly difficult or when pairwise comparisons lack discriminative power.

**First Experiments**:
1. Compare Elo, Bradley-Terry, and TrueSkill aggregation methods on a small validation set to identify the most effective approach
2. Test pairwise comparison quality by examining consistency rates and identifying questions where the model struggles to make preferences
3. Evaluate selective classification performance with different confidence thresholds to find the optimal operating point

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Evaluation is limited to 14 question-answering tasks, potentially missing broader application domains
- The study focuses on zero-shot settings, leaving few-shot and fine-tuned scenarios unexplored
- Different rank aggregation methods have varying computational costs and convergence properties that weren't thoroughly compared
- Potential biases from pairwise comparison format, such as anchoring effects or annotator fatigue, weren't addressed

## Confidence
**High confidence**: Empirical results showing relative confidence estimation outperforming absolute confidence estimation across multiple models and tasks. The core insight about language models' comparative reasoning abilities is well-supported.

**Medium confidence**: Generalizability beyond tested question-answering tasks. While promising, effectiveness in other domains remains to be validated.

**Medium confidence**: Claims about reliability improvements. While selective classification metrics show gains, other reliability aspects weren't thoroughly examined.

## Next Checks
1. Test relative confidence estimation framework on non-QA tasks such as text summarization, translation, or code generation to evaluate broader applicability across different language model applications.

2. Conduct experiments with human evaluators using the pairwise comparison interface to assess whether rank aggregation produces consistent results across different annotators and whether cognitive biases affect relative confidence judgment quality.

3. Evaluate computational efficiency and scalability of different rank aggregation methods (Elo, Bradley-Terry, TrueSkill) on larger datasets with thousands of questions to determine practical trade-offs between accuracy and computational cost in real-world deployment scenarios.