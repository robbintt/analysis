---
ver: rpa2
title: What Does 'Human-Centred AI' Mean?
arxiv_id: '2507.19960'
source_url: https://arxiv.org/abs/2507.19960
tags:
- human
- guest
- cognitive
- what
- labour
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper critiques the ambiguity surrounding "human-centred AI"
  and argues that current conceptions fail to properly acknowledge the centrality
  of human cognition in AI systems. It proposes a radical redefinition of AI as any
  sociotechnical relationship where artifacts appear to perform human cognitive labour,
  analyzed through three types: enhancement, replacement, and displacement.'
---

# What Does 'Human-Centred AI' Mean?

## Quick Facts
- arXiv ID: 2507.19960
- Source URL: https://arxiv.org/abs/2507.19960
- Authors: Olivia Guest
- Reference count: 16
- Primary result: Critiques ambiguity in "human-centred AI," arguing it fails to acknowledge human cognition's centrality and proposes a radical redefinition of AI as sociotechnical relationships involving cognitive labor.

## Executive Summary
This paper critically examines the concept of "human-centred AI" and finds it deeply ambiguous. Through a radical reframing, it argues that AI should be understood not as a property of technology but as a sociotechnical relationship where artifacts appear to perform human cognitive labor. Using historical and contemporary case studies, the analysis demonstrates that AI systems can enhance, replace, or displace human cognition, with displacement being particularly harmful as it deskills humans and obscures the human labor required to operate these systems. The paper concludes that truly human-centred AI requires recognizing these sociotechnical relationships and addressing the harms of displacement AI.

## Method Summary
The paper employs a qualitative analytical framework that classifies AI systems as sociotechnical relationships rather than technological artifacts. The method involves two steps: first, determining whether a relationship exists between technology and human cognition; second, characterizing that relationship using criteria including valence (beneficial/neutral/harmful), effect on cognition (reskilling/deskilling), and labour obfuscation levels. Case studies of historical tools (abacus, calculators, cameras) and modern AI systems (LLMs, generative AI) are analyzed using this framework to demonstrate the three relationship types: enhancement, replacement, and displacement.

## Key Results
- Contemporary AI systems, particularly those involving large language models and generative AI, are fundamentally displacement-type relationships that deskill humans
- Modern AI maximizes labor obfuscation, hiding the human data and labor required to operate systems
- The paper proposes a deflationary reclassification of AI as any sociotechnical relationship where cognitive labor is offloaded to artifacts
- Enhancement relationships develop new cognitive skills while displacement relationships degrade existing skills

## Why This Works (Mechanism)

### Mechanism 1: Deflationary Reclassification
The paper's redefinition of AI as a generic "sociotechnical relationship" rather than a specific technological artifact neutralizes hype and allows for historical continuity. By treating AI as any relationship where cognitive labor is offloaded to an artifact (e.g., an abacus or an LLM), the framework removes the "mysticism" of modern AI and forces analysis toward the external relationship dynamics between human and tool.

### Mechanism 2: The Displacement-Enhancement Diagnostic
The framework evaluates AI based on its effect on human cognition (reskilling vs. deskilling) to predict the system's ethical valence (beneficial vs. harmful). Enhancement relationships develop new cognitive skills, while displacement relationships degrade existing skills ("deskilling"). This diagnostic moves evaluation from "Does it work?" to "Does it harm the operator?"

### Mechanism 3: Obfuscation Auditing (The Ghost in the Machine)
The paper argues that modern AI maximizes "labour obfuscation"—hiding the human data, maintenance, and RLHF required to run the system. By auditing this hidden labor, one demystifies the "autonomous" system, revealing it as a distributed sociotechnical process.

## Foundational Learning

- **Concept: Socio-technical Systems**
  - Why needed here: The paper rejects the "tool" view of AI in favor of a "relationship" view. You cannot understand the argument without accepting that AI is a process, not just an object.
  - Quick check question: Can you explain why a knocker-upper (human alarm clock) and a digital alarm clock represent different sociotechnical relationships despite achieving the same output?

- **Concept: Cognitive Labour vs. Physical Labour**
  - Why needed here: The definition of AI hinges on the offloading of *cognitive* work. Distinguishing this from physical automation is crucial for using the paper's classification tables.
  - Quick check question: Does an industrial robot welding a car door constitute "AI" under this paper's definition? (Hint: Does it displace *cognitive* or *physical* labour?)

- **Concept: Fetishism of Technology (Commodity Fetishism)**
  - Why needed here: The paper utilizes Marx's concept of fetishism to explain how social relations (human labour) are obscured by the technical object (the AI model).
  - Quick check question: How does "sweatshop" data labelling disappear when a user interacts with a "magical" chatbot?

## Architecture Onboarding

- **Component map:** The Artifact -> The Cognitive Labour -> The Relationship -> The Metric (Labour Obfuscation vs. Skill Effect)
- **Critical path:** Identify the human cognitive capacity the system targets → Map the "Human-in-the-loop" → Classify the relationship using Table 1 based on "deskilling" metric
- **Design tradeoffs:** Systems designed for maximal user convenience (displacement) often maximize deskilling; High labour obfuscation often creates smoother user experience but creates ethical debt and fragility
- **Failure signatures:** Pygmalion Displacement (system claims to be "human-level" but actually degrades human capacity); Opacification (users cannot trace why output was generated)
- **First 3 experiments:**
  1. Apply the paper's Table 1 definition to a non-computational tool in your organization (e.g., a checklist or filing system). Is it "AI"?
  2. Pick a generative AI tool and trace the "human-in-the-loop" required for it to function (data curators, RLHF labelers, prompters). Rate the obfuscation level.
  3. Measure a specific skill (e.g., writing, coding) in a group using an "Enhancement" tool vs. a "Displacement" tool over 2 weeks to validate the deskilling hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
Under what specific conditions does a sociotechnical relationship transition from "replacement" (neutral) to "displacement" (harmful), particularly regarding the user's pre-existing cognitive skills? The paper demonstrates this depends heavily on the user's baseline competence but lacks a formal model for predicting this transition boundary.

### Open Question 2
Can the theoretical framework of "displacement" be operationalized to quantitatively measure the extent of cognitive deskilling caused by generative AI? While the paper classifies contemporary AI as "displacement," it relies on qualitative historical analogy rather than quantitative metrics to assess severity or rate of deskilling.

### Open Question 3
How can we develop auditing methods to identify and make transparent the "obfuscated human-in-the-loop" within closed-source AI systems? The paper critiques "maximal obfuscation" but does not propose technical or sociological methods to pierce this opacity in industry-standard black-box models.

## Limitations
- Subjective nature of relationship classification may vary between evaluators
- Deskilling/reskilling metric lacks quantitative thresholds for consistent application
- Framework primarily focuses on individual cognitive skills rather than collective or organizational benefits

## Confidence

**High Confidence:** The theoretical framework for treating AI as a sociotechnical relationship is well-grounded in existing scholarship. Historical case studies provide robust support.

**Medium Confidence:** The displacement/deskilling diagnosis for contemporary AI systems is persuasive but relies on contested assumptions about skill value.

**Low Confidence:** The assertion that all current "autonomous" AI systems maximize labour obfuscation requires further empirical validation.

## Next Checks

1. Design a controlled experiment comparing skill development in users of enhancement versus displacement AI tools over 3-6 months, measuring both immediate performance and long-term capability retention.

2. Conduct a systematic audit of 10 popular AI systems to map and quantify the human labour required for operation, including data curation, maintenance, and RLHF processes, to test the "maximal obfuscation" hypothesis.

3. Apply the framework to AI systems used in different cultural contexts (e.g., Japanese manufacturing AI vs. Western creative AI) to test whether the enhancement/displacement classification holds across different value systems and work practices.