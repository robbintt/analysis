---
ver: rpa2
title: An Empirical Study on Prompt Compression for Large Language Models
arxiv_id: '2505.00019'
source_url: https://arxiv.org/abs/2505.00019
tags:
- compression
- prompt
- methods
- context
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper conducts a comprehensive empirical study on six prompt
  compression methods for large language models (LLMs) across 13 datasets spanning
  summarization, reconstruction, and QA tasks. The methods include KiS, SCRL, Selective
  Context, LLMLingua, LongLLMLingua, and LLMLingua-2, evaluated on three (M)LLMs:
  GPT-3.5-turbo, GPT-4o-mini, and Claude-3-Haiku.'
---

# An Empirical Study on Prompt Compression for Large Language Models

## Quick Facts
- arXiv ID: 2505.00019
- Source URL: https://arxiv.org/abs/2505.00019
- Reference count: 40
- Six prompt compression methods evaluated across 13 datasets on three LLMs

## Executive Summary
This paper presents a comprehensive empirical study comparing six prompt compression methods for large language models across summarization, reconstruction, and QA tasks. The methods evaluated include KiS, SCRL, Selective Context, LLMLingua, LongLLMLingua, and LLMLingua-2, tested on three LLMs: GPT-3.5-turbo, GPT-4o-mini, and Claude-3-Haiku. The study reveals that compression methods generally improve efficiency but introduce trade-offs in performance and hallucination rates, with LLMLingua-2 and LongLLMLingua showing the best overall effectiveness. The authors also release PCToolkit, a unified toolkit for prompt compression research.

## Method Summary
The study evaluates six prompt compression methods across 13 datasets spanning summarization, reconstruction, and QA tasks. Methods include KiS (regeneration-based), SCRL (regression-based classification), Selective Context (information-theoretic pruning), LLMLingua, LongLLMLingua (question-aware), and LLMLingua-2. All methods were tested at compression ratio ρ=0.5 and evaluated using BLEU, ROUGE-L, BERTScore, F1, and hallucination metrics (MiHR/MaHR) on three LLMs: GPT-3.5-turbo, GPT-4o-mini, and Claude-3-Haiku. The analysis also explores response length changes and word omission patterns across different compression ratios.

## Key Results
- LLMLingua and LLMLingua-2 generally outperform other methods, especially at high compression ratios
- LongLLMLingua excels in long-context QA tasks due to its question-aware compression
- All compression methods increase hallucination rates, primarily due to information loss
- GPT-3.5-turbo and GPT-4o-mini produce longer responses while Claude-3-Haiku generates shorter ones after compression

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LongLLMLingua's question-aware compression selectively retains query-relevant tokens, reducing information dilution in long contexts.
- **Mechanism:** LongLLMLingua uses a question-aware coarse-to-fine compression technique that evaluates token relevance relative to the user's query before pruning. This prioritizes semantic content linked to the question while discarding peripheral information.
- **Core assumption:** Long contexts contain proportionally more irrelevant details that dilute task-relevant signals; query-conditioned filtering mitigates this.
- **Evidence anchors:**
  - [abstract] "LongLLMLingua excels in long context QA tasks"
  - [section 5.1] "LongLLMLingua is question-aware, meaning it compresses prompts by considering the user's question... helps to ensure that the most critical information related to the question is retained"
  - [corpus] Related work (SCOPE, ICPC) confirms query-conditioned approaches improve compression-quality trade-offs, but causal mechanisms remain empirically observed, not theoretically proven.
- **Break condition:** Question-awareness provides no advantage when (a) all context is equally relevant to the query, or (b) the query is ambiguous or under-specified.

### Mechanism 2
- **Claim:** Moderate compression of long contexts can improve LLM performance by reducing noise and focusing attention on salient content.
- **Mechanism:** Long documents contain redundant or low-information segments that compete for attention capacity. Pruning these segments reduces cognitive load on the model's attention mechanism, improving signal-to-noise ratio for critical information.
- **Core assumption:** LLM attention has finite effective capacity; excessive context length introduces competing signals that degrade retrieval and reasoning.
- **Evidence anchors:**
  - [abstract] "In the Longbench evaluation, moderate compression even enhances LLM performance"
  - [section 5.1, Figure 5] "For longer contexts, a different trend emerges: performance initially improves with increasing compression ratio up to a point, after which it begins to deteriorate"
  - [corpus] PIS and Dynamic Compressing Prompts papers observe similar inverted-U patterns, but do not establish causal proof of attention capacity limits.
- **Break condition:** Performance gains disappear when (a) compression removes task-critical details, or (b) the original context is already concise and well-structured.

### Mechanism 3
- **Claim:** Prompt compression induces hallucinations primarily through information loss, forcing LLMs to infer missing content.
- **Mechanism:** Compression removes tokens that, while seemingly low-information, provide grammatical structure, disambiguation, or factual anchors. When presented with incomplete sentences, LLMs generate bridging content to maintain coherence, which may not align with original context.
- **Core assumption:** LLMs have a strong drive toward grammatical and semantic completeness; gaps trigger generative filling.
- **Evidence anchors:**
  - [section 5.2] "All methods result in some degree of enhanced hallucination... Information loss is a primary trigger for hallucinations in prompt compression"
  - [section 5.2, Figure 6] Two hallucination types identified: Altered Semantic Hallucination (ASH) from incorrect compression, and Information Loss Hallucination (ILH) from missing content
  - [corpus] Related work corpus does not directly address hallucination mechanisms in compression; this remains an empirical observation requiring further investigation.
- **Break condition:** Hallucination rates remain low when (a) compression preserves complete sentence structures, or (b) the task requires minimal factual grounding.

## Foundational Learning

- **Concept: Self-information and entropy-based token scoring**
  - Why needed here: Methods like Selective Context compute self-information using a base language model to identify "less informative" tokens for pruning. Understanding information theory basics helps interpret why certain words are targeted.
  - Quick check question: Given the sentence "The cat sat on the mat," which token would have lowest self-information in a typical English corpus?

- **Concept: Transformer attention and context window constraints**
  - Why needed here: Compression aims to reduce tokens competing for attention. Knowing how attention distributes across sequences explains why long-context dilution occurs and why pruning might help.
  - Quick check question: In a 4096-token context with uniform attention, how much attention weight does each token receive? How does this change if 50% of tokens are removed?

- **Concept: Extractive vs. abstractive compression**
  - Why needed here: The six methods differ fundamentally—KiS regenerates text (abstractive), while SCRL and LLMLingua-2 use token classification (extractive). This affects grammatical integrity and hallucination risk.
  - Quick check question: If you compress "She went to the store yesterday" by removing "to the," does an extractive method produce grammatical output? What about an abstractive method?

## Architecture Onboarding

- **Component map:**
  ```
  PCToolkit/
  ├── compressors/     # 6 methods: KiS, SCRL, SelectiveContext, LLMLingua, LongLLMLingua, LLMLingua-2
  ├── datasets/        # 13 datasets across summarization, reconstruction, QA, VQA
  ├── metrics/         # BLEU, ROUGE-L, BERTScore, F1, MiHR, MaHR
  └── runners/         # Unified evaluation interface
  ```

- **Critical path:**
  1. Load dataset (e.g., LongBench for long-context QA)
  2. Initialize compressor with target ratio (start at 0.5)
  3. Run compression → LLM inference → metric evaluation
  4. Compare against original prompt baseline

- **Design tradeoffs:**
  | Method | Speed | Quality (High Compression) | Hallucination Risk | Best For |
  |--------|-------|----------------------------|-------------------|----------|
  | SCRL | Fastest | Moderate | Medium | Resource-constrained |
  | LLMLingua-2 | Fast | High | Lower | Reconstruction/Summarization |
  | LongLLMLingua | Moderate | High (long ctx) | Lower (long ctx) | Long-context QA |
  | KiS | Slowest | Variable | Higher | When fluency matters |

- **Failure signatures:**
  - Hallucination spike: Check for sentence fragments in compressed output
  - Performance cliff at high ratios: Compression >0.7 removes critical tokens
  - Model-specific length anomalies: GPT models lengthen responses; Claude shortens—this is expected behavior, not a bug

- **First 3 experiments:**
  1. **Baseline calibration:** Run all 6 compressors on GSM8K at ratio=0.5; measure BLEU and inference time. Identify top-2 performers for your compute budget.
  2. **Long-context sweep:** On LongBench SingleDoc, test compression ratios [0.1, 0.3, 0.5, 0.7] with LongLLMLingua. Plot F1 vs. ratio to find the performance peak.
  3. **Hallucination audit:** Sample 50 compressed prompts from ShareGPT; manually annotate ASH vs. ILH. Compare LLMLingua-2 vs. LongLLMLingua to validate paper's hallucination rankings on your data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can prompt compression methods be specifically optimized for multimodal tasks to overcome the inconsistency observed in text-only trained models?
- Basis in paper: [explicit] Section 5.3 states: "Since all prompt compression methods are designed and trained based on text-only tasks, their applicability to multimodal tasks remains to be explored."
- Why unresolved: The study found that methods like SCRL and LLMLingua-2 performed inconsistently across VQA datasets (IconQA, OK-VQA), indicating that text-based compression strategies do not generalize effectively to visual reasoning contexts.
- What evidence would resolve it: The development of a multimodal-specific compression algorithm that maintains or improves performance on VQA benchmarks compared to the "Original Prompt" baseline, or the successful fine-tuning of existing compressors on image-text pairs.

### Open Question 2
- Question: Do low-information tokens (e.g., stop words) function as computational "registers" in LLMs to facilitate intermediate processing?
- Basis in paper: [explicit] In Section 5.4, the authors speculate: "We speculate that a similar mechanism may exist in LLMs, where tokens for less informative words could serve as registers that facilitate intermediate computations."
- Why unresolved: The empirical results showed that removing seemingly uninformative words (like "a") negatively impacted performance, particularly in long contexts. The authors drew an analogy to Vision Transformers (ViTs) but provided no internal model analysis to confirm this functional role in LLMs.
- What evidence would resolve it: Mechanistic interpretability studies (e.g., probing classifier analysis) demonstrating that specific attention heads utilize stop-word tokens to store or pass along state information during inference.

### Open Question 3
- Question: What are the underlying architectural or training mechanisms that cause model-specific changes in response length (lengthening for GPT vs. shortening for Claude) after compression?
- Basis in paper: [explicit] Appendix B notes: "Our future work may delve into the underlying mechanisms driving these differences and provide further insights."
- Why unresolved: While the study observed a consistent trend where GPT models produced longer responses and Claude produced shorter ones to compressed prompts, the explanation remains speculative (e.g., "attempt to mitigate loss of information") rather than proven.
- What evidence would resolve it: A comparative analysis of the attention distributions and token probability distributions in different model families when processing incomplete or compressed syntactic structures.

### Open Question 4
- Question: How does the performance of "text-in, text-out" compression compare to internal state modification methods (KV cache compression) for open-source models?
- Basis in paper: [explicit] The authors state in the Limitations section: "In terms of the compression methods for open-source models, there are approaches on modifying internal states or KV cache... We leave the further study to our future work."
- Why unresolved: This study restricted its scope to methods that operate solely on the text layer to ensure applicability to closed-source APIs. Consequently, it is unknown if the hallucination rates or information loss observed in text-compression methods are higher or lower than those in internal-state compression methods.
- What evidence would resolve it: A comparative benchmark evaluating methods like Scissorhands or H2O (KV cache) against LLMLingua-2 (Text) on the same datasets (e.g., LongBench) within an open-source model environment.

## Limitations

- The study evaluates only three (M)LLMs, limiting generalizability across different model architectures
- The optimal compression ratio analysis is incomplete, not establishing task-specific sweet spots
- Hallucination analysis lacks mechanistic depth, identifying information loss as a trigger without detailed error categorization

## Confidence

**High Confidence** (Extensive empirical support, minimal assumptions):
- LongLLMLingua's superior performance in long-context QA tasks
- Compression-induced response length changes being model-dependent
- All methods increasing hallucination rates relative to uncompressed prompts

**Medium Confidence** (Strong empirical support but with acknowledged limitations):
- LLMLingua-2 showing lower hallucination rates in reconstruction/summarization
- Performance improvement at moderate compression ratios for long contexts
- SCRL's inconsistent performance across multimodal tasks

**Low Confidence** (Limited evidence, requires further validation):
- Claim that removing "less informative" words impacts performance in long contexts
- Generalizability of compression effectiveness rankings across unseen datasets
- Mechanisms underlying model-specific response length changes

## Next Checks

1. **Cross-model validation**: Evaluate the top-3 compression methods (LLMLingua-2, LongLLMLingua, SCRL) on additional (M)LLMs including open-source models like Llama-3-8B and Mistral-7B. Compare performance rankings to assess whether the current hierarchy holds across different model architectures and parameter scales.

2. **Hallucination type decomposition**: Conduct detailed linguistic analysis of hallucinated outputs from the most effective methods. Categorize hallucinations by type (factual, grammatical, semantic) and content (entities, relations, events) to identify systematic patterns in information loss vulnerability. Test whether preserving specific linguistic structures reduces hallucination rates.

3. **Ratio optimization study**: For each method-task combination, identify the optimal compression ratio range by testing increments of 0.1 across [0.1, 0.9]. Plot performance curves to determine whether the inverted-U pattern generalizes beyond the LongBench dataset and whether different tasks require different compression sweet spots.