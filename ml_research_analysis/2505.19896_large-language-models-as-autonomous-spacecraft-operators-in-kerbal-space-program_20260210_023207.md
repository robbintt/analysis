---
ver: rpa2
title: Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program
arxiv_id: '2505.19896'
source_url: https://arxiv.org/abs/2505.19896
tags:
- llama
- fine-tuning
- files
- arxiv
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel approach to spacecraft control using
  Large Language Models (LLMs) as autonomous agents. By leveraging prompt engineering,
  few-shot prompting, and fine-tuning techniques, we developed LLM-based agents capable
  of real-time spacecraft maneuvering in the Kerbal Space Program Differential Games
  (KSPDG) challenge.
---

# Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program

## Quick Facts
- arXiv ID: 2505.19896
- Source URL: https://arxiv.org/abs/2505.19896
- Authors: Alejandro Carrasco; Victor Rodriguez-Fernandez; Richard Linares
- Reference count: 15
- Primary result: LLM-based agents achieved performance close to top-ranked differential equations approach in spacecraft maneuvering challenge

## Executive Summary
This work introduces a novel approach to spacecraft control using Large Language Models (LLMs) as autonomous agents. By leveraging prompt engineering, few-shot prompting, and fine-tuning techniques, the authors developed LLM-based agents capable of real-time spacecraft maneuvering in the Kerbal Space Program Differential Games (KSPDG) challenge. The approach addresses limitations of traditional control methods and reinforcement learning by using LLMs to interpret mission telemetry and generate appropriate control actions. The results demonstrate that fine-tuned LLaMA models significantly outperformed baseline models and achieved performance close to the top-ranked differential equations approach in the competition.

## Method Summary
The methodology employs a multi-stage approach combining prompt engineering, few-shot learning, and fine-tuning of LLaMA models for autonomous spacecraft operations. The process begins with task-level fine-tuning on a small dataset (1,200 examples) to establish baseline capabilities, followed by trajectory-level fine-tuning on a larger dataset (3,000 examples) to improve precision and adaptability. The models are trained to interpret telemetry data and generate control actions in natural language format, which are then converted to executable commands. The approach leverages the reasoning capabilities of LLMs to handle complex decision-making tasks traditionally solved by differential equations or reinforcement learning methods.

## Key Results
- Fine-tuned LLaMA models significantly outperformed baseline models in the KSPDG challenge
- Achieved performance metrics comparable to top-ranked differential equations approaches
- Demonstrated successful real-time spacecraft maneuvering through LLM-based autonomous control

## Why This Works (Mechanism)
The approach works by leveraging the natural language processing and reasoning capabilities of LLMs to interpret complex telemetry data and make autonomous control decisions. By framing spacecraft control as a language task, the models can leverage their ability to understand context, patterns, and relationships in the input data to generate appropriate control actions. The fine-tuning process allows the models to learn specific control patterns and trajectories relevant to spacecraft maneuvering, while maintaining the general reasoning capabilities of the underlying LLM architecture.

## Foundational Learning
- **Prompt Engineering**: Why needed - To effectively communicate control objectives to the LLM; Quick check - Test different prompt structures for clarity and consistency
- **Few-shot Learning**: Why needed - To efficiently adapt pre-trained models to spacecraft control tasks; Quick check - Verify performance with varying numbers of examples
- **Fine-tuning Process**: Why needed - To specialize general LLMs for specific spacecraft control scenarios; Quick check - Monitor validation loss during training
- **Telemetry Interpretation**: Why needed - To enable LLMs to understand spacecraft status and environment; Quick check - Validate interpretation accuracy against ground truth
- **Control Action Generation**: Why needed - To convert reasoning into executable spacecraft commands; Quick check - Test generated actions in simulation
- **Trajectory Optimization**: Why needed - To ensure efficient and safe spacecraft maneuvering; Quick check - Compare trajectories against optimal solutions

## Architecture Onboarding

**Component Map**: Telemetry Data -> LLM Processing -> Action Generation -> Control Execution -> Feedback Loop

**Critical Path**: The critical path involves receiving telemetry data, processing it through the fine-tuned LLM, generating control actions, executing them in the simulation environment, and receiving updated telemetry for the next iteration.

**Design Tradeoffs**: The approach trades computational efficiency for flexibility and adaptability. While traditional control methods may be faster, LLMs offer superior generalization capabilities and can handle unexpected scenarios better. The fine-tuning process requires careful balancing between task-specific specialization and maintaining general reasoning capabilities.

**Failure Signatures**: Potential failures include misinterpretation of telemetry data, generation of invalid or unsafe control actions, inability to handle novel scenarios outside training distribution, and computational latency in real-time applications.

**First 3 Experiments**:
1. Test baseline LLM performance on simple spacecraft maneuvers without fine-tuning
2. Evaluate task-level fine-tuned model on a subset of the competition challenges
3. Compare trajectory-level fine-tuned model against traditional control methods on identical scenarios

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Evaluation based entirely on game-based simulation (Kerbal Space Program), which may not capture real spacecraft complexities
- Fine-tuning relies on relatively small datasets (1,200-3,000 examples), potentially limiting generalization
- Performance metrics based on single competition benchmark without real-world validation
- Natural language processing approach may be vulnerable to misinterpretation in mission-critical contexts

## Confidence

**High Confidence**: LLMs can be adapted for autonomous spacecraft control tasks using prompt engineering and fine-tuning techniques.

**Medium Confidence**: Fine-tuned LLaMA models significantly outperform baseline models and achieve performance close to top-ranked differential equations approaches.

**Low Confidence**: LLMs are a viable alternative to traditional control methods and reinforcement learning for real-world spacecraft operations.

## Next Checks
1. Validate LLM-based control approach in a high-fidelity spacecraft simulator or hardware-in-the-loop setup
2. Evaluate models' ability to handle novel and unseen scenarios beyond the competition benchmark
3. Conduct thorough safety and robustness analysis, including response to ambiguous or incomplete telemetry data