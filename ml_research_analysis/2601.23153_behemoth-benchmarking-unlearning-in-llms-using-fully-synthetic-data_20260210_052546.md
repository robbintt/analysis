---
ver: rpa2
title: 'Behemoth: Benchmarking Unlearning in LLMs Using Fully Synthetic Data'
arxiv_id: '2601.23153'
source_url: https://arxiv.org/abs/2601.23153
tags:
- accuracy
- blocks
- rank
- editing
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Behemoth, a synthetic data generation framework
  designed to benchmark model unlearning in large language models (LLMs). The authors
  address the challenge of studying knowledge editing in LLMs by creating fully synthetic
  data with controlled vocabularies and grammar, allowing precise measurement of model
  editing effects without the complexities of real-world data.
---

# Behemoth: Benchmarking Unlearning in LLMs Using Fully Synthetic Data

## Quick Facts
- arXiv ID: 2601.23153
- Source URL: https://arxiv.org/abs/2601.23153
- Reference count: 40
- Authors: Eugenia Iofinova; Dan Alistarh

## Executive Summary
This paper introduces Behemoth, a synthetic data generation framework designed to benchmark model unlearning in large language models (LLMs). The authors address the challenge of studying knowledge editing in LLMs by creating fully synthetic data with controlled vocabularies and grammar, allowing precise measurement of model editing effects without the complexities of real-world data. Using a custom tokenizer and modular framework, they generate tabular data in the form of {subject, relationship, object} tuples, which are then transformed into sentences using artificial grammars. The framework is demonstrated by training a 31-million-parameter Pythia transformer model on synthetic data and evaluating various model editing strategies, including full-rank and low-rank fine-tuning, as well as the ROME method. Experiments reveal that model editing can be more effective when only a subset of model layers are updated, and the optimal choice of layers depends on the editing strategy and the scale of the change. Notably, fine-tuning the entire model is not always necessary or desirable, and interpretability techniques like activation patching provide limited guidance for layer selection. The results highlight the importance of understanding the interaction between training data distribution and model editing, offering insights into improving the reliability and effectiveness of knowledge editing in LLMs.

## Method Summary
The framework generates {subject, relationship, object} tuples with configurable distributions (independent, correlated, nested) using a custom tokenizer with fully partitioned token spaces to prevent collisions. Sentences are constructed via artificial grammar and used to train a Pythia-31M model (6 blocks, 256 hidden dim) for 460 epochs. Model editing experiments use fine-tuning with 2:1:4 ratios (edited tuples : unchanged related : random), testing ROME, full-rank FT, and LoRA (ranks 32-128) across different layer selections.

## Key Results
- Single fact edits can be accomplished with rank-32 LoRA while maintaining high remaining data accuracy
- Full fine-tuning is not always necessary or desirable for effective model editing
- Interpretability techniques like activation patching provide limited guidance for layer selection during editing
- Correlated facts can be selectively decoupled during editing, revealing incomplete information coupling

## Why This Works (Mechanism)

### Mechanism 1: Controlled Synthetic Data Enables Precise Edit Measurement
- Claim: Fully synthetic data with partitioned token spaces allows exact verification of editing success and collateral damage.
- Mechanism: The framework generates {subject, relationship, object} tuples with custom tokenization where each token belongs uniquely to subject, object, or relationship spaces. This prevents token collisions and enables deterministic verification of whether a specific fact was edited without relying on proxy measurements.
- Core assumption: Synthetic tabular data approximates the factual knowledge storage patterns relevant to model editing, even though it lacks natural language complexity.
- Evidence anchors:
  - [abstract]: "We therefore propose Behemoth, a fully synthetic data generation framework... allowing complete control over data distribution and tokenization."
  - [section 2]: "Unlike other works... we use a fully synthetic vocabulary and a custom tokenization, with all tokens fully partitioned... This allows us to fully control not only the fact, but also the token distribution."
  - [corpus]: Weak direct evidence; related work focuses on editing methods rather than synthetic data control benefits.
- Break condition: If token collisions occur in natural language significantly alter editing dynamics beyond what synthetic data can capture.

### Mechanism 2: Update Rank Determines Edit Scope vs. Model Preservation Trade-off
- Claim: Lower-rank updates are sufficient and often better for simple edits; complex edits require higher rank but cause more collateral damage.
- Mechanism: Single or identical edits naturally occupy low-dimensional subspaces (effective rank ~40 for 31M param model), allowing rank-32 LoRA to succeed. Diverse edits and relationship forgetting require full-rank updates because they involve more dispersed weight changes across multiple MLP layers.
- Core assumption: The effective rank of weight changes during full fine-tuning indicates the minimal LoRA rank needed.
- Evidence anchors:
  - [section 4.8]: "when only a single model edit or ten identical edits are made, it is already possible to make the edit and retain high remaining data accuracy with a rank-32 update"
  - [section 6.2]: "when only a single edit is made, the effective rank of the layer weight changes is quite low - under 40... when a larger change is made... the change becomes much higher rank"
  - [corpus]: Related work (Biderman et al. 2024) confirms "LoRA underperforms FFT on more complex tasks... but also does a better job maintaining other skills."
- Break condition: If scaling to larger models changes the rank-edit relationship; current evidence is from 31M parameter models only.

### Mechanism 3: Correlated Facts Can Be Selectively Decoupled During Editing
- Claim: Even perfectly correlated relationships stored in the model can be independently edited, revealing incomplete information coupling.
- Mechanism: The model learns correlations between relationships (e.g., r1 and r2 always have matching values) but stores them with some independence. Low-rank fine-tuning or specific layer choices can edit r1 without updating r2, though this depends on which layers are modified.
- Core assumption: Correlation learning doesn't create fully shared representations at the neuron/circuit level.
- Evidence anchors:
  - [section 4.9]: "even in the case of two perfectly correlated relationships, remapping the label of r1 does not necessarily remap the label of r2... with ROME, remapping a single tuple was effective 100% of the time, but the fully dependent tuple was only updated 90% of the time."
  - [section 4.9]: "the 'forgetting' of the fully dependents... decreases with finetuning rank, and is almost completely ineffective when done with full finetuning"
  - [corpus]: Related work (Wu et al. 2024, Joshi et al. 2024) shows fine-tuning for forgetting is brittle and can reveal other relationships.
- Break condition: If different architectures store correlations differently; tested only on Pythia-31M.

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: The paper extensively uses LoRA as the primary editing method, varying ranks from 32-128. Understanding how LoRA constrains weight updates to low-rank matrices is essential for interpreting the rank-dependent results.
  - Quick check question: Can you explain why a rank-32 LoRA update can only express weight changes in a 32-dimensional subspace?

- Concept: Transformer Block Structure (MLP + Attention layers)
  - Why needed here: The paper investigates editing different layer types (MLP only, Attention only, both) and finds layer choice significantly impacts editing success and collateral damage.
  - Quick check question: In a standard transformer, what is the functional difference between the MLP and Attention layers in terms of information processing?

- Concept: Model Editing vs. Unlearning
  - Why needed here: The paper uses "forgetting" (remapping all objects of a relationship to one value) as a proxy for unlearning, and discusses how fine-tuning-based editing differs from methods like ROME.
  - Quick check question: What is the difference between editing a fact (changing one object to another) and unlearning (removing knowledge)?

## Architecture Onboarding

- Component map:
  - Data Generator -> Tokenizer -> Sentence Constructor -> Training Pipeline -> Editing Suite
- Critical path:
  1. Define data distribution parameters (# subjects, # relationships, correlation structure)
  2. Run binary search to find data size achieving ~95% accuracy after 460 epochs
  3. Train model from scratch using shuffled sentence sequences
  4. Configure editing experiment (edit type, rank, target layers)
  5. Execute fine-tuning with edit data + clean data mixture (specified ratios in Section 4.7)
  6. Evaluate on full tuple set for exact success/collateral measurement
- Design tradeoffs:
  - Model size (31M) chosen for experimental throughput; may not generalize to larger models
  - Simple grammar sacrifices natural language realism for measurement precision
  - Custom tokenization prevents real-world token collision dynamics
- Failure signatures:
  - Low remaining accuracy after editing: Likely used full-rank update for diverse edits without sufficient clean data mixing
  - Correlated fact not updated: Check if editing only attention layers (Figure 9 shows this can preserve dependent relationships)
  - Low editing success with high-rank LoRA: May need more training steps; rank-32 requires more blocks, rank-64+ can succeed with fewer blocks
- First 3 experiments:
  1. Single fact edit with LoRA rank 32 on MLP layer of last block only - should achieve ~100% edit success with ~95% remaining accuracy
  2. Relationship forgetting with full fine-tuning on all layers - expect near-complete forgetting but remaining accuracy drop to 70-80%
  3. Correlated relationship edit - try editing r1 with attention-only on first block to observe whether r2 is preserved or co-edited

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can models successfully "grok" implied nested relationships if trained on significantly larger datasets or modified architectures?
- Basis in paper: [explicit] The authors observe that models fail to efficiently store implied nested relationships (e.g., Mary $\to$ Acme $\to$ Springfield) even after extended training. They state, "We leave it to future work to establish if providing many additional examples... would enable the dependent relationship to be 'grokked'."
- Why unresolved: The current experiments showed that models did not efficiently store the low bit-cost of nested dependencies, and extending training time (5x) did not resolve this capacity issue.
- What evidence would resolve it: Experiments scaling the number of subjects/instances or altering model depth to observe if the implied relationships are learned without explicit supervision.

### Open Question 2
- Question: How do token collisions (polysemy) impact the effectiveness and locality of model editing methods?
- Basis in paper: [explicit] The authors designed the framework to strictly partition tokens to avoid collisions but note, "We believe that token collisions are interesting to study in their own right, we propose that any such token overuse be created intentionally and leave this investigation to future work."
- Why unresolved: Real-world LLMs inevitably share tokens across concepts (e.g., "bank" as a financial institution vs. river edge), which may cause unintended side effects during editing that are not captured in the collision-free Behemoth dataset.
- What evidence would resolve it: Introducing intentional token overlaps in the synthetic vocabulary and measuring the resulting drop in editing precision or "collateral damage" to unrelated facts sharing the token.

### Open Question 3
- Question: How does the complexity of the synthetic grammar influence the rank requirements for effective unlearning?
- Basis in paper: [explicit] Section 7 (Conclusion) acknowledges the limitation of using a very simple grammar and states, "We leave the expansion of the experiments to more complex grammars and data relationships to future work."
- Why unresolved: The paper demonstrates that update rank requirements depend on edit diversity, but it is unknown if syntactic complexity increases the necessary rank or alters the layer-specific editing dynamics.
- What evidence would resolve it: Repeating the rank sensitivity experiments using more sophisticated, natural-language-like grammatical structures provided by the Behemoth framework.

## Limitations

- The artificial grammar sacrifices natural language realism for measurement precision
- 31-million-parameter model size may not scale to the rank-edit relationships observed in larger models
- The framework prevents real-world token collision dynamics through custom tokenization

## Confidence

**High Confidence:** The core claim that synthetic data enables precise measurement of editing effects is well-supported by the framework's design and experimental setup.

**Medium Confidence:** The assertion that layer choice significantly impacts editing outcomes and collateral damage is supported by experiments but may be architecture-dependent.

**Low Confidence:** The generalizability of these findings to larger models remains uncertain, as does the applicability to different editing methods beyond the ones tested.

## Next Checks

1. **Scale-Up Validation:** Replicate the editing experiments with a larger model (e.g., 1-3B parameters) using the same synthetic data distribution to test whether the rank-edit relationships and layer selection effects scale proportionally or change fundamentally.

2. **Real-World Translation:** Generate a small synthetic dataset that mimics the tokenization collisions and contextual embeddings of a real-world corpus, then compare editing dynamics between synthetic and natural data to identify which measurement differences are most critical.

3. **Architecture Generalization:** Implement the editing experiments on a different transformer architecture (e.g., Llama or Mistral) to verify whether the layer-specific editing effects and correlated fact decoupling patterns hold across architectures or are specific to the Pythia implementation.