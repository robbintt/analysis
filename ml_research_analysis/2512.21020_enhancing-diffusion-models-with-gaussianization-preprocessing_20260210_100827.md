---
ver: rpa2
title: Enhancing diffusion models with Gaussianization preprocessing
arxiv_id: '2512.21020'
source_url: https://arxiv.org/abs/2512.21020
tags:
- data
- distribution
- gaussianization
- diffusion
- gaussianized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Gaussianization preprocessing to enhance
  the efficiency of diffusion models. The key idea is to transform training data distributions
  to more closely resemble an independent Gaussian distribution, which aligns with
  the initial noise distribution of diffusion models.
---

# Enhancing diffusion models with Gaussianization preprocessing

## Quick Facts
- arXiv ID: 2512.21020
- Source URL: https://arxiv.org/abs/2512.21020
- Authors: Li Cunzhi; Louis Kang; Hideaki Shimazaki
- Reference count: 7
- Primary result: Gaussianization preprocessing reduces diffusion model sampling steps from 80+ to 20 while maintaining quality

## Executive Summary
This paper introduces Gaussianization preprocessing to improve diffusion model sampling efficiency. The core insight is that diffusion models start sampling from a standard Gaussian distribution, so aligning training data distributions to be more Gaussian-like reduces the mismatch that causes slow sampling. The method uses iterative Independent Component Analysis (ICA) combined with marginal Gaussianization via Kernel Density Estimation (KDE) and probability integral transforms. Experiments on synthetic Gaussian Mixture Model data demonstrate that this preprocessing achieves high log-likelihood values within 20 sampling steps compared to 80+ steps for baseline models, while also accelerating training convergence and avoiding bifurcation instability.

## Method Summary
The method applies iterative Gaussianization preprocessing to training data before training diffusion models. For each iteration, ICA identifies independent components by maximizing non-Gaussianity, then each component is transformed to Gaussian using KDE-based CDF estimation and inverse Gaussian CDF mapping. After K iterations, the Gaussianized data trains a standard DDPM. During inference, the reverse process generates samples in the Gaussianized space, which are then transformed back through inverse CDF and inverse ICA operations applied in reverse order. The DDPM architecture uses 3 linear layers with ReLU activations, and the approach is evaluated on 2D Gaussian Mixture Model data with log-likelihood as the primary metric.

## Key Results
- Gaussianization preprocessing achieves high log-likelihood values within the first 20 sampling steps compared to baseline's 80+ steps
- The method accelerates training convergence and maintains sample quality across different network widths
- Gaussianized pipeline exhibits smoother, more stable reconstruction trajectories without bifurcation instability
- Benefits are most pronounced for narrower networks (width 16-32), with diminishing returns for wider models

## Why This Works (Mechanism)

### Mechanism 1: Distribution Alignment Reduces Bifurcation Delay
- Diffusion models begin inference from a standard Gaussian. When target distribution differs substantially, reverse trajectories traverse large distribution space before meaningful reconstruction. Gaussianization narrows this gap, enabling faster convergence. Core assumption: bifurcation delay is primarily driven by distributional mismatch rather than network capacity. Evidence: abstract states preprocessing reduces mismatch leading to faster sampling; section 1 explains reverse dynamics don't need long approach phase.

### Mechanism 2: ICA Identifies Directions Most Relevant to Gaussianization
- ICA rotates data into directions maximizing non-Gaussianity, enabling targeted transformation along statistically independent axes. This is more effective than original feature space where dependencies are entangled. Core assumption: independent components capture structure relevant to generative task. Evidence: section 2.1 explains ICA leverages non-Gaussianity as independence measure; section 2.3 notes ICA is inherently non-orthogonal transformation.

### Mechanism 3: Iterative Gaussianization Eliminates Residual Dependencies
- Single pass leaves residual non-Gaussianity because transforming one component perturbs others. Iteration allows successive refinement, with each round targeting remaining structure. Core assumption: transformation is sufficiently invertible with no information loss. Evidence: section 2.3 states method progressively eliminates correlations; Figure 3 shows progressive Gaussianization across iterations.

## Foundational Learning

- Concept: Diffusion model forward/reverse processes (DDPM formulation)
  - Why needed here: Understanding noise addition (Eq. 1-2) and denoising (Eq. 3-7) is essential to see why distributional mismatch causes inefficiency.
  - Quick check question: Given data point x₀, can you write expression for xₜ after t forward diffusion steps?

- Concept: Independent Component Analysis (ICA)
  - Why needed here: Method relies on ICA to identify statistically independent directions before Gaussianization.
  - Quick check question: Why does ICA seek directions of maximum non-Gaussianity rather than maximum variance (like PCA)?

- Concept: Probability integral transform and inverse CDF Gaussianization
  - Why needed here: Core transformation (Eq. 10-12) maps data through CDF to uniform, then to standard Gaussian via inverse Gaussian CDF.
  - Quick check question: If random variable has CDF F, what is distribution of U = F(X)?

## Architecture Onboarding

- Component map: Data → [Forward Gaussianization × K] → Gaussianized Data → [DDPM Training] → [DDPM Sampling] → Gaussianized Samples → [Inverse Gaussianization × K] → Final Samples

- Critical path: Preprocessing module (ICA → KDE → CDF → Inverse Gaussian CDF) → DDPM (3 layers + ReLU) → Postprocessing module (Inverse Gaussian CDF → Inverse CDF → Inverse ICA)

- Design tradeoffs:
  - K (iteration count): More iterations yield more Gaussian-like distributions but increase preprocessing cost and error accumulation
  - KDE bandwidth h: Smaller h captures finer structure but risks overfitting; larger h smooths but may lose information
  - Network width: Gaussianization benefits most pronounced for narrow networks (16-32), diminishing for high-capacity models

- Failure signatures:
  - Poor KDE bandwidth causing noisy CDFs and reconstruction artifacts
  - ICA failing to find meaningful components, adding cost without benefit
  - Out-of-order inverse transforms causing reconstruction failure

- First 3 experiments:
  1. Reproduce Figure 4 comparison on 2D GMM: train baseline vs Gaussianized DDPM (K=1-3, width=16), plot reconstruction snapshots at t=0,20,40,60,80,100
  2. Ablation on K: measure log-likelihood at t=20 for K=0 (baseline), K=1, K=2, K=3
  3. Bandwidth sensitivity test: fix K=2, vary h (Silverman's rule, 0.5×, 2×), compute reconstruction MSE after full forward/inverse Gaussianization

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on high-dimensional real-world data remains unknown beyond synthetic 2D GMM scenarios
- Critical hyperparameters (KDE bandwidth, iteration count K) were manually selected without systematic optimization
- Computational overhead of preprocessing is not quantified relative to sampling speedup gains

## Confidence

- High Confidence: Theoretical mechanism linking distribution alignment to reduced bifurcation delay is sound and well-supported by diffusion model theory
- Medium Confidence: Experimental results on synthetic GMM data demonstrating improved log-likelihood and faster convergence
- Low Confidence: Claims about applicability to complex real-world distributions and overall practical utility beyond controlled synthetic scenarios

## Next Checks

1. **High-dimensional extension test:** Apply Gaussianization preprocessing to CIFAR-10 images using U-Net DDPM architecture, comparing sampling efficiency (FID, steps to target quality) against baseline

2. **Hyperparameter sensitivity analysis:** Systematically vary KDE bandwidth and iteration count K on 2D GMM data to quantify impact on reconstruction quality and computational cost

3. **Reconstruction fidelity validation:** On held-out test data, compute reconstruction error after full forward-inverse Gaussianization (without diffusion) to isolate and quantify preprocessing-induced information loss