---
ver: rpa2
title: Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability
  in Settling Challenging Optimization Tasks
arxiv_id: '2511.03137'
source_url: https://arxiv.org/abs/2511.03137
tags:
- mllm
- optimization
- design
- information
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework leveraging multimodal large language
  models (MLLMs) to enhance the Fireworks Algorithm (FWA) for solving complex optimization
  problems. The framework introduces the concept of Critical Part (CP), which allows
  for the flexible design of both global and local FWAs by incorporating visual information
  from the optimization process.
---

# Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks

## Quick Facts
- arXiv ID: 2511.03137
- Source URL: https://arxiv.org/abs/2511.03137
- Reference count: 31
- One-line primary result: MLLM-driven framework with visual feedback evolves Fireworks Algorithm operators achieving state-of-the-art results on TSP and EDA tasks.

## Executive Summary
This paper presents a framework leveraging multimodal large language models (MLLMs) to enhance the Fireworks Algorithm (FWA) for solving complex optimization problems. The framework introduces the concept of Critical Part (CP), which allows for the flexible design of both global and local FWAs by incorporating visual information from the optimization process. This approach significantly broadens the applicability of FWAs to high-dimensional problems like the Traveling Salesman Problem (TSP) and Electronic Design Automation (EDA).

Experiments on TSP instances demonstrate that the proposed framework outperforms leading TSP heuristics and algorithms, achieving or surpassing state-of-the-art results on numerous benchmarks. Notably, some paths obtained with visual information are better than those given by TSPLIB in the floating-point sense. For EDA tasks, the framework delivers state-of-the-art performance on six out of eight benchmarks using extremely low computational resources.

## Method Summary
The framework uses Doubao-1.5-pro-vision MLLM to evolve Fireworks Algorithm operators via mutation and crossover operations. It employs a population-based approach with size 5, greedy selection, and 200 iterations. For TSP, the framework evolves full FWA operators (explosion, mutation, selection) using visual feedback including route plots, crossing heatmaps, and density maps. For high-dimensional EDA problems, it evolves local step-size controllers within the DreamPlace framework rather than global optimizers. The method incorporates visual information when beneficial, particularly for geometrically distinct instances, and generates customized operators based on visual cues.

## Key Results
- Outperforms leading TSP heuristics and algorithms on numerous benchmarks, with some solutions better than TSPLIB optima in floating-point sense
- Achieves state-of-the-art performance on six out of eight EDA benchmarks using extremely low computational resources
- Visual information significantly increases algorithm heterogeneity across geometrically distinct TSP instances but may amplify commonalities in high-dimensional EDA layouts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The Critical Part (CP) abstraction enables FWA to scale from low-dimensional combinatorial problems to million-dimensional continuous optimization by switching between global optimizer evolution and local heuristic evolution.
- **Mechanism**: CP classifies problems into those suitable for direct FWA optimization (e.g., TSP with hundreds of cities) versus those requiring integration into existing frameworks (e.g., EDA with millions of variables). For the latter, the framework evolves only the step-size controller within DreamPlace rather than the full optimizer, reducing the search space while preserving FWA's exploration principles.
- **Core assumption**: The critical bottleneck in high-dimensional problems can be localized to a specific subcomponent (e.g., step-size selection) rather than requiring full optimizer redesign.
- **Evidence anchors**:
  - [abstract]: "introduces the concept of Critical Part (CP), which allows FWA to be applied to high-dimensional problems like Electronic Design Automation (EDA) by evolving local optimizers rather than global ones"
  - [Section III]: "For cases exhibiting high dimensionality and challenging optimization landscapes exemplified by EDA, we forgo global FWA optimization. Instead, we evolve local FWA to refine step-size rule"
  - [corpus]: No direct corpus evidence for CP concept specifically; this appears novel to this framework
- **Break condition**: If the target problem's critical bottleneck cannot be isolated to a well-defined subcomponent with clear input/output signatures, CP evolution may fail to improve over baselines.

### Mechanism 2
- **Claim**: Visual optimization information increases algorithm heterogeneity across problem instances when visual features clearly distinguish instances, but may amplify surface commonalities when visual representations are similar across instances.
- **Mechanism**: MLLM receives three visualizations for TSP (route with convex hull, path crossing heatmap, route density analysis) and placement layouts for EDA. When visual features capture instance-specific geometry (TSP), MLLM generates more customized operators. When visual features are dominated by common topological patterns (EDA high-dimensional layouts), MLLM converges toward similar solutions.
- **Core assumption**: MLLM can extract actionable optimization cues from visual representations that correlate with solution quality improvements.
- **Evidence anchors**:
  - [Section IV-A]: "The average cross-instance similarity with visual information is 0.8848±0.03, significantly exceeding that without visual information (0.8325±0.06)"
  - [Section IV-B]: "In EDA task, the calculated p-values no longer support the statistically significant conclusion that the use of visual modality optimization information significantly reduces the similarity"
  - [corpus]: GAMA paper (arxiv:2511.07850) demonstrates graph-aware multi-modal attention for VRP, suggesting visual/spatial information can enhance optimization but effectiveness is representation-dependent
- **Break condition**: If visual representations do not provide discriminative information about solution quality differences across instances, adding visual modality may increase noise without improving guidance.

### Mechanism 3
- **Claim**: The code + performance feedback loop with mutation and crossover operations enables iterative refinement of optimization heuristics without requiring gradient information about the algorithm design space.
- **Mechanism**: Each iteration generates new CP code via MLLM using either mutation (modifying one operator from a single parent) or crossover (combining operators from two parents). The Problem Evaluator executes the code and returns both textual (tour length, wHPWL) and visual feedback. The population pool (size 5) is updated via greedy selection over 200 iterations.
- **Core assumption**: The MLLM has sufficient code generation capability to produce syntactically correct and semantically meaningful operator modifications that inherit useful properties from parents.
- **Evidence anchors**:
  - [Section IV]: "we have a pool with population capacity of 5 managed via greedy selection and a maximum number of iteration of 200, with one mutation and one crossover operation per iteration"
  - [Section III]: "Then the MLLM generates and returns the new code of the critical part, which will be sent to the problem evaluator for evaluation"
  - [corpus]: Related work (EOAGP, CALM, FunSearch) demonstrates LLM-based algorithm evolution but typically text-only; corpus provides no direct comparison for mutation/crossover with visual feedback
- **Break condition**: If MLLM code generation produces frequent syntax errors or semantic drift that breaks compatibility with the evaluation framework, the feedback loop fails to accumulate improvements.

## Foundational Learning

- **Concept: Fireworks Algorithm (FWA) operators**
  - Why needed here: The entire framework evolves FWA operators (explosion, mutation, selection); understanding their roles is prerequisite to interpreting generated code.
  - Quick check question: Can you explain how the explosion operator generates candidate solutions and how its amplitude affects exploration versus exploitation?

- **Concept: Swarm intelligence optimization principles**
  - Why needed here: FWA is a swarm intelligence method; understanding population-based search helps diagnose why visual information affects algorithm heterogeneity.
  - Quick check question: Why might maintaining diversity in a population of algorithm designs be important for generalization across problem instances?

- **Concept: First-order optimizer step-size heuristics (Barzilai-Borwein)**
  - Why needed here: The EDA experiments replace BB step-size with learned FWA-based step controllers; understanding BB's limitations motivates the approach.
  - Quick check question: What failure modes of the BB step-size heuristic might a learned step controller address in non-convex optimization landscapes?

## Architecture Onboarding

- **Component map**:
  - MLLM (Doubao-1.5-pro-vision) -> Prompt Pattern (mutation/crossover templates) -> Problem Evaluator (TSP/EDA execution) -> Critical Part (FWA operators or step-size function) -> Population Pool (size 5, greedy selection)

- **Critical path**:
  1. Classify problem -> determine CP type (global FWA vs. local idea evolution)
  2. Initialize population with baseline CP implementations
  3. For each iteration: generate 2 candidates (1 mutation, 1 crossover)
  4. Execute candidates -> collect textual + visual feedback
  5. Update population via greedy selection
  6. After convergence, deploy best CP to target problem

- **Design tradeoffs**:
  - Visual information: May help for geometrically distinct instances (TSP), but adds no value or degrades performance when visual features are similar across instances (EDA); requires ablation testing per task
  - Population size vs. iteration budget: Small population (5) with many iterations (200) works for simple CPs; high-dimensional problems may benefit from larger populations
  - Computational overhead: TSP evaluation is cheap; EDA requires full DreamPlace execution per candidate

- **Failure signatures**:
  - Generated CP code fails syntax validation -> prompt refinement needed
  - Population converges to homogeneous solutions early -> increase mutation diversity or reduce greedy selection pressure
  - Visual modality degrades performance -> ablate visual information; check if visualizations highlight meaningful differences
  - EDA step-size controller fails to improve over BB baseline -> verify `fn` evaluation is being called; check if base_step baseline is too strong

- **First 3 experiments**:
  1. **TSP ablation on small instance (eil51)**: Run framework with and without visual information; compare tour quality and algorithm heterogeneity to establish baseline sensitivity to visual modality
  2. **EDA single-instance step-size evolution**: Target one benchmark (adaptec1); confirm MLLM can successfully call `fn` for multi-evaluation step-size selection and beat the base_step baseline
  3. **Cross-instance generalization test**: Train CP on one TSP instance, evaluate on another without further evolution; measure generalization gap to understand instance-specific customization

## Open Questions the Paper Calls Out

- **Question**: What are the comparative advantages and optimal use-cases for heuristic-based algorithm design versus optimizer-based algorithm design when using Large Language Models?
  - **Basis in paper**: [explicit] Section III states, "The comparative effectiveness of these two paradigms remains an open question," distinguishing between discovering heuristic rules (e.g., AlphaEvolve) and improving optimizers (e.g., discrete FWA).
  - **Why unresolved**: The paper utilizes optimizer-based design for TSP and heuristic-like local evolution for EDA but does not provide a theoretical or empirical comparison of the two distinct paradigms on the same task.
  - **What evidence would resolve it**: A comprehensive study applying both paradigms to identical optimization tasks to measure efficiency, solution quality, and convergence speed.

- **Question**: Under what specific conditions does visual modality information enhance versus hinder the performance of the MLLM-driven optimization framework?
  - **Basis in paper**: [explicit] The paper notes in the contributions and analysis that "the introduction of visual modalities in the optimization process does not guarantee to improve the performance," observing performance drops on some TSP instances (eil51, st70) and mixed results in EDA.
  - **Why unresolved**: The authors observe that visual info helps MLLMs capture instance-specific features in low dimensions but amplifies "surface commonalities" in high dimensions, lacking a predictive model for when visual inputs are beneficial.
  - **What evidence would resolve it**: Identification of quantitative metrics (e.g., visual variance, information entropy of the image) that correlate with performance gains when visual inputs are included.

- **Question**: Can the Critical Part (CP) concept and the MLLM framework be effectively generalized to first-order optimization methods or other swarm intelligence algorithms?
  - **Basis in paper**: [explicit] The conclusion claims, "it's obvious that this framework can be quickly migrated to other swarm intelligence optimization algorithms or even first-order optimizers," but provides no experimental validation for this transferability.
  - **Why unresolved**: The current experimental scope is limited to the Fireworks Algorithm (FWA) applied to TSP and EDA tasks.
  - **What evidence would resolve it**: Successful application of the CP framework to generate or improve algorithms like Particle Swarm Optimization (PSO) or Gradient Descent on standard benchmarks.

## Limitations
- Framework's reliance on visual information shows inconsistent benefits across problem domains, with clear advantages for geometrically distinct instances but no improvement or degradation for structurally similar instances
- Critical details about FWA hyperparameters, initial population generation, and exact MLLM API integration are omitted, making faithful reproduction challenging
- Claims of state-of-the-art performance lack comprehensive comparison against the strongest recent optimization approaches, particularly for EDA tasks

## Confidence
- **High confidence**: Technical feasibility of CP abstraction and its application to both TSP and EDA tasks, as mechanism descriptions are coherent and technically sound
- **Medium confidence**: Claimed performance improvements, given strong results but limited baseline comparisons and potential hyperparameter sensitivity
- **Low confidence**: General applicability of visual information, as paper itself acknowledges contradictory effects across different problem domains without clear predictive criteria

## Next Checks
1. **Ablation study on visual modality**: Run the complete framework on TSP (eil51, kroA100) with and without visual information across multiple random seeds to quantify the consistency and magnitude of visual benefits, particularly examining whether geometric distinctiveness correlates with visual advantage
2. **EDA step-size integration verification**: Implement the exact step-size replacement in DreamPlace, verifying that the `fn` callable is properly integrated into the conjugate gradient loop and that the generated code can successfully call `fn` for multi-evaluation step selection, then test whether the evolved step-size beats the BB baseline on a single instance (adaptec1)
3. **Cross-instance generalization test**: Train CP on one TSP instance, then evaluate directly on another instance without further evolution to measure the generalization gap, helping determine whether the framework learns instance-specific heuristics or general optimization principles