---
ver: rpa2
title: 'SEAL: Structure and Element Aware Learning to Improve Long Structured Document
  Retrieval'
arxiv_id: '2508.20778'
source_url: https://arxiv.org/abs/2508.20778
tags:
- retrieval
- document
- documents
- seal
- structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SEAL introduces structure-aware learning and element-level alignment
  to improve long structured document retrieval. By leveraging HTML transformation
  and masked element prediction, it enables fine-grained semantic discrimination and
  hierarchical structure preservation.
---

# SEAL: Structure and Element Aware Learning to Improve Long Structured Document Retrieval

## Quick Facts
- arXiv ID: 2508.20778
- Source URL: https://arxiv.org/abs/2508.20778
- Reference count: 10
- Primary result: SEAL achieves 4.65% absolute NDCG@10 improvement over baselines on long structured document retrieval

## Executive Summary
SEAL introduces structure-aware learning and element-level alignment to improve long structured document retrieval. By leveraging HTML transformation and masked element prediction, it enables fine-grained semantic discrimination and hierarchical structure preservation. The method addresses limitations of existing contrastive learning approaches that ignore structural metadata and fine-grained semantics. SEAL was evaluated on a newly released dataset (StructDocRetrieval) and industrial data, demonstrating consistent performance improvements across multiple PLMs. On BGE-M3, it achieved NDCG@10 of 77.84%, compared to 73.96% for baselines, representing a 4.65% absolute gain. Online A/B testing validated practical effectiveness with measurable improvements in click-through rates.

## Method Summary
SEAL employs a two-stage training approach for long structured document retrieval. First, Element-Aware Alignment (EAL) masks 10% of structural element tags to force fine-grained semantic discrimination. Second, Structure-Aware Learning (SAL) contrasts tagged and tag-stripped document variants to preserve hierarchical structure. The method processes HTML documents through standardization, tag processing, and element masking before applying contrastive learning objectives. Training uses BGE-M3 encoder with 4096-token context, Adam optimizer (lr=1e-5), 8 negatives per query, and cross-device negative sampling. The approach requires sequential EAL→SAL training rather than joint optimization.

## Key Results
- SEAL achieved NDCG@10 of 77.84% on BGE-M3, a 4.65% absolute improvement over baselines (73.96%)
- Element-Aware Alignment alone outperformed Structure-Aware Learning alone (80.85% vs 80.08% NDCG@10)
- Online A/B testing demonstrated measurable improvements in click-through rates
- Consistent improvements observed across multiple PLMs including mE5 and GTE-Qwen2

## Why This Works (Mechanism)

### Mechanism 1: Structure-Aware Learning via HTML Tag Contrast
Incorporating HTML structural tags during contrastive learning enables models to recognize semantic hierarchies that plain text processing discards. The model is trained with dual document variants—intact HTML (d⁺_tag) and tag-stripped plain text (d⁺_untag)—forcing the encoder to map both to the same query-aligned embedding space. This teaches the model that structural semantics should be invariant to tag presence. Core assumption: Semantic hierarchies encoded in HTML tags (e.g., h1/h2 headings, bold markers) carry retrieval-relevant signals that PLMs pre-trained on plain text do not naturally exploit.

### Mechanism 2: Element-Aware Alignment via Masked Tag Prediction
Randomly masking structural element tags (not content tokens) forces models to perform fine-grained semantic discrimination at the element level. A subset of elements (10% optimal per ablation) has their tags removed while preserving text content. The contrastive objective requires the model to infer document relevance from partial structural cues, preventing over-reliance on any single tag. Core assumption: Element-level structural patterns (heading → paragraph relationships) encode query-relevance signals that aggregate-level representations miss.

### Mechanism 3: Progressive Two-Stage Training (EAL → SAL)
Sequential training—EAL first, then SAL—outperforms simultaneous joint training by establishing local semantic foundations before global structure integration. EAL learns element-level sensitivity (simpler task), producing high-quality text representations. SAL then integrates global hierarchical structure (harder task), building on EAL's foundation rather than learning from noise. Core assumption: Structure-aware learning benefits from pre-aligned local representations; simultaneous optimization creates conflicting gradients.

## Foundational Learning

- Concept: Contrastive Learning for Dense Retrieval
  - Why needed here: SEAL builds on standard contrastive objectives (InfoNCE-style) where queries are pulled closer to relevant documents and pushed from irrelevant ones in embedding space.
  - Quick check question: Can you explain why in-batch negatives improve over single-negative training?

- Concept: HTML DOM Structure and Semantic Markup
  - Why needed here: The method relies on extracting and manipulating HTML tag hierarchies (h1, h2, p, lists) as structural signals.
  - Quick check question: What distinguishes semantic HTML tags from presentational tags, and why would the former matter for retrieval?

- Concept: Masked Prediction Objectives (MLM variants)
  - Why needed here: EAL adapts masked prediction from token-level (MLM) to element-tag-level, requiring understanding of how masking creates auxiliary training signals.
  - Quick check question: How does masking 10% of element tags differ from masking 15% of tokens in standard BERT pre-training?

## Architecture Onboarding

- Component map:
Input Documents → HTML Standardization → Tag Processing (retain/remove variants) → Element Masking (10% stochastic) → Stage 1: EAL Loss (masked element contrast) → Stage 2: SAL Loss (tag/untag contrast) → PLM Encoder (e.g., BGE-M3) → Unified Embedding Space

- Critical path:
1. Dataset preparation (HTML conversion, tag processing) determines data quality ceiling
2. Element mask ratio (10% default) controls fine-grained signal strength
3. Training stage order (EAL→SAL) determines final representation quality
4. Base PLM selection (BGE-M3, mE5) affects absolute performance but not relative gains

- Design tradeoffs:
  - Chunk-based vs. SEAL: Chunking (512 tokens) discards document-level structure; SEAL preserves it but requires 4096-token context
  - MCLS vs. SEAL: MCLS averages multiple [CLS] tokens for long documents but ignores hierarchy; SEAL explicitly encodes it
  - Encoder-only vs. decoder-only: LLM-based embedders (GTE-Qwen2) add latency without proportionate gains

- Failure signatures:
  - NDCG improvement <1%: Check if documents actually contain semantic HTML or only auto-generated wrapper tags
  - Training loss plateaus early: Verify element masking is applied (not just tag removal); check mask ratio
  - Embedding visualizations show no query-doc clustering: EAL may not be running; verify two-stage training sequence

- First 3 experiments:
1. Baseline reproduction: Run BGE-M3 with standard contrastive fine-tuning on your data; establish NDCG@10 baseline
2. Ablation check: Train with EAL-only and SAL-only separately; verify EAL contributes more (per Table 5 patterns)
3. Mask ratio sweep: Test [1%, 5%, 10%, 30%, 50%] on a validation split; confirm 10% optimal or find domain-specific optimum

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does SEAL generalize effectively to structured retrieval tasks with distinct grammars, such as code retrieval?
- Basis in paper: [explicit] The authors state that reliance on alignment signals "raises open questions about its general superiority over baseline models in all downstream tasks, such as code retrieval."
- Why unresolved: The current evaluation focuses exclusively on natural language documents (HTML/web articles), whereas code retrieval involves different structural logic and syntax.
- What evidence would resolve it: Experimental results on code retrieval benchmarks (e.g., CodeSearchNet) comparing SEAL against standard code retrieval baselines.

### Open Question 2
- Question: Does the structure-aware learning approach maintain its effectiveness when applied to English or multilingual corpora?
- Basis in paper: [explicit] The authors note that "current empirical validation focuses primarily on the Chinese-language community" and they are "actively constructing an English-language corpus."
- Why unresolved: Structural cues in HTML may interact differently with morphological features in non-Chinese languages, and current datasets are monolingual.
- What evidence would resolve it: Evaluation of SEAL on the planned English corpus showing comparable NDCG@10 improvements over baselines.

### Open Question 3
- Question: Can Structure-Aware Learning (SAL) and Element-Aware Alignment (EAL) be adapted as pre-training objectives rather than fine-tuning methods?
- Basis in paper: [explicit] The Limitations section notes that the "approach preserves the potential of exploiting the document structure during pre-training."
- Why unresolved: The paper validates SEAL only as a continual fine-tuning framework; it is unknown if the objectives function effectively at the pre-training scale.
- What evidence would resolve it: A comparative study of models pre-trained with SEAL objectives versus fine-tuned models on the same downstream retrieval tasks.

## Limitations
- Dataset Representativeness: StructDocRetrieval may not fully capture industrial HTML document diversity with proprietary formatting and varied semantic markup practices
- Computational Overhead: Dual-document processing and 4096-token context increase requirements by 30-40% compared to standard chunking approaches
- Structural Tag Quality Dependence: Effectiveness hinges on meaningful semantic HTML tags; auto-generated markup may yield diminishing returns

## Confidence
- High Confidence: SAL mechanism (4.65% NDCG@10 improvement) and EAL contribution (ablation study)
- Medium Confidence: Two-stage training hypothesis (EAL→SAL) supported by Table 7 but lacks independent validation
- Low Confidence: Industrial dataset results cannot be independently verified due to unavailability

## Next Checks
1. **Mask Ratio Sensitivity Analysis**: Conduct controlled experiments varying the element mask ratio from 1% to 50% on StructDocRetrieval to confirm the 10% optimum and identify domain-specific sweet spots. Monitor how performance degrades as masking exceeds 30%.

2. **Cross-Domain Generalization Test**: Apply SEAL to domains with minimal semantic HTML (e.g., plain text academic papers converted to HTML, or documents with primarily presentational markup) to quantify the structural tag quality dependence and identify failure thresholds.

3. **Single-Stage vs. Two-Stage Training Comparison**: Implement a parameterized training regime that can switch between joint (simultaneous) and sequential (EAL→SAL) training to independently verify the Table 7 results across different PLM backbones and document types.