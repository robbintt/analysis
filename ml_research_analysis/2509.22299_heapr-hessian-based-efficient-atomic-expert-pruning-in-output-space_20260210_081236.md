---
ver: rpa2
title: 'HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space'
arxiv_id: '2509.22299'
source_url: https://arxiv.org/abs/2509.22299
tags:
- atomic
- expert
- pruning
- experts
- heapr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of compressing Mixture-of-Experts
  (MoE) models by proposing a novel atomic expert pruning approach called HEAPr. Existing
  methods focus on coarse-grained expert-level pruning, which often leads to accuracy
  degradation.
---

# HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space

## Quick Facts
- arXiv ID: 2509.22299
- Source URL: https://arxiv.org/abs/2509.22299
- Authors: Ke Li; Zheng Yang; Zhongbin Zhou; Feng Xue; Zhonglin Jiang; Wenxiao Wang
- Reference count: 12
- Primary result: Achieves nearly lossless MoE compression at 20%-25% pruning ratios using atomic expert decomposition and Hessian-based importance scoring

## Executive Summary
HEAPr introduces a novel atomic expert pruning approach for Mixture-of-Experts models that overcomes the accuracy degradation issues of existing expert-level pruning methods. By decomposing experts into smaller atomic experts and leveraging second-order information to quantify their importance, HEAPr achieves substantial compression while preserving model performance. The method requires only three passes through a small calibration set, making it computationally efficient compared to existing approaches.

## Method Summary
HEAPr transforms the second-order information from expert parameters to atomic expert outputs, reducing space complexity from O(d^4) to O(d^2). The approach computes atomic expert importance using two forward passes and one backward pass on a small calibration set. This fine-grained perspective enables more precise pruning decisions compared to coarse-grained expert-level methods, allowing for higher compression ratios without significant accuracy loss.

## Key Results
- Achieves nearly lossless compression at 20%-25% pruning ratios across multiple MoE models
- Reduces FLOPs by approximately 20% while maintaining core model performance
- Outperforms existing expert-level pruning methods on DeepSeekMoE-16B-Base, Qwen1.5-MoE-A2.7B-Chat, Qwen2-57B-A14B, and Qwen3-30B-A3B architectures

## Why This Works (Mechanism)
HEAPr's effectiveness stems from its atomic expert decomposition approach that provides finer granularity for pruning decisions. By leveraging second-order information (Hessian matrix) to quantify atomic expert importance, the method can identify and remove less critical components while preserving the most impactful ones. The transformation of second-order information from parameter space to output space enables efficient computation while maintaining the accuracy of importance scores.

## Foundational Learning
1. **Atomic Expert Decomposition** - Why needed: Enables fine-grained pruning beyond expert-level granularity; Quick check: Verify that atomic experts are truly indivisible and maintain functional independence
2. **Hessian Matrix Computation** - Why needed: Captures second-order sensitivity information for importance scoring; Quick check: Confirm O(d^2) space complexity through empirical measurement
3. **Output Space Transformation** - Why needed: Reduces computational complexity while preserving pruning accuracy; Quick check: Validate that transformed importance scores correlate with parameter-space scores
4. **Mixture-of-Experts Architecture** - Why needed: Understanding MoE gating mechanisms and expert specialization; Quick check: Examine gating distribution before and after pruning
5. **Pruning Ratio Scaling** - Why needed: Determines practical limits of compression without accuracy loss; Quick check: Test at 50%+ compression ratios to identify breaking points
6. **Regularization During Fine-tuning** - Why needed: Maintains accuracy during iterative pruning; Quick check: Compare convergence speed with and without regularization

## Architecture Onboarding
**Component Map:** Input -> Gating Network -> Atomic Experts -> Output Space -> Hessian Transformation -> Importance Scoring -> Pruning Decision
**Critical Path:** Gating Network → Atomic Experts → Output → Hessian Computation → Pruning Selection
**Design Tradeoffs:** Fine-grained atomic decomposition vs. computational overhead; Second-order information vs. storage requirements; Compression ratio vs. accuracy preservation
**Failure Signatures:** Accuracy degradation at high compression ratios; Computational bottlenecks in Hessian transformation; Gating network imbalance after pruning
**First Experiments:**
1. Validate atomic expert independence by testing isolated removal effects
2. Benchmark O(nk^2) complexity scaling on models with 1000+ experts
3. Test architectural generalization on MoE variants with multiple feedforward networks per expert

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Computational complexity of O(nk^2) Hessian transformation may become prohibitive for very large models with many atomic experts
- Effectiveness of expert activation regularization during fine-tuning lacks thorough empirical validation
- Experimental scope limited to 20%-25% compression ratios, with unclear behavior at higher ratios
- Assumes specific MoE architecture with single feedforward networks per expert, limiting generalizability

## Confidence
- High confidence in theoretical framework and Hessian transformation approach
- Medium confidence in practical efficiency claims given limited scaling experiments
- Medium confidence in accuracy preservation claims based on current experimental scope
- Low confidence in generalization to different MoE architectures

## Next Checks
1. Evaluate HEAPr at higher compression ratios (50%+) to determine accuracy degradation thresholds and identify the breaking point of the method
2. Test the approach on MoE variants with multiple feedforward networks per expert to verify architectural generalizability
3. Benchmark the actual computational overhead of the O(nk^2) Hessian transformation step on large-scale models (1000+ experts) to validate the practical efficiency claims