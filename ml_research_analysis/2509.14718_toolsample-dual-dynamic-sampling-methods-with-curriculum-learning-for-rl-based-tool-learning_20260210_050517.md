---
ver: rpa2
title: 'ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based
  Tool Learning'
arxiv_id: '2509.14718'
source_url: https://arxiv.org/abs/2509.14718
tags:
- tool
- learning
- training
- arxiv
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DSCL, a dual dynamic sampling framework for
  reinforcement learning-based tool learning. The key insight is that standard dynamic
  sampling methods fail in tool learning due to its multi-task structure and multi-valued
  reward mechanisms.
---

# ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning

## Quick Facts
- **arXiv ID**: 2509.14718
- **Source URL**: https://arxiv.org/abs/2509.14718
- **Reference count**: 17
- **Primary result**: 3.29% improvement over baselines on BFCLv3 and API-Bank benchmarks

## Executive Summary
This paper introduces DSCL, a dual dynamic sampling framework for reinforcement learning-based tool learning. The key insight is that standard dynamic sampling methods fail in tool learning due to its multi-task structure and multi-valued reward mechanisms. DSCL combines Reward-Based Dynamic Sampling, which uses mean and variance statistics to prioritize informative samples, with Task-Based Dynamic Curriculum Learning, which adaptively focuses on sub-tasks based on difficulty and convergence status. Experiments on BFCLv3 and API-Bank benchmarks show DSCL achieves a 3.29% improvement over baselines, with the method successfully filtering out low-value samples while maintaining training stability.

## Method Summary
DSCL is a dual dynamic sampling framework that addresses sample inefficiency in RL-based tool learning by combining two complementary mechanisms: (1) Reward-Based Dynamic Sampling (RDS) uses multi-dimensional reward statistics (mean and variance) to categorize samples into four difficulty levels with sampling ratios of 0.0, 0.5, or 1.0, and (2) Task-Based Dynamic Curriculum Learning (TDCL) implements a three-stage curriculum that adjusts reward weights for sub-tasks based on their difficulty hierarchy and convergence status. The method requires a warmup period where RDS is disabled until format stability is achieved, after which dynamic sampling activates to filter redundant samples while the curriculum ensures proper focus on progressively harder sub-tasks. Implemented within the GRPO framework with 8 rollouts per sample on 8 H20 GPUs (96GB), DSCL demonstrates superior performance across all sub-tasks, particularly excelling in multi-turn dialogue scenarios requiring complex reasoning.

## Key Results
- DSCL achieves 3.29% improvement over baselines on BFCLv3 and API-Bank benchmarks
- The method successfully filters out homogeneous samples that provide diminishing learning value while maintaining training stability
- Particularly excels in multi-turn dialogue scenarios where complex reasoning is required
- RDS activation after warmup prevents training collapse from premature sample rejection

## Why This Works (Mechanism)

### Mechanism 1: Multi-Dimensional Reward Statistics for Sample Filtering
In tool learning with multi-valued rewards, mean and variance provide independent signals for sample value assessment, enabling more nuanced filtering than binary reward regimes. The RDS component computes three indicators per sample: (1) mean reward across G rollouts, (2) sample-level variance across rollouts, and (3) epoch-level variance tracking historical reward evolution. Samples are categorized and assigned sampling ratios: easy data (mean=4, ratio=0), hard data with high variance (ratio=1.0), hard data with low variance (ratio=0), intermediate with high variance (ratio=1.0), intermediate with low variance (ratio=0.5). This filters out homogeneous samples that provide diminishing learning value while retaining exploratory samples.

### Mechanism 2: Task-Based Curriculum Staged by Sub-task Dependencies
Tool learning sub-tasks exhibit asynchronous convergence and hierarchical dependencies that can be exploited through staged reward weighting. TDCL implements three stages with dynamically adjusted reward weights: Stage 1 amplifies format reward (2.5x) while reducing other weights (0.5x) to establish structural competence; Stage 2 shifts focus to tool name and parameter key extraction (1.5x); Stage 3 emphasizes parameter value completion (2.5x) as the hardest sub-task. Stage transitions are determined by monitoring rewards across recent batches.

### Mechanism 3: Warmup-Gated Dynamic Sampling Activation
Dynamic sampling in tool learning requires a format-stability warmup period; premature activation causes training collapse due to insufficient valid training data. RDS is disabled during initial training. Activation occurs only when mean reward exceeds 1.0 for seven consecutive batches. Before stability, format errors cause most rollouts to receive zero reward, leaving insufficient valid samples. Post-warmup, the model has sufficient format proficiency that filtered samples still provide adequate training signal.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**: Why needed here: DSCL is implemented within GRPO framework; it computes advantages across G rollouts per sample and applies sampling ratios to advantage values before model update. Understanding how rollouts are generated and compared is essential.
  - Quick check question: Can you explain why GRPO uses group-relative advantages rather than absolute rewards, and how DSCL's sampling ratio modification interacts with this?

- **Multi-valued Reward Functions vs. Binary Rewards**: Why needed here: The paper's core insight is that multi-valued rewards (e.g., partial credit for correct tool name but wrong parameters) decouple mean-variance relationships that are locked in binary settings. This enables independent signals but requires different filtering logic.
  - Quick check question: Given a reward range of [-3, 4] with multiple sub-task components, why would a sample with mean=2.0 and variance=0.1 be treated differently than one with mean=2.0 and variance=1.5?

- **Tool Learning Sub-task Decomposition**: Why needed here: The framework depends on four sub-tasks (format, tool name, parameter key, parameter value) with defined reward functions. TDCL stages are designed around their dependency structure.
  - Quick check question: If format reward is 0 (malformed output), what happens to R_name, R_key, and R_value, and why does this matter for warmup timing?

## Architecture Onboarding

- **Component map**:
```
Input Query → Policy Model → G Rollouts → Reward Calculator
                                              ↓
                            ┌─────────────────────────────────┐
                            │ Sub-task Rewards (R_format,     │
                            │ R_name, R_key, R_value)         │
                            └─────────────────────────────────┘
                                     ↓              ↓
                          ┌──────────────┐  ┌──────────────┐
                          │     RDS      │  │    TDCL      │
                          │ (per-sample  │  │ (stage-      │
                          │  ratio)      │  │  weighted    │
                          └──────────────┘  │  rewards)    │
                                     ↓      └──────────────┘
                              Ratio × Adjusted_R
                                     ↓
                            Advantage Calculation
                                     ↓
                            Policy Update (GRPO)
```

- **Critical path**:
  1. Implement four sub-task reward functions exactly per equations (1-5) with correct mapping function M
  2. Build warmup monitor tracking 7-batch rolling mean against threshold 1.0
  3. Implement RDS categorization logic with configurable t_mean (default 0.5) and t_var (default 0.1)
  4. Implement TDCL stage detection based on sub-task reward trends
  5. Apply: Advantage = Advantage × Ratio (line 8, Algorithm 1)

- **Design tradeoffs**:
  - t_mean threshold: Lower values retain more hard samples but risk including unsolvable examples; higher values may filter too aggressively
  - Warmup threshold (1.0): Conservative choice ensures stability but delays RDS benefits; paper does not ablate this
  - Stage weighting multipliers (2.5x/0.5x/1.5x): Heuristically chosen; paper does not provide sensitivity analysis
  - G=8 rollouts: Follows ToolRL; fewer rollouts reduce variance estimation reliability

- **Failure signatures**:
  - Training rewards plateau early with RDS enabled → likely premature activation; verify warmup criterion
  - Format accuracy degrades mid-training → TDCL stage 2-3 may reduce format weight too aggressively; consider minimum format weight floor
  - Model overfits to simple multi-turn cases → RDS may be filtering hard samples with low variance; check if t_var threshold is appropriate for your reward scale
  - Advantage values collapse to near-zero → sampling ratios may be too sparse; verify category distribution

- **First 3 experiments**:
  1. Warmup ablation: Replicate Table 3 comparison (RDS w/o CL vs. RDS with CL) on your target model to confirm warmup necessity before investing in full implementation
  2. Threshold sensitivity: Sweep t_mean ∈ {0.3, 0.5, 0.7} and t_var ∈ {0.05, 0.1, 0.2} on held-out validation set; monitor both final accuracy and training stability (variance of per-batch rewards)
  3. Stage transition analysis: Log which samples are retained/discarded per stage and which sub-task rewards are amplified; verify that Stage 3 correlates with increased parameter-value accuracy per Figure 3 pattern

## Open Questions the Paper Calls Out

### Open Question 1
How should the reward variance and mean thresholds (t_var, t_mean) be optimally scheduled or adapted during training, rather than set as fixed hyperparameters? The authors state "t_mean and t_var are treated as hyperparameters that are adjusted during training. This allows the method to accommodate the sensitivity of the reinforcement learning process to variations in data and model states." The paper does not provide a principled method for adapting these thresholds; they are manually tuned. As training progresses and reward distributions shift (Figure 1 shows variance collapse), static thresholds may become suboptimal.

### Open Question 2
What is the theoretical justification for the specific curriculum multipliers (2.5×, 1.5×, 0.5×) used in TDCL, and would learned or adaptive weightings improve performance? These appear to be heuristics; the paper does not explore whether the optimal weighting scheme varies by model size, dataset, or sub-task difficulty distribution.

### Open Question 3
Can the DSCL framework generalize to other multi-task RL domains with fine-grained reward structures beyond tool learning (e.g., code generation with unit-test rewards, multi-skill robotics)? The authors note that "sample selection methods... have proven effective in various RL tasks such as mathematics and code generation, but their application in tool learning remains unexplored." They position DSCL as tool-learning-specific, implying uncertainty about broader applicability.

### Open Question 4
How does DSCL scale to larger models (e.g., 70B+ parameters) and significantly larger training datasets? Experiments use only Qwen2.5-7B and a 4K-sample training set. The variance-collapse phenomenon (Figure 1) may manifest differently at scale, and the computational overhead of computing per-sample statistics may become prohibitive.

## Limitations

- Critical reproducibility gaps: The paper lacks precise specification of TDCL stage transition criteria and the mapping function M's operational details
- Sample efficiency claims: The 3.29% improvement is measured against baseline ToolRL implementations without comparison against other curriculum learning approaches
- Generalization scope: Results are validated only on BFCLv3 and API-Bank benchmarks with specific reward structures

## Confidence

**High Confidence**: The fundamental insight that multi-valued rewards enable mean-variance decoupling for sample filtering, and that format stability is prerequisite for effective dynamic sampling in tool learning. The warmup mechanism's necessity is well-supported by ablation results.

**Medium Confidence**: The specific categorization thresholds (t_mean=0.5, t_var=0.1) and stage weighting multipliers (2.5x/0.5x/1.5x) were chosen heuristically without sensitivity analysis. The TDCL stage transition logic remains underspecified.

**Low Confidence**: Claims about RDS's superiority over static sampling in scenarios with abundant simple samples would benefit from direct comparison against established curriculum learning methods beyond the current baseline.

## Next Checks

1. **Threshold Sensitivity Analysis**: Perform systematic sweeps of t_mean ∈ {0.3, 0.5, 0.7} and t_var ∈ {0.05, 0.1, 0.2} while monitoring both final accuracy and training stability. Verify that the reported values represent optimal tradeoffs rather than arbitrary choices.

2. **Component Ablation on New Datasets**: Implement DSCL without TDCL (RDS only) and without RDS (TDCL only) on at least one additional tool learning benchmark not used in the original paper. This isolates each mechanism's contribution and tests generalization.

3. **Comparison Against State-of-the-Art Curriculum Methods**: Benchmark DSCL against DUMP or other automated curriculum learning approaches using identical reward structures and datasets. This validates whether the specific RDS/TDCL combination provides unique advantages over general curriculum frameworks.