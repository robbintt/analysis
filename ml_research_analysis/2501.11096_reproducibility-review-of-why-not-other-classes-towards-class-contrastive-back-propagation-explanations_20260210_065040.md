---
ver: rpa2
title: 'Reproducibility review of "Why Not Other Classes": Towards Class-Contrastive
  Back-Propagation Explanations'
arxiv_id: '2501.11096'
source_url: https://arxiv.org/abs/2501.11096
tags:
- original
- explanation
- weighted
- contrastive
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work reproduces and extends the paper "Why Not Other Classes?":
  Towards Class-Contrastive Back-Propagation Explanations, which proposes a method
  for explaining why a neural network chooses one class over others using back-propagation
  from after the softmax layer rather than before. The reproduced experiments show
  similar results to the original paper, validating that the weighted contrastive
  method increases target class probability and accuracy when perturbing input features.'
---

# Reproducibility review of "Why Not Other Classes": Towards Class-Contrastive Back-Propagation Explanations

## Quick Facts
- arXiv ID: 2501.11096
- Source URL: https://arxiv.org/abs/2501.11096
- Reference count: 11
- Primary result: Back-propagating from softmax outputs provides more accurate contrastive explanations than from logits, validated across multiple architectures and datasets

## Executive Summary
This work reproduces and extends "Why Not Other Classes?": Towards Class-Contrastive Back-Propagation Explanations, which proposes explaining neural network class choices by back-propagating from softmax outputs rather than logits. The reproduction validates that the weighted contrastive method increases target class probability and accuracy when perturbing input features. The method generalizes well to Vision Transformers and alternative back-propagation methods like XGradCAM and FullGrad, though some require modifications like removing ReLU to work properly. The authors provide open-source code to address reproducibility issues in the original paper.

## Method Summary
The method generates contrastive explanations for image classifiers by back-propagating gradients from after the softmax layer rather than before it. This weighted contrastive approach explains why one class is chosen over others by highlighting features that increase the target class probability while decreasing competing class probabilities. The method is implemented across multiple architectures including VGG-16, Vision Transformers, and with alternative back-propagation techniques like GradCAM and FullGrad. Experiments use datasets including ILSVRC2012, CUB-200, and Food-101, with perturbation-based evaluation measuring changes in softmax outputs and accuracy.

## Key Results
- Weighted contrastive method increases target class probability and accuracy under feature perturbation
- Back-propagating from softmax provides more accurate contrastive explanations than from logits
- Method generalizes to Vision Transformers and alternative back-propagation techniques (with some modifications)

## Why This Works (Mechanism)
The weighted contrastive approach works by back-propagating from softmax outputs, which normalizes class probabilities and directly captures the relative importance of features in distinguishing between classes. This differs from logit-based approaches that operate on unnormalized scores, leading to more accurate identification of features that make one class more likely than others. The method effectively highlights image regions relevant to class discrimination by focusing on the actual probability distribution rather than raw activations.

## Foundational Learning
- **Softmax back-propagation**: Understanding how gradients flow through softmax for contrastive explanations - needed to modify standard back-propagation approaches
- **Perturbation-based evaluation**: Testing explanation quality by measuring probability changes under controlled input modifications - critical for quantitative validation
- **GradCAM variants**: Adapting gradient-based visualization techniques for contrastive settings - essential for heatmap generation
- **Vision Transformer architectures**: Applying contrastive explanations to transformer-based models - extends method beyond CNNs
- **Normalization in visualizations**: Handling signed saliency maps for proper heatmap display - necessary for accurate qualitative results

## Architecture Onboarding

Component map: Input images → CNN/ViT backbone → Softmax layer → Back-propagation → Saliency maps/heatmaps

Critical path: Feature extraction → Softmax probability calculation → Gradient computation → Perturbation application → Accuracy/probability measurement

Design tradeoffs: Back-propagating from softmax vs logits (better contrastive accuracy vs computational overhead), using absolute values in normalization (better visualization vs potential information loss), perturbation strength ε=3×10^-3 (effective feature modification vs input preservation)

Failure signatures: Poor contrastive explanations when ReLU remains in GradCAM (blocks negative gradients), all-negative saliency maps in FullGrad (requires normalization), weak spatial coherence in ViT GradCAM (needs attention rollout)

First experiments:
1. Fine-tune VGG-16-BN on CUB-200 with specified hyperparameters and validate convergence
2. Implement weighted contrastive back-propagation from softmax and generate basic saliency maps
3. Run perturbation experiments with ε=3×10^-3 and measure target class probability changes

## Open Questions the Paper Calls Out
None

## Limitations
- Exact visualization procedure remains unclear, leading to discrepancies in heatmap outputs between reproduction and original paper
- Perturbation strength stated in original paper (ε=10^-3) differs from actual value used (ε=3×10^-3)
- Some visualization normalization and colormap handling details are unspecified, resulting in larger negative areas in reproduced heatmaps

## Confidence
- High confidence: Weighted contrastive method increases target class probability and accuracy under perturbation
- High confidence: Back-propagating from softmax provides more accurate contrastive explanations than from logits
- Medium confidence: Generalization to Vision Transformers and alternative methods (some require specific modifications)

## Next Checks
1. Compare visualization outputs using the exact same random seed and PyTorch model versions as the original paper
2. Validate perturbation strength by testing both ε=10^-3 and ε=3×10^-3 to quantify the impact on explanation quality
3. Implement the corrected perturbation equations with proper clamping and test on additional ViT variants to confirm generalization claims