---
ver: rpa2
title: 'Still Not There: Can LLMs Outperform Smaller Task-Specific Seq2Seq Models
  on the Poetry-to-Prose Conversion Task?'
arxiv_id: '2511.08145'
source_url: https://arxiv.org/abs/2511.08145
tags:
- sanskrit
- prose
- rules
- llms
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper compares general-purpose LLMs with smaller task-specific\
  \ Seq2Seq models for Sanskrit poetry-to-prose conversion, a linguistically complex\
  \ task requiring compound segmentation, dependency resolution, and syntactic linearisation.\
  \ Two modelling paradigms were evaluated: (1) LLMs with instruction fine-tuning\
  \ and in-context learning using linguistically grounded prompts based on P\u0101\
  \u1E47inian grammar, and (2) a ByT5-Sanskrit Seq2Seq model fine-tuned on the same\
  \ task."
---

# Still Not There: Can LLMs Outperform Smaller Task-Specific Seq2Seq Models on the Poetry-to-Prose Conversion Task?

## Quick Facts
- **arXiv ID**: 2511.08145
- **Source URL**: https://arxiv.org/abs/2511.08145
- **Reference count**: 40
- **Primary result**: Task-specific ByT5-Sanskrit Seq2Seq model outperforms LLMs on Sanskrit poetry-to-prose conversion with BLEU scores of 38.63 (Mahābhārata) and 39.50 (Rāmāyaṇa) versus LLM best of 33.12 and 31.95

## Executive Summary
This paper investigates whether large language models can outperform smaller, task-specific sequence-to-sequence models on the linguistically complex task of converting Sanskrit poetry to prose. The study evaluates both LLMs (using instruction fine-tuning and in-context learning with linguistically grounded prompts) and a ByT5-Sanskrit model fine-tuned on the same task. Results show that the task-specific model significantly outperforms all LLM approaches on both Mahābhārata and Rāmāyaṇa datasets. Human evaluation confirms these findings, with Kendall's Tau showing strong correlation with expert judgments, while BLEU scores do not. The study demonstrates that for morphologically rich, low-resource languages like Sanskrit, task-specific fine-tuning remains essential despite advances in LLM prompting strategies.

## Method Summary
The study evaluates two modelling paradigms: (1) LLMs with instruction fine-tuning and in-context learning using linguistically grounded prompts based on Pāṇinian grammar, and (2) a ByT5-Sanskrit Seq2Seq model fine-tuned on the same task. The poetry-to-prose conversion task requires compound segmentation, dependency resolution, and syntactic linearisation of Sanskrit śloka poetry. Both approaches were evaluated on Mahābhārata and Rāmāyaṇa datasets using BLEU scores and human evaluation by expert annotators. Cross-domain evaluation tested the Seq2Seq model's ability to generalise syntactic principles across epic styles.

## Key Results
- ByT5-Sanskrit model achieved BLEU scores of 38.63 (Mahābhārata) and 39.50 (Rāmāyaṇa), significantly outperforming all LLM approaches
- Best LLM performance reached BLEU scores of 33.12 (Mahābhārata) and 31.95 (Rāmāyaṇa)
- Human evaluation strongly corroborated automated metrics, with Kendall's Tau showing high correlation with expert judgments
- Cross-domain evaluation demonstrated the Seq2Seq model's ability to generalise syntactic principles across epic styles

## Why This Works (Mechanism)
The task-specific ByT5-Sanskrit model outperforms LLMs because it was directly optimized for the specific linguistic challenges of Sanskrit poetry-to-prose conversion through fine-tuning on relevant data. The model learned task-specific patterns for compound segmentation, dependency resolution, and syntactic linearisation that are difficult to capture through instruction tuning alone. LLMs, while possessing broad linguistic knowledge, lack the specialized training on this particular transformation task and the specific morphological and syntactic features of classical Sanskrit. The fine-tuning process allowed the Seq2Seq model to develop precise handling of Pāṇinian grammatical rules and epic Sanskrit stylistic conventions that are not adequately addressed through general-purpose LLM prompting strategies.

## Foundational Learning
- **Sanskrit morphology**: Understanding of complex inflectional patterns and sandhi rules is essential for accurate compound segmentation and word form generation
- **Dependency parsing in classical languages**: Required for resolving syntactic relationships in poetry where word order is flexible and often deviates from prose conventions
- **Pāṇinian grammar principles**: The formal grammatical framework provides the theoretical foundation for linguistically grounded prompt engineering and evaluation criteria
- **Cross-domain generalization**: Ability to apply learned syntactic principles across different epic styles and literary genres without task-specific fine-tuning
- **Evaluation metric selection**: Understanding why BLEU scores may not correlate with human judgments for this specific task, necessitating expert evaluation
- **Low-resource language adaptation**: Strategies for building effective models when training data is limited and morphologically complex

## Architecture Onboarding

**Component Map**: Input Sanskrit Poetry → Tokenization (ByT5 or LLM tokenizer) → Encoder-Decoder Network → Output Sanskrit Prose

**Critical Path**: Raw Sanskrit text → Text preprocessing and tokenization → Model inference → Post-processing and evaluation

**Design Tradeoffs**: Task-specific fine-tuning vs. instruction tuning approach; model size vs. performance; automated metrics vs. human evaluation; cross-domain generalization vs. task-specific optimization

**Failure Signatures**: BLEU score underperformance indicating syntactic linearization errors; compound segmentation failures showing morphological analysis issues; preservation of poetic devices suggesting inadequate transformation of stylistic elements

**First Experiments**:
1. Compare model performance on poetry-to-prose conversion across different classical languages with similar morphological complexity
2. Test the impact of varying amounts of task-specific fine-tuning data on LLM performance
3. Evaluate model robustness to noise and out-of-distribution Sanskrit text

## Open Questions the Paper Calls Out
None

## Limitations
- Results are specific to morphologically rich classical Sanskrit and may not transfer to other low-resource languages
- Manual prompt engineering based on Pāṇinian grammar may not scale to languages lacking formalized grammatical frameworks
- Human evaluation involved only three expert annotators and 100 samples, potentially limiting statistical power
- Asymmetric evaluation setup: ByT5-Sanskrit was fine-tuned while LLMs relied on instruction tuning and few-shot in-context learning
- Does not explore alternative LLM optimization strategies such as continued pre-training or adapter-based fine-tuning

## Confidence

**High Confidence**: Comparative performance results between ByT5-Sanskrit and LLMs, supported by both automated metrics and human evaluation

**Medium Confidence**: Cross-domain generalization findings based on limited testing between two epic styles

**Medium Confidence**: Conclusion about task-specific fine-tuning being essential for morphologically rich languages, requiring additional validation across different language families

## Next Checks
1. Replicate the experimental framework with other morphologically rich classical languages (e.g., Ancient Greek, Classical Arabic) to test generalizability of the task-specific advantage
2. Conduct ablation studies comparing different LLM adaptation strategies including continued pre-training, LoRA adapters, and parameter-efficient fine-tuning against the current instruction-tuning approach
3. Expand human evaluation to include a larger pool of annotators and additional linguistic quality dimensions (e.g., preservation of poetic devices, semantic fidelity) to strengthen the correlation analysis between automated and human metrics