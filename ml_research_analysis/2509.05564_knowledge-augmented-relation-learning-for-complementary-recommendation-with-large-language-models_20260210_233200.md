---
ver: rpa2
title: Knowledge-Augmented Relation Learning for Complementary Recommendation with
  Large Language Models
arxiv_id: '2509.05564'
source_url: https://arxiv.org/abs/2509.05564
tags:
- item
- complementary
- learning
- data
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes KARL, a framework combining active learning
  with large language models to classify complementary item relationships in e-commerce.
  It addresses the challenge of creating reliable function-based labels (FBLs) by
  efficiently expanding a high-quality dataset through selective sampling of uncertain
  pairs and LLM-based annotation.
---

# Knowledge-Augmented Relation Learning for Complementary Recommendation with Large Language Models

## Quick Facts
- arXiv ID: 2509.05564
- Source URL: https://arxiv.org/abs/2509.05564
- Authors: Chihiro Yamasaki; Kai Sugahara; Kazushi Okamoto
- Reference count: 38
- Primary result: Framework improves out-of-distribution (OOD) accuracy by up to 37% through uncertainty-based active learning with LLM annotation

## Executive Summary
This paper addresses the challenge of creating reliable function-based labels (FBLs) for complementary recommendation in e-commerce by combining active learning with large language models (LLMs). The proposed KARL framework iteratively samples uncertain item pairs, uses LLMs to annotate them with consensus filtering, and retrains classifiers on the augmented data. Experiments demonstrate significant OOD accuracy improvements while highlighting a critical tradeoff: the same diversity-inducing sampling strategy that benefits OOD performance can degrade ID accuracy. This asymmetry suggests the need for context-aware dynamic sampling strategies that adapt to prediction contexts.

## Method Summary
KARL employs an iterative active learning loop where a logistic regression classifier trained on human-annotated function-based labels (FBLs) identifies uncertain item pairs for LLM annotation. The framework uses either Query-by-Committee (QBC) or Margin-based uncertainty sampling to select candidates from a large unlabeled pool. Each candidate receives three independent GPT-4o-mini annotations, with only unanimously agreed labels accepted into the training set. The augmented dataset then retrains a bagging ensemble of classifiers, with predictions feeding back into the uncertainty scoring loop. This process continues for multiple rounds, expanding the dataset while maintaining label quality through consensus requirements.

## Key Results
- OOD accuracy improved by up to 37% compared to baseline, with uncertainty sampling outperforming random sampling
- ID accuracy showed minimal improvement (≤ 0.5%) and degraded with prolonged learning using uncertainty-based methods
- Diversity gain from active learning correlated positively with OOD macro-F1 gains but negatively with ID gains beyond a threshold
- The framework successfully expanded dataset coverage while maintaining label quality through LLM consensus filtering

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty Sampling Drives Beneficial Data Diversity
- **Claim**: Selecting item pairs where the classifier has low confidence preferentially samples diverse, informative examples that expand the model's effective feature space coverage.
- **Mechanism**: Uncertainty-based sampling (QBC or Margin) selects pairs near decision boundaries. These boundary cases tend to span underexplored feature regions, creating training data with higher diversity. This diversity directly enables knowledge expansion into previously unlearned regions of the item relationship space.
- **Core assumption**: Uncertain predictions correspond to genuinely informative edge cases rather than annotation noise or inherently ambiguous relationships.
- **Evidence anchors**:
  - [abstract]: "These contrasting results are due to the data diversity driven by KARL's knowledge expansion"
  - [Section 4.4]: "In the OOD setting... the diversity gain exhibits a strong and consistent positive correlation with the macro-F1 gain... resulting in a substantial improvement in accuracy of up to approximately 50%"
  - [corpus]: Limited direct corpus validation; related work on complementary recommendation does not specifically address uncertainty-diversity dynamics.
- **Break condition**: When uncertain samples are predominantly noisy or when training data already has high diversity, additional uncertain samples provide diminishing returns.

### Mechanism 2: LLM Consensus Filtering Approximates Human Annotation Quality
- **Claim**: Requiring multiple independent LLM annotations to agree before acceptance filters out low-quality labels, enabling reliable dataset expansion without human annotators.
- **Mechanism**: Each candidate pair receives three independent GPT-4o-mini annotations using the 9-class FBL schema. Only pairs with unanimous agreement proceed to training. This consensus requirement acts as a quality gate, trading annotation volume for reliability.
- **Core assumption**: LLM agreement across independent calls correlates with label correctness, and the LLM possesses sufficient product domain knowledge to make these judgments.
- **Evidence anchors**:
  - [Section 3, Step 3]: "We applied a consistency protocol to ensure the reliability of the LLM-annotated labels. Under this protocol, only pairs in which the three independent labels are identical are adopted"
  - [Section 2]: Prior work found "GPT-4o-mini achieved a macro-F1 score of 0.849 with human ground truth in 3-class classification"
  - [corpus]: Related paper "Function-based Labels for Complementary Recommendation" validates LLM-as-Judge methodology for this specific task.
- **Break condition**: When item descriptions lack sufficient detail, or when relationships require physical/contextual knowledge absent from the LLM's training distribution.

### Mechanism 3: Context-Dependent Value of Diversity (ID vs. OOD Asymmetry)
- **Claim**: The same diversity-inducing sampling strategy produces opposite effects depending on whether the target distribution matches training (ID) or differs (OOD).
- **Mechanism**: In ID settings, the classifier has already learned stable decision boundaries; diverse uncertain samples act as distributional noise that disrupts these boundaries. In OOD settings, the classifier lacks coverage; diverse samples provide the exact exploration needed to extend decision boundaries into new regions.
- **Core assumption**: The ID/OOD split meaningfully captures distributional shift, and the model cannot simultaneously optimize for refined ID boundaries and expanded OOD coverage using a single sampling strategy.
- **Evidence anchors**:
  - [Section 4.2]: "KARL offered only marginal gains (≤ 0.5%) before steadily degrading the accuracy" with "uncertainty-based sampling methods such as QBC and Margin" causing the most degradation
  - [Section 4.3]: "KARL dramatically improved macro-F1 by up to 37% compared to the baseline" with uncertainty methods outperforming random
  - [corpus]: No corpus papers directly address this ID/OOD active learning tradeoff in recommendation contexts.
- **Break condition**: When ID and OOD distributions overlap substantially, or when model capacity is sufficient to accommodate both refinement and exploration simultaneously.

## Foundational Learning

- **Concept: Active Learning with Uncertainty Sampling**
  - **Why needed here**: KARL's core loop depends on understanding why selecting uncertain samples is more efficient than random sampling, and the tradeoffs between Query-by-Committee and Margin-based approaches.
  - **Quick check question**: Given a classifier with prediction probabilities [0.45, 0.35, 0.20] versus [0.90, 0.05, 0.05], which sample would Margin-based uncertainty sampling prefer and why?

- **Concept: Distribution Shift and Generalization Bounds**
  - **Why needed here**: The paper's central finding is that the same strategy has opposite effects on ID vs. OOD accuracy. Understanding why models fail on shifted distributions is prerequisite to interpreting these results.
  - **Quick check question**: If a model trained on stationery items (pens, paper) is tested on kitchenware (utensils, containers), what specific relationship patterns might fail to transfer even if the model learned general "complementarity" features?

- **Concept: Ensemble Methods for Class Imbalance**
  - **Why needed here**: The paper uses bagging with random undersampling to handle the inherent imbalance in complementary relationship data (most pairs are unrelated).
  - **Quick check question**: Why would training 10 separate classifiers on different balanced subsets and averaging their predictions be preferable to training one classifier on the full imbalanced dataset?

## Architecture Onboarding

- **Component map**: Unlabeled Pool (P_U) -> [Hierarchical Sampler] -> Candidate subset (P_U^t) -> [Uncertainty Scorer] -> Select 1 uncertain pair per category -> P_S^t -> [LLM Annotator + Consensus Filter] -> D_L^t (only unanimous labels) -> [Data Merger] -> D_H ∪ D_L -> Training set -> [Bagging Ensemble Trainer] -> 10 classifiers on balanced subsets -> [Aggregated Predictor] -> Probabilities feed back to Uncertainty Scorer

- **Critical path**: The loop from Uncertainty Scorer -> LLM Annotator -> Retraining -> back to Uncertainty Scorer must complete within cost/time constraints. LLM annotation cost (3 calls per pair, only unanimous pairs kept) is the primary bottleneck.

- **Design tradeoffs**:
  - **QBC vs. Margin sampling**: QBC uses prediction variance across bagging models; Margin uses probability gap between top two classes. QBC leverages the existing ensemble but may inherit ensemble biases. Margin is simpler but less informed by model uncertainty structure.
  - **Consensus threshold**: Three-way unanimous agreement maximizes quality but reduces yield (more rejected pairs = more LLM calls for same dataset size). Two-way agreement increases yield but introduces noise.
  - **Number of active learning rounds**: Paper shows OOD gains plateau around round 15; continuing beyond wastes annotation budget and risks ID degradation.

- **Failure signatures**:
  - **ID accuracy drops sharply after round 5-10**: Diversity threshold exceeded; switch to confidence-weighted sampling or halt active learning for ID-focused deployments.
  - **LLM agreement rate < 30%**: Prompt may be underspecified or items lack sufficient description; add structured prompt elements or filter items with missing attributes.
  - **OOD accuracy plateaus despite continued rounds**: Either exhausted informative uncertain samples or hit LLM annotation quality ceiling; validate LLM labels against human ground truth on a sample.

- **First 3 experiments**:
  1. **Baseline calibration**: Train classifier on D_H alone, measure macro-F1 on both D_H^id (5-fold CV) and D_H^ood. Confirm the distributional shift (paper reports baseline OOD macro-F1 ~0.44).
  2. **Ablation on sampling method**: Run 5 rounds of KARL comparing Random, QBC, and Margin sampling. Measure cost efficiency (LLM calls per percentage point of OOD accuracy gain).
  3. **Diversity-accuracy correlation**: For each round, compute `diversity(X)` metric on the augmented training set and plot against accuracy delta. Confirm the paper's finding that diversity positively correlates with OOD gain but negatively with ID gain past a threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a context-aware dynamic sampling strategy be implemented to automatically switch between preserving learned distributions for in-distribution (ID) contexts and aggressively exploring for out-of-distribution (OOD) contexts?
- Basis in paper: [explicit] The authors state in the Conclusion that future frameworks should implement "context-aware dual modes" to address the issue where diversity degrades ID accuracy while improving OOD accuracy.
- Why unresolved: The current KARL framework uses a static uncertainty sampling approach that consistently increases diversity, which was shown to act as noise that disrupts stable ID distributions.
- What evidence would resolve it: A modified framework demonstrating that it can maintain or improve ID accuracy (preventing the observed degradation) while retaining the significant OOD gains reported in the study.

### Open Question 2
- Question: To what extent can non-linear models, such as gradient boosting or neural networks, improve classification accuracy in OOD settings compared to the logistic regression classifier currently used in KARL?
- Basis in paper: [explicit] The Conclusion notes that "the use of logistic regression may limit accuracy in OOD settings" because its linear decision boundaries may be insufficient for complex non-linear relationships.
- Why unresolved: The study limited its methodology to Bayesian-optimized logistic regression, leaving the potential performance gains of more complex model architectures untested.
- What evidence would resolve it: Experimental results comparing the OOD macro-F1 scores of KARL when implemented with non-linear classifiers versus the logistic regression baseline.

### Open Question 3
- Question: How does the performance and cost-efficiency of the KARL framework vary when utilizing different Large Language Models (LLMs) for annotation beyond GPT-4o-mini?
- Basis in paper: [explicit] The authors acknowledge that "while our framework relies on GPT-4o-mini for annotation, the generalizability to other LLMs remains unexplored."
- Why unresolved: The experiments relied exclusively on a single specific model (GPT-4o-mini), and LLM performance is known to be highly dependent on the specific task and data characteristics.
- What evidence would resolve it: A comparative analysis of KARL's convergence rate and final accuracy using alternative LLMs (e.g., Llama, Claude, or different GPT variants) on the same FBL datasets.

## Limitations
- LLM consensus filtering effectiveness across diverse product domains remains uncertain, with potential degradation in complex relationship classification
- Cross-domain generalization untested beyond single e-commerce domain with specific item categories
- ID accuracy degradation mechanism needs validation to distinguish between true over-exploration and optimization challenges

## Confidence

- **High confidence**: The ID vs. OOD asymmetry finding and its correlation with diversity measures. The experimental design and statistical significance are robust.
- **Medium confidence**: The LLM consensus filtering mechanism's effectiveness. While the 0.849 macro-F1 on a 3-class task is promising, the 9-class FBL schema presents substantially greater complexity.
- **Low confidence**: The claim that uncertainty sampling consistently produces beneficial diversity across all OOD scenarios. The mechanism may be domain-dependent or sensitive to the specific distribution shift characteristics.

## Next Checks

1. **Human validation of LLM annotations**: Sample 100 unanimous LLM-annotated pairs and have human experts verify correctness. Calculate precision, recall, and error patterns to quantify LLM annotation reliability.

2. **Cross-domain transfer test**: Apply KARL trained on stationery items to kitchenware and clothing domains. Measure whether the diversity-accuracy correlation holds and quantify the performance drop relative to in-domain training.

3. **Dynamic sampling strategy evaluation**: Implement an adaptive sampling method that switches between uncertainty-based and confidence-based sampling based on current ID/OOD accuracy metrics. Compare against static QBC/Margin approaches across multiple rounds.