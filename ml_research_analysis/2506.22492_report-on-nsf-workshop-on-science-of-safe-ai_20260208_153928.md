---
ver: rpa2
title: Report on NSF Workshop on Science of Safe AI
arxiv_id: '2506.22492'
source_url: https://arxiv.org/abs/2506.22492
tags:
- safety
- systems
- these
- attacks
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report synthesizes discussions from a 2025 NSF workshop on
  safe AI, uniting experts from ML theory, foundation models, formal methods, and
  human-AI interaction. It identifies the challenge of developing AI systems that
  are not only accurate but also safe and trustworthy, especially in high-stakes domains
  like autonomous robotics and healthcare.
---

# Report on NSF Workshop on Science of Safe AI

## Quick Facts
- arXiv ID: 2506.22492
- Source URL: https://arxiv.org/abs/2506.22492
- Reference count: 16
- One-line primary result: Workshop report identifying research agenda for developing AI systems that are accurate, safe, and trustworthy through multi-disciplinary approaches integrating learning, formal methods, and human-AI interaction.

## Executive Summary
This report synthesizes discussions from a 2025 NSF workshop on safe AI, uniting experts from ML theory, foundation models, formal methods, and human-AI interaction. It identifies the challenge of developing AI systems that are not only accurate but also safe and trustworthy, especially in high-stakes domains like autonomous robotics and healthcare. The report outlines a research agenda focused on defining context-dependent safety, integrating safety into learning algorithms, developing formal verification tools, and defending against attacks. Key research directions include neurosymbolic programming, benchmarks for multi-modal and agentic systems, interpretability for defenses, and human-AI collaboration. The goal is to foster cross-disciplinary collaboration to ensure robust, reliable, and ethical AI deployment.

## Method Summary
This is a workshop report outlining a research agenda rather than a reproducible experiment. The core challenge addressed is developing AI systems that are accurate, safe, and trustworthy across domains (autonomous robotics, healthcare, human-AI interaction). The report synthesizes discussions from 4 working groups: Defining Safety, Design for Safety, Safety Analysis, and Attacks/Defenses. No specific datasets, architectures, or training procedures are provided. The report identifies promising directions including neurosymbolic programming, adversarial training, formal verification, shielding/monitoring, and interpretability-based defenses, but does not implement or evaluate any specific methods.

## Key Results
- Safety must be a first-class design objective integrated during training rather than added post-hoc
- No single assurance method suffices; layered approach combining formal verification, probabilistic guarantees, and runtime monitoring is needed
- Context-dependent safety definitions are required, bridging abstract human-centric notions with formal computational specifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating safety as a first-class design objective during training—rather than as a post-hoc constraint—may produce systems with more robust safety properties.
- Mechanism: Safety objectives (e.g., constraints, loss terms, architectural choices) are embedded directly into the learning process alongside accuracy, potentially shaping the learned representation to respect safety boundaries natively.
- Core assumption: Safety properties can be formulated as differentiable or constrainable objectives that do not catastrophically conflict with accuracy.
- Evidence anchors:
  - [abstract] "...integrating safety into learning algorithms..." identified as key research direction
  - [section] Executive Summary: "The science of safe AI should make safety a first-class design objective in learning algorithms and architectures."
  - [corpus] Weak direct evidence; related workshop reports are expository, not empirical.
- Break condition: If safety and accuracy objectives prove fundamentally irreconcilable in high-dimensional spaces, or if safety constraints are too context-dependent to formalize tractably.

### Mechanism 2
- Claim: Neurosymbolic programming—combining neural networks with symbolic reasoning—may enable both the representational capacity of neural methods and the verifiability of symbolic systems.
- Mechanism: Symbolic components provide interpretable, formally analyzable structure; neural components handle perception and approximation. The interface between them allows properties to be checked on the symbolic portion even when the neural portion is opaque.
- Core assumption: Safety-critical properties can be captured in the symbolic subsystem, and the neural-to-symbolic mapping preserves enough semantic structure for verification.
- Evidence anchors:
  - [section 4.3] "Neuro-symbolic programming...seeks to combine the strengths of neural networks and symbolic reasoning, to enable explainability and verifiability."
  - [section 3.1] Mentions bridging robustness and explainability as a research direction.
  - [corpus] No empirical validation in corpus; related work is conceptual.
- Break condition: If the neural-symbolic interface introduces unverifiable complexity, or if symbolic abstractions cannot adequately represent safety properties for semantic inputs (e.g., natural language).

### Mechanism 3
- Claim: Layered assurance—combining worst-case formal verification, probabilistic guarantees, and runtime monitoring—provides complementary coverage that any single method cannot achieve alone.
- Mechanism: Formal verification handles bounded, analyzable components; probabilistic methods address uncertainty and scale; runtime monitoring catches violations during deployment and triggers fallbacks (e.g., shielding).
- Core assumption: Different safety properties require different assurance methods; no single technique scales to all components of a learning-enabled system.
- Evidence anchors:
  - [section 4.2] Explicitly categorizes worst-case guarantees, probabilistic guarantees, and runtime assurance with their trade-offs.
  - [section 4.2] "Runtime Assurance: Complementary runtime verification techniques offer formal assurance through monitoring executions..."
  - [corpus] Indirect support from CPS/RL safety frameworks (e.g., SAFE-RL), but no direct empirical validation in provided neighbors.
- Break condition: If composition of multiple assurance methods introduces integration gaps, or if runtime monitors lack ground truth for semantic properties.

## Foundational Learning

- **Temporal logic and formal specification**
  - Why needed here: Safety properties (e.g., reach-avoid, liveness) are specified using temporal logics like LTL or STL; understanding these is prerequisite to reading safety requirements and verification results.
  - Quick check question: Can you express "the system must eventually reach a safe state and never enter an unsafe region" in a formal temporal logic?

- **Adversarial machine learning fundamentals**
  - Why needed here: The attacks/defenses section assumes familiarity with threat models (white-box vs. black-box), attack categories (prompt injection, data poisoning, model extraction), and defense strategies (adversarial training, certified robustness).
  - Quick check question: Explain the difference between a data poisoning attack and a prompt injection attack on an LLM.

- **Probabilistic verification and statistical guarantees**
  - Why needed here: The report emphasizes that worst-case verification does not scale; probabilistic methods (e.g., confidence intervals, PAC-style bounds) are the practical alternative for large models.
  - Quick check question: What assumptions does a probabilistic safety guarantee make about the input distribution, and what happens under distribution shift?

## Architecture Onboarding

- **Component map**:
  - Specification layer: Formal or natural-language safety requirements (context-dependent)
  - Learning layer: Training with safety-aware objectives (constrained optimization, adversarial training)
  - Verification layer: Static analysis (formal verification), statistical testing, or falsification
  - Runtime layer: Monitors, shields, and safety guardrails enforcing properties during deployment
  - Human interface: Explainability and communication of safety status to operators/users

- **Critical path**:
  1. Define safety properties for your specific context (domain-specific, possibly iterative)
  2. Integrate those properties into training (loss terms, constraints, or architecture)
  3. Verify what is statically analyzable; add runtime monitors for what is not
  4. Design fallback behaviors for monitor-triggered violations

- **Design tradeoffs**:
  - Worst-case guarantees vs. scalability (formal verification is expensive; probabilistic is weaker)
  - Safety vs. performance (over-constraining may produce overly cautious or less capable systems)
  - Open-set robustness vs. tractable threat models (real-world perturbations are hard to bound formally)

- **Failure signatures**:
  - System passes verification but fails at runtime due to out-of-distribution inputs
  - Safety monitor triggers constantly, indicating specification-train mismatch
  - Multi-agent systems exhibit emergent unsafe behavior not present in individual agents

- **First 3 experiments**:
  1. **Specification exercise**: Take your deployment context and write 3 formal or semi-formal safety properties; identify which are verifiable and which require runtime monitoring
  2. **Safety-aware training prototype**: Add a simple safety constraint (e.g., output bound, constraint satisfaction) to a small model; measure accuracy-safety tradeoff on a validation set
  3. **Runtime monitor integration**: Implement a basic monitor that checks one safety property at inference time; log violation rates and compare against formal verification bounds (if available)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can safety requirements be defined to be both context-dependent and computationally analyzable?
- Basis in paper: [explicit] Listed as a representative problem: "How to define safety requirements that are both context dependent and computationally analyzable?" (p. 3).
- Why unresolved: Current definitions struggle to bridge the gap between grounded, explicit constraints (e.g., collision avoidance) and abstract, human-centric notions like "perceived safety" or avoiding "bad things" (p. 5).
- What evidence would resolve it: The development of novel specification languages that can formally capture abstract properties and statistical definitions alongside absolute constraints.

### Open Question 2
- Question: What are the effective benchmarks for evaluating safety in agentic systems that utilize tool-use or robotics?
- Basis in paper: [explicit] Section 5.2 asks, "What should a benchmark look like when thinking about either robots or agents and ensuring safety with respect to LLMs composing these APIs?" (p. 14).
- Why unresolved: Existing safety benchmarks often target static vision or text tasks, failing to account for the dynamic, long-horizon interactions and API compositions inherent in modern AI agents.
- What evidence would resolve it: The creation of standardized, realistic testbeds and metrics that specifically measure the safety of API calls and physical actions initiated by foundation models.

### Open Question 3
- Question: How can symbolic reasoning be effectively integrated into neural architectures to provide rigorous assurance guarantees?
- Basis in paper: [explicit] Identified as a research direction: "How to integrate symbolic reasoning in neural architectures to improve assurance guarantees?" (p. 3).
- Why unresolved: Neural networks lack transparency, while symbolic systems struggle with high-dimensional data; combining them to be both verifiable and performant remains a scalability challenge.
- What evidence would resolve it: Formal verification tools that can successfully prove safety properties on neuro-symbolic systems without encountering state-explosion issues.

## Limitations
- No empirical validation of proposed methods; report is research agenda rather than experimental study
- Specific architectures, hyperparameters, and training configurations are not provided for any approach
- Standardized safety metrics and test procedures for multi-modal/agentic systems remain undefined
- Integration challenges between multiple assurance methods (formal + probabilistic + runtime) are not addressed

## Confidence
- High Confidence: Claims about need for multi-disciplinary collaboration and importance of context-dependent safety definitions
- Medium Confidence: Claims about specific technical approaches (neurosymbolic programming, adversarial training, formal verification) based on current literature
- Low Confidence: Claims about scalability and practical effectiveness in real-world deployment scenarios

## Next Checks
1. **Specification Validation**: Take a concrete deployment scenario (e.g., medical diagnosis system) and attempt to formally specify 3-5 safety properties. Test whether these specifications can be meaningfully integrated into training objectives without rendering the system unusable.

2. **Neurosymbolic Feasibility Test**: Implement a simple neurosymbolic system for a safety-critical task (e.g., autonomous driving perception-to-planning pipeline). Measure the trade-off between symbolic verifiability and neural performance, and identify which safety properties can be captured symbolically.

3. **Layered Assurance Composition**: Design an experiment that combines formal verification of a bounded subsystem, probabilistic guarantees for a neural component, and runtime monitoring. Evaluate the system under distribution shift to identify gaps in coverage between the three assurance layers.