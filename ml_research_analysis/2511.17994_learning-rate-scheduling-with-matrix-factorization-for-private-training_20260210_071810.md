---
ver: rpa2
title: Learning Rate Scheduling with Matrix Factorization for Private Training
arxiv_id: '2511.17994'
source_url: https://arxiv.org/abs/2511.17994
tags:
- learning
- rate
- logn
- meanse
- maxse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses differentially private model training with
  stochastic gradient descent (SGD) under learning rate scheduling and correlated
  noise via matrix factorization. Prior work focused on constant learning rates, but
  learning rate schedules are widely used in practice to accelerate training and improve
  convergence.
---

# Learning Rate Scheduling with Matrix Factorization for Private Training

## Quick Facts
- **arXiv ID:** 2511.17994
- **Source URL:** https://arxiv.org/abs/2511.17994
- **Reference count:** 40
- **Primary result:** General bounds for learning rate scheduling in DP-SGD, learning-rate-aware factorization outperforms prefix-sum methods

## Executive Summary
This paper bridges the gap between practical learning rate scheduling in deep learning and theoretical differential privacy analysis. Prior work focused on constant learning rates with prefix-sum factorizations, but real-world training uses schedules to accelerate convergence. The authors derive upper and lower bounds for a broad class of learning rate schedules and propose a learning-rate-aware factorization that achieves better error bounds than existing methods. Their theoretical framework is validated experimentally on CIFAR-10 and IMDB datasets, showing improved accuracy in private training.

## Method Summary
The method extends DP-SGD with matrix factorization by incorporating learning rate schedules into the workload matrix structure. The workload matrix A_χ = A₁D captures the actual SGD update with varying learning rates, where D is a diagonal matrix of the schedule. The factorization A_χ = BC induces correlated noise, where C determines the noise correlation structure and B is the post-processing matrix. Three factorization approaches are considered: prefix-sum (B_χ, A₁^{1/2}), Toeplitz for exponential decay (B_α, C_α), and BISR for multi-epoch settings. The approach uses Algorithm 1 with clipping norm ζ=1, gradient noise scaled by sensitivity ||C||_{1→2}, and optimized for the learning rate workload.

## Key Results
- Learning-rate-aware factorization achieves improvements over prefix-sum factorizations under both MaxSE and MeanSE error metrics
- A learning-rate-aware Toeplitz factorization achieves optimal MaxSE for exponential learning rate decay
- Schedule-aware optimizations improve accuracy in private training on CIFAR-10 and IMDB datasets
- The approach maintains differential privacy guarantees while reducing reconstruction error

## Why This Works (Mechanism)

### Mechanism 1
Correlated noise via matrix factorization improves accuracy over independent Gaussian noise in DP-SGD while preserving privacy guarantees. Instead of adding independent noise at each gradient step, factorize the workload matrix A = BC. The algorithm computes CG + Z (noisy gradients), then post-processes with B. The noise correlation structure is induced by C⁻¹, allowing the same privacy budget to achieve lower reconstruction error. The core assumption is that the procedure remains differentially private even when gradients adaptively depend on the current model. If ||C||_{1→2} (sensitivity) grows too large relative to the error reduction from B, the noise becomes prohibitive and accuracy degrades.

### Mechanism 2
Encoding learning rate schedules directly into the workload matrix structure enables schedule-aware factorizations that improve error bounds. The workload matrix A_χ = A₁D where D = diag(χ₁,...,χₙ) captures the actual SGD update with varying learning rates. This differs from the prefix-sum A₁ assumption of constant learning rates used in prior work. The core assumption is that the learning rate schedule satisfies smoothness conditions (∆_t ≤ c/(t(1+log t)) or Σ∆_t² = o(log n/n)) per Theorem 1. Chaotic or non-smooth learning rate schedules may violate conditions in Theorem 1, breaking the derived bounds.

### Mechanism 3
A learning-rate-aware Toeplitz factorization achieves optimal MaxSE for exponential learning rate decay. For exponential decay χ_t = α^{t-1}, construct C_α = (A_χ^{Toep})^{1/2} with explicit lower-triangular structure. This achieves MaxSE = O(log n / log(1/β)), matching the lower bound. The core assumption is β ∈ (0, 1/e) for the final learning rate ratio; exponential decay specifically. Non-exponential schedules (polynomial, linear, cosine) do not yield the same optimal structure; the explicit C_α construction is specific to exponential decay.

## Foundational Learning

- **Concept**: Differential Privacy (DP) basics: (ε, δ)-DP, sensitivity, Gaussian mechanism
  - **Why needed here**: Understanding why noise scale depends on sens(C) = ||C||_{1→2} and σ_{ε,δ}
  - **Quick check question**: Given a matrix C, can you compute its sensitivity (maximum column ℓ₂ norm)?

- **Concept**: Matrix norms: Frobenius norm ||·||_F, operator norm ||·||₂, and mixed norms ||·||_{1→2}, ||·||_{2→∞}
  - **Why needed here**: MaxSE and MeanSE error metrics are defined via these norms on B and C
  - **Quick check question**: Why does MaxSE(B,C) = ||B||_{2→∞} · ||C||_{1→2} · σ_{ε,δ} · ζ?

- **Concept**: Lower triangular workload matrices and their factorization
  - **Why needed here**: The training process maps to A_χ being lower triangular; factorization choice determines error
  - **Quick check question**: What is the difference between A₁ (prefix-sum) and A_χ (learning-rate-aware) workload?

## Architecture Onboarding

- **Component map**: Learning rate scheduler → Workload matrix A_χ → Factorization (B, C) → Noise generation (Z) → Clipped gradients + noise → Post-processing (B)

- **Critical path**:
  1. Define learning rate schedule χ and construct A_χ = A₁D
  2. Choose factorization: prefix-sum (B_χ, A₁^{1/2}), Toeplitz (B_α, C_α), or BISR for multi-epoch
  3. Compute correlation matrix C and its sensitivity ||C||_{1→2}
  4. During training: clip gradients, add noise Z scaled by sens(C) · σ_{ε,δ} · ζ, multiply by C⁻¹

- **Design tradeoffs**:
  - MaxSE vs MeanSE optimization: MaxSE bounds worst-case error per iteration; MeanSE bounds average
  - Memory vs accuracy: Dense factorizations are optimal but O(n²); banded structures (BISR, BandInvMF) reduce to O(np)
  - Schedule-aware vs generic: Schedule-aware factorizations improve error but require knowing χ in advance

- **Failure signatures**:
  - Noise scale (σ · sens(C)) too large → accuracy collapses; check if ||C||_{1→2} is unexpectedly high
  - Memory blowup → switch to banded factorizations with bandwidth p
  - Schedule mismatch → if training uses different χ than factorization was computed for, error bounds don't hold

- **First 3 experiments**:
  1. **Baseline comparison**: Run DP-SGD with independent noise vs matrix factorization on CIFAR-10 with exponential LR decay; measure validation accuracy at (9, 10⁻⁵)-DP
  2. **Factorization comparison**: Compare prefix-sum (B_χ, A₁^{1/2}) vs Toeplitz (B_α, C_α) for exponential decay with β ∈ {1/2, 1/4, 1/8}; plot MaxSE and MeanSE vs β and n
  3. **Multi-participation test**: Train with BISR (w/ and w/o LRS) for k=4 and k=8 participations on CIFAR-10; verify if schedule-aware optimization helps at higher participation counts

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed learning-rate-aware factorization be effectively adapted to warm-starting schedules where the learning rate increases over time? The current theoretical analysis and constructions are derived specifically for decay schedules (decreasing learning rates). The conclusion states, "We have primarily studied learning rate decay, but similar techniques can be applied to warm-starting... adapting our learning-rate-aware factorization to this setting may pose extra challenges." Deriving theoretical bounds and factorizations for increasing learning rate sequences and demonstrating their empirical utility would resolve this.

### Open Question 2
How can the discrepancy between minimizing theoretical error metrics (MeanSE/MaxSE) and achieving practical model accuracy be resolved? The authors note that "optimizing the factorization with respect to learning rate workload does not necessarily lead to additional gains" and identify this as an open problem. Lower error bounds do not perfectly predict utility because optimizing the workload increases per-iteration noise, which interacts non-linearly with the training process. Developing new metrics or factorization constraints that correlate more strongly with final test accuracy than RMSE would resolve this.

### Open Question 3
What are the exact leading constants for the MaxSE and MeanSE bounds in learning-rate-scheduled workloads? The paper notes, "our results do not report the leading constant in the error bounds" because the workload matrices are harder to analyze than standard prefix-sum matrices. A fine-grained analysis of these non-Toeplitz matrices is required to move beyond asymptotic Θ and Ω notations. A theoretical derivation of the precise constants for schedules like exponential decay, matching the lower bounds exactly, would resolve this.

## Limitations
- Exact CNN architecture for CIFAR-10 experiments is not fully specified, affecting reproducibility
- Learning-rate-aware Toeplitz factorization achieving optimal MaxSE is specific to exponential decay; performance guarantees for other schedules are weaker
- Theoretical bounds assume specific smoothness conditions on learning rate schedules that may not hold for all practical schedules

## Confidence

- **High**: Correlated noise mechanisms improve accuracy over independent noise in DP-SGD (well-established in prior literature)
- **High**: Learning-rate-aware workload matrices A_χ = A₁D correctly capture the effect of scheduling
- **Medium**: Theoretical bounds for schedule-aware factorizations under various conditions (Theorem 1 assumptions may be restrictive)
- **Medium**: Experimental improvements in accuracy (depends on exact architecture and hyperparameters not fully specified)

## Next Checks

1. **Theoretical bound verification**: Re-derive Theorem 1 bounds for different learning rate schedules (polynomial, linear, cosine) to confirm the stated conditions hold and error bounds are tight

2. **Factorization comparison under controlled conditions**: Implement a minimal synthetic experiment with known optimal factorization (e.g., small n, simple schedule) to verify that schedule-aware approaches achieve lower MaxSE/MeanSE than prefix-sum methods

3. **Multi-epoch sensitivity analysis**: Test BISR with and without LRS optimization on multi-epoch experiments, varying the number of participations k and bandwidth p to identify when schedule-awareness provides measurable benefits versus when it degrades performance