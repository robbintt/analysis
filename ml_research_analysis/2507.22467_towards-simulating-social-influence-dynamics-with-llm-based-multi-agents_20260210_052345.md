---
ver: rpa2
title: Towards Simulating Social Influence Dynamics with LLM-based Multi-agents
arxiv_id: '2507.22467'
source_url: https://arxiv.org/abs/2507.22467
tags:
- arxiv
- social
- agents
- group
- stance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether LLM-based multi-agent simulations
  can reproduce social influence dynamics observed in human online forums. A structured
  round-robin conversation framework was implemented, where agents with defined personas
  engage in five rounds of text-based interactions.
---

# Towards Simulating Social Influence Dynamics with LLM-based Multi-agents

## Quick Facts
- arXiv ID: 2507.22467
- Source URL: https://arxiv.org/abs/2507.22467
- Reference count: 0
- One-line primary result: LLM-based multi-agent simulations reproduce social influence dynamics with architecture-dependent conformity rates, where reasoning models show significantly lower susceptibility to peer influence than standard generative models.

## Executive Summary
This study investigates whether LLM-based multi-agent simulations can reproduce social influence dynamics observed in human online forums. Using a structured round-robin conversation framework with defined agent personas, the research examines conformity, polarization, and fragmentation across four model groups: smaller generative models, mid-sized generative models, large proprietary models, and reasoning-optimized models. Results demonstrate that model architecture significantly shapes the emergence and intensity of social influence phenomena, with reasoning-optimized models showing notably lower conformity rates (as low as 3.13%) compared to smaller models (up to 19.45%). The findings validate LLM-based simulations as a viable tool for studying social dynamics while highlighting the importance of model selection for accurate representation of influence phenomena.

## Method Summary
The study employs a structured round-robin conversation framework where six agents with defined personas engage in five rounds of text-based interactions on controversial topics. Agents are initialized with structured persona prompts including demographics, communication style, and fixed stances on a 5-point scale. The Microsoft AutoGen framework facilitates the conversation through a central manager node that broadcasts all messages to all agents. Conformity Rate (CR), Polarization Index (Pr), Polarization Change (ΔP), and Fragmentation Index (F) are computed across 25 independent simulation trials. Four model groups are compared: smaller generative models (7-8B parameters), mid-sized generative models (70-72B parameters), large proprietary models, and reasoning-optimized models. Stance extraction is performed after each round to quantify social influence dynamics.

## Key Results
- Smaller parameter models exhibited higher conformity rates (up to 19.45%) compared to reasoning-optimized models (as low as 3.13%).
- Reasoning-optimized models demonstrated significantly lower conformity and greater resistance to social influence.
- Polarization and fragmentation metrics revealed that reasoning models retained stronger opposing factions and stance diversity compared to smaller generative models.
- Model architecture and reasoning capabilities significantly shape the emergence and intensity of social influence phenomena in LLM-based multi-agent simulations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning-optimized models exhibit lower conformity to majority opinions than standard generative models.
- Mechanism: Extended chain-of-thought processing in reasoning models appears to stabilize internal belief representations, reducing the weight given to peer signals during stance updates. The model's explicit deliberation step creates a buffer against immediate social pressure.
- Core assumption: The reasoning process itself (not just model size) drives independence; the causal attribution to "reasoning modules" is inferred rather than directly tested via ablation.
- Evidence anchors:
  - [abstract] "models optimized for reasoning are more resistant to social influence"
  - [section 4] "reasoning-oriented models in Group D showed considerably lower CRs, with ChatGPT-o1-mini at just 3.13%"
  - [corpus] Related work "Do as We Do, Not as You Think: the Conformity of Large Language Models" documents conformity bias in LLMs but does not isolate reasoning as a moderator; corpus evidence for this specific mechanism is indirect.
- Break condition: If reasoning models are placed in discussions where counter-arguments are logically superior to their initial stance, conformity may actually increase toward the stronger argument—not the majority, but the rationally persuasive position.

### Mechanism 2
- Claim: Smaller parameter models converge toward majority consensus more readily than larger or reasoning-focused models.
- Mechanism: Lower-capacity models may lack the representational depth to maintain nuanced dissenting positions when exposed to repeated majority signals. Their stance embeddings are more easily shifted by context, leading to higher stance volatility.
- Core assumption: The causal direction is from capacity → susceptibility, not from susceptibility → apparent capacity differences.
- Evidence anchors:
  - [abstract] "smaller models exhibit higher conformity rates (up to 19.45%) compared to reasoning-optimized models (as low low as 3.13%)"
  - [section 4] "Groups A and B have relatively higher ΔP and lower F than Groups C and D, which suggests greater openness of Groups A and B to external influence"
  - [corpus] "LLM-Based Social Simulations Require a Boundary" warns that model behavior may not generalize across contexts; claims about capacity effects should be treated as conditional on this specific forum setup.
- Break condition: If discussion topics require domain expertise encoded primarily in larger models, smaller models may fragment rather than converge due to inconsistent reasoning, reversing the consensus pattern.

### Mechanism 3
- Claim: Structured persona prompts with fixed stances enable controlled simulation of social influence dynamics.
- Mechanism: Pre-defining agent personas (demographics, communication style, stance) creates consistent behavioral baselines. This isolates the effect of model architecture on stance evolution because the same persona is tested across different LLMs.
- Core assumption: Persona prompts produce stable behavioral priors across model architectures; persona stability is not empirically validated in this paper.
- Evidence anchors:
  - [section 3] "Each agent is defined by a structured persona prompt encompassing demographic attributes, communicative style, and a fixed stance... This design ensures that a given persona maintains the same baseline position across different LLMs"
  - [section 3] "This standardization allows us to isolate the effects of model architecture on social influence and stance evolution"
  - [corpus] "Simulating Influence Dynamics with LLM Agents" similarly uses structured agent definitions but does not provide comparative validation of persona stability across models.
- Break condition: If persona prompts contain ambiguity or if models interpret persona attributes differently, cross-model comparisons become confounded by prompt interpretation variance rather than architecture alone.

## Foundational Learning

- Concept: **Agent-Based Modeling (ABM) for Social Simulation**
  - Why needed here: The paper builds on ABM traditions where simple behavioral rules generate emergent collective phenomena. Understanding this background clarifies why LLMs are positioned as "enhanced" agents rather than a fundamentally new paradigm.
  - Quick check question: Can you explain why traditional ABM agents are considered "oversimplified" compared to LLM-based agents in the context of social simulation?

- Concept: **Group Polarization vs. Fragmentation**
  - Why needed here: These are distinct outcomes measured separately. Polarization tracks extremity shift; fragmentation tracks subgroup division. Confusing them will mislead interpretation of ΔP versus F metrics.
  - Quick check question: If all agents move from "neutral" to "strongly support," what happens to polarization index versus fragmentation index?

- Concept: **Chain-of-Thought and Reasoning Models**
  - Why needed here: The paper attributes low conformity in Group D to "reasoning modules." Understanding what differentiates reasoning models (e.g., GPT-o1, Deepseek-R1) from standard generative models is essential for interpreting the causal claim.
  - Quick check question: What architectural or training difference distinguishes a reasoning-optimized LLM from a standard generative LLM of similar parameter count?

## Architecture Onboarding

- Component map:
  Central Manager Node -> 6 Agent Instances -> Conversation Log -> Stance Extraction Module -> Metrics Engine

- Critical path:
  1. Define controversial topic prompt.
  2. Initialize 5 agents with distinct persona prompts.
  3. Execute 5 rounds of round-robin posting with broadcast.
  4. After each round, extract stance from each agent's output.
  5. Compute CR, ΔP, and F metrics across 25 independent trials.
  6. Compare metrics across model groups (A–D).

- Design tradeoffs:
  - **Fixed persona vs. adaptive persona**: Fixed personas improve cross-model comparability but sacrifice realism (humans update beliefs and communication styles).
  - **Round-robin vs. free-form posting**: Round-robin ensures equal participation but removes timing-based influence dynamics.
  - **5-point stance scale vs. continuous**: Discrete scale simplifies metric computation but may lose nuance in moderate positions.

- Failure signatures:
  - **Stance extraction errors**: If agents express qualified or ironic positions, stance parsing may misclassify, inflating or deflating CR.
  - **Persona drift**: If models ignore persona constraints in later rounds, cross-model comparisons become invalid.
  - **Topic sensitivity**: Results may not generalize to topics with different emotional salience or factual grounding.

- First 3 experiments:
  1. Replicate the baseline conformity comparison between one Group A model (e.g., Llama3.1-7B) and one Group D model (e.g., GPT-o1-mini) on a single topic; verify CR差距 matches reported range.
  2. Ablate persona prompts: run the same simulation with minimal persona definition (only stance, no demographics or style) to test whether persona richness affects conformity rates.
  3. Cross-topic validation: run the same model group comparison on a low-stakes topic (e.g., "coffee vs. tea preference") to test whether conformity patterns are topic-dependent or architecture-driven.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Are LLM-generated forum conversations linguistically and behaviorally indistinguishable from authentic human interactions?
- **Basis in paper:** [explicit] The authors identify a critical gap in the literature, stating, "it is unclear whether LLM-generated conversation is linguistically and behaviorally indistinguishable from authentic human interactions in online forums."
- **Why unresolved:** The study focuses on the internal consistency of social dynamics (conformity, polarization) across different model architectures, but it does not compare the simulation outputs against a ground-truth dataset of human conversations to validate realism.
- **What evidence would resolve it:** A comparative study using human annotators (Turing test) or statistical linguistic metrics to benchmark the simulation logs against real-world BBS archives.

### Open Question 2
- **Question:** What specific cognitive or architectural mechanisms enable reasoning-optimized models to resist social influence?
- **Basis in paper:** [inferred] The authors observe that reasoning-focused models maintain independence from peer pressure and speculate this is "possibly due to more consistent internal reasoning processes," but they do not isolate the causal mechanism.
- **Why unresolved:** The research establishes a correlation between reasoning capabilities and resistance to conformity, but it does not perform ablation studies or internal state analysis to prove why these models prioritize logic over social cues.
- **What evidence would resolve it:** Attention head analysis or ablation studies comparing how reasoning models vs. generative models weigh "peer messages" versus "internal persona" prompts during the stance update process.

### Open Question 3
- **Question:** Do the observed conformity and polarization dynamics persist under diverse network topologies beyond the fully connected, broadcast model?
- **Basis in paper:** [inferred] The methodology relies on a "central manager node" that broadcasts all messages to all agents in a round-robin fashion, which differs from the fragmented or threaded visibility structures of many real-world online forums.
- **Why unresolved:** It is unclear if the high conformity rates observed in smaller models (19.45%) are an artifact of the fully observable, synchronous environment or if they would hold true in sparser, asynchronous network structures (e.g., social networks, reply threads).
- **What evidence would resolve it:** Re-running the simulations using varied network topologies (e.g., small-world networks, scale-free networks) where agents only view a subset of the total conversation, to measure changes in the Conformity Rate and Fragmentation Index.

## Limitations
- The study does not validate whether persona stability is maintained across different model architectures, which could confound cross-model comparisons.
- The stance extraction methodology introduces potential measurement error that could affect reported conformity rates.
- The experimental setup uses a fully connected, synchronous discussion format that differs from real-world online forum structures.

## Confidence
- **High Confidence**: The experimental framework for measuring conformity, polarization, and fragmentation is methodologically sound and the metric definitions are clearly specified. The comparative patterns across model groups (A vs D) are robustly demonstrated within the controlled experimental conditions.
- **Medium Confidence**: The attribution of lower conformity in reasoning models to their "reasoning modules" is plausible but not directly validated through ablation studies. The claims about capacity effects on social influence are conditional on the specific forum setup and may not generalize to other social dynamics.
- **Low Confidence**: Cross-context generalization claims are not supported by the experimental design, which tests only one type of social interaction (forum discussions) on a limited set of controversial topics.

## Next Checks
1. Conduct ablation studies on reasoning models by disabling chain-of-thought processing to isolate the effect of reasoning capabilities on conformity resistance.
2. Test the same model groups across multiple topic domains with varying emotional salience and factual grounding to assess topic-dependence of the observed patterns.
3. Implement persona stability validation by measuring intra-model persona consistency across rounds and comparing this variance across different model architectures.