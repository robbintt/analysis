---
ver: rpa2
title: Multimodal Information Retrieval for Open World with Edit Distance Weak Supervision
arxiv_id: '2506.20070'
source_url: https://arxiv.org/abs/2506.20070
tags:
- data
- properties
- retrieval
- property
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FemmIR addresses the challenge of multimodal information retrieval
  without similarity labels by proposing a graph-based similarity metric using weak
  supervision from edit distance. The framework constructs hierarchical attributed
  relational graphs (HARGs) from pre-extracted properties and computes Content Edit
  Distance (CED) via Munkres' algorithm to serve as weak labels.
---

# Multimodal Information Retrieval for Open World with Edit Distance Weak Supervision

## Quick Facts
- arXiv ID: 2506.20070
- Source URL: https://arxiv.org/abs/2506.20070
- Authors: KMA Solaiman; Bharat Bhargava
- Reference count: 40
- Primary result: FemmIR achieves mAP of 28%-37% across cross-modal retrieval tasks using edit distance weak supervision, outperforming FGCross-Net without requiring similarity labels.

## Executive Summary
FemmIR introduces a novel approach to multimodal information retrieval that eliminates the need for similarity labels by leveraging Content Edit Distance (CED) as weak supervision. The framework constructs hierarchical attributed relational graphs (HARGs) from pre-extracted properties and uses Munkres' algorithm to compute edit distances, which serve as training labels for a neural network. This approach enables retrieval across images, text, and video without fine-tuning, achieving strong performance on the newly introduced MuQNOL dataset.

## Method Summary
FemmIR operates by first extracting high-level properties from multimodal data using modality-specific identifiers (ResNet50 for images/video, HART for text). These properties are converted into hierarchical attributed relational graphs (HARGs) with Entity-with-Property-in-Leaf (EPL) vertices. During training, Content Edit Distance is computed between graph pairs using Munkres' algorithm, providing weak labels. An end-to-end neural network (SimGNN) learns to map graph embeddings to similarity scores, enabling efficient inference without recomputing edit distances at query time.

## Key Results
- FemmIR achieves mean average precision (mAP) scores of 28%-37% across cross-modal retrieval tasks
- Outperforms FGCross-Net (0.07-0.31 mAP) without requiring similarity labels
- HART attribute recognition model achieves up to 90% F1-score for extracting human attributes from unstructured text
- Demonstrates effectiveness on MuQNOL dataset combining MARS and InciText for person re-identification tasks

## Why This Works (Mechanism)

### Mechanism 1
CED provides viable weak supervision for multimodal retrieval when similarity labels are unavailable. The edit distance quantifies the cost of transforming one object's property graph into another's using Munkres' algorithm for optimal bipartite matching. The distance is normalized and converted to similarity scores via exponential decay, then used to train a neural approximator. This works under the assumption that edit operations correlate with human-judged relevance, though the quality depends on property extraction accuracy.

### Mechanism 2
HARGs enable cross-modal comparison by normalizing disparate data into shared structural representation. Each data sample converts to a multi-level tree with root, entities, and attribute values. EPL vertices encapsulate objects with direct properties, allowing structured comparison via graph edit operations rather than raw feature alignment. This assumes high-level properties are sufficient for relevance matching, though inconsistent vocabularies across modalities can degrade alignment.

### Mechanism 3
Neural Tensor Networks approximate CED-based similarity, enabling efficient inference without recomputing edit distance at query time. SimGNN computes graph embeddings from HARGs via GCN layers and attention pooling. Neural Tensor Networks generate interaction scores between query and candidate embeddings, which are fed to an MLP to predict similarity. The model is trained with MSE loss against CED-derived labels, assuming the CED-to-similarity mapping is learnable and generalizes to unseen data.

## Foundational Learning

- **Concept: Graph Edit Distance (GED)**
  - Why needed here: CED is a domain-specific variant of GED. Understanding GED fundamentals—insertion, deletion, substitution costs—is essential to grasp how FemmIR derives weak labels.
  - Quick check question: Given two graphs with 3 and 5 nodes respectively, what is the minimum number of edit operations needed if no nodes match?

- **Concept: Bipartite Matching / Hungarian Algorithm (Munkres)**
  - Why needed here: CED computation formulates entity alignment as an assignment problem. Munkres' algorithm solves this in polynomial time O(n³), enabling scalable weak label generation.
  - Quick check question: If you have a 3×4 cost matrix, what does the Hungarian algorithm output?

- **Concept: Weak Supervision**
  - Why needed here: FemmIR's core innovation is avoiding annotation overhead. Understanding how imperfect signals can train models—and their failure modes—is critical.
  - Quick check question: Name two risks of using automatically-generated labels instead of human annotations.

## Architecture Onboarding

- **Component map:**
  1. Property Identifiers (modality-specific): ResNet50 for images/video; HART for text—extract attributes like gender, clothing color
  2. HARG Constructor: Converts extracted properties into hierarchical graphs with EPL vertices
  3. CED Calculator (training only): Munkres-based edit distance computation with configurable replacement/insertion costs
  4. Graph Embedder: GCN layers + attention pooling produces fixed-size graph embeddings
  5. Similarity Predictor: Neural Tensor Network + MLP maps embedding pairs to similarity scores
  6. Ranking Module: Sorts candidates by predicted similarity at inference

- **Critical path:**
  1. Verify property identifiers produce consistent vocabularies across modalities (e.g., "blue" vs. "navy" handling)
  2. Validate HARG construction on sample data—check entity hierarchy and EPL vertex extraction
  3. Train similarity predictor on CED labels; monitor MSE loss convergence
  4. Evaluate retrieval mAP on held-out queries; compare against exact-match baseline (EARS)

- **Design tradeoffs:**
  - Exact vs. Approximate Inference: EARS provides exact matching but requires SQL queries per property; FemmIR approximates but enables neural generalization
  - CED Cost Weights: Higher penalty for gender (3) than top-color (1) encodes domain knowledge; adjust per application
  - Property Identifier Quality: HART achieves 90% F1 on clothes; lower-quality identifiers will degrade downstream retrieval proportionally

- **Failure signatures:**
  - Low recall with high precision: Property extraction missing values—check identifier coverage
  - High MSE loss that plateaus: CED labels may be too noisy; inspect cost matrix calibration
  - Cross-modal retrieval significantly worse than intra-modal: Property vocabularies misaligned between modalities

- **First 3 experiments:**
  1. Property Identifier Ablation: Replace HART with baseline SBERT-only extraction; measure impact on text→image retrieval mAP
  2. CED Cost Sensitivity: Vary replacement costs (e.g., gender penalty 1 vs. 3 vs. 5); plot mAP against cost configuration
  3. Scalability Test: Measure inference latency with increasing corpus size (100, 1K, 10K candidates); identify bottleneck (embedding computation vs. ranking)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can FemmIR be extended to support multi-objective queries and dynamically evolving information needs in real-time applications?
- Basis in paper: The conclusion states, "In the future, we plan to extend FemmIR to include multi-objective and evolving information needs to support more real-world use cases."
- Why unresolved: The current framework focuses on static property matching based on a user-provided example. It does not yet handle trade-offs between conflicting objectives or adapt to changing user preferences over time without re-processing.
- What evidence would resolve it: An extension of the FemmIR framework that can optimize for multiple weighted properties simultaneously and a performance evaluation on a time-series dataset where user intent shifts.

### Open Question 2
- Question: Can the POSID attribute recognition algorithm be generalized to non-human entities without manual re-engineering of search strings?
- Basis in paper: Section IV-B notes, "Algorithm 2 assumes that the property identifier is intended for human-properties. POSID can be generalized... [but] The search string for fixed-valued properties has to be re-designed."
- Why unresolved: The current heuristic relies on specific syntactic patterns (e.g., verbs like 'wearing') and manually defined search strings tailored to human descriptions (gender, clothes).
- What evidence would resolve it: A modified version of POSID tested on a general entity dataset (e.g., vehicle or product descriptions) achieving comparable F1-scores to the human-attribute task without manual pattern overrides.

### Open Question 3
- Question: How robust is the FemmIR retrieval performance when upstream property identifiers suffer from high noise or low recall?
- Basis in paper: The paper notes that "FemmIR's correlation with the property identifier performance" suggests a dependency on identifier quality. Additionally, results show HART has lower recall (57%) for properties like "height," yet the impact of this specific noise on the final CED weak supervision label is not isolated.
- Why unresolved: While the paper demonstrates that better identifiers improve results, it does not quantify the degradation curve or the "breaking point" where noisy property extraction invalidates the Content Edit Distance calculation.
- What evidence would resolve it: An ablation study injecting synthetic noise (false positives/negatives) into the property extraction step to measure the subsequent drop in Mean Average Precision (mAP) for the retrieval task.

## Limitations
- Core mechanism relies on high-level properties that may miss semantic nuances captured by low-level features
- CED metric is sensitive to property extraction quality, with errors propagating directly to weak labels
- HART's 90% F1-score on clothing attributes is strong but unspecified for other properties like "race" or "height"
- MuQNOL dataset is relatively small (MARS: 20K images; InciText: unspecified size), potentially limiting generalization

## Confidence

- **High Confidence:** The architectural design of HARG + CED weak supervision is well-specified and theoretically sound. The integration of SimGNN for graph embedding is standard practice.
- **Medium Confidence:** The empirical results (mAP 28%-37%) are promising but based on a limited dataset. The superiority over FGCross-Net is clear but may not generalize to domains outside person re-identification.
- **Low Confidence:** The long-term robustness of the CED-based weak supervision approach is untested. No ablation studies quantify the impact of property identifier errors on final retrieval performance.

## Next Checks

1. **Property Identifier Robustness:** Conduct an ablation study where HART is replaced with lower-quality (e.g., SBERT-only) text attribute extraction. Measure the degradation in cross-modal mAP to quantify identifier sensitivity.

2. **Dataset Scaling Test:** Evaluate FemmIR on a 10x larger corpus (synthetic or external) to identify bottlenecks in CED computation and graph embedding latency. Compare scaling behavior against exact-match baseline (EARS).

3. **CED Cost Sensitivity Analysis:** Systematically vary replacement costs (e.g., gender penalty 1→3→5) and plot retrieval mAP. Determine if the current cost configuration (GENDER=3, TOP-COLOR=1) is optimal or dataset-specific.