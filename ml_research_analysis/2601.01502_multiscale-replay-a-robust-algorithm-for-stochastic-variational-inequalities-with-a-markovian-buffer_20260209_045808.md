---
ver: rpa2
title: 'Multiscale replay: A robust algorithm for stochastic variational inequalities
  with a Markovian buffer'
arxiv_id: '2601.01502'
source_url: https://arxiv.org/abs/2601.01502
tags:
- stochastic
- algorithm
- have
- where
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multiscale Experience Replay (MER), a novel
  algorithm for solving stochastic variational inequalities (VIs) with Markovian data
  when samples are stored in a memory buffer. Unlike standard serial algorithms or
  experience replay methods that uniformly sample from the buffer, MER employs a multi-scale
  sampling scheme across epochs, with the replay gap decreasing over time.
---

# Multiscale replay: A robust algorithm for stochastic variational inequalities with a Markovian buffer

## Quick Facts
- arXiv ID: 2601.01502
- Source URL: https://arxiv.org/abs/2601.01502
- Authors: Milind Nakul; Tianjiao Li; Ashwin Pananjady
- Reference count: 18
- Key outcome: Introduces Multiscale Experience Replay (MER) algorithm for stochastic variational inequalities with Markovian data, achieving accelerated convergence without requiring knowledge of the Markov chain's mixing time.

## Executive Summary
This paper introduces Multiscale Experience Replay (MER), a novel algorithm for solving stochastic variational inequalities (VIs) with Markovian data when samples are stored in a memory buffer. Unlike standard serial algorithms or experience replay methods that uniformly sample from the buffer, MER employs a multi-scale sampling scheme across epochs, with the replay gap decreasing over time. This design emulates the behavior of VI algorithms for i.i.d. data, overcoming bias from serial sampling and accelerating convergence.

## Method Summary
MER operates in epochs with a decreasing replay gap τ_k = B/2^k, where B is the buffer size. In each epoch, the algorithm samples from the buffer at intervals of τ_k, performs stochastic VI updates, and progressively reduces the gap to recover serial sampling behavior. This multiscale approach bridges the gap between i.i.d. and Markovian regimes without requiring knowledge of the mixing time, achieving iteration complexity bounds comparable to sample-skipping methods like CTD while tracking i.i.d. performance in early epochs.

## Key Results
- MER achieves iteration complexity bounds comparable to CTD with optimal skipping
- In early epochs with large replay gaps, MER closely tracks the error of i.i.d. sampling
- Algorithm successfully applied to policy evaluation in reinforcement learning and generalized linear models
- Robust performance across different mixing regimes without requiring mixing time knowledge

## Why This Works (Mechanism)

### Mechanism 1: Multi-scale Decorrelation
- **Claim:** MER achieves accelerated convergence without knowledge of the mixing time by varying the separation between samples across epochs.
- **Mechanism:** The algorithm operates in epochs k. In epoch k, it samples from the buffer with a "replay gap" τ_k = B/2^k. Early epochs use large gaps, effectively treating samples as independent; later epochs reduce the gap, recovering the standard serial scheme. This multiscale approach ensures that the effective mixing time τ_M falls within the range of used gaps, bridging the i.i.d. and Markovian regimes.
- **Core assumption:** The Markov chain possesses geometric mixing properties (Assumptions 4 & 5) such that sample correlation decays with time.
- **Evidence anchors:**
  - [Abstract] "...MER employs a multi-scale sampling scheme across epochs, with the replay gap decreasing over time."
  - [Section 3] "The replay gap τ_k progressively decreases the scale at which the algorithm operates."
  - [Corpus] The corpus discusses general Markovian noise challenges in RL (e.g., TD learning), validating the problem MER solves, though it lacks specific analysis of the multiscale approach.
- **Break condition:** Convergence fails if the buffer size B is too small to support the initial large replay gaps required to bridge the mixing time (Theorem 1).

### Mechanism 2: Controlled Bias Reduction
- **Claim:** Separating samples by the replay gap τ_k reduces the conditional bias of the stochastic operator, allowing standard Variational Inequality (VI) updates to proceed.
- **Mechanism:** In Markovian settings, E[hat{F}(x, ξ_t)|F_{t-1}] ≠ F(x). The paper models this bias decaying exponentially with time separation (Assumption 4). By enforcing a gap τ_k, MER reduces the bias term C_M · ρ^τ_k, making the stochastic oracle approximately unbiased for that epoch.
- **Core assumption:** The bias at the optimal solution decays geometrically with the distance between samples (Assumption 4).
- **Evidence anchors:**
  - [Section 1] "...overcoming bias in the de facto serial scheme..."
  - [Section 4.1] "Assumption 4... inequality holds... ||F(x*) - E[hat{F}(x*, ξ_t')|F_{t-1}]|| ≤ C_M · ρ_1^{t'-t}."
- **Break condition:** If the mixing is non-geometric or extremely slow (e.g., uniform mixing does not hold), the required replay gap may exceed the buffer capacity.

### Mechanism 3: Wasserstein Stability for i.i.d. Tracking
- **Claim:** In early epochs with large replay gaps, MER tracks the trajectory of the same algorithm run on i.i.d. data.
- **Mechanism:** The paper bounds the difference between the MER iterate error and the i.i.d. iterate error using the Wasserstein-1 distance. If the replay gap τ_k is large relative to the mixing time, the distance between the Markovian sample distribution and the stationary (i.i.d.) distribution shrinks exponentially, causing the algorithmic errors to converge.
- **Core assumption:** The stochastic operator is Lipschitz in both the query point and the sample space (Assumptions 2 & 7).
- **Evidence anchors:**
  - [Section 4.3] "Theorem 2 demonstrates that when the replay gap τ_k is sufficiently large... the error... closely resembles the error of standard SA on an i.i.d. sample sequence."
  - [Corpus] Corpus evidence is weak regarding the specific application of Wasserstein metrics for replay buffers, focusing instead on concentration bounds for specific RL algorithms.
- **Break condition:** If the operator is not Lipschitz in the sample space (Assumption 7), small deviations in the Markov state could cause large deviations in the operator, breaking the tracking guarantee.

## Foundational Learning

- **Concept: Variational Inequalities (VIs)**
  - **Why needed here:** VIs provide the unified mathematical framework for the problems solved (optimization, fixed-points, game theory). The paper solves finding x* such that ⟨F(x*), x-x*⟩ ≥ 0.
  - **Quick check question:** Can you explain how a strongly monotone operator F relates to the strong convexity in standard optimization?

- **Concept: Markov Chain Mixing Time**
  - **Why needed here:** This is the "effective mixing time" τ_M that the algorithm attempts to bridge. Understanding how fast the chain forgets its initial state is crucial to understanding why sample skipping works.
  - **Quick check question:** If a chain has a mixing time of 100, what would happen if you sampled every 5 steps versus every 200 steps in terms of sample correlation?

- **Concept: Stochastic Approximation (SA)**
  - **Why needed here:** The MER update (Eq 12) is a form of projected stochastic approximation. The theoretical bounds are derived by analyzing the recursion of the expected squared error.
  - **Quick check question:** What role does the step-size η_k play in balancing the deterministic error (decay) vs. stochastic error (noise accumulation)?

## Architecture Onboarding

- **Component map:** Memory Buffer -> Sampler -> Operator Oracle -> Solver -> Epoch Controller
- **Critical path:** The synchronization between the *Epoch Controller* and the *Sampler*. The algorithm must correctly calculate indices t·τ_k to ensure the correct "scale" of separation is applied for the current epoch.
- **Design tradeoffs:**
  - **Buffer Size (B):** Theoretical bounds require B ~ O(τ_M). A buffer too small cannot support the initial large replay gaps necessary for fast convergence.
  - **Static vs. Dynamic Buffer:** The paper notes a static buffer (offline) allows sample reuse, while dynamic (online) consumes new samples. The static mode is theoretically cleaner but may be less data-efficient in non-stationary environments.
- **Failure signatures:**
  - **Stagnation:** If the mixing time τ_M is vastly underestimated by the buffer size, the initial epochs will operate on highly correlated data, failing to achieve the i.i.d. acceleration.
  - **Index Overflow:** Implementation must handle boundary conditions where t · τ_k exceeds the buffer size B.
- **First 3 experiments:**
  1. **Policy Evaluation (MRP):** Implement the policy evaluation example (Section 2.2) with varying transition stickiness (m parameter) to visualize how MER bridges the gap between TD (serial) and CTD (skipping).
  2. **Generalized Linear Model (GLM):** Run the logistic regression experiment with autoregressive covariates (Section 6.2). Verify that MER achieves lower error than TD without the hyperparameter tuning required by CTD.
  3. **Buffer Ablation:** Vary the buffer size B against a chain with known mixing time to verify the threshold effect predicted by Theorem 1 (where B becomes too small to support the necessary replay gap).

## Open Questions the Paper Calls Out
None

## Limitations
- The algorithm's effectiveness critically depends on the buffer size being at least as large as the mixing time of the Markov chain. When the buffer is too small relative to the mixing time, the initial epochs cannot achieve sufficient decorrelation, preventing the algorithm from emulating i.i.d. behavior and negating the theoretical acceleration benefits.
- The geometric mixing assumption (Assumption 4) is central to the bias reduction mechanism. Real-world Markov chains with non-geometric or extremely slow mixing may not satisfy the required exponential decay of correlation, potentially causing the algorithm to fail or require impractically large replay gaps.

## Confidence
- **High Confidence**: The mechanism for controlled bias reduction through geometric mixing (Mechanism 2) is well-supported by the theoretical framework and explicit assumption statements in the paper.
- **Medium Confidence**: The Wasserstein stability argument for i.i.d. tracking (Mechanism 3) has theoretical backing but relies on Lipschitz assumptions that may not hold for all practical operators, particularly those sensitive to small state perturbations.
- **Medium Confidence**: The overall iteration complexity bounds are comparable to CTD with optimal skipping, but this comparison assumes the same problem structure and may not generalize to all VI formulations.

## Next Checks
1. **Buffer Size Threshold Test**: Systematically vary the buffer size B relative to a known mixing time τ_M to empirically verify the threshold effect where MER transitions from serial-like to accelerated behavior, validating Theorem 1's requirement that B ≳ τ_M.
2. **Non-Geometric Mixing Stress Test**: Implement MER on a Markov chain with provably non-geometric mixing (e.g., polynomial decay) to assess algorithm performance degradation and identify failure modes beyond the geometric assumption.
3. **Lipschitz Sensitivity Analysis**: Evaluate MER's i.i.d. tracking performance across operators with varying degrees of Lipschitz continuity in the sample space to quantify the robustness of the Wasserstein stability guarantee under realistic violations of Assumption 7.