---
ver: rpa2
title: 'CSP: A Simulator For Multi-Agent Ranking Competitions'
arxiv_id: '2502.11197'
source_url: https://arxiv.org/abs/2502.11197
tags:
- ranking
- competitions
- documents
- rounds
- players
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CSP, a highly configurable simulator for
  ranking competitions using large language models (LLMs) as document authors. CSP
  enables scalable simulation of ranking competitions that were previously limited
  to human participants, addressing the challenge of studying competitive search dynamics
  at scale.
---

# CSP: A Simulator For Multi-Agent Ranking Competitions

## Quick Facts
- **arXiv ID:** 2502.11197
- **Source URL:** https://arxiv.org/abs/2502.11197
- **Reference count:** 40
- **Primary result:** CSP enables scalable simulation of ranking competitions using LLMs as document authors, revealing that LLM choice affects dynamics more than ranking functions or prompts

## Executive Summary
This paper introduces CSP, a highly configurable simulator for ranking competitions using large language models (LLMs) as document authors. CSP enables scalable simulation of ranking competitions that were previously limited to human participants, addressing the challenge of studying competitive search dynamics at scale. The platform includes tools for analyzing individual competitions and comparing multiple competition configurations. Experiments with 22 datasets comparing LLM-based and human-based competitions reveal that LLM agents exhibit similar strategic behaviors but reduce content diversity more than humans. Analysis shows the choice of LLM has greater impact on competition dynamics than the ranking function or prompt type, with ranking functions showing minimal effect on outcomes. The simulator and datasets are publicly available for research.

## Method Summary
CSP is a simulator for multi-agent ranking competitions where LLM agents act as document authors modifying content over multiple rounds to achieve highest rankings for assigned queries. The simulator uses lightweight instruct-tuned LLMs (<10B parameters) with 30 commercial-intent queries from TREC09-TREC12, initial documents from Mordo et al. dataset, and 5 personas (BSc student, writer, editor, teacher, professor). The platform runs 30 games × 30 rounds with 4-5 players per game, using ranking functions (E5, Contriever, BM25) and prompts combining Instructional and Contextualized parts. The system computes five measure classes: mimicking-the-winner dynamics, diversity, convergence, quality/relevance annotations, and proportion of wins.

## Key Results
- LLM agents exhibit similar strategic behaviors to humans but reduce content diversity more significantly
- The choice of LLM has greater impact on competition dynamics than ranking function or prompt type
- Ranking functions show minimal effect on competition outcomes across diverse metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-based agents adopt a "mimicking-the-winner" strategy to improve future rankings, similar to human participants but with greater intensity.
- **Mechanism:** Agents receive only ranking signals (not the underlying ranking function) and modify documents to align with features of top-ranked documents from previous rounds. This creates convergence toward similar content features.
- **Core assumption:** Agents cannot observe the ranking function directly and must infer optimal strategies from observed rankings alone.
- **Evidence anchors:** [abstract] "LLM agents exhibit similar strategic behaviors but reduce content diversity more than humans"; [Section 5.3, Figure 1] "A general decreasing trend in the feature values was observed across rounds... This observation suggests that, similar to human participants, LLM-based agents may adopt a mimicking-the-winner strategy"; [corpus] Related work (Raifer et al. [42]) established mimicking-the-winner as equilibrium strategy in theoretical analysis.

### Mechanism 2
- **Claim:** The choice of LLM affects competition dynamics more than ranking function or prompt type.
- **Mechanism:** Different LLMs have inherent tendencies toward convergence/divergence in generated content. Gemma converged faster and produced higher inter-document similarity than Llama in controlled comparisons, suggesting model-specific generation biases dominate over external signals.
- **Core assumption:** The model architecture/training creates stable, reproducible behavioral differences in competitive settings.
- **Evidence anchors:** [abstract] "Analysis shows the choice of LLM has greater impact on competition dynamics than the ranking function or prompt type, with ranking functions showing minimal effect on outcomes"; [Section 5.4, Figure 3] "The Gemma-LLM exhibits a stronger tendency to mimic documents compared to the Llama LLM competitions... the similarity levels for Gemma were consistently higher."

### Mechanism 3
- **Claim:** Ranking function choice has minimal effect on competition dynamics across diverse metrics.
- **Mechanism:** The three tested ranking functions (E5, Contriever, BM25) produce qualitatively similar ranking signals that agents interpret similarly. The feedback loop—showing ranked lists rather than scores—abstracts away ranker-specific details.
- **Core assumption:** Agents respond to ordinal rankings, not the underlying scoring distributions that might differ across rankers.
- **Evidence anchors:** [abstract] "Ranking functions showing minimal effect on outcomes"; [Section 5.5] "Overall, the ranking function appears to have minimal to no significant impact on the competition dynamics... We showed above that the type of LLM and the prompt have a much larger effect."

## Foundational Learning

- **Concept: Competitive Search / Ranking Games**
  - **Why needed here:** The entire simulator is built on game-theoretic assumptions about authors modifying documents to maximize rankings.
  - **Quick check question:** Can you explain why the Probability Ranking Principle may be suboptimal in competitive settings (per Ben Basat et al.)?

- **Concept: Retrieval Ranking Functions (dense vs. sparse)**
  - **Why needed here:** The simulator supports configurable rankers; understanding E5, Contriever, and BM25 is essential for interpreting results.
  - **Quick check question:** What is the difference between dense retrieval (embedding cosine similarity) and sparse retrieval (BM25)?

- **Concept: Prompt Engineering for LLM Agents**
  - **Why needed here:** The paper uses two prompt structures (Pairwise vs. Listwise contextualized parts) that significantly affect agent behavior.
  - **Quick check question:** How does providing ranked lists (Listwise) vs. pairwise comparisons change the information available to an agent?

## Architecture Onboarding

- **Component map:** CSP Simulator -> CSP Analyzer -> CSP Compare
- **Critical path:**
  1. Define competition configuration (query set, ranking function, LLM + prompt, game parameters)
  2. Run simulation (30 games × 30 rounds × 4-5 players per game in paper's setup)
  3. Export dataset (documents + rankings per round)
  4. Apply Analyzer for single-competition metrics or Compare for multi-competition analysis
- **Design tradeoffs:**
  - Lightweight LLMs (<10B params) chosen for computational efficiency but may exhibit different behaviors than larger models
  - Personas introduced to create behavioral diversity among agents; effectiveness depends on persona quality
  - Token truncation (256 tokens) aligns with prior human experiments but may cut off LLM-generated content
- **Failure signatures:**
  - Rapid convergence to identical documents (unique count → 1) indicates over-aggressive mimicking
  - Low inter-annotator agreement on quality/relevance (κ < 0.5) suggests documents are ambiguous or low-quality
  - Dominant player winning most rounds (high "proportion of wins") may indicate configuration imbalance
- **First 3 experiments:**
  1. Baseline replication: Run Llama-E5-Listwise with 5 players, 30 rounds, 10 queries. Verify mimicking-the-winner pattern via similarity metrics.
  2. LLM comparison: Compare Llama vs. Gemma with identical prompts/ranker; measure convergence rate and final diversity.
  3. Ranker robustness test: Swap E5 → BM25 → Contriever with fixed LLM; confirm minimal metric differences as claimed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do competition dynamics change when heterogeneous LLMs compete against one another, or when humans compete directly against LLM agents?
- **Basis in paper:** [explicit] The authors explicitly state: "For future work, we plan to use CSP so as to study competitions that include different types of LLMs competing against each other, as well as humans competing against LLM agents."
- **Why unresolved:** The current study only analyzed homogeneous competitions (where all agents use the same LLM) or compared entirely separate human vs. LLM datasets.
- **What evidence would resolve it:** Running new simulations using the CSP platform with mixed agent pools (e.g., a game containing both GPT-4 and Llama agents, or humans and LLMs).

### Open Question 2
- **Question:** Do larger, frontier-scale LLMs exhibit the same rapid convergence and herding behaviors observed in the lightweight (<10B parameter) models tested?
- **Basis in paper:** [inferred] The methodology restricted agents to "lightweight instruct-tuned (<10B parameters) language models," leaving the behavior of more capable models uncertain.
- **Why unresolved:** The tendency to reduce content diversity may be an artifact of smaller model capabilities or training data, rather than a general property of LLM agents.
- **What evidence would resolve it:** Comparative experiments utilizing significantly larger models (e.g., 70B+ parameters or proprietary frontier models) analyzing diversity metrics over rounds.

### Open Question 3
- **Question:** Does the choice of ranking function remain insignificant when utilizing supervised or LLM-based ranking methods rather than the unsupervised methods tested?
- **Basis in paper:** [inferred] The authors concluded that "ranking function has less effect on the dynamics" but limited the experiment to three unsupervised methods (E5, Contriever, BM25).
- **Why unresolved:** Supervised rankers or LLM-based rankers might provide stronger or different feedback signals that agents could exploit, potentially altering the "mimicking-the-winner" strategy effectiveness.
- **What evidence would resolve it:** Integrating supervised cross-encoders or listwise LLM rankers into the CSP Simulator and measuring the resulting changes in competition dynamics.

## Limitations
- The simulation results depend heavily on the quality of LLM personas and anti-copying mechanisms, which are not fully specified
- The choice of lightweight LLMs (<10B parameters) may limit generalizability to larger models or human behavior
- The truncation to 256 tokens could impact the realism of document modifications, particularly for longer documents

## Confidence
- **High Confidence:** The simulator architecture and implementation are well-documented and reproducible. The finding that LLM choice affects dynamics more than ranking functions is supported by controlled experiments.
- **Medium Confidence:** The mimicking-the-winner mechanism is theoretically grounded but relies on LLM-specific behavioral assumptions that need further validation across model families.
- **Medium Confidence:** The claim about reduced diversity in LLM vs. human competitions is supported by experiments but depends on the quality of the initial human datasets for comparison.

## Next Checks
1. **Cross-model validation:** Run CSP with larger LLMs (e.g., GPT-4 class) and compare convergence/diversity patterns to validate if lightweight model behavior generalizes.
2. **Ranking function sensitivity:** Test with dramatically different ranking functions (e.g., diversity-promoting rankers, learning-to-rank with feature interactions) to verify the claim of minimal ranker impact.
3. **Human comparison replication:** Replicate the human-LLM comparison using the same initial datasets and annotation protocols to validate the diversity reduction claims.