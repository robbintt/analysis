---
ver: rpa2
title: Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational
  Recommendations
arxiv_id: '2508.18132'
source_url: https://arxiv.org/abs/2508.18132
tags:
- retrieval
- multimodal
- product
- conversational
- test-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving generative retrieval
  performance in multimodal conversational recommendation systems, where traditional
  single-turn approaches struggle to capture evolving user intent across dialogue
  turns. The authors propose integrating test-time scaling (TTS) into generative retrieval
  through a novel test-time reranking (TTR) mechanism that dynamically adjusts retrieval
  scores during inference.
---

# Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations

## Quick Facts
- arXiv ID: 2508.18132
- Source URL: https://arxiv.org/abs/2508.18132
- Reference count: 40
- Primary result: Test-time reranking improves generative retrieval MRR by 14.5 points and nDCG@1 by 10.6 points in multimodal conversational recommendation

## Executive Summary
This paper addresses the challenge of improving generative retrieval performance in multimodal conversational recommendation systems, where traditional single-turn approaches struggle to capture evolving user intent across dialogue turns. The authors propose integrating test-time scaling (TTS) into generative retrieval through a novel test-time reranking (TTR) mechanism that dynamically adjusts retrieval scores during inference. This approach leverages a multimodal large language model (MLLM) to infer user intent and refine semantic ID-based retrieval results. To support evaluation, the authors curate and refine three datasets: Multi-turn Fashion IQ (MFRcrt), Multimodal Dialogue (MMDflt), and MUSE. Experimental results show consistent improvements across benchmarks, with average gains of 14.5 points in MRR and 10.6 points in nDCG@1 when TTR is applied. The framework demonstrates effectiveness across both unimodal and multimodal settings, and across different model architectures (decoder-only and encoder-decoder), validating the potential of test-time scaling for enhancing conversational product search in multimodal contexts.

## Method Summary
The framework operates in three stages: (1) an MLLM infers user intent from dialogue history to produce a reformulated query; (2) a generative retriever produces semantic IDs via FM-index constrained beam search; (3) test-time reranking evaluates and adjusts scores using external LLM verification. The method uses semantic ID substrings as product identifiers and processes both text and reference images across dialogue turns. Three datasets were curated for evaluation: MFRcrt (FashionIQ with captions), MMDflt (filtered MMD), and MUSE (synthetic dialogues). The approach supports both decoder-only and encoder-decoder architectures, with fine-tuning performed using LLaMA-Factory toolkit.

## Key Results
- Average gains of 14.5 points in MRR and 10.6 points in nDCG@1 when TTR is applied
- Consistent improvements across all three datasets (MFRcrt, MMDflt, MUSE)
- Effectiveness demonstrated across both unimodal and multimodal settings
- Performance improvements observed across different model architectures (decoder-only and encoder-decoder)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Test-time reranking (TTR) improves retrieval accuracy by combining generative confidence scores with external semantic alignment verification.
- Mechanism: The TTR scoring function computes: `RMTTR(cj) = σ(log(cj)) × wEval(cj |bqM)` where σ normalizes log-probability scores to [0,1] and wEval represents an LLM evaluator's confidence in how well a semantic ID aligns with the inferred user intent. This dual-signal approach catches mismatches between high-confidence but semantically misaligned candidates.
- Core assumption: An external evaluator (GPT-4o-mini) can reliably assess semantic alignment between retrieved products and evolving user intent better than the retriever's generation probability alone.
- Evidence anchors:
  - [abstract]: "average gains of 14.5 points in MRR and 10.6 points in nDCG@1 when TTR is applied"
  - [Section 3.3]: "TTR augments the original reward model by incorporating a test-time evaluator, yielding a revised scoring function"
  - [corpus]: Related work MetaEmbed (arXiv:2509.18095) also explores test-time scaling for multimodal retrieval with flexible late interaction, suggesting broader validity of the TTS paradigm for retrieval tasks.
- Break condition: If the evaluator model has poor calibration or systematically favors certain product descriptions regardless of query relevance, TTR could amplify noise rather than correct errors.

### Mechanism 2
- Claim: User intent inference from dialogue history enables single-turn-compatible retrievers to handle multi-turn conversations.
- Mechanism: An MLLM processes the dialogue history `(dM<t, qMt)` to generate a reformulated textual query `bqtext_t = LLM(dM<t, qMt)`, which is then combined with visual inputs. This compresses evolving preferences into a standalone query representation that standard generative retrievers can process.
- Core assumption: The MLLM can accurately synthesize prior context and current query into a coherent intent representation without losing critical preference signals.
- Evidence anchors:
  - [Section 3.2.1]: "an LLM is utilized to infer the user's intent at turn t, producing a revised query representation"
  - [Section 5.3.2]: "models that support multimodal queries (i.e., text accompanied by a reference image) generally outperform their text-only counterparts"
  - [corpus]: Related work on conversational query reformulation (e.g., ConvGQR, CONQRR cited in Section 2.1) supports the general approach, though these prior methods lack multimodal capability.
- Break condition: If dialogues contain contradictory signals or the MLLM hallucinates preferences not present in history, the inferred query will misguide retrieval.

### Mechanism 3
- Claim: FM-index constrained decoding enables scalable generative retrieval over large product corpora.
- Mechanism: Products are represented as semantic IDs (substrings of descriptions). The FM-index restricts beam search outputs to valid corpus substrings, ensuring generated tokens always correspond to real products while maintaining efficient O(N) lookup over N candidates.
- Core assumption: Semantic ID substrings capture sufficient product information for retrieval decisions, and the FM-index provides comprehensive coverage of valid token sequences.
- Evidence anchors:
  - [Section 3.2.2]: "Each product may be associated with multiple SIDs, each capturing different aspects of it"
  - [Section 5.2.4]: "FM-index, a data structure chosen for its superior efficiency in constrained decoding over large corpora, outperforming alternatives like MarisaTrie"
  - [corpus]: Prior generative retrieval work (Bevilacqua et al., Tay et al. cited in Section 2.2) validates the substring-based approach for large-scale retrieval.
- Break condition: If product descriptions are sparse or uninformative, semantic IDs will fail to discriminate between similar products regardless of decoding efficiency.

## Foundational Learning

- Concept: **Test-Time Scaling (TTS)**
  - Why needed here: The paper's core contribution extends TTS from reasoning tasks to retrieval. Understanding why TTS works (allocating inference compute for refinement) helps explain why TTR is structurally sound.
  - Quick check question: Can you explain why best-of-N sampling works differently for mathematical reasoning versus product retrieval?

- Concept: **Generative Retrieval via Semantic IDs**
  - Why needed here: The entire framework builds on treating retrieval as autoregressive generation of product identifiers. Without this foundation, the TTR mechanism makes little sense.
  - Quick check question: How does constrained beam search differ from standard beam search, and why is it necessary for generative retrieval?

- Concept: **Multimodal Fusion in Conversational Context**
  - Why needed here: The framework processes both text and reference images across dialogue turns. Understanding late fusion vs. early fusion strategies clarifies why the architecture separates intent inference from retrieval.
  - Quick check question: If a user provides a reference image at turn 2 but only text at turn 3, how should the system handle the visual signal from turn 2?

## Architecture Onboarding

- Component map:
  - Dialogue history → Intent inference (MLLM) → Multimodal query construction → Constrained beam search → Top-K candidates → TTR scoring → Final ranking

- Critical path: Dialogue history → Intent inference (MLLM) → Multimodal query construction → Constrained beam search → Top-K candidates → TTR scoring → Final ranking

- Design tradeoffs:
  - Evaluator choice: GPT-4o-mini provides strong alignment assessment but adds latency and cost per query
  - Beam size: Larger beams improve recall but increase TTR computation quadratically (O(AB) where A=candidates, B=SIDs per product)
  - Checkpoint selection: Later checkpoints show more TTR benefit but may indicate overfitting baseline retrievers

- Failure signatures:
  - TTR degrades performance: Check evaluator calibration on held-out queries; may need evaluator fine-tuning
  - Retrieval returns invalid products: FM-index may be corrupted or semantic ID vocabulary changed
  - Late-turn accuracy drops: Intent inference may be discarding early-turn preference signals

- First 3 experiments:
  1. **Ablate TTR components**: Run retrieval with (a) retriever scores only, (b) evaluator scores only, (c) both combined to quantify each signal's contribution.
  2. **Evaluator swap test**: Replace GPT-4o-mini with smaller model (e.g., Qwen2.5-7B) to measure quality-cost tradeoff for production deployment.
  3. **Cross-dataset checkpoint analysis**: Train retriever on MMDflt, evaluate on MFRcrt at multiple checkpoints with/without TTR to test robustness claims across domain shifts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generative retrievers be trained to perform robust *internal* self-correction without relying on external verification models, thereby satisfying the standard conditions for test-time scaling?
- Basis in paper: [explicit] The Abstract states that TTS effectiveness typically relies on "the model's ability to self-correct—conditions that are rarely met in conversational product search," which motivated the use of an external TTR mechanism.
- Why unresolved: The proposed framework bypasses the lack of intrinsic self-correction by using an external LLM (GPT-4o-mini) for verification; it remains unexplored if the base retriever can be trained to self-verify its semantic IDs.

### Open Question 2
- Question: To what extent does the distribution shift between synthetic dialogues (MUSE) and real human interactions limit the generalizability of retrieval models trained on such data?
- Basis in paper: [explicit] Page 3 notes that while synthetic datasets offer promise, they "cannot fully replace real-world data," and Page 6 mentions the use of MUSE to assess generalizability across these settings.
- Why unresolved: The paper benchmarks performance on both synthetic and curated real datasets, but the specific transfer learning challenges or biases introduced by training on LLM-generated dialogues are not isolated or quantified.

### Open Question 3
- Question: Does the computational overhead of the Test-time Reranking (TTR) mechanism remain tractable when scaling to industrial-sized corpora (e.g., millions of items) given the O(AB) complexity?
- Basis in paper: [inferred] Section 3.3 provides an efficiency analysis claiming "practical viability," yet the experiments (Table 1) are restricted to small datasets (N < 100k).
- Why unresolved: While the FM-index handles large vocabularies, the iterative evaluation of multiple semantic IDs per candidate by an external LLM may introduce latency that violates the strict real-time constraints of conversational systems at scale.

## Limitations

- Test-time reranking introduces computational overhead proportional to candidate count and semantic ID diversity, potentially limiting real-time deployment
- Dependence on GPT-4o-mini for both intent inference and evaluation creates a bottleneck for scalability and raises questions about cost-effectiveness
- Semantic ID representation strategy may struggle with highly abstract or novel products that lack distinctive substrings in their descriptions

## Confidence

- **High Confidence**: The 14.5-point MRR improvement and consistent gains across architectures (decoder-only and encoder-decoder) are well-supported by the experimental results across three distinct datasets.
- **Medium Confidence**: The mechanism explaining how test-time reranking corrects semantically misaligned high-confidence retrievals is plausible but relies heavily on the evaluator's calibration quality.
- **Medium Confidence**: The claim that FM-index constrained decoding enables scalable retrieval is supported by comparison with alternatives, though corpus size limits in the experiments don't fully stress-test this scalability claim.

## Next Checks

1. **Evaluator Calibration Study**: Systematically test the GPT-4o-mini evaluator's performance across different product categories and query types to quantify its reliability and identify potential systematic biases.
2. **Latency-Cost Analysis**: Measure end-to-end inference latency and API costs for production-scale deployments, particularly focusing on the quadratic scaling of TTR with candidate and SID counts.
3. **Domain Transfer Experiment**: Train retrievers on one dataset (e.g., MMDflt) and evaluate on another (e.g., MFRcrt) with and without TTR to validate cross-domain robustness claims beyond the reported dataset-specific results.