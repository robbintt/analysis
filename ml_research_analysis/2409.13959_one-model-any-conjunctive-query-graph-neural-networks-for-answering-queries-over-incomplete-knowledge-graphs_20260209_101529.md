---
ver: rpa2
title: 'One Model, Any Conjunctive Query: Graph Neural Networks for Answering Queries
  over Incomplete Knowledge Graphs'
arxiv_id: '2409.13959'
source_url: https://arxiv.org/abs/2409.13959
tags:
- query
- anycq
- graph
- queries
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces two new query answering tasks for incomplete
  knowledge graphs: query answer classification (QAC) and query answer retrieval (QAR).
  QAC classifies whether a given tuple is an answer to a query, while QAR retrieves
  a correct answer tuple or returns None.'
---

# One Model, Any Conjunctive Query: Graph Neural Networks for Answering Queries over Incomplete Knowledge Graphs

## Quick Facts
- **arXiv ID:** 2409.13959
- **Source URL:** https://arxiv.org/abs/2409.13959
- **Reference count:** 40
- **One-line primary result:** Introduces AnyCQ, a neuro-symbolic framework using GNNs for conjunctive query answering over incomplete KGs, achieving state-of-the-art performance on large, structurally complex queries.

## Executive Summary
This paper introduces two new query answering tasks for incomplete knowledge graphs: query answer classification (QAC) and query answer retrieval (QAR). QAC classifies whether a given tuple is an answer to a query, while QAR retrieves a correct answer tuple or returns None. To solve these tasks, the authors propose AnyCQ, a neuro-symbolic framework that uses a graph neural network to search over possible variable assignments guided by a link predictor. AnyCQ is trained with reinforcement learning and can handle conjunctive queries of arbitrary structure, scaling to queries much larger than those seen during training. Experiments show that AnyCQ matches state-of-the-art performance on simple queries while significantly outperforming existing methods on large, structurally complex queries. Notably, AnyCQ generalizes well to new knowledge graphs and achieves strong performance even with imperfect link predictors.

## Method Summary
AnyCQ addresses query answering over incomplete knowledge graphs by framing it as a constraint satisfaction problem (CSP). A computational graph is constructed from the query and current variable assignments, with nodes for terms, values, and literals. A GNN processes this graph, generating distributions over entity domains for each variable. Assignments are iteratively sampled and refined through a reinforcement learning loop, using link predictor scores to guide search. The framework integrates dual edge labels (PE for global exploration, LE for local exploitation) and handles both QAC and QAR tasks through fuzzy logic scoring of candidate assignments.

## Key Results
- AnyCQ matches or exceeds state-of-the-art performance on small conjunctive queries while significantly outperforming existing methods on large, structurally complex queries.
- The framework generalizes to queries with more variables and cycles than seen during training, handling up to 20-term queries with 3 free variables.
- AnyCQ achieves strong transfer performance to new knowledge graphs without fine-tuning, demonstrating robustness across domains.
- Performance remains robust even with imperfect link predictors, though accuracy degrades gracefully with predictor quality.

## Why This Works (Mechanism)

### Mechanism 1: GNN-Guided Stochastic Search over Assignment Space
AnyCQ finds satisfying variable assignments for conjunctive queries by iteratively refining assignments through a learned search policy. A GNN processes a computational graph encoding the query structure and current assignment, outputting distributions over entity domains for each variable. New assignments are sampled from these distributions, and the process repeats. The GNN learns to direct search toward high-satisfiability regions via reinforcement learning (REINFORCE). The search space contains satisfiable assignments that can be reached through local, GNN-guided perturbations, with training on small queries transferring to larger, structurally complex queries.

### Mechanism 2: Link Predictor Integration for Missing Edge Inference
AnyCQ reasons over incomplete KGs by using a link predictor to estimate the truth of unobserved facts during search. The link predictor scores candidate triples, which are binarized to determine literal satisfiability in edge labels. The GNN uses these labels to evaluate whether proposed assignments satisfy query literals. Performance depends on the link predictor providing sufficiently accurate estimates of missing facts, with degradation occurring gracefully as predictor accuracy decreases.

### Mechanism 3: Dual Edge Label System for Exploration vs. Exploitation
PE (Potential Edge) and LE (Light Edge) labels provide complementary guidance for exploring promising assignments and exploiting local improvements. PE labels are precomputed, assignment-independent signals indicating if a value could participate in any satisfying assignment for a literal. LE labels are computed dynamically per assignment, indicating if a single-variable change would satisfy a literal. The GNN conditions on both to navigate the search space, with PE labels constraining search to plausible regions and LE labels enabling hill-climbing.

## Foundational Learning

- **Concept: Conjunctive Queries (CQ) and Query Graphs**
  - **Why needed here:** AnyCQ operates on conjunctive queries expressed as ∃y.Φ(y) where Φ is a conjunction of literals. Understanding query structure (tree-like vs. cyclic, free vs. bound variables) is essential to interpret computational graphs and debug search behavior.
  - **Quick check question:** Given Q = ∃y.(r(a,y) ∧ s(y,b)), draw the query graph and identify the existentially quantified variable. Answer: y is the quantified variable; query graph has edges r from a to y, s from y to b.

- **Concept: Constraint Satisfaction Problems (CSP)**
  - **Why needed here:** AnyCQ casts query answering as a CSP: variables are query terms, domains are KG entities, constraints are literals. Familiarity with CSP concepts (variable assignments, constraint satisfaction, backtracking) illuminates the search process.
  - **Quick check question:** For a CSP with variables {x,y}, domains {a,b,c}, and constraint x≠y, list all satisfying assignments. Answer: (x→a,y→b), (x→a,y→c), (x→b,y→a), (x→b,y→c), (x→c,y→a), (x→c,y→b).

- **Concept: Reinforcement Learning (Policy Gradient / REINFORCE)**
  - **Why needed here:** AnyCQ's GNN is trained via REINFORCE, not supervised learning. The reward is the fuzzy logic score of sampled assignments. Understanding policy gradients is necessary to modify training objectives or debug convergence.
  - **Quick check question:** In REINFORCE, why is the reward weighted by log P(action|policy)? Answer: To increase the probability of actions that yield high rewards, using the log-likelihood trick to compute gradients.

## Architecture Onboarding

- **Component map:**
  - Query Q (conjunctive formula) -> Computational Graph G_Q,α -> GNN θ -> Distribution μ over entity domains -> Sampled assignment α(t) -> Fuzzy score S_π,G(Φ(α(t))) -> Best assignment and score

- **Critical path:**
  1. **Query encoding:** Parse Q into computational graph structure. Verify term/literal node creation.
  2. **PE label precomputation:** Compute PE labels offline using link predictor. Use CWA approximation for validation.
  3. **Search initialization:** Set h(0) = pre-trained vector; sample α(0) uniformly.
  4. **Per-step execution:** Update LE labels -> GNN forward -> sample α(t) -> compute fuzzy score.
  5. **Termination:** After T steps, return max score and (for QAR) best assignment.

- **Design tradeoffs:**
  - **Link predictor choice:** ComplEx is fast but transductive; NBFNet/ULTRA are inductive but heavier. NBFNet works best in experiments.
  - **PE label computation:** Exact PE labels require expensive precomputation. CWA approximation is faster but may miss valid assignments.
  - **Search steps T:** More steps improve completeness but increase latency. Paper uses T=20 for small queries, T=200 for large.
  - **Domain restriction:** Restricting D(y) speeds up search but risks excluding correct answers. Not explored in paper.

- **Failure signatures:**
  - **Score never exceeds 0.5:** Search fails to find any satisfying assignment. Check PE labels (if all zero, predictor or CWA approximation is too restrictive) and LE label dynamics (if constant, predictor may be uninformative).
  - **Random search baseline:** F1≈0 without training. If trained model behaves like random search, REINFORCE may not have converged—check learning rate, reward scaling.
  - **Timeout on large queries:** SQL baseline times out at 60s. If AnyCQ also times out, reduce T or use domain restriction.
  - **Poor transfer to new KG:** If performance drops sharply on new KG, link predictor may be misaligned. Use inductive predictor (ULTRA) or retrain.

- **First 3 experiments:**
  1. **Reproduce small-query QAC results:** Train AnyCQ on FB15k-237 train split. Evaluate on FB15k-237-QAC '2p', '3p' splits. Expected F1: ~72-82%. Debug by comparing to random search baseline.
  2. **Ablate PE labels:** Train and evaluate AnyCQ with all PE labels set to 0. Expected: F1≈0 on large queries, confirming PE importance. If not, check LE label computation or GNN capacity.
  3. **Scale to large queries:** Evaluate trained model on FB15k-237-QAR '3-hub' split. Compare F1 by number of free variables. If F1 drops sharply with k>1, examine assignment sampling diversity and predictor performance on multi-hop reasoning.

## Open Questions the Paper Calls Out

- **Question:** Can simpler search strategies, such as greedy search or hill-climbing, serve as competitive baselines for the new classification-based query answering formulation?
- **Basis in paper:** In Section B.4, the authors note that these strategies were inapplicable in prior ranking formulations but are now feasible, stating: "Exploring these strategies could yield competitive baselines... however, we consider this beyond the scope of the current study."
- **Question:** Can domain reduction heuristics be safely applied to AnyCQ to scale it to million-scale knowledge graphs without losing the ability to retrieve correct answers?
- **Basis in paper:** Section B.8.2 discusses the necessity of reducing search domains for scalability but highlights the risk that "improper application can render correct answers unreachable," concluding that the exploration of this direction is left for future work.
- **Question:** Is it possible to extend the framework to handle arbitrary existential queries without the computational overhead of converting to Disjunctive Normal Form (DNF)?
- **Basis in paper:** Section 7 lists this as a specific limitation: "One potential limitation is considering by default the input query in disjunctive normal form, converting to which may require exponentially many operations."
- **Question:** Can AnyCQ be integrated with fully inductive link predictors (e.g., ULTRA) to create a "foundation model" capable of zero-shot query answering on arbitrary, unseen knowledge graphs?
- **Basis in paper:** Section E.5 states this as a future objective: "In future work, we look forward to exploring combinations of AnyCQ search engines trained over broad, multi-dataset data... to achieve foundation models capable of answering arbitrary queries over arbitrary... knowledge graphs."

## Limitations
- **Scalability bottleneck:** Current implementation scales linearly with graph size due to expensive PE label precomputation and search step requirements.
- **DNF conversion overhead:** Framework requires queries in disjunctive normal form, with conversion potentially requiring exponential operations for complex queries.
- **Domain restriction risks:** While necessary for scalability, improper application of domain reduction can exclude correct answers from consideration.

## Confidence

- **High:** Core mechanism of GNN-guided stochastic search and dual-label system (PE/LE) for exploration vs. exploitation, as these are clearly specified and experimentally validated.
- **Medium:** Transfer capability to larger queries and new knowledge graphs, given limited empirical evidence beyond two datasets.
- **Low:** Scalability claims in extremely large KGs due to potential computational bottlenecks in PE label generation and search step limits.

## Next Checks

1. **Ablate PE labels** on large queries to confirm their necessity for performance (expected F1≈0 if removed).
2. **Test on a third knowledge graph** (e.g., Wikidata) to validate generalization claims beyond FB15k-237 and NELL.
3. **Measure PE label computation time** and explore approximations to assess scalability in real-world settings.