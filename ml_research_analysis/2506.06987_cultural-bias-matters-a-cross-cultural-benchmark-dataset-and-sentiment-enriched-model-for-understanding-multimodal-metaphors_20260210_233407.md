---
ver: rpa2
title: 'Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched
  Model for Understanding Multimodal Metaphors'
arxiv_id: '2506.06987'
source_url: https://arxiv.org/abs/2506.06987
tags:
- metaphor
- sentiment
- chinese
- english
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses cultural bias in multimodal metaphor understanding
  by introducing MultiMM, a cross-cultural dataset of 8,461 Chinese and English text-image
  advertisement pairs, and SEMD, a sentiment-enriched model that improves metaphor
  detection and sentiment analysis across cultural contexts. MultiMM includes fine-grained
  annotations for metaphor identification, source/target domains, and sentiment classification.
---

# Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors

## Quick Facts
- **arXiv ID:** 2506.06987
- **Source URL:** https://arxiv.org/abs/2506.06987
- **Reference count:** 33
- **Primary result:** Introduces MultiMM dataset and SEMD model achieving 80.16% F1 for metaphor detection and 75.69% F1 for sentiment analysis across Chinese and English cultures

## Executive Summary
This paper addresses cultural bias in multimodal metaphor understanding by introducing MultiMM, a cross-cultural dataset of 8,461 Chinese and English text-image advertisement pairs, and SEMD, a sentiment-enriched model that improves metaphor detection and sentiment analysis across cultural contexts. MultiMM includes fine-grained annotations for metaphor identification, source/target domains, and sentiment classification. SEMD integrates sentiment embeddings with multimodal features to enhance cross-cultural metaphor comprehension. Experimental results show SEMD outperforms all baselines, achieving 80.16% F1 for metaphor detection and 75.69% F1 for sentiment analysis, particularly excelling on Chinese data where cultural bias impacts performance most strongly.

## Method Summary
The paper introduces MultiMM, a cross-cultural dataset of 8,461 text-image advertisement pairs annotated for metaphor identification, source/target domains, and sentiment classification. The SEMD model uses multilingual BERT and ViT encoders to extract text and image features, then concatenates these with sentiment embeddings before classification. The architecture employs frozen pre-trained weights to reduce overfitting, with sentiment features serving as an auxiliary signal to improve cross-cultural performance. Training uses AdamW optimizer with batch size 64 and dropout 0.3.

## Key Results
- SEMD achieves 80.16% F1 for metaphor detection and 75.69% F1 for sentiment analysis
- Multimodal approaches significantly outperform text-only and image-only baselines
- SEMD shows superior performance on Chinese data, narrowing the cultural performance gap

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sentiment enrichment improves cross-cultural metaphor detection by providing a culture-agnostic auxiliary signal.
- **Mechanism:** The model concatenates sentiment embeddings with text and image features before classification. Since emotional cognition is linked to shared neurophysiological structure (per cited literature), sentiment features act as a stabilizing anchor when metaphorical mappings vary across cultures.
- **Core assumption:** Sentiment representations learned from pre-trained encoders transfer reliably across Chinese and English cultural contexts.
- **Evidence anchors:** [abstract]: "SEMD integrates sentiment embeddings with multimodal features to enhance cross-cultural metaphor comprehension." [section 5]: "Existing studies indicate that sentimental cognition is influenced by our shared neurophysiological structure... and is thus universal across different cultures." [corpus]: Limited direct corpus validation; related work (CULEMO, arXiv:2503.10688) notes cultural dimensions in emotion understanding remain underexplored.
- **Break condition:** If sentiment annotations in source culture do not align with target culture interpretations (e.g., "fire" as passion in English vs. neutral in Chinese), auxiliary signal introduces noise rather than benefit.

### Mechanism 2
- **Claim:** Multimodal fusion outperforms unimodal baselines because visual metaphors compensate for culturally conditioned linguistic ambiguity.
- **Mechanism:** SEMD extracts 768-dim features via ViT (image) and BERT (text), then concatenates them. The fusion allows the model to resolve incongruity between modalities—key for identifying metaphorical "A is B" mappings where visual symbols (e.g., deer shape from paper) provide the source domain when text is ambiguous.
- **Core assumption:** Visual symbols carry recoverable metaphorical mappings even when cultural-specific textual cues are sparse.
- **Evidence anchors:** [abstract]: "8,461 text-image advertisement pairs... fine-grained annotations, providing a deeper understanding of multimodal metaphors beyond a single cultural domain." [section 6.3]: "Multimodal approaches significantly outperform both text-only and image-only models." [corpus]: CultureVLM (arXiv:2501.01282) reports VLMs misinterpret culturally-specific visual artifacts, suggesting fusion benefits may not generalize without targeted visual-cultural training.
- **Break condition:** If image encoders are trained on Western-centric visual data, they may fail to recognize Eastern cultural symbols (e.g., "loong," "auspicious clouds"), reducing fusion gains.

### Mechanism 3
- **Claim:** Cultural implicitness gradient creates differential detection difficulty, mitigated by sentiment-aware modeling.
- **Mechanism:** Chinese metaphors tend toward implicit, culturally-embedded symbolism; English metaphors are more direct. This yields lower baseline accuracy for Chinese (73% vs 86%). Sentiment enrichment narrows this gap by providing an additional inference pathway.
- **Core assumption:** Implicit metaphors have consistent sentiment associations across speakers of the same culture.
- **Evidence anchors:** [section 4.1]: "Chinese advertisements frequently draw on source vocabulary that reflects traditional symbolism... Animals such as loong and pandas symbolize power and national pride." [section A.2]: "Chinese metaphors tend to be more implicit, while in English advertising, metaphors are often more direct." [corpus]: No direct corpus paper addresses implicitness gradients; "Do Large Language Models Truly Understand Cross-cultural Differences?" (arXiv:2512.07075) flags LLM generalization failures across cultures without quantifying implicitness.
- **Break condition:** If implicit metaphors exhibit high within-culture variation (individuals interpret the same metaphor with different sentiments), the sentiment shortcut becomes unreliable.

## Foundational Learning

- **Concept: Conceptual Metaphor Theory (CMT)**
  - **Why needed here:** The dataset annotation model and SEMD's detection task assume metaphors map from a source domain to a target domain. Understanding this mapping is essential for interpreting source/target vocabulary annotations.
  - **Quick check question:** Given the text "No one grows ketchup like Heinz" with an image of tomatoes forming a bottle, what are the source and target domains?

- **Concept: Cross-Cultural Annotation Bias Mitigation**
  - **Why needed here:** The paper uses annotators with cross-cultural exposure to reduce bias. Without understanding this strategy, you may misinterpret inter-annotator agreement scores as pure data quality rather than culturally-informed consensus.
  - **Quick check question:** Why does the paper select Chinese annotators who have lived in Western countries for 2+ years, rather than using only monolingual annotators?

- **Concept: Multimodal Feature Fusion Strategies**
  - **Why needed here:** SEMD uses concatenation fusion, but the ablation study compares add, max, and concat. Understanding why concat outperforms alternatives informs future architecture decisions.
  - **Quick check question:** In the ablation study, why does concatenation achieve 80.88% F1 (EN) while element-wise addition achieves only 77.76% F1 with sentiment features included?

## Architecture Onboarding

- **Component map:** OCR extracts text from images → Text Encoder (BERT) and Image Encoder (ViT) extract 768-dim features → Sentiment Encoder generates 768-dim sentiment features → Concatenation fusion combines all features → Feed-forward network processes fused representation → Sigmoid outputs metaphor probability or sentiment class

- **Critical path:**
  1. OCR extracts text from advertisement images (manually corrected for errors)
  2. BERT encodes text and sentiment; ViT encodes images (pre-trained weights frozen)
  3. Concat fusion combines modalities
  4. Feed-forward network processes fused representation
  5. Sigmoid outputs metaphor probability (binary) or sentiment class (3-way)

- **Design tradeoffs:**
  - **Frozen vs. fine-tuned encoders:** Paper freezes pre-trained weights to reduce overfitting on limited data; tradeoff is reduced adaptation to advertisement-specific visual styles
  - **Concatenation vs. attention-based fusion:** Concat chosen for simplicity and strong ablation results; attention might capture cross-modal interactions more expressively but increases complexity
  - **Sentiment as auxiliary vs. joint task:** Sentiment integrated as feature rather than multi-task objective; reduces task interference but may underutilize sentiment-metaphor co-dependencies

- **Failure signatures:**
  - **Chinese sentiment misclassification:** Neutral samples containing positive words (e.g., "珍爱") incorrectly classified as positive due to lexical ambiguity
  - **Cross-cultural transfer drop:** Metaphor detection via translated text (EN↔CN) shows 2–3% F1 drop, indicating loss of cultural nuance
  - **Visual symbol blindness:** Models may fail on culture-specific imagery (e.g., "auspicious clouds") not well-represented in pre-training data

- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Train mBERT, ViT, and mBERT-Res on MultiMM training split; verify you achieve reported F1 within ±2% before attempting SEMD
  2. **Ablate sentiment features:** Run SEMD with and without sentiment embeddings (w/ vs. w/o Si) on both EN and CN test sets to confirm reported gains (Table 3)
  3. **Cross-cultural transfer test:** Train SEMD on EN data only, evaluate on CN test set (and vice versa) to quantify cultural gap; compare against bidirectional translation results (Table 4) to isolate language vs. cultural effects

## Open Questions the Paper Calls Out
- **Question:** How does multimodal metaphor understanding perform across genres beyond advertising, such as social media, news, and literature?
  - **Basis in paper:** [explicit] The authors state in Limitations that "the dataset is currently limited to advertising, as annotating multimodal metaphors across genres (e.g., social media, news, literature) poses challenges in consistency and scalability. Expanding to these domains would enable a more comprehensive analysis of multimodal metaphors in diverse contexts."
  - **Why unresolved:** Advertising metaphors may differ systematically from those in other genres in both structure and cultural expression; the current dataset does not capture this variation.
  - **What evidence would resolve it:** Construction and evaluation of multimodal metaphor datasets across multiple genres, comparing cross-genre and cross-cultural performance.

- **Question:** How do multimodal metaphors vary across languages and cultures beyond the English–Chinese comparison?
  - **Basis in paper:** [explicit] The authors note that their work "focuses on English and Chinese, representing Western and Eastern cultures, respectively. While this provides a foundational comparison, it does not capture the full diversity of global languages and cultures."
  - **Why unresolved:** The current study provides only a binary cultural comparison; metaphorical mappings and sentiment associations likely differ across other linguistic and cultural contexts.
  - **What evidence would resolve it:** Extension of MultiMM to additional languages and cultural frameworks with parallel annotations and cross-cultural transfer experiments.

- **Question:** Can improved strategies for handling lexical ambiguity and polysemy enhance sentiment recognition in culturally-specific metaphors?
  - **Basis in paper:** [inferred] Error analysis reveals that "many misclassified cases contain positive words (e.g., cherish [珍爱], civilization [文明]) but lack an overall positive sentiment, leading to errors" and that "the challenges in multimodal sentiment recognition primarily stem from the scarcity of negative sentiment data, the polysemy of neutral sentiment, and cultural influences."
  - **Why unresolved:** Current models appear to rely on surface-level sentiment cues without adequately modeling context-dependent meaning shifts common in metaphorical language.
  - **What evidence would resolve it:** Development and evaluation of context-aware sentiment models specifically designed for metaphorical expressions, with analysis of error reduction on ambiguous cases.

## Limitations
- **Sentiment universality assumption:** The paper assumes sentiment embeddings are culture-agnostic based on neurophysiological claims, but empirical validation across Chinese and English contexts is limited.
- **Cultural implicitness quantification:** While the paper claims Chinese metaphors are more implicit than English metaphors, no quantitative measure of implicitness is provided.
- **Visual-cultural bias in pre-training:** The paper acknowledges that pre-trained ViT models may be Western-centric, but does not validate whether visual metaphors specific to Chinese culture are well-represented in the training data.

## Confidence
- **High confidence:** Metaphor detection F1 scores (80.16%) and sentiment analysis F1 scores (75.69%) are directly measurable from the reported experiments. The architectural components (BERT, ViT, concatenation fusion) are explicitly specified.
- **Medium confidence:** The superiority of SEMD over baselines is well-supported by ablation studies, but the extent to which sentiment enrichment specifically drives cross-cultural improvements requires further validation, particularly given the sentiment universality assumption.
- **Low confidence:** The claim that sentiment embeddings act as a "culture-agnostic stabilizing anchor" is based on theoretical reasoning rather than direct empirical evidence.

## Next Checks
1. **Cross-cultural sentiment transfer validation:** Train SEMD with sentiment features frozen on English data, then evaluate on Chinese test set (and vice versa) to quantify sentiment embeddings' cross-cultural transferability. Compare against models trained without sentiment features to isolate sentiment's contribution to cultural generalization.

2. **Implicit metaphor sentiment consistency:** Select a subset of highly implicit Chinese metaphors and conduct human annotation studies to measure inter-annotator agreement on sentiment labels. Low agreement would indicate sentiment features may not provide reliable auxiliary signals for implicit metaphors.

3. **Visual-cultural symbol recognition audit:** Identify culture-specific visual symbols in MultiMM (e.g., "loong," "auspicious clouds") and conduct targeted experiments where these symbols are removed from training data. Measure performance drop to quantify model's reliance on culturally-specific visual features versus general multimodal patterns.