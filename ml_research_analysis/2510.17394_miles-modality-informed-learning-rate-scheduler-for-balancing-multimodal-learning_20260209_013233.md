---
ver: rpa2
title: 'MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal
  Learning'
arxiv_id: '2510.17394'
source_url: https://arxiv.org/abs/2510.17394
tags:
- learning
- multimodal
- modality
- training
- miles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MILES, a modality-informed learning rate
  scheduler for balanced multimodal learning. MILES dynamically adjusts modality-specific
  learning rates based on epoch-wise conditional utilization rates to prevent modality
  overfitting and enhance both multimodal and unimodal predictions.
---

# MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning

## Quick Facts
- arXiv ID: 2510.17394
- Source URL: https://arxiv.org/abs/2510.17394
- Reference count: 23
- Primary result: MILES outperforms seven baselines, achieving higher multimodal accuracy/F1 scores and stronger unimodal encoders with minimal performance gap (1.8-2.0%) compared to top unimodal models

## Executive Summary
MILES addresses modality overfitting in multimodal learning by dynamically adjusting modality-specific learning rates based on conditional utilization rates. The scheduler identifies when one modality dominates learning and reduces its learning rate to allow underutilized modalities to catch up, while maintaining auxiliary unimodal classification losses to prevent encoder collapse. Evaluated on four bimodal datasets using feature concatenation and summation, MILES consistently achieves better balanced multimodal performance and stronger unimodal encoders compared to state-of-the-art baselines, requiring minimal hyperparameter tuning.

## Method Summary
MILES is a learning rate scheduler that monitors epoch-wise conditional utilization rates to identify modality imbalance during joint multimodal training. When the absolute difference in utilization rates between modalities exceeds a threshold τ, the scheduler reduces the learning rate of the dominant modality by a factor μ. The method combines multimodal loss with auxiliary unimodal classification losses to maintain encoder quality. MILES wraps the optimizer, intercepting after each validation epoch to compute utilization metrics and adjust learning rates, requiring only two hyperparameters (τ and μ) while preserving standard training procedures for the multimodal head.

## Key Results
- Outperforms seven state-of-the-art baselines on four multimodal datasets (CREMA-D, S-MNIST, LUMA, MM-IMDb)
- Achieves multimodal accuracy/F1 scores 1.8-2.0% higher than top unimodal models on average
- Consistently improves non-dominant modality performance while maintaining top accuracy for dominant modalities
- Requires minimal hyperparameters (τ=0.2, μ=0.5) with strong generalization across datasets and fusion mechanisms

## Why This Works (Mechanism)

### Mechanism 1
Conditional utilization rate serves as a proxy for each modality's marginal contribution to multimodal performance. The utilization rate (u_A = (M(ŷ_AB) - M(ŷ_B)) / M(ŷ_AB)) quantifies how much the multimodal model relies on modality A by measuring the performance drop when A is removed. When u_A >> u_B, modality A dominates learning.

### Mechanism 2
Reducing the learning rate of the dominant modality slows its learning speed, allowing underutilized modalities to catch up. When δ_AB > τ, MILES scales the dominant modality's learning rate by factor μ (μ < 1). This creates a "learning speed brake" on the dominant pathway while maintaining full learning speed for underutilized modalities.

### Mechanism 3
Adding auxiliary unimodal classification losses ensures encoders remain trainable even when the fusion module overfits to one modality. The total loss L = L_AB + L_A + L_B forces each encoder to learn meaningful representations independently, preventing complete encoder collapse when the fusion pathway ignores a modality.

## Foundational Learning

- **Joint/intermediate fusion**: MILES operates on joint fusion networks where features from different modalities are combined at intermediate layers and trained end-to-end. *Quick check*: Can you explain the difference between early fusion (feature-level), late fusion (decision-level), and joint fusion (intermediate)?

- **Learning rate scheduling**: MILES is fundamentally a learning rate scheduler that modality-specifically adjusts learning rates per epoch based on utilization metrics. *Quick check*: What happens to model training if the learning rate is too high vs. too low?

- **Overfitting detection via train/val gap**: MILES uses validation metrics to compute utilization rates, avoiding contamination from training set overfitting signals. *Quick check*: Why would using training accuracy for utilization computation risk reinforcing modality imbalance?

## Architecture Onboarding

- **Component map:**
  Modality A Input → Encoder f_A → z_A ─┐
                                        ├→ Fusion Layer → Multimodal Head → ŷ_AB
  Modality B Input → Encoder f_B → z_B ─┘
                     │                  │
                     └→ Unimodal Head A → ŷ_A   └→ Unimodal Head B → ŷ_B

- **Critical path:**
  1. Run training epoch with current learning rates (α_A, α_B, α_AB)
  2. Run validation epoch; collect metrics M(ŷ_AB), M(ŷ_A), M(ŷ_B)
  3. Compute u_A, u_B (Eq. 4) and δ_AB (Eq. 5)
  4. If δ_AB > τ, identify dominant modality and scale its learning rate by μ
  5. Repeat for N epochs

- **Design tradeoffs:**
  - **τ (threshold)**: Lower τ forces stricter balance but may hurt dominant modality; higher τ allows imbalance but risks overfitting
  - **μ (reduction factor)**: Smaller μ slows dominant modality more aggressively; too small causes degradation (μ=0.01 showed 10-13% accuracy drop in ablation)
  - **Training vs. validation metrics**: Validation converges faster; training metrics work if no validation set available but may be noisier

- **Failure signatures:**
  - Non-dominant modality encoder accuracy remains low (e.g., < 50%) despite MILES → τ too high or μ too close to 1
  - Dominant modality accuracy drops significantly below unimodal baseline → μ too small (< 0.1)
  - Negative utilization rates (u_A < 0, u_B < 0) persist beyond early epochs → check fusion layer initialization or learning rate magnitude

- **First 3 experiments:**
  1. **Baseline calibration**: Train vanilla multimodal model (no MILES) and record u_A, u_B across epochs to identify which modality dominates and when overfitting begins.
  2. **Hyperparameter sweep with τ=0.2, μ=0.5**: These are recommended starting points. Run on validation set, compare multimodal accuracy and encoder balance (Δ_AB metric).
  3. **Ablation on μ**: Fix τ=0.2 and vary μ ∈ {0.1, 0.25, 0.5, 0.75} to find the sweet spot where non-dominant modality improves without sacrificing dominant modality performance.

## Open Questions the Paper Calls Out

### Open Question 1
How can MILES be effectively adapted to handle scenarios involving more than two modalities? The current algorithm computes conditional utilization rates and differences pairwise (δ_AB), and it is unclear if these pairwise balances generalize to complex interactions in trimodal or higher-dimensional settings.

### Open Question 2
How can the framework be made robust to network parameter initialization to avoid excessive penalization of the dominant modality? Depending on initialization, MILES might be excessively penalizing the learning of the dominant modality, hindering its effective learning.

### Open Question 3
Does MILES provide significant improvements when applied to architectures with advanced, learnable fusion mechanisms (e.g., cross-attention Transformers)? It is uncertain if dynamic learning rate scheduling offers additive value in architectures where the fusion layer itself is trained to filter or prioritize modalities.

## Limitations
- Optimal global learning rate range remains unspecified, making hyperparameter selection a significant practical challenge
- Specific fusion head architecture details (hidden dimensions, activation functions, normalization) are not fully specified
- Relative weighting of auxiliary unimodal losses versus multimodal loss in the total objective function is ambiguous

## Confidence

**High confidence**: The core mechanism of conditional utilization rate as a proxy for modality contribution and the dynamic learning rate adjustment based on δ_AB thresholds

**Medium confidence**: The effectiveness of auxiliary unimodal losses in preventing encoder collapse, as the evidence is indirect through encoder performance metrics

**Medium confidence**: The specific hyperparameter recommendations (τ=0.2, μ=0.5) work well across datasets, though optimal values may be dataset-dependent

## Next Checks

1. **Calibration check**: Train vanilla multimodal models without MILES and record utilization rates u_A, u_B across training epochs to verify that modality imbalance exists and can be detected by the proposed metric

2. **μ sensitivity sweep**: Fix τ=0.2 and systematically vary μ ∈ {0.1, 0.25, 0.5, 0.75} to identify the sweet spot where non-dominant modality performance improves without sacrificing dominant modality accuracy

3. **Cross-dataset generalization**: Apply MILES to a held-out multimodal dataset not used in the original study to verify that the conditional utilization metric consistently identifies imbalance patterns across different domains and fusion mechanisms