---
ver: rpa2
title: 'Seeing the Big Picture: Evaluating Multimodal LLMs'' Ability to Interpret
  and Grade Handwritten Student Work'
arxiv_id: '2510.05538'
source_url: https://arxiv.org/abs/2510.05538
tags:
- student
- work
- mathematical
- visual
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated multimodal large language models (MLLMs)
  on two handwritten mathematics tasks: arithmetic problems (288 responses) and mathematical
  illustrations (150 responses). On arithmetic, the best model achieved 95% accuracy
  (k=0.90), matching human performance, though with some error patterns humans wouldn''t
  make.'
---

# Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret and Grade Handwritten Student Work

## Quick Facts
- **arXiv ID**: 2510.05538
- **Source URL**: https://arxiv.org/abs/2510.05538
- **Reference count**: 38
- **Primary result**: MLLMs match human accuracy on simple arithmetic grading (k=0.90) but struggle with complex mathematical illustrations (k=0.20-0.38), though performance improves when given human descriptions (k=0.43-0.47)

## Executive Summary
This study evaluates multimodal large language models' (MLLMs) capability to interpret and grade handwritten student work through two distinct mathematical tasks: arithmetic problems and mathematical illustrations. The research reveals a striking performance dichotomy - MLLMs achieve human-level accuracy on routine arithmetic grading but demonstrate significant limitations when interpreting complex visual mathematical representations. The findings suggest that while MLLMs are ready for deployment in grading straightforward mathematical tasks, they currently lack the visual and pedagogical capabilities needed for more sophisticated mathematical assessment, particularly those requiring interpretation of student-drawn illustrations.

## Method Summary
The study employs two assessment tasks using student work from the Connected Mathematics Project 4 curriculum: 288 handwritten arithmetic responses involving single-operation problems, and 150 mathematical illustrations where students create visual representations of mathematical concepts. Multiple state-of-the-art MLLMs were evaluated including GPT-4V, Gemini 1.5 Pro, Claude 3.5 Sonnet, Llama 3.2 Vision, and Qwen 2.5-VL-7B-Instruct. Human performance was established through controlled grading by experts, with the mathematical illustrations task employing a rubric-based approach. The research specifically examines both direct image interpretation and performance when provided with human-generated descriptions of the illustrations.

## Key Results
- Best MLLM achieved 95% accuracy on arithmetic problems, matching human performance with Cohen's kappa of 0.90
- Direct visual interpretation of mathematical illustrations showed poor agreement (kappa 0.20-0.38) compared to human-to-human agreement of 0.45
- When provided with human descriptions of illustrations, MLLM performance improved significantly to kappa 0.43-0.47, approaching human agreement levels
- Models exhibited error patterns in arithmetic grading that humans would not make, suggesting different failure modes

## Why This Works (Mechanism)
The study demonstrates that MLLMs can effectively process and grade straightforward mathematical tasks that involve clear, unambiguous visual patterns and symbolic representations. Their strong performance on arithmetic problems stems from their ability to accurately recognize handwritten digits, mathematical symbols, and operational structures. However, their struggles with mathematical illustrations reveal fundamental limitations in visual reasoning and pedagogical understanding, as these tasks require interpreting student-generated visual representations that often contain creative, non-standard approaches to mathematical concepts.

## Foundational Learning
**Multimodal Large Language Models (MLLMs)**: AI systems that process and generate both text and visual inputs, combining language understanding with visual perception. Why needed: Enables evaluation of handwritten student work that contains both mathematical symbols and diagrams. Quick check: Can the model process an image alongside text prompts?

**Cohen's Kappa Coefficient**: A statistical measure of inter-rater reliability that accounts for agreement occurring by chance. Why needed: Provides robust comparison between model and human grading consistency. Quick check: Kappa values above 0.81 indicate almost perfect agreement.

**Mathematical Illustrations Assessment**: Evaluation of student-generated visual representations of mathematical concepts, requiring interpretation of non-standard visual approaches. Why needed: Represents complex mathematical communication that goes beyond symbolic manipulation. Quick check: Does the response show a student's conceptual understanding through visual means?

## Architecture Onboarding

**Component Map**: Student Work Images -> MLLM Vision Encoder -> Multimodal Fusion -> Text Generation -> Grading Output

**Critical Path**: The vision encoder must accurately identify handwritten elements, the fusion layer must correctly integrate visual and textual understanding, and the generation component must produce appropriate grading decisions based on mathematical rubrics.

**Design Tradeoffs**: Vision encoders optimized for general image recognition may not capture the nuances of handwritten mathematical notation, while specialized mathematical recognition systems may lack the broader visual reasoning needed for illustrations. The balance between precision in symbol recognition versus flexibility in visual interpretation represents a key architectural tension.

**Failure Signatures**: Models struggle with ambiguous visual representations, creative non-standard approaches, and tasks requiring pedagogical judgment about student thinking processes. Error patterns often involve overconfidence in incorrect interpretations or systematic biases toward conventional representations.

**3 First Experiments**:
1. Test arithmetic grading with increasing complexity (multi-step problems, word problems) to identify performance thresholds
2. Evaluate whether fine-tuning on mathematical illustration datasets improves visual interpretation capabilities
3. Compare model performance when given structured rubrics versus open-ended grading instructions

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample sizes (288 arithmetic, 150 illustrations) from single educational context limit generalizability
- Arithmetic task focuses on single-operation problems, representing narrow mathematical assessment complexity
- Mathematical illustrations involve highly variable student representations that may not represent broader mathematical communication patterns

## Confidence

**High Confidence**: MLLMs' ability to match human accuracy on simple arithmetic grading (k=0.90)

**Medium Confidence**: MLLMs' struggles with direct visual interpretation of mathematical illustrations (k=0.20-0.38)

**Medium Confidence**: Performance improvement when provided with human descriptions of illustrations (k=0.43-0.47)

## Next Checks
1. Replicate the study across multiple grade levels and mathematical domains to assess generalizability of the performance gap between routine and complex mathematical assessment tasks.

2. Conduct inter-rater reliability studies with multiple human graders to establish whether the observed model performance gaps reflect genuine limitations or human grading inconsistencies.

3. Test whether performance improvements from human descriptions can be replicated using automated image description systems, potentially enabling scalable assessment without full human intervention.