---
ver: rpa2
title: 'LLMs as Repositories of Factual Knowledge: Limitations and Solutions'
arxiv_id: '2501.12774'
source_url: https://arxiv.org/abs/2501.12774
tags:
- knowledge
- subject
- llms
- entity
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines the reliability of Large Language Models (LLMs)
  as repositories of factual knowledge, focusing on their accuracy and consistency
  in handling time-sensitive facts. A dynamic benchmarking framework, DyKnow, is introduced
  to evaluate 24 state-of-the-art LLMs on real-time factual questions.
---

# LLMs as Repositories of Factual Knowledge: Limitations and Solutions

## Quick Facts
- arXiv ID: 2501.12774
- Source URL: https://arxiv.org/abs/2501.12774
- Reference count: 40
- Primary result: LLMs produce significant portions of outdated and irrelevant responses to time-sensitive factual questions, with ENAF improving consistency through entity-aware fine-tuning

## Executive Summary
This work examines the reliability of Large Language Models as repositories of factual knowledge, focusing on their accuracy and consistency when handling time-sensitive facts. The authors introduce DyKnow, a dynamic benchmarking framework that evaluates 24 state-of-the-art LLMs on real-time factual questions against Wikidata ground truth. Results show significant portions of outdated and irrelevant responses across all models, with low consistency when prompts are perturbed. To address these issues, the paper proposes ENtity-Aware Fine-tuning (ENAF), a soft neurosymbolic approach that incorporates structured entity representations during fine-tuning, improving model consistency by helping LLMs map different lexical variations of the same entity back to a unique symbolic reference.

## Method Summary
The study employs a two-pronged approach: first, DyKnow evaluates LLM performance on time-sensitive factual knowledge by querying Wikidata at evaluation time and classifying responses as correct, outdated, or irrelevant. Second, ENAF fine-tunes models on entity-tagged data to improve consistency across subject and property perturbations. The training uses the Dolma corpus filtered to Wikipedia documents containing 28 target athletes, with entities tagged using five different strategies. Models are fine-tuned for 2 epochs and evaluated on prompt agreement metrics comparing outputs across perturbed inputs.

## Key Results
- GPT-4 (2023) produces 20% outdated/irrelevant responses to time-sensitive questions despite being a recent model
- Knowledge editing methods (ROME, MEMIT) show poor cross-model generalization, with MistralI achieving 0% efficacy
- ENAF improves subject agreement by 10-20 percentage points compared to vanilla fine-tuning across GPT-2 and OLMo models
- Standard fine-tuning degrades consistency, with GPT-2 subject agreement dropping from 28% to 12%

## Why This Works (Mechanism)

### Mechanism 1
Structured entity representations improve factual consistency by creating symbolic links between lexical variations. ENAF augments fine-tuning data with entity tags (e.g., `<CristianoRonaldo>CR7</>`) that associate different surface forms with a unique identifier. During training, the model learns to map perturbed inputs to the same internal representation, improving `p(attribute|e_j*, property)` alignment with `p(attribute|e_j, property)`. Core assumption: The model's feed-forward layers can encode symbolic-to-lexical mappings that persist across inference perturbations.

### Mechanism 2
Knowledge editing methods (ROME, MEMIT) localize factual associations to specific feed-forward layers but struggle with real-world updates due to concentrated recall patterns. ROME identifies parameters responsible for a subject-attribute pair and performs least-squares optimization to insert new key-value associations. MEMIT extends this for batch edits. Both concentrate recall in upper layers, optimizing for specificity over generalization. Core assumption: Factual knowledge is stored in localized, editable MLP weights without disrupting broader knowledge.

### Mechanism 3
Dynamic benchmarking via DyKnow exposes time-sensitive knowledge gaps that static benchmarks miss, by evaluating against ground truth at inference time. DyKnow queries Wikidata at evaluation time, classifying responses as correct (matches current value), outdated (matches historical value), or irrelevant (not in Wikidata). This avoids benchmark contamination and staleness. Core assumption: Wikidata maintains accurate, up-to-date attribute values with reliable temporal qualifiers.

## Foundational Learning

- **Concept: Prompt Agreement / Input-Bound Uncertainty**
  - Why needed here: The paper uses prompt agreement to measure consistency across subject/property perturbations. Understanding this metric is essential to interpret ENAF's claimed improvements.
  - Quick check question: Given three paraphrased prompts asking about the same fact, would you expect a perfectly consistent model to produce identical outputs? What if the entity name changes?

- **Concept: Neurosymbolic Integration**
  - Why needed here: ENAF is described as a "soft neurosymbolic approach." Engineers need to understand how symbolic tags interface with neural training without explicit graph architectures.
  - Quick check question: How does wrapping an entity in XML-like tags (`<TAG>entity</>`) differ from replacing it with a canonical ID during preprocessing?

- **Concept: Layer-Wise Factual Recall**
  - Why needed here: The recall process analysis (Figures 3-4) attributes factual retrieval to lower layers and attribute selection to upper layers, explaining why ENAF's distributed recall pattern differs from editing methods.
  - Quick check question: If early layers retrieve all subject associations and later layers select one, where would you intervene to change a specific fact without affecting others?

## Architecture Onboarding

- **Component map:** DyKnow Evaluator -> Wikidata query module -> Response classifier (correct/outdated/irrelevant) -> Agreement calculator; ENAF Pipeline -> Dolma/Wikipedia corpus -> Entity tagger (spaCy + custom ID mapper) -> Tag-augmented fine-tuning data -> Standard fine-tuning; Knowledge Editing -> ROME/MEMIT locate layers via causal tracing -> Apply least-squares update

- **Critical path:** 1. Define subject entities and properties (e.g., 28 athletes, "current team") 2. Retrieve temporal ground truth from Wikidata at evaluation time 3. Generate subject/property perturbations (GPT-4 + human validation) 4. Apply ENAF tagging strategy to fine-tuning corpus 5. Fine-tune for 2 epochs, evaluate prompt agreement

- **Design tradeoffs:** ID Tags vs. Named Entity Tags: ID tags improve subject agreement but not property agreement; NE tags help property consistency but lack subject specificity. Hybrid (ID + Selected NE) balances both. Specificity vs. Generalization: Editing methods optimize for single-fact precision; ENAF preserves multi-association flexibility at the cost of targeted accuracy. RAG vs. Editing: RAG depends on retrieval quality; editing internalizes knowledge but risks catastrophic forgetting.

- **Failure signatures:** ENAF shows 0% subject agreement when generic NE tags are applied without unique identifiers (Table III, "Named Entity Tags" for GPT-2). Knowledge editing produces special-token outputs on unsupported architectures (MistralI with ROME/MEMIT). Vanilla fine-tuning degrades consistency (GPT-2 subject agreement drops from 28% to 12%).

- **First 3 experiments:** 1. Baseline assessment: Run DyKnow evaluation on your target LLM across 130 time-sensitive facts; record correct/outdated/irrelevant split and subject/property agreement scores. 2. ENAF ablation: Fine-tune on tagged data using each of the five strategies (NE Tags, Selected NE, Normalized, ID Tags, ID+Selected NE); compare agreement deltas. 3. Editing vs. RAG comparison: Apply MEMIT and IKE to outdated facts identified in Experiment 1; compare harmonic mean of efficacy and paraphrase success against RAG with gold documents.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does LLM reliability as a factual repository degrade when queried about less frequent, "long-tail" entities compared to the high-frequency subjects prioritized in DyKnow? Basis: The authors note focusing on prominent entities represents a limitation as it excludes less frequent subject entities where performance may differ significantly.

- **Open Question 2:** Can explicit graph-based representations or deep symbolic learning capture richer entity relationships more effectively than the lightweight neurosymbolic tagging used in ENAF? Basis: The authors state future research may explore more advanced methods such as explicit graph-based representations or deep symbolic learning.

- **Open Question 3:** How can knowledge editing techniques be evolved to effectively handle the addition of new facts and the deletion of obsolete ones, beyond the current focus on updating? Basis: The paper highlights that research has been narrowly focused on the updating operation, often overlooking the importance of deletion and addition.

## Limitations

- Training hyperparameter dependence: The paper specifies 2 epochs for ENAF fine-tuning but omits critical details like learning rate, batch size, and optimizer configuration, preventing faithful reproduction.
- Tagging strategy evaluation gap: The ablation showing zero subject agreement for "Named Entity Tags" alone raises questions about the sufficiency of the "ID Tags + Selected NE Tags" approach.
- Evaluation classification ambiguity: The "Irrelevant" classification in DyKnow relies on exact string matching against Wikidata values, risking false negatives when the model produces valid synonyms.

## Confidence

- **High Confidence**: The core finding that LLMs exhibit significant outdated and irrelevant responses for time-sensitive facts (supported by DyKnow evaluation showing 20-60% non-correct responses across models).
- **Medium Confidence**: The mechanism by which ENAF improves consistency through distributed recall patterns is plausible but not fully validated.
- **Low Confidence**: The claim that ENAF outperforms knowledge editing methods in maintaining factual consistency requires stronger validation due to unfair comparison conditions.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Reproduce the ENAF fine-tuning across a grid of learning rates (1e-5 to 5e-5) and batch sizes to determine whether the reported consistency improvements are robust or dependent on specific training configurations.

2. **Untagged Inference Evaluation**: Test the ENAF model on raw natural language prompts without entity tags to verify that consistency improvements generalize beyond the tag-augmented training domain.

3. **Fair Knowledge Editing Comparison**: Re-evaluate knowledge editing methods (ROME/MEMIT) using the edited facts as ground truth rather than the original facts, then measure prompt agreement to determine whether ENAF's consistency advantage persists under comparable evaluation conditions.