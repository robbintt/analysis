---
ver: rpa2
title: Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition
arxiv_id: '2504.20938'
source_url: https://arxiv.org/abs/2504.20938
tags:
- lorsa
- heads
- attention
- head
- mhsa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Low-Rank Sparse Attention (Lorsa) is proposed as a replacement
  model for Multi-Head Self Attention (MHSA) in Transformer networks, designed to
  disentangle atomic attention units from attention superposition by decomposing MHSA
  into overcomplete, sparsely activated heads with one-dimensional OV circuits. Lorsa
  learns to predict MHSA outputs while enforcing sparsity and parameter sharing, enabling
  interpretability through top activation analysis and z-pattern decomposition.
---

# Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition

## Quick Facts
- arXiv ID: 2504.20938
- Source URL: https://arxiv.org/abs/2504.20938
- Reference count: 40
- Primary result: Lorsa achieves parity with SAEs in interpretability while discovering finer-grained attention circuits

## Executive Summary
This paper introduces Low-Rank Sparse Attention (Lorsa), a method to decompose Multi-Head Self Attention (MHSA) into overcomplete, sparsely activated heads with one-dimensional OV circuits. The approach disentangles atomic attention units from attention superposition by enforcing sparsity constraints and parameter sharing on QK circuits while allowing independent decomposition of OV circuits. Lorsa learns to predict MHSA outputs while enabling interpretability through top activation analysis and z-pattern decomposition. The method rediscovers known attention mechanisms and uncovers new interpretable behaviors, showing quantitative parity with Sparse Autoencoders in feature interpretability.

## Method Summary
Lorsa replaces MHSA with overcomplete heads (e.g., 6,000 for Pythia, 32,000 for Llama) using 1D OV circuits. Each head has independent value and output projections but shares QK weights across groups equal to the original head dimension. During forward pass, scalar activations are computed and only Top-K heads contribute to output. The model is trained via MSE loss to reconstruct original MHSA output. Key design choices include maintaining QK dimension equal to original head dimension, using binding ratios to control parameter sharing, and enforcing sparsity through top-K activation selection.

## Key Results
- Lorsa achieves Fraction of Variance Unexplained (FVU) below 15% on held-out data
- Automated interpretability shows Lorsa achieves parity with SAEs in feature interpretability
- Method rediscovers known attention mechanisms (induction heads, successor heads, attention sinks)
- Discovers new interpretable behaviors including arithmetic-specific heads and thematic broadcasting heads in Llama-3.1-8B

## Why This Works (Mechanism)

### Mechanism 1
1D OV circuits increase interpretability by restricting each head to read from and write to a single residual stream direction. Each Lorsa head uses 1-dimensional value and output vectors, producing scalar activation that is broadcast to single output direction. This decomposes d_h-dimensional OV circuit into d_h independent 1D circuits, each potentially corresponding to atomic attention unit. Assumes underlying attention units are approximately linear and can be captured by 1D read/write operations on residual stream.

### Mechanism 2
Top-K sparse activation enforces competition among overcomplete heads, extracting interpretable units from attention superposition. For each token position, only K heads with highest scalar activations contribute to output. This forces specialization—each head must "win" on specific contexts to survive training. Sparsity constraint parallels TopK-SAEs and promotes monosemanticity by reducing interference. Assumes attention superposition can be linearly decomposed and true number of atomic attention units is smaller than overcomplete dictionary size for any given context.

### Mechanism 3
QK parameter sharing across head groups maintains reconstruction quality while enabling OV circuit decomposition. Every D_QK^Lorsa heads share same QK weights while OV circuits are independently decomposed with sparsity. Binding ratio controls how many Lorsa heads share each QK circuit. Assumes QK circuits are inherently multi-dimensional and cannot be decomposed to 1D without severe performance loss; OV circuits carry interpretable "what to write" information while QK provides reusable "where to attend" computation.

## Foundational Learning

- **Concept: Multi-Head Self Attention (MHSA) decomposition into QK and OV circuits**
  - Why needed: Lorsa treats OV circuits as target for decomposition while preserving QK structure. Understanding what each circuit computes is essential to grasp why this split makes sense.
  - Quick check: Can you explain why QK circuit determines "where to attend" while OV circuit determines "what to write"?

- **Concept: Sparse dictionary learning / Sparse Autoencoders (SAEs)**
  - Why needed: Lorsa is explicitly framed as sparse dictionary learning method analogous to SAEs but applied to attention outputs. Paper assumes familiarity with overcomplete dictionaries, sparsity constraints, and monosemantic features.
  - Quick check: What does it mean for feature to be "monosemantic" versus "polysemantic," and why does sparsity promote monosemanticity?

- **Concept: Attention superposition hypothesis**
  - Why needed: Core motivation for Lorsa is that MHSA heads contain superposed attention units. Extends superposition hypothesis from neurons to attention heads.
  - Quick check: What are three lines of evidence for attention superposition listed in Section 2?

## Architecture Onboarding

- **Component map:** Residual stream X -> QK projection (shared per group) -> Attention computation -> Scalar activation z -> Top-K selection -> 1D output projection -> Sum outputs

- **Critical path:** QK weight initialization matters (random vs MHSA copy vs fixed MHSA copy); binding ratio selection must have ≥ original MHSA heads as independent QK groups; QK dimension must be ≥ original MHSA head dimension; training data scale 800M tokens adequate for convergence

- **Design tradeoffs:** More Lorsa heads → finer decomposition but more dead heads and compute; lower K → stronger sparsity but higher reconstruction error; higher binding ratio → fewer parameters but less QK independence; 1D vs higher-dimensional OV: paper chooses 1D for interpretability despite theoretical suboptimality

- **Failure signatures:** High FVU > 20% indicates check QK dimension and binding ratio; many inactive Lorsa heads >50% normal but indicates overcomplete dictionary not fully utilized; poor autointerp scores in early layers expected (descending scores in deeper layers may indicate polysemanticity patterns); QK attribution shows no clear patterns may indicate QK unbinding is incomplete

- **First 3 experiments:** 1) Sanity check: Train Lorsa on single layer with H = 10× MHSA heads, K = 0.1× H, binding ratio = D_QK^MHSA. Verify FVU < 15% on held-out data before scaling. 2) Ablation study: Replicate Figure 7 for target model—test QK dimensions [4, 8, 16, 32, 64, 128] with fixed parameter budget. Identify critical threshold below which performance collapses. 3) Interpretability validation: For top-activating Lorsa heads, manually inspect z-patterns on 10 max-activating examples. Verify they show coherent attention patterns before running automated interpretability at scale.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can performance degradation associated with low-dimensional QK circuits be overcome to enable fully independent Lorsa heads rather than relying on parameter sharing?
  - Basis: Section 6 states "If we could overcome performance degradation of low-dimensional QK circuits, it is possible to scale up Lorsa with more independent QK circuits."
  - Why unresolved: Ablation studies show reducing QK dimension below original MHSA head dimension results in severe performance degradation
  - What evidence would resolve: Modified architecture or training objective maintaining low FVU with decoupled, low-dimensional QK weights

- **Open Question 2:** Can cross-layer variant of Lorsa be developed to decompose attention superposition spanning multiple layers?
  - Basis: Section 6 notes current Lorsa limited to single layer and cannot capture dependencies where features interact across layer boundaries
  - Why unresolved: Current Lorsa formulation cannot capture interpretable units computed collectively across multiple layers
  - What evidence would resolve: Successful training and interpretability analysis of Lorsa module taking inputs from and reconstructing outputs across multiple residual stream layers

- **Open Question 3:** Why do both Lorsa and attention output SAEs exhibit high frequency of inactive features/heads, implying fundamental structural difference from residual stream?
  - Basis: Section 6 observes both contain majority of inactive feature/heads, raising questions about structure of attention output space
  - Why unresolved: This phenomenon renders much computation wasted and it's unclear if this is sparsity constraint failure or inherent property of attention output distribution
  - What evidence would resolve: Comparative density analysis showing attention outputs require fewer monosemantic directions than residual streams, or method activating these heads without performance loss

## Limitations
- QK unbinding limitation: Maintaining shared QK circuits may not fully separate attention units requiring distinct attention patterns
- Cross-layer superposition: Cannot address attention-based features computed collectively across multiple layers
- Inactive head prevalence: Majority of Lorsa heads remain inactive, suggesting overcomplete dictionary not fully utilized

## Confidence
- **Confidence: Low** - Evidence relies heavily on pattern recognition that could potentially be discovered through other decomposition methods
- **Confidence: Medium** - QK unbinding limitation explicitly acknowledged as fundamental tradeoff between reconstruction quality and interpretability completeness
- **Confidence: Medium** - Cross-layer superposition identified as key limitation limiting method to partial representations of attention-based features

## Next Checks
1. **Controlled reconstruction fidelity test**: Systematically vary QK dimension and binding ratio across multiple layers while holding all else constant. Generate quantitative FVU curves similar to Figure 7 for each layer, then correlate with interpretability scores.

2. **Independent QK ablation study**: Implement variant of Lorsa with fully independent QK circuits (no parameter sharing) and compare interpretability scores against standard bound version.

3. **Cross-layer superposition analysis**: Extend z-pattern decomposition methodology to analyze Lorsa heads across consecutive layers. Track how scalar activations and top patterns evolve through network to identify evidence of collective computation spanning multiple layers.