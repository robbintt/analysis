---
ver: rpa2
title: Studying the Soupability of Documents in State Space Models
arxiv_id: '2505.24033'
source_url: https://arxiv.org/abs/2505.24033
tags:
- documents
- state
- soup
- document
- souping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether independently encoded document
  representations from Structured State Space Models (SSMs) can be merged post hoc
  to support downstream reasoning. The authors propose a "state souping" approach
  where documents are encoded in parallel by an SSM encoder, and their per-layer hidden
  states are pooled using simple commutative operations (e.g., averaging) into a single
  representation.
---

# Studying the Soupability of Documents in State Space Models

## Quick Facts
- arXiv ID: 2505.24033
- Source URL: https://arxiv.org/abs/2505.24033
- Reference count: 40
- One-line primary result: SSM document souping achieves competitive or superior performance vs concatenation across multi-hop QA, long-document QA, and sparse retrieval tasks.

## Executive Summary
This paper investigates whether independently encoded document representations from Structured State Space Models (SSMs) can be merged post hoc to support downstream reasoning. The authors propose a "state souping" approach where documents are encoded in parallel by an SSM encoder, and their per-layer hidden states are pooled using simple commutative operations (e.g., averaging) into a single representation. This pooled state is then used to condition the decoder alongside the query, enabling modular document encoding and reuse without reprocessing the full input for each query. The authors demonstrate that Mamba2 models with souped representations achieve competitive or superior performance compared to traditional concatenation-based encoding across multi-hop QA (HotpotQA), long-document QA (RACE, QuALITY), and sparse retrieval tasks. For example, on HotpotQA, souping achieves 47.8 EM / 61.3 F1 on 5 documents versus 42.3 EM / 55.6 F1 for concatenation. On QuALITY, the souping approach achieves 50.05% accuracy versus 47.46% for concatenation. The method scales to hundreds of documents (tested up to 256) while delivering substantial inference cost savings through cached, composable document states. Crucially, this modular design is uniquely suited to SSMs, as an analogous technique fails in standard Transformer architectures.

## Method Summary
The method involves encoding each document independently using a shared SSM encoder, collecting per-layer hidden states, and pooling them using commutative operations like averaging. The pooled states are then injected into the decoder alongside the query. The approach uses full encoder-decoder fine-tuning on soup-formatted data where documents are separated by a special token. The method employs activation checkpointing to maintain constant memory complexity with respect to document count, and uses average pooling without normalization as the default strategy.

## Key Results
- On HotpotQA with 5 documents: souping achieves 47.8 EM / 61.3 F1 vs 42.3 EM / 55.6 F1 for concatenation
- On QuALITY: souping achieves 50.05% accuracy vs 47.46% for concatenation
- SSM souping enables constant memory complexity (O(1)) with document count through activation checkpointing
- Transformer KV cache souping fails catastrophically (0.53 EM vs 48.50 EM for concat), demonstrating architectural specificity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSM hidden states can be merged via commutative pooling operations because the underlying recurrence is fundamentally linear.
- Mechanism: Each document d_i passes through a shared SSM encoder, producing per-layer hidden states h_i^(l). These states are pooled (e.g., averaged) into h_soup^(l), which conditions the decoder alongside the query. Linearity of the recurrent update means linear combinations of states remain semantically meaningful.
- Core assumption: The fixed-size recurrent state compresses information in a way that preserves task-relevant content across documents, and averaging does not catastrophically interfere with that compression.
- Evidence anchors:
  - [abstract] "pooling, via simple operations like averaging, into a single context state... enables modular encoding and reuse"
  - [section 2.1] "Given their foundation in linear recurrence, we hypothesize that a linear combination of these states could form a meaningful composite representation."
  - [corpus] Weak direct corpus support; related work (PICASO, State Soup) explores representation mixing but not document-level factual souping.
- Break condition: Non-linear state interactions or attention-based architectures where state size grows with input length (e.g., Transformers) break this approach—KV cache souping fails dramatically (0.53 EM vs. 48.50 EM for concat).

### Mechanism 2
- Claim: Full encoder-decoder fine-tuning is required to unlock soupability; pretrained SSMs cannot interpret pooled states out-of-the-box.
- Mechanism: During fine-tuning, gradients propagate through multiple independent document encoders. The encoder learns to produce mergeable states; the decoder learns to condition on pooled representations. Decoder-only fine-tuning improves over pretrained but underperforms full fine-tuning.
- Core assumption: The model can learn to distribute information across documents such that averaging preserves what the decoder needs.
- Evidence anchors:
  - [abstract] "finetuned Mamba2 models with souped representations achieve competitive or superior performance"
  - [section 4.1, Table 1] Pretrained: 2.6 EM on 5 docs; Decoder-only: 38.8 EM; Full enc-dec: 47.8 EM.
  - [corpus] No direct corpus evidence on fine-tuning requirements for souping.
- Break condition: Zero-shot or decoder-only adaptation yields poor performance; full fine-tuning is necessary.

### Mechanism 3
- Claim: Averaging without normalization is the most robust pooling strategy because it implicitly bounds activation magnitude while preserving relative information.
- Mechanism: Average pooling divides by k (document count), preventing unbounded growth. Sum pooling without normalization causes magnitude explosion at scale; normalization before pooling destroys useful magnitude signals.
- Core assumption: Magnitude information across documents carries useful signal that should not be fully discarded.
- Evidence anchors:
  - [section 4.2, Table 1] Average: 47.8 EM / 61.3 F1; Sum: 44.2 → 25.0 EM as docs increase; Sum+NormAfter recovers to 34.7 but still underperforms average.
  - [section 4.2] "averaging without normalization offers the best trade-off between simplicity, stability, and generalization."
  - [corpus] No corpus evidence on pooling strategies.
- Break condition: Sum pooling without normalization degrades sharply beyond training distribution; pre-normalization consistently fails.

## Foundational Learning

- Concept: **Structured State Space Models (SSMs)**
  - Why needed here: SSMs compress sequences into fixed-size hidden states via linear recurrence, enabling constant-memory aggregation—unlike Transformers with O(L) KV caches.
  - Quick check question: Can you explain why a fixed-size state enables document-level caching and reuse while a KV cache does not?

- Concept: **Model/Representation Souping**
  - Why needed here: This work extends weight-space model souping to hidden-state souping. Understanding weight averaging helps contextualize why representation mixing might work.
  - Quick check question: How does document souping differ fundamentally from averaging model weights?

- Concept: **Multi-hop vs. Single-hop Reasoning**
  - Why needed here: The paper tests souping on HotpotQA (multi-hop) and RULER QA (single-hop). Multi-hop requires preserving relational structure across documents.
  - Quick check question: Why might souping be harder for multi-hop reasoning than single-hop retrieval?

## Architecture Onboarding

- Component map:
  Encoder -> SSM encoder (Mamba2) -> produces per-layer hidden states -> Pooling layer (avg/sum/max) -> Decoder -> SSM decoder conditions on pooled states + query

- Critical path:
  1. Fine-tune encoder-decoder jointly on soup-formatted data (documents separated by ⟨DOC SEP⟩).
  2. Use average pooling, no normalization.
  3. Train with more segments than you expect at inference to improve generalization.

- Design tradeoffs:
  - Memory vs. compute: Activation checkpointing trades compute for constant memory—essential for large k or large models.
  - Training segments vs. generalization: More segments during training → better extrapolation, but over-fragmentation can hurt single-hop sparse tasks.
  - Pooling choice: Average is stable; sum requires post-norm; max loses information; pre-norm consistently underperforms.

- Failure signatures:
  - Pretrained model souping: Near-random performance (EM < 3).
  - Sum pooling at scale: Sharp performance collapse (e.g., 44.2 → 25.0 EM from 5→10 docs).
  - Transformer KV cache souping: Catastrophic failure (0.53 EM); architecture mismatch.
  - Extrapolation beyond training: Model trained on 64 segments fails at 256 (0.10 EM) without continued fine-tuning.

- First 3 experiments:
  1. Baseline check: Run pretrained Mamba2-8B on HotpotQA with soup inference (no fine-tuning). Expect failure (< 3 EM). Confirm architecture is set up correctly.
  2. Pooling ablation: Fine-tune on 5-document HotpotQA with average vs. sum vs. max pooling. Verify average yields ~47-48 EM; sum degrades at 10 docs.
  3. Generalization test: Fine-tune on 2 segments, evaluate at 2, 4, 8, 16 segments on NIAH. Observe sharp drop-off vs. model trained on 8 segments. Confirm scaling behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can document souping transfer effectively to other SSM architectures beyond Mamba2, such as S4, S5, or hybrid SSM-attention models?
- Basis in paper: [explicit] The authors state "The properties that enable souping... should therefore carry over to other SSM architectures with comparable state representations" (Page 2), but all experiments use only Mamba2.
- Why unresolved: No empirical validation is provided for other SSM families; the claim remains theoretical.
- What evidence would resolve it: Systematic experiments on S4, S5, and hybrid architectures using the same souping methodology and benchmarks.

### Open Question 2
- Question: What are the specific failure modes that emerge when attempting document souping, and what input or task characteristics predict them?
- Basis in paper: [explicit] The authors explicitly pose: "what failure modes arise when one attempts to do so?" (Page 2) regarding souping factual knowledge across documents.
- Why unresolved: The paper focuses primarily on success cases and performance comparisons, with limited analysis of when or why souping fails beyond noting normalization issues.
- What evidence would resolve it: Controlled experiments varying document characteristics (length, overlap, contradiction) and systematic analysis of error patterns across tasks.

### Open Question 3
- Question: Can learned or attention-based pooling operators outperform simple commutative operators (average, sum, max) for document souping?
- Basis in paper: [inferred] The paper evaluates only "simple commutative operations like averaging or summation" (Page 2) and references related work on more sophisticated operators like PICASO without comparing them.
- Why unresolved: The design space of pooling operators is underexplored; non-commutative or learned operators might better preserve relational information.
- What evidence would resolve it: Comparison experiments with attention-weighted pooling, learned aggregation functions, or the PICASO operator on the same benchmarks.

### Open Question 4
- Question: What is the fundamental information capacity limit of souped representations as corpus size scales?
- Basis in paper: [inferred] Results show performance collapse from 39.75 to 0.10 EM when extrapolating from 64 to 256 documents (Table 8), but the underlying bottleneck—state dimension, training data, or architecture—is unclear.
- Why unresolved: The paper demonstrates practical scalability limits without characterizing the theoretical constraints on how much information can be preserved in fixed-size pooled states.
- What evidence would resolve it: Ablation studies varying hidden state dimensions, model sizes, and corpus sizes to identify the binding constraint on information capacity.

## Limitations
- The approach is architecturally specific to SSMs and fails for Transformers due to KV cache size scaling
- Not validated on tasks requiring fine-grained temporal reasoning or multi-turn dialogue
- Performance degrades sharply when extrapolating beyond training document counts without continued fine-tuning
- Simple pooling mechanisms may miss complex cross-document interactions that attention-based methods capture

## Confidence
**High confidence** in:
- SSMs' linear recurrence enabling fixed-size state aggregation (Mechanism 1) - strongly supported by architecture and ablation evidence
- Full encoder-decoder fine-tuning being necessary for soupability (Mechanism 2) - ablation shows pretrained and decoder-only variants fail dramatically
- Average pooling without normalization being the most robust pooling strategy (Mechanism 3) - extensive ablation across tasks and scales

**Medium confidence** in:
- Souping's superiority over concatenation for long-document QA and multi-hop reasoning - supported by empirical results but not yet validated on external, out-of-domain datasets
- Generalization to 256+ documents with continued fine-tuning - demonstrated but not stress-tested for robustness or efficiency trade-offs

**Low confidence** in:
- Transformer KV cache souping failure being solely due to state size - architecture differences may include other factors (e.g., attention patterns, normalization) not fully isolated

## Next Checks
1. **Architecture Transfer Test**: Implement and evaluate souping in a hybrid model combining SSM and Transformer layers (e.g., SSM encoder + Transformer decoder) to determine whether linear recurrence is necessary or whether other mechanisms could enable similar aggregation.

2. **Temporal Reasoning Stress Test**: Evaluate souping on procedural reasoning or multi-turn dialogue tasks where per-document state granularity and temporal ordering are critical. This would test the limits of commutative pooling for tasks requiring fine-grained state preservation.

3. **Extreme Scaling Validation**: Fine-tune and test the model on 512-1024 document scenarios to identify inflection points where performance degrades, memory constraints dominate, or pooling strategies break down. This would validate the claimed constant-memory advantage at scale.