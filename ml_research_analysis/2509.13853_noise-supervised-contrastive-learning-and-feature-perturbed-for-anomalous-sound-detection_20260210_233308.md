---
ver: rpa2
title: Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound
  Detection
arxiv_id: '2509.13853'
source_url: https://arxiv.org/abs/2509.13853
tags:
- detection
- machine
- anomalous
- learning
- sound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces one-stage supervised contrastive learning
  (OS-SCL) for unsupervised anomalous sound detection, addressing the challenge of
  frequent false alarms when normal samples from different machines are similar. The
  method combines feature perturbation in embedding space with noise-supervised contrastive
  learning and cross-entropy classification in a single training stage.
---

# Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection

## Quick Facts
- arXiv ID: 2509.13853
- Source URL: https://arxiv.org/abs/2509.13853
- Reference count: 36
- One-stage supervised contrastive learning (OS-SCL) achieves 94.64% AUC and 88.42% pAUC on DCASE 2020 Challenge Task 2 using Log-Mel features

## Executive Summary
This paper introduces one-stage supervised contrastive learning (OS-SCL) for unsupervised anomalous sound detection, addressing the challenge of frequent false alarms when normal samples from different machines are similar. The method combines feature perturbation in embedding space with noise-supervised contrastive learning and cross-entropy classification in a single training stage. A time-frequency feature (TFgram) is also proposed, extracted from raw audio. On the DCASE 2020 Challenge Task 2, the method achieved 94.64% AUC and 88.42% pAUC using Log-Mel features alone, and 95.71% AUC and 90.23% pAUC with TFgram. The approach outperforms state-of-the-art methods while requiring fewer parameters than large pre-trained models.

## Method Summary
The proposed OS-SCL method integrates feature perturbation, noise-supervised contrastive learning, and cross-entropy classification into a unified training framework. Feature perturbation is applied in the embedding space to enhance model robustness, while noise-supervised contrastive learning helps the model learn discriminative features by leveraging both normal and noisy samples. The time-frequency feature (TFgram) is extracted from raw audio to capture essential acoustic patterns. This one-stage approach simplifies the training process compared to traditional two-stage methods and achieves competitive performance with fewer parameters.

## Key Results
- Achieved 94.64% AUC and 88.42% pAUC on DCASE 2020 Challenge Task 2 using Log-Mel features alone
- With TFgram feature, improved to 95.71% AUC and 90.23% pAUC
- Outperformed state-of-the-art methods while requiring fewer parameters than large pre-trained models
- Demonstrated that machine anomaly detection does not rely on high-frequency components

## Why This Works (Mechanism)
The OS-SCL method works by combining multiple learning objectives in a single stage, allowing the model to simultaneously learn discriminative features and classify anomalies. Feature perturbation in the embedding space enhances the model's ability to generalize across different machine conditions, while noise-supervised contrastive learning leverages both normal and noisy samples to improve feature discrimination. The TFgram feature captures essential time-frequency patterns that are particularly useful for distinguishing between normal and anomalous sounds.

## Foundational Learning
- **Contrastive Learning**: Needed to learn discriminative features by pulling similar samples together and pushing dissimilar ones apart. Quick check: Verify that the model learns meaningful embeddings by visualizing them in 2D space.
- **Feature Perturbation**: Required to improve model robustness and generalization across different operating conditions. Quick check: Test model performance with and without perturbation on unseen data.
- **Time-Frequency Analysis**: Essential for capturing the temporal and spectral characteristics of sound signals. Quick check: Compare performance using different time-frequency representations.
- **One-Stage Training**: Simplifies the training process by combining multiple objectives, reducing computational overhead. Quick check: Measure training time and parameter count compared to two-stage methods.

## Architecture Onboarding
- **Component Map**: Raw audio -> TFgram/Log-Mel extraction -> Feature Perturbation -> Noise-Supervised Contrastive Learning + Cross-Entropy Classification -> Anomaly Detection
- **Critical Path**: The most important sequence is feature extraction (TFgram or Log-Mel) followed by the combined learning objectives (contrastive learning and classification).
- **Design Tradeoffs**: One-stage training simplifies the pipeline but may limit the ability to fine-tune each component separately. The choice of TFgram vs. Log-Mel affects both performance and computational cost.
- **Failure Signatures**: Poor performance on unseen machines may indicate overfitting to specific operating conditions or insufficient feature perturbation.
- **First Experiments**: 1) Compare OS-SCL with traditional two-stage methods on the same dataset. 2) Evaluate the impact of feature perturbation by training with and without it. 3) Test the model's robustness to varying noise levels.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance generalizability beyond the DCASE 2020 Challenge Task 2 dataset remains untested
- Effectiveness of TFgram compared to established features across diverse acoustic environments is unclear
- Impact of feature perturbation strategies on model robustness under varying noise conditions requires further investigation
- Claim about high-frequency irrelevance needs broader validation beyond specific experimental conditions

## Confidence
- Core methodology (one-stage training, supervised contrastive learning, feature perturbation): High
- TFgram feature's contribution: Medium
- Conclusion about high-frequency irrelevance: Low

## Next Checks
1. Evaluate OS-SCL performance on multiple anomalous sound detection datasets (e.g., MIMII, ToyADMOS) to assess generalizability.
2. Conduct ablation studies to isolate the contributions of feature perturbation and noise-supervised contrastive learning to overall performance.
3. Test the model's robustness to varying noise levels and acoustic environments to validate the noise-supervised learning approach.