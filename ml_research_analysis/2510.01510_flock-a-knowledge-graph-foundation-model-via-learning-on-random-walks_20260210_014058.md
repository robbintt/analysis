---
ver: rpa2
title: 'Flock: A Knowledge Graph Foundation Model via Learning on Random Walks'
arxiv_id: '2510.01510'
source_url: https://arxiv.org/abs/2510.01510
tags:
- graph
- prediction
- random
- relation
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of zero-shot link prediction
  in knowledge graphs, where models must generalize to unseen entities and relations.
  The key limitation of existing approaches is their reliance on deterministic node-relation
  equivariance, which forces structurally similar but semantically distinct relations
  to have identical representations, reducing expressiveness.
---

# Flock: A Knowledge Graph Foundation Model via Learning on Random Walks

## Quick Facts
- arXiv ID: 2510.01510
- Source URL: https://arxiv.org/abs/2510.01510
- Reference count: 40
- Primary result: FLOCK achieves state-of-the-art performance on zero-shot link prediction across 54 diverse KGs while solving a diagnostic benchmark where existing KGFMs fail

## Executive Summary
FLOCK introduces a knowledge graph foundation model that addresses zero-shot link prediction by leveraging random walks with anonymization and probabilistic node-relation equivariance. Unlike existing approaches that enforce deterministic equivariance between structurally similar relations, FLOCK uses principled randomization to break symmetries during inference while preserving transferability. The model demonstrates universal approximation capabilities for link-invariant functions and achieves state-of-the-art performance on both entity and relation prediction tasks across diverse knowledge graphs.

## Method Summary
FLOCK uses random walks as its core sampling mechanism, recording anonymized sequences that hide graph-specific identities while preserving structural information. A bidirectional GRU processes these sequences, and multi-head softmax-weighted consensus aggregates proposals across multiple walks. The model employs 6 residual update steps and uses adversarial negative sampling during pretraining. Key innovations include probabilistic node-relation equivariance that allows structurally isomorphic but semantically distinct relations to be distinguished, and test-time adaptation of walk counts based on harmonic mean scaling for size generalization.

## Key Results
- Perfect performance on PETALS benchmark where existing KGFMs achieve only 50% accuracy
- State-of-the-art results on entity and relation prediction across 54 diverse KG datasets
- Demonstrates superior generalization to unseen entities and relations compared to message-passing approaches
- Shows improved size generalization through test-time adaptation of walk counts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic node-relation equivariance enables distinction between structurally isomorphic but semantically distinct relations while preserving transferability.
- Mechanism: Instead of enforcing that structurally isomorphic relations receive identical representations (deterministic equivariance), FLOCK requires only that representations be equivalent in distribution over the model's stochastic processes. Random walk sampling provides the principled randomization that "breaks symmetries during inference" while maintaining inductive bias.
- Core assumption: Relations that appear structurally identical but carry different semantics will be visited differently across stochastic forward passes, creating distinguishable representation distributions.
- Evidence anchors:
  - [abstract] "probabilistic node-relation equivariance, which preserves equivariance in distribution while incorporating a principled randomization to break symmetries during inference"
  - [Section 1] Motivating example with like/dislike relations that are structurally isomorphic but semantically opposite
  - [corpus] Related work on probabilistic invariance (Abboud et al., 2021; Kim et al., 2025) shows randomization enhances expressivity while preserving distributional invariance
- Break condition: If random walks do not differentially sample from regions that distinguish semantic differences (e.g., all walks produce identical visit patterns for both relations), the mechanism fails to create distinguishable distributions.

### Mechanism 2
- Claim: Random walks with anonymization and recording protocol enable transfer to unseen entities and relations by learning from structural roles alone.
- Mechanism: The recording protocol anonymizes nodes (1,2,3...) and relations (α,β,γ...) in discovery order, hiding graph-specific identities while preserving structural patterns. This allows the sequence processor to learn transferable structural representations.
- Core assumption: Structural roles (position in graph topology, connectivity patterns) are sufficient for link prediction and transfer across graphs with different vocabularies.
- Evidence anchors:
  - [Section 4.1] "The recording protocol w: η → z transforms each walk into a graph-agnostic sequence that only leaves structural information"
  - [Section 5.2] Zero-shot results on 54 KGs demonstrate transfer without seeing test entities/relations
  - [corpus] Weak direct corpus evidence on anonymization specifically; related work (DeepWalk, node2vec) uses random walks but without node-relation anonymization for transfer
- Break condition: If structural roles alone are insufficient (e.g., semantic content of relation names carries critical information not captured by structure), transfer performance degrades.

### Mechanism 3
- Claim: Universal approximation for link-invariant functions is achieved when random walks have sufficient length to cover all edges with high probability.
- Mechanism: As walk length increases, probability of sampling walks that traverse all edges approaches 1 (bounded cover time). A sufficiently powerful sequence processor can reconstruct the full graph structure from anonymized walk recordings and compute any link-invariant function.
- Core assumption: The sequence processor (bidirectional GRU) has sufficient capacity to encode graph structure from walk sequences.
- Evidence anchors:
  - [Proposition 4.1] "FLOCK is a universal approximator of link invariant functions over K_{n,m} for all pairs (n,m)"
  - [Appendix C.1] Proof constructs cover time bound and shows walks witnessing all edges enable graph reconstruction
  - [corpus] "Toward a Graph Foundation Model" paper also uses random walks for pre-training transformers, suggesting walk-based approaches scale
- Break condition: For very large graphs, required walk length becomes computationally infeasible; approximation fails if walks cannot achieve sufficient coverage.

## Foundational Learning

- **Equivariance vs. Invariance**
  - Why needed here: Understanding the difference between deterministic equivariance (forcing identical outputs for isomorphic inputs) and probabilistic equivariance (allowing distributional equivalence) is central to FLOCK's contribution.
  - Quick check question: Can you explain why forcing structurally isomorphic relations to have identical representations prevents distinguishing "like" from "dislike" in a symmetric graph?

- **Random Walks as Stochastic Processes**
  - Why needed here: FLOCK uses random walks as its core sampling mechanism; understanding Markov properties, cover time, and stationarity helps explain why longer walks improve coverage.
  - Quick check question: What happens to the probability of visiting all edges as walk length increases, and why does this matter for universality?

- **Sequence Models for Graph Encoding**
  - Why needed here: FLOCK processes anonymized walks with a bidirectional GRU; understanding sequence modeling helps explain how structural patterns are extracted.
  - Quick check question: Why might a bidirectional model be preferred over unidirectional for encoding walks that represent graph structure?

## Architecture Onboarding

- **Component map**: Random Walk Sampler -> Recording Protocol -> Sequence Processor -> Consensus Protocol -> Update Loop -> Classification Head

- **Critical path**: Query → Walk sampling (n walks × length l) → Recording → Sequence encoding → Consensus aggregation → State update → Classification

- **Design tradeoffs**:
  - Walk count (n) vs. computational cost: More walks improve coverage but scale O(n)
  - Walk length (l) vs. cover probability: Longer walks approach universality but face exponential cover time
  - Ensemble size (P) vs. variance reduction: Paper shows plateau around P=12 for inference
  - Test-time adaptation of n: Adaptive scaling by harmonic mean of graph sizes improves size generalization

- **Failure signatures**:
  - 50% accuracy on PETALS benchmark indicates deterministic equivariance limitation (existing KGFMs)
  - Performance degradation on very large transductive graphs suggests walk coverage insufficiency
  - High variance in predictions without ensembling indicates insufficient walk sampling

- **First 3 experiments**:
  1. **Reproduce PETALS benchmark**: Train FLOCK and baselines (ULTRA, TRIX) on PETALS instances; verify FLOCK achieves ~100% while baselines hit 50%
  2. **Ablate walk length**: Vary l ∈ {16, 32, 64, 128, 256} on medium-sized inductive KGs; observe performance vs. coverage tradeoff
  3. **Test ensemble scaling**: Run inference with P ∈ {1, 2, 4, 8, 16} on held-out KGs; confirm variance reduction plateau pattern from Figure 4b

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What approximation strategies can reduce random walk sampling costs while retaining FLOCK's downstream performance on large knowledge graphs?
- Basis in paper: [explicit] "One limitation is scalability... ensuring coverage of the sampled random walk in a large KG requires an extensive number of longer walks, which can quickly become computationally infeasible. A future direction is to develop approximation strategies that reduce the cost of random walk sampling while retaining FLOCK's downstream performance."
- Why unresolved: Current random walk sampling dominates runtime and memory, making large KGs impractical; no approximation methods have been explored.
- What evidence would resolve it: Demonstration of an approximation technique that achieves comparable MRR/Hits@10 on large transductive KGs with significantly reduced wall-clock time and memory.

### Open Question 2
- Question: Can highly optimized GPU kernels for random walk sampling close the training efficiency gap between FLOCK and message-passing KGFMs?
- Basis in paper: [explicit] "Unlike ULTRA/TRIX, FLOCK does not rely on GNN message passing where highly optimized fused sparse kernels... accelerate computation... One avenue for future work is to develop similarly highly optimized kernels for random-walk sampling to speed up the process."
- Why unresolved: No such optimized kernels currently exist; FLOCK pretraining takes approximately three days.
- What evidence would resolve it: Implementation of optimized random walk kernels that reduce training time by a measurable factor while maintaining model performance.

### Open Question 3
- Question: Does incomplete random walk coverage explain FLOCK's relative underperformance on large transductive KGs, and can coverage-aware sampling strategies close this gap?
- Basis in paper: [inferred] "We hypothesize that this gap stems from random walk coverages. Unlike ULTRA and TRIX whose message passing guarantees a full k-hop neighborhood coverage over the queried node, FLOCK relies on sampling random walks, which may not fully cover the target nodes of interest."
- Why unresolved: The hypothesis is stated but not empirically validated; the relationship between coverage metrics and performance degradation remains unquantified.
- What evidence would resolve it: Correlation analysis between coverage statistics (e.g., proportion of edges/nodes visited) and MRR on large KGs, plus experiments with coverage-aware sampling strategies.

### Open Question 4
- Question: What is the optimal trade-off between ensemble size and computational cost for test-time inference in FLOCK?
- Basis in paper: [inferred] "As shown in Figure 4b, performance improves from 1 to 8 passes and then begins to plateau beyond 12."
- Why unresolved: The paper reports empirical observations but does not provide a principled method for selecting ensemble size; the plateau behavior suggests diminishing returns but optimal operating points remain unspecified.
- What evidence would resolve it: Systematic study of performance-per-compute trade-offs across diverse KG sizes and domains, potentially yielding adaptive ensemble size selection rules.

## Limitations
- Scalability constraints due to random walk sampling requirements on large knowledge graphs
- Reliance on structural patterns may miss semantic information in relation names
- Performance degradation on large transductive graphs due to incomplete walk coverage

## Confidence
- High confidence: Zero-shot performance claims on 54 KG datasets
- Medium confidence: Universal approximation theoretical guarantee
- Medium confidence: Probabilistic equivariance mechanism

## Next Checks
1. **Cross-domain generalization**: Evaluate FLOCK on KGs from diverse domains (biomedical, social, financial) to test structural role transferability across fundamentally different graph types.
2. **Scalability stress test**: Systematically vary KG size and density to identify the point where walk coverage becomes insufficient and performance degrades.
3. **Semantic sensitivity analysis**: Create KG variants where relation names carry semantic information, test whether anonymization hurts performance, and compare against models using relation semantics.