---
ver: rpa2
title: Learning Dynamics from Input-Output Data with Hamiltonian Gaussian Processes
arxiv_id: '2511.05330'
source_url: https://arxiv.org/abs/2511.05330
tags:
- learning
- data
- hamiltonian
- system
- input-output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses learning Hamiltonian dynamics from input-output
  data, extending prior work that relied on full-state measurements. The authors propose
  a reduced-rank Hamiltonian Gaussian Process model that uses only position measurements
  while maintaining physical consistency.
---

# Learning Dynamics from Input-Output Data with Hamiltonian Gaussian Processes

## Quick Facts
- arXiv ID: 2511.05330
- Source URL: https://arxiv.org/abs/2511.05330
- Reference count: 33
- Primary result: Reduced-rank Hamiltonian Gaussian Process learns dynamics from position-only measurements with O(M) complexity independent of training data size

## Executive Summary
This paper addresses the challenge of learning Hamiltonian dynamics from input-output data where only position measurements are available, extending prior work that required full-state measurements. The authors propose a reduced-rank Hamiltonian Gaussian Process model that maintains physical consistency while using only position data. The key innovation is modeling the Hamiltonian gradient as a linear combination of basis functions, enabling efficient computation and closed-form learning. The approach is evaluated on a nonlinear oscillator system, demonstrating comparable accuracy to full-state methods despite the reduced measurement setup.

## Method Summary
The method models the Hamiltonian gradient as a linear combination of Laplace operator eigenfunctions on a bounded domain, creating a reduced-rank GP approximation. The state-space model follows port-Hamiltonian structure with damping and input matrices, where the Hamiltonian gradient is represented as ∇H(x) = J_φ(x)a. Learning proceeds via a fully Bayesian inference scheme using Particle Gibbs with Ancestor Sampling to estimate both latent states (including unknown momentum) and hyperparameters. The closed-form parameter updates exploit conjugacy between the Gaussian likelihood and Normal-Inverse-Gamma prior, while structural parameters like damping coefficients are updated via Metropolis-within-Gibbs sampling.

## Key Results
- The reduced-rank GP achieves O(M) prediction complexity independent of training data size T
- Learning from position-only data yields flow map RMSE of approximately 3 J in magnitude and 0.13 rad in angle for the nonlinear oscillator
- The method provides physically consistent predictions with uncertainty quantification while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reduced-rank GP approximation achieves O(M) prediction complexity independent of training data size T
- Mechanism: The kernel function k(x,x') is approximated by a finite eigenfunction expansion using Laplace operator eigenfunctions on domain Ω. The Hamiltonian gradient becomes ∇H(x) = J_φ(x)a, which is linear in parameters a, enabling closed-form updates
- Core assumption: The domain bounds L₁,...,L_nx are sufficiently large and basis count M sufficiently high to approximate the exact GP (assumes convergence as M, L → ∞)
- Evidence anchors: [Abstract]: "computational complexity independent of training data size"; [Section 4]: "reduced-rank GP... features computational complexity O(M) using simple basis function expansion"; [corpus]: Weak corpus support—neighbor papers use exact GPs; reduced-rank approach appears novel here
- Break condition: If system state dimension n_x is high (basis grows as M ~ O(n_x)), complexity explodes; authors acknowledge this limitation in Conclusion

### Mechanism 2
- Claim: Learning from position-only data is enabled by joint Bayesian inference over latent states {q_t, p_t} and Hamiltonian parameters
- Mechanism: Particle Gibbs with Ancestor Sampling (PGAS) iteratively samples latent trajectories z_{0:T} given parameters ξ, then samples parameters ξ given trajectories. The latent momentum p is treated as unknown hidden state inferred from position dynamics through the Hamiltonian structure
- Core assumption: Assumption 1—matrix structures J, R, G are known (sparsity patterns); Assumption 2—noise covariances and output function g are known
- Evidence anchors: [Section 5]: "performing state inference and parameter learning can be challenging. To tackle this task, we rely on a Particle Markov Chain Monte Carlo approach"; [Section 6]: "the method provides density estimates for the model (hyper-)parameters, and accurate smoothing estimates for all state variables (including unknown hidden states)"; [corpus]: Learning Generalized Hamiltonian Dynamics (arXiv:2509.07280) similarly infers Hamiltonians from "noisy, sparse phase-space data" but requires full state
- Break condition: If damping coefficient d is structurally unidentifiable from position-only data (noted in Section 6: "slightly biased, which we attribute to identifiability issues"), posterior will be diffuse or biased

### Mechanism 3
- Claim: Closed-form parameter posterior for θ = {a, σ²} via conjugate Normal-Inverse-Gamma prior
- Mechanism: The likelihood p(h_t|x_t, θ) is Gaussian, and NIG prior is conjugate. Posterior statistics update by summing prior statistics with trajectory-derived statistics: s⁺ = s̃ + Σs_t, r⁺ = r̃ + Σr_t
- Core assumption: The reduced-rank model's linearity in parameters a preserves conjugacy; the basis functions φ_k(x) are pre-computed and fixed
- Evidence anchors: [Section 5.1]: "parameter posterior p(θ|z_{0:T}, ϑ) is available in closed form by summation of the prior statistics and new statistics"; [Supplementary A]: Full derivation showing canonical exponential family form with statistics s₁ = J_φ(x)^T h, r₁ = J_φ(x)^T J_φ(x); [corpus]: No direct corpus precedent for this conjugacy exploitation in Hamiltonian GP context
- Break condition: If hyperparameters ϑ_K or structural parameters ϑ_S require updating, closed-form breaks—requires Metropolis-within-Gibbs (Section 5.2)

## Foundational Learning

- **Gaussian Process regression and kernel methods**: Understanding how GP priors encode smoothness, and why the reduced-rank approximation approximates the kernel via spectral decomposition. Quick check: Can you explain why k(x,x') ≈ Σ_k S(√ϱ_k)φ_k(x)φ_k(x') approximates a squared-exponential kernel?

- **Hamiltonian mechanics (conservative and dissipative)**: The state-space model embeds physics via ẋ = (J-R)∇H + Gu; understanding J (skew-symmetric interconnection), R (dissipation), and ∇H (energy gradient) is essential. Quick check: What physical role does the matrix R play, and why must it be positive semi-definite?

- **Particle Markov Chain Monte Carlo (PMCMC) and Particle Gibbs**: The inference algorithm alternates between conditional SMC for latent states and parameter updates; understanding why this samples from the true posterior asymptotically (Remark 1) is critical. Quick check: Why does fixing one reference trajectory in conditional SMC still yield valid posterior samples?

## Architecture Onboarding

- **Component map**: 
  1. Basis function layer: Eigenfunctions φ_k(x) with domain bounds L_i; indices j_{k,i} determine frequency
  2. Hamiltonian gradient model: ∇H(x) = J_φ(x)a with learnable weights a ~ N(0, σ²V)
  3. State dynamics: Discretized ẋ = (J̄-R̄)∇H̄ + Ḡu via Euler/symplectic integrator
  4. Observation model: y = g(x) + e (Assumption 2: g known)
  5. Inference engine: PMCMC loop—conditional SMC for z_{0:T}, then closed-form θ update, then MH for ϑ
  6. Hyperpriors: NIG for θ; broad Gaussian for ϑ (kernel params ϑ_K and structural params ϑ_S like damping)

- **Critical path**: 
  1. Define domain bounds L_i and basis count M (tradeoff: accuracy vs. O(M³) operations)
  2. Initialize z_{0:T}^{[0]}, ϑ^{[0]}, θ^{[0]} (Algorithm 1)
  3. Run K iterations (e.g., 20,000), discard burn-in (e.g., 15,000)
  4. For prediction: draw posterior samples, forward simulate via symplectic integrator

- **Design tradeoffs**:
  - M vs. accuracy: Higher M improves approximation but cubic cost in hyperparameter learning
  - Domain bounds L_i: Too small truncates dynamics; too large requires more basis functions
  - Symmetry constraints: Anti-symmetry constraint H̄(x) = -H̄(-x) aids identifiability from position-only data but restricts model class
  - Euler vs. symplectic integration: Training uses Euler; testing uses symplectic (Table 2 caption)—mismatch may inject error

- **Failure signatures**:
  - Divergent momentum estimates: If damping d is poorly identified, momentum posterior spreads without converging
  - Non-physical energy predictions: If basis functions insufficient, Hamiltonian may violate energy conservation
  - Slow MCMC mixing: If hyperproposals poorly tuned, chain requires many more iterations

- **First 3 experiments**:
  1. Replicate oscillator case: Implement reduced-rank GP with M=15 basis functions on 1D Hamiltonian H(q,p) = q²/2 + p²/2 + 2cos(q); verify RMSE flow magnitude ~3 J from position-only data
  2. Ablation on M: Test M ∈ {5, 10, 15, 20, 30} with input-state data; plot flow RMSE vs. M to find diminishing-returns point
  3. Identifiability stress test: Run with damping d ∈ {0.05, 0.15, 0.3} and reduce training trajectory length; quantify posterior bias in d̂ from position-only vs. full-state data

## Open Questions the Paper Calls Out

- **Question:** How can the computational complexity be mitigated when scaling the reduced-rank Hamiltonian GP to high-dimensional systems?
- **Basis in paper:** [explicit] The Conclusion states that "the computational complexity of the reduced-rank GP increases rapidly with the regression input dimension, which future work should consider."
- **Why unresolved:** The method relies on a basis function expansion where the number of required basis functions grows significantly with the state dimension, potentially limiting applicability to complex mechanical systems with many degrees of freedom
- **What evidence would resolve it:** An extension of the method applied to systems with high-dimensional state spaces (e.g., >10 dimensions) demonstrating tractable training times

## Limitations

- Identifiability issues when learning damping coefficients from position-only data may yield biased estimates
- The method assumes structural parameters (J, R, G) are known, limiting black-box system identification capabilities
- Computational complexity increases rapidly with state dimension due to basis function expansion requirements

## Confidence

- **High confidence**: The reduced-rank GP approximation achieves O(M) complexity independent of training data size; closed-form parameter updates via conjugate priors; symplectic integration preserves physical consistency
- **Medium confidence**: Learning from position-only data is feasible with anti-symmetry constraints; computational efficiency claims hold for moderate dimensions
- **Low confidence**: Generalization to high-dimensional systems (n_x > 3); robustness to unknown structural parameters; performance with real-world noisy data

## Next Checks

1. **Ablation study on basis count**: Test M ∈ {5, 10, 15, 20, 30} with input-state data; plot flow RMSE vs. M to identify diminishing-returns point and verify O(M) complexity claims
2. **Identifiability stress test**: Compare damping coefficient posterior estimates from position-only vs. full-state data across varying trajectory lengths and noise levels; quantify bias and uncertainty
3. **High-dimensional extension**: Implement 3D oscillator system; measure how computational time scales with M and n_x; test basis function stability and numerical conditioning