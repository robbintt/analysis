---
ver: rpa2
title: 'Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase'
arxiv_id: '2510.27002'
source_url: https://arxiv.org/abs/2510.27002
tags:
- jasmine
- world
- training
- modeling
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Jasmine is a JAX-based world modeling codebase that achieves an
  order-of-magnitude faster training speed compared to prior implementations for the
  CoinRun case study. The codebase provides fully reproducible training with support
  for diverse sharding configurations and includes performance optimizations across
  data loading, training, and checkpointing.
---

# Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase

## Quick Facts
- arXiv ID: 2510.27002
- Source URL: https://arxiv.org/abs/2510.27002
- Authors: Mihir Mahajan, Alfred Nguyen, Franz Srambical, Stefan Bauer
- Reference count: 40
- Primary result: Achieves order-of-magnitude faster training (under 9 hours on single GPU vs over 100 hours reported) for CoinRun world modeling

## Executive Summary
Jasmine is a JAX-based codebase for training world models on video data that achieves significant speedups through infrastructure optimizations. The codebase implements a complete video world modeling pipeline including tokenizer, latent action model, and dynamics model using ST-Transformer architectures. Key innovations include data loading optimizations using ArrayRecord format with Grain dataloader, architectural modifications like prepending latent actions for autoregressive generation, and performance optimizations including FlashAttention and mixed precision training. Jasmine demonstrates training a CoinRun world model in under 9 hours on a single GPU, compared to over 100 hours in prior work, while maintaining comparable or better performance metrics.

## Method Summary
The codebase implements a complete world modeling pipeline for video data using JAX. It consists of three main components: a video tokenizer (ST-Transformer encoder → VQ-VAE → decoder), a latent action model (LAM) that learns discrete action representations, and a dynamics model that generates future frames conditioned on actions. The critical modification is prepending latent actions to video token sequences rather than adding them to embeddings, which is essential for faithful autoregressive generation. Training uses MaskGIT-style objectives with random masking, and the codebase achieves high throughput through ArrayRecord data formats, Grain dataloader with prefetching, FlashAttention, and bfloat16 mixed precision. The implementation is designed to be fully reproducible with support for diverse sharding configurations.

## Key Results
- Achieved 10x+ faster training than prior implementations (under 9 hours vs over 100 hours on single GPU for CoinRun)
- Data pipeline optimizations (ArrayRecord + Grain) account for majority of speedup
- Prepending latent actions (not adding) is critical for generating faithful CoinRun simulations
- Architectural modifications like reducing FFN expansion factors and warmup-stable-decay schedules improve throughput while maintaining performance
- Achieved reproducible training with bitwise determinism across different hardware configurations

## Why This Works (Mechanism)

### Mechanism 1: Latent Action Prepending for Faithful Autoregressive Generation
Prepending latent actions to video token sequences appears necessary for generating faithful CoinRun simulations under MaskGIT training. By placing action tokens before frame tokens in the sequence, the transformer receives action conditioning prior to attending over spatial positions. Element-wise addition may dilute this signal during masked prediction, though the exact cause remains hypothesized. Validation loss is similar between configurations, so loss alone cannot detect this failure; rollout metrics (PSNR, SSIM) are required.

### Mechanism 2: ArrayRecord + Grain Data Pipeline for Throughput
The combination of ArrayRecord file format with optimized chunking and Grain dataloader with prefetching accounts for the majority of the 10x+ speedup over prior implementations. ArrayRecord enables efficient random access through indexed record storage; chunking at 100 records/file with 160 frames/record balances I/O granularity. Grain's prefetching overlaps data transfer with computation. At batch_size=36, removing Grain reduces throughput to 0.25x; at batch_size=2048, to 0.11x.

### Mechanism 3: ST-Transformer Factorization for Tractable Video Attention
Decomposing full spatiotemporal attention into sequential spatial-then-temporal operations enables processing video sequences that would otherwise exceed memory limits. ST-Transformer performs intra-frame attention over spatial patches first, then inter-frame attention across resulting frame representations. This reduces sequence length from (patches × frames) to max(patches, frames) per attention stage, making large video sequences tractable while maintaining sufficient representational capacity for world modeling.

## Foundational Learning

- **Concept: MaskGIT Masked Prediction**
  - Why needed here: Dynamics model uses MaskGIT-style training with random masking probability p ~ U(0.5, 1); understanding iterative decoding and mask scheduling is essential for training and inference.
  - Quick check question: Why does MaskGIT sample a new masking probability per sequence (Jasmine) rather than per batch (Jafar), and what effect does this have on loss variance?

- **Concept: VQ-VAE Discrete Bottlenecks**
  - Why needed here: Both tokenizer and LAM use VQ-VAE with learnable codebooks (1024 codes for video, 6 codes for actions); understanding straight-through gradient estimation and commitment loss is required.
  - Quick check question: What happens to gradient flow if the commitment loss coefficient is set to zero?

- **Concept: JAX XLA Compilation and Sharding**
  - Why needed here: Jasmine's scalability relies on XLA compilation and Shardy for distributed training; understanding jit compilation, device placement, and shard maps is necessary for multi-accelerator experiments.
  - Quick check question: Why does bitwise determinism require xla_gpu_deterministic_ops=true on GPUs but not TPUs?

## Architecture Onboarding

- **Component map:** Video Tokenizer (ST-Transformer encoder → VQ quantization → decoder) → Latent Action Model (Frame pair encoder → 6-code discrete bottleneck) → Dynamics Model (Decoder-only ST-Transformer with prepended action tokens)

- **Critical path:** Preprocess videos to ArrayRecord with verified chunking → Train tokenizer (300k steps) → Co-train LAM + dynamics with action prepending (200k steps) → Inference: discard LAM, autoregressive decode with user actions

- **Design tradeoffs:**
  - FFN expansion 4× vs 1×: 4× yields 1.00x vs 0.79x throughput at bs=2048
  - Co-training vs pre-training LAM: Co-training simpler; pre-training reduces peak memory
  - MaskGIT vs Diffusion: Diffusion shows higher rollout SSIM but 25 denoising steps per frame at inference

- **Failure signatures:**
  - Validation loss stable but rollout quality degrades: Check action prepending vs. adding
  - High loss variance across steps: Verify per-sample masking probabilities
  - Low throughput despite high GPU utilization: Check ArrayRecord chunking and Grain prefetching
  - Non-reproducible runs on GPU: Verify xla_gpu_deterministic_ops=true

- **First 3 experiments:**
  1. Reproduce Jasmine-base CoinRun with action prepending; verify 9-hour training and rollout quality
  2. Ablate Grain data loading; measure throughput impact (expect 4× from Table 2)
  3. Compare MaskGIT vs. diffusion baseline on identical data split; report rollout PSNR/SSIM and inference latency

## Open Questions the Paper Calls Out

### Open Question 1
How do compute and data requirements scale with environment complexity for world models used in downstream agent training? Jasmine provides foundational infrastructure for future empirical investigation, but no scaling experiments were conducted beyond the CoinRun case study. Systematic experiments across environments of varying complexity with controlled compute budgets would resolve this.

### Open Question 2
Can the fully causal baseline match or exceed MaskGIT performance with longer training and separately tuned hyperparameters? The causal baseline underperforms at 200k steps using hyperparameters tuned for MaskGIT. Training beyond 200k steps with architecture-specific hyperparameter searches would determine if this gap is fundamental.

### Open Question 3
Does the shortcut objective improve upon diffusion-forcing for world modeling? The diffusion baseline uses diffusion-forcing but not the shortcut objective, which could offer faster sampling. Implementing shortcut models and comparing sample quality and inference speed would resolve this.

### Open Question 4
What specific optimizations would close the throughput gap between Jasmine and frontier language model implementations? While Jasmine accelerates training compared to prior work, it hasn't matched throughput efficiencies of mature LLM codebases. Profiling comparisons against mature implementations and targeting identified bottlenecks would provide answers.

## Limitations
- The mechanism by which additive action conditioning fails under MaskGIT remains speculative without direct evidence
- Generalization of the action prepending finding to other environments beyond CoinRun is unproven
- Performance characteristics at extreme scales (multi-node, massive models) are not characterized

## Confidence

**High Confidence:**
- Jasmine achieves 10x+ faster training than prior implementations for CoinRun
- Data pipeline optimizations account for majority of speedup
- Prepending actions (not adding) is critical for faithful CoinRun rollouts
- ST-Transformer factorization is implemented correctly and reduces memory usage

**Medium Confidence:**
- Architectural modifications improve throughput while maintaining performance
- FlashAttention provides consistent speedups across model scales
- Codebase provides truly reproducible training with specified determinism flags

**Low Confidence:**
- Exact mechanism by which additive action conditioning fails under MaskGIT
- Generalization of action prepending finding to other environments
- Performance at extreme scales (multi-node, massive models)

## Next Checks

1. **Generalization Test:** Apply the action prepending approach to a different environment (e.g., Atari Breakout or DMLab) and verify whether the same mechanism applies or if the failure mode differs.

2. **Multi-Node Scaling:** Characterize Jasmine's performance when scaling from single-node to multi-node configurations, measuring both throughput and scaling efficiency with varying numbers of accelerators.

3. **Attention Factorization Analysis:** Systematically measure the quality degradation from ST-Transformer factorization by comparing against full attention baselines on tasks that specifically require distant spatial-temporal attention.