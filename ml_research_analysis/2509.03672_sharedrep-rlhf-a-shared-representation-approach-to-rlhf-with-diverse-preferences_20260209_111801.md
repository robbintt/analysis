---
ver: rpa2
title: 'SharedRep-RLHF: A Shared Representation Approach to RLHF with Diverse Preferences'
arxiv_id: '2509.03672'
source_url: https://arxiv.org/abs/2509.03672
tags:
- sharedrep-rlhf
- minority
- reward
- rlhf
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of promoting group fairness
  in offline reinforcement learning from human feedback (RLHF) when dealing with diverse
  preferences across subpopulations. The state-of-the-art MaxMin-RLHF framework learns
  separate reward models for each group and optimizes for the minimum-reward group,
  but it struggles when the minority group is underrepresented.
---

# SharedRep-RLHF: A Shared Representation Approach to RLHF with Diverse Preferences

## Quick Facts
- arXiv ID: 2509.03672
- Source URL: https://arxiv.org/abs/2509.03672
- Authors: Arpan Mukherjee; Marcello Bullo; Deniz Gündüz
- Reference count: 40
- Primary result: SharedRep-RLHF achieves up to 20% improvement in win rate over MaxMin-RLHF, particularly in low minority representation settings

## Executive Summary
This paper addresses the challenge of promoting group fairness in offline reinforcement learning from human feedback (RLHF) when dealing with diverse preferences across subpopulations. The state-of-the-art MaxMin-RLHF framework learns separate reward models for each group and optimizes for the minimum-reward group, but it struggles when the minority group is underrepresented. The proposed SharedRep-RLHF framework addresses this limitation by learning shared traits across all groups while capturing group-specific nuances, rather than learning separate reward models. The method leverages a universal feature extractor to capture common preferences and improves estimation accuracy, especially for underrepresented groups.

## Method Summary
The SharedRep-RLHF framework consists of two main components: a shared representation learning phase and a pessimistic RL fine-tuning phase. First, a frozen SFT backbone produces embeddings φ(x,y) for (prompt, response) pairs. A shared linear layer B captures common preference traits across all subpopulations, while group-specific heads w_u capture nuances. The framework optimizes parameters using maximum likelihood estimation on the combined dataset. During RL fine-tuning, PPO or GRPO optimizes a MaxMin objective with a pessimism bonus derived from feature covariance, selecting policies that maximize the minimum (over groups) pessimistic KL-regularized reward.

## Key Results
- SharedRep-RLHF consistently outperforms MaxMin-RLHF across three diverse natural language tasks
- Achieves up to 20% improvement in win rate, particularly in low minority representation settings
- Improves estimation accuracy for underrepresented groups by leveraging data from the entire population
- Provides theoretical guarantees showing better estimation fidelity and characterized sample complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared representation improves estimation accuracy for underrepresented groups by leveraging data from the entire population.
- Mechanism: A universal feature extractor B* captures common preference traits across all subpopulations. Instead of estimating group-specific parameters θ_u from limited minority data, the framework decomposes θ_u* = B*w_u*, where B* is learned from all N samples and only w_u* requires group-specific data. This reduces estimation error scaling from O(1/N_u) to O(1/N).
- Core assumption: Subpopulations share underlying preference traits (e.g., conciseness, correctness) that can be captured in a low-dimensional representation (K << d).
- Evidence anchors: [abstract] "learns shared traits across all groups while capturing group-specific nuances... leverages a universal feature extractor to capture common preferences and improves estimation accuracy, especially for underrepresented groups"; [section 3, Lemma 3.3] Confidence bound scales with 1/N rather than 1/N_u.

### Mechanism 2
- Claim: The worst-case subpopulation under MaxMin optimization corresponds to the group with maximal entropy in its induced Gibbs distribution.
- Mechanism: The KL-regularized MaxMin objective induces Gibbs distributions ν_r for each group. The paper proves (Lemma 4.1) that identifying the minimum-reward group is equivalent to finding the group with maximum conditional entropy H(ν_r·|X). This connection enables sample complexity analysis.
- Core assumption: The reward gap Δ_min between the worst-case group and others is non-zero; small gaps make identification harder.
- Evidence anchors: [section 4, Lemma 4.1] Formal equivalence proof between minimum-reward and maximum-entropy selection; [section 4, Theorem 4.2] Sample complexity includes O(1/Δ_min^4) term for MaxMin identification cost.

### Mechanism 3
- Claim: Pessimistic estimation prevents unbounded suboptimality in offline RLHF by accounting for uncertainty in reward estimates.
- Mechanism: Rather than using maximum likelihood estimates directly, the algorithm constructs confidence sets Θ_SR around estimates and optimizes the minimum value function within these sets. An uncertainty bonus term η_SR·Γ(Σ,λ,π) is subtracted from the reward, penalizing policies that venture into poorly-estimated regions.
- Core assumption: The linear reward parameterization (Assumption 2.1) and reward gap (Assumption 2.2) hold; the feature covariance Σ is well-conditioned.
- Evidence anchors: [section 3, equations 11-14] Formal definition of confidence sets and pessimistic value functions; [section 3, Theorem 3.4] Performance comparison showing SharedRep outperforms MaxMin when policies differ.

## Foundational Learning

- Concept: **Bradley-Terry choice model**
  - Why needed here: Models the probability that annotators prefer response y over y' as σ(r(x,y) - r(x,y')), where σ is sigmoid. All reward learning in the paper builds on this.
  - Quick check question: Given rewards r(x,y)=2 and r(x,y')=0, what is the probability an annotator prefers y?

- Concept: **KL-regularized policy optimization**
  - Why needed here: The objective balances reward maximization with staying close to the reference policy π_ref. The β parameter controls this tradeoff.
  - Quick check question: If β→∞, what happens to the optimal policy?

- Concept: **Pessimism in offline RL**
  - Why needed here: Offline data may not cover all state-action pairs. Pessimism penalizes uncertainty to avoid exploiting poorly-estimated rewards.
  - Quick check question: Why might MLE estimates lead to unbounded suboptimality in offline settings?

## Architecture Onboarding

- Component map: Frozen SFT backbone -> φ(x,y) embedding -> Shared linear layer B -> Group-specific heads w_u -> Reward computation r(x,y) = ⟨φ(x,y), Bw_u⟩
- Critical path:
  1. Collect preference dataset D with group labels
  2. Estimate B (shared) and {w_u} (group-specific) via MLE minimizing cross-entropy loss
  3. Compute covariance matrix Σ from all difference vectors φ(x,y) - φ(x,y')
  4. During RL, for each policy candidate, compute pessimistic reward: estimated reward - η_SR·Γ term
  5. Select policy maximizing minimum (over groups) pessimistic KL-regularized reward
- Design tradeoffs:
  - Inner dimension K: Larger K captures more nuanced shared traits but risks overfitting; paper uses K=2 for sentiment, K=16 for math reasoning
  - Group granularity: Finer partitions capture more diversity but reduce per-group data
  - Assumption: Group membership must be known at annotation time
- Failure signatures:
  - Minority win rate stagnates despite training: K too small to capture group-specific nuances, or shared traits dominate
  - Majority performance collapses: MaxMin over-optimizing for minority; check if statistical minority is actually reward minority
  - Reward estimates diverge: Check Σ conditioning; may need stronger regularization λ
- First 3 experiments:
  1. Replicate Figure 1: Train MaxMin-RLHF on IMDb with varying minority proportions (1%, 5%, 10%, 20%) to establish baseline degradation
  2. Ablate K: Train SharedRep-RLHF with K∈{2,4,8,16,32} on a held-out validation set; plot minority score vs K to find sweet spot
  3. Controlled stress test: Create synthetic preference data where groups share exactly one trait (e.g., correctness) but differ on another; verify SharedRep captures the shared trait while MaxMin struggles at low minority proportions

## Open Questions the Paper Calls Out

- The framework relies on the assumption that "the partitioning of the annotators is known" (Page 4), although it briefly notes clustering as an alternative in a footnote. The sensitivity of the shared representation learning to mislabeled or latent group structures is not analyzed.
- The authors explicitly state their focus is on "offline RLHF" (Page 2) and distinguish their work from online approaches where annotators are available for real-time feedback. The algorithm relies on a fixed dataset D, and it is unclear if the sample efficiency gains from shared representation learning transfer to an adaptive setting.
- The theoretical analysis defines U subpopulations, but all empirical evaluations are restricted to a binary split (majority vs. minority). The empirical impact of learning a single shared matrix B across many diverse groups (e.g., U > 10) is not validated.

## Limitations

- Strong assumptions about shared preference traits across subpopulations that may not hold in real-world settings
- Equivalence between maximum-entropy and minimum-reward groups relies on specific reward gap conditions that could be violated
- Pessimistic estimation framework assumes well-conditioned feature covariance matrices, which may not hold with high-dimensional or sparse features

## Confidence

- **High confidence**: The shared representation mechanism for improving estimation accuracy (Mechanism 1) has strong theoretical grounding and clear empirical support across all three tasks
- **Medium confidence**: The entropy-reward equivalence (Mechanism 2) is formally proven but lacks direct empirical validation; the sample complexity analysis depends on conditions that may not hold in practice
- **Medium confidence**: The pessimistic estimation framework (Mechanism 3) is theoretically sound but implementation details are sparse, particularly regarding efficient computation of the uncertainty penalty during RL

## Next Checks

1. Test SharedRep-RLHF on synthetic data where groups share exactly one preference trait versus data where groups share no traits, to validate when the shared representation provides benefits
2. Implement a controlled experiment varying the reward gap Δ_min between groups to empirically verify the sample complexity predictions for MaxMin identification
3. Benchmark SharedRep-RLHF against personalized RLHF approaches on tasks with known individual preference distributions to compare effectiveness