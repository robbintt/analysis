---
ver: rpa2
title: 'Generative Artificial Intelligence: Implications for Biomedical and Health
  Professions Education'
arxiv_id: '2501.10186'
source_url: https://arxiv.org/abs/2501.10186
tags:
- clinical
- generative
- found
- education
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Generative AI, particularly large language models (LLMs), is transforming
  biomedical and health professions education by demonstrating high performance on
  medical exams, clinical reasoning tasks, and academic assessments. While LLMs show
  promise in assisting students and educators, concerns exist regarding factual accuracy,
  citation errors, bias, and the potential undermining of critical thinking skills.
---

# Generative Artificial Intelligence: Implications for Biomedical and Health Professions Education

## Quick Facts
- arXiv ID: 2501.10186
- Source URL: https://arxiv.org/abs/2501.10186
- Authors: William Hersh
- Reference count: 0
- Primary result: Generative AI, particularly LLMs, shows high performance on medical exams and clinical tasks while raising concerns about factual accuracy, bias, and potential erosion of critical thinking skills in biomedical education.

## Executive Summary
Generitive AI, particularly large language models (LLMs), is transforming biomedical and health professions education by demonstrating performance comparable to human experts on medical board examinations and clinical reasoning tasks. While LLMs show promise in assisting both students and educators as personalized tutors, simulators, and knowledge resources, significant concerns exist regarding factual accuracy, citation errors, bias propagation, and the potential undermining of critical thinking skills. The review emphasizes that successful integration requires developing AI competencies among both students and faculty, implementing careful balance between LLM use and traditional learning methods, and establishing clear usage policies. Future challenges include optimizing real-world LLM integration, establishing comprehensive evaluation methods, and ensuring robust safety measures while maintaining professional competence.

## Method Summary
This is a narrative review article summarizing LLM performance on biomedical education tasks rather than an experimental study. The paper surveys existing studies using datasets like MedQA (USMLE), NEJM clinicopathologic conferences, and course examinations, reviewing metrics from cited studies including accuracy on multiple-choice questions, diagnostic accuracy, and concordance with clinical guidelines. No original training data or models were provided, and the paper reviews external LLMs (GPT-3.5, GPT-4, Med-Gemini, o1, Claude, Llama) and their reported results without conducting new experiments.

## Key Results
- LLMs achieve passing thresholds and top percentile performance on medical board exams like USMLE and MedQA
- LLMs can function as personalized educational agents (tutors, simulators, mentors) through prompt engineering
- Excessive LLM reliance may create a "jagged technological frontier" that masks erosion of critical thinking and knowledge acquisition
- Hallucination and factual accuracy remain primary safety concerns, with citation fabrication rates up to 55% in some models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can approximate expert-level performance in simulated clinical reasoning and medical board examinations through pattern matching on massive text corpora.
- Mechanism: Deep learning models trained on vast datasets can retrieve and synthesize medical knowledge to answer multiple-choice questions and solve clinical vignettes, often augmented by chain-of-thought prompting.
- Core assumption: High scores on standardized tests correlate with the ability to process medical logic rather than simply retrieving memorized answers.
- Evidence anchors: GPT-4 and Med-Gemini surpassing passing thresholds on MedQA with OpenAI's o1 model reaching 96.0%; Foundation Model in Biomedicine supports capability of large-scale pre-trained models to excel in diverse biomedical downstream tasks.
- Break condition: Performance degrades in conversational settings or real-world workflows where instructions are complex or integration with physical exam findings is required.

### Mechanism 2
- Claim: LLMs can function as personalized educational agents by adapting to specific prompt contexts and roles.
- Mechanism: Users define a persona or task via prompt engineering, causing the model to constrain its output generation to specific pedagogical modes.
- Core assumption: The "fluent style" and interactivity of the LLM provide sufficient scaffolding to enhance learning engagement despite lacking true understanding.
- Evidence anchors: "Assigning AI" approaches where AI acts as Mentor, Tutor, Coach, or Simulator; effective use requires specific, iterative, and context-aware prompt engineering.
- Break condition: The user enters "unsafe" queries or the model "hallucinates" a flawed rationale while role-playing, potentially cementing incorrect logic.

### Mechanism 3
- Claim: Excessive reliance on LLMs for task completion creates a "jagged technological frontier" that may mask the erosion of underlying critical thinking and knowledge acquisition.
- Mechanism: The reduction of "mental effort" via easy shortcuts prevents the cognitive encoding necessary for long-term professional competence.
- Core assumption: Struggle and "mental effort" are required for neuroplasticity and learning; bypassing this via AI tools creates dependency that fails when the tool is removed.
- Evidence anchors: Studies showing students using ChatGPT for math improved 48-127%, but scores averaged 17% worse than baseline when LLMs were removed; LLMs allow "easy answering of questions but not resulting in mastery of the material."
- Break condition: Educators redesign assessments to focus on process and verification rather than output generation.

## Foundational Learning

- Concept: **Hallucination and Factuality**
  - Why needed here: LLMs speak with a "confident tone" that can mislead users; distinguishing between fluency and accuracy is the primary safety requirement.
  - Quick check question: Can you identify the specific "truthfulness challenge" in an LLM's output (e.g., undersourcing, confabulation) versus a mere stylistic error?

- Concept: **Prompt Engineering**
  - Why needed here: The quality of LLM output is highly conditional on input specificity; knowing how to iterate, set roles, and provide context is the core operational skill.
  - Quick check question: How does adding "temporal awareness" or "one-shot prompts" change the reliability of the model's response in a clinical context?

- Concept: **Cognitive Offloading vs. Scaffolding**
  - Why needed here: To distinguish between using AI to "think for you" (offloading/harm) versus using it to "guide thinking" (scaffolding/benefit).
  - Quick check question: If a student uses an LLM to generate a differential diagnosis, what specific mental task must they perform afterward to ensure learning occurs?

## Architecture Onboarding

- Component map: LLM Core -> Interface Layer -> Verification Module -> Data Context
- Critical path: 1. Define specific learning task 2. Apply Prompt Engineering constraints 3. Generate output 4. Apply Factuality Checks 5. Synthesize/Verify
- Design tradeoffs:
  - Speed vs. Accuracy: LLMs generate drafts quickly but require slow human verification
  - Capability vs. Competence: Using LLMs allows high performance on tasks but risks degrading student's future professional competence
  - RAG vs. Raw Model: RAG improves accuracy but adds complexity and latency
- Failure signatures:
  - The "Copy-Paste" Decay: High assignment grades but failure on proctored/in-person exams
  - Hallucinated Citations: References that look valid but do not exist (55% fabrication rate in GPT-3.5)
  - Bias Propagation: Outputting race-based medicine or stereotypes without clinical justification
- First 3 experiments:
  1. **Prompt Variance Test**: Run the same clinical vignette through an LLM 3 times to measure consistency and observe if "leading diagnosis" changes
  2. **Socratic Tutor Mode**: Configure the LLM as a "Coach" to solve a case without giving the answer; measure student engagement vs. direct answer generation
  3. **Citation Audit**: Ask the LLM to generate a literature review and manually verify the existence of 5 randomly selected references

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal policies for use of LLMs in biomedical and health professions education?
- Basis in paper: Listed in Section 7 as a specific future issue for education
- Why unresolved: Current surveys show lack of consistent or comprehensive policies by educational institutions, with students using AI on assignments while educators lack guidance on acceptable boundaries
- What evidence would resolve it: Comparative studies across institutions with different policy frameworks, measuring learning outcomes, academic integrity, and student/faculty satisfaction over multiple academic cycles

### Open Question 2
- Question: How can student learning be accurately assessed when generative AI tools are readily available?
- Basis in paper: Section 7 states: "How will we assess student learning when generative AI tools are readily available?" and Section 3.8 notes AI systems "raising questions about student assessment in higher education"
- Why unresolved: LLMs achieve passing grades and even top percentile performance on knowledge-based assessments; both machine and human detection of AI-generated text is inconsistent and easily evaded
- What evidence would resolve it: Development and validation of new assessment modalities (e.g., oral exams, real-time problem-solving, performance-based assessments) that demonstrate reliability in distinguishing genuine student competence from AI-assisted responses

### Open Question 3
- Question: How can LLMs be safely optimized and validated for real-world clinical practice beyond simulated settings?
- Basis in paper: Section 7 states: "How will the use of LLMs be optimized and validated beyond simulated use and implemented in the larger workflow of biomedical and health professional practice?" Section 4.5 notes "most of the studies showing impressive application of LLMs were conducted in simulated settings" with "few clinical trials that have assessed patient or health care delivery outcomes"
- Why unresolved: Current performance data comes from board exams, clinical vignettes, and simulated cases; real-world integration requires addressing workflow constraints, instruction-following limitations, and safety concerns
- What evidence would resolve it: Prospective clinical trials evaluating patient outcomes, error rates, clinician efficiency, and safety incidents when LLMs are integrated into actual clinical workflows

### Open Question 4
- Question: How can educators prevent overreliance on LLMs that undermines critical thinking and professional competence?
- Basis in paper: Section 7 asks: "How will students and instructors minimize overreliance on LLMs that may undermine their professional broader professional competence?" Section 5.3 discusses studies showing LLM use can improve performance while the tool is available but impair learning when removed
- Why unresolved: Studies show students using LLMs for solutions rather than explanations show no learning benefit; when tools are removed, performance drops below baseline in some cases
- What evidence would resolve it: Longitudinal studies tracking professional competence and clinical reasoning skills of graduates who used different levels of AI assistance during training, measured against real-world clinical performance indicators

## Limitations
- The review synthesizes existing studies but does not provide original experimental data, making direct validation of many claims difficult
- Most cited studies use proprietary datasets (medical board exams, clinical vignettes) not publicly available
- Prompt engineering strategies vary widely across studies, creating reproducibility challenges
- Limited evidence on long-term cognitive impacts of LLM integration in education
- Most validation focuses on multiple-choice performance rather than clinical reasoning in real-world contexts

## Confidence
- **High confidence**: LLM performance on standardized medical exams (USMLE, MedQA) - supported by multiple independent studies with consistent results
- **Medium confidence**: LLM effectiveness as personalized educational agents - evidence exists but heavily dependent on prompt engineering quality and user expertise
- **Medium confidence**: Risks of cognitive offloading and critical thinking degradation - supported by emerging studies but long-term impacts remain uncertain
- **Low confidence**: Optimal real-world integration strategies - field is too new with insufficient longitudinal data on implementation outcomes

## Next Checks
1. **Replication study**: Select one specific medical exam dataset (e.g., MedQA-USMLE) and test GPT-4 performance using standardized prompting to verify reported accuracy rates
2. **Prompt engineering experiment**: Compare student learning outcomes using direct answer generation versus Socratic tutor mode across identical clinical cases
3. **Citation verification audit**: Generate literature reviews using different LLMs and independently verify reference existence rates to quantify hallucination frequency in academic contexts