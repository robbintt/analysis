---
ver: rpa2
title: Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)
arxiv_id: '2508.04894'
source_url: https://arxiv.org/abs/2508.04894
tags:
- graph
- attacks
- node
- llms
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work explores adversarial vulnerabilities in graph-aware\
  \ large language models (LLMs) for node classification, an area dominated by Graph\
  \ Neural Networks (GNNs). The authors investigate two representative graph-aware\
  \ LLM approaches\u2014LLAGA (node templates + linear projector) and GRAPH PROMPTER\
  \ (GNN + linear projector)\u2014using existing adversarial attacks from GNNs, including\
  \ NETTACK and METAATTACK, as well as new node sequence template injection attacks."
---

# Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)

## Quick Facts
- arXiv ID: 2508.04894
- Source URL: https://arxiv.org/abs/2508.04894
- Reference count: 40
- This work explores adversarial vulnerabilities in graph-aware large language models (LLMs) for node classification, an area dominated by Graph Neural Networks (GNNs).

## Executive Summary
This work explores adversarial vulnerabilities in graph-aware large language models (LLMs) for node classification, an area dominated by Graph Neural Networks (GNNs). The authors investigate two representative graph-aware LLM approaches—LLAGA (node templates + linear projector) and GRAPH PROMPTER (GNN + linear projector)—using existing adversarial attacks from GNNs, including NETTACK and METAATTACK, as well as new node sequence template injection attacks. They find that structural attacks (evasion) are highly effective (up to 51% accuracy drop), especially against LLAGA due to its reliance on graph structure. Feature perturbation attacks, previously ineffective on GNNs, prove surprisingly powerful here (up to 84% accuracy drop), often surpassing structural attacks. A unified attack combining both is even more effective. The authors propose GALGUARD, an end-to-end defense integrating LLM-based feature correction with adapted GNN structural defenses, which significantly improves robustness. Key findings: graph encoding design choices critically impact vulnerability, LLMs are highly sensitive to feature-level perturbations, and tailored defenses are necessary for this emerging paradigm.

## Method Summary
The study evaluates two graph-aware LLM architectures (LLAGA and GRAPHPROMPTER) on standard node classification datasets (Cora, Citeseer, PubMed). Attacks include structural (NETTACK, METAATTACK) and feature perturbation (Homoglyph, Reordering) methods. The unified attack combines both. GALGUARD defense integrates LLM-based feature correction with GNNGuard-style structural purification. Training uses frozen LLMs with learned projectors; attacks target either training (poisoning) or inference (evasion).

## Key Results
- Structural attacks are highly effective (up to 51% accuracy drop), especially against LLAGA due to its reliance on graph structure.
- Feature perturbation attacks, previously ineffective on GNNs, prove surprisingly powerful here (up to 84% accuracy drop), often surpassing structural attacks.
- Unified attacks combining feature and structural perturbations achieve near-complete model failure.
- GALGUARD defense significantly improves robustness by integrating LLM-based feature correction with adapted GNN structural defenses.

## Why This Works (Mechanism)

### Mechanism 1
LLAGA exhibits higher vulnerability to structural attacks than GRAPHPROMPTER due to direct coupling between graph structure and token sequences. LLAGA constructs fixed-length node-level sequences where each position corresponds to a specific structural role in the graph. When edges are perturbed, the resulting sequence embeddings change directly, propagating corrupted structural information to the LLM. GRAPHPROMPTER decouples textual node embeddings from structural perturbations—the GNN encoder absorbs structural noise while textual features remain unperturbed. The linear projector in LLAGA does not provide sufficient regularization to smooth over structural noise.

### Mechanism 2
Feature perturbation attacks outperform structural attacks on graph-aware LLMs because LLMs prioritize semantic content over graph topology. Imperceptible text attacks (homoglyph substitution, character reordering) corrupt the semantic encoding while appearing visually identical to humans. Since LLMs process tokenized text directly, corrupted features propagate unchanged. GNNs, by contrast, aggregate neighborhood features through message-passing, which dilutes localized feature noise—making feature attacks historically ineffective on GNNs. The LLM's tokenization and embedding layers cannot detect or correct imperceptible Unicode perturbations.

### Mechanism 3
Unified attacks combining feature and structural perturbations achieve near-complete model failure due to dual-pathway corruption. Structural perturbations corrupt neighborhood context; feature perturbations corrupt node self-representation. When applied jointly, the model loses both relational and semantic grounding simultaneously—there is no clean signal path for recovery. The model lacks any cross-modal consistency check between structure and features.

## Foundational Learning

- **Message-passing in Graph Neural Networks**: Understanding why GNNs dilute feature noise (neighborhood aggregation) explains why feature attacks were historically ineffective—and why LLMs without aggregation are vulnerable. Quick check: Given a node with 10 neighbors, if one neighbor's feature is perturbed by 10%, what is the maximum impact on the node's aggregated representation under mean aggregation?

- **Unicode normalization and text sanitization**: Imperceptible attacks exploit Unicode homoglyphs and bidirectional control characters; understanding canonical normalization (NFC, NFD) is prerequisite to building preprocessing defenses. Quick check: What is the difference between NFC and NFD normalization, and which would collapse the homoglyph "а" (Cyrillic) to Latin "a"?

- **Evasion vs. poisoning attacks**: The paper distinguishes training-time (poisoning) vs. inference-time (evasion) attacks with different effectiveness profiles; defensive strategies differ for each. Quick check: If an attacker can only modify test-time inputs but not training data, which attack class are they restricted to?

## Architecture Onboarding

- **Component map**: Graph Encoder (sequence template or GNN) -> Projector (2-layer MLP) -> Foundation LLM (Vicuna-7B/LLaMA2-7B) -> Classification output
- **Critical path**: Input graph → Graph encoder (sequence template or GNN) → Embeddings → Projector → Token-aligned representations → Frozen LLM → Classification output
- **Design tradeoffs**: Sequence templates (LLAGA) offer better clean performance but higher structural vulnerability; GNN encoder (GRAPHPROMPTER) provides higher structural robustness but lower clean performance; frozen LLM ensures computational efficiency but limits adaptation to graph domain.
- **Failure signatures**: Sudden accuracy drop (>50%) on evasion but not poisoning → structural attack exploiting sequence template; high accuracy drop (>60%) with minimal edge changes → feature perturbation attack (check for homoglyphs via Unicode inspection); near-zero accuracy with <10% edge/feature changes → unified attack.
- **First 3 experiments**: 1) Baseline vulnerability audit: Run NETTACK and METAATTACK (poisoning + evasion) on your graph-aware LLM; log accuracy drops for structural vs. feature perturbations separately to identify dominant attack pathway. 2) Feature preprocessing defense test: Apply Unicode NFC normalization + homoglyph detection as a preprocessing layer; measure reduction in Reordering/Homoglyph attack effectiveness. 3) GALGUARD component ablation: Test feature corrector alone, purification alone, and full GALGUARD; quantify marginal benefit of each component.

## Open Questions the Paper Calls Out

- How do the proposed unified adversarial attacks affect graph-aware LLMs that use "textual description" encoding rather than learned projectors?
- Does unfreezing and fine-tuning the LLM backbone mitigate the high sensitivity to feature perturbations observed in frozen models?
- Can the GALGUARD defense maintain its efficacy using smaller, open-source models as feature correctors?
- Do the identified vulnerabilities and the GALGUARD defense transfer to graph-level tasks like link prediction?

## Limitations

- Limited dataset diversity: The evaluation relies exclusively on Cora, Citeseer, and PubMed—small, standard citation networks.
- Defender dependency on black-box API: GALGUARD's feature corrector relies on GPT-4 Turbo, introducing runtime cost and latency.
- No adaptive attack evaluation: The study uses static attack budgets (10% edges/features).
- LLM architecture dependency unclear: The evaluation is limited to Vicuna-7B and Llama2-7B.

## Confidence

- **High confidence**: Structural attacks are more effective on LLAGA than GRAPHPROMPTER; feature perturbation attacks outperform structural attacks; unified attacks achieve near-complete failure.
- **Medium confidence**: GALGUARD provides consistent robustness gains; LLMs are inherently more vulnerable to feature-level attacks than GNNs.
- **Low confidence**: The observed feature attack superiority generalizes to all graph-aware LLM variants; the projector architecture alone determines robustness.

## Next Checks

1. **Dataset generalization audit**: Evaluate LLAGA and GRAPHPROMPTER on two additional graph datasets (e.g., OGB-Arxiv and Coauthor-CS) with varied density and attribute distributions. Measure whether the relative vulnerability patterns hold.

2. **Offline feature corrector benchmark**: Replace GPT-4 Turbo in GALGUARD with a distilled LLM (e.g., DistilBERT fine-tuned on text correction) and measure trade-offs between cost, latency, and robustness.

3. **Adaptive attack stress test**: Implement a simple adaptive attack where the attacker iteratively probes the model with increasing perturbation budgets until a predefined accuracy threshold is breached. Compare the final perturbation budgets required for structural vs. feature attacks.