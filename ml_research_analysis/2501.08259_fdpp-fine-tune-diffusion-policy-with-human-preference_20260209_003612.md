---
ver: rpa2
title: 'FDPP: Fine-tune Diffusion Policy with Human Preference'
arxiv_id: '2501.08259'
source_url: https://arxiv.org/abs/2501.08259
tags:
- policy
- diffusion
- learning
- reward
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Fine-tuning Diffusion Policy with Human Preference
  (FDPP), a method to adapt pre-trained diffusion policies to align with human preferences
  through preference-based reward learning and reinforcement learning. FDPP learns
  a reward function from human preference labels, then fine-tunes the pre-trained
  policy using this reward while incorporating KL regularization to preserve original
  task performance.
---

# FDPP: Fine-tune Diffusion Policy with Human Preference

## Quick Facts
- arXiv ID: 2501.08259
- Source URL: https://arxiv.org/abs/2501.08259
- Reference count: 37
- Primary result: Fine-tuning diffusion policies with human preferences using KL regularization preserves task performance while aligning behavior to preferences

## Executive Summary
FDPP introduces a method to customize pre-trained diffusion policies by incorporating human preferences through preference-based reward learning and reinforcement learning. The approach learns a reward function from human-labeled state pairs, then fine-tunes the policy using this reward while maintaining performance on the original task through KL regularization. Experiments on 2D pushing and 3D block stacking tasks demonstrate effective preference alignment across three different preference types while preserving task success rates.

## Method Summary
FDPP operates in two stages: first, it trains a reward model using Bradley-Terry preference learning on state pairs labeled by humans (1024 pairs per reward model). Second, it fine-tunes the pre-trained diffusion policy using PPO on the denoising MDP, optimizing for the learned reward while incorporating KL regularization to maintain similarity to the original policy. The KL-regularized objective balances preference alignment against preserving original task performance. The method uses a DDIM sampler with K=50 steps and η=1 for trajectory generation during fine-tuning.

## Key Results
- Successfully aligns diffusion policy behavior to human preferences across three preference types in both 2D pushing and 3D block stacking tasks
- Maintains original task performance (success rates) while satisfying preference constraints through KL regularization
- KL weight α=0.1 achieves optimal trade-off between preference alignment and task performance
- Demonstrates effectiveness of preference-based reward learning for policy customization

## Why This Works (Mechanism)
The method works by creating a feedback loop between human preferences and policy behavior. Human preferences are encoded as a reward function that guides policy updates through reinforcement learning. The KL regularization term acts as a trust region constraint, preventing catastrophic forgetting of the original task while allowing the policy to adapt to preferences. This creates a smooth transition from the pre-trained policy to a preference-aligned version without sacrificing core task capabilities.

## Foundational Learning
- **Diffusion policy fundamentals**: Stochastic trajectory generation conditioned on states; needed to understand how policies are represented and trained
- **Bradley-Terry preference learning**: Pairwise comparison modeling for reward learning; needed to understand how human preferences are converted to rewards
- **KL regularization in RL**: Information-theoretic constraint for policy updates; needed to understand how original performance is preserved
- **PPO for diffusion policies**: Actor-critic algorithm adapted for continuous denoising MDPs; needed to understand the fine-tuning procedure
- **DDIM sampling**: Deterministic diffusion sampling with noise schedule; needed to understand trajectory generation during fine-tuning

## Architecture Onboarding

**Component map**: Human labels → Reward model → PPO fine-tuning → KL-regularized objective → Updated diffusion policy

**Critical path**: Reward model training → PPO fine-tuning with KL regularization → Preference-aligned policy

**Design tradeoffs**: 
- Higher KL weight preserves task performance but reduces preference alignment
- More preference labels improve reward model accuracy but increase human effort
- DDIM sampling parameters affect trajectory quality and computational cost

**Failure signatures**: 
- Sharp drop in task success rate indicates insufficient KL regularization
- Poor preference alignment indicates weak reward model or insufficient labels
- Unstable training indicates inappropriate PPO hyperparameters

**First experiments**: 
1. Verify reward model predicts preferences correctly on held-out pairs
2. Test policy fine-tuning with varying KL weights to find optimal balance
3. Compare preference alignment metrics before and after fine-tuning

## Open Questions the Paper Calls Out

**Open Question 1**: Can Vision Language Models (VLMs) effectively replace human annotators to automatically generate preference labels from text descriptions in FDPP? The authors aim to utilize VLMs to reduce human effort, but current implementation relies entirely on manual labeling of 1024 state pairs.

**Open Question 2**: How does FDPP perform when applied to long-horizon robotic tasks on physical hardware in real-world environments? All current results are from simulated environments, with real-world deployment identified as a specific future goal.

**Open Question 3**: Can the KL regularization weight (α) be determined automatically to optimize the trade-off between preference alignment and policy performance? The authors note that choosing the appropriate KL weight is essential and aim to implement automatic hyper-parameter tuning.

**Open Question 4**: Can the FDPP framework be modified to prevent the significant increase in trajectory length observed when satisfying spatial constraints? Current results show constraint satisfaction causes roll-out lengths to double, which may be suboptimal for real-world deployment.

## Limitations
- Significant increase in trajectory length when satisfying spatial constraints (from 58.20 to 120.20 steps in PUSH-T)
- Critical hyperparameters for reward model and PPO fine-tuning remain underspecified
- Performance highly sensitive to KL regularization weight, requiring manual tuning
- All experiments conducted in simulation, not validated on physical hardware

## Confidence
- **High confidence**: Core methodology (reward learning + KL-regularized fine-tuning) is sound and reproducible in principle
- **Medium confidence**: Effectiveness of KL regularization for balancing objectives, depends on specific hyperparameter choices
- **Low confidence**: Exact implementation details needed for perfect replication, particularly reward model architecture and PPO hyperparameters

## Next Checks
1. **Reward model validation**: Test whether the preference-based reward model can accurately predict human preferences on held-out data, and whether increasing the number of preference labels beyond 1024 improves alignment quality
2. **KL regularization sensitivity**: Systematically vary the KL weight α across a wider range and measure both preference alignment and task performance to verify the optimal balance point
3. **Generalization test**: Apply FDPP to a different robotic task (e.g., pick-and-place) with a different pre-trained diffusion policy to verify the method's robustness across task domains