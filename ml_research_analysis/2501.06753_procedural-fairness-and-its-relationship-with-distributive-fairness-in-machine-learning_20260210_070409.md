---
ver: rpa2
title: Procedural Fairness and Its Relationship with Distributive Fairness in Machine
  Learning
arxiv_id: '2501.06753'
source_url: https://arxiv.org/abs/2501.06753
tags:
- fairness
- distributive
- procedural
- dataset
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the under-explored area of procedural fairness
  in machine learning (ML), which examines the fairness of the decision-making process,
  in contrast to the more commonly studied distributive fairness that focuses on decision
  outcomes. The authors propose a novel method to achieve procedural fairness during
  the ML model training phase by optimizing a procedural fairness metric, GPFF AE,
  using a regularization approach.
---

# Procedural Fairness and Its Relationship with Distributive Fairness in Machine Learning

## Quick Facts
- **arXiv ID:** 2501.06753
- **Source URL:** https://arxiv.org/abs/2501.06753
- **Reference count:** 12
- **One-line primary result:** Optimizing procedural fairness during training improves both procedural and distributive fairness metrics.

## Executive Summary
This paper addresses the under-explored area of procedural fairness in machine learning, examining the fairness of the decision-making process rather than just outcomes. The authors propose a novel method to achieve procedural fairness during model training by optimizing a procedural fairness metric (GPFF AE) using a regularization approach. Through comprehensive experiments on seven datasets, they demonstrate that this method significantly improves procedural fairness. The paper also investigates the relationship between procedural and distributive fairness, finding that dataset bias and procedural unfairness are primary sources of distributive unfairness.

## Method Summary
The method introduces a procedural fairness loss term (L_GPF) calculated as the L1 distance between gradient-based explanations of similar data points from different sensitive groups. This loss is combined with standard cross-entropy loss during training. The approach uses a 2-layer MLP with Adam optimizer, pre-computing similar pairs across sensitive groups before training. The key innovation is using attribution explanation similarity as a proxy for procedural fairness, forcing the model to adopt consistent decision logic regardless of group membership.

## Key Results
- Procedural fairness regularization significantly improves GPFF AE scores across all tested datasets
- Models trained with procedural fairness constraints achieve better distributive fairness than those optimized for distributive fairness directly
- Dataset bias combined with procedural unfairness creates distributive unfairness, while unbiased data with procedurally fair models yields distributively fair outcomes

## Why This Works (Mechanism)

### Mechanism 1: Attribution Alignment via Regularization
- **Claim:** Optimizing similarity of feature attribution explanations for matched pairs across groups forces consistent decision logic
- **Mechanism:** Introduces L_GPF loss term calculated as L1 distance between gradient-based explanations of similar data points from different groups, minimizing this alongside cross-entropy loss
- **Core assumption:** Gradient-based attributions serve as valid, differentiable proxy for model's internal decision logic during training
- **Evidence anchors:** Proposes method in abstract; details loss function in section 3.2; gradient vs SHAP comparison supported by corpus

### Mechanism 2: The Origin of Distributive Unfairness
- **Claim:** Distributive unfairness stems from superposition of dataset bias and procedural unfairness
- **Mechanism:** Procedural fairness prevents model from amplifying data biases, resulting in better distributive fairness than optimizing for distributive fairness directly
- **Core assumption:** Relationship between dataset bias, procedural fairness, and distributive fairness is approximately additive
- **Evidence anchors:** Abstract identifies primary sources; section 4.2 shows experimental results; corpus provides context on outcome-based equality

### Mechanism 3: Counterbalancing via Process Inversion
- **Claim:** Optimizing for distributive fairness on biased data forces procedurally unfair decision process favoring disadvantaged group
- **Mechanism:** To equalize positive outcome rates when data favors one group, optimization algorithm assigns positive weight to sensitive attribute for disadvantaged group
- **Core assumption:** Achieving statistical parity requires discriminatory treatment if base rates differ between groups
- **Evidence anchors:** Abstract describes this trade-off; section 4.3 visualizes SHAP value shifts; corpus echoes process vs outcome distinction

## Foundational Learning

- **Concept: Feature Attribution Explanation (FAE)**
  - **Why needed here:** Core tool to measure "procedural fairness" by quantifying feature importance for specific predictions
  - **Quick check question:** Why does the paper use "Grad" for training but "SHAP" for evaluation?

- **Concept: Group Procedural Fairness (GPF FAE)**
  - **Why needed here:** Specific metric measuring distance between explanation distributions rather than outcome distributions
  - **Quick check question:** How does GPF FAE define "similar" data points when comparing two groups?

- **Concept: The Bias-Variance-Covariance Trade-off in Fairness**
  - **Why needed here:** Paper frames problem as managing tension between accuracy, distributive fairness (DP), and procedural fairness (GPF FAE)
  - **Quick check question:** According to paper, does optimizing for distributive fairness remove bias in data or mask it?

## Architecture Onboarding

- **Component map:** Input -> Matching Module -> Backbone (MLP) -> Explainer (Grad) -> Loss Aggregator
- **Critical path:** Matching Module is operational bottleneck, must find most similar counterfactual instance for every data point before training
- **Design tradeoffs:**
  - Exact vs. Approximate Matching: O(N²) complexity requires ANN index or pre-computation
  - Grad vs. SHAP: Uses Grad for speed despite noise; accepts volatility during training
- **Failure signatures:**
  - Trivial Solution: Model learns constant attributions or predictions
  - Pair Mismatch: Poor similarity definition causes unstable training
  - Slow Convergence: Conflicting gradients cause loss plateau
- **First 3 experiments:**
  1. Pair Quality Audit: Visualize matched pairs to verify semantic similarity
  2. Loss Component Ablation: Compare baseline, DP-optimized, and procedural-optimized models
  3. Hyperparameter α Sensitivity: Sweep weighting parameter to identify knee in fairness-accuracy curve

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can multi-objective evolutionary algorithms be utilized to effectively navigate trade-offs between accuracy, procedural fairness, and distributive fairness?
- **Basis in paper:** [explicit] Conclusion states plan to use multi-objective optimization algorithms for different trade-offs
- **Why unresolved:** Current work analyzes relationships but doesn't implement framework to simultaneously optimize conflicting objectives
- **What evidence would resolve it:** Pareto front of solutions demonstrating optimal trade-off curves between accuracy, GPFF AE, and DP

### Open Question 2
- **Question:** Does combining dataset preprocessing with procedural fairness optimization reliably achieve distributive fairness without direct distributive optimization trade-offs?
- **Basis in paper:** [explicit] Section 5.2 proposes "new path" ensuring unbiased data and procedural fairness rather than optimizing distributive metrics
- **Why unresolved:** Validates components separately but doesn't verify if combination consistently yields both procedurally and distributively fair model
- **What evidence would resolve it:** Comparative experiments showing LFR + procedural fairness maintains high accuracy while achieving lower DP than distributive constraints

### Open Question 3
- **Question:** How does procedural fairness regularization perform with multiple sensitive attributes or multi-class classification?
- **Basis in paper:** [inferred] Methodology restricts to single sensitive attribute and binary classification without loss of generality
- **Why unresolved:** Paper restricts experiments to binary classification and single attributes; unclear if approach scales to complex scenarios
- **What evidence would resolve it:** Evaluation on datasets with intersecting attributes or multi-class tasks, reporting stability across all groups

## Limitations
- Distance metric for finding similar pairs across groups is not explicitly defined
- Computational feasibility questionable for large datasets due to O(N²) matching complexity
- Procedural fairness claim based on attribution similarity rather than actual decision logic examination

## Confidence
- **High Confidence:** Experimental results showing procedural fairness regularization improves procedural fairness metrics
- **Medium Confidence:** Relationship between procedural and distributive fairness requires more nuanced analysis
- **Low Confidence:** Generalizability across different model architectures and complex fairness scenarios remains untested

## Next Checks
1. **Reproduce the distance metric:** Implement similarity matching using different distance metrics (Euclidean, cosine, Mahalanobis) to test sensitivity
2. **Scale experiment:** Test approach on medium-sized dataset (10k-50k samples) to measure computational costs and identify bottlenecks
3. **Causal validation:** Beyond attribution similarity, examine whether model uses same decision logic across groups by analyzing feature importance stability and counterfactual interventions on protected attributes