---
ver: rpa2
title: 'MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents'
arxiv_id: '2508.13186'
source_url: https://arxiv.org/abs/2508.13186
tags:
- reasoning
- zhang
- agents
- checklist
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MM-BrowseComp is a novel benchmark for multimodal browsing agents,
  addressing the gap left by text-only benchmarks like BrowseComp. It consists of
  224 challenging questions that require agents to search and reason with multimodal
  content such as images and videos embedded in web pages.
---

# MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents

## Quick Facts
- **arXiv ID**: 2508.13186
- **Source URL**: https://arxiv.org/abs/2508.13186
- **Reference count**: 40
- **Primary result**: 224-question benchmark reveals current agents achieve <10% accuracy on multimodal browsing tasks, with visual content proving significantly harder than text

## Executive Summary
MM-BrowseComp introduces the first comprehensive benchmark for evaluating multimodal browsing agents that must search and reason with web content containing images and videos. The benchmark consists of 224 challenging questions requiring agents to perform multi-step web searches and extract information from multimodal sources. Unlike existing text-only benchmarks, MM-BrowseComp includes a verified checklist for each question to enable fine-grained evaluation of reasoning processes. Comprehensive experiments demonstrate that even state-of-the-art models struggle significantly on this task, with OpenAI o3 achieving only 29.02% overall accuracy while other models remain below 10%.

## Method Summary
The benchmark evaluates multimodal browsing agents using 224 questions spanning 22 subtasks, with 57% of questions containing images in the prompt. Each question includes a verified reasoning checklist (text, image, and video items) for fine-grained evaluation. Tool-free VLMs are evaluated using direct API calls with temperature=1.0 and top_p=1.0, while tool-augmented VLMs use official web interfaces. Open-source agents (Agent-R1, OWL, DeerFlow, WebDancer) are evaluated on a 54-instance subset. Responses are evaluated using GPT-4o-2024-11-20 with a structured prompt extracting CHECKLIST_SCORE, CHECKLIST_RESULT vector, and OVERALL_CORRECTNESS.

## Key Results
- OpenAI o3 achieves only 29.02% overall accuracy, while other models remain below 10%
- Visual checklist items show significantly worse performance than text items across all models
- Native multimodal reasoning (o3) outperforms captioning-based approaches, suggesting information loss as a key bottleneck
- Neither strong reasoning nor comprehensive tools alone suffice—both are required for high performance

## Why This Works (Mechanism)

### Mechanism 1: Native Multimodal Integration
- Claim: Native multimodal integration preserves more information than captioning-based approaches
- Mechanism: Agents that can directly process images within their reasoning loop avoid information loss from captioning tools
- Core assumption: Information loss from captioning is a primary bottleneck
- Evidence: OpenAI o3 autonomously downloads and analyzes images locally, outperforming captioning-based agents

### Mechanism 2: Reasoning-Tool Synergy
- Claim: High performance requires both strong reasoning and comprehensive tooling
- Mechanism: Models with either strong reasoning or rich tools alone show minimal improvement
- Core assumption: The bottleneck is multiplicative, not additive
- Evidence: Gemini-2.5-Pro with limited tools shows minimal improvement over tool-free baselines

### Mechanism 3: Reflective Agent Robustness
- Claim: ReAct paradigm agents are more robust than orchestrated multi-agent frameworks
- Mechanism: Unified architecture prevents systemic failure when single sub-agents fail
- Core assumption: Error propagation in multi-agent systems outweighs specialization benefits
- Evidence: Agent-R1 (reflective) achieves 5.56% OA vs OWL's comparable configurations

## Foundational Learning

- **Multimodal Retrieval-Augmented Generation**: Why needed here - The benchmark requires agents to reason with images/videos embedded in web pages. Quick check: Can you explain why a text-only RAG system would fail on a question asking about the color of gems on a video game character's weapon?

- **ReAct Agent Architecture**: Why needed here - The paper shows agents interleaving reasoning and action outperform those with separated planning. Quick check: What is the difference between an agent that plans entirely upfront vs. one that interleaves thought, action, and observation?

- **Strict vs. Loose Evaluation Metrics**: Why needed here - Strict Accuracy requires correct answer AND complete checklist to distinguish genuine reasoning from lucky guessing. Quick check: Why does Overall Accuracy increase with more sampling attempts while Strict Accuracy remains flat?

## Architecture Onboarding

- **Component map**: Backbone VLM (reasoning engine) → Tool orchestration layer → Multimodal tools (search, browser, image/video analyzers, PDF) → Context management

- **Critical path**: 1) Query analysis (may include input image) → 2) Tool selection and invocation (search/browse) → 3) Multimodal content extraction from web pages → 4) Checklist-style reasoning chain construction → 5) Answer synthesis with verification

- **Design tradeoffs**: Reflective (ReAct) vs. Orchestrated - Reflective is more robust but may lack specialization; Orchestrated can fail catastrophically on single sub-agent errors. Native multimodal vs. Captioning tools - Native preserves information but requires capable VLM; Captioning adds latency and information loss but works with text-only LLMs. Test-time compute vs. Base capability - Additional sampling improves OA but not SA—indicates surface-level gains, not deeper reasoning.

- **Failure signatures**: Visual hallucination (9-23% of errors) - Model describes non-existent visual elements. Tool execution failure (up to 19%) - CAPTCHAs, timeouts, blocking. Confirmation bias - Early plausible answer terminates search prematurely. Knowledge override - Model ignores visual evidence in favor of parametric knowledge.

- **First 3 experiments**: 1) Establish baseline with tool-free VLM to measure intrinsic knowledge; expect <10% OA. 2) Add browsing tools to same VLM; compare OA improvement—if minimal, indicates tool integration gap. 3) Evaluate checklist completion rate separately for text vs. image/video items to diagnose modality-specific weaknesses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agent architectures be redesigned to achieve native multimodal reasoning that processes images as equal information sources to text?
- Basis: The authors identify captioning tools as causing "significant information loss and hallucinations" while o3 achieves native multimodal capabilities
- Why unresolved: The paper identifies this as a critical bottleneck but does not propose specific architectural solutions
- Evidence needed: Comparative study of native multimodal agent architectures versus captioning-based approaches

### Open Question 2
- Question: Can test-time scaling strategies be developed that improve strict reasoning accuracy rather than merely increasing the probability of guessing correctly?
- Basis: The authors find that "the SA exhibits only marginal gains from increased test-time compute" while OA improves
- Why unresolved: Current sampling and aggregation strategies do not address the fundamental limitation that models lack robust reasoning processes
- Evidence needed: Demonstration of a test-time scaling method that achieves proportional improvements in Strict Accuracy alongside Overall Accuracy

### Open Question 3
- Question: How can the checklist-based evaluation framework be leveraged to provide dense reward signals for training multimodal browsing agents via reinforcement learning?
- Basis: The conclusion states that "the checklist's granular structure is particularly promising for agent training"
- Why unresolved: The paper introduces checklists for evaluation purposes only; application to training remains unexplored
- Evidence needed: An RL training pipeline using checklist completion as intermediate rewards

## Limitations
- The benchmark's claims about visual reasoning superiority rely heavily on OpenAI o3's performance, which is not independently reproducible
- Dataset construction process (manual question generation and verification) introduces potential selection bias
- Closed nature of top-performing models limits verification of architectural advantages

## Confidence

**Major Uncertainties:**
- Benchmark claims rely heavily on OpenAI o3's performance
- Dataset construction introduces potential selection bias
- Closed nature of top-performing models limits verification

**Confidence Assessment:**
- **High Confidence**: The benchmark successfully identifies multimodal browsing as a challenging frontier where current agents struggle
- **Medium Confidence**: The mechanism claiming native multimodal integration is superior to captioning tools is plausible but not definitively proven
- **Low Confidence**: The claim that reflective agents are inherently more robust than orchestrated systems lacks sufficient comparative evidence

## Next Checks
1. Replicate core findings using open-source multimodal models (e.g., GPT-4o, Claude 3.5) to determine if o3's advantage is model-specific
2. Conduct ablation studies where captioning-based agents use higher-quality vision models to isolate the information loss effect
3. Perform error analysis on visual vs. text checklist items across multiple agent types to quantify the modality-specific reasoning gap