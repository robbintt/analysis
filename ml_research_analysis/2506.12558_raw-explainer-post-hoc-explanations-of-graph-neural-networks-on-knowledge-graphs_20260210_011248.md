---
ver: rpa2
title: 'RAW-Explainer: Post-hoc Explanations of Graph Neural Networks on Knowledge
  Graphs'
arxiv_id: '2506.12558'
source_url: https://arxiv.org/abs/2506.12558
tags:
- graph
- explanations
- knowledge
- explanation
- subgraphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes RAW-Explainer, a method to generate connected
  subgraph explanations for link prediction in knowledge graphs. Unlike previous approaches
  that focus on homogeneous graphs or return disconnected subgraphs, RAW-Explainer
  leverages a random walk mechanism to ensure connectivity and uses a neural network
  to parameterize the explanation generation process, making it scalable.
---

# RAW-Explainer: Post-hoc Explanations of Graph Neural Networks on Knowledge Graphs
## Quick Facts
- arXiv ID: 2506.12558
- Source URL: https://arxiv.org/abs/2506.12558
- Reference count: 21
- Primary result: Generates connected subgraph explanations for link prediction in knowledge graphs using random walk and neural network parameterization, outperforming baselines on MRR while maintaining computational efficiency.

## Executive Summary
RAW-Explainer addresses a critical gap in explainability for graph neural networks operating on knowledge graphs by generating connected subgraph explanations for link prediction tasks. Unlike prior approaches that either focus on homogeneous graphs or return disconnected subgraphs, RAW-Explainer employs a random walk mechanism to ensure connectivity while using a neural network to parameterize the explanation generation process for scalability. The method also introduces a robust evaluator with distance-based edge-dropping to handle distribution shift when evaluating subgraphs significantly smaller than the original graph.

The approach demonstrates superior performance over baselines like PaGE-Link and Power-Link on MRR metrics while maintaining computational efficiency. By producing interpretable, connected explanations, RAW-Explainer advances the field of post-hoc explainability for GNNs on knowledge graphs, addressing both the connectivity and scalability challenges that have limited previous methods.

## Method Summary
RAW-Explainer generates connected subgraph explanations for link prediction in knowledge graphs through a novel combination of random walk mechanisms and neural network parameterization. The method uses random walks to ensure that generated subgraphs remain connected, addressing a fundamental limitation of previous approaches that often produced disconnected explanations. A neural network parameterizes the explanation generation process, enabling scalability to larger graphs compared to traditional methods. To evaluate the quality of these explanations, RAW-Explainer employs a robust evaluator trained with distance-based edge-dropping, which specifically addresses the distribution shift problem that occurs when evaluating much smaller subgraphs compared to the original knowledge graph.

## Key Results
- Outperforms baselines (PaGE-Link, Power-Link) on MRR metrics for link prediction explainability
- Generates connected subgraphs as explanations, unlike previous disconnected approaches
- Maintains computational efficiency through neural network parameterization
- Robust evaluator with distance-based edge-dropping addresses distribution shift issues

## Why This Works (Mechanism)
The effectiveness of RAW-Explainer stems from its dual approach to addressing key challenges in KG explainability. The random walk mechanism ensures subgraph connectivity by design, preventing the disconnected components that plague many explanation methods. Meanwhile, the neural network parameterization enables efficient scaling to larger graphs by learning to generate explanations rather than computing them through expensive graph algorithms. The robust evaluator with distance-based edge-dropping specifically targets the evaluation challenge that arises when comparing small explanation subgraphs to the full knowledge graph, maintaining evaluation quality despite the significant size difference.

## Foundational Learning
- Random walk mechanisms on graphs: Why needed - Ensures connectivity in generated subgraphs; Quick check - Verify random walk maintains connectivity across different graph topologies
- Neural network parameterization for graph algorithms: Why needed - Enables scalable explanation generation; Quick check - Confirm parameter count scales sublinearly with graph size
- Distance-based edge-dropping strategies: Why needed - Addresses distribution shift in evaluation; Quick check - Test evaluator performance with varying edge-dropping rates
- Post-hoc explanation methods for GNNs: Why needed - Provides interpretability without retraining; Quick check - Validate explanations align with known graph patterns
- Knowledge graph link prediction: Why needed - Target task for explanation generation; Quick check - Ensure explanations improve prediction confidence metrics

## Architecture Onboarding
Component map: Knowledge Graph -> Random Walk Generator -> Neural Network Parameterizer -> Connected Subgraph Explanation -> Robust Evaluator (with Edge-Dropping) -> MRR Score

Critical path: The most critical path involves the random walk generator feeding into the neural network parameterizer, as this combination ensures both connectivity and scalability. The robust evaluator with distance-based edge-dropping forms a secondary critical path for ensuring reliable evaluation despite distribution shift.

Design tradeoffs: The method trades some precision in explanation quality for significant gains in computational efficiency and guaranteed connectivity. The random walk approach may miss some optimal but disconnected explanations, while the neural parameterization sacrifices some interpretability of the generation process for scalability.

Failure signatures: Explanations may become too generic if the random walk parameters are not tuned properly, leading to loss of specificity. The neural network may fail to capture complex graph structures if underparameterized. The robust evaluator could produce inconsistent results if the edge-dropping strategy doesn't match the target distribution.

First experiments:
1. Test random walk connectivity preservation on graphs with varying diameter and clustering coefficients
2. Evaluate neural network parameterizer performance on incrementally larger knowledge graphs
3. Benchmark robust evaluator against standard evaluators using synthetic distribution shift scenarios

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Generalizability beyond link prediction tasks remains untested, potentially requiring mechanism adaptation
- Scalability claims lack validation on industrial-scale graphs with millions of nodes and edges
- Evaluation robustness depends heavily on the specific edge-dropping strategy chosen, which may affect consistency

## Confidence
- Generalizability claims: Medium
- Scalability assertions: Low
- Evaluation robustness: Medium

## Next Checks
1. Test RAW-Explainer on a benchmark dataset with 10M+ edges to evaluate true scalability and computational efficiency compared to existing methods.
2. Apply the method to node classification tasks on knowledge graphs and assess whether the random walk mechanism requires modification for non-link prediction scenarios.
3. Conduct ablation studies on the edge-dropping strategy in the robust evaluator, testing different dropping patterns and their impact on evaluation consistency and final explanation quality metrics.