---
ver: rpa2
title: 'BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains'
arxiv_id: '2510.25409'
source_url: https://arxiv.org/abs/2510.25409
tags:
- uni00000044
- uni00000051
- uni00000048
- uni00000003
- uni0000004c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BhashaBench V1 is a comprehensive, domain-specific, bilingual benchmark
  designed to evaluate large language models on India-centric knowledge systems across
  Agriculture, Legal, Finance, and Ayurveda domains. It contains 74,166 question-answer
  pairs (52,494 in English, 21,672 in Hindi) curated from authentic government and
  professional examination papers, spanning 90+ subdomains and 500+ topics.
---

# BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains

## Quick Facts
- **arXiv ID:** 2510.25409
- **Source URL:** https://arxiv.org/abs/2510.25409
- **Reference count:** 40
- **Primary result:** 29+ LLMs evaluated on India-centric knowledge systems, showing significant performance disparities across domains (76.49% in Legal vs 59.74% in Ayurveda) and languages, with top models like GPT-4o and Qwen3-235B-A22B-Instruct leading but struggling with traditional knowledge systems.

## Executive Summary
BhashaBench V1 is a comprehensive, domain-specific, bilingual benchmark designed to evaluate large language models on India-centric knowledge systems across Agriculture, Legal, Finance, and Ayurveda domains. It contains 74,166 question-answer pairs (52,494 in English, 21,672 in Hindi) curated from authentic government and professional examination papers, spanning 90+ subdomains and 500+ topics. Evaluation of 29+ LLMs reveals significant performance disparities across domains and languages, with models achieving 76.49% accuracy in Legal but only 59.74% in Ayurveda, and consistently performing better on English content compared to Hindi. Top models like GPT-4o and Qwen3-235B-A22B-Instruct demonstrate strong performance in domains requiring mathematical reasoning, while struggling with traditional knowledge systems and low-resource language contexts.

## Method Summary
BhashaBench V1 employs zero-shot evaluation of LLMs on 74,166 bilingual QA pairs across four India-centric domains (Agriculture/BBK, Legal/BBL, Finance/BBF, Ayurveda/BBA) with 91 subdomains and 500+ topics. The benchmark uses authentic exam questions from 40+ government sources (1995-2025) with six formats including MCQ, Assertion/Reasoning, and Fill-in-the-Blanks. Open-source models are evaluated via log-likelihood scoring using LM-EVALUATION-HARNESS, while proprietary models use API-based generative approaches with temperature=0. Accuracy is computed per domain, subdomain, language, difficulty level, and question type. The evaluation pipeline involves data loading, zero-shot assessment using standardized prompt templates, metric calculation, and comprehensive slicing by various dimensions to identify specific weaknesses.

## Key Results
- Models achieve 76.49% accuracy in Legal domain but only 59.74% in Ayurveda domain
- Consistent English-Hindi performance gap across all domains, with models performing better on English
- Top models (GPT-4o, Qwen3-235B-A22B-Instruct) excel in mathematical reasoning domains but struggle with traditional knowledge systems
- Subdomain analysis reveals Cyber Law and International Finance performing well while Panchakarma, Seed Science, and Human Rights remain notably weak

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its use of authentic, professional-grade examination questions that ensure cultural and contextual relevance to India. Zero-shot evaluation tests inherent model knowledge rather than task-specific adaptation, providing a pure measure of pre-trained understanding. The bilingual structure (English/Hindi) captures India's major linguistic contexts while the six question formats (MCQ, Assertion/Reasoning, Fill-in-the-Blanks, Match the Column, Reading Comprehension, Rearrange the Sequence) test diverse cognitive abilities. The comprehensive coverage across 90+ subdomains and 500+ topics ensures robust assessment of domain-specific expertise rather than surface-level knowledge.

## Foundational Learning

- **Concept:** Zero-Shot Evaluation
  - **Why needed here:** BhashaBench evaluates models without providing examples, testing their inherent, pre-trained knowledge of the Indian domains. This measures true understanding rather than rapid task-learning.
  - **Quick check question:** How would the evaluation results differ if you used a 5-shot prompting setup instead of zero-shot?

- **Concept:** Data Curation from Authentic Sources
  - **Why needed here:** The benchmark's validity hinges on its data. Unlike synthetic datasets, these questions are from real exams, ensuring they test practical, professional-grade knowledge that is culturally and contextually relevant to India.
  - **Quick check question:** What potential biases might be introduced by sourcing all questions from formal examination systems?

- **Concept:** Model Stratification by Scale
  - **Why needed here:** The results clearly separate models into performance tiers based on parameter count. Understanding this helps set realistic expectations for a model's capability based on its size and informs decisions about computational budget.
  - **Quick check question:** Why might a smaller, instruction-tuned model sometimes outperform a larger base model on specific subdomains?

## Architecture Onboarding

- **Component map:** BhashaBench JSON/CSV files (74,166 QA pairs) → LM-EVALUATION-HARNESS (open-source) or custom API calls (proprietary) → log-likelihood/generative scoring → accuracy metrics by domain, subdomain, language, difficulty → analysis reports

- **Critical path:**
    1.  **Data Loading:** Ingest BhashaBench JSON/CSV files, ensuring correct mapping for each domain and split (English/Hindi)
    2.  **Evaluation Pipeline:** Configure LM-eval harness or API script. Ensure prompt template matches reported baselines
    3.  **Metric Calculation:** Run zero-shot evaluation. Compute accuracy as percentage where model's top-log-probability option matches gold answer
    4.  **Analysis:** Slice results by domain, subdomain, difficulty, and language to identify specific weaknesses

- **Design tradeoffs:**
    - **Authentic vs. Synthetic:** Real exam questions ensure relevance but may contain biases or ambiguities. Synthetic questions can be perfectly clear but may lack real-world nuance.
    - **English vs. Hindi:** Current benchmark is bilingual. Tradeoff exists between depth in these two languages and breadth across other official Indian languages.
    - **MCQ-Dominance:** Benchmark is >90% multiple-choice questions. Allows automated, scalable evaluation but may not capture open-ended reasoning or generative capabilities.

- **Failure signatures:**
    - **Consistent Low Performance on a Subdomain:** If a model scores <40% on a specific subdomain (e.g., Panchakarma), it indicates fundamental lack of knowledge, not formatting issue
    - **Large English-Hindi Gap:** Significant drop in performance on Hindi questions signals poor multilingual transfer or representation in model's pre-training data
    - **Instruction Tuning Regression:** If instruct-tuned model performs worse than base version, it may indicate instruction-tuning data poisoned domain knowledge or introduced undesirable bias

- **First 3 experiments:**
    1.  **Establish a Baseline:** Run zero-shot evaluation of target model (or Llama-3.1-8B-Instruct) on full BhashaBench suite to get comprehensive baseline score
    2.  **Analyze Subdomain Failure:** Identify top 3 and bottom 3 performing subdomains. Qualitatively analyze sample of failed questions from bottom subdomains to determine root cause
    3.  **Cross-Lingual Transfer Test:** Select 50 parallel questions from English and Hindi. Compare model performance on both. If gap is large, experiment with prompting model in English to answer Hindi question to diagnose if issue is linguistic or knowledge-based

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does LLM performance vary across BhashaBench domains when extended to Indic languages other than Hindi?
- **Basis in paper:** [explicit] The authors explicitly state in Appendix A (Limitations and Biases) that the current evaluation cannot capture the full linguistic diversity and that "Future iterations will expand to include additional Indian languages to enhance coverage."
- **Why unresolved:** The current benchmark is restricted to English and Hindi, leaving performance gap for the other 20+ scheduled Indian languages (e.g., Tamil, Bengali, Telugu) unknown.
- **What evidence would resolve it:** Evaluation results of state-of-the-art models on the BhashaBench dataset translated or curated into major regional Indic languages.

### Open Question 2
- **Question:** What are the primary drivers of the consistent performance gap between English and Hindi across all four domains?
- **Basis in paper:** [inferred] The paper reports that "Models consistently perform better on English content compared to Hindi across all domains," but results don't isolate whether gap is caused by tokenizer inefficiency, limited Hindi pre-training data, or complexity of domain-specific translation.
- **Why unresolved:** Paper quantifies gap but doesn't perform ablation studies to determine specific architectural or data-centric factors responsible for lower Hindi scores.
- **What evidence would resolve it:** Error analysis comparing model performance on Hindi questions versus their English-translated counterparts, alongside analysis of tokenization compression ratios for domain-specific terminology in both languages.

### Open Question 3
- **Question:** Can specialized pre-training on Indic-domain corpora close the performance gap between "high-resource" domains (Legal) and "low-resource" domains (Ayurveda)?
- **Basis in paper:** [inferred] Authors highlight significant disparity where GPT-4o achieves 76.49% in Legal but only 59.74% in Ayurveda, noting "urgent need for specialized model development strategies."
- **Why unresolved:** Unclear if lower scores in domains like Ayurveda and Agriculture are strictly due to lack of training data or if reasoning required for traditional knowledge systems is fundamentally more challenging for current LLM architectures.
- **What evidence would resolve it:** Comparing baseline models against versions continually pre-trained on specific Indic-domain corpora (e.g., Agri-Param or Ayur-Param models mentioned in Related Work) specifically using BhashaBench evaluation metrics.

## Limitations
- Benchmark scope limited to four domains and bilingual coverage (English/Hindi), potentially missing nuances in other official Indian languages
- MCQ-dominated format (>90%) limits assessment of generative capabilities and open-ended reasoning
- Reliance on formal examination papers may introduce systemic biases toward standardized knowledge while underrepresenting practical, contextual understanding

## Confidence
- **High Confidence:** Benchmark construction methodology, data curation from authentic sources, and evaluation pipeline using lm-evaluation-harness are well-documented and reproducible
- **Medium Confidence:** Cross-model comparisons are reliable within defined size categories, but absolute performance rankings may shift with different prompting strategies or evaluation conditions not fully specified
- **Low Confidence:** Generalizability beyond four domains remains uncertain, and benchmark may not fully capture breadth of India-specific knowledge required for practical applications

## Next Checks
1. **Cross-Lingual Transfer Validation:** Select 100 parallel English-Hindi question pairs from different subdomains and evaluate whether performance gaps persist when models are prompted in English to answer Hindi questions, distinguishing linguistic from knowledge-based failures
2. **Open-Ended Reasoning Extension:** Convert 200 representative MCQ questions to open-response format and evaluate top-performing models on their ability to generate answers rather than select from options, measuring format-dependent performance drop
3. **Domain Expansion Pilot:** Curate small validation set (100 questions) from additional Indian domain (e.g., Education or Healthcare) and assess whether models that perform well on BhashaBench V1 maintain similar accuracy, testing domain transfer capabilities