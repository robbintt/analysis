---
ver: rpa2
title: 'LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language
  Model Hallucinations with External Knowledge'
arxiv_id: '2504.21132'
source_url: https://arxiv.org/abs/2504.21132
tags:
- data
- tools
- have
- agent
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the LLM-ENHANCER system, a merged approach
  using vector embeddings to reduce hallucinations in large language models by integrating
  external knowledge from sources like Google, Wikipedia, and DuckDuckGo. The system
  operates in parallel, combining data from multiple sources, splitting it into chunks,
  and using vector embeddings to identify the most relevant information for the LLM.
---

# LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge

## Quick Facts
- arXiv ID: 2504.21132
- Source URL: https://arxiv.org/abs/2504.21132
- Reference count: 22
- Primary result: LLM-ENHANCER achieves 5.2% improvement in F1 score over sequential tool usage and 32.4% over GPT-3.5 Turbo on recent data

## Executive Summary
This paper introduces LLM-ENHANCER, a merged approach using vector embeddings to reduce hallucinations in large language models by integrating external knowledge from multiple online sources. The system operates in parallel, combining data from Google, Wikipedia, and DuckDuckGo, splitting it into chunks, and using vector embeddings to identify the most relevant information for the LLM. The approach significantly outperforms sequential tool usage and improves accuracy on both recent and older datasets while enabling accurate responses without extensive retraining.

## Method Summary
LLM-ENHANCER uses LangChain to query Google, Wikipedia, and DuckDuckGo in parallel, merges results, splits text into chunks (size 400, overlap 100), embeds them using all-MiniLM-L6-v2, stores in ChromaDB, retrieves top-10 relevant chunks, and feeds to Mistral 7B for generation. The system is evaluated on WikiQA and a custom Dataset2023-24 using F1-score, precision, and recall metrics with an Answer Comparator based on sentiment analysis.

## Key Results
- LLM-ENHANCER achieves 5.2% improvement in F1 score over sequential tool usage
- 32.4% improvement over GPT-3.5 Turbo on recent data (2023-2024)
- 19% improvement in F1 score (from 0.58 to 0.77) on older WikiQA dataset
- Enables open-source models like Mistral 7B to achieve F1=0.85 without extensive retraining

## Why This Works (Mechanism)

### Mechanism 1: Parallel Source Merging
Merging data from multiple homogeneous sources in parallel improves retrieval relevance over sequential tool selection. Instead of an LLM agent choosing one tool based on description, the system executes all search tools simultaneously, concatenates results, and uses vector similarity to surface the most relevant passages. Relevant information is distributed across sources; aggregating increases the probability that at least one source contains the correct answer.

### Mechanism 2: Vector Embedding-Based Relevance Filtering
Storing merged chunks as embeddings and retrieving top-k by similarity reduces hallucinations by grounding responses in external evidence. Text is split into chunks, embedded via Sentence Transformers, stored in ChromaDB, and the top 10 chunks most similar to the query are passed to the LLM as context. The embedding model captures semantic relevance sufficiently to rank chunks accurately; retrieval quality determines answer quality.

### Mechanism 3: Plug-and-Play Augmentation Without Retraining
External knowledge integration via retrieval avoids the cost and complexity of fine-tuning LLM parameters. The system treats the LLM as frozen; all adaptation occurs through context injection of retrieved chunks. This preserves response naturalness while improving factual grounding. The LLM can effectively use provided context without instruction tuning for RAG-style tasks.

## Foundational Learning

- **Vector Embeddings and Semantic Search**: Core to LLM-ENHANCER; determines which chunks get retrieved. Quick check: Can you explain why cosine similarity on embeddings approximates semantic relevance?

- **RAG (Retrieval-Augmented Generation)**: The system is a RAG variant with multi-source parallel retrieval. Quick check: How does RAG differ from fine-tuning in how it incorporates new knowledge?

- **LangChain Agents and Tool Selection**: The baseline sequential approach uses agents; understanding the contrast clarifies the merged approach's advantage. Quick check: What is the role of the AgentExecutor in deciding which tool to call?

## Architecture Onboarding

- **Component map**: User query → ZeroShot React Agent → Parallel searches (Google, Wikipedia, DuckDuckGo) → Merged output → RecursiveCharacterTextSplitter → ChromaDB (with all-MiniLM-L6-v2 embeddings) → Top-10 retrieved chunks + query → Mistral 7B → Response

- **Critical path**: Query triggers parallel searches across all three tools → Results concatenated → Split into overlapping chunks → Chunks embedded and stored in ChromaDB → Top-10 chunks retrieved by similarity → LLM generates answer conditioned on retrieved context

- **Design tradeoffs**: Accuracy vs. latency (merged approach is slower but more accurate); Token efficiency vs. coverage (merging uses more tokens but reduces source-selection errors); Chunk size (400) and overlap (100) balance context coherence with retrieval granularity

- **Failure signatures**: High latency (>2 min) indicates embedding bottleneck; Low precision on domain queries suggests embedding model misalignment; Answer ignores retrieved context means LLM may not be following RAG prompts

- **First 3 experiments**: Ablate single sources to quantify per-source contribution; Vary chunk parameters (sizes 200-800, overlaps 0-200) on F1 score and latency; Swap embedding model (all-MiniLM-L6-v2 vs all-mpnet-base-v2) on retrieval quality and answer accuracy

## Open Questions the Paper Calls Out

### Open Question 1
What are the optimal chunk size and overlap parameters for maximizing retrieval accuracy while minimizing latency in the LLM-ENHANCER pipeline? The paper used fixed parameters (chunk_size=400, chunk_overlap=100, min_chunks=10) without systematic ablation. Evidence would require a grid search over chunk sizes (e.g., 100-800) and overlaps (e.g., 0-200) reporting F1-score and execution time.

### Open Question 2
How much accuracy and latency improvement can be achieved by replacing the default "all-MiniLM-L6-v2" embedding model with larger, more recent embedding models? Only one embedding model was tested; the trade-off between embedding quality and speed remains unexplored. Evidence would require comparative evaluation with models like bge-large-en, e5-large, or OpenAI embeddings, measuring F1-score and per-query embedding time.

### Open Question 3
Can the token consumption of the merged approach be reduced (currently ~170 tokens per query vs. ~20 for sequential) without degrading F1-score? Limitations highlight token size as a main drawback. Evidence would require implementing relevance-based chunk filtering or query-aware summarization before LLM input, then comparing token counts and F1-scores.

### Open Question 4
Does scaling beyond three merged sources yield diminishing returns or improved robustness for real-time question answering? Only Google, DuckDuckGo, and Wikipedia were merged; the approach's scalability and redundancy benefits with additional sources remain untested. Evidence would require adding 2-3 additional sources (e.g., Bing, news APIs) and measuring F1-score changes.

## Limitations
- Missing code and dataset availability, particularly the custom Dataset2023-24 (500 recent QA pairs)
- Higher token usage (170 vs. 20-50) and processing time due to vector embedding operations
- Assumes relevant information is distributed across sources and that the chosen embedding model captures semantic relevance accurately, but these assumptions are not rigorously validated across different domains

## Confidence
- **High Confidence**: Architectural design and implementation details are well-specified, including chunk sizes (400), overlap (100), and use of ChromaDB with all-MiniLM-L6-v2 embeddings
- **Medium Confidence**: Performance improvements are based on authors' datasets and evaluation framework, but independent validation is limited by missing data and code
- **Low Confidence**: Generalizability to other domains beyond tested WikiQA and recent news datasets is unknown

## Next Checks
1. **Independent Dataset Evaluation**: Recreate evaluation using publicly available QA dataset (e.g., Natural Questions or SQuAD) to verify 32.4% improvement claim independently
2. **Component Ablation Study**: Systematically test each source (Google, Wikipedia, DuckDuckGo) individually and in combinations to quantify marginal contribution
3. **Embedding Model Sensitivity Analysis**: Replace all-MiniLM-L6-v2 with alternative embedding models (e.g., all-mpnet-base-v2 or domain-specific embeddings) to assess retrieval quality robustness