---
ver: rpa2
title: 'Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer'
arxiv_id: '2505.09114'
source_url: https://arxiv.org/abs/2505.09114
tags:
- counterfactual
- action
- crdt
- learning
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CRDT addresses the challenge of improving Decision Transformer
  performance when optimal training data is scarce or underrepresented. The core idea
  is to enable counterfactual reasoning by learning to predict action probabilities
  and outcomes, then generating counterfactual experiences that improve the agent's
  generalization.
---

# Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer

## Quick Facts
- arXiv ID: 2505.09114
- Source URL: https://arxiv.org/abs/2505.09114
- Reference count: 40
- Decision Transformer improved by 3.5% on locomotion tasks and 2.7% on Ant tasks using counterfactual reasoning

## Executive Summary
CRDT enhances Decision Transformer performance when optimal training data is scarce or underrepresented by enabling counterfactual reasoning. The framework generates synthetic experiences that improve generalization through a three-step process: training Treatment and Outcome models, generating counterfactual experiences via Counterfactual Action Selection and Filtering, and retraining the DT agent with this augmented data. Experiments demonstrate consistent improvements across Atari and D4RL benchmarks, particularly in data-limited scenarios, and enable trajectory stitching without architectural modifications.

## Method Summary
CRDT introduces a three-step framework to improve Decision Transformer performance through counterfactual reasoning. First, Treatment and Outcome transformer models are trained on the environment dataset to estimate action probabilities and potential outcomes respectively. Second, counterfactual experiences are generated by identifying outlier actions (low probability or extreme values) and filtering them based on predicted return and uncertainty estimation using dropout variance. Third, the main Decision Transformer agent is trained on a mixture of factual and filtered counterfactual experiences, enabling improved generalization and trajectory stitching capabilities. The framework maintains the original DT architecture while augmenting the training data with synthetic high-return trajectories.

## Key Results
- CRDT achieves 3.5% performance gain on locomotion tasks and 2.7% on Ant tasks compared to baseline DT
- With only 10% of training data, CRDT maintains 75% performance versus DT's 10% drop on Maze2d tasks
- Enables trajectory stitching, achieving 90% success rate in a toy environment versus DT's 40%
- Shows robust generalization to modified environment dynamics across benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Action Selection
Low-probability actions from the behavioral distribution, when evaluated for outcomes, can reveal higher-return alternatives absent from the dataset. The Treatment Model estimates conditional action distributions, and Counterfactual Action Selection identifies outlier actions below a probability threshold (discrete) or outside Gaussian bounds (continuous) for evaluation by the Outcome Model.

### Mechanism 2: Uncertainty-Aware Filtering
Filtering counterfactual candidates by predicted return and uncertainty prevents noisy or harmful synthetic experiences from degrading policy learning. Counterfactual Action Filtering retains actions yielding lower returns-to-go (higher return) and rejects trajectories where accumulated state prediction variance exceeds threshold α, using dropout-based uncertainty estimation across multiple forward passes.

### Mechanism 3: Data-Augmented Policy Learning
Training the Decision Transformer on a mixture of factual and filtered counterfactual experiences enables trajectory stitching without architecture changes. Successful counterfactual trajectories populate a buffer that is used alongside the original dataset, implicitly creating "stitched" trajectories by following higher-return branches at trajectory intersections.

## Foundational Learning

- **Decision Transformer basics**: Understanding returns-to-go conditioning and sequence modeling is prerequisite since CRDT builds on DT as its backbone.
  - Quick check: Can you explain why DT uses returns-to-go g_t rather than immediate rewards as input?

- **Potential Outcomes Framework**: CRDT's counterfactual reasoning is grounded in PO framework assumptions (consistency, sequential overlap, sequential ignorability).
  - Quick check: What does sequential overlap guarantee in the context of offline RL datasets?

- **Transformer sequence modeling**: All three models (T, O, M) use transformer architectures; understanding attention over trajectory history is essential.
  - Quick check: How does the transformer attend across timesteps in a trajectory τ = (g1, s1, a1, ..., gT, sT, aT)?

## Architecture Onboarding

- **Component map**: 
  - Treatment Model T: Transformer estimating action distribution
  - Outcome Model O: Transformer with dropout layers predicting (st+1, gt+1)
  - Agent M: Standard Decision Transformer trained on mixed data
  - Dcrdt buffer: Stores filtered counterfactual trajectories

- **Critical path**: 
  1. Train T and O on Denv (supervised action and next-state/return prediction)
  2. Generate counterfactual trajectories via Counterfactual Action Selection
  3. Filter candidates via uncertainty check and return comparison
  4. Store successful trajectories in Dcrdt buffer
  5. Train M on 50/50 mixture of Denv and Dcrdt batches

- **Design tradeoffs**: 
  - More search actions (na) → better coverage but higher compute
  - Larger Dcrdt (ne) → better performance with diminishing returns
  - Lower uncertainty threshold α → stricter filtering, fewer but higher-quality counterfactuals

- **Failure signatures**: 
  - Performance drops on Pong: complex observation spaces may amplify counterfactual noise
  - REINF backbone + CRDT underperforms on Maze2d: mismatch between counterfactual distribution and backbone's objective
  - Random dataset performance unstable: limited behavioral coverage violates overlap assumptions

- **First 3 experiments**: 
  1. Implement Figure 1 toy environment with 10:1 bad-to-good trajectory ratio; verify DT ~40% vs. CRDT ~90% success
  2. Ablation on walker2d-med-rep: compare full CRDT vs. "W/o comparing g" vs. "W/o Uα(Sk)" vs. "a + noise ϵ"
  3. Limited data sweep on hopper-med-rep: train with 10%, 25%, 50%, 100% of dataset; plot DT vs. CRDT vs. REINF degradation curves

## Open Questions the Paper Calls Out

- **Unified architecture**: Can Treatment and Outcome models be merged into a single shared architecture to reduce computational overhead without degrading performance? The current framework requires training three distinct Transformer models, increasing memory usage and training time.

- **Iterative training**: Does iteratively re-training the framework with self-generated counterfactual samples improve the agent's robustness or final returns? The current implementation generates a static buffer rather than continuously updating the data distribution based on the agent's improved policy.

- **Model capacity limits**: Is the performance ceiling in complex discrete domains (e.g., Atari) primarily a result of insufficient model capacity rather than the counterfactual reasoning mechanism itself? The lack of improvement in specific games may be due to the backbone Transformer's inability to process high-dimensional visual inputs effectively.

## Limitations

- The counterfactual reasoning framework relies heavily on the sequential overlap assumption, which may be violated in sparse or highly stochastic datasets
- Uncertainty estimation using dropout variance may not perfectly capture epistemic uncertainty, particularly in high-dimensional observation spaces like Atari
- The framework assumes synthetic high-return trajectories will generalize effectively when used to retrain the main agent

## Confidence

- **High confidence**: Performance improvements on locomotion and ant tasks (3.5% and 2.7% gains)
- **Medium confidence**: Atari improvements - limited to 1% dataset, and Pong performance actually degraded
- **Medium confidence**: Trajectory stitching capability - demonstrated in toy environment but not extensively validated on complex tasks
- **Low confidence**: Robust generalization to modified environment dynamics - mentioned but not thoroughly tested

## Next Checks

1. **Violate sequential overlap systematically**: Remove small-probability actions from a locomotion dataset and measure CRDT performance degradation versus DT baseline to quantify the framework's dependence on this assumption.

2. **Stress-test uncertainty estimation**: Replace dropout-based variance with ensemble-based epistemic uncertainty estimation on Atari tasks, comparing filtering effectiveness and downstream performance.

3. **Counterfactual divergence analysis**: Measure the state-action visitation distribution distance between real and counterfactual trajectories in Maze2d environments to quantify how far synthetic experiences deviate from the data distribution.