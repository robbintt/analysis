---
ver: rpa2
title: 'MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement'
arxiv_id: '2508.09670'
source_url: https://arxiv.org/abs/2508.09670
tags:
- reasoning
- learning
- arxiv
- meml-grpo
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEML-GRPO addresses reward sparsity in RLVR by leveraging diverse
  expert prompts and inter-expert mutual learning to improve reasoning performance.
  It fine-tunes models on responses from heterogeneous LLMs under distinct system
  prompts, enabling exploration beyond a model's initial policy limitations.
---

# MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement

## Quick Facts
- arXiv ID: 2508.09670
- Source URL: https://arxiv.org/abs/2508.09670
- Reference count: 12
- Key outcome: MEML-GRPO achieves 4.89% (Qwen) and 11.33% (Llama) average accuracy gains over SOTA RLVR methods on GSM8K, MathQA, and StrategyQA datasets

## Executive Summary
MEML-GRPO addresses reward sparsity in RLVR by leveraging diverse expert prompts and inter-expert mutual learning to improve reasoning performance. It fine-tunes models on responses from heterogeneous LLMs under distinct system prompts, enabling exploration beyond a model's initial policy limitations. The framework introduces Reinforced Inter-Expert Learning (RIEL) for knowledge transfer among experts and Hard Example Accumulation via SFT Buffer to ensure learning progress on challenging problems. Experiments show MEML-GRPO achieves significant improvements over state-of-the-art RLVR methods, with average performance gains of 4.89% on Qwen and 11.33% on Llama across GSM8K, MathQA, and StrategyQA datasets.

## Method Summary
MEML-GRPO operates in two stages: (1) Multi-Expert Fine-Tuning (MEF) where a base model is fine-tuned on responses from heterogeneous LLMs (DeepSeek-r1, GPT4o, Doubao-1.5-thinking) conditioned with expert-specific system prompts, and (2) Reinforced Inter-Expert Learning (RIEL) which combines GRPO with KL-based mutual learning between best/worst performing experts and hard example accumulation via an SFT buffer. The framework uses binary exact-match accuracy as reward, with 8 rollouts per expert, and employs a 64-capacity SFT buffer that triggers SFT on ground truth when an expert fails on 75% of rollouts. Training runs on 8× A800 GPUs with learning rates of 1e-6 for RL and 1e-5 for SFT.

## Key Results
- Achieves 4.89% average accuracy improvement on Qwen2.5-1.5B-Math across GSM8K, MathQA, and StrategyQA
- Achieves 11.33% average accuracy improvement on Llama3.2-1B-Instruct across the same three datasets
- Single experts match ensemble performance (MoE-SFT) with delta of -0.3 to -0.5 vs +1.2 for MoE-SFT, demonstrating effective mutual learning
- Hard example accumulation contributes 2-4% improvements in ablation studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heterogeneous expert prompts increase the probability of generating at least one correct solution per problem, mitigating reward sparsity.
- Mechanism: Distinct system prompts condition a single base model to emulate diverse reasoning styles from heterogeneous LLMs (DeepSeek-r1, GPT4o, Doubao-1.5-thinking). Each expert samples G responses, and because error distributions across heterogeneous models have low overlap (~3.06% on GSM8K per Table 1), at least one expert is likely to produce a correct trajectory that yields positive reward signal.
- Core assumption: The base model can successfully internalize multiple distinct reasoning personas via prompt-conditioned SFT without catastrophic interference between expert behaviors.
- Evidence anchors:
  - [abstract]: "utilizes diverse expert prompts as system prompts to generate a broader range of responses, substantially increasing the likelihood of identifying correct solutions"
  - [Page 2, Table 1]: Only 3.06% error overlap across three models on GSM8K; 90.11% of GPT4o errors correctable by other models
  - [corpus]: Neighbor papers confirm RLVR struggles with exploration—"current methods such as GRPO rely only on problems where model responses differ in correctness, while ignoring those where all responses [are identical]" (arXiv:2509.21880)
- Break condition: If experts converge to similar error patterns (high overlap), diversity advantage collapses. Monitor per-expert error correlation during training.

### Mechanism 2
- Claim: Inter-expert KL regularization transfers successful reasoning paths from high-performing experts to low-performing ones.
- Mechanism: For each question, identify best-performing expert E+ and worst-performing expert E− based on average reward (Equation 8). Apply KL loss (Equation 9) that penalizes differences between E−'s output distribution and E+'s distribution over correct responses. This forces weaker experts to adopt reasoning patterns that yielded rewards.
- Core assumption: Correct reasoning paths from E+ generalize to E−'s prompt context; the KL penalty is sufficient to overcome E−'s initial policy bias without destabilizing training.
- Evidence anchors:
  - [Page 3]: "log pθ(O+|Q, promptE−) − log pθ(O+|Q, promptE+)" implements the KL regularization
  - [Page 6, Table 6]: MEML-GRPO experts achieve delta of -0.3 to -0.5 vs. majority voting, while MoE-SFT-GRPO shows positive deltas (1.2), indicating mutual learning enables single experts to rival ensemble performance
  - [corpus]: Limited direct corpus validation of this specific KL mechanism; primarily paper-internal evidence
- Break condition: If E+ also produces incorrect responses, KL transfer propagates errors. The paper acknowledges this risk and introduces the SFT buffer as mitigation.

### Mechanism 3
- Claim: Hard example accumulation via SFT buffer ensures learning progress when all experts fail.
- Mechanism: When an expert produces >K incorrect answers out of G samples, add (Q, Pi) → O_gt to a fixed-capacity buffer (B=64). When buffer fills, perform SFT on ground truth. This guarantees non-zero gradients on challenging problems that RL alone cannot solve.
- Core assumption: Ground truth is available for hard examples; SFT on accumulated failures generalizes to similar problems rather than overfitting to the buffer.
- Evidence anchors:
  - [Page 4]: Buffer capacity 64, threshold K=6/G=8 (75% failure rate triggers buffer addition)
  - [Page 6, Table 5]: Ablation shows HSFT contributes ~2-4% average improvement; full configuration (MoE+HSFT+IML) achieves 77.3% vs 76.4% without IML for Qwen
  - [corpus]: Neighbor work (arXiv:2511.04800) discusses "data left behind" in RLVR, aligning with the need to recover learning from zero-reward samples
- Break condition: If buffer fills too slowly (easy dataset) or too quickly (impossibly hard dataset), the SFT trigger frequency becomes suboptimal. Tune K/G threshold per dataset difficulty.

## Foundational Learning

- Concept: **Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: MEML-GRPO builds on GRPO, an RLVR algorithm. Understanding that RLVR uses binary correctness signals (e.g., exact match) as rewards is prerequisite.
  - Quick check question: Can you explain why RLVR faces reward sparsity when a model's initial policy produces only incorrect responses?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: MEML-GRPO extends GRPO's advantage estimation (Equation 5-6). GRPO normalizes rewards within a group of responses to compute advantages.
  - Quick check question: Given G responses with rewards [0, 0, 1, 0], what advantage does GRPO assign to the correct response? (Answer: 1 - 0.25 = 0.75)

- Concept: **KL Divergence as Distribution Alignment**
  - Why needed here: Inter-expert mutual learning uses KL divergence to align weaker experts' distributions with stronger experts'. Understanding that KL measures how one distribution diverges from another is essential.
  - Quick check question: Why is the KL loss formulated as log pθ(O+|promptE−) − log pθ(O+|promptE+) rather than a symmetric measure?

## Architecture Onboarding

- Component map:
  - Multi-Expert Fine-Tuning (MEF) -> Reinforced Inter-Expert Learning (RIEL) -> Single Expert Inference
  - Heterogeneous expert response collection -> Expert-conditioned SFT dataset -> Base model fine-tuning -> GRPO training with KL mutual learning and SFT buffer

- Critical path:
  1. Generate responses from N heterogeneous models (DeepSeek-r1, GPT4o, Doubao-1.5-thinking) on training set Q
  2. Construct D_ME by pairing each response with its expert-specific system prompt
  3. Fine-tune base model on D_ME (MEF stage)
  4. For each training step in RIEL: sample G responses per expert → compute rewards → identify E+/E− → compute L_GRPO + L_KL → check buffer → if full, compute L_SFT → backprop L_total

- Design tradeoffs:
  - Number of experts N: Paper uses N=3. More experts increase diversity but also training cost and potential interference.
  - Buffer capacity B: Set to 64. Smaller buffer = more frequent SFT, larger = more diverse hard examples per SFT batch.
  - KL weight λ1 vs SFT weight λ2: Not specified in paper; requires tuning per dataset.
  - Rollouts G per expert: Set to 8. More rollouts improve advantage estimation but increase compute.

- Failure signatures:
  - **Reward plateau early in training**: Likely indicates insufficient expert diversity or all experts failing on same problems. Check per-expert error correlation.
  - **SFT buffer never fills**: Dataset may be too easy; reduce K threshold or skip HSFT.
  - **SFT buffer fills immediately on every batch**: Dataset too hard or experts too weak; verify MEF stage quality.
  - **Single expert dominates consistently**: KL transfer may not be working; verify λ1 > 0 and E− identification logic.

- First 3 experiments:
  1. Replicate Table 2 (MoE-SFT vs individual expert SFT) on a held-out subset to verify your MEF implementation produces functional expert conditioning.
  2. Run single-expert GRPO vs MEML-GRPO (your implementation) on GSM8K with N=2 experts to isolate the contribution of inter-expert learning before scaling to N=3.
  3. Ablate the SFT buffer by setting B=0 and compare training curves; verify that hard example accumulation prevents reward collapse on the most challenging 10% of problems.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does MEML-GRPO facilitate the emergence of genuinely new reasoning capabilities in the base model, or does it primarily optimize the model's ability to select and mimic the off-policy reasoning paths of the heterogeneous experts?
- Basis in paper: [explicit] The introduction highlights the debate cited from Yue et al. (2025) regarding whether RLVR methods simply steer models toward existing reasoning paths rather than enabling the "acquisition of new information."
- Why unresolved: While the results show improved accuracy, the paper does not perform an analysis to determine if the generated solutions represent novel reasoning trajectories unseen in the combined expert training data.
- What evidence would resolve it: A comparative analysis of generated reasoning chains against the union of all expert trajectories to identify if the model generalizes beyond the specific reasoning patterns it was fine-tuned on.

### Open Question 2
- Question: How does the framework's performance and training stability scale with the number of heterogeneous experts ($N$) beyond the three models tested?
- Basis in paper: [inferred] The methodology and experiments strictly utilize a fixed set of three experts ($N=3$), leaving the sensitivity of the "Inter-Expert Mutual Learning" mechanism to the number of experts unstated.
- Why unresolved: Increasing the number of experts introduces more diverse reasoning styles but also increases the likelihood of conflicting gradients and computational overhead during the RIEL stage.
- What evidence would resolve it: A scaling study evaluating accuracy and convergence speed on GSM8K while incrementally varying $N$ (e.g., from 3 to 10 experts).

### Open Question 3
- Question: Is the effectiveness of the Inter-Expert Mutual Learning (RIEL) mechanism dependent on a low error-overlap ratio among the heterogeneous experts?
- Basis in paper: [inferred] The paper motivates the approach using Table 1, which shows very low error overlap (3.06%) between experts, implying that mutual learning works because experts complement each other.
- Why unresolved: It is unclear if the RIEL mechanism would yield negative transfer or reduced gains if applied to a set of experts with highly correlated error distributions.
- What evidence would resolve it: Experiments comparing MEML-GRPO performance using the current diverse expert set against a control set of experts with high error correlation.

## Limitations
- The specific KL loss weighting hyperparameters (λ₁, λ₂) are not specified, making it unclear whether reported performance gains are robust across different weighting schemes
- The exact system prompt templates and format for each expert are incompletely detailed, which could significantly impact the MEF stage's effectiveness
- The claim of "substantially increasing the likelihood of identifying correct solutions" relies heavily on the 3.06% error overlap statistic, but this may not generalize across different task domains or model combinations

## Confidence

- **High confidence**: The core mechanism of using heterogeneous expert prompts to mitigate reward sparsity is well-supported by the error overlap analysis and ablation results
- **Medium confidence**: The inter-expert mutual learning via KL regularization is supported by Table 6 showing single experts matching ensemble performance, but the specific KL formulation's effectiveness needs more validation
- **Medium confidence**: The hard example accumulation via SFT buffer shows consistent 2-4% improvements in ablation studies, but the buffer management strategy may require dataset-specific tuning

## Next Checks

1. Implement and test the exact MEF stage with three heterogeneous experts on a small held-out dataset to verify that expert conditioning produces functionally distinct reasoning patterns
2. Systematically vary λ₁ and λ₂ hyperparameters to determine their sensitivity and identify whether the reported performance gains are robust to hyperparameter choices
3. Conduct per-expert error correlation analysis during training to quantify diversity maintenance and verify that the mutual learning mechanism is actually transferring successful reasoning patterns rather than just enforcing convergence