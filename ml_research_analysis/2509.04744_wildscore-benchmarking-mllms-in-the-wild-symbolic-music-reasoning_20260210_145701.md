---
ver: rpa2
title: 'WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning'
arxiv_id: '2509.04744'
source_url: https://arxiv.org/abs/2509.04744
tags:
- music
- symbolic
- arxiv
- image
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WildScore, the first in-the-wild multimodal
  symbolic music reasoning benchmark. It uses real musical scores paired with authentic
  user-generated questions from Reddit, covering complex musicological queries.
---

# WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning

## Quick Facts
- arXiv ID: 2509.04744
- Source URL: https://arxiv.org/abs/2509.04744
- Authors: Gagan Mundada; Yash Vishe; Amit Namburi; Xin Xu; Zachary Novack; Julian McAuley; Junda Wu
- Reference count: 20
- Primary result: First in-the-wild multimodal symbolic music reasoning benchmark using real musical scores and Reddit-sourced questions

## Executive Summary
WildScore introduces the first in-the-wild multimodal symbolic music reasoning benchmark that evaluates MLLMs using real musical scores paired with authentic user-generated questions from Reddit. The benchmark covers complex musicological queries through a systematic five-category taxonomy spanning twelve subcategories of music theory. Empirical results show that even state-of-the-art MLLMs like GPT-4.1-mini achieve only moderate accuracy (68.31%) on symbolic music reasoning tasks, with significant performance variation across different categories and subcategories.

## Method Summary
The WildScore benchmark systematically constructs a taxonomy of music theory spanning five categories and twelve subcategories, framing reasoning tasks as multiple-choice QA for controlled evaluation. The benchmark uses real musical scores paired with authentic user-generated questions sourced from Reddit to create realistic in-the-wild scenarios. This approach captures the complexity of real-world musicological queries while maintaining experimental control through standardized multiple-choice formatting.

## Key Results
- State-of-the-art MLLMs achieve only 68.31% accuracy on symbolic music reasoning tasks
- Performance varies significantly across the five taxonomy categories and twelve subcategories
- Even advanced models like GPT-4.1-mini show moderate performance, highlighting substantial room for improvement
- Real-world musicological queries prove challenging for current MLLM capabilities

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its realistic data provenance and structured evaluation framework. By using authentic Reddit questions paired with real musical scores, it captures genuine musicological reasoning challenges that users encounter. The systematic taxonomy provides comprehensive coverage of music theory concepts while the multiple-choice format enables controlled, reproducible evaluation of model reasoning capabilities.

## Foundational Learning

**Symbolic Music Notation** - Understanding musical scores as multimodal data combining visual symbols with auditory meaning. Needed because MLLMs must interpret notation systems to reason about music theory. Quick check: Can the model correctly identify basic musical elements like clefs, key signatures, and rhythmic values.

**Music Theory Taxonomy** - Hierarchical organization of music theory concepts into categories and subcategories. Required for systematic evaluation and identifying specific reasoning weaknesses. Quick check: Does the model perform differently across taxonomy categories, revealing conceptual gaps.

**Multimodal Reasoning** - Integration of visual score interpretation with textual musicological knowledge. Essential for connecting symbolic notation to theoretical concepts. Quick check: Can the model answer questions requiring both score reading and theoretical understanding.

## Architecture Onboarding

**Component Map:** Reddit Questions -> Music Score Processing -> Taxonomy Categorization -> Multiple-Choice QA Evaluation -> Performance Analysis

**Critical Path:** Score Interpretation -> Music Theory Reasoning -> Answer Selection

**Design Tradeoffs:** Multiple-choice format enables controlled evaluation but may not capture open-ended reasoning complexity; Reddit sourcing provides authenticity but introduces question quality variability.

**Failure Signatures:** Poor performance on specific taxonomy subcategories indicates conceptual gaps; inconsistent results across categories reveal limitations in multimodal integration; accuracy below human baselines suggests fundamental reasoning deficiencies.

**First Experiments:** 1) Test model on individual taxonomy categories to identify weakest areas, 2) Evaluate performance on questions requiring only score reading versus those needing theoretical knowledge, 3) Compare results using real scores versus simplified notation to assess visual processing capabilities.

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark's reliance on Reddit-sourced questions may introduce biases toward Western classical music traditions
- Fixed five-category taxonomy may not capture all dimensions of symbolic music reasoning
- Multiple-choice format may not fully represent open-ended real-world musicological inquiry complexity

## Confidence
- High confidence: MLLMs show moderate performance (68.31%) on symbolic music reasoning tasks
- Medium confidence: This represents a significant gap requiring improvement given benchmark design constraints
- Medium confidence: Systematic nature of taxonomy pending validation across diverse musical traditions

## Next Checks
1. Evaluate benchmark generalizability by testing MLLMs on musicological questions from diverse cultural traditions and non-Western notation systems
2. Conduct human expert study to validate difficulty calibration and assess if multiple-choice format adequately represents real-world complexity
3. Perform ablation studies removing Reddit-sourced questions to determine impact of question provenance on performance patterns