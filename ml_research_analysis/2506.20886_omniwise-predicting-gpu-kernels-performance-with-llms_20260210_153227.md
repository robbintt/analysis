---
ver: rpa2
title: 'Omniwise: Predicting GPU Kernels Performance with LLMs'
arxiv_id: '2506.20886'
source_url: https://arxiv.org/abs/2506.20886
tags:
- performance
- cache
- code
- omniwise
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Omniwise is an end-to-end pipeline that uses large language models
  (LLMs) to predict GPU kernel performance metrics such as cache hit rates, memory
  bandwidth, and GFLOPs without requiring code execution or profiling tools. The approach
  involves generating a diverse dataset of GPU kernels through synthetic generation,
  AI generation, and curated sources, followed by normalization and formatting of
  performance counters into a JSON schema.
---

# Omniwise: Predicting GPU Kernels Performance with LLMs

## Quick Facts
- **arXiv ID:** 2506.20886
- **Source URL:** https://arxiv.org/abs/2506.20886
- **Reference count:** 38
- **Primary result:** LLM-based pipeline predicts GPU kernel performance metrics with >90% accuracy within 10% relative error, achieving near-perfect arithmetic intensity prediction

## Executive Summary
Omniwise presents an end-to-end pipeline that leverages large language models to predict GPU kernel performance metrics including cache hit rates, memory bandwidth, and GFLOPs without requiring code execution or profiling tools. The system generates a diverse dataset of GPU kernels through synthetic generation, AI generation, and curated sources, then normalizes and formats performance counters into a JSON schema. A fine-tuned LLaMA 3.2 3B model is trained to map code semantics to hardware performance, achieving strong predictive accuracy across AMD MI250 and MI300X architectures.

## Method Summary
Omniwise employs a comprehensive pipeline beginning with diverse GPU kernel dataset generation, followed by normalization and JSON schema formatting of performance counters. The core innovation involves fine-tuning a LLaMA 3.2 3B model on this dataset to learn the mapping between code semantics and hardware performance metrics. The model is then served through a Visual Studio Code plugin, enabling real-time profiling-free performance feedback during development. The approach successfully predicts cache hit rates, memory bandwidth, and GFLOPs with high accuracy while maintaining near-perfect precision for arithmetic intensity calculations.

## Key Results
- Achieves over 90% of predictions within 10% relative error across key performance metrics
- Demonstrates near-perfect accuracy for arithmetic intensity predictions
- Provides real-time performance feedback through VS Code plugin without requiring code execution

## Why This Works (Mechanism)
The LLM's ability to capture semantic patterns in code and map them to performance characteristics enables accurate predictions without runtime profiling. The diverse dataset generation approach ensures broad coverage of kernel patterns, while the JSON schema normalization provides consistent input representation for the model.

## Foundational Learning
- **GPU performance metrics**: Cache hit rates, memory bandwidth, and GFLOPs are fundamental to understanding kernel efficiency
- **JSON schema normalization**: Standardizing performance counter representation enables consistent model input
- **LLM fine-tuning**: Adapting pre-trained models to domain-specific tasks improves prediction accuracy
- **Code semantics extraction**: Understanding code structure and patterns is crucial for performance prediction
- **VS Code plugin deployment**: Real-time integration enables practical developer workflows
- **AMD MI250/MI300X architecture**: Understanding specific hardware characteristics improves prediction accuracy

## Architecture Onboarding
**Component map:** Dataset Generation -> JSON Schema Normalization -> LLaMA 3.2 3B Fine-tuning -> VS Code Plugin Deployment
**Critical path:** Code input → Semantic extraction → Performance prediction → Real-time feedback
**Design tradeoffs:** Model size (3B parameters) balances accuracy and deployment efficiency vs. larger models with potentially better accuracy but higher resource requirements
**Failure signatures:** Poor predictions may indicate dataset limitations, architecture mismatches, or semantic understanding gaps in the model
**First experiments:**
1. Validate prediction accuracy on synthetic vs. real-world code patterns
2. Test model performance across different GPU architectures
3. Evaluate robustness to code obfuscation and variable renaming

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation scope to AMD MI250 and MI300X architectures, lacking cross-vendor validation
- Potential synthetic artifacts in dataset may not represent real-world code patterns
- Fine-tuned LLaMA 3.2 3B model choice and its impact on performance predictions remain unexplored

## Confidence
- **High** for arithmetic intensity predictions (near-perfect accuracy reported)
- **Medium** for other performance metrics (90% threshold achievement)
- **Low** for cross-architecture generalization (limited evaluation scope)

## Next Checks
1. Evaluate Omniwise on NVIDIA and Intel GPU architectures to assess cross-vendor generalization
2. Compare predictions against ground truth measurements on real production codebases beyond synthetic and curated datasets
3. Test model robustness by introducing code obfuscation and variable renaming to verify semantic understanding