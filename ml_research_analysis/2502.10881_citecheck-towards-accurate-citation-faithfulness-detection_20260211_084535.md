---
ver: rpa2
title: 'CiteCheck: Towards Accurate Citation Faithfulness Detection'
arxiv_id: '2502.10881'
source_url: https://arxiv.org/abs/2502.10881
tags:
- samples
- negative
- wang
- zhang
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CiteCheck, the first large-scale Chinese
  dataset for citation faithfulness detection in retrieval-augmented generation (RAG)
  systems. The dataset addresses the challenge of constructing high-quality negative
  samples (unsupported citations) by proposing a cost-effective two-stage manual annotation
  framework combined with LLM-based document modification.
---

# CiteCheck: Towards Accurate Citation Faithfulness Detection

## Quick Facts
- arXiv ID: 2502.10881
- Source URL: https://arxiv.org/abs/2502.10881
- Authors: Ziyao Xu; Shaohang Wei; Zhuoheng Han; Jing Jin; Zhe Yang; Xiaoguang Li; Haochen Tan; Zhijiang Guo; Houfeng Wang
- Reference count: 19
- Primary result: First large-scale Chinese dataset for citation faithfulness detection in RAG systems

## Executive Summary
This paper introduces CiteCheck, the first large-scale Chinese dataset for citation faithfulness detection in retrieval-augmented generation (RAG) systems. The dataset addresses the challenge of constructing high-quality negative samples (unsupported citations) by proposing a cost-effective two-stage manual annotation framework combined with LLM-based document modification. The first stage involves human annotation of original RAG outputs to identify positive and negative samples, while the second stage uses LLM-generated modifications to create additional negative samples from positive ones. The resulting dataset contains 11,307 samples (2,000 for development/testing, 9,796 for training). Experimental results show that state-of-the-art LLMs achieve only 69.4-83.7% accuracy on the test set, while smaller models fine-tuned with the augmented training data achieve 88.5-91.4% accuracy, demonstrating the effectiveness of the proposed approach in enabling smaller models to handle challenging negative samples.

## Method Summary
CiteCheck is constructed through a two-stage manual annotation framework combined with LLM-based document modification. The process begins with collecting 3,000 Chinese question-answer pairs from multiple sources, which are processed by a RAG system to generate answers with citations. Human annotators then label these outputs as positive (faithful) or negative (unfaithful) samples. To address the scarcity of natural negative samples, GPT-4o is used to generate additional negative samples by modifying key supporting segments in the documents of positive samples, with human review ensuring quality and coherence. The dataset contains 11,307 samples total, with 2,000 reserved for development and testing and 9,796 for training.

## Key Results
- State-of-the-art LLMs achieve only 69.4-83.7% accuracy on the challenging test set
- Fine-tuned smaller models (7B-8B parameters) achieve 88.5-91.4% accuracy, outperforming SOTA models
- Two-stage annotation framework effectively balances positive and negative samples while reducing annotation costs
- Human review ensures LLM-generated negative samples maintain quality and coherence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-stage annotation framework that combines human labeling with LLM-based document modification creates a cost-effective, high-quality dataset for citation faithfulness detection.
- Mechanism: The process creates data in two steps. First, human annotators label original RAG outputs to identify a base set of positive (supported) and negative (unsupported) samples. Second, to address the scarcity of natural negative samples from a high-quality RAG system, an LLM (GPT-4o) is used to generate new negative samples by modifying the key supporting segments in the documents of a positive sample. These modified documents are then reviewed by humans to ensure they are high-quality, fluent, and non-contradictory negative examples.
- Core assumption: LLMs can be guided to modify a document's key supporting segments in a way that makes a statement unsupported, while maintaining the document's overall coherence and avoiding trivial contradictions.
- Evidence anchors:
  - [abstract] The paper states the dataset was "constructed via a cost-effective approach using two-stage manual annotation" that "balances positive and negative samples while significantly reducing annotation expenses."
  - [section 2.2, 2.3] Section 2 details "Data Augmentation" using GPT-4o and a "Two-stage Manual Annotation" process to generate and validate negative samples.
  - [corpus] Corpus evidence is weak for the specific details of this paper's novel two-stage method but supports the broader challenge of RAG faithfulness. Related work "RAG-RL" focuses on improving generation models, while "MedTrust-RAG" addresses evidence verification, aligning with the problem this dataset aims to help solve.
- Break condition: If LLM-generated modifications are easily detectable by simple heuristics (e.g., extreme word changes) or if the human verification step in stage two becomes as expensive as creating samples from scratch.

### Mechanism 2
- Claim: Fine-tuning smaller models (7B-8B parameters) on this augmented dataset allows them to outperform much larger, state-of-the-art LLMs on the specific task of citation faithfulness detection.
- Mechanism: The CiteCheck dataset provides a focused, high-quality training signal. By training on a large number of positive and intelligently-augmented negative samples, smaller models learn to recognize the subtle patterns of support versus non-support. This specialized knowledge surpasses the general-purpose reasoning of large, zero-shot models which are not specifically trained for this binary classification task.
- Core assumption: The features learned by the fine-tuned models generalize to real-world, unmodified negative samples, and are not just overfitting to the style of the LLM-generated modifications.
- Evidence anchors:
  - [abstract] "training data augmented with LLM-generated negative samples enables smaller models to attain strong performance."
  - [section 3.2, Table 2] Experimental results show fine-tuned models like Llama-3.1-8B and Qwen2.5-7B achieve 90.6% and 88.5% test accuracy, respectively, compared to GPT-4o's 83.9%.
  - [corpus] Related work such as "ScholarCopilot" also demonstrates that fine-tuning can improve citation accuracy in RAG for academic writing, supporting the principle that specialized training is effective.
- Break condition: If the test set distribution is too similar to the training set's artificially generated negatives, the high performance may not translate to real-world RAG errors.

### Mechanism 3
- Claim: The difficulty of the constructed test set is validated by the poor performance of state-of-the-art LLMs on negative samples, which indicates the dataset captures challenging, realistic failure cases.
- Mechanism: The test set is composed of unmodified RAG outputs. The low accuracy of SOTA LLMs, particularly on negative samples (e.g., DeepSeek-v3 at ~39-40%), proves that these are genuinely difficult examples. This confirms the dataset's value as a benchmark and suggests that the errors in the test set are subtle and not based on simple flaws, making it a robust tool for evaluation.
- Core assumption: The poor performance of SOTA LLMs is due to the inherent difficulty of the task and not because the test set contains ambiguous or incorrectly labeled examples.
- Evidence anchors:
  - [abstract] "...test samples are highly challenging, with even state-of-the-art LLMs failing to achieve high accuracy."
  - [section 3.2, Table 2] Results table explicitly shows the low accuracy of models like DeepSeek-v3 and Qwen2.5-Plus on the dev and test sets.
  - [corpus] "Faithfulness-Aware Uncertainty Quantification..." supports the claim that RAG outputs are hard to verify and that this is a known, unsolved problem.
- Break condition: If further analysis shows the negative samples in the test set are ambiguous or if humans also struggle to agree on their labels.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The entire CiteCheck dataset is built on the outputs of a RAG system. Understanding that RAG involves retrieving documents and using them to generate a cited response is fundamental.
  - Quick check question: Can you explain the basic two-step process of a RAG system (retrieval and generation)?

- Concept: Binary Classification in NLP
  - Why needed here: The core task defined by the CiteCheck dataset is a binary classification problem: given a statement and cited documents, is the citation "faithful" (positive) or "unfaithful" (negative)? All model training and evaluation is based on this task.
  - Quick check question: If a model correctly identifies 90 out of 100 positive samples and 80 out of 100 negative samples, what is its overall accuracy?

- Concept: Data Augmentation
  - Why needed here: This paper's primary methodological contribution is a novel data augmentation strategy for creating negative samples. You must understand the concept of expanding a training set to improve model robustness.
  - Quick check question: Why is data augmentation particularly important when one class of data (e.g., negative samples from a good RAG system) is rare?

## Architecture Onboarding

- Component map: Question Collection -> RAG System -> Annotation & Augmentation Pipeline -> Evaluator Models
- Critical path: The critical path for a new engineer is the Data Creation Pipeline. You must understand how an original sample (question, answer, statement, documents) flows through the two-stage annotation process to become either a test sample (if it's an original negative) or a training sample (if it's an original positive that gets augmented into a negative).
- Design tradeoffs: The central tradeoff is between cost and realism. Creating negative samples by collecting real RAG errors is prohibitively expensive and slow. The paper's solution (synthetic modification) is cheap and fast but risks creating unnatural or easily detectable negative samples. The human-in-the-loop stage is designed to mitigate this risk.
- Failure signatures:
  - Overfitting to synthetic artifacts: A fine-tuned model performs well on the CiteCheck test set but fails on a held-out set of real-world RAG errors.
  - Low Inter-Annotator Agreement (IAA): Human annotators disagree frequently on the 'faithful' vs. 'unfaithful' label, indicating the task definition is too ambiguous.
  - Trivial modifications: The LLM augmentation makes obvious changes (e.g., negating a sentence) that don't require complex reasoning to detect.
- First 3 experiments:
  1. Baseline Performance: Run the provided zero-shot prompts with GPT-4o (or a similar SOTA model) on the development set to establish a baseline and reproduce the paper's initial results.
  2. Data Inspection & Analysis: Manually inspect 20-30 samples from the training set, comparing the original positive samples with their LLM-augmented negative counterparts. Document any patterns in the modifications (e.g., are they mostly deletions or changes?).
  3. Fine-tuning Run: Use a parameter-efficient fine-tuning technique (like LoRA as described in the paper) to train a 7B model (e.g., Llama-3.1-8B or Qwen2.5-7B) on the training set and evaluate on the test set to replicate the claimed performance gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does training on CiteCheck's LLM-augmented negative samples generalize to detecting real-world citation errors from diverse Chinese RAG systems?
- Basis in paper: [explicit] The authors state: "The main limitation of the experiments is the lack of more experiments on other test sets for the model obtained from training to show the generalization performance. This limitation comes from the lack of relevant Chinese datasets."
- Why unresolved: The training set relies on synthetic negative samples created via GPT-4o document modification, while the test set contains only original RAG outputs. Whether models learn to detect synthetic artifacts versus genuine faithfulness violations across different RAG architectures remains untested.
- What evidence would resolve it: Performance evaluation on citation faithfulness test sets constructed from multiple independent Chinese RAG systems (e.g., different retrieval methods, different generator LLMs) without LLM-based augmentation.

### Open Question 2
- Question: Would fine-grained annotations of unsupported statement spans and supporting document evidence improve detection performance?
- Basis in paper: [explicit] The authors acknowledge: "The main limitation of the dataset is the availability of only binary judgment labels (positive or negative). We do not manually label which part of the statement in the negative sample is unsupported, nor do we manually label the evidence in the documents that the statement in the positive sample is supported."
- Why unresolved: Binary labels preclude training models to localize unsupported content or provide explanatory evidence, potentially limiting both model capability and interpretability. The LLM-generated key segments provided during annotation are imperfect and used only as annotator aids.
- What evidence would resolve it: Comparative experiments between binary-label training and span-level/evidence-level annotation training, measuring both classification accuracy and localization precision.

### Open Question 3
- Question: Do LLM-generated negative samples introduce systematic biases that differ from naturally-occurring citation errors?
- Basis in paper: [inferred] The augmentation methodology modifies documents to create negatives with constraints: "The modified documents should not be inconsistent or incoherent, so as not to provide the trained model with a false basis for judging the negative samples." However, real RAG citation errors may involve different error types (misattribution, partial support, fabricated details) not captured by systematic segment modification.
- Why unresolved: The paper reports strong fine-tuned model performance but does not analyze error type distributions in synthetic vs. real negatives or measure whether models learn modification pattern artifacts rather than general faithfulness detection.
- What evidence would resolve it: Fine-grained error taxonomy analysis comparing LLM-augmented negatives against natural RAG errors, plus ablation studies testing performance on held-out error types excluded from training.

### Open Question 4
- Question: Can cross-lingual transfer from English citation faithfulness datasets improve Chinese detection performance?
- Basis in paper: [inferred] The introduction notes that "English benchmarks have emerged (Yue et al., 2023), Chinese datasets remain notably absent." The paper addresses the Chinese gap but does not explore whether existing English resources could augment training through translation or multilingual models.
- Why unresolved: Citation faithfulness detection may share cross-lingual structural patterns (claim-evidence alignment, support verification) that multilingual models could exploit, potentially reducing annotation costs for lower-resource languages.
- What evidence would resolve it: Experiments with multilingual models (e.g., fine-tuned on combined English and Chinese data, or zero-shot transfer from English to Chinese) compared to Chinese-only training baselines.

## Limitations
- Data provenance uncertainty: The dataset construction relies on LLM-generated modifications for negative samples, creating potential domain-specific artifacts that may not generalize to real-world RAG errors
- Single LLM dependency: The augmentation process uses GPT-4o exclusively, introducing potential bias toward its generation patterns
- Language restriction: Limited to Chinese, preventing direct comparison with English RAG faithfulness benchmarks

## Confidence
- High confidence: The experimental methodology and evaluation protocol are sound, with clear baselines and reproducible results
- Medium confidence: The claim that smaller models outperform SOTA LLMs is well-supported, but the generalization to unseen RAG errors remains uncertain
- Low confidence: The assertion that augmented negative samples are indistinguishable from real errors lacks direct validation beyond human review

## Next Checks
1. Cross-lingual validation: Apply the fine-tuned models to English RAG outputs to test cross-linguistic generalization
2. Human error comparison: Collect a small set of real RAG errors and compare model performance on these versus LLM-generated negatives
3. Ablation study: Train models on subsets of the data (human-only negatives vs. LLM-generated) to quantify the contribution of each data source