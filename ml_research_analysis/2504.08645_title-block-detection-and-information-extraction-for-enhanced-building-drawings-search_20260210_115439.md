---
ver: rpa2
title: Title block detection and information extraction for enhanced building drawings
  search
arxiv_id: '2504.08645'
source_url: https://arxiv.org/abs/2504.08645
tags:
- title
- block
- drawings
- drawing
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of information extraction (IE)
  from building drawings, particularly for historical drawings with poor quality and
  no uniformity in title blocks. A combined inference pipeline is proposed, integrating
  a lightweight Faster R-CNN model for title block detection and GPT-4o for structured
  metadata extraction.
---

# Title block detection and information extraction for enhanced building drawings search

## Quick Facts
- arXiv ID: 2504.08645
- Source URL: https://arxiv.org/abs/2504.08645
- Authors: Alessio Lombardi; Li Duan; Ahmed Elnagar; Ahmed Zaalouk; Khalid Ismail; Edlira Vakaj
- Reference count: 1
- Key outcome: 97.1% accuracy in title block detection and 95% accuracy in text extraction for building drawings

## Executive Summary
This work presents a novel pipeline for extracting structured metadata from building drawings, addressing the challenge of processing historical drawings with poor quality and no title block uniformity. The approach combines a lightweight Faster R-CNN model for title block detection with GPT-4o for structured metadata extraction. The method achieves high accuracy while maintaining efficiency, enabling enhanced search capabilities for engineering drawings. A key contribution is the development of a novel, extensible dataset of 1,385 engineering drawings created through an AEC-friendly annotation workflow using familiar tools like Bluebeam Revu.

## Method Summary
The proposed method employs a two-stage pipeline: first, a Faster R-CNN model with MobileNetV3 backbone detects title block regions in rasterized drawings; second, GPT-4o extracts structured metadata from the cropped title block regions using prompt engineering. The approach separates spatial detection from semantic extraction, allowing GPT-4o to focus on parsing tabular layouts without interference from main drawing content. The system includes post-processing for semantic key normalization and a user interface for metadata-based drawing search.

## Key Results
- Achieves 97.1% accuracy in title block detection using Faster R-CNN with MobileNetV3 backbone
- Extracts structured metadata with 95% accuracy using GPT-4o from cropped title blocks
- Enables up to 80% time savings in drawing search operations on real projects
- Processes both CAD and scanned historical drawings effectively

## Why This Works (Mechanism)

### Mechanism 1
Decomposing the pipeline into spatial detection followed by structured extraction improves robustness on complex historical drawings. Faster R-CNN first localizes the title block region, then GPT-4o extracts key-value pairs from the cropped region. This separation prevents GPT-4o from misattributing text from the main drawing content. Core assumption: Title blocks have consistent spatial features detectable by CNNs, and multimodal LLMs can reliably parse cropped tabular layouts into structured formats. Evidence: GPT-4o failed to correctly identify title blocks in 6 out of 10 images when given full drawings, but achieved perfect text extraction when manually-cropped title blocks were provided.

### Mechanism 2
Domain-expert annotation using existing AEC tools (Bluebeam Revu) enables efficient dataset creation without specialized ML annotation training. Engineers annotate drawings in familiar tools; a converter transforms PDF annotations to COCO format. This reduces annotation friction and leverages domain expertise directly. Core assumption: Engineers can accurately identify title block boundaries without ML-specific annotation training, and the COCO conversion preserves bounding box fidelity. Evidence: The novel dataset of 1,385 drawings was created via this AEC-friendly workflow, which the authors claim has advantages over other CV annotation workflows.

### Mechanism 3
Prompt-engineered GPT-4o can extract structured metadata from title blocks without task-specific fine-tuning. GPT-4o receives cropped title block images and a structured prompt requesting JSON output with cell titles as keys. The model's multimodal capabilities handle both printed and handwritten text. Core assumption: GPT-4o's pre-trained vision-language representations generalize to engineering drawing conventions without domain-specific training. Evidence: The system achieved 95% accuracy in text extraction across matching keys with a maximum fuzzy distance of 4.

## Foundational Learning

- Concept: Object Detection with Region Proposal Networks (RPN)
  - Why needed here: Understanding how Faster R-CNN generates region proposals helps diagnose localization failures and interpret confidence scores.
  - Quick check question: Can you explain why an RPN might produce high-confidence false positives on drawing elements that resemble title blocks (e.g., legends)?

- Concept: Prompt Engineering for Structured Extraction
  - Why needed here: The quality of GPT-4o's JSON output depends on prompt specificity; vague prompts produce inconsistent key naming.
  - Quick check question: What prompt modifications would you test if GPT-4o consistently misses handwritten dates in title blocks?

- Concept: Intersection over Union (IoU) for Detection Evaluation
  - Why needed here: The paper uses 70% IoU threshold for true positives; understanding this metric is essential for reproducing and extending results.
  - Quick check question: If your detection model achieves 85% accuracy at 50% IoU but only 60% at 70% IoU, what does this indicate about bounding box precision?

## Architecture Onboarding

- Component map: Input (rasterized drawings) -> Detector (Faster R-CNN) -> Cropper -> Extractor (GPT-4o) -> Post-processor (semantic normalization) -> Search UI (metadata database)
- Critical path: Detector accuracy directly constrains extractor performance. If the detector includes main content in the bounding box, GPT-4o will extract irrelevant text.
- Design tradeoffs: MobileNetV3 backbone chosen for inference speed over accuracy; larger models (YOLO, ResNet) may improve detection but increase latency. GPT-4o API costs are amortized (one-time inference per drawing) but may be prohibitive for very large archives. COCO dataset format enables tooling compatibility but requires conversion pipeline maintenance.
- Failure signatures: Detector includes adjacent elements (notes, legends) → extraction contains mixed content. Low confidence detections (<0.5) often indicate rotated or irregular title blocks. GPT-4o produces inconsistent key names for the same semantic field (e.g., "Dwg No" vs "Drawing No") → requires post-processing normalization.
- First 3 experiments: 1) Reproduce detection accuracy on the 1,385-drawing dataset: train Faster R-CNN with MobileNetV3, evaluate IoU@70%, compare to reported 97.1% title block accuracy. 2) Ablate the detector by feeding GPT-4o full drawings vs. cropped title blocks; quantify extraction error rate difference. 3) Test GPT-4o prompt variations: compare single-prompt extraction vs. two-step (identify cells → extract values) on a held-out set of 50 drawings with ground-truth JSON.

## Open Questions the Paper Calls Out

### Open Question 1
How can a domain-specific knowledge graph (KG) effectively link title block metadata with main drawing content to enable neuro-symbolic reasoning for compliance checking? The authors state future work will "investigate the design of a domain-specific knowledge graph (KG) to represent the drawing information" to support neuro-symbolic approaches. This remains unresolved because the current work isolates title blocks and does not interpret the semantic relationships between the metadata and the visual elements in the main drawing area.

### Open Question 2
Can the current annotation workflow and detection pipeline be extended to standardize the extraction of technical symbols without significant computational overhead? The authors identify "defin[ing] a set of drawing symbols to expand the dataset categories" as a specific avenue for future work to cover additional use cases. This remains unresolved because the current dataset and Faster R-CNN model focus primarily on high-level bounding boxes rather than granular technical symbols within the drawing content.

### Open Question 3
Does the Faster R-CNN model's detection accuracy degrade when applied to engineering disciplines underrepresented in the training data, such as MEP or structural drawings? The paper notes the dataset is "majority... facade engineering," suggesting a potential domain bias that may affect generalization to other disciplines mentioned (MEP, structural). This remains unresolved because the reported 97.1% accuracy is an aggregate figure; performance variance across the specific sub-domains of the dataset is not detailed.

## Limitations

- Dataset representativeness uncertainty: Claims rest heavily on the quality and representativeness of the 1,385-drawing dataset, with unclear distribution across drawing types and quality degradation patterns
- Single case study for time savings: The reported 80% time savings is based on a single case study without methodological details on measurement or comparison baseline
- Sparse manual verification details: The 95% text extraction accuracy metric is well-defined, but manual verification process details are sparse

## Confidence

- Title block detection accuracy (97.1%): High confidence - Clear experimental setup with IoU threshold and baseline comparisons
- Text extraction accuracy (95%): Medium confidence - Accuracy metric is well-defined, but manual verification process details are sparse
- Time savings (80%): Low confidence - Single case study without methodological details on measurement or comparison baseline

## Next Checks

1. Conduct cross-validation on dataset subsets stratified by drawing quality levels (high/medium/low) to verify detection robustness degrades gracefully rather than catastrophically
2. Implement a confusion matrix analysis for the extraction pipeline to identify which metadata fields (e.g., handwritten dates vs. printed drawing numbers) have systematically lower accuracy
3. Benchmark against the DocTR method mentioned in related work on a held-out test set to quantify the actual performance gap and identify specific failure modes where DocTR outperforms the proposed approach