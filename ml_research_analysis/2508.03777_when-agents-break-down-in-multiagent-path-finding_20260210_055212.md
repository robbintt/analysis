---
ver: rpa2
title: When Agents Break Down in Multiagent Path Finding
arxiv_id: '2508.03777'
source_url: https://arxiv.org/abs/2508.03777
tags:
- agents
- agent
- turn
- schedule
- vertex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Multiagent Path Finding with Malfunctioning
  Agents (MAPFMA) problem, which extends the classical MAPF problem by allowing agents
  to experience delays due to malfunctions. The authors show that centralized adaptation
  of schedules is computationally intractable, as deciding whether a given schedule
  can be adapted without increasing makespan is NP-hard even for planar graphs.
---

# When Agents Break Down in Multiagent Path Finding

## Quick Facts
- arXiv ID: 2508.03777
- Source URL: https://arxiv.org/abs/2508.03777
- Authors: Foivos Fioravantes; Dušan Knop; Nikolaos Melissinos; Michal Opler
- Reference count: 26
- Primary result: Centralized schedule adaptation for MAPF with malfunctions is NP-hard; two distributed protocols achieve makespan increase of at most k turns for k malfunctions

## Executive Summary
This paper addresses the Multiagent Path Finding with Malfunctioning Agents (MAPFMA) problem, where agents can experience delays during execution of a pre-computed schedule. The authors prove that adapting schedules centrally to maintain optimal makespan is computationally intractable (NP-hard) even for planar graphs. To overcome this, they propose two distributed protocols - Check Before Moving (CBM) and Check Counter Before Moving (CCBM) - that achieve robust recovery without global replanning, with makespan increases bounded by the number of malfunctions.

## Method Summary
The paper introduces two distributed protocols for adapting MAPF schedules when agents malfunction. CBM uses local state (Delayed/On-time) and neighborhood checks to prioritize delayed agents, ensuring makespan increases by at most one turn for a single malfunction. CCBM offloads computation to vertices by maintaining counters that track agent passage, allowing synchronization without global communication and achieving makespan increases bounded by the number of malfunctions k. Both protocols avoid the NP-hardness of centralized adaptation by performing local decision-making rather than global replanning.

## Key Results
- Centralized schedule adaptation to maintain optimal makespan is NP-hard, even for planar graphs
- CBM protocol ensures makespan increases by at most one turn for a single malfunction through local priority rules
- CCBM protocol achieves makespan increases bounded by k turns for k malfunctions using vertex counters
- Both protocols are localized and avoid the computational intractability of global replanning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: If agents prioritize delayed agents during local collision checks, the system bounds the makespan increase to at most one turn for a single malfunction.
- Mechanism: The **Check Before Moving (CBM) protocol** requires agents to inspect their immediate neighborhood before moving. If a "delayed" agent is present, "on-time" agents perform a delay-1 operation (wait), effectively propagating a wave of delays only as far as necessary.
- Core assumption: Agents can reliably detect the state (delayed vs. on-time) of immediate neighbors and adhere strictly to the priority rule without communication latency or failure.
- Evidence anchors:
  - [abstract] "The Check Before Moving (CBM) protocol... ensuring a makespan increase of at most one turn for a single malfunction."
  - [page 14] "The CBM protocol... assigns to each agent a state... switches their state to delayed as soon as they perform their first delay-1 operation."
  - [corpus] Related work in "Robust Multi-Agent Path Finding" suggests robustness typically requires buffering delays, which this protocol achieves via local prioritization.
- Break condition: If a malfunction lasts longer than the agents' lookahead capability (e.g., k+1 turns in a system designed for k), or if agents cannot determine neighbor status, deadlocks or collisions may occur.

### Mechanism 2
- Claim: If vertices track the number of agents that have passed through them, agents can synchronize passage without global replanning, bounding makespan increase by the number of malfunctions $k$.
- Mechanism: The **Check Counter Before Moving (CCBM) protocol** offloads computation to the network nodes. Vertices maintain a counter $c_v$. Agents move only if the destination vertex counter matches their expected schedule value. If a malfunction creates a gap, agents wait until the counter increments sufficiently.
- Core assumption: The network infrastructure (vertices) possesses passive computational capability (storage/incrementing) and agents have prior knowledge of expected counter values from the initial schedule.
- Evidence anchors:
  - [abstract] "The Check Counter Before Moving (CCBM) protocol uses vertex counters to track agent passage and achieves a makespan increase of at most k turns for k malfunctions."
  - [page 2] "shifts the necessary computations onto the network's nodes, ensuring robustness without requiring enhanced agent processing power."
  - [corpus] Weak direct corpus evidence for vertex-counting specific to MAPF; this appears novel to the paper's specific "Hansel and Gretel" heuristic.
- Break condition: If the vertex counting mechanism fails (e.g., data corruption) or if the expected counter list $l_a$ inside an agent is desynchronized from the actual vertex state.

### Mechanism 3
- Claim: Centralized schedule adaptation is computationally infeasible for real-time recovery, necessitating the distributed protocols above.
- Mechanism: The paper reduces the problem of maintaining optimal makespan during malfunctions to 3-SAT. It demonstrates that determining if a schedule can be adapted without increasing makespan is **NP-hard**, even for planar graphs.
- Core assumption: The computational resources required for global replanning exceed those available in real-time operational constraints.
- Evidence anchors:
  - [page 1] "deciding whether a given schedule can be adapted without increasing makespan is NP-hard even for planar graphs."
  - [page 7] "The key point is that the schedule σ hinges on the brown agent moving on the first round... throws off the timing... [requiring solving] 3-SAT."
- Break condition: If the graph topology is highly restrictive (e.g., trees) or the number of agents is very small, the NP-hardness reduction may not imply practical intractability, potentially allowing centralized solutions.

## Foundational Learning

- Concept: **Schedule Feasibility vs. Optimality**
  - Why needed here: The paper distinguishes between finding *any* path (feasibility) and finding one that minimizes total time (makespan). The protocols aim to preserve makespan optimality (or near-optimality) during faults.
  - Quick check question: Does a "feasible" schedule guarantee the shortest possible arrival time for all agents? (Answer: No, it only guarantees collision-free paths).

- Concept: **NP-Hardness Reductions**
  - Why needed here: To understand why the authors propose complex local protocols instead of a "smart" central controller. The proof in Theorem 1 relies on reducing 3-SAT to the MAPF adaptation problem.
  - Quick check question: Why does reducing 3-SAT to the Schedule Adaptation problem prove that adaptation is hard? (Answer: It shows that if we could solve adaptation efficiently, we could solve 3-SAT efficiently, which is widely believed to be impossible).

- Concept: **Distributed vs. Centralized State**
  - Why needed here: CBM relies on agent-local state (Delayed/On-time), whereas CCBM relies on vertex-local state (Counters). Understanding where state lives is key to implementing the architecture.
  - Quick check question: In CCBM, if an agent fails to move, does the vertex counter still increment? (Answer: No, the counter tracks passage, so a stalled agent prevents counter increments, forcing subsequent agents to wait).

## Architecture Onboarding

- Component map:
  - Agents -> Vertices (passive in CBM, active in CCBM) -> Initial Schedule (σ)
  - Scheduler (oracle) -> Agents (maintain state) -> Vertices (maintain counters in CCBM)

- Critical path:
  1. **Initialization:** Compute optimal schedule σ.
  2. **Execution:** Agents attempt moves per σ.
  3. **Disruption:** Adversary forces a malfunction-1 (delay) on Agent a.
  4. **Resolution:**
     - (CBM) Neighbors of a detect delay state and pause.
     - (CCBM) Agent b attempts to enter vertex v; checks c_v against l_b; waits if mismatch.
  5. **Recovery:** Delay propagates locally until the delayed agent catches up or the queue clears.

- Design tradeoffs:
  - **CBM:** Lower infrastructure requirement (dumb vertices) vs. higher agent sensing requirement (neighborhood scan). Limited to k-turn lookaheads.
  - **CCBM:** Higher infrastructure requirement (smart vertices) vs. simpler agent logic (read counter). Better theoretical bounds for unbounded k.
  - **Centralized:** Optimal but intractable (NP-hard) vs. Distributed (Sub-optimal by at most k turns but polynomial/fast).

- Failure signatures:
  - **Deadlock:** Agents waiting on each other in a cycle (prevented in these protocols by strict priority/counter ordering).
  - **Infinite Delay:** A specific collision case mentioned in Example 1 where an agent is permanently blocked from its target because it lost priority access to a critical node.

- First 3 experiments:
  1. **Baseline Failure (Theorem 1/Ex 1):** Implement a naive "ignore malfunction" protocol on the graph in Figure 1. Confirm that a single 1-turn delay causes an infinite blockage or collision.
  2. **CBM Stress Test:** Simulate a single malfunction in a dense graph. Verify that the makespan increases by exactly 1 and that the "delay" signal propagates only to immediate neighbors.
  3. **CCBM Scaling:** Simulate k=5 malfunctions in a larger graph. Verify that the makespan is ℓ + 5 and confirm that vertex counters correctly queue agents without global communication.

## Open Questions the Paper Calls Out

- **Question:** Can heuristic algorithms effectively overcome the theoretical NP-hardness of centralized schedule adaptation?
  - Basis: [explicit] The conclusion explicitly asks, "Is there some heuristic that circumvents the hardness shown in Theorem 1?"
  - Why unresolved: While Theorem 1 proves the problem is NP-hard for planar graphs, the authors note this does not preclude the existence of efficient practical solvers for typical instances.
  - What evidence would resolve it: Development of a heuristic algorithm that solves the adaptation problem efficiently in practice or average-case complexity analysis showing tractability.

- **Question:** How does the performance of the proposed protocols change if malfunctions follow a random distribution rather than an adversarial model?
  - Basis: [explicit] The conclusion specifically asks, "What if the malfunctions were not performed by an adversary, but followed some random distribution?"
  - Why unresolved: The current makespan guarantees (bounded by k for k malfunctions) rely on a worst-case adversarial analysis, which may be too pessimistic for real-world stochastic scenarios.
  - What evidence would resolve it: A probabilistic analysis of the CBM or CCBM protocols providing expected makespan bounds under stochastic delay models.

- **Question:** Is the centralized schedule adaptation problem tractable on restricted graph topologies like grids or trees?
  - Basis: [inferred] Theorem 1 establishes NP-hardness for planar graphs of maximum degree 10, but the complexity for simpler topologies common in warehouse settings (e.g., grids) remains unclassified.
  - Why unresolved: The paper's hardness reduction relies on specific graph constructions that may not apply to the more rigid structure of grids or trees.
  - What evidence would resolve it: A polynomial-time algorithm for centralized adaptation on grids or trees, or an extension of the NP-hardness proof to these specific topologies.

## Limitations
- The NP-hardness reduction doesn't quantify practical computational burden for real-world graph sizes
- Theoretical bounds assume perfect adherence to protocols without communication failures or state corruption
- Protocols haven't been validated through empirical experiments on benchmark instances
- Scalability of vertex counters in large networks remains unverified

## Confidence
- **CBM Protocol Bound (makespan ≤ ℓ + 1 for single malfunction)**: High confidence
- **CCBM Protocol Bound (makespan ≤ ℓ + k for k malfunctions)**: Medium confidence
- **NP-Hardness of Centralized Adaptation**: High confidence
- **Distributed Protocols as Practical Solutions**: Medium confidence

## Next Checks
1. **Counter Synchronization Stress Test**: Implement CCBM with 100 agents and 20 malfunctions. Measure vertex counter accuracy and detect any desynchronization events that could violate makespan bounds.
2. **Asynchronous Operation Simulation**: Modify CBM to allow agents to operate on different clocks (e.g., 10% random delay per agent). Test whether the priority-based collision avoidance still guarantees makespan increase ≤ 1.
3. **Extended Malfunction Impact**: Implement a scenario where a single agent experiences a 5-turn malfunction. Verify whether CBM's "single-turn" bound assumption breaks down and quantify the actual makespan increase.