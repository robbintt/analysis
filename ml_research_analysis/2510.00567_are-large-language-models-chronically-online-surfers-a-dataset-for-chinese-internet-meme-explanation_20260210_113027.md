---
ver: rpa2
title: Are Large Language Models Chronically Online Surfers? A Dataset for Chinese
  Internet Meme Explanation
arxiv_id: '2510.00567'
source_url: https://arxiv.org/abs/2510.00567
tags:
- meme
- memes
- meaning
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CHIME, a dataset for Chinese Internet meme
  explanation, to evaluate whether large language models understand culturally nuanced,
  rapidly evolving online content. The dataset comprises 1,458 phrase-based memes
  with detailed annotations including meaning, origin, example sentences, and meme
  types.
---

# Are Large Language Models Chronically Online Surfers? A Dataset for Chinese Internet Meme Explanation

## Quick Facts
- **arXiv ID:** 2510.00567
- **Source URL:** https://arxiv.org/abs/2510.00567
- **Reference count:** 40
- **Primary result:** LLMs struggle with culturally and linguistically complex Chinese memes, particularly homophonic puns and quotations, despite reasonable performance on simpler meme types.

## Executive Summary
This paper introduces CHIME, a dataset for Chinese Internet meme explanation, to evaluate whether large language models understand culturally nuanced, rapidly evolving online content. The dataset comprises 1,458 phrase-based memes with detailed annotations including meaning, origin, example sentences, and meme types. Two tasks were designed: (1) meme explanation, where models must explain meaning, provide origin, and generate examples; (2) multiple-choice questions requiring selection of the most appropriate meme to fill a contextual blank. Results show that while models perform reasonably on straightforward memes, they struggle significantly with culturally and linguistically complex meme types like homophonic puns and quotations. Models also consistently fail to identify accurate meme origins. In the MCQ task, model accuracy remains below human performance, with accuracy varying across meme types. The study reveals that meme understanding requires cultural and linguistic nuance that current LLMs find challenging, particularly for content relying on wordplay and contextual references.

## Method Summary
The CHIME dataset was constructed through crawling Geng Baike (gengbaike.cn) for memes collected between August 2020 and September 2024. GPT-4o extracted key information from raw data, followed by manual verification. The final dataset contains 1,458 phrase-based memes with annotations for meaning, origin (525 have this), 1-3 example sentences, meme type (6 categories), and profanity/offense labels. Two zero-shot evaluation tasks were designed: explanation task requiring models to generate meaning, origin, and example sentences for given memes, and multiple-choice questions where models select the most appropriate meme from 5 options to fill a contextual blank. Evaluation used automatic metrics (cosine similarity, BERTScore, BARTScore) with BGE embeddings and human evaluation on 240 memes (40 per type) using 3-point Likert scales with 3 raters per batch.

## Key Results
- Models perform significantly worse on homophonic puns and quotations compared to other meme types, with BERTScore gaps of approximately 10-15%
- Origin identification is the weakest task component, with all models showing significantly lower performance than meaning and example generation
- Providing meme meanings in MCQ prompts improves accuracy by 8.8-27.0% across models, with smaller models showing larger gains
- Human performance exceeds model performance in all evaluation metrics, with humans achieving ~83% accuracy in MCQ selection compared to models' ~73%
- Zero-shot prompting outperformed one-shot prompting for both explanation and MCQ tasks

## Why This Works (Mechanism)

### Mechanism 1
Meme type complexity causally predicts LLM performance degradation patterns. Memes requiring cultural-linguistic integration (homophonic puns, quotations) demand knowledge of phonetic wordplay or specific provenance that may be underrepresented or temporally misaligned in training corpora. Direct-meaning memes (experience, slang) rely on more distributable semantic patterns. Core assumption: Training data contains fewer annotated examples linking phonetic variants to their intended meanings and lacks systematic provenance documentation for viral content. Evidence: Models struggle significantly with culturally and linguistically complex meme types like homophonic puns and quotations. Break condition: If a model were trained on temporally-aligned, phonetically-annotated meme corpora, homophonic pun performance should converge toward slang-level performance.

### Mechanism 2
Task format modulates which knowledge components become performance bottlenecks. In MCQ selection, models perform context-to-meme matching using embedding similarity and semantic coherence, which can succeed without explicit origin knowledge. Explanation tasks require generative retrieval of provenance and precise definition—exposing knowledge gaps masked in recognition tasks. Core assumption: Recognition (selecting from options) activates shallower semantic associations than generation (producing explanations from scratch). Evidence: Models that correctly explain meme meanings achieve around 83% accuracy in MCQ selection; conversely, models that select correct memes in context only achieve around 73% accuracy in explanation. Break condition: If explanation and MCQ performance converge across all meme types, the task-format mechanism would be weakened as an explanatory factor.

### Mechanism 3
Providing explicit semantic context reduces—but does not eliminate—cultural-linguistic knowledge gaps. When meme meanings are supplied in MCQ prompts, models can bypass internal knowledge retrieval and perform contextual integration directly, improving accuracy (Δ = +8.8% to +27.0% across models). Core assumption: The remaining gap reflects limitations in pragmatic inference—understanding how a meme's connotation fits discourse context—not just definitional knowledge. Evidence: Accuracy improves when meanings are provided, with smaller models (Qwen2.5-7B) showing larger gains (+27.0%) than larger models (DeepSeek-V3, +8.8%). Break condition: If meaning provision eliminated all performance gaps between models and humans, pragmatic inference would not be a separate bottleneck.

## Foundational Learning

- **Chinese Homophonic Puns (谐音梗)**
  - Why needed here: This was the hardest meme type for all models; understanding why requires knowing that Chinese characters with identical or near-identical pronunciations can carry radically different meanings (e.g., "肾炎" nephritis vs. "神颜" divine appearance).
  - Quick check question: Can you explain why "虾仁猪心" (shrimp meat, pig heart) functions as a pun for "杀人诛心" (kill the person, condemn their heart)?

- **Productive vs. Receptive Language Tasks**
  - Why needed here: The paper's cross-task analysis shows asymmetric performance—recognizing appropriate meme usage is easier than generating accurate explanations. This distinction is essential for designing evaluation protocols.
  - Quick check question: If a model selects the correct meme in an MCQ 85% of the time but explains it correctly only 60% of the time, which task tests "deeper" knowledge?

- **Meme Taxonomy and Cultural Specificity**
  - Why needed here: The paper's six-category taxonomy (experience, quotation, stylistic device, homophonic pun, slang, abbreviation) directly predicts difficulty patterns; ignoring this leads to conflated evaluation results.
  - Quick check question: Why would "quotation" memes be harder than "slang" memes for a model trained primarily on formal text corpora?

## Architecture Onboarding

- **Component map:** Geng Baike crawl -> GPT-4o key information extraction -> manual verification -> CHIME dataset (1,458 memes) -> zero-shot evaluation tasks (explanation, MCQ) -> automatic metrics (cosine, BERTScore, BARTScore) + human evaluation

- **Critical path:** Dataset curation (human filtering + LLM extraction + manual verification) -> determines benchmark quality -> Prompt design (zero-shot vs one-shot) -> paper found zero-shot superior for this task -> Metric selection (automatic: cosine/BERTScore/BARTScore; human: 3-point Likert on accuracy) -> MCQ construction (target meme masked, 4 distractors via embedding similarity)

- **Design tradeoffs:** Text-only focus excludes multimodal memes (images, videos)—reduces complexity but limits real-world applicability; Single-language (Simplified Chinese) culturally specific findings may not transfer to Traditional Chinese or other languages; Zero-shot evaluation tests pre-trained knowledge but may underestimate model adaptability

- **Failure signatures:** Origin confusion: Models give vague attributions ("from social media") for quotation memes—signals missing provenance knowledge; Semantic shift: Homophonic puns interpreted as literal meanings or wrong homophones (e.g., "肾炎" -> kidney disease vs. "神颜" divine appearance); Cross-type confusion: Abbreviations misclassified as homophonic puns—suggests weak type-discrimination

- **First 3 experiments:** 1) Baseline replication: Run the zero-shot explanation task on all 1,458 memes using GLM-4-Plus or DeepSeek-V3; compute cosine/BERTScore/BARTScore and compare to Table 2 values; 2) Ablation by meme type: Subset evaluation on homophonic puns (133 items) vs. experience memes (561 items)—expect ~10-15% BERTScore gap; 3) Context-augmented MCQ: Replicate Table 5 experiment—provide meme meanings in prompts and verify that smaller models show larger accuracy gains than larger models

## Open Questions the Paper Calls Out

### Open Question 1
To what extent does the integration of multimodal data (images or videos) improve LLM performance on Chinese Internet meme explanation compared to the text-only baselines established in this study? Basis: The Conclusion states, "Future work could explore expanding the dataset to include multimodal memes," acknowledging the current text-only limitation. Why unresolved: The CHIME dataset and experiments were restricted to phrase-based text, yet the authors note that many memes rely on visual formats or non-textual elements which were excluded from this evaluation. What evidence would resolve it: A comparative study evaluating current Multimodal LLMs (e.g., GPT-4V) on a visual extension of the CHIME dataset against the text-only results reported in the paper.

### Open Question 2
How does the temporal stability of LLMs' meme understanding degrade as new memes emerge after the models' training data cutoffs? Basis: The paper identifies the rapid evolution of memes as a key characteristic (Introduction) and lists the static nature of the dataset as a limitation (Limitations). Why unresolved: The study provides a snapshot of performance on memes collected between 2020 and 2024, but it does not measure how quickly this knowledge becomes obsolete or how well models generalize to future, unseen meme trends. What evidence would resolve it: A longitudinal evaluation benchmarking model performance specifically on memes that originated after the specific release dates of the evaluated model versions.

### Open Question 3
Are the specific failure modes observed in "homophonic puns" and "abbreviations" inherent to the Chinese language's orthographic features, or do they generalize to other languages with different writing systems? Basis: The Limitations section notes the dataset is limited to Chinese memes and "may not fully represent the diversity of memes across different cultures and languages." Why unresolved: The error analysis highlights unique struggles with phonetic transformations (puns) and character-based abbreviations specific to Chinese, but it remains unclear if this is a linguistic processing issue or a general lack of cultural reasoning. What evidence would resolve it: A cross-lingual transfer study applying the same taxonomy and evaluation tasks to comparable English or Japanese meme datasets to see if similar error patterns emerge.

## Limitations
- Dataset focus on phrase-based memes excludes multimodal content that characterizes much of Chinese internet culture
- Zero-shot evaluation paradigm may not reflect fine-tuned or task-adapted model performance
- Human evaluation sample size (40 memes per type) may limit statistical robustness for fine-grained performance comparisons

## Confidence
- High confidence in dataset construction methodology and evaluation protocol
- Medium confidence in claim that models "struggle significantly" with homophonic puns and quotations
- Medium confidence in task-format mechanism (recognition vs. generation asymmetry)
- Low confidence in extrapolating findings to English or other languages

## Next Checks
1. Replicate the zero-shot explanation task on the full 1,458-meme dataset using GLM-4-Plus and DeepSeek-V3 to verify reported BERTScore/F values across all meme types
2. Conduct a systematic ablation study comparing homophonic pun (133 items) versus experience meme (561 items) performance to quantify the claimed 10-15% performance gap
3. Test the context-augmentation hypothesis by providing meme meanings in MCQ prompts and measuring whether smaller models (Qwen2.5-7B) show significantly larger accuracy gains than larger models (DeepSeek-V3)