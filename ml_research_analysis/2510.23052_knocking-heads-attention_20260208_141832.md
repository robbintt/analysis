---
ver: rpa2
title: Knocking-Heads Attention
arxiv_id: '2510.23052'
source_url: https://arxiv.org/abs/2510.23052
tags:
- attention
- head
- heads
- knocking-heads
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces knocking-heads attention (KHA), a novel attention
  mechanism that enables cross-head communication in multi-head attention models while
  maintaining head specialization. KHA achieves this by applying shared, diagonally-initialized
  projection matrices across all heads before the scaled dot-product attention computation,
  with the diagonal initialization allowing heads to start independently and gradually
  develop cross-head interactions during training.
---

# Knocking-Heads Attention

## Quick Facts
- arXiv ID: 2510.23052
- Source URL: https://arxiv.org/abs/2510.23052
- Authors: Zhanchao Zhou; Xiaodong Chen; Haoxing Chen; Zhenzhong Lan; Jianguo Li
- Reference count: 20
- Primary result: Introduces KHA mechanism achieving 1.26 average score improvement across downstream tasks with minimal overhead

## Executive Summary
This paper introduces knocking-heads attention (KHA), a novel attention mechanism that enables cross-head communication in multi-head attention models while maintaining head specialization. KHA achieves this by applying shared, diagonally-initialized projection matrices across all heads before the scaled dot-product attention computation. The method adds minimal computational overhead (<1% additional FLOPs and parameters) and can be seamlessly integrated into various attention variants including MHA, GQA, and GTA. Extensive experiments demonstrate KHA's effectiveness across different model scales and architectures, including 6.1B parameter MoE models trained on 1T tokens.

## Method Summary
KHA introduces shared projection matrices applied after standard head-specific projections but before attention computation. The key innovation is diagonal initialization of these shared matrices, allowing heads to start independently and gradually develop cross-head interactions during training. The best-performing variant uses an MLP structure on values only: `Ṽ_i = 2·(V_i·W_up ⊙ Sigmoid(V_i·W_gate))·W_down`, with W_up and W_down initialized as diagonal matrices and W_gate as zeros. This approach adds minimal computational overhead while improving training stability and model performance across various attention variants and model scales.

## Key Results
- KHA achieves 1.26 average score improvement across downstream tasks
- Particularly excels in language understanding (+4.32), code (+3.9), and math (+1.62) tasks
- Adds minimal computational overhead (<1% additional FLOPs and parameters)
- Demonstrates significant improvements in training stability with reduced loss spikes
- Effective across different model scales including 6.1B parameter MoE models

## Why This Works (Mechanism)

### Mechanism 1: Cross-Head Feature-Level Interaction via Shared Projections
Applying shared projection matrices across attention heads enables beneficial cross-head communication (feature-level interaction), improving model performance and training stability. The paper introduces knocking-heads projections (shared matrices T, applied to Q, K, or V) before scaled dot-product attention, allowing heads to influence each other's feature representations. This overcomes the limitation of standard MHA where heads operate independently until final concatenation.

### Mechanism 2: Preserving Head Specialization via Diagonal Initialization
Initializing shared projection matrices diagonally allows the model to start with independent heads (like standard MHA) and gradually learn cross-head interactions, preserving beneficial head specialization. Shared matrices (T for linear, W_up/W_down for MLP) are initialized as diagonal/identity mappings, with off-diagonal elements free to learn non-zero values during training. This creates a curriculum: heads start specialized and only learn to collaborate as beneficial.

### Mechanism 3: Implicit Regularization via Head-Sharing
The head-sharing mechanism acts as an implicit regularizer, improving training stability and reducing loss spikes. By tying parameters across heads via shared T, the model's capacity is constrained and representations are smoothed. This prevents extreme or erratic behavior in individual heads from destabilizing the entire training process.

## Foundational Learning

- Concept: Multi-Head Attention (MHA) - Q, K, V projections, scaled dot-product, concatenation.
  - Why needed here: KHA is a modification on top of MHA. Without understanding standard MHA, the "knocking" addition doesn't make sense.
  - Quick check question: Can you write down the formula for standard multi-head attention, including the dimensionality of Q, K, V?

- Concept: Parameter Sharing in Deep Learning.
  - Why needed here: The core of KHA is a shared projection matrix. Understanding why sharing parameters can be beneficial (regularization, reduced redundancy) is crucial.
  - Quick check question: In a CNN, what is an example of parameter sharing, and what is its primary benefit?

- Concept: Initialization Strategies (Identity/Diagonal vs. Random).
  - Why needed here: The paper explicitly claims random initialization of shared matrices is bad and diagonal/identity initialization is critical. Understanding why initialization matters is key.
  - Quick check question: What would happen if you initialized all weights in a neural network to zero? How does diagonal initialization relate to the identity function?

## Architecture Onboarding

- Component map:
  Input: X (L x d) -> Standard MHA Projections (W_Q, W_K, W_V) -> KHA Projections (shared T or MLP) -> Scaled Dot-Product Attention -> Concatenation & Output Projection

- Critical path:
  1. Input X is projected by head-specific W_Q, W_K, W_V
  2. KHA Step: Resulting Q_i, K_i, V_i are transformed by shared T (or MLP). Crucially, initialization must be diagonal.
  3. Transformed Q_i', K_i', V_i' are fed into standard scaled dot-product attention
  4. Outputs are concatenated and projected

- Design tradeoffs:
  - Linear vs. MLP: Linear is faster, can be absorbed for zero inference cost. MLP is more expressive but has non-zero overhead.
  - Q/K/V Selection: Paper finds T_V is most critical. T_Q and T_K are optional and provide smaller gains.
  - Head Configuration: Benefits scale with KV head count. Most effective with GQA (group=4, 4 KV heads).

- Failure signatures:
  - Random initialization of shared matrices -> high training loss (heads too similar)
  - Applying KHA-MLP to all Q, K, and V -> parameter explosion
  - Using KHA-MLP with MLA -> incompatible (MLA doesn't materialize Q/K/V explicitly)
  - Degraded performance if regularization is too strong or head specialization is lost

- First 3 experiments:
  1. Sanity Check: Implement KHA-Linear on a small transformer (2-layer, 4 heads) on a simple copy task. Compare diagonal initialization vs. random initialization of T. Verify diagonal init converges and random init fails/degrades.
  2. Ablation on Position: Train a small language model (e.g., on Wikitext-2) with KHA-Linear. Run three sweeps: (a) Apply T only to V. (b) Apply T only to Q. (c) Apply T to Q, K, and V. Compare validation losses to confirm V is the most impactful.
  3. Integration with GQA: Take a standard GQA implementation. Integrate KHA-MLP (diagonal init on W_up/W_down, zero init on W_gate). Train on a medium-scale dataset and monitor for loss spikes compared to a baseline GQA model. Verify the <1% FLOP overhead.

## Open Questions the Paper Calls Out

### Open Question 1
Does the performance improvement from KHA depend on the specific cognitive demands of the task (e.g., reasoning vs. factual recall)? The paper reports significant improvements in Language Understanding (+4.32), Code (+3.9), and Math (+1.62), but negligible or negative results in Professional Knowledge (-0.42) and General Knowledge (+0.23). It remains undetermined whether cross-head interaction is inherently more beneficial for multi-step reasoning than for knowledge retrieval tasks.

### Open Question 2
Is the diagonal initialization strategy effective for stabilizing training in non-softmax attention mechanisms? The paper focuses specifically on softmax-based attention, leaving the interaction between KHA and linear attention architectures unexplored. Linear attention mechanisms approximate the softmax operation differently, so it's unclear if the "knocking" projections would retain their regularization benefits or interfere with kernel approximations.

### Open Question 3
Does the training stability provided by KHA stem primarily from implicit regularization or from learned feature interaction? The paper hypothesizes that reduced loss spikes stem from the head-sharing mechanism acting as implicit regularization, but the visualization shows learned non-diagonal structures. It remains undetermined if the reduction in loss spikes is solely due to the structural constraint of shared weights (regularization) or if the model learning to share features actively smooths the optimization landscape.

## Limitations

- Training data dependence: Primary evaluation relies on proprietary 1T-token dataset with unspecified composition
- Architecture specificity: Results primarily demonstrated on GQA and MoE architectures, with limited ablation on pure MHA or LRA
- Scale sensitivity: Relationship between model size and KHA's relative benefit not thoroughly explored
- Initialization sensitivity: Limited exploration of initialization space beyond diagonal matrices
- Training dynamics: Beyond loss spikes, other training dynamics not extensively characterized

## Confidence

**High Confidence (Level 4/5)**: Claims about cross-head communication via shared projections and importance of diagonal initialization are directly supported by mathematical formulation and ablation experiments.

**Medium Confidence (Level 3/5)**: Claims about magnitude of performance improvements (1.26 average score, specific task gains) are consistent across experiments but limited by proprietary dataset and lack of public code.

**Medium-Low Confidence (Level 2/5)**: Claims about generality across all attention variants and model scales are demonstrated on GHA and MoE but lack comprehensive validation across diverse architectures.

## Next Checks

1. **Initialization Ablation Study**: Systematically vary the initialization strength of KHA matrices (diagonal values of 0.1, 0.5, 1.0, 2.0) and measure training loss trajectories to clarify whether diagonal initialization is truly critical or if a broader initialization range works.

2. **Architecture Transfer Test**: Implement KHA on a standard pure MHA architecture (without GQA grouping) and train on a public dataset like C4 or The Pile. Compare to baseline MHA to verify claimed generality across attention variants.

3. **Head Specialization Analysis**: During KHA training, measure head similarity metrics (cosine similarity between head outputs) and attention pattern diversity over training steps to validate whether diagonal initialization truly preserves head specialization while enabling gradual cross-head communication.