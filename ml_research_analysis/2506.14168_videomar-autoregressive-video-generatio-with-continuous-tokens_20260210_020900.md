---
ver: rpa2
title: 'VideoMAR: Autoregressive Video Generatio with Continuous Tokens'
arxiv_id: '2506.14168'
source_url: https://arxiv.org/abs/2506.14168
tags:
- video
- generation
- videomar
- autoregressive
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VideoMAR, a decoder-only autoregressive video
  generation model with continuous tokens. It proposes next-frame diffusion loss for
  mask-video integration, temporal short-to-long curriculum learning, spatial progressive
  resolution training, and progressive temperature strategy to mitigate accumulation
  error.
---

# VideoMAR: Autoregressive Video Generatio with Continuous Tokens

## Quick Facts
- arXiv ID: 2506.14168
- Source URL: https://arxiv.org/abs/2506.14168
- Reference count: 40
- Key outcome: VideoMAR achieves state-of-the-art performance on VBench-I2V benchmark, surpassing Cosmos I2V with 9.3% fewer parameters, 0.5% less training data, and 0.2% fewer GPU resources while demonstrating spatial and temporal extrapolation capabilities via 3D rotary embeddings.

## Executive Summary
VideoMAR introduces a decoder-only autoregressive video generation model with continuous tokens that combines temporal frame-by-frame generation with spatial masked generation. The model employs a frame-wise causal attention mask (temporal causality with spatial bidirectionality), next-frame diffusion loss for continuous token integration, temporal short-to-long curriculum learning, spatial progressive resolution training, and progressive temperature strategy to mitigate accumulation error. VideoMAR achieves 82.56 total score on VBench-I2V benchmark, outperforming Cosmos I2V with significant efficiency gains.

## Method Summary
VideoMAR is a 36-layer decoder-only transformer that generates video autoregressively using continuous VAE latents instead of discrete tokens. The model uses frame-wise causal attention where frame t attends to all previous frames (0 to t-1) and all tokens within frame t itself (bidirectional spatial attention). Training employs a next-frame diffusion loss applied only to masked tokens in frame t while preserving complete context from previous frames. The model is trained with temporal curriculum learning (5→13→25 frames) and spatial progressive resolution (256×256→480×768). At inference, a progressive temperature strategy reduces temperature from 1.0 to 0.9 for later frames to mitigate accumulation error.

## Key Results
- Achieves 82.56 total score on VBench-I2V benchmark, surpassing Cosmos I2V
- Uses 9.3% fewer parameters than baseline while maintaining superior performance
- Demonstrates effective spatial and temporal extrapolation capabilities using 3D rotary embeddings
- Shows 0.5% reduction in training data and 0.2% fewer GPU resources compared to baseline

## Why This Works (Mechanism)

### Mechanism 1: Temporal Causality with Spatial Bi-directionality
A decoder-only architecture generates coherent video by treating time as a strictly causal sequence while treating spatial dimensions of each frame as bidirectional. The model uses frame-wise causal attention mask: frame t attends to all tokens in frames 0 to t-1 (causal) and all tokens within frame t itself (bidirectional). This preserves sequential nature of video while allowing parallel generation of intra-frame tokens via masked modeling.

### Mechanism 2: Next-Frame Diffusion Loss
Integrating diffusion loss on masked continuous tokens enables autoregressive modeling without vector quantization. Instead of predicting next discrete token ID, the model denoises continuous latent tokens. During training, a frame t is randomly selected; preceding frames are visible, frame t is partially masked, and subsequent frames are fully masked. Diffusion loss is applied only to masked tokens in frame t.

### Mechanism 3: Progressive Temperature for Accumulation Error
Reducing sampling temperature for later frames mitigates exposure bias and accumulation errors in long autoregressive sequences. The model schedules temperature from 1.0 down to 0.9, effectively "sharpening" predictions as video sequence extends to maintain visual quality and suppress noise accumulation.

## Foundational Learning

- **Concept: Continuous vs. Discrete Latents**
  - Why needed: VideoMAR rejects VQ-VAE in favor of continuous VAE latents for better gradient flow
  - Quick check: Can you explain why standard Cross-Entropy loss cannot be applied to output of standard VAE?

- **Concept: Masked Autoregression (MAR)**
  - Why needed: Paper builds on MAR, extending it to video; understand how "masking" replaces "next-token prediction"
  - Quick check: In standard GPT (NTP), token t attends to t-1. In MAR, what does masked token attend to?

- **Concept: KV Cache**
  - Why needed: Paper claims massive efficiency gains via "simultaneous temporal-wise KV cache"
  - Quick check: When generating Frame 10, how does KV cache prevent need to re-compute attention for Frames 1-9?

## Architecture Onboarding

- **Component map:** Image & Text -> Qwen2-1.5B (Text Encoder) & Cosmos-Tokenizer (VAE) -> 36-layer Decoder-only Transformer -> Diffusion MLP (3 blocks) for predicting continuous noise/velocity

- **Critical path:** The Attention Masking Logic is most critical implementation detail. Must enforce that query tokens in Frame t can see Key/Value tokens in all 0 to t (subject to spatial masking within t), but strictly not Frame t+1.

- **Design tradeoffs:**
  - Continuous vs. Discrete: Gains reconstruction quality and gradient flow but loses semantic stability of discrete codebooks, requiring diffusion head which is slower to sample
  - Curriculum Learning: Requires re-starting or staging training (5→13→25 frames), complicating training pipeline but necessary for convergence

- **Failure signatures:**
  - Late-frame collapse: Visual quality degrades into noise or static towards end of video (check progressive temperature)
  - Motion Stagnation: Video looks like slideshow (check if temporal attention is mistakenly bidirectional)
  - Spatial Inconsistency: Objects morph within single frame (check spatial bidirectional attention implementation)

- **First 3 experiments:**
  1. Sanity Check (Overfit): Train on single video snippet, verify perfect reconstruction using Next-Frame Diffusion Loss
  2. Ablation (Causal Mask): Compare Frame-wise Causal Mask against "Total Mask" (bidirectional across time) on VBench-I2V
  3. Efficiency Benchmark: Profile inference speed with and without KV-caching on 25-frame sequence

## Open Questions the Paper Calls Out

- Can VideoMAR effectively unify diverse generative tasks such as text-to-image, text-to-video, and video editing within single autoregressive framework? (Section 6 mentions capacity for "multiple tasks unification" but only explored image-to-video and video-to-video)

- Can VideoMAR function as controllable world model by replacing standard text prompts with frame-level interactive action conditions? (Section 6 mentions can "naturally function as interactive world model via replacing the prompt with frame-level interactive action condition")

- Does scaling training data with high-quality, complex motion examples significantly improve model's ability to generate large and dynamic motions? (Section H notes current deficiency in large and complex motions due to data scale)

## Limitations

- Training data (0.5M internal video-text pairs) is not publicly available, limiting reproducibility and external validation
- Temporal extrapolation capability demonstrated only up to 25 frames, uncertainty about performance on significantly longer sequences
- Spatial progressive resolution training requires careful curriculum scheduling that may not generalize to all video generation scenarios

## Confidence

- **High Confidence (4/5):** Core architectural innovations well-supported by ablation studies showing significant performance gains (82.56 vs 78.81 total score on VBench-I2V)
- **Medium Confidence (3/5):** Extrapolation claims demonstrated but rely on synthetic test conditions; real-world scenarios may present different challenges
- **Low Confidence (2/5):** Claims about avoiding "slow-start problem" in MAR based on qualitative comparisons without direct quantitative validation

## Next Checks

1. **Cross-Dataset Generalization Test:** Evaluate VideoMAR on held-out external video dataset (e.g., Kinetics-600 or HD-VILA-100M) to verify spatial and temporal extrapolation capabilities generalize beyond internal dataset.

2. **Long-Form Generation Stability:** Generate videos of 100+ frames (beyond 25-frame training maximum) and measure degradation rate in quality metrics across time to quantify practical limits of progressive temperature strategy.

3. **Alternative Continuous Token Comparison:** Replace Cosmos-Tokenizer continuous latents with alternative continuous token system while keeping all other components constant to isolate whether performance gains stem from token representation or architectural innovations.