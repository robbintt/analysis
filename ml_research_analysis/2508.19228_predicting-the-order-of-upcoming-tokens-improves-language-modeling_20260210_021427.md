---
ver: rpa2
title: Predicting the Order of Upcoming Tokens Improves Language Modeling
arxiv_id: '2508.19228'
source_url: https://arxiv.org/abs/2508.19228
tags:
- loss
- token
- training
- language
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Token Order Prediction (TOP) as a more efficient
  auxiliary training objective for language models compared to Multi-Token Prediction
  (MTP). TOP predicts the order of upcoming tokens using a learning-to-rank loss rather
  than exactly predicting future tokens, requiring only a single additional unembedding
  layer instead of multiple transformer layers.
---

# Predicting the Order of Upcoming Tokens Improves Language Modeling

## Quick Facts
- arXiv ID: 2508.19228
- Source URL: https://arxiv.org/abs/2508.19228
- Reference count: 3
- Introducing Token Order Prediction (TOP) as an efficient auxiliary training objective for language models

## Executive Summary
This paper introduces Token Order Prediction (TOP) as a more efficient auxiliary training objective for language models compared to Multi-Token Prediction (MTP). TOP predicts the order of upcoming tokens using a learning-to-rank loss rather than exactly predicting future tokens, requiring only a single additional unembedding layer instead of multiple transformer layers. The authors pretrain models of 340M, 1.8B, and 7B parameters using NTP, MTP, and TOP objectives, evaluating them on eight standard NLP benchmarks. Results show that TOP outperforms both NTP and MTP baselines even at scale, with the 7B TOP model surpassing both 7B NTP and MTP baselines on general tasks. The method demonstrates improved performance as parameter count grows, suggesting its potential value for larger-scale language models.

## Method Summary
Token Order Prediction (TOP) is a novel auxiliary training objective that predicts the order of upcoming tokens rather than their exact identities. The method employs a learning-to-rank loss function to optimize the ordering of predicted tokens, requiring only a single additional unembedding layer compared to standard next-token prediction. The authors pretrain three model sizes (340M, 1.8B, and 7B parameters) using NTP, MTP, and TOP objectives on the SlimPajama dataset. These pretrained models are then evaluated on eight standard NLP benchmarks including MMLU, ARC Challenge, HellaSwag, and GSM8K. The training procedure follows standard transformer pretraining protocols with careful hyperparameter tuning for each objective.

## Key Results
- TOP outperforms both NTP and MTP baselines across multiple model sizes on eight standard NLP benchmarks
- The 7B TOP model surpasses both 7B NTP and MTP baselines on general tasks
- TOP requires only a single additional unembedding layer compared to multiple transformer layers needed for MTP
- Performance improvements of TOP increase with model scale, suggesting better scaling behavior

## Why This Works (Mechanism)
TOP leverages relative ordering information rather than absolute token prediction, which is a more compact representation that requires fewer parameters. The learning-to-rank loss optimizes the ordering of multiple candidate tokens simultaneously, creating a richer training signal than standard NTP. This approach reduces the computational overhead of predicting multiple exact tokens while still capturing the distributional properties of future token sequences. The single unembedding layer requirement makes TOP architecturally simpler and more parameter-efficient than MTP.

## Foundational Learning
- **Next Token Prediction (NTP)**: Standard language modeling objective predicting the next token given context; foundational baseline for comparison
- **Multi-Token Prediction (MTP)**: Extension predicting multiple future tokens; requires multiple transformer layers for each additional token position
- **Learning-to-rank loss**: Optimization objective that orders items rather than predicting exact values; enables more efficient multi-token modeling
- **Unembedding layer**: Transposes token embeddings to project hidden states back to vocabulary space; single layer sufficient for TOP vs multiple for MTP
- **Transformer pretraining**: Process of training language models on large text corpora; evaluated with three different objectives (NTP, MTP, TOP)
- **Fine-tuning benchmarks**: Standard evaluation tasks including MMLU, ARC, HellaSwag, GSM8K; used to measure generalization across domains

## Architecture Onboarding

**Component Map**: Input sequence -> Transformer layers -> [Optional MTP layers] -> Unembedding layer(s) -> Output predictions

**Critical Path**: Token embeddings → Transformer blocks → TOP unembedding layer → Order predictions

**Design Tradeoffs**: TOP sacrifices exact token prediction accuracy for ordering accuracy, trading precision for efficiency and reduced parameter count

**Failure Signatures**: Degraded performance on tasks requiring exact token matching; potential loss of fine-grained semantic information in token representations

**First Experiments**:
1. Implement TOP unembedding layer and learning-to-rank loss for a small transformer model
2. Compare training convergence speed between NTP, MTP, and TOP on a subset of SlimPajama
3. Evaluate ordering accuracy vs exact prediction accuracy on held-out validation data

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation focused primarily on LLaMA-7B architecture with limited ablation studies on model depth and width
- Scaling analysis based on only three model sizes (340M, 1.8B, and 7B parameters) may not capture full scaling behavior
- No investigation of different pretraining dataset compositions or training dynamics over time

## Confidence

**Efficiency of TOP objective** (Medium): Parameter count comparisons show TOP is more efficient than MTP, but comprehensive training time and inference measurements are lacking

**Superiority over NTP and MTP** (High): Consistent improvements across multiple tasks and model sizes with statistical significance supported by results

**Scaling behavior** (Medium): Trend suggests improvements grow with scale, but limited model size range prevents definitive conclusions about scaling properties

## Next Checks

1. Conduct controlled experiments measuring wall-clock training time, memory consumption, and throughput for NTP, MTP, and TOP objectives across all three model sizes

2. Train intermediate model sizes (e.g., 500M, 3B parameters) and evaluate performance to better characterize scaling behavior

3. Evaluate TOP with different transformer architectures (variations in depth, width, attention mechanisms) to determine architecture dependency of observed benefits