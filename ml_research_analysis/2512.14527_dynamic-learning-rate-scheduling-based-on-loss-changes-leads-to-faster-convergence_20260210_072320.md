---
ver: rpa2
title: Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence
arxiv_id: '2512.14527'
source_url: https://arxiv.org/abs/2512.14527
tags:
- greedylr
- loss
- learning
- rate
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GreedyLR is a novel learning rate scheduler that dynamically adjusts
  the learning rate based on loss changes during training. It multiplies the learning
  rate by a factor F when loss improves and divides by F when loss worsens, providing
  adaptive optimization without requiring explicit gradient information.
---

# Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence

## Quick Facts
- arXiv ID: 2512.14527
- Source URL: https://arxiv.org/abs/2512.14527
- Authors: Shreyas Subramanian; Bala Krishnamoorthy; Pranav Murthy
- Reference count: 40
- Primary result: Novel loss-based learning rate scheduler that dynamically adjusts LR based on loss changes, achieving 5.4% lower final loss in pre-training and 3-5× faster recovery from noise perturbations

## Executive Summary
GreedyLR introduces a novel learning rate scheduler that dynamically adjusts the learning rate based on loss changes during training. The scheduler multiplies the learning rate by a factor F when loss improves and divides by F when loss worsens, providing adaptive optimization without requiring explicit gradient information. It demonstrates strong performance across diverse scenarios, including small models (<500M parameters) where it outperforms baseline schedulers in 86.73% of experiments, and large models (500M-7B parameters) with 83.33% as-good-or-better performance in fine-tuning. The scheduler shows exceptional robustness to five types of noise perturbations, with 37% lower median final loss than the best traditional scheduler and 3-5× faster recovery after disruptions.

## Method Summary
GreedyLR is a novel learning rate scheduler that dynamically adjusts the learning rate during training based on the current loss. The scheduler multiplies the learning rate by a factor F when loss improves and divides by F when loss worsens, providing adaptive optimization without requiring explicit gradient information. Key parameters include F ∈ (0,1) (typically 0.5-0.95), patience=10 epochs, min_lr=10% of initial LR, and smoothing window=50. The method works across diverse scenarios including small models (<500M parameters), large models (500M-7B parameters), and various tasks from fine-tuning to pre-training. The scheduler demonstrates exceptional robustness to noise perturbations and achieves stable convergence when F≥0.5.

## Key Results
- GreedyLR achieves 5.4% lower final loss versus cosine scheduling in pre-training experiments on Llama-3.2-1B
- Shows 86.73% better performance than baseline schedulers on small models (<500M parameters)
- Demonstrates 83.33% as-good-or-better performance on large models (500M-7B parameters) in fine-tuning
- Exhibits 37% lower median final loss than the best traditional scheduler in noise-robustness tests
- Achieves 3-5× faster recovery speed (median: 12 steps vs 45 steps for Cosine) after training disruptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adjusting learning rate based on loss direction accelerates convergence by adapting step size to local optimization progress.
- Mechanism: When loss decreases (lt < lt-1), the algorithm divides learning rate by factor F (increasing it), taking larger steps in a promising direction. When loss increases, it multiplies by F (decreasing it), becoming more conservative. This creates a feedback loop between optimization progress and step size.
- Core assumption: Loss changes serve as a reliable proxy for gradient direction quality; improving loss indicates productive parameter updates.
- Evidence anchors:
  - [abstract] "GreedyLR... adaptively adjusts the learning rate during training based on the current loss"
  - [Section 3] "If the loss value decreases... we want to take a larger step in the same direction by increasing the learning rate"
  - [corpus] Related work on adaptive scheduling (VolSched, Seesaw) confirms adaptive LR mechanisms can improve convergence, though corpus lacks direct validation of loss-based triggering specifically
- Break condition: Highly non-convex landscapes with saddle points or high curvature where loss fluctuations reflect noise rather than genuine optimization dynamics.

### Mechanism 2
- Claim: A scaling factor threshold at F ≥ 0.5 provides stable convergence without requiring precise hyperparameter tuning.
- Mechanism: Theoretical analysis derives optimal F = 1 - 1/Lmax (where Lmax is the smoothness constant), but empirical results show that any F ≥ 0.5 achieves stable convergence with performance variation within 1.5%.
- Core assumption: The smoothness constant Lmax is typically unknown in practice; practitioners need a robust heuristic.
- Evidence anchors:
  - [Section 4.3] "F=0.25 caused catastrophic divergence... while all F ≥ 0.5 achieved stable convergence with nearly identical performance (losses 1.89, 1.92, 1.91—within 1.5%)"
  - [Section A.7] "practitioners can simply ensure F ≥ 0.5 for LLM fine-tuning tasks"
  - [corpus] No direct corpus validation of this specific threshold; related schedulers focus on different adaptation mechanisms
- Break condition: Assumption: Threshold was established on LLM fine-tuning; generalization to all training regimes requires further verification per the authors' limitations section.

### Mechanism 3
- Claim: Loss-based adaptation provides superior recovery from training perturbations compared to fixed schedules.
- Mechanism: When noise spikes cause loss increases, GreedyLR automatically reduces learning rate, preventing divergence. As training stabilizes and loss improves, it gradually increases LR again. This creates 3-5× faster recovery to baseline performance.
- Core assumption: The patience, cooldown, and smoothing mechanisms prevent overreaction to transient noise while maintaining responsiveness to genuine optimization shifts.
- Evidence anchors:
  - [Section 4.4] "GreedyLR demonstrates exceptional recovery capability with median recovery of 134× and best-case recovery of 72,999×"
  - [Section 4.4] "GreedyLR exhibits 3-5× faster recovery speed (median: 12 steps vs 45 steps for Cosine)"
  - [corpus] Weak corpus support; related papers on WSD schedulers discuss cooldown dynamics but not noise recovery specifically
- Break condition: Perturbation patterns not captured by the five tested noise types (Gaussian, periodic spike, random spike, adversarial, clean); adversarial scenarios with sustained opposition may degrade performance.

## Foundational Learning

- **Learning Rate Scheduling Paradigms**:
  - Why needed here: GreedyLR departs from fixed schedules (cosine decay, exponential) by adapting dynamically; understanding why schedules matter provides context for what GreedyLR replaces.
  - Quick check question: Can you explain why cosine annealing might underperform in high-variance training scenarios?

- **Zeroth-Order Optimization Signals**:
  - Why needed here: GreedyLR uses loss changes rather than gradient information; this is a zeroth-order approximation that trades precision for computational simplicity.
  - Quick check question: What information is lost when using loss changes instead of gradient norms for adaptation decisions?

- **Convergence Theory Basics (Smoothness and Convexity)**:
  - Why needed here: The theoretical guarantees (Theorem A.1, A.2) assume Lmax-smooth convex objectives; understanding these assumptions reveals where guarantees may not hold in practice.
  - Quick check question: Why might the O(1/T) convergence guarantee not apply when training modern LLMs with Adam on non-convex objectives?

## Architecture Onboarding

- **Component map**:
  - Initialize LR and F ≥ 0.5 → Compare current loss vs previous → If improved: γt = γt-1 / F; If worsened: γt = γt-1 × F → Apply patience, cooldown, warmup → Clamp to [min_lr, max_lr] bounds

- **Critical path**:
  1. Initialize with base LR, set F ≥ 0.5, configure patience (default: 10), enable smoothing (window: 50)
  2. Each step: Compute loss → Apply smoothing → Compare to previous → Increment good/bad epoch counters
  3. When patience exceeded: Adjust LR multiplicatively → Reset counters → Enter cooldown/warmup phase
  4. Clamp to [min_lr, max_lr] bounds

- **Design tradeoffs**:
  - Simplicity vs. precision: Uses loss (zeroth-order) instead of gradients (first-order), reducing computational overhead but potentially missing directional information
  - Responsiveness vs. stability: Lower patience = faster adaptation but more sensitive to noise; higher patience = more stable but slower to respond
  - Aggressiveness vs. safety: Lower F = more aggressive LR changes (faster convergence risk); higher F = conservative (slower but safer)

- **Failure signatures**:
  - Catastrophic divergence: Loss explodes early → Likely F < 0.5; increase to ≥0.5
  - Stagnant loss with no improvement: LR may have hit min_lr bound too early; increase min_lr or reduce initial LR
  - Oscillating loss: Patience too low or smoothing disabled; increase patience/window_size
  - No adaptation occurring: Threshold too high; reduce threshold parameter

- **First 3 experiments**:
  1. **Stability validation**: Fine-tune a small model (e.g., BERT-base) with F ∈ {0.5, 0.75, 0.95}, patience=10, smoothing=50. Verify all converge and compare final loss vs. cosine baseline. Expected: All F ≥ 0.5 within 1.5% of each other.
  2. **Noise robustness test**: Add Gaussian noise to loss during training (simulating mini-batch variance), compare GreedyLR vs. cosine recovery time. Expected: GreedyLR recovers 3-5× faster per Section 4.4.
  3. **Scale transfer check**: Pre-train or fine-tune a 1B+ parameter model with GreedyLR using same hyperparameters as small model. Monitor for divergence or unexpected LR trajectories. Expected: 83%+ as-good-or-better performance per Section 4.2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does GreedyLR maintain its convergence advantages in full-scale pre-training runs (hundreds of thousands of steps) for models larger than 7B parameters?
- Basis in paper: [explicit] The authors note their pre-training experiments on Llama-3.2-1B were "limited to 1000 steps on a single architecture with one random seed due to computational constraints," leaving long-term dynamics at scale unverified.
- Why unresolved: The computational cost of running full-scale frontier model pre-training (thousands of GPU-hours) was prohibitive for this study.
- What evidence would resolve it: Empirical results from pre-training runs exceeding 100k steps on multi-billion parameter models, comparing GreedyLR against cosine decay in terms of final loss and training stability.

### Open Question 2
- Question: Can the theoretical convergence guarantees for GreedyLR be extended to adaptive optimizers like AdamW operating on non-convex landscapes?
- Basis in paper: [explicit] The paper states that the convergence analysis is "formulated for SGD with smooth convex objectives," whereas "modern deep learning predominantly employs adaptive optimizers like Adam and AdamW on non-convex problems where these assumptions may not hold."
- Why unresolved: There is a gap between the theoretical framework (SGD, convex) and the practical implementation (AdamW, non-convex), leaving the interaction between GreedyLR's global adjustments and Adam's per-parameter adaptation theoretically undefined.
- What evidence would resolve it: A formal proof of convergence for GreedyLR when used with adaptive gradient methods, or empirical analysis showing whether loss-based adjustments provide complementary information to Adam's moment estimates.

### Open Question 3
- Question: How does GreedyLR perform in optimization domains outside of NLP and Computer Vision, such as Reinforcement Learning (RL) or Graph Neural Networks (GNNs)?
- Basis in paper: [explicit] The authors explicitly list "reinforcement learning... audio processing, graph neural networks, or scientific computing" as domains that "remain underexplored" and require further investigation to validate generalizability.
- Why unresolved: The current experiments focused on supervised learning tasks (NLP, CV), but RL utilizes reward signals with different statistical properties than supervised loss functions.
- What evidence would resolve it: Benchmarking experiments on standard RL environments or GNN datasets to evaluate if the "loss change" heuristic translates effectively to these distinct optimization landscapes.

### Open Question 4
- Question: Is the empirical stability threshold of F ≥ 0.5 consistent across all model architectures and training regimes, or does it require adjustment for specific use cases?
- Basis in paper: [explicit] While the paper identifies a stability threshold, it notes this threshold was "established only for LLM fine-tuning and its generalization to all training regimes requires further verification."
- Why unresolved: The relationship between the scaling factor F and the smoothness constant Lmax suggests the threshold could theoretically shift depending on the loss landscape of different architectures.
- What evidence would resolve it: A systematic sweep of the scaling factor F across diverse architectures (e.g., CNNs, Transformers, MLPs) in pre-training versus fine-tuning scenarios to map the boundaries of stability.

## Limitations
- The F ≥ 0.5 stability threshold was established empirically on LLM fine-tuning tasks and may not generalize to all training regimes
- Pre-training experiments were limited to 1000 steps on a single architecture with one random seed due to computational constraints
- Performance claims rely heavily on specific noise perturbation models that may not capture all real-world training disruptions
- Comparative experiments against cosine scheduling don't account for potential benefits from other adaptive schedulers

## Confidence
- **High confidence**: The core mechanism of loss-based learning rate adjustment is well-specified and the stability threshold at F ≥ 0.5 is empirically validated across multiple experiments
- **Medium confidence**: The claims about superior performance on small models (<500M parameters) and noise robustness are supported by experimental data but may not generalize to all training scenarios
- **Medium confidence**: The pre-training results on Llama-3.2-1B show promising 5.4% improvement, but single-model validation limits generalizability

## Next Checks
1. **Threshold generalization test**: Validate the F ≥ 0.5 stability claim on non-LLM architectures (CNNs, Transformers on vision tasks) and different optimization objectives beyond standard cross-entropy loss
2. **Noise perturbation expansion**: Test GreedyLR against additional noise types including label noise, gradient clipping instability, and hardware-induced gradient variance to verify the 37% improvement claim holds across broader failure modes
3. **Scheduler comparison benchmark**: Conduct head-to-head comparison with other adaptive schedulers (OneCycle, ReduceLROnPlateau, Ranger) on identical tasks to establish relative performance positioning beyond the cosine baseline