---
ver: rpa2
title: Anomaly Detection Based on Critical Paths for Deep Neural Networks
arxiv_id: '2505.14967'
source_url: https://arxiv.org/abs/2505.14967
tags:
- detection
- paths
- critical
- anomaly
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method called Anomaly Detection based on
  Critical Paths (ADCP) for detecting various types of anomaly samples in deep neural
  networks. The core idea is to extract critical detection paths from the DNN using
  genetic evolution and mutation, and then use these paths to distinguish between
  normal and anomaly samples.
---

# Anomaly Detection Based on Critical Paths for Deep Neural Networks

## Quick Facts
- arXiv ID: 2505.14967
- Source URL: https://arxiv.org/abs/2505.14967
- Reference count: 40
- Key outcome: ADCP method detects adversarial, out-of-distribution, and noise anomalies with superior accuracy across multiple DNN models and datasets.

## Executive Summary
This paper introduces ADCP, a method for detecting various types of anomaly samples in pre-trained deep neural networks. The approach extracts critical detection paths from the DNN using genetic evolution and mutation, then uses these paths to distinguish between normal and anomaly samples through SVDD models. Experimental results on MNIST, CIFAR-10, and SVHN datasets show ADCP outperforms state-of-the-art methods for detecting adversarial attacks, out-of-distribution samples, and noise, achieving high accuracy and good generalization across different anomaly types.

## Method Summary
ADCP extracts critical detection paths from DNNs using a genetic mutation algorithm that evolves neuron sequences based on their ability to distinguish normal from anomalous samples. For each class, the method generates 21 critical paths (one neuron per layer) by iteratively mutating random paths and selecting those that maximize true positive rate on a mixed validation set. These paths are then used to train SVDD models, which detect anomalies by measuring how far test samples deviate from the normal training distribution. Detection involves extracting features along the critical paths, computing SVDD scores, applying min-max normalization, and aggregating results through a voting mechanism to determine if samples are anomalous.

## Key Results
- ADCP outperforms state-of-the-art methods (ODIN, Mahalanobis, CSI, DDU, SINC) on MNIST, CIFAR-10, and SVHN for all three anomaly types
- Achieves higher AUROC and TPR@95%TNR compared to baselines across adversarial, out-of-distribution, and noise samples
- Demonstrates good generalization ability when trained on one anomaly type and tested on others
- Shows effective performance with different DNN architectures including LeNet, VGG16, and ResNet

## Why This Works (Mechanism)
ADCP works by identifying and leveraging the most discriminative neuron sequences (critical paths) that a DNN naturally uses to distinguish between classes. The genetic evolution process discovers paths that are particularly sensitive to deviations from normal input patterns, making them effective for anomaly detection. By training SVDD models on these specialized paths, the method creates multiple decision boundaries that collectively capture the manifold of normal data, allowing detection of various anomaly types that fall outside these boundaries.

## Foundational Learning
- **Genetic Algorithm for Path Selection**: Why needed - To automatically discover discriminative neuron sequences without manual feature engineering; Quick check - Verify path diversity across layers and convergence behavior
- **Support Vector Domain Description (SVDD)**: Why needed - To model the boundary of normal data without requiring labeled anomaly examples; Quick check - Test different kernel bandwidth selection methods
- **True Positive Rate Optimization**: Why needed - To ensure selected paths effectively distinguish between normal and anomalous samples; Quick check - Monitor TPR improvement during evolution iterations
- **Voting Mechanism for Score Aggregation**: Why needed - To combine multiple SVDD outputs into a robust final decision; Quick check - Validate threshold calculation and min-max normalization

## Architecture Onboarding
**Component Map**: Input -> Critical Path Extraction -> SVDD Training -> Feature Extraction -> Score Aggregation -> Anomaly Decision
**Critical Path**: The genetic evolution algorithm that selects 21 discriminative neuron sequences per class based on TPR maximization
**Design Tradeoffs**: Path extraction is computationally expensive (5000 iterations per class) but enables model-agnostic anomaly detection without retraining the base DNN
**Failure Signatures**: Low detection accuracy indicates poor SVDD parameter selection or insufficient path diversity; path extraction convergence stalls suggest mutation strategy needs adjustment
**First Experiments**: 1) Validate SVDD kernel parameter selection on small dataset, 2) Test path extraction with synthetic anomalies, 3) Verify voting mechanism threshold calculation

## Open Questions the Paper Calls Out
- Can ADCP be adapted for non-image data modalities like NLP or network intrusion detection?
- How can the computational efficiency of the genetic evolution algorithm be optimized for large-scale DNNs?
- Can a mathematically rigorous method replace the current heuristic genetic algorithm for path selection?
- How sensitive is path topology to the specific composition of anomalies in the mixed set used during evolution?

## Limitations
- Computational expense of genetic evolution algorithm limits scalability to large DNNs
- Lack of mathematical proof for path selection optimality
- SVDD parameter selection (bandwidth) not explicitly specified
- Performance on SVHN dataset not reported in main results

## Confidence
- **High Confidence**: Overall framework and path extraction mechanism
- **Medium Confidence**: Experimental results on MNIST and CIFAR-10
- **Low Confidence**: Implementation details of voting mechanism and threshold determination

## Next Checks
1. Systematically test different RBF kernel bandwidth selection methods for SVDD to optimize detection accuracy
2. Analyze path diversity across layers to ensure meaningful feature extraction and avoid convergence to similar neurons
3. Evaluate threshold sensitivity by testing detection performance across different anomaly types and dataset distributions