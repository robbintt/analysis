---
ver: rpa2
title: Quantifying task-relevant representational similarity using decision variable
  correlation
arxiv_id: '2506.02164'
source_url: https://arxiv.org/abs/2506.02164
tags:
- neural
- decision
- representations
- accuracy
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Decision Variable Correlation (DVC) to quantify
  task-relevant representational similarity between models and brains. DVC measures
  the correlation between decoded decisions on individual samples, focusing on task-relevant
  information rather than general representational alignment.
---

# Quantifying task-relevant representational similarity using decision variable correlation

## Quick Facts
- arXiv ID: 2506.02164
- Source URL: https://arxiv.org/abs/2506.02164
- Reference count: 40
- Key outcome: DVC quantifies task-relevant similarity between neural and model representations, revealing that models with higher ImageNet performance show lower similarity to monkey V4/IT.

## Executive Summary
This paper introduces Decision Variable Correlation (DVC) to measure task-relevant representational similarity between neural recordings and deep neural network models. Unlike traditional metrics that focus on representational geometry, DVC isolates the information relevant to task performance by decoding decision variables from neural and model representations. The method was validated using macaque V4/IT recordings from an image categorization task and 43 ImageNet-trained models, revealing that model-monkey similarity is consistently lower than model-model similarity and surprisingly decreases with model performance.

## Method Summary
DVC quantifies similarity by projecting high-dimensional neural/model representations onto a one-dimensional decision axis using Linear Discriminant Analysis (LDA), then computing the correlation between these decision variables across observers. To correct for measurement noise, the method employs a split-half normalization procedure that divides units into two halves and computes a noise-corrected correlation. The pipeline involves PCA dimensionality reduction (to 25 components), LDA fitting for each binary class pair, decision variable extraction, and split-half reliability correction, averaged across all class pairs.

## Key Results
- Model-model similarity was comparable to monkey-monkey similarity (~0.57), while model-monkey similarity was consistently lower
- DVC decreased with increasing ImageNet-1k performance, contrary to expectations
- Adversarial training enhanced robustness but did not improve model-monkey similarity in task-relevant dimensions
- Pre-training on larger datasets did not improve model-monkey similarity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DVC isolates task-relevant information by projecting high-dimensional neural representations onto a one-dimensional decision axis, effectively discarding task-irrelevant variance.
- **Mechanism:** The method applies Linear Discriminant Analysis (LDA) to find the axis that maximizes class separation for a binary classification task. By projecting representations onto this axis, the system reduces the complexity of the comparison to a single scalar (decision variable) per trial, reflecting the observer's evidence state.
- **Core assumption:** The assumption is that an optimal linear readout (LDA) is sufficient to approximate the decision variable utilized by the brain or model, and that relevant information is linearly decodable.
- **Evidence anchors:**
  - [abstract] "DVC quantifies the image-by-image correlation between the decoded decisions based on the internal neural representations."
  - [section 3.1] "We can take its neural representation and find the optimal decision axis for solving the categorization task... project... onto the decision axis and obtain its decision variable."
  - [corpus] Related work "Bridging Functional and Representational Similarity via Usable Information" suggests functional similarity (which DVC measures) is distinct from representational geometry, supporting the focus on the decision axis.
- **Break condition:** This mechanism may fail if the true decision boundary is highly non-linear or if the dimensionality reduction (PCA applied prior to LDA to handle few samples) inadvertently removes the relevant signal.

### Mechanism 2
- **Claim:** The split-half normalization procedure recovers the "true" correlation between observers by correcting for the attenuation bias caused by independent measurement noise.
- **Mechanism:** The system splits the units (neurons/channels) into two halves ($A_1, A_2$ and $B_1, B_2$). It computes a noise-corrected correlation ($\rho_{corrected}$) by dividing the geometric mean of the cross-observer correlations by the geometric mean of the within-observer reliabilities (Eq 1).
- **Core assumption:** Assumes noise is independent, additive, and symmetric across the split halves, and that the signal of interest is distributed across the units.
- **Evidence anchors:**
  - [section 3.2] "To correct for the under-estimation of DVCs due to measurement noise, we develop a split-half procedure... This correction removes the bias introduced by independent, additive symmetric noise."
  - [appendix a.1] Proof provided showing $\rho_{corrected} = \rho_{true}$.
  - [corpus] Corpus evidence is weak regarding specific noise correction in similarity metrics, though "Spectral Analysis of Representational Similarity" acknowledges challenges in limited neuron counts, implicitly supporting the need for robust estimation.
- **Break condition:** If noise is correlated across units (structured noise) or if the number of units is too low to form reliable splits, the normalization may become unstable or produce values > 1.

### Mechanism 3
- **Claim:** DVC captures the consistency of decision strategies independent of decision biases or overall accuracy, allowing it to detect divergences that metrics like Cohen's Kappa miss.
- **Mechanism:** Unlike Cohen's Kappa, which relies on discrete choice agreement and is sensitive to criterion shifts (bias), DVC operates on the continuous decision variable before the decision criterion is applied. This allows it to distinguish between two observers who make the same errors (high DVC) vs. those who use different internal features (low DVC).
- **Core assumption:** Assumes that the internal decision variable is continuous and normally distributed (standard Signal Detection Theory assumptions).
- **Evidence anchors:**
  - [section 4.5] "DVC is insensitive to the decision biases, while the error consistency quantified based on Cohen's Kappa captures both shared behavior biases and consistency."
  - [figure 5g] Simulation showing DVC remains constant as bias increases, whereas Kappa fluctuates.
  - [corpus] "Beyond topography..." discusses robustness and representation, highlighting that standard metrics may miss functional nuances.
- **Break condition:** If the underlying decision variables are not normally distributed or are categorical in nature, the Pearson correlation used in DVC may not accurately reflect the similarity of the decision strategies.

## Foundational Learning

- **Concept: Signal Detection Theory (SDT)**
  - **Why needed here:** DVC is a direct extension of SDT to two observers. Understanding "decision variables" (internal evidence) vs. "criterion" (bias) is essential to grasp what DVC measures.
  - **Quick check question:** If two observers have identical decision variables but opposite decision criteria (e.g., one is conservative, one is liberal), would DVC be high or low? (Answer: High, because DVC correlates the internal variables, not the final choices).

- **Concept: Linear Discriminant Analysis (LDA)**
  - **Why needed here:** This is the decoder used to extract the 1D decision variable from the high-dimensional brain/model activity. Understanding it explains why the paper focuses on "task-relevant" dimensions.
  - **Quick check question:** Why might LDA require dimensionality reduction (PCA) before application in this context? (Answer: To prevent overfitting/instability when the number of features (neurons/units) exceeds the number of samples (images)).

- **Concept: Representational Similarity Analysis (RSA) vs. Functional Similarity**
  - **Why needed here:** To contrast DVC with existing methods. RSA measures geometric structure (geometry of the manifold), whereas DVC measures functional output consistency.
  - **Quick check question:** If two representations have high RSA similarity but low DVC, what does that imply about their task-solving strategies? (Answer: They encode similar information geometry but use different dimensions to solve the specific task).

## Architecture Onboarding

- **Component map:** Input representations -> Unit split -> PCA (25 dims) -> LDA decoder -> Decision variables -> Pearson correlation -> Split-half normalization -> DVC score
- **Critical path:** The stability of the LDA step is critical. If the PCA dimensionality is too low, the decision variable is noisy; if too high, LDA becomes unstable. The paper standardizes on 25 PC dimensions.
- **Design tradeoffs:**
  - **Linear vs. Non-linear Decoders:** The paper opts for linear (LDA) for stability and interpretability. Non-linear decoders (like MLPs) showed lower DVC, suggesting they overfit to noise rather than capturing the shared signal (Appendix C.3).
  - **Binary vs. Multi-class:** DVC operates on binary class pairs. This is granular but computationally intensive for many classes (requires averaging over all pairs).
- **Failure signatures:**
  - **Inflated DVC (>1):** Can occur if the split-half normalization denominator ($r_{self}$) is very small due to high internal noise or insufficient splits (Appendix C.6).
  - **Negative Correlation with Performance:** Expect to see DVC decrease as ImageNet Top-1 accuracy increases. A failure to reproduce this suggests the model class being tested has fundamentally different training dynamics (e.g., perhaps explicitly trained on neural data).
- **First 3 experiments:**
  1.  **Sanity Check (Monkey-Monkey):** Verify that two monkeys performing the same task yield a DVC significantly > 0 (benchmark is ~0.57). This validates the signal-to-noise ratio of the recordings.
  2.  **Ablation on Decoders:** Compare LDA-based DVC vs. Logistic Regression-based DVC vs. Cohen's Kappa. Confirm that Kappa is sensitive to accuracy differences while DVC is not (replicating Fig 5).
  3.  **Architecture Sweep:** Run DVC on a standard ResNet vs. a Vision Transformer (ViT) against the neural data. Check if the negative correlation with ImageNet accuracy holds across distinct architectures (replicating Fig 2c).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Decision Variable Correlation (DVC) be accurately recovered under more general noise conditions beyond independent additive noise?
- **Basis in paper:** [explicit] Section 3.2 states, "How to best recover DVC under more general noise conditions remains an interesting question for future work."
- **Why unresolved:** The proposed split-half normalization method assumes independent, additive, symmetric noise to correct for attenuation bias, but biological and model noise may violate these assumptions.
- **What evidence would resolve it:** Deriving or validating a correction formula for DVC that remains unbiased under correlated or non-Gaussian noise distributions.

### Open Question 2
- **Question:** Can DVC be combined at behavioral and neural representation levels to dissect the specific contributions of decision consistency versus shared decision biases?
- **Basis in paper:** [explicit] Section 5 suggests, "It would be interesting to combine the analysis of DVC at the behavioral level [20] and the neural representation level to dissect the contribution of consistency of DVs and the shared biases of the observers."
- **Why unresolved:** The current study focuses on inferring DVC from neural representations, whereas previous work focused on behavior; a unified framework to separate bias from consistency across these levels is not yet established.
- **What evidence would resolve it:** An analysis pipeline that simultaneously processes spike counts and choice data to yield independent estimates of neural alignment and behavioral strategy overlap.

### Open Question 3
- **Question:** Does training networks on ecologically motivated datasets or with explicit internal noise modeling close the fundamental divergence in task-relevant representations found between models and monkeys?
- **Basis in paper:** [inferred] The Discussion notes that "it is unclear how to close the gap" and lists "training networks using datasets that better resemble the stimulus statistics... [or] capture the stimulus noise and internal noise" as "promising" directions.
- **Why unresolved:** The study demonstrates that adversarial training and larger datasets fail to improve modelâ€“monkey similarity, leaving the proposed ecological and noise-based solutions as untested hypotheses.
- **What evidence would resolve it:** Experiments showing a positive correlation (or at least a flattening of the negative trend) between ImageNet performance and monkey DVC for models specifically trained on these alternative paradigms.

### Open Question 4
- **Question:** How does DVC systematically compare to other similarity measures (e.g., RSA, CKA) in terms of sensitivity to task-irrelevant dimensions across different datasets?
- **Basis in paper:** [explicit] Section 5 states, "It would also be interesting to systematically compare DVC to other proposed similarity measures [15; 5; 19; 17]."
- **Why unresolved:** While the paper demonstrates that DVC and RSA differ in a simple simulation regarding task-irrelevant noise, a comprehensive empirical comparison across diverse architectures and brain areas is lacking.
- **What evidence would resolve it:** A large-scale benchmark comparing DVC scores against RSA and linear regression scores across multiple neural datasets, analyzing the divergence in their rankings of model-brain alignment.

## Limitations

- The negative correlation between DVC and ImageNet accuracy is surprising and requires deeper investigation to rule out alternative explanations beyond fundamental representation divergence
- The split-half normalization assumes independent, symmetric noise, but real-world neural recordings may violate these assumptions
- The study focuses on a specific image categorization task, limiting generalizability to other cognitive domains

## Confidence

- **High:** The core methodological framework of DVC is sound and well-defined. The split-half normalization procedure is mathematically correct.
- **Medium:** The interpretation of the negative DVC-performance correlation. While the evidence is presented, the mechanistic explanation is speculative and requires further testing.
- **Medium:** The claim that adversarial training enhances model-model but not model-monkey similarity is well-supported, but the underlying reasons are not fully elucidated.

## Next Checks

1. **Alternative Decoder Comparison:** Replicate the key findings using a non-linear decoder (e.g., small MLP) to confirm that the linear LDA choice is not the primary driver of the observed effects. Specifically, check if the negative correlation with performance persists.

2. **Dataset Ablation:** Test DVC on a different image dataset (e.g., CIFAR-10) to see if the model-monkey divergence is specific to ImageNet or a more general phenomenon across natural image domains.

3. **Noise Structure Analysis:** Conduct an empirical analysis of the noise structure in the neural recordings (e.g., noise correlation across units) to validate the assumptions of the split-half normalization procedure.