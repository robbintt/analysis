---
ver: rpa2
title: Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform
  Large-Scale Data Training
arxiv_id: '2510.20059'
source_url: https://arxiv.org/abs/2510.20059
tags:
- reasoning
- medical
- language
- arxiv
- persian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of enhancing reasoning capabilities
  in small Persian medical language models, which typically struggle with deliberate,
  multi-step reasoning tasks due to limited data and computational resources. The
  proposed method employs a two-stage framework combining Reinforcement Learning with
  AI Feedback (RLAIF) and Direct Preference Optimization (DPO) to improve reasoning
  skills.
---

# Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training

## Quick Facts
- arXiv ID: 2510.20059
- Source URL: https://arxiv.org/abs/2510.20059
- Reference count: 25
- Primary result: gaokerena-R (8B) outperforms gaokerena-V (57M tokens) on medical reasoning benchmarks with average gains of 8.39% (MMLU) and 10.73% (IBMSEE)

## Executive Summary
This study demonstrates that small Persian medical language models (8B parameters) can achieve superior reasoning performance through targeted preference-based fine-tuning rather than simply increasing training data volume. The authors propose a two-stage framework combining Reinforcement Learning with AI Feedback (RLAIF) and Direct Preference Optimization (DPO) to enhance multi-step reasoning capabilities in medical question-answering tasks. The approach successfully translates and verifies medical MCQs, generates preference pairs through teacher-student feedback loops, and fine-tunes a baseline model to achieve significant accuracy improvements on medical reasoning benchmarks while using substantially less training data than previous approaches.

## Method Summary
The study employs a two-stage framework combining RLAIF and DPO to enhance reasoning in small Persian medical LLMs. The process begins with translating ~18K medical multiple-choice questions from English to Persian, verified by two AI judges requiring unanimous agreement. Preference pairs are generated using DeepSeek-R as teacher and aya-expanse-8b as student through two strategies: (1) student generates incorrect response, teacher provides correct Chain-of-Thought (CoT) reasoning, and (2) teacher provides critique without answer, student retries, and correct attempts are marked preferred. The resulting ~11K preference pairs (2M preferred tokens, 2.5M rejected tokens) are used to fine-tune the student model via DPO. The gaokerena-R model demonstrates significant improvements over its predecessor trained on 57 million tokens while using only 2.5 million rejected tokens.

## Key Results
- gaokerena-R achieves average accuracy gains of 8.39 percentage points on FA MED MMLU compared to gaokerena-V
- gaokerena-R shows 10.73 percentage points improvement on IBMSEE dataset using Chain-of-Thought prompting
- Model trained on 2 million preferred tokens and 2.5 million rejected tokens outperforms model trained on 57 million tokens

## Why This Works (Mechanism)
The method works by focusing the model's learning capacity on reasoning quality rather than raw data quantity. By using teacher-student feedback loops, the model learns to recognize and correct its reasoning errors through exposure to preferred response patterns. The two-stage approach first establishes quality preferences through RLAIF, then optimizes the model to align with these preferences via DPO, creating a more efficient learning trajectory that emphasizes reasoning correctness over memorization.

## Foundational Learning
- **Reinforcement Learning with AI Feedback (RLAIF)**: Why needed - Provides automated quality assessment without human annotation costs. Quick check - Verify AI feedback quality by sampling and evaluating outputs.
- **Direct Preference Optimization (DPO)**: Why needed - Enables efficient fine-tuning using preference pairs rather than full reward modeling. Quick check - Monitor preference loss during training for convergence.
- **Chain-of-Thought (CoT) Prompting**: Why needed - Improves complex reasoning by breaking down problems into sequential steps. Quick check - Compare performance with and without CoT on held-out data.
- **Teacher-Student Learning**: Why needed - Leverages stronger models to guide weaker models' learning process. Quick check - Measure student performance gap reduction over training epochs.
- **Translation Quality Verification**: Why needed - Ensures Persian medical content accuracy for effective learning. Quick check - Calculate inter-annotator agreement rates for translation verification.
- **Preference Pair Generation**: Why needed - Creates training signal that distinguishes good from bad reasoning. Quick check - Sample preference pairs and verify logical consistency.

## Architecture Onboarding
- **Component Map**: Persian MCQs -> Translation & Verification -> Teacher-Student Preference Generation -> DPO Fine-tuning -> gaokerena-R
- **Critical Path**: The teacher-student feedback loop is the critical path, as it generates the preference pairs that drive all subsequent learning improvements.
- **Design Tradeoffs**: Prioritizes reasoning quality over data quantity, sacrificing model scale for efficiency; uses AI judges instead of human annotators to reduce costs but introduces potential bias; focuses on medical domain specificity rather than general reasoning.
- **Failure Signatures**: Low inter-annotator agreement in translation verification; teacher generates flawed reasoning despite ground truth; model overfits to CoT prompt format and underperforms on direct prompting.
- **First Experiments**: 1) Generate and verify 100 Persian medical MCQs to test translation pipeline. 2) Run teacher-student loop on 10 sample questions to validate preference generation. 3) Train on 1K preference pairs to test DPO convergence.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on specific high-resource teacher models (DeepSeek-R, DeepSeek-V3) and proprietary judges (grok-3-mini, gpt-4.1-mini) limits accessibility and reproducibility.
- Lack of quantitative quality metrics for translation verification introduces uncertainty about dataset consistency across the full corpus.
- Evaluation is narrowly focused on multiple-choice medical QA with Chain-of-Thought prompting, leaving unclear whether improvements transfer to other reasoning tasks or direct prompting scenarios.

## Confidence
- **High confidence**: The two-stage RLAIF+DPO framework is technically sound and represents a valid approach to preference-based fine-tuning. The observation that gaokerena-R outperforms gaokerena-V on medical reasoning benchmarks is clearly demonstrated with specific accuracy gains.
- **Medium confidence**: The translation methodology and judge-based verification process is adequate for producing usable Persian medical MCQs, though the lack of quantitative quality metrics introduces uncertainty about consistency across the dataset.
- **Low confidence**: The generalizability of the reasoning improvements beyond medical multiple-choice questions and Chain-of-Thought prompting remains uncertain without additional experimental validation.

## Next Checks
1. Evaluate gaokerena-R on non-medical reasoning tasks and with direct prompting (no Chain-of-Thought) to assess task and prompting generalization of the reasoning improvements.
2. Perform ablation studies comparing RLAIF+DPO against alternative fine-tuning methods (RLHF, standard supervised fine-tuning) using identical datasets and hyperparameters to isolate the contribution of each component.
3. Measure inter-annotator agreement and translation quality metrics for a subset of the Persian medical MCQs to quantify the reliability of the translation and verification process.