---
ver: rpa2
title: 'FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for
  Financial Trading'
arxiv_id: '2502.11433'
source_url: https://arxiv.org/abs/2502.11433
tags:
- policy
- trading
- financial
- value
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FLAG-Trader integrates LLM-based language reasoning with RL policy
  optimization for financial trading. It uses a partially fine-tuned LLM as the policy
  network, where frozen layers retain pre-trained knowledge while trainable layers
  adapt to financial decision-making.
---

# FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading

## Quick Facts
- arXiv ID: 2502.11433
- Source URL: https://arxiv.org/abs/2502.11433
- Reference count: 40
- Primary result: FLAG-Trader consistently outperforms buy-and-hold strategies and LLM-agentic baselines across multiple stocks

## Executive Summary
FLAG-Trader integrates LLM-based language reasoning with RL policy optimization for financial trading. It uses a partially fine-tuned LLM as the policy network, where frozen layers retain pre-trained knowledge while trainable layers adapt to financial decision-making. The model processes market data via structured text prompts and employs policy gradient methods like PPO to optimize trading performance. Experiments show FLAG-Trader consistently outperforms both buy-and-hold strategies and LLM-agentic baselines across multiple stocks, achieving superior cumulative returns and Sharpe ratios. Notably, a 135M-parameter open-source LLM model surpasses much larger proprietary models, demonstrating the effectiveness of RL fine-tuning in enhancing LLM-driven trading strategies.

## Method Summary
FLAG-Trader formulates financial trading as a finite-horizon POMDP with discrete actions {Buy, Sell, Hold}. The LLM backbone (SmolLM2-135M-Instruct) uses frozen base layers and trainable top layers to balance pre-trained knowledge with domain adaptation. Market states are converted to structured text prompts containing historical prices, account balances, and previous actions. PPO with actor-critic architecture optimizes trading performance using Sharpe ratio improvement as the reward signal. The policy and value heads share trainable LLM layers while maintaining separate parameters.

## Key Results
- FLAG-Trader consistently outperforms buy-and-hold strategies and LLM-agentic baselines across multiple stocks
- A 135M-parameter open-source LLM model surpasses much larger proprietary models in trading performance
- Achieves superior cumulative returns and Sharpe ratios compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Partial fine-tuning preserves general reasoning while adapting to financial decision-making.
- **Mechanism:** Frozen base layers (θ_frozen) retain pre-trained linguistic knowledge; trainable top layers (θ_train) adapt to domain-specific patterns. Policy and value heads share these trainable layers while maintaining separate parameters (θ_P, θ_V).
- **Core assumption:** Financial reasoning builds on general language understanding rather than requiring fundamental representational changes.
- **Evidence anchors:** "a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning"; "This separation allows the model to retain general language understanding while adapting to financial decision-making with minimal computational overhead"
- **Break condition:** If task requires modifying how the LLM processes numerical relationships at a fundamental level, partial fine-tuning may prove insufficient.

### Mechanism 2
- **Claim:** Text-based state representation enables LLMs to process market data through their native reasoning capabilities.
- **Mechanism:** Numerical states s_t converted to structured prompts lang(s_t) via templates containing historical prices, account status, and previous metrics; LLM outputs action logits; softmax with action masking produces valid probability distribution.
- **Core assumption:** Trading-relevant patterns can be adequately captured in text form; LLM reasoning transfers to this domain.
- **Evidence anchors:** "processes market data via structured text prompts"; Shows prompt format with JSON-like structure encoding prices, cash balance, asset positions, recent rewards
- **Break condition:** If critical signals are inherently numerical/multi-dimensional (e.g., order book microstructure), text encoding may lose essential information.

### Mechanism 3
- **Claim:** Policy gradient optimization (PPO) aligns LLM outputs with trading objectives through reward-driven updates.
- **Mechanism:** PPO computes advantage estimates via GAE; clipped objective prevents large policy deviations; gradients flow through trainable LLM layers to optimize for Sharpe ratio rewards.
- **Core assumption:** The reward signal (Sharpe ratio changes) adequately captures trading objectives; PPO constraints prevent catastrophic forgetting.
- **Evidence anchors:** "the system converges to a relatively stable policy that becomes less sensitive to the initial prompts, indicating that RL training allows the LLM-based agent to refine its strategy"; Reward defined as R(s_t, a_t) = SR_t - SR_{t-1} (Sharpe ratio improvement)
- **Break condition:** If rewards are too sparse, noisy, or delayed, gradient signal may not effectively shape policy.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: Core training algorithm; must understand clipped objectives, advantage estimation, and update constraints to debug convergence issues.
  - Quick check question: Can you explain why PPO clips the probability ratio rather than using unconstrained policy gradient?

- **Concept: Actor-Critic Architecture**
  - Why needed here: FLAG-Trader uses shared LLM backbone with separate policy (actor) and value (critic) heads; understanding their interaction is essential for debugging.
  - Quick check question: What happens if the value network consistently overestimates returns?

- **Concept: Parameter-Efficient Fine-Tuning**
  - Why needed here: Framework relies on partial fine-tuning; knowing what gets frozen vs. trained affects debugging and extension strategies.
  - Quick check question: Why might fine-tuning only top layers fail for tasks requiring different low-level representations?

## Architecture Onboarding

- **Component map:**
  Environment (market data) → State s_t → Prompt Template → lang(s_t) [text representation] → LLM Backbone: Frozen layers (θ_frozen) → Trainable layers (θ_train) → Policy Head θ_P → Action probs; Value Head θ_V → State value → Action sampling and Advantage estimation (GAE) → PPO Update

- **Critical path:** Prompt design → State encoding quality → LLM representation → Action masking correctness → Reward computation → Gradient flow through trainable layers. Errors in prompt formatting or action masking will silently corrupt training.

- **Design tradeoffs:**
  - Smaller LLM (135M params): Faster training, lower inference cost, but may lack reasoning depth. Paper shows it works, but generalization bounds unknown.
  - Discrete action space {Buy, Sell, Hold}: Simpler optimization but limits position sizing flexibility.
  - Sharpe ratio reward: Risk-adjusted but may underemphasize tail risk (MDD not in objective).

- **Failure signatures:**
  - Policy collapses to single action: Check action masking, reward distribution, exploration entropy coefficient.
  - Value loss diverges: Check learning rate separation (η vs β), gradient clipping.
  - Inference produces invalid actions: Verify prompt template matches training format exactly.
  - No improvement over buy-and-hold: Check warm-up period data quality, reward signal scaling.

- **First 3 experiments:**
  1. **Sanity check:** Run FLAG-Trader on a single stock with synthetic data where optimal policy is known; verify convergence to expected behavior.
  2. **Ablation on frozen layer ratio:** Compare freezing different percentages of layers (e.g., 50% vs 75% vs 90%) to identify minimal trainable parameters needed.
  3. **Reward function variant:** Replace Sharpe ratio reward with simple return; compare final Sharpe and MDD to understand reward-objective alignment.

## Open Questions the Paper Calls Out
- **Question:** Can the FLAG-Trader framework be effectively extended to handle dynamic portfolio optimization and explicit risk constraints in a multi-asset setting?
  - **Basis in paper:** The authors state in the Limitations section that "real-world trading requires stringent risk management" and that the current model "optimizes for financial returns without explicitly incorporating risk-sensitive constraints."
  - **Why unresolved:** The current experimental design is limited to single-asset trading tasks (one stock or crypto at a time) with a discrete action space (Buy, Sell, Hold), which does not address the complexity of allocating capital across multiple assets simultaneously while managing risk limits.
  - **What evidence would resolve it:** A demonstration of the framework managing a diverse portfolio with specific constraints (e.g., volatility targeting or maximum drawdown limits) across multiple assets simultaneously.

- **Question:** How can continual learning or meta-learning techniques be integrated to ensure the agent generalizes across non-stationary market regimes over long time horizons?
  - **Basis in paper:** The paper notes that "financial markets exhibit high volatility and non-stationarity," posing challenges for long-term generalization, and explicitly suggests exploring "continual learning or meta-learning" in future work.
  - **Why unresolved:** The experiments utilize specific, finite test periods (e.g., Oct 2020–May 2021), leaving the model's ability to adapt to evolving market dynamics over extended periods or drastic regime shifts (e.g., financial crises) unverified.
  - **What evidence would resolve it:** Longitudinal studies or "out-of-sample" tests across distinct market cycles (e.g., transitioning from bull to bear markets) showing stable performance without catastrophic forgetting.

- **Question:** Does the reliance on structured text prompts introduce decision-making biases, and can retrieval-augmented methods enhance robustness?
  - **Basis in paper:** The authors acknowledge in the Limitations section that "its reliance on structured prompts could introduce biases in decision-making" and propose exploring "retrieval-augmented methods" as a solution.
  - **Why unresolved:** The paper uses a fixed template to convert numerical market states into text (e.g., "Price: $p...") but does not analyze how specific phrasing or the loss of numerical precision in this conversion impacts the agent's reasoning or introduces systematic errors.
  - **What evidence would resolve it:** An ablation study comparing different prompt formats or integrating a retrieval mechanism to assess changes in decision quality and robustness.

## Limitations
- Text-based state representation may lose critical numerical information from order book microstructure
- Sharpe ratio reward doesn't explicitly penalize maximum drawdown, potentially creating tail risk exposure
- Experiments focus on relatively stable assets during test period; performance in crisis conditions untested

## Confidence
- **High confidence** in the core mechanism: Integration of LLM reasoning with RL policy optimization is well-established, and ablation showing open-source models outperforming proprietary ones is compelling evidence
- **Medium confidence** in generalization: Outperforms baselines across multiple assets but test period (Oct 2020–May 2021) was relatively stable
- **Low confidence** in reward alignment: Sharpe ratio reward assumes this metric adequately captures trading objectives, but alternative reward designs not explored

## Next Checks
1. **Regime change stress test**: Run FLAG-Trader on historical crisis periods (2008 financial crisis, 2020 COVID crash) to evaluate performance degradation and risk management capabilities. Monitor Sharpe ratio and maximum drawdown across these periods compared to buy-and-hold baselines.

2. **Numerical signal ablation**: Replace the text-based state representation with direct numerical inputs (prices, volumes) processed through a small neural network. Compare performance to identify whether text encoding loses critical information for trading decisions.

3. **Reward function sensitivity analysis**: Implement alternative reward formulations (e.g., risk-aware rewards incorporating MDD, asymmetric utility functions) and compare final Sharpe ratios and drawdowns. This will reveal whether the Sharpe ratio reward creates unintended incentives or blind spots.