---
ver: rpa2
title: 'AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis'
arxiv_id: '2512.23424'
source_url: https://arxiv.org/abs/2512.23424
tags:
- kernel
- agent
- optimization
- code
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AKG kernel agent, a multi-agent system that
  automates kernel generation across diverse DSLs and hardware backends. The core
  innovation is a decoupled architecture that separates high-level optimization strategy
  from low-level code synthesis, using hardware-agnostic Unified Sketches that capture
  parallelization, memory access patterns, and tiling strategies.
---

# AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis

## Quick Facts
- arXiv ID: 2512.23424
- Source URL: https://arxiv.org/abs/2512.23424
- Reference count: 26
- Primary result: Multi-agent system achieving 90.9% pass@4 on dynamic shapes benchmark with 1.46× speedup over PyTorch Eager baselines

## Executive Summary
This paper presents AKG kernel agent, a multi-agent system that automates kernel generation across diverse DSLs and hardware backends. The core innovation is a decoupled architecture that separates high-level optimization strategy from low-level code synthesis, using hardware-agnostic Unified Sketches that capture parallelization, memory access patterns, and tiling strategies. The system employs four specialized agents (Designer, Coder, Verifier, Conductor) working in a coordinated workflow, with document-driven integration enabling support for new targets without modifying core agent logic. Evaluated on KernelBench Level 1, Triton-CUDA achieves 100% pass@4 across all operator categories. On a newly developed benchmark with dynamic input shapes, the system achieves 90.9% (Triton-CUDA) and 85.4% (Triton-Ascend) overall pass rates. Performance-wise, Triton-Ascend achieves a geometric mean speedup of 1.46× over PyTorch Eager baselines. The system demonstrates robust correctness and competitive performance across five DSL-backend combinations while addressing the critical challenges of performance, portability, and automation in modern kernel engineering.

## Method Summary
The AKG kernel agent system uses a four-agent architecture to automate kernel generation across DSLs (Triton, TileLang, CPP, CUDA-C) and hardware backends (NVIDIA GPU, Huawei Ascend NPU, CPU). The Designer agent creates hardware-agnostic Unified Sketches from operator specifications and hardware documentation, capturing parallelization strategies, memory access patterns, and tiling approaches. The Coder agent translates these sketches into target-specific DSL code using API documentation and retrieved examples. The Verifier performs compilation checks, numerical validation against reference implementations, and performance profiling. The Conductor orchestrates the workflow and classifies errors adaptively, routing syntax/API mistakes to the Coder and algorithmic/memory pattern flaws to the Designer. The system employs document-driven integration via structured DocSpec documentation for each DSL-backend combination, enabling zero-code extension to new targets. Hierarchical retrieval with feature extraction, semantic search, hard filtering, and shape matching supports the generation process. Iterative search uses an island model with stratified sampling for optimization.

## Key Results
- KernelBench Level 1: 100% pass@4 on Triton-CUDA across all operator categories (element-wise, reduction, normalization, tensor manipulation, matmul, indexing, sorting, fused)
- Dynamic shapes benchmark: 90.9% pass@4 (Triton-CUDA) and 85.4% pass@4 (Triton-Ascend) across 198-214 operators
- Performance: Triton-Ascend achieves geometric mean speedup of 1.46× over PyTorch Eager baselines
- Cross-platform robustness: System demonstrates competitive performance across five DSL-backend combinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling optimization strategy from code synthesis improves both correctness and portability.
- Mechanism: The Designer agent produces a hardware-agnostic Unified Sketch capturing parallelization, memory access patterns, and tiling strategies using primitives like `alloc`, `load`, `store`, and `compute` with semantic hints (e.g., "fastest" for registers, "fast" for shared memory). The Coder then translates this DSL-agnostic representation into target-specific code (Triton, CUDA-C, TileLang, etc.). This separation reduces cognitive load on individual LLM calls and enables the same sketch to map to GPU (grididx/threadidx), NPU (coreidx), or CPU (OpenMP/SIMD) backends.
- Core assumption: LLMs perform better when reasoning about algorithmic intent and syntax separately rather than simultaneously.
- Evidence anchors:
  - [abstract]: "core innovation is a decoupled architecture that separates high-level optimization strategy from low-level code synthesis"
  - [section 3.1]: "Decoupling of Strategy and Implementation... enables each agent to focus on a well-defined subtask"
  - [corpus]: KForge similarly uses collaborative agents separating generation from performance optimization, achieving platform-agnostic synthesis.

### Mechanism 2
- Claim: Document-driven integration enables zero-code extension to new DSLs and hardware backends.
- Mechanism: Rather than hard-coding DSL support, the system ingests structured documentation (DocSpec) covering: (1) basic syntax/execution model, (2) API signatures, (3) expert optimization suggestions, and (4) reference examples. Agents dynamically load and compress relevant documentation at runtime based on task requirements. New targets require only documentation preparation—not agent modification.
- Core assumption: LLMs can extract sufficient operational knowledge from structured documentation without fine-tuning.
- Evidence anchors:
  - [abstract]: "document-driven integration enabling support for new targets without modifying core agent logic"
  - [section 3.3]: "treats documentation as a first-class interface for system extension"
  - [corpus]: Weak comparative evidence—neighbor papers do not explicitly test documentation-only extension mechanisms.

### Mechanism 3
- Claim: Adaptive error routing reduces iteration count by directing failures to the appropriate agent.
- Mechanism: The Conductor classifies errors (compilation, runtime, numerical) via LLM analysis and routes accordingly: syntax/API mistakes return to Coder with fix suggestions; algorithmic/memory pattern flaws escalate to Designer for structural revision. This avoids wasting iterations on misdirected fixes.
- Core assumption: Error classification is sufficiently accurate to distinguish implementation from design failures.
- Evidence anchors:
  - [section 3.2.4]: "a simple API typo can be resolved by the Coder in one iteration, while a fundamental parallelization flaw triggers Designer intervention"
  - [algorithm 1]: Explicit classification logic routing SYNTAX/APIMISUSE to Coder, ALGORITHM/MEMORYPATTERN to Designer
  - [corpus]: STARK and cuPilot use similar multi-agent coordination but do not report explicit error-classification routing metrics.

## Foundational Learning

- Concept: GPU/NPU memory hierarchy (registers → shared memory/L1 → L2 → global memory)
  - Why needed here: Unified Sketch uses semantic hints like "fastest," "fast," "accumulator" that map to specific memory levels. Understanding this hierarchy is essential for interpreting sketches and predicting performance impact.
  - Quick check question: Can you explain why loading data once into shared memory and reusing it across thread blocks improves bandwidth utilization compared to repeated global memory access?

- Concept: Parallelization primitives (grid, block, thread; tilng; vectorization)
  - Why needed here: The Designer must reason about parallelism scope (parallel, grididx, coreidx) and tiling strategies. Without this foundation, sketches will produce suboptimal or incorrect parallelization.
  - Quick check question: For a matrix multiplication kernel, what determines optimal tile size given a GPU's shared memory capacity and warp size?

- Concept: Pass@k metric for code generation evaluation
  - Why needed here: The paper reports pass@4 correctness. Understanding this metric is necessary to interpret results and compare against baselines fairly.
  - Quick check question: If a system generates 4 samples per task and 3 out of 100 tasks have zero correct samples, what is the pass@4?

## Architecture Onboarding

- Component map: Operator specification → Designer (Unified Sketch) → Coder (DSL code) → Verifier (compile + correctness + perf) → Conductor (error classification → route back or finish)

- Critical path: Operator specification → Designer (Unified Sketch) → Coder (DSL code) → Verifier (compile + correctness + perf) → Conductor (error classification → route back or finish)

- Design tradeoffs:
  - Unified Sketch abstraction enables portability but adds translation complexity vs. direct code generation
  - Document-driven integration reduces engineering for new targets but depends on documentation quality
  - Island model preserves diversity but increases parallel computation cost

- Failure signatures:
  - Syntax errors in generated code → Coder iteration (API misuse)
  - Numerical mismatches exceeding tolerance → Conductor classifies as algorithmic vs. implementation
  - Compilation failures on specific backends → Check DocSpec completeness for that target
  - Performance regression after optimization rounds → Possible overfitting to specific input shapes

- First 3 experiments:
  1. Reproduce pass@4 on KernelBench Level-1 Triton-CUDA subset (MatMul + Elementwise categories) using provided configuration (4 samples, τ=0.004 for fp16) to validate baseline.
  2. Ablate the Conductor by forcing all errors to route through Coder only; measure iteration count increase and pass@4 degradation to quantify adaptive routing benefit.
  3. Add a new DSL target (e.g., HIP for AMD GPUs) by preparing DocSpec documentation only; test on 10 operators from the benchmark to validate document-driven integration claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Unified Sketch representation be extended to capture complex optimization patterns such as tensor core utilization and asynchronous execution?
- Basis in paper: [explicit] Authors state: "extending the Unified Sketch language to capture more complex optimization patterns such as tensor core utilization and asynchronous execution" as future work.
- Why unresolved: Current sketch primitives (alloc, load, store, compute) with @llm_hint decorators express basic parallelization and memory hierarchy but lack constructs for hardware-specific features like tensor cores or async pipelines.
- What evidence would resolve it: Demonstrated correct generation of kernels utilizing tensor cores or async memory operations across multiple backends with measurable performance gains.

### Open Question 2
- Question: How can automated documentation generation from existing codebases accelerate integration of new DSLs and hardware targets?
- Basis in paper: [explicit] Authors identify "developing automated techniques for generating high-quality documentation from existing codebases to accelerate integration of new targets" as future work.
- Why unresolved: Current document-driven integration requires manually prepared DocSpec structures; this creates a bottleneck when adding new targets, especially for proprietary or less-documented platforms.
- What evidence would resolve it: A tool that automatically extracts API signatures, optimization patterns, and example code from codebases, validated by successfully integrating a new DSL without manual DocSpec preparation.

### Open Question 3
- Question: Can reinforcement learning improve iterative search-based optimization by learning from performance feedback across generations?
- Basis in paper: [explicit] Authors list "incorporating reinforcement learning to guide the search process based on performance feedback" as a future direction.
- Why unresolved: Current optimization uses LLM-based comparative analysis between high/low-performing kernels but does not learn persistent policies; each operator optimization starts fresh without transferring learned strategies.
- What evidence would resolve it: An RL-based system demonstrating faster convergence to optimal implementations or improved final performance compared to the current island model with stratified sampling.

## Limitations
- Unknown LLM prompt templates and DocSpec documentation structure prevent faithful reproduction of reported results
- Performance claims difficult to validate without access to exact hardware configurations and baseline implementations
- System depends heavily on documentation quality for new DSL/backend integration

## Confidence
- **High confidence** in the decoupled architecture design and its theoretical benefits for portability and correctness separation
- **Medium confidence** in the document-driven integration mechanism, though effectiveness likely depends on documentation quality
- **Low confidence** in the specific numerical results without access to implementation details and evaluation configurations

## Next Checks
1. Conduct an ablation study comparing the full system against variants that route all errors through a single agent to quantify the benefit of adaptive error classification
2. Test the system's ability to support a new DSL target by preparing documentation only, without modifying core agent logic, and measuring success rate on a representative operator subset
3. Analyze the impact of Unified Sketch abstraction by comparing against a baseline that generates target code directly, measuring both correctness rates and iteration counts