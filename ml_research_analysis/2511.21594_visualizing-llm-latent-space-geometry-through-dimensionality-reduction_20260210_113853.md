---
ver: rpa2
title: Visualizing LLM Latent Space Geometry Through Dimensionality Reduction
arxiv_id: '2511.21594'
source_url: https://arxiv.org/abs/2511.21594
tags:
- latent
- states
- sequence
- llama
- gpt-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper visualizes the internal representations of Transformer-based
  language models through dimensionality reduction techniques. The authors extract
  latent states from GPT-2 and LLaMa models at multiple points within Transformer
  blocks, then apply PCA and UMAP to reveal geometric patterns in the latent space.
---

# Visualizing LLM Latent Space Geometry Through Dimensionality Reduction

## Quick Facts
- arXiv ID: 2511.21594
- Source URL: https://arxiv.org/abs/2511.21594
- Reference count: 28
- Primary result: Novel visualization of Transformer internal representations reveals attention-MLP geometric separation pattern across intermediate layers

## Executive Summary
This paper presents a systematic visualization framework for understanding Transformer-based language models' internal representations through dimensionality reduction. The authors extract latent states from GPT-2 and LLaMa models at multiple points within Transformer blocks, then apply PCA and UMAP to reveal geometric patterns in the latent space. The key finding is a clear separation between attention and MLP component outputs across intermediate layers - a pattern not previously documented. The work provides new insights into how these models organize and process information through their internal representations, revealing layerwise evolution patterns, positional embedding structures, and sequence-wise geometric organization.

## Method Summary
The authors extract latent states from Transformer models at six capture points per block (pre/post norm, attention/MLP outputs pre-add and post-add) using forward hooks. They process this data through dimensionality reduction using both PCA (for consistent axes and shape preservation) and UMAP (for clustering/local structure), with unit-vector normalization to focus on directional patterns rather than magnitude. The visualization pipeline captures geometry across layers, comparing attention and MLP outputs, analyzing positional patterns, and tracking layerwise evolution of latent states.

## Key Results
- Clear geometric separation between attention and MLP component outputs across intermediate layers, a novel finding
- Characterization of high-norm latent states at initial sequence position (position 0) that develops mid-network and attenuates near output
- Visualization of GPT-2's high-dimensional helical structure of positional embeddings and LLaMa's sequence-wise geometric patterns
- Demonstration of layerwise evolution of latent states showing how representations transform through the network

## Why This Works (Mechanism)

### Mechanism 1: Attention-MLP Geometric Separation
The residual stream framing suggests components write to different subspaces of the shared representation. Attention outputs may specialize in relational/contextual features while MLPs handle position-wise transformations, yielding geometrically separable output distributions. The Linear Representation Hypothesis justifies interpreting this directional separation as meaningful feature organization.

### Mechanism 2: Initial Position Massive Activation
Position 0 accumulates outsized activations across layers—emerging after initial blocks, peaking mid-network, and attenuating near the output. This occurs even in LLaMa with RoPE, suggesting the model learns position-0 detection beyond explicit positional embeddings, potentially serving as a bias term or global context anchor.

### Mechanism 3: Positional Encoding Geometry Transfer
GPT-2's learned embeddings form a high-dimensional helix preserved across PCA dimension pairs. LLaMa's RoPE yields a lower-dimensional pattern—early positions form a "tail" converging to a dense cloud—possibly reflecting relative-position computation or attention-head averaging effects.

## Foundational Learning

- Concept: Residual Stream Perspective
  - Why needed here: The paper frames components as reading from and writing to shared subspaces; understanding this is prerequisite to interpreting attention vs. MLP separation.
  - Quick check question: Can you explain why the residual stream view makes component output comparison meaningful in a shared coordinate system?

- Concept: Linear Representation Hypothesis (LRH)
  - Why needed here: Justifies using dimensionality reduction to reveal feature organization; underpins interpretation of directional separation.
  - Quick check question: What does LRH imply about the relationship between features and vector directions in latent space?

- Concept: PCA vs. UMAP Trade-offs
  - Why needed here: The paper uses both methods—PCA for consistent axes and shape preservation, UMAP for clustering/local structure.
  - Quick check question: When would you choose PCA over UMAP for comparing geometries across experimental conditions?

## Architecture Onboarding

- Component map: Text input → tokenization → forward pass with hook-based capture at all 6 points per block → save latent states with metadata → filter/normalize → fit dimensionality reduction → transform and visualize
- Critical path: The pipeline captures 6 points per block, converts to unit vectors, excludes position 0 outliers, applies PCA/UMAP, and visualizes with consistent color-coding (blue=attention, red=MLP)
- Design tradeoffs: PCA preserves global axes for ablation comparison; UMAP reveals clusters but lacks consistent coordinate frames; unit-vector conversion emphasizes direction over norm
- Failure signatures: Position-0 norms dominating visualizations; UMAP with Euclidean distance sensitive to norm outliers; averaging over wrong dimension erasing target structure
- First 3 experiments:
  1. Replicate attention-MLP separation visualization using provided code on a small dataset; verify separation persists with unit-vector normalization disabled
  2. Plot layerwise norm curves for position 0 vs. other positions; confirm peak in intermediate layers for both GPT-2 and LLaMa
  3. Fit PCA on intermediate-layer post-add latent states, project all 15 dimension pairs, and compare helix clarity (GPT-2) vs. tail-to-cloud pattern (LLaMa)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are the sequence-wise geometric patterns in RoPE models caused by the encoding method or relative feature convergence?
- Basis in paper: The authors state they "leave interpreting the sequence-wise latent state geometric patterns of RoPE models to future research."
- Why unresolved: Visualizations alone cannot disentangle RoPE's influence from the contributions of self-attention heads.
- What evidence would resolve it: Ablation studies isolating RoPE components or comparing models with different context lengths.

### Open Question 2
- Question: Does the geometric separation between attention and MLP outputs serve a functional computational purpose?
- Basis in paper: The authors identify this separation as a novel pattern but do not establish if it is required for model performance.
- Why unresolved: Observational geometry does not prove functional necessity.
- What evidence would resolve it: Intervention experiments that perturb or collapse the separation to observe performance changes.

### Open Question 3
- Question: What specific mechanism allows LLaMa to detect initial sequence positions to generate high-norm latent states without explicit embeddings?
- Basis in paper: The authors note LLaMa likely develops a "more complicated mechanism" for position 0 detection.
- Why unresolved: RoPE provides relative position information, making absolute position detection non-trivial.
- What evidence would resolve it: Activation patching or probing classifiers to identify circuits responsible for position 0 detection.

## Limitations
- Limited to two model architectures (GPT-2, LLaMa) and single dataset (PG-19), requiring cross-model validation
- Visualization methodology sensitivity to hyperparameters and normalization choices may influence observed patterns
- Challenge of separating positional from content effects in LLaMa's latent geometry not fully resolved

## Confidence

- **High Confidence**: Position 0 high-norm observation and GPT-2 helix structure are well-supported with consistent patterns and external corroboration
- **Medium Confidence**: Attention-MLP separation requires quantitative validation to confirm it's not an artifact of visualization choices
- **Low Confidence**: Interpretation of LLaMa's positional patterns remains speculative without mechanistic analysis

## Next Checks

1. **Quantitative separation validation**: Apply clustering metrics (silhouette score, Davies-Bouldin index) to measure attention-MLP separation across different layers, models, and datasets to determine if the pattern is robust or artifact-driven.

2. **Cross-architecture replication**: Test the visualization pipeline on additional model families (OPT, Mistral, or open-source GPT-3 variants) to verify whether attention-MLP separation and positional patterns generalize beyond GPT-2 and LLaMa.

3. **Positional vs. content disentanglement**: Conduct controlled experiments ablating positional information and varying sequence positions to isolate whether observed geometric patterns reflect true positional structure or content-position interactions.