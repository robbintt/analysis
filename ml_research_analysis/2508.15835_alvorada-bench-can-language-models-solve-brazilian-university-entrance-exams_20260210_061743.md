---
ver: rpa2
title: 'Alvorada-Bench: Can Language Models Solve Brazilian University Entrance Exams?'
arxiv_id: '2508.15835'
source_url: https://arxiv.org/abs/2508.15835
tags:
- accuracy
- performance
- language
- brazilian
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Alvorada-Bench, a benchmark of 4,515 questions
  from Brazilian university entrance exams covering humanities, sciences, and mathematics.
  Twenty language models were evaluated under three prompting strategies, yielding
  270,900 responses.
---

# Alvorada-Bench: Can Language Models Solve Brazilian University Entrance Exams?

## Quick Facts
- **arXiv ID**: 2508.15835
- **Source URL**: https://arxiv.org/abs/2508.15835
- **Reference count**: 13
- **One-line primary result**: Top language models achieved over 94% accuracy on Brazilian university entrance exams, though struggled with engineering mathematics.

## Executive Summary
This paper introduces Alvorada-Bench, a comprehensive benchmark of 4,515 questions from Brazilian university entrance exams covering humanities, sciences, and mathematics. Twenty language models were evaluated under three prompting strategies, yielding 270,900 responses. The study reveals that reasoning-supervised architectures demonstrate exceptional prompt invariance, making prompt engineering largely redundant. Models achieved strong calibration between self-reported confidence and actual accuracy, though performance degraded significantly on engineering-focused exams (IME/ITA), indicating persistent weaknesses in multi-step symbolic reasoning.

## Method Summary
The study evaluates 20 language models (OpenAI, Anthropic, DeepSeek) on 4,515 Brazilian university entrance exam questions using three prompting strategies: zero-shot, role-playing, and chain-of-thought. Questions span 1981-2025 from five exam sources (ENEM, FUVEST, UNICAMP, IME, ITA) and are filtered to text-only format. Models output structured JSON with answers, confidence scores (0-10), difficulty assessments (0-10), and Bloom's taxonomy levels. The dataset and code are publicly available, with 270,900 total API queries executed.

## Key Results
- Top models achieved over 94% overall accuracy on Brazilian university entrance exams
- Reasoning-optimized models (O3, O1) showed minimal performance variance (<0.3 pp) across different prompting strategies
- Performance dropped to 68.1% on engineering-oriented ITA exams, revealing limitations in multi-step symbolic reasoning
- Models demonstrated strong calibration with self-reported confidence correlating with actual accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning-supervised architectures decouple multi-step problem solving from prompt engineering reliance
- Mechanism: O3 and O1 models show minimal variance (<0.3 pp) across zero-shot, role-playing, and chain-of-thought prompts, suggesting internal decomposition capability
- Core assumption: Performance invariance implies architectural shift rather than better instruction following
- Evidence anchors: Abstract notes accuracy improvement "coinciding with reasoning-supervised architectures"; section 3.7 shows exceptional prompt invariance
- Break condition: If ablations reveal reliance on hidden scratchpads equivalent to chain-of-thought, mechanism shifts from internal capacity to hidden context

### Mechanism 2
- Claim: Metacognitive calibration emerges from alignment on structured outputs rather than intrinsic introspection
- Mechanism: Strong correlation between JSON-reported confidence and actual accuracy may result from models predicting token probabilities and translating to requested scale
- Core assumption: Confidence integer proxies for internal token probability distribution, not separate epistemic state
- Evidence anchors: Section 3.4 shows confidence well calibrated with bin-wise curves exceeding 90% accuracy; section 4.1 warns structured output may introduce instruction-following artifacts
- Break condition: If models report high confidence on hallucinated citations in unstructured settings, calibration is strictly localized artifact of JSON constraint

### Mechanism 3
- Claim: Performance degradation in engineering exams suggests scaling limit in symbolic manipulation versus semantic retrieval
- Mechanism: Humanities rely on semantic correlation while engineering requires strict symbolic transformation; 24.8 pp drop on IME/ITA versus ENEM indicates text-prediction alone insufficient for rigorous multi-step symbolic logic
- Core assumption: Drop due to reasoning complexity rather than data contamination
- Evidence anchors: Abstract notes accuracy declines on Mathematics and engineering exams; section 3.6 shows performance dropping to 68.1% for ITA
- Break condition: If code-execution tools close the IME/ITA gap, limitation is tool-use rather than raw reasoning capacity

## Foundational Learning

- **Prompt Invariance vs. Sensitivity**: The study challenges prevailing view that better prompting equals better performance, specifically for reasoning models. Understanding this helps design evaluation pipelines that don't conflate prompt-tuning skill with model capability. *Quick check*: Does adding "Let's think step by step" improve O3 on IME math questions? (Paper suggests it likely won't).

- **Bloom's Taxonomy "Application" Bottleneck**: Paper identifies non-monotonic performance curve where "Apply" (using concepts in new situations) is harder for standard models than "Analyze" or "Evaluate". This counters intuition that lower-order cognitive tasks are always easier for LLMs. *Quick check*: If model can critique an essay (Evaluate) but cannot solve novel physics problem (Apply), which architectural component is likely missing?

- **Data Contamination vs. Generalization**: Paper uses public exams (1981-2025). High scores on ENEM 2024 might reflect memorization rather than reasoning if model saw training data. *Quick check*: Why does performance drop on ITA/IME if contamination were sole explanation? (Difficulty/Reasoning requirements are more likely differentiator).

## Architecture Onboarding

- **Component map**: PDF Extraction -> Regex Pattern Matching -> LLM-based Filtering (removes image-dependent Qs) -> Text Normalization -> API Client (OpenAI/Anthropic/DeepSeek) -> Prompt Router (Zero-shot/Role-play/CoT) -> JSON Validator -> Accuracy Calculator -> Calibration Plotter -> Subject/Exam Stratifier

- **Critical path**: The LLM-based Filtering stage is critical. If image-based questions slip through (e.g., Geometry graphs), text-only model fails not due to reasoning but modality mismatch, directly impacting validity of "text-only" claims.

- **Design tradeoffs**: 
  - Text-Only vs. Multimodal: Strips multimodal questions to ensure fair comparison across text-only models. *Tradeoff*: May artificially inflate performance on exams like ITA/IME by removing hardest visual-spatial problems, or leave text descriptions of figures unsolvable without images.
  - Structured Output (JSON): Forces models to quantify uncertainty. *Tradeoff*: May consume reasoning tokens budget and induce "instruction following" artifacts in confidence reporting.

- **Failure signatures**:
  - Modal Mismatch: Model attempting to solve geometry problem described in text alone, resulting in hallucinations of geometric properties
  - Format Drift: Model failing to constrain output to valid JSON (e.g., outputting `{"confidence": "high"}` instead of `{"confidence": 8}`), breaking automated parser

- **First 3 experiments**:
  1. **Modality Ablation**: Manually evaluate 50 filtered "image-dependent" questions using multimodal model (e.g., GPT-4o Vision) vs. text-only baseline to quantify exactly what was lost by excluding visuals
  2. **Contamination Probe**: Evaluate performance on "held-out" 2025 exam questions released after training cutoff to establish lower bound for generalization
  3. **Calibration Stress Test**: Force models to answer "trick" questions (valid syntax, nonsensical semantic premise) to see if high confidence scores persist despite invalid premises

## Open Questions the Paper Calls Out
None

## Limitations
- Unknown sampling parameters (temperature, top_p, max_tokens) used during evaluation could significantly affect outputs and calibration
- Text-only restriction eliminates visual-spatial reasoning components integral to engineering exams, potentially underestimating capabilities
- 270,900 API calls required create significant cost barriers and may introduce rate-limiting artifacts

## Confidence

**High Confidence**:
- Language models demonstrate strong performance on Brazilian university entrance exams overall (94%+ accuracy)
- Reasoning-optimized models show prompt invariance across zero-shot, role-playing, and chain-of-thought strategies
- Calibration between self-reported confidence and actual accuracy is well-established

**Medium Confidence**:
- Reasoning-supervised architectures decouple multi-step problem solving from prompt engineering reliance
- Performance drops on engineering exams reflect fundamental limitations in symbolic manipulation rather than data contamination

**Low Confidence**:
- Mechanism by which metacognitive calibration emerges (alignment on structured outputs vs. intrinsic introspection)
- Exact contribution of text-only filtering to observed performance gaps in engineering domains

## Next Checks
1. **Contamination Validation**: Evaluate model performance on 2025 exam questions released after training cutoffs to establish lower bound for generalization versus memorization.

2. **Multimodal Ablation Study**: Re-run evaluation using multimodal model (e.g., GPT-4o Vision) on originally filtered image-dependent questions to quantify exactly what reasoning capability was lost through text-only restriction.

3. **Symbolic Reasoning Enhancement**: Provide reasoning-optimized models with code-execution tools (Python interpreters) during evaluation on engineering exams to determine whether performance gap reflects raw reasoning limitations or tool-use deficiencies.