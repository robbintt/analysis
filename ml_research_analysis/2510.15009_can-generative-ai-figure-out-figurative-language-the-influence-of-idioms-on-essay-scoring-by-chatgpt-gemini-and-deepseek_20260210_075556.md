---
ver: rpa2
title: Can generative AI figure out figurative language? The influence of idioms on
  essay scoring by ChatGPT, Gemini, and Deepseek
arxiv_id: '2510.15009'
source_url: https://arxiv.org/abs/2510.15009
tags:
- idioms
- human
- generative
- raters
- scoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated whether generative AI models can reliably score
  student essays containing idioms, a type of figurative language. Using a large corpus
  of essays, the researchers compared human and AI scores across three AI models (ChatGPT,
  Gemini, DeepSeek) for essays with and without idioms.
---

# Can generative AI figure out figurative language? The influence of idioms on essay scoring by ChatGPT, Gemini, and Deepseek

## Quick Facts
- arXiv ID: 2510.15009
- Source URL: https://arxiv.org/abs/2510.15009
- Authors: Enis Oğuz
- Reference count: 6
- One-line primary result: Gemini demonstrated the highest consistency with human raters for essays containing idioms, while other models showed reduced reliability in such cases

## Executive Summary
This study evaluated how well generative AI models can score student essays containing idioms, a form of figurative language. The researchers compared human and AI scores across three AI models (ChatGPT, Gemini, DeepSeek) for essays with and without idioms. The results showed that Gemini maintained the highest consistency with human raters, particularly for essays containing idioms, while the other models showed reduced reliability in such cases. Idiom repetition negatively impacted scores, but Gemini mimicked human scoring patterns most closely. The findings suggest that while all models showed good internal consistency, Gemini is the most promising candidate for automated essay scoring, especially in handling figurative language.

## Method Summary
The study used a large corpus of essays, comparing human and AI scores across three AI models (ChatGPT, Gemini, DeepSeek) for essays with and without idioms. The research design controlled for idiom density and analyzed scoring consistency between human raters and AI models, with particular attention to how figurative language affected scoring reliability.

## Key Results
- Gemini demonstrated the highest consistency with human raters for essays containing idioms
- Idiom repetition negatively impacted scores across all models
- All models showed good internal consistency, but Gemini most closely mimicked human scoring patterns

## Why This Works (Mechanism)
None

## Foundational Learning
- Automated Essay Scoring (AES): Computer-based evaluation of written essays
  - Why needed: Forms the basis for understanding the study's evaluation methodology
  - Quick check: The study uses AES to compare human and AI scoring consistency
- Figurative Language Processing: AI's ability to interpret non-literal expressions
  - Why needed: Central to understanding why idioms pose challenges for AI scoring
  - Quick check: Gemini showed superior performance in handling idioms compared to other models
- Corpus Linguistics: Analysis of large collections of texts for linguistic patterns
  - Why needed: Explains the methodology for selecting and analyzing essay samples
  - Quick check: The study utilized a large corpus of essays with controlled idiom densities

## Architecture Onboarding
- Component map: Essays -> Human Scoring -> AI Scoring (ChatGPT, Gemini, DeepSeek) -> Consistency Analysis
- Critical path: Essay collection → Idiom identification → Human scoring → AI scoring → Statistical comparison
- Design tradeoffs: Accuracy vs. consistency, model capability vs. safety constraints
- Failure signatures: Central tendency bias (DeepSeek), reduced reliability with idioms (ChatGPT, DeepSeek)
- 3 first experiments:
  1. Test different prompting strategies across all three models
  2. Vary temperature settings for DeepSeek to assess score distribution changes
  3. Fine-tune a model on idiom-dense corpus and compare performance

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the scoring reliability of Generative AI models for idiomatic essays hold true for second language (L2) writers?
- Basis in paper: The discussion explicitly compares AI struggles to those of L2 learners ("challenge second language learners experience"), yet the PERSUADE corpus utilized consists solely of native English speakers.
- Why unresolved: The study isolated native speakers; it remains unknown whether L2 grammatical errors interact negatively with idiom processing to further reduce scoring reliability.
- What evidence would resolve it: A replication of the study using a corpus of L2 essays with controlled idiom densities.

### Open Question 2
- Question: Can models with restricted scoring ranges, such as DeepSeek, be calibrated to match the variance of human raters without losing consistency?
- Basis in paper: The results section notes that DeepSeek's high consistency might be "misleading" because it avoids scores on the "extreme ends" (central tendency bias).
- Why unresolved: It is unclear if this limited variety is an inherent feature of the model's safety alignment or a byproduct of the specific prompting strategy used.
- What evidence would resolve it: Testing DeepSeek with varied temperature settings or few-shot prompting examples to see if score distribution widens to match human variance.

### Open Question 3
- Question: Does increasing the representation of idiomatic expressions in training data specifically improve AI scoring reliability for figurative language?
- Basis in paper: The author hypothesizes that the decline in reliability for ChatGPT and DeepSeek is "likely due to the underrepresentation of idioms in their datasets."
- Why unresolved: This is stated as a probable cause, but the study evaluates pre-trained models rather than modifying training inputs to test the hypothesis directly.
- What evidence would resolve it: Fine-tuning a model on a idiom-dense corpus and comparing its scoring performance against the baseline models used in this study.

## Limitations
- Small sample size of 50 essays per condition
- Limited diversity of idioms tested
- Focus on single language (English) and specific figurative expression types

## Confidence
- Gemini's superior performance: Medium
- Generalizability to other figurative language types: Low
- Impact of training data composition: Medium

## Next Checks
1. Replication with a larger and more diverse corpus of essays spanning multiple academic disciplines and language proficiency levels
2. Testing with a broader range of figurative language types beyond idioms, including metaphors, similes, and culturally specific expressions
3. Comparative analysis of different prompting strategies and their impact on AI scoring consistency across all three models