---
ver: rpa2
title: Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object
  Recognition
arxiv_id: '2602.00360'
source_url: https://arxiv.org/abs/2602.00360
tags:
- sentiment
- image
- text
- data
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TEMSA, a novel approach for multimodal sentiment
  analysis that leverages textual cues derived from object recognition to improve
  sentiment classification. The method extracts object names from images and combines
  them with associated text data (TEMS) to address challenges posed by modality differences
  and contextual ambiguity.
---

# Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition

## Quick Facts
- arXiv ID: 2602.00360
- Source URL: https://arxiv.org/abs/2602.00360
- Authors: Sumana Biswas; Karen Young; Josephine Griffith
- Reference count: 40
- Primary result: TEMS approach achieves 79% (SIMPSoN) and 84% (MVSA-Single) accuracy using BERT, outperforming unimodal methods by 10-15 percentage points

## Executive Summary
This paper introduces TEMSA, a novel approach for multimodal sentiment analysis that leverages textual cues derived from object recognition to improve sentiment classification. The method extracts object names from images and combines them with associated text data (TEMS) to address challenges posed by modality differences and contextual ambiguity. Four experiments were conducted on two datasets (SIMPSoN and MVSA-Single), comparing image-only, text-only, and TEMS-based multimodal analysis. Results show that TEMS significantly outperforms individual modalities when all detected object names are included, with BERT-based TEMS achieving accuracy improvements of up to 10-15 percentage points over unimodal approaches. The approach demonstrates statistical significance (p < 0.05) and addresses data format dissimilarities by converting visual information to text. Limitations include relatively small dataset sizes, with future work planned for larger datasets and additional visual features.

## Method Summary
The TEMSA approach extracts object names from images using DETR and Faster R-CNN detectors, then concatenates these names with associated caption or superimposed text to form TEMS (Text and Extracted Image Names with associated Superimposed text). The combined textual representation is processed using BERT for sentiment classification. The method addresses modality differences by converting visual objects to textual names, enabling unified processing through text-based models. Four experimental setups were tested: image-only, text-only, TEMS with one object name, and TEMS with all detected object names. Datasets used include SIMPSoN (2830 Instagram images) and MVSA-Single (2486 Twitter image-text pairs with joint labels).

## Key Results
- TEMS with all object names significantly outperforms single-object TEMS and unimodal approaches (p < 0.05)
- BERT-based TEMS achieves 79% accuracy on SIMPSoN and 84% on MVSA-Single, outperforming text-only (62% and 69%) and image-only approaches
- Including all detected object names provides better coverage than using single objects, demonstrating the value of comprehensive visual information
- The approach successfully bridges modality gaps by converting visual information to textual representation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting visual objects to textual names enables unified processing of multimodal data through text-based models.
- Mechanism: Object detection models extract object names from images; these names are concatenated with associated caption/superimposed text to form TEMS. This transforms heterogeneous modalities into a homogeneous text representation processable by language models.
- Core assumption: Object names capture sentiment-relevant visual information that, when expressed textually, complements the caption text for sentiment inference.
- Evidence anchors:
  - [abstract] "Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS."
  - [section 1, page 2] "derieved textual content from the image and associated textual content may be useful when combined and used with a deep learning model to bridge the gap between these dissimilar modalities"
  - [corpus] Related papers emphasize multimodal fusion but do not directly validate object-name-as-text approach; corpus evidence is weak for this specific mechanism.

### Mechanism 2
- Claim: BERT's self-attention enables learned associations between detected object names and caption text for sentiment inference.
- Mechanism: The self-attention mechanism computes attention weights across all tokens in TEMS, allowing the model to learn which object-text combinations are predictive of sentiment without explicit alignment rules.
- Core assumption: Sentiment arises from contextual relationships between visual objects and text, not from either modality independently.
- Evidence anchors:
  - [section 3.4, page 9] "self-attention mechanisms enable BERT to simultaneously assess the relationships between all words in the input sequence. Thus, the detected objects' names make a relation with the caption or superimposed text."
  - [section 4, page 11] "when TEMS is used in the BERT model, the mechanism of the BERT model has been advantageous in bridging the gap between word-to-word relations"
  - [corpus] AdaptiSent paper (arxiv 2507.12695) supports cross-modal attention mechanisms for MABSA, providing indirect validation.

### Mechanism 3
- Claim: Including all detected object names provides more complete visual information than single-object extraction.
- Mechanism: Multiple object names capture richer scene context; truncating to one object loses potentially sentiment-relevant visual cues.
- Core assumption: More objects → more information → better sentiment inference; no threshold beyond which additional objects add noise.
- Evidence anchors:
  - [abstract] "results showed statistically significant improvements in sentiment classification accuracy when all object names were included in TEMS compared to using only one object name."
  - [section 4, page 11-12, Table 6 vs Table 7] TEMS with all objects (BERT): SIMPSoN 79% accuracy vs. single object: 62%; MVSA-Single 84% vs. 69%
  - [corpus] No direct corpus validation; related papers do not test object quantity effects.

## Foundational Learning

- Concept: **Object Detection (DETR, Faster R-CNN)**
  - Why needed here: Understanding how object names are extracted, what objects are detectable, and detection confidence thresholds.
  - Quick check question: Given an image with a person holding a cup near a tree, which detector would you use to maximize object coverage, and what object list would you expect?

- Concept: **BERT Self-Attention Mechanism**
  - Why needed here: Explains how TEMS tokens interact to form sentiment predictions; critical for debugging attention patterns.
  - Quick check question: How does BERT's attention differ from BiLSTM's sequential processing when relating the word "beautiful" to object names like "sunset" and "beach"?

- Concept: **Multimodal Fusion Strategies**
  - Why needed here: Positions TEMSA against alternative approaches (feature-level concatenation, cross-modal alignment) to evaluate tradeoffs.
  - Quick check question: Why might converting objects to text before fusion outperform direct image-text feature concatenation for sentiment tasks?

## Architecture Onboarding

- Component map:
  Image + associated text -> Object Detection (DETR + Faster R-CNN) -> TEMS formation -> BERT classification -> Sentiment output

- Critical path:
  1. Ensure object detectors are properly loaded and configured for both Coconames and Visual Genome vocabularies
  2. Verify text preprocessing matches training (lowercase, remove non-alphanumeric, tokenize)
  3. Confirm TEMS sequence length limits (75 for SIMPSoN, 41 for MVSA-Single with max 20 objects)
  4. Check BERT input formatting (token separation, padding)

- Design tradeoffs:
  - **Coverage vs. noise**: More object categories (combining DETR + Faster R-CNN) increases coverage but may introduce irrelevant detections
  - **Dataset reduction**: Images with no detected objects (36% of SIMPSoN) are excluded from TEMS experiments
  - **Label alignment**: MVSA-Single requires creating joint labels, removing conflicting image-text pairs (reduced from 4071 to 2486)

- Failure signatures:
  - Accuracy drops to ~60%: Likely using single-object subset or text-only without TEMS
  - Large accuracy variance between datasets: Check object detection coverage (Table 3 shows 36% of SIMPSoN has 0 detected objects vs. 15% for MVSA-Single)
  - BERT underperforms BiLSTM: Check learning rate (BERT: 6e-06, BiLSTM: 1e-02) and embedding dimensions

- First 3 experiments:
  1. **Baseline validation**: Replicate Table 4 (image-only) and Table 5 (text-only) results to confirm pipeline integrity
  2. **Object count ablation**: Test TEMS with 1, 5, 10, 15, 20 max objects to find optimal threshold for your dataset
  3. **Detector combination test**: Compare DETR-only, Faster R-CNN-only, and combined object lists to measure contribution of each detector

## Open Questions the Paper Calls Out

- **Question**: Does incorporating additional visual features (face detection, facial expressions, scene comprehension) into TEMS further improve multimodal sentiment classification accuracy?
  - **Basis in paper**: [explicit] Authors explicitly state in the conclusion: "we intend to investigate more effective techniques for extracting features from images, such as face detection, facial expression analysis, and scene comprehension; combining these features with the available text will undoubtedly enable future improvements."
  - **Why unresolved**: The current TEMSA approach only uses object names from images; other potentially sentiment-relevant visual features are not yet extracted or incorporated.
  - **What evidence would resolve it**: Experiments extending TEMS to include textual descriptions of faces, expressions, and scenes, comparing performance against the current object-only TEMS on the same datasets.

- **Question**: How does TEMSA perform on larger and more diverse multimodal datasets beyond the two relatively small datasets tested?
  - **Basis in paper**: [explicit] Authors acknowledge: "We conducted all the experiments on relatively small datasets, which must be acknowledged. We plan to extend our research... on large and more diverse datasets."
  - **Why unresolved**: SIMPSoN (2,830 samples after filtering) and MVSA-Single (2,486 joint-labeled samples) are limited in size and source diversity (Instagram and Twitter only).
  - **What evidence would resolve it**: Evaluation on larger benchmark multimodal sentiment datasets (e.g., YFCC100M, larger Twitter/Instagram corpora) with statistical comparison to current results.

- **Question**: How should TEMSA handle images where no objects are detected (36% of SIMPSoN, 15% of MVSA-Single)?
  - **Basis in paper**: [inferred] Table 3 shows substantial portions of both datasets have zero detected objects, yet the paper does not discuss how TEMS is constructed or performs for these cases.
  - **Why unresolved**: The TEMS approach relies on concatenating object names with text; when no objects are detected, the multimodal fusion benefit may be lost, but this scenario is not analyzed.
  - **What evidence would resolve it**: Ablation study reporting TEMSA performance separately on images with zero, one, and multiple detected objects, with analysis of fallback strategies.

## Limitations
- Dataset size constraints with SIMPSoN (2830 samples) and MVSA-Single (2486 samples after filtering) limit generalizability
- 36% of SIMPSoN images have zero detected objects, reducing effective dataset size for TEMS experiments
- Object detection coverage limited to 64 Coconames and 200 Visual Genome categories, potentially missing sentiment-relevant objects

## Confidence
- **TEMS Approach Outperforms Unimodal Methods** (High Confidence): Consistent statistically significant improvements (p < 0.05) across both datasets with substantial magnitude
- **All Object Names Provide Better Coverage Than Single Objects** (Medium Confidence): Clear performance differences shown, but optimal object count not explored
- **BERT Self-Attention Bridges Modality Gap** (Medium Confidence): Plausible mechanism but lacks direct validation through attention pattern analysis

## Next Checks
1. **Object Detection Ablation Study**: Systematically vary the number of detected objects (1, 5, 10, 15, 20) and measure sentiment classification accuracy to identify optimal threshold

2. **Cross-Dataset Generalizability Test**: Apply TEMS to a larger, more diverse multimodal sentiment dataset (e.g., Meme Sentiment Analysis Dataset) to evaluate scalability and generalization

3. **Attention Pattern Analysis**: Extract and visualize BERT's self-attention weights for TEMS inputs to validate whether object names receive meaningful attention in relation to sentiment-carrying words