---
ver: rpa2
title: Targeted Data Protection for Diffusion Model by Matching Training Trajectory
arxiv_id: '2512.10433'
source_url: https://arxiv.org/abs/2512.10433
tags:
- data
- target
- trajectory
- protection
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TAFAP, the first method to achieve Targeted
  Data Protection (TDP) in diffusion models by aligning the entire training trajectory
  rather than using snapshot-based optimization. Unlike existing methods that passively
  degrade image quality or fail to achieve meaningful redirection, TAFAP employs trajectory-matching
  inspired by dataset distillation to persistently guide protected data toward user-specified
  target concepts throughout fine-tuning.
---

# Targeted Data Protection for Diffusion Model by Matching Training Trajectory

## Quick Facts
- arXiv ID: 2512.10433
- Source URL: https://arxiv.org/abs/2512.10433
- Authors: Hojun Lee; Mijin Koo; Yeji Song; Nojun Kwak
- Reference count: 13
- Primary result: First method to achieve targeted data protection in diffusion models by aligning training trajectories rather than using snapshot-based optimization

## Executive Summary
TAFAP introduces the first method to achieve Targeted Data Protection (TDP) in diffusion models by aligning the entire training trajectory rather than using snapshot-based optimization. Unlike existing methods that passively degrade image quality or fail to achieve meaningful redirection, TAFAP employs trajectory-matching inspired by dataset distillation to persistently guide protected data toward user-specified target concepts throughout fine-tuning. The method demonstrates the first successful targeted transformation in diffusion models, achieving simultaneous control over identity and visual patterns. Extensive experiments show TAFAP significantly outperforms existing TDP attempts, achieving robust redirection while maintaining high image quality.

## Method Summary
TAFAP protects data by optimizing adversarial perturbations that align the fine-tuning trajectory of protected data with that of target data. The method records an "expert trajectory" by fine-tuning on target data and storing weight checkpoints at each iteration. During protection, it optimizes noise perturbations so that when protected data is fine-tuned, the weight updates follow the same trajectory as the expert. This is achieved through normalized L2 distance between weight checkpoints over k update steps, using sign-based gradient updates within a noise budget constraint. The approach targets the fine-tuning process itself rather than just the final model state, enabling persistent, verifiable transformations throughout training.

## Key Results
- First successful targeted transformation in diffusion models, achieving controlled identity redirection toward user-specified concepts
- Outperforms existing TDP methods (Mist, T-ASPL) by achieving semantic modifications rather than afterimage/overlap artifacts
- Demonstrates effectiveness under image preprocessing (JPEG compression, Gaussian blur) and cross-model scenarios
- Achieves simultaneous control over identity preservation and visual pattern transformation

## Why This Works (Mechanism)

### Mechanism 1: Trajectory-Level Alignment Overcomes Snapshot Dilution
Aligning the complete training trajectory produces persistent protection that snapshot-based methods cannot achieve. Pre-compute an "expert trajectory" by fine-tuning on target data, then optimize adversarial noise so that fine-tuning on protected data follows the same weight-update path. The normalized L2 distance between weight checkpoints drives alignment.

### Mechanism 2: Weight-Distance Loss Captures Learning Dynamics
Normalized weight-distance loss preserves gradient direction across training steps better than latent-space matching. Instead of matching latent representations, TAFAP matches weight differences: L = ||W_{i+k,p} - W_{i+k,t}||² / ||W_{i+k,t} - W_{i,t}||². This accounts for how the model evolves, not just its state.

### Mechanism 3: Sign-Based Optimization with Budget Constraint
Sign-based gradient updates with noise budget constraint produce semantically effective perturbations without visible image degradation. Following FGSM/PGD, the gradient direction determines noise updates. Budget ε = 0.05 limits perturbation magnitude, preventing visible artifacts while preserving protective effect.

## Foundational Learning

- **Concept: Diffusion Models (DDPM, LDM)**
  - Why needed here: Protection targets the denoising training objective; understanding the forward/reverse process is essential for grasping what gets disrupted.
  - Quick check question: Can you explain how the LDM loss (Eq. 1) relates to the denoising U-Net's role?

- **Concept: DreamBooth Personalization**
  - Why needed here: TAFAP assumes attackers use DreamBooth+LoRA to personalize on protected data; the method explicitly optimizes against this training regime.
  - Quick check question: What does prior preservation loss do in DreamBooth, and why does TAFAP not need to model it explicitly?

- **Concept: Dataset Distillation (MTT)**
  - Why needed here: TAFAP directly repurposes Matching Training Trajectories from dataset distillation; understanding MTT's goal (synthetic data that matches real-data learning dynamics) clarifies the adaptation.
  - Quick check question: How does MTT's goal differ from TAFAP's, and what remains the same?

## Architecture Onboarding

- **Component map**: Expert Trajectory Recorder -> Noise Optimizer -> Perturbed Image Generator -> Evaluation Pipeline
- **Critical path**: 1) Select target and protected identities. 2) Record expert trajectory (3 separate LoRA models). 3) Initialize noise δ=0, iterate: sample target checkpoint i, update protected-data weights k steps, compute distance to target checkpoint i+k, backprop to δ, apply sign update with clipping. 4) Add final δ to protected images before release.
- **Design tradeoffs**: More expert trajectories → better robustness but higher storage/computation. Larger k → better trajectory lookahead but slower optimization. Larger noise budget ε → stronger protection but more visible artifacts.
- **Failure signatures**: Identity blending rather than clean redirection (insufficient N or trajectory mismatch). Visible afterimages (latent-space matching instead of trajectory matching). High ISM with protected identity + low ISM with target (protection failed entirely).
- **First 3 experiments**: 1) Ablation on trajectory depth: vary k ∈ {1, 4, 8, 16}. 2) Cross-model transfer: test protection optimized on SD1.4+LoRA against SD1.4 without LoRA and SD2.1. 3) Preprocessing robustness: apply JPEG compression and Gaussian blur before fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the degree of identity transformation be precisely controlled to enable intentional interpolation between protection levels?
- Basis in paper: [explicit] The conclusion states: "The observed identity blending in certain cases, rather than being a simple failure, suggests intriguing possibilities for controllable interpolation between protection levels."
- Why unresolved: The paper demonstrates binary targeted transformation but does not provide a mechanism for partial or graded protection; the phenomenon of identity blending is observed but not systematically characterized or controlled.

### Open Question 2
- Question: Does trajectory-based protection transfer effectively across diverse personalization methods beyond DreamBooth+LoRA?
- Basis in paper: [inferred] The paper evaluates only DreamBooth+LoRA, noting it was selected as "a representative attacker model due to its widespread adoption," but does not test Textual Inversion, Custom Diffusion, or other fine-tuning approaches.
- Why unresolved: Different personalization methods optimize different parameters, which may alter how trajectory alignment influences the learning process.

### Open Question 3
- Question: Can protection effectiveness be maintained when attackers employ adaptive strategies specifically designed to counter trajectory-matching?
- Basis in paper: [inferred] The robustness experiments test preprocessing defenses but assume standard fine-tuning; no evaluation addresses whether informed attackers could detect and circumvent trajectory-aligned perturbations.
- Why unresolved: Adversarial perturbations in other domains have shown vulnerability to adaptive attacks; whether trajectory-level alignment provides inherent robustness against countermeasures remains unknown.

### Open Question 4
- Question: What are the theoretical and practical limits of cross-model transfer for trajectory-based protection?
- Basis in paper: [explicit] Section 5.8 states: "Exploring this direction could further enhance the practical impact of our approach, particularly in scenarios involving different model versions or architectures."
- Why unresolved: Figure 8 shows "varying degrees of target resemblance" across SD1.4 and SD2.1, indicating incomplete transfer; the conditions enabling or limiting cross-model effectiveness are not characterized.

## Limitations
- Computational expense: Requires 2000 iterations of unrolled gradient computation, making it significantly more expensive than snapshot-based alternatives
- Trajectory dependency: Protection relies on matching fine-tuning dynamics; mismatches in hyperparameters between expert and student training can break protection
- Limited cross-model robustness: While showing partial effectiveness, protection transfer across different model versions or architectures is not complete

## Confidence

- **High confidence**: Trajectory-matching outperforms snapshot-based methods in identity redirection (ISM metrics, Figure 4, Table 1-2)
- **Medium confidence**: Protection effectiveness against preprocessing (Table 3 shows partial but not complete robustness)
- **Medium confidence**: Cross-model applicability (Figure 8 demonstrates effectiveness but with reduced magnitude)
- **Low confidence**: Computational efficiency claims (no direct comparison to Mist in wall-clock time or memory)

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary k ∈ {1, 4, 8, 16} and measure both protection strength (ISM gap between protected and target identities) and optimization stability (gradient norm trends, convergence behavior)

2. **Real-world attack scenario testing**: Evaluate protection against practical threat models including prompt engineering attacks, multi-step fine-tuning, and combination with other protection methods like watermarking

3. **Computational overhead measurement**: Benchmark TAFAP against Mist using identical hardware, measuring total runtime (expert trajectory recording + noise optimization + evaluation) and memory usage for different batch sizes and image resolutions