---
ver: rpa2
title: 'Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling
  through Component-Level Evaluation'
arxiv_id: '2510.16943'
source_url: https://arxiv.org/abs/2510.16943
tags:
- optimization
- problem
- constraints
- metrics
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a component-level evaluation framework for
  LLM-generated optimization formulations. It evaluates decision variables, constraints,
  and objectives separately, using metrics like precision, recall, RMSE, and solver-level
  optimality gap.
---

# Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation

## Quick Facts
- **arXiv ID**: 2510.16943
- **Source URL**: https://arxiv.org/abs/2510.16943
- **Reference count**: 40
- **Primary result**: Solver performance depends primarily on constraint recall and RMSE, with concise outputs improving efficiency

## Executive Summary
This paper introduces a component-level evaluation framework for assessing LLM-generated optimization formulations. The framework evaluates decision variables, constraints, and objectives separately using metrics like precision, recall, RMSE, and solver-level optimality gap. Tested across four optimization problems with six prompting strategies on three LLMs (GPT-5, LLaMA 3.1, DeepSeek), the study reveals that complete constraint coverage and constraint RMSE are the primary drivers of solver performance. GPT-5 consistently outperforms other models, particularly when using chain-of-thought and modular prompting approaches. The research highlights how concise formulations enhance efficiency while maintaining accuracy.

## Method Summary
The authors developed a comprehensive framework that evaluates LLM-generated optimization models at the component level, separating analysis of decision variables, constraints, and objectives. The evaluation employs multiple metrics including precision, recall, RMSE for mathematical correctness, and solver-level optimality gap to assess practical performance. The study tests four optimization problems of varying difficulty (easy to hard) using six different prompting strategies across three large language models. The framework systematically identifies which components most significantly impact solver performance and formulation quality, moving beyond traditional end-to-end evaluation approaches.

## Key Results
- Solver performance is driven primarily by constraint recall and constraint RMSE metrics
- GPT-5 consistently outperforms LLaMA 3.1 and DeepSeek across all evaluation metrics
- Chain-of-thought and modular prompting strategies yield the best results for optimization formulation
- Concise outputs improve computational efficiency without sacrificing accuracy

## Why This Works (Mechanism)
The component-level evaluation framework works by systematically isolating and analyzing each element of optimization formulations. By measuring precision and recall for decision variables, constraints, and objectives separately, the framework identifies which components contribute most to solver success. The RMSE metric captures mathematical correctness at the component level, while solver-level optimality gap provides practical validation. This granular approach reveals that constraints are the critical bottleneck—incomplete constraint coverage leads to infeasible solutions, while constraint RMSE directly impacts solution quality. The framework's effectiveness stems from its ability to pinpoint specific failure modes and guide improvements in both LLM prompting and formulation generation.

## Foundational Learning
- **Component-level evaluation**: Breaking down optimization formulations into variables, constraints, and objectives for isolated assessment; needed to identify specific failure points that aggregate metrics miss; quick check: verify each component's mathematical correctness independently
- **Solver-level optimality gap**: Measuring the difference between generated and optimal solutions; needed to validate practical utility beyond mathematical correctness; quick check: compare generated solution against known optimal or benchmark solutions
- **Prompt engineering strategies**: Testing chain-of-thought, modular, and other prompting approaches; needed to optimize LLM performance for technical tasks; quick check: evaluate consistency across multiple problem instances
- **Precision and recall metrics**: Measuring completeness and accuracy of generated components; needed to quantify coverage and correctness; quick check: calculate true/false positives and negatives for each component type
- **RMSE for mathematical expressions**: Quantifying numerical accuracy in generated formulas; needed to capture subtle mathematical errors; quick check: compute error between generated and reference expressions
- **Constraint coverage analysis**: Ensuring all necessary constraints are present; needed to prevent infeasible solutions; quick check: verify constraint completeness against problem requirements

## Architecture Onboarding

**Component Map**
LLM Model -> Prompt Strategy -> Component Generation -> Component Evaluation -> Solver Validation -> Performance Metrics

**Critical Path**
Prompt Strategy Selection → Constraint Generation → Constraint Recall Calculation → Constraint RMSE → Solver Performance

**Design Tradeoffs**
- Granular evaluation provides detailed insights but increases computational overhead
- Component isolation enables targeted improvements but may miss inter-component dependencies
- Multiple prompting strategies increase robustness but require more extensive testing
- Mathematical metrics capture correctness but may not reflect practical usability

**Failure Signatures**
- Low constraint recall → infeasible or suboptimal solutions
- High constraint RMSE → mathematical errors propagating to solver failures
- Poor decision variable precision → incorrect problem formulation
- Objective function errors → solutions optimizing wrong targets

**First 3 Experiments**
1. Compare component-level vs. end-to-end evaluation on a simple knapsack problem
2. Test prompt strategy impact on constraint generation quality for a scheduling problem
3. Evaluate trade-off between formulation conciseness and solver performance

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation limited to four specific optimization problems, restricting generalizability
- Focus on mathematical formulation generation rather than broader optimization capabilities
- Does not address computational efficiency beyond solver runtimes
- Potential bias from LLM training data not explored

## Confidence
- **High**: Constraint recall and RMSE are primary drivers of solver performance
- **High**: GPT-5 consistently outperforms other LLMs across all metrics
- **Medium**: Prompting strategies generalizability to other problem domains
- **Medium**: Component-level approach effectiveness for complex real-world problems

## Next Checks
1. Test the component-level evaluation framework on a broader set of optimization problems, including real-world applications with larger scale and complexity
2. Evaluate the impact of different LLM training data distributions on formulation quality across all components
3. Conduct ablation studies to isolate the individual contribution of each component (decision variables, constraints, objectives) to overall solver performance