---
ver: rpa2
title: 'StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with
  Multi-Dimensional Evaluation'
arxiv_id: '2507.21340'
source_url: https://arxiv.org/abs/2507.21340
tags:
- text
- data
- extraction
- information
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StructText is an automated framework for generating high-fidelity
  benchmarks to evaluate key-value extraction from text using tabular data as ground
  truth. It follows a two-stage plan-then-execute pipeline where an LLM first analyzes
  sample rows to identify natural column groupings and then generates synthetic natural-language
  reports for each row.
---

# StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation

## Quick Facts
- arXiv ID: 2507.21340
- Source URL: https://arxiv.org/abs/2507.21340
- Authors: Satyananda Kashyap; Sola Shirai; Nandana Mihindukulasooriya; Horst Samulowitz
- Reference count: 39
- Primary result: Automated framework for generating high-fidelity benchmarks to evaluate key-value extraction from text using tabular data as ground truth

## Executive Summary
StructText is an automated framework for generating high-fidelity benchmarks to evaluate key-value extraction from text using tabular data as ground truth. It follows a two-stage plan-then-execute pipeline where an LLM first analyzes sample rows to identify natural column groupings and then generates synthetic natural-language reports for each row. The framework includes multi-dimensional evaluation combining LLM-based judgments on factuality, hallucination, and coherence with objective metrics for numeric and temporal accuracy. Evaluated on 71,539 examples across 49 datasets spanning SEC filings and WikiDBs, results show LLMs achieve strong factual accuracy and numerical/temporal fidelity (F1 > 0.9) but struggle with narrative coherence (F1 ~0.45) and information extractability.

## Method Summary
The framework uses a two-stage pipeline: (1) Planning stage where an LLM analyzes 10 sample rows to identify natural column groupings, and (2) Generation stage where an LLM produces narrative reports per row based on these groupings. Evaluation combines LLM-as-a-judge scores for factuality, hallucination, and coherence with deterministic parsers for numeric and temporal accuracy. The system filters generated examples using quality thresholds (τ=0.9) to produce clean benchmarks. The approach leverages the asymmetry of difficulty between data-to-text and text-to-data tasks, treating high-fidelity synthetic text as ground truth for evaluation.

## Key Results
- LLMs achieve high numeric and temporal fidelity (F1 > 0.9) when generating text from tables
- Text-to-table extraction F1 drops to ~0.45 despite high generation accuracy, revealing a critical gap between generation quality and machine processability
- Hybrid evaluation combining LLM judges and deterministic parsers is necessary as surface-level metrics fail to capture structural faithfulness
- 71,539 examples across 49 datasets show the framework's scalability across SEC filings and WikiDBs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing table-to-text generation into a "plan-then-execute" pipeline improves the semantic coherence of synthetic reports by explicitly grouping related columns before text generation begins.
- **Mechanism:** In the planning stage, an LLM analyzes a sample of rows (empirically set to 10) to identify natural column groupings (e.g., grouping revenue with net income). In the execution stage, the LLM generates text strictly adhering to these pre-defined groupings rather than processing all columns simultaneously.
- **Core assumption:** LLMs can more effectively structure narratives when the cognitive load of schema identification is separated from the load of language generation (Assumption: based on the observation that single-stage generation struggles with logical organization in complex tables).
- **Evidence anchors:**
  - [abstract]: "follows a two-stage 'plan-then-execute' pipeline... synthetically generate corresponding natural-language text."
  - [Section 3.1]: "Our LLM based system first analyzes ten sample rows... to produce meaningful report structures... [then] grounded text generation uses the planned report type name."
  - [corpus]: Corpus signals mention "Multi-Dimensional Summarization Agents" with context-aware reasoning, supporting the move away from monolithic generation, but specific validation of this specific 2-stage split is weak/absent in the provided neighbors.
- **Break condition:** If the input table contains columns with no semantic correlations (random noise), the planning stage will either fail to produce groupings or force arbitrary ones, leading to incoherent text.

### Mechanism 2
- **Claim:** Leveraging the "asymmetry of difficulty" allows for the creation of high-fidelity ground truth; LLMs generate text from tables with higher numeric/temporal fidelity (F1 > 0.9) than they can extract that same information back from text.
- **Mechanism:** The framework uses an LLM to convert structured data (ground truth) into unstructured text. Because LLMs are currently better at data-to-text than text-to-data, the resulting synthetic text is sufficiently high-quality to serve as a proxy for human-authored documents in benchmarks.
- **Core assumption:** The evaluation of "extraction" is the bottleneck, not the generation. If the generator introduces errors, the benchmark fails. The paper assumes the measured high fidelity (F1 > 0.9 for numbers) is sufficient to treat the synthetic text as "ground truth" for the reverse task.
- **Evidence anchors:**
  - [Section 1]: "As demonstrated by prior work such as SynthIE... there is an inherent asymmetry in difficulty... generating fluent text from structured data inputs seems to be easier."
  - [Section 6.2]: "Validation Type... Numeric Validation SEC Financial F1 0.927."
  - [corpus]: OmniStruct (neighbor) discusses universal text-to-structure but does not validate the specific asymmetry claim regarding benchmark generation.
- **Break condition:** If the generation model is smaller or less capable, the generation fidelity may drop below the extraction capability, rendering the benchmark circular or trivial.

### Mechanism 3
- **Claim:** A hybrid evaluation approach combining LLM-as-a-Judge (for semantic coherence/hallucination) and deterministic parsers (for numeric/temporal accuracy) is necessary because surface-level overlap metrics (e.g., BLEU) fail to capture structural faithfulness.
- **Mechanism:** The system validates generated text in two parallel tracks: (1) An LLM judge scores factuality and hallucination against the source table using a 1-5 rubric; (2) A specialized pipeline using NER (Stanford CoreNLP) and regex validates that specific numbers and dates appear exactly.
- **Core assumption:** LLM judges are reliable for semantic checks (hallucination), but too imprecise for strict numeric verification; conversely, regex/NER is precise for numbers but cannot judge semantic flow.
- **Evidence anchors:**
  - [Section 3.2]: "To ensure alignment between text and structured source, we introduce a multi-dimensional evaluation strategy that combines (a) LLM-based judgments... and (b) objective extraction metrics."
  - [Section 6.1]: "LLM-as-judge evaluation results... hallucination [score] 4.90... coherence 3.28."
  - [corpus]: FinReflectKG (neighbor) proposes multi-dimensional evaluation for financial KGs, corroborating the need for multi-axis assessment, though not the specific hybrid mechanism here.
- **Break condition:** If the LLM judge suffers from calibration drift (e.g., rating incoherent text as coherent), the filtering mechanism will retain low-quality semantic examples even if the numbers are correct.

## Foundational Learning

- **Concept:** **LLM-as-a-Judge**
  - **Why needed here:** The framework relies on an LLM (Llama-3.3-70B) to grade the output of another LLM (Qwen2.5-72B). You must understand that this is not a deterministic test but a probabilistic evaluation using rubrics (1-5 scale).
  - **Quick check question:** If the Judge model assigns a "Hallucination" score of 5 (No hallucination), does that guarantee 100% factuality? (Answer: No, it only indicates the Judge did not detect ungrounded content).

- **Concept:** **Context Window vs. Schema Complexity**
  - **Why needed here:** The "Plan" stage analyzes 10 rows to determine column groupings. This implies a need to fit table schema semantics into the context window to derive a "Plan."
  - **Quick check question:** Why does the system sample only 10 rows for planning instead of the whole table? (Answer: To balance computational efficiency with coverage of data patterns/variance).

- **Concept:** **Bipartite Matching for Evaluation**
  - **Why needed here:** When evaluating extraction, the system must match predicted column names to ground truth names. Exact matching fails if the LLM outputs "Revenue" vs "Total Revenue."
  - **Quick check question:** How does the system handle a prediction of "Fiscal Year 2024" when the ground truth is "2024"? (Answer: It uses bipartite matching based on similarity scores, not exact string equality).

## Architecture Onboarding

- **Component map:** Input Tables -> Planner (Qwen2.5-72B) -> Column Groupings -> Generator (Qwen2.5-72B) -> Synthetic Text -> Validator (LLM-Judge + NER/Regex) -> Filter (threshold τ) -> Clean Benchmark

- **Critical path:** The **Text Generation → Numeric Validation** loop. If the generator produces text that fails numeric validation (e.g., rounding errors or dropped dates), the example is discarded. The benchmark size depends heavily on the generator's ability to pass this strict filter.

- **Design tradeoffs:**
  - **Strictness vs. Size:** Setting the filtering threshold τ high (e.g., 1.0) ensures high quality but discards ~50% of data (See Fig 4).
  - **1-Row-1-Report:** The architecture assumes text segments map to single rows. This simplifies evaluation but reduces realism (real documents often aggregate multiple rows).

- **Failure signatures:**
  - **Temporal Precision Drop:** The paper notes precision drops because the generator infers valid context (e.g., "fiscal year 2024") not explicitly in the ground truth columns, causing a "False Positive" in the strict validation logic.
  - **Extraction Collapse:** While generation F1 is >0.9, the baseline extraction F1 is ~0.45. This indicates the text is "fluent" but "structurally opaque" (information is buried in narrative).

- **First 3 experiments:**
  1. **Run the Planner on a new table:** Input a 20-column CSV from your domain; verify if the Planner groups columns logically or if it requires manual schema hints.
  2. **Inspect Validation Failures:** Generate 100 reports and filter at τ=0.9. Manually inspect the discarded reports to see if they are "bad text" or just "contextually enriched text" that confused the parser.
  3. **Test Extraction Baseline:** Try to extract key-values from the *filtered* benchmark using a smaller model (e.g., Llama-8B) to gauge the difficulty floor of the benchmark.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does varying the sample size in the planning stage affect the quality and semantic coverage of the generated report types?
- **Basis in paper:** [explicit] The authors note they "empirically chose 10 samples" and state that "ablations on sample size remain as future work."
- **Why unresolved:** It is unclear if 10 samples optimally balances computational efficiency with the discovery of data patterns across diverse datasets.
- **What evidence would resolve it:** Ablation studies comparing different sample sizes against the diversity of column groupings and subsequent text quality scores.

### Open Question 2
- **Question:** To what extent do LLM-based evaluation judgments align with human assessments when using the proposed factuality and coherence rubrics?
- **Basis in paper:** [explicit] Section 7.3 identifies "inter-human and inter-model judge capability assessment" as necessary future work to validate the scoring system.
- **Why unresolved:** The study relies on Llama-3.3 as a judge without quantifying the agreement gap between this model and human annotators.
- **What evidence would resolve it:** Correlation coefficients (e.g., Cohen's Kappa) comparing LLM judge scores against human expert evaluations on a subset of generated reports.

### Open Question 3
- **Question:** Can generation strategies be optimized to improve narrative coherence and machine processability without compromising the high factual fidelity observed in current models?
- **Basis in paper:** [inferred] The conclusion highlights a critical gap where models achieve high factuality (F1 > 0.9) but struggle with "narrative coherence" and "information extractability" (F1 ~0.45).
- **Why unresolved:** The paper establishes that accurate text currently resists automated extraction, but does not propose methods to bridge this specific gap.
- **What evidence would resolve it:** New generation techniques that yield synthetic text with higher extraction F1 scores in the text-to-table baseline while maintaining temporal and numeric accuracy.

## Limitations
- The framework achieves high numeric/temporal fidelity (F1 > 0.9) but struggles with information extractability (F1 ~0.45), revealing a critical gap between text generation quality and machine processability
- Exact prompt templates for planning and generation stages remain unspecified, blocking exact reproduction
- The architecture assumes 1-row-1-report mapping, limiting realism compared to multi-row aggregated documents
- Temporal precision drops occur when the generator infers valid context not explicitly in ground truth columns

## Confidence
- **High Confidence:** Numeric and temporal accuracy measurements (F1 > 0.9), asymmetric difficulty between generation and extraction tasks, hybrid evaluation approach combining LLM-judges with deterministic parsers
- **Medium Confidence:** The two-stage "plan-then-execute" pipeline's effectiveness in improving semantic coherence, the sufficiency of F1 > 0.9 for treating synthetic text as ground truth
- **Low Confidence:** The generalizability of the framework to domains with complex multi-row aggregations, the robustness of LLM judges against calibration drift in semantic evaluation

## Next Checks
1. **Prompt Template Validation:** Implement the framework with the provided HuggingFace dataset and GitHub code, then systematically test different prompt formulations for the planning stage to verify if column grouping quality correlates with downstream text coherence scores
2. **Temporal Precision Diagnostic:** Generate 1000 reports from SEC filings, manually inspect the 10% that fail temporal precision validation to distinguish between parser limitations versus genuine hallucination, then adjust the validation threshold accordingly
3. **Extraction Baseline Stress Test:** Using the filtered benchmark (τ=0.9), attempt key-value extraction with progressively smaller models (Llama-8B, Llama-3B) to establish a difficulty gradient and verify if the ~0.45 F1 represents a fundamental information embedding problem versus model capability limitation