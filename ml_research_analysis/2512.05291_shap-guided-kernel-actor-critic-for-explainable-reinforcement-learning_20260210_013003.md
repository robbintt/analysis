---
ver: rpa2
title: SHAP-Guided Kernel Actor-Critic for Explainable Reinforcement Learning
arxiv_id: '2512.05291'
source_url: https://arxiv.org/abs/2512.05291
tags:
- contact
- neff
- kernel
- value
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the interpretability challenge in actor-critic
  reinforcement learning by proposing an attribution-aware kernel-based algorithm
  called RSA2C. The core method integrates state attribution computed via RKHS-SHAP
  (kernel mean embedding and conditional mean embedding) into the learning process,
  using these attributions to modulate policy gradients and advantage function targets
  through Mahalanobis-weighted kernels.
---

# SHAP-Guided Kernel Actor-Critic for Explainable Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.05291
- Source URL: https://arxiv.org/abs/2512.05291
- Authors: Na Li; Hangguan Shan; Wei Ni; Wenjie Zhang; Xinyu Li
- Reference count: 40
- Achieves stable training with intrinsic interpretability through state attribution visualization across three continuous control environments

## Executive Summary
This paper addresses the interpretability challenge in actor-critic reinforcement learning by proposing an attribution-aware kernel-based algorithm called RSA2C. The method integrates state attribution computed via RKHS-SHAP (kernel mean embedding and conditional mean embedding) into the learning process, using these attributions to modulate policy gradients and advantage function targets through Mahalanobis-weighted kernels. RSA2C consists of three RKHS-enhanced components: an actor in a vector-valued RKHS, an advantage critic, and a value critic, all maintained via sparse dictionaries for computational efficiency.

The primary results demonstrate that RSA2C achieves stable and efficient training across three continuous control environments while providing intrinsic interpretability through state attribution visualization. Theoretical contributions include a global non-asymptotic convergence bound under state perturbations, decomposing the learning gap into attribution-induced and two-timescale convergence terms. Empirically, RSA2C improves performance over baselines (up to 49.2% higher returns) with only modest computational overhead, and the CME variant shows superior stability under noisy state perturbations compared to KME.

## Method Summary
RSA2C integrates SHAP-based state attribution into kernel actor-critic methods by computing state attributions through RKHS embeddings (KME and CME variants) and using these attributions to weight kernel-based updates. The actor and critics operate in vector-valued reproducing kernel Hilbert spaces (RKHS), with sparse dictionary learning for computational efficiency. Attribution information modulates policy gradients and advantage function targets through Mahalanobis-weighted kernels, providing both improved learning performance and intrinsic interpretability. The algorithm maintains three RKHS-based components that are updated through two-timescale stochastic approximation, with theoretical convergence guarantees under state perturbations.

## Key Results
- RSA2C achieves up to 49.2% higher returns compared to baseline actor-critic methods
- The algorithm demonstrates stable training across three continuous control environments
- CME variant shows superior stability under noisy state perturbations compared to KME
- Provides intrinsic interpretability through state attribution visualization with only modest computational overhead

## Why This Works (Mechanism)
RSA2C works by incorporating state attribution information directly into the kernel-based learning process. The SHAP-based attributions computed through RKHS embeddings identify which state features contribute most to value predictions, and these attributions then modulate the learning updates through Mahalanobis-weighted kernels. This attribution-aware modulation helps the algorithm focus on relevant state features during policy updates, leading to more stable and efficient learning. The RKHS framework provides a principled way to incorporate this attribution information while maintaining theoretical convergence guarantees, and the sparse dictionary learning ensures computational efficiency.

## Foundational Learning
**Reproducing Kernel Hilbert Spaces (RKHS)**: Function spaces with kernel-induced inner products that enable nonlinear function approximation
- Why needed: Provides the mathematical framework for kernel-based actor-critic methods and attribution computation
- Quick check: Verify kernel functions satisfy Mercer's conditions and RKHS properties hold

**Kernel Mean Embedding (KME)**: Technique for representing probability distributions as elements in RKHS
- Why needed: Enables computation of state attributions through distribution comparisons
- Quick check: Confirm embedding operator is injective and bounded

**Conditional Mean Embedding (CME)**: Extension of KME for conditional distributions
- Why needed: Provides alternative attribution computation that may be more stable under perturbations
- Quick check: Verify conditional embedding operator satisfies required properties

**SHAP (SHapley Additive exPlanations)**: Game-theoretic approach for feature attribution
- Why needed: Provides principled method for computing state feature importance
- Quick check: Confirm attribution values satisfy efficiency, symmetry, and linearity properties

**Two-timescale Stochastic Approximation**: Algorithm with different learning rates for different components
- Why needed: Enables stable simultaneous learning of actor and critics
- Quick check: Verify learning rates satisfy required timescale separation conditions

**Sparse Dictionary Learning**: Technique for maintaining compact representations in RKHS
- Why needed: Ensures computational efficiency in high-dimensional state spaces
- Quick check: Confirm dictionary update maintains required approximation accuracy

## Architecture Onboarding

**Component Map**: State features -> SHAP attribution computation -> Mahalanobis-weighted kernel updates -> Actor and critics in RKHS -> Policy and value predictions

**Critical Path**: State → Attribution computation (KME/CME) → Weighted kernel updates → Policy gradient/advantage estimation → Actor/critic parameter updates

**Design Tradeoffs**: KME vs CME attribution computation (accuracy vs stability), kernel bandwidth selection (sensitivity vs robustness), dictionary size (accuracy vs computation)

**Failure Signatures**: Degraded performance with poor kernel bandwidth selection, attribution instability under state noise, dictionary collapse from aggressive sparsification

**First Experiments**: 1) Validate attribution computation on simple MDPs with known feature importance, 2) Test kernel bandwidth sensitivity on synthetic data, 3) Benchmark computational overhead vs dictionary size on benchmark control tasks

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical convergence relies on assumptions about Lipschitz continuity and kernel smoothness that may not hold in all scenarios
- Empirical validation limited to three continuous control tasks, leaving uncertainty about performance in complex or sparse-reward environments
- Attribution-based interpretability requires further validation to confirm causal relationships rather than spurious correlations

## Confidence
- **High**: Core algorithmic claims with rigorous theoretical framework and consistent empirical improvements
- **Medium**: Interpretability claims from attribution visualization lacking comprehensive validation against ground-truth causal structures
- **Low**: Scalability claims due to limited testing across diverse problem domains

## Next Checks
1. Evaluate RSA2C on benchmark suites with varying state dimensionalities and reward sparsity to assess scalability limits
2. Conduct ablation studies comparing RKHS-SHAP attributions against established feature importance methods to validate interpretability claims
3. Test performance under different noise distributions and perturbation intensities to verify theoretical robustness claims