---
ver: rpa2
title: HLTCOE at TREC 2024 NeuCLIR Track
arxiv_id: '2510.00143'
source_url: https://arxiv.org/abs/2510.00143
tags:
- plaid
- distill
- documents
- clir
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The HLTCOE team applied PLAID, an mT5 reranker, GPT-4 reranker,
  score fusion, and document translation to the TREC 2024 NeuCLIR track. They experimented
  with multiple training techniques including Translate Distill (TD), Generate Distill
  (GD), and multilingual translate-distill (MTD) for fine-tuning mPLMs for CLIR.
---

# HLTCOE at TREC 2024 NeuCLIR Track

## Quick Facts
- **arXiv ID**: 2510.00143
- **Source URL**: https://arxiv.org/abs/2510.00143
- **Reference count**: 20
- **Primary result**: Extractive report generation outperformed abstractive; GPT-4 heapsort reranking most effective for CLIR; MLIR benefited from GPT-4 reranking over PLAID TD results

## Executive Summary
The HLTCOE team participated in the TREC 2024 NeuCLIR track using PLAID dense retrieval, mT5 rerankers, GPT-4 reranking, score fusion, and document translation. They experimented with Translate Distill (TD), Generate Distill (GD), and multilingual translate-distill (MTD) for fine-tuning mPLMs for cross-lingual information retrieval (CLIR). For the report generation task, they developed both extractive and abstractive approaches. The most effective CLIR runs used GPT-4 heapsort reranking, while MLIR benefited from GPT-4 reranking over PLAID TD results. Token pooling reduced index size but slowed indexing substantially.

## Method Summary
The team implemented PLAID/PLAID-X dense retrieval with XLM-RoBERTa-Large, training via Translate-Distill by transferring mT5-XXL teacher scores from English MS MARCO to target-language passages. Generate-Distill used Mixtral-8x7B-Instruct to generate queries over target corpora before applying the TD pipeline. For multilingual training, they explored mixed-entry, mixed-passage, and round-robin batching strategies. Reranking used both mT5-XXL pointwise rerankers and GPT-4 4-ary heapsort over top-30 candidates. Score fusion combined multiple runs for MLIR. Report generation employed extractive methods (DSPy + GPT-4o/GPT-3.5-Turbo) and abstractive methods (GPT-Researcher + GPT-4o/Claude-3.5-Sonnet) with fact grouping for citations.

## Key Results
- GPT-4 heapsort reranking produced the most effective CLIR runs
- MLIR benefited from GPT-4 reranking over PLAID TD results
- Extractive report generation approach outperformed abstractive in citation precision and nugget recall
- Token pooling reduced index size by 50% but significantly slowed indexing time

## Why This Works (Mechanism)

### Mechanism 1: Translate-Distill Knowledge Transfer
- Claim: Transferring relevance judgments from a powerful monolingual reranker to a multilingual dense retriever via translated documents may enable effective cross-lingual retrieval without in-language relevance labels.
- Mechanism: English MS MARCO passages are translated to target languages; the mT5 cross-encoder scores English query–translated passage pairs; a ColBERT-X student model is trained to mimic these scores using KL-divergence loss, learning to associate English queries with non-English document representations.
- Core assumption: Translation quality is sufficiently high that relevance signals transfer meaningfully; the teacher model's English-language relevance judgments generalize to translated content.
- Evidence anchors:
  - [abstract] "TD uses scores from the mT5 model over English MS MARCO query-document pairs to learn how to score query-document pairs where the documents are translated to match the CLIR setting."
  - [section 4.1] "Finally, we train the ColBERT-X model using these hard passages in the document language, queries in English, and scores from the mT5-xxl reranker with a KL Divergence loss."
  - [corpus] Weak direct evidence—neighbor papers discuss NeuCLIR but do not independently validate TD's transfer efficiency.
- Break condition: If translation quality degrades significantly (e.g., low-resource languages, domain-specific terminology), or if the teacher model exhibits strong English-specific biases, relevance signal transfer may fail.

### Mechanism 2: LLM-Based Heapsort Reranking
- Claim: Using an LLM to compare passages via heapsort may improve top-k ranking quality compared to neural pointwise rerankers, particularly for precision-focused metrics.
- Mechanism: GPT-4 is prompted to select the most relevant passage among small subsets (4–5 passages); a 4-ary heapsort algorithm iteratively builds a ranked ordering, leveraging the LLM's comparative judgment rather than absolute scoring.
- Core assumption: LLMs provide more reliable comparative relevance judgments than pointwise probability outputs; the overhead of multiple LLM calls is acceptable for the target use case.
- Evidence anchors:
  - [abstract] "The most effective CLIR runs used GPT-4 heapsort reranking, while MLIR benefited from GPT-4 reranking over PLAID TD results."
  - [section 3] "Heapsort was used to identify the top 20 passages. The remaining passages were left in an unsorted order since the primary measure used in NeuCLIR is nDCG@20."
  - [corpus] Neighbor paper "Beyond Sequential Reranking" discusses reranker limitations but does not specifically validate heapsort-based LLM reranking.
- Break condition: If API costs or latency become prohibitive, or if the LLM exhibits position bias in comparative prompts, ranking quality may degrade.

### Mechanism 3: Extractive Report Generation via Fact Grouping
- Claim: Structured extraction and grouping of facts from retrieved documents may yield higher fact coverage and citation precision than abstractive summarization for evaluation metrics prioritizing nugget recall.
- Mechanism: Queries are decomposed; retrieved documents are processed to extract key facts; facts are clustered into canonical groups; each canonical fact description becomes a report sentence with citations derived from source documents.
- Core assumption: Evaluation metrics (ARGUE score, nugget recall) reward fact density over narrative coherence; LLMs can reliably extract and group facts without introducing hallucinations.
- Evidence anchors:
  - [abstract] "The extractive approach outperformed the abstractive approach in report generation."
  - [section 5] "Since the evaluation only rewards individual sentences in scoring reports instead of favoring a coherent, human-readable report, we treat each canonical fact as the report sentence."
  - [corpus] "Incorporating Q&A Nuggets into Retrieval-Augmented Generation" supports the general principle of nugget-based generation but is not direct validation.
- Break condition: If evaluation criteria shift to prioritize coherence or narrative flow, or if fact extraction introduces systematic errors, this approach's advantages may diminish.

## Foundational Learning

- Concept: **Knowledge Distillation for Retrieval**
  - Why needed here: Translate-Distill and Generate-Distill both rely on transferring ranking knowledge from a teacher model to a student retriever.
  - Quick check question: Can you explain why KL-divergence loss is used instead of binary cross-entropy for distilling ranking signals?

- Concept: **Late Interaction Architecture (ColBERT/PLAID)**
  - Why needed here: All dense retrieval runs use PLAID, which encodes tokens individually and computes MaxSim similarity at query time.
  - Quick check question: How does late interaction differ from bi-encoder approaches that produce a single vector per document?

- Concept: **Heapsort for Ranking**
  - Why needed here: GPT-4 reranking uses 4-ary heapsort to produce a ranked list from pairwise/comparative LLM judgments.
  - Quick check question: Why might comparative sorting be more robust than pointwise scoring when using LLMs as judges?

## Architecture Onboarding

- Component map: Query Input → Query Expansion (LLM decomposition for reports) → PLAID Dense Retrieval (ColBERT-X with TD/GD training) → First-stage Ranking → mT5 Reranker (optional) → GPT-4 Heapsort Reranker (top-30 passages) → Score Fusion (multiple run combination) → Report Generation (Extractive or Abstractive pipeline)

- Critical path: PLAID retrieval with Translate-Distill training → GPT-4 heapsort reranking → This combination produced the best CLIR and MLIR results.

- Design tradeoffs:
  - Token pooling: Reduced index size by 50% but significantly slowed indexing time (Section 4.3, Section 6 conclusion).
  - Passage length: 450-token non-overlapping passages improved effectiveness over 180-token strided passages.
  - Extractive vs. abstractive reports: Extractive yields higher fact coverage but lower coherence.

- Failure signatures:
  - If mT5 reranking underperforms fusion baselines (as in some CLIR configurations), check whether domain mismatch or language-specific issues are affecting the cross-encoder.
  - If GPT-4 heapsort returns no document ID, the system defaults to the first document—this may introduce ranking errors during heapify (Section 3).
  - GPT-3.5Turbo struggled with citation precision in report generation (Section 6.4)—use stronger models for extraction tasks.

- First 3 experiments:
  1. **Baseline comparison**: Run PLAID with standard Translate-Train vs. Translate-Distill on a held-out CLIR query set to isolate distillation's contribution.
  2. **Reranker ablation**: Compare GPT-4 heapsort vs. mT5 pointwise reranking on the same top-30 candidate set; measure nDCG@20 and latency.
  3. **Report generation metric sensitivity**: Evaluate extractive vs. abstractive reports under both nugget-based metrics (ARGUE) and coherence-focused metrics to understand tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does Generate-Distill (GD) offer distinct advantages over Translate-Distill (TD) for cross-lingual dense retrieval?
- Basis in paper: [explicit] The conclusion explicitly states, "Directions for future research include more investigation into training CLIR and MLIR models using generate-distill."
- Why unresolved: The results were mixed; GD (run c1) was highly effective for Persian but underperformed significantly for Chinese compared to other methods like standard TD or fusion.
- What evidence would resolve it: A systematic ablation study comparing GD and TD across diverse languages and domains to isolate why GD succeeded for Persian but failed for Chinese.

### Open Question 2
- Question: Can token pooling strategies be optimized to reduce index size without incurring substantial indexing latency penalties or effectiveness losses?
- Basis in paper: [explicit] The paper notes that reducing the index size using token pooling "was less effective and slowed indexing time substantially."
- Why unresolved: While token pooling theoretically offers efficiency gains via reduced storage, the implementation resulted in a practical bottleneck during the indexing phase and degraded retrieval scores.
- What evidence would resolve it: Profiling data identifying the specific computational overhead (likely k-means clustering) in the pooling process and effectiveness comparisons at different pooling compression rates.

### Open Question 3
- Question: How can abstractive report generation systems be better aligned with strict factuality constraints to compete with extractive methods in citation precision?
- Basis in paper: [explicit] The authors note that LLMs "can be prone to hallucination" and observed that the extractive approach "outperformed the abstractive approach... especially in terms of citation precision."
- Why unresolved: The inherent trade-off between the fluency of abstractive models and the strict adherence to retrieved evidence remains a challenge, as evidenced by GPT-3.5's struggles.
- What evidence would resolve it: Experiments combining the extractive "grouping" mechanism with abstractive summarization to see if grounding improves citation accuracy without losing fluency.

## Limitations

- Translation quality assumption underlying Translate-Distill approach remains untested and may limit effectiveness for low-resource languages
- Token pooling implementation reduced index size but significantly slowed indexing time without quantitative measurements of the trade-off
- Evaluation framework for report generation (ARGUE score, nugget recall) may not fully capture real-world usability requirements for coherent, human-readable reports

## Confidence

- **High confidence**: PLAID/PLAID-X dense retrieval architecture implementation; GPT-4 heapsort reranking mechanism; extractive vs. abstractive report generation comparison results
- **Medium confidence**: Translate-Distill knowledge transfer effectiveness; Generate-Distill methodology without specific prompt templates; score fusion strategy for MLIR
- **Low confidence**: Translation quality impact on TD effectiveness; token pooling implementation details and indexing performance; meta-system prompt for report combination

## Next Checks

1. **Translation quality validation**: Measure retrieval effectiveness when using original vs. machine-translated MS MARCO documents as training data to isolate translation impact on TD performance
2. **Indexing performance quantification**: Benchmark token pooling implementation with concrete measurements of index size reduction vs. indexing time overhead across different k-means configurations
3. **Evaluation framework expansion**: Test report generation approaches under additional metrics including coherence scores, factual consistency, and human readability assessments to validate that nugget-based evaluation fully captures practical utility