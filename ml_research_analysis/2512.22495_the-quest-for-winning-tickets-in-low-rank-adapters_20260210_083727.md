---
ver: rpa2
title: The Quest for Winning Tickets in Low-Rank Adapters
arxiv_id: '2512.22495'
source_url: https://arxiv.org/abs/2512.22495
tags:
- lora
- layer
- sparsity
- performance
- loras
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether the Lottery Ticket Hypothesis (LTH)
  applies to parameter-efficient fine-tuning (PEFT) methods, specifically focusing
  on Low-Rank Adaptation (LoRA). The authors show that sparse subnetworks ("winning
  tickets") within LoRAs can achieve performance comparable to dense LoRA while significantly
  reducing trainable parameters.
---

# The Quest for Winning Tickets in Low-Rank Adapters

## Quick Facts
- arXiv ID: 2512.22495
- Source URL: https://arxiv.org/abs/2512.22495
- Authors: Hamed Damirchi; Cristian Rodriguez-Opazo; Ehsan Abbasnejad; Zhen Zhang; Javen Shi
- Reference count: 40
- Primary result: Partial-LoRA reduces trainable parameters by up to 87% while maintaining or improving accuracy across 8 vision and 12 language tasks.

## Executive Summary
This paper investigates whether the Lottery Ticket Hypothesis extends to parameter-efficient fine-tuning methods, specifically focusing on Low-Rank Adaptation (LoRA). The authors demonstrate that sparse subnetworks ("winning tickets") within LoRAs can achieve performance comparable to dense LoRA while significantly reducing trainable parameters. They introduce Partial-LoRA, a method that derives task-specific sparsity ratios from pre-trained models to systematically identify and apply random masking to LoRA components. Experiments show that Partial-LoRA can reduce trainable parameters from 110M to 120k for ViT-B-16 while maintaining accuracy.

## Method Summary
Partial-LoRA derives per-layer sparsity ratios by progressively masking pre-trained weights by importance until few-shot accuracy drops below 90% of baseline. Importance is computed using SVD-based leverage scores from truncated SVD of weight matrices. Random masks are then applied to LoRA matrices using these derived ratios, with only unmasked parameters remaining trainable. The method trains with standard LoRA hyperparameters but significantly fewer parameters. The theoretical analysis establishes that performance depends more on the amount of sparsity per layer than on exact weight selection, and that precise subnetwork selection is unnecessary for effective fine-tuning.

## Key Results
- Partial-LoRA reduces trainable parameters by up to 87% while maintaining or improving accuracy across 8 vision and 12 language tasks
- Random masking (Partial-LoRA) outperforms deterministic selection (Targeted-LoRA) on average, with 81.78% vs 81.48% accuracy
- Flow preservation mechanisms from traditional pruning are unnecessary for LoRA masking due to the residual formulation
- Effectiveness depends more on per-layer sparsity ratios than on exact weight selection

## Why This Works (Mechanism)

### Mechanism 1
Randomly masked LoRA subnetworks can approximate full LoRA performance when sparsity ratios are calibrated per-layer using pre-trained model structure. The pre-trained model's weight matrices encode task-relevant subspaces identifiable via SVD-based leverage scores. By masking LoRA components proportionally to these importance scores, fine-tuning capacity is concentrated in task-critical directions while discarding redundant parameters. Theoretical bounds show a randomly masked adapter can approximate a target adapter if width margins satisfy logarithmic constraints.

### Mechanism 2
Deterministic subnetwork selection is unnecessary—random masking with correct per-layer sparsity ratios outperforms targeted selection. Partial-LoRA (random masking) vs. Targeted-LoRA (deterministic masking using importance indices) comparisons show random masking achieves comparable or superior accuracy. The capacity per layer, not specific indices, determines performance. Random masks provide regularization benefits in overfitting-prone scenarios.

### Mechanism 3
Flow preservation mechanisms from traditional pruning are unnecessary for LoRA masking due to the residual formulation. In standard pruning, consecutive layers with misaligned masks can zero out activations. LoRA's formulation ensures the frozen base weights always provide signal flow even when residual weights are masked to zero. The additive structure guarantees non-zero outputs regardless of mask configuration.

## Foundational Learning

- **Lottery Ticket Hypothesis (LTH)**: Understanding "winning tickets" as sparse trainable subnetworks is prerequisite. Quick check: Can you explain why a sparse subnetwork trained from scratch might match a dense network's performance?

- **Low-Rank Adaptation (LoRA) mechanics**: Understanding ΔW = BA decomposition, rank constraints, and why only residuals are trainable is essential. Quick check: Given W ∈ R^{m×n}, how many parameters does LoRA with rank d require versus full fine-tuning?

- **Singular Value Decomposition and leverage scores**: The primary importance measure uses truncated SVD to identify rows/columns aligned with principal subspaces. Quick check: If W ≈ P_k Λ_k Q_k^T, what does a high row leverage score for row i indicate about that row's importance?

## Architecture Onboarding

- **Component map**: Few-shot samples → Importance scores (SVD/SNIP/IMP) → Sparsity ratio derivation (Algorithm 1) → Mask generation → Partial-LoRA training loop
- **Critical path**: 1) Sample few-shot data (16 shots typical for vision, 2-5% for language) 2) Compute baseline accuracy on frozen pre-trained model 3) Run Algorithm 1 per layer: compute importance → progressive masking → stop at 90% threshold 4) Generate random masks using derived sparsity ratios 5) Train LoRA with masked parameters only
- **Design tradeoffs**: 90% vs. stricter threshold (lower thresholds reduce parameters further but risk underfitting); SVD vs. gradient-based importance (SVD requires no forward pass; SNIP/IMP require gradient computation); Random vs. targeted masking (random provides regularization; targeted ensures important indices preserved)
- **Failure signatures**: Accuracy collapse on small datasets (sparsity too aggressive; increase accuracy threshold); Inconsistent results across runs (mask randomness too high; reduce sparsity or use targeted variant); No parameter reduction achieved (baseline accuracy already low; pre-trained model poorly suited to task)
- **First 3 experiments**: 1) Apply Partial-LoRA to CIFAR-10 with standard LoRA rank, compare parameter count and accuracy against dense LoRA baseline 2) Run SVD, SNIP, and IMP variants on same task; measure overlap of extracted subnetworks and final performance differences 3) Sweep accuracy threshold (70%, 80%, 90%) on Flowers dataset; observe parameter-accuracy tradeoff curve

## Open Questions the Paper Calls Out
- Analysis of training dynamics and convergence behavior between Partial-LoRA and dense LoRA
- Exploration of higher sparsity ratios beyond the demonstrated 87% on complex tasks
- Understanding why random masking occasionally outperforms deterministic subnetwork selection

## Limitations
- Theoretical analysis largely confined to single-layer LoRA components, not extended to multi-layer network behavior
- Empirical validation relies on specific architectures (ViT-B-16, Deberta-V3-Base, LLAMA2-7b, LLAMA3.1-8b, Qwen2.5-14b) and may not generalize to other model families
- Correlation between leverage scores and fine-tuning utility remains empirically validated rather than theoretically proven

## Confidence
- **High Confidence**: Empirical results showing Partial-LoRA achieves comparable accuracy to dense LoRA while reducing trainable parameters by up to 87% across multiple datasets and models
- **Medium Confidence**: Theoretical approximation bounds in Theorem 4.1, though the multi-layer extension remains unvalidated
- **Low Confidence**: Generalization of sparsity ratios derived from few-shot samples to full fine-tuning performance for language models

## Next Checks
1. Extend Theorem 4.1 to establish approximation bounds for the full LoRA adapter across multiple layers
2. Apply Partial-LoRA to model architectures not evaluated in the paper (e.g., ConvNets for vision, smaller language models like BERT-base)
3. Systematically compare SVD-based leverage scores against gradient-based measures (SNIP, IMP) across diverse tasks