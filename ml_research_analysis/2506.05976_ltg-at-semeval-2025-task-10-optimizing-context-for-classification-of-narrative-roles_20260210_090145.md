---
ver: rpa2
title: 'LTG at SemEval-2025 Task 10: Optimizing Context for Classification of Narrative
  Roles'
arxiv_id: '2506.05976'
source_url: https://arxiv.org/abs/2506.05976
tags:
- text
- entity
- language
- context
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a context selection approach for entity framing
  classification in news articles. The authors use a multilingual XLM-RoBERTa model
  to classify 22 fine-grained narrative roles assigned to entities across Bulgarian,
  English, Hindi, Portuguese, and Russian.
---

# LTG at SemEval-2025 Task 10: Optimizing Context for Classification of Narrative Roles

## Quick Facts
- arXiv ID: 2506.05976
- Source URL: https://arxiv.org/abs/2506.05976
- Reference count: 16
- Primary result: Entity-oriented context selection with XLM-RoBERTa-large achieved 47.75 Micro F1 on multilingual narrative role classification

## Executive Summary
This work addresses entity framing classification in news articles across five languages using a multilingual XLM-RoBERTa model with limited context windows. The authors develop a simple entity-oriented heuristic that extracts text segments between entity mentions, combined with an explicit entity prefix, to enable effective classification. Their approach significantly outperforms larger language models with supervised fine-tuning, particularly for low-resource languages, demonstrating that simple heuristics can effectively prepare data for models with context constraints while maintaining computational efficiency.

## Method Summary
The authors fine-tuned XLM-RoBERTa-large on merged multilingual data (BG, EN, HI, PT, RU) for 10 epochs using standard Transformers settings. They implemented an "entity-to-entity" (ent2ent) context extraction heuristic that captures the sentence containing a target entity plus all subsequent sentences until another entity appears. Crucially, they prepend "Regarding <entity>:\n" to each extracted segment to disambiguate multi-entity contexts. The model performs multi-label classification across 22 fine-grained narrative roles with three main roles (protagonist, antagonist, innocent).

## Key Results
- ent2ent extraction with entity prefix achieved 47.75 Micro F1 on development data
- Multilingual joint training improved results, especially for low-resource languages (BG: 0.0 → 26.67, RU: 0.0 → 52.33 Micro F1)
- XLM-RoBERTa-large significantly outperformed larger models (Llama: 31.78, Mistral: 29.52 Micro F1)
- The entity prefix alone contributed a 17-point improvement (30.11 vs 47.75 Micro F1 without prefix)

## Why This Works (Mechanism)

### Mechanism 1
Entity-oriented context selection enables MLMs with 512-token limits to classify narrative roles in long documents. The ent2ent heuristic extracts the sentence containing a target entity plus all subsequent sentences until another entity appears, filtering irrelevant text while preserving multi-sentence framing context. Core assumption: Narrative framing signals cluster near entity mentions and between adjacent entity mentions.

### Mechanism 2
Prepending an explicit entity reference prefix dramatically improves classification when the same text span may apply to multiple entities. Adding "Regarding <entity>:\n" before the extracted segment gives the model unambiguous focus, disambiguating cases where multiple entities appear in the same context window. Core assumption: XLM-R's attention mechanism can use the prefix to weight entity-relevant tokens more heavily.

### Mechanism 3
Multilingual joint training improves performance on low-resource languages more than monolingual fine-tuning. Training XLM-R on all 5 languages simultaneously provides positive transfer, with languages sharing representations that benefit smaller training splits. Core assumption: Languages share cross-lingual representations in XLM-R's pretrained embeddings that transfer to narrative role classification.

## Foundational Learning

- **Concept: Subword tokenization and context windows**
  - Why needed here: XLM-R's 512-token limit is in subword tokens, not words; a typical news article exceeds this. Understanding tokenization helps estimate how much text fits.
  - Quick check question: If an article is 800 words, approximately how many subword tokens would it be for XLM-R?

- **Concept: Multi-label classification with hierarchical labels**
  - Why needed here: Each entity has one main role (3 classes) but can have multiple fine-grained roles (22 classes). This requires multi-label output heads, not standard softmax.
  - Quick check question: Why can't standard cross-entropy with argmax work for fine-grained roles?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper compares against SFT of 7-8B models using LoRA; understanding this helps interpret why smaller fine-tuned XLM-R matched larger models.
  - Quick check question: What are the key LoRA hyperparameters (r, α) mentioned, and what do they control?

## Architecture Onboarding

- **Component map:**
  Raw document + entity annotations -> Text span extraction (ent2ent heuristic) -> Entity prefix prepending -> XLM-R-large tokenizer (512-token truncation) -> Fine-tuned XLM-R classifier -> Multi-label output (22 roles)

- **Critical path:** The ent2ent extraction logic is the core contribution. Implement it first: for each entity, extract its sentence + subsequent sentences until next entity. Then add prefix.

- **Design tradeoffs:**
  - ent2ent vs. sentence-only: ent2ent captures more context but risks including distractor text; Table 2 shows ent2ent (47.75) slightly beats sentence (46.06)
  - Multilingual vs. monolingual: Multilingual is essential for low-resource languages but may introduce noise for high-resource
  - XLM-R vs. LLM SFT: XLM-R is far cheaper; LLM SFT was surprisingly worse (Llama: 31.78, Mistral: 29.52 vs. XLM-R: 47.75)

- **Failure signatures:**
  - Micro F1 near 0 for a language → likely insufficient in-language training data; switch to multilingual training
  - Large gap between Micro F1 and Macro F1 → model biased toward frequent classes
  - Poor English performance despite adequate data (Table 2, EN: 31.25) → may indicate annotation quality issues or domain mismatch

- **First 3 experiments:**
  1. **Baseline replication:** Implement ent2ent extraction + entity prefix + XLM-R-large fine-tuning on merged multilingual data. Target: ~47-48 Micro F1 on dev.
  2. **Ablation: Remove prefix:** Run same setup without "Regarding <entity>:" prefix. Expect ~30 Micro F1; confirms prefix importance.
  3. **Compare context strategies:** Test sentence-only, paragraph, and full-text extraction on same model. Expect ent2ent ≈ sentence > paragraph > full-text per Table 2.

## Open Questions the Paper Calls Out

### Open Question 1
Does applying the "entity-to-entity" (ent2ent) context optimization strategy to larger generative LLMs during Supervised Fine-Tuning (SFT) improve their performance relative to full-text fine-tuning? The authors note that applying context optimization might prove beneficial for SFT but testing this was beyond their resource allocations.

### Open Question 2
Does simplifying the SFT task to single-entity classification per prompt significantly improve performance compared to multi-entity classification? The reported SFT results required the model to return roles for all entities in a document simultaneously, which may have introduced unnecessary complexity.

### Open Question 3
What specific dataset characteristics or linguistic features explain the discrepancy in performance across languages, particularly the poor results on English compared to Portuguese? The authors observed that XLM-R performed best on Russian (low resource) and Portuguese (high resource) but poorly on English, but did not analyze the specific textual or annotation complexities that caused this variance.

### Open Question 4
Can learned or generative context extraction methods be refined to outperform simple rule-based heuristics for this task? The authors found that the "gpt-extracted" context strategy underperformed compared to the simple "ent2ent" heuristic, but it remains unclear if this is a limitation of the specific prompt or a fundamental limitation of generative extraction.

## Limitations

- The ent2ent heuristic assumes narrative framing information clusters near entity mentions and between adjacent entities, potentially missing framing that appears far from entity mentions
- The prefix mechanism's effectiveness may be specific to this particular task and dataset, with limited investigation into alternative formulations or entity ambiguity handling
- The approach may not scale well to languages with divergent narrative structures, and the analysis of which language pairs benefit most from transfer is limited

## Confidence

**High Confidence (9/10):**
- Entity-oriented context selection enables classification within 512-token limits
- Multilingual joint training improves low-resource language performance
- XLM-R outperforms larger models in cost-efficiency terms

**Medium Confidence (7/10):**
- Prefix addition dramatically improves classification accuracy
- ent2ent extraction performs better than sentence-only or paragraph strategies
- The relative performance ranking of context strategies (ent2ent > sentence > paragraph > full-text)

**Low Confidence (4/10):**
- Cross-lingual transfer mechanisms and which language pairs benefit most
- Generalizability of prefix approach to other entity types and naming conventions
- Performance when framing information appears outside entity clusters

## Next Checks

1. **Context extraction ablation study:** Systematically compare ent2ent against sentence-only, paragraph, and sliding window approaches on the same model and data. Measure not just overall F1 but also examine cases where each strategy succeeds/fails, particularly for documents where framing appears distant from entity mentions.

2. **Prefix mechanism analysis:** Test the prefix approach with alternative formulations (e.g., "About <entity>:", "Focus on <entity>:", or positional prefixes like "Paragraph 3: <entity>"). Also test with ambiguous entity names and common nouns to assess robustness to entity ambiguity.

3. **Cross-lingual transfer analysis:** Train separate bilingual models (e.g., English-Portuguese, English-Russian) and analyze which language pairs show the strongest transfer. Compare against random baselines and analyze whether transfer correlates with linguistic similarity or shared narrative conventions.