---
ver: rpa2
title: 'Causal Tree Extraction from Medical Case Reports: A Novel Task for Experts-like
  Text Comprehension'
arxiv_id: '2503.01302'
source_url: https://arxiv.org/abs/2503.01302
tags:
- case
- causal
- medical
- evaluation
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Causal Tree Extraction (CTE), a novel task
  that extracts multi-layered causal relationships from medical case reports into
  a tree structure with the primary disease as the root. The authors constructed J-Casemap,
  a Japanese case report dataset with 14,094 annotated causal trees.
---

# Causal Tree Extraction from Medical Case Reports: A Novel Task for Experts-like Text Comprehension

## Quick Facts
- **arXiv ID:** 2503.01302
- **Source URL:** https://arxiv.org/abs/2503.01302
- **Reference count:** 39
- **Primary result:** Novel task extracting hierarchical causal trees from medical case reports, achieving 82.7 human eval score (20.2 pts above baseline)

## Executive Summary
This paper introduces Causal Tree Extraction (CTE), a novel task that transforms medical case reports into hierarchical tree structures with the primary disease as root. The authors constructed J-Casemap, a Japanese case report dataset with 14,094 annotated causal trees, and proposed a generation-based method using LLMs that achieved 82.7 in human evaluation. The approach outperforms traditional relation extraction methods by 20.2 points by leveraging hierarchical structure and domain-specific pretraining. The work also develops evaluation metrics with heuristic weighting that better reflect clinician preferences, demonstrating potential for various medical applications including improved medical QA performance.

## Method Summary
The method involves two key stages: (1) continual pretraining on 2B Japanese medical tokens to inject domain knowledge, followed by (2) LoRA-based SFT fine-tuning on J-Casemap. The model generates causal trees using depth-first linearization with indentation to represent hierarchical relationships. Unlike traditional relation extraction, this generation approach maintains consistency across multi-layered inferences and leverages the LLM's pre-training on code syntax. The approach is evaluated using weighted triplet F1 metrics that prioritize entities by tree depth and relation type, better aligning with clinical validity where root disease identification is paramount.

## Key Results
- Proposed generation-based CTE method achieved 82.7 human evaluation score, outperforming RE baseline by 20.2 points
- Weighted evaluation metrics with depth-based weighting (W = 1/(1 + C^d × x_relation)) better reflect clinician preferences than standard Triplet F1
- J-Casemap dataset of 14,094 Japanese case reports enables improved medical QA performance
- Continual pretraining on medical corpora proves critical for low-resource fine-tuning effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structuring causal extraction as tree generation with depth-first linearization captures hierarchical diagnostic logic better than flat relation extraction
- **Mechanism:** The model generates text using indentation to represent nested `parent_of` relationships, leveraging LLM's code pre-training to maintain consistency across multi-layered inferences
- **Core assumption:** LLM's ability to parse code syntax transfers effectively to medical semantic structures
- **Evidence anchors:** [abstract] CTE generates causal trees vs treating relations equally; [section 3.2] indentation represents nested structures; [corpus] CR3G supports causal reasoning needs
- **Break condition:** If input lacks explicit hierarchical cues, model may hallucinate connections or generate syntactically invalid trees

### Mechanism 2
- **Claim:** Continual pretraining on high-quality medical corpora injects domain-specific causal priors critical for low-resource fine-tuning
- **Mechanism:** Exposing model to vast medical text before task-specific training teaches linguistic patterns of medical causality (e.g., "occlusion causes infarction")
- **Core assumption:** Signal-to-noise ratio in medical pretraining data is sufficient to teach diagnostic reasoning patterns
- **Evidence anchors:** [abstract] combine continual pretraining with Japanese medical data; [section 6.2] CPT more effective in low-resource settings
- **Break condition:** If target domain differs significantly from CPT domain, transfer benefits may degrade

### Mechanism 3
- **Claim:** Evaluation metrics weighting entities by tree depth and relation type align more closely with clinical validity than standard Triplet F1
- **Mechanism:** Applying weight decay factor based on depth penalizes errors at top of tree more heavily, mirroring diagnostic stakes
- **Core assumption:** Wrong root node constitutes "total failure" while missing leaf node is minor error
- **Evidence anchors:** [abstract] introduce evaluation metrics that reflect clinician preferences; [section 4.2] weighting method reduces gap
- **Break condition:** If model produces tree with correct root but fictional intermediate nodes, metric might still assign deceptively high score

## Foundational Learning

- **Concept:** Bottom-up Diagnostic Inference
  - **Why needed here:** CTE is designed to reverse-engineer this process (findings → intermediate conditions → primary disease)
  - **Quick check question:** In the tree `-A @ B \n -C`, is A the cause of C, or is C the cause of A? (Answer: C is the parent/root cause of A; A is modified by B)

- **Concept:** Relation Types (Causal vs. Modifiers)
  - **Why needed here:** The paper distinguishes `parent_of` (causal) from `located` (@), `polarity` (/), `tested` (=), and `featured` (*). Confusing these leads to structurally invalid trees
  - **Quick check question:** Does "SpO2 / Low" indicate that SpO2 caused the disease, or is it a test result describing patient's state? (Answer: It is a modifier/test result)

- **Concept:** Depth-First Linearization
  - **Why needed here:** The model inputs/outputs trees as text. Indentation represents structure
  - **Quick check question:** If a line has 2 indents and previous line had 1, is the new line a child of previous line or the root? (Answer: Child of the previous line)

## Architecture Onboarding

- **Component map:** Raw Japanese Case Report -> LLM Encoder (with CPT) -> LoRA Adapter -> Output Parser (converts indented text to tree) -> Weighted Triplet F1 Evaluator

- **Critical path:**
  1. Data Prep: Format J-Casemap pairs (Report -> Indented Tree)
  2. CPT: Train base LLM on 2B tokens of medical text
  3. SFT: Fine-tune with LoRA on formatted J-Casemap data
  4. Inference: Prompt model with report; receive indented text
  5. Eval: Parse text to triplets; apply depth-based weighting

- **Design tradeoffs:**
  - Generation vs. Extraction: Chosen generation (high consistency, risk of hallucination) over RE (no hallucination, low consistency/no span info). *Mitigation:* Use constrained decoding or post-hoc entity linking
  - Metric complexity: Weighted F1 is harder to implement than standard F1 but necessary for model selection that aligns with doctors

- **Failure signatures:**
  - Root Hallucination: Model invents primary disease not in text
  - Syntactic Collapse: Indentation breaks, resulting in flat list or malformed tree
  - Relation Confusion: Using `@` (location) instead of `parent_of` (causality)

- **First 3 experiments:**
  1. **Sanity Check (Zero-shot):** Prompt base model with case report to see if it naturally outputs any structure (likely fails)
  2. **Ablation (CPT vs. No-CPT):** Train two models with only 25% of SFT data - one with CPT, one without - to verify domain injection hypothesis
  3. **Metric Correlation:** Evaluate baseline RE model vs. Generation model using both vanilla F1 and Weighted F1 to confirm 20.2-point human eval gap is visible in automatic metric

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent can hallucinations in generated causal trees be mitigated using advanced methods like RAG?
- **Basis in paper:** [explicit] Section 9 states hallucination problems were observed but solutions were not discussed
- **Why unresolved:** Generative models occasionally produce entities not present in source text, a flaw inherent to the generation method
- **What evidence would resolve it:** Comparative study showing significantly reduced entity hallucination rates when applying RAG or constrained decoding

### Open Question 2
- **Question:** How can automatic evaluation metrics be refined to more closely mimic clinician preferences beyond current heuristic weighting?
- **Basis in paper:** [explicit] Section 9 notes despite improvements, "developing more comprehensive and accurate automatic metrics that more closely resemble manual evaluation is necessary"
- **Why unresolved:** Correlation between proposed weighted metric and human evaluation is only approximately 0.6, leaving substantial gap
- **What evidence would resolve it:** Evaluation framework achieving correlation coefficient significantly higher than 0.6 with expert human scoring

### Open Question 3
- **Question:** Can the Causal Tree Extraction framework be effectively generalized to medical departments outside internal medicine?
- **Basis in paper:** [explicit] Section 9 identifies exclusive use of internal medicine data as limitation that "potentially limits scope of this study"
- **Why unresolved:** Annotation schema and model training were tailored to internal medicine case reports; distinct clinical structures in surgery/pediatrics may require schema modifications
- **What evidence would resolve it:** Successful application and evaluation on case reports from external departments using existing or unified schema

## Limitations

- **Dataset accessibility:** J-Casemap is not publicly available, only annotation schema plus 100 samples from national exam data will be released
- **Hallucination risk:** Generative models occasionally produce entities not present in source text, identified in case studies but only partially mitigated
- **Limited clinical validation:** Practical utility demonstrated only through one medical QA experiment, lacking broader validation across different medical use cases

## Confidence

**High Confidence:** The fundamental contribution of framing causal extraction as tree generation with depth-first linearization is well-supported by ablation study showing 20.2-point improvement over RE baselines in human evaluation.

**Medium Confidence:** The weighted evaluation metrics that prioritize shallow layers and parent_of relations show good correlation with human judgment (~0.6 with vanilla F1), but their generalizability beyond J-Casemap dataset remains unproven.

**Low Confidence:** The long-term clinical utility of extracted causal trees for downstream applications is demonstrated only through one medical QA experiment, lacking broader validation.

## Next Checks

1. **Dataset Generalization Test:** Evaluate the proposed weighted metrics on external medical case report datasets (e.g., MIMIC-III clinical notes or CaseReportBench) to verify metric robustness across different annotation styles and medical domains.

2. **Hallucination Audit:** Conduct systematic error analysis comparing entity generation against source text spans, quantifying frequency and severity of hallucinated primary diseases or intermediate nodes across test set.

3. **Clinical Task Validation:** Test extracted causal trees in additional medical applications beyond QA - such as differential diagnosis support or treatment recommendation systems - to validate practical utility of tree structure for clinical decision-making.