---
ver: rpa2
title: 'LLM-based Embeddings: Attention Values Encode Sentence Semantics Better Than
  Hidden States'
arxiv_id: '2602.01572'
source_url: https://arxiv.org/abs/2602.01572
tags:
- sentence
- layers
- value
- token
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# LLM-based Embeddings: Attention Values Encode Sentence Semantics Better Than Hidden States

## Quick Facts
- arXiv ID: 2602.01572
- Source URL: https://arxiv.org/abs/2602.01572
- Authors: Yeqin Zhang; Yunfei Wang; Jiaxuan Chen; Ke Qin; Yizheng Zhao; Cam-Tu Nguyen
- Reference count: 22
- Primary result: Attention value vectors (VA) outperform hidden states (HS) for sentence embedding retrieval tasks

## Executive Summary
This paper introduces Value Aggregation (VA), a method for generating sentence embeddings from decoder-only LLMs by extracting and aggregating attention value vectors instead of the traditional hidden states. The authors demonstrate that VA, particularly when combined with prompt-based weighted aggregation (AlignedWVA), achieves superior performance on semantic tasks compared to existing methods like MetaEOL. The approach is training-free and provides a novel perspective on where semantic information is most effectively encoded within the transformer architecture.

## Method Summary
The core method extracts attention value vectors ($v^l_{n,h}$) from each head in selected layers, mean-pools them across tokens, then aggregates across a specified layer set $S$. For optimal performance, the authors introduce AlignedWVA, which applies prompts to generate semantically meaningful weights for the value vectors, followed by alignment with the residual stream via the output projection matrix $W_O$. The approach is evaluated on LLaMA-2 and Qwen-3 models across 14 tasks from the Massive Text Embedding Benchmark (MTEB).

## Key Results
- VA outperforms HS on segment-matching retrieval tasks at deeper layers (recall@10: VA ~0.75 vs HS ~0.55 at layer 28 for LLaMA-2)
- AlignedWVA with prompt engineering improves performance by ~20 points over weighted VA with last token only
- For GQA models (Qwen-3), W_O alignment provides ~15 point average improvement
- VA produces 4x smaller embeddings (1024-dim vs 4096-dim) with only ~2 point performance gap

## Why This Works (Mechanism)

### Mechanism 1
Value vectors encode sentence semantics more effectively than hidden states because they capture a sentence's influence on continuations rather than next-token prediction. Hidden states are optimized for next-token discrimination, which collapses semantically distinct sentences that share similar local continuations. Value vectors, aggregated across tokens and layers, approximate the continuation distribution that serves as a proxy for truth-conditional semantics.

### Mechanism 2
Weighted Value Aggregation using last-token attention scores improves retrieval by prioritizing tokens most relevant to continuation prediction. The last token's attention weights over prefix tokens encode which preceding tokens most influence the model's prediction. Using these as weights for value aggregation preserves information predictive of subsequent tokens.

### Mechanism 3
Applying the output projection matrix (W_O) aligns weighted value vectors with the residual stream space, enabling compatibility with standard embedding operations. Attention outputs live in a head-concatenated space. W_O maps these to the model's residual stream dimension, allowing comparison with hidden-state-based methods and downstream tools expecting that geometry.

## Foundational Learning

- **Residual Stream Architecture (Pre-LN Transformer)**: VA operates on internal attention values, not final outputs. Understanding how xl_n = xl-1_n + al_n + fl_n flows clarifies where value vectors sit in the computation graph.
- **Truth-Conditional Semantics**: The paper's theoretical justification relies on treating continuation distributions as proxies for truth conditions. Without this framing, the claim that VA captures "semantics" is underspecified.
- **Multi-Head Attention Value Computation**: VA concatenates head-level values (vl_n = [vl,1_n; ...; vl,H_n]). Understanding head specialization helps interpret why aggregation works.

## Architecture Onboarding

- **Component map**: Input tokens → Token/Position Embeddings → x^0_n → For each layer l ∈ {1...L}: LN → Q, K, V projections → vl,h_n (VALUE VECTORS - extraction point) → Attention scores α^l,h_{n,j} → Weighted sum → z^l,h_n → Concat heads → W_O projection → a^l_n (ATTENTION OUTPUT) → + FFN(f^l_n) → x^l_n (HIDDEN STATE - traditional extraction point)
- **Critical path**: Layer selection (Section 4.2) → Token pooling strategy (mean vs. weighted) → Optional W_O alignment → Final embedding
- **Design tradeoffs**: VA (mean pooling) vs. AlignedWVA (weighted + aligned): ~2-4 point gain for AlignedWVA but requires prompt engineering
- **Failure signatures**: Early-layer aggregation: Figure 2 shows VA underperforms HS at layers 0-10
- **First 3 experiments**: 
  1. Layer ablation on your target task: Run single-layer VA (S={l}) across all layers on a held-out validation set matching your downstream task category.
  2. VA vs. HS baseline comparison: For your chosen backbone, compare VA (using paper's default layer selection), HS (Half), and last-token pooling on 3-5 diverse tasks.
  3. Prompt sensitivity test for AlignedWVA: Compare WVA (LT), WVA (PromptEOL), and AlignedWVA (PromptEOL) to quantify the contribution of weighting vs. alignment.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can training objectives be specifically designed to maximize the representation capabilities of Value Aggregation (VA) beyond standard contrastive learning?
- **Open Question 2**: Can a learnable or dynamic layer-selection mechanism improve upon the manual, static layer selection required for optimal VA performance?
- **Open Question 3**: Does the performance of Aligned Weighted VA rely on the specific semantic content of the "FutureEOL" or "PromptEOL" prompts, or can similar weighting be achieved via prompt-free attention patterns?

## Limitations
- Optimal layer selection is model- and task-dependent, requiring manual calibration
- The theoretical justification for why value vectors capture semantics better than hidden states relies on specific assumptions about continuation distributions
- The best performance requires prompt engineering, adding complexity compared to the training-free baseline

## Confidence
- VA outperforms HS on retrieval tasks: High
- Value vectors capture continuation semantics better than next-token semantics: Medium
- Weighted aggregation with prompt-based weights improves performance: High

## Next Checks
- Verify that extracted value vectors have 1/4 the dimensionality of hidden states for GQA models
- Confirm that performance peaks in the late-middle layers (70-85% depth) before selecting aggregation range
- Test whether W_O alignment is necessary for your specific downstream task or if raw value vectors suffice