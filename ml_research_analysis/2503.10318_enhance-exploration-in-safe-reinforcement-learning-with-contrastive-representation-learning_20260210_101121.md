---
ver: rpa2
title: Enhance Exploration in Safe Reinforcement Learning with Contrastive Representation
  Learning
arxiv_id: '2503.10318'
source_url: https://arxiv.org/abs/2503.10318
tags:
- safety
- learning
- safe
- agent
- unsafe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of balancing exploration and
  safety in reinforcement learning for sparse-reward environments. The authors propose
  a method that uses contrastive representation learning to map observations to a
  latent space, distinguishing between safe and unsafe states.
---

# Enhance Exploration in Safe Reinforcement Learning with Contrastive Representation Learning

## Quick Facts
- **arXiv ID:** 2503.10318
- **Source URL:** https://arxiv.org/abs/2503.10318
- **Reference count:** 33
- **Primary result:** Method outperforms baselines in training efficiency while maintaining good balance between safety and exploration in sparse-reward environments

## Executive Summary
This paper addresses the challenge of balancing exploration and safety in reinforcement learning for sparse-reward environments. The authors propose a method that uses contrastive representation learning to map observations to a latent space, distinguishing between safe and unsafe states. They combine this with a safety critic trained on transferable domain priors to bias exploration towards safe actions while allowing for adequate exploration. The method consists of two phases: pre-training safety critics and using them to guide exploration during policy learning. Experiments on three MiniGrid navigation environments show that the proposed method outperforms baselines in training efficiency while maintaining a good balance between safety and exploration.

## Method Summary
The method employs a two-phase approach: (1) pre-train an auto-encoder with contrastive loss to separate safe/unsafe states in latent space, and train a prior Q-function from domain priors; (2) train a DQN with state safety check (latent distance to unsafe buffer) and action safety check (prior Q-function). The DQN uses prioritized replay and double Q-learning, with a latent dimension of 50 for the autoencoder. Training occurs over 200K steps with 3 seeds.

## Key Results
- The proposed method outperforms DQN+Domain Priors baseline in terms of training efficiency and safety violations
- Contrastive learning component allows the agent to generalize safety knowledge across similar environments
- The approach effectively reduces safety violations compared to methods without safety mechanisms while still enabling task completion

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating state safety checks from action safety checks improves exploration in sparse-reward environments by reducing unnecessary action restrictions in safe states.
- **Mechanism:** A contrastive loss trains an autoencoder to map observations to a latent space where Euclidean distance reflects safety similarity. The agent maintains a buffer of unsafe state embeddings and compares the current state's embedding against this buffer using mean L2 distance. Only when the current state is deemed unsafe (distance < threshold) does the action safety critic intervene to replace potentially unsafe actions. This decoupling allows free exploration in safe states while maintaining protection near dangerous regions.
- **Core assumption:** Assumes the latent space can meaningfully separate safe and unsafe states based on visual observations alone, and that proximity in this learned space correlates with safety risk.
- **Evidence anchors:**
  - [abstract] "A further contrastive learning objective is employed to distinguish safe and unsafe states...the latent distance is used to construct an additional safety check, which allows the agent to bias the exploration if it visits an unsafe state."
  - [section 4] "By separating state safety from action safety, we can ensure that exploration is only biased in undesirable states."
  - [corpus] Weak corpus evidence—related papers discuss constraint-aware RL but don't address state-action safety decoupling specifically.
- **Break condition:** Fails if the latent space fails to generalize across environments, if the distance threshold is poorly calibrated, or if the buffer lacks diversity in unsafe state representations.

### Mechanism 2
- **Claim:** Transferable domain priors provide a pre-trained safety critic that identifies unsafe actions, but cause over-conservative exploration when applied universally to all states.
- **Mechanism:** The prior Q-function Q_p is trained by aggregating optimal Q-functions from related tasks in the same domain. State-action pairs that are consistently undesirable across tasks are used to construct reward signals for training Q_p. During exploration, actions with Q_p below the state's mean are flagged as unsafe. The original method applies this check universally, creating false positives in states where all actions are actually safe.
- **Core assumption:** Assumes access to pre-trained Q-functions from related tasks, and that safety knowledge transfers across environments within the same domain.
- **Evidence anchors:**
  - [section 3.2] "However, because of the large number of false positives, some safe actions are never executed, leading to inadequate exploration in sparse-reward environments."
  - [section 5.2] "The reason is that exploration is biased in almost every states when the probability of forcing an agent to execute an alternative action is high."
  - [corpus] Weak corpus evidence—neighbor papers don't discuss domain priors for safe RL specifically.
- **Break condition:** Fails when source and target domains have different safety structures, or when sparse rewards require traversing states that appear undesirable to the prior.

### Mechanism 3
- **Claim:** Contrastive learning with margin-based loss creates a latent representation where safe and unsafe states are separated by a configurable distance, enabling generalization to unseen environments.
- **Mechanism:** The autoencoder encoder maps observations to latent vectors. The contrastive loss minimizes distance between embeddings of states sharing the same safety label while enforcing a margin α between states with different labels. A reconstruction loss preserves semantic information. The margin parameter controls the minimum separation, providing a buffer zone in latent space.
- **Core assumption:** Assumes safety labels can be obtained during pre-training (from domain priors), and that the margin-based contrastive objective creates a generalizable safety boundary.
- **Evidence anchors:**
  - [section 4.1] "The encoder fθ takes as input the corresponding observation ot and outputs a latent representation zt, where the Euclidean distance between two latent vectors are small if the corresponding states have the same safety characteristic and large otherwise."
  - [section 5.2] "The embedding network is only trained on LavaCrossingS9N1...the embedding network is able to generalize well enough so that the agent can distinguish between safe and unsafe observations."
  - [corpus] No direct corpus evidence for contrastive safety representations.
- **Break condition:** Fails if the margin α is too small (causing safety class confusion) or too large (preventing generalization), or if observation spaces differ significantly between training and deployment environments.

## Foundational Learning

- **Concept: Contrastive Learning**
  - **Why needed here:** The core mechanism requires learning a latent space where safe and unsafe states are separated by distance. Contrastive learning provides the objective function to achieve this clustering structure.
  - **Quick check question:** Can you explain why a margin parameter α is necessary in the contrastive loss, rather than simply minimizing intra-class distance and maximizing inter-class distance without a margin?

- **Concept: Safe Reinforcement Learning via Shielding**
  - **Why needed here:** Understanding shielding-based approaches provides context for why the baseline method (transferable domain priors) fails—the shield is applied too broadly, blocking exploration.
  - **Quick check question:** What is the difference between modifying the optimization criterion (e.g., CMDP with Lagrange multipliers) and modifying the exploration process (shielding)? Why might shielding be preferred in sparse-reward settings?

- **Concept: Domain Transfer in RL**
  - **Why needed here:** The action safety critic depends on transferring knowledge from related tasks. Understanding how Q-functions aggregate across tasks clarifies both the benefit and the limitation (false positives).
  - **Quick check question:** In the undesirability metric w_i(s,a), why is the Q-value normalized by the maximum Q-value for that state? What does this normalization achieve when aggregating across multiple tasks?

## Architecture Onboarding

- **Component map:**
  Autoencoder (f_θ encoder, g_φ decoder) -> Latent vectors -> Unsafe Embedding Buffer (B) -> Safety Module -> DQN Policy
  Prior Q-function (Q_p) -> Safety Module -> DQN Policy

- **Critical path:**
  1. Pre-training: Train Q_p on 4 source tasks using undesirability-weighted rewards.
  2. Pre-training: Train autoencoder with contrastive loss using safety labels from domain priors.
  3. Deployment: Encode observation → check state safety → (if unsafe) check action safety → execute → update buffer if violation occurred.

- **Design tradeoffs:**
  - **Latent dimension (50):** Higher dimensions capture more nuance but risk overfitting; lower may fail to separate safety classes.
  - **Margin α (10):** Large margin enforces stronger separation but may prevent generalization.
  - **Threshold d_max (2.5):** Critical—too low causes false negatives; too high causes over-conservative behavior.
  - **Interference probability ρ (0.95):** Balances safety enforcement vs exploration freedom when in unsafe states.

- **Failure signatures:**
  - **Zero reward with low violations:** Agent confined to safe region near start. Check d_max (may be too low) or ρ (too high).
  - **High violations with moderate reward:** Latent space separation is poor. Check contrastive loss convergence.
  - **Inconsistent cross-environment performance:** Embedding network fails to generalize. Verify source tasks are representative.

- **First 3 experiments:**
  1. **Ablation on state safety check:** Compare DQN + Domain Priors (no state check) vs full method vs DQN only. Measure return, violations, and visitation heatmaps.
  2. **Latent space visualization:** Train autoencoder on S9N1, plot embeddings for S9N1/S9N2/S9N3 colored by safety. Verify separation and generalization.
  3. **Threshold sensitivity analysis:** Sweep d_max from 1.0 to 4.0 on S9N2. Plot return vs violations curve to identify pareto frontier.

## Open Questions the Paper Calls Out
- Can an adaptive mechanism for the safety latent distance threshold ($d_{max}$) improve stability over the fixed threshold approach?
- How would varying the unsupervised latent learning loss function impact the agent's ability to separate safe and unsafe states?
- Can the method maintain low variance and high efficiency in more complex, higher-dimensional environments than MiniGrid?

## Limitations
- Assumes access to domain priors and safety labels for pre-training, which may not be available in many real-world scenarios
- Effectiveness depends on quality of domain transfer—poor alignment between source and target environments can lead to false positives
- Method's effectiveness depends on careful hyperparameter tuning (margin α, distance threshold d_max) that significantly impacts performance

## Confidence
- **High confidence:** The decoupling mechanism between state and action safety checks is well-justified and the experimental results show clear improvements over baselines.
- **Medium confidence:** The effectiveness of the contrastive learning component for safety generalization across environments is demonstrated but relies on specific hyperparameter choices that may not transfer well.
- **Medium confidence:** The two-phase training approach is sound, but the exact contribution of each component is difficult to isolate due to ablation study limitations.

## Next Checks
1. **Cross-domain generalization test:** Evaluate the method on environments with different visual appearances (e.g., different lava colors or textures) to test the robustness of the contrastive safety representation.
2. **Safety-critical ablation study:** Systematically remove the state safety check, action safety check, and both to quantify their individual contributions to safety violations versus exploration efficiency.
3. **Buffer size sensitivity analysis:** Vary the unsafe embedding buffer size (10, 50, 200) to determine the optimal balance between safety coverage and computational efficiency, measuring both safety performance and policy learning speed.