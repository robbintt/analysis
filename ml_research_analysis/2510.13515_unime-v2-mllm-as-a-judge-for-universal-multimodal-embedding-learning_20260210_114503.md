---
ver: rpa2
title: 'UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning'
arxiv_id: '2510.13515'
source_url: https://arxiv.org/abs/2510.13515
tags:
- unime-v2
- hard
- retrieval
- semantic
- candidates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UniME-V2, a universal multimodal embedding
  model that leverages MLLM-as-a-Judge to enhance representation learning. The approach
  first constructs a hard negative set via global retrieval, then uses MLLMs to assess
  semantic alignment between query-candidate pairs and generate soft matching scores.
---

# UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning

## Quick Facts
- arXiv ID: 2510.13515
- Source URL: https://arxiv.org/abs/2510.13515
- Authors: Tiancheng Gu; Kaicheng Yang; Kaichen Zhang; Xiang An; Ziyong Feng; Yueyi Zhang; Weidong Cai; Jiankang Deng; Lidong Bing
- Reference count: 14
- Primary result: Achieves 68.0 average score on MMEB benchmark, surpassing prior methods

## Executive Summary
UniME-V2 introduces a novel approach to universal multimodal embedding learning that leverages MLLM-as-a-Judge for enhanced representation learning. The method constructs hard negative sets through global retrieval, then uses MLLMs to assess semantic alignment and generate soft matching scores. These scores guide hard negative mining and enable semantic distinction learning among candidates. The approach includes a reranking model trained with joint pairwise and listwise optimization, achieving state-of-the-art performance on the MMEB benchmark with an average score of 68.0.

## Method Summary
UniME-V2 addresses universal multimodal embedding learning by first constructing a hard negative set via global retrieval. MLLMs are then employed to assess semantic alignment between query-candidate pairs and generate soft matching scores. These scores guide hard negative mining, reducing false negatives and enabling semantic distinction learning among candidates. Additionally, a reranking model (UniME-V2-Reranker) is trained using joint pairwise and listwise optimization. The method is evaluated on the MMEB benchmark and multiple retrieval tasks, demonstrating superior performance compared to existing approaches.

## Key Results
- Achieves 68.0 average score on MMEB benchmark
- Outperforms VLM2Vec (60.1) and QQMM (70.7)
- Demonstrates state-of-the-art performance on multiple retrieval tasks

## Why This Works (Mechanism)
The MLLM-as-a-Judge component provides semantically-aware assessment of candidate relevance, which helps distinguish between hard negatives that are semantically similar versus truly irrelevant. This semantic understanding enables more effective hard negative mining and reduces false negatives. The soft matching scores generated by MLLMs capture nuanced semantic relationships that traditional similarity metrics might miss. By incorporating these scores into the learning process, the model can better differentiate between semantically similar but distinct concepts.

## Foundational Learning
- Multimodal embedding learning: Why needed - to represent diverse modalities in a shared space; Quick check - verify cross-modal retrieval performance
- Hard negative mining: Why needed - to improve model discrimination; Quick check - measure recall@K improvements
- MLLM-based semantic assessment: Why needed - to capture nuanced semantic relationships; Quick check - compare against traditional similarity metrics
- Pairwise and listwise optimization: Why needed - to handle both individual and ranked list predictions; Quick check - evaluate ranking metrics like NDCG
- Cross-modal retrieval: Why needed - to validate universal embedding capabilities; Quick check - test retrieval accuracy across modality pairs

## Architecture Onboarding

Component map: Input -> MLLM-as-a-Judge -> Hard Negative Mining -> Embedding Model -> Reranker -> Output

Critical path: Input → MLLM semantic assessment → Hard negative selection → Embedding model training → Reranker optimization → Final output

Design tradeoffs: The approach trades increased computational cost (MLLM inference) for improved semantic understanding and performance. The MLLM-as-a-Judge component adds complexity but provides more nuanced semantic assessment compared to traditional methods.

Failure signatures: Performance degradation if MLLM assessments are noisy or biased; reduced effectiveness if hard negative mining fails to identify truly challenging examples; potential overfitting to MMEB benchmark if not properly regularized.

First experiments to run:
1. Evaluate cross-modal retrieval performance on held-out data
2. Compare MLLM-as-a-Judge against traditional similarity metrics
3. Test model generalization to new multimodal domains

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Lack of detailed ablation studies on MLLM-as-a-Judge component's contribution
- No discussion of computational costs associated with MLLM inference
- Limited analysis of generalization beyond MMEB benchmark
- Potential biases in MLLM judgment not addressed

## Confidence
High: Reported SOTA performance on MMEB benchmark with specific metrics
Medium: Novelty contribution relative to prior hard negative mining approaches
Low: Computational overhead and bias analysis

## Next Checks
1. Conduct ablation studies removing the MLLM-as-a-Judge component to quantify its specific contribution to performance gains
2. Evaluate the model on additional multimodal benchmarks beyond MMEB to assess generalization
3. Measure and report the computational overhead of the MLLM-based semantic assessment during training