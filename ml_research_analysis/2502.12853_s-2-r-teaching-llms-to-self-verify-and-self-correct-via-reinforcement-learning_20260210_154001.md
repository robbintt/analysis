---
ver: rpa2
title: 'S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning'
arxiv_id: '2502.12853'
source_url: https://arxiv.org/abs/2502.12853
tags:
- arxiv
- answer
- reasoning
- training
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: S2R is an efficient framework that teaches large language models
  to self-verify and self-correct during inference, enabling them to adaptively refine
  their reasoning process. It first initializes models with these behaviors via supervised
  fine-tuning on 3.1k carefully curated samples, then strengthens them through outcome-level
  and process-level reinforcement learning.
---

# S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning

## Quick Facts
- arXiv ID: 2502.12853
- Source URL: https://arxiv.org/abs/2502.12853
- Authors: Ruotian Ma; Peisong Wang; Cheng Liu; Xingyan Liu; Jiaqi Chen; Bang Zhang; Xin Zhou; Nan Du; Jia Li
- Reference count: 40
- Primary result: Qwen2.5-math-7B accuracy improves from 51.0% to 81.6% on Math500 using only 3.1k samples and 1 GPU-day

## Executive Summary
S$^2$R introduces a two-stage framework that teaches large language models to self-verify and self-correct during inference through a combination of masked supervised fine-tuning and reinforcement learning. The approach first initializes verification and correction behaviors via carefully constructed trajectories, then strengthens them through outcome-level or process-level RL. This method achieves significant accuracy improvements on mathematical reasoning tasks while using minimal resources compared to traditional long-chain-of-thought distillation approaches.

## Method Summary
S$^2$R operates in two stages: (1) masked supervised fine-tuning on 3.1k self-verifying and self-correcting trajectories where only the final correct solution and all verification actions are optimized, and (2) reinforcement learning that either optimizes entire trajectories based on final correctness (outcome-level) or provides step-wise guidance for each verification/correction action (process-level). The framework constructs diverse trajectory lengths based on problem difficulty, uses confirmative verification to avoid solving bias, and employs group-based baselines for process-level RL. This combination enables models to learn adaptive trial-and-error reasoning patterns while maintaining computational efficiency.

## Key Results
- Qwen2.5-math-7B accuracy improves from 51.0% to 81.6% on Math500
- Outperforms models trained on equivalent long-chain-of-thought data
- Achieves results using only 3.1k samples and 1 GPU-day of training
- Outcome-level RL generally outperforms process-level RL across tested models

## Why This Works (Mechanism)

### Mechanism 1: Behavior Initialization via Masked Supervised Fine-Tuning
The framework constructs dynamic trajectories of varying lengths where failed attempts are sampled from the model's own responses, verification uses confirmative checking, and only the final correct solution is optimized. A masking strategy trains all verification actions and only the terminal correct solution—intermediate incorrect solves are masked out. This allows models to learn the pattern of iterative refinement through imitation, even if they cannot initially perform valid self-correction from SFT alone.

### Mechanism 2: Outcome-Level Reinforcement Learning (RLOO)
Outcome-level RL allows models to explore more flexible trial-and-error paths by optimizing entire trajectories based on final answer correctness. The algorithm samples multiple trajectories per question, computes leave-one-out mean reward as a baseline, and calculates advantages with KL regularization to the SFT reference policy. The entire trajectory receives a binary reward based on whether the final solution is correct, incentivizing discovery of which intermediate patterns lead to success without constraining the path.

### Mechanism 3: Process-Level RL with Group-Based Baselines
Process-level supervision provides granular guidance for intermediate verification steps by giving each action (solve or verify) an individual reward. The baseline is computed by averaging rewards of actions sharing the same reward context (sequence of prior rewards), grouping actions with comparable information states. This approach may be particularly effective for models with limited reasoning abilities by constraining intermediate steps while still allowing some flexibility.

## Foundational Learning

- **Concept: Self-Verification vs Self-Correction**
  - Why needed: The paper treats these as distinct skills. Verification is initialized via SFT with confirmative checking; correction validity is primarily strengthened through RL. Understanding this separation clarifies why two-stage training is necessary.
  - Quick check: Why does the masking strategy optimize all verify actions but only the final solve action?

- **Concept: Outcome-Level vs Process-Level RL Trade-offs**
  - Why needed: The choice shapes emergent behavior. Outcome-level enables flexible exploration suited to stronger models; process-level constrains intermediate steps, potentially helping weaker models but limiting creativity.
  - Quick check: Why might process-level RL show larger gains on verification accuracy while outcome-level achieves higher final answer accuracy?

- **Concept: Test-Time Scaling via Adaptive Trial Allocation**
  - Why needed: The core goal is enabling models to dynamically allocate compute based on problem difficulty. This is learned by constructing training trajectories with lengths correlated to estimated difficulty.
  - Quick check: How does the paper determine trajectory length during data construction, and why does this matter for inference behavior?

## Architecture Onboarding

- **Component map:**
  Base Model → Sample K responses/problem → Difficulty estimation → Construct solve-verify trajectories → Filter/refine verifications → Masked SFT (train: all verifies + final solve) → SFT Model → Sample trajectories → Reward computation (outcome or process) → Baseline estimation (LOO or group-based) → Advantage + KL penalty → Policy update

- **Critical path:**
  1. Verification quality: Confirmative verification (not re-solving) is harder to construct but less biased. Query GPT-4 with explicit "verify without re-solving" instructions, then filter with LLM-as-judge.
  2. Masking correctness: Verify mask indices match the specification—training on intermediate incorrect solves will teach wrong answers.
  3. RL stability: KL coefficient (β) and baseline estimation quality are the primary levers. Paper uses β ∈ {0.01, 0.05, 0.1}.

- **Design tradeoffs:**
  - Confirmative vs Problem-Solving Verification: Confirmative is more balanced (less bias toward "correct") but requires more filtering effort.
  - Online vs Offline RL: Offline is more resource-efficient; the paper finds process-level excels offline (better baseline estimation from fixed samples), outcome-level excels online (exploration benefits from on-policy sampling).
  - Trajectory length diversity: Longer trajectories enable more correction opportunities but increase training instability and inference latency.

- **Failure signatures:**
  - Reward hacking: Model outputs "correct" regardless of actual validity. Fix: Use regex-based parsing for verification judgments, not model-generated text.
  - Infinite loops: Model cycles without terminating. Fix: Cap max actions per trajectory (paper uses 20).
  - Accuracy collapse: All trajectories converge to uniform length. Fix: Enforce diversity in training data construction.

- **First 3 experiments:**
  1. Replicate Stage 1 SFT on a held-out 500-problem subset. Sample 5 responses/problem, construct confirmative verification trajectories. Measure: trajectory completion rate (>80% target), verification accuracy. Purpose: Validate data construction pipeline before RL.
  2. Ablate verification type (confirmative vs problem-solving). Train two SFT models, evaluate on verification accuracy and prediction balance. Expected: confirmative shows lower overall accuracy but more balanced predictions. Purpose: Confirm bias reduction hypothesis.
  3. Compare online RLOO vs offline RL with accuracy-grouped baselines. Train both on identical 10k problem set, measure MATH500 accuracy and GPU hours. Expected: offline ~2x more efficient; outcome-level wins online, process-level wins offline. Purpose: Identify which RL variant suits your infrastructure constraints.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does offline RL favor process-level supervision while online RL favors outcome-level supervision in the S2R framework?
- Basis in paper: [explicit] Section 3.5 notes the "interesting phenomenon" that process-level supervision outperforms outcome-level in offline settings, whereas the opposite is true for online RL.
- Why unresolved: The authors hypothesize this is due to differences in baseline estimation accuracy and exploration flexibility, but do not isolate the specific causal mechanism behind this inversion.
- What evidence would resolve it: Ablation studies comparing baseline estimation variance and trajectory diversity between online and offline sampling methods within the same model architecture.

### Open Question 2
- Question: Can the "Confirmative Verification" behavior be initialized without relying on data distilled from significantly stronger models?
- Basis in paper: [inferred] Appendix A.1 and Table 6 reveal that existing models (including GPT-4o) struggle to generate "Confirmative Verification" (verifying without re-solving), forcing the authors to rely on specific proprietary models for data curation.
- Why unresolved: The current methodology depends on a stronger teacher to bootstrap the verification behavior, creating a potential ceiling for self-improvement loops in smaller models.
- What evidence would resolve it: A study showing successful S2R training where the initial verification data is generated solely by the base model or a model of equivalent size, perhaps using iterative self-distillation.

### Open Question 3
- Question: How does the framework perform on tasks where binary "correctness" cannot be determined by rule-based parsers (e.g., open-ended generation)?
- Basis in paper: [inferred] Section 2.1 defines the reward function $V_{golden}$ and the `Parser` based on matching golden answers, which is feasible for math but limits the problem scope to verifiable domains.
- Why unresolved: The authors claim generalizability to tasks like StrategyQA, but the training pipeline (Stage 1 and 2) is heavily optimized for problems with discrete, verifiable solutions.
- What evidence would resolve it: Experiments applying S2R to tasks with semantic or model-based rewards (e.g., summarization or creative writing) rather than strict regex-equivalence rewards.

## Limitations

- Efficiency vs. Robustness Trade-offs: The paper emphasizes minimal resource usage (3.1k samples, 1 GPU-day), but does not extensively characterize robustness to data quality variations or base model quality thresholds.
- Verification Quality Concerns: While using LLM-as-judge filtering for verification quality, the specific criteria and potential biases in this filtering process are not fully transparent.
- Generalization Beyond Math: The framework is demonstrated exclusively on mathematical reasoning tasks, with untested extent of transfer to other domains.

## Confidence

**High Confidence:** The two-stage SFT+RL approach outperforms both pure SFT and pure RL baselines on MATH500. The efficiency claims (3.1k samples, 1 GPU-day) are supported by the reported experimental setup.

**Medium Confidence:** The claim that S2R is "more efficient than equivalent long-CoT distillation" is supported by comparison to specific baselines, but lacks direct comparison to other efficient reasoning methods. The superiority of outcome-level RL for stronger models is demonstrated but may depend on specific hyperparameters.

**Low Confidence:** Claims about the general applicability of the framework to non-mathematical domains are entirely theoretical. The assertion that process-level RL is particularly effective for "models with limited reasoning abilities" is based on single data points without systematic ablation across model sizes.

## Next Checks

1. **Cross-Domain Generalization Test:** Apply S2R to a non-mathematical reasoning dataset (e.g., coding problems or scientific reasoning tasks) using the same 3.1k sample budget. Measure whether the 30%+ accuracy gains transfer, and if trajectory construction remains feasible with different verification requirements.

2. **Base Model Quality Threshold Validation:** Systematically test trajectory construction and SFT+RL effectiveness across base models with accuracy ranging from 1% to 50% on the target domain. Identify the minimum accuracy threshold where the framework becomes viable, and characterize how training dynamics change across this spectrum.

3. **Reward Function Ablation Study:** Replace the binary ±1 outcome reward with continuous rewards (e.g., confidence-weighted correctness, step-wise progress metrics) while maintaining the same sample budget. Measure impact on both final accuracy and the quality/distribution of learned verification strategies.