---
ver: rpa2
title: 'SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social Engineering
  Behaviors'
arxiv_id: '2505.24458'
source_url: https://arxiv.org/abs/2505.24458
tags:
- social
- sear
- multimodal
- conversation
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The SEAR Dataset addresses the lack of multimodal resources for
  studying AR-LLM-driven social engineering attacks by providing synchronized AR-captured
  visual/audio cues, environmental context, and curated social profiles across 180
  conversations with 60 participants. It uses AR glasses with MediaPipe for facial
  landmarks, YOLO11m for object detection, and a multimodal LLM (Gemma 3-12B) to generate
  personalized social profiles and adaptive dialogue via a ReAct-based social agent.
---

# SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social Engineering Behaviors

## Quick Facts
- arXiv ID: 2505.24458
- Source URL: https://arxiv.org/abs/2505.24458
- Authors: Tianlong Yu; Chenghang Ye; Zheyu Yang; Ziyi Zhou; Cui Tang; Zui Tao; Jun Zhang; Kailong Wang; Liting Zhou; Yang Yang; Ting Bi
- Reference count: 14
- One-line primary result: 93.3% phishing compliance and 76.7% trust surge in AR-LLM social engineering experiments

## Executive Summary
The SEAR Dataset addresses the lack of multimodal resources for studying AR-LLM-driven social engineering attacks by providing synchronized AR-captured visual/audio cues, environmental context, and curated social profiles across 180 conversations with 60 participants. It uses AR glasses with MediaPipe for facial landmarks, YOLO11m for object detection, and a multimodal LLM (Gemma 3-12B) to generate personalized social profiles and adaptive dialogue via a ReAct-based social agent. Key findings show high efficacy in eliciting compliance: 93.3% phishing link clicks, 85% call acceptance, and a 76.7% post-interaction trust surge. The dataset enables research into detecting AR-driven SE attacks, designing defensive frameworks, and understanding multimodal adversarial manipulation, with rigorous ethical safeguards including IRB approval and anonymization.

## Method Summary
The study used RayNeo X2 AR glasses (6GB RAM, 128GB storage) to capture synchronized video and audio during 180 conversations with 60 participants across three conditions: basic conversation, AR+LLM, and SEAR. A server with RTX 4090 and Intel Platinum 8352 processed multimodal data using MediaPipe Holistic for 543 facial landmarks, YOLO11m for object detection, Vosk for speech-to-text, and HandGestureClassifier for posture analysis. Gemma 3-12B multimodal LLM generated personalized social profiles and adaptive dialogue through a ReAct-based social agent. Post-interaction surveys measured phishing compliance (93.3%), call acceptance (85%), trust surge (76.7%), and subjective experience across 11 dimensions.

## Key Results
- 93.3% phishing link click compliance rate
- 85% call acceptance rate
- 76.7% post-interaction trust surge
- 180 total conversations across 3 experimental conditions

## Why This Works (Mechanism)
The system's effectiveness stems from real-time multimodal fusion of visual (facial landmarks, object detection), auditory (speech transcription), and contextual (environmental) signals processed by a ReAct-based social agent. The Gemma 3-12B LLM generates personalized social profiles and adaptive dialogue based on participant responses, creating natural conversation flow that builds trust through personalized engagement. The AR interface provides continuous contextual awareness, enabling the agent to adjust tactics dynamically based on participant behavior and emotional cues.

## Foundational Learning
- **Multimodal Data Synchronization**: Why needed - Aligns visual, audio, and contextual streams for coherent analysis. Quick check - Verify timestamp alignment across all sensor streams.
- **ReAct Reasoning Framework**: Why needed - Enables step-by-step planning and reflection for natural conversation flow. Quick check - Validate reasoning chain coherence in agent responses.
- **Facial Landmark Analysis**: Why needed - Captures micro-expressions and engagement levels for adaptive responses. Quick check - Confirm landmark detection accuracy across different lighting conditions.
- **Social Profile Personalization**: Why needed - Creates believable personas that increase trust and compliance. Quick check - Assess profile relevance through participant feedback.
- **Trust Metric Measurement**: Why needed - Quantifies manipulation effectiveness for defensive research. Quick check - Correlate subjective trust ratings with objective compliance metrics.

## Architecture Onboarding

**Component Map**: AR Glasses -> MediaPipe/YOLO11m -> Server -> Gemma 3-12B -> ReAct Agent -> Participant Response

**Critical Path**: AR capture → facial landmark/object detection → server processing → LLM inference → dialogue generation → participant interaction

**Design Tradeoffs**: Real-time processing vs. model accuracy (RTX 4090 chosen for balance), comprehensive data collection vs. privacy (anonymization implemented), conversation naturalness vs. experimental control (structured conversation flow maintained)

**Failure Signatures**: High latency causing unnatural pauses, inconsistent facial landmark detection in poor lighting, irrelevant social profile generation reducing trust, speech transcription errors breaking conversation flow

**First 3 Experiments**:
1. Validate end-to-end latency from AR capture to LLM response generation
2. Test social profile personalization accuracy across different participant demographics
3. Measure trust metric consistency across multiple conversation iterations

## Open Questions the Paper Calls Out
- **Defensive Framework Efficacy**: What specific defensive frameworks can effectively mitigate the "trust hijacking" and high compliance rates observed in AR-LLM-driven interactions? The study proved attack viability but did not test countermeasures.
- **Real-time Detection Systems**: Can real-time detection systems accurately distinguish between benign AR-assisted conversation and adversarial social engineering manipulation? The dataset provides necessary labels but no detection architecture is proposed.
- **Generalization to Real-world Environments**: How does the efficacy of AR-LLM-driven social engineering generalize to broader demographics and real-world environments outside of controlled academic settings? The study involved only 60 participants in simulated scenarios.

## Limitations
- Limited demographic diversity with specific age distribution (peaks at ages 25 and 32)
- Controlled experimental environment may not reflect real-world attack scenarios
- ReAct agent architecture and specific implementation details remain underspecified

## Confidence
- High: Dataset collection methodology, hardware/software configuration, and survey design
- Medium: Efficacy metrics and attack success rates
- Low: Real-world applicability and scalability without further validation

## Next Checks
1. Conduct a pilot study with varied participant demographics to assess demographic bias in compliance rates and trust measurement consistency
2. Perform latency profiling across the full AR-to-LLM pipeline to identify bottlenecks and quantify impact on conversation naturalness
3. Implement cross-validation with independent AR-LLM configurations (e.g., different hardware, models) to test robustness of reported attack success rates