---
ver: rpa2
title: Application of Generative Adversarial Network (GAN) for Synthetic Training
  Data Creation to improve performance of ANN Classifier for extracting Built-Up pixels
  from Landsat Satellite Imagery
arxiv_id: '2501.19283'
source_url: https://arxiv.org/abs/2501.19283
tags:
- pixels
- built-up
- original
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of insufficient high-quality
  training data for pixel-based land use classification using low-resolution Landsat
  imagery. To overcome this limitation, the authors propose a generative adversarial
  network (GAN) to create synthetic built-up pixels that match the distribution of
  original training data.
---

# Application of Generative Adversarial Network (GAN) for Synthetic Training Data Creation to improve performance of ANN Classifier for extracting Built-Up pixels from Landsat Satellite Imagery

## Quick Facts
- arXiv ID: 2501.19283
- Source URL: https://arxiv.org/abs/2501.19283
- Reference count: 0
- Primary result: GAN-generated synthetic built-up pixels significantly improved ANN classifier accuracy from 0.9331 to 0.9983 for Landsat land use classification

## Executive Summary
This study addresses the challenge of insufficient high-quality training data for pixel-based land use classification using low-resolution Landsat imagery. To overcome this limitation, the authors propose a generative adversarial network (GAN) to create synthetic built-up pixels that match the distribution of original training data. The GAN architecture, consisting of a generator and discriminator with 4-layer neural networks, produces 300 synthetic built-up pixels from an original set of 100. The validity of the generated pixels is confirmed through Kolmogorov-Smirnov tests for marginal distributions and Ball Divergence tests for joint distributions across all spectral bands. These synthetic pixels are then incrementally added to the original training set to train an artificial neural network (ANN) classifier. The results demonstrate a significant improvement in classification performance, with overall accuracy increasing from 0.9331 to 0.9983 and Cohen's kappa coefficient rising from 0.8277 to 0.9958.

## Method Summary
The approach uses a simple 4-layer GAN architecture with uniform noise input (U[-1,1]) to generate synthetic 6-band spectral vectors matching Landsat7 built-up pixels. The generator and discriminator each have 4 layers with sigmoid and ReLU activations. After GAN training on 100 original built-up pixels, 300 synthetic pixels are generated and validated using non-parametric statistical tests (KS and Ball Divergence). These synthetic samples are incrementally added to the original training set (100→400 built-up pixels) while training an ANN classifier with L2 regularization tuned via 10-fold cross-validation. The classifier distinguishes built-up from non-built-up areas using 1 hidden layer with 2 units.

## Key Results
- Overall accuracy improved from 0.9331 to 0.9983 with synthetic data inclusion
- Cohen's kappa coefficient increased from 0.8277 to 0.9958
- All Kolmogorov-Smirnov test p-values > 0.05 (0.21-0.91) confirming marginal distribution matching
- Ball Divergence test p-values 0.23-0.54 confirming joint distribution equality
- Weight decay parameter reduced from 0.4 to 0.1 as training data expanded

## Why This Works (Mechanism)

### Mechanism 1
GAN-generated synthetic pixels statistically match the distribution of original training samples, enabling effective data augmentation. The generator network learns to map random noise vectors (z ~ U[-1,1]) to 6-dimensional spectral feature vectors that satisfy the discriminator's realism criterion. Through adversarial training, the generator approximates p_g ≈ p_data for the built-up pixel distribution across all Landsat7 bands (B1-B6).

### Mechanism 2
Incremental addition of GAN-synthetic samples to minority class training data improves classifier discrimination boundaries. The ANN classifier, initially trained on imbalanced data (100 built-up vs 400 non-built-up), benefits from expanded built-up representation (100→400). This provides denser sampling of the minority class decision boundary, reducing overfitting to sparse original examples.

### Mechanism 3
Regularization (weight decay λ) adapts to training set size, preventing overfitting as synthetic data expands the training set. Grid search with 10-fold cross-validation adjusts λ downward (0.4 → 0.1) as training data increases (100 → 400 built-up pixels). Larger datasets require less L2 regularization since the variance reduction from more samples partially substitutes for regularization's bias-variance tradeoff.

## Foundational Learning

- **GAN Minimax Objective**
  - Why needed here: The paper's GAN trains via adversarial game where G minimizes log(1-D(G(z))) while D maximizes correct classification—understanding this equilibrium is essential for debugging training dynamics.
  - Quick check question: What happens to generator loss when the discriminator becomes too strong too quickly?

- **Non-parametric Distribution Testing (KS Test, Ball Divergence)**
  - Why needed here: The paper validates synthetic quality via KS test (marginal distributions per band) and Ball Divergence (multivariate joint distribution)—these are your quality gates before using synthetic data.
  - Quick check question: Why use non-parametric tests rather than assuming Gaussian distributions for spectral band data?

- **L2 Regularization (Weight Decay)**
  - Why needed here: The ANN classifier uses weight decay with λ tuned via grid search—understanding regularization strength vs. data quantity tradeoff is critical for reproduction.
  - Quick check question: As training data doubles, would you expect optimal λ to increase, decrease, or stay the same? Why?

## Architecture Onboarding

- **Component map**:
  - Generator (G): 4-layer MLP [100→100→100→6]; activations: Sigmoid → ReLU → ReLU → (linear output); input: z ~ U[-1,1] dim=100; output: 6-band spectral vector
  - Discriminator (D): 4-layer MLP [6→100→100→1]; activations: Sigmoid → ReLU → ReLU → Sigmoid; input: 6-band real or synthetic pixel; output: probability real
  - ANN Classifier: 1 hidden layer [2 units]; output: binary (built-up vs non-built-up); regularization: L2 weight decay λ∈[0.1,0.4]

- **Critical path**:
  1. Collect pure built-up pixels (n=100) with verified labels
  2. Train GAN to convergence on these 100 samples
  3. Generate 3 sets × 100 synthetic pixels
  4. Run KS test (per band) and Ball Divergence (joint) on each set—only proceed if p > 0.05
  5. Incrementally train ANN: original → +100 synthetic → +200 → +300
  6. Grid search λ with 10-fold CV at each increment
  7. Evaluate on held-out test set (2000 built-up, 5000 non-built-up)

- **Design tradeoffs**:
  - Simple 4-layer GAN chosen over VAE/Diffusion to avoid overkill on low-dimensional (6D) data—larger models risk overfitting to 100 samples
  - Uniform noise prior U[-1,1] vs. Gaussian: paper doesn't justify this choice; may affect sample diversity
  - Single hidden layer (2 units) in ANN limits capacity but works given Universal Approximation Theorem; may not scale to multi-class

- **Failure signatures**:
  - **KS/Ball tests fail (p < 0.05)**: GAN not converging or mode collapse—increase training iterations or reduce learning rate
  - **Accuracy plateaus or degrades**: Synthetic samples introducing distribution shift—inspect band-wise histograms
  - **Discriminator dominates (D(x)→1, D(G(z))→0)**: Generator cannot learn—reduce D learning rate or use label smoothing
  - **High variance across cross-validation folds**: Training set still too small or synthetic samples inconsistent

- **First 3 experiments**:
  1. **Reproduction baseline**: Train GAN on paper's 100 built-up pixels, generate 100 synthetic, verify KS p-values > 0.3 and Ball p-value > 0.2 (match Table 1-2 ranges)
  2. **Ablation on synthetic quantity**: Test accuracy with +50, +100, +150, +200 synthetic pixels (not just 100-increment steps) to find saturation point
  3. **Noise prior sensitivity**: Replace U[-1,1] with N(0,1) and compare KS/Ball test results—determine if uniform is critical or arbitrary

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do other generative architectures (VAEs, Diffusion Models) compare against GANs for synthetic training pixel generation in terms of both distributional fidelity and downstream classification gains?
- Basis in paper: [explicit] "any other generative models (like VAE, Diffusion models etc.) could also be developed for the same purpose"
- Why unresolved: Only a simple 4-layer GAN was implemented; no comparative study with alternative generative models was conducted.
- What evidence would resolve it: Side-by-side experiments using VAEs and Diffusion Models on the same training set, reporting KS/Ball Divergence tests and ANN accuracy/kappa improvements.

### Open Question 2
- Question: What is the optimal ratio of synthetic to original training samples before performance saturates or degrades?
- Basis in paper: [inferred] Only 300 synthetic pixels (3x original) were tested; the trend suggests possible saturation but the upper bound remains unknown.
- Why unresolved: The paper incrementally added synthetic data but stopped at 300 generated pixels, leaving the saturation or degradation point unidentified.
- What evidence would resolve it: Systematic experiments varying synthetic-to-original ratios (e.g., 1x to 10x+) and plotting accuracy/kappa curves to identify inflection points.

### Open Question 3
- Question: What is the minimum size of high-quality original training samples required for the GAN to learn a valid distribution without mode collapse?
- Basis in paper: [inferred] The study used 100 original built-up pixels successfully, but did not investigate lower bounds or failure modes.
- Why unresolved: No ablation study on original training set size was conducted; it is unclear whether fewer samples would still yield valid synthetic distributions.
- What evidence would resolve it: Experiments reducing original sample sizes (e.g., 10, 25, 50, 75) and assessing GAN convergence, distributional tests, and downstream ANN performance.

### Open Question 4
- Question: How well does this approach generalize to other satellite sensors (e.g., Sentinel-2, high-resolution imagery) and to different geographic regions with varying urban patterns?
- Basis in paper: [explicit] "multi-spectral Landsat7 data has been used but the same idea could be applied to other types of satellite images as well"
- Why unresolved: Only Landsat7 imagery from Jaipur, India was tested; no cross-sensor or cross-region validation was performed.
- What evidence would resolve it: Replication of the methodology on different sensors and geographic regions, reporting distributional tests and classification performance metrics.

## Limitations
- The entire GAN training relies on only 100 labeled built-up pixels, raising concerns about data representativeness
- KS and Ball Divergence tests represent minimal statistical significance thresholds rather than practical similarity
- The ANN uses only 2 hidden units, which may be insufficient for more complex classification tasks

## Confidence
**High Confidence**: The GAN architecture can generate spectral vectors that pass basic distribution tests against original samples. The incremental accuracy improvements with added synthetic data are reproducible.

**Medium Confidence**: The practical significance of these accuracy gains for operational land cover mapping. The 0.9983 accuracy represents performance on a specific test set with similar characteristics to training data.

**Low Confidence**: The GAN's ability to generate diverse, high-quality synthetic samples beyond the specific 100-pixel training set. The long-term stability of the GAN training process with such limited data.

## Next Checks
1. **Distribution Mismatch Quantification**: Calculate Wasserstein distance or other metrics that measure practical similarity between original and synthetic distributions, not just statistical test pass/fail.

2. **Cross-Scene Generalization**: Apply the trained GAN and ANN to a completely different Landsat scene (different city, different acquisition date) to test whether the synthetic samples maintain their utility.

3. **Sample Size Sensitivity Analysis**: Systematically vary the original training set size (25, 50, 100, 200 pixels) to determine the minimum viable training set for reliable GAN generation and classifier performance.