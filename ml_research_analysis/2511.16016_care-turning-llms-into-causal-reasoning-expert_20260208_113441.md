---
ver: rpa2
title: 'CARE: Turning LLMs Into Causal Reasoning Expert'
arxiv_id: '2511.16016'
source_url: https://arxiv.org/abs/2511.16016
tags:
- causal
- llms
- discovery
- data
- outputs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CARE, a supervised fine-tuning framework to
  improve LLMs' causal reasoning. Unlike standard prompting, which often fails to
  elicit data-driven causal discovery due to LLMs relying on variable semantics rather
  than actual data patterns, CARE trains models to integrate world knowledge with
  outputs from established causal discovery algorithms.
---

# CARE: Turning LLMs Into Causal Reasoning Expert

## Quick Facts
- arXiv ID: 2511.16016
- Source URL: https://arxiv.org/abs/2511.16016
- Reference count: 40
- Primary result: CARE finetuned Qwen2.5-1.5B outperforms traditional causal discovery algorithms and much larger LLMs on causal reasoning benchmarks

## Executive Summary
CARE introduces a supervised fine-tuning framework that transforms LLMs into causal reasoning experts by teaching them to integrate world knowledge with outputs from established causal discovery algorithms. Unlike standard prompting which often degrades performance when algorithm outputs are provided, CARE trains models through data augmentation to suppress semantic shortcuts and leverage algorithmic evidence. Experiments demonstrate that a CARE-finetuned Qwen2.5-1.5B model achieves state-of-the-art performance, particularly under challenging conditions like permuted variable names, outperforming both traditional algorithms and much larger LLMs.

## Method Summary
CARE uses supervised fine-tuning on a small language model (Qwen2.5-1.5B) with data augmentation techniques including variable name permutation, column reordering, and variable omission. The framework trains models to integrate world knowledge with sufficient statistics from causal discovery algorithms (PC, GES, ICA-LiNGAM, DirectLiNGAM, FCI, GRaSP, BOSS). Training employs LoRA-based parameter-efficient fine-tuning on observational data sampled from benchmark networks, with evaluation using an LLM-as-judge approach that parses predicted edges and computes F1 scores against ground truth DAGs.

## Key Results
- CARE-finetuned Qwen2.5-1.5B outperforms both traditional causal discovery algorithms and much larger LLMs
- Achieves state-of-the-art performance especially under challenging conditions like permuted variable names
- Demonstrates 70%+ gains on permuted scenarios compared to baseline models
- Shows effective knowledge integration beyond semantic cues through targeted fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing causal discovery algorithm outputs as prompts degrades LLM performance; supervised fine-tuning is required for effective integration.
- Mechanism: Pretrained LLMs treat algorithm outputs as conflicting signals against strong semantic priors. SFT reweights this integration by training on ground-truth-aligned examples where algorithm outputs serve as "sufficient statistics" rather than distractions.
- Core assumption: The conflict arises from misaligned attention patterns, not irreconcilable information incompatibility.
- Evidence anchors:
  - [abstract] "prompting the LLMs with these sufficient statistics decreases the LLMs' performance in causal discovery"
  - [Section 4, Table 1] With original names, algorithm outputs degrade GPT-4o-mini from 0.688 to 0.497 (N=0); similar pattern across models
  - [corpus] Neighbor paper "LLM Cannot Discover Causality" corroborates LLMs' inherent limitations in causal reasoning without targeted intervention
- Break condition: If algorithm outputs consistently contained information orthogonal to ground truth, SFT would fail to learn productive integration.

### Mechanism 2
- Claim: Data augmentation targeting semantic biases forces genuine statistical reasoning.
- Mechanism: Variable name permutation (A1) creates scenarios where semantic priors actively contradict data patterns; column reordering (A2) removes positional cues; variable omission (A3) tests marginalization. Training on these augmentations prevents shortcut learning.
- Core assumption: LLMs can learn to suppress semantic shortcuts when trained on sufficient counterexamples.
- Evidence anchors:
  - [Section 5.2, Figure 3] "Semantic Variable Name Permutation: Original semantic variable names are shuffled; data matrix remain unchanged"
  - [Section 6.3, Table 2] CARE achieves 0.460 on ASIA-Permuted vs. baseline Qwen2.5-1.5B at 0.300; 0.792 vs. 0.268 on Survey-Permuted
  - [corpus] "Learning to Defer for Causal Discovery with Imperfect Experts" addresses expert knowledge unreliability but does not propose augmentation-based correction
- Break condition: If LLM's semantic knowledge is so deeply embedded that no counterexample set can override it, augmentation would fail. Current evidence (70%+ gains on permuted scenarios) suggests this is partially overcome.

### Mechanism 3
- Claim: Bayesian posterior updating perspective explains knowledge-algorithm synergy.
- Mechanism: Pretrained LLM encodes prior over causal structures from text corpora. SFT with algorithm outputs and ground truth computes posterior, teaching model when to trust each source.
- Core assumption: The pretraining corpus contains sufficient causal knowledge to serve as informative prior; algorithm outputs provide independent signal.
- Evidence anchors:
  - [Section 5.2] "We view the pretrained LLM's background knowledge as a prior over causal structures... During SFT, this prior is updated into a posterior"
  - [Section 1, Figure 1] Framework diagram showing "synergistically integrate" of world knowledge with CD algorithm outputs
  - [corpus] "Dynamic Expert-Guided Model Averaging" (FMR 0.566) explores expert integration but via averaging, not SFT-based posterior learning
- Break condition: If algorithm outputs and LLM priors are systematically anti-correlated, Bayesian updating would amplify errors.

## Foundational Learning

- Concept: **Causal Discovery vs. Causal Reasoning**
  - Why needed here: The paper distinguishes discovering causal structure from data (CD) from reasoning about known relationships. LLMs fail at discovery because they substitute semantic recall for statistical analysis.
  - Quick check question: Given a dataset where columns [A, B, C] have pairwise correlations but A→B→C is the true structure, can you explain why PC algorithm might return only the skeleton (undirected edges)?

- Concept: **Sufficient Statistics**
  - Why needed here: Algorithm outputs (conditional independencies, candidate graphs) are framed as "sufficient statistics"—compact summaries of data evidence. Understanding this explains why they replace raw data in prompts.
  - Quick check question: If a LiNGAM algorithm returns edge X→Y with confidence 0.7, what information from the raw dataset does this summarize, and what information might it discard?

- Concept: **Shortcut Learning and Distribution Shift**
  - Why needed here: LLMs exploit semantic variable names as shortcuts. Augmentation creates distribution shift between training and naive deployment, forcing genuine pattern learning.
  - Quick check question: If you train a classifier on MNIST digits with digit labels in corner text, then test with corner text removed, what failure mode would you expect? How does this relate to variable name permutation?

## Architecture Onboarding

- Component map:
  - Data Augmentation Module -> Causal Discovery Algorithm Suite -> Prompt Constructor -> SFT Trainer -> LLM-as-Judge Evaluator

- Critical path:
  1. Sample observational data from benchmark DAGs (ASIA, SURVEY, EARTHQUAKE, ALARM)
  2. Apply augmentations → generate diverse training scenarios
  3. Run CD algorithms → collect outputs
  4. Construct prompt-response pairs (algorithm outputs + variable names → ground-truth DAG)
  5. SFT with LoRA on Qwen2.5-1.5B
  6. Evaluate on new data samples from same DAGs with held-out augmentations

- Design tradeoffs:
  - Small base model (1.5B) vs. larger models: Paper cites computational constraints; Section D acknowledges larger models may yield further gains
  - LoRA vs. full fine-tuning: PEFT chosen to mitigate overfitting and catastrophic forgetting; may limit capacity to learn complex integration patterns
  - LLM-as-Judge vs. exact matching: Handles output format variations but introduces judge model's parsing errors as confound

- Failure signatures:
  - Semantic leakage: Model performs well on Original/Permuted but poorly on Random names → still relying on partial semantic cues
  - Algorithm over-reliance: Model F1 ≈ algorithm baseline F1 without improvement → SFT failed to teach knowledge integration
  - Format drift: Judge extracts spurious edges from reasoning text → answer section parsing failure

- First 3 experiments:
  1. Reproduce Table 1 ablation: Run baseline LLMs (Qwen2.5-1.5B, GPT-4o-mini) on ASIA with/without algorithm outputs across Original/Random/Permuted conditions; confirm prompting degradation pattern
  2. Ablate single augmentations: Train CARE variants removing one augmentation type at a time; measure F1 delta on held-out test to isolate contribution of each perturbation
  3. Transfer test: Train CARE on ASIA+SURVEY, evaluate zero-shot on EARTHQUAKE; assess whether learned integration generalizes to unseen graph structures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the CARE framework yield further performance gains when applied to significantly larger base language models (e.g., 7B or 70B parameters)?
- Basis in paper: [explicit] The authors note in the Limitations section that applying the SFT methodology to larger base models "is anticipated to yield further substantial gains" and is a "promising direction for future work."
- Why unresolved: The study was computationally constrained to the Qwen2.5-1.5B model.
- What evidence would resolve it: Empirical evaluation of CARE-finetuned versions of larger models (e.g., Llama-3-8B or Qwen-72B) on the same benchmark datasets.

### Open Question 2
- Question: Can CARE maintain robust causal discovery performance on graphs with significantly more variables (hundreds or thousands of nodes)?
- Basis in paper: [explicit] The Limitations section states that investigating CARE’s performance on "extremely large numbers of variables (hundreds or thousands)... remains an important area."
- Why unresolved: The current evaluation was limited to smaller networks like ALARM (37 nodes), and context windows may limit larger inputs.
- What evidence would resolve it: Experiments applying the framework to large-scale causal discovery benchmarks or high-dimensional real-world datasets.

### Open Question 3
- Question: How effectively does CARE generalize to specialized domains outside the training distribution without requiring extensive data curation?
- Basis in paper: [inferred] While Appendix D mentions resource considerations for "new, specialized domains," the paper relies on data augmentation of specific benchmark networks (bnlearn). It is unclear if the learned reasoning generalizes to entirely new structural domains.
- Why unresolved: SFT was performed on derived data from 4 specific benchmark networks; generalization to unseen domain structures (e.g., financial or social networks) is not tested.
- What evidence would resolve it: Zero-shot evaluation of the finetuned model on causal discovery datasets from distinct domains not represented in the SFT data.

## Limitations

- Computational constraints limited evaluation to Qwen2.5-1.5B model, with potential for larger models to achieve superior performance
- Evaluation uses held-out augmentations rather than entirely new DAGs, leaving open possibility of overfitting to four benchmark networks
- LLM-as-judge evaluation introduces potential parsing errors and biases from the judge model, though manual spot checks are reported

## Confidence

- High Confidence: The observation that prompting with algorithm outputs degrades LLM performance is well-supported by ablation results (e.g., GPT-4o-mini F1 drops from 0.688 to 0.497 on ASIA-Original when algorithm outputs are included)
- Medium Confidence: The claim that augmentation forces genuine statistical reasoning is supported by performance gains on permuted variable names, but the mechanism remains correlational
- Low Confidence: The Bayesian posterior updating framework is conceptually plausible but lacks direct empirical validation

## Next Checks

1. Ablate individual augmentations: Train CARE variants with one augmentation type removed at a time and measure F1 deltas on held-out test sets to isolate each perturbation's contribution to performance gains

2. Transfer to unseen DAGs: Train CARE on ASIA+SURVEY, then evaluate zero-shot on EARTHQUAKE to test whether learned integration generalizes beyond the four benchmark networks

3. Judge model robustness check: Manually verify a random sample of LLM-as-judge outputs to quantify parsing errors and assess whether they systematically bias reported F1 scores