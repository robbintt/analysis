---
ver: rpa2
title: 'Position: The Most Expensive Part of an LLM should be its Training Data'
arxiv_id: '2504.12427'
source_url: https://arxiv.org/abs/2504.12427
tags:
- training
- data
- costs
- text
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper quantifies the often-overlooked human labor cost of
  creating training data for Large Language Models (LLMs). The authors estimate the
  cost of producing LLM training datasets from scratch, based on conservative assumptions
  about writing speed and wages.
---

# Position: The Most Expensive Part of an LLM should be its Training Data

## Quick Facts
- arXiv ID: 2504.12427
- Source URL: https://arxiv.org/abs/2504.12427
- Reference count: 16
- Authors: Nikhil Kandpal; Colin Raffel
- Primary result: Training data production costs for LLMs exceed model training costs by 10-1000x

## Executive Summary
This paper reveals that the human labor cost of creating training data for Large Language Models is vastly underestimated and often exceeds the computational cost of training the models themselves. Through conservative estimates of writing speeds and wages, the authors demonstrate that even modest assumptions about data production costs result in expenses that dwarf model training costs by factors of 10 to 1000. The analysis highlights a critical economic reality: while companies invest heavily in GPU infrastructure and model architecture, the foundational data that makes these models valuable remains the most expensive component. This creates an ethical dilemma where the true cost of data production makes it impractical to fairly compensate the creators whose work forms the backbone of these systems.

## Method Summary
The authors conducted a comprehensive analysis of training data costs for prominent LLMs by estimating the labor required to produce datasets from scratch. They established conservative assumptions about writing productivity (100 words per hour) and wage rates ($15-20 per hour for general and research writers respectively). The methodology involved calculating the total word count in major LLM training datasets, estimating the human hours required to generate this content, and converting these hours to monetary costs using the assumed wage rates. The analysis focused on text generation costs while acknowledging that data preparation involves additional activities like cleaning and curation. By comparing these data production costs to the well-documented computational costs of training these models, the authors quantified the economic imbalance between data and model training expenses.

## Key Results
- Training data production costs exceed model training costs by 10-1000x across analyzed LLMs
- GPT-4's dataset would cost $300x more to produce than the model's training
- DeepSeek-V3's dataset would cost $6000x more to produce than the model's training
- Conservative assumptions still yield data costs far exceeding computational training expenses

## Why This Works (Mechanism)
The economic analysis works because it directly quantifies the human labor component that is typically invisible in LLM cost calculations. By establishing conservative baseline assumptions for writing productivity and compensation, the methodology creates a lower-bound estimate that is difficult to dispute. The approach systematically breaks down the total dataset size into human-readable units, applies realistic production rates, and converts these to monetary values. This reveals the hidden infrastructure cost of human creativity and expertise that underlies all LLM capabilities, demonstrating that the computational bottleneck is actually a data bottleneck when viewed through an economic lens.

## Foundational Learning
**Data Economics**: Understanding the monetary value of human-generated content in the AI supply chain is essential for recognizing the true cost structure of LLMs. Quick check: Compare reported training costs vs. data production estimates for any major model.

**Labor Value Theory**: The paper demonstrates how undervaluing human creative work in AI development creates unsustainable economic models. Quick check: Calculate the minimum viable wage for data creators given current dataset sizes.

**Cost Attribution**: Properly allocating expenses between computational resources and human resources reveals the real bottleneck in AI development. Quick check: Map all cost components in a typical LLM training pipeline.

## Architecture Onboarding
**Component Map**: Human Writers -> Content Generation -> Dataset Curation -> Model Training -> Inference Deployment
**Critical Path**: Data production (human writing) is the slowest and most expensive step, creating the fundamental bottleneck
**Design Tradeoffs**: The paper highlights the tension between model complexity (which requires more data) and data production costs (which scale linearly with model size)
**Failure Signatures**: Models trained on artificially cheap or free data often exhibit quality issues, biases, and ethical concerns
**First Experiments**: 
1. Calculate dataset production costs for a hypothetical 1T token model using different wage assumptions
2. Compare quality metrics of models trained on paid vs. unpaid data sources
3. Model the break-even point where data costs equal training costs under various productivity assumptions

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis relies on conservative assumptions about writing speed and wages that may not reflect actual conditions
- Does not account for variations in data quality, complexity, or domain-specific expertise requirements
- Focuses primarily on text generation costs, excluding data cleaning, annotation, and curation activities

## Confidence
- **High confidence** in the conclusion that training data costs are substantial and often exceed model training costs
- **Medium confidence** in the specific numerical estimates, given the sensitivity to assumptions about wages and productivity
- **Low confidence** in the comparison of these costs to alternative data acquisition methods (e.g., web scraping, synthetic data)

## Next Checks
1. Conduct empirical studies measuring actual writing speeds and costs for different types of training data across various domains
2. Analyze real-world datasets to determine what proportion of data is newly created versus repurposed or synthetically generated
3. Compare the cost-effectiveness of compensating data creators versus alternative data acquisition strategies (web scraping, synthetic data generation, data augmentation)