---
ver: rpa2
title: 'From Conversation to Query Execution: Benchmarking User and Tool Interactions
  for EHR Database Agents'
arxiv_id: '2509.23415'
source_url: https://arxiv.org/abs/2509.23415
tags:
- agent
- user
- epilepsy
- table
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "EHR-ChatQA is an interactive benchmark for evaluating database\
  \ agents in the EHR domain, focusing on real-world challenges like query ambiguity\
  \ and value mismatch. It assesses agents\u2019 ability to clarify vague user questions,\
  \ use tools to resolve terminology mismatches, and generate accurate SQL queries\
  \ through simulated conversations."
---

# From Conversation to Query Execution: Benchmarking User and Tool Interactions for EHR Database Agents

## Quick Facts
- arXiv ID: 2509.23415
- Source URL: https://arxiv.org/abs/2509.23415
- Reference count: 40
- EHR-ChatQA benchmark evaluates database agents in EHR domain focusing on query ambiguity and value mismatch challenges

## Executive Summary
EHR-ChatQA introduces an interactive benchmark for evaluating database agents in electronic health record (EHR) systems. The benchmark focuses on real-world challenges including query ambiguity, value mismatches between user queries and database terminology, and the need for clarification dialogues. It assesses agents' abilities to clarify vague questions, use tools to resolve terminology mismatches, and generate accurate SQL queries through simulated conversations. The benchmark includes two interaction flows - Incremental Query Refinement and Adaptive Query Refinement - grounded in MIMIC-IV and eICU databases.

## Method Summary
The benchmark employs a simulated interaction environment where agents must navigate between user queries and database operations. Agents can utilize tools for task planning, database schema access, and semantic search to bridge gaps between user intent and database structure. The evaluation uses two metrics: Pass@5 (whether the agent succeeds within 5 steps) and Pass^5 (whether the agent succeeds consistently without backtracking). The benchmark generates synthetic queries and corresponding interaction data, with human validation for a subset of examples to ensure realism.

## Key Results
- Significant performance gap between Pass@5 and Pass^5 metrics, particularly in Adaptive Query Refinement tasks (over 60% gap)
- Current agents struggle with consistency in handling complex EHR queries
- Leading LLMs show varying performance across different interaction flows and query types

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focus on realistic EHR query challenges including ambiguity resolution and terminology mismatch handling. By incorporating both incremental and adaptive query refinement flows, it captures the complexity of real-world clinical database interactions. The use of actual EHR databases (MIMIC-IV and eICU) provides authentic schema and data characteristics.

## Foundational Learning
- **Query Ambiguity Resolution**: Understanding how agents handle unclear user questions and request clarifications
- **Value Mismatch Resolution**: How agents bridge terminology gaps between user queries and database schema
- **Incremental vs Adaptive Refinement**: Different approaches to query construction based on conversation flow
- **Tool Usage Patterns**: How agents leverage available tools for task planning and semantic search
- **Consistency Metrics**: Understanding the difference between single-success and reliable performance
- **Schema Navigation**: How agents understand and navigate complex EHR database structures

## Architecture Onboarding

**Component Map**: User Query -> Agent -> Tool Selection -> Database Query -> Results -> Response

**Critical Path**: User Query → Task Planning Tool → Semantic Search Tool → SQL Generation → Database Execution → Result Validation

**Design Tradeoffs**: Simulated vs real user interactions; optimistic vs consistent success metrics; balance between query complexity and agent capability

**Failure Signatures**: High Pass@5 to Pass^5 gaps indicate inconsistency; failure in value mismatch resolution suggests tool usage problems; difficulty with adaptive refinement indicates poor context maintenance

**First Experiments**:
1. Evaluate agents on simple queries with clear intent to establish baseline performance
2. Test agents on queries with known terminology mismatches to assess tool effectiveness
3. Compare performance on incremental vs adaptive refinement flows to identify interaction style challenges

## Open Questions the Paper Calls Out
None

## Limitations
- Simulated user interactions may not fully capture real-world query complexity and variability
- Limited to two specific EHR databases (MIMIC-IV and eICU), potentially limiting generalizability
- Automated correctness checks may miss nuanced semantic errors in clinical queries

## Confidence
- Benchmark design and dataset creation: High
- Performance gap observations: Medium
- Broader generalizability claims: Low

## Next Checks
1. Evaluate agents on additional diverse EHR schemas to assess cross-database generalization
2. Conduct user studies with healthcare professionals to validate the realism of simulated interactions
3. Perform ablation studies to isolate which components of the interaction flow most impact agent reliability