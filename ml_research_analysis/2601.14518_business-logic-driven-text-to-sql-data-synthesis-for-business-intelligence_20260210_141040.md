---
ver: rpa2
title: Business Logic-Driven Text-to-SQL Data Synthesis for Business Intelligence
arxiv_id: '2601.14518'
source_url: https://arxiv.org/abs/2601.14518
tags:
- business
- data
- question
- persona
- logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Business Logic-Driven Data Synthesis framework
  to generate business-realistic Text-to-SQL evaluation data in private BI settings.
  The framework models business logic as personas, work scenarios, and workflows,
  guiding schema selection and SQL generation to reflect real-world business activities.
---

# Business Logic-Driven Text-to-SQL Data Synthesis for Business Intelligence

## Quick Facts
- arXiv ID: 2601.14518
- Source URL: https://arxiv.org/abs/2601.14518
- Reference count: 39
- Key outcome: Introduces a framework for generating business-realistic Text-to-SQL evaluation data grounded in business personas, scenarios, and workflows, achieving 98.44% realism and 98.59% alignment on a Salesforce database.

## Executive Summary
This paper presents a novel approach to synthesizing Text-to-SQL evaluation data that reflects real-world business intelligence workflows. The framework models business logic as a hierarchy of personas, work scenarios, and workflows, guiding schema selection and SQL generation to produce questions that naturally arise from role-specific responsibilities. It introduces a business reasoning complexity control strategy that characterizes query difficulty by reasoning steps rather than SQL syntax, enabling systematic generation of data across diverse analytical tasks. Experiments on a production-scale Salesforce database demonstrate substantially higher realism and alignment than existing methods, while also revealing significant performance gaps in state-of-the-art Text-to-SQL models on complex queries.

## Method Summary
The framework operates through a pipeline that begins with hierarchical business logic modeling, generating personas with specific roles, goals, and pain points, then mapping them to work scenarios and workflows with concrete analytical tasks. Schema selection filters tables based on business relevance scores, retaining only those most pertinent to each (persona, scenario) pair. SQL generation is conditioned on both the business logic and one of four complexity levels (Single/Comparative/Derived/Compositional), followed by execution-based refinement to ensure semantic correctness. The validated SQL queries are then converted to natural language questions, and quality is evaluated using LLM-as-a-Judge rubrics for both realism and question-SQL alignment. The method is validated on a production Salesforce database with 710 tables.

## Key Results
- Synthesized dataset achieves 98.44% question realism and 98.59% question-SQL alignment on Salesforce database
- Outperforms existing methods (OmniSQL +19.5%, SQL-Factory +54.7%) on realism and alignment metrics
- State-of-the-art Text-to-SQL models show significant performance gaps, with strongest achieving only 42.86% accuracy on most complex queries
- Systematic complexity control yields balanced distribution across four reasoning levels (entropy 0.85 vs 0.54 without control)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Grounding synthetic data generation in structured business logic hierarchies (persona → work scenario → workflow) produces substantially more realistic evaluation questions than schema-driven approaches.
- **Mechanism:** The hierarchical business logic structure provides rich contextual constraints at each generation step—personas define job roles and goals, scenarios specify business situations, and workflows decompose into concrete analytical tasks. This multi-level context guides the LLM to generate questions that naturally arise from realistic work objectives rather than generic database exploration.
- **Core assumption:** Business questions in real BI settings emerge from role-specific responsibilities and workflows, not random schema traversal.
- **Evidence anchors:**
  - [abstract] "framework generates data grounded in business personas, work scenarios, and workflows"
  - [section 7, ablation] Removing business logic causes question realism to drop from 97.34% to 63.38% (−34%)
  - [corpus] Agent Bain vs. Agent McKinsey benchmark similarly argues for business-domain grounding, though focuses on complex response types rather than synthesis methodology

### Mechanism 2
- **Claim:** Characterizing query difficulty by business reasoning steps (rather than SQL syntactic features) enables systematic coverage of diverse analytical complexity levels.
- **Mechanism:** The four-level complexity taxonomy (Single Metric → Comparative → Derived → Compositional) captures progressively more sophisticated reasoning patterns. By explicitly conditioning SQL generation on these levels, the framework ensures balanced coverage across difficulty tiers. Normalized Shannon entropy quantifies this distribution.
- **Core assumption:** Business complexity correlates more strongly with reasoning steps (compare, derive, compose) than with SQL features like join count or nesting depth.
- **Evidence anchors:**
  - [section 3.3] Four levels explicitly defined: "Single Metric Analysis," "Comparative Metric Analysis," "Derived Metric Analysis," "Compositional Task Analysis"
  - [section 7, ablation] Removing complexity control drops complexity diversity from 0.85 to 0.54
  - [corpus] LogicCat benchmark similarly emphasizes reasoning complexity in Text-to-SQL, but focuses on chain-of-thought evaluation rather than synthesis control

### Mechanism 3
- **Claim:** SQL-first generation with execution-based refinement preserves semantic correctness while business logic grounding improves realism.
- **Mechanism:** SQL queries are generated first (conditioned on business logic and complexity level), validated through database execution, then converted to natural language questions. This ordering ensures the ground truth is executable and the question is derived from a verified query, reducing hallucination while maintaining alignment.
- **Core assumption:** Executable SQL provides a more reliable ground truth than generating questions first and attempting to match SQL.
- **Evidence anchors:**
  - [section 3.3] "We adopt the SQL-first generation strategy, treating the SQL query as the executable ground truth from which natural language questions are later derived"
  - [section 5] Question-SQL alignment remains ≥90% across all complexity levels even as realism stays >95%
  - [corpus] PaVeRL-SQL similarly uses execution feedback but through reinforcement learning; this paper uses explicit refinement loops

## Foundational Learning

- **Concept: Text-to-SQL Task Definition**
  - Why needed here: The framework assumes familiarity with the core problem—mapping natural language questions to executable SQL queries against a database schema.
  - Quick check question: Can you explain why execution accuracy is a more reliable metric than exact string match for SQL evaluation?

- **Concept: Business Intelligence Workflows**
  - Why needed here: The hierarchical modeling (persona → scenario → workflow) requires understanding how analytical questions naturally emerge in organizational contexts.
  - Quick check question: Given a "Sales Development Manager" persona, can you identify at least two distinct work scenarios that would lead to different question types over the same opportunity data?

- **Concept: LLM-as-a-Judge Evaluation**
  - Why needed here: Data quality is assessed using LLMs with structured rubrics; understanding the limitations and biases of this approach is critical for interpreting reported metrics.
  - Quick check question: What are two failure modes when using an LLM to evaluate whether a question is "business realistic"?

## Architecture Onboarding

- **Component map:**
  Business Logic Modeling -> Schema Selection -> SQL Generation (with complexity control) -> Execution Refinement -> Question Conversion -> Quality Evaluation

- **Critical path:** Business Logic Modeling → Schema Selection → SQL Generation (with complexity control) → Execution Refinement → Question Conversion. Breaks in business logic modeling propagate through the entire pipeline.

- **Design tradeoffs:**
  - **SQL-first vs. Question-first:** SQL-first ensures executability but may limit natural question phrasing; question-first enables more natural language but risks unanswerable questions.
  - **Strict persona alignment vs. cross-functional queries:** Higher complexity queries tend to span multiple personas' responsibilities (see Section 6 error analysis); loosening persona constraints improves realism for senior roles but reduces controllability.
  - **Schema exposure:** Limiting to business-relevant tables reduces noise but may miss legitimate edge-case queries; full schema exposure increases coverage at the cost of realism.

- **Failure signatures:**
  - High alignment, low realism → Schema-driven generation without business logic grounding
  - High realism, low alignment → Question generation detached from executable SQL
  - Low complexity diversity → Missing or ineffective complexity control during SQL generation
  - Persona-question misalignment at high complexity → Compositional queries combining tasks from different roles

- **First 3 experiments:**
  1. **Ablation by component:** Run data synthesis with business logic removed but complexity control retained (replicates Table 4 "w/o BL" condition) to isolate the contribution of hierarchical context.
  2. **Complexity calibration:** Generate 50 queries per complexity level, manually verify that assigned labels match actual reasoning steps required; adjust taxonomy definitions if misalignment exceeds 20%.
  3. **Cross-domain transfer:** Apply the framework to a different BI domain (e.g., marketing analytics) with the same pipeline; compare realism scores to assess generalizability beyond sales.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Business Logic-Driven Data Synthesis framework be extended to support multi-turn, interactive analytical tasks?
- Basis in paper: [explicit] The Conclusion states, "In future work, we plan to extend our framework to support multi-turn, interactive analytical tasks that more closely reflect how business users iteratively explore data in real BI workflows."
- Why unresolved: The current framework generates independent question-SQL pairs based on static workflows. It lacks the mechanism to maintain context or dependency chains required for iterative data exploration where the second question depends on the results of the first.
- What evidence would resolve it: A demonstration of the framework generating coherent dialogue sessions with context-dependent follow-up questions and executable SQL sequences, evaluated for contextual consistency.

### Open Question 2
- Question: Can this synthesis approach maintain high question realism and alignment when applied to BI domains outside of sales, such as finance or healthcare?
- Basis in paper: [explicit] The Limitations section notes, "We validate our framework primarily in the sales domain... further validation is needed to assess the generalizability of our approach across other BI domains, such as finance, marketing, or operations."
- Why unresolved: The framework was tested exclusively on a Salesforce schema using sales-specific personas (e.g., Sales Development Manager). It is unclear if the hierarchical persona-scenario-workflow modeling transfers effectively to domains with different regulatory constraints or logical structures.
- What evidence would resolve it: Experimental results on production-scale databases from non-sales domains (e.g., ERP or medical records) showing that the method achieves Question Realism scores comparable to the reported 98.44%.

### Open Question 3
- Question: How can the framework mitigate the observed degradation in persona-question alignment when generating highly complex, compositional queries?
- Basis in paper: [inferred] The Error Analysis notes that persona alignment drops significantly (from 95.27% to 85.46%) as complexity increases because "higher-complexity questions increasingly combine different analytical tasks that are owned by different personas."
- Why unresolved: The current strategy for increasing "Business Reasoning Complexity" involves combining analytical operations (compositional tasks), which naturally breaks the single-persona boundaries established in the modeling phase.
- What evidence would resolve it: An improved complexity control mechanism that generates high-difficulty queries while maintaining persona alignment scores above 90%, potentially by modeling cross-functional personas or restricting compositional logic to single-role scopes.

## Limitations
- Reliance on GPT-5 for synthesis and evaluation creates significant reproducibility barriers as this model is not publicly accessible
- Framework effectiveness demonstrated only on a single private Salesforce database, raising questions about generalizability across different BI domains and schema complexities
- Hierarchical business logic modeling may over-constrain the question space if personas and workflows are not representative of actual user behavior in the target domain

## Confidence

- **High Confidence:** The SQL-first generation strategy with execution-based refinement is a sound methodological approach that directly addresses the executability challenge in Text-to-SQL synthesis. The business reasoning complexity taxonomy is well-defined and systematically applied.
- **Medium Confidence:** The reported quality metrics (98.44% realism, 98.59% alignment) are impressive but depend heavily on the specific LLM judge implementation and may not translate directly when using substitute models. The ablation studies provide reasonable evidence for component contributions but are limited in scope.
- **Low Confidence:** The framework's performance on state-of-the-art models (42.86% accuracy on complex queries) indicates a significant gap, but the sample size and distribution across complexity levels is not fully specified, making it difficult to assess statistical significance.

## Next Checks

1. **Model-Agnostic Quality Assessment:** Replicate the data synthesis pipeline using publicly available LLMs (GPT-4o, Claude, Gemini) for both generation and evaluation. Compare quality metrics against the reported GPT-5 results to establish model sensitivity and identify the minimum viable model requirements.

2. **Cross-Domain Generalization Test:** Apply the framework to a different business domain (e.g., marketing analytics, customer support) using a comparable multi-table database schema. Measure realism and alignment metrics to validate whether the hierarchical business logic modeling generalizes beyond sales-focused use cases.

3. **Complexity Taxonomy Validation:** Conduct a controlled human evaluation where annotators independently assess the reasoning complexity of generated queries against the four-level taxonomy. Calculate inter-annotator agreement and compare against the automated classification to validate the taxonomy's reliability and identify potential misalignment between assigned and actual complexity levels.