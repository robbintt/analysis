---
ver: rpa2
title: Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization
  of Prolonged Exposure Therapy Elements
arxiv_id: '2506.09707'
source_url: https://arxiv.org/abs/2506.09707
tags:
- fidelity
- therapy
- lora
- exposure
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating therapist fidelity
  in Prolonged Exposure (PE) therapy for PTSD, which traditionally requires labor-intensive
  manual review of session recordings. The authors present a method for automatically
  localizing key PE therapy elements by fine-tuning a large pre-trained audio-language
  model (Qwen2-Audio) using Low-Rank Adaptation (LoRA) to process focused 30-second
  windows of audio-transcript input.
---

# Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements

## Quick Facts
- arXiv ID: 2506.09707
- Source URL: https://arxiv.org/abs/2506.09707
- Reference count: 0
- Key outcome: Achieves 5.3s MAE in localizing PE therapy phases using 30s windows and LoRA fine-tuning

## Executive Summary
This paper addresses the challenge of evaluating therapist fidelity in Prolonged Exposure (PE) therapy for PTSD by developing an automated method to localize key therapy elements within session recordings. The approach fine-tunes a large pre-trained audio-language model (Qwen2-Audio) using Low-Rank Adaptation (LoRA) to process 30-second windows of audio-transcript input and predict normalized temporal offsets for three core protocol phases. Using a soft-supervision pipeline with LLM-based prompting and rater verification, the method achieves a mean absolute error of 5.3 seconds across tasks on a dataset of 308 real PE sessions, falling within typical rater tolerance for timestamp review. The study demonstrates that shorter input windows and appropriate LoRA configurations significantly improve temporal localization precision, enabling practical automated fidelity tracking for clinical quality assurance.

## Method Summary
The method processes PE therapy session recordings by first generating transcripts with sentence-level timestamps using Amazon HealthScribe. Ground-truth labels for three therapy phases (orientation, imaginal exposure, post-imaginal processing) are generated through LLM prompting (Claude Sonnet 3.5) and verified by trained raters on a 10% sample. The model uses Qwen2-Audio-7B-Instruct with QLoRA adapters (4-bit NF4 quantization) and applies LoRA to the LLM backbone with ranks 2, 4, or 8. The architecture includes a regression head that predicts normalized offsets within fixed-duration windows (30s, 60s, or 120s). Training uses AdamW optimizer with learning rate 1e-4, cosine schedule, and early stopping based on validation MAE. Predictions are denormalized to absolute timestamps for evaluation.

## Key Results
- Achieves 5.3s mean absolute error across tasks using 30-second windows
- Shorter windows (30s) yield significantly better precision than longer windows (12.2s at 60s, 25.2s at 120s)
- LoRA fine-tuning with r=8 and dropout=0.1 provides optimal balance of precision and stability
- Results fall within typical rater tolerance for timestamp review in clinical settings

## Why This Works (Mechanism)

### Mechanism 1: Context-Granularity Tradeoff in Windowed Input
Shorter input windows (30s) yield more precise temporal boundary predictions than longer windows because the model predicts a normalized offset [0,1] within each window. With shorter windows, the boundary occupies a larger relative position, reducing the search space and amplifying discriminative signals. Longer windows provide more semantic context but dilute temporal precision—the same absolute error spans a smaller normalized range.

### Mechanism 2: Multimodal Joint Encoding Captures Prosodic Markers
Joint audio-transcript processing captures boundary signals (silences, prosody, vocal affect) that text-only approaches miss. Qwen2-Audio's end-to-end architecture learns aligned representations across modalities. Phase transitions often occur in pauses or intonation shifts between words—signals discarded by text-only models relying on ASR timestamps.

### Mechanism 3: Soft Supervision Reduces Annotation Burden
LLM-proposed timestamps verified by trained raters provide scalable ground truth without exhaustive manual annotation. Raters verify a 10% sample to establish validity (94.4% timestamp accuracy within 5-10s, 97.7% label accuracy). This "human-in-the-loop" approach balances annotation cost with clinical validity.

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: Core fine-tuning technique; rank selection (r=2,4,8) directly affects MAE and training stability. Understanding the α=2r scaling and dropout=0.1 is needed for reproduction.
  - Quick check question: At 120s windows, which LoRA rank achieved lowest P2 Avg MAE, and what does this suggest about overfitting? (Answer: r=2 achieved 18.0s; higher ranks may overfit when input spans are too broad)

- **Concept: Normalized Offset Regression**
  - Why needed here: The task is framed as continuous regression (predicting 0.0-1.0 offset) rather than classification. This enables variable-length windows and augmentation via random boundary positioning.
  - Quick check question: Why predict normalized offset instead of absolute timestamp? (Answer: Normalization allows the model to generalize across windows of different durations and enables data augmentation by randomly positioning the boundary within each window)

- **Concept: Soft Supervision Pipeline**
  - Why needed here: Annotation strategy is critical for extending to new therapy types or datasets; understanding the LLM → rater verification flow informs replication costs.
  - Quick check question: What was the human verification sample size and accuracy threshold? (Answer: 10% of sessions; 94.4% timestamp accuracy within 5-10s)

## Architecture Onboarding

- **Component map:**
  [Audio @ 16kHz, normalized WAV] + [Transcript w/ sentence-level timestamps] → [Window Extraction] → [Qwen2-Audio-7B-Instruct] → [QLoRA Adapters] → [Regression Head] → [Normalized Offset ō ∈ (0,1)] → Denormalize to absolute timestamp

- **Critical path:**
  1. Verify audio-transcript alignment (Amazon HealthScribe or equivalent with sentence-level timestamps)
  2. Generate/verify fidelity labels (LLM proposal + 10% rater verification)
  3. Extract windows with random boundary positioning for augmentation
  4. Prepend task-specific prompt (e.g., "Identify the normalized offset... of this precise START")
  5. Train with AdamW (lr=1e-4, cosine schedule, early stopping patience=3 on validation MAE)
  6. Denormalize predictions: t_abs = t_start + (predicted_offset × window_duration)

- **Design tradeoffs:**
  - Window size vs precision: 30s = best MAE (~5s) but less context; 120s = more semantic context but 4x worse MAE
  - LoRA rank vs stability: r=8 gives best 30s/60s results but higher seed variance; r=2 more stable for 120s
  - Head-only vs LoRA: Head-only stable across seeds (low S.D.); LoRA achieves lower MAE but with higher variance

- **Failure signatures:**
  - MAE > 15s at 60s windows with r=8: Likely overfitting—reduce rank or add regularization
  - High seed variance (S.D. > 2s): Check data augmentation randomization; consider more training data
  - Systematic early/late predictions: Verify audio-transcript alignment; check prompt wording consistency
  - Poor phase-specific performance (e.g., P3 worse than P1/P2): Check label distribution balance in train/val splits

- **First 3 experiments:**
  1. Reproduce baseline: 30s windows, r=8, three seeds—target MAE ~5.3s ± 1.5s; validates pipeline correctness
  2. Ablate audio input: Text-only (transcript only, zero audio) vs full multimodal—quantifies prosodic contribution
  3. Cross-therapist generalization: Train on subset of therapists, test on held-out therapists—if MAE degrades >50%, investigate domain adaptation or data augmentation strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the fine-tuned model generalize to additional PE fidelity markers beyond the three core phases (P1-P3) and to diverse therapy settings outside the Emory University dataset?
- Basis in paper: [explicit] "Future work will expand to additional fidelity markers and test generalization across diverse therapy settings."
- Why unresolved: The model was evaluated only on three phases from a single clinical site; no experiments tested transfer to other institutions, therapist populations, or fidelity markers.
- What evidence would resolve it: Cross-site evaluation on datasets from multiple clinics and performance metrics on additional PE protocol elements beyond orientation, imaginal exposure, and processing.

### Open Question 2
- Question: What is the relationship between window size and optimal LoRA rank, and can it be characterized systematically rather than requiring empirical tuning per configuration?
- Basis in paper: [inferred] Results showed inconsistent optimal ranks across window sizes (r=8 best at 60s, but r=2 outperformed r=8 for P2 at 120s), with higher ranks potentially overfitting broader contexts, yet no principled explanation was provided.
- Why unresolved: No theoretical framework or systematic experiments explaining why optimal rank varies with window duration and phase.
- What evidence would resolve it: Controlled ablations varying window size, rank, and phase simultaneously to identify predictive patterns or derive theoretical bounds.

### Open Question 3
- Question: What verification coverage is sufficient for LLM-generated timestamp annotations to maintain clinical validity, given that only 10% of sessions were rater-verified?
- Basis in paper: [inferred] The soft supervision pipeline verified only 30 of 308 sessions (10%), leaving the scalability-reliability trade-off of this labeling strategy untested.
- Why unresolved: No experiments varied verification rates to determine how model performance degrades with less oversight.
- What evidence would resolve it: Systematic experiments comparing downstream MAE across different verification coverage levels against fully rater-verified ground truth.

## Limitations
- Annotation quality depends on LLM proposals verified on only 10% of sessions, potentially missing systematic biases
- 30-second window optimization may limit ability to handle gradual phase transitions or complex therapeutic interactions
- Performance on diverse clinical populations, audio qualities, and real-world deployment scenarios remains untested

## Confidence
- **High Confidence**: Core technical methodology (LoRA fine-tuning, normalized offset regression, 30-second window approach) is well-specified and reproducible
- **Medium Confidence**: Soft supervision pipeline's scalability and accuracy claims depend on representativeness of 10% rater verification sample
- **Low Confidence**: Model's performance on therapy sessions significantly different from training corpus (different accents, poor audio quality, non-standard approaches) remains unknown

## Next Checks
1. **Cross-Dataset Generalization**: Test the trained model on an independent PE therapy dataset from different clinics or geographic regions to measure performance degradation and identify domain adaptation needs
2. **Robustness to Audio Quality**: Systematically degrade audio quality (add noise, compress, simulate poor recording conditions) and measure MAE impact to validate practical utility in real clinical environments
3. **Ablation of Soft Supervision**: Compare model performance when trained on fully manual annotations versus LLM-assisted labels to quantify trade-off between annotation cost and model accuracy, and validate whether soft supervision introduces systematic biases