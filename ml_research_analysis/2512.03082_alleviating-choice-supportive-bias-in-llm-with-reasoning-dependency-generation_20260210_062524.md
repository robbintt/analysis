---
ver: rpa2
title: Alleviating Choice Supportive Bias in LLM with Reasoning Dependency Generation
arxiv_id: '2512.03082'
source_url: https://arxiv.org/abs/2512.03082
tags:
- bias
- feature
- chosen
- while
- option
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses choice-supportive bias (CSB) in LLMs, where
  models retroactively favor chosen options. The proposed Reasoning Dependency Generation
  (RDG) framework generates balanced training data that explicitly varies dependencies
  between choices, facts, and justifications.
---

# Alleviating Choice Supportive Bias in LLM with Reasoning Dependency Generation

## Quick Facts
- **arXiv ID**: 2512.03082
- **Source URL**: https://arxiv.org/abs/2512.03082
- **Reference count**: 40
- **Primary result**: RDG framework reduces choice-supportive bias by 81.5-94.3% while maintaining BBQ benchmark performance

## Executive Summary
This work addresses choice-supportive bias (CSB) in large language models (LLMs), where models retroactively favor chosen options. The authors propose Reasoning Dependency Generation (RDG), a framework that generates balanced training data with explicit dependencies between choices, facts, and justifications. RDG employs two complementary strategies: Contextual Dependency Data strengthens appropriate factual dependencies, while Dependency Decoupling Data breaks inappropriate choice-feature associations. The framework demonstrates significant improvements in reducing CSB while maintaining performance on BBQ benchmarks, contributing to more reliable AI-assisted decision support systems.

## Method Summary
The Reasoning Dependency Generation (RDG) framework generates training data that explicitly controls the relationships between choices, supporting facts, and justifications. It uses two complementary strategies: Contextual Dependency Data strengthens appropriate factual dependencies to help models learn correct reasoning patterns, while Dependency Decoupling Data breaks inappropriate associations between choices and features to prevent biased justifications. The framework creates synthetic scenarios where these dependencies can be precisely controlled and varied during training. By exposing models to both dependency patterns, RDG teaches them to evaluate choices based on actual merits rather than post-hoc rationalizations, effectively reducing choice-supportive bias while maintaining decision-making capabilities.

## Key Results
- 81.5% reduction in memory-based choice-supportive bias
- 94.3% reduction in evaluation-based choice-supportive bias
- Maintained similar performance on BBQ benchmarks compared to baseline models
- Demonstrated effectiveness through controlled synthetic experiments with varying dependency patterns

## Why This Works (Mechanism)
RDG works by systematically exposing LLMs to both appropriate and inappropriate reasoning dependencies during training. When models encounter choices, they often retroactively generate justifications that favor the selected option (choice-supportive bias). RDG counteracts this by training on data where the relationship between choices and supporting facts is explicitly varied - sometimes strengthening correct dependencies, sometimes decoupling incorrect ones. This dual approach teaches models to recognize valid reasoning patterns and reject spurious justifications. The Contextual Dependency Data helps models learn when factual support legitimately connects to choices, while Dependency Decoupling Data prevents the formation of false associations that lead to biased post-hoc reasoning.

## Foundational Learning
**Choice-Supportive Bias (CSB)**: A cognitive bias where people retroactively favor chosen options and create justifications that support their decisions, even when contrary evidence exists. This bias is particularly problematic in AI decision support systems where reliability matters.

Why needed: Understanding CSB is crucial because it affects how LLMs justify decisions, potentially leading to unreliable or biased recommendations that users cannot trust.

Quick check: Can you identify examples where you've retroactively justified a choice? How might this manifest in LLM outputs?

**Dependency Modeling**: The explicit representation of relationships between choices, facts, and justifications in training data. RDG treats these dependencies as first-class elements that can be controlled and varied.

Why needed: By modeling dependencies explicitly, RDG can systematically teach models when connections between choices and facts are valid versus when they represent post-hoc rationalization.

Quick check: How would you represent the relationship between "I chose option A" and "Option A has feature X" in a dependency graph?

**Synthetic Data Generation**: Creating controlled training examples where specific attributes (like dependencies) can be precisely manipulated to teach desired behaviors.

Why needed: Real-world data rarely provides the level of control needed to isolate and correct specific cognitive biases like CSB.

Quick check: What are the advantages and limitations of using synthetic data versus real decision-making data for bias mitigation?

## Architecture Onboarding

**Component Map**: RDG Generator -> Training Data Pipeline -> LLM Fine-tuning Module -> Bias Evaluation System

**Critical Path**: The core workflow involves generating dependency-controlled data (RDG Generator), incorporating it into the training pipeline, fine-tuning the LLM on this balanced dataset, and then evaluating bias reduction through controlled tests measuring memory-based and evaluation-based CSB metrics.

**Design Tradeoffs**: The framework trades computational overhead for bias reduction - generating synthetic data with controlled dependencies requires additional processing compared to standard training. However, this investment yields significant improvements in reliability. The dual-strategy approach (strengthening correct dependencies while breaking incorrect ones) provides comprehensive coverage but adds complexity to the training pipeline.

**Failure Signatures**: Models trained without RDG may show post-hoc rationalizations that favor chosen options regardless of factual merit. When RDG fails, we might observe either: (1) over-correction where models reject valid justifications, or (2) incomplete bias reduction where some choice-supportive patterns persist in justifications.

**First Experiments**:
1. Test RDG on a simple binary choice scenario with clearly defined dependencies to verify basic functionality
2. Evaluate bias reduction on a synthetic dataset where ground truth dependencies are known
3. Measure performance trade-offs on a standard reasoning benchmark while tracking CSB metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on synthetic data with controlled dependencies, which may not fully capture real-world decision complexity
- Computational overhead of RDG framework not thoroughly explored for training efficiency impact
- Generalizability to different LLM architectures beyond tested variant remains uncertain
- Real-world decision scenarios often involve incomplete information and nuanced trade-offs not captured in controlled experiments

## Confidence
**High confidence**: The core methodology for generating training data with explicit dependency relationships is sound and the controlled experiments demonstrate clear effectiveness in reducing CSB within the tested framework.

**Medium confidence**: The claim that RDG maintains similar performance on BBQ benchmarks while reducing CSB is supported, though the interplay between bias reduction and task performance across diverse domains needs further validation.

**Low confidence**: The assertion that this work "pioneers" CSB mitigation in LLMs is difficult to verify definitively given the rapidly evolving nature of bias research in language models.

## Next Checks
1. Test RDG's effectiveness on real-world decision datasets where choices have measurable outcomes, comparing model recommendations against human expert decisions.

2. Evaluate the method's transferability across different LLM architectures (e.g., transformer variants, encoder-decoder models) and scales to determine generalizability.

3. Conduct ablation studies to isolate the individual contributions of Contextual Dependency Data versus Dependency Decoupling Data to the overall CSB reduction.