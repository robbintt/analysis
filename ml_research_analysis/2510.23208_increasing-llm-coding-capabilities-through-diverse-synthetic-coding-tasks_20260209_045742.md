---
ver: rpa2
title: Increasing LLM Coding Capabilities through Diverse Synthetic Coding Tasks
arxiv_id: '2510.23208'
source_url: https://arxiv.org/abs/2510.23208
tags:
- reasoning
- code
- dataset
- generation
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a synthetic data generation pipeline that produces
  nearly 800k instruction-reasoning-code-test quadruplets to improve LLM coding capabilities.
  The pipeline combines curated contest problems, web-mined content filtered by relevance
  classifiers, and a genetic mutation algorithm to expand task diversity while maintaining
  consistency.
---

# Increasing LLM Coding Capabilities through Diverse Synthetic Coding Tasks

## Quick Facts
- arXiv ID: 2510.23208
- Source URL: https://arxiv.org/abs/2510.23208
- Authors: Amal Abed; Ivan Lukic; Jörg K. H. Franke; Frank Hutter
- Reference count: 10
- Fine-tuning phi-2 on synthetic dataset achieved 56.1% HumanEval, 65.6% MBPP

## Executive Summary
This paper presents a synthetic data generation pipeline that produces nearly 800k instruction-reasoning-code-test quadruplets to improve LLM coding capabilities. The pipeline combines curated contest problems, web-mined content filtered by relevance classifiers, and a genetic mutation algorithm to expand task diversity while maintaining consistency. The resulting dataset is validated through execution-based testing. Fine-tuning phi-2 on this dataset achieved 56.1% on HumanEval and 65.6% on MBPP, representing nearly +10 absolute points over baseline. The approach demonstrates that reasoning-augmented synthetic data can substitute for model scaling, generalize across architectures, and outperform leading open-source alternatives under identical sample budgets.

## Method Summary
The method uses a multi-stage pipeline: curated seed problems from coding contests and web mining are expanded through a genetic algorithm that applies crossover and mutation operators to create diverse tasks. Each instruction generates quadruplets (instruction, reasoning, code, tests) using Qwen2.5-Coder-7B-Instruct, with execution-based validation in isolated containers filtering for functional correctness. A judge LLM (Gemma-3-27B-IT) performs quality gating before deduplication using FAISS and LLM verification. The final dataset is used for QLoRA fine-tuning on phi-2, achieving state-of-the-art results for its size class.

## Key Results
- Fine-tuning phi-2 on 25k synthetic samples achieved 56.1% HumanEval, 65.6% MBPP
- Synthetic dataset outperforms open-source alternatives (Qwen2.5-Coder-14B-Instruct, DeepSeek-Coder-6.7B) on identical sample budgets
- Diverse 5k subsets outperformed homogeneous 5k subsets (56.7% vs 50.0% HumanEval)
- Pipeline generated 800k quadruplets in 48 hours on 2×8xH100 GPUs
- Dataset shows no contamination with HumanEval, MBPP, or APPS benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Training on instruction-reasoning-code-test quadruplets improves code generation more than instruction-code pairs alone. The reasoning trace provides explicit supervision for the "how" of problem solving, not just the "what," creating a richer gradient signal that helps the model internalize structured problem decomposition and algorithm selection.

### Mechanism 2
Genetic mutation and crossover operators on validated instructions increase task diversity without sacrificing functional correctness. By recombining constraints, objectives, and reasoning patterns from parent tasks, the pipeline generates novel problems that remain solvable, expanding the problem distribution beyond curated seeds.

### Mechanism 3
Execution-based validation with multi-candidate refinement is necessary to ensure reasoning traces and code are consistent and correct. Generating three candidate solution-test pairs per instruction and executing them in isolated containers filters out hallucinated or non-terminating code, creating a hard correctness constraint that synthetic data pipelines without execution lack.

## Foundational Learning

- **QLoRA (Quantized Low-Rank Adaptation)**
  - Why needed here: All fine-tuning experiments use QLoRA with r=16, α=16. Understanding low-rank updates is necessary to reproduce results and run ablations.
  - Quick check question: Can you explain why QLoRA reduces memory overhead compared to full fine-tuning?

- **Execution sandboxing (Apptainer/Singularity containers)**
  - Why needed here: The pipeline executes generated code in isolated containers with resource limits. Safe execution is critical to prevent runaway or malicious code from crashing the pipeline.
  - Quick check question: How would you configure a container to enforce CPU, memory, and wall-clock limits on untrusted Python code?

- **Approximate nearest-neighbor search (FAISS + sentence embeddings)**
  - Why needed here: Deduplication uses MiniLM-L6-v2 embeddings and FAISS to detect near-duplicates at 800k scale. This is necessary to preserve diversity.
  - Quick check question: Why is cosine similarity thresholding alone insufficient for deduplication, and why does the paper add an LLM verifier?

## Architecture Onboarding

- **Component map:**
  Seed sources -> Quadruplet generator -> Validator -> Expander -> Deduplicator -> Trainer

- **Critical path:**
  1. Seed problem ingestion and classifier-guided web mining
  2. LLM-based quadruplet generation (reasoning + code + tests)
  3. Execution validation (discard failures)
  4. Genetic expansion (crossover/mutation → validation → judge acceptance)
  5. Deduplication and decontamination (remove redundancy, check benchmark leakage)
  6. QLoRA fine-tuning and EvalPlus benchmarking

- **Design tradeoffs:**
  - Qwen2.5-Coder-7B vs larger models: 7B chosen for efficiency; larger models may improve quality at 4–5× compute cost
  - 3 candidates per instruction: Increases retention rate but multiplies generation cost
  - 90% relevance threshold for web mining: High precision, but may exclude useful edge-case material
  - Deduplication at 0.90 cosine + LLM verify: Aggressive dedup preserves diversity but risks over-merging distinct problems

- **Failure signatures:**
  - High discard rate during validation (>50%): LLM is generating poor code/tests
  - Benchmark performance plateaus early: Dataset may lack diversity
  - Model overfits to synthetic style: Evaluate on held-out real-world coding tasks
  - Leakage detection triggers: Hash-based decontamination missed similar-but-not-identical problems

- **First 3 experiments:**
  1. Reproduce baseline: Fine-tune phi-2 on 5k synthetic samples; verify HumanEval/MBPP scores within reported ranges
  2. Ablate reasoning traces: Train on instruction-code-test triplets (drop reasoning); compare against quadruplet training
  3. Test scaling: Train on 5k, 10k, 25k subsets and plot learning curve; verify 25k matches reported ~56.1% HumanEval

## Open Questions the Paper Calls Out

### Open Question 1
Can the execution-based validation framework be effectively adapted to support multilingual programming languages (e.g., Java, C++, JavaScript) while maintaining functional correctness guarantees? The current pipeline supports only Python and would require substantial infrastructure changes for other languages.

### Open Question 2
What performance gains would result from deploying this synthetic dataset at pretraining scale rather than only for fine-tuning? All experiments use fine-tuning with QLoRA on already-pretrained models.

### Open Question 3
How does the quality of generated quadruplets scale with the capability of the generator LLM, and what is the cost-quality tradeoff? Only Qwen2.5-Coder-7B-Instruct was used, without systematic comparison to larger generators.

### Open Question 4
Does reasoning-augmented fine-tuning transfer to real-world software engineering tasks beyond competitive programming benchmarks? Evaluation is limited to HumanEval and MBPP, which focus on short, self-contained algorithmic problems.

## Limitations
- Heavy dependence on correctness of synthetic reasoning traces, which are difficult to verify automatically
- Pipeline's scalability claims depend on execution-based validation remaining efficient at scale
- Generalization claims across architectures based on single fine-tuning runs per architecture
- Dataset may overfit to competitive programming style rather than real-world software engineering

## Confidence
- **High confidence:** Execution-based validation mechanism works as described; reported HumanEval/MBPP scores are reproducible
- **Medium confidence:** Reasoning trace contribution to performance gains is real but difficult to precisely quantify
- **Low confidence:** Exact contribution of genetic expansion versus base curated seed quality is unclear

## Next Checks
1. Ablation study on reasoning traces: Fine-tune phi-2 on same 25k samples with reasoning traces removed; compare against full quadruplet training
2. Leakage and contamination audit: Run contamination detection on final 800k dataset against HumanEval, MBPP, and APPS; report exact matches found
3. Mutation parameter sensitivity: Vary genetic mutation parameters and measure impact on HumanEval performance using fixed-size 10k subsets