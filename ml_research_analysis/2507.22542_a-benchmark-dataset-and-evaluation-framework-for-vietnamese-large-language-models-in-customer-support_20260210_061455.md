---
ver: rpa2
title: A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language
  Models in Customer Support
arxiv_id: '2507.22542'
source_url: https://arxiv.org/abs/2507.22542
tags:
- nguyen
- villms
- vietnamese
- customer
- csconda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CSConDa, the first large-scale Vietnamese
  dataset for customer support QA, addressing the gap in domain-specific benchmarks
  for evaluating Vietnamese LLMs. The dataset comprises over 9,000 QA pairs derived
  from real customer interactions, annotated and categorized into three types (General,
  Simple, Complex) based on conversational complexity.
---

# A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language Models in Customer Support

## Quick Facts
- arXiv ID: 2507.22542
- Source URL: https://arxiv.org/abs/2507.22542
- Authors: Long S. T. Nguyen; Truong P. Hua; Thanh M. Nguyen; Toan Q. Pham; Nam K. Ngo; An X. Nguyen; Nghi D. M. Pham; Nghia H. Nguyen; Tho T. Quan
- Reference count: 33
- Key outcome: Introduces CSConDa, first large-scale Vietnamese dataset for customer support QA, revealing lightweight ViLLMs struggle with conversational fluency despite adequate semantic understanding

## Executive Summary
This paper addresses the critical gap in Vietnamese domain-specific benchmarks by introducing CSConDa, a dataset of over 9,000 QA pairs from real customer support interactions. The authors evaluate 11 lightweight Vietnamese LLMs using both traditional and syntactic metrics, revealing that while models achieve reasonable semantic scores, they consistently produce verbose, structurally rigid responses that differ significantly from human conversational patterns. The study establishes a comprehensive evaluation framework that penalizes models for generation failures and provides evidence that current ViLLMs require structurally-aware fine-tuning to improve customer support performance.

## Method Summary
The authors create CSConDa from real customer-advisor logs, preserving informal linguistic artifacts like teencode and typos. They evaluate 11 lightweight ViLLMs (7-9B parameters) on a 1,500-question test split using zero-shot inference on a single NVIDIA A100 (40GB). The evaluation combines traditional metrics (BLEU-2, ROUGE-L, METEOR, BERTScore, Cosine Similarity, Hallucination Score via GPT-4) with syntactic analysis (Word Count, POS Ratio, Phrase Ratio, Named Entity Difference, Dependency Length) to assess both semantic alignment and structural properties of generated responses.

## Key Results
- Vistral 7B and SeaLLMs 7B achieve highest overall performance but all ViLLMs struggle with concise, human-like responses
- Models exhibit significantly higher dependency lengths than humans, indicating structural rigidity and potential hallucination risks
- All lightweight ViLLMs fail to match human brevity, with generated responses showing inflated word counts and complex syntactic structures
- The penalty factor mechanism effectively identifies models that fail to generate valid responses for difficult inputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Domain-specific evaluation requires datasets containing informal linguistic artifacts (teencode, typos) rather than structured text to accurately reflect real-world model performance.
- **Mechanism:** The CSConDa dataset curates QA pairs from raw customer-advisor logs, preserving high-frequency abbreviations (0.10) and typos (0.02) typically sanitized in Wikipedia-derived datasets. This forces models to demonstrate robustness to noise rather than just formal grammar.
- **Core assumption:** Customer support quality is heavily dependent on handling informal input, and models performing well on clean text will not necessarily transfer to noisy chat environments.
- **Evidence anchors:** [abstract] "...queries frequently include teencode, abbreviations, and domain-specific jargon." [Section 3.3] Comparison table showing CSConDa is the only dataset with checked boxes for Abbreviations, Acronyms, and Typos.

### Mechanism 2
- **Claim:** Syntactic analysis (dependency length and phrase ratios) serves as a proxy for detecting "semantic drift" and non-human-like rigidity in generated responses.
- **Mechanism:** The authors utilize dependency parsing to measure the average distance between heads and dependents. They find that while models achieve acceptable semantic scores (BERTScore), they exhibit significantly higher dependency lengths than humans, indicating overly complex, rigid sentence structures that correlate with hallucination risks.
- **Core assumption:** Human-like brevity and structural flexibility are negatively correlated with hallucination frequency in customer support contexts.
- **Evidence anchors:** [abstract] "...syntactic analysis to reveal model strengths, weaknesses, and linguistic patterns." [Section 4.3] "ViLLMs Are Verbose and Structurally Inflated... verbosity induces semantic drift, reducing alignment with the original query."

### Mechanism 3
- **Claim:** A penalty factor based on the ratio of successfully generated answers prevents over-estimation of models that fail to respond to difficult inputs.
- **Mechanism:** The evaluation framework applies a penalty ρ = (A/N)^Mc to the final score. If a model fails to generate a valid response for a subset of the test set (A < N), the metric score is scaled down proportionally, ensuring reliability is factored into the quality score.
- **Core assumption:** A non-response or generation loop is a critical failure mode that is numerically distinct from a low-quality response.
- **Evidence anchors:** [Section 4.1] "To address this, we introduce a penalty factor ρ in the final metric computation." [Section 4.1] "...ensures that models failing to generate valid responses are fairly penalized."

## Foundational Learning

- **Concept: Vietnamese "Teencode" and Word Segmentation**
  - **Why needed here:** Standard tokenizers often split informal Vietnamese abbreviations (e.g., "ko" for "không") into meaningless sub-words, degrading model comprehension.
  - **Quick check question:** How does the frequency of out-of-vocabulary (OOV) tokens in the CSConDa dataset likely compare to a Wikipedia-based Vietnamese dataset?

- **Concept: Extractive vs. Abstractive Evaluation**
  - **Why needed here:** The paper critiques existing datasets for being "extractive" (copying answers from text) whereas customer support is "conversational" (generating new responses). Metrics like BLEU favor extraction; semantic metrics favor abstraction.
  - **Quick check question:** Why would a high BLEU score on a customer support dataset actually indicate a *lack* of conversational ability?

- **Concept: Syntactic Dependency Parsing**
  - **Why needed here:** The authors use "Dependency Length" to quantify structural complexity. Understanding that a longer dependency distance implies greater cognitive load or syntactic rigidity is essential for interpreting Figure 6-9.
  - **Quick check question:** If Model A has a higher average dependency length than Model B, what does this imply about the structural complexity of Model A's outputs?

## Architecture Onboarding

- **Component map:** CSConDa Test Split -> Target ViLLM -> Vietnamese NLP Toolkits -> Hybrid Metric Calculator
- **Critical path:**
  1. Data Sanitization: Ensure inputs retain noise (typos) but remove PII
  2. Inference: Run zero-shot generation; log failures/timeouts separately
  3. Structural Analysis: Compute POS/Dependency ratios before semantic scoring to diagnose failure modes
- **Design tradeoffs:**
  - Zero-shot vs. RAG: The paper evaluates intrinsic capability (zero-shot). Do not use RAG for this specific benchmark, as it masks the model's internal knowledge gaps
  - Metric Selection: Use BERTScore for semantic alignment and Dependency Length for fluency/structure. Relying solely on BLEU/ROUGE will miss the "human-like" quality requirement
- **Failure signatures:**
  - Semantic Drift: High word count + High Hallucination Score (model rambles and invents features)
  - Structural Rigidity: High Dependency Length + Low Lexical Diversity (model sounds robotic)
  - Input Brittle-ness: High failure rate (A/N penalty) on "Complex" type questions containing mixed English/Vietnamese code-switching
- **First 3 experiments:**
  1. Baseline Validation: Run the top performer (Vistral 7B) on the "General" test split to replicate the BERTScore (~0.65)
  2. Stress Test: Evaluate the model on the "Complex" split to measure the delta between BERTScore and Hallucination Score
  3. Efficiency Analysis: Plot "Word Count" vs. "Dependency Length" for human answers vs. model answers to quantify the "verbose and rigid" failure mode

## Open Questions the Paper Calls Out

- **Question:** Can CSConDa be effectively augmented with contextual information to improve model performance without sacrificing the dataset's task-oriented realism?
  - **Basis in paper:** [explicit] The authors state in the conclusion that "Future work could enhance CSConDa by incorporating contextual information... ensuring such extensions preserve its realism."
  - **Why unresolved:** The current study utilized the dataset primarily for intrinsic evaluation without external context, leaving the balance between adding information and maintaining real-world representation unexplored.
  - **What evidence would resolve it:** A study benchmarking ViLLMs on a context-augmented version of CSConDa, showing improved metrics while maintaining linguistic authenticity.

- **Question:** What specific fine-tuning methodologies can successfully reduce the structural rigidity and verbosity observed in lightweight ViLLMs?
  - **Basis in paper:** [explicit] The discussion concludes that "Addressing these limitations requires a refined fine-tuning approach that enhances structural efficiency, promotes syntactic adaptability, and optimizes linguistic economy."
  - **Why unresolved:** The paper identifies that current models produce longer, structurally complex responses compared to humans, but only proposes the need for "structurally-aware" training without testing a solution.
  - **What evidence would resolve it:** Experiments applying structure-constrained loss functions or specialized rewards to reduce dependency length and word count to human levels.

- **Question:** How does the integration of external knowledge sources (RAG) impact the hallucination rates and factual accuracy of lightweight ViLLMs on this benchmark?
  - **Basis in paper:** [inferred] The limitation section notes the focus is "solely on intrinsic model capabilities," and the introduction mentions evaluating reliability "before applying external knowledge augmentation."
  - **Why unresolved:** The study isolates base model performance, but real-world deployment typically involves RAG; the interaction between the dataset's informal queries and external retrieval remains untested.
  - **What evidence would resolve it:** Comparative benchmarks showing hallucination scores (Hallu. Score) and semantic similarity (BERTScore) on CSConDa with and without a retrieval component.

## Limitations

- **Metric Validity Concerns:** The study relies heavily on reference-based metrics (BLEU, ROUGE, METEOR) for evaluating conversational responses, which may not accurately capture the quality of abstractive generation
- **Dataset Representativeness:** CSConDa derives from one e-commerce platform's customer interactions, potentially limiting generalizability across different Vietnamese customer support domains
- **Causal Relationship Uncertainty:** The paper interprets higher dependency lengths as evidence of structural rigidity and potential hallucination, but this causal relationship isn't empirically validated

## Confidence

- **CSConDa fills a critical benchmark gap:** High - Multiple papers cited establish the absence of Vietnamese domain-specific benchmarks
- **Lightweight ViLLMs underperform on customer support:** High - Clear quantitative evidence across all tested models
- **Syntactic rigidity correlates with hallucination:** Medium - Theoretical mechanism proposed but not empirically validated in this study
- **Noise robustness is essential for real-world deployment:** Medium - Assumption-driven; would benefit from A/B testing in actual customer support environments

## Next Checks

1. **Human Evaluation Correlation Study:** Conduct a small-scale human judgment experiment comparing model outputs to reference-based metric scores to validate whether BLEU/ROUGE actually predict conversational quality in the Vietnamese customer support context

2. **Domain Transferability Test:** Evaluate the same ViLLM models on CSConDa and then on a different Vietnamese customer support dataset (if available) or a different e-commerce platform's data to measure domain-specific vs. general conversational ability

3. **Controlled Dependency Length Experiment:** Create a synthetic test set where the same semantic content is expressed with varying syntactic complexity, then measure whether higher dependency lengths actually correlate with increased hallucination scores across models