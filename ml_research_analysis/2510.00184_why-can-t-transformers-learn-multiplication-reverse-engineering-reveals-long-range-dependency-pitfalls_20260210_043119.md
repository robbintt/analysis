---
ver: rpa2
title: Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range
  Dependency Pitfalls
arxiv_id: '2510.00184'
source_url: https://arxiv.org/abs/2510.00184
tags:
- layer
- head
- icot
- attention
- long-range
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates why Transformers fail to learn multi-digit
  multiplication, a task that seems simple yet remains challenging for large language
  models. The authors compare a standard fine-tuned model, which fails, with a model
  trained using implicit chain-of-thought (ICoT), which succeeds.
---

# Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls

## Quick Facts
- arXiv ID: 2510.00184
- Source URL: https://arxiv.org/abs/2510.00184
- Reference count: 21
- Standard fine-tuned Transformers fail at multi-digit multiplication due to a lack of long-range dependencies, but models trained with implicit chain-of-thought (ICoT) succeed by encoding intermediate states and using sparse attention DAGs.

## Executive Summary
This paper investigates why Transformers struggle with multi-digit multiplication despite its apparent simplicity. Through reverse-engineering a successful model trained with implicit chain-of-thought (ICoT), the authors uncover that multiplication requires encoding long-range dependencies and constructing a sparse directed acyclic graph (DAG) to cache and retrieve partial products. In contrast, standard fine-tuning converges to a local optimum where middle digits never receive proper gradients, preventing the formation of these dependencies. The paper demonstrates that introducing an auxiliary loss to predict intermediate "running sums" provides the necessary inductive bias to overcome this pitfall, enabling standard models to achieve near-perfect accuracy without ICoT.

## Method Summary
The authors compare a standard fine-tuned (SFT) model that fails at 4-digit multiplication with an ICoT-trained model that succeeds. They use logit attribution, linear regression probes, and PCA to analyze hidden states, revealing that the ICoT model encodes intermediate sums and uses attention to form a DAG-like structure for partial product aggregation. They also identify that digits are represented using a Fourier basis forming a pentagonal prism geometry. To address the SFT failure, they introduce an auxiliary loss predicting the "running sum" intermediate variable, which provides the inductive bias needed for successful learning.

## Key Results
- ICoT models succeed at multiplication by encoding long-range dependencies and constructing DAGs to cache/retrieve partial products, unlike standard fine-tuned models.
- Mechanistically, successful models use attention to build sparse binary-tree-like graphs for partial product aggregation and represent digits using Fourier bases and Minkowski sums.
- Standard fine-tuning fails due to gradient starvation where middle digits receive insufficient gradient signal to form long-range dependencies, locking the model in a local optimum.
- An auxiliary loss predicting the "running sum" intermediate variable provides the inductive bias needed for standard models to achieve near-perfect multiplication accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transformers can solve multi-digit multiplication by organizing attention into a sparse Directed Acyclic Graph (DAG) to cache and retrieve partial products.
- **Mechanism:** In successful models (ICoT), Layer 1 attention heads focus on specific digit pairs $(a_i, b_j)$ to compute and "cache" partial products into hidden states. Layer 2 heads then act as "retrievers," attending to these specific cache locations rather than the raw digits, effectively constructing a binary-tree-like structure to aggregate necessary terms for the final output.
- **Core assumption:** The model has sufficient depth and head count to sparsely isolate digit pairs before aggregation; specifically, a 2-layer, 4-head architecture is cited as the minimal viable configuration.
- **Evidence anchors:**
  - [abstract]: "...encodes long-range dependencies using attention to construct a directed acyclic graph to 'cache' and 'retrieve' pairwise partial products."
  - [section 3.3]: "Attention heads $ATT^2_3, ATT^2_4$ each attend to positions... Inspecting what was 'cached' in the first layer at those timesteps reveals the necessary partial products..."
  - [corpus]: Corpus evidence on "locality bias" in Long Range Arena (arXiv:2501.14850) supports the general difficulty Transformers face with non-local dependencies without such structural intervention.
- **Break condition:** If attention heads become dense (attending uniformly) or if the sequence length exceeds the capacity of the "tree" structure to maintain distinct cache sites without interference.

### Mechanism 2
- **Claim:** Efficient multiplication relies on representing digits as Minkowski sums and Fourier bases to lower the computational complexity of the product operation.
- **Mechanism:** Digit embeddings are structured such that the attention output approximates a Minkowski sum ($\alpha A + (1-\alpha)B$). Furthermore, digits are encoded using a Fourier basis (specifically frequencies $k \in \{0, 1, 2, 5\}$), forming a "pentagonal prism" geometry where modular arithmetic (mod 10) maps to rotations in this space.
- **Core assumption:** The model learns to project digits into a subspace where arithmetic operations map to linear or geometric transformations.
- **Evidence anchors:**
  - [abstract]: "...implements partial products in attention heads by forming Minkowski sums between pairs of digits..."
  - [section 4.2]: "The first principal component (PC1) aligns with the parity vector... Second and third principal components span the $k=2$ Fourier pair... yielding the pentagonal-prism geometry..."
  - [corpus]: Related work (arXiv:2602.02385) suggests Transformers generally learn "factored representations" in orthogonal subspaces, aligning with the separation of digit features here.
- **Break condition:** If regularization or initialization prevents the formation of the specific periodicity required for the Fourier basis (e.g., random projection initialization dominates).

### Mechanism 3
- **Claim:** Standard fine-tuning (SFT) fails due to a "gradient starvation" pitfall where middle digits never receive sufficient gradient signal to form long-range dependencies.
- **Mechanism:** In SFT, the loss drops rapidly for easy tokens (start/end digits), causing gradient norms for these positions to vanish. Middle digits, which require complex long-range dependencies, stop receiving gradients before these dependencies can form, locking the model in a local optimum.
- **Core assumption:** The default Adam/SGD dynamics with autoregressive loss prioritize short-range or easy-to-fit patterns over structural algorithmic ones.
- **Evidence anchors:**
  - [abstract]: "...standard fine-tuning converges to a local optimum lacking required long-range dependencies, with middle digits never receiving proper gradients."
  - [section 5]: "Despite only the middle digits receiving gradients... their losses plateau, suggesting that the model is stuck in a local optimum..."
  - [corpus]: arXiv:2410.15580 ("Language Models are Symbolic Learners...") supports the idea that models prefer "greedy" shortcuts (pattern matching) over algorithmic computation when available.
- **Break condition:** If an auxiliary loss or specific inductive bias is introduced to explicitly supervise the intermediate "running sum," forcing gradients to flow through the dependency chain.

## Foundational Learning

- **Concept: Implicit Chain-of-Thought (ICoT)**
  - **Why needed here:** This is the training method that successfully induces the DAG mechanism. It acts as a bridge between explicit scratchpads and latent reasoning.
  - **Quick check question:** Can you explain how ICoT differs from standard fine-tuning regarding the visibility of intermediate reasoning steps during training?

- **Concept: Logit Attribution & Linear Probing**
  - **Why needed here:** These are the primary diagnostic tools used to verify that the model is actually computing intermediate sums ($\hat{c}_k$) rather than memorizing lookup tables.
  - **Quick check question:** If a linear probe can predict the "running sum" $\hat{c}_k$ from hidden states with low error, what does that imply about the information encoded in that layer?

- **Concept: Inductive Bias via Auxiliary Loss**
  - **Why needed here:** This is the proposed solution to the SFT failure mode. It forces the model to compute intermediate states.
  - **Quick check question:** Why does adding a regression head to predict the "running sum" help the model learn the multiplication task itself?

## Architecture Onboarding

- **Component map:** Input (4-digit operands) -> Layer 1 Attention (Cache: sparse attention to digit pairs) -> Layer 2 Attention (Retrieve: aggregate partial sums) -> Output (8-digit product) -> Aux Head (Optional: predict running sum)

- **Critical path:** The path flows from Input $\to$ Layer 1 Attention (where Minkowski sums form) $\to$ Layer 2 Attention (where aggregation occurs) $\to$ Aux Head (if used). If Layer 1 fails to sparsify (i.e., attends to everything), the specific partial products are not cached, and Layer 2 cannot retrieve the correct combinations.

- **Design tradeoffs:**
  - **ICoT vs. SFT:** ICoT requires specialized curriculum learning (gradual token removal) but works reliably. SFT is simpler but fails on this task without the auxiliary loss modification.
  - **Architecture Size:** A 2-layer, 4-head model is sufficient; scaling to 12 layers does not resolve the SFT failure, indicating a learning dynamics issue rather than a capacity issue.

- **Failure signatures:**
  - **The "Middle Digit" Plateau:** Loss curves show $c_0, c_1$ and $c_7$ dropping to zero, while $c_3, c_4, c_5, c_6$ remain high.
  - **Dense Attention:** Failed models show attention heads attending broadly/uniformly rather than the sparse "binary tree" pattern seen in ICoT.
  - **Lack of Fourier Structure:** PCA of hidden states in failed models does not reveal the pentagonal prism geometry.

- **First 3 experiments:**
  1. **Baseline Gradient Analysis:** Train a standard 2-layer GPT on 4x4 multiplication. Plot gradient norms per token position to confirm the "middle digit" starvation effect described in Section 5.
  2. **Auxiliary Loss Ablation:** Implement the auxiliary loss ($L_{aux}$) predicting $\hat{c}_k$ on the baseline architecture. Verify if this unlocks >90% accuracy without ICoT.
  3. **Probing for Algorithmic State:** Train linear probes on the hidden states of the successful model to see if the intermediate running sum $\hat{c}_k$ is linearly recoverable (Section 3.2), confirming the algorithm is internalized.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a generic, task-agnostic inductive bias be developed to resolve long-range dependency pitfalls in Transformers without requiring task-specific auxiliary losses?
- Basis in paper: [explicit] The authors state they "speculate that there are other generalizing inductive biases... and leave this for future work."
- Why unresolved: The proposed "running sum" auxiliary loss is tailored specifically to the mechanics of multiplication.
- What evidence would resolve it: Identifying a regularization term or architectural modification that improves performance on diverse tasks (e.g., addition, sorting) requiring long-range dependencies.

### Open Question 2
- Question: Do large pre-trained models fail at multiplication due to the same lack of long-range dependency structures observed in the from-scratch models?
- Basis in paper: [inferred] The analysis is conducted on 2-layer models trained from scratch to remove confounding factors, though the introduction notes failures in models like Llama-3.
- Why unresolved: The mechanism of failure (local optimum lacking gradient flow to middle digits) is demonstrated only on small, randomly initialized networks.
- What evidence would resolve it: Probing the hidden states of large pre-trained models (e.g., Llama, GPT) for the intermediate variable $\hat{c}_k$ to confirm the absence of long-range dependencies.

### Open Question 3
- Question: Is the local optimum encountered during standard fine-tuning escapable through curriculum learning or specific weight initialization strategies?
- Basis in paper: [inferred] The paper identifies that standard training converges to a local optimum where middle digits receive no gradient, but does not explore non-auxiliary-loss interventions.
- Why unresolved: The solution provided relies on altering the loss function, leaving the potential for optimization-centric solutions (like curriculum learning) untested.
- What evidence would resolve it: Demonstrating that a specific learning rate schedule or initialization allows standard fine-tuning to escape the plateau and achieve high accuracy.

## Limitations
- The findings are based on a controlled setting with 4-digit multiplication, which may not generalize to arbitrary precision or other arithmetic operations.
- The mechanisms identified (DAG structure, Fourier encoding, Minkowski sums) are highly specific to the task and model architecture studied.
- The reliance on specialized training methods like ICoT raises questions about practical applicability without extensive hyperparameter tuning.

## Confidence
- **High Confidence:** The identification of gradient starvation as a cause of failure in standard fine-tuning is well-supported by empirical evidence and aligns with known optimization challenges in deep learning.
- **Medium Confidence:** The mechanistic claims about DAG construction and Fourier basis encoding are derived from detailed analysis but rely on specific architectural choices (2-layer, 4-head) that may not be universally optimal.
- **Medium Confidence:** The auxiliary loss solution is demonstrated to work in this setting, but its effectiveness across different model scales and tasks remains to be validated.

## Next Checks
1. **Generalization Test:** Evaluate the ICoT and auxiliary-loss models on multiplication tasks with variable digit lengths (e.g., 2x6, 8x8) to test robustness beyond the 4-digit case.
2. **Mechanistic Probe Transfer:** Apply the same linear probing and PCA analysis to models trained on related tasks (e.g., addition, subtraction) to determine if similar Fourier or DAG structures emerge.
3. **Architecture Scaling Study:** Test whether the identified mechanisms persist or change when scaling the model to more layers (e.g., 12-layer) or different attention patterns (e.g., local window vs. full attention).