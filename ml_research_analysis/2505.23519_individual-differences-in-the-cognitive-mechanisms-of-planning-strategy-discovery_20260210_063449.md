---
ver: rpa2
title: Individual differences in the cognitive mechanisms of planning strategy discovery
arxiv_id: '2505.23519'
source_url: https://arxiv.org/abs/2505.23519
tags:
- planning
- learning
- participants
- strategy
- mechanisms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated individual differences in cognitive mechanisms
  underlying the discovery of planning strategies through metacognitive reinforcement
  learning (MCRL). Building on prior work showing that MCRL models explain human strategy
  discovery but lag behind actual human performance, the researchers enhanced a metacognitive
  hybrid Reinforce model with three additional mechanisms: intrinsically generated
  metacognitive pseudo-rewards, subjective effort valuation, and termination deliberation.'
---

# Individual differences in the cognitive mechanisms of planning strategy discovery

## Quick Facts
- arXiv ID: 2505.23519
- Source URL: https://arxiv.org/abs/2505.23519
- Reference count: 16
- Primary result: ~70% of participants used at least one of three additional cognitive mechanisms (pseudo-rewards, subjective effort valuation, termination deliberation) when discovering planning strategies, with significant individual differences in their usage.

## Executive Summary
This study investigated individual differences in cognitive mechanisms underlying the discovery of planning strategies through metacognitive reinforcement learning (MCRL). Building on prior work showing that MCRL models explain human strategy discovery but lag behind actual human performance, the researchers enhanced a metacognitive hybrid Reinforce model with three additional mechanisms: intrinsically generated metacognitive pseudo-rewards, subjective effort valuation, and termination deliberation. When analyzing planning task data, approximately 70% of participants used at least one of these mechanisms, with significant individual differences in their usage. Participants whose behavior was best explained by models incorporating pseudo-rewards or subjective effort valuation showed significantly higher performance scores and engaged in more planning operations, while those using termination deliberation performed worse. However, despite these insights into individual differences, none of the enhanced model variants significantly closed the performance gap between model and human strategy discovery rates.

## Method Summary
The study used a planning task where 349 participants discovered adaptive planning strategies over 120 trials. The researchers implemented a metacognitive hybrid Reinforce model with 63 features representing planning strategies, then added three optional mechanisms: pseudo-rewards (PR), subjective effort valuation (SE), and termination deliberation (TD). They fitted eight model variants to each participant's click sequence data using Bayesian optimization with 60,000 iterations. Model selection was performed using BIC (lower = better) and family-level Bayesian model selection with exceedance probability. The primary metrics were model likelihood, strategy discovery rate, planning operation count, and performance scores.

## Key Results
- ~70% of participants used at least one of the three additional cognitive mechanisms (PR, SE, or TD)
- Participants best explained by PR or SE models scored significantly higher and performed more planning operations than those using plain Reinforce
- TD participants performed worse, terminating planning early (0.43 operations vs. 1.70 for non-TD, p < .001)
- None of the enhanced model variants significantly closed the performance gap between model and human strategy discovery rates

## Why This Works (Mechanism)

### Mechanism 1: Metacognitive Pseudo-Rewards (PR)
- **Claim:** Intrinsically generated pseudo-rewards may accelerate strategy discovery when external feedback is sparse, but only for a subset of individuals (~30%).
- **Mechanism:** Computes the difference in expected value of the optimal path before and after a mental state transition: `PR(bt, c, bt+1) = E[R^π_bt+1|bt+1] - E[R^π_bt|bt+1]`. This internal signal supplements external rewards for learning.
- **Core assumption:** The brain generates self-evaluative signals to fill feedback gaps in metacognitive learning.
- **Evidence anchors:** [abstract] "Metacognitive pseudo-rewards... were found to facilitate strategy discovery"; [section 3.4] "PR was added to the metacognitive Reinforce model to quantify the value of information from recent planning operations"
- **Break condition:** When external rewards are already dense, or when computational cost of generating pseudo-rewards exceeds learning benefit.

### Mechanism 2: Subjective Effort Valuation (SE)
- **Claim:** Individual differences in how effort is perceived affect planning engagement and strategy discovery success.
- **Mechanism:** Adds a free hyperparameter modulating the subjective (un)pleasantness of cognitive effort, individualizing the cost term in the reward signal.
- **Core assumption:** Effort aversion/seeking exists on a continuous spectrum across individuals.
- **Evidence anchors:** [abstract] "subjective effort valuation... facilitate strategy discovery"; [section 3.4] "People vary in how they perceive the effort of processing information... This continuous variation in the (un)pleasantness of effort was not accounted for in the plain Reinforce model"
- **Break condition:** When task incentives dwarf effort costs, or when planning is trivially short.

### Mechanism 3: Termination Deliberation (TD)
- **Claim:** Explicitly computing the value of stopping planning can lead to premature termination and worse outcomes.
- **Mechanism:** After each planning operation, calculates expected return at current belief state and assigns it as the Q-value of the termination action.
- **Core assumption:** People can and should engage in metareasoning about when to stop planning.
- **Evidence anchors:** [section 4] "participants who engaged in TD tended to terminate their planning early, leading to poorer performance"; [Table 1b] TD participants: mean 0.43 planning operations vs. 1.70 for non-TD (p < .001)
- **Break condition:** When additional planning has low marginal value, TD may be adaptive; in this task, it led to under-planning.

## Foundational Learning

- **Concept: Reinforcement Learning basics (Q-values, reward prediction error, policy gradients)**
  - Why needed here: The entire MCRL framework extends RL from external actions to internal cognitive operations.
  - Quick check question: Can you write the Q-learning update rule and explain what each term represents?

- **Concept: Metacognitive MDPs (meta-level decision-making)**
  - Why needed here: Planning strategies are learned in a meta-level MDP where "actions" are cognitive operations and states are belief states.
  - Quick check question: How does a meta-MDP differ from a standard MDP in terms of what constitutes an "action"?

- **Concept: Planning as tree search with resource limits**
  - Why needed here: The task involves discovering efficient strategies for bounded search over decision trees.
  - Quick check question: Why does tree search complexity explode with branching factor and planning horizon?

## Architecture Onboarding

- **Component map:** Base Metacognitive hybrid Reinforce model (63 features) -> Optional modules (PR, SE, TD) -> Softmax over cognitive operations using learned Q-meta values

- **Critical path:**
  1. Represent planning strategies as weighted feature vectors
  2. Update weights via policy gradient after each trial using reward signal (external + optional PR/SE)
  3. Optionally compute TD value for termination after each operation
  4. Fit hyperparameters (α, γ, τ, PR weight, SE weight) per participant via Bayesian optimization (60K iterations)

- **Design tradeoffs:**
  - More mechanisms → better individual-level fit but does not close human-model performance gap
  - PR/SE associated with better performance but used by only 30-35% of participants
  - TD provides computational grounding for termination but empirically led to worse outcomes in this task

- **Failure signatures:**
  - TD participants terminate too early (0.43 ops vs. optimal ~1.67)
  - All model variants discover strategies more slowly than humans
  - Plain Reinforce still explains ~30% of participants better than enhanced variants

- **First 3 experiments:**
  1. Fit plain Reinforce to participant click sequences; establish baseline likelihood and discovery rate gap
  2. Add PR mechanism; test whether PR variants better explain high-performing participants (compare BIC)
  3. Run model simulations with fitted parameters; verify that no variant closes the discovery rate gap to human levels

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What additional cognitive mechanisms beyond pseudo-rewards, subjective effort valuation, and termination deliberation do humans employ to achieve faster strategy discovery than current MCRL models?
- **Basis in paper:** [explicit] "none of the enhanced model variants significantly closed the performance gap between model and human strategy discovery rates" and the abstract states this "prompt[s] further exploration of additional factors that people might use to discover new planning strategies."
- **Why unresolved:** The tested mechanisms explained 70% of participants but still lagged behind human discovery rates; the paper does not propose or test additional candidate mechanisms.
- **What evidence would resolve it:** Identifying and modeling new mechanisms (e.g., analogical transfer, social learning, hierarchical abstraction) that, when added, produce simulated discovery rates matching human performance.

### Open Question 2
- **Question:** Why do only approximately 30-35% of participants employ pseudo-rewards or subjective effort valuation despite their association with higher performance?
- **Basis in paper:** [inferred] The paper found PR and SE participants "scored significantly higher" and "performed significantly more planning operations," yet only 30% and 35% of participants were best explained by these mechanisms respectively.
- **Why unresolved:** The paper speculates these mechanisms "may demand additional cognitive effort, making people hesitant to use them unless the benefits justify the cost" but does not empirically test this hypothesis.
- **What evidence would resolve it:** Studies measuring cognitive effort costs and individual differences in effort tolerance, correlated with mechanism adoption.

### Open Question 3
- **Question:** Why does termination deliberation lead to worse performance when it theoretically should help optimize when to stop planning?
- **Basis in paper:** [inferred] TD participants "performed worse than those who learned the value of termination like the other planning operations" and "tended to terminate their planning early."
- **Why unresolved:** The paper does not explain why a theoretically beneficial mechanism produces counterproductive behavior.
- **What evidence would resolve it:** Analysis of whether TD causes overestimation of current plan value or whether TD users have systematically different task representations.

## Limitations

- Despite incorporating three additional cognitive mechanisms, enhanced models still fail to close the performance gap between human and model strategy discovery rates
- Pseudo-rewards mechanism lacks direct empirical validation beyond this study
- The finding that termination deliberation leads to worse outcomes is counterintuitive and may reflect task-specific properties
- Individual differences analysis relies on model-based classification, inheriting all limitations of the underlying computational models

## Confidence

**High Confidence:** Individual differences exist in cognitive mechanisms used for planning strategy discovery (70% of participants used at least one mechanism). The observation that TD participants performed worse (0.43 vs 1.70 planning operations) is supported by strong statistical evidence (p < .001).

**Medium Confidence:** PR and SE mechanisms facilitate strategy discovery for some individuals. While the direction of effects is clear, the magnitude and generalizability beyond this specific task remain uncertain. The finding that these mechanisms benefit only 30-35% of participants suggests strong individual variation in cognitive architecture.

**Low Confidence:** The claim that TD is universally maladaptive for planning tasks. This conclusion is based on a single task where early termination happened to be suboptimal. TD may be beneficial in tasks with different cost-benefit structures.

## Next Checks

1. **Cross-task validation:** Test whether PR and SE mechanisms show similar individual differences and performance associations in different planning tasks with varying feedback density and complexity.

2. **Ablation study:** Systematically remove each mechanism from high-performing participants' fitted models to verify that PR and SE are causally responsible for their superior performance, not just correlated.

3. **Alternative model comparison:** Compare MCRL models against non-reinforcement learning approaches (e.g., Bayesian learning, rule-based strategies) to ensure the proposed mechanisms are necessary rather than just sufficient explanations for the data.