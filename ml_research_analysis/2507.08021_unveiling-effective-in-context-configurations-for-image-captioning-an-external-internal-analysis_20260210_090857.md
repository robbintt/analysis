---
ver: rpa2
title: 'Unveiling Effective In-Context Configurations for Image Captioning: An External
  & Internal Analysis'
arxiv_id: '2507.08021'
source_url: https://arxiv.org/abs/2507.08021
tags:
- image
- arxiv
- in-context
- attention
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how In-Context Example (ICE) configurations
  affect multimodal Large Language Models (LMMs) on image captioning. The study combines
  external experiments, varying ICE retrieval, caption assignment, and shot numbers,
  with internal attention analysis to quantify model behavior.
---

# Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis

## Quick Facts
- **arXiv ID**: 2507.08021
- **Source URL**: https://arxiv.org/abs/2507.08021
- **Reference count**: 40
- **Key outcome**: External experiments show increasing shots improves linguistic coherence but reduces visual-text alignment, while internal analysis identifies anchor tokens and emergent attention windows in multimodal ICL

## Executive Summary
This paper investigates how In-Context Example (ICE) configurations affect multimodal Large Language Models (LMMs) on image captioning. The study combines external experiments, varying ICE retrieval, caption assignment, and shot numbers, with internal attention analysis to quantify model behavior. Six key findings emerged: increasing shots improves linguistic coherence but reduces visual-text alignment; pre-training corpus biases induce multimodal disparities; ICE quality sensitivity triggers multimetric conflicts; linguistic style imitation undermines visual fidelity; similarity-based retrieval inflates CIDEr scores via shortcuts; and similarity retrieval amplifies ICE caption influence. Internally, anchor token and emergent attention window patterns were identified, and attention-based metrics (ACAR, IEAR, VCAR) were developed. The study also explored lightweight model acceleration via attention pruning. The dual external/internal approach reveals nuanced performance dynamics and offers guidance for future LMM design and training.

## Method Summary
The study uses MSCOCO dataset with Karpathy split and evaluates OpenFlamingo v2-9B and IDEFICS v1-9B models. Configuration strategies include shot numbers (4, 8, 16, 32), image retrieval methods (Random Sampling, Similarity-based Image-to-Image Retrieval using CLIP embeddings), and caption assignment (First Human-Labelled, Machine-Generated Caption from Transformer or LMM, Model-Guided Human-Labelled). The research combines external evaluation using standard captioning metrics (CIDEr, CLIPScore, CHAIR, Short-cut CIDEr) with internal analysis of attention patterns. Attention-based metrics (ACAR, IEAR, VCAR) were developed to quantify model behavior, and attention pruning strategies were explored for lightweight acceleration. Inference was performed on single RTX-3090 GPU with FP16 precision.

## Key Results
- Increasing shot numbers improves linguistic coherence but degrades visual-text alignment, indicating shot augmentation primarily enhances linguistic pattern recognition
- Similarity-based image retrieval (SIIR) inflates CIDEr scores via shortcut pathways, producing nearly identical outputs to ICE captions (Short-cut CIDEr of 1000 vs 2.70 for random sampling)
- Anchor tokens (BOS, image tokens, punctuation marks) emerge as information aggregation points in middle layers, enabling potential 50% KV cache reduction through pruning
- Linguistic dominance causes multimetric conflicts—improvements in CIDEr come at the cost of CLIPScore and increased hallucination
- ICE quality sensitivity triggers conflicts between metrics, with high-quality captions improving fluency but potentially introducing visual grounding issues

## Why This Works (Mechanism)

### Mechanism 1: Anchor Token Information Routing
- Claim: Anchor tokens aggregate information in shallow layers and are disproportionately attended to in deeper layers during multimodal ICL inference
- Mechanism: Special tokens (`<BOS>`, `<image>`, punctuation marks, delimiters) serve as information hubs that collect context early and propagate it forward. This creates an efficient routing pathway where the model doesn't need to compute full attention across all token pairs in deeper layers.
- Core assumption: The identified tokens function as attention sinks that preserve positional stability and enable layer-wise information compression.
- Evidence anchors:
  - [Section III-C1]: "We observe that in the shallow layer (Layer 1), attention interactions are relatively evenly distributed, but in the deeper layer (Layer 3), certain columns appear brighter, indicating increased attention from other tokens."
  - [Section IV-D1]: "ACAR reaches particularly high values in the middle and later layers. The anchor phenomenon is more prominent in deeper layers than in shallower ones."
  - [Corpus]: Related work on attention sinks (StreamingLLM, attention-driven reasoning) supports this pattern in LLMs, though corpus provides weak direct evidence for multimodal-specific anchor behavior.
- Break condition: If masking anchor tokens in middle layers (10-30) causes >20% performance degradation, the mechanism is weakened. The paper shows 2-3% degradation when masking starts at layer 15, but 20%+ when masking layers 10-30.

### Mechanism 2: Cross-Attention Shortcut Pathways
- Claim: Similarity-based image retrieval (SIIR) amplifies the influence of in-context captions through cross-attention shortcut pathways
- Mechanism: When query and ICE images are visually similar, the cross-attention mechanism produces nearly identical visual features. Since captions act as Queries and image features serve as Keys/Values in gated cross-attention, similar images yield similar attention outputs, making the model more likely to copy ICE caption patterns rather than reason from the query image.
- Core assumption: The cross-attention architecture creates a bias toward linguistic shortcuts when visual features are redundant across ICEs.
- Evidence anchors:
  - [Section IV-C3]: "When ICEs and the query image are highly similar, the cross-attention outputs become nearly identical, which can hinder accurate comprehension and reasoning."
  - [Figure 8c]: SIIR setting produces nearly identical output to ICE caption with Short-cut CIDEr of 1000 vs 2.70 for RS.
  - [Corpus]: "What do vision-language models see in the context?" investigates similar ICL sensitivity but does not directly validate the cross-attention shortcut mechanism.
- Break condition: If dissimilar images with identical captions produce equivalent shortcut rates, the mechanism is incorrect. The paper shows SIIR amplifies caption influence for both low and high-quality captions, supporting the visual similarity hypothesis.

### Mechanism 3: Modality Imbalance Dominance
- Claim: Linguistic dominance in LMMs causes multimetric conflicts—improvements in CIDEr come at the cost of CLIPScore and increased hallucination
- Mechanism: Language modules (7B parameters, 1T+ tokens) vastly outscale visual encoders (~400-600M parameters, 400M pairs). This imbalance causes models to prioritize linguistic pattern matching over visual grounding. High-quality ICE captions improve fluency but may introduce linguistic patterns disconnected from visual content.
- Core assumption: Parameter and data scale differences directly cause modality dominance during inference.
- Evidence anchors:
  - [Section IV-B2]: "Language module employs LLaMA/MPT with 7B parameters, pre-trained on 1 trillion tokens. Visual module uses CLIP ViT-L/14 with only 428M parameters."
  - [Section IV-C1]: "Increasing shots improves linguistic coherence but degrades visual-text alignment, indicating shot augmentation primarily enhances linguistic pattern recognition."
  - [Corpus]: Weak corpus evidence for this specific mechanism. "Meta-Adaptive Prompt Distillation" mentions ICL inconsistency in smaller LMMs but doesn't address modality imbalance.
- Break condition: If models with balanced visual/linguistic parameters show no multimetric conflicts, the scale hypothesis is weakened. Not tested in this paper.

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: This paper's core subject. You must understand that ICL allows models to perform tasks using demonstrations without parameter updates.
  - Quick check question: Can you explain why ICE configuration (shot number, retrieval, caption quality) affects ICL performance but doesn't change model weights?

- Concept: Cross-Attention in Multimodal Models
  - Why needed here: The paper's shortcut mechanism depends on understanding how Flamingo-style gated cross-attention integrates visual features into text generation.
  - Quick check question: In cross-attention, which modality provides Queries vs Keys/Values, and how does this affect shortcut behavior?

- Concept: Attention Map Analysis
  - Why needed here: The paper's internal analysis relies on interpreting attention weights to identify anchor tokens and emergent windows.
  - Quick check question: If you see a column with high attention weights across all rows in layer 3 but not layer 1, what does this indicate?

## Architecture Onboarding

- Component map: Input tokens → Vision Encoder (CLIP ViT) → Gated Cross-Attention Dense Block → Language Decoder (LLaMA/MPT) → Generated caption

- Critical path:
  1. ICE retrieval → Caption assignment → Tokenization
  2. Vision encoder processes each image independently
  3. Cross-attention layers integrate visual features into corresponding text tokens
  4. Anchor tokens emerge at `<image>`, punctuation, and delimiters in middle layers
  5. Final prediction attends primarily to anchor tokens and query context

- Design tradeoffs:
  - More shots: Better linguistic fluency, worse visual alignment, O(n²) attention cost
  - SIIR retrieval: Higher CIDEr but increased shortcuts/hallucinations
  - Model-generated captions: Better style consistency but risk propagating model biases
  - Anchor-based pruning: 50% KV cache reduction but limited speedup due to current context length constraints (~1K tokens max)

- Failure signatures:
  - Shortcut behavior: Generated caption nearly identical to ICE caption → check Short-cut CIDEr > 100
  - Hallucination: Caption includes objects not in image → check CHAIR metric
  - Visual grounding failure: High CIDEr but low CLIPScore → VCAR < 1 suggests text-dominant reasoning
  - Long-context collapse: Performance plateaus or degrades beyond 16-32 shots (especially OpenFlamingo)

- First 3 experiments:
  1. **Reproduce ACAR curves**: Extract attention weights from OpenFlamingo on 4-shot RS setting. Compute ACAR per layer. Verify anchor tokens show >10x higher attention values in layers 15-25 vs context tokens. Expected: ACAR peaks at 200-300 in deeper layers.
  2. **Validate SIIR shortcut**: Run RS vs SIIR retrieval on 100 MSCOCO samples with 4-shot FHL. Measure Short-cut CIDEr and VCAR. Expected: SIIR should show >50x higher Short-cut CIDEr and lower VCAR.
  3. **Test anchor-based pruning**: Implement token pruning at layer 15 (retain anchors + prediction tokens, recover at final layer). Measure CIDEr change and KV cache reduction. Expected: <5% CIDEr drop, 40-50% memory reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Will anchor-centric pruning and Emergent Attention Window masking strategies yield significant inference speedups in LMMs with much longer context windows (e.g., >32 shots)?
- **Basis in paper:** [Explicit] The authors note that acceleration effects were limited because current models handle short sequences, but "shows potential... when future LMMs become more capable of handling many-shot scenarios."
- **Why unresolved:** Current LMMs are limited to short context lengths (approx. 1K tokens), preventing the testing of acceleration strategies in "many-shot" regimes where LLMs see benefits.
- **What evidence would resolve it:** Replicating the pruning/masking experiments on future LMMs with native support for significantly longer interleaved contexts.

### Open Question 2
- **Question:** Do the "short-cut" inference behaviors and trade-offs between linguistic coherence and visual-text alignment observed in captioning persist in more complex tasks like Visual Question Answering?
- **Basis in paper:** [Explicit] The authors restrict the case study to Image Captioning for its straightforward format but suggest the methodology "can be applied to broader research areas."
- **Why unresolved:** Image captioning has a simple input-output structure (one image, one text). It is unclear if the identified configuration sensitivities hold when the output space requires reasoning rather than description.
- **What evidence would resolve it:** Applying the defined external metrics (e.g., ShortCut-CIDEr) and internal metrics (VCAR) to VQA or dense captioning tasks.

### Open Question 3
- **Question:** Do the observed internal attention patterns, specifically the reliance on "anchor tokens," generalize to LMM architectures that do not use gated cross-attention (e.g., purely decoder-based models)?
- **Basis in paper:** [Inferred] The study is limited to OpenFlamingo and IDEFICS, which both utilize the Flamingo framework with gated cross-attention dense blocks.
- **Why unresolved:** The identified "anchor tokens" and attention windows might be artifacts of the Flamingo-specific architecture rather than universal properties of multimodal in-context learning.
- **What evidence would resolve it:** Analyzing attention maps of non-Flamingo architectures (like LLaVA or MiniGPT-4) under identical ICE configurations.

## Limitations

- The internal analysis findings (anchor tokens, emergent attention windows) may be model-specific rather than universal across LMM architectures, as the study is limited to OpenFlamingo and IDEFICS models
- The proposed lightweight model acceleration via attention pruning shows limited practical utility with current context length constraints (~1K tokens max), though it may become relevant for future longer-sequence LMMs
- The metric computation reliability for attention-based metrics (ACAR, IEAR, VCAR) depends on indirect estimation methods that may introduce measurement noise

## Confidence

- **High Confidence**: External experimental findings on shot number effects, multimetric conflicts, and similarity-based retrieval biases are based on extensive quantitative experiments across multiple metrics and model variants
- **Medium Confidence**: Anchor token identification and emergent attention window patterns have strong computational evidence but rely on specific model architecture assumptions
- **Low Confidence**: The proposed lightweight model acceleration via attention pruning is acknowledged as having limited current utility and remains uncertain without validation on longer sequences

## Next Checks

1. **Cross-Model Anchor Pattern Validation**: Test whether anchor token patterns (high ACAR in middle layers) appear in other LMM architectures (e.g., IDEFICS, LLaVA, MiniGPT-4). Apply the same ACAR computation to these models on MSCOCO and compare anchor prominence patterns.

2. **Threshold Calibration for Short-cut Detection**: Establish clear thresholds for problematic shortcut behavior by analyzing the relationship between Short-cut CIDEr and human perceptual similarity scores. Compute human ratings for captions with varying Short-cut CIDEr values (0-1000) to determine when copying becomes perceptually obvious.

3. **Long-Context Pruning Evaluation**: Test the proposed attention pruning approach on models with extended context windows (>8K tokens). Implement the anchor-based pruning strategy and measure actual inference speedup vs CIDEr degradation at different context lengths to determine practical utility boundaries.