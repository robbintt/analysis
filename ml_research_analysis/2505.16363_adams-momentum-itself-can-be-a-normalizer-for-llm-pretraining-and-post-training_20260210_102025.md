---
ver: rpa2
title: 'AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training'
arxiv_id: '2505.16363'
source_url: https://arxiv.org/abs/2505.16363
tags:
- adams
- adamw
- learning
- arxiv
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdamS, a memory-efficient optimizer that
  eliminates the need for second-moment estimates by using a novel denominator based
  on the root of weighted sums of squares of momentum and current gradient. AdamS
  achieves the same memory and compute footprint as SGD with momentum while matching
  AdamW's optimization performance.
---

# AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training

## Quick Facts
- arXiv ID: 2505.16363
- Source URL: https://arxiv.org/abs/2505.16363
- Reference count: 40
- Primary result: AdamS achieves 50% memory reduction compared to AdamW while matching optimization performance in LLM pretraining and post-training

## Executive Summary
AdamS is a novel optimizer that eliminates the need for second-moment estimates in AdamW by leveraging momentum as a natural normalizer. The key insight is that for transformer-based models, local smoothness is proportional to gradient magnitudes, making momentum-based normalization both theoretically sound and practically effective. This design choice results in an optimizer with the same memory and compute footprint as SGD with momentum while maintaining AdamW's strong optimization performance. The authors provide rigorous convergence guarantees under relaxed smoothness assumptions and demonstrate AdamS's effectiveness across large language model pretraining and reinforcement learning post-training tasks.

## Method Summary
AdamS modifies the AdamW update rule by replacing the second-moment estimate with a novel denominator based on the root of weighted sums of squares of momentum and current gradient. This leverages the observation that transformer objectives exhibit (L0, L1)-smoothness where local smoothness is proportional to gradient magnitudes. The optimizer maintains the same memory efficiency as SGD with momentum (requiring only momentum storage) while achieving similar optimization performance to AdamW. The design is motivated by both theoretical analysis showing convergence under relaxed smoothness conditions and empirical observations of the correlation between gradient magnitudes and local smoothness in transformer objectives.

## Key Results
- Achieves 50% memory reduction compared to AdamW by eliminating second-moment storage
- Matches or exceeds AdamW's optimization performance in GPT-2 and Llama2 pretraining up to 13B parameters
- Demonstrates strong performance in DeepSeek R1-Zero reinforcement learning post-training tasks
- Maintains optimal Ω(1/√T) convergence rate under relaxed smoothness assumptions

## Why This Works (Mechanism)
AdamS exploits the inherent structure of transformer objectives where local smoothness correlates with gradient magnitudes. By using momentum as a normalizer, the optimizer adapts the learning rate based on the observed gradient history without explicitly tracking second moments. The weighted combination of squared momentum and current gradient in the denominator provides adaptive normalization that is particularly effective when the (L0, L1)-smoothness property holds, as is common in transformer-based models.

## Foundational Learning

**AdamW**: An adaptive optimization algorithm that maintains per-parameter learning rates based on exponentially decaying averages of past gradients and squared gradients. Required for understanding the baseline AdamS improves upon.

**Momentum-based optimization**: Methods that accumulate velocity vectors to accelerate convergence and smooth optimization paths. Critical for understanding how AdamS uses momentum as a normalizer.

**Smoothness in optimization**: A property where the gradient changes smoothly with respect to parameters, often proportional to local gradient magnitudes. Essential for understanding why momentum normalization works in transformer objectives.

**Second-moment estimation**: The process of tracking the moving average of squared gradients to adapt learning rates. Understanding this helps explain what AdamS eliminates and why it's beneficial.

## Architecture Onboarding

**Component map**: Gradient computation -> Momentum accumulation (v) -> Adaptive normalization using √(v² + g²) -> Parameter update -> Loss evaluation

**Critical path**: The optimization loop where gradient computation feeds into momentum accumulation, which then determines the adaptive learning rate scaling through the novel denominator calculation.

**Design tradeoffs**: Memory vs. performance (eliminating second-moment storage saves 50% memory but relies on specific smoothness assumptions), computational overhead (slightly higher per-step compute but overall savings), hyperparameter sensitivity (inherits AdamW's hyperparameters but may require tuning for non-transformer architectures).

**Failure signatures**: Poor convergence when (L0, L1)-smoothness assumption fails, instability with inappropriate learning rate schedules, potential underperformance on architectures where gradient magnitudes don't correlate with local smoothness.

**First experiments**:
1. Compare AdamS vs AdamW on a small transformer model with varying β1 and β2 parameters
2. Test memory consumption and training speed on a medium-sized model (1-10B parameters)
3. Evaluate convergence on non-transformer architectures (CNNs or RNNs) to test assumption generality

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance on non-transformer architectures remains unproven, as the (L0, L1)-smoothness assumption may not hold universally
- Limited empirical validation beyond transformer-based models and reinforcement learning tasks
- Theoretical convergence guarantees assume specific noise characteristics that may not match real-world stochastic gradients

## Confidence
- High confidence in memory efficiency claims (50% reduction) and basic computational properties
- Medium confidence in optimization performance claims across different model architectures
- Medium confidence in theoretical convergence guarantees under assumed conditions

## Next Checks
1. Test AdamS on non-transformer architectures (CNNs, RNNs, graph neural networks) to verify the generality of the (L0, L1)-smoothness assumption
2. Conduct systematic ablation studies varying β1 and β2 parameters to understand their impact on convergence and stability
3. Measure actual wall-clock training time and energy consumption comparisons between AdamS and AdamW on large-scale training runs