---
ver: rpa2
title: 'QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented Dialogue
  Summarization'
arxiv_id: '2509.26302'
source_url: https://arxiv.org/abs/2509.26302
tags:
- dialogue
- summary
- summaries
- quartz
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QUARTZ is an unsupervised framework for task-oriented dialogue
  summarization that generates multiple summaries and QA pairs, then selects the best
  summary using LLM-based evaluation. It improves factual consistency and task relevance
  by focusing on QA-based evaluation rather than surface-level metrics.
---

# QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented Dialogue Summarization

## Quick Facts
- arXiv ID: 2509.26302
- Source URL: https://arxiv.org/abs/2509.26302
- Authors: Mohamed Imed Eddine Ghebriout; GaÃ«l Guibon; Ivan Lerner; Emmanuel Vincent
- Reference count: 40
- QUARTZ achieves competitive performance to fully supervised state-of-the-art methods, outperforming them in several cases

## Executive Summary
QUARTZ introduces an unsupervised framework for task-oriented dialogue summarization that generates multiple summaries and QA pairs, then selects the best summary using LLM-based evaluation. The approach improves factual consistency and task relevance by focusing on QA-based evaluation rather than surface-level metrics. QUARTZ demonstrates competitive performance against fully supervised methods across multiple datasets, with notable improvements on SAMSum (+19% BLEU, +3% BERT-Score relative to best supervised method).

## Method Summary
QUARTZ generates multiple summaries (n=3, 5, or 7) using a pre-trained LLM, then creates corresponding QA pairs for each summary. The framework evaluates these summaries using LLM-based scoring for factual consistency and task relevance. The best-performing summary is selected and can be further fine-tuned on the chosen summaries. This approach eliminates the need for human-annotated summaries while maintaining high quality through automated QA-based evaluation.

## Key Results
- QUARTZ achieves competitive performance to fully supervised state-of-the-art methods
- On SAMSum, QUARTZ achieves +19% BLEU and +3% BERT-Score relative to the best supervised method
- LLM-as-judge evaluation shows QUARTZ summaries score higher than reference summaries across coherence, consistency, fluency, and relevance

## Why This Works (Mechanism)
None provided in source material

## Foundational Learning
- **LLM-based evaluation**: Using language models to assess summary quality through automated scoring mechanisms, needed for scalable evaluation without human annotations
- **QA-based consistency checking**: Generating questions from summaries to verify factual accuracy, needed to ensure summaries contain correct information from source dialogues
- **Unsupervised selection**: Choosing the best summary from multiple candidates using automated metrics, needed to eliminate dependence on labeled training data

## Architecture Onboarding

### Component Map
Input Dialogue -> Multiple Summary Generation -> QA Pair Generation -> LLM-based Evaluation -> Best Summary Selection -> Optional Fine-tuning

### Critical Path
The critical path is: Dialogue -> Summary Generation -> QA Generation -> Evaluation -> Selection. This sequence must complete before any downstream tasks can use the summary.

### Design Tradeoffs
The framework trades computational efficiency for accuracy by generating multiple summaries and performing QA generation for each. This increases processing time but improves selection quality. The choice of n (3, 5, or 7) represents a key tradeoff between performance and resource usage.

### Failure Signatures
- Poor summary quality if LLM evaluation metrics are not properly calibrated
- Inconsistent results across different LLM versions or providers
- Increased processing time proportional to n value chosen
- Potential overfitting during fine-tuning phase if not properly regularized

### First Experiments
1. Test with n=3 summaries to establish baseline performance before scaling up
2. Evaluate on a held-out validation set to verify QA generation quality
3. Compare LLM-as-judge scores against human evaluation on a small sample to validate the evaluation protocol

## Open Questions the Paper Calls Out
None

## Limitations
- Domain specificity: Performance gains demonstrated primarily on task-oriented dialogue datasets, limiting generalizability to other dialogue types
- Computational overhead: Generating multiple summaries and QA pairs increases inference costs significantly
- LLM dependency: Framework relies on large language models, with performance potentially varying across different versions or providers

## Confidence

### Confidence Labels
- High Confidence: Claims about improving factual consistency and task relevance through QA-based evaluation
- Medium Confidence: Claims about competitive performance against supervised methods, limited by evaluation on small set of datasets
- Medium Confidence: Human evaluation results showing QUARTZ favored in nearly half of cases, based on limited sample sizes

## Next Checks
1. Evaluate QUARTZ on non-task-oriented dialogue datasets (e.g., DailyDialog, Ubuntu IRC logs) to assess generalizability
2. Measure inference time and memory usage for different n values to establish practical deployment constraints
3. Systematically remove components (QA generation, LLM-based selection) to quantify individual contributions