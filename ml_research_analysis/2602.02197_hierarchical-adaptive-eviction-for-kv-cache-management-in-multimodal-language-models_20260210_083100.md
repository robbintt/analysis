---
ver: rpa2
title: Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language
  Models
arxiv_id: '2602.02197'
source_url: https://arxiv.org/abs/2602.02197
tags:
- eviction
- tokens
- visual
- attention
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of KV cache management in
  Multimodal Large Language Models (MLLMs), where the quadratic memory and computational
  costs of Transformer architectures create bottlenecks, especially when handling
  heterogeneous attention distributions between visual and text tokens. The core method
  idea is Hierarchical Adaptive Eviction (HAE), a two-stage framework that optimizes
  text-visual token interaction by implementing Dual-Attention Pruning (DAP) during
  pre-filling to evict redundant visual tokens using attention variance and sparsity,
  and a Dynamic Decoding Eviction Strategy (DDES) during decoding inspired by OS recycle
  bins to retain relevant KV states dynamically.
---

# Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models

## Quick Facts
- arXiv ID: 2602.02197
- Source URL: https://arxiv.org/abs/2602.02197
- Authors: Xindian Ma; Yidi Lu; Peng Zhang; Jing Zhang
- Reference count: 19
- Primary result: 41% KV cache reduction with 0.3% accuracy loss in image understanding tasks

## Executive Summary
This paper introduces Hierarchical Adaptive Eviction (HAE), a two-stage framework for efficient KV cache management in Multimodal Large Language Models (MLLMs). The method addresses the quadratic memory and computational costs of Transformer architectures when handling heterogeneous attention distributions between visual and text tokens. HAE achieves significant memory savings (41-47%) and inference speedup (1.5x) while maintaining task performance across image understanding and story generation benchmarks.

## Method Summary
HAE employs a training-free, two-stage approach to optimize KV cache eviction in MLLMs. During pre-filling, Dual-Attention Pruning (DAP) uses layer 1 attention patterns to identify and evict redundant visual tokens based on attention variance and sparsity. During decoding, Dynamic Decoding Eviction Strategy (DDES) maintains a recycling bin that dynamically retains relevant KV states using cumulative attention scores. The framework leverages the observation that visual tokens exhibit sparse attention patterns in early layers while text tokens dominate later layers, enabling targeted pruning without significant accuracy loss.

## Key Results
- 41% KV cache reduction with only 0.3% accuracy loss on image understanding tasks
- 1.5x inference speedup for story generation on Phi3.5-Vision-Instruct
- Outperforms existing eviction strategies in both efficiency and task generalization
- Maintains story quality scores (Style/Engagement/Coherence) at 1-10 scale

## Why This Works (Mechanism)
The method exploits heterogeneous attention distributions in MLLMs where visual tokens show sparse patterns in early layers while text tokens dominate later processing. By pruning redundant visual tokens during pre-filling and using dynamic recycling during decoding, HAE reduces memory footprint without losing critical information. The layer 1 broadcasting strategy leverages high correlation (approx. 90% overlap) between attention patterns across layers, enabling efficient global eviction decisions from local information.

## Foundational Learning
- **KV Cache Management**: Why needed - Prevents memory explosion in long-sequence generation; Quick check - Monitor cache size relative to input length
- **Attention Variance Analysis**: Why needed - Identifies redundant tokens for eviction; Quick check - Verify attention distribution skewness across token types
- **Recycling Bin Mechanism**: Why needed - Maintains flexibility for autoregressive generation; Quick check - Track eviction timing vs. generation progress

## Architecture Onboarding
- **Component Map**: Input -> DAP (pre-filling) -> DDES (decoding) -> Evicted KV cache -> Output
- **Critical Path**: Attention computation → Variance calculation → Threshold comparison → Eviction decision → KV cache update
- **Design Tradeoffs**: Static vs. dynamic thresholds (r=0.0015, α=0.0005) balance accuracy vs. compression; Fixed recycling bin size (56-128) vs. adaptive sizing affects overhead
- **Failure Signatures**: Accuracy degradation >2% indicates over-aggressive pruning; Speed slowdown indicates recycling bin overhead
- **First Experiments**: 1) Test DAP threshold sensitivity on GQA; 2) Profile DDES overhead on short vs. long generation tasks; 3) Validate layer correlation for broadcasting across different MLLM architectures

## Open Questions the Paper Calls Out
1. Can HAE be extended into an end-to-end learnable sparsification mechanism rather than relying on static attention-based heuristics?
2. Does the "Layer 1 broadcasting" strategy remain theoretically optimal as model depth increases, or does attention pattern divergence necessitate layer-specific eviction decisions?
3. How does HAE scale to extremely large models (e.g., 70B+ parameters) and datasets?

## Limitations
- Fixed attention thresholds may not generalize across different MLLM architectures
- Layer-1 broadcasting assumes homogeneous attention patterns across layers
- Recycling bin mechanism introduces non-negligible overhead for short-generation tasks
- Training-free approach may miss optimization opportunities from fine-tuning

## Confidence
- **High confidence**: 41% KV cache reduction and 1.5x inference speedup on Phi3.5-Vision-Instruct
- **Medium confidence**: ≤0.3% accuracy retention across all benchmarks with fixed thresholds
- **Medium confidence**: Story quality evaluation relies on DeepSeek-R1's scoring

## Next Checks
1. Test HAE with varying attention thresholds (r, α) across different MLLM architectures
2. Profile recycling bin overhead on short-context tasks to confirm 1.5x speedup holds
3. Implement exact attention score accumulation decay rate λ from Theorem 2.1