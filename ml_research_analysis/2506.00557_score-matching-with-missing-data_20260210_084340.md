---
ver: rpa2
title: Score Matching With Missing Data
arxiv_id: '2506.00557'
source_url: https://arxiv.org/abs/2506.00557
tags:
- score
- matching
- data
- missing
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces missing score matching, a framework for
  adapting score matching to handle incomplete data. The authors propose two complementary
  methods: an importance weighting (IW) approach and a variational approach.'
---

# Score Matching With Missing Data

## Quick Facts
- **arXiv ID:** 2506.00557
- **Source URL:** https://arxiv.org/abs/2506.00557
- **Reference count:** 40
- **Primary result:** Introduces missing score matching framework with importance weighting and variational approaches for learning score functions from incomplete data

## Executive Summary
This paper addresses the problem of score matching when data contains missing values. The authors propose two complementary methods: an importance weighting (IW) approach and a variational approach. Both methods adapt the score matching framework to handle partially observed data by matching the marginal Fisher divergence rather than the full Fisher divergence. The IW method is shown to be effective for lower-dimensional problems with small sample sizes, while the variational approach excels in complex, high-dimensional scenarios. The paper provides finite sample bounds for the IW approach in bounded domain settings and demonstrates strong empirical performance on both synthetic and real-world datasets for parameter estimation and Gaussian graphical model edge detection tasks.

## Method Summary
The core innovation is the introduction of Marginal Fisher Divergence, which allows score matching to be performed using only observed data. The IW approach uses importance sampling to estimate the marginal score by drawing samples from a proposal distribution for the missing coordinates. The variational approach takes a "gradient-first" perspective, expressing the gradient of the loss as an expectation involving the full score and a covariance term, which is then estimated using a variational network that learns to approximate the conditional distribution of missing data. Both methods are extended to truncated score matching, sliced score matching, and denoising score matching, making them applicable to a wide range of generative modeling scenarios.

## Key Results
- Marg-IW achieves strong performance in low-dimensional settings (d < 50) with small sample sizes, outperforming the naive "Zeroed" baseline
- Marg-Var excels in high-dimensional, unstructured problems and maintains performance as dimensionality increases
- Both methods successfully recover edge structures in Gaussian graphical models, with Marg-Var showing superior performance under high missingness rates
- Finite sample bounds are provided for Marg-IW in truncated bounded domain settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Fisher divergence of a full distribution can be minimized using only samples from its marginal distributions, provided the objective is transformed to handle variable missingness masks.
- **Mechanism:** The paper introduces a "Marginal Fisher Divergence" ($F_M$). Instead of matching the score of the full data $s(x)$, the method matches the score of the observed coordinates $s_\lambda(x_\lambda)$. Crucially, using integration by parts (Proposition 4.3), the authors show that minimizing the marginal Fisher divergence $E[\|s_\Lambda(X_\Lambda) - s_{\Lambda;\theta}(X_\Lambda)\|^2]$ is equivalent to minimizing an objective $L_M(\theta)$ that depends only on the model's score and the observed data, bypassing the need for the true score.
- **Core assumption:** The data is Missing Completely At Random (MCAR), meaning the missingness mask $\Lambda$ is independent of the data $X$. The model must also satisfy identifiability conditions (Assumption 4.2e) where marginal equality implies joint equality.
- **Evidence anchors:**
  - [Section 4.1]: Definition 4.1 defines the marginal score; Proposition 4.3 proves $L_M(\theta) = F_M(\theta) - C$.
  - [Corpus]: Corpus evidence is weak; neighbors focus on robust score matching or change points, not missing data mechanisms.
- **Break condition:** If data is Missing Not At Random (MNAR) and the mechanism is ignored (Appendix A.1.4 extension not used), the estimated marginal distribution will not match the true data generating process, causing parameter bias.

### Mechanism 2
- **Claim:** Importance Weighting (IW) allows the intractable integral of the marginal score to be estimated via Monte Carlo sampling from a proposal distribution.
- **Mechanism:** The marginal score $s_\lambda(x_\lambda)$ requires the gradient of a log-integral (Eq 2). The IW method (Section 4.2) uses samples $X'_{-\lambda}$ from a proposal $p'$ to approximate this integral. By reformulating the gradient as a weighted expectation (Eq 6), the method constructs a tractable estimate $\hat{s}_{\lambda,r;\theta}$ (Definition 4.4) that can be plugged into the objective.
- **Core assumption:** The proposal distribution $p'$ has sufficient support over the data domain. For theoretical guarantees, the domain must be truncated (bounded) to ensure finite sample bounds (Theorem 4.6).
- **Evidence anchors:**
  - [Section 4.2]: Eq (6) shows the transformation from integral to expectation; Eq (7) defines the Monte Carlo estimate.
  - [Corpus]: Neighbors like "Why Heuristic Weighting Works" discuss weighting but in the context of denoising, not marginalization integrals.
- **Break condition:** If the proposal distribution $p'$ is far from the true conditional $p(x_{-\lambda}|x_\lambda)$, the variance of the importance weights explodes, leading to unstable training, particularly in high dimensions.

### Mechanism 3
- **Claim:** A variational "gradient-first" approach decouples the gradient computation from the integral estimation, reducing the bias found in naive Importance Weighting.
- **Mechanism:** Instead of estimating the marginal score (which introduces nested Monte Carlo bias), the Variational method (Section 4.3) differentiates the loss *first*. Using Lemma 4.8, the gradient of the loss is expressed as an expectation involving the *full* score $s_\theta(x)$ and a covariance term. This expectation is then estimated using a variational network $p'_\phi$ that learns to approximate the conditional distribution of missing data.
- **Core assumption:** The variational model $p'_\phi$ is expressive enough to approximate $p_\theta(x_{-\lambda}|x_\lambda)$ sufficiently well during the inner optimization loop.
- **Evidence anchors:**
  - [Section 4.3]: Corollary 4.9 derives the gradient objective $\nabla_\theta L_M$; Algorithm 2 details the training loop.
  - [Corpus]: No direct corpus evidence for this specific "gradient-first" missing data mechanism.
- **Break condition:** If the variational network fails to converge or underfits the conditional distribution, the gradient estimates for the score model $\theta$ will be incorrect, potentially preventing the recovery of the true precision matrix structure.

## Foundational Learning

- **Concept: Score Matching (Hyvärinen, 2005)**
  - **Why needed here:** This is the base technique being extended. You must understand that Score Matching avoids calculating the partition function (normalizing constant) by matching the gradient of the log-density (the score) rather than the density itself.
  - **Quick check question:** Why does standard Score Matching fail if the density $p(x)$ does not approach 0 at the boundary of the domain?

- **Concept: Fisher Divergence**
  - **Why needed here:** This is the metric the paper optimizes. It measures the distance between distributions based on the difference of their score functions ($E[\|s(x) - s_\theta(x)\|^2]$).
  - **Quick check question:** Can a model have a low Fisher divergence but high KL-Divergence? (Hint: Consider normalizing constants).

- **Concept: Monte Carlo Integration & Importance Sampling**
  - **Why needed here:** The core difficulty of the paper is an intractable integral (marginalization). The IW solution relies entirely on standard Importance Sampling theory to convert this integral into a weighted sum.
  - **Quick check question:** If your proposal distribution $q(x)$ has tails lighter than the target distribution $p(x)$, what happens to the variance of your estimator?

## Architecture Onboarding

- **Component map:**
  - Data -> Score Model -> Imputer -> Objective
  - (Corrupted batch) -> (s_θ(x)) -> (IW: p' or Var: p'_φ) -> (L_M(θ))

- **Critical path:**
  1.  **Mask Handling:** Identify observed indices $\Lambda$ and missing indices $-\Lambda$ for the batch.
  2.  **Imputation:**
      - *If IW:* Sample $r$ "hallucinated" vectors for missing coords from $p'$. Compute importance weights $w = q_\theta/p'$.
      - *If Var:* Run inner loop: Train $p'_\phi$ to predict missing coords given observed (Algorithm 2). Sample $r$ vectors from $p'_\phi$.
  3.  **Score Evaluation:** Construct full vectors $(x_\lambda, x'_{-\lambda})$. Compute the score model's output $s_\theta$ and its divergence/Jacobian on the *observed* coordinates only (or use the gradient-first trick).
  4.  **Optimization:** Update $\theta$ using the estimated gradient.

- **Design tradeoffs:**
  - **Marg-IW:** Simplest implementation. Works well for low-dimensional data ($d < 50$) and small sample sizes. *Cons:* High variance in high dimensions; requires differentiable score model (can be black-box but needs gradients).
  - **Marg-Var:** Best for high-dimensional, structured data (e.g., graphs). *Cons:* Computationally expensive (bi-level optimization); requires training a neural network for the variational conditioner.
  - **Zeroed (Baseline):** Fastest. *Cons:* Mathematically incorrect for any dependent distribution; fails to recover edge structures in graphs.

- **Failure signatures:**
  - **Naive Marginalization:** You see the "Zeroed" method converging, but the estimated precision matrix has no off-diagonal entries (Figure 4/5 behavior), or the Fisher divergence stagnates at a high value (Figure 1).
  - **IW High Variance:** Loss fluctuates wildly; gradient norms explode. Check if $q_\theta$ and $p'$ have disjoint support or if weights are exploding.
  - **Var Mode Collapse:** The variational network ignores the input $x_\lambda$ and outputs a constant mean for $x_{-\lambda}$. Check inner loop loss (KL or Fisher) for convergence.

- **First 3 experiments:**
  1.  **Sanity Check (Truncated Gaussian):** Replicate Figure 1. Use Marg-IW on a 10D truncated Gaussian with 20% missingness. Verify you can outperform the "Zeroed" baseline. This confirms your integration-by-parts logic is coded correctly.
  2.  **Dimensional Scaling (Non-Gaussian):** Replicate Figure 2. Switch to Marg-Var. Increase dimension $d$ (10 $\to$ 50) for an ICA-inspired model. Verify that Marg-IW degrades while Marg-Var maintains low error.
  3.  **Structure Recovery (Star Graph):** Replicate Figure 3. Apply Marg-Var to a Star-Shaped Gaussian Graphical Model. Plot the ROC curve for edge detection. Confirm that increasing missingness probability degrades performance gracefully compared to the baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions can the marginal Fisher divergence be bounded by or related to the full Fisher divergence, enabling theoretical guarantees on parameter accuracy rather than just marginal score accuracy?
- Basis in paper: [explicit] Remark 4.7 states: "Relating these two quantities requires connecting the fully observed distribution to its marginals... Investigating the assumptions and conditions under which this connection can be made offers an interesting and valuable direction for future research."
- Why unresolved: The finite sample bounds (Theorem 4.6) only bound the marginal Fisher divergence, not the full divergence, which is the actual quantity of interest for parameter recovery.
- What evidence would resolve it: Derivation of bounds connecting marginal and full Fisher divergences under specific distributional assumptions (e.g., Gaussian, exponential family), or identification of necessary regularity conditions.

### Open Question 2
- Question: What explains the empirical equivalence between the EM-based approach of Uehara et al. (2020) and the Marg-IW approach, despite their different derivations?
- Basis in paper: [explicit] Section 5.1.2 notes: "the precise mechanism for this similarity remains unclear and warrants further exploration."
- Why unresolved: Both methods use self-normalized importance weighting but are derived from different theoretical frameworks, yet produce indistinguishable performance across experiments.
- What evidence would resolve it: Theoretical analysis showing equivalence of their objectives under certain conditions, or controlled experiments identifying settings where they diverge.

### Open Question 3
- Question: Can finite sample bounds be derived for the non-truncated (standard) score matching setting, or are bounded density requirements (Assumption A.11) fundamentally necessary?
- Basis in paper: [inferred] Theorem 4.6 only provides bounds for truncated score matching. Remark A.12 notes the bounded density assumption "restricts us from obtaining a similar result in the non-truncated case."
- Why unresolved: The proof technique requires densities bounded away from zero and infinity, which is incompatible with standard score matching's requirement that p(x)→0 at infinity.
- What evidence would resolve it: Alternative proof techniques using different tail assumptions, or a proof that such bounds are impossible without truncation.

### Open Question 4
- Question: How can the MNAR extension be made practical when the missingness probabilities φλ are unknown and must be estimated from data?
- Basis in paper: [explicit] Appendix A.1.4 states regarding the known φλ assumption: "This is often an unrealistic assumption however this allows us the flexibility of having a method which is independent of how the φλ are learned."
- Why unresolved: The MNAR framework requires known missingness mechanisms, but in practice these must be estimated, introducing additional estimation error not accounted for in the theoretical analysis.
- What evidence would resolve it: A joint estimation framework for φλ and θ with theoretical guarantees, or sensitivity analysis quantifying the impact of misspecified φλ.

### Open Question 5
- Question: Can the variational approach be extended to diffusion-based generative modeling with theoretical guarantees on sample quality?
- Basis in paper: [explicit] Conclusion states: "since our method is compatible with denoised score matching, it can naturally be extended to diffusion-based model. This paves the way for future work on applying our approach to generative modelling with diffusion processes in the presence of missing data."
- Why unresolved: While the compatibility with denoising score matching is shown, application to full diffusion pipelines with missing training data remains unexplored both empirically and theoretically.
- What evidence would resolve it: Implementation and evaluation on standard diffusion benchmarks with corrupted training data, with analysis of how missing data affects generated sample quality.

## Limitations

- The MCAR assumption may not hold in many real-world scenarios where MNAR mechanisms are common
- Finite-sample bounds are limited to truncated bounded domains, which may not reflect practical applications with unbounded data
- The computational cost of the variational approach scales poorly with dimensionality due to bi-level optimization
- Empirical evaluation focuses primarily on synthetic data and relatively structured real datasets

## Confidence

- **High confidence:** The mathematical derivation of the marginal Fisher divergence (Proposition 4.3) and the equivalence between minimizing the marginal score and the proposed objective is sound and well-established through integration by parts
- **Medium confidence:** The finite-sample bounds for the IW approach are valid under the stated assumptions (bounded domains, truncated distributions), but their practical applicability to real-world unbounded data remains uncertain
- **Medium confidence:** The empirical superiority of Marg-Var in high-dimensional settings is demonstrated convincingly through simulations and real data experiments, though the comparison is limited to specific problem types and baselines
- **Low confidence:** The claim that Marg-IW is optimal for small sample sizes and low dimensions is based on limited experimental evidence (Figure 1) and may not generalize across different data distributions

## Next Checks

1. Test the methods on real-world datasets with known MNAR mechanisms to evaluate robustness when MCAR assumptions are violated
2. Conduct ablation studies varying the proposal distribution quality for IW to quantify the sensitivity to proposal choice in high dimensions
3. Extend the theoretical analysis to unbounded domains to understand the practical limitations of the finite-sample bounds beyond truncated settings