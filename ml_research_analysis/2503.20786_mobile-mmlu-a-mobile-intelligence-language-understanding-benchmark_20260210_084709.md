---
ver: rpa2
title: 'Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark'
arxiv_id: '2503.20786'
source_url: https://arxiv.org/abs/2503.20786
tags:
- mmlu
- mobile
- questions
- mobile-mmlu
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mobile-MMLU and Mobile-MMLU-Pro are benchmark datasets specifically
  designed for evaluating large language models in mobile contexts, addressing the
  gap in existing benchmarks that primarily target desktop/server environments. The
  benchmarks consist of 16,186 questions across 80 mobile-relevant topics, with Mobile-MMLU-Pro
  providing a more challenging subset of 9,497 questions created through multi-model
  consistency filtering.
---

# Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark

## Quick Facts
- arXiv ID: 2503.20786
- Source URL: https://arxiv.org/abs/2503.20786
- Reference count: 40
- Key result: Mobile-MMLU achieves Mobile Relevance Score of 5.88, nearly double that of traditional benchmarks

## Executive Summary
Mobile-MMLU and Mobile-MMLU-Pro are benchmark datasets specifically designed for evaluating large language models in mobile contexts, addressing the gap in existing benchmarks that primarily target desktop/server environments. The benchmarks consist of 16,186 questions across 80 mobile-relevant topics, with Mobile-MMLU-Pro providing a more challenging subset of 9,497 questions created through multi-model consistency filtering. The datasets use order-invariant multiple-choice questions focusing on practical mobile scenarios, with questions deliberately designed to avoid LLM biases such as preference for longer answers or specific answer positions.

## Method Summary
The benchmark was created through a multi-stage process involving LLM-assisted topic generation and question drafting, followed by human review and consistency filtering. The Pro variant underwent additional filtering using multi-model consistency across GPT-4o, Claude-3.5, and Gemini-2.0 to remove ambiguous questions and retain items that better differentiate model capabilities. Questions were carefully constructed to maintain order-invariance through randomized answer positions and length-balanced distractors to mitigate positional and length biases in LLM responses.

## Key Results
- Mobile-MMLU achieves Mobile Relevance Score of 5.88 (average), nearly double that of traditional benchmarks like MMLU at 3.13
- Smaller models (1-3B parameters) show wider relative performance gaps compared to larger models, demonstrating the benchmark's effectiveness in differentiating mobile-optimized model capabilities
- The consistency filtering approach for Mobile-MMLU-Pro successfully creates a more challenging subset while maintaining discriminative power across model sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filtering questions using multi-model consistency improves benchmark discriminative power and reliability.
- Mechanism: By removing questions answered inconsistently by diverse high-performing models or consistently answered correctly by smaller models, the benchmark retains items that better differentiate capability levels and reduce evaluation variance.
- Core assumption: Disagreement among strong models signals ambiguous or multi-correct questions, and agreement among weaker models on correct answers signals low difficulty.
- Evidence anchors:
  - [abstract] "Results show Mobile-MMLU achieves higher mobile relevance scores (5.88 average) compared to traditional benchmarks (3.13 for MMLU), with wider performance variance among models highlighting its effectiveness in differentiating capabilities."
  - [section] Section 4 describes the Mobile-MMLU-Pro creation pipeline using a two-group (small-scale and moderate-scale) filtering, plus removal of items with inconsistent predictions across GPT-4o, Claude-3.5, and Gemini-2.0.
- Break condition: If model families evaluated are overly homogeneous, the consistency filter may produce a dataset that artificially narrows the performance distribution.

### Mechanism 2
- Claim: Enforcing length parity or greater for distractors relative to the ground-truth option mitigates LLM positional and length bias in multiple-choice evaluation.
- Mechanism: By ensuring incorrect options are equal to or longer than the correct answer while preserving similar wording and structure, the benchmark prevents models from exploiting superficial heuristics like "longer option is more likely correct."
- Core assumption: Assumption: Observed bias toward longer options (especially in smaller LLMs) generalizes across architectures and languages.
- Evidence anchors:
  - [abstract] "The dataset maintains order-invariance properties through careful construction, ensuring consistent evaluation regardless of answer position."
  - [section] Section 3.1 ("Some observed issues and precautions") and Figure 3 describe adjusting incorrect option lengths to address LLM bias toward longer options.
- Break condition: If models develop more sophisticated reasoning or different pattern-matching heuristics, length parity may no longer suffice to neutralize selection bias.

### Mechanism 3
- Claim: Mobile-specific topic selection and shorter question length better align evaluation with realistic on-device usage constraints and information needs.
- Mechanism: Curating 80 practical, mobile-relevant topics (e.g., First Aid, Travel Planning, Social Media) and keeping questions brief (~30.84 words average) reduces mismatch between benchmark tasks and actual mobile queries, yielding higher Mobile Relevance Scores.
- Core assumption: Assumption: Human- and LLM-assisted field selection accurately reflects typical mobile information-seeking patterns.
- Evidence anchors:
  - [abstract] "Mobile users interact differently with LLMs compared to desktop users, creating unique expectations and data biases."
  - [section] Section 5 (Data Statistics) and Figure 10 report average question length (~30.84 words) and topic distributions; Section 2 (Related Work) cites literature on mobile information needs.
- Break condition: If future mobile use cases shift toward complex multi-turn or multimodal interactions not captured by short multiple-choice questions, the benchmark may lose ecological validity.

## Foundational Learning

- Concept: Multiple-choice evaluation and answer position bias
  - Why needed here: Understanding how LLMs may prefer certain answer positions or option lengths is critical for interpreting Mobile-MMLU results and designing robust evaluations.
  - Quick check question: Can you describe at least two sources of bias in multiple-choice LLM evaluation and how to mitigate them?

- Concept: Mobile-specific constraints (latency, memory, energy, privacy)
  - Why needed here: Mobile-MMLU emphasizes metrics beyond accuracy; engineers must understand how these constraints affect model selection and deployment tradeoffs.
  - Quick check question: Why might a model that performs well on MMLU not be optimal for on-device mobile inference?

- Concept: Multi-model consistency filtering and benchmark discriminability
  - Why needed here: The Pro variant uses a novel filtering approach; understanding its rationale helps practitioners decide when to use Mobile-MMLU vs. Mobile-MMLU-Pro.
  - Quick check question: What does it signal if a question is consistently answered differently by multiple strong LLMs?

## Architecture Onboarding

- Component map:
  - Dataset files (questions, choices, ground truth, topic metadata) -> Evaluation harness (lm-eval-harness integration) -> Consistency filtering pipeline (GPT-4o, Claude-3.5, Gemini-2.0, small-model groups) -> Similarity filtering module (MPNet embeddings, cosine similarity) -> Human-AI verification workflow

- Critical path:
  1. Load Mobile-MMLU / Mobile-MMLU-Pro dataset
  2. Run evaluation via harness with randomized answer order (R1-R4) to confirm order-invariance
  3. Log accuracy per topic and per model; analyze variance

- Design tradeoffs:
  - Mobile-MMLU (larger, broader) vs. Mobile-MMLU-Pro (smaller, harder, more consistent)
  - Multiple-choice vs. open-ended generation: MCQ enables fast, reproducible evaluation but may not capture conversational nuance
  - Length-balanced distractors reduce bias but increase curation effort and may affect difficulty calibration

- Failure signatures:
  - Large accuracy variance across answer order permutations (>5% swing)
  - Disproportionately high selection of the longest option across models
  - Near-zero variance in model scores on Pro subset (suggesting insufficient discriminability)

- First 3 experiments:
  1. Evaluate a small model (1-3B) on Mobile-MMLU with both original and randomized answer orders; measure variance and compare to MMLU
  2. Run the consistency filtering pipeline on a custom domain subset to observe question removal rates and resulting difficulty
  3. Profile latency and memory usage of candidate models on mobile hardware while running Mobile-MMLU inference to correlate accuracy with resource constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does converting Mobile-MMLU questions to open-ended generation tasks affect model assessment compared to the current multiple-choice format?
- Basis in paper: [explicit] The authors note that converting tasks to open-ended generation eliminates selection order bias and state they "plan to conduct further research in this area."
- Why unresolved: The current benchmark relies exclusively on multiple-choice accuracy, leaving the robustness of an open-ended evaluation protocol untested.
- What evidence would resolve it: A comparative study measuring model performance on Mobile-MMLU using generation-based metrics versus the current multiple-choice protocol.

### Open Question 2
- Question: What is the degradation rate of model accuracy on Mobile-MMLU's time-sensitive questions, and what is the optimal update frequency?
- Basis in paper: [explicit] The authors identify that 3.5% of the dataset consists of time-sensitive questions that "may become outdated over time."
- Why unresolved: While the authors commit to regular updates, the impact of information drift on benchmark validity has not been quantified.
- What evidence would resolve it: Longitudinal evaluation of model performance on the time-sensitive subset (e.g., population statistics) across multi-year intervals.

### Open Question 3
- Question: How do the latency and energy consumption profiles of the evaluated mobile-friendly models correlate with their accuracy on Mobile-MMLU?
- Basis in paper: [inferred] The abstract emphasizes "inference latency, energy consumption, [and] memory usage" as critical constraints, yet the experimental results only report accuracy and order bias.
- Why unresolved: The paper establishes the benchmark to evaluate mobile constraints but does not present hardware metric data for the tested models.
- What evidence would resolve it: On-device benchmarks measuring battery drain and inference time for models like Llama-3.2-1B while processing the Mobile-MMLU dataset.

## Limitations

- The multi-model consistency filtering approach may not generalize well if evaluated models are too homogeneous, potentially producing a dataset that artificially narrows the performance distribution
- The benchmark is limited to 1B-9B parameter models, leaving uncertainty about how larger models would perform and whether observed performance gaps would persist at scale
- The reliance on multiple-choice format constrains assessment of conversational and open-ended reasoning capabilities that are increasingly important for mobile applications

## Confidence

- **High Confidence**: The observation that Mobile-MMLU achieves significantly higher Mobile Relevance Scores (5.88) compared to traditional benchmarks (3.13 for MMLU) is well-supported by the dataset statistics and evaluation results presented.
- **Medium Confidence**: The effectiveness of the length-balanced distractor design in mitigating LLM positional bias is plausible given the documented issues, but the actual impact on evaluation outcomes requires further validation across diverse model architectures.
- **Medium Confidence**: The claim that the Pro variant better differentiates model capabilities through consistency filtering is supported by the methodology, but the specific threshold choices and their impact on benchmark quality warrant additional investigation.

## Next Checks

1. **Cross-Architecture Bias Validation**: Evaluate models from diverse architectural families (transformers, state-space models, hybrid approaches) on Mobile-MMLU to assess whether the length-balanced distractor design and consistency filtering generalize across different model types, or if specific architectures exhibit unique biases not addressed by current design choices.

2. **Dynamic Difficulty Calibration**: Implement an iterative difficulty calibration where questions are evaluated across a wider range of model sizes (including sub-1B and 10B+ parameters) to verify that the Pro subset maintains appropriate difficulty gradients and discriminative power across the full spectrum of mobile-optimized models.

3. **Real-World Mobile Performance Correlation**: Deploy top-performing models from Mobile-MMLU evaluations on actual mobile hardware with varying resource constraints (CPU-only, GPU-accelerated, different RAM configurations) to validate whether benchmark accuracy correlates with practical mobile deployment performance, including latency, memory usage, and energy efficiency metrics.