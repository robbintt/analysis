---
ver: rpa2
title: Hybrid Least Squares/Gradient Descent Methods for DeepONets
arxiv_id: '2508.15394'
source_url: https://arxiv.org/abs/2508.15394
tags:
- adam
- deeponet
- training
- solution
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid least squares/gradient descent (LSGD)
  method to accelerate DeepONet training. DeepONets, neural operators that learn mappings
  between infinite-dimensional function spaces, are computationally expensive to train
  due to their complex structure involving two coupled neural networks.
---

# Hybrid Least Squares/Gradient Descent Methods for DeepONets

## Quick Facts
- arXiv ID: 2508.15394
- Source URL: https://arxiv.org/abs/2508.15394
- Authors: Jun Choi; Chang-Ock Lee; Minam Moon
- Reference count: 37
- Primary result: 10-20x training acceleration for DeepONets using hybrid LS+Adam method

## Executive Summary
This paper addresses the computational challenge of training DeepONets by introducing a hybrid least squares/gradient descent method that exploits the linearity of the DeepONet output with respect to the branch network's last layer parameters. The method decomposes a prohibitively large least squares system into two manageable subproblems using Kronecker factorization, enabling efficient closed-form optimization of these parameters while maintaining the flexibility of gradient descent for the remaining network weights. Experiments on various PDE problems demonstrate significant training acceleration (10-20x speedup) while maintaining or improving accuracy compared to standard Adam training.

## Method Summary
The method exploits the linear structure of DeepONet outputs with respect to the branch network's final layer parameters, enabling these to be optimized via least squares (LS) instead of gradient descent. To handle the computational complexity of forming and solving the full LS system, the authors decompose it using Kronecker products and commutation matrices, transforming it into a generalized Sylvester equation solvable via spectral decomposition. The approach alternates between LS updates for the final layer and Adam updates for hidden layers, generalizing to include L2 regularization and physics-informed losses. The method requires structured input data (Cartesian products) and a linear activation in the branch network's last layer.

## Key Results
- LS+Adam achieved mean relative L2 error of 1.34e-3 for Poisson equation at 10,000 work units vs 4.50e-3 for Adam-only at 100,000 work units
- 10-20x training acceleration demonstrated across advection, diffusion-reaction, and Poisson equations
- Method generalizes to both supervised and unsupervised learning with physics-informed losses
- Optimal configuration found: 5 Adam epochs per LS step balances computational cost and convergence

## Why This Works (Mechanism)

### Mechanism 1: Output Layer Linearity Exploitation
The DeepONet output $G(u)(y) \approx \langle b(u), t(y) \rangle$ is linear with respect to the branch network's final layer parameters, allowing closed-form LS optimization instead of iterative gradient descent. This eliminates the "search" required for these weights while maintaining non-linearity in hidden layers.

### Mechanism 2: Kronecker Factorization for System Decomposition
The paper exploits data structure (Cartesian products) to factorize the design matrix using Kronecker products, transforming a massive matrix inversion into a generalized Sylvester equation solvable via spectral decomposition of two smaller matrices.

### Mechanism 3: Alternating Optimization (LS+Adam)
Alternating between LS updates for the final layer and Adam updates for hidden layers stabilizes training by frequently optimizing the linear coefficients for the current basis functions while allowing Adam to focus on shaping those basis functions.

## Foundational Learning

- **Concept:** Least Squares & Regularization (Tikhonov)
  - **Why needed:** Required to understand how the branch output layer is optimized without gradient descent
  - **Quick check:** Can you explain why adding regularization ($\lambda > 0$) ensures $A^TA + \lambda I$ is invertible even if $A^TA$ is singular?

- **Concept:** Kronecker Product & Vectorization
  - **Why needed:** The paper relies on $\text{vec}(AXB) = (B^T \otimes A)\text{vec}(X)$ for efficient solver implementation
  - **Quick check:** If matrix $A$ is $m \times n$ and $B$ is $p \times q$, what are dimensions of $A \otimes B$?

- **Concept:** DeepONet Architecture (Branch & Trunk)
  - **Why needed:** Method specifically targets the Branch network's last layer; must distinguish branch (encodes input function) from trunk (encodes domain coordinates)
  - **Quick check:** In $\langle b(u), t(y) \rangle$, which network determines the "coefficients" optimized by LS?

## Architecture Onboarding

- **Component map:** Input Function $\rightarrow$ Branch Network $\rightarrow$ Branch Hidden Layers $\rightarrow$ Branch Final Layer (Linear, LS target) $\rightarrow$ Inner Product $\rightarrow$ Trunk Network $\rightarrow$ Output

- **Critical path:**
  1. Ensure input data is batched as Cartesian product of input functions and domain points
  2. Forward pass: compute trunk outputs $T$ and branch pre-outputs $B$ (before final linear layer)
  3. Spectral solve: compute $B^T B$ and $T^T T$, perform eigen-decomposition, solve Sylvester equation for $C$
  4. Backward pass: update $\theta_B, \theta_T$ using Adam on loss, keeping solved $C$ fixed

- **Design tradeoffs:**
  - Memory vs. Speed: Factorization avoids full matrix construction but requires holding $B$ and $T$ in memory
  - Batch Constraints: Cannot use random mini-batches; strictly need grid structure

- **Failure signatures:**
  - NaNs in $C$: Check regularization $\lambda$; eigenvalues likely near zero
  - No convergence speedup: Verify branch last layer has no activation function (linear) and no bias

- **First 3 experiments:**
  1. **Sanity Check (Regression):** Small DeepONet on 1D regression; compare Adam-only vs. LS+Adam (1 WU = 5 Adam + 1 LS); plot loss curves
  2. **Ablation on $\lambda$:** Poisson equation with varying $\lambda \in [10^{-4}, 10^{-12}]$; identify stability threshold
  3. **PDE Generalization:** Replicate Advection experiment to verify physics-informed losses work correctly

## Open Questions the Paper Calls Out

### Open Question 1
How can the method be extended to handle unsupervised learning for PDEs with nonlinear residual operators? The current factorization relies on linearity condition $L_k[\tilde{b}_j t_i] = \tilde{b}_j L_k[t_i]$ which breaks down for nonlinear PDEs. A modified formulation that converges for nonlinear diffusion-reaction equation using physics-informed loss without labeled data would resolve this.

### Open Question 2
Can the LS formulation be adapted for operators that depend on the input function (e.g., variable coefficients)? The efficient Kronecker decomposition assumes $L_k$ acts only on trunk output and is independent of branch input. A generalized derivation solving LS for variable coefficient Poisson equation in unsupervised setting would resolve this.

### Open Question 3
Is the empirical configuration of five Adam epochs per LS step optimal for convergence across different PDE complexities? The paper states this balances computational cost and convergence but lacks theoretical justification. An ablation study showing impact of varying work unit ratios on final accuracy and training speed would resolve this.

## Limitations
- Requires structured input data as Cartesian products (grids) of functions and domain points, limiting applicability to unstructured data
- Requires branch network's last layer to be linear (no activation function), constraining architectural flexibility
- Sensitive to regularization parameter tuning across different problem scales and dimensions

## Confidence
**High Confidence:** Acceleration mechanism is mathematically sound with well-supported proofs and measured 10-20x speedup claims.

**Medium Confidence:** Generalization to L2 regularization and physics-informed losses appears correct but relies on separability assumptions not exhaustively tested.

**Low Confidence:** Practical impact of data structure requirements on real-world applications is unclear as most scientific datasets don't naturally form Cartesian products.

## Next Checks
1. **Unstructured Data Test:** Implement synthetic test with randomly sampled (u, y) pairs to measure memory/time cost of dense LS solve versus standard Adam training.

2. **Activation Function Ablation:** Systematically test method with various activation functions in branch last layer (ReLU, tanh, sigmoid) to quantify performance degradation and identify break point.

3. **Dimensionality Scaling Study:** Replicate Poisson experiments in 1D, 2D, and 3D to verify scaling properties and identify dimension-dependent regularization requirements.