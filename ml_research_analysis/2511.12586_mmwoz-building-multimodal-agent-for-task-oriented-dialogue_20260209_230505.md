---
ver: rpa2
title: 'MMWOZ: Building Multimodal Agent for Task-oriented Dialogue'
arxiv_id: '2511.12586'
source_url: https://arxiv.org/abs/2511.12586
tags:
- dialogue
- task-oriented
- user
- mate
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMWOZ, a multimodal task-oriented dialogue
  dataset that extends MultiWOZ 2.3 by incorporating web-style GUI interactions. The
  dataset is built by developing a GUI, converting dialogue states and system actions
  into operation instructions, and collecting web page snapshots along with their
  corresponding instructions.
---

# MMWOZ: Building Multimodal Agent for Task-oriented Dialogue

## Quick Facts
- arXiv ID: 2511.12586
- Source URL: https://arxiv.org/abs/2511.12586
- Reference count: 40
- Primary result: MMWOZ dataset and MATE model show OCR text is more critical than image features for entity accuracy, but suffer from layout overfitting and poor domain transfer

## Executive Summary
This paper introduces MMWOZ, a multimodal task-oriented dialogue dataset built on MultiWOZ 2.3 by converting dialogue states into web GUI operations and collecting corresponding screenshots. The authors propose MATE, a multimodal transformer-based model that uses dialogue history, action logs, OCR text, and image features to generate GUI operation instructions or natural language responses. Experimental results demonstrate that while MATE outperforms its variants in most metrics, the model struggles with layout generalization and domain transfer, highlighting challenges in learning semantic operational logic versus memorizing GUI coordinates.

## Method Summary
The MMWOZ dataset is constructed by developing a web GUI, converting MultiWOZ 2.3 dialogue states into operation instructions (type, coordinates, value), and collecting web page snapshots with their corresponding instructions. The MATE baseline model uses a T5-small encoder-decoder architecture enhanced with visual features from a CLIP ViT-B/16 encoder. The model projects image features into the text embedding space, concatenates them with dialogue history, action log, and OCR text, then generates either operation instructions or natural language responses through the T5 decoder.

## Key Results
- OCR text contributes more to entity accuracy than image features (MATE_text > MATE_image in entity accuracy)
- Adding reference coordinates improves entity accuracy and BLEU but reduces turn joint accuracy
- Layout changes cause >50% drop in turn joint accuracy, indicating overfitting to specific GUI coordinates
- Domain transfer shows poor generalization with turn joint accuracy dropping to ~10-20% in target domains

## Why This Works (Mechanism)
The MATE architecture works by projecting visual features from CLIP into the embedding space of T5, then concatenating them with dialogue history, action log, and OCR text to generate operation instructions. The model leverages text information more effectively than image features for entity extraction, suggesting that understanding GUI content relies primarily on readable text rather than visual layout patterns. The action log provides critical context for predicting subsequent operations, improving action type accuracy by approximately 2%.

## Foundational Learning

- **Concept: Multimodal Fusion in Transformers**
  - **Why needed here:** The MATE architecture relies on projecting visual features from a CLIP encoder into the embedding space of a T5 language model. Understanding how to align and concatenate these modalities is critical for building the Action Generator.
  - **Quick check question:** How does a projector layer (linear mapping) align the feature dimensions of a frozen image encoder (ViT-B/16) with a pre-trained text model (T5-small)?

- **Concept: OCR and Vision-Language Grounding**
  - **Why needed here:** The system uses OCR to extract text and image features to understand layout. Its performance is heavily text-dependent (MATE_text outperforms MATE_image). Understanding the strengths and limitations of OCR vs. raw pixel features is essential for diagnosing entity extraction failures.
  - **Quick check question:** In this system, why does removing image features (MATE_text) yield better entity accuracy than removing OCR text (MATE_image)?

- **Concept: Dialogue State Tracking (DST)**
  - **Why needed here:** The MMWOZ dataset is constructed by converting dialogue states (e.g., {food: indian, price: expensive}) into GUI operations. The dataset's integrity depends on the quality of these underlying states from MultiWOZ 2.3.
  - **Quick check question:** Given a user utterance "I want an expensive Indian restaurant in the centre," what is the resulting dialogue state, and which GUI operations would this trigger in the MMWOZ collection script?

## Architecture Onboarding

- **Component map:** User Utterance → Dialogue History + Action Log → Web Page Snapshot → OCR Parser + Image Encoder → Projected Image Features → Action Generator (T5-small) → GUI Operation Instructions/Natural Language Response

- **Critical path:** The Action Generator is the central bottleneck. Its performance hinges on the quality of the OCR text (Table 2 shows text is more important than image features for entity accuracy) and the Action Log (ablation shows ~2% drop in action type accuracy without it).

- **Design tradeoffs:**
  - **Text vs. Image:** Using only OCR text (MATE_text) yields higher entity accuracy than using only image features (MATE_image). Image features at 224x224 resolution lose critical text detail. The multimodal MATE model trades peak entity accuracy for better turn-level joint accuracy, suggesting image features aid in overall GUI state understanding.
  - **Explicit Grounding:** Adding coordinate outputs (Reference Impact) improves response quality (BLEU, Entity Acc) but can reduce the accuracy of the full operation sequence (Turn Joint Acc), indicating a harder generation task.

- **Failure signatures:**
  - **Low Entity Accuracy:** Likely an OCR failure or image encoder resolution issue (check MATE_image baseline).
  - **Low Turn Joint Accuracy with Layout Change:** The model has overfit to specific element coordinates rather than learning the GUI's operational logic. It cannot adapt when interactive elements move (Table 5 shows >50% drop).
  - **Poor Domain Transfer:** The model fails to generalize GUI operations to unseen domains because it hasn't learned to map new slot names to new GUI elements (Table 4 shows turn joint accuracy drops to ~10-20% in target domains).

- **First 3 experiments:**
  1. **Ablate Input Modalities:** Train MATE_text, MATE_image, and full MATE models. Compare entity accuracy and turn joint accuracy to quantify the contribution of OCR vs. visual layout features.
  2. **Layout Robustness Test:** Evaluate a trained MATE model on a modified GUI where the coordinates of interactive elements (buttons, input fields) are shifted. Measure the drop in location/command accuracy to diagnose coordinate overfitting.
  3. **Domain Transfer Probe:** Train MATE on all domains except one (e.g., leave 'Hotel' out). Evaluate its zero-shot performance on the held-out domain, measuring both its ability to extract entities (EA) and operate the new GUI (TJA). This reveals how well GUI operation strategies transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can multimodal agents be trained to learn semantic operational logic for interactive GUI elements rather than relying on direct associations between dialogue context and screen coordinates?
- **Basis in paper:** [explicit] Section 6.3 states that when interactive elements change position, turn joint accuracy drops to ~10%, suggesting the model "does not effectively learn the operational logic... but rather relies on a direct association between coordinates and values."
- **Why unresolved:** The current MATE architecture appears to memorize spatial locations for operations rather than understanding the functional role of UI elements, causing failure upon layout shifts.
- **What evidence would resolve it:** An architecture that maintains high operation accuracy (>80%) when tested on dynamically randomized or permuted GUI layouts.

### Open Question 2
- **Question:** What techniques enable robust zero-shot domain transfer for GUI operation instructions in task-oriented dialogue without requiring pre-training on massive, diverse layout corpora?
- **Basis in paper:** [explicit] Section 6.2 highlights that while entity recognition transfers well, the ability to manipulate new GUIs is significantly reduced (~10-20% accuracy), necessitating exposure to diverse layouts.
- **Why unresolved:** The semantic gap between natural language instructions and specific GUI element locations in unseen domains remains large with current transfer learning methods.
- **What evidence would resolve it:** A model achieving comparable turn joint accuracy in unseen target domains as in source domains without domain-specific fine-tuning.

### Open Question 3
- **Question:** How can reference coordinates for visual grounding be integrated into response generation to enhance entity accuracy without degrading turn-level action planning?
- **Basis in paper:** [explicit] Section 6.1 notes that while adding reference coordinates improves entity accuracy and BLEU scores, it causes a decline in turn joint accuracy, introducing a "trade-off" that should be approached cautiously.
- **Why unresolved:** The mechanism forcing the model to generate coordinates for text entities appears to interfere with its ability to predict the subsequent operation sequence correctly.
- **What evidence would resolve it:** A training objective or decoding strategy that yields improvements in both entity accuracy and turn joint accuracy simultaneously compared to the baseline.

## Limitations

- The dataset construction relies heavily on MultiWOZ 2.3 dialogue state accuracy, propagating existing annotation errors
- MATE demonstrates significant overfitting to specific GUI layouts, with >50% performance drop when interactive elements are repositioned
- Domain transfer experiments reveal poor generalization, with turn joint accuracy falling to approximately 10-20% in unseen domains
- Heavy reliance on OCR text makes the system vulnerable to OCR failures with complex or low-quality web page snapshots

## Confidence

- **High Confidence:** The claim that OCR text contributes more to entity accuracy than image features is well-supported by the ablation study results showing MATE_text outperforming MATE_image in entity accuracy.
- **Medium Confidence:** The assertion that layout changes severely impact performance is supported by experimental data, but the real-world severity depends on the frequency and nature of GUI changes in production environments.
- **Medium Confidence:** The domain transfer results indicate poor generalization, but the specific experimental setup (training on all but one domain) may not reflect practical scenarios where models encounter new domains gradually.

## Next Checks

1. **End-to-End Task Success Evaluation:** Beyond measuring operation accuracy, evaluate whether MATE can successfully complete entire user tasks (e.g., booking a restaurant) from start to finish, measuring success rates and identifying where failures occur in the task flow.

2. **OCR Robustness Testing:** Systematically degrade OCR quality (through image noise, font variations, or resolution changes) and measure the corresponding impact on entity accuracy and task completion to quantify the system's vulnerability to OCR failures.

3. **Incremental Domain Learning:** Instead of zero-shot domain transfer, train MATE incrementally on new domains with limited data to assess whether the model can adapt to new GUI structures more effectively than in the current setup, potentially revealing strategies for improving generalization.