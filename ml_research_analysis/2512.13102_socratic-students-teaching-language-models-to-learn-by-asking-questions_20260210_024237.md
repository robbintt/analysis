---
ver: rpa2
title: 'Socratic Students: Teaching Language Models to Learn by Asking Questions'
arxiv_id: '2512.13102'
source_url: https://arxiv.org/abs/2512.13102
tags:
- student
- math
- questions
- question
- turns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models typically answer questions passively, but
  many real-world tasks require them to ask targeted questions to fill knowledge gaps.
  This work introduces Outcome-Driven Question Optimization Strategy (ODQS), a framework
  that trains language models to ask better questions based on downstream task performance
  rather than human judgments.
---

# Socratic Students: Teaching Language Models to Learn by Asking Questions

## Quick Facts
- arXiv ID: 2512.13102
- Source URL: https://arxiv.org/abs/2512.13102
- Reference count: 40
- Large language models trained to ask targeted questions based on task performance rather than human judgment, achieving up to 54.7% absolute improvement in task completion

## Executive Summary
This paper addresses the challenge of training language models to ask effective questions for knowledge acquisition, rather than just answering them. The proposed Outcome-Driven Question Optimization Strategy (ODQS) trains models to generate questions that maximize downstream task performance by using task completion metrics (Pass@k) as the optimization signal. Unlike traditional approaches that rely on human judgments of question quality, ODQS uses a teacher model to evaluate multiple candidate questions and select those that most improve task outcomes.

The framework demonstrates significant improvements on math and coding benchmarks, with ODQS-trained models achieving up to 54.7% absolute improvement in Pass@5 on GSM8K math problems. The approach enables models to match static baseline performance in three fewer turns by asking more advanced questions earlier in the interaction. The method represents a shift from human-centric evaluation to task-centric optimization for developing questioning capabilities in language models.

## Method Summary
ODQS works by generating multiple candidate questions at each interaction turn, having a teacher model answer each one, and scoring them based on how much they improve task completion (measured via Pass@k). The highest-scoring question becomes a positive example for supervised fine-tuning and Direct Preference Optimization (DPO) of the student model. This process iterates without requiring human-labeled question quality data. The framework trains language models to optimize question generation based on downstream task performance rather than human judgments of question quality.

## Key Results
- Up to 54.7% absolute improvement in Pass@5 on GSM8K math problems
- 22.9% improvement on HumanEval/OpenCoder coding benchmarks
- Matches static baseline performance in three fewer interaction turns
- Significant gains from asking more advanced questions earlier in interactions

## Why This Works (Mechanism)
The framework works by shifting the optimization objective from human-perceived question quality to actual task performance improvement. By generating multiple candidate questions and evaluating them based on their impact on task completion, the model learns to ask questions that provide the most useful information for solving the problem at hand. The use of DPO alongside supervised fine-tuning allows the model to learn nuanced preferences about which questions lead to better outcomes. The iterative nature of the approach enables continuous refinement of questioning strategies based on actual performance data rather than static human judgments.

## Foundational Learning

1. **Question Generation and Evaluation Pipeline**: Understanding how to generate multiple candidate questions and systematically evaluate them is crucial for the ODQS framework to work effectively. Quick check: Can the teacher model reliably distinguish between high and low-quality questions for a given task?

2. **Task Performance Metrics (Pass@k)**: The framework relies on accurate measurement of task completion to guide question optimization. Quick check: Does the Pass@k metric appropriately capture the quality of task completion for different problem types?

3. **Supervised Fine-Tuning with Positive Examples**: The model needs to learn from successful question-answer pairs to improve its questioning strategy. Quick check: Are the positive examples representative of the full space of effective questions?

4. **Direct Preference Optimization (DPO)**: This technique helps the model learn subtle preferences about question quality beyond simple classification. Quick check: Does DPO provide meaningful improvements over pure supervised fine-tuning in this context?

## Architecture Onboarding

**Component Map**: Student Model -> Question Generator -> Teacher Model -> Task Evaluator -> Feedback Loop -> Student Model

**Critical Path**: The student generates candidate questions → teacher answers each → task evaluator scores improvements → best question used for fine-tuning → student updated

**Design Tradeoffs**: 
- Multiple candidates increase computational cost but enable better selection
- Task-based optimization may sacrifice human interpretability for performance
- Teacher model dependency limits real-world applicability

**Failure Signatures**: 
- Questions that improve metric but don't provide meaningful information
- Overfitting to specific teacher model's answering style
- Premature convergence on suboptimal questioning strategies

**First Experiments**:
1. Test candidate question generation with varying numbers of candidates
2. Compare DPO vs supervised fine-tuning alone for question optimization
3. Evaluate impact of different task performance metrics on question quality

## Open Questions the Paper Calls Out

The paper acknowledges several important limitations: the dependence on having access to a capable teacher model that can evaluate multiple question candidates and provide accurate answers is a significant practical constraint; the framework assumes that better questions lead to better task outcomes in a relatively straightforward manner, but complex real-world problems often involve nuanced information needs; the evaluation focuses primarily on quantitative task completion metrics without qualitative analysis of question quality or user experience.

## Limitations

- Strong dependence on having access to a capable teacher model for evaluation
- Limited evaluation to controlled benchmark environments rather than real-world scenarios
- Uncertainty about whether models learn to ask genuinely insightful questions or merely task-optimized ones
- Potential overfitting to specific teacher model's answering style

## Confidence

- **High confidence**: The methodology for generating and selecting candidate questions is clearly described and technically sound
- **Medium confidence**: The reported improvements over baselines are significant and well-supported by the data, though the benchmarks are relatively narrow
- **Medium confidence**: The claim about matching static baselines in fewer turns is supported, but real-world efficiency gains remain to be demonstrated

## Next Checks

1. Test the trained models on out-of-distribution problems or real-world datasets where the teacher model's reliability cannot be guaranteed
2. Conduct human evaluation studies to assess whether the questions generated are genuinely insightful or merely task-optimized
3. Evaluate the approach's robustness when the teacher model has limited capabilities or makes errors in answering questions