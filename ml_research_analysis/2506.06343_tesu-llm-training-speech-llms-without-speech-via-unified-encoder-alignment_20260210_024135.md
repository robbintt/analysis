---
ver: rpa2
title: 'TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment'
arxiv_id: '2506.06343'
source_url: https://arxiv.org/abs/2506.06343
tags:
- speech
- encoder
- arxiv
- text
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training speech-capable language
  models without requiring paired speech-text data or extensive computational resources.
  The proposed TESU-LLM framework leverages a unified text-speech encoder to map semantically
  equivalent text and speech inputs to a shared latent space, then aligns these representations
  with a pre-trained LLM through a lightweight projection network.
---

# TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment

## Quick Facts
- arXiv ID: 2506.06343
- Source URL: https://arxiv.org/abs/2506.06343
- Authors: Taesoo Kim; Jong Hwan Ko
- Reference count: 30
- Achieves competitive speech-related performance using text-only supervision through unified encoder alignment

## Executive Summary
TESU-LLM introduces a framework for training speech-capable language models without requiring paired speech-text data or extensive computational resources. The approach leverages a unified text-speech encoder to map semantically equivalent inputs to a shared latent space, then aligns these representations with a pre-trained LLM through a lightweight projection network. By training only the projection network using text-only supervision, TESU-LLM achieves strong performance on speech-related benchmarks while avoiding the need for time-aligned segmentation, TTS synthesis, or speech data.

The framework demonstrates competitive results on the VoiceBench benchmark (4.17 on AlpacaEval, 3.91 on CommonEval, 58.23 on SD-QA), matching or exceeding models trained with large-scale multimodal datasets. TESU-LLM also shows solid zero-shot performance on ASR (3.61 WER on test-clean) and speech-to-text translation (24.30 BLEU on CoVoST2), highlighting the effectiveness of modality alignment for scalable and resource-efficient speech-language modeling.

## Method Summary
TESU-LLM addresses the challenge of training speech-capable language models without requiring paired speech-text data or extensive computational resources. The framework leverages a unified text-speech encoder to map semantically equivalent text and speech inputs to a shared latent space, then aligns these representations with a pre-trained LLM through a lightweight projection network. By training only the projection network using text-only supervision, TESU-LLM achieves strong performance on speech-related benchmarks while avoiding the need for time-aligned segmentation, TTS synthesis, or speech data. The approach demonstrates competitive results on the VoiceBench benchmark and solid zero-shot performance on ASR and speech-to-text translation tasks.

## Key Results
- Achieves 4.17 on AlpacaEval, 3.91 on CommonEval, and 58.23 on SD-QA on VoiceBench benchmark
- Demonstrates 3.61 WER on test-clean for zero-shot ASR performance
- Achieves 24.30 BLEU on CoVoST2 for speech-to-text translation

## Why This Works (Mechanism)
The framework works by establishing a shared latent space between text and speech modalities through a unified encoder, then leveraging the rich semantic representations from a pre-trained LLM via a lightweight projection network. The key insight is that semantic equivalence between text and speech can be captured through alignment in the latent space, allowing the LLM to process speech representations as if they were text. The projection network serves as a bridge that adapts the unified encoder's outputs to the LLM's expected input format, enabling cross-modal understanding without requiring paired data during training.

## Foundational Learning

**Unified Text-Speech Encoder**
- Why needed: Enables semantic mapping between modalities without paired data
- Quick check: Verify encoder produces consistent representations for semantically equivalent text and speech inputs

**Latent Space Alignment**
- Why needed: Creates shared representation space for cross-modal processing
- Quick check: Measure cosine similarity between aligned text and speech representations

**Projection Network Architecture**
- Why needed: Adapts unified encoder outputs to LLM-compatible format
- Quick check: Validate projection network preserves semantic information during transformation

**Zero-shot Transfer Learning**
- Why needed: Enables model to perform speech tasks without explicit speech training
- Quick check: Test model performance on held-out speech tasks during training

## Architecture Onboarding

**Component Map**
Pre-trained LLM <- Projection Network <- Unified Text-Speech Encoder -> Text Inputs
Pre-trained LLM <- Projection Network <- Unified Text-Speech Encoder -> Speech Inputs

**Critical Path**
Text/Speech Input → Unified Encoder → Projection Network → Pre-trained LLM → Output

**Design Tradeoffs**
- **Resource Efficiency**: Claims to avoid extensive computational resources, but requires pre-trained encoders and LLM
- **Modality Alignment**: Unified encoder must capture semantic equivalence, potentially missing modality-specific nuances
- **Training Complexity**: Only projection network requires training, but unified encoder setup is non-trivial

**Failure Signatures**
- Poor performance on accented or noisy speech despite clean benchmark success
- Degradation when text and speech semantic mappings diverge
- Computational overhead from maintaining dual encoders

**First Experiments**
1. Test semantic alignment between text and speech representations using cosine similarity
2. Validate projection network preserves information through encoder-decoder reconstruction
3. Measure baseline performance on simple speech tasks before full integration

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead of maintaining dual text and speech encoders contradicts resource efficiency claims
- Performance on accented speech, noisy environments, and low-resource languages not thoroughly validated
- Assumes semantic equivalence between text and speech representations may not hold for all linguistic phenomena

## Confidence
- **High**: The core methodology of using a projection network for cross-modal alignment is technically valid and reproducible
- **Medium**: Benchmark performance claims are reasonable but lack comprehensive baseline comparisons
- **Low**: Resource efficiency claims and real-world robustness assertions require additional validation

## Next Checks
1. Evaluate TESU-LLM performance on noisy speech conditions and accented speech datasets to assess robustness beyond clean benchmark data
2. Conduct ablation studies measuring the actual computational requirements for training the unified encoders versus the claimed resource efficiency benefits
3. Test the model's ability to handle low-resource languages and dialects not represented in the training data to validate generalization claims