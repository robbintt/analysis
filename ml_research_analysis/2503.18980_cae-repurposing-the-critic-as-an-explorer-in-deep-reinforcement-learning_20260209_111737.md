---
ver: rpa2
title: 'CAE: Repurposing the Critic as an Explorer in Deep Reinforcement Learning'
arxiv_id: '2503.18980'
source_url: https://arxiv.org/abs/2503.18980
tags:
- learning
- exploration
- deep
- network
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CAE (Critic as an Explorer), a lightweight
  exploration method for deep reinforcement learning that repurposes the value network's
  embedding layers to generate exploration bonuses using linear multi-armed bandit
  techniques, requiring no additional parameters. For complex environments, CAE+ adds
  a small auxiliary network with less than 1% parameter increase.
---

# CAE: Repurposing the Critic as an Explorer in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2503.18980
- Source URL: https://arxiv.org/abs/2503.18980
- Authors: Yexin Li; Pring Wong; Hanfang Zhang; Shuo Chen; Siyuan Qi
- Reference count: 40
- Primary result: CAE improves deep RL exploration by repurposing critic embeddings for uncertainty bonuses, achieving consistent performance gains on MuJoCo and MiniHack benchmarks

## Executive Summary
CAE (Critic as an Explorer) introduces a lightweight exploration method for deep reinforcement learning that repurposes the value network's embedding layers to generate exploration bonuses using linear multi-armed bandit techniques. For complex environments, CAE+ adds a small auxiliary network with less than 1% parameter increase. Both methods use a scaling strategy for stability. Theoretical analysis shows sub-linear regret bounds. Experiments on MuJoCo and MiniHack demonstrate consistent performance improvements over state-of-the-art baselines, with CAE+ outperforming E3B on all tested MiniHack tasks by up to 348% in challenging navigation tasks.

## Method Summary
CAE extracts embeddings φ(s,a) from the penultimate layer of the value network, then computes an exploration bonus β(s,a) = √(φ^T A^(-1) φ) using linear multi-armed bandit techniques (UCB or Thompson Sampling). The covariance matrix A is updated incrementally using φφ^T. A critical scaling strategy normalizes bonuses using running statistics to prevent training instability. For sparse reward environments, CAE+ adds a small inverse dynamics network that predicts actions from consecutive state embeddings, effectively decoupling value estimation from environment dynamics while adding less than 1% more parameters.

## Key Results
- CAE achieves consistent performance improvements across all MuJoCo-v4 tasks when integrated with SAC, PPO, TD3, and DSAC
- CAE+ outperforms E3B on all tested MiniHack tasks, with up to 348% improvement on MultiRoom-N6-Locked
- Theoretical analysis proves sub-linear regret bounds for CAE
- CAE requires zero additional parameters while CAE+ adds less than 1% parameters

## Why This Works (Mechanism)

### Mechanism 1: Embedding-Repurposed Uncertainty Quantification
The method decomposes the standard value network Q(s,a) = θ^T φ(s,a|W) into a deep feature extractor φ and a linear head θ. It repurposes the feature vector φ(s,a) (the embedding) to calculate an uncertainty bonus β using Linear UCB or Thompson Sampling, rather than training a separate exploration network. The core assumption is that the embedding layers of the value network learn meaningful state-action representations that correlate with novelty, even before the value function converges.

### Mechanism 2: Variance-Normalized Bonus Scaling
Because the embeddings φ(s,a) evolve during training, the magnitude of the raw uncertainty bonus √(φ^T A^(-1) φ) fluctuates. The paper applies a Welford-style online algorithm to scale the bonus by its running standard deviation, ensuring the exploration signal does not drown out the environment reward. This scaling strategy is critical for stability.

### Mechanism 3: Decoupled Inverse Dynamics (CAE+)
In sparse reward settings, value network embeddings are insufficient. CAE+ adds an Inverse Dynamics Network (IDN) to predict the action a_t given s_t, s_{t+1}. Crucially, it projects the critic's embeddings through a linear transformation U before feeding them to the IDN. This prevents the value network from overfitting to environment dynamics, maintaining distinct representations for value and exploration.

## Foundational Learning

- **Linear Multi-Armed Bandits (UCB/TS)**: CAE relies on LinUCB or Linear Thompson Sampling to calculate the exploration bonus from the embeddings. How does the covariance matrix A relate to the uncertainty of an embedding vector?
- **Actor-Critic Architecture Components**: You must identify and hook into the specific "embedding layers" (φ) vs. the "value head" (θ) of your base RL algorithm (PPO, SAC, etc.). In a standard PyTorch RL model, which layer output corresponds to φ(s,a)?
- **Inverse Dynamics Models (IDM)**: Understanding how predicting actions from state transitions helps learn state features relevant for exploration. Why is predicting the action a given s and s' often easier and more useful for exploration than predicting s' given s and a?

## Architecture Onboarding

- **Component map**: Base RL Algo -> Feature Hook (extract φ) -> Bandit Module (maintain A, compute β) -> Scaler (normalize β) -> CAE+ Add-on (optional: U + IDN)
- **Critical path**: 1) Hook: Modify forward pass to expose embeddings φ(s,a). 2) Update: After environment step, update covariance A ← A + φφ^T. 3) Inference: Calculate bonus β = √(φ^T A^(-1) φ) (or TS equivalent). 4) Scale: Normalize β using running stats. 5) Train: Add β to environment reward; proceed with standard Bellman update.
- **Design tradeoffs**: Use Sherman-Morrison (Rank-1 update) for A^(-1) to avoid O(d^3) cost per step. Start with CAE (0 params). If environment has sparse rewards and critic learns slowly, switch to CAE+ (adds IDN).
- **Failure signatures**: Unstable Learning (rewards spike then flatline) - missing scaling strategy. Slow Exploration (agent sticks to initial states) - switch to CAE+. Compute Bottleneck (step time too high) - implement Rank-1 updates.
- **First 3 experiments**: 1) Implement CAE on MuJoCo "Swimmer" using SAC. 2) Run same experiment but disable scaling algorithm. 3) Implement CAE+ on sparse MiniHack task "MultiRoom-N4".

## Open Questions the Paper Calls Out
- The authors suggest investigating the application of additional linear MAB techniques beyond the two adopted in this study (UCB and Thompson Sampling)
- The paper does not explore adaptive mechanisms for the exploration coefficient α, which varies significantly across environments
- The impact of the empirical scaling strategy on the theoretical sub-linear regret bounds is not analyzed

## Limitations
- Performance depends heavily on the critic's internal representation quality, which is not specified
- Implementation details for the auxiliary network structure in CAE+ remain partially unspecified
- The scaling strategy serves as a heuristic stability patch without theoretical justification
- Exact parameter counts for CAE+ are not provided, though claimed to be <1%

## Confidence
- **High confidence**: The mechanism of repurposing critic embeddings for exploration bonuses is clearly specified and theoretically justified
- **Medium confidence**: The scaling strategy (Algorithm 1) is well-described and ablation studies confirm its importance
- **Medium confidence**: CAE+ decoupling through inverse dynamics is conceptually sound, but implementation details remain partially unspecified

## Next Checks
1. Implement CAE on Swimmer-v4 with SAC and verify it matches or exceeds baseline performance
2. Disable Algorithm 1 scaling on Walker2d-v4 and confirm performance collapse as shown in Figure 3
3. Implement CAE+ on MultiRoom-N4-Locked and verify it significantly outperforms CAE and random exploration baselines