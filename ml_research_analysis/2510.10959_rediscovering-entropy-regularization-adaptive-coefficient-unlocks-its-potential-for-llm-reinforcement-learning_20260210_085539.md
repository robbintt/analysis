---
ver: rpa2
title: 'Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential
  for LLM Reinforcement Learning'
arxiv_id: '2510.10959'
source_url: https://arxiv.org/abs/2510.10959
tags:
- entropy
- exploration
- arxiv
- policy
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of policy entropy collapse
  in LLM reinforcement learning, where overly deterministic policies hinder exploration
  and limit reasoning performance. The authors propose Adaptive Entropy Regularization
  (AER), a framework that dynamically adjusts entropy regularization coefficients
  through three components: difficulty-aware coefficient allocation, initial-anchored
  target entropy, and dynamic global coefficient adjustment.'
---

# Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential for LLM Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2510.10959
- **Source URL**: https://arxiv.org/abs/2510.10959
- **Reference count**: 23
- **Primary result**: Adaptive entropy regularization improves LLM RL reasoning accuracy and exploration, with Qwen3-8B-Base achieving up to 55.4% pass@1 and 76.0% pass@32 on mathematical benchmarks.

## Executive Summary
This paper addresses the challenge of policy entropy collapse in LLM reinforcement learning, where overly deterministic policies hinder exploration and limit reasoning performance. The authors propose Adaptive Entropy Regularization (AER), a framework that dynamically adjusts entropy regularization coefficients through three components: difficulty-aware coefficient allocation, initial-anchored target entropy, and dynamic global coefficient adjustment. Experimental results on mathematical reasoning benchmarks show that AER consistently improves reasoning accuracy and exploration capability compared to baseline methods, with Qwen3-8B-Base achieving up to 55.4% pass@1 and 76.0% pass@32 accuracy. The adaptive approach enables more effective exploration without causing instability from entropy explosion.

## Method Summary
The paper proposes Adaptive Entropy Regularization (AER) to address entropy collapse in LLM reinforcement learning. AER dynamically adjusts entropy regularization coefficients through three components: difficulty-aware coefficient allocation (assigning higher coefficients to harder samples), initial-anchored target entropy (maintaining target entropy relative to initial policy entropy), and dynamic global coefficient adjustment (scaling all coefficients based on overall entropy trends). This framework is applied to mathematical reasoning tasks using PPO, enabling better exploration and more effective learning by preventing premature policy convergence to deterministic behaviors.

## Key Results
- Qwen3-8B-Base achieves 55.4% pass@1 and 76.0% pass@32 accuracy on mathematical reasoning benchmarks
- AER consistently outperforms baseline entropy regularization methods across multiple model sizes
- The adaptive coefficient mechanism prevents both entropy collapse and explosion during training

## Why This Works (Mechanism)
Standard entropy regularization in LLM reinforcement learning often leads to entropy collapse, where policies become overly deterministic and exploration diminishes. AER addresses this by dynamically adjusting regularization coefficients based on sample difficulty and overall entropy trends. The difficulty-aware allocation ensures harder problems receive more exploration support, while the initial-anchored target prevents the policy from becoming too deterministic. The dynamic global adjustment maintains appropriate entropy levels throughout training, balancing exploration and exploitation.

## Foundational Learning
- **Entropy regularization**: Technique to prevent policy collapse by penalizing low entropy in action distributions. Needed to maintain exploration in RL. Quick check: Does the policy show high entropy early in training?
- **Policy collapse**: Phenomenon where RL policies become overly deterministic, reducing exploration. Critical to understand failure modes in LLM reasoning. Quick check: Are the top-k action probabilities converging to 1.0?
- **Difficulty-aware allocation**: Assigning different entropy coefficients based on sample complexity. Enables targeted exploration where needed. Quick check: Are harder samples receiving higher entropy coefficients?
- **PPO (Proximal Policy Optimization)**: RL algorithm used for stable policy updates. Foundation for implementing AER. Quick check: Are policy updates stable and bounded by clipping?
- **Pass@k metric**: Evaluation metric measuring if any of k sampled solutions are correct. Standard for reasoning tasks. Quick check: Does pass@1 improve consistently with AER?

## Architecture Onboarding
- **Component map**: Problem samples -> Difficulty analyzer -> Coefficient allocator -> PPO policy -> Entropy regularizer -> Model output
- **Critical path**: Input problem → Difficulty assessment → Coefficient assignment → Policy sampling → Entropy regularization → Output generation
- **Design tradeoffs**: Fixed vs. adaptive coefficients (adaptive enables better exploration but adds complexity), global vs. per-sample adjustment (per-sample is more precise but computationally heavier), target entropy anchoring (maintains stability but requires careful initialization)
- **Failure signatures**: Entropy explosion (coefficients too high, leading to random policies), premature convergence (coefficients too low, leading to policy collapse), instability in training curves
- **3 first experiments**: 1) Train baseline PPO with fixed entropy coefficient and observe entropy collapse, 2) Apply AER and compare entropy trajectories, 3) Evaluate pass@1 and pass@32 metrics for both approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to mathematical reasoning benchmarks, unclear if AER generalizes to other domains
- Improvements in pass@1 accuracy are modest (e.g., 55.4% for Qwen3-8B-Base) and may not be transformative
- No ablation study to determine individual contributions of AER's three components

## Confidence
- **High**: Entropy collapse occurs with standard regularization and AER mitigates this problem
- **Medium**: AER achieves 55.4% pass@1 for Qwen3-8B-Base on mathematical benchmarks
- **Low**: The specific mechanisms and relative contributions of AER's three components are not quantitatively analyzed

## Next Checks
1. Conduct ablation studies to isolate the impact of each AER component (difficulty-aware allocation, initial-anchored target entropy, dynamic global adjustment) on performance.
2. Extend experiments to diverse reasoning domains (e.g., code generation, commonsense reasoning) to assess generalizability beyond mathematical tasks.
3. Perform quantitative analysis of coefficient trajectories and their correlation with exploration metrics to clarify the adaptive mechanism's role.