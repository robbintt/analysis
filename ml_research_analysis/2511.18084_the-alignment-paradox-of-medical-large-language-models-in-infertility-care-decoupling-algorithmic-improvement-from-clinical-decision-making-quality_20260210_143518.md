---
ver: rpa2
title: 'The Alignment Paradox of Medical Large Language Models in Infertility Care:
  Decoupling Algorithmic Improvement from Clinical Decision-making Quality'
arxiv_id: '2511.18084'
source_url: https://arxiv.org/abs/2511.18084
tags:
- clinical
- alignment
- reasoning
- grpo
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study compares four alignment strategies\u2014SFT, DPO, GRPO,\
  \ and ICL\u2014for medical LLMs in infertility treatment decision-making. Using\
  \ a dataset of 8,000+ EHRs and a dual-layer evaluation combining automatic metrics\
  \ with blinded physician assessments, the authors reveal an alignment paradox: while\
  \ GRPO achieves highest algorithmic accuracy (77.14% avg."
---

# The Alignment Paradox of Medical Large Language Models in Infertility Care: Decoupling Algorithmic Improvement from Clinical Decision-making Quality

## Quick Facts
- **arXiv ID**: 2511.18084
- **Source URL**: https://arxiv.org/abs/2511.18084
- **Reference count**: 36
- **Primary result**: GRPO achieves highest algorithmic accuracy (77.14%) but clinicians prefer SFT for clearer reasoning (p=0.035) and higher treatment feasibility (p=0.019)

## Executive Summary
This study investigates the alignment paradox in medical LLMs for infertility treatment decision-making, revealing that algorithmic optimization through reinforcement learning can erode clinical trust. Using a dataset of 8,000+ EHRs and dual-layer evaluation combining automatic metrics with blinded physician assessments, the authors demonstrate that while GRPO achieves superior algorithmic accuracy, clinicians consistently prefer the SFT model for clearer reasoning and higher treatment feasibility. The findings highlight a fundamental tension between optimizing for benchmark performance and maintaining clinically interpretable, feasible reasoning pathways.

## Method Summary
The study compares four alignment strategies—SFT, DPO, GRPO, and ICL—for medical LLMs in infertility treatment decision-making. A Pyramid dataset was constructed from 8,201 EHRs, organized into General, Confusion, and Human Enhancement layers targeting rare/complex cases. The OpenBioLLM-8B backbone was first trained with SFT, then fine-tuned using each alignment method. Evaluation employed both automatic metrics and blinded physician assessments through a Doctor-in-the-loop framework, measuring accuracy, reasoning quality, treatment feasibility, and hallucination rates.

## Key Results
- GRPO achieves highest algorithmic accuracy (77.14% avg. accuracy, 50.64% macro F1) but SFT wins clinician preference (51.2% winning rate vs. GRPO's 26.2%)
- SFT significantly outperforms GRPO in reasoning clarity (p=0.035) and treatment feasibility (p=0.019)
- GRPO excels in rare/long-tail cases (e.g., PGT-M +20.9%) but underperforms in ambiguous categories like ICSI
- Reinforcement-based alignment decouples decision accuracy from reasoning transparency, optimizing for outcomes while sacrificing interpretability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reinforcement-based alignment (GRPO) decouples decision accuracy from reasoning transparency.
- **Mechanism:** GRPO optimizes a composite reward based on structured output correctness, incentivizing shortcuts that maximize reward without preserving stepwise causal logic. SFT retains sequential probability of the entire clinical narrative, preserving "human-ness" even with slightly lower accuracy.
- **Core assumption:** The composite reward function fails to capture process-level constraints of clinical validity.
- **Evidence anchors:** GRPO achieves highest algorithmic accuracy while clinicians prefer SFT for clearer reasoning; reward overoptimization produces statistically correct yet clinically plausible answers; benchmark success with clinical failure corroborates RL can optimize for benchmarks without improving patient-level utility.

### Mechanism 2
- **Claim:** Hierarchical data curation skews RL capacity toward long-tail precision at cost of ambiguous, mid-frequency stability.
- **Mechanism:** The "Human Enhancement" layer targeting rare/complex cases provides concentrated gradient signals for low-frequency events, but mid-frequency ambiguous categories (e.g., ICSI) have fuzzier reward boundaries, causing models to solve rare cases while losing robustness for nuanced ambiguous cases.
- **Core assumption:** ICSI represents a clinically ambiguous region where multiple treatment paths are valid, confusing binary reward signals.
- **Evidence anchors:** GRPO excels in rare/long-tail cases but underperforms in ambiguous categories like ICSI; GRPO amplifies structured decision pathways while dampening mid-range, ambiguous ones.

### Mechanism 3
- **Claim:** Token-level preference alignment fails in long-context clinical reasoning due to signal dilution.
- **Mechanism:** DPO optimizes by contrasting preferred vs. rejected responses, but in long Chain-of-Thought reasoning the majority of tokens are identical, diluting gradient signals from few differing tokens and causing convergence to simplified dominant patterns rather than distinct preference boundaries.
- **Core assumption:** Initial Diagnosis task length exceeds effective context window for stable DPO signal propagation without token-level weighting.
- **Evidence anchors:** DPO shows instability in initial diagnosis task with substantial drops in partial and exact matches; long-form reasoning dilutes token-level preference signal, causing convergence to over-simplified patterns.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) vs. Outcome Supervision**
  - **Why needed here:** The alignment paradox exists because authors evaluate both journey (CoT/Reasoning) and destination (Accuracy/Outcome). Understanding RL optimizes destination while SFT preserves journey is central.
  - **Quick check question:** If a model predicts correct ART strategy but justifies with hallucinated medical fact, would GRPO or SFT be more likely to generate this? (Answer: GRPO, as reward is likely outcome-based).

- **Concept: Reward Hacking (Goodhart's Law)**
  - **Why needed here:** The paper demonstrates Goodhart's Law—when a measure becomes a target, it ceases to be a good measure. Models "hacked" accuracy metric by producing clinically infeasible plans matching ground truth label.
  - **Quick check question:** Why did SFT's winning rate (51.2%) decouple from GRPO's accuracy (77.14%)?

- **Concept: The "Alignment Tax"**
  - **Why needed here:** Trade-off between specialized performance and general reasoning validity. This paper quantifies that tax: you pay for accuracy with reasoning clarity.
  - **Quick check question:** What dimension of clinical trust did GRPO actually improve compared to SFT despite overall preference drop?

## Architecture Onboarding

- **Component map:** OpenBioLLM-8B (LLaMA derivative) -> Pyramid Dataset (General/Confusion/Human-enhanced layers) -> SFT Foundation -> GRPO/DPO alignment -> Dual-Layer Evaluation (Auto-metrics + Blinded Doctor-in-the-loop)

- **Critical path:**
  1. SFT Foundation must establish basic CoT format; failure corrupts all downstream alignment
  2. Pyramid Construction must correctly identify "Confusion" layer; misdiagnosing ICSI ambiguity misguides GRPO rewards
  3. Doctor-in-the-loop is only mechanism to detect alignment paradox; automatic metrics are insufficient

- **Design tradeoffs:**
  - GRPO vs. SFT: GRPO for long-tail accuracy (PGT-M cases); SFT for clinician trust and feasibility
  - DPO vs. GRPO: GRPO robust but complex; DPO simpler but unstable on long reasoning chains
  - Reward Complexity: Simple accuracy rewards drive paradox; complex process rewards harder to define and verify

- **Failure signatures:**
  - "ICSI Collapse": RL models systematically degrade on ambiguous, mid-frequency categories with overlapping guidelines
  - "Confident Hallucination": GRPO reduces factual hallucinations but may increase reasoning hallucinations (valid logic, invalid premises)
  - DPO Diagnosis Drop: Sudden crash in partial match scores for unstructured text tasks

- **First 3 experiments:**
  1. Verify paradox by retraining GRPO with SFT-only baseline on held-out set; run Doctor-in-the-loop evaluation to confirm clinician preference (p=0.035) holds
  2. Stress test ambiguity by isolating ICSI and Short-protocol IVF subsets; measure variance in GRPO rewards vs. SFT likelihoods to confirm fuzzy boundaries cause RL instability
  3. Reward ablation by modifying GRPO reward function to include "Feasibility Penalty" (penalizing incompatible protocol/drug combinations) and observe if winning rate gap between GRPO and SFT closes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the alignment paradox persist in larger medical LLMs (>30B parameters), or do richer clinical priors at scale mitigate reward-induced distribution drift?
- **Basis in paper:** "Another critical direction is conducting scaling experiments with mid-size and large medical LLMs (>30B parameters) to determine whether the alignment paradox persists at larger scales, as bigger models may encode richer clinical priors and become less sensitive to reward-induced distribution drift."
- **Why unresolved:** This study evaluated only OpenBioLLM-8B due to compute constraints; scaling effects on alignment dynamics remain unknown.
- **What evidence would resolve it:** Training GRPO-aligned models at 30B+ parameters on identical infertility data, then comparing algorithmic metrics versus physician preference ratings to determine if divergence narrows or persists.

### Open Question 2
- **Question:** Can reasoning-oriented reward designs that penalize inconsistent causal steps preserve clinical interpretability while maintaining outcome accuracy?
- **Basis in paper:** "Future work should prioritize developing clinically grounded reward engineering, especially by incorporating structured ART guidelines to penalize inconsistent reasoning steps, unsupported causal jumps, and deviations from established treatment pathways."
- **Why unresolved:** Current GRPO implementation uses answer-centric rewards that neglect process-level reasoning structure, causing reasoning drift without penalty.
- **What evidence would resolve it:** Designing multi-component rewards combining outcome correctness with reasoning fidelity scores from structured clinical guidelines, then re-evaluating physician trust ratings.

### Open Question 3
- **Question:** How does the alignment paradox manifest across multi-center datasets with varying treatment philosophies and inter-practitioner variability?
- **Basis in paper:** "Ground-truth diagnoses and treatment plans represent 'single-center ground truth,' which reflects one institution's practice style. Variability in stimulation preferences, dose selection philosophies, or PGT indications across centers may reduce the generalizability of the aligned model."
- **Why unresolved:** All 8,201 EHRs came from West China Second University Hospital; institutional and demographic biases may limit generalizability.
- **What evidence would resolve it:** Replicating SFT/GRPO comparison on multi-center ART datasets from geographically and philosophically diverse fertility clinics, then measuring preference consistency across institutional contexts.

### Open Question 4
- **Question:** Does the asymmetric improvement pattern (GRPO gains in rare PGT cases but losses in mid-frequency ICSI) generalize to other clinical domains with similar decision space distributions?
- **Basis in paper:** The authors show GRPO "amplifies structured decision pathways while dampening mid-range, ambiguous ones" and note this reflects "sensitivity to data distribution and reward granularity," but whether this pattern is domain-specific or fundamental to RL alignment remains untested.
- **Why unresolved:** This study examined only infertility treatment; structural relationship between decision category frequency, ambiguity, and alignment behavior requires cross-domain validation.
- **What evidence would resolve it:** Applying identical pyramid dataset construction and GRPO alignment to other multi-category clinical domains (e.g., oncology staging, cardiology risk stratification) with similar long-tail distributions.

## Limitations

- **Generalizability concerns:** The alignment paradox was demonstrated only in infertility care domain using OpenBioLLM-8B architecture, limiting broader applicability
- **Sample size bias:** The study relies on a relatively small pool of 15 physician evaluators, potentially introducing selection bias in clinical preference judgments
- **Dataset construction transparency:** The Pyramid dataset's "Human Enhancement" layer lacks detailed documentation on how rare cases were identified and validated, raising questions about sampling bias

## Confidence

- **High Confidence:** The empirical finding that GRPO achieves superior algorithmic accuracy (77.14%) while SFT receives higher clinician preference (51.2% winning rate, p=0.035) is well-supported by blinded evaluation methodology and statistical analysis
- **Medium Confidence:** Proposed mechanisms explaining decoupling between accuracy and reasoning quality (reward overoptimization, hierarchical data skew, token-level preference dilution) are plausible but require further mechanistic validation through controlled experiments
- **Low Confidence:** Generalizability of "ICSI collapse" phenomenon to other ambiguous clinical categories across different medical domains remains speculative without additional cross-domain validation

## Next Checks

1. **Cross-Domain Replication**: Apply same four alignment strategies (SFT, DPO, GRPO, ICL) to different medical specialty (e.g., oncology or cardiology) with comparable dataset size and conduct blinded physician evaluations to test if alignment paradox persists across domains.

2. **Process Reward Engineering**: Design and implement modified GRPO reward function that explicitly incorporates process-level reasoning constraints (e.g., penalizing non-sequitur transitions in Chain-of-Thought) and measure whether this reduces accuracy-reasoning decoupling while maintaining competitive performance.

3. **Long-Tail vs. Ambiguous Category Stress Test**: Conduct controlled ablation study isolating 20% rarest cases and 20% most ambiguous cases from dataset, training separate GRPO models on each subset to quantify whether observed performance patterns are driven by frequency distribution versus inherent task complexity.