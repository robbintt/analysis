---
ver: rpa2
title: A Practical Approach for Building Production-Grade Conversational Agents with
  Workflow Graphs
arxiv_id: '2505.23006'
source_url: https://arxiv.org/abs/2505.23006
tags:
- agent
- response
- tool
- agents
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid approach combining workflow graphs
  with fine-tuning to build production-grade conversational agents. The method addresses
  challenges in strict compliance and output formatting by structuring agent behavior
  into a directed acyclic graph, where each node has distinct prompts and execution
  rules.
---

# A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs

## Quick Facts
- arXiv ID: 2505.23006
- Source URL: https://arxiv.org/abs/2505.23006
- Authors: Chiwan Park; Wonjun Jang; Daeryong Kim; Aelim Ahn; Kichang Yang; Woosung Hwang; Jihyeon Roh; Hyerin Park; Hyosun Wang; Min Seok Kim; Jihoon Kang
- Reference count: 15
- Primary result: Hybrid workflow graph + fine-tuning approach improves task accuracy by 52% and format adherence by 50% over baselines

## Executive Summary
This paper presents a hybrid approach for building production-grade conversational agents that combines workflow graphs with fine-tuning to address strict compliance and output formatting challenges. The method structures agent behavior into a directed acyclic graph where each node has distinct prompts and execution rules, with fine-tuning using response masking to prevent interference between nodes. Experiments demonstrate significant improvements over baseline approaches, with the proposed method outperforming GPT-4o in both task accuracy and format adherence.

## Method Summary
The approach combines workflow graphs with fine-tuning to build production-grade conversational agents. The workflow graph is a directed acyclic graph where nodes represent LLM calls or tool invocations, each with specific system prompts and constraints. The fine-tuning process uses response masking to exclude assistant outputs from other graph nodes during training, preventing cross-node contamination. The method was validated on an e-commerce agent that recommends products and assists with purchases, showing significant improvements in task accuracy and format adherence compared to baseline approaches.

## Key Results
- 52% improvement in task accuracy compared to baseline approaches
- 50% improvement in format adherence over baseline methods
- Outperformed GPT-4o performance on the same tasks
- Successful deployment of real-world e-commerce agent

## Why This Works (Mechanism)

### Mechanism 1: Constraint Decomposition via Per-State System Prompts
Distributing business rules across graph nodes with node-specific prompts improves compliance compared to monolithic prompting. Each LLM-calling node receives a focused, shorter system prompt encoding only the constraints relevant to that state, reducing prompt length and cognitive load on the model. This addresses the problem that longer prompts degrade accuracy, as shown in related work.

### Mechanism 2: Response Masking During Fine-Tuning
Masking assistant responses generated by other graph nodes during training preserves node-specific instruction fidelity. When training node v1, responses from node v2 appearing in the conversation history are excluded from loss calculation. This prevents the model from learning to produce v2-style responses under v1's system prompt, maintaining clear boundaries between node behaviors.

### Mechanism 3: History Manipulation for Hallucination Control
Selectively truncating conversation history at specific nodes reduces hallucination from irrelevant prior context. Nodes like purchase_message invoke a modify_history subroutine that strips all but the most recent turn, preventing the LLM from hallucinating based on earlier product searches or recommendations. This targeted pruning limits access to irrelevant prior information.

## Foundational Learning

- **Directed Acyclic Graphs (DAGs) for workflow orchestration**: Why needed - models agent execution as graph traversal where nodes represent LLM calls or tool invocations, and edges define valid state transitions. Quick check - Can you sketch a 4-node DAG for an agent that greets → searches → summarizes → exits?
- **Constrained decoding / structured generation**: Why needed - tool-calling nodes require outputs matching JSON schemas; constrained decoding enforces valid syntax before tool invocation. Quick check - What happens if an LLM outputs malformed JSON to a tool expecting {"query": string, "limit": integer}?
- **Loss masking in supervised fine-tuning**: Why needed - prevents gradient updates on tokens that should not influence the training objective (e.g., responses from other graph nodes). Quick check - In a sequence [sys, user, assistant_v1, user, assistant_v2], which assistant tokens would you mask if training node v1?

## Architecture Onboarding

- **Component map**: Workflow Graph (DAG with nodes V, edges E, entry v_init, terminal v_final) -> LLM Nodes (system_prompt_s, modify_history(), constrained decoding) -> Tool Nodes (schemas, API integrations) -> Training Pipeline (prototype -> annotation -> response-masked fine-tuning) -> Serving Layer (vLLM/SGLang + graph orchestration)
- **Critical path**: 1) Define business constraints and map to graph states 2) Implement prototype with SOTA LLM to bootstrap data collection 3) Collect annotated conversations with graph traversal logging 4) Fine-tune with response masking; merge LoRA adapters 5) Deploy with graph orchestration layer
- **Design tradeoffs**: Graph complexity vs. maintainability (more nodes = finer control but harder debugging); History truncation vs. context richness (reduces hallucination but may lose personalization signals); Fine-tuning investment vs. prompt engineering (fine-tuning yields higher compliance but requires annotation infrastructure)
- **Failure signatures**: Low format adherence despite graph (likely prompt too long or few-shot examples misaligned with node context); Tool selection errors after fine-tuning (check if response masking correctly excluded cross-node outputs); Hallucination in final responses (audit modify_history routines - may be retaining irrelevant context)
- **First 3 experiments**: 1) Baseline comparison: Measure accuracy and format adherence of Basic (single prompt) vs. Workflow Graph on 50 test conversations using same underlying LLM 2) Ablation on history manipulation: Disable modify_history in one node; measure hallucination rate increase in that state 3) Fine-tuning validation: Train two models - with and without response masking - on same dataset; compare per-node instruction following on held-out test set

## Open Questions the Paper Calls Out

### Open Question 1
Can LLM-based user simulation effectively replace or augment human annotators for collecting training data in workflow-based agents while maintaining data quality and reducing demographic bias? The authors suggest this could be explored in future work, as their collected conversations may exhibit demographic bias due to limited annotator diversity.

### Open Question 2
What is the actual correlation between LLM-as-a-Judge evaluation scores and human preference judgments for workflow-based agents, particularly regarding language fluency? The paper notes that language fluency significantly influenced human preference in ways that were difficult to capture via LLM-as-a-Judge evaluation.

### Open Question 3
How does the response masking fine-tuning strategy scale to workflow graphs with significantly more states and deeper branching complexity? The paper demonstrates success with a moderate e-commerce workflow graph, but the training strategy's effectiveness with much larger graphs is unstated.

### Open Question 4
Does the DAG-based framework with per-state prompting and fine-tuning generalize effectively to other compliance-sensitive domains (healthcare, financial services, legal)? The authors claim the framework's generic design allows adaptation across various domains, but only provide evidence from e-commerce.

## Limitations
- The paper reports significant improvements but does not provide sufficient methodological details for independent replication, particularly around dataset construction and graph design specifics
- The response masking technique lacks direct ablation studies showing its individual contribution versus other architectural components
- The 100 million token fine-tuning dataset and evaluation methodology are not fully disclosed, making it difficult to assess generalizability of results

## Confidence
- **High confidence**: The workflow graph architecture with per-state system prompts is technically sound and aligns with established best practices for structured generation
- **Medium confidence**: The claimed performance improvements are plausible given the mechanism, but the magnitude may be overstated due to limited transparency in experimental design
- **Low confidence**: The relative contribution of individual components (response masking vs. graph structure vs. history manipulation) to the reported gains is unclear without proper ablation studies

## Next Checks
1. Conduct an ablation study isolating response masking: Train identical models with and without this technique on the same dataset, measuring per-node instruction following and format compliance
2. Implement a minimal prototype of the workflow graph architecture using an open-source dataset (e.g., MultiWOZ with schema-constrained tool calling) to verify the claimed accuracy improvements
3. Perform a cost-benefit analysis comparing the graph-based approach against simpler alternatives like larger context windows or more sophisticated prompt engineering, particularly for domains with fewer conditional constraints