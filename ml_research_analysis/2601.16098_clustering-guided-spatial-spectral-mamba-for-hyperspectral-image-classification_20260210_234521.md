---
ver: rpa2
title: Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification
arxiv_id: '2601.16098'
source_url: https://arxiv.org/abs/2601.16098
tags:
- mamba
- spatial
- cluster
- feature
- spectral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of defining efficient and adaptive
  token sequences for Hyperspectral Image (HSI) classification using Mamba models.
  The proposed Clustering-guided Spatial-Spectral Mamba (CSSMamba) framework integrates
  a clustering mechanism into a spatial Mamba architecture to reduce sequence length
  and improve feature learning.
---

# Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification

## Quick Facts
- **arXiv ID:** 2601.16098
- **Source URL:** https://arxiv.org/abs/2601.16098
- **Reference count:** 9
- **Primary result:** Proposed CSSMamba achieves 97.55% overall accuracy (OA) on Pavia University HSI dataset, outperforming ViT, ConvNeXt, and MambaHSI baselines.

## Executive Summary
This paper addresses the challenge of defining efficient and adaptive token sequences for Hyperspectral Image (HSI) classification using Mamba models. The proposed Clustering-guided Spatial-Spectral Mamba (CSSMamba) framework integrates a clustering mechanism into a spatial Mamba architecture to reduce sequence length and improve feature learning. It also incorporates a spectral Mamba module and an Attention-Driven Token Selection mechanism to optimize token sequencing. Experiments on Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.

## Method Summary
CSSMamba introduces a Learnable Clustering Module that uses ground truth labels to group features and update cluster centers via Exponential Moving Average (EMA). The Spatial Mamba branch partitions tokens by cluster, applies Dual Attention for token prioritization and selection, processes through parallel local Mamba blocks, then reconstructs the global map. A separate Spectral Mamba branch handles spectral features. The architecture reduces effective sequence length by clustering semantically similar pixels, mitigating correlation decay in standard Mamba processing. Dual Attention uses a gated fusion of dynamic self-attention and static cluster priors to score and sort tokens before Mamba processing. The framework requires ground truth labels during training for the clustering updates.

## Key Results
- Achieves 97.55% OA on Pavia University dataset, surpassing ViT (93.91%), ConvNeXt (87.11%), and MambaHSI (94.69%)
- Optimal cluster number K=3 identified through ablation studies
- Cluster loss contributes approximately 0.8% OA improvement over cross-entropy alone
- Demonstrates better boundary preservation in classification maps compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1: Clustering-Induced Sequence Compression
Partitioning a global image sequence into semantically coherent clusters reduces effective sequence length, mitigating correlation decay in standard Mamba scanning of long sequences. The Learnable Clustering Module assigns pixels to K semantic groups, allowing the Mamba SSM to process shorter, locally homogeneous subsequences independently before reconstructing the global map. This maintains the receptive field while reducing computational distance between related tokens.

### Mechanism 2: Dual-Attention Token Prioritization
Ordering tokens based on a hybrid "importance score" combining dynamic self-attention and static cluster priors enhances Mamba's state propagation by prioritizing salient features. The Dual Attention Module computes a score using gated fusion of dynamic attention and static cluster prior, sorting tokens by this score before entering the Mamba block. This theoretically allows the state space model to integrate high-value information earlier in the scanning process.

### Mechanism 3: Momentum-Based Center Evolution
Updating cluster centers via Exponential Moving Average (EMA) based on ground truth groups stabilizes training and enforces semantic consistency without requiring backpropagation through clustering logic. The Learnable Clustering Module uses GT labels to group features and update centers using momentum term γ, providing stable anchors for token routing throughout training iterations.

## Foundational Learning

- **Concept:** State Space Models (SSMs) / Mamba Basics
  - **Why needed here:** Understanding how SSMs compress history into a hidden state (avoiding O(N²) attention matrix) is necessary to see why "sequence order" and "length" are critical bottlenecks.
  - **Quick check question:** How does the computational complexity of a standard Mamba block scale with sequence length N compared to a standard Vision Transformer?

- **Concept:** Hyperspectral Data Cubes (H×W×C)
  - **Why needed here:** The architecture explicitly splits processing into "Spatial" and "Spectral" branches. Understanding that HSI data has dense spectral signature at every pixel is necessary to grasp why the Spectral Mamba branch exists alongside the spatial one.
  - **Quick check question:** What does the "Spectral Branch" flatten to create its input sequence? (Answer: It flattens spectral bands, treating pixels or groups of pixels as the sequence).

- **Concept:** Softmax & Gating Mechanisms
  - **Why needed here:** The Dual Attention module relies on a sigmoid gate σ(α) to fuse dynamic and static signals. Understanding how this gate controls flow of information is key to debugging token selection logic.
  - **Quick check question:** In Eq. (3), if σ(α)→1, which component dominates the token scoring?

## Architecture Onboarding

- **Component map:** Input (B, C, H, W) -> Learnable Clustering Module (GT update centers, assign cluster map M) -> Spatial Branch (Partition by M -> Dual Attention (Sort/Filter) -> Local Mamba (Parallel) -> Scatter -> Global Mamba) -> Spectral Branch (Flatten spectra -> Mamba) -> Head (Fusion -> Classification)

- **Critical path:** The dependency on GT-Guided Feature Grouping during training. Unlike fully unsupervised clustering, this system requires ground truth labels to calculate batch means (μ_batch) that drive cluster center updates. If dataloader does not provide valid labels during training, clustering logic will break or degrade.

- **Design tradeoffs:**
  - **Number of Clusters (K):** Table IV suggests K=3 is optimal for Pavia dataset. Setting K too low re-introduces "long sequence" problem; setting K too high fragments spatial context.
  - **Token Selection:** The "Top-k" selection in Dual Attention module reduces computation but permanently discards token information for that pass, which may remove edge details if threshold is too aggressive.

- **Failure signatures:**
  - Fragmentation: Classification maps look "noisy" or speckled
  - Class Collapse: If Cluster Loss dominates, features might over-cluster, pushing distinct classes into overlapping compact balls
  - Boundary Smoothing: If Global Mamba step is too strong relative to Local Mamba, fine boundaries may blur

- **First 3 experiments:**
  1. Cluster Ablation: Run on PaviaU with K ∈ {1, 2, 3, 4, 5} to reproduce the peak at K=3 found in Table IV
  2. Loss Ablation: Train with only Cross Entropy vs. CE + Cluster Loss to verify ~0.8% OA gain
  3. Visual Inspection: Compare classification maps against baseline (e.g., SVM or CNN) specifically looking at "boundary preservation"

## Open Questions the Paper Calls Out

- **Question:** How does the reliance on ground truth labels for the "GT-Guided Feature Grouping" and center updates affect the model's adaptability to unsupervised or semi-supervised scenarios where such labels are unavailable or sparse?
  - **Basis in paper:** Section II.D explicitly states the Learnable Clustering Module "utilizes the ground truth labels to group the feature representations" during training, implying a potential limitation for applications where dense ground truth is unavailable.

- **Question:** Is the empirically determined optimal number of clusters per class (3 for Pavia University) robust across datasets with different class imbalance ratios or spectral complexities, or does it require extensive dataset-specific tuning?
  - **Basis in paper:** Table IV demonstrates that performance varies significantly with the number of clusters (peaking at 3), but the paper does not provide a theoretical justification or a rule of thumb for generalizing this hyperparameter to new datasets.

- **Question:** Does the added computational overhead of the Learnable Clustering Module and Dual Attention Token Selection negate the inference speed benefits of the Mamba architecture compared to standard transformer baselines?
  - **Basis in paper:** The results section focuses entirely on classification accuracy (OA, AA, Kappa) and visual quality, omitting metrics on training/inference time or FLOPs, leaving the actual efficiency of the proposed "efficient" token sequencing unquantified.

## Limitations
- Reliance on ground truth labels for clustering during training limits unsupervised or semi-supervised applications
- Performance sensitivity to the number of clusters (K) requires dataset-specific tuning
- Computational overhead of clustering and dual-attention mechanisms remains unquantified
- Permanent token discarding may lose information for minority classes or fine-grained features

## Confidence

- **High Confidence:** Core architectural components are clearly defined and implementable; EMA-updated cluster centers based on ground truth are well-specified
- **Medium Confidence:** Performance improvements are supported by experimental results, though comparison methodology and statistical significance testing are not fully detailed
- **Low Confidence:** Generalization claims beyond three tested datasets are not substantiated; computational complexity analysis is absent

## Next Checks

1. **Cross-Dataset Generalization Test:** Evaluate CSSMamba on additional HSI datasets with varying characteristics (different spatial resolutions, class distributions, and noise levels) to verify whether the reported K=3 optimization generalizes or requires dataset-specific tuning.

2. **Computational Overhead Analysis:** Measure and compare training time, memory consumption, and inference speed between CSSMamba and baseline methods (CNN, Transformer, MambaHSI) on identical hardware to quantify practical efficiency trade-offs of the clustering-based approach.

3. **Robustness to Label Noise:** Systematically introduce varying levels of label corruption (0% to 30%) in training data and evaluate how CSSMamba's performance degrades compared to standard MambaHSI, particularly examining whether GT-guided clustering mechanism amplifies or mitigates label noise effects.