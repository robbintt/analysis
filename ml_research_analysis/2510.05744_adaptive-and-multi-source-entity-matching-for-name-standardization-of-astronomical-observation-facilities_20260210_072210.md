---
ver: rpa2
title: Adaptive and Multi-Source Entity Matching for Name Standardization of Astronomical
  Observation Facilities
arxiv_id: '2510.05744'
source_url: https://arxiv.org/abs/2510.05744
tags:
- data
- entities
- mapping
- sssom
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a methodology for generating multi-source
  mappings of astronomical observation facilities by computing adaptable similarity
  scores using NLP techniques (Bag-of-Words, sequential, and surface approaches) across
  eight semantic artifacts including Wikidata and astronomy-oriented resources. The
  method employs filtering criteria, multiple similarity scores (Levenshtein, TF-IDF,
  sentence transformers, and LLM embeddings), and LLM validation to ensure accurate
  entity matching.
---

# Adaptive and Multi-Source Entity Matching for Name Standardization of Astronomical Observation Facilities

## Quick Facts
- arXiv ID: 2510.05744
- Source URL: https://arxiv.org/abs/2510.05744
- Reference count: 29
- Primary result: 100% accuracy on 30 annotated candidate pairs from AAS and PDS facility lists using LLM validation with DeepSeek-V3

## Executive Summary
This work presents a methodology for generating multi-source mappings of astronomical observation facilities by computing adaptable similarity scores using NLP techniques across eight semantic artifacts including Wikidata and astronomy-oriented resources. The method employs filtering criteria, multiple similarity scores (Levenshtein, TF-IDF, sentence transformers, and LLM embeddings), and LLM validation to ensure accurate entity matching. An experimental evaluation using DeepSeek-V3 achieved 100% accuracy on 30 annotated candidate pairs from AAS and PDS facility lists. The resulting mapping provides multi-source synonym sets with standardized labels per entity, intended for integration into IVOA Vocabularies and the OntoPortal-Astro platform.

## Method Summary
The method implements a pipeline that first applies structural filtering (location distance, launch date, aperture, class, identifier) to reduce candidate pairs, then computes multiple similarity scores (surface-level Levenshtein, acronym probability, digit matching, TF-IDF cosine, sentence transformer embeddings, and LLM embeddings) which are combined via weighted sum. Candidate pairs are ranked by global score and validated by an LLM (DeepSeek-V3) with early stopping after consecutive rejections. Results are exported in SSSOM format with justifications. The approach handles astronomical entities like spacecraft, observatories, and telescopes across eight sources including Wikidata, AAS, PDS, IAU-MPC, NAIF, NSSDC, SPASE, and IMCCE.

## Key Results
- 100% accuracy achieved on 30 annotated candidate pairs (19 true positives, 11 true negatives) from AAS and PDS facility lists
- LLM validation with DeepSeek-V3 outperformed AstroLLaMA embeddings despite being a generalist model
- Multi-source mapping approach provides flexible synonym sets with standardized labels per entity for integration into IVOA Vocabularies and OntoPortal-Astro

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Candidate Filtering Before Expensive Computation
Pre-filtering candidate pairs on structural attributes reduces the search space and prevents false positives from propagating to semantic scoring stages. The system applies hard rejection rules (location distance >4km, launch date mismatch, aperture mismatch, class mismatch, identifier mismatch) and acceptance rules (exact label match) before computing any similarity scores. This creates a three-tier difficulty split: (1) easy cases resolved by external ID linking, (2) moderate cases requiring surface/semantic scores, and (3) hard cases requiring LLM disambiguation.

### Mechanism 2: Weighted Multi-Signal Similarity Aggregation
Combining surface-level string matching with semantic embeddings captures both orthographic variations and conceptual equivalence that single metrics miss. Each candidate pair receives multiple scores: Levenshtein similarity (typos, translations), acronym probability, digit matching ratio, TF-IDF cosine similarity (token importance), sentence transformer embeddings (all-MiniLM-L6-v2), and LLM embeddings. A weighted sum produces a global score used for ranking.

### Mechanism 3: Iterative LLM Validation with Early Stopping
LLM validation provides semantic disambiguation beyond embedding similarity, and early stopping after consecutive rejections bounds computational cost. Candidate pairs are sorted by global score. Starting from the highest, each pair is presented to an LLM (DeepSeek-V3) with all textual features. The LLM must accept/reject and provide justification. After a configurable number of consecutive rejections, the process terminates.

## Foundational Learning

- **SKOS (Simple Knowledge Organization System) and SKOS:exactMatch**: The output ontology uses SKOS vocabulary to represent synonym relationships across sources; understanding prefLabel, altLabel, and exactMatch semantics is required to interpret the TTL output format. Quick check: If entity A has `skos:exactMatch` entity B, and entity B has `skos:exactMatch` entity C, should A and C be considered synonyms?

- **SSSOM (Simple Standard for Sharing Ontology Mappings)**: All LLM-validated mappings are exported in SSSOM format with metadata (justification, confidence, tool, date); understanding this schema is necessary to consume or audit the mapping outputs. Quick check: What SSSOM field would you query to find all mappings where an LLM rejected a candidate pair?

- **Elasticsearch Name Resolution Architecture**: The final JSON output feeds an Elasticsearch API with `/resolve` and `/aliases` endpoints; understanding inverted index lookup patterns helps debug why certain aliases don't resolve. Quick check: If a user queries "Voyager" and the JSON only contains "Voyager 1" and "Voyager 2" as separate entries, what mechanism ensures both are returned?

## Architecture Onboarding

- Component map: [External Sources] → update.py → [Turtle files per source] → Internal ID resolving, enrichment → map_ontologies.py → [Candidate pairs N×M] → Filtering criteria → [Compatible pairs] → Score computation (Levenshtein, TF-IDF, embeddings) → Weighted sum → [Ranked pairs] → LLM validation loop → [SSSOM mappings] → Application views: CSV (IVOA) / JSON (Resolver) / TTL (OntoPortal)

- Critical path: The LLM validation step is the computational bottleneck; the paper notes DeepSeek-V3 (671B parameters) was used, and calls are sequential per candidate pair. Parallelization is not described.

- Design tradeoffs: Pivot vs. multi-source mapping (previous version used Wikidata as pivot; abandoned due to incompleteness), fixed weights vs. learned weights (weights are manually set, no learning from annotated data), generalist LLM vs. domain-specialized model (DeepSeek-V3 outperformed AstroLLaMA).

- Failure signatures: Low Levenshtein + high semantic score (likely acronym or multilingual variation), high score + LLM rejection (often indicates meronymy confusion), consecutive LLM rejections without matches (may indicate source lists have insufficient overlap or filtering too aggressive).

- First 3 experiments: (1) Reproduce the 30-pair evaluation to establish baseline score calibration, (2) Ablate one score at a time to measure each score's contribution to ranking quality, (3) Test early-stopping threshold sensitivity to identify optimal stopping point before deploying at scale.

## Open Questions the Paper Calls Out

- Can a fine-tuned Small Language Model (SLM) trained on the specific annotated facility dataset outperform the generalist DeepSeek-V3 model in validation accuracy and inference speed? The authors state they "hope to fine-tune a Small Language Model... that could run quicker and outperform DeepSeek due to its training on the specific task and data."

- Does integrating an agentic Model Context Protocol (MCP) server to allow online information retrieval improve the quality of mappings for entities with sparse metadata? The authors plan to "explore the use of an agentic MCP... to allow the validation LLM to search for further information online... countering the lack of information of some resources."

- How does the 19.8% error rate in the preliminary automated classification step impact the false negative rate of the subsequent "type mismatch" filtering criterion? The paper notes that the classifier reached only 80.20% accuracy, and the mapping strategy uses "type" as a rejecting criterion.

## Limitations
- Limited validation scale with only 30 annotated candidate pairs insufficient to characterize performance across full diversity of astronomical facility names
- Weight configuration opacity with unspecified complete weight values for the weighted sum formula
- Computational cost and scalability concerns with sequential LLM validation loop using 671B parameter model

## Confidence
- **High confidence**: Filtering mechanism (structural attribute validation before semantic scoring) is well-specified and methodologically sound
- **Medium confidence**: Weighted multi-signal similarity aggregation produces reasonable results but lacks ablation studies or cross-validation across entity types
- **Low confidence**: 100% accuracy claim and LLM validation approach lack sufficient evidence due to small annotated sample and absence of alternative comparisons

## Next Checks
1. Ablation study across entity types: Remove each similarity signal individually and measure precision@k for different astronomical facility categories
2. Scale validation to full source datasets: Apply complete pipeline to all 8 sources and measure precision/recall on 200-500 candidate pairs with error analysis
3. Alternative LLM validation approach: Replace DeepSeek-V3 with smaller open-source model and compare performance and computational cost with and without entity type information