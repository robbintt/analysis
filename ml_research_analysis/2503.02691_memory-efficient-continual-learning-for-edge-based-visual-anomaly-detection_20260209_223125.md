---
ver: rpa2
title: Memory Efficient Continual Learning for Edge-Based Visual Anomaly Detection
arxiv_id: '2503.02691'
source_url: https://arxiv.org/abs/2503.02691
tags:
- replay
- memory
- paste
- feature
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying Visual Anomaly
  Detection (VAD) models on edge devices under Continual Learning (CL) settings, where
  models must adapt to new data distributions while maintaining performance on previously
  seen tasks. The authors investigate STFPM and PaSTe VAD models combined with Replay-based
  CL strategies, proposing Compressed Replay techniques to reduce memory overhead.
---

# Memory Efficient Continual Learning for Edge-Based Visual Anomaly Detection

## Quick Facts
- arXiv ID: 2503.02691
- Source URL: https://arxiv.org/abs/2503.02691
- Reference count: 30
- Primary result: PaSTe VAD model with Compressed Replay reduces memory usage by up to 91.5% while maintaining competitive anomaly detection performance on edge devices

## Executive Summary
This paper addresses the challenge of deploying Visual Anomaly Detection (VAD) models on edge devices under Continual Learning (CL) settings, where models must adapt to new data distributions while maintaining performance on previously seen tasks. The authors investigate STFPM and PaSTe VAD models combined with Replay-based CL strategies, proposing Compressed Replay techniques to reduce memory overhead. The key finding is that PaSTe outperforms STFPM by 10% in pixel-level F1 score while being more memory-efficient. The structure of PaSTe, with shared frozen layers between student and teacher networks, reduces catastrophic forgetting compared to STFPM. Furthermore, Compressed Replay techniques achieve up to 91.5% memory reduction compared to traditional Replay, with Feature Replay providing 50% reduction without performance loss, while more aggressive compression (Feature Quantization and Product Quantization) provides greater memory savings at the cost of some accuracy.

## Method Summary
The study evaluates two VAD architectures (STFPM and PaSTe) under CL scenarios using MVTec Dataset with 10 sequential tasks. Both models employ student-teacher knowledge distillation where the student learns to match teacher features on normal images. PaSTe differs by sharing frozen backbone layers between student and teacher networks. The Replay strategy stores 100 samples (10% of dataset) from previous tasks to mitigate forgetting. Compressed Replay variants store intermediate features instead of raw images, with Feature Replay, Feature Quantization (8-bit), and Product Quantization (6 subvectors, 7 bits each) providing progressively more aggressive compression. Models are evaluated on pixel-level F1 score, image-level AUC ROC, and average forgetting percentage across all tasks.

## Key Results
- PaSTe outperforms STFPM by 10% in pixel-level F1 score while being more memory-efficient
- Compressed Replay achieves up to 91.5% memory reduction compared to traditional Replay
- Feature Replay provides 50% memory reduction without performance loss, while more aggressive compression reduces accuracy
- PaSTe's shared frozen layers reduce catastrophic forgetting compared to STFPM

## Why This Works (Mechanism)

### Mechanism 1: Shared Frozen Layers Reduce Catastrophic Forgetting
- Claim: PaSTe's shared frozen layers between student and teacher networks produce more consistent feature representations across tasks, reducing the need for aggressive weight updates during task transitions.
- Mechanism: By freezing initial layers and sharing them between both networks, the feature extractor remains stable across continual learning tasks. Only the trainable downstream portion adjusts, limiting the scope of weight changes that could overwrite prior knowledge.
- Core assumption: Consistent feature representations across tasks correlate with reduced forgetting in student-teacher VAD architectures.
- Evidence anchors:
  - [abstract] "The structure of PaSTe, with shared frozen layers between student and teacher networks, reduces catastrophic forgetting compared to STFPM."
  - [Section 5.1] "The motivation for this lower forgetting value can be found in the structure of the PaSTe model, where some layers are kept frozen and are shared between the Student and the Teacher. As a consequence, the initial layers generate more consistent feature representations across tasks..."
  - [corpus] Neighbor paper "On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing" addresses similar CL-VAD challenges but does not specifically validate the frozen-layer mechanism.
- Break condition: If input distributions across tasks require fundamentally different low-level features, frozen layers may produce inconsistent representations and increase forgetting rather than reduce it.

### Mechanism 2: Feature Replay Eliminates Redundant Forward Passes
- Claim: Storing intermediate feature maps instead of raw images in replay memory reduces memory by 50% without performance loss because the frozen shared layers produce deterministic outputs.
- Mechanism: Since PaSTe's initial layers are frozen, the same input image always produces the same intermediate feature map. Storing features directly skips redundant forward passes through frozen layers during replay training, and the feature representation occupies less space than the original image.
- Core assumption: Frozen layers produce sufficiently stable features that storing them is equivalent to storing images; no distribution shift affects feature validity over time.
- Evidence anchors:
  - [Section 5.2] "A big improvement in the Replay memory size is achieved when switching from Replay to Feature Replay. Indeed, with the same number of saved samples... there is a reduction of 50% of the Replay Memory size in terms of MBs, with no reduction in the F1 pixel level metric."
  - [Section 3.3] "Since these layers are shared and frozen, the trainable parameters of the network start after an intermediate layer, so the saved samples for Replay can be forwarded from there."
  - [corpus] "Online Continual Learning via Spiking Neural Networks with Sleep Enhanced Latent Replay" explores latent replay but in a different architectural context; direct comparison not available.
- Break condition: If frozen layers are later unfrozen for fine-tuning, stored features become invalid and must be recomputed.

### Mechanism 3: Aggressive Compression Trades Precision for Memory
- Claim: Feature Quantization (FQ) and Product Quantization (PQ) progressively reduce memory at the cost of anomaly detection precision because VAD requires high-fidelity feature representations.
- Mechanism: Quantization reduces the precision of stored feature values (8-bit quantization: 4x compression) or replaces features with codebook indices (PQ: higher compression). However, the student-teacher matching process depends on fine-grained feature differences to detect anomalies; compression introduces approximation errors that degrade this matching.
- Core assumption: The observed performance degradation is primarily due to feature quality loss, not other factors like codebook design.
- Evidence anchors:
  - [Section 5.2] "The Visual Anomaly Detection task is very precise, and a lot of precision in the extracted features is needed to perform the task well... altering them with a quantization process or more aggressively with Product Quantization, can lead to a decrease in the performances because it changes the feature distribution..."
  - [Table 2] Shows progressive F1 pixel score decline: Feature Replay (0.40) → FQ Replay (0.32) → PQ Replay (0.29)
  - [corpus] No direct corpus validation of the precision-compression tradeoff specific to VAD; this remains an empirical finding from the paper.
- Break condition: If anomaly detection tolerates coarser feature granularity (e.g., image-level rather than pixel-level), compression may have smaller performance impact.

## Foundational Learning

- Concept: **Student-Teacher Knowledge Distillation**
  - Why needed here: Both STFPM and PaSTe rely on a teacher network providing target features that the student learns to match on normal images. During inference, deviations indicate anomalies.
  - Quick check question: Can you explain why the student network cannot perfectly replicate teacher features on anomalous images after training only on normal images?

- Concept: **Catastrophic Forgetting in Continual Learning**
  - Why needed here: The paper's core challenge is adapting to new objects (tasks) without losing detection ability on previously learned objects. Understanding why neural networks overwrite prior knowledge is essential.
  - Quick check question: What happens to a model's performance on Task 1 data after it is trained sequentially on Tasks 2, 3, and 4 without any mitigation strategy?

- Concept: **Experience Replay Buffer**
  - Why needed here: The Replay approach stores samples from previous tasks and interleaves them with current task training. This is the baseline strategy that Compressed Replay variants optimize.
  - Quick check question: If your replay buffer has capacity for 100 samples and you have 10 tasks, how would you allocate samples across tasks to balance forgetting?

## Architecture Onboarding

- Component map:
  - Backbone (MCUNet-IN3): Pre-trained feature extractor; initial layers frozen in PaSTe, shared between student and teacher
  - Student branch: Trainable layers that learn to match teacher features on normal data
  - Teacher branch: Provides target feature representations; shares frozen backbone with student in PaSTe
  - Replay memory: Stores either images (Replay), feature maps (Feature Replay), or compressed features (FQ/PQ Replay)
  - Anomaly scoring: Computes distance between student and teacher feature maps at inference

- Critical path:
  1. Forward pass through frozen shared backbone → intermediate feature map
  2. Student and teacher branches process features separately
  3. Training: minimize student-teacher distance on mixed batch (current task + replay samples)
  4. Inference: large student-teacher distance indicates anomaly

- Design tradeoffs:
  - Memory vs. Performance: Feature Replay (50% reduction, no loss) vs. PQ Replay (91.5% reduction, ~27% F1 loss)
  - STFPM vs. PaSTe: PaSTe offers better CL performance but requires architectural commitment to frozen layers
  - Replay buffer size: More samples reduce forgetting but increase memory; paper uses 100 samples (10% of dataset)

- Failure signatures:
  - Sudden performance drop on early tasks after new task training → replay memory too small or sampling imbalance
  - Pixel-level F1 significantly lower than image-level → feature representations lack spatial precision (over-compression)
  - High variance across task performance → frozen layers may not generalize well across task distributions

- First 3 experiments:
  1. **Baseline validation**: Reproduce STFPM vs. PaSTe comparison with Replay on a 3-task subset; verify PaSTe shows lower average forgetting metric
  2. **Memory sweep**: Test Feature Replay with buffer sizes [25, 50, 100, 200] to characterize forgetting vs. memory curve for your target device constraints
  3. **Compression tolerance**: Apply FQ Replay to your most challenging object category; quantify F1 pixel degradation to determine acceptable compression level

## Open Questions the Paper Calls Out
- Can distillation-based Continual Learning strategies provide a viable memory-free alternative to Replay for PaSTe, or do they incur prohibitive computational costs on edge devices?
- Do alternative feature compression techniques exist that can achieve the >90% memory reduction of Product Quantization (PQ) without the associated ~27% drop in pixel-level F1 performance?
- How robust are the proposed Compressed Replay techniques when applied to heterogeneous or noisy domains such as medical imaging, compared to the controlled industrial setting of MVTec?

## Limitations
- The study is limited to MVTec Dataset with only 10 objects, restricting generalizability to more complex scenarios
- The assumption that frozen backbone features remain stable across tasks may not hold for datasets with diverse visual characteristics
- Lack of validation on non-image data types or industrial-grade edge hardware constraints

## Confidence
- **High Confidence**: PaSTe architecture's superior CL performance (10% F1 improvement) and Feature Replay's memory reduction claims are well-supported by quantitative metrics in Tables 1 and 2.
- **Medium Confidence**: The catastrophic forgetting reduction mechanism through shared frozen layers is theoretically sound but relies on the assumption that low-level features remain consistent across tasks.
- **Low Confidence**: The claim that VAD requires "high-fidelity feature representations" for pixel-level detection lacks direct empirical validation beyond the observed performance degradation under compression.

## Next Checks
1. **Distribution Shift Validation**: Test PaSTe with frozen layers on datasets where early and late tasks have substantially different low-level visual statistics (e.g., metal objects vs. fabric textures) to verify the frozen-layer forgetting reduction mechanism.
2. **Compression Quality Analysis**: Quantify feature reconstruction error after quantization and compare it to the observed F1 score degradation to establish the relationship between compression artifacts and detection performance.
3. **Buffer Sampling Strategy**: Implement and compare random vs. curriculum-based replay sampling to determine optimal buffer utilization for the trade-off between memory constraints and forgetting reduction.