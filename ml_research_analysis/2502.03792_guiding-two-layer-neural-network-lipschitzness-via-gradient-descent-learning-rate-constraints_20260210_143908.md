---
ver: rpa2
title: Guiding Two-Layer Neural Network Lipschitzness via Gradient Descent Learning
  Rate Constraints
arxiv_id: '2502.03792'
source_url: https://arxiv.org/abs/2502.03792
tags:
- learning
- neural
- rate
- lipschitz
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether decaying the learning rate during\
  \ gradient descent (GD) training of two-layer neural networks can enforce Lipschitz\
  \ regularity without sacrificing convergence. The authors show that an eventual\
  \ learning rate decay\u2014while keeping a constant step size for an initial period\u2014\
  ensures that the trained network has a small Lipschitz constant, bounded by a function\
  \ of the width, depth, and sample size."
---

# Guiding Two-Layer Neural Network Lipschitzness via Gradient Descent Learning Rate Constraints

## Quick Facts
- arXiv ID: 2502.03792
- Source URL: https://arxiv.org/abs/2502.03792
- Reference count: 40
- Primary result: Decaying learning rates during GD training of two-layer networks ensures Lipschitz regularity while maintaining O(1/√T) convergence

## Executive Summary
This paper investigates whether decaying the learning rate during gradient descent (GD) training of two-layer neural networks can enforce Lipschitz regularity without sacrificing convergence. The authors show that an eventual learning rate decay—while keeping a constant step size for an initial period—ensures that the trained network has a small Lipschitz constant, bounded by a function of the width, depth, and sample size. They also prove that this schedule maintains the optimal O(1/√T) convergence rate to a critical point of the empirical risk measured with the Huber loss. A generalization bound is derived with sublinear dependence on the number of parameters, implying that overparameterization does not harm statistical performance.

## Method Summary
The method trains two-layer MLPs f_Θ(x) = B^T · σ(Wx + b) + c with gradient descent, using a hybrid learning rate schedule: constant step size for an initial phase, then decaying according to rate function G. The learning rate bounds (Conditions 3.1-3.4) control the cumulative growth of weight matrices, which in turn bounds the network's Lipschitz constant. The authors prove that this approach maintains O(1/√T) convergence to a critical point while ensuring Lip(f_Θ) is controlled by G(t). Generalization bounds follow from Kantorovich-Rubinstein duality, showing that bounded Lipschitz constant enables statistical guarantees with sublinear dependence on width.

## Key Results
- Theorem 3: Under decaying learning rate constraints, Lip(f_Θ_T) is bounded by a function of width, depth, sample size, and rate function G
- Theorem 5: Hybrid learning rate schedule maintains O(1/√T) convergence to critical points
- Corollary 4: Generalization gap scales as O(1/√N) with width-independent constants
- Experiments confirm decaying rates produce smaller, more stable Lipschitz constants than constant rates while maintaining comparable accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining learning rates proportional to a decaying rate function controls the cumulative growth of weight matrices during gradient descent, which in turn bounds the network's Lipschitz constant.
- Mechanism: The GD LR Decay Conditions (3.1-3.4) set upper bounds on each learning rate that scale as α_s ≤ C·g(s)/[data-dependent normalization], where g(s) is the derivative of rate function G. Since weight updates accumulate as sums of α_s × gradient terms, and gradients are bounded by current weight norms times Lipschitz constants of activations, controlling α_s limits how far weights can grow from initialization.
- Core assumption: Activation function σ is L_σ-Lipschitz and differentiable; initialization weights are sub-Gaussian; the rate function G is bounded, non-decreasing, and absolutely continuous.
- Evidence anchors: [section 3.1] Theorem 3 gives explicit bound; [appendix A] Lemma 6 derives norm bounds; [corpus] Weak direct support, related work focuses on regularization penalties.

### Mechanism 2
- Claim: A hybrid learning rate schedule—constant for an initial phase, then decaying—preserves optimal O(1/√T) convergence while still achieving Lipschitz regularity.
- Mechanism: Standard constant-step-size GD achieves O(1/√T) convergence to stationary points under Lipschitz-smooth objectives. The key insight is that this only requires the step size to be ≤ 1/Lip(R_S) for the first T iterations. By choosing free parameters C_W, C_B large enough, the prescribed LR bounds satisfy min{1/Lip(R_S), ᾱ_t} = 1/Lip(R_S) during the constant phase.
- Core assumption: The empirical risk R_S has finite Lipschitz constant on bounded parameter space; σ, σ', σ'' are bounded; input data has finite second moment.
- Evidence anchors: [abstract] O(1/√T) convergence claim; [section 3.3] Theorem 5 proof; [corpus] Related convergence analyses provide parallel evidence.

### Mechanism 3
- Claim: Bounded Lipschitz constant enables generalization bounds with sublinear dependence on network width, implying overparameterization does not harm statistical performance.
- Mechanism: By the Kantorovich-Rubinstein duality, the generalization gap R(f_Θ) - R̂_S(f_Θ) is bounded by Lip(f_Θ) × W₁(Q, Q_N), where W₁ is the 1-Wasserstein distance between true and empirical measures. Since Theorem 3 controls Lip(f_Θ) and W₁ concentration scales as O(1/√N) with dimensional constants independent of p, the overall bound depends linearly on √p but the key statistical term is width-independent.
- Core assumption: Data drawn i.i.d. from compactly-supported distribution Q; d + D > 2 for dimensional constant.
- Evidence anchors: [section 3.2] Corollary 4 explicit bound; [appendix A.2] Proof applies W₁ concentration; [corpus] Related work on benign overfitting touches on overparameterization effects.

## Foundational Learning

- Concept: **Lipschitz continuity and Lipschitz constant**
  - Why needed here: The entire paper is about controlling Lip(f) during training. Understanding that Lip(f) measures the maximum rate of change of a function is essential for interpreting Theorem 3 and the regularity guarantees.
  - Quick check question: If f: ℝ² → ℝ has Lip(f) = 5, what is the maximum possible value of |f(1,1) - f(4,5)|?

- Concept: **Gradient descent convergence rates**
  - Why needed here: Theorem 5 claims O(1/√T) convergence; you need to understand that this means the minimum gradient norm encountered in T steps scales as 1/√T, and why this is "optimal" for smooth non-convex optimization.
  - Quick check question: If GD achieves O(1/√T) convergence, approximately how many iterations are needed to reduce the gradient norm by a factor of 10?

- Concept: **Generalization bounds and the bias-variance tradeoff**
  - Why needed here: Corollary 4's claim that overparameterization doesn't hurt contradicts classical intuition. Understanding that Lipschitz control provides a capacity measure that can be independent of parameter count is key.
  - Quick check question: In classical VC theory, how does generalization typically scale with the number of parameters, and why does Corollary 4 differ?

## Architecture Onboarding

- Component map: Two-layer MLP f_Θ(x) = B^T · σ(Wx + b) + c, where W ∈ ℝ^{p×d} (hidden weights), B ∈ ℝ^{p×1} (output weights), b ∈ ℝ^p (hidden bias), c ∈ ℝ (output bias)

- Critical path: 1) Initialize Θ_0 with sub-Gaussian weights (Assumption 1); 2) Run GD with constant step size ≤ 1/Lip(R_S) for T iterations (convergence phase); 3) Switch to decay phase where α_t ≤ C·g(t)/[data-dependent denominator] per Conditions 1; 4) The resulting network satisfies Lip(f_Θ_T) bounded by Theorem 3 formula; 5) Generalization follows from Corollary 4

- Design tradeoffs: Larger C_W, C_B: looser Lipschitz bound but allows larger step sizes → faster initial convergence; Longer constant phase T: better convergence before decay but delays regularity enforcement; Larger sample size N: tighter Lipschitz bound (G(T)/N term) but more computation; Faster decay rate r: tighter eventual Lipschitz bound but slower final convergence

- Failure signatures: Lip(f_Θ) growing unboundedly during training → learning rates violating Conditions 1; Poor test accuracy despite low training loss → constant phase too short or decay too aggressive; Unstable training dynamics → Lip(R_S) underestimated, causing oversized initial step; Generalization gap widening with width → not enforcing decay phase properly

- First 3 experiments: 1) Lip(f_Θ) vs. sample size N: Train on synthetic regression with varying N (50-500). Plot Lip(f_Θ_T) across training iterations. Expected: larger N → smaller and more stable Lip(f_Θ); 2) Lip(f_Θ) vs. width p: Fix N=150, vary hidden dimension p (50-500). Expected: Lip(f_Θ) grows roughly linearly with √p, confirming width dependence is controlled; 3) Constant vs. decaying LR comparison: Train identical architectures with (a) constant α=0.01, (b) hybrid schedule with τ=50 then exponential decay. Plot both Lip(f_Θ) and Huber test loss. Expected: decaying LR has smaller/stabler Lip(f_Θ) with comparable accuracy.

## Open Questions the Paper Calls Out

- Question: Can the learning rate decay conditions for Lipschitz regularity be extended to deep (multilayer) neural networks while maintaining convergence guarantees?
  - Basis in paper: [explicit] The authors state in the conclusion: "In future work, we aim to extend our results to multilayer neural networks..."
  - Why unresolved: The current theoretical proofs and bounds are derived specifically for the two-layer architecture, relying on the specific parameterization of shallow networks.
  - What evidence would resolve it: A proof of Theorem 3 (Lipschitz Control) and Theorem 5 (Convergence) adapted for a depth L > 2 using similar learning rate constraints.

- Question: Do the Lipschitz regularity guarantees hold for stochastic gradient descent (SGD) or momentum-based methods like Heavy-Ball?
  - Basis in paper: [explicit] The conclusion proposes exploring "GD-based algorithms with more complex dynamics, such as Conjugate Gradients or Heavy-Ball methods... Stochastic variants..."
  - Why unresolved: The analysis relies on the deterministic update rules of standard GD; stochastic noise or momentum terms introduce dynamics not covered by the current inequalities.
  - What evidence would resolve it: Theoretical bounds on the Lipschitz constant for SGD trajectories or experimental validation showing similar regularity on standard benchmarks using Adam/SGD.

- Question: Does standard gradient descent with a constant learning rate inherently enforce Lipschitz regularity similar to the proposed decay schedule?
  - Basis in paper: [inferred] The abstract notes that experiments show networks trained with constant step sizes exhibit similar regularity properties to those with decay, suggesting "neural networks trained with standard GD may already be highly regular learners."
  - Why unresolved: The paper proves guarantees for decaying rates, but the empirical observation that constant rates perform similarly lacks a theoretical justification.
  - What evidence would resolve it: A theoretical analysis showing that constant-rate GD implicitly bounds the Lipschitz constant under specific initialization or data distribution conditions.

## Limitations

- The theoretical bounds rely on initialization sub-Gaussian conditions and Lipschitz-smooth loss functions that may not hold in practice for unbounded activations like Swish
- The exact method for estimating Lip(f_Θt) during training is unspecified
- The data-dependent LR constraints (3.1-3.4) require practical implementation details not provided

## Confidence

- **High confidence**: The O(1/√T) convergence rate under hybrid learning rates (Theorem 5) - this follows standard smooth non-convex optimization theory
- **Medium confidence**: The Lipschitz bound Theorem 3 - the mechanism is sound but constants C_W, C_B need empirical tuning
- **Medium confidence**: The generalization bound Corollary 4 - depends on W₁ concentration which requires i.i.d. data and bounded support

## Next Checks

1. Estimate Lipschitz stability: Run multiple training seeds with decaying LR and plot distribution of Lip(f_ΘT) to verify consistency across runs
2. Vary constant phase duration: Compare networks trained with different τ (constant phase length) to identify optimal tradeoff between convergence speed and Lipschitz control
3. Test with different activations: Repeat experiments with bounded activations (ReLU6, tanh) to isolate effects of activation Lipschitz properties on the theoretical bounds