---
ver: rpa2
title: 'Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis
  and Report Generation'
arxiv_id: '2508.13068'
source_url: https://arxiv.org/abs/2508.13068
tags:
- report
- gaze
- clinical
- image
- lung
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A two-stage multimodal framework was proposed to enhance chest
  X-ray diagnosis and report generation using the MIMIC-Eye dataset. The first stage
  integrates visual, textual, bounding-box, and gaze-tracking signals via a gaze-guided
  contrastive learning architecture with a novel multi-term gaze-attention loss, improving
  F1 score from 0.597 to 0.631 and AUC from 0.821 to 0.849.
---

# Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation

## Quick Facts
- arXiv ID: 2508.13068
- Source URL: https://arxiv.org/abs/2508.13068
- Reference count: 40
- Proposed gaze-supervised multimodal framework improves chest X-ray classification F1 from 0.597 to 0.631 and AUC from 0.821 to 0.849

## Executive Summary
This study presents a two-stage multimodal framework that leverages gaze-tracking data to enhance both chest X-ray diagnosis and report generation. The approach integrates visual, textual, bounding-box, and eye-tracking signals through a novel gaze-guided contrastive learning architecture, demonstrating significant improvements in classification performance over baseline models. The framework also generates clinically relevant, anatomically aware radiology reports using a modular pipeline that combines diagnostic keyword extraction with region-aware structured prompts for large language models.

The work introduces the MIMIC-Eye dataset, the first publicly available collection of chest X-rays paired with radiologist gaze trajectories, bounding boxes, and reports. By incorporating gaze supervision through a multi-term gaze-attention loss, the model learns to focus on clinically relevant regions, improving both diagnostic accuracy and report quality. The approach addresses the critical challenge of aligning model attention with human expertise in medical imaging.

## Method Summary
The framework operates in two stages: first, it performs gaze-guided contrastive learning on chest X-ray images using a multi-modal encoder that processes visual, textual, and gaze-tracking signals. This stage introduces a novel gaze-attention loss that supervises the model's attention mechanism using radiologist eye-tracking data. Second, it generates reports through a modular pipeline that extracts confidence-weighted diagnostic keywords, maps them to anatomical regions, and creates structured prompts for LLMs. The system uses visual grounding to ensure anatomical accuracy and incorporates gaze-derived attention weights to emphasize clinically relevant findings in the final reports.

## Key Results
- Classification performance improved from baseline F1 of 0.597 to 0.631 and AUC from 0.821 to 0.849
- Report generation achieved BERTScore of 0.732 and CheXpert F1 of 0.546
- Gaze supervision significantly enhanced model attention alignment with radiologist focus regions

## Why This Works (Mechanism)
The framework leverages human gaze patterns as implicit supervision to guide model attention toward clinically relevant regions of chest X-rays. By incorporating eye-tracking data through contrastive learning and attention mechanisms, the model learns to prioritize the same anatomical areas that radiologists focus on during diagnosis. This alignment between model and human attention improves both classification accuracy and the generation of anatomically grounded, clinically meaningful reports. The gaze-guided approach bridges the gap between raw image features and clinical reasoning patterns.

## Foundational Learning

### Multimodal Contrastive Learning
**Why needed**: To learn shared representations across visual, textual, and gaze-tracking modalities
**Quick check**: Verify that cross-modal embeddings are semantically aligned in the joint space

### Gaze-Guided Attention Mechanisms
**Why needed**: To direct model focus toward clinically relevant regions based on radiologist eye movements
**Quick check**: Confirm attention weights correlate with gaze fixation patterns

### Visual Grounding for Report Generation
**Why needed**: To ensure generated reports accurately reference specific anatomical regions
**Quick check**: Validate that bounding boxes correctly map to anatomical labels

## Architecture Onboarding

### Component Map
Image Encoder -> Gaze Attention Module -> Contrastive Loss -> Multimodal Encoder -> Report Generation Pipeline

### Critical Path
1. Chest X-ray image input
2. Gaze attention module applies radiologist fixation weights
3. Multimodal encoder processes visual and textual features
4. Contrastive learning aligns modalities
5. Diagnostic keyword extraction with confidence weighting
6. Anatomical region mapping
7. Structured LLM prompt generation

### Design Tradeoffs
The framework balances complexity against interpretability, using gaze data to improve attention but requiring specialized eye-tracking equipment. The modular design allows for independent optimization of classification and report generation components, though this may introduce optimization challenges between stages.

### Failure Signatures
- Misaligned gaze data leading to incorrect attention guidance
- Over-reliance on gaze patterns that don't generalize across radiologists
- Generated reports that reference incorrect anatomical regions
- Performance degradation when gaze data quality is poor

### 3 First Experiments to Try
1. Ablation study removing gaze supervision to quantify its contribution
2. Cross-population validation with radiologists of varying expertise levels
3. Comparison with attention mechanisms trained on clinical annotations instead of gaze data

## Open Questions the Paper Calls Out
The study acknowledges uncertainty about how gaze patterns generalize across different observer populations and imaging systems. The potential bias introduced by the gaze-tracking setup and whether improvements translate to real-world clinical workflows remain open questions. The authors also note that the MIMIC-Eye dataset may not capture the full diversity of clinical scenarios and observer behaviors.

## Limitations
- Gaze data generalizability across different observer populations is uncertain
- Potential bias introduced by specific gaze-tracking setup and equipment
- Limited external validation beyond the MIMIC-Eye dataset
- Automated evaluation metrics may not fully capture clinical utility

## Confidence
- Classification performance improvements (F1: 0.597→0.631, AUC: 0.821→0.849): High
- Report generation quality (BERTScore 0.732, CheXpert F1 0.546): Medium
- Gaze-guided attention mechanism effectiveness: Medium

## Next Checks
1. External validation on diverse chest X-ray datasets from multiple institutions to assess generalizability
2. Clinician evaluation of generated reports for clinical accuracy and utility beyond automated metrics
3. Ablation studies to quantify the specific contribution of gaze data versus other multimodal inputs under different observer expertise levels