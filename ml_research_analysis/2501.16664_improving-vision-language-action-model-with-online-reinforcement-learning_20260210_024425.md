---
ver: rpa2
title: Improving Vision-Language-Action Model with Online Reinforcement Learning
arxiv_id: '2501.16664'
source_url: https://arxiv.org/abs/2501.16664
tags:
- tasks
- learning
- arxiv
- large
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving vision-language-action
  (VLA) models for robotic control using online reinforcement learning (RL). The authors
  propose the iRe-VLA framework, which iteratively alternates between RL stages (freezing
  the VLM backbone to stabilize training) and supervised learning stages (fine-tuning
  the entire model on successful trajectories).
---

# Improving Vision-Language-Action Model with Online Reinforcement Learning

## Quick Facts
- arXiv ID: 2501.16664
- Source URL: https://arxiv.org/abs/2501.16664
- Authors: Yanjiang Guo; Jianke Zhang; Xiaoyu Chen; Xiang Ji; Yen-Jen Wang; Yucheng Hu; Jianyu Chen
- Reference count: 40
- Key outcome: iRe-VLA improves vision-language-action model performance for robotic control through iterative RL and supervised fine-tuning, achieving higher success rates on both seen and unseen tasks

## Executive Summary
This paper addresses the challenge of improving vision-language-action (VLA) models for robotic control using online reinforcement learning (RL). The authors propose the iRe-VLA framework, which iteratively alternates between RL stages (freezing the VLM backbone to stabilize training) and supervised learning stages (fine-tuning the entire model on successful trajectories). This approach stabilizes the unstable RL process while leveraging the large VLM's expressive power. Experiments on MetaWorld, FrankaKitchen, and real-world manipulation tasks show that iRe-VLA outperforms standard PPO, achieving higher success rates on both seen and unseen tasks.

## Method Summary
The iRe-VLA framework uses a VLA model (BLIP-2 3B VLM backbone + lightweight action head) and follows three stages: (0) SFT on expert data with MSE loss; (1) Freeze VLM, train action head via RL (PPO/SACfD), collect success trajectories; (2) Fine-tune full model on expert + RL datasets. The method iterates stages 1-2, using LoRA for VLM fine-tuning and alternating between local RL training (fast inference, low latency) and cloud-based SL fine-tuning.

## Key Results
- iRe-VLA achieved 0.83 success rate on FrankaKitchen left-door-open task compared to 0.43 for PPO
- In real-world picking tasks, success rates increased from 0.35 to 0.80 for eggplants
- iRe-VLA maintained or improved performance on expert tasks while learning new skills, unlike PPO which showed catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1: Selective Plasticity via Action-Head Isolation
- Claim: Freezing VLM parameters during RL reduces gradient instability without limiting policy adaptation
- Evidence: Performance drops when fully fine-tuning (Fig 4), neighbors like ConRFT and STARE-VLA also explore structured fine-tuning
- Break condition: New tasks requiring visual concepts significantly different from pre-training data

### Mechanism 2: Iterative Distillation of Exploration
- Claim: Converting RL exploration data to supervised format allows stable integration of new skills
- Evidence: iRe-VLA-freeze (skipping full model SFT) performs worse, neighbors like Dual-Actor Fine-Tuning support alternating optimization
- Break condition: Large distribution shift between expert and RL datasets

### Mechanism 3: Replay-Based Forgetting Prevention
- Claim: Continually re-training on original expert dataset prevents catastrophic forgetting
- Evidence: iRe-VLA maintains expert task performance vs PPO-Replay degradation, neighbors like SimpleVLA-RL note generalization difficulties
- Break condition: Model capacity saturation or insufficient LoRA expressiveness

## Foundational Learning
- **Concept: Catastrophic Forgetting**
  - Why needed: Method explicitly designs two-stage loop to prevent learning new tasks from destroying VLA's ability to perform original expert tasks
  - Quick check: Why doesn't Stage 2 SFT erase knowledge gained in Stage 1?

- **Concept: VLA (Vision-Language-Action) Architecture**
  - Why needed: Understanding VLM outputs hidden states which action head translates to robot commands is crucial for grasping why freezing VLM is valid
  - Quick check: What component maps VLM's text/image embeddings to robot torque/position commands?

- **Concept: Policy Gradient Instability**
  - Why needed: Motivation for iRe-VLA is that standard online RL introduces noise that destabilizes large Transformer models
  - Quick check: Why is updating 3B parameter model with RL gradients riskier than updating small MLP?

## Architecture Onboarding
- **Component map**: RGB Image + Text Instruction → BLIP-2 VLM (Frozen in RL, Trainable in SFT) → Token Learner → MLP Action Head (Trainable in RL & SFT) + Critic Head (Trainable in RL only)
- **Critical path**: Stage 0: Pre-train VLA on expert data (SFT) → Stage 1: Freeze VLM → Train Action Head + Critic via RL → Collect success trajectories → Stage 2: Unfreeze VLM → Train full model on De ∪ DRL → Iterate
- **Design tradeoffs**: Local vs Cloud (RL on 4090 for speed, SFT on A100s for capacity), LoRA vs Full Fine-Tuning (efficiency vs maximal plasticity)
- **Failure signatures**: Model Collapse (performance drops), Stagnation (frozen features insufficient), Catastrophic Forgetting (new tasks succeed, old tasks fail)
- **First 3 experiments**: (1) Ablation on freezing vs full-model RL on sparse reward MetaWorld task; (2) 3 cycles of RL→Collect→SFT showing success rates on new and hold-out expert tasks; (3) Real-world transfer fine-tuning on new object and verifying generalization

## Open Questions the Paper Calls Out
- The method can only improve skills within seen types and cannot learn entirely new skills under sparse-reward online RL conditions
- Standard online RL induces catastrophic performance drops in large VLA models, but the underlying mechanisms are not theoretically analyzed
- Performance and computational efficiency scaling to larger VLM backbones (7B+ parameters) remains uncertain

## Limitations
- The method's success depends heavily on hyperparameter tuning with no systematic sensitivity analysis provided
- Assumes frozen VLM features remain sufficient for new tasks, which may not hold for tasks requiring novel visual concepts
- Real-world experiments are limited to specific manipulation tasks, with generalization to diverse scenarios untested

## Confidence
- **High Confidence**: Freezing VLM parameters during RL prevents training instability (supported by empirical evidence and theoretical reasoning)
- **Medium Confidence**: Iterative distillation effectively prevents catastrophic forgetting (supported by results but could benefit from more ablation)
- **Low Confidence**: Long-term generalization to truly unseen tasks and objects remains uncertain (limited testing in diverse scenarios)

## Next Checks
1. Systematically vary LoRA rank and layer selection to determine optimal balance between computational efficiency and plasticity for complex manipulation tasks
2. Test the method on tasks involving objects/environments significantly different from pre-training data to validate frozen VLM feature sufficiency
3. Evaluate ability to learn multiple new tasks sequentially without performance degradation, measuring both acquisition speed and retention of previously learned skills