---
ver: rpa2
title: 'Majority of the Bests: Improving Best-of-N via Bootstrapping'
arxiv_id: '2511.18630'
source_url: https://arxiv.org/abs/2511.18630
tags:
- answer
- reward
- adaptive
- distribution
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to improve the Best-of-N (BoN) selection
  algorithm by estimating the distribution of BoN's outputs via bootstrapping and
  selecting the mode. The key idea is that, while BoN often fails to reliably select
  the correct answer with imperfect rewards, the correct answer is frequently the
  most probable outcome of BoN.
---

# Majority of the Bests: Improving Best-of-N via Bootstrapping

## Quick Facts
- **arXiv ID**: 2511.18630
- **Source URL**: https://arxiv.org/abs/2511.18630
- **Reference count**: 40
- **Primary result**: MoB outperforms BoN in 25 out of 30 experimental setups across five benchmarks, three base LLMs, and two reward models by selecting the mode of the bootstrapped BoN output distribution.

## Executive Summary
The paper proposes Majority of the Bests (MoB), a method to improve the Best-of-N (BoN) selection algorithm for LLM outputs when using imperfect reward models. The key insight is that while BoN often fails to reliably select the correct answer, the correct answer is frequently the most probable outcome of the BoN selection process. MoB estimates this distribution via bootstrapping and selects its mode, outperforming BoN in 25 out of 30 experimental setups. The method is theoretically grounded and provides a simple yet effective alternative to existing selection mechanisms.

## Method Summary
MoB improves BoN by estimating the distribution of BoN's outputs through bootstrapping and selecting the mode. Instead of generating k independent sets of size m (which would require k × m total samples), MoB generates one pool of N samples and creates B synthetic subsets by resampling with replacement. It runs BoN on each subset and aggregates the results, allowing the algorithm to simulate thousands of BoN trials using the same initial generation budget. The method uses a subset size m < N (typically m ≈ √N) to prevent the single highest-reward sample from dominating the aggregation.

## Key Results
- MoB outperforms BoN in 25 out of 30 experimental setups across five benchmarks (MATH500, GSM8K, MMLU-Pro, CommonsenseQA)
- MoB achieves higher accuracy than Self-Consistency in 15 out of 15 setups where both were tested
- The method shows consistent improvements across three base models (Qwen2.5-3B, Llama-3.1-8B, Gemma-2-9B) and two reward models
- Theoretical results support the consistency of the bootstrapping approach

## Why This Works (Mechanism)

### Mechanism 1: Mode Selection Over Max Selection
Standard BoN selects the single output with the highest reward, which is brittle when the reward model is imperfect. MoB recognizes that the correct answer often appears frequently among high-reward outputs, making it the statistical mode of the BoN output distribution. By selecting the mode rather than the max, MoB filters out the noise of any single high-reward incorrect sample.

### Mechanism 2: Efficient Distribution Estimation via Bootstrapping
Bootstrapping allows MoB to estimate the BoN output distribution without requiring expensive generation of independent solution sets. By generating one pool of N samples and creating B synthetic subsets through resampling with replacement, MoB simulates thousands of BoN trials using the same initial generation budget, making it computationally efficient.

### Mechanism 3: Subset Size m < N Prevents Winner Dominance
Using m < N is crucial because if m = N, the single highest-reward sample appears in approximately 63.2% of bootstrap samples and always wins when present, forcing MoB to match standard BoN. By using m < N (e.g., m = √N), MoB increases variance in which sample wins each subset, allowing true robust winners to emerge as the mode.

## Foundational Learning

- **Concept: Best-of-N (BoN) Sampling**
  - **Why needed here**: MoB is fundamentally a modification of BoN, which relies on a Reward Model to score outputs and acts as a "max" operator that is brittle when the RM is imperfect.
  - **Quick check question**: If a reward model has 90% accuracy at distinguishing correct from incorrect, why does BoN still fail to reach 100% accuracy on some tasks?

- **Concept: Bootstrapping (Resampling)**
  - **Why needed here**: This is the statistical engine of MoB. Understanding "sampling with replacement" is critical to grasping how MoB simulates multiple trials from a fixed set of generations.
  - **Quick check question**: Why does sampling with replacement allow us to estimate the variance/uncertainty of a statistic using only a single dataset?

- **Concept: Mode vs. Max**
  - **Why needed here**: The paper shifts from "find the max reward" (BoN) to "find the most frequent winner" (MoB). This distinction is central to why MoB handles noise better.
  - **Quick check question**: In a distribution with a long tail of outliers, why is the Mode often a more robust estimator of the "central tendency" than the Max?

## Architecture Onboarding

- **Component map**: Generator -> Reward Model -> MoB Core (Sampler + Selector + Aggregator) -> Decider
- **Critical path**: The bootstrap estimation (Section 4.2 & Appendix B). Use the closed-form O(N log N) calculation: sort samples by reward, calculate probability of each answer being selected based on rank, sum probabilities for all samples sharing the same answer.
- **Design tradeoffs**: 
  - Choice of m: Large m approaches standard BoN (fails to fix "lucky winner" problem), small m approaches Self-Consistency (ignores reward signal)
  - Compute: MoB adds CPU overhead but zero GPU overhead (reuses same N generations)
- **Failure signatures**: 
  - Degradation to BoN: If MoB accuracy identical to BoN, check if m is set too close to N
  - Degradation to SC: If MoB accuracy mirrors Majority Voting, check if m is set too small
  - Random behavior: If base model has very low pass@1 probability and RM is weak, mode might be an incorrect hallucination
- **First 3 experiments**:
  1. Sanity Check (m=N): Run MoB with m=N and verify output matches standard BoN exactly
  2. Hyperparameter Sweep: Fix N=64, sweep m from [4, 8, 16, 32, 64], plot accuracy vs m to identify sweet spot
  3. Noise Robustness: Corrupt RM scores with Gaussian noise, compare degradation slope of MoB vs BoN

## Open Questions the Paper Calls Out
- **Early Stopping**: Can MoB's selection signal enable early stopping in parallel LLM generation? The paper believes it could but doesn't define a threshold or criterion for stopping based on convergence of the bootstrapped mode.
- **Open-Ended Tasks**: How to adapt MoB for tasks involving open-ended generation or semantic similarity rather than exact discrete answers? The algorithm relies on identifying the mode via exact string matching, which is undefined for semantically equivalent but syntactically diverse outputs.
- **Negative Flips**: Does MoB strictly outperform BoN when the incorrect answer is the most probable outcome of the BoN distribution? The paper notes MoB's determinism removes the "lucky" sampling variance that might allow standard BoN to succeed.

## Limitations
- **Task Scope**: Limited to tasks requiring discrete final answers; efficacy for continuous or open-ended generation remains untested
- **Reward Model Dependence**: Performance contingent on RM being "imperfect but informative"; benefits diminish if RM is too weak or too strong
- **Hyperparameter Sensitivity**: Optimal m value depends on base model's generation diversity and RM's noise profile

## Confidence
- **High Confidence**: Bootstrapping can efficiently estimate BoN output distribution and mode is more robust than max with imperfect RMs (supported by consistent experimental results and theoretical guarantees)
- **Medium Confidence**: MoB serves as a "strong alternative" to Self-Consistency (outperforms in tested setups but comparison limited to subset of benchmarks)
- **Low Confidence**: MoB's improvement is solely due to "filtering out noise" (paper lacks rigorous ablation study isolating bootstrapping effects from subset size effects)

## Next Checks
1. **Continuous Output Validation**: Test MoB on benchmarks with continuous outputs (code generation with pass@1, or long-form generation with semantic similarity scores) to validate mode-based aggregation when no single "correct" answer exists
2. **Reward Model Quality Ablation**: Systematically vary RM quality (add noise, use different tasks) and measure MoB's degradation curve to quantify minimum RM quality required for improvement
3. **Subset Size vs. Bootstrap Trade-off**: Compare MoB (with bootstrapping) against "Best-of-m" baseline (best of m independent samples, m = √N) on same questions to isolate whether improvement comes from bootstrapping or subset size alone