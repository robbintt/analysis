---
ver: rpa2
title: 'COMPKE: Complex Question Answering under Knowledge Editing'
arxiv_id: '2506.00829'
source_url: https://arxiv.org/abs/2506.00829
tags:
- knowledge
- uni00000013
- editing
- question
- complex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COMPKE, a new benchmark for evaluating knowledge
  editing in large language models on complex question answering tasks. COMPKE advances
  beyond existing benchmarks by incorporating diverse question structures, one-to-many
  relations, and expanded edit types including additions, deletions, and substitutions.
---

# COMPKE: Complex Question Answering under Knowledge Editing

## Quick Facts
- arXiv ID: 2506.00829
- Source URL: https://arxiv.org/abs/2506.00829
- Authors: Keyuan Cheng; Zijian Kan; Zhixian He; Zhuoran Zhang; Muhammad Asif Ali; Ke Xu; Lijie Hu; Di Wang
- Reference count: 25
- Key outcome: COMPKE benchmark shows most knowledge editing methods achieve only modest performance on complex questions requiring multi-step reasoning with logical operations

## Executive Summary
This paper introduces COMPKE, a new benchmark for evaluating knowledge editing in large language models on complex question answering tasks. COMPKE advances beyond existing benchmarks by incorporating diverse question structures, one-to-many relations, and expanded edit types including additions, deletions, and substitutions. The benchmark consists of 11,924 complex questions built from Wikidata, requiring multi-step reasoning with logical operations and conditional confirmation. Through extensive experiments on five LLMs spanning different model families, the authors evaluate four knowledge editing methods (ROME, MEMIT, MeLLo, PokeMQA) and find that most methods achieve only modest performance on complex questions. Memory-based methods tend to perform better on larger models with stronger reasoning abilities, while parameter-based methods show better results on smaller models but suffer from overfitting.

## Method Summary
COMPKE is constructed from Wikidata triples with edits applied as additions, deletions, or substitutions. The benchmark includes 11,924 complex questions requiring multi-step reasoning with knowledge mapping, conditional confirmation, and logical operations (intersection/union). Four knowledge editing methods are evaluated: ROME and MEMIT (parameter-based), MeLLo and PokeMQA (memory-based), across five LLMs including both open and closed-source models. Evaluation metrics include Augmentation Accuracy (Aug), Retention Accuracy (Ret), and overall Accuracy (Acc). The dataset is available at https://github.com/kzjkzj666/CompKE, with hyperparameters and hardware requirements specified for reproduction.

## Key Results
- Most knowledge editing methods achieve only modest performance on COMPKE's complex questions
- Memory-based methods (MeLLo, PokeMQA) attain 39.47 accuracy on GPT-4O-MINI but drop to 3.83 on QWEN2.5-3B
- Parameter-based methods experience sharp accuracy decline for batch sizes k ≥ 100, with models losing coherence
- COMPKE proves more challenging than previous benchmarks, with significant performance drops as edit batch sizes increase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memory-based knowledge editing methods leverage external storage and retrieval to apply edits at inference time, with effectiveness conditional on model scale and reasoning capability.
- Mechanism: Edits are stored in external memory; at inference, relevant edits are retrieved via semantic matching and incorporated through prompts. The model must decompose complex questions, retrieve applicable edits, and integrate them with parametric knowledge during reasoning.
- Core assumption: The model possesses sufficient instruction-following ability to interpret prompts and execute multi-step decomposition plans.
- Evidence anchors:
  - [abstract]: "MeLLo attains an accuracy of 39.47 on GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B."
  - [Section 5.2]: "memory-based methods underperform on smaller models (e.g., QWEN 2.5-3B), likely due to their dependence on strong instruction-following and reasoning abilities."
  - [corpus]: Related work "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition" confirms decomposition quality is a bottleneck in memory-based KE.

### Mechanism 2
- Claim: Parameter-based knowledge editing modifies internal model weights to encode new facts, but exhibits overfitting and catastrophic degradation at scale.
- Mechanism: Methods like ROME and MEMIT use causal tracing to locate MLP layers encoding target facts, then apply rank-one or multi-layer updates. The edit directly reshapes the model's latent knowledge representation.
- Core assumption: Knowledge is locally encoded in identifiable parameters, and modifying these locations propagates correctly through downstream reasoning.
- Evidence anchors:
  - [Section 5.2]: "parameter-based methods can achieve surprisingly high accuracy on smaller models... largely due to overfitting... the model tends to overproduce the newly injected information."
  - [Section 5.2, Figure 4]: "parameter-based methods experience a much steeper decline... for k ≥ 100, these models often lose coherence, resulting in inconsistent answers and irrelevant outputs."
  - [corpus]: "Norm Growth and Stability Challenges in Localized Sequential Knowledge Editing" corroborates that sequential edits cause representational instability.

### Mechanism 3
- Claim: Complex question answering under knowledge editing requires compositional reasoning chains that combine knowledge mapping, logical operations, and conditional confirmation.
- Mechanism: Questions are represented as graph-structured reasoning problems with entity sets S and reasoning links L. Links include (i) knowledge mapping across relations, (ii) condition confirmation filtering entities by constraints, and (iii) logical operations (intersection/union). Edits modify object sets within this structure.
- Core assumption: Models can execute multi-step reasoning while simultaneously applying edited knowledge at appropriate steps.
- Evidence anchors:
  - [Section 3]: Formal definition of complex questions as Q = (S, L) with reasoning link categories.
  - [Section 4.1, Step 3]: "questions that lack a valid answer" or have "empty intermediate entity sets" are filtered, indicating reasoning chain integrity is required.
  - [corpus]: "Tracking the Limits of Knowledge Propagation" examines how conflicting knowledge propagates (or fails) through multi-step reasoning.

## Foundational Learning

- **Knowledge Graph Triple Representation (s, r, o)**
  - Why needed here: COMPKE generalizes triples to (s, r, O) for one-to-many relations; understanding this abstraction is prerequisite to grasping edit operations.
  - Quick check question: Given (Avatar, actors_are, {Worthington, Saldana}), what is the subject, relation, and object set?

- **Locate-Then-Edit Paradigm**
  - Why needed here: ROME and MEMIT use causal tracing to identify edit locations; understanding this clarifies why parameter-based methods scale poorly.
  - Quick check question: Why does modifying an MLP layer at one location affect downstream reasoning about related facts?

- **Multi-Hop vs. Complex Question Answering**
  - Why needed here: The paper distinguishes linear multi-hop chains from complex questions with branching logic and set operations.
  - Quick check question: How does "Who is the spouse of the U.S. president?" differ structurally from "Which educational institutions did both X and Y attend?"

## Architecture Onboarding

- **Component map**:
  - Knowledge base D: Wikidata-derived triples (s, r, O)
  - Edit engine E: Applies modifications {addition, deletion, substitution}
  - Question generator: Instantiates reasoning templates with entities
  - Evaluation metrics: Aug (augmentation accuracy), Ret (retention accuracy), Acc (overall)
  - KE methods: ROME/MEMIT (parameter-based), MeLLo/PokeMQA (memory-based)

- **Critical path**:
  1. Sample facts from Wikidata → filter by model recallability
  2. Construct reasoning templates → instantiate with entities
  3. Inject counterfactual edits → filter conflicting edits
  4. Convert to natural language → evaluate with target KE method
  5. Compute Aug/Ret/Acc metrics

- **Design tradeoffs**:
  - Parameter-based: Fast inference after edit, but scales poorly with batch size; not applicable to closed-source models
  - Memory-based: Applicable to any model, but requires strong instruction-following; performance varies with retrieval quality
  - One-to-many relations increase realism but complicate evaluation (partial credit via Aug/Ret vs. binary correctness)

- **Failure signatures**:
  - Overfitting: Model outputs newly injected entities regardless of question context (high Aug, contextually wrong)
  - Omission: Decomposition skips logical operations (e.g., intersection), yielding incomplete answers
  - Collapse: Model outputs incoherent text when edit batch size exceeds threshold (Figure 9 gibberish outputs)

- **First 3 experiments**:
  1. **Baseline sanity check**: Run MeLLo on COMPKE with GPT-4o-mini; verify accuracy is notably lower than on MQuAKE datasets (confirm benchmark difficulty)
  2. **Scale ablation**: Apply MEMIT to Qwen2.5-3B with k={1, 10, 100} edits; plot accuracy degradation curve to identify collapse threshold
  3. **Decomposition diagnosis**: Run MeLLo with original vs. COMPKE-adapted prompts on a subset requiring intersection; quantify omission rate by manually inspecting intermediate reasoning steps

## Open Questions the Paper Calls Out

- How can parameter-based editing methods be regularized to prevent overfitting in smaller models, where they currently tend to reproduce injected edits without performing the required logical operations?
- How does the performance of current knowledge editing methods degrade when applied to many-to-many or many-to-one relationships?
- To what extent do results on counterfactual edits generalize to real-world knowledge updates, which may involve more complex semantic shifts?
- How can memory-based decomposition strategies be improved to guarantee the inclusion of critical logical operations, such as intersection and union, in the reasoning plan?

## Limitations

- COMPKE's questions are grounded in Wikidata facts that may not represent broader complex reasoning scenarios
- Memory-based method performance may be over-estimated for closed-source models given reliance on proprietary API behavior
- Edit stability over time and across model versions is not evaluated, though knowledge editing is known to degrade with model updates

## Confidence

- **High confidence**: Complex question difficulty characterization and KE method performance rankings
- **Medium confidence**: Memory-based vs parameter-based tradeoffs at scale
- **Low confidence**: Generalization of failure modes to other knowledge editing tasks

## Next Checks

1. Test COMPKE with knowledge editing on non-Wikidata domains (e.g., scientific literature or financial data) to assess domain transfer
2. Implement multi-batch sequential editing to measure edit stability and interference effects beyond single-batch evaluations
3. Conduct ablation studies removing specific reasoning operations (e.g., logical intersection) to quantify their contribution to benchmark difficulty