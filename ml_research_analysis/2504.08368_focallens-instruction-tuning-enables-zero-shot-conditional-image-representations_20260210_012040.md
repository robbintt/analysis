---
ver: rpa2
title: 'FocalLens: Instruction Tuning Enables Zero-Shot Conditional Image Representations'
arxiv_id: '2504.08368'
source_url: https://arxiv.org/abs/2504.08368
tags:
- image
- clip
- vision
- representations
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of visual understanding being\
  \ inherently contextual\u2014what we focus on in an image depends on the task at\
  \ hand\u2014yet most existing image encoding paradigms represent an image as a fixed,\
  \ generic feature vector. The authors introduce FocalLens, a conditional visual\
  \ encoding method that produces different representations for the same image based\
  \ on the context of interest, expressed flexibly through natural language."
---

# FocalLens: Instruction Tuning Enables Zero-Shot Conditional Image Representations

## Quick Facts
- arXiv ID: 2504.08368
- Source URL: https://arxiv.org/abs/2504.08368
- Reference count: 19
- Primary result: Conditional image representations enable task-specific feature prioritization, achieving 5-10 point gains on SugarCrepe and MMVP-VLM benchmarks

## Executive Summary
FocalLens addresses the limitation of fixed, generic image representations by introducing conditional visual encoding that produces different embeddings for the same image based on task context expressed through natural language instructions. The method contrastively fine-tunes a pretrained vision encoder (CLIP or MLLM) to take instructions as additional inputs, producing embeddings that better align with instruction-specific visual features. Extensive experiments show FocalLens improves performance on image-image retrieval, classification, and image-text retrieval tasks, with particularly strong gains on challenging benchmarks.

## Method Summary
FocalLens contrastively fine-tunes a pretrained vision encoder to produce conditional image representations by taking natural language instructions as additional inputs. The instruction is encoded into a condition text embedding and treated as an additional token alongside image patches. During training, the model learns to align conditional image embeddings with output text embeddings from instruction tuning triplets. The approach works with both CLIP and MLLM backbones, with CLIP showing superior performance for visual detail preservation. Training uses batch size 2048, 20 epochs, learning rate 2e-5, and no weight decay.

## Key Results
- 5-point average gain on SugarCrepe benchmark
- 10-point average gain on MMVP-VLM benchmark
- 87 mAP on ColorShape color task (vs CLIP's 57)
- 93 mAP on ColorShape shape task (vs CLIP's 90)
- 46.84 mAP on CelebA-Identity (FocalLens-CLIP) vs 14.48 (FocalLens-MLLM)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conditioning vision encoders on natural language instructions enables task-specific feature prioritization within the same embedding space.
- **Mechanism:** The text instruction is encoded and injected into the vision encoder's forward pass as an additional token. The contrastive objective aligns the resulting conditional image representation with the text embedding of the expected output, forcing the encoder to attend selectively to instruction-relevant regions.
- **Core assumption:** Visual features relevant to different downstream tasks can be disentangled when explicitly conditioned; the frozen CLIP text encoder provides sufficient representational capacity for target output embeddings.
- **Evidence anchors:**
  - [abstract] "contrastively fine-tune a pretrained vision encoder to take natural language instructions as additional inputs for producing conditional image representations"
  - [section 3.2] "we consider first converting instruction into a 'condition text embedding', which is then treated as an additional token that is fed into the image encoder alongside the standard image tokens"
- **Break condition:** If instructions require reasoning beyond what's visually present, the mechanism degrades since it cannot ground to observable features.

### Mechanism 2
- **Claim:** Contrastive learning on instruction tuning triplets restructures the embedding space to cluster images by condition-specific similarity rather than generic semantic similarity.
- **Mechanism:** The training objective constructs a similarity matrix between conditional image embeddings and output text embeddings. The contrastive loss pushes correct pairings higher than off-diagonals, while same-image-different-instruction pairs within batches explicitly teach that similarity is conditional.
- **Core assumption:** The instruction tuning dataset covers sufficient diversity of conditions to generalize zero-shot; the frozen text encoder's output embeddings form a meaningful target space.
- **Evidence anchors:**
  - [section 4 Setup] "during training, we expand conversation data within batches to encourage models to output different representations given the same image but different instructions"
  - [table 1] ColorShape results show CLIP at 57 mAP for color-only but 90 for shape-only; FocalLens achieves 87 and 93 respectively
- **Break condition:** If the instruction tuning data is biased toward certain condition types, zero-shot generalization to underrepresented conditions will fail.

### Mechanism 3
- **Claim:** FocalLens-CLIP preserves richer visual detail than FocalLens-MLLM for non-linguistically-describable conditions due to architectural differences in output modality.
- **Mechanism:** CLIP's vision encoder produces embeddings directly, while MLLMs generate text tokens autoregressively. When retrofitted for embedding extraction via a special `<eos token>`, the MLLM representation may over-privilege features that map cleanly to language, discarding visual details difficult to verbalize.
- **Core assumption:** The task of interest requires visual fidelity beyond semantic label prediction.
- **Evidence anchors:**
  - [section 4.3] "FocalLens-MLLM suffers from a clear performance gap compared to FocalLens-CLIP... This suggests that FocalLens-MLLM may rely more on MLLM's original textual output modality"
  - [table 4] CelebA-Identity: FocalLens-MLLM 14.48 vs FocalLens-CLIP 46.84
- **Break condition:** For tasks where relevant visual features are explicitly linguistic, this distinction matters less; MLLM-based approach may suffice.

## Foundational Learning

- **Contrastive Learning (CLIP-style)**
  - Why needed here: FocalLens repurposes CLIP's training objective but applies it to conditional triplets rather than image-text pairs. Understanding how InfoNCE-style losses create embedding structure is essential.
  - Quick check question: Given a batch of 4 (image, instruction, output) triplets, can you sketch the similarity matrix S and explain which entries the contrastive loss pushes up vs down?

- **Vision Instruction Tuning Data Format**
  - Why needed here: The paper leverages LLaVA's instruction tuning dataset (150k examples). Understanding the (image, instruction, output) triplet structure and multi-turn conversation expansion is critical for implementation.
  - Quick check question: If a conversation has 3 turns about the same image, how many unique training triplets can be formed, and why does same-batch inclusion matter?

- **Adapter/Condition Token Injection**
  - Why needed here: FocalLens-CLIP adds a condition token to the ViT input sequence. This is a standard technique but requires understanding of how to properly initialize and train such adapters.
  - Quick check question: Where does the condition token get inserted in the sequence (before CLS, after CLS, anywhere), and does position matter?

## Architecture Onboarding

- **Component map:**
  FocalLens-CLIP: Image → ViT patches → [CLS, patch_1...patch_N, condition_token] → ViT encoder → pooled embedding → projection
  Instruction → Text encoder (frozen, shared weights with output encoder) → condition embedding
  Output text → Frozen CLIP text encoder → target embedding

- **Critical path:**
  1. Initialize from OpenAI CLIP-ViT-L-14-336 (428M params)
  2. Copy text encoder weights for instruction encoder (shared initialization)
  3. Convert instruction → condition token embedding via instruction text encoder
  4. Concatenate condition token to image patch sequence (position: after patches)
  5. Forward through ViT; extract CLS or pooled representation
  6. Compute contrastive loss against frozen output text encoder embeddings
  7. Train with batch size 2048, 20 epochs, lr 2e-5, no weight decay

- **Design tradeoffs:**
  - Frozen vs learnable output text encoder: Paper freezes it, ensuring stable target space. Risk: cannot adapt to domain shift.
  - Shared vs separate instruction/output text encoders: Paper initializes shared but does not ablate separation.
  - MLLM vs CLIP backbone: MLLM better for semantic tasks; CLIP better for visual detail. Choose based on downstream needs.

- **Failure signatures:**
  - Condition token ignored: If learning rate is too low or condition token initialization is poor, the model may produce instruction-agnostic embeddings.
  - Mode collapse: If batch diversity is insufficient, model may learn to ignore instructions entirely.
  - Overfitting to instruction phrasing: If evaluation instructions differ syntactically from training, performance drops.

- **First 3 experiments:**
  1. **ColorShape validation:** Replicate the toy dataset experiment (Table 1). Train on ColorShape with conditions "color", "shape", "both". Verify mAP separation across conditions.
  2. **Ablation on condition token position:** Test inserting condition token before CLS, after CLS, or after all patches. Measure impact on GeneCIS and ColorShape.
  3. **Instruction diversity stress test:** Take a held-out condition type absent from training data (e.g., "camera angle"). Evaluate zero-shot performance on MMVP-VLM camera subtask.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance of FocalLens be significantly improved by designing customized instruction tuning datasets specifically for conditional embedding, rather than relying on general-purpose conversational data?
- Basis in paper: [explicit] The authors state in the Limitations section that "model performance could be enhanced by designing customized datasets for this task, which we leave for future study."
- Why unresolved: The current model is trained on existing LLaVA instruction tuning data (approx. 150k examples) originally designed for text generation, which may lack the density of conditional signals needed for optimal embedding learning.

### Open Question 2
- Question: To what extent does the scale of the visual instruction tuning dataset limit the model's ability to align highly specialized concepts, and does scaling up resolve this?
- Basis in paper: [explicit] The authors note in the Limitations that the "relatively small scale of the visual instruction tuning datasets may hinder alignment accuracy for highly specialized concepts that are entirely absent from the dataset."
- Why unresolved: The paper validates the approach on standard benchmarks but does not test on niche domains or concepts likely missing from the 150k training set.

### Open Question 3
- Question: Can the FocalLens-MLLM architecture be modified to better capture fuzzy visual features (like identity) that are not easily described by text, similar to the FocalLens-CLIP variant?
- Basis in paper: [inferred] Section 4.3 reveals FocalLens-MLLM performs poorly on CelebA-Identity (14.48 mAP) compared to FocalLens-CLIP (46.84 mAP). The authors hypothesize this is due to the MLLM's reliance on textual output modalities.
- Why unresolved: The paper identifies this architectural limitation but does not propose methods to mitigate the MLLM's bias towards semantic concepts over visual fidelity.

## Limitations

- Zero-shot generalization claims rely on assumption that LLaVA instruction tuning dataset comprehensively covers visual conditions without explicit validation
- Choice to freeze output text encoder may limit adaptation to domains where instruction-output mapping differs from pretraining distribution
- Architectural difference between CLIP and MLLM approaches suggests CLIP-based method may not generalize to tasks requiring explicit generative reasoning

## Confidence

**High Confidence:**
- FocalLens outperforms CLIP on specific benchmarks tested (SugarCrepe, MMVP-VLM, GeneCIS, ColorShape)
- Mechanism of condition token injection into CLIP's vision encoder is technically sound and reproducible
- Contrastive training objective for conditional triplets is correctly implemented and validated

**Medium Confidence:**
- Zero-shot generalization claims across arbitrary conditions are supported by experiments but not exhaustively validated
- Architectural advantage of CLIP-based vs MLLM-based approaches is demonstrated but not fully explained mechanistically
- Assumption that frozen text encoders provide sufficient target space for diverse conditions is reasonable but untested under domain shift

**Low Confidence:**
- Claims about instruction tuning dataset's coverage of visual conditions are not empirically verified
- Long-term stability of conditional embeddings under distribution shift is not addressed

## Next Checks

1. **Instruction Coverage Stress Test:** Take the 50 most frequent instruction types from evaluation datasets (e.g., "count", "compare", "identify material") and check their presence in LLaVA instruction tuning dataset. If >30% are missing, zero-shot generalization to these conditions is unlikely.

2. **Domain Shift Robustness:** Fine-tune FocalLens on a held-out domain (e.g., medical images) and evaluate zero-shot transfer to a different domain (e.g., satellite imagery). Measure performance drop compared to fully fine-tuned CLIP baseline. If FocalLens retains >80% of fine-tuned CLIP's performance, the conditional approach is robust.

3. **Condition Token Ablation:** Train a variant of FocalLens where condition token is replaced with learnable prompt prefix (e.g., "Instruction: [text]"). Compare performance on GeneCIS and ColorShape. If prompt-based approach matches or exceeds condition token performance, architectural choice is less critical than contrastive objective.