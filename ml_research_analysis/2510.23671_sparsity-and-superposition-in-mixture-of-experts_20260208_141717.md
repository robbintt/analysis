---
ver: rpa2
title: Sparsity and Superposition in Mixture of Experts
arxiv_id: '2510.23671'
source_url: https://arxiv.org/abs/2510.23671
tags:
- feature
- features
- expert
- experts
- superposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how Mixture of Experts (MoE) architectures
  differ from dense networks in feature representation and superposition. The authors
  extend previous work on superposition in dense models to MoEs, finding that MoEs
  exhibit significantly less superposition than dense models with equivalent parameters,
  resulting in more monosemantic feature representations.
---

# Sparsity and Superposition in Mixture of Experts

## Quick Facts
- **arXiv ID:** 2510.23671
- **Source URL:** https://arxiv.org/abs/2510.23671
- **Reference count:** 23
- **Primary result:** MoEs exhibit significantly less superposition than dense models with equivalent parameters, resulting in more monosemantic feature representations.

## Executive Summary
This paper investigates how Mixture of Experts (MoE) architectures differ from dense networks in feature representation and superposition. The authors extend previous work on superposition in dense models to MoEs, finding that MoEs exhibit significantly less superposition than dense models with equivalent parameters, resulting in more monosemantic feature representations. Unlike dense models that show discrete phase changes in superposition based on feature sparsity and importance, MoEs demonstrate more continuous transitions governed by network sparsity (ratio of active to total experts). The authors propose a new definition of expert specialization based on monosemantic feature representation rather than load balancing, showing that experts naturally organize around coherent feature combinations when appropriately initialized.

## Method Summary
The authors extend toy model superposition analysis to MoE architectures using synthetic data with sparse, independently distributed features weighted by importance. They compare dense models to MoEs with varying numbers of experts and active expert counts, measuring feature norms, Gram matrices, and features-per-dimension across different sparsity and importance levels. The experiments systematically vary router initialization methods (diagonal, ordered k-hot, random k-hot) to test whether experts naturally specialize in monosemantically representing specific feature combinations. Key configurations include architectures with n=2, 3, and 100 features, varying expert counts, and different routing sparsity ratios to characterize the representational dynamics of MoE systems.

## Key Results
- MoEs show significantly less superposition than dense models with equivalent parameters, achieving more monosemantic feature representations
- Network sparsity (k/E ratio) governs MoE representational behavior more than feature sparsity, with continuous transitions replacing dense model phase changes
- Router initialization determines which features experts monosemantically represent, with diagonal and k-hot initializations enabling coherent feature specialization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MoEs achieve reduced superposition by partitioning interference through routing.
- **Mechanism**: The router assigns inputs to experts, creating block-diagonal interference structure. Features routed to different experts have zero interference (global W^TW becomes expert-local W_e^TW_e), reducing optimization pressure to pack features superpositionally.
- **Core assumption**: Router partitions feature space into approximately orthogonal subsets; features within a partition are more correlated than across partitions.
- **Evidence anchors**:
  - [abstract] "MoEs exhibit significantly less superposition than dense models with equivalent parameters"
  - [Section 4.2] "Unlike the dense model (Figure 1b), where any feature can interfere with any other, the MoE enforces a block-diagonal structure where features routed to different experts have zero interference"
  - [corpus] Related work (MoE-X, 2503.07639) shows architectural constraints can enforce monosemanticity, but this paper suggests it emerges from sparse routing itself.
- **Break condition**: If k > 1 or router fails to partition (all inputs route to one expert), interference partitioning degrades.

### Mechanism 2
- **Claim**: Network sparsity (k/E ratio), not feature sparsity, governs MoE representational behavior.
- **Mechanism**: Increasing total experts (E) while keeping active experts (k) fixed reduces features-per-dimension. Experts approach the monosemantic limit (n_learned ≈ m_total) rather than exceeding dimensions through superposition.
- **Core assumption**: Feature importance is non-uniform; experts select important features to represent monosemantically.
- **Evidence anchors**:
  - [abstract] "network sparsity (the ratio of active to total experts) better characterizes MoEs"
  - [Section 5] "discrete phase changes disappeared" in MoEs; transitions become continuous with increasing E
  - [corpus] Optimal Sparsity (2508.18672) suggests sparsity dimension in MoEs behaves differently from dense scaling, supporting distinct governing dynamics.
- **Break condition**: If k/E ratio approaches 1 (all experts active), behavior converges to dense model with phase changes restored.

### Mechanism 3
- **Claim**: Router initialization determines which features experts monosemantically represent.
- **Mechanism**: Diagonal or k-hot router initialization assigns initial "ownership" of features to experts. During training, experts monosemantically represent features they were initialized to route, especially high-importance features.
- **Core assumption**: Training preserves initialization structure; gradient updates don't collapse specialization.
- **Evidence anchors**:
  - [Section 6] "When the gate matrix is initialized with ones along the main diagonal, each expert monosemantically represents the single feature it initially occupied"
  - [Table 1] 100% of monosemantically represented features are "occupied" by their expert under k-hot initialization
  - [corpus] Weak direct evidence in neighbors; initialization-specialization link is a novel contribution here.
- **Break condition**: Random initialization without load balancing can cause expert collapse (Section A.4 shows single expert receiving all inputs).

## Foundational Learning

- **Concept: Superposition and Polysemanticity**
  - Why needed here: The entire paper frames MoE behavior as a departure from dense model superposition dynamics. Without understanding that dense models pack non-orthogonal features into shared dimensions (trading interpretability for capacity), the "less superposition" finding lacks context.
  - Quick check question: Can you explain why sparse features allow superposition with minimal interference cost?

- **Concept: Phase Transitions in Representation**
  - Why needed here: The paper's key contrast is that dense models show discrete phase changes (ignored → superimposed → monosemantic) while MoEs show continuous transitions. Understanding phase diagrams (Fig 4) requires knowing what the axes and regions represent.
  - Quick check question: In Elhage et al.'s toy models, what two input distribution parameters control whether a feature is ignored vs. represented?

- **Concept: MoE Routing and Load Balancing**
  - Why needed here: The paper deliberately varies load balancing (with/without auxiliary loss) and routing initialization. Understanding why load balancing prevents expert collapse is essential for interpreting experimental choices.
  - Quick check question: Why does top-k routing without load balancing sometimes collapse to using only one expert?

## Architecture Onboarding

- **Component map**: Input layer -> Router -> Expert selection (top-k) -> Per-expert processing (W_e, b_e) -> Weighted output sum

- **Critical path**: Router weights → expert selection → expert weight matrices → reconstruction. The router's partitioning of input space (convex cones for k=1) determines which expert learns which features.

- **Design tradeoffs**:
  - More experts (higher E) → less superposition but higher variance in loss (Fig 8) and training instability
  - k=1 routing → clean convex cone partitioning but brittle; k>1 → unions of cones, specialization theory breaks down
  - Load balancing loss → prevents collapse but may interfere with natural specialization patterns
  - m (hidden dim per expert) must be ≥ active features per input for monosemantic representation (Section A.5)

- **Failure signatures**:
  - Expert collapse: All inputs route to single expert (visible in routing visualizations, Section A.4)
  - High superposition in specific experts: Check W_e^T·W_e off-diagonal values; red regions in feature norm plots
  - Unspecialized experts: Low usage variance between "monosemantic feature active" vs. arbitrary inputs (Table 1 pattern breaks)

- **First 3 experiments**:
  1. **Replicate Figure 3**: Train dense (m=20) and MoE (E=4, m=5, k=1) models with n=100 features, measure features-per-dimension across sparsity levels. Confirm MoE approaches monosemantic limit.
  2. **Ablate router initialization**: Compare diagonal, ordered k-hot, random k-hot, and Xavier initialization. Measure correlation between initial feature assignment and final monosemantic representation (following Table 1 protocol).
  3. **Vary network sparsity**: Fix total parameters, sweep E ∈ {2, 4, 10, 20} with proportional k. Plot reconstruction loss vs. superposition score to quantify the interpretability/capacity tradeoff frontier.

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns about whether toy model findings translate to practical MoE systems with complex, real-world sparse features
- Unclear how initialization-based specialization translates to learned routing patterns in pre-trained models
- Limited exploration of how superposition reduction affects downstream task performance versus interpretability gains

## Confidence
- **High Confidence**: The empirical observation that MoEs show less superposition than dense models (Figure 3), supported by direct measurements of feature norms and Gram matrices.
- **Medium Confidence**: The claim that network sparsity (k/E ratio) better characterizes MoE behavior than feature sparsity, based on phase transition experiments but requiring more extensive parameter sweeps for full validation.
- **Low Confidence**: The assertion that router initialization directly determines which features experts represent monosemantically, as this relationship is demonstrated only in toy settings and may not hold under realistic training dynamics.

## Next Checks
1. **Scale-up validation**: Replicate the superposition comparison between dense and MoE architectures using larger feature spaces (n=100-500) and more experts (E=10-50) to test whether the reduced superposition effect persists at scales relevant to production MoE systems.

2. **Dynamic routing analysis**: Track expert specialization over training time in both initialized and random routing settings to determine whether initial assignment patterns persist or whether experts converge to different feature representations through optimization dynamics.

3. **Generalization to real data**: Apply the superposition measurement methodology to pre-trained MoE language models using sparse feature probes (e.g., activation-based feature extraction) to verify whether the theoretical benefits translate to actual model interpretability improvements.