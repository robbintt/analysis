---
ver: rpa2
title: 'FISCAL: Financial Synthetic Claim-document Augmented Learning for Efficient
  Fact-Checking'
arxiv_id: '2511.19671'
source_url: https://arxiv.org/abs/2511.19671
tags:
- claim
- financial
- other
- minicheck-fiscal
- assets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating compact, reliable
  fact-checking models for financial applications, where accuracy is critical but
  large models are computationally expensive. The authors introduce FISCAL, a modular
  framework for generating synthetic claim-document-label triplets tailored to financial
  fact-checking, and use it to train MiniCheck-FISCAL, a 7B parameter verifier.
---

# FISCAL: Financial Synthetic Claim-document Augmented Learning for Efficient Fact-Checking

## Quick Facts
- arXiv ID: 2511.19671
- Source URL: https://arxiv.org/abs/2511.19671
- Authors: Rishab Sharma; Iman Saberi; Elham Alipour; Jie JW Wu; Fatemeh Fard
- Reference count: 40
- Primary result: FISCAL framework generates synthetic data to train MiniCheck-FISCAL (7B) achieving 86.43 F1, outperforming larger models and baselines

## Executive Summary
This paper addresses the challenge of creating compact, reliable fact-checking models for financial applications, where accuracy is critical but large models are computationally expensive. The authors introduce FISCAL, a modular framework for generating synthetic claim-document-label triplets tailored to financial fact-checking, and use it to train MiniCheck-FISCAL, a 7B parameter verifier. FISCAL employs multiple augmentation strategies including paraphrasing, conflict insertion, and value distortion to create diverse, challenging training data. MiniCheck-FISCAL achieves an F1 score of 86.43 on the FISCAL dataset, outperforming its baseline and rivaling much larger models.

## Method Summary
The FISCAL framework generates synthetic training data by extracting numerical atomic claims from FinanceBench documents using Qwen3 32B, then applying six augmentation modules (Claim Paraphraser, Conflict Insertion, Fact Exclusion, Fact Value Distortion, Mis-attribution, Summarization) to create balanced positive/negative triplets. These triplets are validated by two LLM judges with high agreement. MiniCheck-FISCAL, a 7B parameter model, is fine-tuned using LoRA for single-token yes/no prediction on claim-document pairs, with the "yes" token probability serving as confidence score. The model is trained on 14,304 triplets and evaluated on FISCAL-data, FinDVer, and Fin-Fact benchmarks.

## Key Results
- MiniCheck-FISCAL achieves 86.43 F1 on FISCAL-data, outperforming baseline (66.86 F1) and rivaling much larger models
- Ablation study shows Claim Paraphraser module is critical for recall (drops from 84.98 to 53.03 when removed)
- Outperforms Qwen2-72B (31.96 F1) and approaches accuracy of GPT-4o on external benchmarks
- Achieves high precision (84.98) with competitive recall (84.98) on FISCAL-data

## Why This Works (Mechanism)

### Mechanism 1: Modular synthetic data augmentation improves fact-checking recall by exposing the model to diverse failure modes
The FISCAL framework uses six augmentation modules to generate challenging claim-document-label triplets targeting specific factual inconsistencies. Training on this diverse synthetic data teaches the verifier to detect both overt and subtle hallucinations, particularly improving recall by reducing false negatives.

### Mechanism 2: Fine-tuning a lightweight model on domain-specific synthetic data enables it to rival much larger general-purpose LLMs
MiniCheck-FISCAL, a 7B parameter model, is fine-tuned using LoRA on the FISCAL dataset. This domain-specific training allows the small model to specialize in numerical financial claim verification, achieving an F1 of 86.43 on FISCAL-data and outperforming larger models like Qwen2-72B on the same benchmark.

### Mechanism 3: Reformulating verification as a single-token causal language modeling task enables efficient and interpretable inference
The paper reformulates claim verification as a causal language modeling (CLM) task where the model outputs a single token ("yes" or "no"). The probability of the "yes" token serves as a confidence score, making inference efficient and the output directly interpretable.

## Foundational Learning

- **Synthetic Data Generation & Augmentation**: Why needed: The core of the FISCAL framework is its ability to generate synthetic training data. Quick check: What is the primary risk of training a model exclusively on synthetic data compared to real-world data?

- **LoRA (Low-Rank Adaptation) Fine-Tuning**: Why needed: The paper specifies using LoRA to fine-tune MiniCheck-7B. Quick check: What is the main benefit of using LoRA over full fine-tuning for large language models?

- **Evaluation Metrics for Classification (Precision, Recall, F1)**: Why needed: The paper extensively reports Precision, Recall, and F1 scores. Quick check: If a model has very high precision but very low recall on a fact-checking task, what kind of errors is it likely making?

## Architecture Onboarding

- **Component map**: Modular Claim-Document Generator (FISCAL) -> FISCAL dataset -> MiniCheck-FISCAL (7B) fine-tuned with LoRA -> Single-token yes/no prediction with confidence score

- **Critical path**: The data synthesis stage within the FISCAL generator is most critical. The quality and diversity of the 14,304 training triplets directly determine the fact-checker's ability to generalize.

- **Design tradeoffs**: Synthetic data coverage vs. realism (synthetic perturbations cannot capture all real-world error complexity); inference efficiency vs. reasoning depth (single-token output is fast but may lose nuance for complex multi-hop claims); binary classification simplifies evaluation but omits finer distinctions like partial support.

- **Failure signatures**: Overfitting to synthetic artifacts (may perform well on FISCAL-like data but fail on real-world financial documents with different error patterns); low recall (if certain types of hallucinations are not well-represented in synthetic data); LLM-Judge Bias (systematic biases from models used as judges could propagate into final dataset).

- **First 3 experiments**:
  1. Module Ablation Reproduction: Retrain MiniCheck-FISCAL by leaving out one augmentation module at a time to verify reported contributions.
  2. Out-of-Distribution Generalization Test: Evaluate on new external dataset of financial claims not derived from FinanceBench.
  3. Confidence Threshold Analysis: Experiment with different confidence thresholds (Ï„) for the "yes" prediction to find optimal operating point for various risk tolerances.

## Open Questions the Paper Calls Out

- How can the FISCAL framework be effectively extended to handle multimodal financial evidence, such as tables and charts? (Future work aims for multimodal evidence and results' interpretability)

- Can the verification task be reformulated to capture nuanced reasoning, such as partial support or multi-hop evidence, rather than binary classification? (Current evaluation frames verification as binary decision, which simplifies nuanced reasoning involving partial support)

- To what extent does the reliance on LLMs for data synthesis and validation introduce systematic biases or model the limitations of the teacher models? (Pipeline relies on LLMs as both generators and judges, which may introduce systematic biases relative to human annotators)

## Limitations

- Synthetic data coverage cannot fully capture real-world financial error complexity, potentially limiting generalization to novel claim types
- Single-token inference may oversimplify complex multi-hop reasoning tasks requiring deeper contextual understanding
- Reliance on LLM judges for data validation introduces potential systematic biases that could propagate through the dataset

## Confidence

- **High Confidence**: MiniCheck-FISCAL achieves 86.43 F1 on FISCAL-data and outperforms baseline
- **Medium Confidence**: MiniCheck-FISCAL rivals much larger models on external benchmarks
- **Low Confidence**: Framework's applicability to other domains beyond finance

## Next Checks

1. Evaluate MiniCheck-FISCAL on diverse financial documents from sources completely independent of FinanceBench to assess true generalization capability

2. Create and test a benchmark of multi-hop financial claims requiring synthesizing information from multiple document sections to determine if single-token inference limits performance on complex verification tasks

3. Conduct systematic analysis of model's confidence scores across different types of claims to verify that probability outputs are well-calibrated for practical deployment in high-stakes financial environments