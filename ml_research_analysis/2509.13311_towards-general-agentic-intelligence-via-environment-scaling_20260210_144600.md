---
ver: rpa2
title: Towards General Agentic Intelligence via Environment Scaling
arxiv_id: '2509.13311'
source_url: https://arxiv.org/abs/2509.13311
tags:
- arxiv
- tool
- agentic
- agent
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling environments to advance
  general agentic intelligence for Large Language Models. The authors propose a systematic
  approach to automatically construct heterogeneous, fully simulated environments
  by leveraging real-world APIs, domain detection, and programmatic materialization
  of tool schemas into database interactions.
---

# Towards General Agentic Intelligence via Environment Scaling

## Quick Facts
- arXiv ID: 2509.13311
- Source URL: https://arxiv.org/abs/2509.13311
- Reference count: 9
- Models: AgentScaler-4B, AgentScaler-8B, AgentScaler-30B-A3B achieve SOTA among open-source models under 1T params, with 30B-A3B matching trillion-parameter models and closed-source systems on benchmarks

## Executive Summary
This paper addresses the challenge of scaling environments to advance general agentic intelligence for Large Language Models. The authors propose a systematic approach to automatically construct heterogeneous, fully simulated environments by leveraging real-world APIs, domain detection, and programmatic materialization of tool schemas into database interactions. A two-phase agent experience learning strategy is introduced: first, acquiring fundamental tool-calling skills across general domains, and then specializing within target vertical domains. Extensive experiments on benchmarks like τ-bench, τ2-Bench, and ACEBench show that their AgentScaler models (4B, 8B, 30B-A3B) achieve state-of-the-art performance among open-source models under 1T parameters, with AgentScaler-30B-A3B delivering results on par with trillion-parameter models and closed-source systems.

## Method Summary
The authors present AgentScaler, a scalable framework for constructing heterogeneous simulated environments to advance agentic intelligence. The pipeline begins by collecting >30K APIs from ToolBench, API-Gen, and internal sources, then builds a tool dependency graph using parameter similarity and Louvain community detection to partition into domains. For each domain, database schemas are generated and tools materialized as executable Python code operating on these schemas. A two-phase training strategy follows: Stage 1 trains on broad domains for fundamental tool-calling skills, while Stage 2 specializes on vertical domains. Data generation involves simulated user-agent interplay with three-stage trajectory filtering (validity control, environment state alignment, function calling exact match) to ensure high-fidelity supervision.

## Key Results
- AgentScaler-30B-A3B achieves 76.4 Pass@1 on τ-bench, matching results from trillion-parameter models and closed-source systems
- Two-phase training strategy improves performance over single-phase approaches on both τ-bench and τ2-bench
- AgentScaler models outperform all open-source competitors under 1T parameters on τ-bench, τ2-bench, and ACEBench

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automatically constructing heterogeneous simulated environments through tool dependency graph modeling and programmatic materialization may broaden the space of verifiable function-calling scenarios for agent training.
- Mechanism: The pipeline collects APIs → builds a tool dependency graph via parameter similarity → partitions into domains using Louvain community detection → generates domain-specific database schemas → materializes tools as executable Python code operating on those databases. This grounds tool execution in database state transitions, enabling two-level verifiability.
- Core assumption: Parameter similarity (cos-similarity over vectorized parameter lists) captures meaningful functional dependencies between tools, and database-structured environments sufficiently approximate real-world API behavior.
- Evidence anchors:
  - [abstract] "design a scalable framework that automatically constructs heterogeneous environments that are fully simulated, systematically broadening the space of function-calling scenarios"
  - [Section 2.1] "We employ Louvain community detection (Blondel et al., 2008) to identify coherent tool communities that serve as domains. In total, we obtained M domains (exceeding 1,000)."
  - [corpus] Related work (e.g., ARE, Hephaestus) corroborates environment scaling as a research direction, but causal proof that graph-based domain partitioning improves outcomes remains unestablished.
- Break condition: If parameter similarity fails to reflect true functional dependencies, or if database abstractions diverge from actual API semantics, generated environments may produce misleading training signals.

### Mechanism 2
- Claim: A two-phase training strategy—general tool-calling skill acquisition followed by domain-specific specialization—appears to improve agentic performance over single-phase approaches, based on observed benchmark gains.
- Mechanism: Stage 1 trains on broad domains to establish when/how to invoke tools and integrate responses. Stage 2 fine-tunes on vertical domains with realistic scenarios, refining tool selection, parameterization, and domain-aligned responses. The ordering prioritizes breadth-first capability building before specialization.
- Core assumption: Fundamental skills learned in general domains transfer positively to specialized domains (transfer learning assumption), and sequential staging outperforms joint training.
- Evidence anchors:
  - [abstract] "two-phase agent fine-tuning strategy: first endowing agents with fundamental agentic capabilities, then specializing them for domain-specific contexts"
  - [Section 3.2] "This stage emphasizes breadth and generality, ensuring that the agent builds a versatile foundation of agentic behaviors before domain-specific specialization."
  - [Figure 3] Shows Stage 1 and Stage 2 both improve over base, with Stage 2 further lifting Agent subset scores.
  - [corpus] Corpus evidence on general→specific curricula for agents is limited; no direct comparative studies cited.
- Break condition: If early specialization is required for certain domains, or if general-stage training introduces interference/negative transfer, the two-phase ordering could underperform joint or reverse-order training.

### Mechanism 3
- Claim: A three-stage funnel-based trajectory filtering framework (validity control → environment state alignment → function calling exact match) is proposed to yield higher-fidelity supervision, with performance gains observed in the trained models.
- Mechanism: (1) Validity control removes malformed exchanges and n-gram repetitive reasoning; (2) Environment state alignment retains trajectories whose final database state matches the golden state; (3) Function calling exact match preserves only trajectories with exact tool sequence and argument matches. Trajectories with intermediate errors are retained to support robustness.
- Core assumption: Database state consistency and exact tool-sequence matching correlate with task success and represent desirable agent behavior; golden references accurately reflect task requirements.
- Evidence anchors:
  - [Section 3.1] "We adopt a three-stage funnel-based trajectory filtering framework consisting of validity control, environment state alignment, and function calling exact match."
  - [Section 3.1] "A trajectory is preserved only if the sequence of invoked tools and arguments exactly matches the overall intent, ensuring high-fidelity supervision."
  - [corpus] No direct corpus evidence validating this specific filtering hierarchy; remains an unproven heuristic.
- Break condition: If filtering is too strict, it may remove useful behavioral diversity; if too loose, it may admit incorrect trajectories. If golden states do not reflect true task success, supervision signal degrades.

## Foundational Learning

- Concept: **Tool Dependency Graphs**
  - Why needed here: Understanding how tools relate via parameter compatibility is prerequisite to domain partitioning and coherent tool-sequence sampling.
  - Quick check question: Can you explain how an edge is inserted between two tools in the dependency graph, and what threshold governs this decision?

- Concept: **Database-Backed Environment Simulation (Read/Write Abstraction)**
  - Why needed here: The environment design treats each function as either a read (query) or write (state transition) over an underlying database D, grounding verification in state consistency.
  - Quick check question: For a given tool, how would you determine whether it is read-type or write-type, and what database operation does it induce?

- Concept: **Curriculum Learning (General → Specialized)**
  - Why needed here: The two-phase training strategy assumes building general tool-calling competence before domain-specific refinement improves final performance.
  - Quick check question: What risks might arise if domain specialization occurred before general capability acquisition?

## Architecture Onboarding

- Component map:
  - Tool Corpus (30K+ APIs from ToolBench, API-Gen, internal sources) → Filtering & rewriting
  - Tool Dependency Graph (parameter similarity edges + LLM verification)
  - Domain Partitioning (Louvain community detection → 1,000+ domains)
  - Schema Generation (domain-specific database structures)
  - Code Materialization (tools as Python executing read/write on schemas)
  - Task Constructor (sample tool sequences, generate arguments, initialize DB state, synthesize user intent)
  - Interplay Simulator (simulated user + task agent + environment)
  - Trajectory Filter (validity → state alignment → exact match)
  - Two-Phase Trainer (Stage 1: general; Stage 2: vertical domains; train only on tool calls and assistant responses)

- Critical path:
  1. API collection and quality filtering
  2. Graph construction + domain partitioning
  3. Schema + code materialization
  4. Task construction with verifiable tool sequences
  5. Simulated interplay to generate trajectories
  6. Three-stage filtering
  7. Stage 1 training → Stage 2 specialization

- Design tradeoffs:
  - Strict filtering (exact match) vs. diversity retention (keeping error trajectories for robustness)
  - Simulated environments vs. real API calls (cost, latency, stability vs. fidelity)
  - Two-phase vs. joint training (assumed benefit; not comparatively proven)

- Failure signatures:
  - Tool graph poorly connected → insufficient domain coherence
  - Schema mismatches with real APIs → simulation-reality gap
  - Over-aggressive filtering → low data yield, reduced behavioral diversity
  - Stage 1/2 mismatch → negative transfer if general skills don't align with target domains

- First 3 experiments:
  1. Validate domain partition quality: Sample 5–10 domains and manually verify that tools within each domain share coherent read/write patterns and the generated schema supports their operations.
  2. Test verifiability: Run 100 constructed tasks through the interplay simulator; confirm that both database-level state consistency and tool-sequence exact match checks can be programmatically verified.
  3. Ablate filtering stages: Train small models with full filtering vs. each stage removed; measure impact on τ-bench/ACEBench metrics to assess each filter's contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the negative correlation between the number of tool calls and task accuracy be mitigated to enable robust long-horizon agentic planning?
- Basis in paper: [explicit] The analysis section states: "Long-horizon tool calling remains a fundamental challenge... underscoring that handling extended tool-use chains is still an open problem that we plan to address in future work."
- Why unresolved: Current models, including AgentScaler, exhibit a clear drop in accuracy as the number of required tool calls increases (Figure 5), indicating failure in extended reasoning chains.
- What evidence would resolve it: Demonstrated maintenance of high accuracy (comparable to short-horizon tasks) on evaluation sets specifically constructed to require long sequences of interdependent tool calls.

### Open Question 2
- Question: To what extent can reinforcement learning (RL) optimization leverage the proposed fully simulated environments to surpass the performance of supervised fine-tuning (SFT)?
- Basis in paper: [explicit] In the Limitations section, the authors note the "Lack of Reinforcement Learning" and state: "In future work, we aim to integrate RL on top of our simulated environment to further improve the agentic behavior of the model."
- Why unresolved: The current study relies exclusively on a two-stage SFT approach; the potential gains from using the low-latency, verifiable simulated environment for RL policy optimization remain untested.
- What evidence would resolve it: Experimental results comparing AgentScaler-SFT against an AgentScaler-RL variant, showing statistically significant improvements in pass@$k$ metrics or task success rates.

### Open Question 3
- Question: Does the efficacy of environment scaling and two-stage training persist when applied to models with significantly larger parameter counts (exceeding 200B or 1T)?
- Basis in paper: [explicit] The authors explicitly list "Model Scale" as a limitation, writing: "Our method has so far only been validated on a 30B-scale architecture, without extension to larger models exceeding 200B or even trillion-parameter scales."
- Why unresolved: It is unclear if the data efficiency and specific training pipeline designed for compact models (4B-30B) are optimal or necessary for architectures that may possess inherent reasoning capabilities at larger scales.
- What evidence would resolve it: Evaluation of models exceeding 200B parameters trained with the AgentScaler pipeline, showing they achieve new state-of-the-art results or outperform baseline models trained on standard corpora.

## Limitations
- Simulation-reality gap: The paper acknowledges that simulated environments may not fully capture real-world API behavior and behavior may not transfer to real scenarios
- Model scale limitation: The approach has only been validated on models up to 30B parameters, without extension to larger models exceeding 200B or trillion-parameter scales
- Lack of reinforcement learning: The study relies exclusively on supervised fine-tuning, with RL optimization remaining an open avenue for future work

## Confidence

- **High Confidence**: Environment construction methodology (graph clustering + schema generation) and two-phase training outline are clearly specified and reproducible.
- **Medium Confidence**: Benchmark results and filtering pipeline efficacy, given dependence on internal data and unproven simulation-reality alignment.
- **Low Confidence**: Causal claims that graph-based domain partitioning or two-phase curriculum directly improve performance, due to lack of ablation or real-API comparisons.

## Next Checks

1. **Domain Coherence Validation**: Sample and manually inspect 5-10 generated domains to verify that tools within each domain share coherent read/write patterns and the generated schema supports their operations.
2. **Filtering Ablation Study**: Train small models using the full filtering pipeline versus each stage removed; measure impact on τ-bench/ACEBench metrics to quantify each filter's contribution.
3. **Simulation-Reality Gap Assessment**: Select a subset of tasks and execute them against real APIs; compare success rates and trajectory fidelity with simulated environment outcomes to quantify fidelity loss.