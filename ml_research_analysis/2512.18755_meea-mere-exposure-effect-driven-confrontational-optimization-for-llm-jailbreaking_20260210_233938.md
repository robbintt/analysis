---
ver: rpa2
title: 'MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking'
arxiv_id: '2512.18755'
source_url: https://arxiv.org/abs/2512.18755
tags:
- meea
- prompt
- safety
- attack
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of jailbreaking large language
  models (LLMs) by proposing MEEA, a framework inspired by the mere exposure effect
  in psychology. MEEA leverages repeated low-toxicity semantic exposure to progressively
  weaken LLM safety boundaries over multi-turn interactions.
---

# MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking

## Quick Facts
- **arXiv ID:** 2512.18755
- **Source URL:** https://arxiv.org/abs/2512.18755
- **Reference count:** 40
- **Primary result:** MEEA achieves 20% higher Attack Success Rate (ASR) than seven baselines by leveraging repeated semantic exposure to erode LLM safety boundaries

## Executive Summary
This paper introduces MEEA, a framework for jailbreaking large language models (LLMs) by exploiting the mere exposure effect from psychology. MEEA uses simulated annealing optimization to progressively weaken LLM safety boundaries through repeated low-toxicity semantic exposure across multi-turn interactions. The framework constructs semantically progressive prompt chains that incrementally shift the semantic center while maintaining contextual coherence, effectively exploiting the history-dependent nature of LLM safety behavior. Experiments on both closed-source and open-source models demonstrate MEEA's superior effectiveness compared to existing baselines.

## Method Summary
MEEA operates by iteratively constructing semantically progressive prompt chains through a simulated annealing optimization process. The method maintains a "Margin" metric that quantifies the semantic gap between low-toxicity contexts and jailbreak prompts, using this to guide the construction of intermediate prompts. Each iteration optimizes for semantic similarity to maintain context coherence, toxicity scoring to ensure prompts remain low-risk, and effectiveness in advancing toward jailbreak goals. The framework builds multi-turn attack chains where each prompt is semantically aligned with previous contexts while gradually approaching the target jailbreak objective. This progressive exposure strategy exploits the dynamic nature of LLM safety boundaries, demonstrating that repeated exposure to semantically similar but increasingly permissive contexts can erode safety alignments over time.

## Key Results
- MEEA achieves a 20% higher average Attack Success Rate (ASR) compared to seven representative baselines
- The method successfully jailbreaks both closed-source models (GPT-4, Claude-3.5, DeepSeek-R1) and open-source models (LLaMA-3.1-8B, Qwen3-8B)
- Ablation studies confirm the necessity of annealing-based optimization and contextual exposure mechanisms for effectiveness

## Why This Works (Mechanism)
MEEA exploits the dynamic and history-dependent nature of LLM safety behavior by leveraging repeated semantic exposure to progressively erode safety boundaries. The mere exposure effect suggests that repeated exposure to stimuli increases familiarity and acceptance, which MEEA applies to semantic contexts surrounding restricted content. By constructing multi-turn interaction chains where each prompt is semantically similar to previous contexts while gradually shifting toward prohibited content, the framework exploits the LLM's contextual understanding and safety alignment mechanisms. The simulated annealing optimization ensures smooth semantic progression that avoids triggering immediate safety responses while accumulating sufficient contextual exposure to weaken boundary enforcement over time.

## Foundational Learning
- **Simulated annealing optimization**: Needed for exploring the semantic space while avoiding local optima in prompt construction; quick check: verify temperature schedule and acceptance probability calculations
- **Semantic similarity metrics**: Required to maintain contextual coherence while shifting toward jailbreak objectives; quick check: test different similarity measures (cosine, Euclidean) on embedding spaces
- **Toxicity scoring mechanisms**: Essential for ensuring prompts remain low-risk while progressing toward targets; quick check: validate scorer calibration across different model outputs
- **Multi-turn context management**: Critical for exploiting history-dependent safety behavior; quick check: measure context retention and influence across prompt sequences
- **Safety boundary quantification**: Needed to measure the effectiveness of progressive exposure; quick check: validate Margin metric sensitivity to semantic shifts
- **Prompt chain construction**: Fundamental for building coherent multi-turn attacks; quick check: ensure semantic continuity between consecutive prompts

## Architecture Onboarding

**Component Map:** Input context -> Semantic center calculation -> Prompt generation -> Safety scoring -> Effectiveness evaluation -> Optimization loop -> Output prompt chain

**Critical Path:** The optimization loop that iteratively refines prompts based on semantic similarity, toxicity, and effectiveness scores is the core mechanism enabling MEEA's success.

**Design Tradeoffs:** MEEA trades computational complexity for attack effectiveness by using iterative optimization rather than direct prompt generation. The framework prioritizes semantic coherence over speed, accepting longer optimization times to ensure smooth progressive exposure that evades safety mechanisms.

**Failure Signatures:** The framework may fail when semantic similarity metrics cannot adequately capture nuanced contextual shifts, when toxicity scorers are too conservative and block progression, or when the optimization gets trapped in local minima that don't advance toward jailbreak objectives.

**First Experiments:**
1. Test the framework's ability to generate semantically coherent prompt chains using only semantic similarity optimization (no toxicity or effectiveness constraints)
2. Validate the Margin metric's sensitivity by measuring boundary shifts across controlled semantic progressions
3. Evaluate single-turn versus multi-turn effectiveness to quantify the benefit of progressive exposure

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the MEEA framework effectively jailbreak models in non-textual modalities, such as code generation or multimodal systems?
- **Basis in paper:** Section 7 (Limitations) states the study focuses on textual dialogue and the "applicability of MEEA to multimodal systems or models with executable capabilities remains an open question."
- **Why unresolved:** The current optimization relies on semantic similarity and toxicity scorers tailored for natural language, which may not capture the syntax of code or the features of image embeddings.
- **What evidence would resolve it:** Successful application of MEEA to vision-language models (e.g., GPT-4V) or code interpreters using modality-adapted objective functions.

### Open Question 2
- **Question:** How can dynamic alignment drift be formalized into a theoretical model that predicts safety failures based on interaction history?
- **Basis in paper:** The authors explicitly call for "establishing theoretical models to characterize dynamic alignment drift and path-dependent safety behavior" in Section 7.
- **Why unresolved:** The paper demonstrates the phenomenon empirically through the "Margin" metric but lacks a generalized mathematical theory explaining the underlying mechanics of this drift.
- **What evidence would resolve it:** A formal mathematical framework that quantifies the rate of safety boundary erosion as a function of cumulative context vectors.

### Open Question 3
- **Question:** What specific interaction-aware defense mechanisms can mitigate history-dependent attacks like MEEA without flagging benign long-context dialogues?
- **Basis in paper:** The authors conclude that findings "highlight the need for interaction-aware safety evaluation and defense mechanisms," but the paper focuses solely on the attack methodology.
- **Why unresolved:** Existing defenses are static and per-turn; it is unclear how to distinguish between a benign information-seeking session and a malicious "semantic progression" in real-time.
- **What evidence would resolve it:** A defense strategy that successfully monitors multi-turn intent accumulation (e.g., tracking the trajectory of the proposed "Margin" metric) to prevent safety erosion.

## Limitations
- The study focuses exclusively on single-turn attacks despite the framework's design for multi-turn interactions, potentially understating full effectiveness
- Experimental validation relies heavily on simulated environments and automated metrics rather than real-world adversarial scenarios
- The psychological mechanism claim (mere exposure effect) is not empirically validated within the LLM context

## Confidence

**High confidence:** The framework's technical implementation and optimization strategy are well-documented and reproducible.

**Medium confidence:** Comparative performance metrics against baselines, though experimental scope limitations apply.

**Low confidence:** The psychological mechanism claim (mere exposure effect) as the primary driver of observed effects.

## Next Checks
1. Conduct multi-turn attack experiments with actual human participants to validate the mere exposure effect hypothesis in real-world settings.
2. Test MEEA's effectiveness across diverse language models and non-English language pairs to assess cross-linguistic generalizability.
3. Evaluate defensive strategies specifically designed to counter semantic progressive exposure attacks to establish practical robustness metrics.