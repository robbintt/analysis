---
ver: rpa2
title: Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection
  in Large Language Models
arxiv_id: '2511.17946'
source_url: https://arxiv.org/abs/2511.17946
tags:
- n-gram
- features
- training
- tree
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether lexical training data coverage
  can serve as a signal for detecting hallucinations in large language models. The
  authors construct a scalable suffix array over the RedPajama corpus to retrieve
  n-gram statistics for both prompts and model generations, and evaluate their effectiveness
  for hallucination detection across three QA benchmarks.
---

# Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models

## Quick Facts
- arXiv ID: 2511.17946
- Source URL: https://arxiv.org/abs/2511.17946
- Authors: Shuo Zhang; Fabrizio Gotti; Fengran Mo; Jian-Yun Nie
- Reference count: 40
- This paper investigates whether lexical training data coverage can serve as a signal for detecting hallucinations in large language models.

## Executive Summary
This paper investigates whether lexical training data coverage can serve as a signal for detecting hallucinations in large language models. The authors construct a scalable suffix array over the RedPajama corpus to retrieve n-gram statistics for both prompts and model generations, and evaluate their effectiveness for hallucination detection across three QA benchmarks. They find that while occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty.

## Method Summary
The authors build a suffix array over the RedPajama corpus to enable O(log m) n-gram frequency queries. They extract n-grams from prompts and generations, compute raw frequency and n-gram scoring features, and combine these with log-probability features from the model. A classifier (decision tree or MLP) is trained to detect hallucinations using these features, with evaluation on TriviaQA, CoQA, and NQ-Open datasets using balanced splits and multiple seeds.

## Key Results
- Occurrence-based features are weak predictors when used alone (AUROC ≈ 0.6)
- Modest gains achieved when combining with log-probabilities (accuracy improves from 0.776 to 0.781 on TriviaQA)
- Generation-side n-gram features detect format-driven hallucinations where output probability alone fails

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prompt n-gram frequency in pretraining data correlates negatively with hallucination rate in open-domain QA.
- **Mechanism:** Higher-frequency prompt spans indicate the model has seen similar contexts during training, reducing out-of-distribution uncertainty at inference. The suffix array enables O(log m) lookup of raw occurrence counts across a 1.3T-token corpus.
- **Core assumption:** Lexical surface overlap with training data serves as a proxy for model familiarity, independent of semantic similarity.
- **Evidence anchors:**
  - [abstract] "occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities"
  - [section 5.1] "we observe a modest positive association with model faithfulness (AUROC ≈ 0.6) in open-domain QA"
  - [corpus] Related work on uncertainty estimation (semantic entropy, predictive coding) shows complementary signals improve detection; no direct corpus evidence for n-gram frequency specifically.
- **Break condition:** If prompt paraphrasing changes lexical overlap without changing semantic content, the signal degrades. High stopword ratios in prompts dilute discriminative power.

### Mechanism 2
- **Claim:** Combining occurrence features with generation log-probabilities improves classifier stability on uncertain inputs.
- **Mechanism:** Log-probabilities capture model confidence but are sensitive to training perturbations (prediction consistency = 0.00). Occurrence features add a data-aware prior that regularizes decision boundaries, yielding higher consistency (0.06) on NQ-Open.
- **Core assumption:** The two feature types capture orthogonal information—log-prob reflects internal model state, occurrence reflects external data distribution.
- **Evidence anchors:**
  - [abstract] "Classifier accuracy improves from 0.776 to 0.781 on TriviaQA and from 0.546 to 0.588 on NQ-Open"
  - [section 6.1] "the log-probability-only model shows zero consistency (0.00)...whereas the full-feature model achieves a higher consistency of 0.06"
  - [corpus] Predictive Coding and Information Bottleneck (arXiv:2601.15652) similarly combines internal signals with external constraints.
- **Break condition:** When log-probability is already highly predictive (AUROC >> 0.6), marginal gains from occurrence features diminish. Overfitting on small datasets if tree depth exceeds information content.

### Mechanism 3
- **Claim:** Generation-side occurrence features detect format-driven hallucinations where output probability alone fails.
- **Mechanism:** Low 2-gram occurrence scores cluster around degenerate outputs (e.g., model echoing "Q:" instead of answering). Decision trees split on generation 2-gram score thresholds (-1.73 to -1.39) to isolate these cases.
- **Core assumption:** Memorized or templated error patterns exhibit distinct n-gram frequency signatures in training data.
- **Evidence anchors:**
  - [section 6.1] "This split captures outputs with potentially high likelihood but low lexical diversity, particularly sequences that reproduce input-style prompts"
  - [Table 3] Concrete examples where model outputs "Q:" instead of answers, all falling within narrow 2-gram score range
  - [corpus] Corpus evidence is weak; no related work directly addresses n-gram signatures of format failures.
- **Break condition:** If model generates fluent but factually incorrect answers with normal n-gram distributions, this mechanism provides no signal.

## Foundational Learning

- **Concept: Suffix Arrays**
  - Why needed here: Enables sublinear-time n-gram frequency queries over trillion-token corpora without loading data into memory.
  - Quick check question: Explain why suffix arrays support O(log m) substring search while hash-based methods would require O(n) space for all n-grams.

- **Concept: AUROC (Area Under ROC Curve)**
  - Why needed here: Evaluates standalone feature predictive power independent of classification thresholds.
  - Quick check question: If a feature has AUROC = 0.5, what does that imply about its relationship to the target label?

- **Concept: Log-Probability as Confidence Signal**
  - Why needed here: Baseline hallucination detector; the paper measures improvements relative to this intrinsic signal.
  - Quick check question: Compute average generation log-probability given token log-probs [-0.5, -1.2, -0.8] for a 3-token output.

## Architecture Onboarding

- **Component map:** Tokenized corpus + EOS -> Suffix array (SA-IS) -> Python feature extractor -> N-gram queries -> Feature vector -> Classifier (DecisionTree/MLP) -> Hallucination probability

- **Critical path:**
  1. Tokenize corpus with model's tokenizer, flatten with EOS separators
  2. Build suffix array (linear time, one-time cost)
  3. At inference: extract n-grams from prompt/generation, query counts, compute average scores
  4. Combine with log-probabilities in classifier

- **Design tradeoffs:**
  - n-gram length: Longer n (4-5) captures specificity but increases sparsity (44% of 5-grams missing); optimal n=3-4
  - Raw frequency vs. n-gram scoring model: Raw frequency simpler; scoring model (Eq. 4) normalizes by prefix but adds complexity
  - Stopword filtering: Improves short n-gram AUROC slightly but effect is limited

- **Failure signatures:**
  - AUROC near 0.5: occurrence features providing no signal (expected on extractive QA like CoQA)
  - Large train-test accuracy gap: tree depth too high (overfitting)
  - Missing n-gram queries returning zero: normal for 4+ grams; use smoothing or fallback to shorter n

- **First 3 experiments:**
  1. Replicate AUROC analysis on TriviaQA subset: compute prompt 3-gram scores, plot against hallucination labels, verify t-test significance
  2. Train depth-3 decision tree on NQ-Open with log-prob only vs. full features; confirm 0.55→0.73 accuracy jump
  3. Inspect decision tree splits on generation 2-gram scores; identify format-driven hallucination clusters manually

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do lexical coverage features provide similar hallucination detection benefits for larger, proprietary LLMs (e.g., GPT-4, Claude) with different pretraining and alignment procedures?
- Basis in paper: [explicit] The authors state in Limitations: "our findings are based on experiments with the RedPajama-INCITE models, and may not fully generalize to larger models with more sophisticated pretraining and post-training alignments."
- Why unresolved: Resource constraints limited experiments to RedPajama-3B/7B; proprietary model training corpora are inaccessible.
- What evidence would resolve it: Replicating the methodology on an open larger model (e.g., LLaMA-70B) with accessible training data, or partnerships providing controlled access to proprietary training corpora.

### Open Question 2
- Question: Would semantic-aware coverage features (capturing paraphrasing, synonymy, or embedding similarity) substantially improve detection performance over exact n-gram matching?
- Basis in paper: [explicit] The authors note their analysis "is limited to exact n-gram frequency counts and does not account for paraphrasing, synonymy, or semantic similarity."
- Why unresolved: The suffix array infrastructure only supports exact substring queries; semantic matching requires different indexing approaches.
- What evidence would resolve it: Constructing a semantic nearest-neighbor index over the training corpus and comparing hallucination detection AUROC against exact n-gram features.

### Open Question 3
- Question: Is there a causal relationship between training data exposure and hallucination behavior, or is the observed correlation mediated by confounding factors?
- Basis in paper: [explicit] The authors state: "our results are correlational and do not establish a causal relationship between training data exposure and hallucination behavior."
- Why unresolved: Observational study design cannot isolate causation; entity frequency may correlate with other factors affecting model confidence.
- What evidence would resolve it: Interventional experiments where specific knowledge is systematically added/removed from training data while controlling for other variables, measuring resulting hallucination rate changes.

## Limitations
- Occurrence-based features are weak predictors when used alone (AUROC ≈ 0.6)
- Reliance on exact lexical matching misses hallucinations using semantically valid but factually incorrect language
- Study only evaluates on three QA benchmarks, limiting generalizability to other generation tasks

## Confidence
- **High Confidence (4/5):** Suffix array construction methodology and n-gram query implementation are well-specified and reproducible
- **Medium Confidence (3/5):** Observed AUROC improvements (0.5→0.6) and accuracy gains are statistically present but practically modest
- **Low Confidence (2/5):** Mechanism explaining why prompt n-gram frequency correlates with hallucination rates lacks direct corpus evidence

## Next Checks
1. **Semantic Robustness Test:** Evaluate whether paraphrased prompts with identical meaning but different lexical coverage show the same hallucination detection performance
2. **Cross-Task Generalization:** Apply the same lexical coverage features to summarization or story generation tasks where hallucinations manifest differently
3. **Ablation on n-gram Smoothing:** Systematically vary the ε smoothing parameter and observe effects on classifier performance