---
ver: rpa2
title: Variance-Dependent Regret Lower Bounds for Contextual Bandits
arxiv_id: '2503.12020'
source_url: https://arxiv.org/abs/2503.12020
tags:
- variance
- lower
- regret
- bound
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes the first variance-dependent regret lower\
  \ bounds for linear contextual bandits with general variance sequences. The authors\
  \ prove that any algorithm must incur regret of at least \u03A9(d\u221A(\u03A3k\
  \ \u03C3\xB2k/ log K)) for prefixed variance sequences and \u03A9(d\u221A(\u03A3\
  k \u03C3\xB2k/ log\u2076(dK))) for adaptive sequences with weak adversaries."
---

# Variance-Dependent Regret Lower Bounds for Contextual Bandits

## Quick Facts
- arXiv ID: 2503.12020
- Source URL: https://arxiv.org/abs/2503.12020
- Reference count: 4
- Establishes first variance-dependent regret lower bounds for linear contextual bandits with general variance sequences

## Executive Summary
This paper establishes the first variance-dependent regret lower bounds for linear contextual bandits with general variance sequences. The authors prove that any algorithm must incur regret of at least Ω(d√(Σk σ²k/ log K)) for prefixed variance sequences and Ω(d√(Σk σ²k/ log⁶(dK))) for adaptive sequences with weak adversaries. These bounds match existing upper bounds up to logarithmic factors, demonstrating their optimality. The key technical contribution is a novel peeling technique that groups rounds by variance magnitude and constructs separate hard instances for each group.

## Method Summary
The paper uses a peeling technique to partition rounds into groups based on variance magnitude, constructing separate hard instances for each group. For prefixed sequences, it proves a lower bound of Ω(d√(Σσ²k/log K)). For adaptive sequences with weak adversaries, it employs multiple independent instances per group with cyclic visiting to boost success probability, achieving Ω(d√(Σσ²k/log⁶(dK))). The orthogonal decision set construction isolates regret across variance groups. The paper also proves a strong impossibility result: when a strong adversary can select variances after observing decision sets, meaningful variance-dependent lower bounds cannot be established.

## Key Results
- Establishes Ω(d√(Σσ²k/log K)) lower bound for prefixed variance sequences
- Establishes Ω(d√(Σσ²k/log⁶(dK))) lower bound for adaptive sequences with weak adversaries
- Proves impossibility of meaningful variance-dependent bounds under strong adversaries
- Introduces novel peeling technique for handling general variance sequences
- Introduces multi-instance cyclic visiting for adaptive settings

## Why This Works (Mechanism)

### Mechanism 1: Peeling Technique
- **Claim:** Enables analysis of arbitrary variance sequences by reducing them to manageable groups with fixed variance thresholds
- **Mechanism:** The Peeling Technique partitions K rounds into L = ⌈log₂K⌉ + 1 groups (K_i) based on variance magnitude (e.g., rounds where 2^(i-1)/K < σ_k ≤ 2^i/K). Instead of analyzing the complex total sequence, the proof constructs a separate bandit sub-instance for each group with a fixed variance threshold σ(i).
- **Core assumption:** Rounds can be categorized such that variance within a group is bounded by the group's upper threshold, allowing application of simpler fixed-variance lower bounds to each group independently.
- **Break condition:** If K is insufficient to populate enough groups to satisfy the d² round requirement per group, the statistical power to distinguish instances vanishes, breaking the lower bound.

### Mechanism 2: Orthogonal Decision Set Construction
- **Claim:** Allows regret incurred in different variance groups to be summed additively without interference
- **Mechanism:** Creates sub-instances in orthogonal vector spaces. For a round in group i, the decision set consists of vectors padded with zeros in dimensions reserved for other groups (j ≠ i). Consequently, an action taken in group i provides no information about the weight parameters of group j, effectively isolating the regret and learning processes.
- **Core assumption:** The learner cannot infer cross-group information from the reward signal of a single group action.
- **Break condition:** If decision sets were not strictly orthogonal (e.g., overlapping actions), the learner could gain "free" information about one variance group while exploring another, potentially reducing total regret below the theoretical bound.

### Mechanism 3: Multi-Instance Cyclic Visiting
- **Claim:** Converts expected regret lower bounds into high-probability bounds, necessary for handling adaptive adversaries
- **Mechanism:** Creates Ω(log(dK)) independent instances for each variance group. The learner is assigned to these instances cyclically. If an algorithm has low regret, it must succeed on all independent instances. Since the probability of succeeding on all independent instances decreases exponentially, the lower bound holds with high probability.
- **Core assumption:** The adversary cannot predict which specific independent instance the learner will face next in the cycle.
- **Break condition:** If the sequence is not adaptive (prefixed), this mechanism is unnecessarily complex; a single instance suffices. If the cycle length exceeds the number of rounds, instances may never be visited.

## Foundational Learning

- **Concept: Heteroscedastic Noise (σ²k)**
  - **Why needed here:** Unlike standard bandits with constant noise σ, this paper's bounds depend explicitly on the sum of variances Σσ²k. Understanding that noise levels change per round is critical to grasping why standard √K bounds are insufficient.
  - **Quick check question:** If σk = 0 for all rounds, what is the regret? (Answer: 0, highlighting the variance dependence).

- **Concept: The Adversary Hierarchy (Weak vs. Strong)**
  - **Why needed here:** The paper proves a lower bound for "Weak Adversaries" (variance picked before decision set) and an impossibility result for "Strong Adversaries" (variance picked after decision set). Distinguishing the order of operations is essential.
  - **Quick check question:** Can a Strong Adversary set σk=0 specifically when the learner explores a new direction, effectively "helping" the learner achieve O(d) regret?

- **Concept: Regret Lower Bounds (Ω(·))**
  - **Why needed here:** This defines the fundamental limit of any algorithm. You must understand that this is not about a specific algorithm failing, but about the inherent difficulty of the problem class defined by the variances.
  - **Quick check question:** Does a lower bound of Ω(f(x)) mean a specific algorithm will achieve f(x), or that no algorithm can do better than f(x)?

## Architecture Onboarding

- **Component map:** Variance Source -> Peeling Layer -> Instance Factory -> Scheduler
- **Critical path:** The proof relies on the construction of the Hard Instance Class. An engineer verifying this must implement the "scaled Bernoulli distribution" with specific parameters (Δ = 1/√(96K)) to ensure the lower bound holds statistically.
- **Design tradeoffs:**
  - **Prefixed vs. Adaptive:** Prefixed settings yield tighter bounds (log K factor) but require upfront knowledge. Adaptive settings allow online variance selection but suffer looser bounds (log⁶(dK) factor) due to overhead of multi-instance boosting.
  - **Strong Adversary:** Granting the adversary power to see the decision set Dk breaks the mechanism entirely (Regret drops to O(d)), proving that meaningful variance-dependent lower bounds are impossible in that specific setting.
- **Failure signatures:**
  - **Strong Adversary Exploit:** If the adversary sets σk=0 exactly when the learner picks an action orthogonal to the history (exploration), the learner learns the model instantly with zero cost.
  - **Zero-Variance Prefixing:** In stochastic bandits with fixed decision sets, if the first d rounds have σk=0, the learner identifies the optimal arm immediately, making the Ω(d√(Σ)) bound invalid.
- **First 3 experiments:**
  1. **Baseline Verification:** Implement the "Prefixed" setting with homogeneous variance sequence (σk = c). Verify if regret scales as Ω(d√K).
  2. **Peeling Stress Test:** Construct a "spike" variance sequence where variance is high only in specific log-groups (e.g., rounds K/2 to K). Check if the regret lower bound reflects the sum of variances only in those groups.
  3. **Strong Adversary Validation:** Simulate the counter-example described in Theorem 5.4. Have the adversary observe the decision set and set σk=0 if the action is exploratory. Confirm regret is bounded by O(d) despite high total variance Σσ²k.

## Open Questions the Paper Calls Out

- **Can variance-dependent regret lower bounds for general variance sequences be extended to contextual bandits with general function approximation?**
  - **Basis in paper:** [explicit] The conclusion states the authors "leave for future work the generalization of our analysis of general variance sequence to contextual bandits with general function approximation."
  - **Why unresolved:** The current analysis relies on linear structure (e.g., orthogonal decision sets) and specific peeling techniques that do not immediately transfer to general function classes with finite eluder dimension.
  - **What evidence would resolve it:** A proof establishing a lower bound of Ω(√delu Σσ²k) for general function classes, matching the upper bounds in prior work.

- **Can the logarithmic factors in the adaptive variance setting be tightened to match the prefixed setting?**
  - **Basis in paper:** [inferred] Theorem 5.2 establishes a lower bound scaling with 1/log⁶(dK) for adaptive sequences, which is significantly looser than the 1/log K dependence achieved for prefixed sequences in Theorem 4.1.
  - **Why unresolved:** The adaptive proof requires maintaining multiple instances to handle unknown group sizes and boosting success probabilities, introducing substantial logarithmic overhead.
  - **What evidence would resolve it:** A refined analysis or construction for adaptive sequences that achieves a lower bound of Ω(d√(Σσ²k)/log K).

- **Under what conditions can meaningful variance-dependent lower bounds be established for stochastic linear bandits with fixed decision sets?**
  - **Basis in paper:** [explicit] Remark 4.5 notes that the construction "does not extend to stochastic linear bandit problems" and suggests it is "impossible to establish a lower bound" for general prefixed sequences in this setting.
  - **Why unresolved:** With a fixed decision set, if early rounds have zero variance, the learner can identify the optimal arm immediately, breaking the dependence on total cumulative variance.
  - **What evidence would resolve it:** A lower bound construction for fixed decision sets that holds under restricted variance sequences (e.g., strictly positive variance) or specific adversary constraints.

## Limitations
- Requires sufficient total variance (Σσ²k ≥ Ω(d²)) for the lower bound to hold
- Impossibility result for strong adversaries depends on specific counter-example construction
- Some constants in the asymptotic bounds are not explicitly stated

## Confidence
- Prefixed variance lower bounds (Ω(d√(Σσ²k)/log K)): High
- Adaptive variance lower bounds (Ω(d√(Σσ²k)/log⁶(dK))): High
- Impossibility result for strong adversaries: Medium
- Orthogonal decision set construction: Medium
- Multi-instance boosting mechanism: Medium

## Next Checks
1. Implement the base hard instance from Lemma 4.3 and verify the d√(Kσ²)/(16√6) regret lower bound holds with the specified scaled Bernoulli distribution and hypercube action set.
2. Construct a variance sequence with isolated high-variance groups and verify the regret lower bound decomposes additively across groups according to the peeling technique.
3. Simulate the strong adversary counter-example from Theorem 5.4 where variance is set after observing decision sets, confirming regret is bounded by O(d) despite high total variance.