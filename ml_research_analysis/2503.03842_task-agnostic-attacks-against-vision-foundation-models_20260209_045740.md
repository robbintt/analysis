---
ver: rpa2
title: Task-Agnostic Attacks Against Vision Foundation Models
arxiv_id: '2503.03842'
source_url: https://arxiv.org/abs/2503.03842
tags:
- attacks
- adversarial
- vit-b
- attack
- dinov2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces task-agnostic adversarial attacks that target\
  \ the feature representation of vision foundation models rather than specific downstream\
  \ tasks. Unlike traditional task-specific attacks that optimize loss functions tied\
  \ to a particular application (e.g., classification or segmentation), these attacks\
  \ aim to maximally disrupt the model\u2019s latent features, making them effective\
  \ across multiple downstream tasks."
---

# Task-Agnostic Attacks Against Vision Foundation Models

## Quick Facts
- arXiv ID: 2503.03842
- Source URL: https://arxiv.org/abs/2503.03842
- Reference count: 40
- Primary result: Task-agnostic adversarial attacks achieve 92% relative efficiency compared to task-specific attacks across 18 vision foundation models

## Executive Summary
This paper introduces task-agnostic adversarial attacks that target the feature representation of vision foundation models rather than specific downstream tasks. Unlike traditional task-specific attacks that optimize loss functions tied to a particular application, these attacks aim to maximally disrupt the model's latent features, making them effective across multiple downstream tasks. The method uses Projected Gradient Descent in the feature space with cosine similarity loss to generate adversarial examples. Experiments show that task-agnostic attacks achieve comparable or even superior performance to task-specific attacks in terms of classification accuracy and segmentation mIoU drops, with 92% relative efficiency. They also transfer well across different tasks including image retrieval, zero-shot classification, captioning, and visual question answering.

## Method Summary
The attack method generates adversarial examples by maximizing feature disruption in the vision foundation model's latent space using cosine similarity loss. The approach extracts features from the VFM, applies mean-centering, and uses PGD to minimize the cosine similarity between original and adversarial features under an L∞ distortion constraint. The attack can target either the CLS token or patch tokens from the vision transformer, with patch token attacks showing superior cross-task effectiveness. The method achieves task-agnostic transfer by disrupting the shared feature representation that all downstream tasks depend on, rather than optimizing for a specific task loss.

## Key Results
- Task-agnostic attacks achieve 92% relative efficiency compared to task-specific attacks across 18 vision foundation models
- Patch token targeting achieves near-equivalent effectiveness to task-specific attacks (100% vs 92% relative efficiency for classification)
- Attacks transfer effectively across multiple downstream tasks (classification, segmentation, retrieval, VQA, captioning)
- Fine-tuning via LoRA adapters does not provide meaningful defense against task-agnostic attacks

## Why This Works (Mechanism)

### Mechanism 1: Feature Space Disruption via Cosine Similarity Minimization
Minimizing cosine similarity between original and adversarial features in the VFM latent space creates perturbations that transfer across downstream tasks. The attack extracts features z = f_ϕ(x), applies mean-centering, and uses PGD to minimize L_z(z; z_o) = cos_sim(z̃, z̃_o) under an L∞ constraint, pushing the adversarial representation away from the original in the shared feature space.

### Mechanism 2: Mean-Centered Feature Representation
Pre-processing features by subtracting their empirical mean is necessary for cosine similarity to meaningfully measure feature disruption. VFM features are not centered at the origin, so the attack computes μ from training samples and centers features as z̃ = z - μ before computing the loss, ensuring cosine similarity reflects directional change rather than magnitude bias.

### Mechanism 3: Patch Token Targeting for Maximum Disruption
Attacking patch tokens rather than the CLS token yields higher cross-task effectiveness. ViTs output both a CLS token (global) and patch tokens (spatial). The paper evaluates attacking each separately and finds patch token attacks achieve near-TSA effectiveness because downstream tasks like segmentation rely on spatial information.

## Foundational Learning

- Concept: Vision Foundation Models and Self-Supervised Learning
  - Why needed here: TAAs exploit the fact that VFMs create shared representations across tasks; understanding this architecture is essential to understanding attack transferability.
  - Quick check question: Can you explain why a VFM trained with masked image modeling or self-distillation can be repurposed for both classification and segmentation without task-specific pre-training?

- Concept: Adversarial Attacks and Projected Gradient Descent (PGD)
  - Why needed here: The attack builds on standard adversarial optimization techniques; familiarity with distortion budgets (PSNR, L∞) and gradient-based attacks is assumed.
  - Quick check question: Can you describe how PGD iteratively updates an adversarial example while projecting back onto an L∞ constraint ball?

- Concept: Vision Transformer (ViT) Token Architecture
  - Why needed here: Understanding the distinction between CLS tokens and patch tokens is critical for implementing attack variants.
  - Quick check question: For a 224×224 image with patch size 14×14, how many patch tokens does a ViT produce, and what information does the CLS token aggregate?

## Architecture Onboarding

- Component map: Source VFM (f_ϕ) -> Mean computation module -> Attack optimizer (PGD) -> Loss function (cosine similarity) -> Downstream task heads (g_θ) -> Evaluation pipeline
- Critical path: 1) Load pre-trained VFM; compute μ from training set via forward passes (stop-gradient enabled). 2) For each input x_o: extract z_o = f_ϕ(x_o), compute z̃_o = z_o - μ. 3) Initialize x_a = x_o; for t=1 to N: compute z̃_a = f_ϕ(x_a) - μ, evaluate L_z, compute ∇_x_a L_z, update x_a ← x_a - α·∇_x_a L_z, project onto L∞ ball. 4) Clip and quantize x_a to valid image range; save as PNG. 5) Evaluate downstream task performance on x_a.
- Design tradeoffs: CLS-only faster but less effective; patch tokens more compute-intensive but achieve ~97% relative efficiency vs TSA. PSNR=40dB (ϵ_∞≈8/255) balances attack strength with perceptual quality; lower PSNR improves attack but increases visibility. Smaller models (ViT-S) more vulnerable; larger models (ViT-G) show some robustness gains, especially for retrieval. White-box attacks highly effective; transferability across architectures remains limited.
- Failure signatures: Low relative efficiency (<50%) indicates attack not propagating to downstream tasks; check feature centering or token selection. High task-specific performance but poor cross-task transfer may be inadvertently optimizing task-specific loss rather than feature disruption. Transfer attack near-random performance suggests large architectural differences between source and target models; consider same-family models. LoRA fine-tuned models still vulnerable - expected behavior.
- First 3 experiments: 1) Replicate Table 4 on DiNOv2 ViT-S: Compare CLS-only, patch-only, and combined TAA variants against TSA baselines for classification and segmentation on PascalVOC. 2) Cross-task validation: Attack CLIP ViT-B/16 with TAA; evaluate zero-shot classification on ImageNetV2 and image retrieval on R-Oxford to verify multi-task transfer. 3) Transferability test: Generate adversarial examples on DiNOv2 ViT-S (white-box); evaluate on DiNOv2 ViT-B and ViT-L (black-box) to quantify cross-model transfer.

## Open Questions the Paper Calls Out
None

## Limitations
- Cross-model transferability remains limited - adversarial examples often fail to transfer between different model architectures
- Task-agnostic assumption may not hold universally - some tasks may use specialized feature subspaces or have inherent robustness mechanisms
- LoRA fine-tuning defense ineffectiveness - adapter-based fine-tuning may not be sufficient for adversarial robustness

## Confidence
- High confidence: Core mechanism of using cosine similarity in feature space for task-agnostic attacks is well-supported by experimental evidence across 18 VFMs and multiple downstream tasks.
- Medium confidence: Claims about patch token targeting being superior to CLS token attacks have strong empirical support but limited theoretical justification.
- Low confidence: Transferability analysis between different model families shows inconsistent results with high variance.

## Next Checks
1. Cross-architecture transferability benchmarking - Systematically evaluate TAA transfer from source models to target models across the same architecture family versus different families to quantify architectural transfer barriers.
2. Feature subspace analysis - Conduct ablation studies to determine whether TAAs affect all downstream tasks equally or if certain tasks show differential vulnerability.
3. Defense mechanism evaluation - Test whether more substantial architectural modifications (not just LoRA) like adversarial training on the foundation model itself or feature denoising layers can provide meaningful defense against TAAs.