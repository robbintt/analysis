---
ver: rpa2
title: Adapting World Models with Latent-State Dynamics Residuals
arxiv_id: '2504.02252'
source_url: https://arxiv.org/abs/2504.02252
tags:
- dynamics
- real
- world
- environment
- redraw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReDRAW, a method for adapting world models
  from simulation to real-world environments by learning residual corrections in latent
  state space. Unlike traditional approaches that require high-dimensional state residuals
  or extensive real-world data, ReDRAW trains a small residual network on top of a
  frozen pretrained world model to correct dynamics predictions in latent space.
---

# Adapting World Models with Latent-State Dynamics Residuals

## Quick Facts
- arXiv ID: 2504.02252
- Source URL: https://arxiv.org/abs/2504.02252
- Authors: JB Lanier; Kyungmin Kim; Armin Karamzade; Yifei Liu; Ankita Sinha; Kat He; Davide Corsi; Roy Fox
- Reference count: 19
- This paper introduces ReDRAW, a method for adapting world models from simulation to real-world environments by learning residual corrections in latent state space.

## Executive Summary
This paper introduces ReDRAW, a method for adapting world models from simulation to real-world environments by learning residual corrections in latent state space. Unlike traditional approaches that require high-dimensional state residuals or extensive real-world data, ReDRAW trains a small residual network on top of a frozen pretrained world model to correct dynamics predictions in latent space. The method is evaluated on vision-based DeepMind Control Suite environments and a physical Duckiebot lane-following task. ReDRAW consistently outperforms standard finetuning baselines, achieving higher returns while avoiding overfitting in low-data regimes. In real robot experiments, ReDRAW successfully adapts from simulation to reality using only 10K real steps without reward labels, demonstrating robust sim-to-real transfer. The approach combines well with visual adaptation techniques and enables effective deployment in complex, unseen dynamics.

## Method Summary
ReDRAW addresses sim-to-real transfer by learning residual corrections in latent space rather than requiring high-dimensional state residuals or extensive real-world data. The method freezes a pretrained DRAW world model and trains a small residual network to correct dynamics predictions in latent space. The residual network operates on discrete latent states and actions, avoiding the need for continuous state inputs that could lead to overfitting. The approach is evaluated on vision-based DeepMind Control Suite environments with physics modifications and a physical Duckiebot lane-following task, demonstrating superior performance compared to finetuning baselines while using minimal real-world data.

## Key Results
- ReDRAW outperforms finetuning baselines on 4 DeepMind Control Suite environments with physics modifications
- Achieves successful sim-to-real transfer on Duckiebot lane-following task using only 10K real steps without reward labels
- Maintains stable high performance over 3M+ updates while finetuning baselines degrade from overfitting
- Effectively combines with visual adaptation techniques for comprehensive domain adaptation

## Why This Works (Mechanism)
The key insight is that learning residuals in latent space is more sample-efficient than finetuning the entire world model. By freezing the pretrained DRAW model and only adapting a small residual network, ReDRAW prevents catastrophic forgetting while still correcting for domain shifts. The discrete latent representation provides a stable foundation for residual learning, and the KL divergence training objective ensures the corrected dynamics remain consistent with the learned latent space structure. This approach is particularly effective in low-data regimes where traditional finetuning would overfit.

## Foundational Learning
- **DRAW World Models**: Discrete latent state representations for sequential data prediction. Needed for efficient latent-space operations and preventing continuous-space overfitting.
- **Residual Learning**: Small network that learns corrections to existing predictions rather than full function approximation. Needed to avoid overfitting in low-data regimes.
- **KL Divergence Training**: Objective that measures distributional differences between predicted and target latent states. Needed to ensure corrected dynamics remain consistent with learned latent space.
- **Sim-to-Real Transfer**: Adapting models trained in simulation to work in real-world environments. Needed for practical robotics applications where real-world data collection is expensive.
- **Offline Reinforcement Learning**: Learning from pre-collected datasets without environment interaction. Needed for safe and efficient adaptation using existing data.

## Architecture Onboarding

**Component Map**: Pretrained DRAW (discrete latent z_t, dynamics f_θ, encoder/decoder) -> Residual MLP δ_ψ(z_{t-1}, a_{t-1}) -> Actor-Critic (imagined rollouts under rectified dynamics)

**Critical Path**: 
1. Pretrain DRAW in simulation using Plan2Explore for diverse data collection
2. Collect target-domain offline transitions without reward labels
3. Freeze DRAW weights and train residual network via KL divergence
4. Train actor-critic using imagination with rectified dynamics

**Design Tradeoffs**: 
- Discrete vs continuous latent states: Discrete prevents overfitting but may limit expressiveness
- Residual vs full finetuning: Residual prevents catastrophic forgetting but may not capture large dynamics shifts
- Offline vs online adaptation: Offline is safer but may miss important real-world variations

**Failure Signatures**: 
- Overfitting during adaptation: Finetuning baselines show initial improvement then degradation
- Insufficient source diversity: Poor transfer even with extensive target data
- Residual complexity too high: Conditioning on continuous inputs degrades performance

**First Experiments**:
1. Verify DRAW pretraining produces diverse latent representations by checking reconstruction quality and latent space visualization
2. Test residual input conditioning by comparing δ_ψ(z_{t-1}, a_{t-1}) against variants with continuous state inputs
3. Monitor validation return trajectories to distinguish between ReDRAW's stable performance and finetuning's overfitting pattern

## Open Questions the Paper Calls Out
- **POMDP Extension**: Can ReDRAW be extended to handle partially observable Markov decision processes effectively? The current DRAW architecture assumes full observability and lacks recurrent mechanisms for history aggregation.
- **Foundation Models**: Can latent residual adaptation scale to large-scale foundation world models to specialize them for specific environments? It's unclear if the low-capacity residual approach is sufficient for vast, diverse latent spaces.
- **Dynamics Complexity**: How does the complexity of the dynamics shift limit ReDRAW's ability to model changes with low amounts of data? Simple changes may be effectively modeled, but drastic or chaotic physics discrepancies may require more capacity.

## Limitations
- Architectural details remain underspecified (CNN depths, layer configurations, exact physics modification parameters)
- Implementation dependencies on specific versions of DeepMind Control Suite and Albumentations library
- Limited ablation studies on residual architecture complexity and training stability
- Real robot experiments conducted with only one Duckiebot platform, raising questions about generalization

## Confidence
- **High confidence**: Core methodology (latent-space residual correction), main empirical claims (outperformance vs. finetuning baselines, low-data regime effectiveness)
- **Medium confidence**: Visual adaptation combination claims (integration with unspecified visual techniques), quantitative comparisons requiring exact environment configurations
- **Low confidence**: Generalization claims beyond tested domains, long-term stability assertions

## Next Checks
1. Replicate DMC physics modifications exactly (wind magnitude on Cup-Catch, torque values on Finger-Turn) to verify reported performance numbers
2. Test residual input conditioning sensitivity by comparing δ_ψ(z_{t-1}, a_{t-1}) against variants including continuous state inputs
3. Verify low-data regime advantage by training finetuning baseline with identical data budgets and monitoring validation return trajectories for overfitting patterns