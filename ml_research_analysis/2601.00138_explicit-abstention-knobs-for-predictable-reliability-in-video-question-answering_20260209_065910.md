---
ver: rpa2
title: Explicit Abstention Knobs for Predictable Reliability in Video Question Answering
arxiv_id: '2601.00138'
source_url: https://arxiv.org/abs/2601.00138
tags:
- confidence
- evidence
- frames
- coverage
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates whether confidence-based abstention provides
  reliable control over error rates in video question answering under distribution
  shift. Using NExT-QA and Gemini 2.0 Flash, the study finds that confidence thresholding
  does provide smooth, mechanistic control in-distribution, reducing error rates from
  23.6% to 9.4% at 63.7% coverage with well-calibrated predictions (ECE = 0.018).
---

# Explicit Abstention Knobs for Predictable Reliability in Video Question Answering

## Quick Facts
- arXiv ID: 2601.00138
- Source URL: https://arxiv.org/abs/2601.00138
- Authors: Jorge Ortiz
- Reference count: 13
- Primary result: Confidence thresholding provides smooth control in-distribution but fails to track evidence sufficiency under visual degradation

## Executive Summary
This paper evaluates whether confidence-based abstention provides reliable control over error rates in video question answering under distribution shift. Using NExT-QA and Gemini 2.0 Flash, the study finds that confidence thresholding does provide smooth, mechanistic control in-distribution, reducing error rates from 23.6% to 9.4% at 63.7% coverage with well-calibrated predictions (ECE = 0.018). However, this control is not epistemic. When visual evidence is degraded from 18 to 6 frames, median confidence remains 0.9 despite a 3× reduction in information. The model does not "know when it does not know" under shift. This overconfidence problem is fundamental to the model's representations, not an artifact of the self-reporting interface. The results motivate warrant-based selective prediction, where confidence is explicitly bounded by what available evidence can support.

## Method Summary
The study evaluates Gemini 2.0 Flash on NExT-QA video question answering with confidence-based selective prediction. Using 300 stratified validation items (100 each of causal, temporal, and descriptive questions), the model answers questions with either 18 frames (baseline) or 6 frames (sparse evidence). Two prompt interfaces extract confidence: JSON for self-reported confidence with metadata, and letter-only with logprobs=True for pmax-based confidence. A systematic threshold sweep over 25 values from 0 to 1 computes coverage (fraction answered), risk (error rate among accepted), and expected calibration error (ECE). The experiment compares confidence distributions and calibration across evidence conditions to test whether confidence tracks evidence sufficiency.

## Key Results
- Sweeping threshold ε reduces error rates from 23.6% to 9.4% at 63.7% coverage in-distribution (18 frames)
- Median confidence remains 0.9 when evidence is reduced from 18 to 6 frames (67% reduction)
- Logprob-derived pmax increases slightly (0.870 → 0.876) despite evidence degradation
- The model's confidence distribution is insensitive to evidence sufficiency, indicating representational overconfidence

## Why This Works (Mechanism)

### Mechanism 1: Threshold-Gated Selective Prediction
- Claim: Sweeping confidence threshold ε produces predictable, monotone risk-coverage tradeoffs in-distribution.
- Mechanism: When confidence correlates with correctness within a fixed evidence regime, filtering low-confidence predictions removes disproportionate errors. The model's internal uncertainty functions as a proxy for error likelihood.
- Core assumption: Confidence scores are meaningfully correlated with prediction correctness under the target distribution.
- Evidence anchors:
  - [abstract] "Sweeping threshold ε produces smooth risk-coverage tradeoffs, reducing error rates from 23.6% to 9.4% at 63.7% coverage"
  - [Section 5.1] "Tightening ε from 0 to 0.71 reduces coverage from 98.7% to 63.7% while reducing risk from 23.6% to 9.4%"
  - [corpus] Weak corpus support for VLM-specific validation; related work (Kamath et al., Whitehead et al.) addresses text/QA but not video evidence degradation
- Break condition: If confidence-correction correlation degrades under distribution shift, the threshold no longer maps to predictable error rates.

### Mechanism 2: Confidence Insensitivity to Evidence Completeness
- Claim: Model confidence does not contract proportionally when visual evidence is reduced.
- Mechanism: The model's confidence signal appears calibrated to task correlates rather than to evidence sufficiency. When frames drop 18→6, the information-theoretic bound on predictability changes, but the model's reported confidence distribution barely shifts.
- Core assumption: Temporal subsampling reduces task-relevant information; Fano's inequality implies an upper bound on justified confidence.
- Evidence anchors:
  - [abstract] "Median self-reported confidence remains 0.9 in both regimes despite a 3× reduction in visual information"
  - [Section 5.2.1] "A 67% reduction in evidence produces less than 6% change in confidence"
  - [corpus] No direct corpus validation; this appears to be a novel finding for VLMs
- Break condition: If a model is explicitly trained with evidence-aware supervision, confidence may become evidence-sensitive.

### Mechanism 3: Representational (Not Behavioral) Overconfidence
- Claim: The overconfidence problem persists across both self-reported and logprob-derived confidence, indicating it is representational rather than an artifact of the self-reporting interface.
- Mechanism: Self-reported confidence is a behavioral output that could diverge from internal uncertainty. Logprob-derived pmax directly measures the decoder's probability distribution over answer tokens. If overconfidence were behavioral, logprobs should show greater sensitivity to evidence loss. The opposite pattern observed suggests the model's latent representations do not encode evidence sufficiency.
- Core assumption: Logprob distributions over answer options reflect the model's internal uncertainty about the answer.
- Evidence anchors:
  - [Section 5.3.2] "Logprob-derived pmax actually increases slightly (0.870 → 0.876), while self-reported confidence at least decreases modestly"
  - [Section 5.3.2] "The model's token-level decision distribution over A–E does not become more diffuse when evidence is truncated"
  - [corpus] No corpus papers test this dual-signal comparison
- Break condition: If models are architected with explicit evidence-encoding modules, the representation may bound confidence by evidence.

## Foundational Learning

- **Concept: Risk-Coverage Tradeoff**
  - Why needed here: The entire abstention framework is built on trading coverage for risk. Without this, the threshold-sweeping results are uninterpretable.
  - Quick check question: If you raise ε from 0.5 to 0.8, what should happen to coverage and what should happen to risk among accepted predictions?

- **Concept: Expected Calibration Error (ECE)**
  - Why needed here: The paper claims predictions become "well-calibrated" at ε=0.71. You need to understand that calibration means predicted confidence matches empirical accuracy, and ECE quantifies the gap.
  - Quick check question: If a model assigns 0.9 confidence to 100 predictions and 85 are correct, is it calibrated? What is the calibration error for that bin?

- **Concept: Epistemic vs. Aleatoric Uncertainty**
  - Why needed here: The paper's central claim is that confidence provides "mechanistic" but not "epistemic" control. Understanding that epistemic uncertainty relates to knowledge/information availability vs. aleatoric (irreducible noise) is essential for the warrant-based framing.
  - Quick check question: If you observe half the frames and are uncertain, vs. observing all frames and still being uncertain because the event is ambiguous—which uncertainty type is which?

## Architecture Onboarding

- **Component map:**
  - Evidence packet constructor: Deterministic frame extraction (uniform + zoom regions), SHA256 verification
  - VLM interface: Gemini 2.0 Flash with temperature=0, two prompt modes (JSON self-report vs. letter-only logprob extraction)
  - Abstention gate: System-level threshold on confidence, independent of model's internal abstain flag
  - Evaluation sweep: 25 ε values, compute coverage/risk/ECE at each point

- **Critical path:**
  1. Frame extraction must be deterministic (same video → same frames across all conditions)
  2. JSON parsing must succeed (retry once, then treat as abstain)
  3. Confidence extraction (self-reported OR logprob-derived) must be valid
  4. Threshold sweep must exclude points with n<50 accepted predictions

- **Design tradeoffs:**
  - Self-reported confidence: Rich metadata (evidence_span, abstain flag) but may reflect instruction-following rather than uncertainty
  - Logprob-derived confidence: Direct access to token distribution but requires separate prompt interface
  - Frame count (18 vs. 6) vs. temporal ablation: Uniform subsampling is cleaner; temporal ablation guarantees semantic insufficiency for some questions

- **Failure signatures:**
  - Parse failures spike → model not following JSON schema
  - Coverage doesn't change with ε → confidence values are uniform/degenerate
  - Risk increases at high ε → confidence anti-correlated with correctness
  - Logprob pmax=1.0 for all predictions → vocabulary normalization issue

- **First 3 experiments:**
  1. **Pipeline validation (n=50):** Confirm end-to-end flow—frame extraction, API call, JSON parsing, metric computation. Target: 100% parse success, reasonable accuracy baseline.
  2. **Baseline risk-coverage sweep (18 frames, n=300):** Establish the in-distribution operating curve. Verify monotonicity, locate the "knee," confirm ECE improves at higher thresholds.
  3. **Evidence degradation comparison (6 frames, same n=300):** Rerun sweep on matched questions with sparse evidence. Compare CDFs of confidence distributions—key diagnostic is whether Pr(c≥0.9) drops proportionally to frame reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a quantitative "warrant" bound $\zeta(e)$ be derived from visual evidence to enforce confidence contraction, and what architectural or loss modifications are required?
- Basis in paper: [explicit] The authors conclude that results "motivate warrant-based selective prediction" but explicitly state "we do not estimate a warrant quantity $\zeta$... and do not enforce... the inequality."
- Why unresolved: The paper establishes the theoretical need for evidence-conditioned bounds but leaves the definition, estimation, and enforcement of $\zeta(e)$ as undefined future work.
- What evidence would resolve it: A proposed functional form for $\zeta(e)$ (e.g., based on motion density or temporal coverage) and empirical results showing confidence drops below this bound when evidence is removed.

### Open Question 2
- Question: Does fine-tuning with observability proxies (e.g., frame count, motion magnitude) successfully induce evidence-awareness in VLMs?
- Basis in paper: [explicit] Section 6.3.5 suggests fine-tuning could work "with the right supervision signal, one tied to evidence quality" via an "observability proxy."
- Why unresolved: The paper proposes this direction but does not implement it; current models fail to track observability without specific supervision.
- What evidence would resolve it: A training run where the model is penalized for high confidence on low-evidence inputs, resulting in a statistically significant correlation between confidence drop and frame reduction.

### Open Question 3
- Question: Is the insensitivity of confidence to evidence degradation a universal property of VLMs or specific to the Gemini architecture?
- Basis in paper: [inferred] Section 6.4 notes the limitation: "We evaluate a single model (Gemini 2.0 Flash)... Other VLMs may exhibit different confidence behaviors."
- Why unresolved: The study is restricted to one proprietary model; it is unknown if open-source or differently architected models exhibit similar overconfidence.
- What evidence would resolve it: Replication of the 18-frame vs. 6-frame degradation protocol on a diverse set of VLMs showing consistent or divergent failure modes.

## Limitations
- The study is limited to a single VLM (Gemini 2.0 Flash), so the overconfidence pattern may not generalize to other architectures
- The frozen item set (300 validation items) provides good experimental control but limits generalizability to other question distributions
- The warrant-based selective prediction framework is motivated but not implemented, leaving the proposed solution theoretical

## Confidence
- **High confidence**: Confidence thresholding provides smooth, mechanistic control in-distribution (23.6% → 9.4% error reduction at 63.7% coverage with ECE=0.018)
- **Medium confidence**: Overconfidence under evidence degradation is representational rather than behavioral, based on dual-signal comparison
- **Medium confidence**: Warrant-based selective prediction is motivated by empirical findings, though the proposed solution remains theoretical

## Next Checks
1. **Evidence-Aware Model Comparison**: Test a VLM with explicit evidence-encoding modules under the same evidence degradation conditions. If such a model shows evidence-sensitive confidence, it would validate that the overconfidence is architectural rather than fundamental.

2. **Temporal Ablation Validation**: Repeat the evidence degradation experiment using temporal ablation (early-half vs. late-half frames) rather than uniform subsampling. This would confirm whether the overconfidence pattern holds when the semantic content is provably insufficient for certain questions.

3. **Cross-Domain Calibration**: Apply the same threshold-sweep methodology to a different video QA dataset with different question distributions and evidence characteristics. This would test whether the calibration patterns generalize beyond NExT-QA's specific task structure.