---
ver: rpa2
title: Quantifying Return on Security Controls in LLM Systems
arxiv_id: '2512.15081'
source_url: https://arxiv.org/abs/2512.15081
tags:
- control
- loss
- injection
- language
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of quantifying return on investment
  for security controls in large language model (LLM) systems by introducing a decision-oriented
  framework that converts adversarial probe outcomes into financial risk estimates.
  The core method idea combines automated adversarial testing using Garak with Laplace's
  Rule of Succession to estimate attack success probabilities, then uses Monte Carlo
  simulation with calibrated loss distributions to produce expected loss and loss
  exceedance curves for different vulnerability classes.
---

# Quantifying Return on Security Controls in LLM Systems

## Quick Facts
- arXiv ID: 2512.15081
- Source URL: https://arxiv.org/abs/2512.15081
- Reference count: 40
- Primary result: ABAC achieves 94% risk reduction with RoC 9.83; NER achieves 57% reduction with RoC 5.97; Guardrails provides negligible benefit with RoC 0.05

## Executive Summary
This paper introduces a decision-oriented framework for quantifying return on investment for security controls in LLM-based RAG systems. The authors combine automated adversarial testing with probabilistic risk modeling to estimate financial losses from different vulnerability classes. By applying this methodology to three common controls—attribute-based access control (ABAC), named entity recognition (NER), and NeMo Guardrails—they demonstrate that upstream controls that restrict information flow can be significantly more effective than output filtering. The framework produces actionable metrics like Return on Control (RoC) that enable security teams to prioritize investments based on quantified risk reduction.

## Method Summary
The authors implement a RAG system using DeepSeek-R1-Distill-Qwen-1.5B and test it against Garak adversarial probes across five vulnerability classes. They use Laplace's Rule of Succession to estimate attack success probabilities from limited probe outcomes, then run Monte Carlo simulations combining these probabilities with calibrated loss distributions from IBM's breach report. Three controls are evaluated: ABAC (blocks unauthorized PII retrieval), NER (redacts sensitive entities before model input), and Guardrails (filters harmful outputs). The analysis produces loss exceedance curves and RoC metrics for each control scenario.

## Key Results
- ABAC reduces total expected loss by ~94%, achieving an RoC of 9.83
- NER cuts PII leakage risk with an RoC of 5.97 while leaving injection attacks unaffected
- NeMo Guardrails offers negligible benefit with RoC of 0.05 due to post-generation positioning
- Three controls together reduce total simulated expected loss from $313,000 to under $6,000

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Upstream controls that restrict information flow to the model can reduce multiple attack vectors more effectively than output filtering.
- Mechanism: Attribute-Based Access Control (ABAC) evaluates request attributes before retrieval, denying access to sensitive documents for unauthorized requests. By preventing the model from ever receiving harmful context, it attacks the root cause of PII leakage, latent injection, and prompt injection simultaneously.
- Core assumption: Attack success depends on the model having access to exploitable sensitive or harmful content in its context window.
- Evidence anchors:
  - [abstract] "ABAC collapses success probabilities for PII and prompt-related attacks to near zero and reduces the total expected loss by ~94%, achieving an RoC of 9.83."
  - [Page 5, Section III.B.2] "Attributes are evaluated before retrieval. If the request is not authorized, the model receives no PII from the document store."
  - [corpus] Corpus evidence for this mechanism is weak or missing in provided neighbors.

### Mechanism 2
- Claim: Targeted content filtering at inference can eliminate specific data leakage categories without addressing broader vulnerability surfaces.
- Mechanism: Named Entity Recognition (NER) uses a separate model to identify and redact sensitive entities from retrieved documents before they reach the LLM. This breaks the data exfiltration chain for recognized PII types.
- Core assumption: The NER model accurately identifies all instances of the target sensitive information without being bypassed.
- Evidence anchors:
  - [abstract] "NER cut PII leakage risk with an RoC of 5.97."
  - [Page 6, Table I] NER eliminated all observed PII leakage (0 failures) but latent injection and prompt injection remained at 160/160 and 500/500 respectively.
  - [corpus] Corpus evidence for this mechanism is weak or missing in provided neighbors.

### Mechanism 3
- Claim: Output-only controls provide negligible protection against prompt injection and data leakage attacks because they operate after harmful content has already been synthesized.
- Mechanism: NeMo Guardrails validates generated responses against a policy file at the final output stage. It does not prevent harmful retrieval or internal reasoning, only attempting to catch problematic content before it reaches the user.
- Core assumption: The rule-based output filter can reliably detect harmful content that has already been generated, based on its policy configuration.
- Evidence anchors:
  - [abstract] "NeMo Guardrails offered negligible benefit (RoC 0.05)."
  - [Page 7, Section IV.D] "Because it operates after retrieval and generation, the control does not prevent harmful context from being retrieved or incorporated into model reasoning."
  - [corpus] Corpus evidence for this mechanism is weak or missing in provided neighbors.

## Foundational Learning

- Concept: **Laplace's Rule of Succession**
  - Why needed here: The paper uses this rule to estimate attack success probability from limited trials, preventing zero probability estimates when no failures are observed.
  - Quick check question: If you observe 50 successful attacks out of 50 trials, what probability does Laplace's Rule estimate for the next attack? (Answer: (50+1)/(50+2) = 51/52 ≈ 0.98)

- Concept: **Monte Carlo Simulation for Risk Quantification**
  - Why needed here: The paper combines attack success probabilities with uncertain loss distributions to model financial risk and produce loss exceedance curves.
  - Quick check question: In this paper's Monte Carlo simulation, what two uncertain elements are combined in each trial to estimate financial loss? (Answer: Whether the attack succeeds [binary, based on Laplace probability] and the loss amount if it succeeds [sampled from triangle distribution])

- Concept: **Return on Control (RoC)**
  - Why needed here: This is the primary metric for comparing cost-effectiveness of different security controls, enabling data-driven investment decisions.
  - Quick check question: If a control reduces expected loss by $300,000 and costs $30,000 to implement, what is its RoC? (Answer: 300,000/30,000 = 10)

## Architecture Onboarding

- Component map:
  FastAPI server with single RAG endpoint -> DeepSeek-R1 (Distill Qwen 1.5B) via HuggingFace transformers -> FAISS vector store containing 100k synthetic PII records -> Control Points (ABAC pre-retrieval, NER post-retrieval, Guardrails post-generation) -> Garak probing tool with 5 probe families

- Critical path:
  1. User Query → ABAC Policy Check → Vector Store Retrieval → NER Redaction → Context Assembly → Model Generation → Guardrails Check → Response

- Design tradeoffs:
  - **ABAC:** Highest effectiveness (RoC 9.83) but requires comprehensive policy design and may restrict legitimate access if poorly configured
  - **NER:** Strong targeted protection (RoC 5.97) but limited scope—only addresses PII leakage, not prompt injection
  - **Guardrails:** Minimal invasiveness but negligible effectiveness (RoC 0.05) in tested configuration

- Failure signatures:
  - **Baseline pattern:** >98% attack success for PII, latent injection, prompt injection
  - **Partial mitigation:** NER eliminates PII but injection attacks persist at ~99%
  - **Ineffective control:** NeMo shows near-identical failure rates to baseline

- First 3 experiments:
  1. **Baseline Characterization:** Run Garak probes against unmodified RAG system to establish attack success rates across all 5 vulnerability classes
  2. **ABAC Integration Test:** Implement ABAC policy denying retrieval for unauthorized requests; re-run probes and measure reduction in PII/injection failures
  3. **NER Integration Test:** Add Presidio redaction to retrieval pipeline; verify PII leakage elimination while confirming injection vulnerabilities persist

## Open Questions the Paper Calls Out

- Question: How does control effectiveness (ABAC, NER, guardrails) generalize across LLMs of varying scale, architecture, and training paradigms?
  - Basis in paper: [explicit] Authors state: "the methodology should be applied to a broader array of LLMs...yielding insights into how vulnerability profiles differ and which mitigations generalize effectively."
  - Why unresolved: Only DeepSeek-R1 was tested; results may not transfer to proprietary models or different architectures.
  - What evidence would resolve it: Replicating the methodology across GPT-4, Claude, Llama, and other models with identical probe sets and controls.

- Question: Can NeMo Guardrails achieve meaningful risk reduction with alternative configurations, broader policy coverage, or deeper pipeline integration?
  - Basis in paper: [explicit] Authors note: "Further tuning, broader policy coverage, or deeper integration earlier in the RAG pipeline may be necessary for the control to produce meaningful reductions."
  - Why unresolved: Only one configuration was tested, producing negligible RoC (0.05); the Colang scripting language remains underexplored.
  - What evidence would resolve it: Systematic evaluation of NeMo with varied policy files, integrated at retrieval or preprocessing stages rather than only at output.

- Question: How sensitive are RoC rankings to the assumption of uniform control implementation costs ($30,000 across all controls)?
  - Basis in paper: [inferred] The paper explicitly assigns identical costs to enable comparison, but notes this "controlled assumption avoids introducing cost variability." Real-world ABAC, NER, and guardrails deployments likely have divergent costs.
  - Why unresolved: RoC values (9.83, 5.97, 0.05) conflate effectiveness with cost; relative rankings could invert under realistic cost structures.
  - What evidence would resolve it: Empirical measurement of actual deployment and operational costs for each control in production environments.

- Question: How can the vulnerability testing framework incorporate statistical significance testing and reproducible benchmarking across runs?
  - Basis in paper: [explicit] Authors state: "Garak offers broad coverage and insightful qualitative signals, it does not natively support reproducible benchmarking or statistical significance testing across runs."
  - Why unresolved: Current methodology reports single-run outcomes; variability across runs could affect Laplace probability estimates and downstream loss simulations.
  - What evidence would resolve it: Multi-run experiments with confidence intervals on attack success rates and propagated uncertainty in loss exceedance curves.

## Limitations

- The study relies on synthetic data and controlled probe scenarios that may not capture real-world attack complexity and variability
- The uniform $30,000 cost assumption oversimplifies true implementation costs, which vary significantly based on organizational context
- The analysis assumes independence between attack vectors in Monte Carlo simulation, potentially missing correlated or cascading attack patterns

## Confidence

- **High Confidence:** The core methodology of using Laplace's Rule to estimate attack probabilities from limited trials, and the fundamental insight that upstream controls (ABAC) can be more effective than downstream filtering (Guardrails) by addressing root causes rather than symptoms.
- **Medium Confidence:** The specific quantitative results (exact RoC values of 9.83, 5.97, and 0.05) and the assumption that these controls will perform similarly in production environments with different data distributions and attack patterns.
- **Low Confidence:** The generalizability of the findings to other LLM architectures, different vulnerability classes beyond those tested, and the assumption that the synthetic PII corpus adequately represents real sensitive data patterns that attackers would target.

## Next Checks

1. **Real-World Deployment Test:** Deploy the ABAC, NER, and Guardrails controls in a production RAG system handling live user queries for at least 30 days, comparing actual security incidents and operational overhead against the simulated estimates.

2. **Cost Calibration Study:** Conduct a detailed analysis of implementation costs across different organizational contexts (startup vs enterprise, different integration approaches) to validate the $30,000 uniform cost assumption and develop more granular cost models.

3. **Attacker Adaptation Simulation:** Design a second round of adversarial testing that specifically targets the controls themselves (e.g., crafting prompts that bypass ABAC authorization logic or evade NER detection patterns) to assess whether attackers can reduce the reported RoC values through adaptive strategies.