---
ver: rpa2
title: 'GenTREC: The First Test Collection Generated by Large Language Models for
  Evaluating Information Retrieval Systems'
arxiv_id: '2501.02408'
source_url: https://arxiv.org/abs/2501.02408
tags:
- documents
- collections
- gentrec
- test
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GenTREC, the first test collection generated
  entirely by a large language model (LLM), eliminating the need for manual relevance
  judgments in information retrieval (IR) evaluation. The method generates documents
  based on the assumption that LLM-generated content is inherently relevant to its
  prompt.
---

# GenTREC: The First Test Collection Generated by Large Language Models for Evaluating Information Retrieval Systems

## Quick Facts
- arXiv ID: 2501.02408
- Source URL: https://arxiv.org/abs/2501.02408
- Reference count: 40
- Primary result: GenTREC demonstrates high correlation with traditional TREC collections for P@100, MAP, and RPrec metrics, while requiring only $126 to generate 96,196 documents and 18,964 relevance judgments.

## Executive Summary
GenTREC introduces the first test collection for information retrieval evaluation generated entirely by a large language model, eliminating the need for manual relevance judgments. The approach generates documents based on the assumption that LLM-generated content is inherently relevant to its prompt. Using existing TREC topics, the method creates relevant documents with subtopics, generates tricky non-relevant documents that share similarities with relevant ones, and adds random-topic documents to enlarge the collection. Evaluation results show that GenTREC produces system rankings highly similar to traditional TREC collections for P@100 (reaching 1.0 and 0.95 Kendall's τ with TREC6 and Robust2004 respectively), MAP, and RPrec metrics. However, rankings differ significantly for P@10. The generated documents are found to be relevant in 83% of cases, with tricky non-relevant documents achieving 94% accuracy. GenTREC contains shorter documents with lower lexical diversity but requires higher education levels for comprehension compared to human-authored documents.

## Method Summary
The GenTREC methodology generates a complete IR test collection by using GPT-3.5 to create both documents and implicit relevance judgments. The process involves three document types: relevant documents generated from 100 subtopics per topic (resulting in 36,000 relevant documents), tricky non-relevant documents created by masking keywords in topic descriptions and generating similar but non-relevant text (50 per topic), and random non-relevant documents on any topic (59,804 total). Relevance judgments are assigned implicitly based on which topic prompt generated each document, eliminating manual annotation. The approach costs $126 to generate 96,196 documents and 18,964 relevance judgments, using 300 topics from TREC-5 through Robust2004.

## Key Results
- GenTREC achieves Kendall's τ correlation of 1.0 and 0.95 with TREC6 and Robust2004 respectively for P@100 rankings
- System rankings show high correlation for MAP (0.99) and RPrec (0.95-0.99) metrics but diverge significantly for P@10 (τ ≈ 0.2)
- Generated documents demonstrate 83% relevance accuracy with manual validation, while tricky non-relevant documents achieve 94% accuracy
- The collection contains shorter documents with lower lexical diversity but requires higher education levels for comprehension compared to human-authored documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Documents generated by an LLM from a specific prompt are inherently relevant to that prompt.
- Mechanism: The LLM conditions on the prompt's topic and subtopics to produce text that is topically aligned, removing the need for post-hoc human relevance judgments.
- Core assumption: LLM adherence to prompts is sufficiently high that generated content consistently satisfies the information need embedded in the prompt.
- Evidence anchors: Manual checks show an 83% average relevance accuracy, though this varies significantly by topic (0% to 100%).
- Break condition: If LLM hallucinations or prompt misinterpretation cause the generated text to drift from the core information need, relevance accuracy degrades.

### Mechanism 2
- Claim: IR system rankings derived from a synthetic test collection can approximate rankings from traditional human-judged collections.
- Mechanism: By evaluating systems on GenTREC and calculating Kendall's τ against rankings from TREC collections, the study demonstrates strong correlation for metrics like P@100.
- Core assumption: The distribution of relevant documents in GenTREC creates a retrieval landscape where better systems perform better in a rank-compatible way.
- Evidence anchors: High τ scores for P@100, MAP, and RPrec, but low/negative scores for P@10, indicating metric-dependency.
- Break condition: If the retrieval task becomes too easy (too many highly relevant documents), metrics like P@10 lose discriminative power.

### Mechanism 3
- Claim: Introducing synthetically generated "tricky" non-relevant documents increases the challenge and realism of the evaluation.
- Mechanism: By using masked keywords and prompt variations, the authors generate documents that are lexically or semantically similar to relevant ones but are not actually relevant.
- Core assumption: The method for generating tricky negatives produces documents that are genuinely non-relevant but not trivially distinguishable.
- Evidence anchors: Counter-intuitively, removing these documents did not hurt and sometimes slightly improved rank correlation for some metrics.
- Break condition: If the generation process accidentally creates relevant documents (false negatives), it introduces noise.

## Foundational Learning

- Concept: Cranfield Paradigm / Test Collections
  - Why needed here: The entire methodology is a modification of the Cranfield approach, which relies on three components: a document corpus, a set of topics, and relevance judgments. Understanding this triad is essential to grasp what GenTREC replaces (the documents and judgments) and what it keeps (the topics).
  - Quick check question: What are the three traditional components of an IR test collection, and which two are synthetically generated in GenTREC?

- Concept: Evaluation Metrics (MAP, Precision, Kendall's τ)
  - Why needed here: The paper's claims hinge on comparing system performance using MAP, P@100, etc., and measuring the similarity of resulting system rankings using Kendall's τ.
  - Quick check question: If System A has a higher P@10 than System B on GenTREC but a lower P@10 on TREC, what does a low Kendall's τ indicate about GenTREC's reliability for that metric?

- Concept: Prompt Engineering for LLMs
  - Why needed here: The quality of the synthetic collection depends entirely on how prompts are crafted to generate diverse relevant documents and tricky non-relevant ones.
  - Quick check question: Why did the authors choose to generate subtopics before generating documents rather than using the main topic description repeatedly?

## Architecture Onboarding

- Component map:
  Seed Topics -> Prompt Engineering Module -> LLM Generator -> Document Corpus -> (Implicit Judgment) -> Evaluation Engine

- Critical path: Seed Topic -> Prompt Engineering -> LLM Generation -> Document Corpus -> (Implicit Judgment) -> System Evaluation

- Design tradeoffs:
  - Cost vs. Quality: The method is extremely cheap ($126) but produces documents with lower lexical diversity and shorter length than human-authored ones.
  - Control vs. Realism: You gain precise control over document-topic relevance but lose the natural complexity, noise, and lexicon of real-world text.
  - Evaluation Reliability vs. Metric Sensitivity: The collection shows high correlation for some metrics (P@100) but fails for others (P@10), requiring careful metric selection.

- Failure signatures:
  - High Relevance Density: If too many relevant documents exist per topic, top-heavy metrics like P@10 become saturated.
  - Prompt Misalignment: If a topic requires specific historical facts, an LLM may generate related but non-responsive text.
  - False Negatives: If the "tricky" non-relevant document generator accidentally creates relevant content, it introduces systematic error.

- First 3 experiments:
  1. Metric Sensitivity Analysis: Run diverse retrieval systems and calculate rank correlation across a wide range of metrics to identify which are reliable.
  2. Topic-Type Robustness: Manually check relevance accuracy for different types of information needs to identify where generation breaks down.
  3. Negative Document Ablation: Evaluate system rankings using different versions of the corpus to quantify individual contributions to ranking stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the document generation methodology be adjusted to improve the reliability of GenTREC for high-precision metrics like P@10?
- Basis in paper: The authors report that system rankings diverge significantly for P@10 (average Kendall's τ = 0.2) compared to TREC collections.
- Why unresolved: The current prompts generate documents that systems retrieve too easily, limiting P@10's discriminative power.
- What evidence would resolve it: Experiments using modified prompts to generate "harder" relevant documents that result in system rankings with statistically significant correlation to TREC benchmarks for P@10.

### Open Question 2
- Question: How can LLM-based document generation be adapted to effectively handle topics that require references to specific historical incidents?
- Basis in paper: For topics requiring specific past events (e.g., Topic 430 on bee attacks), the relevance rate dropped to 0%.
- Why unresolved: The inherent nature of LLM generation in this setup favors broad topical discussion over specific factual retrieval.
- What evidence would resolve it: A new generation strategy that successfully produces documents containing specific historical details, achieving relevance rates comparable to abstract topics.

### Open Question 3
- Question: Can incorporating multiple language models or advanced prompt engineering mitigate the lower lexical diversity observed in LLM-generated test collections?
- Basis in paper: GenTREC has lower lexical diversity than human-authored Disks 4-5.
- Why unresolved: The study relied exclusively on ChatGPT with a fixed prompt structure.
- What evidence would resolve it: A comparative study showing that test collections generated by an ensemble of LLMs achieve lexical diversity scores statistically indistinguishable from human-authored collections.

## Limitations
- The fundamental assumption that LLM-generated documents are inherently relevant shows wide variance (0% to 100% accuracy across different topics)
- High density of relevant documents may make retrieval too easy, causing metrics like P@10 to lose discriminative power
- The collection shows significant divergence in system rankings for precision-at-10 compared to traditional TREC collections

## Confidence
- **High Confidence**: Cost-effectiveness claim ($126 vs tens of thousands) and high correlation for P@100, MAP, and RPrec rankings
- **Medium Confidence**: Viability claim qualified by P@10 divergence, lower lexical diversity observation, and complex impact of tricky non-relevant documents
- **Low Confidence**: 83% relevance accuracy based on only 10 sampled topics, 94% non-relevance accuracy without systematic false negative analysis

## Next Checks
1. Systematically categorize all 300 topics by type and validate relevance accuracy for each category to identify where the generation heuristic breaks down
2. Conduct comprehensive analysis of rank correlation across a broader range of metrics and precision levels to determine which are most and least reliable
3. Implement adversarial checking mechanism where a separate LLM evaluates relevance of each generated document to systematically improve collection quality