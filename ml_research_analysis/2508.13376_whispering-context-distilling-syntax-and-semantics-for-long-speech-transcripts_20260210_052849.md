---
ver: rpa2
title: 'Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts'
arxiv_id: '2508.13376'
source_url: https://arxiv.org/abs/2508.13376
tags:
- entity
- whisper
- context
- llama
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of improving Automatic Speech
  Recognition (ASR) systems for long audio transcripts, particularly focusing on Named
  Entity Recognition (NER), capitalization, and punctuation. The core method involves
  distilling contextual knowledge from LLaMA models into Whisper using two strategies:
  token-level distillation with optimal transport for alignment, and representation
  loss minimization between sentence embeddings.'
---

# Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts
## Quick Facts
- arXiv ID: 2508.13376
- Source URL: https://arxiv.org/abs/2508.13376
- Authors: Duygu Altinok
- Reference count: 4
- Primary result: WER 0.20, NER F1 0.82 on Spoken Wikipedia dataset

## Executive Summary
This paper addresses the challenge of improving Automatic Speech Recognition (ASR) systems for long audio transcripts, focusing on Named Entity Recognition (NER), capitalization, and punctuation. The core method involves distilling contextual knowledge from LLaMA models into Whisper using two strategies: token-level distillation with optimal transport for alignment, and representation loss minimization between sentence embeddings. The approach is evaluated on the Spoken Wikipedia dataset, which features long audios and rich entities. Results demonstrate significant improvements in Word Error Rate (WER), NER, capitalization, and punctuation success.

## Method Summary
The method leverages knowledge distillation to transfer syntactic and semantic context from LLaMA into Whisper. It employs token-level distillation using optimal transport alignment to map tokens between models, combined with representation loss minimization to align sentence embeddings. This dual strategy aims to enhance Whisper's ability to capture contextual cues, improving transcription accuracy for long-form speech, especially in tasks like NER, capitalization, and punctuation.

## Key Results
- Achieved WER of 0.20 and NER F1 score of 0.82 on Spoken Wikipedia dataset
- Outperformed baseline in capitalization and punctuation success
- Introduced novel NER metrics tailored for long speech transcripts

## Why This Works (Mechanism)
The method works by injecting syntactic and semantic context from LLaMA into Whisper through distillation. Token-level distillation with optimal transport ensures precise token alignment, while representation loss minimization aligns higher-level sentence embeddings. This dual approach enriches Whisper's contextual understanding, enabling better handling of long-form speech with complex entities and punctuation.

## Foundational Learning
1. **Optimal Transport for Token Alignment**
   - Why needed: Ensures accurate token mapping between LLaMA and Whisper during distillation
   - Quick check: Verify alignment accuracy on parallel text pairs

2. **Knowledge Distillation**
   - Why needed: Transfers contextual knowledge from LLaMA to improve Whisper's performance
   - Quick check: Compare performance with and without distillation

3. **Sentence Embedding Alignment**
   - Why needed: Preserves semantic context at the sentence level
   - Quick check: Measure embedding similarity before and after alignment

## Architecture Onboarding
**Component Map**: Whisper -> Optimal Transport Alignment -> LLaMA Embeddings -> Distillation Loss -> Enhanced Whisper

**Critical Path**: Whisper ASR output → Token alignment via optimal transport → Representation loss minimization → Final enhanced Whisper output

**Design Tradeoffs**: Token-level distillation offers precision but increases computational cost; representation loss is lighter but may lose fine-grained details.

**Failure Signatures**: Poor alignment in noisy speech; loss of entity recognition in domain shifts; degraded punctuation in short utterances.

**First Experiments**:
1. Validate token alignment accuracy on parallel text pairs
2. Compare WER and NER performance with and without distillation
3. Test punctuation restoration on short vs. long transcripts

## Open Questions the Paper Calls Out
- Robustness of distillation method on non-Wikipedia domains
- Effectiveness of optimal transport alignment across varied speech styles and noise conditions
- Generalization of proposed NER metrics to other entity types and languages

## Limitations
- Evaluation limited to Spoken Wikipedia dataset, raising cross-domain generalization concerns
- Effectiveness of optimal transport alignment across varied speech styles and noise conditions remains unclear
- Proposed NER metrics lack independent verification and multilingual validation

## Confidence
- Token-level distillation with optimal transport alignment significantly improves WER and NER metrics: Medium
- Integrating linguistic context enhances transcription quality: Medium
- Novelty and superiority of proposed NER metrics: Low

## Next Checks
1. Evaluate the model on diverse ASR datasets (e.g., TED talks, conversational speech) to test cross-domain robustness
2. Conduct ablation studies isolating the impact of token-level distillation versus representation loss minimization
3. Validate the proposed NER metrics on multilingual and multi-domain datasets to confirm their generalizability