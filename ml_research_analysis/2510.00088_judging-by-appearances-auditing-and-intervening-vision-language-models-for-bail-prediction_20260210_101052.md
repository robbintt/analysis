---
ver: rpa2
title: Judging by Appearances? Auditing and Intervening Vision-Language Models for
  Bail Prediction
arxiv_id: '2510.00088'
source_url: https://arxiv.org/abs/2510.00088
tags:
- legal
- case
- vlms
- bail
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper audits vision-language models (VLMs) for bail decision
  prediction, focusing on whether VLMs can handle multimodal legal inputs (images
  of accused individuals plus case reports). The audit reveals that standalone VLMs
  perform poorly, often denying bail to deserving individuals with high confidence,
  regardless of intersectional groups (race/gender).
---

# Judging by Appearances? Auditing and Intervening Vision-Language Models for Bail Prediction

## Quick Facts
- arXiv ID: 2510.00088
- Source URL: https://arxiv.org/abs/2510.00088
- Authors: Sagnik Basu; Shubham Prakash; Ashish Maruti Barge; Siddharth D Jaiswal; Abhisek Dash; Saptarshi Ghosh; Animesh Mukherjee
- Reference count: 40
- Primary result: With targeted interventions (RAG + fine-tuning), VLMs can achieve up to 75.72% accuracy on bail prediction while reducing false negative rates.

## Executive Summary
This paper audits vision-language models for multimodal bail decision prediction, revealing that standalone VLMs perform poorly—often denying bail to deserving individuals with high confidence regardless of race or gender. To address this, the authors introduce two interventions: a precedent-aware retrieval-augmented generation framework and supervised fine-tuning with image attention masking. Results show the offense-type-augmented fine-tuned model with RAG (MO[RAG]) achieves the best performance—up to 75.72% accuracy—while also reducing false negative rates and improving negative predictive value. The work demonstrates that targeted interventions can make VLMs more suitable for sensitive legal judgment tasks, though further development is needed before real-world deployment.

## Method Summary
The study uses paired inputs of mugshot images (from Illinois DOC dataset) and case facts (from HLDC Hindi legal corpus translated to English) to predict binary bail decisions. After preprocessing to extract facts-only and filter to token length ≥50, the dataset contains 12,788 train / 3,316 test pairs. Four 7-8B VLMs are evaluated: LLaVA-NeXt, Qwen2.5-VL, Idefics3, and InternVL 3.5. Interventions include: (1) RAG with top-3 precedents retrieved via Euclidean distance from a vector store of training case facts, and (2) supervised fine-tuning with frozen vision tower and masked image attention. Two fine-tuning variants are tested: vanilla (case facts only) and offense-type-induced (GPT-4o-expanded offense keywords appended). Performance is measured by accuracy, LR- (negative likelihood ratio), and NPV (negative predictive value) across four intersectional groups.

## Key Results
- Standalone VLMs achieve poor accuracy (~40-60%) with high false negative rates, often denying bail to deserving individuals with ~68% confidence
- RAG intervention alone improves accuracy by ~10-16% across models
- Fine-tuned model with offense-type augmentation and RAG (MO[RAG]) achieves best performance: 75.72% accuracy (Llava-NeXt), 74.48% (Qwen2.5-VL), 71.53% (Idefics3), 70.41% (InternVL 3.5)
- MO[RAG] reduces LR- from 0.97 to 0.52 (Llava-NeXt) and improves NPV from 47.87% to 69.41%
- Interventions show consistent improvements across all intersectional groups (White/Black × Male/Female)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Precedent-aware retrieval grounds VLM predictions in established legal patterns rather than relying on model priors.
- Mechanism: The RAG framework retrieves the top 3 most similar case facts from a vector store using Euclidean distance, then appends them to the system prompt. This provides the VLM with concrete examples of how similar cases were adjudicated, enabling analogical reasoning.
- Core assumption: Similarity in case fact embeddings correlates with similarity in appropriate judicial outcomes.
- Evidence anchors:
  - [abstract]: "introduce two interventions: a precedent-aware retrieval-augmented generation (RAG) framework"
  - [section 4.2]: "Using a Euclidean distance–based similarity measure, we retrieve the top three relevant case facts closest to the query case fact"
  - [corpus]: Related work on NyayaRAG [29] supports that structured legal retrieval enhances outcome accuracy, but citation count is 0—insufficient validation for generalization.
- Break condition: If case fact embeddings fail to capture legally salient features (e.g., subtle distinctions in intent), retrieval returns superficially similar but legally distinct precedents, degrading predictions.

### Mechanism 2
- Claim: Supervised fine-tuning with image attention masking forces models to learn case-fact-to-outcome mappings independently of visual identity.
- Mechanism: During fine-tuning, the vision tower parameters are frozen and image tokens are masked (attention set to 0). This prevents the model from associating specific faces with outcomes, ensuring it learns legal reasoning patterns from text alone while still processing the image modality during inference.
- Core assumption: Case facts contain sufficient signal for bail decisions without needing to link faces to specific cases during training.
- Evidence anchors:
  - [section 4.3]: "we make the parameters of the vision tower frozen, and completely mask the images from the input by setting the values in the attention mask corresponding to the image tokens to 0"
  - [section 4.3]: "we want to ensure that the models learn how the case facts lead to bail acceptance or rejection, not which person is linked with which case fact"
  - [corpus]: No direct corpus evidence on this specific masking technique for legal VLMs—assumption remains unvalidated externally.
- Break condition: If visual cues (e.g., demeanor, documented injuries) contain legitimate probative value, masking removes signal that could improve accuracy; alternatively, if masking is imperfect, spurious face-outcome correlations may persist.

### Mechanism 3
- Claim: Offense-type augmentation enriches case facts with semantic category information, improving both retrieval relevance and model calibration.
- Mechanism: GPT-4o expands each offense type (e.g., "homicide" → {murder, manslaughter, first-degree murder...}). Case facts are scanned for keyword matches and tagged with offense types, creating "typed-facts." These typed-facts improve RAG retrieval precision and provide explicit category signals to the model.
- Core assumption: Keyword-based offense categorization reliably maps case facts to legally meaningful offense types.
- Evidence anchors:
  - [section 4.3]: "for each offense type, we obtain similar keywords by querying GPT-4o... If a case fact is found to contain one or more keywords corresponding to an offense type, then that offense type is associated with the case fact"
  - [table 2]: M_O[RAG] achieves best performance across models (e.g., Llava-NeXt: 75.72% accuracy vs. 74.27% for M_V[RAG])
  - [corpus]: Related work on RLJP [34923] emphasizes legal reasoning logic, but doesn't validate offense-type keyword expansion specifically.
- Break condition: If case facts use ambiguous or non-standard terminology, offense-type tagging fails or mislabels, degrading retrieval and prediction.

## Foundational Learning

- Concept: Negative Likelihood Ratio (LR−) = FNR / TNR
  - Why needed here: Measures how much more likely bail is denied to deserving individuals compared to those correctly denied. Critical in legal contexts where false negatives (denying bail to the deserving) are more harmful than false positives.
  - Quick check question: If LR− = 0.97 for a model, what does this imply about its bail denial behavior?

- Concept: Retrieval-Augmented Generation (RAG) with Vector Stores
  - Why needed here: Enables precedent-aware predictions without storing all cases in context. Requires understanding embedding-based similarity and prompt construction with retrieved context.
  - Quick check question: Why retrieve top-3 rather than top-1 or top-10 precedents for this task?

- Concept: Vision-Language Model Attention Masking
  - Why needed here: Fine-tuning technique to modality-selective learning. Requires understanding transformer attention mechanisms and how masking prevents gradient flow to specific token positions.
  - Quick check question: What happens if image tokens are not masked during fine-tuning but vision tower is still frozen?

## Architecture Onboarding

- Component map:
  Input Layer: [Image from Illinois DOC dataset] + [Case facts from HLDC corpus] → paired as [I:C_TST]
  Retrieval Layer: Vector store of training case facts → Euclidean distance retrieval → top-3 precedents
  Augmentation Layer: GPT-4o offense-type expansion → keyword matching → typed-facts
  Fine-tuning Layer: SFTTrainer with frozen vision tower, masked image attention, learning rate 1×10⁻⁵
  Output Layer: Binary prediction (yes/no) with confidence calibration

- Critical path:
  1. Preprocess case facts (token length ≥50, remove argument sentences)
  2. Generate typed-facts via offense-type keyword matching
  3. Build vector store from training case facts/typed-facts
  4. Fine-tune VLM with image attention masking
  5. At inference: retrieve precedents → construct prompt → query fine-tuned model

- Design tradeoffs:
  - Vanilla fine-tuning vs. offense-type induced: Simpler pipeline vs. richer signal but dependent on GPT-4o keyword quality
  - Top-3 retrieval: More context vs. potential noise from less relevant precedents
  - Freezing vision tower: Prevents visual bias vs. cannot adapt vision encoder to legal-domain images

- Failure signatures:
  - High LR− with high confidence: Model consistently denies bail wrongly with certainty—sign of unprompted model bias
  - NPV < 50%: Bail denial predictions are less reliable than random—model uncalibrated
  - No improvement from RAG: Retrieval returning irrelevant precedents—check embedding quality or distance metric

- First 3 experiments:
  1. Audit baseline VLM: Pass [I:C_TST] pairs through unmodified model, measure accuracy/LR−/NPV across intersectional groups to establish failure mode.
  2. Add precedent retrieval only: Implement RAG without fine-tuning, measure if similar-case context alone improves LR− (should see ~10-16% accuracy gain per results).
  3. Fine-tune with offense-type + RAG: Full M_O[RAG] pipeline, validate that typed-facts improve retrieval relevance by inspecting retrieved precedents for semantic match to current case offense type.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific intervention architectures beyond the tested RAG and SFT methods are required to elevate VLM accuracy and trustworthiness to a level viable for real-world legal deployment?
- Basis in paper: [explicit] The conclusion states that "more aggressive research is needed to develop stronger interventions" because current accuracy (~76%) is insufficient for sensitive real-world tasks.
- Why unresolved: While the proposed interventions (RAG, fine-tuning) improved performance, the authors acknowledge the results are still below the threshold required for high-stakes legal application.
- What evidence would resolve it: Development of novel intervention frameworks that consistently achieve significantly higher accuracy and lower false negative rates on this multimodal task.

### Open Question 2
- Question: To what extent does the jurisdictional mismatch between the image dataset (US) and legal case text (India) influence the audit's findings regarding bias and accuracy?
- Basis in paper: [inferred] The methodology (Section 3.3) admits to artificially pairing US mugshots with Indian case facts due to a lack of available multimodal legal benchmarks, creating a validity threat.
- Why unresolved: It is unclear if the poor performance of standalone VLMs stems from the difficulty of the task or the cultural/contextual disconnect between the visual and textual data distributions.
- What evidence would resolve it: Replicating the audit using a curated dataset where images of the accused and the corresponding case reports originate from the same legal jurisdiction.

### Open Question 3
- Question: Can specialized confidence calibration techniques correct the observed phenomenon where base VLMs exhibit high confidence (~68%) in incorrect bail denials (false negatives)?
- Basis in paper: [inferred] The audit revealed a "severely alarming trend" where models are highly confident in wrong denials, and while interventions reduced errors, they did not explicitly address the calibration of this confidence.
- Why unresolved: The paper measures the high confidence as a diagnostic symptom but does not propose or test a specific mechanism to align model confidence with prediction correctness.
- What evidence would resolve it: Experiments applying uncertainty quantification or calibration methods (e.g., temperature scaling) to verify if high-confidence false negatives can be suppressed without degrading overall accuracy.

## Limitations

- The offense-type keyword expansion relies heavily on GPT-4o's judgment without external validation of keyword quality or coverage.
- The vector store retrieval uses Euclidean distance without benchmarking against alternative similarity metrics or embedding models.
- The masking technique assumes visual identity carries no probative value, yet doesn't test scenarios where visual cues (demeanor, injuries) might legitimately inform bail decisions.
- All results are based on Indian legal documents translated to English, raising questions about cross-jurisdictional generalization.

## Confidence

**High confidence**: The baseline audit findings (standalone VLMs perform poorly with high false negative rates) and the general improvement pattern from interventions (RAG and fine-tuning both help, with offense-type augmentation providing additional gains). These conclusions are directly supported by quantitative results across multiple models.

**Medium confidence**: The specific mechanism explanations (RAG grounds predictions in legal patterns, masking prevents visual bias). While the experimental design supports these mechanisms, the evidence relies heavily on controlled experiments without external validation or ablation studies.

**Low confidence**: The claim that the system is "ready for deployment" despite acknowledging further development is needed. The paper doesn't address deployment challenges like real-time retrieval latency, continuous learning, or adversarial manipulation of inputs.

## Next Checks

1. **Cross-jurisdictional validation**: Test the MO[RAG] pipeline on bail decisions from different legal systems (e.g., U.S. state courts or European jurisdictions) to assess whether offense-type keyword expansion and retrieval patterns generalize beyond Indian legal documents.

2. **Ablation study of RAG components**: Systematically vary the number of retrieved precedents (1, 3, 5, 10) and similarity metrics (cosine vs. Euclidean) to quantify the marginal value of each component and identify optimal configuration for this task.

3. **Visual signal validation**: Run controlled experiments comparing masked vs. unmasked fine-tuning on subsets where visual cues contain legitimate legal relevance (e.g., documented injuries, visible intoxication) to determine whether complete masking sacrifices useful signal.