---
ver: rpa2
title: A Joint Learning Approach to Hardware Caching and Prefetching
arxiv_id: '2510.10862'
source_url: https://arxiv.org/abs/2510.10862
tags:
- learning
- replacement
- prefetching
- policies
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores joint learning of hardware cache replacement\
  \ and prefetching policies, which are traditionally trained in isolation. The authors\
  \ argue that these policies are interdependent\u2014effective cache replacement\
  \ can leverage knowledge of future prefetches, and prefetching can avoid fetching\
  \ data likely to be evicted soon."
---

# A Joint Learning Approach to Hardware Caching and Prefetching

## Quick Facts
- arXiv ID: 2510.10862
- Source URL: https://arxiv.org/abs/2510.10862
- Reference count: 33
- Primary result: Joint learning improves cache replacement accuracy by 1-1.3× vs. isolated training

## Executive Summary
This paper addresses the problem of coordinating hardware cache replacement and prefetching policies, which are traditionally trained in isolation. The authors argue that these policies are interdependent—effective cache replacement can leverage knowledge of future prefetches, and prefetching can avoid fetching data likely to be evicted soon. They propose two approaches for joint learning: (1) a joint encoder that learns shared representations from both policies' features, and (2) contrastive learning to align the embeddings of temporally correlated features. Experiments on SPEC CPU traces show that both approaches improve cache replacement accuracy by 1-1.3× compared to baseline models, with some traces achieving nearly 100% accuracy.

## Method Summary
The paper proposes two joint learning approaches to coordinate cache replacement and prefetching policies. The first approach uses a joint encoder that learns shared representations from features used by both policies, with the combined encoder trained alongside individual policy networks. The second approach uses contrastive learning to align embeddings of temporally correlated replacement and prefetching events, pulling positive pairs together while pushing negatives apart. Both methods are evaluated on SPEC CPU traces using ChampSim simulator, with accuracy measured against Belady's MIN oracle.

## Key Results
- Joint learning approaches improve cache replacement accuracy by 1-1.3× compared to baseline models
- Some traces achieve nearly 100% accuracy with joint learning
- Both joint encoder and contrastive learning approaches demonstrate effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint encoding allows the cache replacement policy to implicitly discover correlations between prefetching and replacement feature sets.
- **Mechanism:** The outputs of individual encoders (one for replacement, one for prefetching) are concatenated and passed through a combined encoder. Backpropagation flows from the shared loss through the combined network, splitting gradients so each encoder learns from the other's task signal.
- **Core assumption:** Features used by both policies share latent structure that, when jointly encoded, improves downstream decision quality.
- **Evidence anchors:**
  - [abstract] "We propose a joint learning approach based on developing shared representations for the features used by the two policies."
  - [Section 3 - Joint Learning] "Training the combined encoder jointly with the individual encoders and policy networks ensures that the embeddings evolve to reflect shared structural information."
  - [corpus] Related work (e.g., Athena, Coordinated RL Prefetching) also emphasizes synergistic policy coordination but via RL rather than shared representations.
- **Break condition:** If features from the two policies are fundamentally orthogonal (no shared structure), the joint encoder adds capacity without benefit, potentially overfitting.

### Mechanism 2
- **Claim:** Contrastive learning explicitly aligns embeddings of temporally correlated replacement and prefetching events, improving coordination without requiring a combined encoder.
- **Mechanism:** Temporally co-occurring feature pairs (e.g., a prefetch decision followed shortly by a replacement decision) are treated as positive pairs; uncorrelated pairs are negatives. A contrastive loss pulls positive pairs closer in embedding space while pushing negatives apart. After alignment, downstream policy networks are trained separately.
- **Core assumption:** Temporal proximity between replacement and prefetching events indicates a meaningful causal or semantic relationship that should be captured in shared representation space.
- **Evidence anchors:**
  - [abstract] "...another based on contrastive learning of the embeddings..."
  - [Section 3 - Joint Learning] "This contrastive setup pulls embeddings of causally related events together while pushing unrelated activity apart."
  - [corpus] Limited direct corpus evidence on contrastive learning for cache/prefetch joint training; related work focuses on RL or GNN-based prefetching.
- **Break condition:** If temporal correlation is noisy (many spurious co-occurrences), contrastive loss may align unrelated features, degrading policy decisions.

### Mechanism 3
- **Claim:** Training policies in isolation causes distribution shift at deployment, as each policy encounters inputs influenced by the other's decisions—inputs absent from training.
- **Mechanism:** Each learned policy implicitly assumes a fixed input distribution. When deployed together, the prefetcher alters the cache state distribution seen by replacement, and vice versa, violating training assumptions and causing suboptimal eviction or wasteful prefetches.
- **Core assumption:** The joint system's behavior cannot be decomposed into independently optimized components without loss.
- **Evidence anchors:**
  - [Section 1 - Introduction] "When exposed to inputs outside the training distribution, the policies can result in sub-optimal behavior."
  - [Section 2 - Background and Motivation] "If the prefetcher brings in a line that the replacement policy classifies as cache-averse and immediately evicts, the prefetch is wasted."
  - [corpus] Corroborated by Athena's finding that "naively combining [prefetching and off-chip prediction] often fails to realize their potential."
- **Break condition:** If one policy is highly conservative (e.g., minimal prefetch rate), distribution shift may be negligible, reducing joint learning's marginal benefit.

## Foundational Learning

- **Concept: Belady's MIN algorithm**
  - **Why needed here:** Used as the oracle for ground-truth labeling—classifying cache lines as cache-friendly (will be reused) or cache-averse (dead-on-arrival). All model accuracy is measured against this optimal baseline.
  - **Quick check question:** Can you explain why Belady's MIN is optimal but impractical for real-time cache management?

- **Concept: Embedding layers for program counters and addresses**
  - **Why needed here:** Both Glider (replacement) and Voyager (prefetching) map sparse, high-cardinality inputs (PCs, addresses) to dense embeddings before LSTM processing. Understanding this translation step is essential for modifying encoders.
  - **Quick check question:** Why would a learned embedding outperform a one-hot encoding for program counters in this context?

- **Concept: Contrastive learning objective (positive/negative pairs)**
  - **Why needed here:** The contrastive approach requires defining what counts as a "temporally correlated" positive pair versus a negative. Mis-specifying this directly impacts alignment quality.
  - **Quick check question:** In this paper's context, what constitutes a positive pair vs. a negative pair for the contrastive loss?

## Architecture Onboarding

- **Component map:** Input features (block address, PC, core ID, miss type, stride/stream ID, set index, cycle count) -> Individual encoders (embedding layers) -> Coordination layer (joint encoder or contrastive pretraining) -> Policy networks (LSTM-based) -> Binary classification (replacement) or address predictions (prefetching)

- **Critical path:**
  1. Collect SPEC CPU traces with ChampSim, logging features at every tag lookup
  2. Generate oracle labels via offline Belady's MIN replay
  3. Train encoders and policy networks (jointly or contrastively aligned)
  4. Evaluate on held-out 20% of each trace

- **Design tradeoffs:**
  - **Joint encoder:** Implicit correlation discovery; end-to-end trainable; requires synchronous feature availability from both policies; may entangle irrelevant features
  - **Contrastive learning:** Explicit alignment signal; two-stage training (encoder alignment -> policy training); more robust to asynchronous feature availability; depends on quality of positive/negative pair construction

- **Failure signatures:**
  - Accuracy degrades vs. cache-only baseline -> check for embedding collapse or mis-specified contrastive pairs
  - High variance across traces -> policy may be overfitting to specific access patterns; consider trace diversity
  - Prefetcher floods cache with cache-averse lines -> alignment failed; replacement policy not receiving useful prefetch context

- **First 3 experiments:**
  1. **Baseline replication:** Train cache-only Glider model on SPEC traces; verify accuracy matches reported cache-only numbers (~78-93% across traces).
  2. **Joint encoder ablation:** Train with joint encoder on same traces; compare accuracy lift. If lift < 1%, inspect embedding gradients for signal.
  3. **Contrastive pair sensitivity:** Vary the temporal window defining "co-occurring" positive pairs (e.g., same cycle vs. within 10 cycles); measure impact on replacement accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- No ablation of the individual policy networks (LSTM layers) themselves—improvements could stem from shared embeddings rather than joint coordination
- No experiments on live hardware or multi-programmed workloads where policy interactions are most volatile
- Contrastive learning pair definition is not experimentally validated; window size and causality assumptions are fixed
- Limited evaluation to two SPEC CPU traces; generalizability to other workloads or cache sizes is unclear

## Confidence

- **High**: Cache replacement accuracy improvement (1-1.3×) vs. baseline is measurable and statistically supported
- **Medium**: Joint encoder and contrastive learning approaches improve coordination, but exact contribution of each is not isolated
- **Low**: Claims about broader applicability to other interdependent system policies remain speculative without additional case studies

## Next Checks

1. Perform an ablation study isolating the effect of joint encoder vs. LSTM policy network capacity
2. Test contrastive learning with multiple temporal window sizes and causal heuristics to determine sensitivity
3. Evaluate joint learning approach on a diverse set of workloads (e.g., graph, ML, server traces) to assess generalizability