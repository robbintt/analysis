---
ver: rpa2
title: Characterizing the Effects of Translation on Intertextuality using Multilingual
  Embedding Spaces
arxiv_id: '2501.10731'
source_url: https://arxiv.org/abs/2501.10731
tags:
- intertextuality
- translation
- translations
- human
- greek
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for characterizing intertextuality
  in Biblical texts and their translations using multilingual embedding spaces. The
  authors compute intertextuality ratios by measuring cosine similarity between verse
  embeddings, distinguishing references within and across testaments.
---

# Characterizing the Effects of Translation on Intertextuality using Multilingual Embedding Spaces

## Quick Facts
- arXiv ID: 2501.10731
- Source URL: https://arxiv.org/abs/2501.10731
- Reference count: 11
- Primary result: Human translations consistently preserve higher intertextuality ratios than machine translations, with English showing a ratio of 1.66 versus 1.32 for machine translation

## Executive Summary
This paper introduces a novel approach to measuring intertextuality preservation across Bible translations using multilingual embedding spaces. The authors develop a method to quantify how well cross-references between Biblical passages are maintained when texts are translated into different languages, comparing both human and machine translations. By computing cosine similarity between verse embeddings, they create intertextuality ratios that reveal systematic differences in how various translations preserve textual relationships. The study demonstrates that human translations consistently maintain stronger intertextual connections than machine-generated translations, with particularly pronounced effects in cross-testament references.

## Method Summary
The researchers constructed multilingual embedding spaces using BGE-M3 to represent Biblical verses across five languages: English, Finnish, Turkish, Swedish, and Marathi. They calculated intertextuality ratios by measuring cosine similarity between verse embeddings for both same-testament and cross-testament references. The analysis compared 2,183 ground-truth cross-references across human translations and machine translations generated by Aya23. Translation quality was assessed using COMET scores against human references, and intertextuality preservation was evaluated by comparing ratio distributions between translation types.

## Key Results
- Human translations show significantly higher intertextuality ratios (e.g., 1.66 for English) compared to machine translations (e.g., 1.32 for English)
- English and Turkish translations achieved the highest COMET scores (61.2-72.6 and 65.4-68.2 respectively), while Marathi scored lowest (26.5-29.8)
- Human translators systematically amplify intertextuality between Old and New Testaments more than within-testament connections

## Why This Works (Mechanism)
The method leverages multilingual embedding spaces to create a unified semantic representation of Biblical texts across languages. By computing cosine similarity between verse embeddings, the approach captures semantic relationships that transcend linguistic differences. The distinction between same-testament and cross-testament references allows for nuanced analysis of how different translation approaches handle various types of intertextual connections.

## Foundational Learning
- Multilingual embeddings (why needed: enable cross-lingual semantic comparison; quick check: can verses be meaningfully compared across languages)
- Cosine similarity for semantic matching (why needed: quantifies semantic relatedness between verses; quick check: similar verses should have higher similarity scores)
- Intertextuality ratio calculation (why needed: provides standardized metric for comparing reference preservation; quick check: ratios should be >1 when cross-references are better preserved)

## Architecture Onboarding

Component map:
BGE-M3 embedding model -> Verse embedding generation -> Cosine similarity computation -> Intertextuality ratio calculation -> Cross-translation comparison

Critical path:
Verse embedding generation -> Cosine similarity computation -> Intertextuality ratio calculation

Design tradeoffs:
- Single embedding model vs. ensemble approaches for robustness
- Fixed tokenization vs. language-specific tokenization strategies
- Aggregate statistics vs. weighted reference analysis

Failure signatures:
- Systematic bias in embedding space affecting certain language pairs
- Over-reliance on lexical similarity masking semantic differences
- Inconsistent tokenization leading to embedding misalignment

First experiments:
1. Test embedding quality by checking if parallel verses across languages cluster together
2. Validate cosine similarity scores against known semantically related verse pairs
3. Verify intertextuality ratio calculation by testing on controlled reference sets

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Results may be artifacts of embedding space construction and verse-level tokenization
- COMET scores based on human references may not capture full semantic range relevant to intertextuality
- Analysis treats all cross-references equally without weighting for theological or rhetorical significance

## Confidence
- Core finding (human > machine intertextuality): Medium
- Amplification between testaments claim: Low
- Translation quality rankings by language: Medium

## Next Checks
1. Replicate analysis using alternative multilingual embedding models (LASER, LaBSE) to test robustness
2. Conduct qualitative analysis of sample cross-references to validate preserved intertextuality against theological intent
3. Compare intertextuality ratios across multiple machine translation systems with different architectures and training approaches