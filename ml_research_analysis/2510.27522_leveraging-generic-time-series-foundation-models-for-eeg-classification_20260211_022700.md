---
ver: rpa2
title: Leveraging Generic Time Series Foundation Models for EEG Classification
arxiv_id: '2510.27522'
source_url: https://arxiv.org/abs/2510.27522
tags:
- sleep
- time
- mantis
- foundation
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether general-purpose time series foundation
  models can effectively classify EEG signals, traditionally requiring domain-specific
  models. Two EEG tasks are examined: motor imagery classification and sleep staging.'
---

# Leveraging Generic Time Series Foundation Models for EEG Classification

## Quick Facts
- arXiv ID: 2510.27522
- Source URL: https://arxiv.org/abs/2510.27522
- Authors: Théo Gnassounou; Yessin Moakher; Shifeng Xie; Vasilii Feofanov; Ievgen Redko
- Reference count: 12
- Primary result: Mantis, a general-purpose time series foundation model, outperforms specialized EEG models on motor imagery and sleep staging tasks.

## Executive Summary
This paper investigates whether general-purpose time series foundation models can effectively classify EEG signals, traditionally requiring domain-specific models. Two EEG tasks are examined: motor imagery classification and sleep staging. Two pretrained models are tested: CBraMod, a masked autoencoder designed for EEG, and Mantis, a contrastive time series foundation model pretrained on either real-world or synthetic data. Mantis consistently outperforms both CBraMod and the EEGNet baseline, even though it was trained with minimal EEG data. This is notable because Mantis processes channels univariately, yet still preserves spatial information effectively. For sleep staging, Mantis improves F1 scores by up to 3% over CBraMod, particularly when channels are limited. These results suggest that general-purpose time series models can transfer well to EEG, offering a promising direction for brain signal analysis and reducing the need for extensive domain-specific pretraining.

## Method Summary
The study fine-tunes two foundation models—Mantis (a contrastive time series model) and CBraMod (an EEG-specific masked autoencoder)—against an EEGNet baseline on two EEG classification tasks. BCI datasets (motor imagery) are resampled to 200 Hz with subject splits, while sleep staging datasets are preprocessed to 100 Hz with bipolar channels. Mantis processes channels univariately using a transformer encoder, concatenating embeddings for classification, whereas CBraMod models multivariate channel interactions. Fine-tuning is essential for both models, with Mantis requiring channel descriptors and CBraMod using a 3-layer MLP head. Training uses AdamW, cosine learning rate schedules, and early stopping, with Mantis pretrained on either real or synthetic time series data.

## Key Results
- Mantis consistently outperforms CBraMod and EEGNet across motor imagery and sleep staging tasks.
- Synthetic pretraining of Mantis achieves performance comparable to real-world pretraining for sleep staging.
- Mantis shows greater robustness with limited channels, improving F1 scores by up to 3% over CBraMod.
- Freezing Mantis's encoder leads to severe performance degradation, indicating fine-tuning is necessary for effective transfer.

## Why This Works (Mechanism)
The success of Mantis stems from its contrastive pretraining objective (InfoNCE loss), which forces the model to learn generalizable temporal features by pulling together augmented views of the same sample while pushing apart views of different samples. This enables effective transfer to EEG despite minimal domain-specific pretraining. The univariate channel processing allows Mantis to scale efficiently across varying channel counts, and the late-fusion approach (concatenating channel embeddings) surprisingly preserves spatial information well for these tasks. The pretraining on diverse real or synthetic time series data provides a rich feature space that transfers effectively when fine-tuned on EEG.

## Foundational Learning
- **Concept: Contrastive Learning (InfoNCE Loss)**
  - Why needed here: This is the core pretraining objective for Mantis. Understanding how it forces embeddings of similar samples (augmented views) together and dissimilar samples apart is critical to understanding *why* the model learns generalizable temporal features from either real or synthetic data.
  - Quick check question: Given a batch of N samples, would the InfoNCE loss encourage the embeddings of two differently augmented views of sample A to be closer to each other than to the embedding of an unaugmented view of sample B?

- **Concept: Univariate vs. Multivariate Processing**
  - Why needed here: The paper's counter-intuitive finding hinges on this distinction. Grasping the difference between processing channels independently (Mantis) versus modeling their inter-dependencies from the start (CBraMod) is essential for interpreting the results and deciding which approach to use for a new task.
  - Quick check question: Which model architecture (Mantis or CBraMod) would be inherently more efficient and easier to scale when the number of input channels varies dramatically across datasets?

- **Concept: Transfer Learning (Fine-tuning)**
  - Why needed here: The paper's contribution is demonstrating effective *transfer* from generic time series data to EEG. One must understand the paradigm of taking a pretrained encoder and adapting it to a downstream task by training a new classification head and updating the encoder weights.
  - Quick check question: The paper states freezing Mantis's encoder led to poor performance. What does this imply about the relationship between the generic temporal features and the specific features needed for EEG classification?

## Architecture Onboarding
- **Component map**: Mantis Input (univariate channel) -> Tokenizer (1D Conv + Mean Pooling + Scalar Encoder) -> Encoder (6-layer Transformer with [CLS] token) -> Output (channel descriptor) -> Classification Head (linear) -> Class logits
- **Critical path**: The experiment flow starts with loading a Mantis checkpoint (pretrained on real or synthetic data) -> EEG data is preprocessed and channels are fed independently to the encoder -> channel embeddings are concatenated -> the classification head is trained (and the encoder is fine-tuned) using cross-entropy loss on the labeled EEG task.
- **Design tradeoffs**: The primary tradeoff is **specialization vs. generality**. CBraMod is specialized (pretrained on EEG with multivariate attention), which may be powerful but is less flexible. Mantis is general (pretrained on generic data with univariate processing), which makes it highly adaptable and efficient for tasks with limited channels, as demonstrated. Another tradeoff is in the pretraining data: real-world data may contain more realistic noise but is harder to curate; synthetic data is infinitely scalable but its effectiveness relies on the generator's ability to cover the pattern space.
- **Failure signatures**:
  - Poor performance on high-channel tasks: If a task critically depends on fine-grained, early spatial fusion across many channels, Mantis's late-fusion approach might underperform compared to a multivariate model.
  - Training instability from scratch: The paper notes training Mantis from random initialization on EEG can lead to instability (runtime errors/NaN), highlighting the importance of the pretrained weights for convergence.
  - Freezing the encoder: A key failure mode is treating the pretrained model as a fixed feature extractor. The paper explicitly states this leads to a "huge decrease in performance," indicating fine-tuning is non-optional for this transfer.
- **First 3 experiments**:
  1. Baseline Reproduction: Run Mantis (real pretrained) vs. CBraMod vs. EEGNet on the PhysioNet-MI dataset. Metric: Balanced Accuracy. Goal: Reproduce the paper's finding that the general-purpose model is competitive with the EEG-specific model.
  2. Pretraining Source Ablation: Run Mantis on a sleep staging task (e.g., MASS dataset) using two checkpoints: one pretrained on real data, one on synthetic data. Metric: Weighted F1. Goal: Validate the paper's claim that synthetic pretraining is a viable alternative to real-world data pretraining.
  3. Channel Robustness Test: Evaluate Mantis and CBraMod on a downstream EEG task using progressively fewer input channels (e.g., all channels, then 4, then 2, then 1). Goal: Confirm the hypothesis that the channel-independent design is more robust and advantageous in limited-channel scenarios.

## Open Questions the Paper Calls Out
- Can Mantis generalize its performance advantage to EEG tasks beyond motor imagery and sleep staging, such as emotion recognition?
  - Basis in paper: [explicit] The conclusion states future work should "extend these experiments to include a wider range of BCI datasets and new tasks, like emotion recognition, to fully validate Mantis's generalizability."
  - Why unresolved: The current study is restricted to only two specific tasks: motor imagery classification and sleep stage prediction.
  - What evidence would resolve it: Benchmark results showing Mantis outperforming domain-specific models on diverse tasks like emotion detection or seizure prediction.
- Can specialized normalization techniques enable effective zero-shot learning for EEG with foundation models?
  - Basis in paper: [explicit] The authors suggest "addressing the challenge of zero-shot learning on EEG data, perhaps through specialized normalization techniques such as Monge alignment."
  - Why unresolved: The current study relied on fine-tuning; the authors noted that freezing the encoder leads to a "huge decrease in performance."
  - What evidence would resolve it: Successful classification on unseen subjects or datasets without parameter updates, utilizing only the proposed alignment methods.
- How can larger foundation models be adapted for EEG when standard fine-tuning is unstable or ineffective?
  - Basis in paper: [inferred] The authors excluded the MOMENT model because it was "very difficult to fine-tune due to its large model size," and noted performance drops when freezing encoders.
  - Why unresolved: There is a methodological gap in adapting large-scale, generic backbones to the specific constraints of EEG data without encountering optimization difficulties.
  - What evidence would resolve it: Development of parameter-efficient fine-tuning (PEFT) strategies that allow large generic models to converge on EEG tasks efficiently.

## Limitations
- Generalization is only tested on two EEG tasks (motor imagery and sleep staging), leaving open whether the approach scales to more complex paradigms like seizure detection or ERP analysis.
- The paper does not explore computational efficiency or memory requirements when scaling to high-channel datasets, which could be a bottleneck for Mantis's late-fusion approach.
- Exact model hyperparameters (e.g., CBraMod's MLP dimensions) are unspecified, and training Mantis from scratch on EEG leads to instability, suggesting the pretrained weights are essential.

## Confidence
- **High Confidence**: The claim that Mantis outperforms CBraMod and EEGNet on the tested tasks, as this is directly supported by reported metrics (balanced accuracy, F1 scores).
- **Medium Confidence**: The assertion that synthetic pretraining is a viable alternative to real-world pretraining, since this is demonstrated on a single downstream task (sleep staging) without extensive ablation across diverse datasets.

## Next Checks
- Reproduce the baseline experiment on PhysioNet-MI comparing Mantis (real pretraining) against CBraMod and EEGNet, verifying balanced accuracy results.
- Implement the channel robustness test by evaluating both models on a downstream task with progressively fewer input channels (all, 4, 2, 1) to confirm Mantis's advantage.
- Test Mantis on an additional EEG task (e.g., emotion recognition or seizure detection) to assess generalization beyond the two tasks studied.