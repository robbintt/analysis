---
ver: rpa2
title: 'SITS-DECO: A Generative Decoder Is All You Need For Multitask Satellite Image
  Time Series Modelling'
arxiv_id: '2510.21813'
source_url: https://arxiv.org/abs/2510.21813
tags:
- arxiv
- data
- tasks
- crop
- modelling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SITS-DECO introduces a generative, decoder-only transformer framework\
  \ for Earth Observation (EO) tasks, framing diverse EO data and tasks as unified\
  \ token sequences of continuous sensor data and symbolic tokens. This approach enables\
  \ multi-modal, multi-temporal satellite image time series (SITS) modelling\u2014\
  demonstrated on crop-type classification\u2014without task-specific heads or fine-tuning."
---

# SITS-DECO: A Generative Decoder Is All You Need For Multitask Satellite Image Time Series Modelling

## Quick Facts
- arXiv ID: 2510.21813
- Source URL: https://arxiv.org/abs/2510.21813
- Reference count: 40
- Outperforms much larger EO foundation models on PASTIS-R crop-type classification with up to 60.91% mIoU

## Executive Summary
SITS-DECO introduces a generative, decoder-only transformer framework for Earth Observation (EO) tasks, framing diverse EO data and tasks as unified token sequences of continuous sensor data and symbolic tokens. This approach enables multi-modal, multi-temporal satellite image time series (SITS) modelling—demonstrated on crop-type classification—without task-specific heads or fine-tuning. Through symbolic prompting, the model performs multiple supervised and self-supervised tasks within a single architecture. Despite its simplicity and lack of spatial context, SITS-DECO outperforms much larger EO foundation models on the PASTIS-R dataset, achieving up to 60.91% mIoU versus 43.13% for TerraMind. The method supports flexible task addition, minimal preprocessing, and robust handling of irregular, noisy, multi-modal data. Results highlight the importance of dense temporal modelling and validate a data-centric, token-sequence paradigm for future generative EO foundation models.

## Method Summary
SITS-DECO processes multi-temporal satellite image time series as flattened pixel-level sequences, mixing continuous sensor data (Sentinel-1 SAR, Sentinel-2 optical) with symbolic task tokens. The decoder-only transformer (2 blocks, 16 heads, 128-dim embeddings, ~465K parameters) predicts next tokens in this unified stream, enabling multiple tasks through symbolic prompting without task-specific heads. Temporal encoding uses "days since start" for irregular data, while multi-modal fusion emerges from concatenating raw modalities in sequence dimension. The model handles irregular, noisy, multi-modal data through minimal preprocessing—scaling, orbit pairing, and observation dropout—achieving crop-type classification on PASTIS-R without spatial context or complex encoders.

## Key Results
- Achieves 60.91% mIoU on PASTIS-R crop-type classification versus 43.13% for TerraMind
- Demonstrates implicit multi-modal fusion by concatenating S1/S2 sequences without explicit cross-attention
- Enables multiple supervised and self-supervised tasks through symbolic prompting in unified token sequence
- Shows dense temporal modeling provides higher predictive signal than spatial context for crop classification

## Why This Works (Mechanism)

### Mechanism 1: Unified Token Sequence as Task Interface
A generative decoder performs multiple EO tasks by framing them as continuations of a unified token sequence mixing continuous sensor data and symbolic task tokens. The transformer's self-attention disambiguates task contexts based purely on token order and symbolic identifiers, without specialized encoder/decoder structures. If tasks require conflicting representations that cannot be linearly projected into the shared 128-dim embedding space, task interference may degrade performance below single-task baselines.

### Mechanism 2: Dense Temporal Priority over Spatial Context
Dense temporal modeling provides higher predictive signal than spatial context for dynamic targets like crop classification. By flattening spatial patches and processing individual pixel time-series, the model retains fine-grained, irregular temporal dynamics that are often aggregated out in spatial patch embeddings of ViT-based models. This mechanism fails for tasks where spatial context is the primary signal or where temporal data is sparse/absent.

### Mechanism 3: Data-Centric Modality Fusion
Effective multi-modal fusion emerges from concatenating raw modalities in the sequence dimension rather than using complex cross-attention modules. The decoder treats different sensor modalities as sequential tokens and learns cross-modal relationships through self-attention by minimizing prediction loss across the unified sequence. Performance degrades if modalities are fundamentally misaligned in time or space beyond the model's capacity to infer.

## Foundational Learning

- **Concept: Teacher Forcing with Causal Masking**
  - Why needed: The model predicts next tokens based strictly on preceding context; understanding this is vital to grasp how it generates classification labels after seeing a time series.
  - Quick check: How does the model ensure it doesn't "cheat" by looking at future time steps when predicting the current token?

- **Concept: Symbolic vs. Continuous Tokens**
  - Why needed: The architecture merges distinct data types (floating point reflectance vs. integer class IDs) into one stream.
  - Quick check: How does the model calculate loss differently for a continuous regression token versus a symbolic classification token?

- **Concept: Positional Encoding for Irregular Time Series**
  - Why needed: Satellite data is rarely uniform; the model uses "days since start" to handle irregular cadence.
  - Quick check: Why is a learned or sinusoidal encoding of absolute time essential for this specific model compared to standard positional indices (1, 2, 3...)?

## Architecture Onboarding

- **Component map:** Input (flattened pixel time series + metadata + task identifiers) -> Backbone (2-block decoder-only transformer, 16 heads, 128-dim) -> Positional Encoding ("days since start" for data, index for symbolic) -> Head (single linear layer) -> Output (continuous channels + discrete vocab)

- **Critical path:** 1. Preprocessing: Scale S1/S2 data; concatenate S1 ascending/descending; append "days since start". 2. Tokenization: Construct sequence ([task token, data tokens, task token, categorical token, EOS]). 3. Forward Pass: Apply causal mask; compute attention across time and modalities. 4. Loss: Calculate CrossEntropy on symbolic tokens (and optionally MSE on continuous tokens).

- **Design tradeoffs:** Simplicity vs. SotA (uses only 2 decoder blocks vs. 12+ in standard LLMs, trading raw capacity for accessibility). Pixel-only vs. Spatial (sacrifices spatial context to focus purely on temporal density, resulting in lightweight inference but potential failure on texture-dependent tasks).

- **Failure signatures:** Task Interference (multitask settings show S1+S2 underperforming S2-only, suggesting capacity saturation or negative transfer). Regression Instability (initial experiments with forecasting failed due to temporal autocorrelation trivializing the task without date hints).

- **First 3 experiments:** 1. Overfit Single Patch: Train on one spatial patch to verify token pipeline and loss calculation converge. 2. Ablate Positional Encoding: Run classification with standard indices vs. "days since start" to validate importance of temporal encoding for irregular data. 3. Modality Drop: Train with S2-only vs. S1+S2 to replicate finding that fusion is implicitly learned through sequence attention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can regression tasks (e.g., forecasting, cloud gap imputation) be successfully implemented within the SITS-DECO framework?
- Basis in paper: Section 6.6 states authors "failed to get skilful results" with regression tasks, hypothesizing model size, noise, or lack of temporal hints as causes, and explicitly requests future work address these challenges.
- Why unresolved: The current model configuration cannot effectively predict continuous values for forecasting without specific "temporal hints" or cleaner data handling.
- What evidence would resolve it: Achieving skill in forecasting or imputation comparable to EarthPT using the base SITS-DECO architecture without relying on proprietary pre-processed data.

### Open Question 2
- Question: How can spatial context be effectively integrated into the unified token-sequence framework?
- Basis in paper: Section 7.4 proposes exploring "Pixel-level context via image patch tokens (ViT style)" or "Chip-level context" as key future directions to address lack of spatial awareness.
- Why unresolved: It is currently unknown if patch-based tokens or spatial embeddings can be unified with the temporal token stream without re-introducing architectural complexity or massive compute costs the current model avoids.
- What evidence would resolve it: An architectural extension incorporating patch-based tokens that improves performance on spatially-dependent segmentation benchmarks without sacrificing the model's lightweight nature.

### Open Question 3
- Question: What causes the performance degradation in the massively multi-task setting, and how can it be mitigated?
- Basis in paper: Section 6.5 notes "unexpected inconsistencies" where multimodal performance dropped below unimodal performance, identifying "task interference or model capacity" as likely causes.
- Why unresolved: It is unclear if the drop is due to insufficient model parameters or conflicting optimization signals between the 29 distinct tasks.
- What evidence would resolve it: Ablation studies on model scaling and loss weighting that restore or exceed expected multimodal performance in the multi-task configuration.

### Open Question 4
- Question: Can richer self-supervised strategies within the token-sequence framework close the geographic domain transfer gap?
- Basis in paper: Section 6.8 suggests further exploring "additional combinations of self supervised tasks" to mitigate domain transfer, noting current 10% uplift still leaves 20% gap to supervised models.
- Why unresolved: The current study was time-constrained; it is unknown if the framework's flexibility can be leveraged to fully adapt to new regions without labels using tasks like modality reconstruction.
- What evidence would resolve it: A training regime combining diverse SSL tasks that significantly reduces the mIoU gap between unsupervised domain adaptation and fully supervised baselines.

## Limitations

- Model's minimal architecture (2 decoder blocks, 128-dim embeddings) raises questions about generalizability to more complex EO tasks beyond crop classification
- Exceptional performance appears tightly coupled to PASTIS-R dataset characteristics; robustness across diverse EO datasets is not demonstrated
- Critical preprocessing steps lack full specification, particularly NDSI-based cloud probability formula and exact regression loss weighting

## Confidence

**High Confidence**: Unified token sequence framework successfully enables multitask learning without task-specific heads; Dense temporal modeling provides significant predictive signal for crop classification; Decoder-only architecture with minimal parameters can outperform larger foundation models on specific EO tasks.

**Medium Confidence**: Implicit multi-modal fusion mechanism generalizes beyond Sentinel-1/Sentinel-2 pairs; Data-centric approach scales to more complex EO tasks and datasets; Task interference in multitask settings is primarily due to capacity limitations rather than fundamental architectural flaws.

**Low Confidence**: Model's performance would remain superior on datasets with sparse temporal sampling or different sensor combinations; Minimal architecture can handle regression tasks effectively with appropriate modifications; Unified token sequence approach eliminates need for any task-specific architectural adaptations across full spectrum of EO problems.

## Next Checks

1. **Ablation on Spatial Context**: Systematically test model's performance degradation when spatial context is reintroduced (e.g., adding small convolutional layers or using larger spatial patches) to validate whether temporal density truly dominates spatial relationships across different task types.

2. **Cross-Dataset Generalization**: Evaluate SITS-DECO on a fundamentally different EO dataset (e.g., Sentinel-2 + Landsat time series for forest monitoring, or SAR-only for flood mapping) to test whether success is dataset-specific or indicates broader applicability.

3. **Capacity Scaling Study**: Incrementally increase model size (more decoder blocks, higher embedding dimensions) while monitoring multitask performance and interference patterns to clarify whether current limitations stem from intentional minimalism or insufficient capacity for complex EO problems.