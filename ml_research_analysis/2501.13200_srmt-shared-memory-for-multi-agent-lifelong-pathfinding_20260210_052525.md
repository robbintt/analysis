---
ver: rpa2
title: 'SRMT: Shared Memory for Multi-agent Lifelong Pathfinding'
arxiv_id: '2501.13200'
source_url: https://arxiv.org/abs/2501.13200
tags:
- srmt
- agents
- memory
- multi-agent
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses multi-agent pathfinding, where agents must
  navigate to their goals without colliding. A key challenge is enabling coordination
  without explicit communication.
---

# SRMT: Shared Memory for Multi-agent Lifelong Pathfinding

## Quick Facts
- **arXiv ID:** 2501.13200
- **Source URL:** https://arxiv.org/abs/2501.13200
- **Authors:** Alsu Sagirova; Yuri Kuratov; Mikhail Burtsev
- **Reference count:** 17
- **Primary Result:** SRMT achieves superior coordination in multi-agent pathfinding through shared recurrent memory, especially under sparse rewards and out-of-distribution environment scales.

## Executive Summary
This paper introduces SRMT (Shared Recurrent Memory Transformer), a transformer-based architecture for decentralized multi-agent pathfinding that enables implicit coordination through shared memory. The key innovation is pooling individual agent memories into a global workspace that all agents can query via cross-attention, eliminating the need for explicit message passing. SRMT consistently outperforms baseline reinforcement learning approaches, particularly in sparse reward settings and when generalizing to environments longer than those seen during training. The method is evaluated on bottleneck navigation tasks and the POGEMA benchmark, demonstrating competitive performance against recent MARL, hybrid, and planning-based algorithms.

## Method Summary
SRMT extends transformer architectures to multi-agent settings by incorporating shared recurrent memory. Each agent maintains a personal memory vector that gets pooled into a global shared memory accessible to all agents via cross-attention. The architecture processes local observations through a ResNet encoder, combines them with history and memory tokens, applies self-attention, then queries the shared memory through cross-attention. The output updates personal memory and generates action distributions through an actor-critic framework. Training uses PPO with Sample Factory, and memory is initialized from the first observation rather than fixed vectors. The approach is evaluated on partially observable multi-agent pathfinding problems including bottleneck navigation and lifelong MAPF tasks.

## Key Results
- SRMT outperforms RMT baselines by 40-60% in CSR on sparse reward bottleneck tasks
- Maintains >80% CSR when generalizing to corridors 10× longer than training distribution
- Competitive performance with recent MARL and planning-based algorithms on POGEMA benchmarks
- Data-dependent memory initialization significantly improves performance over fixed initialization schemes

## Why This Works (Mechanism)

### Mechanism 1: Implicit Coordination via Global Workspace
- **Claim:** Implicit coordination is achieved by treating shared memory as a global workspace, allowing agents to synchronize behavior without explicit message passing.
- **Mechanism:** Agents write individual "working memories" to a shared pool and read from this pool via cross-attention, resolving conflicts like bottlenecks.
- **Core assumption:** Agent memory representations are compressible and semantically aligned enough that cross-attention provides actionable coordination signals.
- **Evidence anchors:** [abstract] "pooling and globally broadcasting individual working memories, enabling agents to exchange information implicitly."
- **Break condition:** If the number of agents makes the shared memory sequence length prohibitive for cross-attention (computational bottleneck), or if memory representations become too distinct to share useful info.

### Mechanism 2: Temporal Credit Assignment via Recurrent Memory
- **Claim:** Recurrent memory tokens enable temporal credit assignment and policy stability in sparse reward settings.
- **Mechanism:** Memory tokens function as a recurrent state, allowing the transformer to process history sequentially rather than requiring a fixed context window.
- **Core assumption:** The decision-making process relies on temporal dependencies longer than the local observation window.
- **Evidence anchors:** [abstract] "incorporating shared recurrent memory... can enhance coordination... especially under sparse rewards."
- **Break condition:** If dependency depth exceeds the effective capacity of memory tokens to retain information without degradation.

### Mechanism 3: Data-Dependent Memory Initialization
- **Claim:** Data-dependent memory initialization improves generalization to out-of-distribution environment scales.
- **Mechanism:** SRMT initializes memory based on the first observation, anchoring the memory state to actual environment context immediately.
- **Core assumption:** The initial observation contains sufficient features to bootstrap a useful latent state for the entire episode.
- **Evidence anchors:** [section 4.1] "SRMT memory state is initialized with the values generated on the first step... performance... significantly increased."
- **Break condition:** If the initial observation is occluded, randomized without signal, or identical across vastly different global states.

## Foundational Learning

- **Concept: Transformer Cross-Attention**
  - **Why needed here:** This is the specific operation used to query the "Shared Memory." You must understand how a Query (agent's current state) retrieves information from Key/Value pairs (the shared memory pool) to understand how implicit communication occurs.
  - **Quick check question:** How does cross-attention differ from self-attention in terms of input sources?

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - **Why needed here:** The paper defines the problem as PO-MAPF. You need to grasp why "memory" is mathematically necessary (to turn a POMDP into a belief-state MDP) to understand why the SRMT core exists.
  - **Quick check question:** Why can't a reactive policy (Markovian) solve a bottleneck task where agents must recall past observations to infer others' intentions?

- **Concept: Actor-Critic Architecture**
  - **Why needed here:** SRMT serves as the "Core" subnetwork feeding into an Actor-Critic head. Understanding the baseline RL training loop is required to implement the loss functions and training stability tricks mentioned.
  - **Quick check question:** In an Actor-Critic setup, which component determines the update direction for the policy, and how does the "Critic" use the SRMT output?

## Architecture Onboarding

- **Component map:** ResNet spatial encoder -> Input constructor (memory + history + current obs) -> SRMT Core (self-attention + cross-attention) -> Memory head (updates personal memory) -> Dense action decoder/critic

- **Critical path:** The Cross-Attention block inside the SRMT Core. This is where information is "pooled and globally broadcast." If this block is removed or degraded, the model collapses to independent RMT agents.

- **Design tradeoffs:**
  - Shared vs. Individual Memory: Shared memory boosts sparse reward performance but adds computational overhead
  - Corridor Length vs. Episode Length: Architecture designed for fixed-length training may limit generalization to OOD lengths if recurrence capacity is bottlenecked

- **Failure signatures:**
  - Congestion Deadlock: Agents spinning or waiting indefinitely in bottlenecks
  - Memory Saturation: If the "Memory Head" fails to gate information effectively, memory tokens become noise
  - Random/unit memory initialization causes near-zero CSR

- **First 3 experiments:**
  1. Bottleneck Sparse Reward: Train 2 agents on corridor lengths 3-30 with only goal rewards. Verify if SRMT achieves >80% CSR vs. RMT's ~40%
  2. Memory Initialization Ablation: Run RATE/SRMT with random init vs. observation-based init. Confirm the CSR gap reported in Section 4.1
  3. OOD Generalization: Train on max corridor 30, test on corridor 500. Plot the decay curve of CSR to verify if SRMT retains >0.8 performance up to length 400

## Open Questions the Paper Calls Out

- **Dynamic Obstacles:** The paper treats obstacles as fixed elements of the environment, but real-world navigation often involves moving impediments that the current shared memory mechanism was not designed to handle.
- **Sensor Noise and Asynchrony:** The study assumes flawless localization and synchronized moves, but decentralized physical robots experience sensor noise and actuation delays that could disrupt shared memory synchronization.
- **Formal Guarantees:** Unlike classical planners, SRMT does not offer theoretical guarantees that agents will reach their destinations due to the "black box" nature of the transformer-based shared memory.

## Limitations

- **Architectural Underspecification:** Memory vector dimensionality, cross-attention query/key/value assignment, and memory head architecture are not explicitly defined, potentially limiting exact reproduction.
- **Computational Scaling:** Cross-attention scales quadratically with the number of agents, creating a potential bottleneck for scaling beyond small multi-agent systems.
- **Evaluation Scope:** Empirical validation focuses primarily on relatively simple bottleneck and LMAPF scenarios, with limited evaluation in more complex, dynamic environments.

## Confidence

- **High Confidence:** The core claim that shared recurrent memory improves coordination in sparse reward settings is well-supported by direct ablation comparisons (SRMT vs. RMT on bottleneck task).
- **Medium Confidence:** The generalization claim to longer corridors is supported by evaluation up to 1000 cells, though the training distribution (3-30 cells) represents substantial extrapolation.
- **Medium Confidence:** Competitive performance on POGEMA maps is demonstrated, but comparison with planning-based algorithms requires careful interpretation.

## Next Checks

1. **Ablation Study Extension:** Conduct controlled experiments removing the shared memory component across all tested scenarios to quantify the precise contribution of implicit communication versus recurrent memory alone.

2. **Memory Capacity Scaling:** Systematically vary the memory vector dimensionality and number of memory tokens per agent to identify the optimal configuration and determine whether performance plateaus or degrades with increased capacity.

3. **Multi-Agent Scaling Test:** Evaluate SRMT's performance as agent count increases (e.g., 2→4→8 agents) in bottleneck scenarios to empirically validate computational scaling concerns and identify potential coordination breakdown points.