---
ver: rpa2
title: 'Investigating Language Model Capabilities to Represent and Process Formal
  Knowledge: A Preliminary Study to Assist Ontology Engineering'
arxiv_id: '2509.10249'
source_url: https://arxiv.org/abs/2509.10249
tags:
- reasoning
- language
- grammar
- logical
- clif
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The research investigates whether formal knowledge representations
  can improve small language models' (SLMs) performance on first-order logic reasoning
  tasks. A methodology combining Syllogistic Evaluation Framework (SEF) for task classification
  and Common Logic Grammar Construction (CLGC) pipeline for transforming FOL representations
  was developed.
---

# Investigating Language Model Capabilities to Represent and Process Formal Knowledge: A Preliminary Study to Assist Ontology Engineering

## Quick Facts
- arXiv ID: 2509.10249
- Source URL: https://arxiv.org/abs/2509.10249
- Reference count: 39
- Primary result: Compact formal representations like CLIF achieve reasoning accuracy comparable to natural language baselines (~54%) with significantly smaller vocabularies

## Executive Summary
This research investigates whether formal knowledge representations can substitute for natural language in small language models (SLMs) for first-order logic reasoning tasks. A methodology combining the Syllogistic Evaluation Framework (SEF) for task classification and the Common Logic Grammar Construction (CLGC) pipeline for transforming FOL representations was developed. Multiple formal languages (CLIF, CGIF, TFL, TFL+, MINIFOL) were evaluated against natural language baselines using the FOLIO dataset. Results show that compact formal representations achieve performance comparable to natural language while using significantly smaller vocabularies, supporting the primary research question that more compact formal representations can maintain strong performance on reasoning tasks with SLMs.

## Method Summary
The study used the FOLIO dataset, split into 800 training, 203 validation, and 201 test rows. Data was transformed into 7 languages (NL, FOL, CLIF, CGIF, TFL, TFL+, MINIFOL) using a Python-based CLGC pipeline with Lark parser and BNF grammars. Models tested included Flan-T5-small/base/large, GPT-2, Phi-3.5-mini-instruct, and Gemma-2-2b-it. Three training modes were employed: Supervised Fine-Tuning (5 epochs without PEFT-LoRA, 10 epochs with PEFT-LoRA), Zero-Shot Prompting, and 8-Shot Prompting. PEFT-LoRA configuration used r=16, lora_alpha=32, target_modules=["q", "v"], lora_dropout=0.05, bias="none", task_type="SEQ_2_SEQ_LM".

## Key Results
- Flan-T5-small achieved 0.4384 accuracy on CLIF representation, matching natural language baseline performance
- Flan-T5-large achieved best overall SFT result of 0.6600 accuracy on natural language representation
- Tokenizer re-training showed promise only for smaller models (<500M params), degrading performance in larger models
- In-context grammar passing hindered performance, while supervised fine-tuning showed the highest and most stable results

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Compact formal languages (specifically CLIF) can substitute Natural Language (NL) in SLMs for logical reasoning tasks without significant performance degradation.

**Mechanism:** By converting verbose NL into a compact, structured formal grammar (CLIF), the model processes fewer tokens and less ambiguous syntactic structures. This reduces the reasoning burden to a pattern-matching problem over rigid logical operators (e.g., `forall`, `implies`), allowing smaller models (<3B parameters) to achieve competitive accuracy (0.4384) comparable to NL baselines.

**Core assumption:** The SLM has sufficient pre-training exposure to symbolic tokens or code-like structures to map the formal grammar to logical operations.

**Evidence anchors:**
- [abstract] "Results show that CLIF... achieves competitive performance to natural language... with Flan-T5-small achieving 0.4384 accuracy."
- [Section 4.2] Table 5 shows Flan-T5-small CLIF matching NL accuracy (0.4384) and outperforming complex formats like CGIF.
- [corpus] Weak direct evidence for CLIF specifically; however, neighbor "Knowledge Distillation with Structured Chain-of-Thought" supports the general viability of structured reasoning in SLMs.

**Break condition:** Performance collapses if the formal syntax exceeds the model's context window or involves nested structures too deep for the attention mechanism (e.g., poor results for CGIF).

### Mechanism 2
**Claim:** Supervised Fine-Tuning (SFT) is significantly more effective for learning formal reasoning grammars than Zero-Shot or Few-Shot prompting.

**Mechanism:** SFT allows the model to update its weights to internalize the specific "dialect" of the formal language (e.g., mapping `:-` to implication). In contrast, prompting relies on the model's pre-existing knowledge, which often fails to generalize to novel, strict grammars without gradient updates, leading to flat performance curves in small models.

**Core assumption:** Sufficient labeled training data (premises-conclusions pairs) exists to teach the specific formal syntax.

**Evidence anchors:**
- [Section 4.2] "For SRQ1, the best method shown empirically remains SFT as it provides the highest and most stable results."
- [Section 4.2] Tables 7 and 9 show ZS and FS results often lagging behind SFT, with some small models scoring 0.0 on complex grammars.
- [corpus] "Fine-Tuned Thoughts..." (neighbor paper) corroborates that fine-tuning SLMs is effective for complex reasoning, specifically in industrial settings.

**Break condition:** If the dataset is too small or the grammar is too complex (e.g., TFL for Flan-T5-small), SFT may result in unstable training or "flat" zero accuracy.

### Mechanism 3
**Claim:** Tokenizer re-training aids small models in learning compact formal languages but degrades performance in larger models.

**Mechanism:** Retraining the tokenizer (and restricting vocabulary size) prevents the fragmentation of formal tokens (e.g., splitting `forall` into sub-tokens), reducing sequence length for small models. However, for larger models, this specialization appears to destroy useful pre-trained semantic knowledge, leading to overfitting or capacity reduction.

**Core assumption:** The efficiency gain from shorter sequences outweighs the loss of general pre-trained knowledge for the specific task.

**Evidence anchors:**
- [Section 4.2] Table 12 shows Flan-T5-small accuracy increasing from 0.3596 to 0.4532 on TFL+ with tokenizer re-training, while Flan-T5-large drops from 0.5418 to 0.4039.
- [Section 4.2] "This method is not scalable and breaks down when tokenizer re-training is done [for larger models]... causing overfitting."
- [corpus] No direct corpus evidence for tokenizer re-training trade-offs in this specific context.

**Break condition:** Applying aggressive vocabulary resizing to models >1B parameters, which results in a net loss of reasoning capability.

## Foundational Learning

- **Concept:** **First-Order Logic (FOL) & Satisfiability**
  - **Why needed here:** The entire experimental framework relies on converting NL stories into FOL structures (premises) and determining the truth value of a conclusion.
  - **Quick check question:** Given premises $\forall x (A(x) \rightarrow B(x))$ and $A(c)$, can you derive $B(c)$?

- **Concept:** **Grammar Parsing (BNF - Backus-Naur Form)**
  - **Why needed here:** The CLGC pipeline uses BNF grammars to define the structure of target languages (CLIF, CGIF) and transform parse trees from one format to another.
  - **Quick check question:** How would you define a simple BNF rule for a logical implication `A implies B`?

- **Concept:** **Syllogistic Reasoning Types**
  - **Why needed here:** The Syllogistic Evaluation Framework (SEF) categorizes problems (Disjunctive, Hypothetical, Categorical) to diagnose where the model succeeds or fails.
  - **Quick check question:** What distinguishes a "Categorical" syllogism from a "Hypothetical" one in terms of logical operators?

## Architecture Onboarding

- **Component map:** FOLIO Dataset -> CLGC Pipeline -> Model (Flan-T5/Gemma) -> Training Loop -> SEF Evaluator
- **Critical path:**
  1. Define/Load BNF grammars for source (FOL) and target (CLIF/CGIF) languages.
  2. Execute CLGC pipeline to generate training data in the target formal language.
  3. Run SFT with standard tokenizers (avoid re-training unless using very small models <500M params).
  4. Validate on SEF categories to ensure reasoning isn't just memorizing "Disjunctive" patterns.

- **Design tradeoffs:**
  - **CLIF vs. NL:** CLIF offers compactness and potentially lower inference cost, but NL requires no data transformation pipeline.
  - **Tokenizer Re-training:** High risk of overfitting; only viable for small models on strictly formal tasks where general language capability is secondary.
  - **Model Size:** Larger models (Flan-T5-large) perform better overall but scale poorly with tokenizer re-training.

- **Failure signatures:**
  - **Flat Zero Accuracy:** If Flan-T5-small returns 0.0 accuracy on ZS/FS tasks for a specific grammar (e.g., TFL), the syntax is likely too alien for the pre-trained weights (Table 9).
  - **Context Saturation:** If using NL + FOL or complex CGIF, watch for performance drops due to longer sequences (Table 5).
  - **Grammar Context Interference:** In SFT, passing the BNF grammar in-context (ICGP) actually *hurt* performance (Table 11), likely cluttering the attention window.

- **First 3 experiments:**
  1. **Sanity Check (SFT):** Train Flan-T5-small on the standard FOLIO NL set to establish a baseline accuracy (expect ~0.43-0.47).
  2. **Formal Substitution (SFT):** Train the same model on CLIF-transformed data. Verify that accuracy remains within Â±2% of the NL baseline to confirm the substitution hypothesis.
  3. **Vocabulary Stress Test:** Attempt ZS inference on Flan-T5-small using TFL. If accuracy is 0, attempt SFT. If still 0, re-train tokenizer with reduced vocabulary (Size ~180) and retry.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does combining natural language with formal grammars (e.g., NL + CLIF) yield better reasoning performance than single-modality inputs?
- **Basis in paper:** [explicit] Section 5.1 states the authors will "explore mixed-input representations... to assess whether combining expressiveness with the structure of a formal grammar improves reasoning."
- **Why unresolved:** The current study only evaluated representations in isolation (NL or formal), not hybrids.
- **What evidence would resolve it:** Experiments comparing SLM accuracy on FOLIO tasks using hybrid inputs versus the single-grammar baselines established in this paper.

### Open Question 2
- **Question:** Can injecting high-level ontology knowledge (e.g., DOLCE) via formal languages enable SLMs to efficiently reuse knowledge for ontology extension?
- **Basis in paper:** [explicit] Section 5.1 notes plans to "explore injecting knowledge from a high-level ontology (e.g. DOLCE)... to efficiently reuse relevant knowledge learned by the model in an ontology extension pipeline."
- **Why unresolved:** The current work focused on satisfiability classification rather than the construction or extension of ontologies using external knowledge.
- **What evidence would resolve it:** Demonstrating an SLM successfully extending an ontology based on DOLCE concepts without performance degradation.

### Open Question 3
- **Question:** Do the advantages of compact formal languages (like CLIF) over complex ones (like CGIF) generalize to datasets beyond FOLIO?
- **Basis in paper:** [explicit] Section 5.2 identifies the "extension of the SEF-CLGC evaluation to datasets such as ProofWriter, RuleTaker, the Logical Entailment Dataset and SynLogic" as a future direction.
- **Why unresolved:** The reported results are derived solely from the FOLIO dataset, which has specific structural characteristics.
- **What evidence would resolve it:** Replicating the SEF-CLGC experiments on ProofWriter or RuleTaker to see if CLIF remains competitive with Natural Language.

## Limitations

- Results are based on a single dataset (FOLIO), limiting generalizability to real-world ontology engineering tasks with domain-specific terminology and complex hierarchical relationships
- Tokenizer re-training mechanism is not fully explained, with performance degradation in larger models occurring without clear diagnostic analysis
- Limited hyperparameter exploration with only one PEFT-LoRA configuration tested and fixed training schedules without learning rate tuning

## Confidence

- **Medium:** Core claim that compact formal representations can achieve reasoning accuracy comparable to natural language baselines (~54%)
- **Low:** Tokenizer re-training results showing performance degradation in larger models without clear mechanistic explanation
- **High:** Supervised fine-tuning methodology effectiveness for learning formal reasoning grammars

## Next Checks

1. **Cross-dataset validation:** Apply the CLIF transformation methodology to a second FOL dataset (e.g., from the same domain or a different reasoning benchmark) to verify that the ~54% accuracy performance is reproducible beyond FOLIO.

2. **Tokenizer sensitivity analysis:** Systematically vary vocabulary size and observe the accuracy curve for both small and large models. This would clarify whether performance degradation in larger models is monotonic with vocabulary reduction or exhibits a threshold effect.

3. **Mixed representation experiment:** Train models on a curriculum that alternates between NL and CLIF representations during training, then evaluate which representation the model prefers during inference. This would test whether models can learn to select the most efficient representation dynamically rather than being constrained to one format.