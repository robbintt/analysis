---
ver: rpa2
title: Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising
arxiv_id: '2511.14719'
source_url: https://arxiv.org/abs/2511.14719
tags:
- video
- synthetic
- arxiv
- photorealism
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a zero-shot structure-aware denoising framework
  for enhancing synthetic video realism. The method builds upon a pre-trained diffusion
  video model, using DDIM inversion to encode structural information from the source
  synthetic video into a noise latent.
---

# Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising

## Quick Facts
- arXiv ID: 2511.14719
- Source URL: https://arxiv.org/abs/2511.14719
- Reference count: 10
- Zero-shot framework enhances synthetic video realism while preserving structural consistency and small object details

## Executive Summary
This paper introduces a zero-shot framework for enhancing synthetic video realism by combining DDIM inversion with structure-aware denoising. The method leverages a pre-trained diffusion video model to encode structural information from synthetic videos into noise latents, then uses spatial conditioning maps (depth, segmentation, edges) and Classifier-Free Guidance to regenerate videos with improved photorealism. Evaluated on CARLA synthetic driving videos, the approach shows state-of-the-art performance in preserving object alignment, perceptual similarity, and photorealism compared to baselines.

## Method Summary
The framework uses DDIM inversion to encode source synthetic video structure into a noise latent, which serves as a semantic anchor during denoising. Spatial conditioning maps (depth, segmentation, edges) are extracted using auxiliary models and injected into the first three DiT blocks via ControlNet. The denoising stage uses Classifier-Free Guidance with dual prompts (inversion prompt for structure, positive prompt for photorealism) to regenerate the video. The method requires no fine-tuning and processes videos in 121-frame chunks.

## Key Results
- Outperforms Cosmos-Transfer1 and Wan2.1-VACE on CARLA synthetic driving videos
- Achieves superior object alignment (DINO/CLIP scores) and perceptual similarity (LPIPS)
- Particularly effective at preserving identity and color of small safety-critical objects like traffic lights and signs
- Ablation studies confirm importance of multi-condition guidance and inversion process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DDIM inversion encodes source video structure into a content-aware latent that anchors regeneration.
- Mechanism: Deterministic noise reversal through iterative noise addition computes a latent x_T preserving low-frequency structural information from the synthetic video.
- Core assumption: Inversion prompt accurately describes synthetic content and model's noise prediction is faithful to reconstruct structural information.
- Evidence anchors: Inversion process encodes structural information into final latent; related work uses similar inversion for structure preservation.

### Mechanism 2
- Claim: Multi-condition spatial guidance via ControlNet enforces structural alignment during denoising.
- Mechanism: Auxiliary models extract depth, segmentation, and Canny edge maps processed by parallel ControlNet branch injecting control signals into first three DiT blocks.
- Core assumption: Auxiliary models produce accurate maps from synthetic inputs; ControlNet weights are appropriately calibrated.
- Evidence anchors: Ablation shows removing edge map increases LPIPS; removing ControlNet entirely collapses photorealism.

### Mechanism 3
- Claim: Dual-prompt CFG with inverted latent enables style transfer while preserving semantics.
- Mechanism: Inversion stage uses inversion prompt describing synthetic content; denoising stage uses positive prompt for photorealism with CFG extrapolation away from negative prompt.
- Core assumption: Positive prompt elicits photorealistic style without contradicting source semantics; CFG scale is tuned correctly.
- Evidence anchors: CFG=7 optimal; higher values increase photorealism but degrade semantic fidelity.

## Foundational Learning

- **DDIM Inversion**
  - Why needed here: Understanding how deterministic noise reversal encodes image content into latents is essential for grasping why this method preserves structure better than noise-to-video generation.
  - Quick check question: Can you explain why starting denoising from an inverted latent (vs. random noise) preserves source video content?

- **ControlNet Architecture**
  - Why needed here: The method relies on parallel trainable branches that inject spatial conditions into frozen backbone models; understanding feature injection points and weighting is critical for debugging.
  - Quick check question: In Eq. 2, why might limiting control injection to blocks 1-3 (vs. all blocks) preserve high-frequency detail while enforcing structure?

- **Classifier-Free Guidance (CFG) Tradeoffs**
  - Why needed here: CFG scale directly controls the realism-vs-fidelity tradeoff; ablation shows this is the primary failure mode if misconfigured.
  - Quick check question: What happens to object identity preservation if CFG is set too high during the denoising stage?

## Architecture Onboarding

- **Component map**: Synthetic video → Auxiliary models (depth/seg/edge) → c_spatial maps → Videollama3 (caption generation) → inversion prompt + positive prompt → DDIM Inversion (with ControlNet-guided noise prediction) → inverted latent x_T → Denoising (CFG-guided with positive prompt + c_spatial via ControlNet) → Enhanced video

- **Critical path**: The inversion stage is where content preservation is determined; errors here propagate irrecoverably. The ControlNet weight (w_c) and CFG scale are the two levers that determine output quality.

- **Design tradeoffs**:
  - CFG scale: Higher → better photorealism, worse semantic fidelity
  - Control conditions: Adding blur as condition improves alignment but destroys photorealism
  - Window size: Fixed at 121 frames; longer videos require chunking with boundary artifacts

- **Failure signatures**:
  - Traffic light color distortion/washout → likely CFG too high or missing edge condition
  - Temporal discontinuities at frame boundaries → exceeding 121-frame window without overlap handling
  - Sign shape distortion → ControlNet weight too low or depth map errors
  - Overall desaturation → positive prompt lacking realism descriptors

- **First 3 experiments**:
  1. Reproduce CFG ablation on a 10-video subset: test CFG ∈ {3, 7, 11}, measure LPIPS and DINO alignment to verify tradeoff curve.
  2. Ablate single conditions: run inference with only depth, only segmentation, only edges to quantify each condition's contribution to LPIPS and object alignment.
  3. Test prompt sensitivity: vary positive prompt specificity on 5 diverse weather conditions to identify failure modes.

## Open Questions the Paper Calls Out

- **Question**: Does training on synthetic videos enhanced by this framework improve the performance of downstream autonomous driving tasks compared to training on raw synthetic data or real data?
  - Basis in paper: The authors state in the Discussion that future work will focus on validating downstream utility and determining if enhanced synthetic data can bridge the synthetic-to-real gap for training autonomous systems.
  - Why unresolved: Current evaluation focuses entirely on visual fidelity metrics without measuring efficacy for training perception networks on real-world test sets.
  - What evidence would resolve it: Comparative study benchmarking object detection model trained on "Enhanced CARLA" data versus "Raw CARLA" data, evaluated on real-world dataset like KITTI or Cityscapes.

- **Question**: How can the framework handle video sequences longer than the base model's fixed inference window (121 frames) without introducing temporal discontinuities?
  - Basis in paper: The authors list a limitation: it is constrained by the base model's fixed inference window (121 frames), requiring a chunk-based approach for longer videos, which can introduce temporal discontinuities at the boundaries.
  - Why unresolved: Current implementation necessitates processing videos in fixed-length chunks, leading to visible jumps or inconsistencies when stitching chunks together for long-form driving sequences.
  - What evidence would resolve it: Development of sliding-window or latent-overlap strategy that smooths transition between inference windows, validated by temporal consistency metrics measured across chunk boundaries.

- **Question**: Can the method be made robust to text prompts that conflict with the source video content to prevent generation of visual artifacts?
  - Basis in paper: The authors note that as a zero-shot model, it is sensitive to text prompts that conflict with the source video, which may occasionally produce small visual artifacts.
  - Why unresolved: Reliance on Classifier-Free Guidance means text prompt describing scene inconsistent with inverted latent forces model to hallucinate conflicting features, degrading output.
  - What evidence would resolve it: Ablation study using adversarial or mismatched prompts to test modified guidance mechanism that automatically dampens text conditioning when it diverges significantly from structural information encoded in inverted latent.

## Limitations
- Performance depends on accuracy of auxiliary models (depth, segmentation, edges) which may degrade under extreme synthetic conditions
- 121-frame window limit introduces boundary artifacts for longer videos with no overlap handling described
- Inversion process depends on caption quality from Videollama3 which may miss nuanced object identities
- Evaluated only on CARLA synthetic data and one downstream task, limiting generalizability claims

## Confidence

- **High confidence**: Core mechanism of using inverted latents as content anchors is well-supported by ablation showing DDIM inversion's necessity
- **Medium confidence**: Superiority over baselines demonstrated on CARLA data but may not generalize to other synthetic domains or tasks
- **Low confidence**: Zero-shot generalization claim across "diverse outdoor conditions" is based on CARLA's synthetic variations, not real-world diversity

## Next Checks

1. Test the method on a different synthetic video dataset (e.g., SYNTHIA or virtual KITTI) to verify cross-domain generalization of the structure-aware denoising approach.

2. Evaluate the impact of auxiliary model errors by intentionally degrading depth/segmentation maps and measuring the resulting LPIPS and DINO score degradation.

3. Implement and test overlap handling for videos longer than 121 frames to assess boundary artifact severity and whether simple overlap averaging suffices for temporal consistency.