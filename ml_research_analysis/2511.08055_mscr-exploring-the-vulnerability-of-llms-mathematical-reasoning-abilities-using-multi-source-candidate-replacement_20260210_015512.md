---
ver: rpa2
title: 'MSCR: Exploring the Vulnerability of LLMs'' Mathematical Reasoning Abilities
  Using Multi-Source Candidate Replacement'
arxiv_id: '2511.08055'
source_url: https://arxiv.org/abs/2511.08055
tags:
- llms
- original
- attack
- response
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MSCR, an automated adversarial attack method
  based on multi-source candidate replacement, to systematically evaluate the vulnerability
  of LLMs in mathematical reasoning tasks. By combining cosine similarity in the embedding
  space of LLMs, the WordNet dictionary, and contextual predictions from a masked
  language model, MSCR generates high-quality semantically similar candidates and
  perturbs a single word in the input question.
---

# MSCR: Exploring the Vulnerability of LLMs' Mathematical Reasoning Abilities Using Multi-Source Candidate Replacement

## Quick Facts
- **arXiv ID**: 2511.08055
- **Source URL**: https://arxiv.org/abs/2511.08055
- **Authors**: Zhishen Sun; Guang Dai; Haishan Ye
- **Reference count**: 40
- **Primary result**: Single-word adversarial perturbations can reduce LLM mathematical reasoning accuracy by up to 49.89% on GSM8K and 35.40% on MATH500 benchmarks

## Executive Summary
This paper introduces MSCR, an automated adversarial attack method that systematically evaluates LLM robustness in mathematical reasoning by perturbing single words in input questions. MSCR combines three semantic candidate sources—LLM embedding similarity, WordNet synonyms, and MLM context predictions—to generate high-quality perturbations that preserve human-perceived meaning while breaking model performance. Large-scale experiments on 12 models show dramatic accuracy drops (up to 49.89% on GSM8K) and significant response length increases (up to 2.14× average, with some cases exceeding 10×), indicating both robustness deficiencies and computational inefficiency in current LLMs' reasoning capabilities.

## Method Summary
MSCR operates through a multi-stage pipeline: first generating candidate replacement words by combining embedding space similarity (top-10 similar words from target LLM), WordNet synonyms (top-5 ranked by cosine similarity), and MLM context predictions (probability > 0.1 from bert-large-uncased); second filtering candidates through morphological and character-level constraints; third applying greedy sequential word-by-word replacement using local (embedding) or global (WordNet/MLM) strategies; and finally evaluating perturbed questions against original answers with a two-stage verification using the target LLM and commercial LLM validation. The method focuses on single-word perturbations that maintain semantic similarity while inducing reasoning failures.

## Key Results
- Single-word perturbations reduce LLM accuracy by up to 49.89% on GSM8K and 35.40% on MATH500
- Average response length increases by 2.14×, with some models showing length collapses exceeding 10×
- Perturbations cause both incorrect outputs and substantially longer reasoning paths, indicating reasoning inefficiency
- The multi-source candidate fusion approach outperforms individual sources in attack effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Multi-Source Semantic Candidate Fusion
Combining three distinct information sources produces more effective semantic perturbations than any single source alone. MSCR fuses embedding space similarity, WordNet lexical knowledge, and MLM context predictions to generate diverse, high-quality candidates that maintain semantic consistency while disrupting model reasoning.

### Mechanism 2: Single-Token Reasoning Cascade Failure
A single-word perturbation can catastrophically disrupt the mathematical reasoning process in LLMs. Models appear to rely on fragile surface-pattern matching rather than robust logical reasoning, where minimal perturbations disrupt learned patterns and cause incorrect reasoning paths.

### Mechanism 3: Perturbation-Induced Reasoning Path Destabilization
Adversarial perturbations cause models to generate significantly longer, more redundant reasoning chains, increasing computational costs while decreasing accuracy. When familiar patterns are disrupted, models attempt to re-ground reasoning through extended exploration and dead-end paths.

## Foundational Learning

- **Concept**: Semantic Adversarial Attacks
  - **Why needed here**: MSCR's core objective is generating perturbations that preserve human-perceived meaning while breaking model performance
  - **Quick check question**: Why is replacing "by" with "to" in "increased by 150%" more devastating than adding typos?

- **Concept**: Embedding Space Similarity
  - **Why needed here**: MSCR leverages cosine similarity in LLM embedding spaces as its first candidate source
  - **Quick check question**: Why might embedding-based synonyms differ from dictionary synonyms, and how does this benefit adversarial attacks?

- **Concept**: Masked Language Modeling (MLM)
  - **Why needed here**: MLMs like BERT provide context-aware predictions that ensure perturbations remain grammatically coherent
  - **Quick check question**: Why is context-aware replacement superior to synonym substitution for preserving sentence fluency?

## Architecture Onboarding

- **Component map**: Input Question -> Candidate Generation (3 Parallel) -> Candidate Filtering -> Perturbation Engine (Greedy) -> Two-Stage Evaluation -> Adversarial Sample
- **Critical path**: Candidate quality filtering determines attack success rate. Poor candidates either fail to induce errors or create nonsensical questions detectable by evaluation.
- **Design tradeoffs**:
  - Local vs. global replacement strategy: Embedding candidates replace single instances; WordNet/MLM replace all occurrences
  - Greedy vs. exhaustive search: Sequential word-by-word attack with early termination prioritizes efficiency
  - Secondary evaluation cost: Commercial LLM verification improves reliability but adds latency and expense
- **Failure signatures**:
  - Semantic drift: Perturbation changes problem meaning → different correct answer
  - Grammar breakdown: Incoherent perturbed question → model outputs confusion
  - Robust model: Correct answer maintained despite perturbation → attack failure
  - False positive: Model output differs but is actually correct → caught by secondary evaluation
- **First 3 experiments**:
  1. Baseline validation: Run MSCR on 100 GSM8K samples against Llama-3-8B. Measure attack success rate, average accuracy drop, semantic similarity scores, and perturbation location distribution
  2. Ablation study: Compare attack effectiveness using single sources vs. full MSCR. Hypothesis: fusion approach outperforms individual sources by 15-25% in success rate
  3. Response length analysis: For successful attacks, compute response length ratios across model families. Test hypothesis: reasoning models show larger length increases than general-purpose models

## Open Questions the Paper Calls Out
None

## Limitations
- Candidate quality filtering heuristics for embedding candidates are not fully specified, particularly morphological constraints and character validation
- Commercial LLM verification methodology lacks detailed prompt templates and exact answer parsing logic
- Response length interpretation doesn't clearly distinguish between genuine reasoning expansion versus model confusion or loop generation

## Confidence
- **High Confidence** (⭐⭐⭐): Single-word perturbations can cause significant accuracy drops across diverse LLM families. The core mechanism of semantic candidate fusion is plausible and technically sound.
- **Medium Confidence** (⭐⭐): Response length increases indicate reasoning inefficiency. The reported magnitude seems plausible given similar findings in reasoning model literature.
- **Low Confidence** (⭐): Commercial LLM verification results. Without the exact prompt and evaluation criteria, the reliability of secondary checks remains uncertain.

## Next Checks
1. Ablation study on candidate sources: Systematically compare attack success rates using embedding-only, WordNet-only, and MLM-only approaches against the full MSCR fusion
2. Semantic drift analysis: Manually audit 50 successful attacks to verify that perturbed questions maintain the original problem meaning
3. Response length distribution validation: Plot response length ratios across model families and identify outliers to test whether length correlates with reasoning degradation