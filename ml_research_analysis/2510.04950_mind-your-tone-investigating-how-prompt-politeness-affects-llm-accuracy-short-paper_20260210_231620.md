---
ver: rpa2
title: 'Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short
  paper)'
arxiv_id: '2510.04950'
source_url: https://arxiv.org/abs/2510.04950
tags:
- politeness
- polite
- very
- rude
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how varying levels of politeness in prompts
  affect LLM performance on multiple-choice questions. The authors created a dataset
  of 50 base questions, each rewritten into five politeness variants (Very Polite
  to Very Rude), and evaluated responses from ChatGPT 4o across 10 runs per variant.
---

# Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)

## Quick Facts
- arXiv ID: 2510.04950
- Source URL: https://arxiv.org/abs/2510.04950
- Reference count: 0
- Primary result: Impolite prompts yield higher accuracy (84.8% for Very Rude) than polite ones (80.8% for Very Polite) on MCQs

## Executive Summary
This study challenges the conventional wisdom that polite prompts improve LLM performance by testing how varying politeness levels affect accuracy on multiple-choice questions. The authors created 250 prompts (50 questions × 5 politeness variants) and evaluated responses from ChatGPT 4o across 10 runs per variant. Contrary to prior findings, rude prompts achieved the highest accuracy (84.8%) while polite prompts performed worst (80.8%). Paired t-tests confirmed these differences were statistically significant (p < 0.05) across all tone pairs. The results suggest that newer LLMs may respond differently to tonal variation than earlier models, highlighting the need for further research into pragmatic prompt features.

## Method Summary
The authors created a dataset of 50 base multiple-choice questions, each rewritten into five politeness variants ranging from Very Polite to Very Rude. Each prompt was structured with a standardized instruction block requesting single-letter responses, followed by one of the politeness prefixes and the question text. GPT-4o was called via API with 10 independent runs per variant (250 prompts × 10 runs = 2,500 API calls total). Responses were parsed to extract single letters, and accuracy was computed per tone level. Paired t-tests were used to establish statistical significance across all tone pairs.

## Key Results
- Rude prompts achieved highest accuracy (84.8% for Very Rude) while polite prompts performed worst (80.8% for Very Polite)
- Statistical significance confirmed via paired t-tests (p < 0.05) across all tone pairs
- Accuracy pattern showed inverse relationship between politeness and performance
- Results challenge prior findings that polite prompts improve LLM performance

## Why This Works (Mechanism)

### Mechanism 1: Perplexity-Mediated Performance
- Claim: Politeness prefixes may alter prompt perplexity, affecting how readily the model processes the task
- Mechanism: Different politeness phrasings change the statistical properties of the full prompt; prompts with lower perplexity may better align with training data distributions, potentially improving task execution
- Core assumption: The relationship between perplexity and accuracy follows a consistent pattern across politeness levels
- Evidence anchors: [section] Page 4: "One line of inquiry may be based on notions of perplexity as suggested by Gonen et al. (2022). They note that the performance of an LLM may depend on the language it is trained on, and lower perplexity prompts may perform the tasks better."
- Break condition: If rude prompts exhibit higher perplexity than polite ones yet still outperform them, perplexity alone cannot explain the effect

### Mechanism 2: Training Corpus Distribution Alignment
- Claim: Impolite/direct phrasing may more closely match the distribution of problem-solving contexts in pre-training data
- Mechanism: LLMs trained on internet-scale corpora may have stronger statistical associations between direct imperatives and factual/technical content, versus polite phrasing which may correlate with social rather than task-oriented contexts
- Core assumption: Training data contains systematic correlations between tone and content domain
- Evidence anchors: [section] Page 2: "Many believe that task performance is a matter of how similar the task data is to the pre-training data"
- Break condition: If analysis of pre-training data distributions shows no tone-content correlation

### Mechanism 3: Token Budget and Attention Dilution
- Claim: Longer politeness prefixes may add semantic noise that shifts attention weights away from core task elements
- Mechanism: Elaborate polite phrasing adds tokens that may dilute the effective attention allocated to the actual question, while shorter/ruder prefixes maintain sharper task focus
- Core assumption: Models allocate fixed attention budget across all input tokens
- Evidence anchors: [section] Page 4: "Perplexity is also related to the length of a prompt, and that is another factor worth consideration."
- Break condition: If neutral prompts (no prefix) perform worse than both polite and rude variants with comparable token counts

## Foundational Learning

- Concept: **Prompt Sensitivity**
  - Why needed here: The paper demonstrates that superficial linguistic variations cause measurable performance differences; understanding this is foundational to prompt engineering
  - Quick check question: If a model scores 80% on neutral prompts and 84% on rude prompts, is the 4-point difference meaningful or noise?

- Concept: **Paired Statistical Testing**
  - Why needed here: The paper uses paired t-tests to establish that observed accuracy differences exceed random variation
  - Quick check question: Why use a paired t-test rather than comparing raw accuracy percentages when evaluating prompt variants?

- Concept: **Training Data Influence on Inference Behavior**
  - Why needed here: The paper grounds its discussion in how pre-training data characteristics may explain unexpected tone sensitivity
  - Quick check question: Why might a model respond differently to "Would you kindly solve this?" versus "Solve this now"?

## Architecture Onboarding

- Component map: Base questions (50) → Politeness variants (5) → Prompt templates (250) → GPT-4o API → Response parsing → Accuracy calculation → Statistical testing
- Critical path: 1. Generate/politeness-transform base questions 2. Prepend tone prefix + append standardized instructions 3. Execute 10 runs per variant via API 4. Parse responses to extract single letter 5. Compute accuracy per tone level 6. Run paired t-tests across all tone pairs
- Design tradeoffs: Small controlled dataset (N=50) enables clean comparison but limits generalizability; single model focus provides depth but no cross-architecture conclusions; multiple-choice format yields unambiguous accuracy metrics but doesn't capture reasoning quality
- Failure signatures: High variance across runs within same politeness level (range > 4% suggests instability); p-values > 0.05 indicate differences within random variation; non-monotonic accuracy pattern suggests confounding variables
- First 3 experiments: 1. Cross-model replication: Run identical protocol on Claude and GPT-o3 to test whether the effect persists across architectures 2. Perplexity measurement: Calculate perplexity for each politeness variant to test whether it correlates with accuracy 3. Token-length control: Create length-matched variants across politeness levels to isolate semantic from length effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the inverse relationship between politeness and accuracy persist across different LLM architectures?
- Basis in paper: [explicit] The authors identify relying solely on ChatGPT-4o as a limitation, stating "future work should replicate our experiments across a broader set of models."
- Why unresolved: Different training corpora and alignment techniques (e.g., RLHF variations) may cause models to interpret tonal cues differently.
- What evidence would resolve it: Replicating the benchmark on diverse architectures (e.g., Llama, Claude) to determine if the trend is universal or model-specific.

### Open Question 2
- Question: Is the improved accuracy for rude prompts driven by lower perplexity or prompt length rather than tone?
- Basis in paper: [explicit] The discussion suggests "notions of perplexity" and prompt length as factors worth consideration, noting performance may depend on training data distributions.
- Why unresolved: The study did not control for or measure the perplexity of the specific prefixes used, so the mechanism remains unclear.
- What evidence would resolve it: An ablation study controlling for token count and calculating the perplexity of prompt variants to see if it correlates with accuracy.

### Open Question 3
- Question: Can the accuracy gains from rude prompts be achieved without using toxic or adversarial language?
- Basis in paper: [explicit] The authors state future work should "explore ways to achieve the same gains without resorting to toxic or adversarial phrasing."
- Why unresolved: It is unclear if the performance gain stems from the emotional payload of rudeness or simply the directness of the imperative instructions.
- What evidence would resolve it: Testing prompts that use direct, imperative syntax without hostile sentiment to see if they match the accuracy of "Very Rude" variants.

## Limitations
- Small sample size (N=50 questions) limits generalizability across domains and question types
- Exclusive focus on GPT-4o prevents conclusions about whether this represents a broader architectural pattern
- Politeness transformation protocol remains underspecified in terms of which exact prefix variant was applied to each question
- Does not control for token count across politeness levels, leaving open whether rude prompts simply provide more efficient attention allocation

## Confidence

**High Confidence** (mechanistic plausibility well-supported):
- The statistical methodology (paired t-tests) is appropriate and correctly applied
- The accuracy differences across politeness levels are measurable and consistent
- The experimental design cleanly isolates politeness as the primary variable

**Medium Confidence** (evidence present but incomplete):
- The explanation linking rude prompts to training data alignment is plausible but untested
- The observation that newer models may respond differently to tone than earlier findings suggested
- The potential role of perplexity and token efficiency in driving performance differences

**Low Confidence** (speculation without direct evidence):
- The specific mechanism by which rude phrasing improves accuracy (whether through attention allocation, perplexity, or training data alignment)
- Whether these findings generalize beyond multiple-choice questions or the specific politeness spectrum tested
- The claim that this represents a fundamental shift in how newer LLMs process tone versus earlier models

## Next Checks

1. **Cross-Model Replication**: Run the identical protocol on Claude-3.5-Sonnet and GPT-o3 to determine whether the rude-over-polite accuracy advantage persists across architectures, or whether this is specific to GPT-4o's training or architecture.

2. **Perplexity Measurement and Control**: Calculate the perplexity of each politeness variant to test whether the observed accuracy differences correlate with perplexity scores. Additionally, create length-matched variants across politeness levels to isolate semantic from length effects.

3. **Training Data Distribution Analysis**: Analyze whether the pre-training corpora contain systematic correlations between tone and content domain (e.g., whether imperative phrasing correlates with technical/problem-solving contexts versus polite phrasing correlating with social discourse). This would test the mechanism hypothesis directly.