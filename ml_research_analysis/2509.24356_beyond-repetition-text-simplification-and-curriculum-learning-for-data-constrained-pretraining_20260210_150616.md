---
ver: rpa2
title: 'Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained
  Pretraining'
arxiv_id: '2509.24356'
source_url: https://arxiv.org/abs/2509.24356
tags:
- data
- simp
- text
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how text complexity and curriculum scheduling
  affect language model pretraining in data-constrained settings. Using a parallel
  corpus of human-written and LLM-simplified paragraphs, the authors compare four
  training schedules: repeated exposure, interleaving, simple-to-complex curriculum,
  and complex-to-simple curriculum.'
---

# Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining

## Quick Facts
- arXiv ID: 2509.24356
- Source URL: https://arxiv.org/abs/2509.24356
- Authors: Matthew Theodore Roque; Dan John Velasco
- Reference count: 23
- This paper investigates how text complexity and curriculum scheduling affect language model pretraining in data-constrained settings, finding that interleaving simplified and original text consistently improves performance over repeated exposure.

## Executive Summary
This study explores how text complexity and curriculum scheduling impact language model pretraining efficiency when training data is limited. Using a parallel corpus of human-written and LLM-simplified paragraphs, the authors compare four training schedules: repeated exposure, interleaving, simple-to-complex curriculum, and complex-to-simple curriculum. The research reveals that adding simplified data consistently outperforms repeated exposure, with curriculum effects varying by model size. Smaller models (124M parameters) benefit from simple-to-complex ordering, while larger models (256M parameters) perform better with interleaving. These findings suggest that text simplification and curriculum learning can enhance sample efficiency and generalization in data-constrained pretraining scenarios.

## Method Summary
The authors construct a parallel corpus of human-written (HW) and simplified paragraphs by using GPT-4 to simplify Wikipedia text. They pretrain Transformer-based language models of 124M and 256M parameters on a subset of 1.5M paragraphs using four curriculum schedules: repeated exposure (same data four times), interleaving (alternating simplified and original), simple-to-complex (simplified first, then original), and complex-to-simple (original first, then simplified). Training progresses through these schedules while measuring perplexity on held-out validation data. Models are then fine-tuned on downstream tasks including SQuAD and GLUE benchmarks, with zero-shot evaluation also performed to assess generalization. The curriculum design allows systematic comparison of how text complexity ordering affects learning dynamics and final performance.

## Key Results
- Adding simplified data consistently improves performance over repeated exposure, with INTERLEAVED leading at 256M (+1.8-1.9 over baseline) and SIMPâ†’HW leading at 124M (+0.5-0.8 over baseline)
- Model-size-dependent curriculum effects: smaller models benefit from simple-to-complex ordering while larger models perform better with interleaving
- Zero-shot results show mixed but generally positive effects of simplification, with curriculum benefits most evident for smaller models
- Text simplification and curriculum learning can enhance sample efficiency and generalization when pretraining data is limited

## Why This Works (Mechanism)
The mechanism behind curriculum learning's effectiveness in this context relates to the way text complexity affects learning dynamics. When models are exposed to simpler text first, they can establish foundational language patterns without being overwhelmed by complex syntax and vocabulary. This gradual increase in difficulty allows the model to build robust representations that transfer better to complex text. The interleaving approach appears to work particularly well for larger models because it provides regular "breaks" from complexity, preventing the model from getting stuck in local minima when processing difficult text. For smaller models with limited representational capacity, starting with simpler text helps them avoid catastrophic forgetting of basic patterns when later exposed to complex structures. The consistent improvement over repeated exposure suggests that curriculum learning helps models extract more information from the same data by presenting it in an order that matches the learning process.

## Foundational Learning
- **Language Model Pretraining**: Understanding how models learn language representations from raw text is fundamental to grasping why curriculum ordering matters
  - *Why needed*: Curriculum learning builds on the assumption that learning order affects representation quality
  - *Quick check*: Can you explain how masked language modeling differs from next-token prediction?

- **Text Simplification**: The process of reducing linguistic complexity while preserving meaning is central to this work
  - *Why needed*: Simplified text serves as the "easy" examples in the curriculum
  - *Quick check*: What linguistic features typically change during text simplification?

- **Curriculum Learning**: The pedagogical principle that learning tasks in increasing order of difficulty can improve outcomes
  - *Why needed*: This is the core experimental design being tested
  - *Quick check*: How does curriculum learning differ from standard shuffled training?

- **Perplexity**: The standard metric for evaluating language model quality on held-out data
  - *Why needed*: Used to monitor pretraining progress and compare model performance
  - *Quick check*: What does a lower perplexity score indicate about model performance?

- **Downstream Task Evaluation**: Measuring model utility on specific tasks like question answering and natural language inference
  - *Why needed*: Final evaluation of whether pretraining improvements transfer to practical applications
  - *Quick check*: Why is fine-tuning necessary after pretraining?

## Architecture Onboarding
**Component Map**: Data Preprocessing -> Curriculum Scheduling -> Transformer Pretraining -> Fine-tuning -> Evaluation
**Critical Path**: The most important components are the curriculum scheduling mechanism and the Transformer architecture, as these directly determine how models learn from the simplified and original text.
**Design Tradeoffs**: The study trades off between exploration (trying different curriculum orders) and exploitation (focusing on the most promising schedules), while also balancing model size against curriculum complexity.
**Failure Signatures**: If curriculum learning fails, we would expect to see: no improvement over repeated exposure, increased perplexity during pretraining, or degraded downstream performance compared to standard pretraining.
**First Experiments**:
1. Compare repeated exposure vs. single exposure to verify data repetition isn't the primary driver of improvements
2. Test different interleaving ratios (e.g., 1:1, 2:1, 3:1 simplified:original) to optimize the balance
3. Evaluate whether human-simplified text produces different results than LLM-simplified text

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Experiments focus exclusively on models with 124M and 256M parameters, limiting generalizability to larger models where scaling laws typically dominate
- Evaluation relies on perplexity and fine-tuning accuracy on specific tasks, which may not capture broader generalization capabilities
- Simplified text was generated using GPT-4, introducing potential biases from a single model's simplification strategy rather than diverse human simplification patterns
- The curriculum effects represent relatively modest improvements of 0.5-1.9 percentage points in downstream performance, suggesting practical impact may be limited compared to simply acquiring more training data

## Confidence
- **High Confidence**: The core finding that interleaving simplified and original text improves pretraining over pure repetition is well-supported by consistent results across both model sizes and evaluation metrics
- **Medium Confidence**: The model-size-dependent curriculum effects are observed but require cautious interpretation due to the limited parameter size range
- **Low Confidence**: The zero-shot evaluation results show high variance across tasks, making it difficult to draw definitive conclusions about simplification's impact on generalization

## Next Checks
1. Test curriculum scheduling effects across a broader range of model scales (512M, 1B, 3B parameters) to determine whether the observed size-dependent patterns persist
2. Evaluate models on additional low-resource languages and domains to assess whether simplification benefits generalize beyond English Wikipedia-style text
3. Compare curriculum-based pretraining against alternative data augmentation strategies (back-translation, paraphrasing, synthetic data generation) using identical computational budgets