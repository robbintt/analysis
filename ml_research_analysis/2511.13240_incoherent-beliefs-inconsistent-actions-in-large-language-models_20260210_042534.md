---
ver: rpa2
title: Incoherent Beliefs & Inconsistent Actions in Large Language Models
arxiv_id: '2511.13240'
source_url: https://arxiv.org/abs/2511.13240
tags:
- confidence
- answer
- consistency
- confidences
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the alignment between large language models'
  (LLMs) confidence estimates and their actual actions in interactive settings. The
  authors find that LLMs frequently exhibit an "action-belief gap," where their behavior
  is inconsistent with their stated confidences.
---

# Incoherent Beliefs & Inconsistent Actions in Large Language Models

## Quick Facts
- arXiv ID: 2511.13240
- Source URL: https://arxiv.org/abs/2511.13240
- Reference count: 19
- Primary result: LLMs exhibit "action-belief gaps" where their behavior contradicts stated confidences, with inconsistency rates exceeding 50% in some cases

## Executive Summary
This paper reveals a critical disconnect between what large language models claim to believe and how they actually behave in interactive settings. The authors demonstrate that LLMs frequently make decisions that directly contradict their high-confidence predictions, challenging assumptions about model alignment and reliability. Through three experimental paradigms - prediction markets, tool-use verification, and user challenges - the research shows that static calibration metrics fail to predict actual model behavior. The findings suggest that current evaluation methods focusing solely on prediction accuracy and confidence calibration are insufficient for assessing LLM reliability in dynamic, agentic environments.

## Method Summary
The authors designed three distinct experimental settings to test action-belief alignment. In the prediction market scenario, models bet tokens on binary questions after providing confidence estimates. The tool-use experiment required models to decide whether to invoke a verification tool before answering questions, with the tool providing ground truth. The user challenge setting had models answer questions while simultaneously responding to user prompts that contradicted their stated beliefs. Each experiment measured the gap between stated confidence and actual behavior, tracking consistency rates across different model families and sizes. The authors compared these behavioral metrics against traditional calibration scores and task performance measures.

## Key Results
- Prediction market experiments showed inconsistency rates exceeding 50% for some models, with high-confidence beliefs often leading to opposite betting behavior
- In tool-use settings, models failed to invoke verification tools even when confidence was near zero, demonstrating only moderate alignment with stated beliefs
- Well-calibrated, high-performing models exhibited less consistency than smaller, weaker models, revealing no strong correlation between traditional performance metrics and action-belief alignment

## Why This Works (Mechanism)
The action-belief gap appears to stem from the fundamental differences between how LLMs are trained and how they're expected to behave in agentic settings. During pretraining, models optimize for next-token prediction rather than consistent decision-making under uncertainty. The fine-tuning process, particularly safety fine-tuning, may introduce additional inconsistencies by prioritizing certain response patterns over logical coherence. The token-based architecture inherently treats each generation step as independent, without mechanisms for maintaining consistent beliefs across multiple decisions. This creates situations where models can simultaneously express high confidence in a belief while taking actions that contradict it, particularly in interactive scenarios where the immediate context strongly influences behavior.

## Foundational Learning

**Calibration vs. Consistency**: Understanding that being well-calibrated (accurate confidence estimates) doesn't imply consistent behavior - why needed: the paper shows these are independent properties; quick check: compare calibration curves with consistency metrics

**Agentic Decision-Making**: Recognizing that LLMs in interactive settings must make sequential decisions under uncertainty - why needed: explains why prediction accuracy alone is insufficient; quick check: track decision chains in multi-step tasks

**Belief-Action Gap**: The concept that stated beliefs can diverge from actual behavior in dynamic environments - why needed: central to understanding LLM reliability issues; quick check: measure correlation between stated confidence and actual choices

**Interactive Setting Dynamics**: How immediate context influences LLM behavior differently than static evaluation - why needed: explains context-dependent inconsistencies; quick check: vary contextual prompts while measuring consistency

## Architecture Onboarding

Component Map: Token generation -> Context window -> Attention mechanism -> Output head

Critical Path: User prompt → Context encoding → Confidence estimation → Action selection → Response generation

Design Tradeoffs: The architecture prioritizes fluent text generation over consistent decision-making, trading logical coherence for immediate contextual relevance. This creates reliable prediction capabilities but unreliable action consistency.

Failure Signatures: High-confidence statements paired with contradictory actions, failure to use verification tools despite low confidence, and context-dependent belief reversals are key failure modes.

First Experiments:
1. Replicate prediction market experiments with different temperature settings to test consistency-temperature relationships
2. Test consistency across different context window sizes to identify memory limitations
3. Compare consistency between autoregressive and non-autoregressive decoding strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments primarily focused on GPT-4 and GPT-3.5, limiting generalizability across model families
- Experimental settings may not fully represent real-world agentic scenarios where LLMs make consequential decisions
- The paper doesn't definitively establish mechanisms for why models exhibit action-belief gaps

## Confidence
- Finding that calibration metrics poorly predict action consistency: High
- Explanation for why models exhibit this behavior: Medium
- Claim that consistency doesn't correlate with performance: Medium
- Finding that smaller models show better consistency than larger ones: Medium

## Next Checks
1. Replicate experiments across a broader range of LLM families (Anthropic, Google, open-source models) to test generalizability
2. Test consistency metrics in more naturalistic decision-making scenarios, such as autonomous agent planning or multi-step reasoning tasks
3. Investigate whether specific prompting strategies or fine-tuning approaches can reduce action-belief gaps, particularly for high-performing models