---
ver: rpa2
title: Finding Stable Subnetworks at Initialization with Dataset Distillation
arxiv_id: '2503.17905'
source_url: https://arxiv.org/abs/2503.17905
tags:
- data
- training
- pruning
- subnetworks
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using dataset distillation to improve neural
  network pruning. The authors introduce "distilled pruning," which leverages synthetic,
  condensed datasets to identify stable subnetworks during iterative magnitude pruning.
---

# Finding Stable Subnetworks at Initialization with Dataset Distillation
## Quick Facts
- arXiv ID: 2503.17905
- Source URL: https://arxiv.org/abs/2503.17905
- Reference count: 5
- Primary result: Distilled pruning finds sparse, stable subnetworks using 150x fewer training points than traditional lottery ticket methods

## Executive Summary
This paper introduces "distilled pruning," a method that uses dataset distillation to improve neural network pruning. By leveraging synthetic, condensed datasets, the approach identifies stable subnetworks during iterative magnitude pruning that remain connected in the loss landscape. Unlike traditional lottery ticket methods that struggle with instability in the loss landscape, distilled pruning achieves comparable performance to traditional methods while using 150x fewer training points on ResNet-18 and CIFAR-10. The method also enables finding lottery tickets at significantly higher sparsities (up to 10x fewer parameters) than previously possible.

## Method Summary
The authors combine dataset distillation with iterative magnitude pruning to create a more stable pruning process. They generate synthetic, condensed datasets through dataset distillation techniques, then use these distilled datasets to guide the pruning process. During each pruning iteration, they evaluate parameter importance using the distilled data and remove parameters that contribute to sharp loss landscapes. The process continues iteratively, with the pruned network being retrained after each pruning step. By using synthetic data instead of the full training set, they dramatically reduce computational requirements while maintaining or improving the quality of the found subnetworks.

## Key Results
- Achieves comparable accuracy to traditional lottery ticket methods using 150x fewer training points on ResNet-18/CIFAR-10
- Finds lottery tickets at sparsities up to 10x higher than previously possible (significantly fewer parameters)
- Demonstrates improved stability through linear mode connectivity studies, showing pruned subnetworks remain connected in the loss landscape

## Why This Works (Mechanism)
The method works by identifying and removing parameters that contribute to sharp loss landscapes, which are known to be less stable during training. By using distilled data to guide the pruning process, the approach can more effectively identify which parameters are truly important for the model's function versus those that merely contribute to loss landscape sharpness. The synthetic nature of the distilled data allows for more efficient evaluation of parameter importance while still capturing the essential characteristics needed for identifying stable subnetworks.

## Foundational Learning
**Dataset Distillation**: A technique for creating small synthetic datasets that capture the essential information of a larger dataset
- Why needed: Enables efficient evaluation of model behavior with dramatically reduced computational cost
- Quick check: Can the distilled dataset maintain reasonable training performance compared to the full dataset?

**Iterative Magnitude Pruning (IMP)**: A pruning method that repeatedly trains, prunes, and retrains a network
- Why needed: Allows identification of truly important parameters that survive multiple pruning iterations
- Quick check: Does the method maintain accuracy across multiple pruning cycles?

**Linear Mode Connectivity**: A measure of whether two networks can be linearly interpolated without increasing loss
- Why needed: Indicates whether pruned subnetworks occupy stable regions of the loss landscape
- Quick check: Can pruned and unpruned networks be interpolated without loss increase?

**Loss Landscape Sharpness**: Measures the curvature of the loss function around a minimum
- Why needed: Sharp minima are less stable and generalize worse than flat minima
- Quick check: Does pruning reduce sharpness as measured by Hessian eigenvalues?

**Hessian Analysis**: Uses second-order derivatives to understand loss landscape geometry
- Why needed: Provides quantitative measure of sharpness and parameter importance
- Quick check: Do removed parameters correspond to directions of high curvature?

## Architecture Onboarding
**Component Map**: Distilled Dataset -> Iterative Pruning Loop -> Parameter Importance Evaluation -> Subnetwork Extraction -> Stability Verification
**Critical Path**: Dataset Distillation → Pruning Decision → Retraining → Validation
**Design Tradeoffs**: Computational efficiency (150x reduction) vs. potential synthetic data artifacts; stability vs. maximum achievable sparsity
**Failure Signatures**: Loss landscape disconnection, degraded accuracy despite sparsity, overfitting to synthetic data characteristics
**First Experiments**: 1) Compare distilled vs. full dataset pruning performance; 2) Test stability under data augmentation; 3) Evaluate adversarial robustness of found subnetworks

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Synthetic data generalization limits may affect performance on datasets beyond CIFAR-10/ResNet-18
- 150x data reduction may trade off some robustness for efficiency
- Hessian-based sharpness analysis assumes second-order information is well-approximated, which may not hold for very deep architectures
- Stability claims are validated only through linear mode connectivity, not through data shift or adversarial robustness

## Confidence
**High**: Empirical results showing comparable accuracy with 150x fewer training points and ability to find tickets at higher sparsities are well-supported
**Medium**: Claim that distilled pruning targets sharp-loss-contributing parameters is plausible but needs more rigorous ablation studies
**Low**: Generalizability to other architectures, tasks, or dataset distributions remains largely untested

## Next Checks
1. Test distilled pruning on datasets with larger domain shifts (e.g., ImageNet, medical imaging) to assess synthetic data generalization limits
2. Conduct ablation studies removing the Hessian-based sharpness component to isolate its contribution to stability
3. Evaluate subnetworks found via distilled pruning under adversarial attacks to test if "stable" in the loss landscape sense translates to robustness against worst-case perturbations