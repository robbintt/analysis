---
ver: rpa2
title: 'MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models
  on Italian Medical Entrance Examinations'
arxiv_id: '2509.07135'
source_url: https://arxiv.org/abs/2509.07135
tags:
- italian
- medbench-it
- arxiv
- reasoning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedBench-IT is the first comprehensive benchmark for evaluating
  large language models on Italian medical university entrance examinations. It contains
  17,410 expert-written multiple-choice questions across six subjects and three difficulty
  levels, curated from a leading Italian publisher.
---

# MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations

## Quick Facts
- arXiv ID: 2509.07135
- Source URL: https://arxiv.org/abs/2509.07135
- Reference count: 40
- Primary result: First comprehensive benchmark for evaluating LLMs on Italian medical university entrance exams with 17,410 expert-written multiple-choice questions

## Executive Summary
MedBench-IT is the first comprehensive benchmark designed specifically to evaluate large language models on Italian medical university entrance examinations. The benchmark contains 17,410 expert-written multiple-choice questions across six subjects (Biology, Chemistry, Logic, Mathematics, Physics, General Culture) and three difficulty levels, curated from a leading Italian publisher's materials. It provides standardized evaluation for both proprietary models (including GPT-4o, Claude, and DeepSeek variants) and resource-efficient open-source alternatives using standard and reasoning-eliciting prompts.

The benchmark was used to evaluate model performance across multiple dimensions including overall accuracy, subject-specific performance, reproducibility, and the impact of reasoning prompts. Results showed that top proprietary models achieved approximately 90% accuracy, while the best open-source models exceeded 70%. Logic and Mathematics were consistently the most challenging subjects for all models. The benchmark also revealed minimal ordering bias and demonstrated that explicit reasoning prompts provided little benefit for top-performing models, suggesting these models may already incorporate implicit reasoning capabilities.

## Method Summary
The MedBench-IT benchmark was constructed using 17,410 expert-written multiple-choice questions from a leading Italian publisher's materials for medical entrance exam preparation. Questions were systematically categorized across six subjects and three difficulty levels (basic, intermediate, advanced). The evaluation framework included both proprietary models (GPT-4o, Claude variants, DeepSeek models) and open-source models under 30 billion parameters. Standard evaluation prompts and reasoning-elicited prompts (chain-of-thought) were employed. Reproducibility was tested through multiple evaluations with GPT-4o, and ordering bias was assessed by randomizing question sequences. Performance metrics included accuracy rates, consistency measures, and subject-specific analysis.

## Key Results
- Top proprietary models achieved approximately 90% accuracy on the benchmark
- Best open-source models exceeded 70% accuracy despite being under 30B parameters
- Logic and Mathematics were consistently the most challenging subjects across all model evaluations

## Why This Works (Mechanism)
MedBench-IT provides a standardized evaluation framework that captures the specific knowledge domains and reasoning patterns required for Italian medical entrance examinations. The benchmark's structure, with questions spanning multiple subjects and difficulty levels, allows for comprehensive assessment of both factual knowledge and analytical reasoning capabilities. The use of expert-written questions from a leading publisher ensures content validity and alignment with actual exam requirements. The inclusion of both standard and reasoning-elicited prompting strategies enables evaluation of different cognitive approaches, while reproducibility testing ensures reliability of results across multiple evaluation runs.

## Foundational Learning

**Italian medical education system** - Understanding the structure and requirements of medical entrance exams in Italy
*Why needed*: Provides context for benchmark relevance and question selection
*Quick check*: Verify alignment with actual exam specifications and curricula

**Multiple-choice question design principles** - Knowledge of effective MCQ construction and evaluation
*Why needed*: Ensures benchmark questions properly assess intended competencies
*Quick check*: Review question quality and validity through expert evaluation

**LLM evaluation methodologies** - Familiarity with benchmarking approaches and performance metrics
*Why needed*: Enables proper interpretation of results and comparison across models
*Quick check*: Validate evaluation methodology against established standards

**Prompt engineering techniques** - Understanding of different prompting strategies and their effects
*Why needed*: Critical for interpreting reasoning prompt results and their impact
*Quick check*: Test prompt variations and measure consistency

## Architecture Onboarding

**Component map**: Question Bank -> Model Input -> Prompt Processing -> Response Generation -> Answer Evaluation -> Performance Analysis -> Subject/Difficulty Breakdown

**Critical path**: Question Bank → Model Input → Prompt Processing → Response Generation → Answer Evaluation → Performance Metrics

**Design tradeoffs**: Single publisher source vs. broader coverage (content consistency vs. potential bias), standard vs. reasoning prompts (simplicity vs. deeper analysis), proprietary vs. open-source models (performance ceiling vs. accessibility)

**Failure signatures**: 
- Low consistency across evaluation runs indicates prompt sensitivity or model instability
- Subject-specific performance drops suggest knowledge gaps or reasoning difficulties
- Minimal improvement with reasoning prompts may indicate insufficient prompt engineering

**First experiments**:
1. Run benchmark with different prompting strategies to establish baseline performance patterns
2. Conduct reproducibility tests with multiple evaluation runs to measure consistency
3. Analyze subject-specific performance to identify knowledge gaps and reasoning challenges

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on questions from a single publisher, potentially introducing bias in question style and content coverage
- Reasoning assessment through chain-of-thought prompting may not fully capture true reasoning abilities
- Focus on Italian medical entrance exams limits applicability to other medical education contexts or languages

## Confidence
High confidence in: Benchmark construction methodology, quantitative evaluation results, comparison between proprietary and open-source models

Medium confidence in: Generalizability to other Italian medical education contexts, assessment of reasoning capabilities, implications for medical education technology

Low confidence in: Coverage of all relevant medical knowledge domains, real-world applicability of LLM performance, long-term utility as medical education evolves

## Next Checks
1. Cross-validate benchmark performance with actual medical entrance exam results to verify alignment
2. Have independent medical educators evaluate benchmark questions for content validity and curriculum coverage
3. Test benchmark with emerging medical-specialized models and newer LLM generations to establish longitudinal validity