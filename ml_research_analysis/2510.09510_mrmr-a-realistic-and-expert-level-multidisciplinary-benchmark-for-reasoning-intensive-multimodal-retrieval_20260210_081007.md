---
ver: rpa2
title: 'MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive
  Multimodal Retrieval'
arxiv_id: '2510.09510'
source_url: https://arxiv.org/abs/2510.09510
tags:
- image
- retrieval
- multimodal
- documents
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MRMR is a new benchmark for multimodal retrieval that introduces
  expert-level, reasoning-intensive queries across 23 domains, requiring retrieval
  of image-text interleaved documents. It includes three novel tasks: Knowledge, Theorem,
  and Contradiction retrieval, with the latter being the first of its kind in multimodal
  settings.'
---

# MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval

## Quick Facts
- arXiv ID: 2510.09510
- Source URL: https://arxiv.org/abs/2510.09510
- Reference count: 40
- MRMR shows current multimodal retrieval models struggle with expert-level reasoning, with text+caption approaches outperforming native multimodal models.

## Executive Summary
MRMR introduces a new benchmark for multimodal retrieval featuring expert-level, reasoning-intensive queries across 23 domains. The benchmark includes three novel tasks: Knowledge retrieval (finding web pages to answer expert questions), Theorem retrieval (retrieving relevant theorems for solving calculation problems), and Contradiction retrieval (identifying violated rules in cases). The benchmark contains 1,502 queries with positive documents verified by human experts. Evaluation across 14 models reveals that text-only retrievers with image captioning significantly outperform native multimodal models, with Qwen3-Embedding achieving the highest nDCG@10 of 52.1. The results indicate substantial room for improving multimodal retrieval models' ability to handle abstract reasoning over expert-level content.

## Method Summary
MRMR is a multimodal retrieval benchmark containing 1,502 queries across 23 domains, requiring retrieval of image-text interleaved documents. The benchmark introduces three novel tasks: Knowledge retrieval (retrieve web pages to answer expert questions), Theorem retrieval (retrieve relevant theorems for solving calculation problems), and Contradiction retrieval (identify rules violated by a case). Documents are organized into three corpora: Knowledge (26,223 docs), Theorem (14,257 docs), and Contradiction (varying sizes). Evaluation uses nDCG@10 for most tasks and Hit@1 for Negation subtasks. The benchmark evaluates 14 models across four categories: text retrievers with LLM captions, two-stream CLIP-style models with fusion, MLLM-based multimodal embeddings, and document-as-image approaches. Images are processed using vertical concatenation for multi-image inputs.

## Key Results
- Text-only retrievers with image captioning outperform multimodal models (Qwen3-Embedding 52.1 vs Ops-MM-Embedding 45.4 average nDCG@10)
- Performance degrades significantly on reasoning-intensive tasks (67.4 → 30.1 → 36.6 across Knowledge → Theorem → Contradiction)
- Multimodal models show "visual bias" and struggle with abstract reasoning, particularly on Contradiction tasks with <25% Hit@1 on Negation subtask
- Ops-MM-Embedding (45.4) underperforms Qwen3-Embedding (52.1) despite being a native multimodal model

## Why This Works (Mechanism)
The benchmark's design reveals that text-only retrievers with LLM-generated captions outperform native multimodal models by leveraging the superior semantic understanding and abstraction capabilities of text embedding models. When images are converted to rich, descriptive captions, text retrievers can better extract the abstract relationships and expert-level concepts needed for reasoning-intensive tasks. Native multimodal models, while attempting direct visual reasoning, appear to struggle with the abstraction required to connect visual elements to expert knowledge, particularly in domains requiring high-level logical deduction. This suggests that current multimodal architectures may have a "visual bias" that prioritizes surface-level visual similarity over deeper semantic and logical relationships.

## Foundational Learning
- Multimodal retrieval fundamentals: Understanding how to process and retrieve from interleaved image-text documents is essential for this benchmark
  - Why needed: MRMR requires retrieval from documents containing both images and text
  - Quick check: Can you explain the difference between single-stream and two-stream multimodal retrieval approaches?
- nDCG@10 and Hit@1 metrics: These are the primary evaluation metrics for retrieval performance
  - Why needed: MRMR uses nDCG@10 for most tasks and Hit@1 specifically for Negation
  - Quick check: Do you understand how nDCG@10 differs from precision@k or recall?
- Image captioning for retrieval: Using LLM-generated captions to augment text-only retrieval
  - Why needed: Text-only retrievers with captions outperform native multimodal models in MRMR
  - Quick check: Can you explain why captioning might help with abstract reasoning tasks?

## Architecture Onboarding

**Component Map:** Queries -> Image Captioning (optional) -> Retrieval Model -> Ranking -> nDCG@10/Hit@1

**Critical Path:** Document preprocessing → Query formulation → Model inference → Ranking computation → Metric calculation

**Design Tradeoffs:** Text+caption approach trades multimodal understanding for semantic richness, while native multimodal models attempt direct visual reasoning but struggle with abstraction

**Failure Signatures:** Low scores on Contradiction tasks (<25% Hit@1 on Negation), visual bias prioritizing similar images over logical content, significant performance drop on Theorem vs Knowledge tasks

**First Experiments:**
1. Evaluate Qwen3-Embedding-8B with captions vs without captions on Knowledge task to quantify captioning contribution
2. Compare single-image vs multi-image concatenation performance on Theorem task
3. Test text-only retriever performance on Negation subtask to establish baseline for visual reasoning

## Open Questions the Paper Calls Out
### Open Question 1
**Question:** How can native multimodal retrieval models be improved to match or surpass the performance of text-only retrievers augmented with image captions on reasoning-intensive tasks?
**Basis in paper:** [explicit] The abstract and conclusion explicitly state that "text-only retrievers with image captioning outperform multimodal models" and highlight "substantial room for improving multimodal retrieval models."
**Why unresolved:** Current multimodal models struggle to extract abstract concepts from images (e.g., linking a physics diagram to a theorem) compared to text embedding models that process rich semantic captions.
**What evidence would resolve it:** Development of a native multimodal model that achieves a higher average nDCG@10 than the current state-of-the-art text-based approach (Qwen3-Embedding with captions at 52.1).

### Open Question 2
**Question:** How does expanding the retrieval corpus with additional expert-domain documents affect the trade-off between retrieval difficulty and the probability of false negatives?
**Basis in paper:** [explicit] Page 5, Footnote 4 states, "The corpus could be further expanded by sampling additional expert-domain documents... We leave it as future work."
**Why unresolved:** While a larger corpus increases realism and difficulty, it complicates the verification of negative samples, raising the risk of unlabeled positives (false negatives).
**What evidence would resolve it:** A follow-up study evaluating model performance on an expanded MRMR corpus with a rigorous analysis of false negative rates.

### Open Question 3
**Question:** What architectural or training modifications are necessary to overcome "visual bias" and enable higher-level logical deduction for Contradiction Retrieval?
**Basis in paper:** [inferred] Page 9 details error cases where models prioritize documents with visually similar images (visual bias) or fail to infer logical inconsistencies (failure of deduction), leading to poor performance on the Contradiction task.
**Why unresolved:** Current models rely heavily on surface-level semantic matching rather than the deep reasoning required to identify conflicting concepts in interleaved multimodal data.
**What evidence would resolve it:** A model demonstrating significantly higher Hit@1 scores on the Negation subtask (currently <25%) and improved nDCG on Contradiction tasks without relying on superficial visual similarity.

## Limitations
- Human annotation process for verifying positive documents is not fully specified, creating potential measurement error across 23 domains
- PIN-14M negative sampling approach lacks detailed parameters for how negative samples are selected and balanced
- The Negation subtask uses Hit@1 instead of nDCG@10, complicating cross-task performance comparison

## Confidence
**High confidence:** Text-only retrievers with image captioning outperform multimodal models (Qwen3-Embedding 52.1 vs Ops-MM-Embedding 45.4 average nDCG@10)
**Medium confidence:** Performance degradation on reasoning-intensive tasks (67.4 → 30.1 → 36.6 across Knowledge → Theorem → Contradiction)
**Medium confidence:** Introduction of Contradiction retrieval as a novel task type, though metric inconsistency affects comparison

## Next Checks
1. Replicate the gap between text+caption and multimodal performance using the same model checkpoints, particularly verifying whether Qwen3-Embedding-8B with captions consistently outperforms Ops-MM-Embedding-1.5B across all three main task categories
2. Test model performance on individual domains (e.g., Medicine vs Art) to identify whether certain domains drive the overall performance differences more than others
3. Implement an ablation study removing image captioning from text-only retrievers to quantify exactly how much captioning contributes to their superior performance versus the retrieval model itself