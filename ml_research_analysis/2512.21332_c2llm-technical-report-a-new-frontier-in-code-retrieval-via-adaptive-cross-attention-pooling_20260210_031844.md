---
ver: rpa2
title: 'C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention
  Pooling'
arxiv_id: '2512.21332'
source_url: https://arxiv.org/abs/2512.21332
tags:
- code
- embedding
- zhang
- c2llm
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: C2LLM introduces a novel approach to code retrieval by integrating
  Pooling by Multihead Attention (PMA) into code embedding models. PMA aggregates
  token-level representations from a causal LLM backbone using a single learnable
  query, bypassing the limitations of EOS-based and mean pooling strategies.
---

# C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling

## Quick Facts
- arXiv ID: 2512.21332
- Source URL: https://arxiv.org/abs/2512.21332
- Reference count: 25
- Primary result: C2LLM achieves state-of-the-art performance on MTEB-Code benchmark with 7B model scoring 80.75

## Executive Summary
C2LLM introduces a novel approach to code retrieval by integrating Pooling by Multihead Attention (PMA) into code embedding models. PMA aggregates token-level representations from a causal LLM backbone using a single learnable query, bypassing the limitations of EOS-based and mean pooling strategies. This design preserves the LLM's causal attention structure while effectively aggregating information across the sequence and allowing flexible embedding dimensionality. Trained on 3 million publicly available code datasets, C2LLM achieves state-of-the-art performance on the MTEB-Code benchmark. The 7B variant ranks first overall with an average score of 80.75, surpassing both closed-source and open-source competitors. The 0.5B model also leads among models under 1B parameters with a score of 75.46, demonstrating strong performance across scales. The results highlight PMA's effectiveness in capturing code semantics for retrieval tasks.

## Method Summary
C2LLM addresses code retrieval by employing a novel pooling mechanism called Pooling by Multihead Attention (PMA). Unlike traditional approaches that rely on EOS tokens or mean pooling, PMA uses a single learnable query vector to attend to all token representations from a causal LLM backbone. This query-based aggregation preserves the causal attention structure of the LLM while enabling effective information fusion across the sequence. PMA allows flexible embedding dimensionality, making it adaptable to various retrieval tasks. The model is trained on a large corpus of 3 million publicly available code datasets, enabling it to capture rich semantic relationships in code. The architecture is designed to overcome the limitations of existing pooling strategies, particularly their inability to fully leverage the contextual information provided by modern LLMs.

## Key Results
- C2LLM 7B model achieves state-of-the-art performance on MTEB-Code benchmark with an average score of 80.75
- C2LLM 0.5B model leads among models under 1B parameters with a score of 75.46
- PMA demonstrates superior effectiveness in capturing code semantics compared to EOS-based and mean pooling strategies

## Why This Works (Mechanism)
PMA works by using a single learnable query to attend to all token representations from a causal LLM backbone. This query-based aggregation allows the model to focus on the most relevant parts of the code sequence while preserving the causal attention structure of the LLM. By bypassing the limitations of EOS-based and mean pooling strategies, PMA can effectively capture the semantic relationships in code, leading to improved retrieval performance.

## Foundational Learning
- **Code Retrieval**: The task of finding relevant code snippets from a large corpus based on a query. Why needed: Forms the basis for understanding code search and recommendation systems. Quick check: Ensure familiarity with retrieval metrics like precision and recall.
- **LLM Backbone**: The causal language model that generates token representations. Why needed: Provides contextual embeddings that capture semantic relationships in code. Quick check: Understand the difference between causal and bidirectional attention.
- **Pooling Strategies**: Methods for aggregating token-level representations into a fixed-size embedding. Why needed: Determines how information is compressed and represented for retrieval. Quick check: Compare EOS-based, mean pooling, and attention-based pooling.
- **Multihead Attention**: A mechanism that allows the model to attend to different parts of the input simultaneously. Why needed: Enables the query to focus on relevant token representations. Quick check: Review the mathematical formulation of multihead attention.

## Architecture Onboarding
**Component Map**: Input Code -> LLM Backbone -> Token Representations -> PMA Query -> Aggregated Embedding -> Code Retrieval
**Critical Path**: The sequence from input code through the LLM backbone to the PMA query and final aggregated embedding is critical for performance.
**Design Tradeoffs**: PMA offers flexibility in embedding dimensionality but requires careful tuning of the learnable query. It preserves causal attention structure but may introduce additional computational overhead compared to simpler pooling methods.
**Failure Signatures**: Poor retrieval performance may indicate issues with the learnable query's ability to focus on relevant token representations or insufficient training data diversity.
**First Experiments**: 1) Compare PMA against EOS-based and mean pooling on a small code retrieval dataset. 2) Ablation study on the impact of embedding dimensionality. 3) Evaluate the model's performance on non-code benchmarks to assess domain generalization.

## Open Questions the Paper Calls Out
None

## Limitations
- PMA's effectiveness beyond the code domain remains unverified, with potential limitations in natural language or multimodal data.
- The reliance on 3 million publicly available code datasets raises concerns about data quality, potential contamination from benchmark test sets, or non-representative sampling.
- The technical report lacks a detailed analysis of dataset composition or filtering criteria, which could impact the robustness of the findings.

## Confidence
- **High**: C2LLM's consistent outperformance across multiple scales (7B and 0.5B) and clear methodological improvements over existing pooling strategies.
- **Medium**: Claims about PMA's flexibility in embedding dimensionality, as the report does not thoroughly explore edge cases or extreme dimensional configurations.
- **Low**: Assertions about PMA's potential for broader applicability, as these remain speculative without empirical validation in other domains.

## Next Checks
1) Evaluate C2LLM on non-code benchmarks such as MTEB-NaturalLanguage to assess domain generalization.
2) Conduct ablation studies on PMA with varying embedding dimensions and compare against alternative pooling methods.
3) Perform a detailed analysis of the training dataset for potential biases or contamination, including a breakdown of dataset sources and filtering procedures.