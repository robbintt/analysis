---
ver: rpa2
title: Approximating Shapley Explanations in Reinforcement Learning
arxiv_id: '2511.06094'
source_url: https://arxiv.org/abs/2511.06094
tags:
- shapley
- characteristic
- training
- learning
- behaviour
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FastSVERL introduces a scalable framework for explaining reinforcement
  learning agents using Shapley values. It replaces expensive Monte Carlo sampling
  with parametric models that amortize computation across states and feature subsets.
---

# Approximating Shapley Explanations in Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2511.06094
- **Source URL:** https://arxiv.org/abs/2511.06094
- **Reference count:** 40
- **Primary result:** FastSVERL introduces a scalable framework for explaining reinforcement learning agents using Shapley values.

## Executive Summary
FastSVERL provides a scalable framework for approximating Shapley explanations in reinforcement learning. The method replaces expensive Monte Carlo sampling with parametric models that amortize computation across states and feature subsets. It addresses practical constraints such as explaining policies from off-policy data and adapting to evolving agent behaviors. The approach achieves low approximation error in domains with up to billions of states while significantly reducing computational cost.

## Method Summary
FastSVERL trains parametric models to approximate Shapley values for reinforcement learning agents. The framework consists of characteristic models that predict masked feature expectations and Shapley models that output per-feature attributions. Two variants exist: model-based (using characteristic model queries) and sample-based (using single Monte Carlo samples from replay buffers). The method supports behavior, outcome, and prediction explanations, with particular attention to off-policy scenarios through importance sampling corrections.

## Key Results
- FastSVERL achieves low approximation error in domains with up to billions of states
- The framework converges efficiently while reducing computational cost by eliminating characteristic models in favor of single-sample approximations
- In practice, FastSVERL halves training time while maintaining accuracy, enabling real-time interpretability in complex RL settings

## Why This Works (Mechanism)

### Mechanism 1: Amortized Shapley Estimation via Least-Squares Characterization
A parametric model trained to minimize a weighted least-squares loss over feature subsets converges to exact Shapley values at the global optimum. The Shapley model predicts per-feature contributions and is trained by minimizing expected squared error between true and predicted characteristic values across sampled subsets. A post-hoc correction enforces the efficiency constraint, recovering the unique solution to the constrained least-squares problem.

### Mechanism 2: Characteristic Function Learning via Conditional Expectation
Training a model on masked inputs to predict target values yields the characteristic function as the unique minimizer. When features outside a subset are masked with a fixed out-of-support value, all states sharing the masked features produce identical model outputs. Minimizing squared error across this equivalence class forces the model to predict the conditional mean.

### Mechanism 3: Single-Sample Characteristic Approximation Eliminates Bottleneck
Replacing characteristic model queries with single Monte Carlo samples within the Shapley loss recovers unbiased Shapley estimates while halving training time. For each loss evaluation, sampling from the replay buffer and using the observed value as the characteristic approximation maintains unbiasedness in expectation, though with higher gradient variance.

## Foundational Learning

- **Concept: Shapley values and the efficiency axiom**
  - Why needed: The framework builds on Shapley values as the unique fair attribution satisfying four axioms. The efficiency constraint is critical to correctness.
  - Quick check: If three features contribute to a prediction of 10, and the baseline is 2, what must their Shapley values sum to? (Answer: 8)

- **Concept: MDPs, steady-state distributions, and value functions**
  - Why needed: Characteristic functions are expectations over steady-state distributions. Outcome explanations require estimating expected returns under modified policies.
  - Quick check: If you only observe features C in state s, what distribution do you marginalize over to compute the characteristic function? (Answer: p(s' | s_C))

- **Concept: Off-policy learning and importance sampling**
  - Why needed: Explaining from historical data requires correcting distributional mismatch between the policy being explained and the data collection policy.
  - Quick check: When using a replay buffer from training, why does unweighted sampling bias the explanation loss? (Answer: It overrepresents actions taken by the older policy)

## Architecture Onboarding

- **Component map:** RL Agent -> Replay Buffer B -> Characteristic Models -> Shapley Model -> Explanations
- **Critical path:** 1) Train RL agent to convergence, 2) Train characteristic models on masked inputs, 3) Train Shapley model using characteristic queries or single samples, 4) Apply efficiency correction at inference
- **Design tradeoffs:** Model-based vs. sample-based characteristics (reuse vs. accuracy), on-policy vs. off-policy outcome training (robustness vs. efficiency), update ratio for continual learning (tracking vs. compute)
- **Failure signatures:** Shapley loss plateaus above zero (insufficient capacity), large variance in importance-weighted loss (poor buffer coverage), outcome characteristic diverges (insufficient exploration)
- **First 3 experiments:** 1) Behaviour Shapley with exact characteristics in Gridworld, 2) Sample-based vs. model-based in Mastermind-222, 3) Off-policy explanation with importance sampling

## Open Questions the Paper Calls Out

- **Human understanding impact:** How do FastSVERL explanations quantifiably impact human understanding and decision-making in safety-critical RL deployments? The paper notes formal user studies are essential but not yet conducted.

- **Continuous state spaces:** Can learning a parametric model of the steady-state distribution effectively scale FastSVERL to high-dimensional continuous state spaces? The authors identify this as a key challenge for future work.

- **Convergence guarantees:** Under what specific conditions does the outcome characteristic model converge when using deep function approximation? The paper notes that formal convergence results are generally not known for deep RL with function approximation.

## Limitations

- The method requires sufficient coverage of state-action pairs under the target policy distribution, which may be challenging in sparse or high-dimensional environments.
- Importance sampling correction for off-policy explanations can introduce high variance when buffer coverage is limited.
- The single-sample approximation's effectiveness depends on having representative transitions for all conditional distributions, which may not hold in complex domains.

## Confidence

- **High Confidence:** Least-squares characterization mechanism and efficiency constraint correction are mathematically sound and well-supported.
- **Medium Confidence:** Characteristic function learning via conditional expectation is theoretically valid but practically sensitive to implementation details.
- **Medium Confidence:** Scalability claims rely on assumptions about parametric model generalization across state spaces.

## Next Checks

1. **Replay Buffer Coverage Analysis:** Systematically measure how characteristic function estimation error varies with replay buffer size and diversity in domains with known ground truth Shapley values.

2. **Masking Value Sensitivity:** Conduct ablation studies varying masking values across different magnitudes to identify optimal ranges and quantify sensitivity to this implementation detail.

3. **Out-of-Distribution Generalization:** Test behaviour explanation accuracy on states that were rarely or never visited during training, to assess distributional shift handling.