---
ver: rpa2
title: 'Baba is LLM: Reasoning in a Game with Dynamic Rules'
arxiv_id: '2506.19095'
source_url: https://arxiv.org/abs/2506.19095
tags:
- object
- baba
- reasoning
- game
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether LLMs can solve Baba is You, a 2D
  puzzle game where players manipulate game rules by rearranging text blocks. Six
  LLMs are tested with three prompt types, and two models (Mistral 7B, OLMo 7B) are
  finetuned using game data.
---

# Baba is LLM: Reasoning in a Game with Dynamic Rules

## Quick Facts
- arXiv ID: 2506.19095
- Source URL: https://arxiv.org/abs/2506.19095
- Reference count: 0
- Six LLMs tested on Baba is You game with dynamic rules; finetuning improves level analysis but not solution formulation

## Executive Summary
This paper investigates whether large language models can solve Baba is You, a puzzle game where players manipulate game rules by rearranging text blocks. The study tests six LLMs (GPT-4o, Gemini 1.5 Flash, OLMo 7B/13B, Mistral 7B, Mixtral 8x7B) using three prompt types and finetunes two models on game data. Results show that while larger models like GPT-4o perform better at interpreting game states, all models struggle with the fundamental "use-mention" distinction required to manipulate dynamic rules. Finetuning improves the ability to analyze levels but fails to significantly enhance solution formulation, highlighting a critical gap in LLM reasoning capabilities.

## Method Summary
The study evaluates six LLMs on 14 Baba is You levels encoded as ASCII grids. Three prompt variants are tested: simple (mechanics and definitions), rule-extended (adds active rules), and action-extended (adds possible actions). Two models (Mistral 7B, OLMo 7B) are finetuned using LoRA adapters on a combined dataset of 10,500 Chain-of-Thought reasoning examples, 289 game mechanics examples, and 15 level solutions. Performance is assessed through a 4-step reasoning chain (interpretation → problem statement → solution → actions) with manual error annotation across four categories and five subcategories each, using five runs per model-prompt combination with majority voting.

## Key Results
- GPT-4o and Gemini 1.5 Flash show superior grid interpretation and problem parsing compared to smaller models
- Finetuning improves classification accuracy and level analysis but fails to improve solution formulation
- All models struggle with the "use-mention" distinction, often treating text blocks as immutable objects rather than manipulable rules
- The use-mention distinction represents the fundamental barrier to solving dynamic rule puzzles

## Why This Works (Mechanism)

### Mechanism 1: Scale-Emergent Grid Interpretation
- Larger parameter count correlates with ability to map ASCII characters to spatial game states
- GPT-4o successfully identifies objects and text blocks, while smaller models hallucinate grid contents
- Based on evidence that spatial reasoning emerges at larger scales and smaller models frequently misidentify grid elements

### Mechanism 2: Context-Accountable Rule Adherence (Prompting)
- Explicit active rules and action definitions in prompts shift models from deduction to retrieval
- GPT-4o and Gemini show fewer errors with detailed prompts, while smaller models show no significant improvement
- Fails when solutions require modifying the rules provided in context (use-mention barrier)

### Mechanism 3: Representation-Specific Finetuning
- LoRA finetuning updates weights to better recognize text blocks vs objects (classification)
- Improves parsing of game states but fails to bridge gap to strategic planning
- Dataset size and level complexity limitations prevent generalization of rule manipulation logic

## Foundational Learning

- **Use-Mention Distinction**: Critical for treating text blocks as both physical objects and logical operators; quick check: can model distinguish between word "Rock" and object Rock?
- **Semantic Inertia**: Models carry pre-trained priors that override dynamic game logic; quick check: does model follow game logic or real-world logic when rules contradict common sense?
- **2D Spatial Representation in 1D Token Space**: Models failed to interpret ASCII grid as navigable 2D space; quick check: does model track spatial adjacency effectively from flattened string?

## Architecture Onboarding

- **Component map**: ASCII Grid + Rule Definitions -> Transformer LLM (General Purpose vs Finetuned) -> LoRA Adapter (Optional) -> Text-based reasoning chain + Action steps
- **Critical path**: Reasoning Chain (Level Interpretation -> Problem Statement -> Solution -> Actions); bottleneck typically between Problem Statement and Solution
- **Design tradeoffs**: Prompting vs Finetuning (cheaper but fails for smaller models vs helps reading but not winning); Simplicity vs Action-Space (detailed prompts help high-end but confuse smaller models)
- **Failure signatures**: Immutable Rule Hallucination (treating rules as fixed); Object-Text Conflation (moving objects instead of text blocks)
- **First 3 experiments**:
  1. Run "Simple Prompt" on target model to establish baseline for grid parsing vs reasoning errors
  2. Compare "Rule-extended" vs "Action-extended" prompts to identify retrieval vs execution failures
  3. Verify finetuning dataset includes explicit "rule-breaking" sequences to counter semantic inertia

## Open Questions the Paper Calls Out

1. **Can advanced reasoning methods (Reflexion, Tree of Thoughts) overcome use-mention distinction barriers?** The study used only Plan-and-Solve prompting, which failed on dynamic rules; iterative refinement or search-based prompting might show significant improvement.

2. **Does scaling finetuning dataset size enable transition from analysis to solution formulation?** The small dataset (15 levels) may explain why finetuning improved analysis but not solutions; testing on 1,000+ varied levels could resolve this.

3. **Can LLMs form internal world models through pretraining or extensive finetuning?** Unlike OthelloGPT's emergent board representations, Baba is You models struggle with spatial interpretation; mechanistic interpretability analysis could determine if internal activations track game state accurately.

## Limitations

- Findings based on only 14 test levels, limiting generalizability
- Manual error annotation introduces potential subjectivity in distinguishing error types
- Use-mention distinction appears to be a fundamental architectural limitation rather than a solvable scaling issue
- No comparison against other reasoning benchmarks to establish Baba is You's unique value

## Confidence

- **High Confidence**: Larger models (GPT-4o) perform better at grid interpretation and initial problem parsing
- **Medium Confidence**: Finetuning improves analysis but not solution formulation; specific impact of LoRA hyperparameters uncertain
- **Low Confidence**: Assertion that Baba is You represents a uniquely valuable benchmark lacks comparative evidence

## Next Checks

1. Evaluate models on entirely unseen Baba is You levels (minimum 20 new levels) to verify finetuning benefits and consistency of use-mention failures

2. Implement preprocessing layer that explicitly separates text blocks from game objects before LLM input, then measure reduction in use-mention confusion errors

3. Test whether performance gap between GPT-4o and smaller models persists with chain-of-thought prompting and intermediate verification steps