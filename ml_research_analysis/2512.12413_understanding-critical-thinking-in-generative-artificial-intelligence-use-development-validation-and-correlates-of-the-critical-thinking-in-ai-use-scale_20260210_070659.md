---
ver: rpa2
title: 'Understanding Critical Thinking in Generative Artificial Intelligence Use:
  Development, Validation, and Correlates of the Critical Thinking in AI Use Scale'
arxiv_id: '2512.12413'
source_url: https://arxiv.org/abs/2512.12413
tags:
- thinking
- critical
- scale
- https
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research developed and validated the Critical Thinking in
  AI Use Scale, a 13-item measure assessing individuals' tendencies to verify AI-generated
  content, understand AI systems, and reflect on broader implications. Across six
  studies (N = 1365), the scale demonstrated strong psychometric properties, including
  a stable three-factor structure (Verification, Motivation, Reflection), high reliability,
  convergent/discriminant validity, sex invariance, and good test-retest reliability.
---

# Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale

## Quick Facts
- **arXiv ID:** 2512.12413
- **Source URL:** https://arxiv.org/abs/2512.12413
- **Reference count:** 40
- **Key outcome:** Developed and validated a 13-item scale measuring verification, motivation, and reflection dimensions of critical thinking in AI use, with strong psychometric properties and predictive validity for verification behavior and fact-checking accuracy.

## Executive Summary
This research developed and validated the Critical Thinking in AI Use Scale, a 13-item measure assessing individuals' tendencies to verify AI-generated content, understand AI systems, and reflect on broader implications. Across six studies (N = 1365), the scale demonstrated strong psychometric properties, including a stable three-factor structure (Verification, Motivation, Reflection), high reliability, convergent/discriminant validity, sex invariance, and good test-retest reliability. Higher critical thinking in AI use scores predicted more frequent and diverse verification behaviors, greater accuracy in a ChatGPT-powered fact-checking task, and deeper reflection on responsible AI use. The scale provides a validated tool for measuring and promoting responsible AI engagement.

## Method Summary
The scale was developed through six studies using online and university samples (N = 1365). Initial item pool generation drew from AI literacy literature and expert input. Exploratory factor analysis reduced items to 13, which were then validated through confirmatory factor analysis across samples. Psychometric properties were assessed including reliability, convergent/discriminant validity with personality traits and social desirability, and test-retest stability. Criterion validity was tested using a ChatGPT-powered fact-checking task where participants evaluated 12 statements under different source-link conditions. Automated LLM grading was used to score reflection essays on responsible AI use.

## Key Results
- The 13-item scale demonstrated a stable three-factor structure (Verification, Motivation, Reflection) with high internal consistency (α > .80) and test-retest reliability (r = .76)
- Critical thinking in AI use scores predicted greater accuracy in ChatGPT-powered fact-checking tasks (R² = .06) and more frequent verification behaviors
- Higher scores correlated with openness and extraversion personality traits and showed discriminant validity from social desirability
- The scale showed sex invariance and maintained its factor structure across different samples and time points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Higher critical thinking in AI use predicts proactive evidence-seeking when on-page sources are absent or unreliable, rather than defaulting to uncertainty.
- Mechanism: Users with stronger verification dispositions detect when AI outputs lack usable evidence and compensate by drawing on prior knowledge or external corroboration, converting epistemic uncertainty into definitive judgments.
- Core assumption: Users have sufficient domain knowledge or external resources to resolve evidence gaps; this may not hold for novel topics.
- Evidence anchors:
  - [abstract] "higher critical thinking in AI use scores predicted...greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task"
  - [section] Study 6 showed critical thinking in AI use significantly predicted Evidence-Gap Resolution (b = 10.32, p < .001, R² = .082), while Evidence Extraction from working links was smaller and non-significant
  - [corpus] Limited corpus evidence; neighbor papers focus on AI literacy frameworks but do not test verification behavior mechanisms
- Break condition: If users lack prior domain knowledge AND cannot access external resources, evidence-gap resolution will degrade toward uncertainty or error.

### Mechanism 2
- Claim: Critical thinking in AI use operates through both automated (Type 1) and effortful (Type 2) verification processes, not purely analytic reasoning.
- Mechanism: Frequent AI users develop routinized verification habits (e.g., automatic source-checking) that function as automated Type 1 processes, reducing cognitive load while maintaining vigilance; these can trigger deeper Type 2 processing when heuristic cues signal risk.
- Core assumption: Automation of verification through practice transfers across AI tools and contexts.
- Evidence anchors:
  - [abstract] Scale captures "tendencies to verify AI-generated content, understand AI systems, and reflect on broader implications"
  - [section] "In practice, critical thinking in AI use can be initiated and supported by automatic Type 1 processes, such as increasingly routinised verification habits"
  - [corpus] Weak corpus support; neighbor papers discuss AI literacy but do not test dual-process mechanisms in verification
- Break condition: If verification habits are context-specific (e.g., only for familiar interfaces), transfer to new AI tools will be incomplete.

### Mechanism 3
- Claim: Motivation to understand AI systems mediates the relationship between personality traits (openness, extraversion) and verification behavior.
- Mechanism: Users high in openness/extraversion are more curious about AI mechanisms and more likely to seek human expert input; this epistemic motivation increases both ability and willingness to scrutinize AI outputs.
- Core assumption: Users have access to information about AI systems and human experts.
- Evidence anchors:
  - [abstract] Scale measures "understand how models work and where they fail" as a core dimension
  - [section] Motivation subscale correlated with extraversion (r = .22) and openness (r = .19); extraversion showed no association with Reflection but positive relations with Verification and Motivation
  - [corpus] No direct corpus evidence; neighbor paper "Generative AI Literacy" discusses understanding AI but does not test personality-mediated mechanisms
- Break condition: If users lack access to AI system documentation or human expertise, motivation cannot translate into improved verification.

## Foundational Learning

- **Dual-Process Theory (Type 1 vs. Type 2 processing)**
  - Why needed here: The paper explicitly grounds critical thinking in AI use in dual-process frameworks to explain how heuristic acceptance of fluent AI outputs can be overridden by systematic verification.
  - Quick check question: When a user sees a confident AI output, what determines whether they accept it automatically (Type 1) or initiate verification (Type 2)?

- **Heuristic-Systematic Model (HSM)**
  - Why needed here: HSM provides the theoretical account for how fluency and authority cues in AI interfaces promote heuristic processing, and how sufficient motivation/ability triggers systematic evaluation.
  - Quick check question: What two conditions must be met for systematic processing according to HSM?

- **Confirmatory Factor Analysis and Higher-Order Factor Models**
  - Why needed here: Understanding the three-factor structure (Verification, Motivation, Reflection) and how it loads on a higher-order critical thinking factor is essential for correctly interpreting and using the scale.
  - Quick check question: Why might researchers use subscale scores versus total scores, given the hierarchical structure?

## Architecture Onboarding

- **Component map:**
  - Verification subscale (5 items): Measures tendency to check AI sources, cross-reference with external sources, confirm reliability
  - Motivation subscale (4 items): Measures curiosity about how AI works, desire to understand reasoning and limitations
  - Reflection subscale (4 items): Measures consideration of ethical, societal, and environmental implications
  - Higher-order factor: Overall critical thinking in AI use, explaining ~44-53% of variance in each subscale

- **Critical path:**
  1. Administer all 13 items on 5-point Likert scale (not subscales separately)
  2. Compute subscale means (Verification items 1-5, Motivation items 6-9, Reflection items 10-13)
  3. Compute total score as mean of the three subscale means (not mean of all 13 items directly)
  4. Use total score for general assessment; use subscale scores when testing specific mechanisms

- **Design tradeoffs:**
  - Scale captures disposition, not ability—high scores indicate tendency to verify, not verification skill
  - Self-report may not match behavior in all contexts; Study 6 showed R² = .06 for accuracy prediction
  - No AI literacy measure included—cannot separate disposition from knowledge effects

- **Failure signatures:**
  - Ceiling effects possible: Item means ranged 2.40-4.35, negatively skewed
  - Social desirability: Near-zero correlation with social desirability scale suggests this is mitigated
  - Cross-cultural validity untested: Samples were English-speaking, US/Singapore only

- **First 3 experiments:**
  1. **Baseline assessment**: Administer scale to target population; report subscale means, reliability (α > .80 expected), and factor structure via CFA (CFI > .95, RMSEA < .06)
  2. **Criterion validation**: Use ChatGPT-powered fact-checking task with 12 items (6 true/6 false) across four source-link conditions; predict accuracy from scale scores
  3. **Intervention test**: Compare pre/post scale scores after an AI literacy training program; test whether Motivation subscale changes precede Verification behavior changes

*Assumption: All mechanisms assume users have basic digital literacy and access to AI tools.*

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the Critical Thinking in AI Use Scale function equivalently and maintain its factor structure across different cultures, languages, and organizational contexts (e.g., educational vs. workplace settings)?
- **Basis in paper:** [explicit] The authors state: “Future work should test critical thinking in AI use across cultures, languages, and organisational settings, examining measurement invariance and group differences to strengthen the generalisability of the scale.”
- **Why unresolved:** All six studies used English-speaking online and university samples, primarily from Western, educated, industrialized, rich, and democratic (WEIRD) populations. Cross-cultural validity was not tested.
- **What evidence would resolve it:** Multi-group CFA demonstrating measurement invariance (configural, metric, scalar) of the scale across samples from diverse cultural and linguistic backgrounds, and across different institutional settings (e.g., students vs. employees).

### Open Question 2
- **Question:** What is the relationship between the Critical Thinking in AI Use Scale and validated measures of generative AI literacy, and does the scale predict real-world AI outcomes above and beyond AI literacy?
- **Basis in paper:** [explicit] The authors note: “As a result, we were unable to examine how critical thinking in AI use relates to people’s declarative and procedural knowledge about AI systems, or whether the scale predicts outcomes over and above AI literacy.”
- **Why unresolved:** The studies did not include any direct, validated measure of (generative) AI literacy, preventing an assessment of overlap, discriminant validity, or incremental predictive utility.
- **What evidence would resolve it:** Studies that administer the scale alongside established AI literacy measures to establish convergent/discriminant validity and use hierarchical regression to test if the scale adds predictive power for outcomes like fact-checking accuracy or responsible AI use after controlling for AI literacy.

### Open Question 3
- **Question:** How does domain-specific expertise or prior knowledge interact with critical thinking in AI use to influence verification behavior and the detection of subtle inaccuracies in AI-generated content?
- **Basis in paper:** [explicit] The authors state: “We did not assess participants’ prior knowledge or expertise in the substantive domains of the veracity-judgement tasks... Future studies should therefore measure and experimentally manipulate topic knowledge to clarify how critical thinking in AI use combines with domain-specific expertise in guiding verification behaviour.”
- **Why unresolved:** Study 6’s fact-checking task did not control for or measure participants’ pre-existing knowledge of the topics, leaving the potential moderating role of domain expertise unexplored.
- **What evidence would resolve it:** Experimental or quasi-experimental designs that manipulate or measure domain expertise (e.g., via pre-tests) and examine its interaction with scale scores in predicting verification strategy use and accuracy in domain-relevant AI fact-checking tasks.

### Open Question 4
- **Question:** What is the causal direction and underlying mechanisms of the observed association between frequent AI use and higher critical thinking in AI use?
- **Basis in paper:** [inferred] The authors report a positive correlation but acknowledge the correlational design: “future research should include validated AI literacy measures to test their overlap, discriminant validity, and incremental predictive value.” The discussion also contrasts their finding with prior concerns about AI use undermining critical thinking, implying a need for causal investigation.
- **Why unresolved:** All studies were cross-sectional. The positive link found here (frequent users have higher scores) could be due to self-selection (more critical thinkers use AI more), increased practice, or other confounds.
- **What evidence would resolve it:** Longitudinal studies tracking changes in critical thinking in AI use scores alongside AI usage patterns over time, or experimental interventions that manipulate AI usage frequency/type and measure subsequent changes in critical thinking in AI use.

## Limitations
- The scale measures disposition rather than ability, meaning high scores indicate willingness to verify but not necessarily verification skill
- Cross-cultural validity remains untested beyond English-speaking US and Singapore samples
- The correlation between critical thinking and fact-checking accuracy was modest (R² = .06), suggesting other factors beyond disposition drive successful verification

## Confidence
- **High confidence**: Scale reliability (α > .80), factor structure stability (CFI > .95), and convergent/discriminant validity with personality traits and behavioral correlates
- **Medium confidence**: The dual-process mechanism claims (Type 1/Type 2 processing) and personality-mediated motivation pathway
- **Low confidence**: Claims about the scale's predictive validity for real-world verification accuracy beyond controlled fact-checking tasks

## Next Checks
1. **External validity test**: Administer the scale to diverse populations (different cultures, education levels, and AI exposure) and test whether the three-factor structure holds with confirmatory factor analysis in each group
2. **Behavioral validation**: Conduct an ecological momentary assessment study where participants report their AI verification behaviors in real-time across different contexts (academic, professional, personal use) and correlate with scale scores
3. **Intervention effectiveness**: Design and test an AI literacy intervention specifically targeting each subscale (Verification, Motivation, Reflection) to determine which dimensions most effectively improve actual verification behavior and accuracy