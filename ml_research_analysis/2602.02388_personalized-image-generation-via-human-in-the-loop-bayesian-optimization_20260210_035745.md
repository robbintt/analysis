---
ver: rpa2
title: Personalized Image Generation via Human-in-the-loop Bayesian Optimization
arxiv_id: '2602.02388'
source_url: https://arxiv.org/abs/2602.02388
tags:
- image
- optimization
- user
- multibo
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MultiBO achieves target-aligned image generation through multi-choice
  human-in-the-loop Bayesian optimization in diffusion attention space. Using multi-choice
  preference feedback and constrained warping transformations, MultiBO outperforms
  five baselines in target alignment metrics (CLIP-I2I: 0.9364, LPIPS: 0.5497) while
  maintaining competitive performance on reward metrics.'
---

# Personalized Image Generation via Human-in-the-loop Bayesian Optimization

## Quick Facts
- arXiv ID: 2602.02388
- Source URL: https://arxiv.org/abs/2602.02388
- Reference count: 40
- MultiBO achieves target-aligned image generation through multi-choice human-in-the-loop Bayesian optimization in diffusion attention space

## Executive Summary
MultiBO addresses personalized image generation when text prompts are insufficient by optimizing diffusion model attention features through human-in-the-loop Bayesian optimization. The method uses multi-choice preference feedback (N-out-of-K selection) to iteratively refine images toward a target while maintaining visual quality. By operating in the constrained space of self-attention Q/K/V warping transformations, MultiBO achieves strong target alignment metrics (CLIP-I2I: 0.9364, LPIPS: 0.5497) with only 50 user queries.

## Method Summary
MultiBO combines diffusion models with human-in-the-loop Bayesian optimization by warping self-attention Q, K, V features through constrained Affine+TPS transformations. The system presents users with K image choices per iteration, collecting N-out-of-K preferences to fit a Gaussian Process with multinomial-logit likelihood. A Dynamic Balanced Subspace acquisition function balances exploration-exploitation in the high-dimensional warping space. The method operates on SDXL decoder layers 34-64 during the first 20% of denoising steps, using DDIM sampling with 50 steps and guidance scale 5.0.

## Key Results
- MultiBO outperforms five baselines on target alignment metrics (CLIP-I2I: 0.9364, LPIPS: 0.5497)
- Human evaluation shows 70.82% win rate against baselines
- The method requires only 50 user queries while avoiding reward hacking issues
- Multi-choice feedback (K=4) provides better information density than pairwise comparisons

## Why This Works (Mechanism)

### Mechanism 1: Information-Dense Multi-Choice Feedback
Presenting users with K choices per query constrains the posterior over K partitions of the transformation space simultaneously, reducing the number of queries required. This is validated for K=4 in ablations, though cognitive degradation occurs at K=10.

### Mechanism 2: Low-Dimensional Attention Transformation Space
Constrained optimization in self-attention Q/K/V warping space enables tractable Bayesian optimization while preserving semantic coherence. Warping transformations parameterized by ≈20-30 dimensions reduce search space from ~16k latent dimensions.

### Mechanism 3: Dynamic Balanced Subspace Acquisition
DBS acquisition function balances exploration-exploitation efficiently for high-dimensional spaces without joint K-EI optimization. It constructs bridge vectors between current best and EI-suggested points, then perturbs along a d-dimensional subspace identified by spectral gap analysis.

## Foundational Learning

- **Gaussian Process Regression with Preferential Likelihoods**
  - Why needed: MultiBO models the unknown user satisfaction function f using GPR with multinomial-logit likelihood
  - Quick check: Can you derive why the Bradley-Terry-Luce model uses Φ(z_i) = CDF of Gaussian, and how multinomial-logit extends this for K choices?

- **Diffusion Model Attention Mechanisms**
  - Why needed: The optimization operates on self-attention Q, K, V features
  - Quick check: In cross-attention vs. self-attention, which one controls texture/color vs. spatial layout, and why does MultiBO target self-attention?

- **Bayesian Optimization Acquisition Functions**
  - Why needed: DBS is a custom acquisition function derived from Expected Improvement
  - Quick check: Why does joint K-EI optimization become computationally intractable, and how does DBS's bridge vector approach circumvent this?

## Architecture Onboarding

- **Component map**: User Interface Layer -> MultiwiseGPR Module -> DBS Acquisition Module -> Self-Attention Warping Module -> Diffusion Sampler

- **Critical path**: User preference → MultiwiseGPR posterior update → DBS generates Θ candidates → Warping applied to attention → Images rendered → Next user preference round

- **Design tradeoffs**:
  - K=4 vs higher K: K=4 balances information gain and cognitive load; K=10 shows degraded reliability
  - Attention layer selection: Middle-to-late layers (34-64) optimize spatial control without losing semantic structure
  - Edit timestep window: First 20% of denoising (t_BO to 0.8T) provides editing freedom
  - Subspace dimension d: Spectral gap threshold γ=2.0 adapts d dynamically

- **Failure signatures**:
  - Stalling: User forced to choose among equally bad options; preference signal weakens
  - Semantic collapse: Editing early attention layers (1-24) destroys image coherence
  - Reward hacking: Not observed in MultiBO with human feedback
  - Subspace misidentification: If spectral gap fails to capture key dimensions

- **First 3 experiments**:
  1. Reproduce MultiwiseGPR vs PairwiseGPR ablation: Run B=50 iterations with K=4 multi-choice vs K=2 pairwise
  2. Layer sensitivity analysis: Apply warping to different attention layer ranges (1-24, 34-64, 64-70)
  3. Reward model substitution: Replace human scorer with CLIP-I2I or LPIPS

## Open Questions the Paper Calls Out

- Can pre-trained reward models be integrated as informed priors into the Gaussian likelihood to accelerate convergence or reduce the required user feedback budget?
- Do customized high-dimensional Bayesian optimization strategies adapted to the specific geometry of latent diffusion representations outperform the proposed Dynamic Balanced Subspace method?
- Does the restriction to Affine and Thin-Plate Spline warping transformations fundamentally limit the "arsenal of possible changes" for non-spatial edits?

## Limitations

- The attention warping space is constrained to spatial deformations, limiting applicability to tasks requiring global style changes or novel object insertion
- Performance gains over reward model baselines are modest (0.9364 vs 0.9343 CLIP-I2I), raising questions about practical significance
- The method assumes users can reliably distinguish K=4 options without significant cognitive load, which may not generalize to novice users

## Confidence

- **High**: The core mechanism of attention space optimization is technically sound and well-grounded in diffusion model literature
- **Medium**: Superiority claims depend on controlled ablation studies; real-world user studies are needed
- **Low**: Generalization to other diffusion models and more complex editing scenarios is unverified

## Next Checks

1. Apply MultiBO to SD1.5 and test on same benchmark to verify cross-model generalization
2. Extend optimization to 100+ iterations to check if performance plateaus or user fatigue becomes limiting
3. Run with novice users unfamiliar with image editing to assess accessibility claims