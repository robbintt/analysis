---
ver: rpa2
title: 'HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation'
arxiv_id: '2511.15435'
source_url: https://arxiv.org/abs/2511.15435
tags:
- attack
- image
- knowledge
- query
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces HV-Attack, a method for attacking multimodal
  Retrieval-Augmented Generation (MRAG) systems by adding imperceptible perturbations
  to the image input. The method uses a hierarchical two-stage strategy: first breaking
  cross-modal alignment between the image and its caption, then disrupting semantic
  alignment with reference knowledge.'
---

# HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation

## Quick Facts
- **arXiv ID**: 2511.15435
- **Source URL**: https://arxiv.org/abs/2511.15435
- **Reference count**: 40
- **Primary result**: HV-Attack achieves 57.38% success rate in degrading retrieval performance on fine-tuned CLIP models

## Executive Summary
HV-Attack introduces a hierarchical visual attack method targeting multimodal Retrieval-Augmented Generation (MRAG) systems by adding imperceptible perturbations to image inputs. The approach operates in two stages: first breaking cross-modal alignment between images and captions, then disrupting semantic alignment with reference knowledge. This causes the retriever to fetch irrelevant knowledge, which misleads the generator into producing incorrect answers. The method demonstrates significant performance degradation on OK-VQA and InfoSeek datasets across both off-the-shelf and fine-tuned CLIP models, as well as multiple black-box generators.

## Method Summary
The HV-Attack method employs a hierarchical two-stage strategy to compromise MRAG systems. In the first stage, it adds perturbations to break the cross-modal alignment between the image and its caption, causing the visual encoder to produce embeddings that poorly match the text representation. In the second stage, it further disrupts the semantic alignment between the perturbed image and reference knowledge embeddings, ensuring that the retriever fetches irrelevant documents. The attack optimizes for both objectives simultaneously while maintaining imperceptibility constraints (ε=8/255), effectively degrading both retrieval accuracy and downstream generation quality.

## Key Results
- Achieved 57.38% success rate in degrading Recall@5 on fine-tuned CLIP models
- Demonstrated effectiveness across multiple black-box generators (Vicuna, LLaMA, Baize)
- Showed consistent performance degradation on both OK-VQA and InfoSeek benchmark datasets

## Why This Works (Mechanism)
The attack exploits the fundamental reliance of MRAG systems on accurate cross-modal alignment between visual and textual representations. By adding carefully crafted perturbations that break this alignment at two hierarchical levels - first between image-caption pairs and then between images and knowledge embeddings - the attack forces the retriever to fetch irrelevant documents. Since the generator relies on retrieved knowledge for answer generation, this cascading failure leads to incorrect responses. The method's effectiveness stems from targeting the weakest link in the MRAG pipeline: the cross-modal retrieval stage, which has limited robustness to adversarial perturbations.

## Foundational Learning
- **Cross-modal alignment**: The matching between visual and textual embeddings in multimodal models; needed to understand how MRAG systems bridge vision and language, quick check: verify alignment scores drop after perturbation
- **Retrieval-augmented generation**: The process of retrieving relevant documents before generation; needed to grasp the attack's end-to-end impact, quick check: confirm retriever output changes before generator input
- **Adversarial perturbations**: Small input modifications designed to fool models; needed to understand attack imperceptibility constraints, quick check: measure perturbation magnitude against ε threshold
- **Visual-semantic embeddings**: Vector representations capturing image meaning; needed to understand how attacks manipulate semantic space, quick check: visualize embedding distance changes
- **Knowledge base retrieval**: The process of finding relevant documents from large collections; needed to understand attack target, quick check: verify irrelevant documents are retrieved
- **Multimodal transformers**: Models processing both visual and textual inputs; needed to understand the architecture being attacked, quick check: confirm transformer attention patterns change

## Architecture Onboarding

**Component Map**: Image Encoder -> Cross-modal Retriever -> Generator -> Output

**Critical Path**: Perturbation generation → Image encoding → Cross-modal retrieval → Knowledge fetching → Answer generation

**Design Tradeoffs**: The attack balances perturbation imperceptibility (maintaining ε=8/255) against effectiveness in degrading retrieval performance. Smaller perturbations preserve stealth but may reduce attack success rates.

**Failure Signatures**: The attack manifests as retrieval of semantically irrelevant documents, leading to factually incorrect or nonsensical generated answers. The system appears to function normally but produces wrong outputs.

**First Experiments**: 
1. Test retrieval performance degradation with varying perturbation magnitudes (ε values)
2. Measure cross-modal alignment scores before and after attack
3. Evaluate attack transferability across different MRAG model architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes white-box access to image encoder and generator, limiting real-world applicability
- Evaluation focused on specific datasets (OK-VQA, InfoSeek) may not generalize to other domains
- Does not explore computational scalability for large image and knowledge collections

## Confidence
- **High confidence**: The methodology description and two-stage hierarchical attack framework are clearly articulated and reproducible based on the provided details
- **Medium confidence**: The experimental results showing performance degradation are supported by quantitative metrics, though external validation would strengthen these claims
- **Medium confidence**: The generalizability claims to other black-box generators are supported by experiments on multiple models, but the specific conditions for success are not fully characterized

## Next Checks
1. Test HV-Attack under black-box conditions where the image encoder architecture and weights are completely unknown, only query access is available
2. Evaluate attack effectiveness against defense mechanisms such as adversarial training or input sanitization in the multimodal retrieval pipeline
3. Assess performance on diverse multimodal knowledge bases beyond OK-VQA and InfoSeek, including domain-specific datasets with different knowledge distributions