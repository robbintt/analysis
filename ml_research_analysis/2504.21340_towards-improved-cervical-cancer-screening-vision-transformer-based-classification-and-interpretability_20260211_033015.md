---
ver: rpa2
title: 'Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification
  and Interpretability'
arxiv_id: '2504.21340'
source_url: https://arxiv.org/abs/2504.21340
tags:
- feature
- image
- loss
- tokens
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an automated approach for cervical cell image
  classification to improve cervical cancer screening. The method employs a transformer-based
  EVA-02 model for feature extraction, followed by feature selection using multiple
  machine learning models (logistic regression, random forest, and gradient boosting)
  to identify the most important features.
---

# Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification and Interpretability

## Quick Facts
- arXiv ID: 2504.21340
- Source URL: https://arxiv.org/abs/2504.21340
- Reference count: 0
- Primary result: Transformer-based pipeline achieves 0.85227 F1-score on cervical cell classification

## Executive Summary
This study presents an automated approach for cervical cell image classification to improve cervical cancer screening. The method employs a transformer-based EVA-02 model for feature extraction, followed by feature selection using multiple machine learning models (logistic regression, random forest, and gradient boosting) to identify the most important features. These selected features are then used to train a deeper artificial neural network with optional loss weighting to handle class imbalance. The proposed pipeline achieves an F1-score of 0.85227, outperforming the baseline EVA-02 model (0.84878). Additionally, Kernel SHAP analysis reveals that feature #10, which correlates with cell morphology and staining characteristics, is the most influential in the model's decision-making process.

## Method Summary
The proposed four-step pipeline begins with fine-tuning the EVA-02 vision transformer on the PS3C cervical cell dataset. Feature extraction follows using three approaches: Class token only, Image tokens only, or concatenation of both (All tokens), each pooled to 768 dimensions. Feature selection is then performed using logistic regression, random forest, and gradient boosting models to identify the most relevant features through importance scores and threshold filtering. Finally, the selected features train a deeper 3-hidden-layer artificial neural network (768→1024→512→256→3) with optional class-weighted cross-entropy loss to address the dataset's class imbalance (Rubbish: 50,371; Healthy: 28,895; Unhealthy: 5,814).

## Key Results
- The pipeline achieves 0.85227 F1-score, outperforming the baseline EVA-02 model (0.84878 F1)
- All tokens feature extraction (Class + Image tokens) performs best with a maximum effectiveness of 0.85227 F1
- Feature selection successfully reduces dimensionality while maintaining competitive effectiveness
- Kernel SHAP identifies feature #10 as most influential, correlating with cell morphology and staining characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining global (Class token) and local (Image tokens) representations improves classification effectiveness over either alone.
- Mechanism: The Class token aggregates dataset-wide contextual information through transformer self-attention, while Image tokens preserve spatially-localized patch-level features (positional + visual). Concatenating both provides the ANN classifier with complementary signals—global semantic context and fine-grained morphological details.
- Core assumption: The 768-dimensional pooled representation from "All tokens" contains non-redundant information that a deeper ANN can disentangle better than EVA-02's single linear classifier.
- Evidence anchors: [abstract] "We developed a four-step pipeline: fine-tuning EVA-02, feature extraction, selecting important features..."; [section 3.1] "Among the three extraction approaches... the All tokens method generally achieved the highest F1-scores, with a maximum effectiveness of 0.85227"
- Break condition: If Class token and Image tokens encode highly correlated information, pooling both adds noise without discriminative gain—expect no improvement over single-token baselines.

### Mechanism 2
- Claim: Feature selection via classical ML models reduces overfitting risk while maintaining effectiveness in low-data regimes.
- Mechanism: Training logistic regression, random forest, and gradient boosting on extracted features yields feature importance scores (coefficient magnitudes, Gini importance). Filtering low-importance dimensions removes noise and reduces the effective dimensionality before ANN training, which is critical when training data (N) is limited relative to feature dimension (E=768).
- Core assumption: The importance rankings from shallow ML models transfer meaningfully to the deeper ANN's feature utilization patterns.
- Evidence anchors: [section 2.3] "Given the limited amount of training data available relative to the image size, we trained these features... to reduce the risk of overfitting and eliminate less substantial features"; [section 3.2] "The competitive effectiveness of models with filtered features suggests that our selection criteria successfully identified the most relevant features"
- Break condition: If selected features are model-specific (e.g., important for random forest but not ANN), filtering may inadvertently discard useful ANN inputs—monitor for effectiveness drops vs. "All selection."

### Mechanism 3
- Claim: Kernel SHAP identifies interpretable feature-importance rankings that correlate with clinically meaningful visual characteristics.
- Mechanism: Kernel SHAP approximates Shapley values by perturbing input features and measuring impact on model output. The most influential feature (#10) was identified, and qualitative image analysis linked high/low values to cell morphology (size, cytoplasm, nuclear boundaries) and staining intensity.
- Core assumption: The 768 extracted features encode human-interpretable visual concepts rather than abstract distributed representations.
- Evidence anchors: [abstract] "Kernel SHAP analysis reveals that feature #10, which correlates with cell morphology and staining characteristics, is the most influential"; [section 3.5/Fig. 3] "Images with high feature #10 values are characterized by large cells with rich cytoplasm and intense red/blue staining with clear nuclear boundaries"
- Break condition: If SHAP importance concentrates on artifacts (e.g., staining batch effects) rather than diagnostic features, the model may be learning spurious correlations.

## Foundational Learning

- Concept: Vision Transformer (ViT) tokenization (patch embedding, class token, positional encoding)
  - Why needed here: Understanding what Class token vs. Image tokens represent is essential for selecting extraction strategies and interpreting SHAP results.
  - Quick check question: Can you explain why the Class token is described as "global" while Image tokens are "local"?

- Concept: Feature importance extraction from tree-based and linear models
  - Why needed here: The feature selection step relies on coefficient magnitudes (logistic regression) and Gini importance (random forest, gradient boosting) to filter dimensions.
  - Quick check question: How would you extract feature importance from a trained random forest classifier in scikit-learn?

- Concept: Class-weighted cross-entropy loss for imbalanced datasets
  - Why needed here: The dataset is heavily imbalanced (Rubbish: 50,371; Healthy: 28,895; Unhealthy: 5,814), and the paper tests weighted vs. unweighted loss.
  - Quick check question: Given class counts [50371, 28895, 5814], what weights would the paper's formula assign?

## Architecture Onboarding

- Component map: Input image → EVA-02 fine-tuning → Token extraction (Class/Image/All) → Feature selection (LR/RF/GB) → ANN classifier (768→1024→512→256→3) → Output (3 classes)

- Critical path:
  1. Fine-tune EVA-02 (AdamW, lr=1e-5, 20 epochs, 3-class output)
  2. Extract tokens from fine-tuned model (try all three extraction modes)
  3. Train ML models for feature selection, apply thresholds
  4. Train ANN on selected features (100 epochs, validation-loss checkpointing)
  5. Run Kernel SHAP on best model to identify top features

- Design tradeoffs:
  - Extraction: Class token (fast, global) vs. Image tokens (local, higher dim) vs. All tokens (comprehensive but potentially redundant)
  - Selection: Aggressive filtering (gradient boosting: 40% removed) vs. conservative (random forest: 2% removed) vs. none ("All selection")
  - Loss: Weighted (better for minority class recall) vs. unweighted (higher overall F1 in some configurations)

- Failure signatures:
  - Overfitting on small "Unhealthy" class: Check per-class recall; weighted loss should help
  - Feature selection discarding useful signals: Compare "All selection" vs. filtered results; large drops indicate over-aggressive filtering
  - SHAP pointing to artifacts: Inspect high/low feature images for batch effects or background noise

- First 3 experiments:
  1. Reproduce baseline: Fine-tune EVA-02 with 3-class output, report F1 on validation split (target: ~0.848)
  2. Ablate extraction methods: Train ANN with Class token, Image tokens, and All tokens (no selection, unweighted loss); compare F1
  3. Feature selection impact: Apply gradient boosting selection (filters ~40%), train ANN, compare to "All selection"—quantify effectiveness vs. complexity tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Will the proposed pipeline maintain its performance advantage over the baseline when evaluated on the challenge's private test set or external clinical datasets?
- Basis in paper: [explicit] The authors state that "final results are not yet confirmed as the evaluation has only been conducted on the public leaderboard" and acknowledge the limitation of the 1.06% effectiveness range.
- Why unresolved: Public leaderboards often suffer from overfitting, and the small margin of improvement (0.35%) may not generalize to unseen data distributions.
- What evidence would resolve it: Successful evaluation of the model on the final private test set of the PS3C challenge and independent validation on external Pap smear datasets.

### Open Question 2
- Question: Can the feature selection process be automated to remove the need for manually determined thresholds?
- Basis in paper: [inferred] The feature selection thresholds were "determined through manual inspection" for Logistic Regression and "manually identifying the point" for Random Forest.
- Why unresolved: Manual threshold selection introduces subjectivity and may not be robust across different data splits or populations.
- What evidence would resolve it: Implementation of an automated hyperparameter optimization strategy for feature selection that matches or exceeds current performance without manual intervention.

### Open Question 3
- Question: Does the model-reliant "feature #10" correspond to specific, quantifiable biomarkers verified by medical experts?
- Basis in paper: [inferred] While the study identifies feature #10 as influential and visually correlates it with "rich cytoplasm," the link remains qualitative.
- Why unresolved: Explainability via SHAP highlights correlations, but does not prove the model is utilizing clinically valid biological features rather than artifacts.
- What evidence would resolve it: A clinical study where pathologists quantitatively score cells for the identified morphological traits to compute a statistical correlation with the model's feature #10 values.

## Limitations
- The interpretability claims rely on qualitative visual descriptions rather than rigorous validation against ground-truth diagnostic criteria
- Feature selection thresholds were determined through manual inspection, introducing subjectivity
- The model's reliance on feature #10 lacks clinical verification that it corresponds to established diagnostic biomarkers

## Confidence

**High Confidence**: The technical implementation details (EVA-02 fine-tuning procedure, ANN architecture specifications, F1-score calculations) appear reproducible and methodologically sound. The improvement over baseline (0.85227 vs 0.84878 F1) is statistically measurable.

**Medium Confidence**: The claim that concatenating Class and Image tokens provides complementary information is plausible but under-validated. The paper shows All tokens performs best, but doesn't systematically compare against single-token baselines or test whether the improvement stems from genuinely complementary information versus increased model capacity.

**Low Confidence**: The clinical interpretability of SHAP results. While the paper links feature #10 to cell morphology and staining, this correlation is descriptive rather than experimentally validated against expert pathologist annotations or established diagnostic criteria.

## Next Checks

1. **Feature Ablation Test**: Systematically remove feature #10 from the input features and measure F1-score degradation. If the drop is minimal (<0.01), this suggests SHAP importance may not translate to actual predictive power.

2. **Cross-Modal Validation**: Compare SHAP-identified important features against established morphometric measurements (cell area, nuclear-to-cytoplasmic ratio, etc.) computed from the same images. Strong correlations would validate the transformer's learned features capture clinically meaningful patterns.

3. **Model Generalization Test**: Evaluate the trained model on an external cervical cell dataset (if available) or via cross-validation on PS3C splits. Significant performance drops would indicate the model may be overfitting to dataset-specific artifacts rather than learning generalizable diagnostic features.