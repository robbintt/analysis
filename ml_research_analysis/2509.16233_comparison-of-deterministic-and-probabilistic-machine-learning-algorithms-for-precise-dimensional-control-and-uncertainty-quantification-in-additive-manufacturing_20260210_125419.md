---
ver: rpa2
title: Comparison of Deterministic and Probabilistic Machine Learning Algorithms for
  Precise Dimensional Control and Uncertainty Quantification in Additive Manufacturing
arxiv_id: '2509.16233'
source_url: https://arxiv.org/abs/2509.16233
tags:
- data
- uncertainty
- training
- probabilistic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares deterministic and probabilistic machine learning
  algorithms for predicting dimensional accuracy in additive manufacturing. The authors
  analyze a dataset of 405 parts with 2025 measurements, using eight input features
  (mix of continuous and categorical variables) to predict Difference from Target
  (DFT) values.
---

# Comparison of Deterministic and Probabilistic Machine Learning Algorithms for Precise Dimensional Control and Uncertainty Quantification in Additive Manufacturing

## Quick Facts
- arXiv ID: 2509.16233
- Source URL: https://arxiv.org/abs/2509.16233
- Reference count: 0
- Deterministic models achieve process-level accuracy (SVR: 53 µm RMSE), GPR achieves 49 µm RMSE with aleatoric uncertainty, BNNs capture both uncertainty types but with lower accuracy (107 µm RMSE)

## Executive Summary
This paper compares deterministic and probabilistic machine learning algorithms for predicting dimensional accuracy in additive manufacturing. The authors analyze a dataset of 405 parts with 2025 measurements, using eight input features (mix of continuous and categorical variables) to predict Difference from Target (DFT) values. Deterministic models (Support Vector Regression and XGBoost) achieve high accuracy, with SVR reaching 53 µm RMSE close to process repeatability. Probabilistic models—Gaussian Process Regression (GPR) and Bayesian Neural Networks (BNNs)—quantify both aleatoric and epistemic uncertainties. GPR achieves 49 µm RMSE, while BNNs, though less accurate (107 µm RMSE), capture both uncertainty types, offering richer uncertainty quantification. The study highlights trade-offs between predictive accuracy, interpretability, and uncertainty quantification, showing that probabilistic models provide valuable insights for robust decision-making in manufacturing.

## Method Summary
The study uses a dataset of 405 additively manufactured parts with 2025 measurements (5 measurements per part). Eight input features include six manufacturing parameters (hardware set, material, thermal cure, layout, x/y/r coordinates) and two feature descriptors (feature class, feature category). Categorical variables are one-hot encoded. The dataset is split 80:20 for training and testing, with dual Monte Carlo subsampling and nested k-fold cross-validation. Deterministic models include SVR with RBF kernel and XGBoost. Probabilistic models include GPR with Matérn kernel plus white noise for aleatoric uncertainty, and BNNs implemented with variational inference to decompose uncertainty into aleatoric and epistemic components. Uncertainty is estimated through weight sampling across 200 iterations.

## Key Results
- Deterministic models achieve process-level accuracy: SVR reaches 53 µm RMSE, comparable to process repeatability (~50 µm)
- GPR achieves highest accuracy at 49 µm RMSE while quantifying aleatoric uncertainty through learned noise variance
- BNNs capture both aleatoric and epistemic uncertainty types but with lower accuracy (107 µm RMSE), showing epistemic uncertainty decreases with more training data
- The dataset's baseline standard deviation is 180 µm, demonstrating significant improvement over naive predictions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Deterministic regression models (SVR, XGBoost) can achieve prediction accuracy approaching physical process limits when trained on sufficient manufacturing data with proper cross-validation.
- **Mechanism:** Non-linear models learn mappings from mixed categorical/continuous manufacturing features to dimensional deviation targets via kernel transformations (SVR) or gradient-boosted ensembles (XGBoost). Dual Monte Carlo subsampling with nested k-fold CV prevents overfitting and provides unbiased performance estimates.
- **Core assumption:** The target variable (DFT) contains learnable systematic variance beyond random noise; sufficient training samples cover the feature space.
- **Evidence anchors:**
  - [abstract] "Deterministic models, trained on 80% of the dataset, provide precise point estimates, with Support Vector Regression (SVR) achieving accuracy close to process repeatability."
  - [section p.13-14] "SVR model demonstrates a remarkable ability to minimize prediction errors, reaching values approximately as low as 53 µm... close to the process repeatability... and significantly outperforms the standard deviation of the data, which stands at 180 µm."
  - [corpus] Weak corpus support for manufacturing-specific deterministic ML; neighbor papers focus on weather/forecasting domains.
- **Break condition:** When epistemic uncertainty is high (limited data regions), point estimates become unreliable without confidence bounds.

### Mechanism 2
- **Claim:** Gaussian Process Regression provides well-calibrated aleatoric uncertainty estimates alongside predictions, achieving accuracy comparable to deterministic methods while quantifying data-inherent noise.
- **Mechanism:** GPR places a prior over functions (Matérn kernel for smoothness + White kernel for noise). The posterior predictive distribution yields both mean predictions and variance estimates, where the learned noise variance represents aleatoric uncertainty.
- **Core assumption:** The data-generating process can be modeled as a smooth function plus independent Gaussian noise; kernel hyperparameters capture the appropriate length scales.
- **Evidence anchors:**
  - [abstract] "GPR achieves 49 µm RMSE... While the GPR model shows high accuracy in predicting feature geometry dimensions... it does not offer insights into model uncertainty."
  - [section p.15-16] "GPR delivers strong predictive performance and interpretability... can estimate the noise variance inherent in the data, that represents aleatoric uncertainty."
  - [corpus] LVM-GP paper confirms GP-based uncertainty quantification as viable approach for PDE solving with noisy data.
- **Break condition:** GPR does not decompose epistemic uncertainty; computational cost scales cubically with dataset size (not suitable for large datasets).

### Mechanism 3
- **Claim:** Bayesian Neural Networks with variational inference and weight sampling can decompose total uncertainty into aleatoric (data noise) and epistemic (model knowledge) components, enabling identification of data-sparse regions.
- **Mechanism:** BNNs place distributions over weights rather than point estimates. Aleatoric uncertainty is computed as mean of output variances across sampled weights (Eq. 2: σ_aleatoric = √(mean(σ²_i))). Epistemic uncertainty is the standard deviation of mean predictions across the ensemble (Eq. 3: σ_epistemic = stddev(μ_i)). As training data increases, epistemic uncertainty decreases.
- **Core assumption:** Variational posterior approximation (Gaussian) sufficiently captures true posterior; sufficient sampling iterations (200+) for stable uncertainty estimates.
- **Evidence anchors:**
  - [abstract] "BNNs, though less accurate (107 µm RMSE), capture both uncertainty types, offering richer uncertainty quantification."
  - [section p.18-19] "Epistemic uncertainty effectively diminishes with an increase in the quantity of training data, namely from 10% to 50% to 80% training percentage."
  - [corpus] Probabilistic Variational Contrastive Learning paper validates ELBO-based variational inference for uncertainty quantification in embedding spaces.
- **Break condition:** BNN ensemble accuracy (107 µm) lags behind GPR (49 µm) and SVR (53 µm); trade-off required between prediction accuracy and uncertainty richness.

## Foundational Learning

- **Concept: Aleatoric vs. Epistemic Uncertainty**
  - **Why needed here:** The paper's core contribution is decomposing these two uncertainty types. Aleatoric is irreducible noise in data (±47 µm material repeatability, ±15 µm measurement uncertainty). Epistemic is reducible uncertainty from limited knowledge/data.
  - **Quick check question:** If you double training data and uncertainty decreases, was it aleatoric or epistemic? (Answer: Epistemic—aleatoric is irreducible.)

- **Concept: Variational Inference vs. MCMC for BNNs**
  - **Why needed here:** The paper uses VI (not MCMC) for scalability. VI approximates the intractable posterior with a simpler distribution by optimizing ELBO, trading accuracy for speed.
  - **Quick check question:** Why would you choose VI over MCMC for a neural network with 1000+ weights? (Answer: MCMC doesn't scale well with parameter count; VI optimizes analytically tractable bounds.)

- **Concept: KL Divergence Regularization in BNNs**
  - **Why needed here:** The paper penalizes divergence between learned posterior and prior distributions. This prevents overconfident predictions and maintains probabilistic calibration.
  - **Quick check question:** What happens if KL divergence regularization is too weak? (Answer: Posterior collapses toward point estimates; uncertainty quantification degrades.)

## Architecture Onboarding

- **Component map:**
  Input layer: 8 features (6 manufacturing parameters + 2 feature descriptors) → one-hot encoded categoricals
  For deterministic path: SVR with RBF kernel (epsilon=0.03) or XGBoost (100 estimators, depth=5)
  For probabilistic path: GPR (Matérn ν=1.5 + White kernel) or BNN ensemble
  BNN ensemble: DenseVariational layers (8 units, sigmoid activation) with Gaussian weight priors → 200 iteration weight sampling → output mean/variance extraction

- **Critical path:**
  1. Load dataset (405 parts → 2025 measurements after feature downsampling)
  2. Split 80/20 train/test with dual Monte Carlo subsampling (50 inner iterations, 3 outer)
  3. For uncertainty: train BNN with negative log-likelihood loss + KL divergence
  4. Sample weights 200× without shuffling input to build prediction ensemble
  5. Compute σ_aleatoric and σ_epistemic from ensemble statistics

- **Design tradeoffs:**
  | Criterion | SVR/XGBoost | GPR | BNN Ensemble |
  |-----------|-------------|-----|--------------|
  | RMSE | 53 µm | 49 µm | 107 µm |
  | Aleatoric UQ | ✗ | ✓ | ✓ |
  | Epistemic UQ | ✗ | ✗ | ✓ |
  | Scalability | High | Low (O(n³)) | Medium |
  | Interpretability | Medium | High | Low |

- **Failure signatures:**
  - Sigmoid-shaped parity plots (Fig. 10a): indicates saturation at extreme predictions; model under-predicts large deviations
  - Epistemic uncertainty not decreasing with more data: check posterior sampling convergence or prior specification
  - Aleatoric uncertainty spikes at certain training fractions (Fig. S4, 90%): uneven train/test split introducing distribution shift

- **First 3 experiments:**
  1. **Baseline replication:** Train SVR on 80% of data with provided hyperparameters. Verify RMSE ≈ 53 µm. If significantly higher, check feature encoding (one-hot for categoricals) and data normalization (MinMax scaling).
  2. **GPR uncertainty validation:** Train GPR with Matérn+White kernel. Extract predictive variance on test set. Correlate variance magnitude with prediction error—well-calibrated models should show larger errors at higher uncertainty.
  3. **BNN ensemble epistemic decay:** Train BNN at 10%, 50%, 80% training fractions. Plot σ_epistemic vs. training fraction. Expected: monotonic decrease. If not observed, increase sampling iterations (beyond 200) or adjust KL regularization weight.

## Open Questions the Paper Calls Out

- **Question:** Can probabilistic models effectively map process parameters to microstructural features to optimize material properties?
  - **Basis in paper:** [explicit] The authors suggest BNNs could facilitate the optimization of printing conditions to minimize defects (e.g., porosity) by predicting microstructural characteristics.
  - **Why unresolved:** The current study focused on macroscopic dimensional accuracy (DFT) using polymer parts, not microscopic structural analysis.
  - **What evidence would resolve it:** Application of the BNN framework to microstructure datasets with validated predictions of grain size or phase distribution.

- **Question:** Can the predictive accuracy of Bayesian Neural Networks (BNNs) be improved to match deterministic models while retaining full uncertainty decomposition?
  - **Basis in paper:** [inferred] The probabilistic BNN showed significantly higher error (107 µm RMSE) compared to SVR (53 µm) and GPR (49 µm).
  - **Why unresolved:** The paper highlights a trade-off where the model capturing both aleatoric and epistemic uncertainty suffers from lower dimensional accuracy.
  - **What evidence would resolve it:** A BNN architecture or variational inference method that achieves accuracy comparable to SVR (<55 µm RMSE) while separating uncertainty types.

- **Question:** What interpretability techniques are required to ensure the trustworthiness of probabilistic regression in industrial settings?
  - **Basis in paper:** [explicit] The authors identify the development of interpretability techniques for complex industrial settings as a "critical facet of future work."
  - **Why unresolved:** Complex probabilistic deep learning models are often opaque ("black boxes"), hindering their adoption in safety-critical manufacturing environments.
  - **What evidence would resolve it:** Integration of explainability methods (e.g., SHAP adaptations for probabilistic layers) that clarify model decisions for process engineers.

## Limitations
- BNN accuracy (107 µm RMSE) significantly lags behind GPR (49 µm) and SVR (53 µm), raising questions about practical utility when prediction accuracy is prioritized
- The dataset size (405 parts, 2025 measurements) may limit generalization to larger-scale manufacturing processes
- No explicit comparison of computational costs between approaches, which is critical for real-world deployment decisions

## Confidence
- **High Confidence:** Deterministic models (SVR, XGBoost) achieving process-level accuracy; GPR uncertainty quantification capability
- **Medium Confidence:** BNN uncertainty decomposition methodology; trade-offs between accuracy and uncertainty quantification
- **Low Confidence:** Practical deployment recommendations; computational efficiency comparisons

## Next Checks
1. **Scale-up validation:** Test models on larger manufacturing datasets (10×-100× more parts) to verify if GPR's cubic scaling remains tractable and if BNN accuracy improves with more data
2. **Cross-process generalization:** Evaluate model performance when transferring to completely new material types, hardware configurations, or part geometries not present in the original dataset
3. **Computational benchmarking:** Measure and compare training/inference times across all four approaches (SVR, XGBoost, GPR, BNN) to quantify practical trade-offs between accuracy and computational cost for real-time manufacturing control systems