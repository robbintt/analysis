---
ver: rpa2
title: Faithful and Fast Influence Function via Advanced Sampling
arxiv_id: '2510.26776'
source_url: https://arxiv.org/abs/2510.26776
tags:
- influence
- sampling
- data
- functions
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of efficient data sampling for
  computing influence functions on black-box AI models. Traditional random sampling
  methods often yield inaccurate influence estimations due to high variance.
---

# Faithful and Fast Influence Function via Advanced Sampling

## Quick Facts
- arXiv ID: 2510.26776
- Source URL: https://arxiv.org/abs/2510.26776
- Reference count: 20
- One-line primary result: Proposed logit-based and feature-based sampling methods improve influence function accuracy while reducing computation time by 30.1% and memory usage by 42.2%

## Executive Summary
This paper addresses the computational bottleneck of influence function estimation for black-box AI models by proposing advanced sampling strategies. Traditional random sampling in Hessian estimation leads to high variance and unreliable influence estimates. The authors introduce feature-based and logit-based samplers that select representative subsets of data based on feature space topology or model confidence scores. Experiments demonstrate significant improvements in estimation accuracy (2.5% F1-score increase) while reducing computational resources by over 30%.

## Method Summary
The paper proposes advanced sampling methods for influence function computation that reduce variance in Hessian estimation. Feature-based sampling uses K-means clustering in feature space (either intrinsic from the model itself or extrinsic from a pre-trained ViT) to select representative samples via top-k or distance-weighted strategies. Logit-based sampling selects samples based on softmax scores per class using multinomial distributions. These samplers are integrated into the LiSSA algorithm for computing inverse-Hessian-vector products. The approach is evaluated on CIFAR-10 with VGG11 and MNIST with AlexNet, focusing on class removal tasks.

## Key Results
- Logit-based sampler achieves most accurate influence estimations with least computational cost
- Proposed methods reduce computation time by 30.1% and memory usage by 42.2%
- F1-score improves by 2.5% compared to random sampling baseline
- Feature-based sampling with K-means centroids provides better coverage than random sampling

## Why This Works (Mechanism)

### Mechanism 1: Representative Sampling Reduces Hessian Estimation Variance
- **Claim**: Selecting data points based on feature space topology produces more robust Hessian approximations than random sampling, reducing variance in influence estimates.
- **Mechanism**: K-means clustering identifies C centroids in feature space; samples are selected as top-k nearest to each centroid or via distance-weighted multinomial distribution. This ensures coverage of the feature distribution rather than arbitrary subsets.
- **Core assumption**: Feature space topology correlates with gradient/Hessian structure—data points close in feature space have similar influence patterns.
- **Evidence anchors**:
  - [abstract]: "samplers select a small yet representative subset of the entire dataset by considering the stochastic distribution of features or logits"
  - [section 3]: "We assume that organizing data points within a latent feature space and selecting samples based on the space topology can avoid the unexpected variance of random samplers"
  - [corpus]: Limited direct support; related work on data attribution (arXiv:2506.06656) addresses sampling challenges but focuses on rescaling rather than topology-aware selection
- **Break condition**: If feature representations do not correlate with how data affects model parameters (e.g., features are decorrelated from loss landscape), topology-based sampling provides no variance reduction benefit.

### Mechanism 2: Logit-Based Sampling Leverages Full Network Information
- **Claim**: Sampling based on softmax outputs uses the entire neural network's learned decision boundaries, yielding more accurate influence estimates with minimal computational overhead.
- **Mechanism**: For each class y, create a multinomial distribution with probability p_xi,y = [softmax(xi;θ)]y, then sample k points per class. This captures the model's internal confidence about each sample's class membership.
- **Core assumption**: Softmax distributions encode sufficient information about model behavior for influence estimation without requiring explicit gradient computation during sampling.
- **Evidence anchors**:
  - [abstract]: "The logit-based sampler, which uses the entire neural network, demonstrates the most accurate estimations with the least computational cost"
  - [section 4, Results]: "the logit yields the most accurate estimation...likely due to its utilization of the entire neural network, unlike other sampling methods"
  - [corpus]: No direct corroboration; related IF literature (arXiv:2506.12965, arXiv:2506.06656) does not address logit-based sampling strategies
- **Break condition**: If the model is severely miscalibrated (softmax confidences do not reflect true posterior probabilities), logit-based sampling may over-represent spurious high-confidence samples.

### Mechanism 3: Improved LiSSA Convergence Through Stable Hessian Estimates
- **Claim**: Better sampling reduces variance in per-iteration Hessian estimates, accelerating LiSSA convergence and reducing total required iterations.
- **Mechanism**: LiSSA iteratively approximates inverse-Hessian-vector product via I_k = I_0 + (I - Ĥ_θ)I_{k-1}. Random sampling produces high-variance Ĥ_θ estimates, requiring more iterations. Representative sampling yields stable Ĥ_θ, enabling faster convergence to tolerance δ.
- **Core assumption**: The Taylor expansion underlying LiSSA converges more reliably when each iteration uses a representative Hessian estimate with lower variance.
- **Evidence anchors**:
  - [section 2]: "random sampling is responsible for inaccurate and unreliable LiSSA iterations. This is due to the high variance of the average loss associated with a limited number of sampling procedures"
  - [section 3]: "we aim to 'optimize' the computational complexity in terms of r by expediting the convergence of the LiSSA algorithm"
  - [corpus]: Broader IF literature acknowledges computational challenges but does not specifically analyze LiSSA variance from sampling (arXiv:2506.06656, arXiv:2308.03296 referenced indirectly)
- **Break condition**: If the Hessian spectrum has poor conditioning independent of sampling quality, variance reduction alone cannot guarantee convergence improvement.

## Foundational Learning

- **Concept: Influence Functions (IFs)**
  - **Why needed here**: Core technique the paper optimizes; IFs measure how training data affects model parameters using gradients and Hessians rather than expensive retraining.
  - **Quick check question**: Explain why IFs approximate leave-one-out retraining without actually retraining the model.

- **Concept: Hessian Matrix and Inverse-Hessian-Vector Product (iHVP)**
  - **Why needed here**: Central computational bottleneck; paper's sampling methods directly target efficient Hessian estimation for iHVP computation.
  - **Quick check question**: Why is computing H^{-1}·v iteratively (LiSSA) more efficient than inverting H directly for large models?

- **Concept: Variance-Bias Tradeoff in Stochastic Estimation**
  - **Why needed here**: Paper's core argument is that random sampling is theoretically unbiased but high-variance in practice; strategic sampling reduces variance while maintaining representativeness.
  - **Quick check question**: If random sampling is unbiased, why does it produce unreliable influence estimates in finite-sample implementations?

## Architecture Onboarding

- **Component map**: Feature Extractor (intrinsic/extrinsic) -> Sampler Module (Top-k/distance-weighted/logit-based) -> LiSSA Iterator -> Influence Evaluator

- **Critical path**:
  1. Forward pass training data to extract features or logits
  2. Apply sampling strategy to select k×C (feature-based) or k×Y (logit-based) representative points
  3. Compute per-sample Hessians for selected subset only
  4. Run LiSSA iterations until ||I_{k+1} - I_k|| ≤ δ
  5. Return influence estimates for target analysis

- **Design tradeoffs**:
  - Intrinsic vs. extrinsic extractors: Intrinsic avoids trust transfer issues; extrinsic (ViT) adds overhead and potential misalignment
  - Top-k vs. distance-weighted: Top-k is deterministic; distance-weighted adds stochasticity for potentially better coverage
  - Feature vs. logit: Feature-based requires K-means; logit-based is cheapest but assumes softmax informativeness
  - Sample count: More samples → higher accuracy but increased compute/memory

- **Failure signatures**:
  - Inconsistent estimates across runs → sampling variance still too high; increase sample count or switch to logit/distance-weighted
  - Self-loss not increasing after class removal → IF not capturing true influence; verify sampling alignment with architecture
  - Memory OOM on large models → reduce sample count or use intrinsic extractor to avoid ViT overhead
  - LiSSA not converging (high iteration count) → Hessian estimate unstable; switch from Top-k to stochastic samplers

- **First 3 experiments**:
  1. Reproduce random vs. logit sampling on CIFAR-10/VGG11 with sample counts [100, 400, 800, 1200, 1600]; verify F1-score and self-accuracy curves align with Figure 2
  2. Ablate intrinsic vs. extrinsic feature extractors with Top-k sampling; measure feature extraction overhead and F1-score difference
  3. Test logit sampling across IF variants (PIF, GIF, FIF, SIF) using Table 1/2 configurations; confirm consistent improvement over random baseline

## Open Questions the Paper Calls Out

- **Open Question 1**: Does specifically sampling the removal data subset improve the efficiency of class unlearning tasks?
  - **Basis in paper**: [explicit] The authors state, "we aim to explore the efficacy of sampling removal data in expediting class unlearning tasks."
  - **Why unresolved**: The current work focuses on sampling the training data to approximate the Hessian for influence estimation, but it does not specifically optimize the sampling of the data targeted for removal.
  - **What evidence would resolve it**: Experiments demonstrating that an advanced sampling strategy for the unlearned set reduces computation time or improves accuracy in machine unlearning benchmarks compared to random removal.

- **Open Question 2**: Can an effective update rule be devised for influence functions that operates accurately with update rates far exceeding the theoretical small-perturbation assumption?
  - **Basis in paper**: [explicit] The authors list a future plan to "devise an effective update rule for influence functions with a much larger update rate than the theoretical value."
  - **Why unresolved**: Standard influence functions rely on Taylor expansions assuming infinitesimal changes (ε → 0); large updates typically violate these linearity assumptions, leading to estimation errors.
  - **What evidence would resolve it**: A modified influence function formulation that maintains high fidelity to leave-one-out retraining results even when the upweighting parameter ε is significantly increased.

- **Open Question 3**: Do the proposed logit-based and feature-based sampling methods maintain their computational advantages and accuracy when applied to Large Language Models (LLMs) with billions of parameters?
  - **Basis in paper**: [inferred] The introduction identifies "hyperscale AI systems, such as large language models (LLMs)," as a primary motivation for efficient influence functions, yet experiments are restricted to smaller CNNs (VGG11, AlexNet) on vision datasets.
  - **Why unresolved**: The memory overhead of K-means clustering (feature-based) or the computational cost of logit extraction may scale differently or become prohibitive in LLM architectures compared to the tested VGG11 model.
  - **What evidence would resolve it**: Benchmark results showing the proposed samplers reduce memory/runtime and improve estimation fidelity (e.g., F1-score) when applied to transformer-based models (e.g., GPT, Llama) on NLP tasks.

## Limitations

- Core mechanism relies on assumption that feature or logit topology correlates with influence patterns, which is not directly validated
- Claimed efficiency improvements lack statistical error bars or significance testing across multiple seeds
- Paper does not address whether proposed sampling strategies generalize beyond tested architectures
- Relationship between LiSSA convergence improvements and sampling variance reduction is asserted but not experimentally isolated

## Confidence

- **High confidence**: Mathematical formulation of LiSSA and influence functions is correct; experimental methodology is sound
- **Medium confidence**: Empirical improvements in F1-score (2.5%) and efficiency metrics are reproducible if hyperparameters match, but variance of estimates is unclear
- **Low confidence**: Claim that logit-based sampling uses the "entire neural network" to achieve minimal computational cost conflates network usage with computational efficiency; actual computational complexity comparison is not provided

## Next Checks

1. **Ablation study on LiSSA iterations**: Fix a sampling strategy (e.g., logit) and measure LiSSA convergence (iterations to tolerance) and final influence accuracy across different sample counts (k). This isolates whether sampling reduces variance per iteration or simply provides more data.

2. **Feature space coverage analysis**: Visualize or quantify the coverage of feature/logit space by each sampling method (e.g., using coverage metrics like 1 - vol(union(S))/vol(union(X))). Confirm that topology-aware sampling achieves better coverage than random.

3. **Statistical significance testing**: Re-run the CIFAR-10/VGG11 experiments with at least 25 seeds as claimed, and report 95% confidence intervals for F1-score, time, and memory metrics. Verify that improvements are statistically significant.