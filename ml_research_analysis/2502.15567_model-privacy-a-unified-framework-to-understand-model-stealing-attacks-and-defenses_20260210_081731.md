---
ver: rpa2
title: 'Model Privacy: A Unified Framework to Understand Model Stealing Attacks and
  Defenses'
arxiv_id: '2502.15567'
source_url: https://arxiv.org/abs/2502.15567
tags:
- defense
- attacker
- privacy
- defender
- stealing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a statistical framework called model privacy
  to systematically analyze and defend against model stealing attacks, where adversaries
  attempt to reconstruct a machine learning model by querying its predictions. The
  authors formalize the threat model and objectives for both attackers and defenders,
  and propose methods to quantify attack effectiveness and defense performance through
  statistical risk.
---

# Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses

## Quick Facts
- **arXiv ID:** 2502.15567
- **Source URL:** https://arxiv.org/abs/2502.15567
- **Reference count:** 5
- **Primary result:** Develops a statistical framework showing query-dependent perturbations outperform independent noise for defending against model stealing attacks

## Executive Summary
This paper introduces a unified statistical framework called "model privacy" to analyze and defend against model stealing attacks, where adversaries reconstruct machine learning models by querying predictions. The authors formalize attacker and defender objectives, quantify attack effectiveness through statistical risk, and demonstrate that defenses using attack-specific, query-dependent perturbations are far more effective than simple noise addition. Through theoretical analysis and experiments on polynomial regression and classification tasks, they show carefully designed defenses can significantly enhance model privacy while maintaining utility for benign users.

## Method Summary
The framework treats model stealing as a statistical learning problem where the defender adds perturbations to model outputs to protect against query-based reconstruction. For regression tasks, attackers use polynomial regression with model selection via GIC, while for classification, attackers use neural networks on sentence embeddings. Defenses include IID noise, constant shifts, long-range correlated noise, and attack-specific methods like "Order Disguise" for polynomial learners and "Misleading Shift" for classifiers. The framework quantifies privacy-utility tradeoffs through minimax statistical risk, with experiments on synthetic data (quadratic functions, high-dimensional regression) and real-world text classification datasets.

## Key Results
- Query-dependent perturbations exploiting attacker learning structure (e.g., Order Disguise) achieve unboundedly better privacy-utility tradeoffs than IID noise
- Long-range dependent noise provides stronger worst-case privacy guarantees than IID noise against unknown attackers
- Constant shift perturbations achieve rate-optimal privacy for k-NN attackers and linear classifiers
- Defenses adding independent noise are typically ineffective, while attack-specific, query-dependent perturbations are crucial for robust protection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query-dependent perturbations outperform independent noise for defense because they exploit attacker learning structure.
- Mechanism: The defender constructs perturbations correlated across queries to create misleading structure. For polynomial regression attackers, "Order Disguise" adds a combination of perturbation vectors (e₁, e₂) designed to make the attacker overfit to a higher-order polynomial than the true model, achieving privacy level PLₙ ≳ kₙ^γ Uₙ which can grow unboundedly relative to utility loss.
- Core assumption: The defender knows the attacker's learning algorithm class (e.g., polynomial regression with model selection via GIC) and can compute projection operators on query matrices.
- Evidence anchors:
  - [abstract] "...highlighting the importance of the attack-specific, query-dependent perturbations for effective defenses"
  - [Section 3.2, Theorem 2] "PLₙ(M_Poly, F_poly | Q_IID, T_poly) ≳ kₙ^γ Uₙ" with gain-to-loss ratio potentially approaching infinity
  - [corpus] BESA (arXiv:2506.04556) acknowledges perturbation-based defenses hinder attack performance, requiring recovery modules—indirect corroboration that structured perturbations increase attack cost
- Break condition: Fails when attacker uses a learning algorithm outside the assumed class, or when queries arrive sequentially and must be answered immediately (cannot compute batch-dependent perturbations).

### Mechanism 2
- Claim: Long-range dependent noise provides stronger worst-case privacy guarantees than IID noise against unknown attackers.
- Mechanism: Correlated perturbations with autocorrelation r(k) ≍ k^(-γ) for γ ∈ (0,1) slow the attacker's convergence rate to PLₙ ≍ (σ²ₙ/n)^(2α/(2α+d)) + σ²ₙ n^(-γ), adding a second term that scales with noise correlation strength. Smaller γ (stronger correlation) yields higher privacy.
- Core assumption: Assumptions 1-3 hold (IID batch queries from bounded input space with positive density); the Hölder smoothness α of the target function bounds optimal learning rates.
- Evidence anchors:
  - [Section 4.1, Theorem 5] Explicit rate derivation showing correlated noise adds n^(-γ) penalty term
  - [Section 5.1.1, Figure 1] Experimental confirmation that Long-Range Correlated Noising outperforms IID Noising but underperforms attack-specific defenses
  - [corpus] No direct corpus corroboration for long-range dependent noise mechanism; corpus focuses on gradient-based and prediction-perturbation defenses rather than correlation structure
- Break condition: Fails if attacker uses equally-spaced fixed design queries in 1D, where the rate becomes PLₙ ≍ n^(-2αγ/(2α+γ)), slower than IID batch queries.

### Mechanism 3
- Claim: Constant shift perturbations achieve rate-optimal privacy for k-NN attackers and linear classifiers.
- Mechanism: A constant perturbation τₙ = ±√Uₙ added to all responses creates systematic bias the attacker learns alongside the target function. For k-NN on Hölder functions, PLₙ ≍ Uₙ, matching the minimax upper bound for any defense with utility loss Uₙ.
- Core assumption: The attacker does not know or cannot remove the constant bias; the query distribution has positive density on a bounded domain.
- Evidence anchors:
  - [Section 3.1, Theorem 1(iii-iv)] M_Const achieves PLₙ ≍ Uₙ, and no defense can exceed this rate
  - [Section 4.2, Theorem 7(iii)] For binary classification with linear boundaries, constant shift achieves PLₙ ≍ Uₙ while IID label flipping provides no benefit over no defense
  - [corpus] Model-Guardian (arXiv:2503.18081) uses "deceptive predictions" conceptually similar to systematic perturbations, though implementation differs
- Break condition: Fails when attacker can estimate and subtract the constant bias (e.g., via large query budget or prior knowledge of defense strategy).

## Foundational Learning

- **Statistical Risk Minimax Theory**
  - Why needed here: The framework quantifies defenses via worst-case privacy level PLₙ(M, F | Q, T) = inf sup E[ℓ(f*, f̂ₙ)], requiring understanding of how learning rates depend on function class complexity (Hölder exponent α), dimension d, and noise structure.
  - Quick check question: Given a defense with utility loss Uₙ = n^(-0.1) and IID noise, what is the privacy level against a k-NN attacker on 1D Lipschitz functions?

- **Perturbation Dependence Structures**
  - Why needed here: The core thesis distinguishes IID vs. correlated vs. query-dependent perturbations. Long-range dependence with Hurst parameter H > 0.5 creates sum covariance Σ|r(k)| = ∞, fundamentally altering learning rates compared to short-range dependence.
  - Quick check question: Why does IID Gaussian noise yield convergence rate n^(-2α/(2α+d)) while long-range dependent noise adds an n^(-γ) term?

- **Overfitting via Model Mis-specification**
  - Why needed here: Order Disguise works by inducing the attacker to select a higher polynomial order than the true p, inflating estimation error. Understanding AIC/BIC/GIC model selection behavior under adversarial perturbations is essential.
  - Quick check question: If an attacker uses GIC with penalty λₙ, what minimum utility loss Uₙ is required to trigger overfitting to order k > p?

## Architecture Onboarding

- **Component map:**
  ```
  Query Interface → Defense Mechanism Module → Perturbed Response
                           ↓
  [Defense Selector] ← Attacker Knowledge (known/unknown algorithm)
                           ↓
  [Perturbation Generator]: IID / Correlated / Query-Dependent variants
                           ↓
  [Utility Monitor]: Ensures uₙ(M) ≤ Uₙ constraint
  ```

- **Critical path:**
  1. Receive batch of n queries {X₁, ..., Xₙ}
  2. Identify or assume attacker learning algorithm (T ∈ {k-NN, polynomial, neural network, unknown})
  3. Select defense from M ∈ {M_IID, M_Const, M_γ, M_Poly, MVP, Misleading Shift}
  4. Compute perturbation e satisfying ||e||²/n ≤ Uₙ
  5. Return Ŷᵢ = f*(Xᵢ) + eᵢ

- **Design tradeoffs:**
  - **Known vs. Unknown Attacker:** M_Poly requires knowing attacker uses polynomial regression; M_γ (correlated noise) works against arbitrary algorithms but achieves lower privacy levels
  - **Batch vs. Sequential Queries:** Batch enables query-dependent perturbations; sequential constrains to causal noise (must compute eᵢ without Xⱼ for j > i)
  - **Hard vs. Soft Labels:** Classification defenses differ fundamentally—random label flipping (M_IID) provides minimal protection while constant boundary shift (M_Const) is rate-optimal

- **Failure signatures:**
  - Attacker accuracy matches undefended baseline despite defense → likely using IID/weak defense against determined attacker
  - Privacy level PLₙ decreases with n while Uₙ fixed → perturbation magnitude insufficient; increase Uₙ budget
  - Benign user complaints on accuracy → Uₙ too large; verify utility loss calculation matches user query distribution

- **First 3 experiments:**
  1. **Baseline calibration:** Deploy M_NO, M_IID, M_Const against k-NN attacker on synthetic Hölder functions with α = 0.5, d = 2. Measure PLₙ at n = {100, 500, 1000} with Uₙ = 0.1. Expected: M_Const ≍ Uₙ, M_IID ≍ (Uₙ/n)^(2α/(2α+d)).
  2. **Query-dependent defense validation:** Implement M_Poly on quadratic f*(x) = (2x-1)² with Beta(1,3) query distribution. Compare attacker model selection (AIC) order under M_Poly vs. M_IID. Expected: M_Poly induces order kₙ → ∞.
  3. **Real-world stress test:** Deploy Misleading Shift on a text classifier (e.g., toxicity detection) with n = {1000, 5000} queries. Compare attacker F1 score under Random Shuffle vs. Misleading Shift at Uₙ = 0.05. Expected: Misleading Shift drives F1 toward zero by exaggerating majority class.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can optimal attack and defense strategies be designed for sequential query strategies where the attacker adapts queries based on previous responses?
- Basis in paper: [explicit] The conclusion states that while the paper focuses on batch query strategies, "designing optimal attacks and defenses under sequential query strategies remains an open yet crucial challenge."
- Why unresolved: The current theoretical framework primarily assumes IID batch queries to facilitate analysis using classical statistical tools, whereas sequential queries introduce complex dependencies.
- What evidence would resolve it: A theoretical derivation of privacy levels and convergence rates for defense mechanisms in an adaptive, non-IID query setting.

### Open Question 2
- Question: How can the model privacy framework be extended to protect model-related quantities such as hyper-parameters and training data?
- Basis in paper: [explicit] The conclusion suggests an extension is needed to "modify the evaluation criteria to include the protection of any model-related quantities" like parameters and training data.
- Why unresolved: The current framework focuses on the similarity between the reconstructed function and the defender's function (f*), ignoring the risk of revealing internal parameters or training samples (model inversion).
- What evidence would resolve it: A modified formulation of "privacy level" that quantifies the risk of leaking hyper-parameters or training data statistics under specific defense mechanisms.

### Open Question 3
- Question: How can defense mechanisms be adapted when the attacker aims to steal the underlying data distribution function (f_N) rather than the defender's specific trained model (f*)?
- Basis in paper: [explicit] The conclusion notes that an attacker may have "a more ambitious goal of stealing the function f_N underlying the training data" rather than just the model f*.
- Why unresolved: Current defenses target the approximation of f*; however, if an attacker uses the queries to infer the ground truth f_N, defenses optimized to obscure f* might be less effective.
- What evidence would resolve it: Theoretical bounds on the divergence between the attacker's reconstruction and the underlying truth f_N when applying defenses designed under the current framework.

## Limitations

- The framework assumes batch query processing, which may not hold in sequential or online deployment scenarios
- The effectiveness of query-dependent defenses like "Order Disguise" critically depends on accurate knowledge of the attacker's algorithm
- Experimental validation is limited to specific settings (1D regression, synthetic data) with limited real-world complexity

## Confidence

- **High Confidence:** The statistical risk framework for quantifying privacy-utility tradeoffs is mathematically sound. The minimax optimality of constant shift defenses for k-NN and linear classifiers is well-established.
- **Medium Confidence:** The superiority of query-dependent over IID perturbations is theoretically proven but relies on assumptions about attacker algorithm knowledge. Long-range dependent noise benefits are demonstrated but require specific data distribution assumptions.
- **Low Confidence:** Real-world applicability of attack-specific defenses without accurate attacker modeling, and scalability to high-dimensional complex models (e.g., deep neural networks).

## Next Checks

1. Test Order Disguise against adaptive attackers that can detect and compensate for polynomial order manipulation through ensemble methods or higher-order polynomial fitting.
2. Evaluate Long-Range Correlated Noising on high-dimensional data (d > 10) to verify theoretical scaling predictions under realistic query distributions.
3. Implement the framework in a sequential query setting where perturbations must be computed causally, measuring degradation in privacy guarantees compared to batch processing.