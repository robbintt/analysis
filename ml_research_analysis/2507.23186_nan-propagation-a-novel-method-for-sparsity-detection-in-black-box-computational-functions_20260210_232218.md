---
ver: rpa2
title: 'NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational
  Functions'
arxiv_id: '2507.23186'
source_url: https://arxiv.org/abs/2507.23186
tags:
- sparsity
- function
- black-box
- pattern
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NaN-propagation offers a novel approach for sparsity detection
  in black-box computational functions by exploiting IEEE 754 NaN contamination properties.
  The method systematically contaminates inputs with NaN values and observes which
  outputs become NaN, enabling conservative reconstruction of sparsity patterns without
  requiring function introspection.
---

# NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions

## Quick Facts
- arXiv ID: 2507.23186
- Source URL: https://arxiv.org/abs/2507.23186
- Reference count: 8
- Primary result: Eliminates coincidental zero-gradient false negatives in sparsity detection through IEEE 754 NaN contamination

## Executive Summary
NaN-propagation introduces a novel approach for detecting sparsity patterns in black-box computational functions by exploiting IEEE 754 NaN contamination properties. The method systematically replaces each input with NaN while holding others fixed, observing which outputs become NaN to reconstruct dependencies without requiring function introspection. When applied to the TASOPT wing weight model, it uncovered dozens of dependencies missed by conventional finite-difference approaches and achieved a 1.52× speedup in gradient computation. Advanced strategies including NaN payload encoding enable faster-than-linear time complexity for sparse problems, while chunking exploits locality patterns in engineering code.

## Method Summary
NaN-propagation detects input-output dependencies by replacing each input with IEEE 754 NaN values and observing which outputs become NaN, exploiting the universal contamination property of NaN arithmetic. The method creates n contaminated input vectors (each with one input replaced by NaN), evaluates the black-box function, and records which outputs are NaN to build a binary sparsity matrix. This matrix is then compressed using graph coloring to accelerate subsequent gradient computations. The approach eliminates the false-negative failure mode of coincidental zero gradients that affects existing finite-difference methods, at the cost of potential false positives from self-canceling expressions.

## Key Results
- Uncovered dozens of dependencies in TASOPT wing weight model missed by finite-difference methods
- Achieved 1.52× speedup in gradient computation through sparsity exploitation
- Payload encoding strategy theoretically enables O(log N) time complexity for sparse problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NaN-contamination detects input-output dependencies in black-box functions by exploiting IEEE 754 propagation behavior.
- Mechanism: IEEE 754 specifies that NaN values "contaminate" subsequent arithmetic operations—any dyadic operation with a NaN operand returns NaN. By systematically replacing each input with NaN while holding others fixed, observing which outputs become NaN reveals which inputs influence those outputs.
- Core assumption: The black-box function does not internally catch, suppress, or overwrite NaN values before they reach outputs.
- Evidence anchors:
  - [abstract] "exploits the universal contamination property of IEEE 754 Not-a-Number values to trace input-output dependencies"
  - [Section 2] "a dyadic operation with just one NaN operand will return NaN. This behavior is explicitly part of math library APIs that follow the IEEE 754 standard"
  - [corpus] No directly relevant corpus papers on NaN-propagation; related work concerns false negatives in unrelated domains.
- Break condition: Functions with internal NaN handling (error-raising, value overwriting, or aggressive whole-vector contamination) will fail or produce unreliable results.

### Mechanism 2
- Claim: Conservative sparsity patterns from NaN-propagation eliminate the coincidental zero-gradient false-negative failure mode.
- Mechanism: Finite-difference methods evaluate gradients at a single point; if ∂f/∂x = 0 at that point (e.g., f(x) = x² at x = 0), the method incorrectly infers independence. NaN-propagation detects dependency regardless of gradient magnitude because NaN contamination propagates through the computational path, not through derivative values.
- Core assumption: The function's dependency structure is stable across the input space (not pathologically piecewise).
- Evidence anchors:
  - [abstract] "eliminates the false-negative failure mode of coincidental zero gradients that affects existing finite-difference methods"
  - [Section 1.2] "if the evaluation point of x = 0 is chosen... a central finite difference will see zero gradient and incorrectly infer no dependency"
  - [Section 6] Figure 6 shows dozens of false negatives (marked X) in finite-difference results that NaN-propagation correctly detected.
  - [corpus] Weak relevance; corpus papers address false negatives in recommendation and verification, not numerical sparsity.
- Break condition: Pathological piecewise functions with arbitrarily small regions of dependency may still yield false negatives if the representative input misses all dependency regions.

### Mechanism 3
- Claim: NaN payload encoding enables faster-than-O(N) sparsity detection by simultaneously tracing multiple inputs.
- Mechanism: IEEE 754 NaN values have unused "payload" bits (~22 bits in float32, ~51 in float64). By encoding unique identifiers in these bits, multiple inputs can be tagged with distinguishable NaN values in a single function evaluation. Observing which payloads appear in outputs reveals multiple dependencies simultaneously.
- Core assumption: Math libraries propagate one of the input NaN payloads through operations (which one varies by operation and library), creating a "shadowing" effect that limits simultaneous inference.
- Evidence anchors:
  - [abstract] "Advanced strategies including NaN payload encoding enable faster-than-linear time complexity for sparse problems"
  - [Section 7.1] "single-precision NaN representations allow us to encode nearly 22 bits of information... these unique NaN payloads are propagated through mathematical operations"
  - [Section 7.1] "best-case time complexity should approach O(log(N))... worst-case time complexity is O(N)"
  - [corpus] No corpus papers on NaN payload encoding; this appears to be a novel contribution.
- Break condition: Dense sparsity patterns cause significant shadowing (one detected dependency masks others), reducing speedup. Optimal column-selection algorithms for payload encoding remain future work.

## Foundational Learning

- Concept: IEEE 754 floating-point standard and NaN semantics
  - Why needed here: The entire method relies on understanding how NaN propagates (or doesn't) through arithmetic. Without this, you cannot predict which functions will be compatible.
  - Quick check question: Given `a = NaN`, what is the result of `a * 0`, `a - a`, and `sqrt(a)`?

- Concept: Jacobian matrices and finite-difference gradient approximation
  - Why needed here: Sparsity detection is motivated by accelerating gradient computation. You need to understand why computing ∂f/∂x for each of n inputs costs O(n) evaluations.
  - Quick check question: Why does forward finite-difference require n+1 function evaluations for n inputs?

- Concept: Graph coloring for Jacobian compression
  - Why needed here: The sparsity pattern enables compression via coloring—you must understand why independent columns can share a perturbation direction.
  - Quick check question: If inputs x₁ and x₂ never jointly influence any output, why can their finite-difference perturbations be combined?

## Architecture Onboarding

- Component map:
  1. Input preparation: Baseline representative input vector + NaN-contamination generator
  2. Black-box evaluator: Wrapper that calls the target function with contaminated inputs
  3. NaN observer: Checks which outputs are NaN; builds binary sparsity matrix
  4. Jacobian compressor: Converts sparsity → adjacency matrix (via Gramian) → graph coloring → compressed perturbation seeds
  5. Advanced modes (optional): Payload encoder (bit manipulation), chunking engine (adjacent input grouping)

- Critical path:
  1. Obtain representative input (typically initial guess from optimization context)
  2. For each input index i: set input[i] = NaN, evaluate function, record NaN outputs
  3. Build sparsity matrix S where S[i,j] = 1 if output j is NaN when input i is NaN
  4. Compute adjacency matrix via SᵀS, apply graph coloring
  5. Use compressed coloring to reduce future gradient evaluations

- Design tradeoffs:
  - False negatives vs. false positives: NaN-propagation trades potential false positives (from self-canceling expressions like `x - x`) for guaranteed elimination of coincidental-zero false negatives. Paper notes false positives are less harmful (slower gradients, but numerically correct).
  - Evaluation cost vs. sparsity accuracy: Payload encoding and chunking reduce evaluations but introduce shadowing and over-conservative patterns respectively.
  - Generality vs. robustness: Operator-overloading methods (Julia-specific) are more robust but less general; NaN-propagation works across languages but fails on functions with internal NaN handling.

- Failure signatures:
  - Function raises error on NaN input: Immediate detection; fall back to finite-difference method.
  - Overly aggressive NaN propagation: Entire output vector becomes NaN for any input NaN → results in all-false-positives sparsity (still usable but no speedup).
  - Silent NaN overwriting: Magic-number returns without warning → false negatives (rare but dangerous).
  - Branching code paths: Different sparsity patterns for different discrete input values; use greedy union algorithm.

- First 3 experiments:
  1. Baseline validation on known function: Implement NaN-propagation on a simple function with known sparsity (e.g., `f(x,y,z) = [x², y+z, x*y]`). Verify that contaminating x, y, z individually produces correct NaN patterns and that self-canceling terms (if added) produce expected false positives.
  2. Compatibility test on target black-box: Run single NaN-contaminated evaluation on your actual black-box. Check: does it raise an error? Return NaN in expected outputs? Return all NaNs? This determines if the method is applicable.
  3. End-to-end speedup measurement: Run full sparsity trace on a representative engineering model (or the TASOPT wing weight model if available). Compare: (a) detection time, (b) resulting compressed Jacobian size, (c) subsequent gradient computation time vs. dense finite-difference baseline. Target: confirm ~1.5× speedup or better as reported in Section 2.3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal algorithm for selecting input columns to simultaneously encode with NaN payloads?
- Basis in paper: [explicit] Section 7.1 states, "Development of an efficient column selection algorithm is left as an area of future work."
- Why unresolved: The "shadowing effect" (where one detected dependency obscures others) makes this distinct from standard vertex coloring problems, rendering the optimization non-trivial.
- What evidence would resolve it: An algorithm that minimizes function evaluations by dynamically selecting column groups based on the estimated density of the sparsity pattern, potentially achieving the hypothesized O(log(N)) best-case complexity.

### Open Question 2
- Question: Can hybrid approaches effectively combine NaN-propagation with finite-difference methods to mitigate their respective failure modes?
- Basis in paper: [explicit] Section 9 lists "creating hybrid approaches that combine multiple sparsity detection methods" as a primary direction for future work.
- Why unresolved: NaN-propagation eliminates false negatives (coincidental zeros) but introduces false positives (mathematical cancellation), whereas standard methods suffer the opposite failures.
- What evidence would resolve it: A framework that uses NaN-propagation to establish conservative bounds and finite-differences to prune false positives (e.g., $x-x$), proving more accurate than either method alone without risking silent gradient corruption.

### Open Question 3
- Question: How can sparsity detection be robustified for functions with branching execution on continuous variables?
- Basis in paper: [inferred] Section 5 notes the proposed greedy algorithm is a heuristic, and Section 4.3 admits that "piecewise branching on continuous variables remains problematic."
- Why unresolved: Current heuristics assume "flag" inputs are discrete and static; branching based on continuous values implies an infinite set of possible sparsity patterns.
- What evidence would resolve it: A method that guarantees a valid sparsity pattern across continuous input domains, or formally quantifies the risk of false negatives in continuous branching regimes.

## Limitations
- Black-box compatibility uncertainty: Method assumes IEEE 754 NaN behavior without internal suppression, but real engineering codes may catch NaN exceptions, overwrite them, or exhibit library-specific propagation quirks
- Empirical validation gap: Faster-than-linear time complexity through payload encoding is theoretically justified but not empirically demonstrated for large-scale problems
- Shadowing effect severity: Dense sparsity patterns cause significant shadowing where one detected dependency masks others, reducing theoretical speedup

## Confidence

- **High**: The core mechanism (Mechanism 1) of using IEEE 754 NaN contamination to trace dependencies is well-grounded and directly supported by the standard's specification.
- **Medium**: The elimination of coincidental-zero false negatives (Mechanism 2) is logically sound and demonstrated on the TASOPT model, but the paper doesn't quantify how often this occurs in practice versus other sources of sparsity estimation error.
- **Medium**: The payload encoding strategy (Mechanism 3) is theoretically valid but lacks empirical validation on large-scale problems, and optimal algorithms for payload selection remain future work.

## Next Checks

1. **Compatibility Screening Protocol**: Develop and test a systematic protocol for black-box functions to detect NaN handling behavior before attempting full sparsity detection. This should include error-raising detection, whole-vector contamination testing, and selective output checking.

2. **Payload Encoding Performance**: Implement and benchmark the payload encoding strategy on a suite of problems with varying sparsity densities. Measure actual speedup vs. theoretical O(log N) predictions, and quantify shadowing effects across different sparsity patterns.

3. **Generalization Testing**: Apply NaN-propagation to 3-5 diverse black-box functions from different domains (e.g., CFD solvers, finite element codes, economic models) to assess robustness across codebases with varying NaN handling practices and computational patterns.