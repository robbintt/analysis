---
ver: rpa2
title: Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document
  Summarization
arxiv_id: '2509.09852'
source_url: https://arxiv.org/abs/2509.09852
tags:
- topic
- summarization
- source
- computational
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a topic-guided reinforcement learning approach
  to enhance multi-document summarization (MDS) by improving content selection and
  topical relevance. The authors show that prompting LLMs with topic labels improves
  summary informativeness and propose a novel topic reward within the GRPO framework
  to measure topic alignment between generated summaries and source documents.
---

# Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization

## Quick Facts
- arXiv ID: 2509.09852
- Source URL: https://arxiv.org/abs/2509.09852
- Reference count: 35
- Primary result: Topic-guided RL improves MDS by enhancing content selection and topical relevance through explicit topic alignment rewards

## Executive Summary
This paper introduces a topic-guided reinforcement learning approach to enhance multi-document summarization (MDS) by improving content selection and topical relevance. The authors show that prompting LLMs with topic labels improves summary informativeness and propose a novel topic reward within the GRPO framework to measure topic alignment between generated summaries and source documents. Experimental results on Multi-News and Multi-XScience datasets demonstrate consistent improvements over strong baselines. Using Qwen2.5-0.5B as the policy model with Qwen2.5-7B for topic extraction, the approach achieves higher ROUGE scores, better embedding-based similarity, and superior topic alignment metrics compared to both reference-free and reference-based methods.

## Method Summary
The method combines topic extraction with reinforcement learning to guide multi-document summarization. A frozen teacher LLM (Qwen2.5-7B) extracts topic phrases from source documents, which are embedded and used to compute a reference-free topic alignment reward. The policy model (Qwen2.5-0.5B-Instruct) generates summaries using GRPO, where the reward combines topic-F1 (harmonic mean of coverage and precision) with length penalty. Topic phrases are extracted from both source documents and generated summaries, embedded using SentenceTransformer, and compared via cosine similarity. The GRPO framework computes group-relative advantages without requiring a separate value model, and rewards are weighted by inverse standard deviation to balance objectives.

## Key Results
- Topic-guided RL improves ROUGE-1/2/L scores by 0.3-1.8 points over baselines on Multi-News and Multi-XScience
- The approach achieves superior topic alignment metrics compared to reference-based methods like RL_ROUGE and RL_HUMAN-FEEDBACK
- Smaller models (0.5B) with topic guidance outperform larger ones (7B) without such guidance on topic alignment tasks
- Human evaluation confirms that RL-trained models generate more topically aligned summaries

## Why This Works (Mechanism)

### Mechanism 1: Topic Extraction as High-Level Content Filtering
Topic phrases provide compressed semantic anchors that guide both prompt-based inference and RL training toward salient content. A teacher LLM extracts n topic labels from each source document, which are embedded and compared to summary-extracted topics using cosine similarity. The harmonic mean of Coverage and Precision yields a reference-free reward. This assumes topic-level similarity correlates with summary informativeness and relevance.

### Mechanism 2: Group Relative Policy Optimization with Topic-F1 Reward
GRPO enables stable RL fine-tuning by computing relative advantages within groups of sampled summaries, avoiding a separate value model. For each input, generate G candidate summaries and compute R_total (weighted topic-F1 + length penalty). Advantages are normalized within the group, and the clipped surrogate objective with KL penalty updates the policy while regularizing against reference divergence.

### Mechanism 3: Inverse Standard Deviation Weighting for Multi-Objective Balance
Weighting rewards by inverse standard deviation stabilizes training when combining heterogeneous signals. Compute reward variances on a mini-batch, assign initial weights w_r = 1/σ_r, apply emphasis factors, and normalize. This adaptively up-weights stable rewards and down-weights noisy ones.

## Foundational Learning

- **Topic Modeling for Summarization**: Topic phrases serve as the core supervision signal; understanding extraction, embedding, and alignment is prerequisite. Quick check: Given a document, can you identify 5-10 phrases that capture its main themes without redundancy?

- **Reinforcement Learning from Reward Functions (Not Just Human Feedback)**: The method uses a reference-free reward, not human labels; understanding policy gradient advantages is essential. Quick check: Explain how GRPO computes advantages without a value model and why this matters for stability.

- **Multi-Document Summarization Challenges**: MDS requires cross-document synthesis, coherence, and topical consistency—distinct from single-document settings. Quick check: Why might a summary that copies the first paragraph of a source document score high on entailment but fail as an MDS summary?

## Architecture Onboarding

**Component map**: Topic Extractor (7B) -> Embedding Model (SentenceTransformer) -> Reward Calculator (Coverage/Precision/F1) -> Weighting Module (inverse std dev) -> Policy Model (0.5B) -> GRPO Trainer

**Critical path**: Pre-extract source document topics using 7B model; cache. For each training batch: policy generates G summaries per input. Extract topics from each summary; embed and compute topic-F1 vs each source doc; average across docs. Compute length penalty; combine rewards via weighted sum. Compute group advantages; update policy with KL regularization.

**Design tradeoffs**: Teacher model size (7B vs 0.5B), number of topics (n=10 for Multi-News, n=5 for Multi-XScience), group size G (8 due to OOM constraints), reward combination (topic-only vs adding ROUGE).

**Failure signatures**: Degenerate outputs (SFT shows >3% repetitive/overlong; RL models with topic cues show <0.2%), reward hacking (not observed in manual check of 200 samples), topic drift (if summary topics diverge from source topics, topic-F1 drops).

**First 3 experiments**:
1. Ablation on topic reward components: Train with Coverage-only, Precision-only, and F1 (harmonic mean). Compare ROUGE, BERTScore, and topic alignment.
2. Teacher model scaling: Compare 7B vs 0.5B as topic extractor. Measure topic quality via human eval and downstream summary performance.
3. Best-of-n at inference: Generate n=8 summaries from trained model; select by topic-F1. Compare to base model with same strategy to validate metric as selection criterion.

## Open Questions the Paper Calls Out

- Can the topic-guided reward approach maintain its effectiveness when scaled to policy models larger than 0.5B parameters or applied to LLM architectures outside the Qwen family?

- Can advanced neural topic modeling techniques provide more refined topical guidance than the current teacher-LLM extraction method?

- Can the framework be extended to interactive, query-based scenarios where users specify specific points to summarize?

## Limitations

- Topic extraction quality is critical but methodology remains somewhat opaque, particularly for source documents
- Empirical scope is limited to two datasets using one language model family (Qwen)
- Substantial computational requirements (8× A100 40GB GPUs) limit accessibility
- Claims about smaller models matching larger ones lack broader validation across different architectures

## Confidence

**High Confidence**: Explicit topic prompting improves summary informativeness across multiple model scales (0.5B, 1.5B, 7B) with consistent ROUGE improvements.

**Medium Confidence**: Topic-guided RL produces more topically aligned summaries than reference-based methods, though the reference-free nature creates circular validation challenges.

**Low Confidence**: The assertion that the method enables smaller models to "match or exceed" larger ones lacks validation across different model families and architectures.

## Next Checks

1. **Cross-Dataset Generalization Test**: Evaluate trained models on an unseen multi-document summarization dataset (e.g., DUC-2004 or WikiSum) without additional fine-tuning to validate generalizability.

2. **Topic Extraction Ablation with Human Evaluation**: Conduct human evaluation specifically focused on topic quality where annotators rate source document topic phrases on relevance, specificity, and redundancy. Compare topics extracted by 7B vs 0.5B teachers.

3. **Reward Signal Isolation Experiment**: Train identical GRPO configurations with three different reward signals (topic-F1 only, ROUGE only, and combined) using the same policy model and hyperparameters. Compare target metrics and measure for potential reward hacking behaviors.