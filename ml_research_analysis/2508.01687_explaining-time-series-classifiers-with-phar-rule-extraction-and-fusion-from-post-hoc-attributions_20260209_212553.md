---
ver: rpa2
title: 'Explaining Time Series Classifiers with PHAR: Rule Extraction and Fusion from
  Post-hoc Attributions'
arxiv_id: '2508.01687'
source_url: https://arxiv.org/abs/2508.01687
tags:
- anchor
- shap
- lime
- rule
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PHAR transforms post-hoc numeric attributions (SHAP, LIME) into
  interpretable time series rules, fusing them with Anchor rules via strategies like
  Lasso, union, and best-selection to balance confidence, coverage, and simplicity.
  Evaluated on 43 UCR/UEA datasets, it achieves high fidelity and concise rules, outperforming
  or matching Anchor while scaling better to long sequences and reducing ambiguity
  in explanations.
---

# Explaining Time Series Classifiers with PHAR: Rule Extraction and Fusion from Post-hoc Attributions

## Quick Facts
- arXiv ID: 2508.01687
- Source URL: https://arxiv.org/abs/2508.01687
- Reference count: 40
- Primary result: Transforms post-hoc attributions (SHAP, LIME) into interpretable time series rules, outperforming or matching Anchor while scaling better to long sequences.

## Executive Summary
PHAR addresses the challenge of explaining black-box time series classifiers by converting dense, numeric attributions into human-readable "IF-THEN" interval rules. The framework applies percentile-based thresholding and perturbation-based sampling to derive stable temporal intervals, then fuses rules from multiple explainers (SHAP, LIME, Anchor) using strategies like Lasso to balance precision, coverage, and simplicity. Evaluated across 43 UCR/UEA datasets, PHAR achieves high fidelity explanations with concise rules while improving scalability compared to native rule-based methods.

## Method Summary
PHAR extracts interpretable rules from time series classifiers by first obtaining SHAP or LIME importance scores for each time step, then identifying salient features through percentile thresholding. For each important feature, the method performs perturbation-based sampling to find stable intervals where the model's prediction remains unchanged. These intervals are then fused using strategies like intersection, union, or Lasso regularization to create final rules. The framework optimizes hyperparameters (threshold percentile and perturbation scale) using Optuna to maximize coverage and confidence while minimizing complexity, following a compound objective that penalizes excessive feature counts.

## Key Results
- Achieves high fidelity and concise rules across 43 UCR/UEA datasets, outperforming or matching Anchor's performance
- Scales better than Anchor to long sequences while reducing explanation ambiguity
- Lasso fusion strategy effectively balances sparsity and coverage, minimizing feature count while maintaining interpretability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting dense, numeric feature attributions into discrete temporal intervals reduces the cognitive load and ambiguity of explanations.
- **Mechanism:** PHAR applies a percentile-based threshold to importance scores (e.g., from SHAP/LIME) to identify salient time steps. It then performs perturbation-based sampling around these values to define stable, closed intervals $x^{(c)}_{t} \in (l, u]$ that preserve the model's prediction. This discretizes a continuous heatmap into a finite set of "IF-THEN" conditions.
- **Core assumption:** Stable intervals derived from local perturbations correctly approximate the model's global decision boundaries for that instance.
- **Evidence anchors:** [abstract] "PHAR transforms post-hoc numeric attributions... into interpretable time series rules."; [section 3.3] "Stable intervals $(l_f, u_f]$ are derived by controlled perturbation of each important feature..."

### Mechanism 2
- **Claim:** Fusing rules from diverse explainers (e.g., SHAP, LIME, Anchor) mitigates the "Rashomon effect" (conflicting explanations) and balances precision with coverage.
- **Mechanism:** PHAR treats individual explainer outputs as candidates. It aggregates them using set-theoretic or statistical strategies: Intersection conserves only conditions present in all rules (high precision), Lasso uses $\ell_1$ penalization to sparsely select the most predictive interval conditions, and Weighted prioritizes rules based on confidence/coverage metrics.
- **Core assumption:** The consensus or optimized subset of multiple explainers is closer to the "ground truth" logic than any single explainer.
- **Evidence anchors:** [abstract] "...fusing them with Anchor rules via strategies like Lasso, union, and best-selection to balance confidence, coverage, and simplicity."; [section 3.4] "Rule fusion in PHAR consolidates multiple rules into exactly one... using strategies like weighted selection and lasso-based refinement."

### Mechanism 3
- **Claim:** Optimizing rule generation hyperparameters via an objective function ensures the resulting rules are not only faithful but also concise (sparse).
- **Mechanism:** The framework tunes hyperparameters (threshold percentile $p$, perturbation scale $\sigma_p$) using Optuna to maximize an objective $M(n)$. $M(n)$ rewards Coverage (generalizability) and Confidence (fidelity), while penalizing Complexity (Feature count $F(n)$) to adhere to cognitive limits (Miller's Law, $\approx 7 \pm 2$ conditions).
- **Evidence anchors:** [section 3.3.2] "We define an objective function maximizing coverage and confidence while penalizing empty or complex rules..."; [section 4.5.5] "Intersection and both Lasso variants minimized feature count... Union and Best increased it."

## Foundational Learning

- **Concept: Post-hoc Feature Attribution (SHAP/LIME)**
  - **Why needed here:** PHAR does not build a new model; it interprets an existing one. You must understand that SHAP/LIME provide *continuous* weights (importance) but lack *temporal structure* (intervals). PHAR adds the structure.
  - **Quick check question:** Can you distinguish between a feature's "importance score" and a "decision rule interval"?

- **Concept: The Rashomon Effect**
  - **Why needed here:** Time series often allow for many equally accurate explanations. Understanding this trade-off is crucial because PHAR's "Fusion" mechanism is explicitly designed to resolve this by picking the "best" or "most stable" rule from conflicting options.
  - **Quick check question:** If LIME says $t=10$ matters and SHAP says $t=50$ matters for the same class, how would PHAR's Lasso fusion likely resolve this?

- **Concept: Rule Quality Metrics (Coverage vs. Confidence)**
  - **Why needed here:** The paper optimizes a composite metric. You need to know that high confidence (precision) often comes at the cost of low coverage (recall/scope), and the "fusion" step is engineering a trade-off.
  - **Quick check question:** Does a rule with 100% Confidence and 1% Coverage describe the model well or just a single outlier?

## Architecture Onboarding

- **Component map:** Raw TS -> Black-box Prediction -> SHAP/LIME Values -> Thresholding -> Perturbation -> Interval Extraction -> Fusion -> Visualization

- **Critical path:** Raw TS -> Black-box Prediction -> SHAP/LIME Values -> **Thresholding** (identify candidate timesteps) -> **Perturbation** (find stable intervals) -> **Fusion** (select final rule) -> Visualization

- **Design tradeoffs:**
  - **Anchor vs. PHAR:** Anchor is native rule-based but computationally expensive on long sequences; PHAR uses post-hoc conversion to scale better but relies on approximations.
  - **Union vs. Intersection:** Union increases Feature Count (complexity) and Coverage; Intersection reduces Feature Count (simplicity) and Coverage.
  - **Sparsity vs. Fidelity:** Using Lasso fusion enforces sparsity (simpler rules) but might drop nuanced conditions found by Union.

- **Failure signatures:**
  - **Empty Rule Sets:** Occurs if the threshold $p$ is too high or perturbations fail to find stable regions (Coverage $\to$ 0).
  - **Overly Complex Rules:** Occurs if using Union fusion without sparsity constraints (Feature count $F(n) \gg 10$).
  - **Low Confidence:** Indicates the derived intervals do not generalize to the test set; the rule is overfitting the specific instance noise.

- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run PHAR on a simple dataset (e.g., ECG200) using just SHAP -> Rules (no fusion). Verify that intervals align with visual peaks.
  2. **Fusion Ablation:** Compare **Lasso Fusion** vs. **Union Fusion** on a long-sequence dataset. Measure the trade-off: Does Union significantly increase Feature Count ($F(n)$) compared to the accuracy gain?
  3. **Hyperparameter Sensitivity:** Vary the threshold percentile $p$ (e.g., 80 vs 95). Observe if strict thresholding drops Coverage dramatically or if it successfully cleans up noise.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the current PHAR framework guarantee rule stability across similar time series instances or slight input perturbations?
- **Basis in paper:** [explicit] Section 5.1 notes that rule similarity and stability were not explicitly analyzed, and Section 5.2 lists assessing "rule stability across similar instances" as a research direction.
- **Why unresolved:** The evaluation focuses on aggregate metrics (coverage, confidence) rather than the consistency of rule structure (features/intervals selected) for near-identical inputs.
- **What evidence would resolve it:** A sensitivity analysis measuring the variance in derived rules (e.g., using Jaccard similarity on feature sets) when applying minor noise or perturbations to input instances.

### Open Question 2
- **Question:** Does the proposed visualization and rule format actually reduce cognitive load and improve decision-making for human domain experts?
- **Basis in paper:** [explicit] Section 5.1 states that "evaluations with Domain Experts" were not conducted, and the "relationship between rule complexity and perceived clarity remains to be systematically investigated."
- **Why unresolved:** While the paper argues for reduced cognitive load based on theory (Miller's Law), it relies on quantitative proxies rather than user studies.
- **What evidence would resolve it:** A controlled user study with domain experts (e.g., clinicians for ECG data) comparing task performance and subjective interpretability between PHAR rules and raw attributions.

### Open Question 3
- **Question:** Can the PHAR framework be extended to generate counterfactual explanations that identify minimal changes required to flip a classification?
- **Basis in paper:** [explicit] Section 5.2 identifies extending PHAR to "counterfactual explanation generation" as an important future direction to provide "actionable 'WHAT-IF' insights."
- **Why unresolved:** The current framework primarily generates semi-factual "IF-THEN" rules explaining the current prediction, but cannot identify the specific feature intervals needed to transition to a different class.
- **What evidence would resolve it:** An extension of the perturbation methodology to identify boundary-crossing samples and the derivation of contrastive rules for alternative classes.

## Limitations
- Rule stability across similar instances remains untested, with no systematic analysis of how derived intervals generalize beyond sampled perturbations
- Inherits SHAP/LIME's weaknesses including explanation variance and potential misalignment with model reasoning
- Computational intensity of perturbation-based interval extraction may limit scalability for multivariate series with many channels

## Confidence
- **Medium:** PHAR's assumption that local perturbations define global decision boundaries remains weakly validated
- **Medium:** Fusion strategies may mask rather than resolve inconsistencies from multiple explainers
- **High:** Stated scalability gains versus Anchor are well-supported, but overall runtime guarantees for very large datasets are limited

## Next Checks
1. **Perturbation Stability Test:** Run PHAR on a fixed dataset (e.g., ECGFiveDays) with varying random seeds for SHAP/LIME and perturbation sampling. Measure interval overlap and confidence stability across runs.

2. **Generalization Validation:** For rules derived from a single instance, test their coverage on a held-out subset of the same class. Quantify how often the rule correctly predicts class membership on unseen samples.

3. **Counterfactual Comparison:** Generate counterfactuals for PHAR-explained instances (e.g., using *UniCoMTE*). Assess whether minimal changes to the rule's intervals flip the prediction, validating the rules capture the true decision boundary.