---
ver: rpa2
title: 'Understanding Sharpness Dynamics in NN Training with a Minimalist Example:
  The Effects of Dataset Difficulty, Depth, Stochasticity, and More'
arxiv_id: '2506.06940'
source_url: https://arxiv.org/abs/2506.06940
tags:
- sharpness
- minimalist
- training
- figure
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies sharpness dynamics in deep neural network training,\
  \ focusing on the progressive sharpening phenomenon where sharpness increases during\
  \ gradient descent before saturating near the edge of stability. The authors propose\
  \ a minimalist model\u2014a deep linear network with one neuron per layer\u2014\
  to analyze this behavior."
---

# Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More

## Quick Facts
- **arXiv ID**: 2506.06940
- **Source URL**: https://arxiv.org/abs/2506.06940
- **Reference count**: 40
- **Primary result**: Sharpness dynamics in deep neural networks can be predicted by a minimalist model (deep linear network with one neuron per layer) as a function of dataset difficulty, depth, and optimization algorithm

## Executive Summary
This paper investigates sharpness dynamics during neural network training, focusing on the progressive sharpening phenomenon where loss landscape sharpness increases during gradient descent before saturating near the edge of stability. The authors propose a minimalist theoretical model—a deep linear network with one neuron per layer—to analyze this behavior and derive sharpness bounds at global minima. They show that sharpness depends on dataset difficulty (Q), layer imbalance, and network depth, with larger datasets, deeper networks, and smaller learning rates leading to more progressive sharpening. The theoretical predictions are validated on both linear and nonlinear networks, demonstrating that the proposed sharpness metric (σ²₁NQ^(D-1)/D) closely matches observed post-training sharpness. The study provides new insights into how sharpness evolves during training and how it is influenced by dataset properties and optimization algorithms.

## Method Summary
The authors develop a minimalist theoretical framework using deep linear networks with one neuron per layer to study sharpness dynamics during training. They derive theoretical bounds on sharpness at global minima as functions of dataset difficulty (Q), layer imbalance, and network depth. The sharpness metric is defined as the largest eigenvalue of the Hessian matrix at the solution. The analysis covers both gradient descent (GD) and stochastic gradient descent (SGD), showing that SGD increases layer imbalance more than GD, leading to less progressive sharpening. Empirical validation is performed on both linear and nonlinear networks, comparing predicted sharpness values with observed post-training sharpness across various conditions including different depths, learning rates, and dataset difficulties.

## Key Results
- Sharpness at global minima can be theoretically bounded as σ²₁NQ^(D-1)/D for deep linear networks
- Progressive sharpening is more pronounced with larger datasets, deeper networks, and smaller learning rates
- SGD increases layer imbalance more than GD, resulting in less progressive sharpening
- The predicted sharpness metric closely matches observed post-training sharpness in both linear and nonlinear networks
- Sharpness dynamics are fundamentally determined by dataset difficulty and network architecture

## Why This Works (Mechanism)
The progressive sharpening phenomenon emerges from the interplay between optimization dynamics and loss landscape geometry. As training progresses, gradient descent pushes the parameters toward regions where the loss landscape becomes increasingly steep, creating sharper minima. The minimalist model captures this behavior by showing how the largest eigenvalue of the Hessian grows during training based on dataset properties and network depth. The theoretical bounds emerge from analyzing the singular value decomposition of the data matrix and how it propagates through the deep linear network layers. The progressive nature occurs because the optimization process incrementally amplifies certain directions in parameter space, leading to increasing curvature until reaching the edge of stability.

## Foundational Learning

**Sharpness (Loss Landscape Curvature)**: The sensitivity of the loss function to parameter changes, typically measured via eigenvalues of the Hessian matrix. Why needed: Central concept being studied throughout the paper. Quick check: Largest eigenvalue of Hessian indicates maximum curvature direction.

**Progressive Sharpening**: The phenomenon where sharpness increases during gradient descent training before saturating. Why needed: Core phenomenon the paper aims to explain theoretically. Quick check: Sharpness grows from initial to final values during training.

**Edge of Stability**: The point where sharpness reaches a critical threshold and training behavior changes. Why needed: Explains the saturation point of progressive sharpening. Quick check: Sharpness saturates when largest eigenvalue ≈ 2/η (learning rate).

**Layer Imbalance**: The asymmetry in weight magnitudes across different layers of a deep network. Why needed: Key factor determining sharpness in deep linear networks. Quick check: Imbalance grows faster with SGD than GD.

**Dataset Difficulty (Q)**: A measure quantifying how hard a dataset is to fit, based on singular value properties. Why needed: Determines the scale of sharpness at global minima. Quick check: Larger Q leads to sharper minima.

## Architecture Onboarding

**Component Map**: Data matrix (X) -> Deep linear network (W₁, W₂, ..., W_D) -> Loss function (||XW - Y||²) -> Sharpness metric (λ_max(Hessian))

**Critical Path**: X → Σ (singular values) → W_i (layer weights) → Sharpness bounds → Progressive sharpening dynamics

**Design Tradeoffs**: The minimalist model sacrifices biological plausibility and practical applicability for analytical tractability. The single-neuron-per-layer constraint enables closed-form solutions but limits direct applicability to modern deep networks.

**Failure Signatures**: If the predicted sharpness formula fails to match empirical observations, it suggests either violations of the model assumptions (like non-linearities dominating) or that other factors beyond dataset difficulty and depth are influencing sharpness.

**First Experiments**:
1. Verify progressive sharpening in deep linear networks across varying depths and learning rates
2. Compare sharpness evolution under GD vs SGD for identical initializations
3. Test the predicted sharpness formula σ²₁NQ^(D-1)/D against empirical measurements

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical analysis is limited to deep linear networks, which may not capture all complexities of practical deep learning scenarios
- The sharpness metric based on the largest Hessian eigenvalue may not capture all aspects of loss landscape geometry relevant to generalization
- Empirical validation relies on specific experimental conditions and may not generalize to all architectures and datasets
- The analysis focuses on progressive sharpening but does not fully address implications for generalization or edge-of-stability behavior in nonlinear networks

## Confidence
- **High**: Theoretical sharpness bounds for the minimalist model and their dependence on dataset difficulty and layer imbalance
- **Medium**: Empirical validation of sharpness predictions on linear and nonlinear networks
- **Low**: Generalization of findings to broader deep learning contexts and practical implications for optimization

## Next Checks
1. Test the sharpness predictions on a wider range of architectures (e.g., convolutional and recurrent networks) and datasets to assess generalizability
2. Investigate the impact of alternative sharpness metrics (e.g., trace of the Hessian or local curvature measures) on the observed dynamics
3. Explore the relationship between progressive sharpening and generalization performance in both linear and nonlinear networks under varying optimization conditions