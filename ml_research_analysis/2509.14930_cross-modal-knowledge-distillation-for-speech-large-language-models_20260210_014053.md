---
ver: rpa2
title: Cross-Modal Knowledge Distillation for Speech Large Language Models
arxiv_id: '2509.14930'
source_url: https://arxiv.org/abs/2509.14930
tags:
- speech
- text
- knowledge
- distillation
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies catastrophic forgetting and modality inequivalence\
  \ as key challenges in speech large language models (LLMs), where adding speech\
  \ capabilities degrades text-based reasoning and spoken query performance. The authors\
  \ propose a cross-modal knowledge distillation framework that transfers knowledge\
  \ from a text-based teacher LLM to a speech LLM via two complementary channels:\
  \ text-to-text (T\u2192T) and speech-to-text (S\u2192T) distillation."
---

# Cross-Modal Knowledge Distillation for Speech Large Language Models

## Quick Facts
- arXiv ID: 2509.14930
- Source URL: https://arxiv.org/abs/2509.14930
- Authors: Enzhi Wang; Qicheng Li; Zhiyuan Tang; Yuhang Jia
- Reference count: 0
- Primary result: Cross-modal distillation improves speech LLM performance from 75.08% to 77.19% on VoiceBench benchmark

## Executive Summary
This paper addresses catastrophic forgetting and modality inequivalence in speech large language models, where adding speech capabilities degrades text-based reasoning and spoken query performance. The authors propose a cross-modal knowledge distillation framework that transfers knowledge from a text-based teacher LLM to a speech LLM via two complementary channels: text-to-text (T→T) and speech-to-text (S→T) distillation. Using only ~60k samples, the method significantly improves Qwen2.5-Omni's performance across dialogue and audio understanding tasks, demonstrating better cross-modal alignment while preserving textual knowledge.

## Method Summary
The method employs dual-channel knowledge distillation: T→T distillation reinforces text-mode token distributions against teacher logits, while S→T distillation maps acoustic inputs to the same teacher-softened distributions via TTS-synthesized speech. The framework uses temperature-scaled KL divergence combined with cross-entropy loss, with teacher-generated outputs as hard targets. Training uses Open-Orca dataset (22,456 T→T samples, 44,753 S→T samples) converted to speech via CosyVoice 2 TTS, with hyperparameters λ=0.5, τ=2, lr=5×10⁻⁶, over 2 epochs.

## Key Results
- VoiceBench overall accuracy improves from 75.08% to 77.19% (S→T + T→T vs S→T alone)
- Significant gains in open-ended QA, knowledge QA, and instruction following tasks
- Audio analysis reasoning improves, demonstrating better cross-modal alignment
- Teacher-generated hard targets outperform dataset gold labels for cross-modal transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-channel distillation preserves text reasoning while aligning speech representations
- Mechanism: Text-to-text (T→T) distillation reinforces the student's text-mode token distributions against teacher logits via KL divergence, while speech-to-text (S→T) distillation maps acoustic inputs to the same teacher-softened distributions via a shared objective. The shared KL term L_KL(Qt; θ_t, θ_s) with temperature τ couples the two channels so that speech-conditioned logits are pulled toward text-conditioned teacher outputs.
- Core assumption: The teacher text LLM encodes higher-quality reasoning/knowledge than the speech-adapted student, and soft labels transfer more generalizable distributional knowledge than hard labels alone.
- Evidence anchors:
  - [abstract] "cross-modal knowledge distillation framework that leverages both text-to-text and speech-to-text channels to transfer knowledge from a text-based teacher model to a speech LLM"
  - [section 3.2] "L_T→T = L_CE(y or ŷ; Q_t, θ_s) + λτ^2 L_KL(Q_t; θ_t, θ_s)" and paired S2T loss aligning student conditioned on Q_a with teacher on Q_t
  - [corpus] Corpus evidence is consistent but not strongly quantitatively grounded; neighbor papers (e.g., "Teaching Audio Models to Reason") propose distillation to reduce audio-text modality gaps, but without directly validating the dual-channel formulation used here.
- Break condition: If teacher quality is low or the student has already diverged too far (e.g., severe parameter drift), KL-based soft-label transfer may provide insufficient supervision; if τ is poorly tuned, gradients can be too smooth/sharp to transmit useful distributional information.

### Mechanism 2
- Claim: Teacher-generated hard targets outperform dataset gold labels for cross-modal transfer
- Mechanism: Using the text LLM's responses ŷ as hard targets (Teacher CE) rather than ground-truth y reduces distribution mismatch: the student learns to emulate responses it can actually reproduce given its tokenization and capacity. Combined with KL on soft distributions, this provides both precise target guidance and generalizable uncertainty information.
- Core assumption: Gold labels in open instruction datasets may not be well-aligned with the student's output space or the teacher's reasoning patterns, whereas teacher outputs are in-distribution for the student's vocabulary and style.
- Evidence anchors:
  - [section 4.2, Table 2] "using teacher-generated outputs as hard targets (Teacher CE) is more effective" and "Teacher CE + KL offers marginal additional gains"
  - [abstract] Distillation is performed using open instruction datasets (OpenOrca) to transfer teacher knowledge
  - [corpus] Corpus does not provide direct evidence comparing teacher-generated vs gold hard targets for speech LLMs; this remains primarily supported by the current paper's experiments.
- Break condition: If teacher outputs are noisy, inconsistent, or factually incorrect, hard-target imitation can propagate errors; if the student's speech encoder produces poorly aligned representations, even high-quality teacher targets may not be reachable.

### Mechanism 3
- Claim: Cross-modal KL alignment reduces modality inequivalence without requiring architecture changes
- Mechanism: By pairing Q_a (synthetic speech via TTS) with teacher logits from Q_t, the student learns to map speech-conditioned hidden states into regions of output space that overlap with text-conditioned distributions. This reduces the performance gap between S→T and T→T modes while keeping the speech encoder/adapter unchanged.
- Core assumption: The TTS-generated speech Q_a preserves sufficient semantic information from Q_t to enable meaningful alignment, and the student's adapter can route speech features into the LLM's representation space without architectural modification.
- Evidence anchors:
  - [section 3.1] "Q_a = T(Q_t)" and the S2T loss pairs "student conditioned on Q_a with the teacher conditioned on Q_t"
  - [section 4.2] Combined S2T + T2T "achieves the best overall results" and mitigates both forgetting and inequivalence
  - [corpus] Neighbor papers (e.g., "Closing the Gap Between Text and Speech Understanding in LLMs") document text-speech understanding gaps and modality alignment challenges, consistent with the paper's framing, but do not directly validate this specific KL-based alignment mechanism.
- Break condition: If TTS outputs diverge semantically from the source text, or if the speech encoder collapses acoustic/semantic information, the alignment signal becomes noisy; if the student's adapter has insufficient capacity, cross-modal alignment may saturate early.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper attributes text-knowledge degradation to sequential adaptation from text to speech, requiring mitigation via distillation rather than naive fine-tuning.
  - Quick check question: If you fine-tune a text LLM on a new speech task without regularization, would you expect its original text performance to increase, stay the same, or decrease?

- Concept: KL divergence with temperature scaling for knowledge distillation
  - Why needed here: The losses L_T→T and L_S→T use temperature-scaled soft targets; understanding how τ controls distribution smoothness is critical for tuning.
  - Quick check question: As temperature τ increases toward infinity, what happens to the softmax distribution over logits, and how does that affect gradient magnitudes?

- Concept: Cross-modal representation alignment
  - Why needed here: The paper's core thesis is that speech and text modalities are misaligned in learned representation space, causing inequivalence; alignment via shared supervision is the proposed fix.
  - Quick check question: If speech and text embeddings for the same semantic query are far apart in cosine distance, would you expect downstream task accuracy to be higher or lower?

## Architecture Onboarding

- Component map:
  - Teacher text LLM (θ_t) -> frozen, produces logits z_T and optionally hard targets ŷ
  - Student speech LLM (θ_s) -> includes speech encoder + adapter + text LLM backbone (potentially trainable)
  - TTS system T(·) -> synthesizes Q_a from Q_t for S2T channel
  - Loss combiner -> merges L_CE (hard targets) and temperature-scaled L_KL (soft alignment) with weight λ

- Critical path:
  1. Prepare text queries Q_t and teacher outputs (logits z_T and/or responses ŷ)
  2. Generate synthetic speech Q_a via TTS
  3. Compute student logits z_S on both Q_t (T→T) and Q_a (S→T)
  4. For each channel, compute L_CE against ŷ (or y) and L_KL against teacher soft targets
  5. Combine losses and update θ_s (adapter and/or backbone depending on strategy)

- Design tradeoffs:
  - Frozen backbone vs trainable: Freezing preserves text knowledge better but limits cross-modal alignment capacity; this paper trains with distillation to avoid freezing constraints.
  - Teacher CE vs gold CE: Teacher-generated targets improve alignment but require an extra generation pass and may propagate teacher errors.
  - λ and τ selection: Higher τ smooths distributions (more generalizable, less precise); λ balances hard vs soft supervision—paper uses λ=0.5, τ=2.

- Failure signatures:
  - S→T performance far below T→T with no improvement after distillation → likely TTS misalignment or encoder bottleneck
  - T→T degrades during training → excessive regularization or overfitting to soft targets; reduce λ or check data quality
  - Training instability or exploding KL gradients → τ too low (sharp distributions) or λ too high

- First 3 experiments:
  1. Reproduce S2T-only distillation with CE vs Teacher CE on a small subset of OpenOrca (~5k samples), tracking AlpacaEval and SD-QA; verify Teacher CE improves over CE.
  2. Add KL term with τ=2, λ=0.5; compare S2T (Teacher CE + KL) vs Teacher CE alone to confirm marginal gains.
  3. Combine T2T + S2T with Teacher CE on full 67k samples; report VoiceBench S→T and T→T overall scores to match paper's 75.08 → 77.19 lift.

## Open Questions the Paper Calls Out
- How can semantic dialogue knowledge distillation be effectively combined with acoustic audio analysis distillation in a unified training framework?
- Does cross-modal knowledge distillation trained on synthetic TTS speech generalize to real human speech with natural variations in accent, prosody, and environmental noise?
- Does the cross-modal knowledge distillation framework transfer effectively to speech LLM architectures beyond Qwen2.5-Omni?

## Limitations
- TTS-generated speech may not preserve sufficient semantic fidelity for effective cross-modal alignment
- Teacher-generated hard targets may propagate errors if teacher outputs are noisy or inconsistent
- Single-architecture validation limits generalizability claims to other speech LLM designs

## Confidence
- **High Confidence**: The experimental results showing overall performance improvement (75.08 → 77.19 on VoiceBench) and the qualitative observation that both modalities benefit from combined distillation are well-supported by the data.
- **Medium Confidence**: The mechanism explanations (KL-based alignment, teacher CE advantage) are plausible given the results but lack direct causal evidence - alternative explanations (e.g., regularization effects, data augmentation) aren't ruled out.
- **Low Confidence**: Claims about why specific mechanisms work (e.g., "soft labels transfer more generalizable knowledge") are stated without rigorous comparison to alternatives or theoretical grounding.

## Next Checks
1. Ablation of target source: Replace teacher-generated ŷ with ground-truth labels y in both CE and KL losses across T→T and S→T channels; measure impact on overall and per-modality performance.
2. TTS quality assessment: Compute ASR word error rate and semantic similarity (e.g., SBERT) between original text and TTS-synthesized-then-recognized speech; correlate these metrics with S→T distillation effectiveness.
3. Data imbalance study: Retrain with balanced T→T/S→T sample counts (e.g., 22k each) and compare to original imbalanced setup; assess whether modality-specific gains depend on sample ratio.