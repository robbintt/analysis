---
ver: rpa2
title: 'Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents'
arxiv_id: '2502.15840'
source_url: https://arxiv.org/abs/2502.15840
tags:
- agent
- sonnet
- machine
- tool
- business
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vending-Bench is a simulation environment for testing long-term
  coherence of autonomous LLM agents. Agents manage a vending machine business over
  extended periods (20M tokens per run), balancing inventory, pricing, orders, and
  daily fees.
---

# Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents

## Quick Facts
- arXiv ID: 2502.15840
- Source URL: https://arxiv.org/abs/2502.15840
- Authors: Axel Backlund; Lukas Petersson
- Reference count: 20
- Key outcome: Autonomous agents managing vending machines show high variance and long-term coherence failures unrelated to context limits

## Executive Summary
Vending-Bench is a simulation environment testing long-term coherence of LLM agents over extended periods (>20M tokens). Agents manage a vending machine business, balancing inventory, pricing, orders, and daily fees. The benchmark reveals that even top models like Claude 3.5 Sonnet and o3-mini often turn a profit but occasionally fail catastrophically due to state-tracking errors. These failures—including meltdown loops and cascading errors—occur well after context windows fill, suggesting a distinct working memory limitation. The benchmark highlights the challenge of maintaining coherent decision-making over long horizons.

## Method Summary
The benchmark uses the inspect-ai framework where agents operate a vending machine business with $500 initial balance and $2 daily fees. The agent loop receives 30k-token context windows and uses memory tools (scratchpad, key-value store, vector DB) plus task tools (email, web search via Perplexity, sub-agent delegation). Sub-agents handle physical tasks like stocking and pricing. Customer demand follows price elasticity models with modifiers. Five runs per model up to 2,000 messages; early termination if unable to pay daily fee for 10 consecutive days.

## Key Results
- Claude 3.5 Sonnet achieved highest net worth (~$3000) in winning runs but showed extreme variance ($476 to ~$3000)
- All models showed declining tool use after ~120 days, correlating with performance degradation
- Failures occurred regardless of context window fill status (correlation 0.167), suggesting working memory degradation
- Top models often turned profits but all occasionally failed through meltdown loops or cascading errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Long-horizon coherence failures stem from state-tracking degradation, not context window limits
- Mechanism: Agents progressively lose accurate mental models of business state—forgetting pending orders or misremembering delivery dates—distinct from context overflow
- Core assumption: State-tracking degrades nonlinearly over time horizons even when token limits aren't binding
- Evidence anchors: Abstract states failures don't stem from memory limits; section 3.6 shows 0.167 Pearson correlation between failure timing and context fill

### Mechanism 2
- Claim: Small state misinterpretations cascade into unrecoverable "meltdown loops" due to lack of grounded error correction
- Mechanism: Misreading delivery notifications triggers escalating corrective behaviors (legal threats, FBI contact) without external grounding signals
- Core assumption: Models lack meta-cognitive awareness of their own uncertainty in state estimation
- Evidence anchors: Abstract describes "tangential 'meltdown' loops"; section 3.2.2 shows Claude 3.5 Sonnet escalating from stocking error to FBI contact

### Mechanism 3
- Claim: Tool use consistency predicts economic success more than raw model capability
- Mechanism: Successful runs show sustained, regular tool usage patterns; declining tool use correlates with net worth decline
- Core assumption: Consistent execution routines can compensate for imperfect state tracking by creating redundant observations
- Evidence anchors: Section 3.2 shows all models decrease daily tool use after ~120 days; longest runs show stable tool patterns

## Foundational Learning

- **Context window vs. working memory**
  - Why needed here: Paper distinguishes context window (token limit) from working memory (state maintenance). Models failed despite 100k-2M token windows available.
  - Quick check question: Can you explain why a model with a 200k token context might still "forget" an order placed 50k tokens ago?

- **Cascading failures in autonomous systems**
  - Why needed here: Single-point failures (misreading one email) can trigger complete system collapse. Understanding this is essential for recovery mechanisms.
  - Quick check question: What's the difference between a recoverable error and a cascade-triggering error in an autonomous agent?

- **High-variance performance distributions**
  - Why needed here: Even top models show extreme variance—Claude 3.5 Sonnet ranges from $476 to ~$3000 net worth across runs. Single-run evaluations are misleading.
  - Quick check question: Why might the same model architecture produce both a profitable run and complete bankruptcy?

## Architecture Onboarding

- **Component map**: Main agent (inspect-ai) -> Memory tools (scratchpad, KV store, vector DB) -> Task tools (email, search, sub-agent) -> Environment simulation (supplier/customer) -> Scoring (net worth)

- **Critical path**: 1) Agent researches products → contacts suppliers via email 2) Supplier replies next day with catalog 3) Agent places order → delivery arrives after delay 4) Agent delegates stocking to sub-agent 5) Agent sets prices → daily customer purchases 6) Agent collects cash periodically 7) Daily $2 fee deducted automatically

- **Design tradeoffs**: Memory cap (30k tokens) prevents runaway context growth but may truncate history; main/sub-agent split simulates digital-physical boundary but adds coordination overhead; early termination (10 days without fee payment) reduces compute costs

- **Failure signatures**: Meltdown loop (repetitive escalation emails), tool-call format drift (typing tool names as text), waiting loop (excessive wait_for_next_day calls), inventory confusion (ordering instead of restocking)

- **First 3 experiments**:
  1. Recovery intervention test: Inject "state check" tool forcing inventory/order summarization before major actions
  2. Memory architecture ablation: Compare scratchpad-only vs. vector-DB-only vs. combined memory usage
  3. Error injection robustness: Deliberately delay one delivery notification by 1 day to measure recovery vs. cascade

## Open Questions the Paper Calls Out

- Why does increasing an agent's token memory capacity lead to worse performance on long-horizon tasks? The paper observed larger memory capacities performed worse but didn't investigate whether degradation is caused by distraction, attention dilution, or other factors.

- What specific mechanisms trigger the "meltdown" loops and unrecoverable tangents if not context window overflow? The paper rules out memory limits but leaves the actual cognitive or architectural trigger for hallucinations undefined.

- How can autonomous agents be modified to reliably detect and recover from catastrophic reasoning errors? The paper notes models "rarely recover" from failures but highlights a lack of robustness or self-correction capabilities.

## Limitations
- Task domain is narrow (vending machine management) limiting generalizability
- Insufficient specification for critical components like supplier simulation system prompts and economic parameters
- Five runs per model may be insufficient to establish reliable rankings given extreme performance variance

## Confidence
- **High confidence**: Claims about existence of long-horizon coherence failures and benchmark's ability to expose them
- **Medium confidence**: Claims that failures are not caused by context window limits (0.167 correlation provides evidence but needs more validation)
- **Low confidence**: Claims about relative importance of tool use consistency versus raw capability (observational evidence only)

## Next Checks
1. **State-tracking intervention test**: Implement "state check" tool forcing inventory/order summarization before major actions; measure meltdown frequency reduction
2. **Context boundary experiment**: Run identical scenarios with varying context window sizes (30k vs 100k tokens) while keeping 30k-token memory cap
3. **Cross-task transferability**: Apply same agent architecture to different long-horizon task (e.g., simulated restaurant); compare failure patterns to determine generalizability