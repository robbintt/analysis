---
ver: rpa2
title: Wildfire spread forecasting with Deep Learning
arxiv_id: '2505.17556'
source_url: https://arxiv.org/abs/2505.17556
tags:
- fire
- wildfire
- spread
- temporal
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a deep learning framework for forecasting
  wildfire spread by predicting final burned areas using data available at ignition.
  Leveraging a multi-year, multi-modal dataset covering the Mediterranean, the approach
  combines satellite imagery, meteorological data, vegetation maps, topography, and
  thermal anomalies.
---

# Wildfire spread forecasting with Deep Learning

## Quick Facts
- arXiv ID: 2505.17556
- Source URL: https://arxiv.org/abs/2505.17556
- Authors: Nikolaos Anastasiou; Spyros Kondylatos; Ioannis Papoutsis
- Reference count: 40
- Primary result: U-Net3D model achieves 53.6% Dice score and 36.6% IoU for predicting final burned areas

## Executive Summary
This study introduces a deep learning framework for forecasting wildfire spread by predicting final burned areas using data available at ignition. Leveraging a multi-year, multi-modal dataset covering the Mediterranean, the approach combines satellite imagery, meteorological data, vegetation maps, topography, and thermal anomalies. Models are trained using temporal windows from four days before to five days after ignition, with both Vision Transformers and U-Nets explored. Ablation experiments show that including post-ignition data significantly improves predictive accuracy, with the best-performing U-Net3D model achieving a 53.6% Dice score and 36.6% IoU—an improvement of nearly 5% over baseline models.

## Method Summary
The framework uses a multi-modal dataset covering the Mediterranean from 2012-2023, processing 27 variables including meteorological data, vegetation indices, topography, and thermal anomalies. The input is a spatio-temporal cube spanning four days before to five days after ignition, with both Vision Transformers and U-Net architectures tested. The best-performing U-Net3D model employs 3D convolutional filters to process spatial and temporal dimensions simultaneously, achieving superior feature learning across consecutive time steps compared to 2D alternatives.

## Key Results
- U-Net3D achieves 53.6% Dice score and 36.6% IoU for predicting final burned areas
- Post-ignition data inclusion improves F1 score and IoU by almost 5% compared to ignition-only baselines
- 3D convolutions outperform 2D convolutions (53.6% vs 51.7% Dice) by explicitly modeling temporal dynamics

## Why This Works (Mechanism)

### Mechanism 1: Post-Ignition Temporal Conditioning
- **Claim**: Providing the model with observations from the days following ignition significantly improves the accuracy of final burned area extent predictions compared to ignition-only baselines.
- **Mechanism**: The model leverages the temporal evolution of dynamic variables—specifically wind vectors and thermal anomalies—during the initial growth phase to constrain the prediction of the final spatial footprint.
- **Core assumption**: Assumes that the conditions observed in the post-ignition window (up to 5 days) are representative of the forces driving the fire to its final extent.
- **Evidence anchors**: Best-performing model incorporating temporal window improves F1 score and IoU by almost 5%; model performance declines as post-ignition days decrease; autoregressive properties support temporal evolution importance.
- **Break condition**: Performance degrades if meteorological forecasts diverge significantly from observations used during training.

### Mechanism 2: Volumetric Spatio-Temporal Feature Extraction
- **Claim**: Treating the time dimension as a spatial volume via 3D convolutions allows for more effective integration of temporal dynamics than channel-stacking in 2D architectures.
- **Mechanism**: 3D convolutional kernels slide across (Time, Height, Width) simultaneously, learning local spatio-temporal features (e.g., the movement of a fire front relative to wind direction) directly.
- **Core assumption**: The temporal resolution (daily) is sufficient to capture meaningful motion vectors for fire spread.
- **Evidence anchors**: 3D U-Net employs 3D convolutional filters that simultaneously process spatial and temporal dimensions; U-Net3D (53.6% Dice) outperforms U-Net2D (51.7% Dice); industry trend toward spatio-temporal benchmarks.
- **Break condition**: Performance may degrade if 1km resolution is too coarse to capture rapid, fine-grained spread within a single daily time step.

### Mechanism 3: Multi-Modal Driver Fusion
- **Claim**: Jointly processing static topography with dynamic vegetation and weather data allows the model to learn non-linear spread barriers and acceleration corridors.
- **Mechanism**: The architecture concatenates static barriers (e.g., water bodies, elevation) with dynamic drivers (e.g., wind, NDVI). Convolutional layers learn interactions, such as fire slowing down at high elevations or changing direction with wind.
- **Core assumption**: The input variables (27 features) sufficiently capture the physical constraints of the system.
- **Evidence anchors**: Visualizations show 10-day U-Net3D captures anisotropic propagation better than baseline; diverse variables represent fuel, weather, and terrain.
- **Break condition**: Model may overpredict spread across barriers if critical local variables are missing or coarsely resolved.

## Foundational Learning

- **Concept: Semantic Segmentation (Binary)**
  - **Why needed here**: The task is framed as image segmentation (masking the shape of the burn) rather than regression.
  - **Quick check question**: How does the model handle a pixel where the ground truth is "burned" but the model predicts "unburned" (False Negative) in the context of Dice Score vs. Accuracy?

- **Concept: Dice Coefficient / IoU (Intersection over Union)**
  - **Why needed here**: Standard accuracy is misleading due to class imbalance (most pixels are unburned). Dice/IoU penalize the model for misaligning the predicted shape with ground truth.
  - **Quick check question**: If the model predicts a burn area that is perfectly overlapping but twice as large as ground truth, how does IoU penalize this compared to 50% overlap?

- **Concept: U-Net Architecture**
  - **Why needed here**: Backbone used (both 2D and 3D variants) with encoder-decoder structure and skip connections to preserve spatial details lost during downsampling.
  - **Quick check question**: Why are skip connections critical for predicting the precise boundaries of a fire, as opposed to just predicting the center of the burn?

## Architecture Onboarding

- **Component map**: Input (Spatio-temporal cube) -> Encoder (Modified U-Net with GELU) -> Core Logic (3D/2D Convolutions) -> Loss (BCEDice) -> Output (64x64 mask)

- **Critical path**:
  1. Data Pre-processing: Converting raw Mesogeos data into 152-channel tensors (14 dynamic × 10 steps + 12 static)
  2. Temporal Ablation: Experimenting with window size to test sufficiency of post-ignition data
  3. Model Selection: Implementing U-Net3D vs. U-Net2D to verify 3D convolutions handle temporal dynamics better

- **Design tradeoffs**:
  - U-Net2D vs. U-Net3D: 2D is computationally cheaper but treats time as features; 3D is heavier but explicitly models motion (paper suggests 3D is worth ~2% Dice gain)
  - ViT vs. CNN: ViT underperforms (43.7% Dice) likely due to dataset size (~9,500 samples)
  - Resolution: 1km limits detection of small fires but is computationally manageable

- **Failure signatures**:
  - Radial Bias: Model predicts circular burn centered on ignition (insufficient temporal context)
  - Conservatism: Low Recall (<50%) vs. Precision (misses parts of fire to avoid false positives)
  - Large Fire Underestimation: Performance drops for fires >14,000 ha due to training data imbalance

- **First 3 experiments**:
  1. Baseline Reproduction: Train 2D U-Net using only Day 0 (Ignition) data to establish lower bound (~48% Dice)
  2. Temporal Ablation: Train 3D U-Net with [T-4 to T+5] vs. [T-4 to T+0] to quantify post-ignition data value
  3. Loss Function Ablation: Test BCE vs. BCEDice to confirm Dice loss is necessary for class imbalance

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on post-ignition data limits operational forecasting use, as real-time predictions must rely on meteorological forecasts rather than observations
- 1km resolution limits detection of small fires and rapid spread events within single time steps
- Performance drops for extreme fire events (>14,000 ha) due to training data imbalance

## Confidence
- **High Confidence**: Post-ignition data improves prediction accuracy (53.6% vs 48.5% Dice) supported by ablation experiments
- **Medium Confidence**: 3D convolutions superiority (53.6% vs 51.7% Dice) demonstrated, but computational cost-benefit needs consideration
- **Medium Confidence**: Multi-modal fusion effectiveness supported by visual evidence, but feature importance analysis not conducted

## Next Checks
1. Test model performance using meteorological forecasts rather than observations in post-ignition window to assess real-world operational viability
2. Conduct feature importance analysis to identify which of the 27 input variables contribute most to prediction accuracy
3. Evaluate model performance on extreme fire events (>14,000 ha) that are underrepresented in training data