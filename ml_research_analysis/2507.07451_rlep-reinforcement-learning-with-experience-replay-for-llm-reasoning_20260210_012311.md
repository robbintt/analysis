---
ver: rpa2
title: 'RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning'
arxiv_id: '2507.07451'
source_url: https://arxiv.org/abs/2507.07451
tags:
- training
- experience
- learning
- policy
- rlep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RLEP tackles the instability and energy inefficiency of reinforcement
  learning for large language models by introducing a two-phase framework that first
  collects verified reasoning trajectories and then replays them during training.
  At each update step, the model is optimized on mini-batches blending fresh rollouts
  with previously successful solutions.
---

# RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning

## Quick Facts
- arXiv ID: 2507.07451
- Source URL: https://arxiv.org/abs/2507.07451
- Reference count: 17
- Improves AIME-2024 accuracy from 38.2% to 39.9% using experience replay

## Executive Summary
RLEP introduces a two-phase reinforcement learning framework that leverages experience replay to address instability and inefficiency in training large language models for reasoning tasks. The method first collects verified reasoning trajectories, then replays them during training by blending fresh rollouts with successful past solutions. Applied to Qwen2.5-Math-7B, RLEP achieves faster convergence and higher final accuracy than baselines on challenging math benchmarks including AIME and AMC competitions.

## Method Summary
RLEP operates through a two-phase approach where verified reasoning trajectories are first collected and stored, then replayed during subsequent training updates. At each optimization step, the model is trained on mini-batches that combine newly generated rollouts with previously successful solutions from the replay buffer. This mechanism guides learning away from unproductive exploration paths and accelerates convergence by focusing on high-quality experiences. The approach is particularly effective for mathematical reasoning tasks where stable, directed learning is critical.

## Key Results
- Improves AIME-2024 accuracy from 38.2% to 39.9%
- Improves AIME-2025 accuracy from 19.8% to 22.3%
- Improves AMC-2023 accuracy from 77.0% to 82.2%
- Matches baseline peak accuracy in significantly fewer training steps

## Why This Works (Mechanism)
RLEP works by addressing the core instability of reinforcement learning for LLMs through experience replay. By maintaining a buffer of verified, successful reasoning trajectories and blending them with fresh rollouts during training, the method provides stable gradients that prevent the model from wandering into unproductive solution spaces. The two-phase design ensures that learning is guided by proven reasoning paths while still allowing for exploration and adaptation to new problem types.

## Foundational Learning
- **Reinforcement Learning**: Needed for optimizing LLMs on reasoning tasks through reward signals. Quick check: Does the reward function capture mathematical correctness?
- **Experience Replay**: Needed to stabilize training by reusing successful experiences. Quick check: Is the replay buffer sufficiently diverse to prevent overfitting?
- **Trajectory Verification**: Needed to ensure only high-quality reasoning paths are replayed. Quick check: How are "verified" trajectories selected and validated?
- **Mini-batch Blending**: Needed to balance exploration with exploitation. Quick check: What ratio of fresh to replayed experiences yields optimal performance?
- **Mathematical Reasoning Benchmarks**: Needed to evaluate reasoning capabilities. Quick check: Do the benchmarks cover a representative range of problem types?

## Architecture Onboarding

**Component Map:**
Trajectory Collector -> Replay Buffer -> Mini-batch Sampler -> Model Optimizer -> Performance Evaluator

**Critical Path:**
Verified trajectory collection → Replay buffer storage → Mini-batch blending → Model optimization → Accuracy evaluation

**Design Tradeoffs:**
The method trades computational efficiency for stability by maintaining and sampling from a replay buffer. While this increases memory requirements, it significantly reduces the number of training steps needed to reach peak performance.

**Failure Signatures:**
- Over-reliance on replayed solutions leading to lack of novelty
- Insufficient diversity in replay buffer causing overfitting to specific problem types
- Poor trajectory verification resulting in propagation of incorrect reasoning patterns

**First Experiments:**
1. Compare training with and without experience replay on simple math problems
2. Vary the ratio of fresh to replayed experiences in mini-batches
3. Test the impact of replay buffer size on final performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but implicit questions remain about the generalizability of the two-phase replay framework to non-mathematical reasoning domains and the optimal mechanisms for trajectory selection and verification.

## Limitations
- The selection mechanism for "verified" trajectories is not detailed, potentially introducing bias
- Reliance on curated solutions may limit effectiveness for problems requiring novel reasoning paths
- Lack of ablation studies makes it difficult to isolate the impact of experience replay from other optimizations

## Confidence
- **High**: Final accuracy improvements on AIME-2024 and AMC-2023
- **Medium**: Convergence speed gains
- **Medium**: Generalizability of the two-phase replay framework to other domains

## Next Checks
1. Conduct ablation studies to isolate the contribution of experience replay from other training optimizations.
2. Test the method on a broader set of reasoning domains (e.g., code, logic puzzles) to assess generalizability.
3. Analyze the diversity and novelty of solutions in the replay buffer to ensure the method is not overfitting to a narrow set of reasoning paths.