---
ver: rpa2
title: Mitigating Label Length Bias in Large Language Models
arxiv_id: '2511.14385'
source_url: https://arxiv.org/abs/2511.14385
tags:
- label
- labels
- calibration
- bias
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a previously overlooked issue called label
  length bias in large language models, where predictions are biased toward labels
  of certain lengths rather than based on semantic alignment with the input. Existing
  calibration methods fail to address this bias, especially for multi-token class
  labels.
---

# Mitigating Label Length Bias in Large Language Models

## Quick Facts
- arXiv ID: 2511.14385
- Source URL: https://arxiv.org/abs/2511.14385
- Authors: Mario Sanz-Guerrero; Katharina von der Wense
- Reference count: 40
- Key result: NCC achieves up to 10% F1 improvement over raw probabilities by addressing label length bias in LLMs

## Executive Summary
This paper identifies a previously overlooked issue called label length bias in large language models, where predictions are biased toward labels of certain lengths rather than based on semantic alignment with the input. Existing calibration methods fail to address this bias, especially for multi-token class labels. To mitigate this, the authors propose normalized contextual calibration (NCC), a method that normalizes label probabilities by their token count and then calibrates them using prior probabilities computed from content-free inputs. NCC is evaluated across multiple datasets and models, achieving statistically significant improvements over prior approaches with gains of up to 10% F1. The method also extends beyond text classification to tasks like multiple-choice question answering.

## Method Summary
The paper proposes normalized contextual calibration (NCC) to address label length bias in LLMs. The method works in two steps: first, it normalizes label probabilities by their token count using geometric mean to remove length penalty; second, it calibrates these normalized probabilities by dividing by content-free baseline probabilities computed from empty or meaningless inputs. This two-step approach effectively removes both length bias and prior label preferences while preserving semantic alignment. The method is evaluated on 8 classification datasets and 3 MCQA datasets using models ranging from 6B to 70B parameters, with k=5 few-shot examples and 5 random seeds.

## Key Results
- NCC achieves statistically significant F1 improvements over raw probabilities, with gains up to 10% on challenging datasets
- The method is more robust to few-shot example selection and requires fewer examples for competitive performance
- NCC produces more reliable confidence estimates as measured by Expected Calibration Error (ECE)
- The approach extends beyond text classification to multiple-choice question answering tasks

## Why This Works (Mechanism)

### Mechanism 1
Multi-token label probabilities are systematically suppressed compared to single-token labels due to the multiplicative nature of autoregressive probability computation. For a label y with n tokens, PM(y|context) = ∏ PM(ti|context, t1...ti-1). Each additional token multiplies by a value < 1, creating inherent bias against longer labels regardless of semantic fit.

### Mechanism 2
Length normalization alone (geometric mean of token probabilities) removes length penalty but introduces over-prediction of labels with highly predictable token continuations. P_norm = ⁿ√P_raw averages token contributions, but later tokens conditioned on earlier ones have near-certain probabilities, inflating normalized scores for frequent sequences.

### Mechanism 3
Dividing normalized probabilities by content-free baseline probabilities removes prior label biases while preserving length-adjusted semantic alignment. P_calibrated = P_norm(y|input) / P_baseline(y|content-free). Content-free inputs reveal the model's intrinsic preference for each label absent semantic context, and division normalizes out this prior.

## Foundational Learning

- **Autoregressive probability decomposition**: Understanding why multi-token labels have lower raw probabilities requires grasping how LLMs compute sequence probabilities as products of conditional token probabilities.
  - Quick check: Given a 3-token label with individual token probabilities [0.8, 0.9, 0.7], what is the raw sequence probability and the normalized (geometric mean) probability?

- **Calibration vs. normalization distinction**: The paper separates length normalization (making scores comparable across lengths) from calibration (removing prior biases). Conflating these leads to incorrect implementations.
  - Quick check: If a model assigns 90% probability to label "A" and 10% to "B" on content-free input, how should you adjust raw probabilities [0.6, 0.4] for an actual input?

- **Expected Calibration Error (ECE)**: The paper uses ECE to measure confidence reliability. Understanding ECE is necessary to interpret why NCC produces "more reliable confidence estimates."
  - Quick check: If a model predicts with 80% confidence on average but only achieves 60% accuracy, is it overconfident or underconfident? What would the ECE be approximately?

## Architecture Onboarding

- **Component map**: Token probability extractor -> Sequence probability calculator -> Length normalizer -> Baseline estimator -> Calibration divider -> Argmax selector
- **Critical path**: 1) Extract token-level logprobs for all candidate labels 2) Convert logprobs to probabilities, compute sequence products 3) Apply geometric mean normalization per label 4) Pre-compute or retrieve cached baseline probabilities 5) Divide and select argmax
- **Design tradeoffs**: Content-free input selection (5 inputs used; fewer increases variance, more increases compute), caching baselines (label-set-specific but input-independent), tokenization sensitivity (different tokenizers split labels differently), when to skip NCC (for datasets where all labels appear in few-shot examples)
- **Failure signatures**: CC achieving near-zero F1 on datasets with mixed-length labels (indicates over-penalization of short labels), length normalization alone over-predicting labels with common phrases, negative calibrated probabilities (indicates implementation error in baseline computation)
- **First 3 experiments**: 1) Baseline verification: Confirm Raw Prob predicts >50% short labels on Yahoo dataset, then verify Norm Prob shifts toward 3-token labels 2) Content-free input ablation: Compare calibration using single input vs. 5-input ensemble, measuring variance across 5 random seeds 3) Few-shot scaling test: On Banking77, plot F1 vs. k∈{2,5,10,20,50,77} for Raw Prob and NCC

## Open Questions the Paper Calls Out

- **Can NCC be adapted for closed-source models that only provide limited token probability access?** The method fundamentally requires computing probabilities for all candidate labels, which current APIs don't expose fully.
- **How does NCC interact with retrieval-based example selection methods for in-context learning?** The paper treats retrieval and calibration as separate approaches rather than complementary techniques.
- **Does NCC's effectiveness scale predictably to models beyond 70B parameters?** Only one larger model was tested, leaving unclear whether improvements generalize to frontier-scale models.
- **Is geometric mean the optimal normalization approach compared to alternatives like average cross-entropy?** The choice prioritized interpretability and calibration compatibility, not empirical optimization over alternatives.

## Limitations
- NCC's performance may degrade in domains with extreme label characteristics or highly variable tokenization patterns
- The method relies on consistent tokenization across content-free inputs and actual inference, with no complete implementation details provided
- Computational overhead of running inference on 5 content-free inputs for each label in large candidate sets is not quantified
- The calibration mechanism assumes content-free inputs elicit unbiased prior probabilities, which may not hold for models with strong inductive biases

## Confidence

**High confidence**: The identification of label length bias as a distinct phenomenon affecting LLM classification performance is well-supported by empirical evidence across multiple datasets and models.

**Medium confidence**: The effectiveness of NCC's two-step approach (normalization followed by calibration) is statistically significant but lacks sufficient ablation studies to isolate relative contributions of each component.

**Low confidence**: Claims about NCC being "more robust to few-shot example selection" and "requiring fewer examples" are primarily comparative rather than establishing absolute robustness, with limited statistical analysis of variance.

## Next Checks

1. **Ablation study of normalization vs. calibration**: Implement and test three variants separately (normalization only, calibration only, NCC) on Yahoo, TREC-50, SST-5, Banking77, and CLINC150 with identical k=5 few-shot examples and 5 random seeds to quantify relative contributions.

2. **Edge case dataset evaluation**: Create or identify datasets specifically designed to test NCC's limits (all labels with identical token counts, predominantly long labels >5 tokens, labels with highly predictable token sequences) to understand when the method fails or provides no benefit.

3. **Computational overhead benchmarking**: Measure and report wall-clock time and API costs for NCC versus baseline methods across different label set sizes (5, 20, 50, 77 classes), including both one-time baseline computation and per-inference overhead.