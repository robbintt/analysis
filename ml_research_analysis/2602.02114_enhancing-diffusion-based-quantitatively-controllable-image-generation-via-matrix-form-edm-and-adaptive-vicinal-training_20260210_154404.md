---
ver: rpa2
title: Enhancing Diffusion-Based Quantitatively Controllable Image Generation via
  Matrix-Form EDM and Adaptive Vicinal Training
arxiv_id: '2602.02114'
source_url: https://arxiv.org/abs/2602.02114
tags:
- diffusion
- ccdm
- sampling
- iccdm
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces iCCDM, an improved continuous conditional\
  \ diffusion model that addresses limitations in both generation quality and sampling\
  \ efficiency compared to its predecessor CCDM. The method extends the Elucidated\
  \ Diffusion Model (EDM) framework by incorporating conditioning information into\
  \ the noise perturbation process, formulating matrix-form SDEs and PF-ODEs, and\
  \ introducing a non-negative weighting coefficient \u03BBy to flexibly modulate\
  \ condition influence."
---

# Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training

## Quick Facts
- **arXiv ID:** 2602.02114
- **Source URL:** https://arxiv.org/abs/2602.02114
- **Reference count:** 40
- **Primary result:** Introduces iCCDM, achieving higher generation quality with 4× faster sampling (32 steps vs. 150+) compared to CCDM while maintaining quantitative controllability.

## Executive Summary
This paper introduces iCCDM, an improved continuous conditional diffusion model that addresses limitations in both generation quality and sampling efficiency compared to its predecessor CCDM. The method extends the Elucidated Diffusion Model (EDM) framework by incorporating conditioning information into the noise perturbation process, formulating matrix-form SDEs and PF-ODEs, and introducing a non-negative weighting coefficient λy to flexibly modulate condition influence. A novel vicinal score estimate and adaptive vicinal training strategy are developed, along with a lightweight CNN replacing the heavy covariance embedding network. Extensive experiments on four benchmark datasets (64×64 to 256×256) demonstrate iCCDM consistently outperforms existing methods including state-of-the-art text-to-image diffusion models, achieving higher generation quality while significantly reducing sampling cost—requiring only 32 steps versus 150+ for CCDM, with a 4× faster sampling speed and lower memory consumption.

## Method Summary
iCCDM extends the Elucidated Diffusion Model framework to continuous conditional generation by introducing a matrix-form noise covariance structure that incorporates conditioning information. The forward diffusion process uses condition-specific noise covariance matrices, while the reverse process employs adaptive vicinal training to handle label sparsity. The method replaces CCDM's heavy covariance embedding network with a lightweight 5-layer CNN and introduces decoupled training/sampling condition weights (λy^t ≠ λy^s) for task-specific control. Sampling is performed using a 32-step stochastic ODE solver with classifier-free guidance. The framework is trained end-to-end using vicinal loss with adaptive vicinity radii computed based on local sample density.

## Key Results
- iCCDM consistently outperforms existing methods including state-of-the-art text-to-image diffusion models across four datasets (64×64 to 256×256)
- Achieves 4× faster sampling speed (32 steps vs. 150+) compared to CCDM while maintaining higher generation quality
- Introduces adaptive vicinal training that effectively handles label sparsity in regression datasets
- Replaces CCDM's 61.4M parameter covariance embedding network with a 2.8M parameter CNN with minimal performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Condition-specific noise covariance matrices enable finer-grained control over generated samples under regression labels.
- Mechanism: The perturbation process xt = x0 + Σ(t,y)^(1/2)ε extends EDM's condition-agnostic noise injection by making each dimension's noise level depend on a learned embedding of y via ˜h_l(y). The diagonal matrix G(t,y) = Diag([gi(t,y)]) introduces per-dimension noise scaling with a tunable λ_y coefficient, allowing flexible modulation of condition influence absent in original CCDM.
- Core assumption: The conditioning variable y has structured, learnable relationships with spatial dimensions that can be captured by a diagonal covariance approximation.
- Evidence anchors:
  - [abstract] "iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy"
  - [section III-A] "we introduce a condition-specific forward perturbation to enable finer control of generated samples under regression labels"
  - [corpus] Related work on controllable generation (MaRS, MVRoom) focuses on score function modification rather than noise structure; iCCDM's matrix-form approach is distinct.
- Break condition: If λ_y = 0 and adaptive vicinity is disabled, the framework collapses to vanilla EDM (per Remark 3).

### Mechanism 2
- Claim: Vicinal score estimation via adaptive vicinity addresses label sparsity in regression datasets while maintaining conditional distribution accuracy.
- Mechanism: Instead of using only exact-label samples (often zero or few), the method computes vicinal weights W_i,y using Hard Adaptive Vicinity—dynamically adjusting the vicinity radius κ_y based on local sample density N_y and hyperparameter N_AV. This yields ˆp^v_data(x|y) = C_y Σ W_i,y δ(x - x_{0,i}), enabling robust estimation of p(x|y) from nearby labels.
- Core assumption: Samples with labels near y provide useful gradient signal for estimating the conditional distribution at y.
- Evidence anchors:
  - [abstract] "A novel vicinal score estimate and adaptive vicinal training strategy are developed"
  - [section III-D.1] "As pointed out by Ding et al. [26], Eq. (11) provides a poor approximation... The key issue lies in the sparsity of regression datasets"
  - [corpus] CcGAN-AVAR (2508.01725) introduces adaptive vicinity mechanisms; iCCDM adopts Hard AV which proved more robust than Soft/Hybrid AV for challenging datasets.
- Break condition: Fixed vicinity fails when local sample density varies significantly across label space; Hard AV mitigates this by computing κ_y dynamically via Algorithm 1 in [27].

### Mechanism 3
- Claim: Decoupled training/sampling condition weights (λ_y^t ≠ λ_y^s) enable task-specific control over condition influence.
- Mechanism: The non-negative weighting coefficient λ_y scales the condition-dependent term in gi(t,y) = √(2σ̇σ + λ_y ˜h_l,i(y)σ̇). For Steering Angle at 128×128 and 256×256, setting λ_y^t = 0.01 (training) and λ_y^s = 0.1 (sampling) amplifies condition influence during generation while maintaining stable training dynamics.
- Core assumption: Optimal condition influence differs between learning the score function and applying it for sampling.
- Evidence anchors:
  - [abstract] "introduces a non-negative weighting coefficient λ_y to flexibly modulate the influence of the condition"
  - [section III-C, Remark 3] "λ_y enables flexible control over the strength of the conditioning variable y in the diffusion process across different datasets"
  - [corpus] Corpus lacks direct evidence for train/sample decoupling; this appears specific to iCCDM.
- Break condition: Fig. 3 shows optimal λ_y is dataset-dependent; wrong values degrade SFID and label consistency.

## Foundational Learning

- Concept: **Itô Stochastic Differential Equations and PF-ODE**
  - Why needed here: iCCDM formulates forward/reverse diffusion as matrix-form SDEs (dX_t = Ṡ^(1/2) dB_t) and derives the probability flow ODE for deterministic sampling.
  - Quick check question: Given a variance-exploding SDE with zero drift, what determines the covariance of X_t | X_0 = x_0?

- Concept: **Score Matching and Denoiser-Score Connection**
  - Why needed here: The vicinal score estimate ∇_{x̃} log p_Σ(x̃|y) ≈ Σ^{-1}(D_θ(x̃, y, Σ) - x̃) links the learned denoiser to the conditional score function.
  - Quick check question: How does preconditioning D_θ via C_skip, C_out, C_in stabilize training across noise levels?

- Concept: **EDM Framework Fundamentals**
  - Why needed here: iCCDM extends EDM's noise-conditioned parameterization and preconditioning coefficients to matrix-form (Eq. 20), inheriting EDM's decoupling of training/sampling from fixed schedules.
  - Quick check question: Why does EDM sample σ from a log-normal distribution during training rather than using a fixed schedule?

## Architecture Onboarding

- Component map:
  F_θ backbone (CCDM UNet or DiT) -> Covariance embedding CNN (5-layer) -> Matrix-form preconditioning -> Vicinal loss computation

- Critical path:
  1. Pre-train covariance embedding CNN ϕ(y) or train jointly
  2. Compute adaptive vicinity radii κ_y for all training labels
  3. Training loop: sample (x_i, y_j), noise level σ, compute Σ(t, y; λ_y) via Eq. 9, apply matrix preconditioning Eq. 19, optimize vicinal loss Eq. 22
  4. Sampling: initialize x_N ~ N(0, Σ(t_N, y; λ_y)), run 32-step SDE/ODE solver with CFG

- Design tradeoffs:
  - **CCDM UNet vs. DiT**: UNet better at low resolutions; DiT excels at 256×256 (Table V) but requires more compute
  - **SDE vs. ODE sampler**: SDE-32 consistently outperforms ODE-32 (Table VII)—unlike standard EDM where ODE often matches/exceeds SDE
  - **Hard AV vs. Soft/Hybrid AV**: Hard AV more robust on challenging datasets (Steering Angle); Soft AV slightly better on UTKFace but less stable overall (Table VI)

- Failure signatures:
  - **High Label Score with low visual quality**: λ_y too low—condition not influencing diffusion adequately
  - **Mode collapse with low Diversity**: Vicinity radius κ_y too narrow or N_AV too high
  - **Memory blowup at 256×256**: Using MLP covariance embedding (61M params) instead of CNN (2.8M)

- First 3 experiments:
  1. **Sanity check**: On UTKFace 64×64, train with λ_y = 0 (vanilla EDM) vs. λ_y = 0.05 + Hard AV—verify SFID drops from ~0.36 to ~0.35 and Label Score improves (Table IV).
  2. **Sampler ablation**: On Steering Angle 64×64, compare SDE-10, SDE-32, SDE-50, ODE-32—confirm SDE-32 is optimal and that increasing steps beyond 32 provides no benefit (Table VII).
  3. **Covariance embedding swap**: Replace MLP5 with CNN5 on both datasets—verify parameter reduction from 61.4M to 2.8M with minimal SFID change (Table VIII).

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The framework's reliance on dynamic vicinity radius computation (Algorithm 1 from [27]) and the specific architecture of the lightweight CNN covariance embedding remain underspecified
- The superiority of SDE-32 over ODE-32 samplers contradicts standard EDM behavior, suggesting dataset-specific optimizations that may not generalize
- The decoupled λ_y^t/λ_y^s strategy, while effective for Steering Angle, lacks broader validation across diverse regression tasks

## Confidence
- **High Confidence:** Matrix-form EDM formulation and its integration with condition-specific noise covariance (Mechanism 1) - well-grounded in established SDE theory and validated through consistent SFID improvements.
- **Medium Confidence:** Adaptive vicinal training's effectiveness for label sparsity (Mechanism 2) - supported by comparisons with MaRS and CCDM, but dependent on external algorithm [27].
- **Medium Confidence:** Decoupled training/sampling condition weights (Mechanism 3) - demonstrated on Steering Angle but not extensively validated across datasets.

## Next Checks
1. **Architecture Fidelity Test:** Implement the 5-layer CNN covariance embedding with specified channel dimensions and validate parameter reduction (61.4M → 2.8M) without SFID degradation on UTKFace 64×64.
2. **Sampler Robustness Check:** Systematically compare SDE-10, SDE-32, SDE-50, and ODE-32 across all four datasets to confirm SDE-32's consistent superiority and understand dataset-specific behavior.
3. **Vicinity Radius Sensitivity:** Vary N_AV and κ_y parameters across datasets to map the relationship between local sample density and optimal adaptive vicinity performance, particularly for datasets with heterogeneous label distributions.