---
ver: rpa2
title: 'COPO: Consistency-Aware Policy Optimization'
arxiv_id: '2508.04138'
source_url: https://arxiv.org/abs/2508.04138
tags:
- global
- optimization
- grpo
- copo
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses a critical challenge in reinforcement learning
  for Large Language Models: when multiple responses to a single prompt yield identical
  outcomes, group-based advantage estimation collapses to zero, leading to vanishing
  gradients and training inefficiency. To solve this, the authors propose COPO, a
  consistency-aware policy optimization framework that introduces a structured global
  reward based on outcome consistency and an entropy-based soft blending mechanism.'
---

# COPO: Consistency-Aware Policy Optimization

## Quick Facts
- arXiv ID: 2508.04138
- Source URL: https://arxiv.org/abs/2508.04138
- Reference count: 2
- Qwen2.5-Instruct 7B improves MATH-500 accuracy by 2.22% (65.8% vs 63.58%) and AIME24 by 0.99% (13.85% vs 12.86%) compared to GRPO

## Executive Summary
COPO addresses a critical challenge in reinforcement learning for LLMs where multiple responses to a single prompt yield identical outcomes, causing group-based advantage estimation to collapse to zero. The authors propose a consistency-aware policy optimization framework that introduces structured global rewards based on outcome consistency and an entropy-based soft blending mechanism. This allows the model to receive meaningful learning signals even when intra-group consistency is high, balancing local and global optimization throughout training.

## Method Summary
COPO extends GRPO by introducing a structured global reward based on outcome consistency and an entropy-based soft blending mechanism. The method computes local advantages within each prompt group (G responses per prompt) while also calculating global advantages across the batch using mean rewards per prompt. Consistency entropy measures outcome diversity, and blending weights smoothly transition between local and global optimization based on this entropy. For prompts with all-incorrect responses, the method defaults to global-only optimization, providing learning signals for samples that would otherwise be discarded.

## Key Results
- COPO achieves 65.8% accuracy on MATH-500 (mean@8) for Qwen2.5-Instruct 7B, improving upon GRPO's 63.58%
- For AIME24, COPO reaches 13.85% (mean@64) versus GRPO's 12.86%
- GO-Selective variant (global optimization only for all-incorrect samples) achieves +3.05% mean improvement over baseline
- Consistent improvements across majority voting metrics for both MATH-500 and AIME benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Global optimization provides non-zero gradients when intra-group advantages vanish due to response uniformity.
- Mechanism: COPO computes a prompt-level reward $\hat{R}(q) = \frac{1}{G}\sum_{i=1}^G r_i$ and normalizes it against the batch distribution: $\hat{A}^{global}_q = \frac{\hat{R}(q_j) - \text{mean}(\{\hat{R}(q_j)\}_{j=1}^B)}{\text{std}(\{\hat{R}(q_j)\}_{j=1}^B)}$. This cross-prompt comparison yields non-zero advantages even when all responses for a single prompt are identical.
- Core assumption: The batch contains prompts with varying difficulty levels so that batch-level reward variance remains non-zero.
- Break condition: If the entire batch converges to uniform success/failure rates across all prompts, global advantages also vanish.

### Mechanism 2
- Claim: Entropy-based soft blending adaptively allocates optimization focus between local and global objectives based on response consistency.
- Mechanism: Consistency entropy $H(q) = -\sum_{\tau \in T_q} p(\tau) \log p(\tau)$ measures outcome diversity. The blending weight $w_{local}(H) = \sigma(\gamma(H - \rho))$ smoothly transitions between local optimization (high entropy → diverse responses → meaningful intra-group ranking) and global optimization (low entropy → uniform responses → need cross-prompt signal).
- Core assumption: High response diversity correlates with meaningful local advantage signals; low diversity indicates the need for global rescue.
- Break condition: If entropy is consistently high but local advantages still vanish (e.g., all responses different but equally wrong), the blending mechanism over-relies on local optimization.

### Mechanism 3
- Claim: Samples with all-incorrect responses retain learning value through global optimization rather than being discarded.
- Mechanism: DAPO filters out all-0 or all-1 samples, wasting training data. COPO's global optimization assigns these samples negative advantages (relative to batch mean), providing a penalizing gradient signal that discourages the policy from those reasoning paths.
- Core assumption: Incorrect trajectories contain learnable information about what to avoid, even without correct exemplars in the same group.
- Break condition: If the batch mean is also near-zero (most samples all-incorrect), the normalization may yield unstable or uninformative advantages.

## Foundational Learning

- **Group-Relative Advantage (GRPO)**
  - Why needed here: COPO extends GRPO; you must understand that GRPO computes advantages as $\hat{A}_i = \frac{r_i - \mu_r}{\sigma_r}$, which vanishes when $\sigma_r \to 0$.
  - Quick check question: Given rewards [0, 0, 0, 0, 0, 0], what is the GRPO advantage for each response? (Answer: undefined/zero due to division by zero variance)

- **Proximal Policy Optimization (PPO) Objective**
  - Why needed here: COPO inherits PPO's clipped surrogate objective and KL regularization structure.
  - Quick check question: Why does PPO clip the probability ratio $\frac{\pi_\theta}{\pi_{\theta_{old}}}$? (Answer: to prevent excessively large policy updates that destabilize training)

- **Entropy as a Diversity Measure**
  - Why needed here: COPO uses Shannon entropy over outcome distributions to quantify response consistency.
  - Quick check question: For outcomes [A, A, A, B, B, C] (G=6), what is the consistency entropy? (Answer: $H = -(\frac{3}{6}\log\frac{3}{6} + \frac{2}{6}\log\frac{2}{6} + \frac{1}{6}\log\frac{1}{6}) \approx 1.46$)

## Architecture Onboarding

- **Component map:**
  - Policy model $\pi_\theta$ -> generates G responses per prompt
  - Local reward function $R(\cdot)$ -> rule-based (correctness only)
  - Global reward aggregator -> computes $\hat{R}(q)$ per prompt
  - Consistency entropy calculator -> extracts unique outcomes and computes $H(q)$
  - Soft blending module -> sigmoid-weighted combination of local/global losses
  - (Optional) Reference model -> for KL divergence

- **Critical path:**
  1. Sample batch of B prompts, generate G responses each
  2. Extract final answers, compute rule-based rewards
  3. Compute local advantages (per-group normalization)
  4. Compute global rewards and advantages (per-batch normalization)
  5. Calculate consistency entropy per prompt
  6. Blend losses via sigmoid weighting
  7. Backpropagate combined objective with KL term

- **Design tradeoffs:**
  - Higher $\gamma$: sharper binary transition between local/global; risks premature commitment
  - Higher $\rho$: more samples use global optimization; may dilute fine-grained local credit assignment
  - Setting $w_{local}=0$ for all-incorrect samples (COPO's default): maximizes global signal but may over-penalize

- **Failure signatures:**
  - Training plateaus early with high all-incorrect sample proportion → check if global optimization is active
  - Unstable gradients with small batches → batch-level normalization may have high variance
  - Performance degradation on math-tuned small models → local/global objectives may conflict for specialized models

- **First 3 experiments:**
  1. **Reproduce GRPO baseline:** Train Qwen2.5-3B-Instruct on DAPO-MATH-17k for 60 steps; measure percentage of all-0 samples and mean@8 on MATH-500 to confirm vanishing gradient problem.
  2. **Ablate global-only optimization (GO-Only):** Set $w_{local}=0$ for all samples; verify that learning signals persist even for all-incorrect groups (expect ~4% gain per Table 3).
  3. **Sweep $(\gamma, \rho)$:** Test $\gamma \in \{3, 5, 10, 20\}$ and $\rho \in \{0.5, 1.0, 1.5\}$; observe that higher values favor global optimization and improve MATH-500 accuracy (per Table 4).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does COPO underperform GRPO on smaller, math-specific models (e.g., Qwen2.5-Math-1.5B), and does the composite loss conflict with domain-specific pretraining?
- **Basis in paper:** The "Limitation" section (Page 12) explicitly notes that COPO lags behind GRPO by approximately 1% on the Qwen2.5-Math-1.5B-Instruct model. The authors hypothesize that the local and global optimization objectives may conflict or weaken the structural representations in models specifically fine-tuned for math.
- **Why unresolved:** The paper identifies the performance drop but does not isolate the mechanism of failure, leaving uncertainty regarding whether the issue stems from model capacity, the specific math-tuning, or the blending of local/global losses in specialized domains.
- **What evidence would resolve it:** Ablation studies comparing the gradient alignment of local versus global losses in specialized models versus generalist models to identify if optimization directions contradict each other in smaller parameter spaces.

### Open Question 2
- **Question:** Can COPO effectively train small base models that lack instruction-following capabilities, or is a "cold-start" fine-tuning phase strictly necessary?
- **Basis in paper:** The "Experiments on Small Model and Base Model" section (Page 10) reports that base models (Qwen2.5-0.5B/3B) failed to learn using GRPO due to an inability to maintain output format. The authors suggest "cold-start strategies may be necessary prerequisites," but do not confirm if COPO's global signal mitigates this.
- **Why unresolved:** While COPO addresses gradient vanishing for "ineffective data," it is unclear if this signal is strong enough to bootstrap reasoning in models that cannot yet format answers, or if the sample wastage is simply due to the inability to acquire the format reward.
- **What evidence would resolve it:** Experiments applying COPO to a base model using only a format reward to determine if the inter-group optimization provides a sufficient learning signal to bootstrap instruction-following capabilities from scratch.

### Open Question 3
- **Question:** Is the entropy-based blending mechanism robust for tasks with non-discrete or non-exact outcomes, such as code generation?
- **Basis in paper:** The paper relies on rule-based rewards with exact equivalence checks for mathematical answers and calculates consistency entropy based on discrete unique outcomes. The method's applicability to complex tasks like code generation (mentioned in the Introduction) is inferred but not tested.
- **Why unresolved:** The consistency entropy calculation depends on counting exact outcome duplicates. In code generation, syntactically different code can yield the same execution result, potentially making the "consistency" metric noisy or misleading if based on string matching.
- **What evidence would resolve it:** Evaluation of COPO on coding benchmarks (e.g., HumanEval) where outcomes are defined by test-case passes rather than exact string matches, to verify if the entropy blending remains stable when outcome equivalence is functional rather than textual.

## Limitations
- COPO underperforms GRPO by approximately 1% on math-specific small models (Qwen2.5-Math-1.5B-Instruct), suggesting potential conflicts between local/global objectives in specialized domains
- Base models lacking instruction-following capabilities failed to learn even with COPO, indicating cold-start fine-tuning may be necessary prerequisites
- The entropy-based blending mechanism's effectiveness for non-discrete outcomes (e.g., code generation) remains untested and may be unreliable when outcome equivalence is functional rather than textual

## Confidence
- **High**: Global optimization provides non-zero gradients when local advantages vanish; this is mathematically guaranteed by batch-level normalization
- **Medium**: Entropy-based soft blending effectively allocates optimization focus; while theoretically sound, the specific hyperparameter choices lack extensive validation
- **Medium-High**: All-incorrect samples retain learning value through global optimization; strong ablation support but potential instability with highly homogeneous batches

## Next Checks
1. **Batch Homogeneity Stress Test**: Systematically evaluate COPO performance as batch composition varies from heterogeneous (diverse prompt difficulties) to homogeneous (similar difficulty). Measure how quickly global advantages vanish as batch variance approaches zero, confirming the break condition for Mechanism 1.

2. **Entropy Calibration Analysis**: For each prompt, compute both consistency entropy and local advantage variance across multiple training runs. Quantify the correlation between high entropy and meaningful local advantage signals to validate the core assumption of Mechanism 2. Test whether alternative diversity measures (e.g., pairwise response similarity) yield better blending performance.

3. **All-Incorrect Sample Dynamics**: Track the proportion and distribution of all-incorrect samples throughout training. Measure whether the global optimization signal for these samples stabilizes or becomes noisier as the model improves. Compare learning trajectories between COPO and variants that discard versus retain all-incorrect samples to confirm the value proposition of Mechanism 3.