---
ver: rpa2
title: Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal
  Planning Guidance using Reinforcement Learning
arxiv_id: '2505.13372'
source_url: https://arxiv.org/abs/2505.13372
tags:
- planning
- learning
- function
- heuristic
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents a reinforcement learning framework for synthesizing
  domain-specific temporal planning guidance. The key innovation is leveraging symbolic
  heuristics throughout the learning and planning pipeline: using them for reward
  shaping, learning residuals instead of full value functions, and combining learned
  guidance with systematic search via a multi-queue approach.'
---

# Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance using Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2505.13372
- **Source URL**: https://arxiv.org/abs/2505.13372
- **Reference count**: 35
- **Primary result**: RL framework learns domain-specific temporal planning guidance by leveraging symbolic heuristics for reward shaping, residual learning, and multi-queue planning, significantly outperforming baseline planners.

## Executive Summary
This work presents a reinforcement learning framework for synthesizing domain-specific temporal planning guidance that leverages symbolic heuristics throughout the learning and planning pipeline. The key innovations include using symbolic heuristics for reward shaping and bootstrap values, learning residuals rather than full value functions, and combining learned guidance with systematic search via a multi-queue approach. Experiments on three benchmark domains (MAJSP, Kitting, MatchCellar) demonstrate that all proposed techniques significantly improve planner performance compared to using no learned information.

## Method Summary
The framework encodes temporal planning problems as Markov Decision Processes where states are search states and actions are planning events. It trains neural networks to approximate value functions using two reward schemata (binary and counting) with symbolic heuristics integrated for bootstrap values and residual learning. The learned guidance is then used in a multi-queue planner that alternates between weighted A* with the symbolic heuristic and greedy best-first with the learned ranking. Training uses value iteration with replay buffers, and planning employs a round-robin selection between queues.

## Key Results
- All proposed techniques (residual learning, heuristic bootstrap, multi-queue planning) significantly outperform baseline planners without learned information
- Learning residuals accelerates convergence by learning corrections to symbolic heuristics rather than full value functions
- Multi-queue approach consistently improves robustness across domains by balancing greedy exploitation with systematic exploration
- The framework achieves improved coverage on benchmark domains while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1: Residual Learning Accelerates Convergence
- Learning a correction to an existing symbolic heuristic converges faster than learning the full heuristic from scratch because the residual function is more regular (closer to zero) than the full value function, simplifying the optimization landscape.
- Core assumption: The symbolic heuristic provides a reasonable baseline; grossly inaccurate symbolic heuristics may not help.
- Break condition: When h_sym is systematically biased in one direction, the residual may still require large corrections, reducing benefit.

### Mechanism 2: Truncation Bootstrap Reduces Sparse Reward Problems
- Using symbolic heuristics to estimate terminal values at episode truncation provides better learning signals than constant defaults, addressing the sparse reward problem in temporal planning MDPs.
- Core assumption: The symbolic heuristic correlates with true distances beyond the truncation depth.
- Break condition: When h_sym provides poor estimates at large depths, bootstrap values introduce systematic bias into value function learning.

### Mechanism 3: Multi-queue Planning Balances Exploitation with Systematic Search
- Alternating between learned ranking and symbolic heuristic queues consistently improves robustness across domains by balancing greedy exploitation with systematic exploration.
- Core assumption: Neither the learned ranking nor the symbolic heuristic is consistently superior; both have complementary failure modes.
- Break condition: When h_sym is very weak, the overhead of maintaining two queues outweighs benefits.

## Foundational Learning

- **Concept**: Markov Decision Processes (MDPs) and Value Functions
  - Why needed here: The framework encodes training problems as MDPs where states are planning search states and actions are planning events. Understanding V*(s) and how it relates to heuristic h* is essential.
  - Quick check question: Given an MDP state s with V*(s) = 0.5, discount γ = 0.99, and binary reward schema, what is the approximate heuristic distance to goal? (Answer: Using Eq. 1, h* ≈ log_0.99(0.5) + 1 ≈ 69.3 steps)

- **Concept**: A* Search and Heuristic Properties
  - Why needed here: The multi-queue planner combines weighted A* (systematic search) with greedy best-first (learned ranking). Understanding admissibility, consistency, and how heuristics affect search efficiency is prerequisite.
  - Quick check question: Why can the learned ranking function be used in GBFS but not directly in A*? (Answer: Rankings only provide relative ordering, not absolute distance estimates; A* requires admissible heuristics to guarantee optimality)

- **Concept**: Residual Learning in Neural Networks
  - Why needed here: The core innovation reframes learning from V*(s) to r*(s) = V*(s) - φ(s). Understanding why residual connections ease optimization (gradient flow, smoother target function) is critical.
  - Quick check question: If the symbolic heuristic is perfect (h_sym = h*), what should the residual network learn? (Answer: The zero function r(s) = 0, since no correction is needed)

## Architecture Onboarding

- **Component map**: Training problems P^k_D → MDP construction M[P^k_D] → RL algorithm (value iteration with replay buffer) → Neural network V_nn or residual r_nn → State encoding → Multi-queue search (Q[0]: weighted A* with h_sym, Q[1]: GBFS with learned ranking u = -V_nn) → Planning execution

- **Critical path**:
  1. Define object bound k and collect training instances P^k_D
  2. Configure MDP: choose reward schema (binary vs. counting), set Δ_RL (truncation depth), set Δ_h (dead-end penalty for counting)
  3. Train neural network: select full vs. residual learning, constant vs. heuristic bootstrap; run 25K-100K episodes
  4. Configure planner: set w for weighted A*, set Δ_H for heuristic capping; choose single-queue vs. multi-queue
  5. Deploy: load trained network, run on test instances from same distribution

- **Design tradeoffs**:
  - **Binary vs. Counting Reward**: Binary (γ < 1) requires log transformation, numerically unstable; Counting (γ = 1) directly learns h* but requires careful Δ_h tuning to separate good/uncertain/bad states
  - **Full vs. Residual Learning**: Residual converges faster but requires selecting h_sym; Full is more general but slower
  - **Single vs. Multi-queue**: Multi-queue is more robust but adds overhead; Single-queue better when h_sym is very weak
  - **Δ_RL vs. Δ_H**: Δ_RL limits RL exploration; Δ_H caps heuristic during planning. Setting Δ_H >> Δ_RL assumes network generalizes beyond training depths

- **Failure signatures**:
  - **Low coverage, early plateau**: Δ_RL too small (network can't reach goal states during training) or learning rate too high
  - **Heuristic values explode**: Δ_H too large relative to g-values in A*, causing learned states to be deprioritized; check capping in Eq. 3
  - **Multi-queue underperforms single-queue**: h_sym is very weak (coverage near 0 without learning), adding computational overhead without benefit
  - **Residual learning fails to improve**: h_sym is systematically biased (always over/underestimates), requiring residual to learn large corrections; check if r_nn outputs are near range limits
  - **Negative values treated as good states**: With counting reward, if Δ_h < Δ_RL, the "uncertain" range overlaps with "good" range (Section 3.2); increase Δ_h

- **First 3 experiments**:
  1. **Ablation on residual vs. full learning**: Train identical networks with counting reward, heuristic bootstrap, comparing residual vs. full learning. Measure episodes to reach 90% of final coverage. Expect residual to converge in ~50% fewer episodes based on Figure 3 patterns.
  2. **Multi-queue sensitivity to symbolic heuristic quality**: On a domain where h_sym has moderate coverage (e.g., MAJSP with h_ff ≈ 416/655), compare single-queue with learned heuristic vs. multi-queue. Vary w in weighted A* (0.5, 0.8, 0.95). Expect multi-queue to be less sensitive to w.
  3. **Truncation bootstrap impact at different depths**: Train with Δ_RL = 50, 100, 200 using both constant and heuristic bootstrap. On problems requiring ~150 steps, expect constant bootstrap to fail at Δ_RL = 50 (can't reach goals) while heuristic bootstrap maintains some performance via distance estimates beyond truncation.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can Graph Neural Networks (GNNs) be effectively adapted to this temporal planning framework to remove the constraint on the maximum number of objects?
  - Basis in paper: The authors state, "For future work, we plan to relax the assumption on the maximum number of objects by adapting recent results using Graph Neural Networks (GNN)."
  - Why unresolved: The current architecture requires a fixed-size input vector based on a bounded number of objects, limiting applicability to variable-sized problems without retraining.
  - What evidence would resolve it: A modification of the network architecture using GNNs that maintains or improves coverage on problems with object counts significantly larger than those in the training set.

- **Open Question 2**: Do multiplicative residuals provide better convergence or planning performance compared to the additive residuals currently implemented?
  - Basis in paper: The authors plan to "experiment with multiplicative residuals instead of additive residuals."
  - Why unresolved: The current work only evaluates learning an additive correction (r) to the symbolic value function (V* = r* + φ), leaving the utility of scaling the heuristic untested.
  - What evidence would resolve it: An empirical comparison of learning curves and planner coverage between additive and multiplicative residual learning techniques on the benchmark domains (MAJSP, Kitting, MatchCellar).

- **Open Question 3**: Can the counting reward schema (which allows γ=1) be modified to support classical planning domains with cyclic state spaces?
  - Basis in paper: The paper notes the counting reward is specific to temporal planning because the search space is a tree. It states that in classical planning, "it is not possible to set γ = 1 because there can be cycles in the graph of the state space."
  - Why unresolved: The theoretical benefits of directly learning the heuristic value (h*) via counting rewards are currently lost when moving to graph-based search spaces common in classical planning.
  - What evidence would resolve it: A theoretical adaptation of the reward formulation or a practical mechanism (e.g., loop detection) that allows the undiscounted counting reward to converge in cyclic domains.

## Limitations

- **Neural network architecture and state encoding**: The paper references prior work for network architecture but provides no details on layer counts, hidden sizes, or activation functions. State vectorization from complex temporal planning states is unspecified.
- **Symbolic heuristic dependence**: The framework's effectiveness depends critically on the quality of h_ff. In domains where h_ff has very low coverage, the residual learning and bootstrap mechanisms provide limited benefit.
- **Generalization assumptions**: The approach assumes training problems span the difficulty range needed for test problems. If test problems require significantly more steps than training problems, the learned heuristic may not generalize well.

## Confidence

- **High confidence**: The core mechanism of using symbolic heuristics for reward shaping and bootstrap is well-supported by MDP theory and experimental evidence. The residual learning acceleration effect is directly observable in learning curves.
- **Medium confidence**: The multi-queue approach's robustness claim is supported by cross-domain results, but the mechanism relies on unstated assumptions about complementary failure modes between learned and symbolic heuristics.
- **Medium confidence**: The truncation bootstrap improvement is logically sound but lacks direct empirical comparison to alternatives; the benefit depends heavily on symbolic heuristic quality at truncation depths.

## Next Checks

1. **Residual learning ablation study**: Systematically compare learning curves for full vs. residual learning across multiple domains, measuring episodes to reach 90% of final coverage. Verify the claimed ~50% convergence acceleration.

2. **Symbolic heuristic quality sensitivity**: Test the multi-queue approach across domains with varying h_ff coverage (low, medium, high). Measure performance sensitivity to weighted A* parameter w, expecting multi-queue to be more robust when h_ff coverage is moderate.

3. **Bootstrap method comparison**: Train identical networks with constant vs. heuristic bootstrap at different Δ_RL depths. On problems requiring more steps than Δ_RL, verify that heuristic bootstrap maintains performance while constant bootstrap fails due to sparse rewards.