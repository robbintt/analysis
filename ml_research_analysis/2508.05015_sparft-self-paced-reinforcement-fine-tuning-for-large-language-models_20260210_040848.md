---
ver: rpa2
title: 'SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models'
arxiv_id: '2508.05015'
source_url: https://arxiv.org/abs/2508.05015
tags:
- training
- examples
- sparft
- difficulty
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of training small language\
  \ models for reasoning tasks, where standard reinforcement learning fine-tuning\
  \ (RFT) methods require extensive data and compute. The proposed SPaRFT framework\
  \ introduces a two-stage approach: first, it clusters training data by semantics\
  \ and difficulty to reduce redundancy while maintaining diversity, then uses a multi-armed\
  \ bandit to dynamically select the most informative examples based on the model\u2019\
  s current performance."
---

# SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models
## Quick Facts
- **arXiv ID**: 2508.05015
- **Source URL**: https://arxiv.org/abs/2508.05015
- **Reference count**: 40
- **Primary result**: Achieves comparable or better accuracy than state-of-the-art baselines while using up to 100× fewer training samples for mathematical reasoning with tiny models (<1B parameters).

## Executive Summary
SPaRFT introduces a self-paced reinforcement fine-tuning framework for small language models, addressing the high data and compute demands of standard RFT. It combines semantic and difficulty-based clustering with a multi-armed bandit to select informative training examples dynamically. This approach enables efficient learning by focusing on examples the model is ready to learn from, achieving strong results on mathematical reasoning tasks with minimal training data.

## Method Summary
SPaRFT operates in two stages: first, it clusters training data by semantics and difficulty to reduce redundancy while maintaining diversity; second, it employs a multi-armed bandit to dynamically select the most informative examples based on the model's current performance. This self-paced curriculum enables efficient learning by focusing on examples the model is ready to learn from, achieving strong results on mathematical reasoning tasks with minimal training data.

## Key Results
- SPaRFT achieves 79.5% accuracy on GSM8K using only 100 examples, outperforming standard R1 (77.9%) with thousands of samples.
- Up to 100× reduction in training samples compared to state-of-the-art baselines.
- Lightweight method with minimal computational overhead.

## Why This Works (Mechanism)
SPaRFT leverages a self-paced curriculum that combines clustering and adaptive selection to efficiently train small models. By clustering data semantically and by difficulty, it reduces redundancy and ensures diverse training examples. The multi-armed bandit dynamically selects the most informative examples based on the model's current performance, focusing learning on what the model is ready to learn. This approach addresses the data inefficiency of standard RFT, particularly for small models on reasoning tasks.

## Foundational Learning
- **Reinforcement Learning Fine-Tuning (RFT)**: Why needed: Standard RFT requires extensive data and compute; quick check: Compare training curves with and without RFT.
- **Clustering for Curriculum Learning**: Why needed: Reduces redundancy and ensures diverse examples; quick check: Evaluate clustering quality on held-out data.
- **Multi-Armed Bandit for Adaptive Selection**: Why needed: Dynamically selects informative examples; quick check: Analyze example selection distribution over time.
- **Semantic and Difficulty-based Data Organization**: Why needed: Enables targeted learning progression; quick check: Measure improvement on examples of varying difficulty.

## Architecture Onboarding
- **Component Map**: Data Clustering -> Multi-Armed Bandit -> Example Selection -> Model Training
- **Critical Path**: Clustering organizes data, bandit selects examples, model is fine-tuned on selected examples.
- **Design Tradeoffs**: Clustering accuracy vs. computational overhead; bandit exploration vs. exploitation.
- **Failure Signatures**: Poor clustering leads to redundant or irrelevant examples; unstable bandit causes suboptimal selection.
- **First Experiments**: 1) Ablate clustering and measure performance drop. 2) Replace bandit with random selection and compare results. 3) Test on a non-mathematical dataset to assess generalizability.

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on effective clustering for initial curriculum construction.
- Need for stable bandit mechanism to adapt example selection.
- Performance gains demonstrated only on small models and reasoning tasks; scalability to larger models or other domains is uncertain.

## Confidence
- **High**: Core claims of improved accuracy and sample efficiency on GSM8K with tiny models.
- **Medium**: Broader claims of efficiency gains across diverse datasets and model types.
- **Low**: Claims about robustness to reward model choices without extensive ablation.

## Next Checks
1. Conduct ablation studies removing either the clustering or bandit module to isolate their individual contributions to performance.
2. Test SPaRFT on larger language models (>1B parameters) and on non-mathematical reasoning datasets to assess scalability and generalizability.
3. Compare SPaRFT's sample efficiency and final performance against other state-of-the-art reinforcement learning fine-tuning methods, including those using different reward formulations or curriculum strategies.