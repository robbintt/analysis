---
ver: rpa2
title: Transport Based Mean Flows for Generative Modeling
arxiv_id: '2509.22592'
source_url: https://arxiv.org/abs/2509.22592
tags:
- flow
- matching
- optimal
- generation
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational cost of generative modeling
  by accelerating flow-matching inference. The authors introduce OT-MF, which integrates
  optimal transport-based couplings into mean-flow frameworks, enabling efficient
  one-step generative trajectories.
---

# Transport Based Mean Flows for Generative Modeling

## Quick Facts
- arXiv ID: 2509.22592
- Source URL: https://arxiv.org/abs/2509.22592
- Reference count: 40
- Primary result: OT-MF achieves 1.9179 FID on MNIST vs 3.5119 for vanilla mean flow

## Executive Summary
This paper addresses the computational cost of generative modeling by accelerating flow-matching inference. The authors introduce OT-MF, which integrates optimal transport-based couplings into mean-flow frameworks, enabling efficient one-step generative trajectories. By leveraging transport couplings, OT-MF preserves fidelity and diversity while avoiding multi-step ODE integration. The method generalizes conditional flow matching and is enhanced with scalable OT solvers like linear and hierarchical OT for training efficiency. Experiments on low-dimensional synthetic data, image generation, point cloud generation, and image-to-image translation show that OT-MF significantly outperforms vanilla mean flow, achieving lower Wasserstein distances and FID scores.

## Method Summary
OT-MF replaces independent pairings between source and target distributions in mean flow matching with optimal transport couplings. The framework learns a time-averaged velocity field u(t,r,x) that enables single-step generation instead of multi-step ODE integration. During training, mini-batch optimal transport computes geometry-aware pairings between source and target samples, producing straighter generative trajectories. The method generalizes conditional flow matching by replacing the independent coupling with OT-based couplings. To address computational complexity, the authors introduce linearized OT variants (LOT-LR, LOT-HR) that use low-rank or hierarchical structures to reduce coupling computation from O(n³ log n) to O(rn(r+n)). The framework is trained using MSE loss between predicted mean flow and target mean flow computed via a PDE identity involving stop-gradient operators.

## Key Results
- OT-MF achieves 1.9179 FID on MNIST vs 3.5119 for vanilla mean flow
- On ShapeNet chairs, OT-MF reaches W₂=0.0121 at NFE=1, significantly lower than vanilla mean flow's 0.19
- LOT-LR achieves W₂=0.0168 on ShapeNet with comparable training time to OT-MF

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal transport couplings produce straighter generative trajectories than independent pairings, enabling more accurate one-step generation.
- Mechanism: Mini-batch OT computes geometry-aware pairings π₀,₁ between source and target samples that minimize total squared displacement. These optimal pairings reduce trajectory curvature because the interpolation X_t = (1-t)X₀ + tX₁ follows near-straight paths when (X₀, X₁) are optimally matched.
- Core assumption: The empirical OT coupling on mini-batches approximates the population-level geometric structure of the data manifold.
- Evidence anchors:
  - [abstract] "By replacing independent couplings with OT-based couplings between source and target distributions, OT-MF produces straighter and more efficient generative trajectories."
  - [section 1] "leverage optimal transport to define the joint sampling between source (Gaussian) and target data, yielding straighter trajectories and improved efficiency"
  - [corpus] Related work "Minibatch Optimal Transport and Perplexity Bound Estimation in Discrete Flow Matching" similarly uses mini-batch OT for improved discrete flow matching.

### Mechanism 2
- Claim: Learning the time-averaged velocity field u(t,r,x) instead of instantaneous velocity v(t,x) enables single-step generation.
- Mechanism: The mean flow u(t,r,x) = (1/(t-r))∫ᵣᵗ v(τ,x)dτ represents average displacement over interval [r,t]. The key identity (Eq. 12) relates u to v and enables supervision via u_tgt = v - (t-r)(v·∂ₓu + ∂ₜu). At inference, one step x₁ ≈ x₀ + u(1,0,x₀) replaces ODE integration.
- Core assumption: The PDE identity (12) provides stable supervision; the network can learn to approximate both instantaneous and average velocities consistently.
- Evidence anchors:
  - [section 2.3] "Mean-Flow model, which directly learns the average vector field: u(t,r,xₜ) := 1/(t-r) ∫ᵣᵗ v(τ,x)dτ"
  - [section 5] "This one-step transport significantly accelerates sampling while maintaining competitive generation quality"
  - [corpus] "Mean Flows for One-step Generative Modeling" introduces the foundational mean flow framework this work extends.

### Mechanism 3
- Claim: Linearized OT variants (LOT-LR, LOT-HR) reduce computational cost while preserving coupling quality.
- Mechanism: Instead of solving O(n³ log n) discrete OT, linearized methods use a reference measure σ and compute low-rank or hierarchical plans: γ^LOT-LR = γ₁^T diag(1/σ)γ₂ with complexity O(rn(r+n)). This factorizes coupling through an intermediate pivot.
- Core assumption: The low-rank/hierarchical structure captures essential geometric relationships with small rank r.
- Evidence anchors:
  - [section 3.1] "The computational cost of solving the discrete OT problem via network flow or linear programming is prohibitively high (O(n³ log n))"
  - [Table 3] LOT-LR achieves W₂=0.0168 vs OT-MF's 0.0121 on ShapeNet with comparable training time

## Foundational Learning

- Concept: **Optimal Transport (Benamou-Brenier formulation)**
  - Why needed here: Understanding why minimizing ∫||v_t||²p_t dt produces straight trajectories; the dynamic OT formulation directly motivates trajectory straightening.
  - Quick check question: Explain why the Benamou-Brenier formulation (minimizing kinetic energy) produces constant-speed straight-line interpolations between distributions.

- Concept: **Conditional Flow Matching**
  - Why needed here: OT-MF extends CFM by changing the coupling π₀,₁; understanding the conditional velocity v(x_t|X₀,X₁) = X₁ - X₀ is essential.
  - Quick check question: Derive why the conditional velocity for affine interpolation X_t = (1-t)X₀ + tX₁ equals X₁ - X₀.

- Concept: **Stop-gradient operator (sg) in training**
  - Why needed here: The target u_tgt depends on the model u_θ (from "previous moment"); stop-gradient prevents circular dependencies during backpropagation.
  - Quick check question: Why does u_tgt = v - (t-r)(v·∂ₓu_θ + ∂ₜu_θ) require stop-gradient on u_θ terms?

## Architecture Onboarding

- Component map:
Source/Target Data → Mini-batch Sampler → OT Solver: π₀,₁
                                                    ↓
Time Sampler (t,r) + (X₀,X₁)∼π₀,₁ → Interpolator: X_t, v_t
                                                    ↓
Mean Flow Network u_θ(t,r,xₜ) ← Target Computer: u_tgt(v_t,t,r)
              ↓
         MSE Loss → Optimizer

- Critical path: The OT coupling computation (step 4 in Algorithm 1) determines pairing quality; target computation (Eq. 14) must correctly apply stop-gradient and Jacobian-vector products; inference is single forward pass: x₁ = x₀ + u_θ(1,0,x₀).

- Design tradeoffs:
  - Exact OT vs. Sinkhorn vs. LOT: Exact OT gives best FID (Table 2: 1.9179) but O(n³); Sinkhorn adds ε-regularization (O(n²) per iteration); LOT trades rank r for speed
  - Batch size: Larger batches improve OT coupling quality but increase memory; paper uses B=256 for images, B=32 for point clouds
  - EMA vs. raw weights: EMA consistently improves metrics (Table 4a vs. 4b)

- Failure signatures:
  - Training divergence → Check stop-gradient on u_tgt; verify Jacobian-vector product implementation
  - Poor one-step FID despite low training loss → OT coupling may be weak; try exact OT or increase batch size
  - Slow training per epoch → Switch from exact OT to LOT-LR or Sinkhorn; Table 1 shows TR(ms) increases from ~5ms (MF) to ~12ms (OT-MF)

- First 3 experiments:
  1. **Low-dimensional validation**: Replicate Figure 2 (N→S-curve) to verify trajectory straightening; compare W₂ distances at NFE=1,2 across MF, OT-MF, LOT-LR
  2. **OT solver ablation**: On MNIST latent space, benchmark training time and FID for: exact OT, Sinkhorn (ε=0.01, 0.1), LOT-LR (r=4,8,16), LOT-HR; plot FID vs. training time
  3. **Coupling visualization**: For image-to-image translation (Adult→Child), visualize paired source-target samples from independent vs. OT coupling to verify semantic alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Generalized Sliced Optimal Transport (SOT) be effectively integrated into the Mean Flow framework to improve computational efficiency in high-dimensional settings?
- Basis in paper: [explicit] Appendix E states, "One of our future directions is applying generalized sliced OT into the old (flow matching) and new (mean flow matching) methods."
- Why unresolved: While SOT reduces high-dimensional OT to 1D problems, the authors note in Appendix E.2 that balancing computational efficiency with approximation quality and defining suitable generalized Radon transforms remain open challenges.
- What evidence would resolve it: Successful implementation and evaluation of a Sliced OT Mean Flow model on high-resolution datasets (e.g., ImageNet) showing improved speed over exact OT without loss of fidelity.

### Open Question 2
- Question: How can the feature mapping required for the generalized Radon transform be efficiently trained within the OT-MF framework?
- Basis in paper: [explicit] Appendix E.2 notes, "How to efficiently train such a feature mapping $h(x)$ is still unclear."
- Why unresolved: The generalized Radon transform requires a learnable mapping $h(x)$ to capture important features in high-dimensional spaces, but a standard training procedure for this component within the flow matching loss is not established.
- What evidence would resolve it: A defined optimization protocol for the feature mapping $h$ that leads to stable convergence and high-quality generation in complex data domains.

### Open Question 3
- Question: How can sliced OT plans be integrated with stochastic mini-batch training while preserving stability and convergence guarantees?
- Basis in paper: [explicit] Appendix E.2 lists as a challenge: "It remains unclear how to integrate sliced OT plans with stochastic mini-batch training while preserving stability and convergence guarantees in mean flow training."
- Why unresolved: The stochastic nature of mini-batch training combined with the variability of sliced OT approximations creates instability that has not yet been theoretically or empirically resolved.
- What evidence would resolve it: Theoretical analysis proving convergence bounds or empirical demonstrations of stable training dynamics when using sliced OT plans with mini-batches.

## Limitations
- OT coupling quality vs. computational cost: Exact OT provides superior FID but O(n³) complexity is prohibitive for large-scale datasets, creating uncertainty about the quality gap with scalable approximations.
- Jacobian-Vector Product implementation: Numerical stability of JVP computations during training, particularly near time interval boundaries, requires thorough validation.
- Generalization beyond pretrained encoders: Effectiveness in raw data spaces or with jointly trained encoder-generator systems remains unexplored.

## Confidence
- **High Confidence**: Theoretical foundation linking optimal transport to straighter trajectories; mean-flow identity (Eq. 12) for one-step generation; computational complexity analysis for OT solvers.
- **Medium Confidence**: Empirical results showing improved FID/W₂ metrics; scalability analysis via LOT variants; but lacks extensive ablation studies and direct comparisons.
- **Low Confidence**: Claim that OT couplings "preserve diversity" is weakly supported; needs direct diversity metrics; mini-batch OT approximating population geometry needs more rigorous validation.

## Next Checks
1. **OT Solver Ablation Study**: Systematically compare exact OT, Sinkhorn (varying ε), LOT-LR (varying rank r), and LOT-HR on MNIST latent space. Measure training time per epoch, FID at convergence, and sample quality across different batch sizes.

2. **Direct Trajectory Visualization**: For N→S-curve experiment, generate 100 trajectories from OT-MF and vanilla MF. Compute and visualize average curvature (integrated squared curvature along paths) and endpoint variance to directly verify "straighter trajectories" claim.

3. **Diversity Metric Analysis**: Generate 10,000 samples using OT-MF and vanilla MF on MNIST latent space. Compute LPIPS distance distributions, intra-class variation (for digit classification), and coverage metrics to verify OT-MF preserves diversity while improving fidelity.