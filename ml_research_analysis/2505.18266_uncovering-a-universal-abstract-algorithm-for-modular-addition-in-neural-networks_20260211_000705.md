---
ver: rpa2
title: Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks
arxiv_id: '2505.18266'
source_url: https://arxiv.org/abs/2505.18266
tags:
- neurons
- cosets
- frequencies
- figure
- approximate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unifying theory for neural network solutions
  to modular addition, showing that seemingly disparate mechanisms across architectures
  and depths all implement an abstract approximate Chinese Remainder Theorem (aCRT).
  The authors introduce "approximate cosets" as a generalization of cosets, demonstrating
  that neurons in all layers activate exclusively on these structures.
---

# Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks

## Quick Facts
- arXiv ID: 2505.18266
- Source URL: https://arxiv.org/abs/2505.18266
- Reference count: 40
- This paper presents a unifying theory for neural network solutions to modular addition, showing that seemingly disparate mechanisms across architectures and depths all implement an abstract approximate Chinese Remainder Theorem (aCRT).

## Executive Summary
This paper resolves conflicting interpretations of neural network solutions to modular addition by introducing "approximate cosets" as a generalization of cosets and demonstrating that all networks—regardless of architecture or depth—implement a common abstract algorithm called the approximate Chinese Remainder Theorem (aCRT). The authors show that neurons activate exclusively on approximate cosets, and that learning O(log n) frequencies suffices for good margins between correct and incorrect logits. Empirically, they validate this across MLPs, transformers, and varying depths, showing that trainable embeddings or additional depth compress the number of frequencies from O(n) to O(log n). This work reconciles previous conflicting interpretations and opens a testable universality hypothesis for group multiplication tasks.

## Method Summary
The paper studies neural networks solving c = (a + b) mod n classification tasks using architectures ranging from 1-4 layer MLPs to transformers with either "pizza" (uniform attention α=0) or "clock" (learned attention α=1) mechanisms. Inputs are either one-hot encoded (n² pairs) or via trainable embeddings (n × 128 matrix). Networks are trained with Adam optimizer and L2 regularization (1e-5 to 1e-3) until ≥99.9999% accuracy. The key analysis involves extracting neuron preactivations, computing their DFT to identify frequency clusters, and verifying simple neuron fits (R² > 0.95). The authors prove that O(log n) frequencies suffice for Ω(log n) logit margins and validate this empirically across architectures.

## Key Results
- All neural networks solving modular addition implement an abstract algorithm (aCRT) based on approximate cosets, regardless of architecture or depth.
- Networks with trainable embeddings or depth >1 layers learn O(log n) frequencies instead of O(n), achieving the same accuracy with fewer components.
- Theoretical proof shows O(log n) frequencies suffice for Ω(log n) logit margins between correct and incorrect answers.
- Simple neuron model (sinusoidal approximation) fits first-layer neurons with R² > 0.95 across all architectures tested.

## Why This Works (Mechanism)

### Mechanism 1: Approximate Cosets for Neuron Activation
ReLU neurons learning modular addition activate exclusively on "approximate cosets"—sets of elements with similar behavioral properties rather than strict modular equivalence. Each neuron with frequency f generates a periodic activation pattern where elements within small graph distances on the Cayley graph form approximate cosets where ReLU(x) > 0. The core assumption is that neurons converge to sinusoidal functions (the "simple neuron model"). If neurons exhibit non-sinusoidal activation patterns or activate on structurally disjoint sets, the approximate coset abstraction fails.

### Mechanism 2: Abstract Approximate Chinese Remainder Theorem (aCRT)
Diverse circuit implementations instantiate a common abstract algorithm—selecting O(log n) frequencies and intersecting their approximate cosets to isolate the correct answer. Each frequency defines a modular subsystem where neurons vote for outputs within specific approximate cosets. The intersection of votes across O(log n) frequencies concentrates probability mass on the correct logit via margin amplification. The core assumption is that frequencies are approximately randomly selected from valid candidates. If networks require Ω(n) frequencies or fail to show margin growth with frequency count, aCRT is not the operative algorithm.

### Mechanism 3: Depth and Embedding Compression to O(log n) Frequencies
Adding trainable embeddings or depth (>1 layer) causes a phase transition from ⌊n/2⌋ frequencies to O(log n) frequencies. Trainable embeddings and deeper layers enable more efficient feature composition, reducing the frequency count while maintaining accuracy. Layers beyond the first mix first-order sinusoids (cos(f·a) + cos(f·b)) with second-order terms (cos(f·(a+b-c))) in transformers. The core assumption is that training converges to solutions with sufficient margins. If deep networks with embeddings consistently learn Ω(n) frequencies, the compression mechanism is absent.

## Foundational Learning

- **Concept: Cosets and Modular Arithmetic**
  - Why needed here: Approximate cosets generalize exact cosets; understanding {0, 3}, {1, 4}, {2, 5} as cosets mod 3 in C₆ is prerequisite.
  - Quick check question: What are the cosets of subgroup {0, 4} in Z₈?

- **Concept: Chinese Remainder Theorem (CRT)**
  - Why needed here: The aCRT abstracts the CRT's coset intersection strategy; CRT provides the structural template.
  - Quick check question: If x ≡ 2 (mod 3) and x ≡ 3 (mod 5), what is x mod 15?

- **Concept: Cayley Graphs and Step Size**
  - Why needed here: Approximate cosets are defined via graph distance on Cayley graphs; step size d determines connectivity.
  - Quick check question: For C₆ with step s=2, how many connected components does the Cayley graph have?

## Architecture Onboarding

- **Component map:** Input (one-hot or embeddings) -> Core (1-4 layer MLP/transformer) -> Output (linear layer to n logits)
- **Critical path:** Train → Extract neuron preactivations → DFT to identify frequency clusters → Verify simple neuron fit (R² > 0.95) → Map to approximate cosets via step size d
- **Design tradeoffs:**
  - One-hot vs. embeddings: One-hot yields ⌊n/2⌋ frequencies (interpretable but redundant); embeddings yield O(log n) frequencies (efficient but compressed)
  - Pizza vs. clock transformers: Pizza uses fixed attention (simpler mechanism); clock learns attention (more flexible, learns coset-aligned frequencies earlier)
  - Depth vs. width: Depth reduces frequency count; width improves single-sine approximation quality
- **Failure signatures:**
  - Grokking failure (no generalization): Weight decay too low or training set too small
  - Single-frequency solutions: Hyperparameters at grid edges; poor margins, unstable
  - Non-sinusoidal neurons: Width too narrow; increase hidden dimension
- **First 3 experiments:**
  1. **Baseline validation:** Train 1-layer MLP with one-hot inputs on mod 59; verify ~29 frequencies appear (⌊59/2⌋) and neurons fit simple sine model.
  2. **Embedding compression test:** Add 128-dim trainable embeddings; confirm frequency count drops to 5-7 and margin remains >0.
  3. **Approximate coset visualization:** Pick a neuron with frequency f where gcd(f, n) > 1; plot preactivations, remap via Definition 4.2, and verify activation clusters on predicted coset boundaries.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do neural networks implement the approximate Chinese Remainder Theorem (aCRT) using coset circuits for group multiplication tasks involving groups that lie between simple cyclic groups and complex permutation groups? The authors explicitly identify this as a "promising avenue for future work," stating a limitation is that they did not "explore the groups between these two extremes" despite establishing universality for cyclic and permutation groups.

- **Open Question 2:** What are the underlying training dynamics or learned mechanisms that cause the phase transition from learning O(n) frequencies in shallow, one-hot networks to O(log n) frequencies in deep networks or those with trainable embeddings? The authors state, "we don't know why it occurs," referring to the transition shown in Figure 4, and ask, "are the training dynamics wildly different? What causes it?"

- **Open Question 3:** What is the minimum number of neurons required per learned frequency to successfully implement the aCRT? The authors note that Theorem 4.7 only provides a trivial Ω(1) lower bound and state, "We believe answering this is necessary for the interpretability community to gain a full understanding of a task."

## Limitations
- The framework depends on ReLU networks converging to sinusoidal simple neurons—non-sinusoidal activation patterns break the approximate coset framework.
- The O(log n) frequency bound may not be tight for all edge cases requiring more frequencies for good margins.
- Claims about universality extending beyond modular addition to general group multiplication tasks lack strong corpus support.

## Confidence

- **High Confidence:** The empirical observation that trained networks learn O(log n) frequencies with good logit margins, and the simple neuron approximation fits (R² > 0.95) for first-layer neurons.
- **Medium Confidence:** The theoretical proof that O(log n) frequencies suffice for Ω(log n) margins, and the phase transition from ⌊n/2⌋ to O(log n) frequencies with embeddings/depth.
- **Low Confidence:** Claims about universality extending beyond modular addition to general group multiplication tasks, given limited corpus support.

## Next Checks

1. **Frequency Distribution Test:** Train networks on multiple moduli (n=59, 66, 101) and verify frequency count follows O(log n) scaling, not linear in n. Check if frequencies cluster around divisors of n as predicted by approximate coset theory.

2. **Non-ReLU Activation Test:** Replace ReLU with GeLU or SiLU and verify whether approximate cosets and simple neuron approximations still emerge. This tests the universality of the sinusoidal convergence mechanism.

3. **Cross-Task Transfer:** Apply the aCRT framework to modular multiplication (c = a × b mod n) and verify whether networks learn O(log n) frequencies with similar margin properties, establishing the universality hypothesis for group operations.