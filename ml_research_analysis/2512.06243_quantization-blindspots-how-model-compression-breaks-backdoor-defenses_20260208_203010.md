---
ver: rpa2
title: 'Quantization Blindspots: How Model Compression Breaks Backdoor Defenses'
arxiv_id: '2512.06243'
source_url: https://arxiv.org/abs/2512.06243
tags:
- quantization
- backdoor
- defenses
- neural
- defense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals a critical vulnerability in backdoor defenses
  when models are quantized for deployment. While defenses like Neural Cleanse work
  on full-precision models, INT8 quantization completely eliminates their detection
  capability while leaving backdoors fully functional with 99% attack success rates.
---

# Quantization Blindspots: How Model Compression Breaks Backdoor Defenses

## Quick Facts
- arXiv ID: 2512.06243
- Source URL: https://arxiv.org/abs/2512.06243
- Authors: Rohan Pandey; Eric Ye
- Reference count: 7
- Primary result: INT8 quantization completely eliminates backdoor detection while leaving backdoors fully functional (>99% ASR)

## Executive Summary
This paper reveals a critical vulnerability in backdoor defenses when models are quantized for deployment. While defenses like Neural Cleanse work on full-precision models, INT8 quantization completely eliminates their detection capability while leaving backdoors fully functional with >99% attack success rates. For INT4 quantization, detection failure is dataset-dependent: Neural Cleanse works on GTSRB but fails on CIFAR-10, even though backdoors persist above 90% ASR on both. The study systematically evaluates five defenses across three quantization schemes on two benchmarks, exposing a mismatch between how defenses are evaluated (FP32) versus how models are actually deployed (quantized). This finding has immediate practical implications for security-critical model deployment pipelines.

## Method Summary
The study evaluates five backdoor defenses (Neural Cleanse, Activation Clustering, Spectral Signatures, STRIP, and Fine-Pruning) across three quantization schemes (FP32 baseline, INT8 dynamic, and INT4 simulated) on CIFAR-10 and GTSRB datasets. ResNet-18 models are trained with BadNet attack (3×3 white square trigger at bottom-right, 10% poisoning rate, target class 0). INT8 uses PyTorch's `quantize_dynamic` with qnnpack. INT4 is simulated via per-channel symmetric quantize-dequantize on conv/linear layers. The evaluation measures detection rate and attack success rate for each defense-quantization combination.

## Key Results
- INT8 quantization eliminates Neural Cleanse detection capability (0% detection rate) while maintaining backdoor functionality (>99% ASR)
- INT4 quantization causes dataset-dependent defense failure: Neural Cleanse fails on CIFAR-10 but works on GTSRB
- Backdoor triggers persist through quantization with higher fidelity than defense detection signals due to their "shortcut" nature
- Clean accuracy preservation under INT4 correlates with defense efficacy retention

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** INT8 quantization neutralizes optimization-based defenses (e.g., Neural Cleanse) by flattening the loss landscape, causing distinct class triggers to converge to identical degenerate solutions.
- **Mechanism:** Neural Cleanse relies on gradient descent to find minimal perturbations (triggers) for each class. Quantization discretizes weights, transforming the smooth FP32 loss surface into a piecewise-constant landscape with plateaus. In this flattened landscape, gradients fail to distinguish the backdoor target class from others, causing the optimizer to converge to a shared solution with identical trigger norms for all classes.
- **Core assumption:** The defense failure is driven primarily by the loss of gradient information in the discretized weight space, rather than an increase in detection difficulty alone.
- **Evidence anchors:**
  - [Section 6.3]: "On INT8-quantized models, all classes converge to identical trigger norms of 27.9, completely eliminating the discriminative signal."
  - [Section 7.1]: "Quantization transforms the smooth FP32 loss landscape into a piecewise-constant surface... creating flat regions where gradients vanish."
  - [corpus]: Weak direct support for defense failure mechanics; neighbor papers focus on attack injection during quantization.
- **Break condition:** This mechanism would not hold if the defense used gradient-free optimization or if the quantization step size were significantly smaller than the variance of the target class triggers.

### Mechanism 2
- **Claim:** Aggressive quantization (INT4) causes dataset-dependent defense failure determined by the model's robustness to accuracy degradation.
- **Mechanism:** If INT4 quantization catastrophically drops clean accuracy (e.g., CIFAR-10: 90% → 52%), the model's representational geometry is fundamentally disrupted, destroying the structural anomalies (e.g., clustering separation) that defenses rely on. If the model is robust to INT4 (e.g., GTSRB: minimal accuracy drop), the representation space retains enough fidelity for defenses like Neural Cleanse to succeed.
- **Core assumption:** The preservation of defense efficacy correlates directly with the preservation of clean accuracy (representational fidelity) post-quantization.
- **Evidence anchors:**
  - [Section 6.2]: "INT4 quantization causes catastrophic accuracy degradation on CIFAR-10... while having minimal impact on GTSRB."
  - [Table 2]: Shows the correlation between clean accuracy collapse (CIFAR-10 INT4) and defense failure, vs. accuracy retention (GTSRB INT4) and defense success.
  - [corpus]: Not explicitly addressed in corpus.
- **Break condition:** This mechanism breaks if a defense relies on signals orthogonal to the model's primary classification representations (e.g., weight magnitude statistics independent of activation clusters).

### Mechanism 3
- **Claim:** Backdoor triggers persist through quantization with higher fidelity than defense detection signals due to their "shortcut" nature.
- **Mechanism:** Backdoor triggers are learned as high-magnitude, robust shortcuts that dominate prediction logic. Quantization acts as a uniform perturbation. While this noise is sufficient to collapse subtle statistical anomalies used by defenses (spectral signatures, cluster boundaries), it is insufficient to overcome the strong, consistent signal of the backdoor trigger, leaving Attack Success Rates (ASR) >99%.
- **Core assumption:** The backdoor trigger is effectively a robust feature learned during training, whereas defense signals are fragile artifacts of the full-precision weight distribution.
- **Evidence anchors:**
  - [Abstract]: "INT8 quantization completely eliminates their detection capability while leaving backdoors fully functional with >99% attack success rates."
  - [Table 2]: ASR remains >99% for INT8 and >90% for INT4 across datasets, even where detection fails.
  - [corpus]: "Rounding-Guided Backdoor Injection" (arXiv:2510.09647) supports the premise that backdoors can be robust to or explicitly designed for quantization contexts.
- **Break condition:** Assumption fails if the backdoor trigger relies on fine-grained precision (e.g., small-magnitude weight perturbations) rather than robust features.

## Foundational Learning

- **Concept: Post-Training Quantization (PTQ)**
  - **Why needed here:** To understand how INT8/INT4 reduce precision without retraining and why this changes the loss landscape.
  - **Quick check question:** Does PTQ require the original training dataset to adjust model weights? (Answer: No, it adjusts precision based on weight ranges/calibration data).

- **Concept: Trigger Reverse-Engineering (Neural Cleanse)**
  - **Why needed here:** To grasp why finding a "minimal perturbation" fails when the optimization surface becomes discrete.
  - **Quick check question:** Neural Cleanse flags a class as infected if its reverse-engineered trigger norm is significantly *smaller* or *larger* than the median? (Answer: Smaller).

- **Concept: Attack Success Rate (ASR) vs. Clean Accuracy (CA)**
  - **Why needed here:** To interpret the core asymmetry of the paper—high CA/ASR (attack works) but 0% Detection (defense fails).
  - **Quick check question:** If a model has 99% ASR after quantization, is the backdoor considered "erased"? (Answer: No, it is functional).

## Architecture Onboarding

- **Component map:** Pre-trained FP32 Backdoored Model -> Quantization Operator (Q_INT8/Q_INT4) -> Discretized Weights -> Defense Algorithms (Neural Cleanse, Activation Clustering) -> Detection Rate/ ASR metrics
- **Critical path:** The security failure occurs at the interaction between the **Quantization Operator** (which flattens the optimization landscape) and the **Defense Optimizer** (which requires gradient diversity to find class-specific triggers).
- **Design tradeoffs:**
  - FP32 Detection vs. INT8 Deployment: Validating security on FP32 gives false confidence; validating on INT8 currently yields 0% detection.
  - Precision vs. Security: Lower precision (INT4) may break defenses irreversibly even if the model retains accuracy (dataset dependent).
- **Failure signatures:**
  - Trigger Norm Convergence: Neural Cleanse returns identical ℓ₁ norms for all classes (e.g., all ≈ 27.9) rather than a distinct outlier for the target class.
  - Accuracy-Defense Correlation: If INT4 causes clean accuracy to drop significantly (e.g., CIFAR-10), expect concurrent defense failure.
- **First 3 experiments:**
  1. **Verify Asymmetry:** Train a BadNet model, quantize to INT8, and confirm ASR >90% while running Neural Cleanse to verify detection failure.
  2. **Analyze Trigger Norms:** Extract per-class trigger norms from Neural Cleanse on INT8 vs. FP32 to observe the "convergence/collapse" phenomenon (MAD → 0).
  3. **Test Data Dependency:** Apply INT4 quantization to both a "robust" dataset (e.g., GTSRB) and a "fragile" dataset (CIFAR-10) to reproduce the divergence in Neural Cleanse efficacy based on clean accuracy retention.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the quantization defense failure findings generalize to more sophisticated backdoor attacks (WaNet, LIRA, clean-label, input-aware triggers)?
- **Basis in paper:** [explicit] "More sophisticated attacks...may interact differently with quantization and potentially exhibit different defense evasion characteristics."
- **Why unresolved:** The study only evaluates the canonical BadNet patch-based attack, leaving uncertainty about whether quantization similarly disables defenses against stealthier attack variants.
- **What evidence would resolve it:** Systematic evaluation of the five defenses across quantization schemes using WaNet, LIRA, clean-label, and input-aware attacks on the same benchmarks.

### Open Question 2
- **Question:** Does quantization-aware training (QAT) preserve backdoor defense efficacy compared to post-training quantization?
- **Basis in paper:** [explicit] "Quantization-aware training (QAT), where models are trained with simulated quantization, may yield different results as models explicitly adapt to reduced precision during learning."
- **Why unresolved:** The study focuses exclusively on post-training quantization; QAT models may develop fundamentally different backdoor characteristics through explicit precision adaptation.
- **What evidence would resolve it:** Comparative evaluation where backdoored models undergo QAT versus PTQ, measuring both defense detection rates and attack success rates.

### Open Question 3
- **Question:** What are the precise mechanistic causes of defense failure under quantization?
- **Basis in paper:** [inferred] The paper proposes multiple hypotheses (loss landscape distortion, representation space compression, statistical distribution shift) but does not conclusively validate any mechanism.
- **Why unresolved:** The analysis is correlational; causation between quantization effects and specific defense failure modes remains unproven.
- **What evidence would resolve it:** Controlled experiments isolating each hypothesized mechanism (e.g., probing gradient landscapes pre/post quantization, measuring representation dimensionality changes) with causal intervention studies.

### Open Question 4
- **Question:** Do these findings transfer to vision transformers and large-scale architectures?
- **Basis in paper:** [explicit] "Our evaluation uses ResNet-18 exclusively. Larger models, different architectural families, and particularly vision transformers warrant separate investigation given their distinct quantization behaviors."
- **Why unresolved:** ViTs have different quantization properties and representation structures; defense behavior under compression may differ substantially from CNNs.
- **What evidence would resolve it:** Replication of the evaluation protocol using ViT and larger architectures (e.g., ResNet-50/101) on the same attack-defense-quantization combinations.

## Limitations

- The INT4 quantization implementation is simulated rather than hardware-based, potentially missing implementation-specific effects that could alter defense behavior
- The study focuses on a single backdoor pattern (3×3 white square), limiting generalizability to more sophisticated trigger designs
- No investigation of defense performance when quantization is applied during training (quantization-aware training) rather than post-training

## Confidence

- **High Confidence**: INT8 quantization eliminates Neural Cleanse detection across both datasets (observed 0% detection, ASR >99%)
- **Medium Confidence**: INT4 defense failure is dataset-dependent (confirmed on CIFAR-10 but requires verification on GTSRB for Neural Cleanse)
- **Medium Confidence**: Defense detection correlates with clean accuracy preservation under INT4 (supported by GTSRB/CIFAR-10 comparison but not systematically tested across more datasets)

## Next Checks

1. Test Neural Cleanse on INT4-quantized GTSRB models to confirm whether defense succeeds when clean accuracy is preserved
2. Implement quantization-aware training to determine if defenses can be made robust when quantization is part of the training pipeline
3. Evaluate defenses against alternative backdoor patterns (color triggers, distributed patterns) to assess whether the quantization-blindspot phenomenon extends beyond the tested trigger design