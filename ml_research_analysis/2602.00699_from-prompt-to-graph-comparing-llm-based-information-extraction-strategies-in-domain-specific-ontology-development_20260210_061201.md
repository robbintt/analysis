---
ver: rpa2
title: 'From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies
  in Domain-Specific Ontology Development'
arxiv_id: '2602.00699'
source_url: https://arxiv.org/abs/2602.00699
tags:
- extraction
- terms
- relation
- training
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares three LLM-based methods for extracting terms
  and relations to support ontology development in the casting domain. A pre-trained
  model, few-shot in-context learning, and a fine-tuned model were evaluated using
  limited data.
---

# From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development

## Quick Facts
- arXiv ID: 2602.00699
- Source URL: https://arxiv.org/abs/2602.00699
- Reference count: 32
- Three LLM-based methods compared for ontology development in casting domain; fine-tuned model achieved 93.8% precision and 87.6% recall for term extraction, 86.7% precision and 73.4% recall for relation extraction

## Executive Summary
This study evaluates three LLM-based approaches—pre-trained model, few-shot in-context learning (ICL), and fine-tuning—for extracting domain-specific terms and relations to support ontology development in manufacturing. Using a small dataset of 173 training examples from casting literature, the fine-tuned GPT-4.1-mini model outperformed other methods, achieving the highest precision and recall in both term and relation extraction. The resulting ontology includes 273 concepts and 620 validated relations, demonstrating the feasibility of using LLMs for semi-automated, domain-specific ontology construction in manufacturing contexts.

## Method Summary
The research team developed a three-step pipeline for ontology construction: first, they distilled domain knowledge from academic papers and technical books into 173 short text samples using RAG-based question answering; second, they manually annotated these samples for term extraction and relation triple annotation using Label Studio; third, they evaluated three extraction methods (pre-trained LLM, ICL with k=16 examples, and fine-tuned GPT-4.1-mini) on 30 long test paragraphs. The fine-tuning approach used a learning rate multiplier of 2.0 and trained for 3 epochs with batch size 1. Extracted triples were stored in a Neo4j graph database with automatically generated Cypher queries.

## Key Results
- Fine-tuned model achieved highest performance: 93.8% precision and 87.6% recall for term extraction
- Fine-tuned model achieved 86.7% precision and 73.4% recall for relation extraction, outperforming ICL and pre-trained approaches
- The resulting ontology contains 273 concepts and 620 validated relations from 1,394 extracted relations
- Fine-tuned model detected 20 out of 22 synonym pairs, while ICL detected only 1 pair

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning with limited domain data yields more consistent relation naming and higher extraction quality than prompting-based approaches. Parameter updates during fine-tuning encode domain-specific patterns into model weights, enabling the model to learn standardized relation labels from annotated examples rather than relying on input-text expressions. This reduces output variability seen in ICL where the model copies phrasing from context. Core assumption: Training data annotation quality directly transfers to learned model behavior. Evidence anchors: [abstract] fine-tuned model achieved highest performance; [section 3.2] fine-tuning showed much greater consistency in relation naming; [corpus] related work shows zero-shot triple extraction but doesn't validate fine-tuning advantages. Break condition: Training data contains inconsistent annotations or is too small (<50 samples).

### Mechanism 2
In-context learning with semantic similarity-based example retrieval improves extraction over zero-shot prompting but remains sensitive to example distribution. k-shot examples provide output format templates and domain context without weight updates. Semantic similarity retrieval (UAE-Large-V1) selects training examples matching test input characteristics, leveraging the model's pattern-matching capabilities from demonstrations. Core assumption: Semantically similar training examples contain relevant extraction patterns that transfer to test inputs. Evidence anchors: [section 2.3.1] ICL performance depends on semantic similarity; [section 3.2] relation label inconsistency was a major limitation; [corpus] ontology-guided prompt learning supports generalization. Break condition: k-shot examples have low semantic similarity to test inputs or relation types are underrepresented.

### Mechanism 3
Two-step extraction (terms first, then relations) improves precision over joint extraction by constraining the relation task to validated terms. Separating term extraction from relation extraction reduces the search space for each task. Terms identified in step 1 become candidates for step 2, enabling the model to focus on identifying semantic relationships among known entities rather than simultaneously detecting entities and relations. Core assumption: Errors in term extraction propagate to relation extraction; high term precision enables better relation precision. Evidence anchors: [section 2.3] ICL and fine-tuning follow two-step process; [section 2.3.1] subject and object drawn from terms identified in step 1. Break condition: Term extraction has low recall, missing entities that should participate in relations.

## Foundational Learning

- Concept: **Ontology as (subject, object, relation) triples**
  - Why needed here: The entire extraction framework outputs knowledge in triple format for graph database storage. Understanding this structure is prerequisite to evaluating extraction quality and designing prompts.
  - Quick check question: Given "solidification time controlled by temperature," can you identify the subject, object, and relation?

- Concept: **In-context learning (k-shot) vs fine-tuning**
  - Why needed here: The paper compares these approaches directly. You must understand that ICL uses examples at inference time without weight changes, while fine-tuning permanently modifies model parameters.
  - Quick check question: If you have 50 labeled examples and need to minimize compute cost, which approach should you try first?

- Concept: **Precision vs recall in extraction evaluation**
  - Why needed here: The paper reports both metrics differently across methods. Pre-trained LLM extraction could only report precision (no position information for recall), while ICL and fine-tuning report F1.
  - Quick check question: A model extracts 100 terms, 90 are correct, but misses 50 ground-truth terms. What are precision and recall?

## Architecture Onboarding

- Component map: RAG distillation → expert-guided top concepts (6 categories) → annotation → model training/inference → expert validation → ontology construction in Neo4j
- Critical path: RAG distillation → expert-guided top concepts (6 categories) → annotation → model training/inference → expert validation → ontology construction in Neo4j
- Design tradeoffs:
  - ICL: Lower cost, faster iteration, but inconsistent relation naming (limitation for production ontologies)
  - Fine-tuning: Higher upfront cost (3 epochs, batch size 1), but better consistency and recall
  - Pre-trained only: No training data required, but low completeness (142 vs 590 ground-truth terms)
- Failure signatures:
  - Pre-trained: Hallucinates irrelevant terms (e.g., "rubber" in casting context); low recall
  - ICL: Relation name inconsistency ("processed by" vs "produced by" vs "used in"); poor synonym detection (1/22 pairs)
  - Fine-tuning: Relation name substitution when training data lacks specific relation types; omission of high-frequency generic terms
- First 3 experiments:
  1. Replicate term extraction with ICL (k=8 vs k=16) on your domain texts to validate sensitivity to example count before committing to fine-tuning.
  2. Test synonym detection separately: create a held-out synonym pairs test set and evaluate whether fine-tuning or ICL performs better for your domain's terminology variance.
  3. Pilot fine-tuning hyperparameters: train with epochs=2 vs epochs=3 and LR multiplier 1.0 vs 2.0 on 50 samples to identify overfitting threshold for your data scale.

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset size (173 training examples, 30 test examples) limits generalizability across manufacturing domains
- Evaluation relies on expert validation for a subset of relations, introducing potential subjectivity in ground truth assessment
- No comparison against state-of-the-art specialized ontology extraction systems to validate LLM optimality
- Results based on single domain application (casting) may not extend to domains with richer semantic relationships

## Confidence
**High Confidence**: The comparative performance ranking (fine-tuning > ICL > pre-trained) is well-supported by experimental results with specific precision and recall values directly measured from test set.

**Medium Confidence**: The mechanism explaining why fine-tuning achieves better relation naming consistency is theoretically sound but could benefit from additional ablation studies.

**Low Confidence**: The scalability claims for ontology construction (273 concepts, 620 relations) are based on a single domain application and may not extend to more complex domains.

## Next Checks
1. Apply the fine-tuned model to a different manufacturing domain (e.g., machining or welding) using the same training methodology to verify whether the 93.8% precision performance holds across domains.

2. Measure the actual computational cost difference between ICL (k=16) and fine-tuning (3 epochs) in terms of both training/inference time and monetary expense, then calculate the break-even point where fine-tuning's consistency advantages justify its higher upfront cost.

3. After ontology deployment in Neo4j, track precision degradation over time as new data is added through the same extraction pipeline, and measure whether relation naming inconsistencies re-emerge as the ontology scales beyond the original 620 relations.