---
ver: rpa2
title: 'Large Language Models for Mental Health: A Multilingual Evaluation'
arxiv_id: '2602.02440'
source_url: https://arxiv.org/abs/2602.02440
tags:
- datasets
- health
- mental
- performance
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of large language models
  (LLMs) in detecting mental health issues across eight multilingual datasets. The
  research compares proprietary and open-source LLMs in zero-shot, few-shot, and fine-tuned
  settings against traditional NLP baselines.
---

# Large Language Models for Mental Health: A Multilingual Evaluation

## Quick Facts
- arXiv ID: 2602.02440
- Source URL: https://arxiv.org/abs/2602.02440
- Reference count: 9
- One-line result: Chain-of-thought prompting with emotion infusion enables LLMs to match or exceed state-of-the-art mental health classification across 8 multilingual datasets.

## Executive Summary
This study evaluates large language models for mental health detection across eight multilingual datasets covering depression and suicidal ideation. The research compares proprietary (GPT-4, Claude, Gemini) and open-source (LLaMA, Gemma2, Ministral, R1) models using zero-shot, few-shot, and chain-of-thought emotion-infused prompting strategies. Results show that CoT Emo prompting consistently achieves competitive F1 scores, often surpassing traditional NLP baselines. Fine-tuning further improves open-source model performance, while machine-translated data generally underperforms original data, with degradation strongly correlated to translation quality metrics.

## Method Summary
The study evaluates 7 LLMs across 8 datasets in 6 languages using three prompting strategies: zero-shot, 5-shot, and chain-of-thought emotion-infused (CoT Emo) prompting. Open-source models undergo instruction fine-tuning with specified hyperparameters (batch_size=8, grad_accum=4, epochs=3, lr=5e-5, AdamW 8-bit, cosine scheduler, BF16). Machine translation uses facebook/nllb-200-3.3B for English pivot translation plus back-translation. Performance is measured via macro F1 score, with translation quality assessed using LaBSE, BERTScore, and BLEU metrics.

## Key Results
- CoT Emo prompting improves F1 scores by 0.09-0.12 on Russian and Spanish datasets compared to zero-shot
- Fine-tuning open-source models increases F1 scores by 0.10-0.20 across all evaluated datasets
- Translation quality strongly correlates with MT performance, with highest quality (Portuguese) showing <10% degradation and lowest quality (Spanish) showing >20% degradation
- Open-source models after fine-tuning approach or match proprietary model performance on original data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought prompting with emotion infusion improves mental health classification across multilingual datasets.
- Mechanism: CoT Emo prompting encourages LLMs to attend to affective cues while guiding step-by-step reasoning. This structured approach extracts nuanced signals from text that zero-shot or few-shot methods miss, particularly in sensitive domains where emotional content carries diagnostic weight.
- Core assumption: LLMs possess latent capacity to recognize mental health signals; prompting unlocks rather than creates this capability.
- Evidence anchors:
  - [abstract]: "LLMs, particularly with chain-of-thought prompting, achieve competitive F1 scores and often surpass state-of-the-art methods."
  - [section 4.1]: "GPT-4 increases its F1 score from 0.76 to 0.87 on the Russian dataset and from 0.75 to 0.84 on the Spanish dataset" using CoT.
  - [corpus]: Zahran et al. (2025) corroborates LLM effectiveness for Arabic mental health tasks, though limited to single-language evaluation.
- Break condition: If emotional vocabulary differs fundamentally across cultures (e.g., somatic expression of depression in some languages), affective cue attention may misfire without culturally-grounded emotion lexicons.

### Mechanism 2
- Claim: Translation quality directly modulates LLM performance on machine-translated mental health data.
- Mechanism: Higher semantic preservation (LaBSE, BERTScore) and lexical overlap (BLEU) between original and back-translated text maintain diagnostic signals. When translation introduces structural or lexical mismatches, subtle mental health indicators—hedging, pronoun shifts, temporal references—degrade or disappear.
- Core assumption: Mental health signals are somewhat robust to translation noise but not infinitely so; the relationship is monotonic but may have thresholds.
- Evidence anchors:
  - [abstract]: "High translation quality correlates with better LLM performance, emphasizing the need for robust translation methods."
  - [section 5.3]: Portuguese achieves highest translation quality metrics (LaBSE: 0.8624, BERTScore: 0.9168, BLEU: 69.85) and top-tier MT performance (GPT-4 CoT: 0.87). Spanish with lowest BLEU (6.25) and LaBSE (0.3562) shows CoT scores dropping to 0.77.
  - [corpus]: Skianis et al. (2024a,b) examine multilingual LLMs on translated datasets and reveal performance gaps across six languages, supporting the translation-quality link.
- Break condition: If translation smooths linguistic irregularities in ways that accidentally normalize disordered speech patterns, MT data could paradoxically improve performance for wrong reasons.

### Mechanism 3
- Claim: Instruction fine-tuning of open-source models narrows or closes the performance gap with proprietary LLMs for mental health classification.
- Mechanism: Domain-specific fine-tuning adapts model weights to recognize mental health language patterns, mitigating zero-shot variability. Open-source models gain task-specific priors that proprietary models may already possess through larger pre-training or undisclosed fine-tuning.
- Core assumption: Open-source model architectures have sufficient capacity; the gap is primarily data/alignment, not fundamental capability.
- Evidence anchors:
  - [section 4.4]: "Instruction fine-tuning markedly improves the performance of open-source LLMs, as evidenced by substantial increases in F1 scores across all evaluated datasets."
  - [section 4.4, Table 6]: R1 on Arabic (Helmy) improves from 0.84 (zero-shot) to 0.93 (fine-tuned). Gemma2 on Arabic (Baghdadi) improves from 0.73 to 0.88.
  - [corpus]: Weak direct corpus evidence on fine-tuning specifically for multilingual mental health; Zahran (2025) focuses on evaluation rather than adaptation.
- Break condition: If fine-tuning data contains annotation noise (several datasets lack expert labeling—Table 1 notes "No" for Boonyarat, Helmy), models may overfit to spurious signals rather than true mental health indicators.

## Foundational Learning

- Concept: **Zero-shot vs. Few-shot vs. Chain-of-Thought Prompting**
  - Why needed here: The paper's RQ2 directly compares these strategies; understanding their tradeoffs is prerequisite to reproducing results or selecting deployment strategies.
  - Quick check question: Given a new mental health dataset in a low-resource language, when would you choose few-shot over zero-shot, and what minimum example count would you need before diminishing returns?

- Concept: **Translation Quality Metrics (BLEU, LaBSE, BERTScore)**
  - Why needed here: RQ4 examines how translation quality affects LLM performance; these metrics are the independent variables. BLEU captures n-gram overlap (surface), BERTScore captures contextual similarity, LaBSE captures cross-lingual semantic embeddings.
  - Quick check question: Why might BLEU underperform for analytic languages like Thai or templatic languages like Arabic, and which metric would you trust more for semantic preservation in those cases?

- Concept: **Language Typology (Fusional, Analytic, Templatic)**
  - Why needed here: Section 5.2 links typology to translation quality variance. Fusional languages (Russian, Portuguese) fuse grammatical features; analytic (Thai) use particles; templatic (Arabic) use root-pattern systems.
  - Quick check question: A new mental health dataset arrives in a fusional language not in the training set. Based on the paper's findings, would you expect MT-based approaches to work better or worse than for an analytic language? Why?

## Architecture Onboarding

- Component map:
  Data layer (8 datasets, 6 languages) -> Translation layer (nllb-200-3.3B) -> Model layer (7 LLMs) -> Prompting layer (zero-shot -> few-shot -> CoT Emo) -> Fine-tuning layer (open-source only)

- Critical path:
  1. Start with zero-shot evaluation on original datasets to establish baseline.
  2. Apply CoT Emo prompting to extract maximum proprietary model performance.
  3. If deploying open-source: fine-tune on original data using hyperparameters from Table 5.
  4. For multilingual coverage: evaluate translation quality (LaBSE + BERTScore) before relying on MT data; expect 5-15% F1 degradation depending on typology.

- Design tradeoffs:
  - **Proprietary vs. Open-source**: Proprietary offers best zero-shot/CoT performance but no fine-tuning and higher inference cost. Open-source enables fine-tuning and data privacy but requires GPU infrastructure.
  - **Original vs. MT data**: Original data yields 5-15% higher F1 but requires language-specific collection. MT enables rapid expansion but quality varies sharply by language pair.
  - **Prompting complexity**: CoT consistently wins but increases token usage 2-4x; few-shot adds examples but may not help in all languages (Bengali showed few-shot ≈ CoT).

- Failure signatures:
  - **Translation collapse**: BLEU < 10 combined with LaBSE < 0.5 indicates semantic drift—do not use MT data (see Spanish results).
  - **Fine-tuning degradation**: If fine-tuned model underperforms zero-shot on held-out set, check for label noise (datasets without expert annotation are suspect).
  - **Typology mismatch**: Analytic/templatic languages showing >20% gap between original and MT indicate structural translation issues requiring human validation.

- First 3 experiments:
  1. **Baseline reproduction**: Run GPT-4 and Claude 3.5 zero-shot + CoT on 2 datasets (Russian depression, Portuguese suicide). Verify F1 within ±0.03 of reported values. If not, check prompt formatting and temperature settings.
  2. **Translation sensitivity test**: Take the Portuguese dataset (highest translation quality) and Thai dataset (analytic, moderate quality). Compare original vs. MT performance. Confirm Portuguese shows <10% degradation while Thai shows >10%. This validates your translation pipeline.
  3. **Open-source fine-tuning pilot**: Fine-tune Gemma2 27B on Arabic (Baghdadi) using Table 5 hyperparameters. Target: F1 ≥ 0.85 on test split. If achieved, the fine-tuning infrastructure is validated for production use.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does continual pretraining or synthetic fine-tuning enable open-source LLMs to outperform proprietary models on low-resource mental health languages?
- Basis in paper: [explicit] Future work section states plans to "adapt open-source models to the domain using approaches such as continual pretraining and synthetic fine-tuning."
- Why unresolved: The current study only evaluates zero-shot, few-shot, and standard fine-tuning, leaving advanced domain adaptation techniques for open-source models unexplored.
- What evidence would resolve it: A performance comparison of LLaMA or Gemma models after continual pretraining on mental health corpora against baseline proprietary model scores.

### Open Question 2
- Question: Can translation metrics that specifically capture emotional nuance predict LLM performance on mental health tasks better than standard semantic metrics?
- Basis in paper: [inferred] The Limitations section notes that standard automatic metrics "may not capture all subtle distortions" or "subtle emotional nuances" lost during machine translation.
- Why unresolved: The study correlates performance with LaBSE and BLEU, but these general metrics may not fully account for the loss of affective cues critical to mental health detection.
- What evidence would resolve it: A study correlating LLM F1 scores on MT data with emotion-aware translation metrics versus standard semantic similarity scores.

### Open Question 3
- Question: To what extent does the linguistic typology of a language (e.g., fusional vs. analytic) versus its cultural context drive performance degradation in machine-translated data?
- Basis in paper: [inferred] The paper links performance drops to typology (Section 5.2), but the Introduction notes that models "fail for cross-cultural contexts," suggesting performance variations may be confounded by cultural factors.
- Why unresolved: The current evaluation attributes performance variance largely to translation quality and typology without isolating the specific impact of cross-cultural expression differences.
- What evidence would resolve it: An ablation study comparing LLM performance on culturally adapted translations versus literal translations within the same language family.

## Limitations
- Datasets vary significantly in size and annotation quality, with Spanish containing only 1,000 samples and several lacking expert annotation verification
- Translation quality assessment relies on automatic metrics rather than human evaluation, potentially missing subtle semantic distortions
- English pivot translation may compound errors through double translation
- Fine-tuning experiments limited to open-source models, leaving proprietary model adaptation unexplored
- Evaluation assumes binary classification, while clinical mental health assessment often involves multi-class or severity-level distinctions

## Confidence
- **High Confidence**: CoT prompting consistently improves performance across languages and datasets; translation quality correlation with MT performance is robustly demonstrated
- **Medium Confidence**: Open-source models can match proprietary performance after fine-tuning, but limited by small sample size and lack of direct comparison on identical fine-tuned versions
- **Low Confidence**: Performance projections for languages outside the study or clinical-grade datasets remain speculative

## Next Checks
1. **Human Translation Quality Validation**: Recruit bilingual mental health professionals to evaluate 100 randomly selected translation pairs from the MT datasets, rating semantic preservation on a 5-point scale. Compare human ratings against LaBSE/BERTScore to calibrate metric reliability.

2. **Cross-Dataset Generalization Test**: Apply the best-performing LLM (GPT-4 CoT) to a clinically-validated mental health dataset from a language in this study (e.g., Portuguese) and compare performance to social media data results. This validates clinical applicability beyond social media contexts.

3. **Typology Stress Test**: Identify two additional languages representing underrepresented typologies (e.g., a tonal language and a polysynthetic language) and evaluate MT performance against predicted patterns based on this study's findings. This would test whether the typology-performance relationship holds beyond the current sample.