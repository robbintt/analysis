---
ver: rpa2
title: 'Modular Diffusion Policy Training: Decoupling and Recombining Guidance and
  Diffusion for Offline RL'
arxiv_id: '2506.03154'
source_url: https://arxiv.org/abs/2506.03154
tags:
- guidance
- diffusion
- training
- learning
- module
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modular Diffusion Policy Training addresses the challenge of unstable
  and inefficient joint training in diffusion-based offline RL by decoupling the guidance
  module from the diffusion model. The core idea is to first train the guidance module
  as a value estimator using supervised learning on offline data, then freeze it to
  guide the diffusion model during training.
---

# Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL

## Quick Facts
- **arXiv ID:** 2506.03154
- **Source URL:** https://arxiv.org/abs/2506.03154
- **Reference count:** 8
- **Primary result:** Decoupling guidance from diffusion training improves sample efficiency and reduces variance by 85%+ in offline RL

## Executive Summary
Modular Diffusion Policy Training addresses the instability of joint training in diffusion-based offline RL by separating the guidance module (value estimator) from the diffusion model. The approach first trains the guidance module independently using supervised learning on offline data, then freezes it to guide the diffusion model during training. This decoupling reduces memory usage, accelerates convergence, and enables modular composition of guidance and diffusion components. Experiments on bullet-D4RL benchmarks demonstrate significant improvements in both sample efficiency and final performance.

## Method Summary
The method decouples the training of the guidance module (Q-function) from the diffusion model by first pretraining the guidance independently as a value estimator using supervised TD learning on the offline dataset. Once converged, the guidance module is frozen and used to provide stable gradient signals during diffusion model training. The diffusion model is trained using a combination of behavior cloning and reward-guided losses, with the frozen guidance module providing the reward gradients. This approach eliminates the unstable early training phase where jointly-trained unconverged guidance can misguide learning.

## Key Results
- **9.54% peak reward gain and 4.09% AUC gain** on HalfCheetah-Medium tasks compared to joint training baselines
- **Over 85% variance reduction** by using different guidance modules during training and inference
- **Cross-algorithm transferability** allows guidance modules from one algorithm (e.g., IDQL) to be reused with another (e.g., DQL) without additional training

## Why This Works (Mechanism)

### Mechanism 1
Decoupling guidance training from diffusion model training improves stability and efficiency. The guidance module is trained independently as a value estimator using supervised learning on offline data, then frozen to provide stable gradient signals. This works because the guidance module's training data is independent of the policy's behavior, allowing separate optimization without reducing exploration capability.

### Mechanism 2
Frozen pretrained guidance accelerates diffusion model convergence and improves peak performance. A converged guidance module provides accurate, stable gradient directions from the start of diffusion training, avoiding the "misguidance" phase where jointly-trained unconverged guidance degrades learning. The gradient alignment condition (cos θ_t > 0) ensures directional consistency with reward maximization.

### Mechanism 3
Independently trained guidance modules can be swapped across diffusion models, reducing variance and enabling modular composition. Guidance modules trained with different algorithms provide compatible reward estimation signals as long as they estimate expected rewards with sufficient accuracy. Double Guidance (using a differently-seeded guidance at inference) breaks self-reinforcing bias loops.

## Foundational Learning

- **Concept: Classifier-Free Guidance (CFG) in Diffusion**
  - **Why needed here:** The method builds on CFG, which biases diffusion sampling toward high-reward trajectories by conditioning on reward signals.
  - **Quick check question:** Can you explain how CFG interpolates between unconditional and reward-conditioned outputs during sampling?

- **Concept: Temporal-Difference (TD) Q-Learning**
  - **Why needed here:** The guidance module is trained as a Q-function estimator using TD learning. Understanding how Q-values are updated from rewards is essential to grasp why guidance can be pretrained separately.
  - **Quick check question:** How does TD learning estimate expected return from (s, a, r, s') tuples in offline datasets?

- **Concept: Diffusion Policy Parameterization**
  - **Why needed here:** The diffusion model represents the policy as a conditional denoising process. The loss functions and sampling process are central to understanding what guidance modifies.
  - **Quick check question:** How does a denoising network ε_θ(a_t, t, s) recover a clean action a_0 from noisy samples?

## Architecture Onboarding

- **Component map:**
  - Offline Dataset D -> Guidance Module Q_ϕ (pretrained TD learning) -> Frozen Guidance Module -> Diffusion Policy ε_θ (training with guidance) -> Policy π_θ
  - Optional: Frozen Guidance Module Q_ϕ' (different seed) -> Diffusion Policy ε_θ (inference-only)

- **Critical path:**
  1. Pretrain guidance module Q_ϕ on D until convergence (supervised TD learning)
  2. Freeze Q_ϕ, begin diffusion training ε_θ with Q_ϕ gradients
  3. Evaluate; optionally swap in double guidance Q_ϕ' at inference for variance reduction

- **Design tradeoffs:**
  - **Memory vs. Simplicity:** GFDT reduces peak memory but requires managing separate training runs
  - **Speed vs. Compatibility:** Pretrained guidance accelerates convergence but may underperform on Medium-Replay data
  - **Modularity vs. Performance Ceiling:** Plug-and-play works but may not exceed the best baseline's performance

- **Failure signatures:**
  - Guidance overfitting if not properly frozen
  - Noisy/misaligned guidance degrading performance
  - Data mismatch causing degradation on Medium-Replay environments

- **First 3 experiments:**
  1. Replicate ablation comparing no guidance, baseline joint guidance, and pretrained frozen guidance on HalfCheetah-Expert
  2. Test double guidance variance reduction by using independently trained Q-networks
  3. Demonstrate plug-and-play cross-algorithm by loading IDQL guidance into DQL diffusion model

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the modular GFDT pipeline support plug-and-play reward shaping to adapt pretrained diffusion policies to new downstream tasks without retraining the core generative model? (Section 6 mentions this direction for future exploration)

- **Open Question 2:** Would integrating monotonically increasing guidance loss schedules into GFDT further stabilize training and improve performance in high-variance offline RL scenarios? (Section 6 cites Wang et al. 2024 on this possibility)

- **Open Question 3:** Why does Guidance-First approach fail to improve trajectory-based diffusion models like Diffuser, and how can the method be adapted to handle noisy long-horizon return attribution? (Section 4.1 notes little improvement on trajectory-based methods)

- **Open Question 4:** What specific data distribution characteristics of Medium-Replay datasets cause GFDT performance degradation, and can simple hyperparameter tuning resolve this? (Table 2 omits Medium-Replay results due to significant degradation)

## Limitations
- Performance degradation on Medium-Replay datasets indicates sensitivity to offline dataset quality
- Cross-algorithm transferability works in demonstrated cases but generalizability to arbitrary guidance-diffusion combinations needs validation
- Results limited to PyBullet D4RL MuJoCo tasks; scalability to higher-dimensional continuous control remains untested

## Confidence

- **High Confidence:** Decoupling mechanism and computational efficiency gains are well-supported by ablation studies
- **Medium Confidence:** Variance reduction claims through double guidance are supported but may depend on guidance quality
- **Medium Confidence:** Cross-algorithm transferability works in demonstrated cases but generalizability needs further validation

## Next Checks

1. **Dataset Quality Analysis:** Systematically evaluate GFDT performance across datasets with varying quality metrics to characterize failure boundaries
2. **Guidance Architecture Compatibility:** Test guidance module transferability across diverse architectures to establish compatibility limits
3. **Scaling Validation:** Apply GFDT to higher-dimensional continuous control tasks to assess scalability and identify bottlenecks