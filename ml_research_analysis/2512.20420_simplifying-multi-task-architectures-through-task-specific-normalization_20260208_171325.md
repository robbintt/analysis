---
ver: rpa2
title: Simplifying Multi-Task Architectures Through Task-Specific Normalization
arxiv_id: '2512.20420'
source_url: https://arxiv.org/abs/2512.20420
tags:
- task
- learning
- tasks
- multi-task
- normalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a minimalist soft-sharing approach for multi-task\
  \ learning, where task-specific Batch Normalization (TSBN) layers replace shared\
  \ normalization, enabling effective task disentanglement with negligible parameter\
  \ overhead. Building on this, the authors propose Task-Specific Sigmoid Batch Normalization\
  \ (TS\u03C3BN), which improves stability and performance by incorporating sigmoid\
  \ activation and higher learning rates for the scaling parameters."
---

# Simplifying Multi-Task Architectures Through Task-Specific Normalization

## Quick Facts
- **arXiv ID:** 2512.20420
- **Source URL:** https://arxiv.org/abs/2512.20420
- **Reference count:** 40
- **Primary result:** TSσBN achieves state-of-the-art performance on NYUv2, Cityscapes, CelebA, and PascalContext with negligible parameter overhead compared to complex MTL designs

## Executive Summary
This paper introduces a minimalist soft-sharing approach for multi-task learning by replacing shared normalization layers with task-specific variants. The key innovation is Task-Specific Sigmoid Batch Normalization (TSσBN), which incorporates sigmoid activation on scaling parameters and higher learning rates to enable stable, interpretable capacity allocation across tasks. TSσBN demonstrates that simple architectural changes can outperform elaborate MTL designs, achieving state-of-the-art results while maintaining superior parameter efficiency and providing interpretable insights into task relationships.

## Method Summary
The method replaces all shared Batch Normalization (BN) or Layer Normalization (LN) layers in a multi-task network with task-specific variants. Each task receives dedicated γ and β parameters in normalization layers, enabling independent modulation of shared features. The TSσBN variant adds sigmoid activation (σ(γ) = 1/(1+e^-γ)) to bound scaling parameters between 0 and 1, providing stability at high learning rates. Parameters are initialized differently for pretrained versus random-initialized models, with discriminative learning rates (100× base) applied to normalization parameters to accelerate early specialization.

## Key Results
- TSσBN achieves +8.00% average gain over hard sharing on NYUv2, +6.90% on Cityscapes, and +3.47% on CelebA
- Maintains superior parameter efficiency compared to complex routing schemes or task-specific modules
- Provides interpretable capacity allocation through learned sigmoid parameters
- Shows stable training even at high learning rates where vanilla task-specific BN collapses

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Affine Transformation as Soft Capacity Allocation
Task-specific BN layers allow each task to independently modulate shared features, reducing interference without duplicating feature extractors. Each task receives dedicated γ and β parameters in normalization layers, enabling tasks to up-weight or down-weight specific filters post-normalization, creating implicit task-specific "views" of the shared representation while preserving the MTL inductive bias. The core assumption is that tasks benefit from shared representations but require different emphases on which features matter for each task.

### Mechanism 2: Sigmoid Bounding for Stable, Interpretable Gating
The sigmoid reparameterization provides gradient dampening and bounded outputs [0,1], enabling stable training with high learning rates while yielding interpretable "importance weights." σ(γ) = 1/(1+e^-γ) maps unbounded learnable parameters to a bounded range, preventing extreme scaling factors, regularizing toward selective filter usage, and naturally interpreting as task-filter importance. The core assumption is that bounded gating values are preferable to unbounded scaling for stability and interpretability in MTL.

### Mechanism 3: Discriminative Learning Rates for Early Specialization
Higher learning rates (100× base) for σBN parameters enable rapid early capacity allocation before shared weights update significantly. Fast σBN adaptation allows tasks to "claim" filters early in training, with sigmoid dampening preventing gradient explosion. This reduces conflicting gradient updates throughout training, as evidenced by more orthogonal gradient distributions. The core assumption is that capacity allocation should occur early and then stabilize.

## Foundational Learning

- **Concept: Batch Normalization: Normalization vs. Affine Transformation**
  - Why needed: This paper exploits only the affine part (γ, β) while keeping normalization statistics shared
  - Quick check: Why can normalization statistics (μ_B, σ²_B) remain shared across tasks while only γ, β become task-specific?

- **Concept: Multi-Task Learning Interference (Gradient Conflict)**
  - Why needed: The core problem being solved - conflicting gradients occur when task objectives push shared parameters in opposite directions
  - Quick check: What does a cosine similarity of -0.8 between two task gradients indicate about their relationship?

- **Concept: Soft vs. Hard Parameter Sharing**
  - Why needed: TSσBN is a minimal form of soft sharing; knowing this spectrum helps understand what complexity is being replaced
  - Quick check: How does parameter efficiency of TSσBN compare to Cross-Stitch Networks when scaling from 3 to 10 tasks?

## Architecture Onboarding

- **Component map:** Shared backbone (Conv or Transformer blocks, fully shared) -> Task-specific σBN layers (replace each shared BN/LN with T copies) -> Task-specific γ^t,i parameters (one per task per filter per layer) -> Task-specific heads (unchanged from standard MTL setup)

- **Critical path:**
  1. Identify all normalization layers in the backbone
  2. For each BN/LN, create T task-specific copies with sigmoid-bounded scaling
  3. Initialize: for pretrained models, apply inverse sigmoid to existing γ values; from scratch, initialize γ=0 (σ(γ)=0.5)
  4. Configure optimizer with separate parameter groups: 100× LR for σBN, 10× LR for task-specific LN
  5. During forward pass, route each task batch through its corresponding σBN branch

- **Design tradeoffs:**
  - LR multiplier 100 vs. 1000: Higher → faster specialization but risks binary hard-partitioning; sigmoid prevents collapse
  - BN vs. LN conversion: Transformers with pretrained scales outside sigmoid range require careful initialization
  - Adding shared LoRA adapters: Paper shows this can improve performance further

- **Failure signatures:**
  - All σ(γ) values stuck near 0.5: LR multiplier too low; tasks remain undifferentiated
  - σ(γ) collapsed to 0 or 1 only: LR too high (α=10^3); becomes hard partitioning
  - One task's capacity >> others: Check for loss scale imbalance
  - Performance below hard sharing baseline: Verify per-task BN statistics are correctly computed

- **First 3 experiments:**
  1. TSBN baseline on NYUv2 SegNet: Replace shared BN with task-specific BN (no sigmoid, default LR)
  2. Learning rate sweep: Test α_σBN ∈ {1, 10, 100, 1000} on a 2-3 task setup
  3. Specialization analysis: After training, extract σ(γ) distributions per layer

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does TSσBN transfer effectively to non-vision domains (NLP, audio, multimodal) where LayerNorm dominates over BatchNorm?
- **Basis:** All experiments are limited to vision tasks using CNNs and Vision Transformers
- **Why unresolved:** The theoretical justification relies on batch statistics, which may behave differently across modalities
- **What evidence would resolve it:** Systematic evaluation on standard NLP multi-task benchmarks and multimodal settings

### Open Question 2
- **Question:** Can TSσBN be combined with optimization-based MTL methods for further gains, or do they address orthogonal problems?
- **Basis:** Appendix G briefly compares TSσBN against optimization methods but states they are "orthogonal" without exploring combinations
- **Why unresolved:** The paper demonstrates TSσBN outperforms these methods individually, but combinations remain untested
- **What evidence would resolve it:** Ablation experiments combining TSσBN with representative optimization methods

### Open Question 3
- **Question:** What is the theoretical mechanism by which task-specific BN affine parameters induce gradient orthogonality?
- **Basis:** The paper empirically shows TSσBN yields orthogonal gradients but states this "mirrors optimization-based methods" without explaining why
- **Why unresolved:** The connection between per-task scaling parameters and gradient direction is intuitive but not formally derived
- **What evidence would resolve it:** Theoretical analysis relating the Jacobian of BN scaling parameters to gradient orthogonality

## Limitations
- Generalization to extreme task heterogeneity (e.g., language + vision) is unproven
- Interaction with optimization methods beyond Adam not tested
- Pretrained model initialization assumes BN statistics are transferable
- Parameter efficiency gains scale sublinearly with task count

## Confidence
- **High confidence:** TSσBN outperforms hard sharing and vanilla soft sharing on standard MTL benchmarks; parameter efficiency claims are directly measurable
- **Medium confidence:** Claims about early specialization and stable high-LR training; mechanistic explanation of gradient interference reduction needs more ablation studies
- **Low confidence:** Interpretability claims (filter importance weights); no ablation showing whether sigmoid bounds or just task-specificity drive performance

## Next Checks
1. **Ablation of sigmoid activation:** Replace σBN with vanilla task-specific BN at 100× LR to isolate the effect of sigmoid bounding versus task-specific scaling alone
2. **Cross-dataset robustness test:** Train TSσBN on Cityscapes→NYUv2 transfer setup to verify performance when backbone was pretrained on different task distribution
3. **Multi-task scaling study:** Benchmark TSσBN vs Cross-Stitch Networks on 10+ task configurations to quantify where parameter efficiency advantages break down