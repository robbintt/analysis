---
ver: rpa2
title: Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings
  with Provable Coverage Guarantees
arxiv_id: '2508.05544'
source_url: https://arxiv.org/abs/2508.05544
tags:
- b-instruct
- qwen2
- llama-3
- uncertainty
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reliable uncertainty quantification
  in multiple-choice question answering using Large Language Models (LLMs) under black-box
  settings, where internal logits are inaccessible. The authors propose a frequency-based
  predictive entropy method that leverages multiple independent samplings of the model's
  output distribution, using the most frequent sample as a reference to quantify uncertainty.
---

# Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees

## Quick Facts
- **arXiv ID**: 2508.05544
- **Source URL**: https://arxiv.org/abs/2508.05544
- **Reference count**: 22
- **Primary result**: Frequency-based predictive entropy in black-box LLMs achieves AUROC values comparable to or better than logit-based approaches while providing provable coverage guarantees via conformal prediction.

## Executive Summary
This paper addresses the challenge of reliable uncertainty quantification in multiple-choice question answering using Large Language Models (LLMs) under black-box settings, where internal logits are inaccessible. The authors propose a frequency-based predictive entropy method that leverages multiple independent samplings of the model's output distribution, using the most frequent sample as a reference to quantify uncertainty. This approach replaces traditional logit-based uncertainty measures and is combined with conformal prediction to provide provable coverage guarantees.

Experiments across six LLMs and four datasets (MedMCQA, MedQA, MMLU, MMLU-Pro) demonstrate that the frequency-based method achieves AUROC values comparable to or better than logit-based approaches (e.g., 1.2% higher average AUROC on MMLU, 0.7% on MEDMCQA). The method effectively controls empirical miscoverage rates under user-specified risk levels, validating sampling frequency as a viable substitute for logit-based probabilities in black-box scenarios. The approach provides a distribution-free, model-agnostic framework for uncertainty quantification with guaranteed coverage, enhancing LLM trustworthiness in high-risk applications.

## Method Summary
The method replaces logit-based uncertainty quantification with frequency-based predictive entropy in black-box LLM settings. It samples M=20 outputs per input at temperature=1.0, computes answer frequencies, and calculates predictive entropy from these frequencies. The approach uses conformal prediction by computing nonconformity scores from frequency-based estimates and constructing prediction sets via quantile thresholds. The framework splits data 50/50 for calibration and testing, providing coverage guarantees without requiring access to model internals.

## Key Results
- Frequency-based predictive entropy achieves 1.2% higher average AUROC on MMLU compared to logit-based methods
- Empirical Miscoverage Rates (EMR) are effectively controlled at user-specified risk levels (α=0.1 to 0.9) across all datasets
- The method shows 0.7% AUROC improvement on MEDMCQA while maintaining comparable performance on other benchmarks

## Why This Works (Mechanism)

## Foundational Learning

**Conformal Prediction**: Distribution-free uncertainty quantification method that provides provable coverage guarantees without distributional assumptions. Needed for black-box settings where traditional uncertainty measures are inaccessible. Quick check: Verify exchangeability assumption holds across samples.

**Predictive Entropy**: Information-theoretic measure of uncertainty calculated from probability distributions. In this work, frequency-based PE substitutes for logit-based PE. Quick check: Compare PE values across different sampling temperatures.

**Nonconformity Scores**: Measures of how different a new example is from calibration data. Here computed from frequency-based estimates. Quick check: Examine score distributions for correct vs incorrect predictions.

**Exchangeability**: Statistical assumption that samples are identically distributed and independent. Critical for conformal validity. Quick check: Test for autocorrelation between consecutive samples.

## Architecture Onboarding

**Component Map**: Question -> Sampling (M=20) -> Frequency Distribution -> Predictive Entropy -> Nonconformity Scores -> Quantile Threshold -> Prediction Set

**Critical Path**: The core computational path involves sampling generation, frequency calculation, entropy computation, and conformal set construction. The bottleneck is the M sampling steps required for each input.

**Design Tradeoffs**: Sampling-based approach trades computational efficiency (20x inference cost) for black-box compatibility and provable coverage guarantees. Temperature=1.0 balances diversity with stability.

**Failure Signatures**: High variance in PE_freq with M=20 indicates model uncertainty; EMR exceeding α suggests exchangeability violations or insufficient calibration samples; APSS inflation indicates conservative coverage.

**Three First Experiments**:
1. Generate 20 samples at temp=1.0 for MMLU validation questions using Qwen2.5-3B-Instruct
2. Compute frequency-based PE and compare with logit-based PE on same data
3. Split 50/50, compute nonconformity scores on calibration set, determine quantile threshold at α=0.1, evaluate EMR on test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does varying the sampling temperature affect the stability of frequency-based predictive entropy and the resulting conformal set sizes?
- Basis in paper: [inferred] The authors explicitly fix the sampling temperature at 1.0 "to ensure diverse outputs" but do not conduct ablation studies to determine if this setting is optimal or if results hold at lower temperatures.
- Why unresolved: It is unclear if the superior performance of frequency-based PE is robust to changes in sampling hyperparameters or if it relies heavily on high-temperature stochasticity.
- What evidence would resolve it: Ablation experiments reporting AUROC and Empirical Miscoverage Rate (EMR) across a range of temperatures (e.g., 0.0 to 1.5).

### Open Question 2
- Question: Can the number of required samples ($M$) be reduced below 20 without compromising the finite-sample coverage guarantees?
- Basis in paper: [inferred] The authors use $M=20$ independent samples to calculate frequency, which incurs a 20x increase in inference cost compared to single-pass methods.
- Why unresolved: The trade-off between computational efficiency and the statistical validity of the frequency-based proxy is not quantified for smaller sample sizes.
- What evidence would resolve it: Experiments evaluating the degradation of AUROC and the violation of miscoverage rates as $M$ is systematically reduced (e.g., $M \in [5, 10, 15]$).

### Open Question 3
- Question: Does the coverage guarantee hold when the exchangeability assumption is violated by distribution shift between calibration and test data?
- Basis in paper: [inferred] While the paper notes CP relies on "exchangeable data theory," the experiments split individual datasets (i.i.d. setting) and do not test cross-domain generalization (e.g., calibrating on MMLU and testing on MedMCQA).
- Why unresolved: The method's reliability in high-risk domains depends on its robustness when the test distribution diverges from the calibration set.
- What evidence would resolve it: Analysis of Empirical Miscoverage Rates (EMR) in non-i.i.d. scenarios where the calibration and test sets are drawn from different subject domains or data distributions.

## Limitations
- Computational overhead from requiring 20 sampling steps per input, increasing inference cost 20-fold
- Exchangeability assumption may be violated in sequential sampling, potentially affecting coverage guarantees
- Performance depends on model's ability to generate diverse outputs, which may vary across architectures

## Confidence

**High confidence**: Core claim that frequency-based PE can substitute for logit-based uncertainty in black-box settings is supported by consistent AUROC improvements across multiple datasets and LLMs.

**Medium confidence**: Generalizability across different model sizes and architectures, given evaluation focused on 1B-13B parameter models only.

**Low confidence**: Practical efficiency for real-time applications due to 20x computational overhead and lack of runtime optimization discussion.

## Next Checks

1. **Exchangeability verification**: Conduct statistical tests for exchangeability across M samples on calibration data to formally validate the i.i.d. assumption underlying conformal guarantees.

2. **Scaling analysis**: Evaluate frequency-based method on larger LLMs (70B+ parameters) with varying temperature settings (0.1-2.0) to determine robustness to model scale and sampling temperature.

3. **Multi-token extension**: Implement and test method with multi-token generation for MCQA tasks, comparing coverage and efficiency against single-token approach and evaluating performance on complex answer formats.