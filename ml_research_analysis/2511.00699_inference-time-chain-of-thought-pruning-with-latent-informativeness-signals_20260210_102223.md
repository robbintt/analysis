---
ver: rpa2
title: Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals
arxiv_id: '2511.00699'
source_url: https://arxiv.org/abs/2511.00699
tags:
- reasoning
- accuracy
- arxiv
- pruning
- branches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents KAPPA, an inference-time pruning method for
  chain-of-thought reasoning that uses KL divergence, confidence, and entropy signals
  to selectively truncate unpromising reasoning paths. Unlike existing methods that
  rely on consistency heuristics, KAPPA employs principled scoring to guide progressive
  pruning while maintaining diversity during exploration.
---

# Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals

## Quick Facts
- arXiv ID: 2511.00699
- Source URL: https://arxiv.org/abs/2511.00699
- Reference count: 40
- Key outcome: KAPPA achieves up to 60% reduction in peak memory and 90% reduction in total token generation compared to Best-of-N while maintaining reasoning accuracy

## Executive Summary
This paper introduces KAPPA, an inference-time pruning method for chain-of-thought reasoning that leverages latent informativeness signals to selectively truncate unpromising reasoning paths. Unlike existing methods that rely on consistency heuristics, KAPPA employs principled scoring based on KL divergence, confidence, and entropy to guide progressive pruning while maintaining diversity during exploration. The method demonstrates significant efficiency gains—up to 60% peak memory reduction and 90% token generation reduction—compared to Best-of-N, with minimal accuracy loss on mathematical reasoning benchmarks. Notably, KAPPA stabilizes performance in smaller models, achieving 72.2% accuracy on MATH500.

## Method Summary
KAPPA implements a progressive pruning strategy during inference that uses three latent informativeness signals to evaluate reasoning paths: KL divergence (measuring distributional shift), confidence scores (predicting solution likelihood), and entropy (capturing uncertainty). The method maintains a diverse set of candidate solutions while progressively eliminating unpromising paths based on these principled metrics. Unlike prior approaches that rely on heuristic consistency checks, KAPPA's scoring system directly evaluates the informativeness of intermediate reasoning steps. The pruning operates at inference time, allowing for real-time efficiency gains without requiring model retraining or architectural modifications.

## Key Results
- Achieves up to 60% reduction in peak memory and 90% reduction in total token generation compared to Best-of-N
- Maintains reasoning accuracy with minimal degradation on GSM8K and MATH500 benchmarks
- Stabilizes performance in smaller models (1.5B parameters), achieving 72.2% accuracy on MATH500
- Demonstrates effectiveness across both DeepSeek-R1-Distill-Qwen-1.5B and Qwen2.5-7B-Instruct model variants

## Why This Works (Mechanism)
KAPPA works by identifying and eliminating reasoning paths that are unlikely to lead to correct solutions early in the inference process. The method leverages the observation that many intermediate reasoning steps in chain-of-thought generation are redundant or lead to dead ends. By using principled metrics (KL divergence, confidence, and entropy) rather than heuristic consistency checks, KAPPA can more accurately identify which reasoning paths are truly unpromising. The progressive nature of the pruning allows the system to maintain diversity in the early stages while becoming increasingly selective as reasoning progresses, ensuring that correct but initially uncertain paths aren't prematurely eliminated.

## Foundational Learning
- KL divergence for distributional shift measurement: Needed to quantify how much a reasoning path deviates from promising patterns; Quick check: Verify KL divergence correlates with solution quality across different reasoning domains
- Confidence scoring in probabilistic models: Essential for estimating solution likelihood; Quick check: Compare confidence scores against actual solution accuracy across pruned and unpruned paths
- Entropy as uncertainty quantification: Required to identify reasoning steps with high uncertainty that may warrant additional exploration; Quick check: Test entropy thresholds for optimal pruning balance

## Architecture Onboarding
**Component map:** Input problems → Initial reasoning path generation → KAPPA scoring module (KL divergence + confidence + entropy) → Progressive pruning decisions → Output selected solution

**Critical path:** Problem → Reasoning path generation → Scoring evaluation → Pruning decision → Solution selection

**Design tradeoffs:** The method trades potential exploration of uncertain paths for computational efficiency, requiring careful calibration of pruning thresholds to avoid eliminating correct but initially uncertain solutions.

**Failure signatures:** Overly aggressive pruning leading to loss of correct solutions, particularly for complex problems requiring extended reasoning chains; insufficient pruning resulting in minimal efficiency gains.

**3 first experiments:**
1. Compare KL divergence, confidence, and entropy individually as pruning signals to determine their relative effectiveness
2. Vary pruning aggressiveness thresholds to identify optimal balance between efficiency and accuracy
3. Test pruning performance across reasoning chain lengths to identify at which stages pruning is most effective

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation limited to mathematical reasoning benchmarks (GSM8K, MATH500) and two model sizes (1.5B, 7B parameters)
- Potential sensitivity to hyperparameter settings for pruning thresholds not fully explored
- Possible systematic bias against problem types requiring extended reasoning chains that may be truncated early

## Confidence
- Efficiency improvements (memory and token reduction): High - well-supported by ablation studies and quantitative metrics
- Accuracy preservation with pruning: Medium - demonstrated on limited benchmarks; effectiveness may vary with problem complexity
- Stabilization of smaller model performance: Medium - promising result but based on single dataset and model combination

## Next Checks
1. Test KAPPA across diverse reasoning domains (e.g., commonsense QA, symbolic reasoning) to assess generalizability beyond mathematical problems
2. Conduct ablation studies varying the relative weights of KL divergence, confidence, and entropy signals to identify optimal pruning configurations
3. Evaluate long-term performance by comparing solutions generated with and without pruning on problems requiring extended reasoning chains (>20 steps) to detect potential truncation biases