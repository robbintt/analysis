---
ver: rpa2
title: A word association network methodology for evaluating implicit biases in LLMs
  compared to humans
arxiv_id: '2510.24488'
source_url: https://arxiv.org/abs/2510.24488
tags:
- biases
- llms
- bias
- humans
- implicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a novel word association network methodology
  for evaluating implicit biases in Large Language Models (LLMs) compared to humans.
  The method simulates semantic priming within LLM-generated word association networks
  to provide quantitative and qualitative bias assessments.
---

# A word association network methodology for evaluating implicit biases in LLMs compared to humans

## Quick Facts
- arXiv ID: 2510.24488
- Source URL: https://arxiv.org/abs/2510.24488
- Reference count: 40
- Key outcome: Introduces word association network methodology comparing implicit biases between humans and LLMs across gender, religion, ethnicity, sexual orientation, and political party

## Executive Summary
This paper introduces a novel methodology for evaluating implicit social biases in Large Language Models (LLMs) by simulating semantic priming within LLM-generated word association networks. The approach prompts LLMs to generate free associations mirroring human behavioral data, then applies spreading activation to quantify semantic relatedness between social categories and attributes. Applied to humans and three LLMs (Mistral, Llama3, Haiku), the methodology reveals both convergence and divergence in biases across gender, religion, ethnicity, sexual orientation, and political party. While humans displayed the strongest gender stereotypes, LLMs varied in intensity and direction, generally mirroring human patterns in valence biases but sometimes exhibiting opposite trends. The method offers a flexible, interpretable, and scalable tool for bias evaluation grounded in cognitive psychology and network science.

## Method Summary
The methodology consists of three main steps: (1) Build word association networks from free associations, filtering to WordNet nodes and converting directed edges to undirected while retaining maximum weights; (2) Apply spreading activation simulation using parameters based on network topology (time steps = 2×diameter, retention = 0.5) to calculate activation levels for target nodes; (3) Normalize activation matrices and compute statistical measures of bias through three approaches—stereotypes (Wilcoxon effect sizes on paired differences), valence (GLM coefficients), and emotions (Wilcoxon effect sizes for Plutchik emotions). The method uses SWOW human norms (~12,000 cues, ~3M responses) and LWOW LLM-generated norms, with target nodes defined for each social category.

## Key Results
- Humans displayed the strongest gender stereotypes across all LLMs tested
- LLMs generally mirrored human patterns in valence biases but showed opposite trends in some cases
- The methodology revealed both convergence and divergence in biases between humans and LLMs
- L1 vs. L2 normalization produced different results for political emotion analysis, with L2 yielding contradictory findings
- Network topology varied significantly between agents (e.g., Haiku avg degree 8 vs Humans 26)

## Why This Works (Mechanism)

### Mechanism 1: Free Association as a Proxy for Implicit Structure
- Claim: LLM-generated word associations elicited under behavioral constraints serve as comparable proxies for implicit semantic memory
- Core assumption: LLMs possess conceptual knowledge structure analogous to human semantic memory accessible via specific prompting
- Evidence: Prompts tap into implicit relational structures; free associations correlate with stable implicit attitudes
- Break condition: If outputs are purely stochastic without semantic grounding, human comparisons become invalid

### Mechanism 2: Simulated Spreading Activation Quantifies Association Strength
- Claim: Spreading activation within word association networks provides numerical proxy for semantic relatedness
- Core assumption: Computational parameters (retention rate 0.5, time steps = 2×diameter) approximate cognitive decay and retrieval
- Evidence: Method validated by correlating final activation levels with reaction times from LDT behavioral experiment
- Break condition: If network topology differs drastically, raw activation levels may reflect density rather than semantic relatedness

### Mechanism 3: Differential Activation Reveals Bias Directionality
- Claim: Comparing activation levels of stereotype-consistent vs. inconsistent pairs yields quantitative bias metric
- Core assumption: Implicit bias strength is linearly proportional to difference in activation energy received by target concepts
- Evidence: Results showed both convergence and divergence in biases; effect sizes calculated from paired differences of normalized ALs
- Break condition: If normalization inverts polarity of results, metric may be mathematical artifact rather than robust measure

## Foundational Learning

- **Semantic Priming**: Why needed: Entire methodology rests on activating one concept facilitating recognition of related concepts
  - Quick check: If I activate "doctor" in a semantic network, should activation level of "nurse" rise faster than "apple"?

- **Network Topology (Small World Networks)**: Why needed: Network statistics set simulation parameters; understanding density differences crucial for interpretation
  - Quick check: Why does paper set spreading activation time steps to "twice the diameter" rather than fixed number?

- **Intrinsic vs. Extrinsic Bias**: Why needed: Method positions as intrinsic evaluation but uses output-level approach
  - Quick check: Does methodology measure bias in model's final probability output (extrinsic) or relational structure revealed by associations (intrinsic)?

## Architecture Onboarding

- **Component map**: Generating/Loading Free Associations -> Building Undirected Graph -> Running Spreading Activation -> Normalizing Matrix -> Calculating Effect Sizes

- **Critical path**: Generating/Loading Free Associations -> Building Undirected Graph -> Running Spreading Activation -> Normalizing Matrix -> Calculating Effect Sizes

- **Design tradeoffs**:
  - L1 vs. L2 Normalization: L1 aligns better with emotion theory for political data; L2 is standard for others
  - Directed vs. Undirected Edges: Method converts directed→undirected keeping max weight, potentially losing association asymmetry

- **Failure signatures**:
  - Polarity Flip: Effect sizes changing signs when switching normalization methods
  - Polysemy Contamination: "Mindset streams" passing through unrelated senses of word

- **First 3 experiments**:
  1. Sanity Check: Reproduce gender stereotype heatmaps (Figures 4-7) for target model
  2. Normalization Stress Test: Run political emotion analysis using both L1 and L2 norms
  3. Path Inspection: Extract specific "mindset streams" for high-bias pair to verify semantic coherence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do implicit biases identified in word association networks predict real-world discriminatory behavior?
- Basis: Authors state it "remains uncertain to what extent they predict real-world behaviors" in domains like healthcare or recruitment
- Why unresolved: Methodology relies on free-association norms as proxies for cognitive structures, distinct from complex behavioral choices
- Resolution: Correlating network-based bias metrics with behavioral audits or decision-making outcomes in real-world environments

### Open Question 2
- Question: How can researchers systematically determine appropriate normalization technique to prevent contradictory findings?
- Basis: L1 vs. L2 norms produced different results for emotions approach, with L2 yielding "odd" findings
- Why unresolved: Choice currently depends on post-hoc theoretical interpretation rather than standardized criterion
- Resolution: Validation study comparing normalization techniques against established psychological benchmarks

### Open Question 3
- Question: How can methodology be refined to distinguish between multiple word senses?
- Basis: Networks lack multiple nodes for words with multiple meanings, potentially contaminating results (e.g., "black" as color vs. race)
- Why unresolved: Current methods treat words as single nodes regardless of context
- Resolution: Integrating word sense disambiguation into network construction and verifying if bias metrics change

## Limitations

- Methodology's validity hinges on assumption that LLM free associations faithfully reflect internal semantic structures comparable to human cognition
- Critical limitation in polysemy handling—network cannot distinguish between different senses of word (e.g., "black" as color vs. race)
- Divergence between L1 and L2 normalization methods suggests quantitative metrics may be sensitive to mathematical processing choices

## Confidence

- High confidence in network construction methodology and basic spreading activation simulation
- Medium confidence in validity of comparing human and LLM networks as proxies for cognitive structure
- Medium confidence in bias quantification approach, with concerns about normalization sensitivity
- Low confidence in robustness of emotion analysis results due to L1/L2 polarity inversion

## Next Checks

1. Replicate the gender stereotype heatmaps (Figures 4-7) for a target model to verify pipeline consistency
2. Test both L1 and L2 normalization on political emotion data to observe polarity stability and compare against theoretical expectations
3. Extract and qualitatively inspect specific "mindset streams" for high-bias pairs to verify semantic coherence and rule out random walk artifacts