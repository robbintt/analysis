---
ver: rpa2
title: 'LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA'
arxiv_id: '2510.13494'
source_url: https://arxiv.org/abs/2510.13494
tags:
- literaryqa
- question
- narrativeqa
- summary
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LiteraryQA, a high-quality subset of NarrativeQA
  focusing on literary works. The authors address the dataset's noise by using human
  and LLM-validated pipelines to correct low-quality QA pairs and remove extraneous
  text.
---

# LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA

## Quick Facts
- **arXiv ID**: 2510.13494
- **Source URL**: https://arxiv.org/abs/2510.13494
- **Reference count**: 40
- **Primary result**: Introduction of LiteraryQA, a high-quality subset of NarrativeQA for evaluating long-document narrative question answering

## Executive Summary
This paper introduces LiteraryQA, a high-quality subset of NarrativeQA focused on literary works. The authors address the dataset's noise by using human and LLM-validated pipelines to correct low-quality QA pairs and remove extraneous text. They also conduct a meta-evaluation of automatic metrics, finding that n-gram-based metrics have low system-level correlation with human judgment, while LLM-as-a-Judge evaluations show strong agreement. Finally, they benchmark long-context LLMs on LiteraryQA, demonstrating its effectiveness as a challenging benchmark for reading comprehension tasks.

## Method Summary
The authors created LiteraryQA by curating a high-quality subset of the existing NarrativeQA dataset, specifically focusing on literary works. They employed a multi-stage validation pipeline involving human annotators and LLM-based checks to filter out low-quality QA pairs and remove extraneous text. This process ensured that the resulting dataset contains well-formed questions and answers that are relevant to the literary works. Additionally, the authors conducted a meta-evaluation of various automatic metrics, comparing their correlation with human judgments. They found that n-gram-based metrics perform poorly at the system level, while LLM-as-a-Judge evaluations demonstrate strong agreement with human assessments. Finally, they benchmarked several long-context LLMs on LiteraryQA to showcase its effectiveness as a challenging benchmark for reading comprehension tasks.

## Key Results
- LiteraryQA provides a high-quality subset of NarrativeQA, specifically focused on literary works, addressing the original dataset's noise issues.
- N-gram-based metrics show low system-level correlation with human judgment, while LLM-as-a-Judge evaluations demonstrate strong agreement.
- Long-context LLMs achieve varying performance levels on LiteraryQA, highlighting its effectiveness as a challenging benchmark for reading comprehension tasks.

## Why This Works (Mechanism)
The success of LiteraryQA stems from its rigorous curation process, which combines human and LLM validation to ensure high-quality QA pairs. By focusing on literary works, the dataset presents a unique challenge for reading comprehension models, as these texts often contain complex narratives, figurative language, and nuanced themes. The meta-evaluation of automatic metrics reveals the limitations of traditional n-gram-based approaches, paving the way for more sophisticated evaluation methods like LLM-as-a-Judge. Benchmarking long-context LLMs on LiteraryQA demonstrates its effectiveness in assessing models' ability to comprehend and reason over lengthy literary texts.

## Foundational Learning
1. **Narrative Question Answering (NarrativeQA)**: A dataset for evaluating models' ability to answer questions based on book summaries and full narratives.
   - *Why needed*: Provides a foundation for assessing reading comprehension and reasoning over long documents.
   - *Quick check*: Ensure familiarity with the original NarrativeQA dataset and its limitations.

2. **Long-context Language Models**: Models capable of processing and generating text over extended contexts, such as lengthy literary works.
   - *Why needed*: Essential for understanding the performance of models on LiteraryQA, which contains long literary texts.
   - *Quick check*: Verify understanding of long-context LLMs and their applications in text comprehension tasks.

3. **Evaluation Metrics for Text Generation**: Methods for assessing the quality of generated text, including n-gram-based metrics and LLM-as-a-Judge approaches.
   - *Why needed*: Critical for interpreting the results of the meta-evaluation and understanding the strengths and weaknesses of different evaluation methods.
   - *Quick check*: Familiarize yourself with common text generation evaluation metrics and their limitations.

## Architecture Onboarding

### Component Map
Human annotators -> LLM validation -> LiteraryQA dataset -> Meta-evaluation of metrics -> Benchmarking of long-context LLMs

### Critical Path
1. Human annotators review and filter QA pairs for quality and relevance.
2. LLM validation ensures consistency and further refines the dataset.
3. Meta-evaluation compares automatic metrics against human judgments.
4. Benchmarking of long-context LLMs on LiteraryQA assesses their performance.

### Design Tradeoffs
- Balancing the need for high-quality QA pairs with the cost and time associated with human annotation and LLM validation.
- Choosing between n-gram-based metrics and LLM-as-a-Judge evaluations for assessing model performance.
- Selecting appropriate long-context LLMs for benchmarking, considering factors such as model size, training data, and architecture.

### Failure Signatures
- Residual noise in the LiteraryQA dataset despite the validation pipeline.
- Inconsistencies or biases in the LLM-as-a-Judge evaluation.
- Limited generalizability of the findings to domains outside of literary works.

### First Experiments
1. Conduct a user study to assess the quality and relevance of LiteraryQA questions and answers across different literary genres.
2. Test the performance of various long-context LLMs on LiteraryQA, including models with different architectures and training approaches.
3. Evaluate the robustness of the LLM-as-a-Judge evaluation by comparing its judgments with multiple human evaluators.

## Open Questions the Paper Calls Out
None

## Limitations
- Potential for residual noise in the LiteraryQA dataset despite the validation pipeline.
- The dataset's focus on literary works may limit generalizability to other domains.
- The long-context LLMs' performance may be influenced by factors beyond reading comprehension, such as model architecture and training data.

## Confidence
- Effectiveness of LiteraryQA as a benchmark for long-document narrative QA: High
- Identification of n-gram-based metrics' limitations and the superior performance of LLM-as-a-Judge evaluations: High
- Generalizability of the findings to other domains or document types: Medium
- Impact of the dataset's noise reduction on overall quality and the robustness of the validation pipeline: Medium

## Next Checks
1. Conduct a user study with human evaluators to assess the quality and relevance of LiteraryQA questions and answers across different literary genres.
2. Test the performance of various long-context LLMs on LiteraryQA, including models with different architectures and training approaches, to isolate the factors influencing reading comprehension.
3. Evaluate the robustness of the LLM-as-a-Judge evaluation by comparing its judgments with multiple human evaluators and analyzing potential biases or inconsistencies.