---
ver: rpa2
title: 'InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models'
arxiv_id: '2512.05134'
source_url: https://arxiv.org/abs/2512.05134
tags:
- step
- reuse
- flux
- diffusion
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InvarDiff accelerates diffusion model inference by exploiting temporal
  invariance in deterministic sampling. It computes a per-timestep, per-layer, per-module
  binary cache plan matrix from a small calibration set, enabling both step-level
  and layer-wise reuse.
---

# InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models

## Quick Facts
- arXiv ID: 2512.05134
- Source URL: https://arxiv.org/abs/2512.05134
- Reference count: 40
- InvarDiff achieves 2–3× end-to-end speedups on DiT and FLUX models with minimal perceptual quality loss.

## Executive Summary
InvarDiff is a training-free method that accelerates deterministic diffusion model inference by exploiting temporal invariance in feature trajectories. It computes a binary cache plan matrix during a two-phase calibration, identifying steps and layers where feature changes are minimal and can be reused. A resampling correction prevents error accumulation from chained caching. Applied to DiT and FLUX models, InvarDiff delivers 2–3× speedups with minimal impact on perceptual quality metrics, generalizing across classes and prompts.

## Method Summary
InvarDiff accelerates DiT-style diffusion models by caching intermediate features when their changes across timesteps are minimal. During calibration, it computes layer-wise and step-wise "rate" metrics (ρ) using L1 norms of feature differences, then applies quantile thresholds to generate a binary cache plan. A resampling correction phase refines this plan by simulating caching to avoid drift. At inference, a hierarchical scheduler first checks step-level reuse, then layer/module-level reuse. The method is training-free, requires no model modification, and transfers across model families.

## Key Results
- Achieves 2–3× end-to-end speedup on DiT-XL/2 and FLUX.1-dev models.
- Maintains perceptual quality (LPIPS, SSIM, PSNR) close to full-compute baselines.
- Generalizes across classes and prompts without retraining.

## Why This Works (Mechanism)

### Mechanism 1: Temporal Invariance via Rate Metrics
The method computes a layer-wise "rate" ρ using L1 norms of feature differences (ΔZ) between consecutive timesteps. If the rate of change is low relative to a quantile threshold, the feature is considered invariant and is reused from the previous step's cache. This relies on deterministic sampling ensuring smooth feature trajectories.

### Mechanism 2: Resampling Correction to Prevent Drift
An initial cache plan is refined by simulating the caching process on a calibration set. It recalculates change rates assuming cached values were used, identifying "false positive" stable regions where error would accumulate over consecutive steps. This makes the binary cache plan more conservative.

### Mechanism 3: Hierarchical Step-First Scheduling
A two-tier decision process combines coarse-grained step skipping with fine-grained layer skipping. First, a step-level gate checks if the entire step can be skipped (reusing the full network output z_t). If not, it proceeds to a layer-wise check for specific modules (MHSA/FFN). This captures large constant intervals while salvaging partial redundancy in active steps.

## Foundational Learning

- **Concept: Deterministic vs. Stochastic Sampling (ODE vs. SDE)**
  - Why needed here: The core invariance assumption relies on the absence of injected noise. Stochastic samplers add noise at every step, destroying the correlation between Z_t and Z_{t-1}, rendering the "rate" metric and caching logic invalid.
  - Quick check question: If you switch from DDIM (η=0) to a DDPM sampler, would you expect the InvarDiff speedup to hold? (Answer: No, the feature trajectories would become discontinuous).

- **Concept: Quantile Statistics**
  - Why needed here: The system converts continuous "change rates" into a binary "skip/compute" decision using quantile thresholds (e.g., top 30% most stable features). Understanding quantiles is necessary to tune the trade-off between speed (skipping more) and quality (skipping less).
  - Quick check question: If you lower the quantile threshold for the step-gate (e.g., from 0.7 to 0.5), are you becoming more or less conservative regarding speed? (Answer: Less conservative; you will skip more steps, increasing speed but risking quality).

- **Concept: Transformer Modules (MHSA vs. FFN)**
  - Why needed here: InvarDiff treats MHSA (attention) and FFN (feed-forward) blocks differently, maintaining separate cache plans for each. Empirical observations suggest they have different sensitivities (e.g., MHSA relates to spatial layout, FFN to textures).
  - Quick check question: According to the paper's ablation, which module type is more tolerant of aggressive caching before visual distortion occurs? (Answer: Attention/MHSA is generally more tolerant; FFN errors often manifest as speckles or texture issues).

## Architecture Onboarding

- **Component map:** Feature extraction hooks → Rate Calculator → Resampling Engine → Cache Plan Matrix (C) → Runtime Scheduler
- **Critical path:** The integrity of the system relies on the **Calibration → Resampling** loop. If the calibration prompts are not representative, or if the resampling step is skipped, the binary plan C will contain "false positive" reuse instructions, leading to compounding error and image drift.
- **Design tradeoffs:** Speed vs. Fidelity is controlled by the quantile thresholds (τ). Aggressive thresholds (τ_step > 0.7) yield >3× speedup but lower PSNR. Compute vs. Memory: Caching features requires storing activations Z_{l,t-1} for all layers, increasing memory footprint slightly to save compute.
- **Failure signatures:** Spatial Drift/Collapse indicates the step-gate (c^{step}_t) is too aggressive or resampling correction was insufficient. Speckling/Texture Noise indicates the FFN cache threshold is too high. Patch Edge Artifacts are specifically linked to caching single-stream FFN layers in FLUX models.
- **First 3 experiments:**
  1. Run a single DiT inference pass on 10 classes. Calculate and visualize the MSE/Cosine heatmaps to verify the "temporal invariance" pattern exists.
  2. Run the full pipeline (Calibration + Inference) with Phase 2 (Resampling) enabled vs. disabled. Compare LPIPS/PSNR on a fixed seed to quantify the value of the correction mechanism.
  3. On a small set of prompts, sweep the step-threshold (τ_{step}) while fixing module thresholds. Plot the latency vs. PSNR curve to find the "knee" of optimal performance.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the cache plan be derived adaptively online to eliminate the dependency on offline calibration sets? (The conclusion states future work includes "adaptive online plan updates").
- **Open Question 2:** How does InvarDiff perform when composed with aggressive step-reduction techniques like consistency distillation? (The conclusion explicitly lists "coupling with step-reduction or distillation" as a direction for future work).
- **Open Question 3:** Does the cross-scale invariance assumption hold for long-horizon video generation without introducing temporal drift? (The conclusion calls for "scaling the analysis to long-horizon video models").

## Limitations
- The exact token-pooling method for rate computation (mean vs. L1 vs. sum) is unspecified and could affect cache plan granularity.
- The interpolation method for quantile thresholds is not detailed, potentially impacting reproducibility.
- Boundary timestep handling for rate computation is vague, with unclear behavior at t=0 and t=T-1.

## Confidence
- **High confidence**: The general mechanism of rate-based caching with quantile thresholds, and the Phase 2 resampling correction to prevent drift. These are clearly specified and empirically validated.
- **Medium confidence**: The optimal quantile thresholds for different model families (DiT vs. FLUX) may require dataset-specific tuning. The paper provides specific values but these could be suboptimal for new domains.
- **Low confidence**: The claim that the method generalizes across "unseen prompts" is based on a single holdout prompt in GenEval. The robustness to distribution shift or domain-specific features is not extensively validated.

## Next Checks
1. **Boundary behavior validation**: Run inference with explicit checks at t=0 and t=T-1. Compare outputs with three different boundary strategies to identify if artifacts correlate with rate calculation at boundaries.
2. **Calibration set size ablation**: Systematically vary the calibration set size (1 prompt → 32 prompts) and measure the sensitivity of final LPIPS/PSNR to quantify the minimum representative dataset needed.
3. **Downstream task transfer**: Apply InvarDiff caching plan from ImageNet DiT-XL to a held-out task (e.g., COCO detection pretraining). Measure if the cached model maintains accuracy compared to full compute, testing the "transferability" claim.