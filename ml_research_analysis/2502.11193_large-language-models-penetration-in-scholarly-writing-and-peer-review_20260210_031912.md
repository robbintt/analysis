---
ver: rpa2
title: Large Language Models Penetration in Scholarly Writing and Peer Review
arxiv_id: '2502.11193'
source_url: https://arxiv.org/abs/2502.11193
tags:
- review
- llm-generated
- data
- reviews
- abstract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study reveals increasing Large Language Model (LLM) penetration
  in scholarly writing and peer review, raising concerns about academic credibility.
  Researchers developed a two-component framework: ScholarLens, a curated dataset
  of human-written and LLM-generated scholarly content, and LLMetrica, a tool combining
  rule-based linguistic metrics and model-based detectors to assess LLM usage.'
---

# Large Language Models Penetration in Scholarly Writing and Peer Review

## Quick Facts
- arXiv ID: 2502.11193
- Source URL: https://arxiv.org/abs/2502.11193
- Authors: Li Zhou; Ruijie Zhang; Xunlian Dai; Daniel Hershcovich; Haizhou Li
- Reference count: 40
- Key outcome: LLM-generated scholarly text exhibits distinct linguistic patterns, with model-based detectors achieving F1 scores up to 98% and revealing rising LLM penetration in academic writing post-2023

## Executive Summary
This study systematically investigates the penetration of Large Language Models in scholarly writing and peer review. Researchers developed a two-component framework consisting of ScholarLens, a curated dataset of human-written and LLM-generated scholarly content, and LLMetrica, a tool that combines rule-based linguistic metrics and model-based detectors to assess LLM usage. The framework identifies consistent linguistic patterns in LLM-generated text (longer words, lower readability, positive sentiment) and demonstrates that domain-specific detectors substantially outperform general-purpose AI detectors on scholarly content.

## Method Summary
The study employs a two-component framework: (1) ScholarLens, a dataset containing 5,000 human-written and 6,000 LLM-generated scholarly items (abstracts, reviews, meta-reviews) from top AI conferences, and (2) LLMetrica, which integrates rule-based linguistic metrics (10 general and 4 specific metrics) with model-based detectors (ScholarDetect using Longformer architecture). The approach combines statistical feature analysis with supervised learning to detect LLM usage, with temporal analysis applied to real conference data from 2020-2024 to track penetration trends.

## Key Results
- LLM-generated texts consistently show longer words, lower readability scores, and more positive sentiment across different scholarly domains
- Domain-specific detectors trained on ScholarLens achieved F1 scores up to 98%, outperforming existing baselines
- Temporal analysis indicates rising LLM penetration in abstracts and reviews post-2023, with refined LLM roles dominating in peer reviews

## Why This Works (Mechanism)

### Mechanism 1
LLM-generated scholarly text exhibits quantifiable linguistic differences from human-written text that persist across multiple LLM sources. LLMs preferentially generate longer words and complex-syllabled vocabulary while reducing stopwords and readability scores, stemming from training corpus patterns that favor formal, elaborated expression. The paper identifies specific preferred words (e.g., "enhance," "innovative," "methodologies") that appear disproportionately in LLM output. Core assumption: These linguistic preferences remain stable enough across LLM versions and prompt variations to serve as reliable detection signals.

### Mechanism 2
LLM-generated reviews and meta-reviews show reduced sentence-level specificity and higher semantic similarity to reference texts than human-written counterparts. When synthesizing reviews, LLMs tend toward generic summarization rather than distinct critique. The SF-IRF metric captures this: human reviews contain sentences that are rare across the reference set (high specificity), while LLM sentences cluster around common patterns. Higher MRSim scores indicate LLM meta-reviews mirror source reviews more closely without adding independent analysis. Core assumption: Human reviewers genuinely provide more diverse, paper-specific observations rather than templated responses.

### Mechanism 3
Domain-specific fine-tuned detectors substantially outperform general-purpose AI-text detectors on scholarly content. ScholarDetect models trained on ScholarLens (mixed LLM sources, hybrid data types) learn domain-specific features that generalize across abstracts, reviews, and meta-reviews. The Longformer architecture handles longer review texts better than baselines. Training on mixed LLM outputs prevents overfitting to single-model artifacts. Core assumption: The controlled LLM-generated samples in ScholarLens adequately represent real-world LLM-assisted writing.

## Foundational Learning

- **Concept: TF-IDF and Information Retrieval Metrics** - The SF-IRF metric extends TF-IDF logic to sentence-level analysis within peer review contexts. Understanding how inverse frequency signals "specificity" is essential for interpreting why LLM text scores lower. Quick check: If a sentence appears in 8 out of 10 reviews for different papers, would it have high or low IRF? What does this imply about its diagnostic value?

- **Concept: Transformer-based Sequence Classification (Longformer)** - ScholarDetect uses Longformer to handle document-length inputs (reviews can exceed standard BERT's 512-token limit). Understanding attention mechanisms and sliding-window approaches explains why this architecture was chosen. Quick check: Why would a standard BERT model struggle with full peer review texts, and how does Longformer's attention pattern differ?

- **Concept: Statistical Hypothesis Testing for Feature Selection** - The paper uses Welch's t-test on word proportions to identify LLM-preferred vocabulary. This rigorous approach filters spurious differences and quantifies effect sizes. Quick check: When comparing word frequencies between human and LLM texts, why use Welch's t-test rather than a standard t-test?

## Architecture Onboarding

- **Component map:**
ScholarLens Dataset -> Rule-based Metrics (10 General + 4 Specific) -> Model-based Detectors (ScholarDetect with Longformer) -> Temporal Analysis

- **Critical path:** Dataset curation → Feature engineering (rule-based metrics) → Detector training (ScholarDetect) → Temporal analysis on real conference data. Steps 2 and 3 can parallelize; step 4 depends on both.

- **Design tradeoffs:**
  - Pre-2019 data ensures uncontaminated human baselines but may not reflect current writing conventions
  - Mixed-LLM training improves generalization but may dilute model-specific signals
  - Rule-based + model-based provides interpretable features alongside raw detection scores
  - Abstract focus for authors is accessible and representative but doesn't capture full-paper LLM usage

- **Failure signatures:**
  - High false positive rates on non-native English writers (longer words, formal phrasing may overlap with LLM patterns)
  - Detector performance drops on heavily post-edited LLM text
  - Review detection underperforms when papers have few reviews (RSim metric becomes noisy)
  - Temporal analysis shows anomalous 2023 dips—likely due to metric sensitivity rather than actual usage changes

- **First 3 experiments:**
  1. Baseline validation: Run ScholarDetect-Hybrid (mixed) on held-out ScholarLens test sets; verify F1 > 95% on abstracts, >96% on meta-reviews
  2. Feature ablation: Train detectors using only the 4 robust linguistic features (AWL, LWR, SWR, FRE) vs. full feature set; quantify performance gap
  3. Cross-conference temporal analysis: Apply trained models to a different venue (e.g., NeurIPS abstracts 2020-2024) to test whether ICLR-observed trends generalize; compare rule-based metric shifts vs. detector predictions for consistency

## Open Questions the Paper Calls Out
None

## Limitations
- ScholarLens contains synthetically generated content rather than real-world human-LLM hybrid writing, potentially limiting ecological validity
- Temporal analysis relies on rule-based metrics that may be sensitive to confounding factors (changing writing conventions, non-native English authors)
- Detection models show high performance on curated test sets but may struggle with adversarial prompt engineering or heavy human post-editing

## Confidence
- High: LLM-generated text exhibits consistent linguistic patterns (longer words, lower readability, positive sentiment)
- High: Domain-specific detectors outperform general-purpose AI detectors on scholarly content
- Medium: Rising LLM penetration in abstracts and reviews post-2023 (temporal metric shifts may have confounding factors)
- Medium: LLM meta-reviews show higher semantic similarity to source reviews than human meta-reviews (depends on human review diversity assumption)

## Next Checks
1. Ecological validity test: Apply detection models to real-world preprints with known LLM usage (e.g., openly declared AI assistance cases) and measure false positive/negative rates
2. Adversarial robustness evaluation: Test model performance on LLM-generated text that has undergone human editing or prompt engineering designed to evade detection
3. Cross-domain generalization: Evaluate whether detection models trained on computer science abstracts generalize to other fields (bioinformatics, social sciences, humanities)