---
ver: rpa2
title: ADAM Optimization with Adaptive Batch Selection
arxiv_id: '2512.06795'
source_url: https://arxiv.org/abs/2512.06795
tags:
- lemma
- online
- algorithm
- regret
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adam with Combinatorial Bandit Sampling (AdamCB),
  an optimizer that addresses the limitations of existing adaptive batch selection
  methods in Adam-based training. While prior approaches like AdamBS adaptively sample
  data using bandit frameworks, they suffer from convergence issues and lack rigorous
  theoretical guarantees.
---

# ADAM Optimization with Adaptive Batch Selection

## Quick Facts
- arXiv ID: 2512.06795
- Source URL: https://arxiv.org/abs/2512.06795
- Reference count: 40
- Primary result: Introduces AdamCB, a combinatorial bandit-based optimizer that provably converges faster than standard Adam and AdamBS

## Executive Summary
This paper presents Adam with Combinatorial Bandit Sampling (AdamCB), an optimizer that addresses convergence issues in adaptive batch selection methods for Adam-based training. While prior approaches like AdamBS adaptively sample data using bandit frameworks, they suffer from convergence issues and lack rigorous theoretical guarantees. AdamCB resolves these problems by employing a combinatorial bandit approach, allowing multiple samples to be selected simultaneously without replacement. The paper provides a regret analysis proving that AdamCB achieves faster convergence than both the original Adam (with uniform sampling) and AdamBS.

## Method Summary
AdamCB treats batch selection as a combinatorial bandit problem where a set of K samples is chosen as a single action without replacement. The algorithm maintains weights for each sample based on gradient norms, using exponential weighting to prioritize samples with larger gradients. When a sample is selected, its gradient is scaled by 1/(n·p_{j,t}) to maintain unbiasedness. The batch selection uses Dependent Rounding (DepRound) to ensure distinct samples while maintaining target probabilities. The method updates weights using a loss function inversely related to gradient norms, creating a feedback loop that adapts sampling probabilities over time.

## Key Results
- AdamCB achieves faster convergence than both standard Adam and AdamBS across multiple datasets and model architectures
- Theoretical regret bound scales as O(d√T + d^(1/2)n^(−3/4)(T/K)^(1/4)ln^(1/4)(n/K)), demonstrating improved performance with larger batch sizes
- Extensive experiments on MNIST, Fashion MNIST, and CIFAR-10 confirm consistent improvements in both training and test loss reduction

## Why This Works (Mechanism)

### Mechanism 1
Selecting mini-batches as combinatorial actions without replacement improves convergence rates compared to independent sampling with replacement. Standard bandit methods treat batch selection as K independent single-arm pulls, allowing duplicates. AdamCB utilizes a combinatorial semi-bandit framework where selecting a set of K samples is a single action, forcing distinct samples and ensuring the batch size K effectively reduces variance and improves the regret bound term by a factor related to K.

### Mechanism 2
Prioritizing samples with larger gradient norms accelerates the reduction of empirical risk. The algorithm treats optimization as an adversarial semi-bandit problem, assigning a "loss" to each sample inversely proportional to its squared gradient norm. By minimizing this loss via exponential weighting, the algorithm increases the selection probability of samples that currently exert a stronger influence on the model update.

### Mechanism 3
Using importance sampling allows for adaptive data selection while maintaining the unbiasedness required for Adam's convergence guarantees. To prevent the model from biasing towards easy or hard samples, AdamCB scales the gradient of a selected sample by 1/(n·p_{j,t}), compensating for the lower probability of selection and ensuring the expected value of the estimated gradient equals the full-dataset gradient.

## Foundational Learning

- **Combinatorial Multi-Armed Bandits**: Why needed - The core innovation is treating a batch of data as a single "super-arm" rather than a collection of independent arms. Quick check - How does the "DepRound" algorithm differ from standard sampling techniques in ensuring specific marginal probabilities?
- **Importance Sampling**: Why needed - To understand why the gradient is divided by the probability p_{j,t} in Eq (7). Quick check - If a sample has a 1% chance of being selected, how must its gradient be scaled to maintain an unbiased estimate of the total gradient?
- **Regret Analysis**: Why needed - The paper proves efficacy through "cumulative regret" rather than just loss plots. Quick check - What does a sub-linear regret bound (R(T) = o(T)) imply about the average loss over time?

## Architecture Onboarding

- **Component map**: Weight Manager -> Batch Selector -> Unbiased Estimator -> Adam Core -> Feedback Loop
- **Critical path**: The execution flow is sequential: Update Weights → Select Batch → Compute Gradients → Update Model. The critical dependency is that the Batch Selector must receive updated weights before selecting the next batch.
- **Design tradeoffs**: Exploration (γ) vs. Exploitation creates tension between safety and speed. Larger batch size K improves the regret bound term (T/K)^(1/4) but increases the O(n) cost of the DepRound selection step per iteration.
- **Failure signatures**: Duplicate Indices indicate implementation has reverted to standard bandit sampling with replacement. Weight Collapse occurs if weights become extremely small causing numerical underflow. Bias Drift happens if the gradient scaling in Eq (7) is omitted.
- **First 3 experiments**: 
  1. Sanity Check (MLP): Replicate MNIST experiment to verify Training Loss drops faster than standard Adam and AdamBS
  2. Batch Scaling: Run AdamCB with increasing batch sizes (K=32, 64, 128) on fixed budget to confirm (T/K)^(1/4) scaling
  3. Ablation on Unbiasedness: Run modified version without 1/(np_{j,t}) scaling to demonstrate divergence or bias

## Open Questions the Paper Calls Out

- Can the convergence guarantees of AdamCB be extended to non-convex optimization settings? The theoretical regret analysis relies on convexity assumptions despite conducting experiments on non-convex deep neural networks.
- Does AdamCB maintain theoretical convergence properties under weaker assumptions than bounded gradients? The bounded gradient assumption can be restrictive for deep neural networks where gradient magnitudes may vary.
- Is the computational overhead of the O(n) DepRound algorithm prohibitive for extremely large-scale datasets? While O(n) is efficient compared to combinatorial space, it may become a bottleneck for web-scale datasets.

## Limitations
- Theoretical analysis relies on bounded gradients and full exploration assumptions that may not hold in practice
- Computational overhead of O(n) DepRound algorithm may limit scalability to very large datasets
- Ablation study focuses on unbiasedness but doesn't isolate individual contributions of combinatorial sampling vs adaptive weighting mechanisms

## Confidence
- **High confidence** in unbiased gradient estimation mechanism via importance sampling
- **Medium confidence** in combinatorial bandit sampling improving convergence rates
- **Medium confidence** in prioritizing high-gradient-norm samples accelerating learning

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary exploration parameter γ and batch size K to determine their impact on convergence speed and final performance
2. **Scalability Benchmark**: Evaluate AdamCB's performance and computational overhead on a large-scale dataset (e.g., ImageNet) to assess practical viability
3. **Mechanistic Ablation**: Implement variant using combinatorial sampling with uniform weights to isolate sampling strategy contribution from weighting strategy