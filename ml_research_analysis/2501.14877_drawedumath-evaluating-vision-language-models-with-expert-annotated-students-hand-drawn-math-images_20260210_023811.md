---
ver: rpa2
title: 'DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students''
  Hand-Drawn Math Images'
arxiv_id: '2501.14877'
source_url: https://arxiv.org/abs/2501.14877
tags:
- student
- questions
- question
- teachers
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces DrawEduMath, a dataset of 2,030 images of\
  \ students\u2019 handwritten math responses, annotated by teachers with detailed\
  \ descriptions and QA pairs. The dataset aims to evaluate vision language models\
  \ (VLMs) in interpreting students\u2019 work in real-world educational contexts."
---

# DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students' Hand-Drawn Math Images

## Quick Facts
- arXiv ID: 2501.14877
- Source URL: https://arxiv.org/abs/2501.14877
- Reference count: 30
- The dataset contains 2,030 images of students' handwritten math responses with expert annotations, revealing that even state-of-the-art VLMs struggle to accurately judge mathematical correctness

## Executive Summary
DrawEduMath introduces a benchmark dataset of 2,030 images showing K-12 students' handwritten math work, annotated by teachers with detailed descriptions and QA pairs. The dataset evaluates vision language models' ability to interpret real student work in educational contexts. Teachers provided descriptions and QA pairs, which were supplemented with synthetic QA pairs generated using language models. The authors find that VLMs perform significantly worse on questions about mathematical correctness compared to superficial image features, highlighting the challenge of interpreting student work beyond visual recognition.

## Method Summary
The DrawEduMath dataset consists of 2,030 images of student math work annotated by teachers who provided detailed descriptions and QA pairs. A synthetic QA generation pipeline used language models to decompose teacher descriptions into atomic "facets" and convert them into self-contained QA pairs, creating 44,362 synthetic pairs from teacher descriptions. The dataset was evaluated using four VLMs (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, Llama 3.2-11B Vision) with three automatic metrics: ROUGE-L, BERTScore, and LLM-based similarity using Mixtral 8x22B. The questions were categorized into seven types using K-means clustering and GPT-4o classification to analyze performance across different cognitive demands.

## Key Results
- VLMs achieved only 18-54% accuracy on teacher-written QA pairs, significantly below human-level performance
- Models performed best on "image creation and medium" questions (77.0% average accuracy) but struggled most with "correctness and errors" questions (47.7-60.1% accuracy)
- Synthetic QA pairs, though imperfect, yielded similar model rankings as teacher-written QA pairs while providing 4x more evaluation data
- LLM-based similarity judgments (Mixtral 8x22B) correlated more strongly with human judgments (ρ = 0.801) than traditional n-gram metrics

## Why This Works (Mechanism)

### Mechanism 1: Description-to-QA Decomposition Pipeline
Synthetic QA pairs generated from teacher descriptions can produce similar model rankings to human-written QA pairs at scale. Teachers write detailed free-form descriptions → LLMs decompose descriptions into atomic "facets" → Each facet is converted into a self-contained QA pair. This creates 44,362 synthetic QA pairs from teacher descriptions. The core assumption is that teacher descriptions capture sufficient information about student responses that atomic extraction preserves correctness.

### Mechanism 2: Question-Type Taxonomy for Diagnostic Evaluation
Categorizing questions by type reveals that VLMs struggle most with "correctness and errors" questions (47.7-60.1% accuracy) versus "image creation and medium" questions (77.0% average accuracy). K-means clustering on sentence embeddings of questions (with nouns masked) → qualitative pattern detection → GPT-4o categorization into 7 types → performance disaggregation by category. The core assumption is that question types derived from embedding clusters meaningfully separate difficulty levels and cognitive demands.

### Mechanism 3: LLM-as-Judge Validation Against Human Evaluation
LLM-based similarity judgments (Mixtral 8x22B) correlate more strongly with human judgments (ρ = 0.801) than traditional n-gram metrics. Present model answer and gold answer to LLM with Likert-scale similarity prompt → binarize ratings (1-2 = incorrect, 3-4 = correct) → validate against human annotations on 500 samples. The core assumption is that the LLM evaluator's notion of "similarity" aligns with human pedagogical judgment.

## Foundational Learning

- **Vision-Language Model (VLM) Architecture**: Why needed here - The benchmark evaluates how VLMs process handwritten math images; understanding encoder-projector-LLM structure explains failure modes. Quick check: Can you explain why a VLM might correctly read text but fail to assess mathematical correctness?

- **Reference-Based Evaluation Metrics**: Why needed here - The paper uses BERTScore, ROUGE-L, and LLM-judgment; understanding their differences is essential for interpreting results. Quick check: Why would ROUGE-L perform poorly on teacher-written QA with long, detailed answers?

- **Synthetic Data Generation Pipelines**: Why needed here - 79% of QA pairs are synthetic; understanding LM-based data generation quality tradeoffs is critical for using this benchmark. Quick check: What information is lost when converting a detailed teacher description into atomic QA facets?

## Architecture Onboarding

- **Component map**: Student Image → Teacher Annotation (Description + QA) → LM Decomposition Pipeline → Synthetic QA Generation (Claude/GPT-4o) → Question Taxonomy Classification → VLM Benchmarking (GPT-4o/Claude/Gemini/Llama) → Multi-Metric Evaluation (BERTScore/ROUGE-L/LLM-Judge)

- **Critical path**: Teacher description quality → facet decomposition accuracy → QA pair validity → VLM evaluation reliability. The paper notes 94.2% of descriptions were unchanged in revision, suggesting this is relatively stable.

- **Design tradeoffs**: Teacher QA (11,661 pairs) vs. Synthetic QA (44,362 pairs): Teachers focus on strategy/correctness (46.2%); synthetic focuses on low-level/math content (48-53%). Different coverage. Typed vs. transcribed descriptions: Speech yields longer annotations but introduces transcription errors. Open-source (Llama 3.2-11B) vs. closed models: 35-59% performance gap on teacher QA.

- **Failure signatures**: Dark/blurry images: "models not being able to interpret dark images, even though their contents were visible to human annotators". Correct math, wrong interpretation: VLMs answered "3" when student's number line showed 18/6 aligned with 2. Ambiguous referents: "Where does the second arrow point?" when arrows overlap.

- **First 3 experiments**:
  1. Run GPT-4o on 50 images with teacher-written QA; compare LLM-judge scores against reported 0.628.
  2. Filter predictions on "correctness and errors" category only; manually inspect 20 failure cases to identify patterns (OCR errors vs. reasoning failures).
  3. Sample 50 Claude-generated and 50 GPT-4o-generated QA pairs; manually verify answerability and correctness against Table 5 baseline (85% answerable, 82% correct).

## Open Questions the Paper Calls Out

- How can VLMs be improved to accurately judge the correctness of students' mathematical work, given that this was the lowest-performing question category across all models? The paper identifies this weakness but does not investigate specific causes.

- How can VLMs be trained to distinguish between providing a mathematically correct answer and accurately interpreting a student's potentially incorrect response? The paper documents this failure mode but does not propose or test interventions.

- What weighting schemes should be applied to different question types when evaluating VLMs for real-world educational deployment? The paper introduces a taxonomy but treats all questions equally in evaluation.

## Limitations

- Synthetic QA generation quality is imperfect, with 85% answerability and 82% correctness rates, potentially affecting VLM evaluation reliability
- LLM-as-judge reliability shows asymmetry in error types (46 false positives vs 6 false negatives), suggesting potential inflation of performance estimates
- Question taxonomy validity relies heavily on GPT-4o interpretation, which may introduce circularity in the categorization process

## Confidence

- **High Confidence**: VLM performance on DrawEduMath is significantly below human-level (18-54% accuracy on teacher QA), and models perform better on superficial image features than mathematical correctness
- **Medium Confidence**: Synthetic QA pairs can substitute for teacher-written QA in model ranking, though quality gaps may cause divergence on more challenging questions
- **Low Confidence**: The question taxonomy meaningfully reflects VLM capabilities and failure modes, as semantic interpretation relies heavily on GPT-4o

## Next Checks

1. Stratify VLM performance by image quality metadata (dark, blurry, cropped) to quantify how much performance degradation stems from OCR/vision limitations versus reasoning failures

2. Manually sample 50 Claude-generated and 50 GPT-4o-generated QA pairs to categorize error types and assess whether errors cluster in specific question types

3. On the 500 human-annotated samples, compute per-question-type accuracy differences between LLM-judge and human judgments to identify where automatic evaluation diverges from pedagogical standards