---
ver: rpa2
title: Statistical Hypothesis Testing for Auditing Robustness in Language Models
arxiv_id: '2506.07947'
source_url: https://arxiv.org/abs/2506.07947
tags:
- language
- outputs
- testing
- perturbation
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Distribution-based perturbation analysis (DBPA) is a model-agnostic
  statistical framework for quantifying how input perturbations affect language model
  outputs. The method addresses the challenge of comparing entire output distributions
  despite computational intractability and semantic interpretability issues.
---

# Statistical Hypothesis Testing for Auditing Robustness in Language Models

## Quick Facts
- arXiv ID: 2506.07947
- Source URL: https://arxiv.org/abs/2506.07947
- Authors: Paulius Rauba; Qiyao Wei; Mihaela van der Schaar
- Reference count: 33
- Distribution-based perturbation analysis (DBPA) is a model-agnostic statistical framework for quantifying how input perturbations affect language model outputs

## Executive Summary
Distribution-based perturbation analysis (DBPA) addresses the fundamental challenge of quantifying how input perturbations affect language model outputs through a novel statistical hypothesis testing framework. Traditional approaches struggle with comparing entire output distributions due to computational intractability and semantic interpretability issues. DBPA overcomes these limitations by constructing empirical null and alternative distributions via Monte Carlo sampling and pairwise similarity measures, enabling rigorous frequentist hypothesis testing without restrictive assumptions. The framework provides interpretable p-values, scalar effect sizes, and supports multiple testing with controlled error rates.

The method successfully demonstrates its utility across three distinct case studies: quantifying response changes under persona instructions, measuring true/false positive rates in healthcare scenarios, and evaluating alignment between different language models. DBPA captures statistically significant distributional shifts while controlling for stochastic variability in LLM outputs, offering practitioners a robust tool for auditing model robustness without requiring model-specific modifications or access to internal parameters.

## Method Summary
DBPA constructs empirical null and alternative distributions through Monte Carlo sampling of perturbed inputs and corresponding LLM outputs. For each input perturbation, the framework computes pairwise similarity measures between original and perturbed outputs using embedding distances, string edit distance, and perplexity metrics. These similarity scores form empirical distributions that serve as test statistics for frequentist hypothesis testing. The approach leverages the sampling distribution of the mean to determine statistical significance and effect size, providing interpretable results that quantify distributional changes while accounting for stochastic output variability inherent to LLMs.

## Key Results
- DBPA quantifies response changes under persona instructions with effect sizes ranging from 0.14 to 0.41
- Healthcare scenario testing shows true/false positive rate discrimination with AUC values between 0.43 and 0.65
- The framework successfully evaluates alignment between different language models while controlling multiple testing error rates

## Why This Works (Mechanism)
DBPA works by transforming the intractable problem of comparing entire output distributions into a tractable statistical testing framework. By treating each perturbation as a random variable and constructing empirical sampling distributions through Monte Carlo methods, the approach leverages well-established frequentist statistics. The pairwise similarity measures between original and perturbed outputs provide scalar test statistics that can be analyzed using standard statistical techniques, while the Monte Carlo sampling adequately captures the stochastic nature of LLM outputs.

## Foundational Learning
- Monte Carlo sampling for empirical distribution construction: Needed to approximate intractable output distributions without requiring infinite computational resources
- Pairwise similarity metrics (embedding distance, edit distance, perplexity): Required to quantify output changes in semantically meaningful ways
- Frequentist hypothesis testing framework: Essential for establishing statistical significance and controlling false positive rates
- Effect size computation: Necessary for interpreting practical significance beyond statistical significance
- Multiple testing correction: Critical for maintaining error rate control when evaluating multiple perturbations simultaneously

## Architecture Onboarding
Component map: Input perturbations -> LLM outputs -> Similarity measures -> Empirical distributions -> Hypothesis testing -> p-values and effect sizes

Critical path: Perturbation generation → LLM invocation → Similarity computation → Statistical analysis → Result interpretation

Design tradeoffs: Computational efficiency (Monte Carlo sampling) vs. accuracy (number of samples); Metric sensitivity (embedding choice) vs. interpretability; Statistical power vs. false positive control

Failure signatures: Low effect sizes despite semantic changes (metric sensitivity issues); High p-values despite obvious differences (insufficient sampling); Inconsistent results across runs (random seed sensitivity)

First experiments: 1) Single perturbation testing on simple text classification task; 2) Multiple perturbation analysis on persona-based response generation; 3) Cross-model comparison using alignment evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on similarity metrics introduces subjectivity and sensitivity to embedding model choice
- Monte Carlo sampling assumptions may not hold for high-dimensional or complex input spaces
- Reported modest discriminatory power (AUC 0.43-0.65) raises questions about practical significance

## Confidence
- Statistical methodology: High
- Empirical validation: Medium
- Practical utility: Low

## Next Checks
1. Benchmark DBPA against existing metamorphic testing approaches for LLMs across diverse tasks (reasoning, generation, classification) to establish relative performance
2. Conduct sensitivity analysis varying embedding models, similarity metrics, and Monte Carlo sample sizes to quantify robustness to methodological choices
3. Test DBPA on adversarial perturbation scenarios to evaluate whether it captures robustness to intentionally crafted attacks rather than random perturbations