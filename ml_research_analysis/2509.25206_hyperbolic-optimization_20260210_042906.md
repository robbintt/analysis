---
ver: rpa2
title: Hyperbolic Optimization
arxiv_id: '2509.25206'
source_url: https://arxiv.org/abs/2509.25206
tags:
- hyperbolic
- optimization
- adamw
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work extends Riemannian optimization to hyperbolic manifolds\
  \ by developing a Hyperbolic AdamW optimizer for training diffusion models. The\
  \ key idea is to adapt the AdamW update rules to the Poincar\xE9 ball geometry,\
  \ transforming gradients according to the hyperbolic metric and projecting parameters\
  \ to remain within the manifold."
---

# Hyperbolic Optimization

## Quick Facts
- **arXiv ID**: 2509.25206
- **Source URL**: https://arxiv.org/abs/2509.25206
- **Reference count**: 24
- **Primary result**: Hyperbolic AdamW optimizer accelerates early-stage convergence in diffusion model training compared to Euclidean baselines.

## Executive Summary
This work extends Riemannian optimization to hyperbolic manifolds by developing a Hyperbolic AdamW optimizer for training diffusion models. The key idea is to adapt the AdamW update rules to the Poincaré ball geometry, transforming gradients according to the hyperbolic metric and projecting parameters to remain within the manifold. A hyperbolic time-discretization of the Langevin dynamics is also introduced for the diffusion process. Experiments on a butterfly image generation dataset show that the hyperbolic optimizers achieve faster convergence in terms of FID score compared to standard Euclidean optimizers, particularly in the early training stages.

## Method Summary
The method adapts Riemannian optimization to the Poincaré ball model of hyperbolic space for training diffusion models. The core innovation is transforming Euclidean gradients using the hyperbolic metric scaling factor $(1-||\theta||^2)^2/4$ before applying standard AdamW update rules, then projecting parameters back onto the manifold to maintain validity. The approach also modifies the diffusion process by sampling timesteps via a unit hyperbola function $t = \sqrt{s^2 - 1}$ instead of uniform linear sampling. The loss function can optionally use a Poincaré distance measure based on arcosh. The implementation maintains the same computational structure as standard DDPM but with geometry-aware gradient transformations and manifold constraints.

## Key Results
- Hyperbolic SGD achieves faster early-stage convergence than Euclidean SGD on butterfly image generation
- Hyperbolic AdamW shows similar acceleration benefits compared to standard AdamW
- Hyperbolic timestep sampling accelerates training but doesn't consistently improve final FID scores
- Visual inspection confirms improved sample quality at lower epochs with hyperbolic optimization

## Why This Works (Mechanism)

### Mechanism 1: Metric-Scaled Gradient Transformation
Scaling gradients by $(1-||\theta||^2)^2/4$ accelerates early-stage convergence when parameters are far from the optimum. The Poincaré ball's exponential metric expands distances near the manifold boundary. Parameters far from the origin receive larger effective gradient updates because the scaling factor $(1-||\theta||^2)^2$ approaches its maximum when $||\theta|| \approx 0$ and shrinks as parameters approach the boundary. This creates adaptive step sizes without explicit learning rate scheduling.

### Mechanism 2: Hyperbolic Time-Discretization of Langevin Dynamics
Sampling timesteps via $t = \sqrt{s^2 - 1}$ (unit hyperbola) improves convergence over uniform linear sampling. Standard DDPM uses uniform timestep sampling $t \sim U\{1, T\}$. The hyperbolic sampler non-linearly redistributes timestep density, implicitly emphasizing certain noise levels in the forward/reverse process. The paper does not fully explain why this helps, but the mapping may align timestep distribution with the hyperbolic geometry of the parameter space.

### Mechanism 3: Manifold-Constrained Projection
Projecting parameters back onto the Poincaré ball after each update maintains geometric validity and prevents numerical instability. After gradient updates, parameters may leave the valid region of the Poincaré ball ($||\theta|| \geq 1$). The projection $\text{Proj}(\theta)$ normalizes parameters to the boundary with a small $\epsilon$ margin, ensuring distance computations remain well-defined and avoiding division by zero in arcosh.

## Foundational Learning

- **Poincaré Ball Model of Hyperbolic Space**: The entire optimization method operates on this manifold. You must understand that hyperbolic space has constant negative curvature, and the Poincaré ball is one representation where distances expand exponentially toward the boundary.
  - Quick check question: Can you explain why two points near the boundary of the Poincaré ball can have a large hyperbolic distance despite small Euclidean separation?

- **Riemannian Gradient Descent**: Hyperbolic SGD/AdamW are Riemannian optimizers. The key insight is that gradients on manifolds must be transformed (via the metric tensor) before use in standard update rules.
  - Quick check question: In Riemannian optimization, why can't you directly subtract the Euclidean gradient from parameters?

- **DDPM Forward/Reverse Process**: The paper modifies the timestep sampler and loss function within the diffusion framework. You need to grasp how noise schedules and timesteps govern the forward corruption and reverse denoising.
  - Quick check question: What role does the timestep sampler play in balancing the learning signal across different noise levels?

## Architecture Onboarding

- **Component map**: DDPM U-Net -> Euclidean gradients -> Gradient Transformer (metric scaling) -> Optimizer (AdamW/SGD with transformed gradients) -> Projection Operator -> Poincaré ball parameters -> Loss function (Euclidean/Poincaré) -> Timestep sampler (linear/hyperbolic)

- **Critical path**: Initialize parameters on Poincaré ball ($||\theta_0|| < 1$) → Compute Euclidean gradient via backprop → Transform gradient using `_to_poincare()` → Apply AdamW/SGD update rules with transformed gradient → Project updated parameters back to manifold via `proj()` → (Optional) Use hyperbolic loss and hyperbolic timestep sampler

- **Design tradeoffs**: Hyperbolic vs. Euclidean loss adds computational overhead (arcosh, per-sample norm computations) but can be omitted while retaining most benefits. Hyperbolic vs. linear timesteps accelerates convergence but doesn't consistently improve final FID. SGD vs. AdamW variants show more consistent gains with SGD, suggesting adaptive moments may interact unpredictably with hyperbolic scaling.

- **Failure signatures**: Parameter explosion if projection is disabled or $\epsilon$ too small, causing NaN in arcosh computations. Slow late-stage convergence as metric scaling shrinks steps near the optimum. No improvement over Euclidean if data lacks hierarchical/latent hyperbolic structure.

- **First 3 experiments**: Baseline sanity check with standard SGD+LinearT on butterfly subset to validate environment. Ablate timestep sampler comparing HyperSGD+LinearT vs. HyperSGD+HyperT. Test on non-hierarchical data (CIFAR-10) to check if gains disappear, probing data-geometry alignment assumption.

## Open Questions the Paper Calls Out

- **Generalizability**: Do hyperbolic optimizers provide consistent benefits across diverse datasets and tasks beyond diffusion-based image generation? The paper only verifies findings on one dataset and one model type, suggesting potential applicability to classification, detection, segmentation, and LLMs.

- **SGD vs. AdamW consistency**: Why are benefits more consistent with Hyperbolic SGD than with Hyperbolic AdamW? The paper notes HyperAdamW doesn't show significant improvements, but doesn't analyze how adaptive momentum interacts with Riemannian gradient transformation.

## Limitations

- Experiments limited to one butterfly image dataset (1000 images) and only DDPM training, restricting generalizability claims
- Theoretical mechanism for hyperbolic timestep sampling remains unexplained
- No systematic ablation on dataset hierarchy or architecture details to assess what drives improvements

## Confidence

- **Hyperbolic gradient scaling mechanism**: Medium — supported by geometric intuition but limited empirical validation beyond one domain
- **Hyperbolic timestep sampler benefits**: Low — mechanism unexplained and benefits are inconsistent (acceleration without FID improvement)
- **Projection necessity**: High — standard Riemannian practice with clear failure modes if omitted
- **Early-stage convergence claims**: Medium — observed on butterfly data but unverified across datasets
- **Final FID improvements**: Low — results show acceleration but not consistent FID reduction

## Next Checks

1. **Dataset hierarchy probe**: Apply HyperSGD to a synthetic dataset with known hierarchical structure vs. a flat dataset (CIFAR-10). If gains vanish on flat data, it validates the geometric-alignment hypothesis.

2. **Architecture ablation**: Replace the DDPM backbone with a simpler diffusion model or VAE while keeping hyperbolic optimization. If improvements persist, they're optimizer-driven rather than model-specific.

3. **Gradient scaling ablation**: Implement a non-geometric adaptive step-size schedule (warmup + cosine decay) and compare against hyperbolic scaling. If performance matches, the benefits may come from scheduling rather than hyperbolic geometry.