---
ver: rpa2
title: Variational Task Vector Composition
arxiv_id: '2509.18208'
source_url: https://arxiv.org/abs/2509.18208
tags:
- task
- composition
- gated
- variational
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a variational inference framework for task
  vector composition that addresses three key challenges: lack of uncertainty quantification
  in deterministic composition methods, inability to capture sample-level variability,
  and structural redundancy in task vector spaces. The core idea is to treat composition
  coefficients as latent variables within a Bayesian framework, using an amortized
  inference network to model sample-specific posteriors.'
---

# Variational Task Vector Composition

## Quick Facts
- arXiv ID: 2509.18208
- Source URL: https://arxiv.org/abs/2509.18208
- Authors: Boyuan Zhang; Yingjun Du; Xiantong Zhen; Ling Shao
- Reference count: 40
- Key outcome: Introduces variational inference framework for task vector composition that quantifies uncertainty, captures sample-level variability, and reduces structural redundancy

## Executive Summary
This paper addresses critical limitations in task vector composition methods by introducing a variational inference framework that treats composition coefficients as latent variables. The approach combines an amortized inference network to model sample-specific posteriors with a Spike-and-Slab prior to promote sparsity and reduce redundancy in task vector spaces. A gated sampling mechanism deterministically selects reliable components based on uncertainty estimates, avoiding the high variance of stochastic sampling. Extensive experiments across eight image classification datasets with multiple ViT architectures demonstrate consistent performance improvements over existing methods, particularly on datasets with high variability and cross-domain differences.

## Method Summary
The framework models composition coefficients as latent variables within a Bayesian setting, using an amortized inference network to approximate the posterior distribution conditioned on sample features. To handle redundancy, coefficients are modeled as mixtures of zero-valued spikes and Gaussian slabs (Spike-and-Slab prior), encouraging sparsity. The model employs a gated sampling mechanism that deterministically selects task components based on their estimated uncertainty and importance, avoiding the high variance of stochastic sampling. This approach quantifies uncertainty in composition decisions while capturing sample-specific variability, enabling more effective integration of multiple task vectors compared to deterministic methods.

## Key Results
- Achieves consistent performance improvements over existing methods across eight image classification datasets
- Demonstrates particular gains on datasets with high variability and cross-domain differences
- Reduces structural redundancy in task vector spaces through the Spike-and-Slab prior mechanism
- Provides uncertainty quantification for composition decisions, enabling more reliable integration of task vectors

## Why This Works (Mechanism)
The framework works by addressing three fundamental limitations of existing task vector composition methods. First, by treating composition coefficients as latent variables within a variational inference framework, it quantifies uncertainty in how different task vectors should be combined for each sample. Second, the amortized inference network captures sample-level variability by learning to predict posterior distributions conditioned on individual sample features, rather than using fixed global coefficients. Third, the Spike-and-Slab prior explicitly models coefficients as mixtures of zero-valued spikes and Gaussian slabs, promoting sparsity and eliminating redundant components that would otherwise dilute the composition process. The gated sampling mechanism then deterministically selects the most reliable components based on their uncertainty and importance estimates, avoiding the high variance of traditional stochastic sampling while still maintaining flexibility in the composition process.

## Foundational Learning

**Variational Inference**
- Why needed: To quantify uncertainty in composition coefficients and capture sample-level variability
- Quick check: Verify that the ELBO (Evidence Lower Bound) optimization is correctly implemented and converging

**Amortized Inference**
- Why needed: To efficiently approximate posterior distributions for individual samples without expensive per-sample optimization
- Quick check: Ensure the inference network generalizes well across different samples and datasets

**Spike-and-Slab Priors**
- Why needed: To promote sparsity and eliminate redundant task vector components in the composition process
- Quick check: Monitor the proportion of zero-valued coefficients and verify they correspond to redundant components

**Gated Sampling Mechanisms**
- Why needed: To deterministically select reliable components while avoiding the high variance of stochastic sampling
- Quick check: Validate that the gating mechanism correctly identifies and weights important components

## Architecture Onboarding

**Component Map**
Input features → Amortized Inference Network → Posterior Distribution → Gated Sampling → Composition Coefficients → Task Vector Composition → Output

**Critical Path**
The critical path flows from input features through the amortized inference network to generate posterior distributions, which are then processed by the gated sampling mechanism to select composition coefficients. These coefficients are applied to combine task vectors, producing the final output. The Spike-and-Slab prior operates within the inference network during training to encourage sparsity.

**Design Tradeoffs**
The framework trades computational overhead from the inference network against improved accuracy and uncertainty quantification. The Spike-and-Slab prior adds complexity but reduces redundancy. Gated sampling sacrifices some stochastic exploration for more stable and reliable composition decisions.

**Failure Signatures**
- Poor performance on samples with high uncertainty may indicate issues with the inference network's ability to capture complex posterior distributions
- Excessive non-zero coefficients suggest the Spike-and-Slab prior is not effectively promoting sparsity
- Inconsistent results across similar samples may indicate problems with the gated sampling mechanism's reliability estimates

**First Experiments**
1. Compare performance with and without the Spike-and-Slab prior to isolate its impact on redundancy reduction
2. Evaluate the effect of removing the gated sampling mechanism (using standard stochastic sampling) to measure its contribution to stability
3. Test the framework on a simple binary classification task to verify basic functionality before scaling to more complex datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the variational task vector composition framework generalize effectively to Large Language Models (LLMs) and Convolutional Neural Networks (CNNs)?
- Basis in paper: [explicit] The authors state in the limitations section that "exploring the applicability of our framework to other architectures (e.g., convolutional networks or language models) remains an interesting direction."
- Why unresolved: The experimental validation is strictly limited to Vision Transformer (ViT) architectures under the CLIP framework, leaving the behavior of the Spike-and-Slab prior and gated sampling in different structural domains unknown.
- What evidence would resolve it: Successful application of the variational inference and gating mechanisms to NLP benchmarks (using LLMs) or standard CNN benchmarks, demonstrating maintained sparsity and accuracy improvements.

### Open Question 2
- Question: Is the proposed variational framework applicable to non-additive task arithmetic operations, such as task negation or forgetting?
- Basis in paper: [explicit] The paper notes in the limitations that it "primarily focuses on this operation [task addition] and does not explore other forms of task arithmetic."
- Why unresolved: While addition integrates knowledge, operations like negation are critical for unlearning or bias removal; it is unclear if the current sample-specific coefficient estimation handles the inverse directional gradients required for negation.
- What evidence would resolve it: Experiments demonstrating that the learned composition coefficients can effectively scale or invert task vectors to remove specific capabilities or biases without degrading overall model performance.

### Open Question 3
- Question: What computational optimizations are required to scale the sample-specific inference to foundation models with billions of parameters?
- Basis in paper: [explicit] The authors identify "high GPU memory requirements for combining multiple task vectors" as a constraint that "limit our ability to conduct experiments on larger-scale model architectures."
- Why unresolved: The current method relies on precomputed features and struggles with the memory footprint of multiple task vectors, suggesting the proposed amortized inference network may not scale naively to very large models.
- What evidence would resolve it: Engineering solutions (e.g., quantization, offloading, or low-rank adaptation of the task vectors) that preserve the benefits of the variational posterior while fitting within feasible memory budgets for models larger than ViT-L/14.

## Limitations
- Spike-and-Slab prior introduces additional hyperparameters requiring careful tuning across different datasets and architectures
- Computational overhead of the amortized inference network may pose challenges for extremely large-scale models
- Current implementation focuses primarily on image classification tasks with ViT architectures, limiting cross-domain validation
- Gated sampling mechanism's deterministic selection may miss potentially valuable stochastic variations in certain scenarios

## Confidence

**High Confidence**: The core variational inference framework and its ability to quantify uncertainty in task vector composition is well-supported by both theoretical analysis and experimental results.

**Medium Confidence**: The effectiveness of the Spike-and-Slab prior for reducing redundancy shows consistent improvements but may vary depending on the specific characteristics of task vector spaces.

**Medium Confidence**: The gated sampling mechanism's superiority over stochastic sampling is demonstrated across multiple datasets, though the benefits may be less pronounced in scenarios with lower task vector variability.

## Next Checks

1. Test the framework's performance on non-vision domains (e.g., NLP tasks with transformer models) to assess cross-domain generalization capabilities.

2. Conduct ablation studies specifically isolating the impact of the Spike-and-Slab prior on different types of task vector redundancies (structural vs. semantic).

3. Evaluate the computational efficiency trade-offs by scaling the framework to larger model architectures (e.g., GPT-3 sized models) and measuring both inference time and memory requirements.