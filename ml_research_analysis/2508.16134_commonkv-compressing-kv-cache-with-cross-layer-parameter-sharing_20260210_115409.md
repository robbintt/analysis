---
ver: rpa2
title: 'CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing'
arxiv_id: '2508.16134'
source_url: https://arxiv.org/abs/2508.16134
tags:
- cache
- compression
- cross-layer
- sharing
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CommonKV, a training-free method for cross-layer
  KV cache compression in large language models. The key idea is to improve consistency
  of cross-layer KV cache by sharing weight matrices across adjacent layers using
  Singular Value Decomposition (SVD), then applying adaptive budget allocation based
  on cosine similarity to prevent over-compression of dissimilar caches.
---

# CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing

## Quick Facts
- arXiv ID: 2508.16134
- Source URL: https://arxiv.org/abs/2508.16134
- Reference count: 40
- Key outcome: Maintains 95% performance at 50% compression ratio on long-context benchmarks using training-free cross-layer KV cache compression

## Executive Summary
CommonKV introduces a training-free method for compressing KV cache in large language models by sharing weight matrices across adjacent layers. The approach leverages Singular Value Decomposition (SVD) to enhance cross-layer KV cache consistency and applies adaptive budget allocation based on cosine similarity to prevent over-compression of dissimilar caches. This method achieves significant compression ratios (up to 98%) while maintaining high model performance, outperforming existing compression techniques on standard benchmarks.

## Method Summary
CommonKV works by sharing weight matrices across adjacent layers of a transformer model to improve consistency of cross-layer KV cache. The method uses Singular Value Decomposition (SVD) to identify and compress redundant information in the KV cache. An adaptive budget allocation mechanism based on cosine similarity between adjacent layers ensures that compression is applied judiciously, preventing over-compression when caches are dissimilar. This training-free approach is designed to be orthogonal to other compression methods like quantization and eviction, allowing for synergistic integration.

## Key Results
- Maintains 95% performance at 50% compression ratio on long-context benchmarks
- Achieves 98% compression when integrated with quantization and eviction techniques
- Outperforms existing KV cache compression methods across standard benchmarks

## Why This Works (Mechanism)
The core mechanism relies on exploiting the inherent redundancy across adjacent transformer layers. By sharing weight matrices, CommonKV reduces the storage requirements for KV cache while maintaining the essential information needed for attention computations. The adaptive budget allocation prevents aggressive compression that could degrade performance when layer outputs are not sufficiently similar. This approach effectively trades a small amount of computational overhead for substantial memory savings, making it particularly valuable for long-context processing.

## Foundational Learning
- **KV cache in transformers**: Stores key and value vectors for each token to avoid recomputation during autoregressive generation
  - Why needed: Understanding what KV cache is and why it consumes significant memory
  - Quick check: Verify that KV cache size scales linearly with sequence length and model size

- **Singular Value Decomposition (SVD)**: Matrix factorization technique that identifies principal components
  - Why needed: The method uses SVD to analyze and compress KV cache matrices
  - Quick check: Confirm SVD can effectively identify redundant information in KV cache

- **Cosine similarity**: Measures similarity between vectors in high-dimensional space
  - Why needed: Used for adaptive budget allocation to prevent over-compression
  - Quick check: Validate that cosine similarity effectively measures cache similarity

- **Cross-layer parameter sharing**: Technique where weights are shared across different layers
  - Why needed: Core concept enabling the compression approach
  - Quick check: Ensure shared parameters don't degrade model performance

## Architecture Onboarding

**Component Map**: Input tokens -> Embedding -> Transformer layers (with shared weights) -> SVD-based compression -> Adaptive budget allocation -> Output generation

**Critical Path**: Token generation requires KV cache computation → SVD analysis → budget allocation → attention with compressed cache

**Design Tradeoffs**: Memory savings vs. computation overhead, compression ratio vs. performance retention, training-free approach vs. potential fine-tuning benefits

**Failure Signatures**: Performance degradation when compression ratio exceeds 50-75%, increased latency due to SVD computations, inconsistent results across different model architectures

**First Experiments**: 1) Benchmark performance at varying compression ratios (25%, 50%, 75%) 2) Compare against baseline KV cache methods on standard long-context datasets 3) Measure inference latency overhead with and without SVD-based compression

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation scope focused primarily on standard benchmarks rather than real-world workloads
- SVD computational overhead may become significant at scale despite claims of minimal impact
- Orthogonality to other compression methods needs more rigorous empirical validation
- Performance at extremely high compression ratios (>75%) and very long contexts (>100K tokens) remains unexplored

## Confidence
- Cross-layer KV cache consistency improvement: High
- 95% performance at 50% compression: High (within tested benchmarks)
- Orthogonality to existing methods: Medium
- Minimal inference overhead: Medium (needs production validation)
- 98% compression without significant loss: Low (limited experimental scope)

## Next Checks
1. Conduct comprehensive end-to-end latency measurements on production hardware, comparing CommonKV against existing methods under realistic concurrent request patterns and varying batch sizes.

2. Evaluate performance degradation across the full compression spectrum (25%, 50%, 75%, 90%) on both standard benchmarks and real-world conversational datasets to identify breaking points.

3. Test cross-architectural compatibility by applying CommonKV to models with different attention mechanisms (e.g., linear attention, gated attention) and varying numbers of layers to assess generalizability.