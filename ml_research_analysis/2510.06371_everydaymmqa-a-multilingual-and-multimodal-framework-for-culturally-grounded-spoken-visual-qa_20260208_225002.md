---
ver: rpa2
title: 'EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded
  Spoken Visual QA'
arxiv_id: '2510.06371'
source_url: https://arxiv.org/abs/2510.06371
tags:
- image
- question
- text
- speech
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces EverydayMMQA, a framework for creating large-scale,
  culturally grounded datasets for spoken and visual question answering (SVQA), and
  demonstrates its use by developing OASIS, a multimodal dataset with ~0.92M images
  and 14.8M QA pairs across 18 Arabic-speaking countries in English, Modern Standard
  Arabic (MSA), and regional dialects. OASIS includes 3.7M spoken questions, supporting
  four input combinations: speech-only, text-only, speech+image, and text+image, and
  tests models on culturally aware reasoning beyond object recognition.'
---

# EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA

## Quick Facts
- **arXiv ID**: 2510.06371
- **Source URL**: https://arxiv.org/abs/2510.06371
- **Reference count**: 40
- **Primary result**: Introduces EverydayMMQA framework; creates OASIS dataset (~0.92M images, 14.8M QA pairs) across 18 Arabic-speaking countries in 3 languages; shows visual grounding improves performance and narrows cross-lingual gaps.

## Executive Summary
This work presents EverydayMMQA, a framework for creating large-scale, culturally grounded datasets for spoken and visual question answering (SVQA). Using this framework, the authors develop OASIS, a multimodal dataset spanning 18 Arabic-speaking countries with images, QA pairs, and spoken questions in English, Modern Standard Arabic, and regional dialects. The dataset supports four input modalities (speech-only, text-only, speech+image, text+image) and tests models on culturally aware reasoning beyond object recognition. Benchmarking with both closed and open models, and one fine-tuned model, demonstrates that visual grounding consistently improves performance across all models and languages, narrowing cross-lingual and dialectal disparities.

## Method Summary
The EverydayMMQA framework generates culturally grounded multimodal datasets through a pipeline of seven modules: topic/query generation, image retrieval, image filtering and metadata extraction, QA generation, speech generation, translation, and quality checking. The framework was used to create OASIS, a dataset with ~0.92M images and 14.8M QA pairs across 18 Arabic-speaking countries. QA pairs include open-ended, multiple-choice, and true/false questions in English, MSA, Egyptian, and Levantine Arabic. Speech versions were generated using TTS models and human recordings. The dataset was evaluated using zero-shot and fine-tuned models, with visual grounding shown to consistently improve performance across languages and dialects.

## Key Results
- Visual grounding consistently improves model performance across all languages and dialects, narrowing cross-lingual disparities.
- Fine-tuning a smaller multimodal model (Qwen2.5-3B) on the OASIS dataset makes it competitive with much larger models, especially for speech inputs.
- A mix of QA types (Open-Ended, MCQ, True/False) provides a more complete evaluation of model capabilities than any single type alone.

## Why This Works (Mechanism)

### Mechanism 1: Visual Grounding as a Modality Equalizer
- **Claim**: Providing images alongside text or speech queries systematically improves model performance across languages and dialects, narrowing cross-lingual gaps.
- **Mechanism**: Visual evidence disambiguates referents and constrains the plausible answer space, offloading linguistic burden. This shifts the primary bottleneck from language understanding (harder for low-resource languages/dialects) to faithful answer generation based on visual evidence.
- **Core assumption**: The linguistic ambiguity inherent in text-only queries, particularly for morphologically rich or under-resourced languages, is a primary source of errors that can be resolved by visual context.
- **Evidence anchors**: [abstract] "visual grounding consistently improves performance across all models and languages, narrowing cross-lingual and dialectal disparities"; [section 4, Results] "Visual grounding mitigates this 'language burden', anchoring the context and constraining answers... As a result, the dialectal gap to MSA substantially narrows."
- **Break condition**: If images are not culturally relevant or are of poor quality, visual grounding may introduce noise. If the question requires abstract cultural knowledge not present in the image, visual grounding will not help.

### Mechanism 2: Fine-tuning for Cross-Modal Alignment
- **Claim**: Fine-tuning a smaller multimodal model on a culturally grounded, multimodal dataset can elevate its performance to be competitive with much larger models, particularly for speech inputs.
- **Mechanism**: Targeted training on the specific task and data distribution improves the model's internal alignment between speech, text, and image representations. This stabilizes cross-modal fusion and makes the model more robust to input noise (e.g., ASR errors, accents).
- **Core assumption**: The performance gap between large and small models on this task is primarily due to a lack of domain-specific knowledge and poor cross-modal alignment, rather than a fundamental lack of reasoning capacity.
- **Evidence anchors**: [abstract] "compact models approaching larger systems when fine-tuned."; [section 4, Results] "Fine-tuning the Qwen2.5-3B model with the multimodal data transforms it into a stable multimodal responder... fine-tuning specifically stabilizes cross-modal alignment, making the small model competitive, especially with audio inputs."
- **Break condition**: If the fine-tuning data is small or not representative of the test distribution, the model may overfit. If the base model lacks foundational capacity, fine-tuning will have limited impact.

### Mechanism 3: Diverse QA Types Probe Different Capabilities
- **Claim**: A mix of question types (Open-Ended, MCQ, True/False) provides a more complete evaluation of model capabilities than any single type alone.
- **Mechanism**: Different question types impose different constraints and require different reasoning paths. MCQs test precise discrimination, True/False questions directly measure hallucination, and Open-Ended questions assess coherent, grounded generation.
- **Core assumption**: Performance on one question type does not perfectly predict performance on others; therefore, a comprehensive evaluation requires multiple types to identify specific failure modes.
- **Evidence anchors**: [section 2.4, QA Generation] "Open-ended questions allow free-form answers... MCQs provide objective, reproducible evaluation... T/F questions directly target hallucination and factual grounding."; [section 4, Results] "For evaluation, we recommend reporting ∆(Text→ Image) alongside absolute metrics, and supplementing LLM-as-judge with calibration probes... to avoid over-reliance on saturated MCQ/TF scores."
- **Break condition**: If question types are poorly constructed (e.g., MCQ distractors are too easy), they may not effectively discriminate between model capabilities.

## Foundational Learning

- **Concept: Multimodal LLMs (LMMs/MLLMs)**
  - **Why needed here**: To understand the class of models (e.g., Qwen2.5-Omni, GPT-4o) that the paper benchmarks and fine-tunes. These models are not simply text LLMs with added encoders; their power comes from the fusion of modalities.
  - **Quick check question**: Can you explain how an "omni" model like Qwen2.5-Omni differs from a text-only LLM that takes image captions as input?

- **Concept: Cross-Modal Alignment**
  - **Why needed here**: This is the core technical challenge the paper's fine-tuning approach addresses. Understanding it is crucial to interpreting why fine-tuning helps smaller models perform better.
  - **Quick check question**: What does it mean for a model to have good cross-modal alignment, and what is one symptom of poor alignment?

- **Concept: Cultural Grounding in AI**
  - **Why needed here**: This is the paper's primary motivation. It is key to grasping why a new dataset like OASIS is necessary and what differentiates it from generic VQA datasets.
  - **Quick check question**: Why would a model trained on Western-centric data fail when asked a question about a specific cultural practice in a non-Western country?

## Architecture Onboarding

- **Component map**: Topic/Query Generation -> Image Retrieval -> Image Filtering & Metadata -> QA Generation -> Speech Generation -> Translation -> Quality Checking
- **Critical path**: The most critical path for a new engineer is the data creation pipeline (modules 1-4), as the quality of the final OASIS dataset depends entirely on the image selection and QA generation. A failure here (e.g., retrieving irrelevant images, generating nonsensical questions) will invalidate downstream benchmarking and training results.
- **Design tradeoffs**: Synthetic vs. Human Data: The framework relies heavily on synthetic data (LLMs for QA, TTS for speech) for scalability, using humans only for a subset of quality checks and recordings. This trades off potential noise from synthetic data for massive scale. Model Complexity vs. Control: Using LLMs (GPT-4, Gemini) for generation provides high-quality, diverse outputs but introduces a dependency on expensive, black-box APIs. Open-source models could be swapped but might yield lower-quality data.
- **Failure signatures**: Cultural Hallucination: Generated QAs contain plausible but factually incorrect cultural details, stemming from the LLM's priors rather than the image. Dialectal Inconsistency: Translation to dialects is poor, either too formal (sounding like MSA) or grammatically incorrect. Image-Question Misalignment: Questions refer to objects or details not present in the retrieved image, indicating a failure in the image filtering or QA generation step.
- **First 3 experiments**:
  1. Image Retrieval Ablation: Run the query generation module with and without the cultural-relevance scoring filter (RLLM ≥ 80). Compare the quality of the retrieved images for a sample of countries and subcategories. This tests the necessity of the relevance scoring step.
  2. QA Generation Model Comparison: Use the same set of images and compare the quality of generated QAs (via human evaluation of a sample) from different LLMs (e.g., GPT-4 vs. an open-source model). This quantifies the tradeoff between API cost and data quality.
  3. Modality Ablation on a Small Model: Take a small, open-source multimodal model (e.g., Qwen2.5-3B) and benchmark it on a subset of the OASIS test set using all four input modalities (T, S, T+I, S+I). Reproduce the paper's finding that visual grounding acts as a modality equalizer for speech and transcripts.

## Open Questions the Paper Calls Out
- What performance gains can be achieved by fine-tuning models on the complete OASIS training dataset compared to the limited subset used? (Basis: [explicit] The authors state in the "Limitations" section and "Experiments" section that they used only 6.67% of the training data due to computational constraints, leaving the full potential of the dataset unexplored.)
- How can the speech modality be robustly extended to include regional Arabic dialects? (Basis: [explicit] In Section 2.5, the authors note they focused on English and MSA for speech generation "Due to the lack of robust Arabic dialectal TTS models," explicitly listing the extension to additional dialects as future work.)
- Can more complex, visually confounded evaluation tasks effectively differentiate model performance where current benchmarks saturate? (Basis: [explicit] In Section 4, the authors observe that "model rankings compress on simpler tasks" because accuracy reaches near-ceiling levels. They explicitly state that "Future progress requires harder, visually confounded items (e.g., occlusions).")

## Limitations
- The framework's heavy reliance on synthetic data generation via LLMs introduces potential cultural hallucination, though quality checks mitigate this. The paper does not fully address the risk of LLMs embedding their own cultural biases rather than the target cultures'.
- Fine-tuning results showing compact models approaching larger systems are based on a single fine-tuned model (Qwen2.5-3B). Generalization to other base models remains uncertain.
- The visual grounding "equalizer" effect is primarily derived from the paper's own results; related work validates cultural grounding but not specifically the dialectal gap narrowing mechanism.

## Confidence
- **High confidence**: The core framework pipeline for creating culturally grounded multimodal datasets is well-specified and reproducible. The zero-shot benchmarking methodology is clearly defined.
- **Medium confidence**: The fine-tuning methodology and results are detailed but based on limited experimentation (single base model, one epoch). The claimed improvements from visual grounding across languages/dialects are supported by strong empirical evidence from the paper's own experiments.
- **Low confidence**: The paper's assertion that visual grounding acts as a "modality equalizer" is the weakest link, as it relies heavily on the paper's own results without strong external validation from the related work corpus.

## Next Checks
1. Conduct a human evaluation study on a sample of generated questions to quantify the rate of cultural hallucination in the dataset.
2. Replicate the fine-tuning experiment with a different base model (e.g., Phi-4) to test the generalizability of the compact model performance claims.
3. Perform an ablation study on the visual grounding effect, comparing performance with and without images specifically for low-resource dialects to independently verify the "equalizer" mechanism.