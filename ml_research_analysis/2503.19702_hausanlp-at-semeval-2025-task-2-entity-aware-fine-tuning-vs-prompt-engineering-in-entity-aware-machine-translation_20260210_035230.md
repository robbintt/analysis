---
ver: rpa2
title: 'HausaNLP at SemEval-2025 Task 2: Entity-Aware Fine-tuning vs. Prompt Engineering
  in Entity-Aware Machine Translation'
arxiv_id: '2503.19702'
source_url: https://arxiv.org/abs/2503.19702
tags:
- translation
- named
- entities
- data
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates entity-aware machine translation by comparing\
  \ supervised fine-tuning of the NLLB model against prompt engineering using Google\u2019\
  s Gemini. NLLB was fine-tuned with both task training data and Wikidata-extracted\
  \ named entity translations, while Gemini was evaluated in zero-shot and few-shot\
  \ setups using structured prompts."
---

# HausaNLP at SemEval-2025 Task 2: Entity-Aware Fine-tuning vs. Prompt Engineering in Entity-Aware Machine Translation

## Quick Facts
- **arXiv ID**: 2503.19702
- **Source URL**: https://arxiv.org/abs/2503.19702
- **Reference count**: 5
- **Primary result**: Prompt engineering with Gemini outperformed NLLB fine-tuning in entity translation accuracy across 10 target languages

## Executive Summary
This paper investigates entity-aware machine translation by comparing supervised fine-tuning of the NLLB model against prompt engineering using Google's Gemini. The research team fine-tuned NLLB with task training data and Wikidata-extracted named entity translations, while evaluating Gemini in zero-shot and few-shot setups using structured prompts. Testing across 10 target languages (Arabic, German, Spanish, French, Italian, Japanese, Korean, Thai, Turkish, Chinese), the study found that Gemini consistently outperformed NLLB in entity translation accuracy, with minimal difference between zero-shot and few-shot prompting. Spanish and Italian achieved the highest scores, while Chinese performed the poorest, demonstrating that prompt engineering with large language models can effectively handle named entity translation without extensive fine-tuning.

## Method Summary
The study compared two approaches to entity-aware machine translation: supervised fine-tuning of NLLB and prompt engineering with Gemini. NLLB was fine-tuned using the task training data combined with Wikidata-extracted named entity translations to improve its handling of named entities. For Gemini, the team employed structured prompts in both zero-shot and few-shot configurations, testing how well the model could translate entities without task-specific training. The evaluation covered 10 target languages spanning different linguistic families and scripts, measuring translation quality using COMET and entity accuracy through M-ETA metrics.

## Key Results
- Gemini outperformed NLLB in entity translation accuracy across all 10 target languages
- Minimal difference between zero-shot and few-shot prompting for Gemini's performance
- Spanish and Italian achieved the highest entity translation scores, while Chinese performed the poorest

## Why This Works (Mechanism)
Assumption: Gemini's superior performance in entity translation likely stems from its pre-trained multilingual capabilities and strong contextual understanding, which allow it to handle named entities across different languages without requiring task-specific fine-tuning. The model's ability to leverage contextual information from the prompt may help disambiguate entity translations, particularly for entities with multiple possible translations or transliterations. This suggests that large language models with broad pretraining may have inherent advantages for cross-lingual entity handling compared to models that require domain-specific adaptation.

## Foundational Learning
Unknown: The paper does not explicitly discuss what foundational learning principles from NLP research informed this work. The comparison between fine-tuning and prompt engineering approaches builds on established knowledge about model adaptation strategies, while the use of structured prompts for entity translation likely draws from prompt engineering literature in the LLM community. The integration of Wikidata for named entity data follows established practices in entity linking and knowledge base integration for NLP tasks.

## Architecture Onboarding
**Component Map**: Wikidata extraction -> NLLB fine-tuning -> COMET evaluation -> M-ETA scoring; Gemini zero-shot/few-shot prompting -> structured prompts -> output generation -> COMET evaluation -> M-ETA scoring

**Critical Path**: Entity extraction and translation data preparation -> Model training or prompt configuration -> Translation generation -> Quality evaluation using COMET and M-ETA metrics

**Design Tradeoffs**: Fine-tuning requires extensive training data and computational resources but can be optimized for specific tasks, while prompt engineering leverages existing large models without retraining but depends heavily on prompt quality and may have limitations in specialized domains

**Failure Signatures**: Poor entity translation accuracy indicates inadequate training data coverage or prompt effectiveness; language-specific performance drops suggest linguistic distance challenges or script-related issues; minimal improvement from few-shot to zero-shot suggests prompt engineering limitations

**First Experiments**:
1. Test entity translation accuracy on a held-out validation set to verify model performance
2. Compare prompt variations (zero-shot vs few-shot) to determine optimal configuration
3. Analyze translation outputs for semantic preservation alongside entity accuracy

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly identify open questions. However, the observed performance differences across languages suggest potential research directions regarding how linguistic distance, script differences, and entity types affect translation accuracy. The minimal difference between zero-shot and few-shot prompting also raises questions about the limits of prompt engineering for specialized translation tasks.

## Limitations
- The study only evaluates translation accuracy and entity recognition without assessing semantic preservation or fluency in the target languages
- Comparison is limited to one supervised fine-tuned model (NLLB) and one LLM (Gemini), preventing generalization about other architectures
- Impact of entity types on translation difficulty is not analyzed
- The training data for NLLB and Wikidata extraction methods are not described
- Prompt engineering methodology lacks detail on optimal prompt structures

## Confidence
- **High Confidence**: Gemini's superior entity translation accuracy compared to NLLB; minimal difference between zero-shot and few-shot performance for Gemini
- **Medium Confidence**: Relative performance across language pairs (Spanish/Italian high, Chinese low); effectiveness of prompt engineering for named entity translation
- **Low Confidence**: Generalizability to other language pairs beyond the 10 tested; whether prompt engineering would remain effective for languages with different scripts or linguistic structures

## Next Checks
1. Evaluate semantic preservation and fluency metrics alongside entity accuracy to determine if higher entity accuracy comes at the cost of overall translation quality
2. Test additional fine-tuning approaches and LLM architectures to verify whether the observed superiority of prompt engineering is consistent across different model types
3. Conduct error analysis to identify which entity types (person names, organizations, locations, etc.) are most challenging for different language pairs and whether this correlates with linguistic distance from English