---
ver: rpa2
title: Optimization of embeddings storage for RAG systems using quantization and dimensionality
  reduction techniques
arxiv_id: '2505.00105'
source_url: https://arxiv.org/abs/2505.00105
tags:
- performance
- quantization
- reduction
- dimensionality
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of high memory usage in RAG systems
  due to large-scale storage of high-dimensional embeddings. It investigates the use
  of quantization and dimensionality reduction techniques to optimize embedding storage
  while minimizing performance degradation.
---

# Optimization of embeddings storage for RAG systems using quantization and dimensionality reduction techniques

## Quick Facts
- arXiv ID: 2505.00105
- Source URL: https://arxiv.org/abs/2505.00105
- Reference count: 15
- Primary result: Float8 quantization with PCA achieves 8x compression with minimal performance loss in RAG systems

## Executive Summary
This paper addresses the significant memory overhead in Retrieval-Augmented Generation (RAG) systems caused by large-scale storage of high-dimensional embeddings. The authors systematically evaluate quantization techniques (float16, float8, int8, binary) and dimensionality reduction methods (PCA, Kernel PCA, UMAP, Autoencoders, Random Projections) to optimize embedding storage while preserving retrieval performance. The study provides a comprehensive analysis of trade-offs between compression ratios and system performance, offering practical guidance for RAG system optimization.

## Method Summary
The authors conduct experiments using the Hugging Face BGE-M3 model and the MTEB retrieval task to evaluate various storage optimization techniques. They test quantization formats at multiple precision levels and dimensionality reduction methods both independently and in combination. The evaluation includes measuring storage savings, retrieval accuracy, and performance degradation across different compression configurations. A visualization-based methodology is proposed to help practitioners select optimal configurations based on their specific memory constraints and performance requirements.

## Key Results
- Float8 quantization achieves 4x storage reduction with minimal performance degradation (<0.3%) compared to int8 at the same compression level
- PCA outperforms other dimensionality reduction techniques in balancing storage reduction and performance retention
- Combining PCA with float8 quantization yields 8x total compression with less performance impact than int8 alone
- The proposed visualization methodology effectively helps practitioners select optimal storage-performance trade-offs

## Why This Works (Mechanism)
Quantization reduces storage by representing numerical values with fewer bits while preserving essential information. Lower-precision formats like float8 and int8 compress embeddings by reducing the bit-width of stored values. Dimensionality reduction techniques like PCA project high-dimensional embeddings into lower-dimensional spaces while retaining maximum variance, effectively compressing the data. The combination of these techniques is effective because quantization reduces storage per dimension while dimensionality reduction reduces the number of dimensions, creating multiplicative compression effects.

## Foundational Learning
- **Vector Embeddings**: Dense numerical representations capturing semantic meaning, needed for understanding RAG retrieval mechanisms
  - Quick check: Can you explain how embeddings enable semantic similarity search?
- **Quantization**: Process of reducing numerical precision to save storage, essential for understanding compression techniques
  - Quick check: What's the difference between float8 and int8 quantization in terms of information preservation?
- **Dimensionality Reduction**: Techniques for projecting data into lower-dimensional spaces, crucial for understanding PCA and related methods
  - Quick check: How does PCA determine which dimensions to preserve?
- **Retrieval-Augmented Generation**: RAG systems combine retrieval with generation, foundational for understanding the application context
  - Quick check: What role do embeddings play in the retrieval phase of RAG systems?
- **Storage-Performance Trade-offs**: The balance between compression efficiency and system effectiveness, central to this research
  - Quick check: How would you measure the impact of compression on retrieval quality?

## Architecture Onboarding

**Component Map**
- Raw Embeddings -> Quantization/Reduction Pipeline -> Compressed Embeddings -> Retrieval System -> Generated Output

**Critical Path**
1. Input document embedding generation
2. Storage optimization (quantization/dimensionality reduction)
3. Compressed embedding storage
4. Retrieval query processing
5. Embedding comparison and ranking
6. Document retrieval for generation

**Design Tradeoffs**
- Precision vs. storage: Higher precision (float16) preserves more information but uses more memory
- Dimensionality vs. expressiveness: Lower dimensions reduce storage but may lose semantic information
- Computational overhead vs. compression gains: Some techniques require additional processing time

**Failure Signatures**
- Excessive performance degradation indicating over-compression
- Retrieval of semantically irrelevant documents suggesting information loss
- Increased latency from computational overhead of compression/decompression

**3 First Experiments**
1. Compare retrieval accuracy using original vs. float8-quantized embeddings on a small dataset
2. Test PCA dimensionality reduction at various component counts to find optimal compression ratio
3. Evaluate combined PCA + float8 configuration against baseline for storage savings and performance impact

## Open Questions the Paper Calls Out
None

## Limitations
- Results are based on a specific dataset and model configuration, limiting generalizability to other RAG applications
- Focus on storage optimization without extensive evaluation of retrieval latency or computational overhead during inference
- Evaluation metrics may not fully capture all aspects of retrieval quality in domain-specific contexts

## Confidence

**High confidence:**
- Float8 quantization provides superior storage efficiency compared to int8 at equivalent performance levels

**Medium confidence:**
- PCA demonstrates clear advantages over alternative dimensionality reduction techniques for this specific use case
- The additive benefits of combining PCA with float8 quantization for achieving 8x compression

## Next Checks

1. Evaluate the proposed methodology across multiple diverse datasets (biomedical, legal, general web) to verify consistency of compression-performance trade-offs

2. Measure end-to-end inference latency and memory bandwidth usage for the recommended configurations in real-time RAG deployment scenarios

3. Conduct ablation studies to determine the minimum viable embedding dimensions that maintain acceptable performance thresholds for specific use cases