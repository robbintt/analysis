---
ver: rpa2
title: 'REWARD CONSISTENCY: Improving Multi-Objective Alignment from a Data-Centric
  Perspective'
arxiv_id: '2504.11337'
source_url: https://arxiv.org/abs/2504.11337
tags:
- reward
- preference
- alignment
- objectives
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reward Consistency (RC), a principle for
  identifying data samples that align with multiple preference objectives simultaneously.
  The key insight is that samples satisfying RC constrain gradient divergence during
  optimization, reducing conflicts between competing objectives like helpfulness and
  harmlessness.
---

# REWARD CONSISTENCY: Improving Multi-Objective Alignment from a Data-Centric Perspective

## Quick Facts
- **arXiv ID:** 2504.11337
- **Source URL:** https://arxiv.org/abs/2504.11337
- **Reference count:** 15
- **Primary result:** RCS achieves 13.37% average improvement in both harmless rate and helpfulness win rate compared to original datasets

## Executive Summary
This paper introduces Reward Consistency (RC), a principle for identifying data samples that align with multiple preference objectives simultaneously. The key insight is that samples satisfying RC constrain gradient divergence during optimization, reducing conflicts between competing objectives like helpfulness and harmlessness. The authors propose Reward Consistency Sampling (RCS), a framework that automatically constructs preference datasets by filtering response pairs through RC criteria. Experiments show that RCS-trained models achieve an average 13.37% improvement in both harmless rate and helpfulness win rate compared to using original datasets, while maintaining consistent performance across varying multi-objective scenarios.

## Method Summary
RCS is a data-centric framework that reconstructs preference datasets for multi-objective DPO alignment. For each prompt, it samples n additional responses, scores all responses across all objectives using reward models, filters for reward-consistent pairs (where the chosen response wins on all objectives), and selects the pair with the largest reward gap for the current optimization objective. The method works with both implicit rewards (from DPO reference models) and explicit reward models, and offers flexible control over which objectives to prioritize. RCS is designed to work with DPO-family algorithms (DPO, MODPO, SPO) by ensuring gradient compatibility across objectives.

## Key Results
- RCS-trained models achieve 13.37% average improvement in both harmless rate and helpfulness win rate compared to original datasets
- The method maintains consistent performance across varying multi-objective scenarios with different objective combinations
- RCS works effectively with both implicit and explicit reward signals, with explicit models showing slight advantages for helpfulness

## Why This Works (Mechanism)

### Mechanism 1: Gradient Non-Conflict from Reward Consistency
Claim: Samples satisfying reward consistency produce gradients that do not conflict across objectives during multi-objective DPO optimization. When a preference pair satisfies rj(x, yw) > rj(x, yl) for all objectives j, the additional gradient term introduced by objective 2 has non-negative dot product with G1, preventing optimization from degrading one objective while improving another.

### Mechanism 2: Response Pool Expansion Enables RC-Compliant Pair Discovery
Claim: Expanding the candidate response pool per prompt increases the probability of finding reward-consistent pairs that would not exist in the original dataset. By sampling n additional responses per prompt and combining with original yw, yl, the framework creates an expanded set where reward annotation across all K objectives enables filtering for pairs where the chosen response wins across all dimensions.

### Mechanism 3: Maximal Reward Gap Selection Preserves Learning Signal
Claim: Among RC-compliant candidate pairs, selecting the pair with the largest reward gap for the current optimization objective maintains strong gradient signal while avoiding conflicts. RC filtering alone may select pairs with marginal differences on the current objective, reducing learning efficiency. The two-stage selection ensures the chosen pair is both conflict-free and provides a clear preference signal.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: Understanding DPO's closed-form loss function is essential to grasp why RC affects gradient behavior. Quick check: Can you explain why DPO avoids training an explicit reward model, and where the "implicit reward" comes from?

- **Gradient Conflict in Multi-Objective Optimization**: Understanding cosine similarity between gradient vectors is prerequisite to Lemma 1. Quick check: If two objectives have gradients pointing in opposite directions, what happens to each objective when you take a gradient step?

- **Reward Model Types (Implicit vs. Explicit)**: RCS supports both implicit reward signals (derived from DPO reference models) and explicit reward models (trained separately). Quick check: What is the trade-off between using an implicit reward model (no additional training) versus an explicit reward model (potentially more accurate)?

## Architecture Onboarding

- **Component map:**
  ```
  Input: Original preference dataset Dk for objective k
         ↓
  [Response Sampler] → Sample n responses per prompt using base LLM
         ↓
  [Reward Annotator] → Score all responses on all K objectives using r1...rk
         ↓
  [RC Filter] → Identify pairs (y'w, y'l) where rj(x, y'w) > rj(x, y'l) ∀j
         ↓
  [Max-Gap Selector] → Among RC pairs, select pair with max rk(y'w) - rk(y'l)
         ↓
  Output: Reconstructed preference dataset D'k
  ```

- **Critical path:** The RC Filter is the correctness-critical component. Incorrect reward model scoring or filtering logic will produce pairs that appear RC-compliant but introduce conflicts during training.

- **Design tradeoffs:**
  - Sampling number n: Higher n increases RC pair yield but costs more inference. Paper uses n=8; Figure 4 shows diminishing returns.
  - Implicit vs. explicit reward models: Implicit avoids extra training; explicit may generalize better.
  - Selective RC: Can relax consistency on some objectives for application-specific control.

- **Failure signatures:**
  - High "failed number" to find RC pairs: Indicates sampling number too low or LLM response distribution too narrow.
  - Significant harmless rate degradation after helpful training: RC filtering may be incorrect or reward models misaligned.
  - Low helpful win rate despite RC: Max-gap selection may be selecting pairs with other issues.

- **First 3 experiments:**
  1. Validate RC gradient property by training on RC-only vs. non-RC subsets and measuring conflict (harmless degradation).
  2. Ablate sampling number by running RCS with n∈{2,4,8,16} and plotting failed-to-find-RC rate.
  3. Compare implicit vs. explicit reward models by running identical RCS pipeline with both.

## Open Questions the Paper Calls Out

### Open Question 1
Does the probability of finding reward-consistent samples degrade significantly as the number of preference objectives (K) increases, potentially making RCS infeasible for complex alignment? As constraints accumulate, the intersection of responses satisfying all reward requirements might vanish or require excessive sampling.

### Open Question 2
Can Reward Consistency Sampling be effectively adapted to online iterative DPO frameworks where the reference model and reward distribution evolve dynamically? RCS currently relies on fixed reward models and a static reference model; dynamic policy shifts could destabilize the consistency checks.

### Open Question 3
Is the RCS framework robust across different LLM architectures and parameter scales, particularly for smaller models with less capable implicit reward modeling? The framework was validated solely on Llama-3-SFT; smaller models may struggle to generate the diverse, high-quality samples required for the initial sampling step.

## Limitations
- **Sampling Dependency:** RCS performance critically depends on the base LLM's ability to generate diverse responses across objectives.
- **Reward Model Quality:** The method's effectiveness hinges on accurate reward scoring across all objectives.
- **Computational Overhead:** RCS requires significant additional inference to sample and score multiple responses per prompt.

## Confidence
- **High Confidence:** The core mathematical proof establishing that reward-consistent samples prevent gradient conflicts is sound and well-derived.
- **Medium Confidence:** Empirical results showing 13.37% average improvement are convincing but may be sensitive to specific datasets and reward models used.
- **Medium Confidence:** The claim that sampling more responses increases RC pair yield is supported by Figure 4 but lacks extensive validation across different model scales and domains.

## Next Checks
1. **Generalization Test:** Apply RCS to a different multi-objective alignment scenario (e.g., creativity vs. factuality) to verify the method transfers beyond helpfulness/harmlessness.
2. **Scaling Study:** Evaluate RCS performance across different model sizes (7B, 13B, 34B) to identify where sampling costs become prohibitive.
3. **Robustness to Reward Errors:** Intentionally perturb reward model outputs and measure how much error RCS can tolerate before gradient conflicts reappear.