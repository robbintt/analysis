---
ver: rpa2
title: 'Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated
  Text'
arxiv_id: '2509.17317'
source_url: https://arxiv.org/abs/2509.17317
tags:
- native
- data
- text
- natural
- simplified
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether machine-translated text can be
  used effectively for pretraining language models in low-resource languages. The
  authors compare GPT-2 models pretrained on native text versus machine-translated
  English into Indonesian and Tamil, including versions with and without source-side
  simplification.
---

# Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text

## Quick Facts
- arXiv ID: 2509.17317
- Source URL: https://arxiv.org/abs/2509.17317
- Reference count: 40
- Primary result: Scaling MT-pretrained models improves native text performance; source-side simplification harms generalization; MT pretraining provides strong foundation for continual pretraining on limited native data

## Executive Summary
This paper investigates whether machine-translated text can effectively pretrain language models for low-resource languages. The authors compare GPT-2 models pretrained on native text versus machine-translated English into Indonesian and Tamil, with and without source-side simplification. Their findings show that scaling model size improves performance on native text, contrary to concerns about overfitting to translation artifacts. Source-side simplification, however, harms generalization due to reduced lexical and syntactic diversity. Most importantly, continual pretraining on limited native data significantly boosts performance, often surpassing models trained only on native text, demonstrating that MT pretraining provides a strong starting point for bootstrapping target-language proficiency.

## Method Summary
The authors pretrained GPT-2 models (124M-774M parameters) on machine-translated English text for Indonesian and Tamil, using a 50,257-token BPE tokenizer trained on native corpora. They evaluated scaling effects by training different model sizes, compared natural vs. simplified MT pretraining, and tested continual pretraining on limited native data. Training used AdamW optimizer with 5% warmup and linear decay, peak learning rate 5×10⁻⁴ for pretraining and 5×10⁻⁵ for continual pretraining. Evaluation included cross-entropy loss on held-out native text, LINDSEA syntactic probing, and downstream tasks from SEA-HELM after fine-tuning on translated task data.

## Key Results
- Scaling MT-pretrained models (124M→774M) improves performance on native text, contrary to overfitting concerns
- Source-side simplification before translation harms generalization due to reduced lexical and syntactic diversity
- Continual pretraining on limited native data significantly boosts performance, often surpassing models trained only on native text
- Tamil CPT models surpassed native-only models trained on 5B native tokens
- Toxicity detection performance remains lower for MT-pretrained models (3-11 points behind native)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Scaling model capacity on MT-derived data improves generalization to held-out native text
- **Mechanism:** Larger models capture transferable syntactic and semantic structure present in MT text that overlaps with native distributions
- **Core assumption:** MT-derived text preserves sufficient structural overlap with native text for features to transfer
- **Evidence anchors:** Validation loss on native text decreases with larger model size; related work reports comparable downstream performance from MT pretraining
- **Break condition:** If MT quality is very low, diminishing returns may appear earlier

### Mechanism 2
- **Claim:** Source-side text simplification before translation harms generalization to native text
- **Mechanism:** Simplification reduces lexical diversity and syntactic complexity, producing MT data with impoverished variation
- **Core assumption:** Greater lexical and syntactic diversity in pretraining correlates with better native-text generalization
- **Evidence anchors:** Simplified-MT yields worse loss on native text than Natural-MT across all sizes
- **Break condition:** If target domain uses simplified language, simplification may not harm

### Mechanism 3
- **Claim:** MT-pretrained checkpoints provide efficient starting point for continual pretraining on limited native data
- **Mechanism:** MT pretraining imparts target-language structural knowledge that native CPT can "correct" without catastrophic forgetting
- **Core assumption:** Translationese patterns can be unlearned during CPT without discarding useful linguistic knowledge
- **Evidence anchors:** Tamil CPT models surpassed native-only models trained on 5B native tokens
- **Break condition:** Tasks requiring cultural nuance remain underperforming

## Foundational Learning

- **Concept: Translationese**
  - **Why needed here:** MT output contains artifacts (literal phrasing, source-language bias) that differ from naturally-written text
  - **Quick check question:** Can you name two characteristics that distinguish machine-translated text from native writing?

- **Concept: Cross-entropy loss on held-out data**
  - **Why needed here:** The primary metric for measuring generalization to native text
  - **Quick check question:** Why is loss on held-out native text a better indicator of transfer than loss on MT-derived validation data?

- **Concept: Continual Pretraining (CPT)**
  - **Why needed here:** The paper's core adaptation strategy—resuming training from an MT-pretrained checkpoint on native data with a lower learning rate
  - **Quick check question:** What hyperparameter change does the paper make when transitioning from MT pretraining to native CPT?

## Architecture Onboarding

- **Component map:** GPT-2 decoder-only architecture → 50,257-token BPE tokenizer → AdamW optimizer → 5% warmup + linear decay → cross-entropy loss → LINDSEA probing → SEA-HELM downstream tasks

- **Critical path:** 1) Pretrain on MT-derived corpus (~3–5B tokens) with peak LR 5×10⁻⁴, 2) CPT on native corpus (1–2.5B tokens) with reduced peak LR 5×10⁻⁵, 3) Fine-tune on translated task data, evaluate on native benchmarks

- **Design tradeoffs:** Natural-MT vs. Simplified-MT (diversity vs. cleanliness), CPT token budget (50% of MT budget), fixed vocabulary across conditions (fair comparison vs. underrepresentation of MT-specific tokens)

- **Failure signatures:** Toxicity detection lags 3–11 points behind native, causal reasoning near chance (~50–54%), Tamil Simplified-MT 774M shows slight loss regression

- **First 3 experiments:** 1) Replicate scaling trend: Train Small/Medium/Large on MT data, plot native validation loss vs. parameters, 2) Ablate simplification: Compare Natural-MT vs. Simplified-MT using syntactic probes, 3) Test CPT efficiency: Compare Native-only vs. MT→Native CPT on downstream tasks

## Open Questions the Paper Calls Out

- **Open Question 1:** Do scaling benefits of MT pretraining persist at billions of parameters (7B+), or does overfitting to translationese eventually dominate at larger scales?
  - **Basis in paper:** "extending these experiments to larger models... will clarify when the effects observed here amplify or taper"
  - **Why unresolved:** Only GPT-2 models (124M–774M) were tested; diminishing returns appeared at 774M
  - **What evidence would resolve it:** Pretrain 1B–7B+ models on MT-derived data and evaluate generalization to native text

- **Open Question 2:** Can MT-pretrained models be effectively instruction-tuned and preference-aligned, or do translationese artifacts interfere with alignment quality?
  - **Basis in paper:** "extending this approach to post-training regimes such as instruction tuning and preference alignment remains an open direction"
  - **Why unresolved:** Study only evaluated pretraining and task-specific fine-tuning
  - **What evidence would resolve it:** Apply instruction tuning and preference alignment to MT-pretrained models and compare to native-pretrained baselines

- **Open Question 3:** How much native data is required for culturally-nuanced tasks like toxicity detection to reach parity with native-only models?
  - **Basis in paper:** MT-pretrained models underperform Native models on toxicity detection by 3–11 points
  - **Why unresolved:** Relationship between native data quantity and cultural-nuance task performance was not systematically explored
  - **What evidence would resolve it:** Run a controlled sweep varying native CPT token budgets specifically for toxicity detection

## Limitations

- The controlled nature of translation quality used for pretraining does not explore lower bounds of acceptable MT quality
- Simplification mechanism's harm to generalization lacks corpus-level evidence about specific linguistic features lost
- Optimal allocation ratios between MT and native pretraining budgets remain underexplored
- Cultural nuance gaps in toxicity detection could have multiple explanations beyond translationese artifacts

## Confidence

**High Confidence:** Scaling mechanism showing larger models improve on native text from MT pretraining (directly supported by validation loss curves)
**Medium Confidence:** Source-side simplification harms generalization (experimental results consistent but mechanism inferred)
**Medium Confidence:** MT pretraining provides strong starting point for continual pretraining on limited native data (consistent improvements but optimal strategy undetermined)
**Low Confidence:** Toxicity detection particularly harmed by MT pretraining due to cultural nuance gaps (performance gap reported but not systematically investigated)

## Next Checks

1. **Quality threshold exploration:** Systematically vary MT quality (BLEU 10-40) for a single language and model size to identify when scaling benefits plateau or reverse

2. **Linguistic feature ablation:** Conduct detailed corpus analysis comparing Natural-MT vs. Simplified-MT across specific linguistic dimensions to identify which features most strongly predict downstream generalization

3. **Budget allocation optimization:** For a fixed total token budget (e.g., 5B), systematically vary the MT:native ratio and learning rate schedules to determine optimal pretraining strategy for different downstream task types