---
ver: rpa2
title: 'Closing the Curvature Gap: Full Transformer Hessians and Their Implications
  for Scaling Laws'
arxiv_id: '2510.16927'
source_url: https://arxiv.org/abs/2510.16927
tags:
- hessian
- layernorm
- theorem
- diag
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work closes the theoretical gap in Transformer optimization
  by deriving explicit Hessian expressions for Layer Normalization and feedforward
  networks, completing the full-block second-order analysis. The authors provide rigorous
  bounds on how curvature evolves with dataset size and introduce a Taylor-expansion-based
  framework for quantifying loss landscape convergence.
---

# Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws

## Quick Facts
- **arXiv ID**: 2510.16927
- **Source URL**: https://arxiv.org/abs/2510.16927
- **Reference count**: 40
- **Primary result**: Complete Hessian analysis for Transformers reveals curvature scaling as 1/k with dataset size

## Executive Summary
This work provides the first complete second-order analysis of Transformer architectures by deriving explicit Hessian expressions for Layer Normalization and feedforward networks. The authors establish rigorous bounds showing how curvature evolves with dataset size and introduce a Taylor-expansion framework for quantifying loss landscape convergence. Empirical validation on Vision Transformers demonstrates that the loss landscape stabilizes as 1/k with increasing data, matching theoretical predictions. The analysis reveals that Value- and Key-related Hessian blocks dominate curvature, offering new insights for optimization strategies and scaling law understanding.

## Method Summary
The authors employ matrix calculus to derive explicit Hessian expressions for all Transformer components, completing the theoretical framework for full-block second-order analysis. They introduce a Taylor-expansion-based approach to quantify loss landscape convergence and provide rigorous bounds on curvature evolution. The methodology involves systematic decomposition of Hessian blocks across different architectural components, enabling precise characterization of how each contributes to overall curvature properties.

## Key Results
- Complete Hessian expressions derived for Layer Normalization and feedforward networks
- Loss landscape curvature scales as 1/k with increasing dataset size
- Value- and Key-related Hessian blocks dominate overall curvature
- Empirical validation on Vision Transformers confirms theoretical predictions

## Why This Works (Mechanism)
The analysis works by decomposing the Transformer architecture into fundamental components and deriving their individual Hessian contributions. The Taylor-expansion framework captures how local curvature properties aggregate across the full loss landscape. The dominance of Value- and Key-related blocks emerges from their direct role in attention mechanisms, where parameter interactions create concentrated curvature effects that propagate through the architecture.

## Foundational Learning

**Matrix Calculus**: Essential for deriving Hessian expressions; quick check: verify trace and determinant properties of derived matrices
**Second-Order Optimization**: Understanding curvature's role in convergence; quick check: compare first-order vs second-order descent trajectories
**Scaling Laws**: Framework for dataset-size-dependent behavior; quick check: validate 1/k scaling across different model sizes
**Attention Mechanisms**: Key to understanding dominant Hessian blocks; quick check: isolate Value/Key contributions in ablation studies
**Loss Landscape Topology**: Critical for interpreting curvature bounds; quick check: visualize landscape smoothness across training epochs

## Architecture Onboarding

**Component Map**: Input -> LayerNorm -> Self-Attention -> Feedforward -> LayerNorm -> Output
**Critical Path**: LayerNorm affects curvature through normalization statistics; Self-Attention creates dominant Value/Key blocks
**Design Tradeoffs**: Complete Hessian analysis vs computational complexity; theoretical bounds vs empirical validation
**Failure Signatures**: Non-smooth landscapes when initialization deviates from "typical" patterns; breakdown of 1/k scaling
**First Experiments**:
1. Measure curvature evolution across training epochs
2. Compare dominant Hessian blocks across different attention variants
3. Test scaling law predictions with varying dataset sizes

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the generalizability of results beyond Vision Transformers, the impact of different initialization patterns, and the relationship between theoretical curvature bounds and actual training dynamics with adaptive optimization methods.

## Limitations

- Results primarily validated on Vision Transformers, limiting generalizability
- Assumes "typical" initialization patterns that may not hold across all training regimes
- Computational complexity remains prohibitive for exact Hessian computation in very large models

## Confidence

- **High confidence**: Curvature scaling with dataset size (1/k relationship)
- **Medium confidence**: Dominance of Value- and Key-related Hessian blocks
- **Medium confidence**: General applicability to Transformer family
- **Low confidence**: Performance implications for specific optimization algorithms

## Next Checks

1. Extend analysis to language models and compare curvature patterns against Vision Transformers
2. Test the 1/k scaling law across different learning rate schedules and optimization algorithms
3. Investigate the relationship between identified dominant Hessian blocks and actual training dynamics during early epochs