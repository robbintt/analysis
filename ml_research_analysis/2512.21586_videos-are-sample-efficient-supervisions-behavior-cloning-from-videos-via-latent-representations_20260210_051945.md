---
ver: rpa2
title: 'Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via
  Latent Representations'
arxiv_id: '2512.21586'
source_url: https://arxiv.org/abs/2512.21586
tags:
- latent
- learning
- bcv-lr
- videos
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method called BCV-LR for imitation
  learning from videos (ILV) that can learn effective policies with minimal environmental
  interactions. The key idea is to extract action-related latent features from video
  frames through self-supervised tasks, then predict latent actions between consecutive
  frames using a dynamics-based unsupervised objective.
---

# Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations
## Quick Facts
- arXiv ID: 2512.21586
- Source URL: https://arxiv.org/abs/2512.21586
- Reference count: 40
- Key result: BCV-LR achieves expert-level performance on 28 visual control tasks using only 100k interactions by extracting latent actions from videos

## Executive Summary
This paper introduces BCV-LR, a novel method for imitation learning from videos that achieves extreme sample efficiency by learning latent action representations. The approach extracts action-related features from video frames through self-supervised tasks, then predicts latent actions between consecutive frames using unsupervised dynamics objectives. These pre-trained latent actions are fine-tuned and aligned to real action space through reward-free environmental interactions, enabling efficient behavior cloning without expert actions or rewards. The method demonstrates that videos alone can serve as sufficient supervision for learning effective visual policies.

## Method Summary
BCV-LR works by first extracting latent representations from video frames through self-supervised feature learning, then predicting latent actions between consecutive frames using a dynamics-based unsupervised objective. These latent actions are pre-trained offline from videos, then fine-tuned online through reward-free interactions with the environment. The method iteratively improves both the policy and latent action prediction through repeated cycles of interaction and learning, ultimately aligning the latent action space to the real action space without requiring expert demonstrations or reward signals.

## Key Results
- Achieves expert-level performance on 28 challenging visual control tasks with only 100k interactions
- Outperforms both ILV baselines and RL methods in sample efficiency across 16 Procgen discrete tasks and 12 DMControl/MetaWorld continuous tasks
- Demonstrates for the first time that videos can serve as sole supervision for extremely sample-efficient visual policy learning

## Why This Works (Mechanism)
The method leverages the rich information contained in video demonstrations to learn action representations without requiring expert-provided actions. By using self-supervised feature extraction and unsupervised dynamics prediction, BCV-LR can extract meaningful action-related information from raw video frames. The iterative fine-tuning process allows the model to gradually align these latent actions with the actual action space through environmental interactions, making the learning process both sample-efficient and robust to the absence of expert action labels.

## Foundational Learning
- **Latent representation learning**: Why needed - to extract compact, action-relevant features from high-dimensional video frames; Quick check - verify that latent features capture task-relevant information by visualizing embeddings
- **Unsupervised dynamics prediction**: Why needed - to learn temporal relationships between frames without requiring expert actions; Quick check - ensure predicted latent actions correlate with actual state transitions
- **Iterative policy refinement**: Why needed - to gradually align latent action space with real action space through interaction; Quick check - monitor improvement in policy performance across interaction cycles
- **Self-supervised feature extraction**: Why needed - to learn rich visual features without manual annotations; Quick check - validate feature quality through downstream task performance
- **Reward-free interaction**: Why needed - to collect data for fine-tuning without requiring reward engineering; Quick check - confirm that collected interactions improve latent action alignment
- **Latent-to-real action alignment**: Why needed - to map learned latent actions to actual control actions; Quick check - verify that aligned actions produce desired behaviors

## Architecture Onboarding
- **Component map**: Video frames -> Self-supervised feature extractor -> Latent action predictor -> Policy network -> Environment interactions -> Reward-free data collection -> Latent-to-real alignment module
- **Critical path**: The most important sequence is Video frames → Self-supervised feature extractor → Latent action predictor → Policy network, as this determines the quality of the learned representations and policy
- **Design tradeoffs**: The method trades off computational complexity in offline pre-training for reduced online interaction requirements, choosing to invest in rich latent representations rather than collecting large amounts of interaction data
- **Failure signatures**: Poor performance typically manifests as either (1) inability to extract meaningful action-related features from videos, or (2) failure to align latent actions with real actions during fine-tuning
- **3 first experiments**: (1) Verify latent feature quality by visualizing t-SNE embeddings of video frames, (2) Test latent action prediction accuracy on held-out video sequences, (3) Validate policy performance after initial fine-tuning before full iterative refinement

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses exclusively on relatively short-horizon tasks (<2000 steps), leaving scalability to long-horizon behaviors untested
- Performance claims rely heavily on controlled simulation environments, with real-world applicability uncertain due to domain shift and noise in video data
- Method assumes access to resettable environment for reward-free data collection, which may not be feasible in many real-world settings

## Confidence
- **High** confidence in technical approach and ablation studies
- **Medium** confidence in sample-efficiency claims when extrapolating beyond tested task distributions
- **Low** confidence in real-world applicability without additional validation on physical robots or noisy video datasets

## Next Checks
- Test BCV-LR on temporally extended tasks requiring hundreds of thousands of steps to assess scalability and error accumulation
- Evaluate performance when trained on real-world YouTube-style videos with domain shift, occlusions, and irrelevant content
- Compare against hybrid methods that incorporate expert demonstrations or rewards to establish practical value proposition of pure video supervision