---
ver: rpa2
title: 'Co-Evolving Agents: Learning from Failures as Hard Negatives'
arxiv_id: '2511.22254'
source_url: https://arxiv.org/abs/2511.22254
tags:
- agent
- failure
- hard
- agents
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes a framework where two agents\u2014a target\
  \ and a failure agent\u2014co-evolve to improve learning from failures. The failure\
  \ agent is trained solely on failure trajectories to generate high-quality \"hard\
  \ negatives,\" which are near-success failures that provide stronger supervision\
  \ than typical failure samples."
---

# Co-Evolving Agents: Learning from Failures as Hard Negatives

## Quick Facts
- arXiv ID: 2511.22254
- Source URL: https://arxiv.org/abs/2511.22254
- Reference count: 20
- Key outcome: Co-evolving failure agents generate hard negatives that improve target agent performance by up to 6.8% on benchmarks

## Executive Summary
This paper introduces a novel framework where two agents co-evolve: a target agent and a failure agent. The failure agent is trained exclusively on failure trajectories to generate high-quality "hard negatives" - near-success failures that provide stronger supervision than typical failure samples. These hard negatives are incorporated into the target agent's preference optimization, sharpening decision boundaries and improving generalization. Experiments on WebShop, ScienceWorld, and InterCodeSQL benchmarks demonstrate consistent performance gains across different model architectures.

## Method Summary
The framework employs a two-agent system where a failure agent generates "hard negatives" from failure trajectories, which are then used to improve a target agent through preference optimization. The failure agent is trained solely on failure data using a preference loss, ranking near-success failures (e^+) against worse failures (e^-). These hard negatives are incorporated into the target agent's training objective, which now includes preference optimization against both expert demonstrations and failure-based hard negatives. The two agents undergo multiple rounds of co-evolution, with the failure agent generating increasingly challenging negatives for the target agent to learn from.

## Key Results
- WebShop benchmark: Performance improvements across multiple model architectures
- ScienceWorld benchmark: Consistent gains in task completion rates
- InterCodeSQL benchmark: Enhanced accuracy in code generation tasks
- Average reward improvements up to 6.8% over competitive baselines
- Qualitative analysis shows failure agents produce more diverse, informative failure trajectories

## Why This Works (Mechanism)
The framework leverages the observation that not all failures are equally informative. By training a failure agent to generate "hard negatives" - failures that are close to success but still fail - the target agent receives sharper supervision. This creates an implicit arms race where the failure agent continuously generates more challenging negatives, pushing the target agent to learn finer distinctions between success and failure. The preference optimization framework naturally accommodates these hard negatives by treating them as negative examples alongside expert demonstrations.

## Foundational Learning
- **Preference Optimization**: Needed to rank and incorporate hard negatives alongside expert data; quick check: verify loss function properly weights failure samples
- **Co-evolutionary Training**: Required for the implicit arms race between agents; quick check: monitor training stability across iterations
- **Trajectory-based Learning**: Essential for capturing the sequential nature of failures; quick check: ensure failure agent learns temporal patterns
- **Reward-based Failure Classification**: Needed to identify "hard negatives" from continuous reward signals; quick check: validate threshold selection for hard negative mining
- **Contrastive Learning**: Underlies the preference optimization mechanism; quick check: measure margin between positive and negative examples
- **Agent Specialization**: Required for separate target and failure agent roles; quick check: verify distinct capabilities emerge during training

## Architecture Onboarding

**Component Map**: Target Agent <- Preference Optimization <- Hard Negatives <- Failure Agent <- Failure Trajectories

**Critical Path**: Failure trajectories → Failure agent training → Hard negative generation → Target agent preference optimization → Performance improvement

**Design Tradeoffs**: The framework trades computational overhead (training two agents) for improved performance through better supervision signals. The benefit must outweigh the cost of maintaining a separate failure agent.

**Failure Signatures**: Failure agent produces ineffective hard negatives if it cannot distinguish near-success from far-failure trajectories. Target agent may overfit to failure patterns if hard negatives dominate training.

**First Experiments**:
1. Train failure agent on balanced success/failure dataset to establish baseline capability
2. Generate hard negatives from failure agent and evaluate their quality via human assessment
3. Test target agent with and without hard negatives to quantify performance impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reliance on continuous reward signals for hard negative mining limit applicability to sparse, binary reward environments?
- Basis: Section 5.2.1 utilizes a specific reward threshold (0.6) to identify hard negatives, and the method relies on ranking failure trajectories ($e^+$ vs $e^-$) based on reward deltas.
- Why unresolved: The framework is validated on benchmarks (WebShop, ScienceWorld) with dense, continuous scoring. Its effectiveness is unproven in environments where failures are binary and ungradable, making "near-success" identification impossible.
- What evidence would resolve it: Evaluation on strictly binary-reward tasks (e.g., standard coding benchmarks with simple pass/fail tests) using alternative metrics to rank failures.

### Open Question 2
- Question: Can the failure agent be parameterized with a smaller model size than the target agent while retaining the benefits of co-evolution?
- Basis: The implementation details (Section 5.1) and parameter-matched ablation (Section 5.4) imply a significant computational overhead.
- Why unresolved: While the ablation shows a "positive agent" fails to match performance, it does not test if a *smaller* failure agent could successfully model the failure landscape, which would increase the method's efficiency.
- What evidence would resolve it: Experiments comparing the performance of a 7B target agent co-evolving with smaller failure agents (e.g., 1B, 3B) versus a full 7B failure agent.

### Open Question 3
- Question: Does the co-evolutionary "arms race" eventually destabilize training or lead to diminishing returns as the failure agent generates negatives indistinguishable from positives?
- Basis: Section 4.3 describes an "implicit arms race" where the failure agent produces "harder" negatives.
- Why unresolved: The paper demonstrates convergence over 3-5 iterations but does not analyze the theoretical limit where hard negatives become so close to success that they contradict the ground-truth expert data, potentially confusing the target agent.
- What evidence would resolve it: Analysis of training dynamics and gradient conflicts over extended iteration counts (e.g., >10 iterations) with highly optimized failure agents.

## Limitations
- Assumes failure trajectories can be meaningfully classified and separated from successes
- Computational overhead of maintaining and training a separate failure agent
- Effectiveness depends on availability of sufficient failure samples for training
- May not generalize to domains with ambiguous success/failure boundaries
- Relies on continuous reward signals for hard negative mining

## Confidence
- **High Confidence**: Empirical performance improvements on established benchmarks and basic mechanism of using failure trajectories as training signals
- **Medium Confidence**: Scalability across different model architectures and generalizability of hard negatives concept to new domains
- **Medium Confidence**: Interpretation that hard negatives provide sharper supervision than traditional failure samples

## Next Checks
1. Conduct ablation studies removing the co-evolution mechanism to quantify the specific contribution of hard negatives versus having any failure-based training signal
2. Test the framework's performance on domains with high failure rates and ambiguous success/failure boundaries to assess robustness limits
3. Measure the computational overhead and training time impact of maintaining a separate failure agent across different scale scenarios to evaluate practical deployment feasibility