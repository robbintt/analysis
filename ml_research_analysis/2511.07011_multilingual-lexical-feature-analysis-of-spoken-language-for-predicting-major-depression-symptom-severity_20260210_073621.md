---
ver: rpa2
title: Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major
  Depression Symptom Severity
arxiv_id: '2511.07011'
source_url: https://arxiv.org/abs/2511.07011
tags:
- https
- lexical
- features
- language
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the association between spoken language features
  and major depressive disorder (MDD) symptom severity across three languages (English,
  Dutch, Spanish). Using longitudinal speech data from 586 participants in the RADAR-MDD
  study, interpretable lexical features were extracted and analyzed using linear mixed-effects
  models.
---

# Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major Depression Symptom Severity

## Quick Facts
- arXiv ID: 2511.07011
- Source URL: https://arxiv.org/abs/2511.07011
- Reference count: 0
- Primary result: Spoken language lexical features show limited predictive power for depression severity across three languages, with associations found in English and Dutch but not Spanish

## Executive Summary
This study investigated the relationship between spoken language features and major depressive disorder symptom severity across English, Dutch, and Spanish speakers. Using 5,836 speech recordings from 586 participants in the RADAR-MDD study, researchers extracted interpretable lexical features and employed linear mixed-effects models to identify associations with PHQ-8 depression scores. While several lexical features showed significant associations with depression severity in English and Dutch speakers, no robust associations were found in Spanish. Machine learning models using these features and more complex embeddings performed near chance level, suggesting that spoken language features alone are not strong predictors of depression severity. The findings highlight the potential value of language-based markers while emphasizing the need for larger, more diverse datasets and improved modeling approaches.

## Method Summary
The study used longitudinal speech data from 586 participants in the RADAR-MDD study, collecting 5,836 free-response speech recordings across three languages. Participants responded to the prompt "Tell us about something you are looking forward to in the next seven days." Speech was transcribed using Speechmatics ASR and analyzed using LIWC-22 and NLTK 3.8.1 to extract lexical features including type-token ratio, word count, sentence length, pronoun usage, and absolutist language (English only). The analysis employed linear mixed-effects models with participant random intercepts to account for repeated measures, and machine learning regression models (Random Forest, Elastic Net, SVR, XGBoost) with nested 5-fold cross-validation stratified by participant ID.

## Key Results
- English: Significant associations found between depression severity and lexical diversity (TTR, Brunet's index), absolutist language, negative word frequency, and past-focus word frequency
- Dutch: Significant associations with sentence length, positive word frequency, and negative word frequency
- Spanish: No robust lexical feature associations with depression severity identified
- ML models: Near-chance performance (R²≈0, RMSE>4.9) across all languages, suggesting limited predictive utility of spoken language features alone

## Why This Works (Mechanism)
The study leverages the established relationship between language patterns and mental health states, building on previous research showing that depressed individuals tend to use more absolutist language, negative emotion words, and first-person pronouns. The multilingual approach allows for cross-linguistic validation of these markers while accounting for language-specific variations in expression and linguistic structure.

## Foundational Learning

**Linear Mixed-Effects Models**: Account for repeated measures within participants while examining between-subject variability
*Why needed*: Longitudinal data with multiple recordings per participant requires modeling both within-person changes and between-person differences
*Quick check*: Verify random intercept terms improve model fit compared to fixed-effects only models

**Linguistic Inquiry and Word Count (LIWC)**: Text analysis software that categorizes words into psychologically meaningful categories
*Why needed*: Standardized method for extracting interpretable lexical features associated with psychological states
*Quick check*: Confirm LIWC dictionary coverage matches the study's target features for each language

**Nested Cross-Validation**: Inner loop for hyperparameter tuning, outer loop for unbiased performance estimation
*Why needed*: Prevents overfitting when both feature selection and model parameters are optimized
*Quick check*: Compare training vs. validation performance to ensure reasonable generalization

## Architecture Onboarding

**Component Map**: Speech recordings -> ASR transcription -> Lexical feature extraction (LIWC/NLTK) -> Feature engineering (TF-IDF, embeddings) -> LME/ML models -> Performance evaluation

**Critical Path**: ASR accuracy directly impacts feature extraction quality, which affects both LME association detection and ML prediction performance

**Design Tradeoffs**: Interpretability vs. predictive power (lexical features vs. embeddings), cross-linguistic consistency vs. language-specific adaptations, model complexity vs. generalizability

**Failure Signatures**: Near-chance ML performance despite significant LME associations indicates that while features are associated with depression, they lack individual-level predictive utility; differential results across languages suggest cultural/linguistic factors in depression expression

**First Experiments**:
1. Replicate LME results with single language (English) to verify association patterns
2. Compare ML performance using only lexical features vs. embeddings alone
3. Test model sensitivity to ASR transcription errors using ground-truth transcripts if available

## Open Questions the Paper Calls Out

None

## Limitations
- Single prompt type limits ecological validity of findings
- Differential results across languages raise questions about cross-linguistic generalizability
- Lack of temporal dynamics analysis (same-day vs. different-day recordings)
- Near-chance ML performance suggests limited practical utility for prediction

## Confidence

**Association findings**: Medium - statistically significant but variable across languages
**ML prediction results**: High - clearly demonstrated near-chance performance
**Cross-linguistic comparisons**: Low - inconsistent patterns across languages

## Next Checks

1. Verify data access protocol and timeline for RADAR-MDD speech and PHQ-8 data
2. Confirm LIWC-22 dictionary coverage and category mappings for Dutch/Spanish beyond standard categories
3. Test model robustness by varying random seeds and stratified split indices to ensure results are stable