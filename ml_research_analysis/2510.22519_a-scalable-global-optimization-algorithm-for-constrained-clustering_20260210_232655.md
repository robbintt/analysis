---
ver: rpa2
title: A Scalable Global Optimization Algorithm For Constrained Clustering
arxiv_id: '2510.22519'
source_url: https://arxiv.org/abs/2510.22519
tags:
- constraints
- solution
- clustering
- algorithm
- cplex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sample-Driven Constrained Group-Based Branch-and-Bound
  (SDC-GBB), a scalable deterministic global optimization algorithm for pairwise-constrained
  minimum sum-of-squared clustering (MSSC). The key innovation lies in collapsing
  must-link samples into centroid-based pseudo-samples and applying geometric rules
  to prune cannot-link assignments, thus reducing the search space while maintaining
  global optimality guarantees.
---

# A Scalable Global Optimization Algorithm For Constrained Clustering

## Quick Facts
- arXiv ID: 2510.22519
- Source URL: https://arxiv.org/abs/2510.22519
- Reference count: 37
- Key outcome: SDC-GBB achieves 200-1,500x scalability improvement, handling up to 1.5M samples with global optimality guarantees below 3% gap

## Executive Summary
This paper introduces Sample-Driven Constrained Group-Based Branch-and-Bound (SDC-GBB), a scalable deterministic global optimization algorithm for pairwise-constrained minimum sum-of-squared clustering. The key innovation lies in collapsing must-link samples into centroid-based pseudo-samples and applying geometric rules to prune cannot-link assignments, thus reducing the search space while maintaining global optimality guarantees. A grouped-sample Lagrangian Decomposition further tightens lower bounds, enabling efficient parallel exploration. SDC-GBB scales to datasets with 200,000 samples under cannot-link constraints and 1,500,000 samples under must-link constraints, achieving optimality gaps below 3%. This represents a 200- to 1,500-fold increase in scalability over existing exact methods, making it the first algorithm capable of handling such large-scale constrained clustering problems with provable global optimality.

## Method Summary
SDC-GBB employs a sample-driven framework that integrates must-link and cannot-link constraints into a branch-and-bound structure. The algorithm collapses must-link samples into centroid-based pseudo-samples, reducing the effective sample size. Geometric rules prune cannot-link assignments by exploiting the triangle inequality and bounding centroid distances. A grouped-sample Lagrangian Decomposition tightens lower bounds by partitioning samples into groups and dualizing the assignment constraints. The method enables parallel exploration of the search space while maintaining global optimality guarantees through rigorous bound propagation.

## Key Results
- Achieved 200- to 1,500-fold scalability improvement over existing exact methods
- Handles up to 1.5 million samples under must-link constraints with <3% optimality gap
- Scales to 200,000 samples under cannot-link constraints while maintaining global optimality
- First algorithm capable of handling large-scale constrained clustering with provable global optimality

## Why This Works (Mechanism)
The algorithm's effectiveness stems from its ability to dramatically reduce the search space through intelligent sample aggregation and geometric pruning. By collapsing must-link samples into pseudo-centroids, the effective problem size decreases significantly. The geometric rules for cannot-link pruning exploit the triangle inequality to eliminate infeasible assignments without exhaustive search. The grouped-sample Lagrangian Decomposition provides tighter lower bounds by decomposing the problem into manageable subproblems that can be solved in parallel. This combination of search space reduction, intelligent pruning, and bound tightening enables scalable global optimization that was previously intractable for large datasets.

## Foundational Learning
- **Pairwise constrained clustering**: Clustering with must-link (samples must be in same cluster) and cannot-link (samples must be in different clusters) constraints. Needed because real-world applications often require domain-specific grouping rules. Quick check: Verify constraint satisfaction in final clustering.
- **Lagrangian Decomposition**: A dual optimization technique that breaks complex problems into subproblems by dualizing coupling constraints. Needed for obtaining tight lower bounds efficiently. Quick check: Ensure dual variables converge to optimal multipliers.
- **Branch-and-Bound framework**: Systematic enumeration of solution candidates with pruning based on bounds. Needed for guaranteeing global optimality in nonconvex problems. Quick check: Verify bound consistency throughout search tree.
- **Centroid-based pseudo-samples**: Aggregated representations of must-link groups that preserve cluster center information. Needed to reduce problem size without losing essential structure. Quick check: Confirm pseudo-centroid placement matches original group characteristics.
- **Geometric pruning rules**: Triangle inequality-based elimination of infeasible cannot-link assignments. Needed to reduce branching complexity exponentially. Quick check: Validate that pruned assignments are indeed infeasible.
- **Parallel exploration**: Concurrent processing of independent subproblems in branch-and-bound tree. Needed to exploit modern multi-core architectures for speedup. Quick check: Monitor load balancing across parallel workers.

## Architecture Onboarding
- **Component map**: Data preprocessing -> Must-link aggregation -> Cannot-link pruning -> Lagrangian Decomposition -> Branch-and-Bound search -> Parallel exploration -> Solution extraction
- **Critical path**: Must-link aggregation and cannot-link pruning occur first to reduce problem size, followed by Lagrangian Decomposition for bound tightening, then Branch-and-Bound search with parallel exploration, and finally solution extraction with constraint verification.
- **Design tradeoffs**: The algorithm trades computational complexity in preprocessing (aggregation and pruning) for exponential reduction in search space. Memory usage increases with constraint density but is offset by parallelization benefits. The choice of group size in Lagrangian Decomposition affects both bound quality and computational overhead.
- **Failure signatures**: High optimality gaps indicate insufficient bound tightening or inadequate search depth. Memory exhaustion suggests constraint density is too high for available resources. Slow convergence may result from poor grouping choices or ineffective pruning rules. Constraint violations point to errors in the aggregation or pruning implementation.
- **First experiments**: 1) Run on synthetic dataset with known optimal solution to verify correctness. 2) Benchmark scalability on varying constraint densities to identify performance breakpoints. 3) Profile memory usage and runtime across different parallelization settings to optimize resource allocation.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can dualizing the cannot-link (CL) graph with relaxed binary indicators and subgradient updates significantly tighten the grouped-sample Lagrangian lower bound for dense CL instances?
- Basis in paper: [explicit] The authors state in Section 6: "Future work may consider tightening the grouped-sample Lagrangian lower bound... by dualizing the CL graph with relaxing binary indicators to [0,1] and penalty multipliers $z_{ij}$ updated via subgradient methods."
- Why unresolved: The current algorithm struggles to scale to one million samples with CL constraints due to the NP-hard nature of the constraint type, and this specific Lagrangian modification has not yet been implemented or tested.
- What evidence would resolve it: Demonstrating improved scalability (e.g., handling $n > 200,000$ with CL constraints) or tighter lower bounds on datasets where the current method yields high optimality gaps (e.g., URBANGB).

### Open Question 2
- Question: Do clique inequalities or separation cuts for the cannot-link graph offer a favorable trade-off between bound improvement and the computational cost of solving the relaxation at each node?
- Basis in paper: [explicit] Section 6 proposes incorporating "clique inequalities or separation cuts" but notes that "empirically evaluating the trade-off between bound improvement and per-node cost will be key."
- Why unresolved: While these cuts are standard in MIP methodologies, it is unknown if the reduction in the number of branches justifies the added time required to process each node in the SDC-GBB framework.
- What evidence would resolve it: Empirical benchmarks comparing the total solve time and node count of SDC-GBB with and without these cuts on dense CL graphs.

### Open Question 3
- Question: How can fairness-aware safeguards or constraints be integrated into the SDC-GBB framework to mitigate the risk of bias amplification in high-stakes domains?
- Basis in paper: [explicit] The Ethics Statement identifies the "absence of fairness-aware safeguards in the current implementation" as a limitation and leaves "integrating such constraints or corrections... for future extensions."
- Why unresolved: The current implementation optimizes strictly for the Minimum Sum-of-Squares Criteria (MSSC), which can reproduce or amplify biases in the data, particularly at scale.
- What evidence would resolve it: A modified version of SDC-GBB that includes fairness constraints in the optimization problem while maintaining global optimality guarantees and competitive scalability.

## Limitations
- Cannot-link constraints remain NP-hard, limiting scalability to approximately 200,000 samples in current implementation
- The 3% optimality gap guarantee may not be tight enough for applications requiring near-exact solutions
- Lack of fairness-aware safeguards could amplify biases in high-stakes domains when scaling to large datasets
- Performance depends heavily on constraint density and distribution patterns, with some configurations potentially degrading scalability gains

## Confidence
- **High confidence**: The fundamental algorithmic framework combining centroid-based pseudo-samples with Lagrangian decomposition is mathematically sound and implementable.
- **Medium confidence**: The claimed scalability gains are based on the reported experimental results, but these need independent verification across different data distributions and constraint patterns.
- **Medium confidence**: The 3% optimality gap guarantee is technically valid under the stated conditions, but practical performance may vary with constraint structure.

## Next Checks
1. Reproduce the scalability benchmarks on independent datasets with varying constraint densities to verify the 200-1,500x improvement claims across different scenarios.
2. Conduct stress tests on the geometric pruning rules by generating adversarial cannot-link patterns to identify potential worst-case performance degradation.
3. Perform runtime and memory profiling on the Lagrangian decomposition implementation to identify bottlenecks when scaling to the upper constraint limits reported.