---
ver: rpa2
title: On the Relationship Between Representation Geometry and Generalization in Deep
  Neural Networks
arxiv_id: '2602.00130'
source_url: https://arxiv.org/abs/2602.00130
tags:
- accuracy
- dimension
- effective
- compression
- geometric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the relationship between representation\
  \ geometry and neural network performance through systematic empirical studies across\
  \ vision and language domains. The authors analyze 52 pretrained ImageNet models\
  \ across 13 architecture families and demonstrate that effective dimension\u2014\
  an unsupervised geometric metric requiring no labels\u2014strongly predicts classification\
  \ accuracy."
---

# On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks

## Quick Facts
- **arXiv ID:** 2602.00130
- **Source URL:** https://arxiv.org/abs/2602.00130
- **Reference count:** 0
- **Primary result:** Effective dimension strongly predicts accuracy across vision and language models, with r=0.75 partial correlation after controlling for model capacity

## Executive Summary
This paper investigates the relationship between representation geometry and neural network performance through systematic empirical studies across vision and language domains. The authors analyze 52 pretrained ImageNet models across 13 architecture families and demonstrate that effective dimension—an unsupervised geometric metric requiring no labels—strongly predicts classification accuracy. The key findings include: output effective dimension achieves partial correlation r=0.75 (p < 10^-10) with accuracy after controlling for model capacity, while total compression achieves partial r=-0.72; these geometric signatures generalize across domains, predicting performance for NLP models (r=0.69 on AG News) where model size does not (r=0.07); and most importantly, the authors establish bidirectional causality through controlled interventions: degrading geometry via noise causes accuracy loss (r=-0.94, p < 10^-9) while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). The relationship is noise-type agnostic, with all four tested noise types showing |r| > 0.90.

## Method Summary
The authors extract penultimate layer representations from pretrained models, compute effective dimension via participation ratio (EffDim = (tr(Σ))²/tr(Σ²)), and correlate these geometric metrics with accuracy across 52 torchvision models. They establish causality through two interventions: noise injection (Gaussian, Uniform, Dropout, Salt-and-pepper) to degrade geometry and PCA projection to improve it. For the causal intervention, they train models from scratch with varying noise levels (σ=0.1–0.5) and apply PCA post-hoc, measuring accuracy changes. The study covers vision tasks (ImageNet, CIFAR-10) and NLP (SST-2, MNLI encoders, AG News decoders), using m=2000 samples for geometry computation and 2× Tesla T4 GPUs.

## Key Results
- Output effective dimension achieves partial correlation r=0.75 (p < 10^-10) with accuracy after controlling for model capacity
- Total compression magnitude correlates with output effective dimension (r=0.69, p=0.004) and predicts accuracy (partial r=-0.72)
- All four noise types show strong negative correlations between geometry degradation and accuracy loss (|r| > 0.90)
- PCA projection maintains accuracy across architectures (-0.03pp at 95% variance), requiring only 14-16 principal components (out of 512) to maintain full accuracy

## Why This Works (Mechanism)

### Mechanism 1: Effective Dimension Captures Task-Relevant Subspace
The participation ratio (effective dimension) measures how many dimensions meaningfully contribute to representation variance, predicting accuracy independent of model capacity. Representation matrices with concentrated eigenvalue structure (low effective dimension for encoders, high for decoders) indicate the network has learned a task-relevant subspace. When noise inflates effective dimension by adding uninformative variance across many dimensions, accuracy degrades. The core assumption is that task-relevant information concentrates in a low-dimensional subspace; noise distributes variance more uniformly. Evidence shows output effective dimension achieves partial r=0.75 (p < 10^-10) with accuracy after controlling for model capacity, and networks only require 14-16 principal components (out of 512) to maintain full accuracy.

### Mechanism 2: Compression Magnitude Reflects Information Processing Quality
The magnitude of geometric transformation (|C|), whether compression in encoders or expansion in decoders, correlates with representation quality across architectures. Encoders trained on discriminative objectives compress diverse inputs toward class boundaries (C < 0). Decoders trained on generative objectives expand representations toward large vocabulary spaces (C > 0). In both cases, stronger transformation magnitude indicates more effective information processing. The core assumption is that the log-ratio captures meaningful compositional transformation. Evidence shows compression correlates with output effective dimension (r=0.69, p=0.004) and that model size does NOT predict geometric quality (r=0.07, p=0.82).

### Mechanism 3: Bidirectional Causality Through Geometric Intervention
Manipulating representation geometry directly causes accuracy changes, establishing causal rather than merely correlational relationship. Adding noise to penultimate layer activations increases effective dimension (degrades geometric structure) and causes accuracy loss. PCA projection decreases effective dimension (removes uninformative variance) while maintaining accuracy. The asymmetry reveals that representations concentrate task-relevant information in low-dimensional subspaces. The core assumption is that the intervention affects accuracy primarily through geometry changes, not through other confounding pathways. Evidence shows degrading geometry via noise causes accuracy loss (r=-0.94, p < 10^-9), and all four noise types show strong negative correlations between geometry degradation and accuracy loss.

## Foundational Learning

- **Concept: Participation Ratio / Effective Dimension**
  - Why needed here: Core metric used throughout; measures intrinsic dimensionality via eigenvalue concentration: EffDim = (tr(Σ))²/tr(Σ²)
  - Quick check question: Given eigenvalues [4, 1, 0, 0], what is the effective dimension? (Answer: (4+1)²/(16+1) = 25/17 ≈ 1.47)

- **Concept: Partial Correlation**
  - Why needed here: Critical for ruling out model capacity as confound; isolates geometric effects from size effects
  - Quick check question: Why might raw r=0.65 between compression and accuracy be misleading? (Answer: Larger models may both compress more and achieve higher accuracy; partial correlation controls for this)

- **Concept: Information Bottleneck Principle**
  - Why needed here: Theoretical motivation for compression-quality relationship; representations should compress input while preserving task-relevant information
  - Quick check question: Why does the paper use geometric proxies rather than mutual information directly? (Answer: Mutual information estimation is difficult; geometric metrics provide tractable approximation)

## Architecture Onboarding

- **Component map:** sample activations → center → SVD → compute EffDim per layer → aggregate (compression, bottleneck, output) → intervention: noise injection (degradation path) / PCA projection (improvement path) → evaluate accuracy correlation

- **Critical path:** For a new model, extract penultimate layer representations on held-out data → compute covariance → eigenvalues → effective dimension → compare against baseline models of similar size

- **Design tradeoffs:** Sample size (m=2000 used): smaller samples faster but noisier eigenvalue estimates; Layer selection: output layer strongest predictor but layer-wise analysis reveals training dynamics; Encoder vs decoder: opposite compression sign conventions require domain-aware interpretation

- **Failure signatures:** EffDim approaching ambient dimension (e.g., 500+ for 512-dim layer) suggests untrained or severely degraded representations; Compression near zero with poor accuracy suggests network failing to transform representations meaningfully; Large EffDim variance across samples indicates unstable representation structure

- **First 3 experiments:**
  1. Baseline geometry audit: Extract effective dimension at each layer for 3+ pretrained models across same task; verify output EffDim correlates with known accuracy ranking
  2. Intervention sanity check: Apply Gaussian noise (σ=0.1-0.5) to penultimate activations; confirm EffDim increases and accuracy decreases with strong negative correlation
  3. PCA compression test: Project penultimate activations to 95% variance components; verify accuracy maintained with <5% degradation (validates low-dimensional task subspace)

## Open Questions the Paper Calls Out

- **Open Question 1:** Does effective dimension predict performance in reinforcement learning, speech processing, and other domains beyond vision and NLP? The paper explicitly calls for extension to reinforcement learning, speech, and other domains to further establish generality, as current experiments only cover supervised vision classification, NLP encoders, and decoder-only LLMs.

- **Open Question 2:** Why does compression correlate with accuracy—does it emerge from optimization dynamics, architectural inductive biases, or data structure? The paper acknowledges this mechanism remains unclear and calls for investigation into whether these properties emerge from optimization dynamics, architectural inductive biases, or data structure.

- **Open Question 3:** Can geometric regularization during training directly improve final model performance? The paper suggests extending to additional intervention methods including geometric regularization during training to strengthen causal claims, as current PCA intervention is applied post-hoc during inference.

## Limitations
- The empirical findings rely heavily on ImageNet-trained models and may not generalize to architectures with different training regimes or domains
- The study uses m=2000 samples for geometry computation, which may introduce sampling noise for high-dimensional representations
- While partial correlations control for model size, they cannot rule out all confounding factors from architectural family or training hyperparameters

## Confidence
- **High Confidence:** Geometric metrics predict accuracy (r=0.75 for output EffDim, r=-0.72 for total compression) - supported by extensive empirical validation across 52 models and multiple datasets
- **Medium Confidence:** Bidirectional causality established through interventions - strong correlation (r=-0.94) between geometry degradation and accuracy loss, but intervention effects could have secondary pathways
- **Medium Confidence:** Cross-domain generalization - predicts NLP model performance (r=0.69 on AG News) but limited to 15 decoder models

## Next Checks
1. **Architecture Family Control:** Compute partial correlations controlling for architectural family to verify geometric signatures are not family-specific artifacts
2. **Sample Size Sensitivity:** Repeat geometry computations with varying sample sizes (500-8000) to establish robustness to sampling noise
3. **Alternative Metrics:** Compare effective dimension with other geometric measures (fractal dimension, manifold dimension) to verify results are not metric-specific