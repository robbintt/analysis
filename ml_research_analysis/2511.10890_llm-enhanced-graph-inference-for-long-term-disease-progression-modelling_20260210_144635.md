---
ver: rpa2
title: LLM enhanced graph inference for long-term disease progression modelling
arxiv_id: '2511.10890'
source_url: https://arxiv.org/abs/2511.10890
tags:
- graph
- disease
- progression
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel framework that uses Large Language
  Models (LLMs) to guide graph inference for long-term disease progression modelling.
  Traditional approaches oversimplify disease-spreading substrates by assuming single-modality
  brain connectomes, leading to inaccurate predictions during long-term progression.
---

# LLM enhanced graph inference for long-term disease progression modelling

## Quick Facts
- arXiv ID: 2511.10890
- Source URL: https://arxiv.org/abs/2511.10890
- Authors: Tiantian He; An Zhao; Elinor Thompson; Anna Schroder; Ahmed Abdulaal; Frederik Barkhof; Daniel C. Alexander
- Reference count: 13
- Primary result: LLM-guided graph inference achieves superior long-term disease progression prediction accuracy and interpretability compared to traditional approaches while using fewer parameters

## Executive Summary
This paper introduces a novel framework that uses Large Language Models (LLMs) to guide graph inference for long-term disease progression modeling. Traditional approaches oversimplify disease-spreading substrates by assuming single-modality brain connectomes, leading to inaccurate predictions during long-term progression. The proposed method leverages LLMs to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, simultaneously optimizing disease trajectory construction and biologically-constrained graph structure with improved identifiability. Experiments on Alzheimer's disease tau-PET data demonstrate superior prediction accuracy and interpretability compared to traditional approaches.

## Method Summary
The framework queries multiple LLMs with carefully designed prompts about brain region relationships, averages their probabilistic graph outputs, and thresholds them to create sparse binary graphs. These LLM-guided graphs are then embedded into a diffusion model where pathology spreads according to the normalized Laplacian of the graph structure. The method jointly optimizes subject temporal positions on the disease trajectory and edge weights through a dual optimization approach. This combines LLM-derived constraints with data-driven weight refinement, achieving better identifiability than purely data-driven methods while maintaining biological plausibility.

## Key Results
- LLM-guided graphs achieve better test SSE (Sum of Squared Errors) and correlation metrics than purely data-driven or single-modality connectome approaches
- The framework demonstrates superior parameter efficiency, achieving equivalent or better performance with fewer learnable parameters (284 critical edges vs 314 for alternatives)
- Cross-validation experiments show consistent improvement across different subject splits, with the mixed LLM graph achieving lowest AIC (482.40)

## Why This Works (Mechanism)

### Mechanism 1
LLM-derived graph structures provide better identifiability constraints than purely data-driven or single-modality connectome approaches. LLMs synthesize multi-modal biological knowledge (structural, functional, morphological, geodesic, microstructural connectivity) into probabilistic edge weights, which are thresholded to create sparse binary graphs. This sparsity reduces redundant propagation paths that cause identifiability issues. Core assumption: LLM training corpora contain sufficient domain knowledge about neurodegenerative disease mechanisms to inform meaningful brain region relationships. Break condition: If LLM outputs are inconsistent across runs or models without convergence, the identifiability advantage degrades.

### Mechanism 2
Sparse LLM-guided graphs achieve equivalent or better prediction accuracy with fewer learnable parameters than dense biological connectomes. Thresholding retains only edges above a critical value, eliminating redundant connections while preserving disease-relevant pathways. Data-driven weight refinement then optimizes only the non-zero elements, reducing overfitting risk. Core assumption: The true disease propagation network is sparser than measured connectomes suggest, with key pathways captured by LLM-synthesized knowledge. Break condition: Over-aggressive thresholding may disconnect critical brain regions, causing trajectory estimation failure.

### Mechanism 3
Joint optimization of temporal staging and graph structure enables long-term trajectory reconstruction from irregular snapshots. The dual optimization method iteratively allocates subjects on a pseudo-time axis while refining the graph structure. The dynamical system (dc/dt = -k[Lc(t)] + αc(t)⊙[vp-c(t)]) propagates pathology along LLM-constrained edges. Core assumption: Disease progression follows a smooth, monotonic trajectory that can be described by a diffusion-like process on a fixed graph structure. Break condition: If observation timestamps are extremely sparse or highly irregular, temporal placement becomes underdetermined regardless of graph quality.

## Foundational Learning

- **Concept: Graph Laplacian and network diffusion**
  - Why needed here: The disease propagation model uses the normalized Laplacian (L = D - A) to describe how pathology spreads between connected brain regions.
  - Quick check question: Can you explain why the Laplacian's eigenvalues determine the rate of diffusion across a graph?

- **Concept: Identifiability in graphical models**
  - Why needed here: Purely data-driven graph learning can produce multiple graphs with equivalent loss; the paper addresses this through LLM-derived constraints.
  - Quick check question: Given two different graph structures that produce similar trajectory fits, how would you determine which is more biologically plausible?

- **Concept: Prompt engineering for structured outputs**
  - Why needed here: The framework relies on carefully designed prompts (context, task framing, anatomical framework, biological factors, output structuring) to elicit consistent probabilistic graphs from LLMs.
  - Quick check question: Why might setting temperature to 0.25 and averaging across multiple LLMs improve graph reliability?

## Architecture Onboarding

- **Component map:** LLM Query Module -> Graph Filter -> Weight Learner -> Trajectory Generator -> Stage Optimizer
- **Critical path:** LLM query → graph filtering → weight initialization → trajectory fitting → stage allocation → weight refinement (iterative)
- **Design tradeoffs:** Higher threshold → sparser graph → better identifiability but risk of missing critical edges; More LLM factors (5 vs. 7) → potentially richer knowledge but requires unavailable data modalities; Assumption: Linear combination of connectomes may not capture complex multi-modal interactions that LLM reasoning can
- **Failure signatures:** Disconnected graph components causing regional pathology isolation; Divergent LLM outputs across models (check edge weight variance); Trajectory overfitting to individual subjects rather than cohort-level pattern
- **First 3 experiments:** 1) Replicate the NGM identifiability test (Figure 2): Run data-driven graph learning twice with identical settings to confirm instability; 2) Compare 5-factor vs. 7-factor prompts on a subset of subjects to quantify the value of extended biological knowledge; 3) Validate the critical edge number finding: Gradually increase threshold and plot test SSE to find the sparsity-performance knee point for your own dataset

## Open Questions the Paper Calls Out

- **Generalizability to other biomarkers:** Can the LLM-guided framework effectively generalize to model other neurodegenerative biomarkers, such as amyloid, or entirely different disease domains? The current study validates the method exclusively on tau-PET data in an Alzheimer's cohort, leaving its utility for other pathological substrates or diseases unproven.

- **Biological validity of LLM-suggested connections:** Do the novel brain connections suggested by LLMs but rarely found in biological graphs represent true biological transmission pathways or hallucinations? The paper relies on retrospective literature support and predictive accuracy rather than prospective biological validation to confirm these specific "missing links."

- **Optimal prompt engineering:** How sensitive is the model's performance to the specific selection and phrasing of biological factors in the LLM prompt? The study manually engineers a 5-factor and 7-factor prompt, but does not explore an automated or exhaustive method to determine the optimal set of factors.

## Limitations

- Reliance on LLM training data quality and domain coverage for disease-specific knowledge creates uncertainty about whether synthesized connections accurately reflect true neurodegenerative mechanisms
- Optimal threshold for converting probabilistic graphs to binary structures significantly impacts model performance but may vary across disease types and populations
- The framework assumes disease propagation follows a diffusion-like process on a fixed graph structure, which may not capture complex, non-linear biological mechanisms or feedback loops in neurodegeneration

## Confidence

- **High Confidence:** The identifiability advantage of LLM-guided graphs over purely data-driven approaches is well-supported by reported NGM instability experiments (Figure 2) and theoretical grounding in graph learning literature
- **Medium Confidence:** The parameter efficiency claim (better performance with fewer edges) is supported by AIC comparisons but requires broader validation across different disease types and datasets
- **Low Confidence:** The biological plausibility of LLM-synthesized edge weights depends heavily on the quality and domain coverage of LLM training corpora, which cannot be fully verified

## Next Checks

1. **Cross-model validation:** Test the framework on independent tau-PET datasets (e.g., from different cohorts or imaging centers) to assess generalizability and identify dataset-specific overfitting

2. **Biological mechanism validation:** Compare LLM-guided graph edges with established neuropathological staging schemes and experimentally validated connectivity patterns to assess biological plausibility

3. **Temporal resolution sensitivity:** Evaluate model performance across varying observation frequencies and gaps to determine robustness to data sparsity in long-term progression modeling