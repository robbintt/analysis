---
ver: rpa2
title: 'JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding
  Benchmark via Vibe Benchmark Construction'
arxiv_id: '2512.14620'
source_url: https://arxiv.org/abs/2512.14620
tags:
- image
- lmms
- jmmmu-pro
- question
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline
  Multimodal Understanding Benchmark that embeds both question images and text into
  a single image, requiring integrated visual-textual understanding. To construct
  it efficiently, the authors propose Vibe Benchmark Construction, where a generative
  model (Nano Banana Pro) creates visual questions and humans verify and refine outputs
  through prompt adjustments.
---

# JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction

## Quick Facts
- **arXiv ID**: 2512.14620
- **Source URL**: https://arxiv.org/abs/2512.14620
- **Reference count**: 40
- **Primary result**: Embedding question text and images into a single composite image creates a benchmark requiring integrated visual-textual understanding that all open-source LMMs struggle with, scoring only 45.83% accuracy maximum.

## Executive Summary
This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark that embeds both question images and text into a single image, requiring integrated visual-textual understanding. To construct it efficiently, the authors propose Vibe Benchmark Construction, where a generative model (Nano Banana Pro) creates visual questions and humans verify and refine outputs through prompt adjustments. Experiments show that all open-source LMMs struggle significantly on JMMMU-Pro, with the best scoring only 45.83% accuracy, compared to much higher performance on the original JMMMU. This performance drop highlights the benchmark's value in evaluating true visual-textual integration abilities. Closed-source models perform substantially better, revealing a major capability gap. The findings demonstrate that solving JMMMU-Pro requires not only strong OCR but also deeper multimodal reasoning, making it a crucial tool for guiding future LMM development.

## Method Summary
The paper constructs JMMMU-Pro by composing question images and text from the JMMMU benchmark into single composite images using Nano Banana Pro (gemini-3-pro-image-preview API). The Vibe Benchmark Construction methodology involves generating candidate images with randomized parameters (background type, color, font, margin, state, aspect ratio), human verification with 71% first-pass acceptance rate, prompt adjustment for failed cases, and manual construction for edge cases (67 samples, ~5%). The final benchmark contains 1,320 questions across 28 subjects, with open-source LMMs achieving maximum 45.83% accuracy compared to much higher scores on the original JMMMU, demonstrating the added difficulty of integrated visual-textual understanding.

## Key Results
- All open-source LMMs struggle significantly on JMMMU-Pro, with maximum accuracy of 45.83% compared to much higher performance on original JMMMU
- OCR accuracy correlates with JMMMU-Pro performance (r=0.593), but high OCR alone does not guarantee high benchmark performance
- Closed-source models show substantially better performance, revealing a major capability gap with open-source LMMs
- Performance degradation from JMMMU to JMMMU-Pro is consistent across all models, confirming the benchmark's value in evaluating true visual-textual integration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding question text and images into a single composite image forces models to perform integrated visual-textual understanding through visual perception rather than processing separate modalities independently.
- Mechanism: Traditional benchmarks provide question text as a separate modality (tokens) and question image as visual input. JMMMU-Pro merges these into one image, requiring the model to extract text via OCR/visual processing before reasoning can begin. This eliminates the "free" text comprehension shortcut and forces genuine visual-textual integration.
- Core assumption: Performance drops are caused by the modality merger rather than image quality degradation.
- Evidence anchors:
  - [abstract] "composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception"
  - [section 1] "the inability of current LMMs to perform Japanese Optical Character Recognition (OCR)"
  - [corpus] Related work on vertically written Japanese text (arXiv:2511.15059) confirms OCR remains a bottleneck for Japanese document understanding.

### Mechanism 2
- Claim: The Vibe Benchmark Construction pipeline (generative model + human verification) produces valid benchmark items at scale because the human role is restricted to verification and prompt adjustment rather than creation.
- Mechanism: Nano Banana Pro generates candidate images with embedded Japanese text. Humans verify correctness (71% first-pass acceptance rate) and adjust prompts only for failures. This division of labor works because: (1) the model handles the expensive image synthesis, (2) verification is faster than creation, (3) prompt adjustment allows targeted regeneration without full manual construction.
- Core assumption: The generative model can produce sufficiently accurate Japanese text rendering; failures are detectable and correctable via prompt adjustment.
- Evidence anchors:
  - [section 3.3] "approximately 95% of all the questions in JMMMU-Pro were generated by Nano Banana Pro"
  - [section 3.3] "71% of the questions passed" first review round
  - [corpus] No corpus papers explicitly validate this construction methodology; it appears novel.

### Mechanism 3
- Claim: The strong correlation between OCR accuracy and JMMMU-Pro performance (r=0.593), combined with the finding that high OCR alone does not guarantee high benchmark performance, indicates that solving JMMMU-Pro requires both strong OCR and integrated multimodal reasoning.
- Mechanism: OCR accuracy explains ~35% of variance (r² ≈ 0.35) in JMMMU-Pro scores. The remaining variance stems from the model's ability to jointly interpret visual elements and extracted text, perform domain reasoning, and handle diverse layouts (workbook, whiteboard, TV show, etc.).
- Core assumption: Levenshtein distance-based OCR accuracy meaningfully captures the text extraction bottleneck.
- Evidence anchors:
  - [section 5.2] "correlation coefficient between OCR accuracy and JMMMU-Pro accuracy is 0.593"
  - [section 5.2] "Heron-NVILA and Sarashina2.2-V are comparable for OCR performance, the performance for JMMMU-Pro differs a lot"

## Foundational Learning

- Concept: **Multimodal Input Fusion**
  - Why needed here: Understanding that LMMs process text and images through separate encoders before fusion. JMMMU-Pro forces text to go through the visual encoder, changing the processing pipeline.
  - Quick check question: Can you explain why providing question text as image pixels vs. tokenized text changes what a model must do before reasoning begins?

- Concept: **OCR as a Vision-Language Bridge**
  - Why needed here: The paper explicitly identifies Japanese OCR as a major bottleneck. Understanding OCR limitations (character segmentation, layout variance) is prerequisite to interpreting the results.
  - Quick check question: What types of OCR failures would cause a model to answer incorrectly even if its reasoning capability is perfect?

- Concept: **Benchmark Construction Paradigms**
  - Why needed here: The paper proposes a new paradigm (Vibe Benchmark Construction) distinct from fully manual or fully automated approaches. Understanding the trade-offs helps evaluate its validity.
  - Quick check question: Why might human verification + prompt adjustment be more scalable than human editing of generated content?

## Architecture Onboarding

- Component map:
  JMMMU benchmark (1,320 questions) -> Nano Banana Pro API with parameterized prompts -> Human verification layer -> Regeneration or manual construction -> Final cross-check

- Critical path:
  1. Question image + text from JMMMU → Prompt template with random parameters
  2. Nano Banana Pro generates composite image
  3. Human verifies text accuracy, image correctness, layout validity
  4. If failed: regenerate with same or adjusted prompt (max multiple rounds)
  5. If repeatedly failed: manual construction
  6. Final cross-check for consistency

- Design tradeoffs:
  - **Diversity vs. Control**: Six parameter dimensions enable layout diversity, but image tags within text cannot be controlled (degraded image quality when attempted)
  - **Automation vs. Quality**: 95% automated generation traded off against 5% requiring manual effort; manual cases cluster in predictable hard categories
  - **Realism vs. Reproducibility**: Photorealistic outputs (whiteboard, workbook, TV show) increase ecological validity but introduce generative variance

- Failure signatures:
  - **Generation failures**: Unrelated substituted images, unreadable text, missing question parts, unnatural artifacts
  - **Model failures on JMMMU-Pro**: Perceptual errors (misreading embedded text), reasoning errors (incorrect logic after extraction), layout confusion (misinterpreting visual structure)
  - **Evaluation parsing failures**: Reasoning models listing all options; paper modified parser to ignore auxiliary listings

- First 3 experiments:
  1. **Baseline OCR assessment**: Run all candidate LMMs on the OCR extraction task (Section 5.2 protocol) to establish the correlation benchmark before full evaluation.
  2. **Ablation by layout type**: Evaluate models separately on each background type (workbook, whiteboard, webpage, etc.) to identify layout-specific failure modes beyond OCR.
  3. **Direct vs. CoT comparison**: Run both prompting strategies on all models to confirm that optimal prompting varies by model-task combination (as shown in Figure 5), then select best-performing strategy per model for main results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific capabilities beyond Japanese OCR enable the substantial performance gap between closed-source and open-source LMMs on JMMMU-Pro?
- Basis in paper: [inferred] The OCR-performance correlation is only 0.593, and models with comparable OCR (Heron-NVILA vs. Sarashina2.2-V) show vastly different JMMMU-Pro accuracy, suggesting other factors remain unidentified.
- Why unresolved: The paper demonstrates OCR is necessary but not sufficient, but does not isolate which additional visual-textual integration abilities closed-source models possess.
- What evidence would resolve it: Ablation studies controlling for OCR ability across models, combined with fine-grained error analysis categorizing failure types beyond perceptual errors.

### Open Question 2
- Question: Why does Chain-of-Thought prompting effectiveness vary inconsistently between JMMMU and JMMMU-Pro for the same models?
- Basis in paper: [inferred] Figure 5 shows models like Pangea-7B and LLaVA-OV-1.5-8B prefer different prompting strategies across the two benchmarks, but no explanation is provided.
- Why unresolved: The paper reports the phenomenon but does not investigate whether this stems from modality differences, task complexity, or model-specific reasoning architectures.
- What evidence would resolve it: Systematic analysis of when CoT helps vs. hurts, coupled with inspection of intermediate reasoning steps in both settings.

### Open Question 3
- Question: Can Vibe Benchmark Construction be extended to reliably handle long question texts, small embedded fonts, and specialized domains (chemical formulas, musical notation)?
- Basis in paper: [explicit] The authors state: "We manually created 67 samples that Nano Banana Pro had difficulty generating" due to these specific characteristics (Section 3.3).
- Why unresolved: The current pipeline requires manual fallback for ~5% of cases, limiting scalability for certain domains.
- What evidence would resolve it: Testing alternative generation models or prompt engineering strategies targeting these failure modes, measuring reduction in manual intervention rate.

## Limitations
- The methodology relies on the generative model's ability to render Japanese text accurately, with 29% of generations requiring regeneration
- Manual fallback for 67 cases (~5%) represents a known limitation where extreme cases must be handled separately
- OCR accuracy correlation (r=0.593) shows that while text extraction is a major factor, nearly 65% of performance variance remains unexplained
- Benchmark's exclusive focus on Japanese may limit generalizability to other languages
- Use of closed-source models as upper bounds creates a ceiling effect that prevents understanding theoretical maximum performance

## Confidence
- **High Confidence**: The performance gap between JMMMU and JMMMU-Pro is real and substantial, as demonstrated by consistent results across multiple open-source models. The OCR correlation finding is well-supported by the data.
- **Medium Confidence**: The claim that JMMMU-Pro specifically tests integrated visual-textual understanding beyond OCR capabilities is plausible but not definitively proven, as the unexplained variance could stem from other factors like reasoning complexity or layout interpretation.
- **Low Confidence**: The scalability claim for Vibe Benchmark Construction beyond Japanese contexts is speculative, as the methodology hasn't been validated in other languages or domains.

## Next Checks
1. **OCR Ablation Study**: Run models with perfect OCR (using ground truth text extraction) on JMMMU-Pro to quantify the exact contribution of OCR errors vs. reasoning errors to the performance gap.
2. **Cross-Lingual Adaptation**: Apply the Vibe Benchmark Construction methodology to an English or multilingual multimodal benchmark to test whether the performance degradation pattern replicates, validating the approach's generalizability.
3. **Error Analysis by Domain**: Conduct detailed error analysis separating perceptual failures (OCR/reading errors), reasoning failures (correct text but wrong answer), and layout failures (misinterpreting visual structure) to identify which aspects most limit current LMMs on JMMMU-Pro.