---
ver: rpa2
title: 'No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear
  Probes'
arxiv_id: '2509.10625'
source_url: https://arxiv.org/abs/2509.10625
tags:
- dataset
- triviaqa
- direction
- correctness
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors investigate whether LLMs anticipate their own correctness\
  \ before generating an answer. They extract residual stream activations immediately\
  \ after reading a question (before any tokens are generated), and train linear probes\
  \ to predict whether the model\u2019s forthcoming answer will be correct."
---

# No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes

## Quick Facts
- arXiv ID: 2509.10625
- Source URL: https://arxiv.org/abs/2509.10625
- Reference count: 40
- Primary result: A simple linear probe on residual stream activations before answer generation can predict correctness on factual tasks, generalizing from TriviaQA to diverse knowledge domains but failing on mathematical reasoning

## Executive Summary
This paper investigates whether large language models encode their own answer correctness before generating a response. By extracting residual stream activations immediately after reading a question but before any tokens are generated, the authors train linear probes to predict whether the forthcoming answer will be correct. Across three model families ranging from 7 to 70 billion parameters, they find that a simple difference-of-means linear probe trained on TriviaQA generalizes to diverse out-of-distribution knowledge datasets, outperforming black-box baselines and verbalized confidence. The predictive signal emerges in intermediate layers and correlates with "I don't know" responses, suggesting the direction also captures confidence. However, generalization fails on mathematical reasoning tasks, indicating that factual correctness and arithmetic reasoning may be structurally distinct.

## Method Summary
The method extracts residual stream activations at the final token of each question (before any generation) across all layers. Answers are generated at temperature=0 and labeled correct/incorrect via exact string matching. A difference-of-means linear probe is trained by computing the centroid difference between correct and incorrect activations (w = μ_true - μ_false), then projecting new activations onto this direction to compute AUROC. Layer selection uses 3-fold cross-validation on a 10K subset of TriviaQA, choosing the layer with highest AUROC. The final probe is evaluated on held-out folds and out-of-distribution datasets (Cities, Notable People, Medals), with verbalized confidence and black-box baselines for comparison.

## Key Results
- A linear probe trained on TriviaQA achieves AUROC of 0.67-0.75 on factual knowledge datasets, outperforming black-box baselines by 10-22 points
- Predictive power saturates in intermediate layers (typically between midpoint and final layer), suggesting self-assessment emerges mid-computation
- The correctness direction generalizes from TriviaQA to diverse factual domains but fails completely on GSM8K mathematical reasoning tasks
- The probe shows strong correlation with verbalized confidence ("I don't know" responses) when evaluated

## Why This Works (Mechanism)

### Mechanism 1: Linear Separability of Correctness in Residual Stream
The residual stream aggregates information from all layers, encoding the model's knowledge state about whether it has the answer. This correctness/confidence signal appears as a linearly accessible direction in activation space, computed via centroid difference (w = μ_true - μ_false) and projected onto new activations.

### Mechanism 2: Mid-Computation Crystallization
Early layers primarily encode surface features while intermediate layers integrate semantic and knowledge-related representations. The correctness signal emerges as knowledge retrieval consolidates in these middle layers, with performance saturating around the midpoint before remaining stable through later layers.

### Mechanism 3: Factual-Reasoning Misalignment
Factual correctness relies on a "knowledge retrieval" circuit with distinct geometry, while arithmetic reasoning requires multi-step computation with different internal verification mechanisms. The linear direction captures the former but not the latter, explaining generalization failure on mathematical tasks.

## Foundational Learning

- **Residual Stream Activations**: The accumulated information after processing the question, extracted at the final token position before generation. *Why needed*: The probe operates on this pre-generation state to predict correctness. *Quick check*: If you extracted activations from layer 0 (embedding only), would you expect high AUROC?

- **Difference-of-Means Linear Probe**: Computes w = μ_true - μ_false and uses dot-product projection to score new activations. *Why needed*: This simple method tests the Linear Representation Hypothesis while remaining interpretable. *Quick check*: Why might a linear probe generalize better than a non-linear XGBoost classifier trained on external embeddings?

- **AUROC (Area Under ROC Curve)**: Threshold-invariant metric measuring separability between correct and incorrect samples. *Why needed*: The paper uses AUROC as the primary metric because it directly measures discriminative power. *Quick check*: If AUROC = 0.5, what does that imply about the direction's discriminative power?

## Architecture Onboarding

- **Component map**: Activation Extraction -> Correctness Labeling -> Centroid Computation -> Layer Selection -> OOD Evaluation
- **Critical path**: Activation extraction → correctness labeling → centroid computation → layer selection → OOD evaluation. The layer selection step is key—using the wrong layer degrades performance.
- **Design tradeoffs**: Linear probe vs. non-linear (linear chosen for interpretability and to test Linear Representation Hypothesis); single-layer vs. multi-layer (single layer chosen for simplicity and computational efficiency); TriviaQA chosen for diversity (smaller domain-specific datasets may capture spurious correlations)
- **Failure signatures**: AUROC near 0.5 on all datasets (direction not learned or layer selection failed); high in-distribution AUROC but near-random OOD (overfitting to dataset-specific cues); strong performance on factual tasks but random on GSM8K (expected, not a failure); high variance across folds (insufficient training data or noisy labels)
- **First 3 experiments**: (1) Reproduce layer sweep on TriviaQA: extract activations every 2-4 layers across 6 models; verify AUROC peaks in mid-layers; (2) OOD generalization test: train probe on TriviaQA at optimal layer; evaluate on Cities, Notable People, Medals; expect 10-22 AUROC point improvement over black-box baselines; (3) Math failure confirmation: evaluate the same probe on GSM8K; expect AUROC near 0.5 across all models

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset filtering criteria for the 60K TriviaQA subset are not specified, making exact replication difficult
- Correctness evaluation granularity unclear (exact string match vs. semantic equivalence handling)
- Layer selection methodology uses specific spacing (every 2 layers for <10B, every 4 for >10B) without justification for these intervals

## Confidence
- **High Confidence**: Linear probes can predict correctness from pre-generation activations on factual tasks
- **Medium Confidence**: Mid-computation crystallization pattern (optimal layers in middle range)
- **Low Confidence**: Factual and mathematical reasoning use fundamentally different internal representations

## Next Checks
1. **Probe geometry verification**: Visualize the learned direction by projecting diverse activations onto w and examining the distribution for correct vs incorrect samples
2. **Layer stability analysis**: Repeat layer selection across multiple random seeds and training subsets to quantify variance in optimal layer identification
3. **Cross-model direction transfer**: Train a probe on one model family (e.g., Llama) and evaluate on another (e.g., Qwen) to test whether the correctness direction is model-specific or general