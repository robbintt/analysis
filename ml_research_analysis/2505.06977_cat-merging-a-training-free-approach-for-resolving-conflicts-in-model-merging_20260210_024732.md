---
ver: rpa2
title: 'CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging'
arxiv_id: '2505.06977'
source_url: https://arxiv.org/abs/2505.06977
tags:
- merging
- task
- uni00000014
- knowledge
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CAT Merging introduces a training-free approach for resolving\
  \ knowledge conflicts during multi-task model merging. The method selectively trims\
  \ conflict-prone components from task vectors using parameter-specific strategies\u2014\
  projection for linear weights and masking for normalization scalers and shifts\u2014\
  based on a lightweight forward pass with unlabeled exemplars."
---

# CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging

## Quick Facts
- **arXiv ID:** 2505.06977
- **Source URL:** https://arxiv.org/abs/2505.06977
- **Reference count:** 40
- **Primary result:** Achieves up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) accuracy improvements over state-of-the-art training-free model merging methods.

## Executive Summary
CAT Merging introduces a training-free approach to resolve knowledge conflicts during multi-task model merging. The method selectively trims conflict-prone components from task vectors using parameter-specific strategies—projection for linear weights and masking for normalization scalers and shifts—based on a lightweight forward pass with unlabeled exemplars. Extensive experiments on vision, language, and vision-language tasks demonstrate that CAT Merging significantly outperforms existing methods while maintaining robustness with limited exemplars.

## Method Summary
CAT Merging builds on Task Arithmetic by first computing task vectors as the difference between finetuned and pretrained models. It then performs a forward pass with unlabeled exemplars to collect layer inputs for each task. For linear weights, it computes a removal basis using top eigenvectors of a conflict matrix and projects out conflict-prone components. For normalization layers, it calculates binary masks to remove high-impact dimensions. The trimmed task vectors are aggregated to form the merged model. The method is training-free and requires only a small number of unlabeled exemplars per task.

## Key Results
- Achieves average accuracy improvements of up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods
- Maintains robustness with limited exemplars (2-3 per task)
- Reduces knowledge conflict compared to existing approaches as shown through ablation studies
- Demonstrates consistent improvements across vision, language, and vision-language tasks

## Why This Works (Mechanism)

### Mechanism 1: Conflict-Aware Projection for Linear Weights
Removes components of task vectors that maximize conflict with a target task through projection. Computes a "removal basis" using top eigenvectors of the difference between feature correlations, then projects out components aligned with this basis. Assumes feature-level perturbations are primary conflict drivers and minimizing squared feature differences bounds the conflict.

### Mechanism 2: Masking for Normalization Scalers
Mitigates conflict in normalization layers by removing dimensions with high task-specific impact. Calculates a binary mask by identifying top dimensions where squared task vector elements weighted by input features contribute most to conflict. Assumes element-wise Hadamard product interactions between features and scaling parameters are significant sources of interference.

### Mechanism 3: Masking for Normalization Shifts
Decouples and removes additive interference in normalization shifts using conflict-based masks. Similar to scalers but mask selection depends on raw squared task vector values, assuming shift conflicts are independent of input magnitude and depend on accumulated vector differences.

## Foundational Learning

- **Task Arithmetic**: CAT Merging is built on this concept, using task vectors ($T_k = W_k - W_0$) as fundamental units. Why needed: Defines the basic knowledge manipulation framework. Quick check: Why might simply adding two task vectors ($T_A + T_B$) degrade performance compared to adding them individually?

- **Lipschitz Continuity**: The theoretical justification relies on this assumption to bound knowledge conflict. Why needed: Enables the conflict upper bound theorem. Quick check: How does the Lipschitz constant $\gamma_l$ relate weight perturbations to output feature perturbations?

- **Eigendecomposition**: Used to determine the projection basis for linear weights. Why needed: Identifies the conflict directions to remove. Quick check: Why are the top eigenvectors chosen for the "removal basis" rather than the bottom ones?

## Architecture Onboarding

- **Component map:** Pretrained model + Finetuned models + Unlabeled Exemplars -> Feature Collector -> Conflict Solver -> Trimmer -> Merger -> Merged Model

- **Critical path:** The Feature Collector step. If exemplars are not representative or forward pass is cached incorrectly, the computed conflict basis will be misaligned with the true feature space.

- **Design tradeoffs:**
  - Parameter $c$ (Trimming Dim): Increasing $c$ removes more conflict but risks catastrophic forgetting
  - Parameter $\lambda$ (Trade-off weight): High $\lambda$ preserves donor task knowledge over target task conflict reduction
  - Layer Granularity: Layer-by-layer application is more precise but computationally heavier than global trimming

- **Failure signatures:**
  - Catastrophic Drop: Check if $\lambda$ is too low (aggressively deleting knowledge)
  - No Improvement over Task Arithmetic: Check if $c$ is too small (not removing enough conflict)
  - High Variance: Check if exemplar sample size is too small for reliable covariance estimation

- **First 3 experiments:**
  1. Baseline Reproduction: Implement Task Arithmetic vs. CAT Merging on SUN397 and Cars using ViT-B/32 to verify 2-3% accuracy lift
  2. Sensitivity Sweep: Grid search $c \in \{1, 2, 4, 8\}$ and $\lambda \in \{0.1, 0.5, 1.0\}$ to observe method stability
  3. Ablation by Parameter Type: Disable trimming for Linear weights only (keep Norm) and vice versa to validate Table 5 results

## Open Questions the Paper Calls Out

### Open Question 1
Can the dimensionality of the removal basis `c` be determined adaptively rather than manually? The paper states `c` is a "manually defined dimensionality parameter" without providing an automatic selection method. This requires empirical tuning that may vary across architectures.

### Open Question 2
Does the layer-wise isolation strategy fail to capture global knowledge conflicts spanning multiple layers? The conflict calculation uses only unperturbed parameters, ignoring cascading effects from previous layers. Optimizing only for local layer-wise conflicts might overlook complex non-linear interactions across full network depth.

### Open Question 3
How does CAT Merging perform under extreme data imbalances between tasks? The theoretical derivation for shift parameter masks assumes equal data amounts across tasks. This simplification may degrade if task datasets have vastly different sample sizes.

## Limitations

- **Hyperparameter Sensitivity**: Performance depends critically on manual selection of trimming dimension $c$ and trade-off weight $\lambda$ without principled selection methods
- **Exemplar Quality Dependency**: Conflict detection relies on forward passes with unlabeled exemplars, but the impact of exemplar quality, quantity, and distribution on trimming decisions is not thoroughly explored
- **Theoretical Generalization**: The theoretical upper bound assumes Lipschitz continuity and relies on limited exemplar estimates, with practical tightness and generalization to different architectures not established

## Confidence

- **High Confidence**: The core mechanism of using projection and masking to selectively remove conflict-prone components is clearly defined and supported by ablation studies showing performance degradation when disabled
- **Medium Confidence**: Empirical performance improvements are well-documented across multiple task combinations and model sizes, though hyperparameter sensitivity introduces uncertainty
- **Low Confidence**: The theoretical justification provides a framework for understanding conflict, but practical application and tightness of the bound in real-world scenarios is not fully validated

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Conduct comprehensive grid search over $c \in \{1, 2, 3, 4, 5, 6\}$ and $\lambda \in \{0.1, 0.5, 1.0, 2.0\}$ across multiple task pairs to map stability landscape

2. **Exemplar Quality Experiment**: Compare CAT Merging performance using different exemplar selection strategies (random vs. class-balanced vs. adversarial) to quantify robustness to input data quality

3. **Theoretical Bound Validation**: Implement the theoretical upper bound from Theorem 4.4 and measure its empirical tightness across various model merging scenarios to assess practical relevance of the theoretical framework