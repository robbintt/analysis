---
ver: rpa2
title: 'WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large Language
  Models'
arxiv_id: '2503.01330'
source_url: https://arxiv.org/abs/2503.01330
tags:
- cache
- values
- attention
- merging
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WeightedKV compresses KV cache by discarding unimportant token
  keys and merging their values into neighboring tokens via a convex combination weighted
  by average attention scores. This training-free approach preserves crucial information
  while reducing memory usage, as values contain more evenly distributed information
  compared to keys.
---

# WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large Language Models

## Quick Facts
- arXiv ID: 2503.01330
- Source URL: https://arxiv.org/abs/2503.01330
- Authors: Jian Yuan; Ziwei He; Haoli Bai; Jingwen Leng; Bo Jiang
- Reference count: 21
- Primary result: WeightedKV achieves lower perplexity than baseline methods (StreamingLLM, H2O, TOVA, CaM) on multiple datasets using Llama-2-7B

## Executive Summary
WeightedKV introduces a training-free method for compressing key-value (KV) cache in large language models by selectively discarding unimportant token keys and merging their values into neighboring tokens using convex combinations weighted by average attention scores. The approach leverages the observation that values contain more evenly distributed information compared to keys, allowing for more aggressive compression with minimal impact on subsequent attention weights. Evaluated across four language modeling datasets (PG19, OpenWebText2, ProofPile, ArXiv) using Llama-2-7B, WeightedKV consistently outperforms baseline methods while maintaining context integrity during long text generation.

## Method Summary
The method operates by first computing attention scores between tokens, then using these scores to weight a convex combination when merging values of discarded keys into neighboring tokens. Unlike traditional KV cache compression approaches that simply truncate or merge based on position, WeightedKV's attention-score-weighted merging preserves more relevant information by giving higher weight to tokens that contribute more to the model's attention distribution. The training-free nature of the approach makes it immediately deployable across different model architectures without fine-tuning requirements.

## Key Results
- Achieves lower perplexity than StreamingLLM, H2O, TOVA, and CaM baselines on all tested datasets
- Particularly effective with smaller cache sizes, demonstrating superior compression efficiency
- Shows minimal perturbation to subsequent attention weights, validating information preservation claims
- Maintains context integrity during long text generation tasks

## Why This Works (Mechanism)
The effectiveness stems from the asymmetric nature of information distribution in KV caches - keys tend to have highly peaked attention distributions focused on specific tokens, while values contain more evenly distributed information that can be merged with minimal quality loss. By weighting the merging process with attention scores, the method preserves the most relevant information while discarding redundant or less important tokens. The convex combination ensures that merged values remain within the same representational space as the original values, maintaining compatibility with the model's attention mechanism.

## Foundational Learning

**Attention Score Distribution:** Understanding how attention scores are distributed across tokens is crucial for identifying which tokens carry the most relevant information. Quick check: Verify that high-attention tokens indeed correspond to semantically important positions in sample text.

**KV Cache Compression Trade-offs:** The fundamental tension between memory efficiency and information preservation determines the practical limits of compression methods. Quick check: Measure perplexity degradation rate as compression ratio increases.

**Convex Combination Properties:** The mathematical properties of convex combinations ensure that merged values remain bounded within the original value space, preventing representational drift. Quick check: Confirm that merged values maintain norm constraints similar to original values.

**Attention Mechanism Sensitivity:** Understanding how small perturbations in KV cache affect subsequent attention calculations is essential for validating compression methods. Quick check: Measure attention weight changes before and after compression.

## Architecture Onboarding

**Component Map:** Token Generation -> Attention Scores Computation -> Key-Value Cache -> WeightedKV Compression -> Compressed Cache -> Subsequent Attention Layers

**Critical Path:** The attention computation path remains the primary bottleneck, with WeightedKV operating as a preprocessing step that modifies the KV cache before attention calculations. The method adds minimal computational overhead since attention scores are already computed during normal operation.

**Design Tradeoffs:** The training-free approach sacrifices potential dataset-specific optimization for immediate deployability and generality. The method prioritizes perplexity preservation over other quality metrics, which may not capture all aspects of generation quality.

**Failure Signatures:** Performance degradation occurs when the attention score distribution becomes too uniform (reducing the effectiveness of weighted merging) or when key tokens are incorrectly identified as unimportant due to local attention patterns.

**First Experiments:**
1. Test WeightedKV on a single long-context generation task to verify basic functionality and measure perplexity improvement over baselines
2. Perform ablation study varying the compression ratio to identify the optimal trade-off point
3. Compare attention weight distributions before and after compression to quantify perturbation magnitude

## Open Questions the Paper Calls Out
None

## Limitations
- Training-free nature prevents dataset-specific optimization and adaptation to different model architectures
- Evaluation primarily focuses on perplexity, lacking extensive downstream task performance analysis
- Scalability beyond 7B parameter models remains theoretical without extensive ablation studies
- Qualitative aspects of generation quality not thoroughly examined

## Confidence

**High confidence:** The core technical approach of weighted key-value merging based on attention scores is sound with clear mathematical formulation. The observation about values containing more evenly distributed information is well-supported by empirical results showing minimal perturbation to subsequent attention weights.

**Medium confidence:** Baseline comparisons show consistent improvements, but evaluation setup doesn't fully account for implementation complexity trade-offs or edge cases where performance might degrade. The "minimal perturbation" claim needs more rigorous statistical validation.

**Low confidence:** Scalability claims beyond tested 7B model size are largely theoretical, lacking extensive ablation studies across different model scales. Impact on long-form generation quality beyond perplexity metrics is not thoroughly explored.

## Next Checks

1. Conduct extensive ablation studies testing WeightedKV across multiple model scales (1B, 13B, 70B parameters) to validate scalability claims and identify potential break points in method effectiveness.

2. Implement comprehensive downstream task evaluations (including reasoning, summarization, and code generation) to verify that perplexity improvements translate to meaningful quality gains in practical applications.

3. Perform statistical significance testing on attention weight perturbation measurements across multiple runs and datasets to rigorously quantify the "minimal" impact claim and identify edge cases where the method might cause larger deviations.