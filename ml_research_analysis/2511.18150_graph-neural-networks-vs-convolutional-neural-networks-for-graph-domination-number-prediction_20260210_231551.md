---
ver: rpa2
title: Graph Neural Networks vs Convolutional Neural Networks for Graph Domination
  Number Prediction
arxiv_id: '2511.18150'
source_url: https://arxiv.org/abs/2511.18150
tags:
- graph
- neural
- graphs
- networks
- domination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares graph neural networks (GNNs) and convolutional
  neural networks (CNNs) for predicting the domination number of graphs, a hard combinatorial
  invariant. CNNs operate on adjacency matrices treated as images, while GNNs use
  message-passing to learn directly from graph structure.
---

# Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction

## Quick Facts
- arXiv ID: 2511.18150
- Source URL: https://arxiv.org/abs/2511.18150
- Reference count: 8
- Primary result: GNNs achieve R² = 0.987, MAE = 0.372 on domination number prediction vs CNNs at R² = 0.955, MAE = 0.500

## Executive Summary
This paper compares graph neural networks (GNNs) and convolutional neural networks (CNNs) for predicting the domination number of graphs, a hard combinatorial invariant. CNNs operate on adjacency matrices treated as images, while GNNs use message-passing to learn directly from graph structure. Experiments on 2,000 Erd˝os–Rényi graphs show that GNNs achieve substantially higher accuracy than CNNs. GNNs also provide over 200× speedup compared to exact solvers, whereas CNNs only reach 7.4× speedup.

The results highlight the superiority of GNNs in learning structural graph invariants, positioning them as practical surrogates for NP-hard graph problems. While both architectures show promise, GNNs demonstrate significantly better performance in both accuracy and computational efficiency for this challenging graph prediction task.

## Method Summary
The study evaluates two distinct neural network architectures for predicting graph domination numbers. CNNs process adjacency matrices as grayscale images with 1×1 convolutions, treating the problem as spatial pattern recognition. GNNs employ message-passing frameworks that aggregate neighborhood information through multiple layers, directly leveraging graph topology. Both models are trained on 2,000 Erd˝os–Rényi graphs with 10-20 nodes, using domination number as the target variable. Performance is measured against exact solvers and compared using R² and MAE metrics across different graph families.

## Key Results
- GNNs achieve R² = 0.987 and MAE = 0.372, significantly outperforming CNNs at R² = 0.955 and MAE = 0.500
- GNNs provide over 200× speedup compared to exact solvers, while CNNs only reach 7.4× speedup
- GNNs maintain robust performance across graph sizes and show better generalization to Barabási–Albert graphs

## Why This Works (Mechanism)
GNNs excel at domination number prediction because they inherently capture graph topology through message-passing, while CNNs must learn structural patterns from pixel representations of adjacency matrices. The message-passing mechanism allows GNNs to aggregate information from local neighborhoods in a way that naturally reflects graph connectivity, whereas CNNs treat spatial relationships in the adjacency matrix as image features. This structural advantage enables GNNs to better learn the combinatorial relationships that determine domination numbers.

## Foundational Learning
- **Graph domination number**: The minimum size of a dominating set in a graph. Why needed: It's the target variable being predicted and represents a fundamental NP-hard graph invariant. Quick check: Verify understanding of dominating set definition and why it's computationally hard.
- **Message-passing in GNNs**: Nodes exchange information with neighbors across layers. Why needed: This mechanism allows GNNs to capture local graph structure relevant to domination. Quick check: Understand how information propagates through graph neighborhoods.
- **Adjacency matrix representation**: Square matrix encoding graph edges. Why needed: CNNs require this matrix format to process graphs as images. Quick check: Verify ability to construct adjacency matrices from graph examples.
- **R² and MAE metrics**: Statistical measures of prediction accuracy. Why needed: These metrics quantify model performance for regression tasks. Quick check: Calculate these metrics from sample predictions and ground truth values.

## Architecture Onboarding

Component Map: Input Graph -> Adjacency Matrix Processing -> Feature Extraction -> Prediction Layer -> Domination Number Output

Critical Path: The essential computation path involves graph representation conversion (either as adjacency matrix or graph structure), feature learning through convolutions or message-passing, and final regression to predict the domination number. For GNNs, this includes multiple message-passing steps where node features are aggregated from neighbors. For CNNs, it involves convolution operations on the adjacency matrix image.

Design Tradeoffs: GNNs sacrifice some computational efficiency for better structural awareness, while CNNs gain speed but lose graph-specific inductive biases. The choice between architectures involves balancing prediction accuracy against computational resources and the specific characteristics of the graph domain.

Failure Signatures: CNNs typically fail on graphs with non-local structural patterns that don't translate well to image features. GNNs may struggle with very large graphs due to message-passing depth limitations and can overfit on small graph datasets. Both architectures show domain-specific performance drops when tested on graph families different from training data.

Three First Experiments:
1. Train both models on Erd˝os–Rényi graphs and evaluate on held-out test set
2. Compare prediction times against exact domination number solver on same dataset
3. Test cross-domain generalization by evaluating on Barabási–Albert graphs

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to Erd˝os–Rényi and Barabási–Albert random graphs, raising generalizability concerns to real-world graphs
- Cross-domain generalization remains limited with clear performance drops on different graph families
- Speedup comparisons assume parallel GPU execution and may not reflect practical deployment overhead

## Confidence
- High confidence in relative performance comparison between GNNs and CNNs
- Medium confidence in generalizability claims due to synthetic graph limitations
- Medium confidence in speedup measurements without practical deployment validation

## Next Checks
1. Test model performance on real-world graph datasets from diverse domains (social networks, biological networks, transportation networks) to assess practical applicability beyond synthetic graphs.

2. Evaluate prediction calibration and uncertainty estimates for both GNNs and CNNs to determine reliability in high-stakes applications where domination number predictions inform critical decisions.

3. Investigate hybrid architectures that combine GNN message-passing with CNN spatial features to determine if cross-domain generalization can be improved while maintaining prediction accuracy.