---
ver: rpa2
title: 'Examining Linguistic Shifts in Academic Writing Before and After the Launch
  of ChatGPT: A Study on Preprint Papers'
arxiv_id: '2505.12218'
source_url: https://arxiv.org/abs/2505.12218
tags:
- writing
- science
- academic
- computer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the impact of Large Language Models (LLMs)
  like ChatGPT on academic writing styles using a dataset of 823,798 abstracts from
  arXiv. Through a comprehensive analysis of 30+ linguistic indicators, the research
  reveals significant shifts in academic writing following ChatGPT's release in November
  2022.
---

# Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers

## Quick Facts
- **arXiv ID:** 2505.12218
- **Source URL:** https://arxiv.org/abs/2505.12218
- **Authors:** Tong Bao; Yi Zhao; Jin Mao; Chengzhi Zhang
- **Reference count:** 21
- **Primary result:** Significant linguistic shifts in academic writing following ChatGPT's release in November 2022, including increased lexical complexity, decreased readability, and rise in LLM-preferred words

## Executive Summary
This study investigates how Large Language Models (LLMs) like ChatGPT have influenced academic writing styles by analyzing a dataset of 823,798 abstracts from arXiv. Using 30+ linguistic indicators, the research reveals significant changes in academic writing patterns following ChatGPT's November 2022 release. The findings demonstrate that LLMs are reshaping academic writing through increased lexical complexity, new terminology adoption, and shifts toward more formulaic expressions.

The study particularly highlights that non-native English speakers show more pronounced stylistic changes, and Computer Science exhibits greater LLM influence compared to Mathematics and Physics. These results suggest that LLMs are not merely tools for text generation but are actively transforming the linguistic landscape of academic discourse, with implications for academic integrity, communication standards, and the evolution of scholarly writing practices.

## Method Summary
The researchers analyzed 823,798 abstracts from arXiv, examining linguistic patterns before and after ChatGPT's release in November 2022. They employed a comprehensive set of 30+ linguistic indicators to measure various aspects of academic writing, including lexical complexity, readability scores, syntactic complexity, and specific word usage patterns. The analysis compared writing styles across different disciplines (Computer Science, Mathematics, Physics) and author backgrounds (native vs. non-native English speakers). Statistical methods were used to identify significant shifts in linguistic patterns, with particular attention to changes in vocabulary choices, expression styles, and overall writing complexity metrics.

## Key Results
- Significant increase in lexical complexity and new terminology adoption following ChatGPT's release
- Decrease in readability and syntactic complexity across academic abstracts
- Notable rise in LLM-preferred words (adjectives/adverbs like "innovative" and "comprehensive")
- Non-native English speakers exhibit more pronounced stylistic changes than native speakers
- Computer Science shows greater LLM influence compared to Mathematics and Physics

## Why This Works (Mechanism)
The study demonstrates that LLMs like ChatGPT are fundamentally altering academic writing patterns by providing writers with new linguistic templates and expression styles. The mechanism appears to work through both direct usage (authors using ChatGPT to generate or refine text) and indirect influence (authors reading and internalizing LLM-generated content). The systematic nature of these changes across multiple linguistic indicators suggests that LLMs are standardizing certain aspects of academic expression while simultaneously enabling more complex vocabulary usage. The differential impact on non-native speakers and different disciplines indicates that LLM influence operates through both language proficiency gaps and field-specific communication norms.

## Foundational Learning
**Linguistic Analysis Metrics** - Understanding measures like lexical diversity, readability indices, and syntactic complexity is crucial for quantifying writing style changes. Quick check: Can you calculate Flesch-Kincaid readability scores and explain what they measure?

**LLM Language Patterns** - Familiarity with common LLM writing characteristics (formal tone, specific adjective/adverb usage, hedging phrases) is essential for identifying LLM influence. Quick check: What are the most distinctive linguistic markers of LLM-generated text?

**Academic Writing Norms** - Knowledge of discipline-specific writing conventions helps contextualize how different fields adopt LLM-influenced styles. Quick check: How do writing conventions differ between Computer Science and Mathematics abstracts?

**Cross-Cultural Communication** - Understanding how non-native speakers adapt writing strategies differently than native speakers is key to interpreting differential LLM impacts. Quick check: What linguistic features typically distinguish native from non-native academic writing?

## Architecture Onboarding
**Component Map:** Data Collection -> Linguistic Analysis -> Statistical Comparison -> Attribution Analysis
**Critical Path:** arXiv Dataset → Preprocessing → Feature Extraction → Temporal Analysis → Interpretation
**Design Tradeoffs:** Broad coverage vs. depth of analysis; observational inference vs. controlled experimentation; quantitative metrics vs. qualitative interpretation
**Failure Signatures:** Confounding variables from platform changes; insufficient sample size for certain sub-groups; overreliance on proxy indicators for LLM usage
**First Experiments:** 1) Replicate analysis on full-text papers to validate abstract-level findings; 2) Conduct controlled LLM usage study with tracked writing samples; 3) Implement author-level tracking to distinguish individual behavioral changes from cohort effects

## Open Questions the Paper Calls Out
None

## Limitations
- Observational nature prevents definitive causal attribution between ChatGPT and writing changes
- Limited to arXiv abstracts, potentially missing broader academic writing trends
- Binary treatment of native/non-native status based on country of origin oversimplifies linguistic background complexity

## Confidence
**High confidence** in overall linguistic shift observation supported by large-scale data and consistent patterns across multiple indicators
**Medium confidence** in attributing changes specifically to LLM usage due to observational limitations
**Low confidence** in distinguishing direct LLM usage from secondary influences

## Next Checks
1. Conduct controlled experiments with verified LLM usage to establish causal relationships between ChatGPT and specific linguistic changes
2. Expand analysis to include full-text papers and other academic disciplines to test generalizability of findings
3. Implement author-level tracking to examine individual writing evolution and distinguish between genuine behavioral change and cohort effects