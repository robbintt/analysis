---
ver: rpa2
title: Scaling Clinician-Grade Feature Generation from Clinical Notes with Multi-Agent
  Language Models
arxiv_id: '2508.01956'
source_url: https://arxiv.org/abs/2508.01956
tags:
- feature
- features
- clinical
- cancer
- snow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SNOW, a multi-agent LLM system for scalable
  clinician-grade feature generation from unstructured clinical notes. The system
  mimics expert feature generation workflows through specialized agents that propose
  features, extract values, validate results, and aggregate data.
---

# Scaling Clinician-Grade Feature Generation from Clinical Notes with Multi-Agent Language Models

## Quick Facts
- arXiv ID: 2508.01956
- Source URL: https://arxiv.org/abs/2508.01956
- Reference count: 40
- SNOW achieves 0.767 AUC-ROC for 5-year prostate cancer recurrence prediction, matching expert extraction while reducing human effort 48-fold

## Executive Summary
This paper introduces SNOW, a multi-agent LLM system that automates clinician-grade feature extraction from unstructured clinical notes by mimicking expert workflows. The system uses specialized agents for feature proposal, alignment, extraction, and validation to generate structured variables for outcome prediction. SNOW matches the performance of manual expert curation (AUC-0.762 vs 0.767) while reducing human oversight time from 240 hours to 5 hours for a 147-patient prostate cancer cohort. The approach demonstrates generalizability by improving mortality prediction in an external HFpEF cohort (AUC-0.851 for 30-day mortality) and produces interpretable, auditable features unlike black-box RFG methods.

## Method Summary
SNOW implements a multi-agent LLM architecture where specialized agents handle distinct aspects of feature generation: Feature Proposal Agent defines variables from note samples, Feature Alignment Agent refines the schema based on corpus-wide patterns, Extraction Agent parses values, and Validation Agent audits quality with iterative self-correction loops. The system uses Gemini 2.5 Pro/Flash for HFpEF extraction and produces JSON schemas and Python code for transparent feature generation. Downstream prediction uses regularized logistic regression with 3-fold nested cross-validation repeated 50 times. The approach contrasts with end-to-end RFG methods by maintaining interpretability while automating the labor-intensive aspects of chart abstraction.

## Key Results
- SNOW achieved AUC-0.767 for 5-year prostate cancer recurrence prediction, matching manual expert extraction (AUC-0.762)
- System reduced human effort from 240 hours to 5 hours (48-fold reduction) for a 147-patient cohort
- SNOW improved 30-day mortality prediction in external HFpEF cohort (AUC-0.851) and 1-year mortality prediction (AUC-0.763)
- SNOW outperformed baseline structured methods and RFG approaches across both clinical domains

## Why This Works (Mechanism)

### Mechanism 1: Iterative Self-Correction via the Extract-Validate Loop
The system improves extraction fidelity by autonomously identifying and repairing logic errors through a validation loop that audits extracted values against source text. The Feature Validation Agent detects inconsistencies like unit errors or uncapped outliers, generates new instructions, and triggers re-extraction before final table generation. Evidence shows the system corrected PSA velocity outliers (>600 ng/mL/year) and fixed "percent core involvement" calculations (166.67% â†’ 100%) autonomously.

### Mechanism 2: Workflow Decomposition Mimicking Clinical Reasoning
Decomposing feature generation into specialized sub-tasks (proposal, alignment, extraction) allows better handling of longitudinal clinical data complexity than end-to-end prompting. The system separates "what" (defining concepts based on corpus samples) from "how" (parsing specific notes), mirroring manual CFG protocols where experts first define concepts before extracting data. This architectural pattern is supported by related work using hierarchical structures for oncology and clinical notes.

### Mechanism 3: Data-Driven Feature Pruning and Alignment
The system maintains high model performance by autonomously discarding clinically relevant but practically unusable features due to data sparsity. Feature Alignment/Validation agents analyze data availability and remove features with high missing rates (>97%) to prevent regression instability, distinguishing between clinical importance and statistical robustness. The system dropped "immunocompromised status" and "surgical margin status" when specific document types were absent.

## Foundational Learning

- **Manual Chart Abstraction (CFG)**: Understanding that CFG is an adaptive, expert-driven process (not just data entry) is crucial to understanding why SNOW uses multi-agent reasoning rather than simple regex. Quick check: Why can't a simple regular expression extract "percent core involvement" from a pathology report?

- **Agentic Workflows vs. Chain-of-Thought**: The paper distinguishes between static prompting and an "agentic" system where components can use tools and iterate. The Validation Agent can reject and re-plan work, which differs from a single LLM call. Quick check: In the Extract-Validate Loop, what specific action does the Validation Agent take that prevents the system from outputting "garbage" data immediately?

- **RFG vs. Structured Extraction**: SNOW positions against RFG (e.g., Bag-of-Words, Embeddings) which trades interpretability for convenience. SNOW trades compute/latency for interpretable, auditable structured variables. Quick check: Why did "Baseline + BoW TF-IDF" models underperform compared to SNOW in the prostate cohort?

## Architecture Onboarding

- **Component map**: Feature Proposal Agent -> Feature Alignment Agent -> Feature Extraction Agent -> Feature Validation Agent -> Aggregation Code Generator
- **Critical path**: The Extract-Validate Loop where the Validation Agent reviews extraction quality and triggers re-extraction or pruning
- **Design tradeoffs**: Latency vs. Quality (thinking mode enabled only for Proposal/Alignment agents) and Scalability vs. Auditability (generates auditable artifacts but requires managing intermediate files)
- **Failure signatures**: Infinite Loops (repeated re-extraction requests), Schema Drift (hallucinated categories), High Missingness (features return >90% nulls)
- **First 3 experiments**:
  1. Trace the Loop: Run SNOW on a single patient note and inspect logs for validation agent issues
  2. Corpus Sensitivity: Run Feature Alignment Agent on small (n=5) vs. large (n=50) samples and compare resulting feature lists
  3. Ablation Study: Disable Validation Agent (always "Proceed") and measure drop in AUC-ROC and increase in data noise

## Open Questions the Paper Calls Out

- **Joint training optimization**: Whether end-to-end joint training of SNOW's agentic components improves feature extraction fidelity compared to the current modular approach, as agents are currently tuned independently rather than optimized end-to-end

- **Multimodal extension**: How to extend SNOW to incorporate imaging or waveforms which remain unmodeled, as current agents lack tools to interpret non-textual clinical data

- **Cross-institutional reproducibility**: Whether SNOW can maintain performance and required oversight levels across diverse institutions with varying documentation standards, as current validation was limited to two specific datasets

## Limitations
- The multi-agent architecture trades latency and cost for interpretability, requiring significantly more API calls than end-to-end RFG approaches
- Generalization remains unproven beyond two clinical domains (prostate cancer, HFpEF) with unknown performance in other specialties
- The extract-validate loop relies on agent self-assessment quality which may fail for subtle hallucinations or rare clinical concepts

## Confidence
- **High confidence**: Superior performance over baselines (AUC-0.767 vs 0.762) and 48-fold reduction in human effort are well-supported
- **Medium confidence**: Iterative self-correction mechanism is theoretically sound but requires further testing across diverse clinical scenarios
- **Medium confidence**: Workflow decomposition hypothesis is supported by design but would benefit from direct ablation studies

## Next Checks
1. Test SNOW on a third clinical domain (e.g., diabetes or sepsis) with different documentation patterns to assess true domain generalization
2. Conduct head-to-head comparison with Validation Agent disabled to quantify exact contribution of iterative correction loop
3. Measure system sensitivity to corpus sampling by running Feature Alignment Agent on increasingly small samples (n=5, 10, 20)