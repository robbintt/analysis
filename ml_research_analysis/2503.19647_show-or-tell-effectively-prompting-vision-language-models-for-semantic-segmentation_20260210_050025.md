---
ver: rpa2
title: Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation
arxiv_id: '2503.19647'
source_url: https://arxiv.org/abs/2503.19647
tags:
- visual
- segmentation
- lisa
- text
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work systematically investigates the effectiveness of prompting
  Vision-Language Models (VLMs) for semantic segmentation, comparing text and visual
  prompts. A scalable few-shot prompted semantic segmentation (FPSS) setup is introduced,
  inspired by open-vocabulary segmentation and few-shot learning, using a single annotated
  example per class.
---

# Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation

## Quick Facts
- arXiv ID: 2503.19647
- Source URL: https://arxiv.org/abs/2503.19647
- Reference count: 34
- VLMs significantly underperform specialist models by ~30% IoU on semantic segmentation

## Executive Summary
This work systematically investigates the effectiveness of prompting Vision-Language Models (VLMs) for semantic segmentation, comparing text and visual prompts. A scalable few-shot prompted semantic segmentation (FPSS) setup is introduced, inspired by open-vocabulary segmentation and few-shot learning, using a single annotated example per class. Evaluations on the MESS dataset collection reveal that VLMs significantly underperform compared to specialist models (by ~30% IoU) and that text and visual prompts are complementary—each excelling where the other fails. The analysis shows that anticipating the most effective prompt modality can improve performance by 11% IoU. Based on these findings, PromptMatcher is proposed, a training-free framework that combines both modalities, achieving state-of-the-art results with a 2.5% improvement over the best text-prompted VLM and 3.5% over the best visual-prompted VLM on few-shot prompted semantic segmentation.

## Method Summary
PromptMatcher is a training-free framework that combines LISA (text branch) and SoftMatcher+ (visual branch) for few-shot semantic segmentation. LISA uses a frozen LLaVA LLM and CLIP encoder to generate segmentation prompts from text class names, while SoftMatcher+ extracts features from reference and target images using AM-RADIO, performs probabilistic feature matching to generate point prompts, and decodes masks using SAM. Both branches' masks undergo verification via SoftMatcher+'s rejection pipeline, then merged via binary union. The approach is evaluated on the MESS dataset collection across 22 datasets and 5 domains, comparing text-prompted, visual-prompted, and combined approaches.

## Key Results
- VLMs underperform specialist models by ~30% IoU on MESS dataset collection
- Text and visual prompts are complementary, with each modality failing on samples the other can solve
- PromptMatcher achieves state-of-the-art results with 45.3% avg IoU, improving 2.5% over best text-prompted VLM and 3.5% over best visual-prompted VLM
- Anticipating optimal prompt modality can lead to 11% IoU improvement (Oracle Ensemble+ upper bound)

## Why This Works (Mechanism)

### Mechanism 1: Complementarity of Text and Visual Prompts
Text and visual prompts fail on different samples due to their distinct error modes—text struggles with polysemous words (e.g., "Date" → calendar vs. fruit) and rare class names, while visual prompts struggle with high intra-class variation (e.g., "building" spans huts to skyscrapers). This structural distinction enables combined approaches to recover failures from either modality alone.

### Mechanism 2: Feature Matching for Visual Prompt Transfer
Dense feature correspondence between reference and target images enables localization of semantically similar regions without training. SoftMatcher+ uses AM-RADIO features to compute probabilistic matches, generating point prompts that SAM decodes into masks, with a rejection step discarding inconsistent predictions.

### Mechanism 3: Mask Verification as Hallucination Suppression
Visual-prompt consistency acts as a verifier to reduce false positives from text-prompted VLMs. PromptMatcher applies SoftMatcher+'s rejection pipeline to masks from both LISA and SoftMatcher+, discarding masks that fail consistency checks with the reference, then merging remaining masks via union.

## Foundational Learning

- **Vision-Language Models (VLMs)**: Understand how models like LISA, Florence-2, and CLIP-style encoders connect image patches to text embeddings through shared embedding spaces.
  - Quick check: Can you explain how CLIP-style vision encoders connect image patches to text embeddings?

- **Semantic Segmentation and IoU Metric**: IoU measures segmentation quality as intersection-over-union between prediction and ground truth, penalizing both false positives and false negatives equally.
  - Quick check: Given a prediction mask and ground truth, how would you compute IoU? What does IoU penalize most?

- **Few-shot Prompted Semantic Segmentation (FPSS)**: A task formulation using frozen VLMs with either text prompts (class names) or visual prompts (reference image + mask), without task-specific training, differing from traditional few-shot learning and open-vocabulary segmentation.
  - Quick check: How does FPSS differ from referring segmentation? Why can't referring segmentation datasets like RefCOCO be directly adapted?

## Architecture Onboarding

- **Component map**: LISA text branch (LLaVA + CLIP + SAM) → masks; SoftMatcher+ visual branch (AM-RADIO + matching + SAM) → masks; Verifier (SoftMatcher+ rejection) → filtered masks; Merger (union) → final output

- **Critical path**: Feature extraction from reference and target (dominant compute) → Matching → point sampling → SAM decode (both branches) → Verification (cross-reference consistency) → Mask union

- **Design tradeoffs**: Training-free avoids domain adaptation complexity but limits performance ceiling; union merging increases recall but may cause over-segmentation; single reference image minimizes user overhead but limits class variation representation

- **Failure signatures**: High-variation classes (e.g., "building") underperform with VP (Table 7: Pole VP=7.64 vs TP=41.71); ambiguous text classes (e.g., "tool") underperform with TP (Figure 2: hallucinates camera UI elements); medical domain: VP fails entirely on CHASE DB1 (0.0 IoU)

- **First 3 experiments**:
  1. Reproduce Oracle Ensemble+ on MESS subset: For each image, run both LISA and SoftMatcher+, select higher-IoU mask. Confirm ~11% gap over LISA alone.
  2. Ablate verification: Run PromptMatcher with verification disabled. Quantify hallucination increase (expect more false positives, especially on ambiguous text classes).
  3. Reference count sensitivity: Test 1-shot vs. 3-shot vs. 5-shot visual prompting on a single MESS dataset (e.g., FoodSeg103). Assess if multiple references close the TP-VP gap for ambiguous classes.

## Open Questions the Paper Calls Out

### Open Question 1
Can a dynamic mechanism be developed to automatically select the optimal prompting modality (text or visual) for a specific input instance? The authors calculate that "being able to anticipate the most effective prompt modality can lead to a 11% improvement," contrasting the proposed static method with the Oracle Ensemble+ upper bound. PromptMatcher relies on a fixed strategy of generating masks with both modalities and merging them, rather than learning to predict which modality is superior for a given sample.

### Open Question 2
Do more elaborate, trainable framework designs outperform the training-free PromptMatcher in few-shot prompted semantic segmentation? The authors "leave the exploration of more elaborate framework designs to future work," noting their method is a "remarkably simple training-free baseline." The paper only evaluates a training-free approach utilizing mask union and rejection; the potential benefits of learned attention-based fusion or fine-tuning remain unexplored.

### Open Question 3
To what extent can advanced prompt engineering (e.g., chain-of-thought reasoning or ensembling) resolve the ambiguity failures observed in text-prompted segmentation? The authors note that prompt engineering has become "an art in itself" and "goes beyond the scope of the present work," despite its potential to address the identified linguistic ambiguities. The evaluation relies on elementary prompts (e.g., "Segment all instances of class_name"), leaving the efficacy of complex reasoning prompts for disambiguating classes like "Date" or "Tool" untested.

## Limitations
- VLMs significantly underperform specialist models (~30% IoU gap) on semantic segmentation tasks
- Medical domain performance is near-zero with visual prompting, suggesting fundamental limitations
- Training-free design limits performance ceiling compared to fine-tuned approaches

## Confidence
- **High confidence**: VLMs underperforming specialist models by ~30% IoU is well-supported by direct experimental comparison; architectural descriptions are sufficiently detailed for reproduction
- **Medium confidence**: Complementarity mechanism demonstrated through oracle experiments on MESS but requires external validation; state-of-the-art claims qualified by specific task definition
- **Low confidence**: Medical domain performance claims (0.0 IoU for visual prompting on CHASE DB1) may reflect dataset-specific challenges rather than fundamental VLM limitations

## Next Checks
1. **Cross-dataset generalization test**: Apply PromptMatcher to a non-MESS dataset (e.g., COCO, PASCAL VOC, or a medical dataset not in MESS) and measure the TP-VP complementarity effect. Compare oracle ensemble performance to individual modalities to verify the ~11% IoU improvement holds.

2. **Failure mode characterization study**: Systematically analyze where PromptMatcher fails by running error analysis on 100 randomly selected images from MESS. Categorize failures by type (text hallucination, visual matching error, verification rejection) and compute precision-recall curves for each modality and the combined approach.

3. **Prompt sensitivity analysis**: Test the effect of multiple reference images (1-shot vs 3-shot vs 5-shot) on visual prompting performance for classes with high intra-class variation. Measure whether providing diverse visual examples reduces the performance gap between text and visual prompts for classes like "building" and "boat".