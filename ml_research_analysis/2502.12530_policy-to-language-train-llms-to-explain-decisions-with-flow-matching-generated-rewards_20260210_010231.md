---
ver: rpa2
title: 'Policy-to-Language: Train LLMs to Explain Decisions with Flow-Matching Generated
  Rewards'
arxiv_id: '2502.12530'
source_url: https://arxiv.org/abs/2502.12530
tags:
- learning
- explanation
- arxiv
- flow
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a model-agnostic explanation generator that
  trains an LLM to explain agent decisions using rewards generated by a rectified
  flow matching model. The method leverages a GUIDANCE LLM to provide positive samples
  and uses a specially designed rectified flow network with cross-attention to incorporate
  linguistic cues, enabling effective generalization to negative samples.
---

# Policy-to-Language: Train LLMs to Explain Decisions with Flow-Matching Generated Rewards

## Quick Facts
- **arXiv ID**: 2502.12530
- **Source URL**: https://arxiv.org/abs/2502.12530
- **Reference count**: 40
- **Primary result**: 4%-20% accuracy improvement over SFT and RLHF baselines

## Executive Summary
This paper introduces a model-agnostic explanation generator that trains large language models (LLMs) to explain agent decisions using rewards generated by a rectified flow matching model. The approach leverages a GUIDANCE LLM to provide positive samples and employs a specially designed rectified flow network with cross-attention to incorporate linguistic cues. The method demonstrates effectiveness across both reinforcement learning (SMAC) and LLM tasks (MMLU/MathQA), showing significant accuracy improvements over existing baselines.

## Method Summary
The approach trains an LLM to generate explanations by optimizing rewards produced through a rectified flow matching model. The system uses a GUIDANCE LLM to generate positive samples, which are then used alongside negative samples created through the rectified flow network. The flow network incorporates cross-attention mechanisms to integrate linguistic cues into the reward generation process. This model-agnostic framework can be applied to different base models and tasks, enabling explanation generation for both RL agents and LLM reasoning processes.

## Key Results
- Achieves 4%-20% accuracy improvements over SFT and RLHF baselines on SMAC and MMLU/MathQA tasks
- Demonstrates effective generalization across different base models
- Ablation studies confirm the importance of the rectified flow model for generating reliable rewards
- Outperforms reasoning frameworks in explanation generation tasks

## Why This Works (Mechanism)
The method works by leveraging rectified flow matching to generate high-quality reward signals that guide LLM training for explanation generation. The GUIDANCE LLM provides positive examples, while the rectified flow network creates a structured reward landscape that captures both linguistic nuances and decision-making patterns. The cross-attention mechanism allows the model to incorporate relevant contextual information when generating explanations, making them more coherent and task-appropriate.

## Foundational Learning

**Rectified Flow Matching**: A technique for generating structured reward signals by modeling the flow between distributions. Needed for creating meaningful reward landscapes that capture decision-making quality. Quick check: Verify that the flow model can distinguish between good and poor explanations in validation tests.

**Cross-Attention Mechanisms**: Allows the model to attend to relevant parts of input when generating explanations. Essential for incorporating linguistic cues and contextual information. Quick check: Measure attention weight distributions to ensure they focus on relevant tokens.

**GUIDANCE LLM**: A reference model used to generate positive samples for training. Provides high-quality examples that guide the explanation generation process. Quick check: Evaluate the quality and diversity of generated positive samples.

## Architecture Onboarding

**Component Map**: GUIDANCE LLM -> Rectified Flow Network -> LLM Explanation Generator

**Critical Path**: The core training pipeline involves generating positive samples with GUIDANCE LLM, creating reward signals through the rectified flow network, and using these rewards to train the explanation-generating LLM through optimization.

**Design Tradeoffs**: The model balances between using high-quality reference samples from GUIDANCE LLM versus generating diverse negative samples through the flow network. Cross-attention adds computational overhead but improves explanation quality.

**Failure Signatures**: Poor explanation quality may result from inadequate reward signals, insufficient diversity in training samples, or misalignment between the GUIDANCE LLM and target tasks.

**First Experiments**:
1. Test reward signal quality by comparing explanations generated with and without rectified flow rewards
2. Evaluate cross-attention effectiveness by measuring explanation coherence with different attention configurations
3. Assess generalization by applying the trained model to unseen tasks or base models

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Lacks detailed implementation specifics for the rectified flow matching model architecture
- Limited evaluation to accuracy metrics without qualitative analysis of explanation quality
- Claims of robust performance across base models not fully substantiated with diverse model families
- No human evaluation studies to validate explanation usefulness and coherence

## Confidence

**High confidence**: The general framework of using flow-matching rewards for LLM training is technically sound and the reported accuracy improvements over SFT and RLHF baselines are specific and measurable.

**Medium confidence**: The effectiveness of the rectified flow network with cross-attention for incorporating linguistic cues, as the architectural details are sparse and the linguistic cue mechanism is not fully explained.

**Low confidence**: The robustness claims across different base models and tasks, as the evaluation scope appears limited and the ablation studies don't fully explore model dependency.

## Next Checks
1. Conduct human evaluation studies to assess the quality, coherence, and usefulness of generated explanations beyond accuracy metrics
2. Test the method across a wider range of base LLM architectures (different sizes, training paradigms) to validate generalization claims
3. Provide detailed ablation studies isolating the contributions of the rectified flow matching model versus other components in the training pipeline