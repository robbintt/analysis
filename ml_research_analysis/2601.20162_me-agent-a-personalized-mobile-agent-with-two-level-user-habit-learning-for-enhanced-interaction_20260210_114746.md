---
ver: rpa2
title: 'Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for
  Enhanced Interaction'
arxiv_id: '2601.20162'
source_url: https://arxiv.org/abs/2601.20162
tags:
- user
- mobile
- preference
- music
- experiences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Me-Agent addresses personalization limitations in LLM-based mobile
  agents by introducing a two-level habit learning approach. It uses User Preference
  Learning with a Personal Reward Model at the prompt level to rank responses based
  on user preferences, and Hierarchical Preference Memory at the memory level to store
  long-term and app-specific user behaviors separately.
---

# Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for Enhanced Interaction

## Quick Facts
- arXiv ID: 2601.20162
- Source URL: https://arxiv.org/abs/2601.20162
- Authors: Shuoxin Wang; Chang Liu; Gowen Loo; Lifan Zheng; Kaiwen Wei; Xinyi Zeng; Jingyuan Zhang; Yu Tian
- Reference count: 16
- One-line primary result: Achieves state-of-the-art personalization with App Selection Accuracy of 1.000 on ambiguous instructions

## Executive Summary
Me-Agent introduces a novel two-level habit learning framework for mobile agents that addresses personalization limitations in LLM-based systems. The approach combines User Preference Learning with a Personal Reward Model at the prompt level and Hierarchical Preference Memory at the memory level to store long-term and app-specific behaviors separately. Without requiring additional training, Me-Agent operates effectively through API-based deployment while maintaining competitive instruction execution performance. The framework demonstrates significant improvements in handling personalized tasks through implicit intent inference.

## Method Summary
Me-Agent employs a two-level habit learning approach that leverages existing LLM capabilities without additional training. At the prompt level, User Preference Learning utilizes a Personal Reward Model to rank responses based on user preferences. At the memory level, Hierarchical Preference Memory stores long-term user behaviors separately from app-specific behaviors. This dual approach enables the agent to interpret ambiguous instructions, learn from user interaction history, and handle personalized tasks through implicit intent inference. The framework is designed for practical deployment through API-based LLM integration.

## Key Results
- Achieves App Selection Accuracy of 1.000 on both ambiguous instruction types in User FingerTip benchmark
- Demonstrates 39.7% improvement in task completion rate over baseline on E-dataset (89.3% vs baseline)
- Maintains competitive instruction execution performance while delivering superior personalization

## Why This Works (Mechanism)
The framework's effectiveness stems from its hierarchical approach to personalization. By separating long-term behavioral patterns from app-specific preferences in memory, Me-Agent can make more nuanced decisions about user intent. The Personal Reward Model at the prompt level allows for real-time preference ranking without requiring model retraining. This combination enables the system to handle ambiguous instructions by drawing on both immediate context and historical patterns, while the API-based deployment ensures practical applicability without infrastructure overhead.

## Foundational Learning
- **User Preference Learning**: Why needed - To rank responses based on individual user preferences without retraining; Quick check - Validate ranking accuracy across diverse user profiles
- **Hierarchical Preference Memory**: Why needed - To distinguish between general long-term behaviors and specific app-related patterns; Quick check - Test memory separation effectiveness on mixed instruction types
- **Implicit Intent Inference**: Why needed - To handle ambiguous instructions by inferring user intent from historical patterns; Quick check - Measure accuracy on progressively ambiguous instruction sets
- **API-based LLM Deployment**: Why needed - To enable practical implementation without infrastructure requirements; Quick check - Benchmark latency and cost compared to fine-tuned alternatives
- **Personal Reward Model**: Why needed - To provide preference-based response ranking at inference time; Quick check - Compare reward model accuracy against baseline ranking methods

## Architecture Onboarding

Component Map: User Input -> Personal Reward Model -> Response Ranking -> Hierarchical Preference Memory -> Action Selection -> LLM API -> Output

Critical Path: User Input → Personal Reward Model → LLM API → Action Selection

Design Tradeoffs: The framework prioritizes deployment simplicity over model customization by avoiding additional training, trading potential performance gains from fine-tuning for practical API-based implementation. The two-level memory separation adds architectural complexity but aims to improve personalization accuracy.

Failure Signatures: Perfect app selection accuracy may indicate benchmark limitations rather than true capability; performance degradation on instructions outside training distribution; memory separation may fail to capture complex cross-app behavioral patterns.

First 3 Experiments:
1. Validate Personal Reward Model accuracy on diverse user preference sets
2. Test Hierarchical Preference Memory separation effectiveness with mixed instruction types
3. Benchmark API latency and cost compared to fine-tuned alternatives

## Open Questions the Paper Calls Out
None

## Limitations
- Perfect App Selection Accuracy (1.000) may indicate limited instruction variety in benchmark
- Two-level memory separation lacks empirical validation of advantages over alternative architectures
- No additional training requirement may limit capture of complex user habit patterns

## Confidence

High Confidence:
- Two-level habit learning architecture is technically sound
- Performance improvements on User FingerTip benchmark are well-documented

Medium Confidence:
- Framework's ability to handle ambiguous instructions is supported by experimental results
- Learning from interaction history is demonstrated but perfect accuracy raises questions

Low Confidence:
- No additional training requirement for optimal performance needs further validation
- Two-level memory separation optimality requires testing across diverse populations

## Next Checks
1. Evaluate Me-Agent on multiple diverse benchmark datasets with varying instruction ambiguity levels to assess generalizability beyond User FingerTip benchmark

2. Conduct ablation studies comparing two-level Hierarchical Preference Memory against alternative memory architectures to quantify specific benefits of the proposed design

3. Implement longitudinal study with real users over 3-6 months to evaluate adaptation to evolving user preferences and long-term accuracy of implicit intent inference