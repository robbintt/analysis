---
ver: rpa2
title: 'When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality
  Evaluation'
arxiv_id: '2510.07238'
source_url: https://arxiv.org/abs/2510.07238
tags:
- benchmarks
- search
- evaluation
- benchmark
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Static benchmarks used to evaluate large language models (LLMs)
  become outdated over time as real-world facts change, leading to unreliable factuality
  assessments. This work systematically investigates temporal misalignment in five
  popular factuality benchmarks by extracting time-sensitive questions and retrieving
  up-to-date answers from the web.
---

# When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation

## Quick Facts
- **arXiv ID:** 2510.07238
- **Source URL:** https://arxiv.org/abs/2510.07238
- **Reference count:** 40
- **Primary result:** Static benchmarks become outdated over time, causing unreliable LLM factuality evaluations with over 10% of correct answers being penalized

## Executive Summary
Static benchmarks used to evaluate large language models become unreliable over time as real-world facts evolve. This work systematically investigates temporal misalignment in five popular factuality benchmarks by extracting time-sensitive questions and retrieving up-to-date answers from the web. The analysis reveals that newer LLMs are more vulnerable to evaluation bias, as they frequently produce current answers that conflict with outdated benchmark labels. Benchmark passages often worsen misalignment by anchoring models to obsolete information. The findings suggest that relying on aging benchmarks leads to unreliable factuality evaluations and call for time-aware evaluation frameworks.

## Method Summary
The study extracts time-sensitive questions from five factuality benchmarks using an LLM classifier with majority voting, then retrieves current answers via web search using iterative subgoal decomposition. Three metrics are computed: Dataset Drift Score (percentage of outdated samples), Evaluation Misleading Rate (fraction of correct answers penalized by stale labels), and Temporal Alignment Gap (alignment between model outputs and real-world facts vs benchmark labels). Eight LLMs across different release dates are evaluated on the filtered samples using LLM-as-a-judge for answer equivalence.

## Key Results
- Dataset Drift Score shows 24.19%–63.78% of benchmark samples are outdated
- Evaluation Misleading Rate exceeds 10%, with newer LLMs showing higher vulnerability
- Temporal Alignment Gap is 70% positive, indicating LLMs align more with current facts than benchmark labels
- Outdated benchmark passages worsen misalignment, with some models showing 20.67 percentage point drops in TAG when context is included

## Why This Works (Mechanism)

### Mechanism 1: Temporal Drift in Static Benchmark Labels
- Claim: Static benchmarks accumulate outdated factual answers as real-world knowledge evolves
- Mechanism: Benchmarks capture facts at release time; time-sensitive facts change; benchmark labels remain fixed
- Core assumption: Non-trivial portion of benchmark questions are time-sensitive with verifiable answers
- Evidence: Dataset Drift Score shows 24.19%–63.78% outdated samples; related work notes LLMs suffer temporal misalignment

### Mechanism 2: Evaluation Penalty Through Label-Response Misalignment
- Claim: Newer LLMs produce factually correct (current) answers penalized by stale benchmark labels
- Mechanism: Modern LLMs trained on recent data output updated answers; evaluators compare against outdated gold labels
- Core assumption: Newer models have more recent factual knowledge and output current answers
- Evidence: Evaluation Misleading Rate exceeds 10%; newer LLMs exhibit higher EMR rates

### Mechanism 3: Context Anchoring Amplifies Temporal Misalignment
- Claim: Outdated passages in benchmarks actively mislead models toward obsolete answers
- Mechanism: Models condition on provided context; outdated context overrides updated internal knowledge
- Core assumption: LLMs prioritize context over parametric knowledge in passage-grounded tasks
- Evidence: TAG drops up to 20.67 percentage points when passages are included; all models show more negative TAG with passages

## Foundational Learning

- **Time-sensitive vs. time-invariant questions**
  - Why needed here: The methodology depends on correctly identifying which questions have answers that change over time
  - Quick check question: Would "Who won the 2020 US presidential election?" be time-sensitive under the paper's definition?

- **Static benchmark evaluation paradigm**
  - Why needed here: Understanding why benchmarks remain fixed helps contextualize the drift problem
  - Quick check question: Name two reasons researchers continue using outdated benchmarks despite known temporal issues

- **LLM knowledge cutoff**
  - Why needed here: Model release dates and training data recency directly affect how models respond to time-sensitive queries
  - Quick check question: If two models have the same architecture but different release dates, which would you expect to have a higher EMR on an old benchmark?

## Architecture Onboarding

- **Component map:** Time-sensitive extraction module -> Latest fact retrieval pipeline -> Temporal comparison module -> LLM-as-judge alignment scorer
- **Critical path:** Filter benchmarks to time-sensitive subset -> Retrieve current answers via web search -> Collect LLM responses -> Compare all three sources to compute metrics
- **Design tradeoffs:** Retrieval accuracy (89.52%) vs. cost/latency; classification recall (100%) vs. precision (90%); using commercial APIs vs. reproducibility
- **Failure signatures:** Very low EMR on all models -> benchmark may have few time-sensitive questions; Negative TAG across all models -> benchmark passages heavily outdated; Low inter-annotator agreement -> search pipeline needs refinement
- **First 3 experiments:** Run time-sensitive classifier on new benchmark and manually validate sample; Execute retrieval pipeline on 50 time-sensitive questions and verify against manual search; Compute TAG for single model across two benchmarks with different release dates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do temporal misalignment effects persist or intensify in reasoning-based LLMs and larger proprietary models?
- Basis in paper: Explicitly states they "did not include larger LLMs, reasoning-based LLMs, and more commercial models"
- Why unresolved: Resource and budget constraints limited evaluation to specific open-source families
- What evidence would resolve it: Applying metrics to models with explicit reasoning chains (e.g., o1) and frontier proprietary models

### Open Question 2
- Question: How can evaluation frameworks effectively mitigate the "anchoring" effect where outdated benchmark passages override correct internal knowledge?
- Basis in paper: Section 3.3 demonstrates that outdated passages worsen misalignment but doesn't propose prevention methods
- Why unresolved: Study quantifies the problem but stops short of testing interventions for context-dependent evaluation
- What evidence would resolve it: Experiments with time-stamped or filtered context passages compared against static benchmark passages

### Open Question 3
- Question: Can the fact retrieval pipeline construct a "living" benchmark that automatically updates gold labels in real-time?
- Basis in paper: Suggests work provides a "testbed" and calls for "time-aware evaluation frameworks"
- Why unresolved: Current pipeline designed for analysis and detection, not automatic maintenance of benchmark datasets
- What evidence would resolve it: Longitudinal study deploying pipeline to continuously update benchmark, measuring stability of LLM scores over time

## Limitations
- Relies on automated web retrieval for "ground truth" which may introduce its own errors
- Time-sensitive classification performed by single LLM with majority voting creates potential bias
- Analysis focuses on factuality benchmarks but doesn't examine if these represent full spectrum of LLM evaluation needs
- Retrieval pipeline accuracy (89.52%) leaves room for systematic errors in determining current facts

## Confidence
- **High confidence:** Core observation that static benchmarks accumulate temporal drift (supported by measurable DDS values of 24.19%–63.78%)
- **Medium confidence:** EMR finding that newer models are more vulnerable (based on observed patterns across 8 models but limited to 5 benchmarks)
- **Medium confidence:** TAG mechanism showing context anchoring effects (strong within-study evidence but no external validation)

## Next Checks
1. Conduct human-annotated validation study on 100 randomly selected time-sensitive samples to verify automated retrieval pipeline's accuracy
2. Test same methodology on benchmarks from different domains (scientific, technical) to assess generalizability beyond general knowledge
3. Implement controlled experiment where models are given explicit temporal context with benchmark passages to measure how much anchoring depends on temporal awareness