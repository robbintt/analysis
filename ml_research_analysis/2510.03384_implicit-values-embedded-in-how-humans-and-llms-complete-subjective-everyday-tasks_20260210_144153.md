---
ver: rpa2
title: Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday
  Tasks
arxiv_id: '2510.03384'
source_url: https://arxiv.org/abs/2510.03384
tags:
- llms
- tasks
- values
- humans
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) were evaluated on 30 everyday tasks
  that can elicit implicit values like environmentalism, diversity, and financial
  priorities. Six popular LLMs (GPT-3.5, GPT-4o, Llama 2, Llama 3, Gemini 1.5 Pro,
  Claude 3) and 100 human crowdworkers from the US completed each task 100 times.
---

# Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks

## Quick Facts
- **arXiv ID:** 2510.03384
- **Source URL:** https://arxiv.org/abs/2510.03384
- **Reference count:** 40
- **Primary result:** For 21 of 30 everyday tasks, all six tested LLMs significantly differed from human value distributions, with LLMs exhibiting values misaligned with human expectations.

## Executive Summary
Large language models often exhibit implicit values that diverge significantly from human populations when completing everyday subjective tasks. This study evaluated six popular LLMs (GPT-3.5, GPT-4o, Llama 2, Llama 3, Gemini 1.5 Pro, Claude 3) and 100 US crowdworkers on 30 tasks that can elicit implicit values like environmentalism, diversity, and financial priorities. For every task, the distribution of implicit values differed significantly between humans and at least some LLMs; for 21 tasks, all six LLMs differed from humans. These findings highlight a critical gap between how LLMs and humans make value-laden decisions, raising concerns about the alignment of AI assistants with user expectations.

## Method Summary
The study collected human baseline data from 100 US crowdworkers completing 30 prompts 100 times each. Six LLMs were then prompted with the same tasks using 100 samples per task, with decoding parameters set to top-p=0.9 and top-k=50. Outputs were parsed using regex and compared to human distributions using Fisher's Exact Test (categorical) and Kruskal-Wallis/Mann-Whitney U Tests (quantitative) with Holm correction. The prompts covered tasks like calculating tips, selecting flights, and editing text, designed to elicit implicit values such as environmentalism, privacy, and cultural preferences.

## Key Results
- All six LLMs significantly differed from human value distributions on at least 21 of 30 tasks
- LLMs varied widely from each other, with none matching human patterns closely
- Humans paid an 11% premium for eco-friendly flights, while most LLMs rarely did
- Humans tipped ~19%, but GPT-3.5 averaged 9% while other models ranged from 11-20%
- All LLMs always selected the more privacy-protective retailer, compared to 70% of humans

## Why This Works (Mechanism)

### Mechanism 1: Training Corpus Distribution Mismatch
- **Claim:** LLMs exhibit implicit values that reflect their training data distribution rather than specific user populations.
- **Mechanism:** LLMs trained on internet-scale corpora learn statistical patterns that may over-represent certain demographics, regions, or viewpoints. When completing subjective tasks, models default to these learned patterns rather than the implicit values held by specific user groups (e.g., US crowdworkers).
- **Core assumption:** The paper assumes training data composition systematically differs from the US crowdworker population in value-relevant dimensions.
- **Evidence anchors:** [abstract] "LLMs also varied widely from each other, with none matching human patterns closely." [section 2.3] "LLMs are trained on corpora of mostly human-generated text and via human feedback, it seems possible for LLMs to mirror humans." [corpus] Related work "Co-Constructing Alignment" notes "value misalignment has emerged as a pressing concern" in AI systems embedded in everyday practice.
- **Break condition:** If training data were perfectly representative of target user populations, this mechanism would weaken—but the paper shows persistent divergence across all tested models.

### Mechanism 2: Inconsistent Safety and Alignment Guardrails Across Models
- **Claim:** Different LLM families implement divergent implicit value defaults through their fine-tuning and safety guardrails.
- **Mechanism:** Post-training alignment (RLHF, safety filters, system prompts) encodes developer-chosen values. These vary across companies and even model versions, creating inconsistent value expressions. Some guardrails may over-correct (e.g., always selecting privacy-protective options) while others may be underspecified.
- **Core assumption:** Alignment processes encode value preferences that are neither transparent nor consistently applied.
- **Evidence anchors:** [section 4.3] "All LLMs always selected the more privacy-protective retailer, compared to 70% of humans." [section 5] "Surprisingly, we encountered very few overt guardrails in our study... most LLMs we tested never refused to complete tasks related to race, religion, or other sensitive topics." [corpus] "The Behavioral Fabric of LLM-Powered GUI Agents" examines "how both explicit and implicit user preferences" impact agent behavior, suggesting guardrail implementation varies.
- **Break condition:** If alignment were transparent and standardized, developers could predictably adjust value outputs—but current implementations are opaque and inconsistent.

### Mechanism 3: Absence of Cultural Context Grounding in Prompts
- **Claim:** LLMs default to implicit cultural priors when prompts lack explicit cultural specification, causing systematic misalignment with geographically situated human values.
- **Mechanism:** Without explicit context markers, LLMs rely on learned associations that may reflect dominant patterns in training data (e.g., international date formats, non-US naming conventions). This produces outputs that technically satisfy the prompt but violate cultural expectations.
- **Core assumption:** Explicit cultural context specification would shift LLM outputs toward locally appropriate values.
- **Evidence anchors:** [section 4.5] "While GPT-3.5, GPT-4o, and Claude 3 preferred DD/MM, our US-based human respondents favored the US standard MM/DD (70%)." [section 4.6] "LLMs mostly (57%–100%) discussed Asian countries" when asked about successful countries, while humans selected more diverse regions. [corpus] "Can LLMs Grasp Implicit Cultural Values?" explicitly benchmarks cultural intelligence, noting "existing studies often focus on explicitly stated cultural norms, but fail to capture the subtle, implicit values."
- **Break condition:** Robustness tests in Appendix F showed that explicitly specifying US context "resulted in slightly higher restaurant tips, though still well below the 20% US standard for all LLMs"—suggesting context alone is insufficient.

## Foundational Learning

- **Concept: Implicit vs. Explicit Values**
  - **Why needed here:** The paper distinguishes values that are "fundamental principles in a given context that guide decision-making even though those guidelines were not specified explicitly in the task description." Understanding this distinction is essential for interpreting the results.
  - **Quick check question:** If a prompt asks an LLM to "choose the best option" without specifying criteria, what implicit values might influence its choice?

- **Concept: Alignment as Distribution Matching vs. Personalization**
  - **Why needed here:** The paper critiques aggregate distribution matching: "If 60% of humans would pick the more eco-friendly product, that does not mean an AI assistant should do so with 60% probability." Instead, it argues for personalized alignment reflecting individual user values.
  - **Quick check question:** Why might matching aggregate human behavior be insufficient for value alignment in AI assistants?

- **Concept: Statistical Testing for Distribution Comparison (Fisher's Exact Test, Kruskal-Wallis)**
  - **Why needed here:** The paper uses these tests to quantify divergence between LLM and human outputs. Understanding when and why these tests flag significant differences is critical for interpreting the claim that "for 21 tasks, all six LLMs differed significantly from humans."
  - **Quick check question:** If an LLM's output distribution differs significantly from human distribution on Fisher's Exact Test, does this prove the LLM lacks alignment? What else might explain the difference?

## Architecture Onboarding

- **Component map:** User Prompt → Context Specification Layer → LLM Backbone → Output Parser → Value Audit Module → User Preference Store → Personalized Response

- **Critical path:** 1. Prompt arrives without explicit value specification 2. LLM applies implicit defaults from training/alignment 3. Output reflects model's embedded values rather than user's 4. User may not notice misalignment (values are implicit) 5. Over time, accumulated decisions shift user's outcomes away from their preferences

- **Design tradeoffs:**
  - **Explicit value elicitation vs. user burden:** Asking clarifying questions improves alignment but increases interaction friction
  - **Personalization vs. consistency:** Per-user value learning creates divergent experiences across users
  - **Aggregate alignment vs. individual alignment:** Matching population statistics vs. adapting to individual preferences (paper argues for latter)
  - **Transparency vs. seamlessness:** Making implicit values explicit may improve user awareness but disrupts natural interaction

- **Failure signatures:**
  - **Option ordering brittleness:** Appendix F shows "all six LLMs chose substantially different options based on the ordering"
  - **Version inconsistency:** "Frequently including across versions of the same model family" (e.g., GPT-3.5 vs. GPT-4o tipping behavior)
  - **Over-correction for certain values:** 100% selection of privacy-protective retailer by all LLMs vs. 70% human selection
  - **Cultural default mismatches:** Date format, regionalism retention, country selection divergences

- **First 3 experiments:**
  1. **Value Elicitation Prompting:** Add a pre-task step asking LLMs to identify which implicit values might apply to the task and how they would resolve tradeoffs. Measure whether this reduces human-LLM divergence.
  2. **User Preference Persistence:** Implement a simple preference store (e.g., via RAG) that records user-specific value choices (tipping rate, environmental premium willingness). Test whether personalization improves alignment on repeated tasks.
  3. **Cross-Context Robustness Audit:** Extend the paper's cultural context experiments (US/Denmark/Japan) to a broader set of tasks and models, specifically measuring whether explicit context specification consistently moves LLM outputs toward culturally appropriate values or whether deeper architectural changes are needed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLM-based AI assistants effectively learn and adapt to a specific user's implicit values over time to provide personalized alignment?
- **Basis in paper:** [explicit] The authors argue that aggregate alignment is insufficient and suggest, "The AI assistant should learn a user’s values over time... [leveraging] retrieval-augmented generation."
- **Why unresolved:** The current study establishes that misalignment exists between general models and general populations, but it does not test mechanisms for dynamic, user-specific value adaptation or long-term preference retention.
- **What evidence would resolve it:** Experiments demonstrating that an AI assistant can successfully query user preferences and subsequently modify its decision-making distributions (e.g., tipping percentages, environmental choices) to match a specific user's historical behavior.

### Open Question 2
- **Question:** Do the misalignments in implicit values identified in this study persist across non-Western cultural contexts?
- **Basis in paper:** [explicit] The authors explicitly state as a limitation: "Future work should investigate LLMs’ implicit values in many additional cultural contexts, especially non-Western ones."
- **Why unresolved:** The study restricted human data to US crowdworkers and prompts to English, leaving the transferability of these value conflicts to other languages and cultural norms (e.g., collectivist vs. individualist priorities) unknown.
- **What evidence would resolve it:** A replication of the 30-task audit using human subjects from diverse non-Western regions and prompts in local languages, comparing the resulting value distributions against the LLM outputs.

### Open Question 3
- **Question:** How can the reliability of implicit value commitments in LLMs be validated psychometrically against prompt brittleness?
- **Basis in paper:** [explicit] The authors note that "future work on auditing values should also connect to emerging work on LLM psychometric validation" and highlight that "LLMs’ outputs are brittle to option ordering."
- **Why unresolved:** While the paper identifies that option reordering changes LLM outputs, it does not establish a standardized benchmark or metric for determining if a model's value expression is a consistent trait or merely an artifact of prompt formulation.
- **What evidence would resolve it:** The development of a benchmark suite that tests LLMs across thousands of paraphrased prompts and reordered options to measure the statistical stability (consistency) of the implicit values expressed.

## Limitations

- **Temporal validity:** Model behaviors change with updates; results reflect snapshots from Aug-Sept 2024 and may not hold for current versions
- **Geographic generalizability:** Study uses US crowdworkers; implicit value divergences may differ for other populations
- **Ecological validity:** Lab-based tasks may not capture real-world value alignment challenges in interactive contexts
- **Safety guardrail evolution:** Low refusal rates reported may increase as models are updated with stricter policies

## Confidence

- **High confidence:** Human-LLM value divergences exist and are statistically significant across multiple tasks and models
- **Medium confidence:** LLMs exhibit consistent patterns of over-correction for certain values (privacy) while under-performing on others (environmental considerations)
- **Medium confidence:** Cultural context specification alone is insufficient for value alignment; deeper architectural changes needed

## Next Checks

1. **Temporal drift validation:** Re-run the 10 most divergent tasks across current model versions to quantify behavioral drift since the original study
2. **Interactive value alignment:** Design a Wizard-of-Oz study where humans interact with simulated AI assistants showing varying value alignment patterns to measure real-world impact on user satisfaction and outcomes
3. **Cross-cultural generalization:** Replicate the cultural context experiments (US/Denmark/Japan) with crowdworkers from each region to validate whether observed patterns hold across populations or are specific to US participants