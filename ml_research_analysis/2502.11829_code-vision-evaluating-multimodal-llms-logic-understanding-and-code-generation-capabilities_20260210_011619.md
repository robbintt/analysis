---
ver: rpa2
title: 'Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation
  Capabilities'
arxiv_id: '2502.11829'
source_url: https://arxiv.org/abs/2502.11829
tags:
- code
- mllms
- test
- flowchart
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Code-Vision, a benchmark designed to evaluate
  the logical understanding and code generation capabilities of Multimodal Large Language
  Models (MLLMs). It challenges MLLMs to generate correct programs based on given
  flowcharts, which visually represent desired algorithms or processes.
---

# Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities

## Quick Facts
- **arXiv ID:** 2502.11829
- **Source URL:** https://arxiv.org/abs/2502.11829
- **Reference count:** 15
- **Key outcome:** Code-Vision benchmark reveals significant performance gaps between proprietary and open-source MLLMs on visual flowchart-based code generation, with GPT-4o achieving 79.3% pass@1 on hard problems versus 15% for best open-source model.

## Executive Summary
Code-Vision is a benchmark designed to evaluate multimodal LLMs' ability to generate correct Python code from flowchart inputs. The benchmark challenges models to parse visual algorithmic representations and translate them into executable programs, with three subsets covering basic programming, algorithms, and mathematical problem-solving. Experiments with 12 MLLMs demonstrate that proprietary models significantly outperform open-source alternatives, particularly on complex visual reasoning tasks. The study reveals distinct failure modes between model types and highlights fundamental gaps in open-source models' visual-to-code translation capabilities.

## Method Summary
Code-Vision requires generating Python functions from flowchart images and task descriptions. The benchmark uses 0-shot inference with fixed parameters (temperature=0.2, top_p=0.95, max_tokens=1024) and evaluates functional correctness using pass@1 metric. The dataset consists of flowchart images paired with function signatures, covering three domains: HumanEval-V (basic programming), Algorithm (algorithmic problems), and MATH (mathematical problem-solving). Models are evaluated on their ability to generate syntactically correct and logically accurate code that passes predefined test cases.

## Key Results
- Proprietary models (GPT-4o, Claude-3.5-Sonnet) achieve 79.3% pass@1 on hard problems, while best open-source model reaches only 15%
- Open-source models show substantial performance improvements (up to 20% gain) when flowchart images are replaced with structured Mermaid text
- Error analysis reveals distinct failure modes: open-source models primarily fail on syntax generation, while proprietary models fail on logical assertions despite generating runnable code

## Why This Works (Mechanism)

### Mechanism 1: Visual-Centric Logical Dependency
The benchmark enforces dependency where visual topology (flowchart structure) is the sole carrier of control-flow logic, forcing models to perform visual reasoning rather than text-based pattern matching. Flowcharts serve as primary input with minimized text descriptions, creating a causal chain where geometric shapes and connecting arrows must be parsed to reconstruct execution order. Evidence shows GPT-4o performance drops from 87.9% (Text+Image) to 24.8% (Text Only), proving text insufficiency.

### Mechanism 2: Modality-Induced Complexity Gap (Visual vs. Structural)
Open-source models suffer from a "visual complexity penalty" where cognitive load of parsing rasterized flowcharts consumes resources needed for logic. This constraint relaxes when input switches to structured text (Mermaid). Converting visual input to structured text significantly improves open-source model performance, suggesting failure mechanism is not purely "lack of coding knowledge" but bottleneck in visual-to-logic translation.

### Mechanism 3: Syntactic vs. Semantic Failure Stratification
Capability gap between proprietary and open-source models is mechanistically stratified: open-source models fail at syntax generation (competence), while proprietary models fail at logical assertion (reasoning). Error analysis reveals open-source models frequently generate unrunnable code (SyntaxError), indicating code generation head failure, while proprietary models generate runnable code that fails test cases (AssertionError), indicating successful syntax generation but flawed logical translation.

## Foundational Learning

- **Concept: Control Flow Graphs (CFGs)**
  - **Why needed here:** Core task is translating visual graph (flowchart) into textual CFG (code loops/ifs). Understanding how if/else maps to branching nodes is fundamental logic unit being tested.
  - **Quick check question:** Can you manually convert a 3-node diamond flowchart into a Python if-else block?

- **Concept: Pass@k Metric**
  - **Why needed here:** Paper uses Pass@1 (success if first attempt is correct). This is strict metric compared to "any of 10 attempts," emphasizing reliability over potential capability.
  - **Quick check question:** Why does Pass@1 penalize a model that generates 9 wrong solutions and 1 perfect solution just as much as a model that generates 10 wrong solutions?

- **Concept: Visual Encoder Resolution**
  - **Why needed here:** Flowcharts are dense, structured images. If vision encoder downsamples image too aggressively, fine text in flowchart nodes becomes unreadable, causing logic errors.
  - **Quick check question:** What happens to logic extraction if text inside a flowchart decision node ("Is x > 5?") is blurred or misread by OCR/vision component?

## Architecture Onboarding

- **Component map:** Flowchart Image -> Vision Encoder -> Projector -> LLM Backbone -> Python Code
- **Critical path:** 1. OCR/Parsing: Correctly reading text inside flowchart nodes. 2. Topological Sort: Understanding directional flow of arrows. 3. Code Synthesis: Mapping sorted visual nodes to syntactically correct Python.
- **Design tradeoffs:** Mermaid (Text) vs. Flowchart (Image) Input - Mermaid bypasses visual encoder limitations but fails to test target multimodal reasoning capability. Dataset size is smaller than general benchmarks but offers higher "visual necessity" complexity.
- **Failure signatures:** SyntaxError (open-source models, code generation grammar failure). AssertionError (proprietary models, logic mirroring flowchart but failing edge-case logic). RecursionError (model misunderstands loop base case in flowchart).
- **First 3 experiments:** 1. Ablation (Text vs. Image): Run model on dataset using only text description to establish "Visual Necessity" baseline. 2. Modality Swap (Mermaid vs. PNG): Input Mermaid code for same flowcharts to measure "Visual Complexity Penalty." 3. Error Categorization: Execute all generated code and classify failures into Syntax vs. Assertion to determine bottleneck.

## Open Questions the Paper Calls Out

1. **Question:** How can evaluation of MLLMs be expanded beyond functional correctness to assess qualitative code attributes such as readability, efficiency, and style?
   - **Basis in paper:** Limitations section states current evaluation focuses solely on correctness and "does not adequately evaluate the quality of the generated code, such as the readability, efficiency, and style of the code."
   - **Why unresolved:** Current benchmark relies on pass@1 using test cases, which is binary measure of functionality; lacks metrics for analyzing code structure or execution efficiency.
   - **What evidence would resolve it:** Study incorporating static analysis tools (e.g., pylint) or efficiency metrics (e.g., Big-O analysis) into Code-Vision evaluation pipeline to rate code quality alongside correctness.

2. **Question:** Is poor performance of open-source models on Code-Vision primarily driven by visual perception failures or by deficiencies in logic understanding?
   - **Basis in paper:** Table 4 shows open-source models perform significantly better when given Mermaid text instead of flowchart images. Authors infer this highlights "challenges MLLMs face in perceiving and processing visually complex logic," but exact split between perception and reasoning failure is not isolated.
   - **Why unresolved:** While performance gap suggests visual difficulty, paper doesn't isolate whether models fail to extract logic from image or fail to translate extracted logic into syntax.
   - **What evidence would resolve it:** Ablation study introducing "gold-standard" OCR/text extraction step for flowcharts to determine if reasoning performance matches Mermaid-input baseline.

3. **Question:** What specific capabilities allow proprietary models to succeed on Code-Vision while open-source models fail, despite similar performance on mathematical reasoning benchmarks like MathVista?
   - **Basis in paper:** Authors note open-source models show performance drop of around -30% on Code-Vision compared to MathVista, highlighting "unique challenges our benchmark poses for testing algorithmic reasoning capabilities" that current open-source models cannot meet.
   - **Why unresolved:** Paper identifies performance gap but doesn't pinpoint specific architectural or training data differences that enable proprietary models to handle visual algorithmic logic that open-source models cannot.
   - **What evidence would resolve it:** Analysis comparing attention mechanisms or intermediate representations of proprietary versus open-source models when processing flowchart nodes and edges.

## Limitations
- Performance gaps may be partially attributed to data contamination, as visual reasoning flowcharts could overlap with training corpora
- Benchmark's limited size (3 domains with unspecified total examples) constrains statistical power for fine-grained error analysis
- Study focuses on pass@1 accuracy without examining model calibration or reliability across multiple generations

## Confidence

**High Confidence:** Error stratification analysis showing distinct failure modes (SyntaxError vs AssertionError) between model types is well-supported by execution data and error categorization. Visual necessity mechanism (text-only vs multimodal performance drop from 87.9% to 24.8%) demonstrates clear empirical grounding.

**Medium Confidence:** Modality complexity hypothesis (visual vs mermaid performance gaps) shows strong within-study evidence but requires external validation across different model architectures and flowchart complexities. Dataset construction methodology appears sound but hasn't been independently verified.

**Low Confidence:** Claim about fundamental capability gaps in open-source models should be tempered by unresolved data contamination question and limited benchmark size, which may not capture full distribution of real-world visual reasoning tasks.

## Next Checks

1. **Data Contamination Audit:** Conduct systematic search for flowchart patterns and algorithmic structures from benchmark in pre-training corpora of evaluated models, quantifying overlap percentage.

2. **Cross-Architecture Validation:** Replicate mermaid vs flowchart performance gap experiments using different open-source vision-LLM architectures (e.g., InternVL, Qwen-VL) to verify this isn't model-specific.

3. **Multi-Turn Reliability Test:** Implement pass@k evaluation (k=5-10) with temperature-based diversity sampling to assess whether open-source models can succeed with multiple attempts, distinguishing systematic failure from stochastic underperformance.