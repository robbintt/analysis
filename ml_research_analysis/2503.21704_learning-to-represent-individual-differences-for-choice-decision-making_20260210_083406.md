---
ver: rpa2
title: Learning to Represent Individual Differences for Choice Decision Making
arxiv_id: '2503.21704'
source_url: https://arxiv.org/abs/2503.21704
tags:
- learning
- johujl
- user
- individual
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that using representation learning to model
  individual differences in behavioral decision-making tasks improves prediction accuracy
  compared to standard machine learning models. The Beh2vec architecture, inspired
  by Word2vec and Doc2vec, learns user embeddings from both structured (demographic)
  and unstructured (free-text responses to open-ended questions) data.
---

# Learning to Represent Individual Differences for Choice Decision Making

## Quick Facts
- arXiv ID: 2503.21704
- Source URL: https://arxiv.org/abs/2503.21704
- Reference count: 26
- Models using representation learning improve decision predictions over models without representation learning, achieving 3.6-4.2% higher accuracy than baseline MLP models, with text-based representations yielding the highest accuracy (75.7%).

## Executive Summary
This paper introduces Beh2vec, a representation learning approach for modeling individual differences in behavioral decision-making tasks. The architecture learns user-specific embeddings from structured demographic data and unstructured free-text responses, then concatenates these with scenario features for choice prediction. The method achieves 3.6-4.2% improvement in prediction accuracy over standard MLPs, with text-based representations achieving the highest accuracy (75.7%). The approach offers advantages in flexibility, capturing richer individual differences from diverse data sources, and enabling responsible personalization by allowing selection of privacy-appropriate data types. The paper also proposes using conditional GANs for data synthesis to address cold-start problems in behavioral data.

## Method Summary
The study uses 1,205 participants × 64 choices = 77,120 samples from an economic decision-making experiment involving binary choices between two risky gambling options. Participants made 64 choices between gambles with varying outcomes, probabilities, and recipient options (self, other, charity). Three types of personal information were collected: user IDs, 11 demographic attributes (age, gender, race, education, marital status, child, employment, income, political/religious views/behavior), and free-text responses (~12 words/participant). The Beh2vec architecture adds a user embedding layer indexed by user ID, which is concatenated with gamble features and processed through an MLP. Text responses are converted to user representations by averaging pre-trained Word2vec vectors and fine-tuning through fully connected layers. Models are evaluated using 80/20 train/test splits with 2-fold cross-validation, optimizing binary cross-entropy loss with SGD (lr=0.001).

## Key Results
- Beh2vec with user ID embeddings achieves 73.4% accuracy vs. MLP baseline 69.8% (improvement of 3.6 percentage points)
- G+Text Resp. with Beh2vec achieves 75.7% accuracy, the highest among all models tested
- Beh2vec with text achieves 74.8% accuracy on gains and 74.8% on losses vs. behavioral model's 72.6% and 72.6% respectively
- Text-based representations show competitive performance to the well-validated theory-based behavioral model

## Why This Works (Mechanism)

### Mechanism 1
Learning user-specific embeddings from past choices improves prediction accuracy over models without individual representations. The Beh2vec architecture adds an embedding layer indexed by user ID, which acts as a "contextual indicator" or memory of each user's historical decisions. During training, the embedding matrix is updated via backpropagation so that each user's vector captures latent patterns in their choice behavior. At inference, this vector is concatenated with scenario features (e.g., gamble outcomes, probabilities) and fed through an MLP to predict choices.

### Mechanism 2
Free-text responses to open-ended questions provide a richer and more flexible signal for individual differences than structured demographic data. User responses are tokenized, each word/phrase is mapped to a 300-dimensional vector using pre-trained Word2vec, and the centroid of all vectors forms the user representation. This vector is fine-tuned through fully connected layers before concatenation with gamble features for choice prediction.

### Mechanism 3
Data-driven representation learning can achieve competitive or superior accuracy to theory-based behavioral models, particularly on decisions involving losses. The behavioral model computes subjective value for each gamble using prospect-theory-inspired parameters (α for risk aversion, β for recipient weighting), fitted hierarchically via Bayesian MCMC. Representation learning instead learns embeddings end-to-end without explicit theoretical constraints.

## Foundational Learning

- **Embedding layers for categorical entities**: Beh2vec maps each user ID to a learnable dense vector, analogous to word embeddings in NLP. Understanding how embeddings capture latent structure is essential for debugging and extending this architecture.
  - Quick check question: Can you explain why a learned embedding might outperform one-hot encoding for user representation?

- **Prospect theory basics (risk aversion, loss aversion)**: The behavioral comparison model uses α (risk aversion) and gain/loss framing effects. Grasping these concepts helps interpret why losses are harder to predict and how the behavioral model works.
  - Quick check question: What does α < 1 imply about a person's choices between certain vs. uncertain outcomes?

- **Centroid aggregation of word embeddings**: The text-based model averages Word2vec vectors to form user representations. Understanding the limitations of simple aggregation (e.g., loss of word order, topic mixing) is critical for assessing and improving this approach.
  - Quick check question: Why might a centroid fail to distinguish between two users who mention the same topics but with different sentiments or contexts?

## Architecture Onboarding

- **Component map**: User ID/Text Demographics → Embedding/Fine-tuning layer → Concatenation → MLP backbone [206 or 70 → 128 → 64 → 32 → 4 → 1] → Sigmoid output
- **Critical path**: User representation quality → concatenation → MLP hidden layers → choice probability. Errors in embedding (e.g., cold-start issues, poor text vectors) propagate directly to final predictions.
- **Design tradeoffs**: ID embeddings provide strongest personalization but require sufficient choice history per user; fail for cold-start without synthesis or transfer. Text embeddings offer privacy advantages and work for new users but depend on response quality and pre-trained word vectors. Behavioral model is interpretable and theoretically grounded but inflexible to additional features or data modalities.
- **Failure signatures**: High training accuracy but low test accuracy (overfitting, especially with RF on sparse personal features). Out-of-vocabulary words in text responses causing information loss (~10% OOV rate reported). Poor convergence in behavioral model MCMC if α is unconstrained (values >1.5 can destabilize subjective value calculations).
- **First 3 experiments**:
  1. Ablation on user representation type: Compare ID embedding vs. text centroid vs. demographics on held-out test set, controlling for MLP architecture and training epochs.
  2. Cold-start simulation: Hold out a subset of users entirely during training; evaluate how quickly ID embeddings converge with limited observations vs. text-based predictions.
  3. Gain vs. loss stratification: Evaluate each model separately on gain-only and loss-only trials to diagnose where representation learning provides the largest margin over the behavioral baseline.

## Open Questions the Paper Calls Out
- Can behavioral data synthesis or transfer learning effectively overcome the cold-start problem for new users with sparse data?
- Do alternative modalities (e.g., different media or stimuli) provide more effective measures of individual differences than the text responses analyzed here?
- Can the high-dimensional embeddings learned by the model be interpreted to explain specific psychological traits, similar to the parameters of the behavioral model?

## Limitations
- The 3-4% accuracy improvements, while statistically meaningful, may not be practically significant for real-world applications requiring higher-stakes decisions.
- The lack of ablation studies on individual model components makes it difficult to attribute performance gains to specific mechanisms.
- The reported competitive accuracy to behavioral models rests on assumptions about text signal quality and Word2vec embedding relevance that remain untested.

## Confidence
- **High confidence**: Beh2vec architecture improves prediction over MLP baseline
- **Medium confidence**: Text responses provide richer signal than demographics
- **Low confidence**: Representation learning achieves "competitive" accuracy to behavioral model

## Next Checks
1. Conduct component ablation study to quantify independent contributions of ID embeddings, text representations, and demographics to prediction accuracy.
2. Analyze text response quality by measuring average valid word count, conducting sentiment analysis, and testing correlation between response length and prediction accuracy.
3. Apply the best-performing Beh2vec model to an independent behavioral dataset to test generalization across populations and decision contexts.