---
ver: rpa2
title: 'ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos'
arxiv_id: '2512.03666'
source_url: https://arxiv.org/abs/2512.03666
tags:
- grounding
- objects
- video
- object
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ToG-Bench, the first benchmark for task-oriented
  spatio-temporal grounding in egocentric videos. Unlike prior datasets focused on
  descriptive object localization, ToG-Bench requires identifying and localizing objects
  based on functional tasks rather than visual descriptions.
---

# ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos

## Quick Facts
- arXiv ID: 2512.03666
- Source URL: https://arxiv.org/abs/2512.03666
- Reference count: 40
- Introduces first benchmark for task-oriented spatio-temporal grounding in egocentric videos

## Executive Summary
This paper introduces ToG-Bench, the first benchmark for task-oriented spatio-temporal grounding in egocentric videos. Unlike prior datasets focused on descriptive object localization, ToG-Bench requires identifying and localizing objects based on functional tasks rather than visual descriptions. The benchmark supports explicit-implicit dual grounding, where targets may be directly mentioned or inferred from context, and one-to-many grounding, where a single instruction involves multiple related objects. Built on ScanNet, it contains 100 video clips, 2,704 instructions, and 4,194 object instances, annotated via a semi-automated pipeline combining foundation models with human verification. The authors propose task-level metrics jointly evaluating recognition accuracy and spatio-temporal precision, and benchmark seven MLLMs under zero-shot settings. Results show models perform well on explicit single-object tasks but struggle significantly with implicit reasoning and multi-object coordination, revealing key limitations in bridging perception and interaction for embodied AI. Data and code will be publicly released.

## Method Summary
ToG-Bench introduces task-oriented spatio-temporal video grounding (T-STVG) in egocentric videos, shifting from perceptual similarity to functional relevance as the grounding criterion. The benchmark is built on ScanNet with 100 video clips and 2,704 task instructions (51.26% explicit, 48.74% implicit) covering 177 object categories. A semi-automated pipeline combines Gemini 2.5 Pro for instruction generation, Grounding-DINO for object localization, SAM2 for spatio-temporal tracking, and human verification. The evaluation decouples recognition accuracy (using cosine similarity with all-mpnet-base-v2 embeddings) from spatio-temporal precision (mtIoU, mvIoU with thresholds at 0.3, 0.5, 0.7), reporting both task-level and object-level metrics with strict/standard protocols. Zero-shot evaluation on seven MLLMs reveals significant gaps in implicit reasoning and multi-object coordination capabilities.

## Key Results
- GPT-5 achieves 89.42% Task Accuracy but only 23.11% spatial IoU, revealing recognition-localization gaps
- Performance drops 18-32% between explicit and implicit tasks across models, indicating weak contextual reasoning
- One-to-many grounding causes steep performance degradation, with GPT-5 dropping from 96.05% to 75.00% as object count increases from 1 to 3+
- Open-source models (Qwen3-VL-8B, InternVL-8B) show stronger recognition (75-83%) but much weaker grounding (11-13% spatial IoU) compared to proprietary models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-oriented grounding exposes a recognition-localization gap that descriptive benchmarks miss
- Mechanism: By formulating instructions as goal-directed actions rather than object descriptions, models must first infer functionally relevant objects from task intent, then localize them spatio-temporally. This decoupling reveals where reasoning succeeds but grounding fails.
- Core assumption: Functional relevance is harder to infer than perceptual similarity; grounding precision depends on correct recognition.
- Evidence anchors:
  - [abstract] "ToG-Bench requires identifying and localizing objects based on functional tasks rather than straightforward descriptions"
  - [Section 3.1] "T-STVG shifts the grounding criterion from perceptual similarity to functional relevance"
  - [corpus] TimeScope (arXiv:2509.26360) similarly introduces task-oriented temporal grounding, reinforcing that task-driven paradigms expose distinct failure modes
- Break condition: If models could directly ground from intent without intermediate object categorization, the recognition-then-grounding evaluation paradigm would conflate rather than isolate failure points.

### Mechanism 2
- Claim: Explicit-implicit dual grounding creates asymmetric difficulty through contextual inference requirements
- Mechanism: Explicit tasks provide direct object mentions, while implicit tasks require inferring unmentioned but essential objects from task context. The 18-32% accuracy gap between explicit and implicit recognition shows implicit reasoning demands commonsense knowledge and scene understanding beyond visual perception.
- Core assumption: Implicit objects are functionally necessary but not linguistically specified; inference requires world knowledge.
- Evidence anchors:
  - [Section 1] "turn on the water at the sink involves an explicit object sink, as well as an implicit one, faucet"
  - [Table 4] GPT-5: T-EAcc 98.20% vs T-IAcc 80.20%; Qwen3-VL-8B: 75.97% vs 53.72%
  - [corpus] OmniGround (arXiv:2511.16937) notes similar gaps in complex query scenarios, though focuses on diverse objects rather than explicit-implicit splits
- Break condition: If implicit objects were always co-occurring with explicit mentions in training data, models could rely on statistical association rather than genuine reasoning.

### Mechanism 3
- Claim: One-to-many grounding enforces holistic task understanding through all-or-nothing success criteria
- Mechanism: Single instructions may require localizing multiple interdependent objects. Strict task-level evaluation assigns zero credit if any object is misidentified, penalizing partial understanding and revealing whether models capture object relationships within task contexts.
- Core assumption: Real embodied tasks involve multi-object coordination; success requires complete grounding.
- Evidence anchors:
  - [Section 3.1] "Work on your computer requires a computer, a mouse, and a keyboard"
  - [Table 5] Performance degrades as object count increases: GPT-5 drops from 96.05% T-Acc (1 object) to 75.00% (3+ objects)
  - [corpus] EgoGrasp (arXiv:2601.01050) addresses multi-entity hand-object interactions, showing related complexity in egocentric settings
- Break condition: If tasks could be decomposed into independent single-object subproblems without losing semantic coherence, the multi-object constraint would add computational but not conceptual difficulty.

## Foundational Learning

- Concept: Spatio-Temporal Video Grounding (STVG)
  - Why needed here: The baseline task this benchmark extends; requires understanding both when (temporal) and where (spatial) objects appear relative to text queries.
  - Quick check question: Given a video and query "the person opens the door," can you predict both the time interval and bounding box trajectory?

- Concept: Egocentric vs. Exocentric Perspective
  - Why needed here: ToG-Bench uses first-person video with camera motion, hand occlusion, and attention shifts absent in third-person benchmarks.
  - Quick check question: Why would an object detector trained on exocentric video struggle with egocentric frames showing only a hand gripping an object?

- Concept: Zero-Shot Evaluation Protocol
  - Why needed here: All benchmarked MLLMs are evaluated without task-specific fine-tuning; this measures transfer capability, not fitted performance.
  - Quick check question: What does a 40% gap between recognition accuracy and spatial grounding IoU suggest about pre-training vs. task-specific adaptation?

## Architecture Onboarding

- Component map: Video frames (0.25 fps) -> MLLM backbone (GPT-5, Qwen3-VL, InternVL) -> Output parser (JSON extraction) -> Evaluation layer (T-Acc, T-mtIoU, T-mvIoU metrics)

- Critical path:
  1. Instruction parsing -> task intent extraction
  2. Visual encoding across sampled frames
  3. Object category inference (explicit + implicit)
  4. Temporal boundary prediction (start/end seconds)
  5. Spatial coordinate normalization ([0,1] bounding boxes)
  6. Metric computation with IoU thresholds (0.3, 0.5, 0.7)

- Design tradeoffs:
  - Frame rate (0.25 fps) balances temporal coverage vs. compute; may miss brief object appearances
  - Strict evaluation (ST-* metrics) provides cleaner failure signals but lower absolute scores
  - Semi-automated annotation (Grounding-DINO + SAM2 + human verification) scales efficiently but may inherit model biases

- Failure signatures:
  - High T-Acc, low T-mvIoU: Correct object inference but poor spatial localization (common in open-source models)
  - Large EAcc vs IAcc gap: Weak contextual reasoning for implicit references
  - Sharp degradation with 3+ objects: No multi-object coordination mechanism

- First 3 experiments:
  1. Baseline reproduction: Run Qwen3-VL-8B on 10 explicit single-object tasks; verify T-Acc (~83%) and T-mvIoU (~11%) match Table 5
  2. Ablation on frame rate: Re-sample at 1 fps for short videos (<60s); measure whether T-mtIoU improves for brief object appearances
  3. Error analysis on implicit tasks: For 50 implicit tasks where GPT-5 fails, categorize failure as (a) wrong object inferred, (b) correct object but wrong temporal window, or (c) correct window but poor spatial precision

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can MLLM architectures be modified to bridge the performance gap where models successfully infer task-relevant object categories (high T-Acc) but fail to produce precise spatio-temporal bounding tubes (low mIoU)?
- **Basis in paper:** [explicit] The authors observe in Section 4.3 that while GPT-5 achieves 89.42% Task Accuracy, its temporal and spatial grounding remains limited, revealing that "correct identification does not guarantee precise localization."
- **Why unresolved:** Current models treat recognition and grounding as decoupled processes or lack the fine-grained visual attention mechanisms required to translate high-level semantic intent into pixel-level accuracy over time.
- **What evidence would resolve it:** Demonstration of a model architecture that maintains high task accuracy while significantly narrowing the gap between object-level semantic recognition scores and spatio-temporal IoU metrics.

### Open Question 2
- **Question:** What specific reasoning modules are required for models to effectively handle implicit grounding, where target objects must be inferred from functional context rather than explicit mention?
- **Basis in paper:** [explicit] Section 4.4 highlights a "substantial performance gap" between explicit and implicit tasks, noting that models suffer "severe degradation" when required to infer unmentioned objects (e.g., inferring "faucet" from "turn on water").
- **Why unresolved:** The paper suggests current MLLMs lack the ability to perform the contextual reasoning necessary to link action verbs to associated, but unstated, functional objects in a scene.
- **What evidence would resolve it:** A model capable of matching performance on implicit tasks to explicit tasks, or an ablation study showing that injecting a functional knowledge graph improves implicit grounding T-IAP scores.

### Open Question 3
- **Question:** How can "one-to-many" grounding capabilities be improved to prevent the observed performance collapse when localizing three or more objects simultaneously?
- **Basis in paper:** [explicit] The authors note in Section 4.4 that while proprietary models are robust, open-source models "collapse rapidly" on 3+ object tasks, indicating they "lack mechanisms to jointly localize and disambiguate multiple task-relevant objects."
- **Why unresolved:** This suggests current attention mechanisms may struggle to maintain distinct spatial representations for multiple entities correlated to a single complex instruction.
- **What evidence would resolve it:** A model achieving stable T-mvIoU (e.g., >30%) across 1-object, 2-object, and 3+ object tasks, rather than the steep degradation currently observed.

## Limitations

- Benchmark relies on ScanNet simulated egocentric videos, limiting generalizability to real-world scenarios with richer environmental dynamics and social interactions
- Semi-automated annotation pipeline may inherit biases from foundation models used in instruction generation and object grounding
- 0.25 fps sampling rate could miss brief but task-critical object appearances, potentially underestimating model capabilities in dynamic environments

## Confidence

- **High Confidence:** The recognition-localization gap mechanism and explicit-implicit difficulty asymmetry are well-supported by empirical results (Table 4 shows 18-32% gaps)
- **Medium Confidence:** The one-to-many grounding mechanism is theoretically sound, but the strict evaluation protocol may overestimate difficulty by penalizing partial successes that could be useful in practice
- **Medium Confidence:** The proposed metrics effectively decouple recognition and grounding performance, though their strict task-level variant may be overly conservative for real-world applications

## Next Checks

1. **Generalization Test:** Evaluate the benchmarked models on a subset of real-world egocentric videos (e.g., EPIC-KITCHENS) to assess cross-dataset performance and identify domain-specific limitations
2. **Sampling Rate Ablation:** Compare performance at 0.25 fps versus 1 fps for brief object appearances to quantify the impact of temporal resolution on grounding accuracy
3. **Partial Credit Analysis:** Implement relaxed task-level metrics that award partial credit for correct object recognition even when spatio-temporal localization fails, to better reflect practical utility