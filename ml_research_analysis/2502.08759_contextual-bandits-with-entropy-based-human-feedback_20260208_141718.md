---
ver: rpa2
title: Contextual bandits with entropy-based human feedback
arxiv_id: '2502.08759'
source_url: https://arxiv.org/abs/2502.08759
tags:
- expert
- feedback
- entropy
- human
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incorporating human feedback
  into contextual bandits while managing model uncertainty and variable feedback quality.
  The proposed method uses entropy-based thresholds to selectively solicit human feedback
  only when the agent's policy exhibits high uncertainty, dynamically balancing exploration
  and exploitation.
---

# Contextual bandits with entropy-based human feedback

## Quick Facts
- arXiv ID: 2502.08759
- Source URL: https://arxiv.org/abs/2502.08759
- Reference count: 40
- Primary result: Entropy-based feedback solicitation achieves lower cumulative regret with <30% expert queries

## Executive Summary
This paper addresses the challenge of incorporating human feedback into contextual bandits while managing model uncertainty and variable feedback quality. The proposed method uses entropy-based thresholds to selectively solicit human feedback only when the agent's policy exhibits high uncertainty, dynamically balancing exploration and exploitation. The approach integrates two feedback mechanisms: Action Recommendation (AR), where experts suggest actions, and Reward Manipulation (RM), where penalties adjust the reward signal. Experimental results across multiple datasets and algorithms show significant performance improvements compared to baseline methods, with AR and RM achieving lower cumulative regret while requiring less than 30% of training steps for expert queries.

## Method Summary
The method queries experts when policy entropy exceeds a threshold λ, using two feedback types: Action Recommendation (AR) which directly constrains action selection, and Reward Manipulation (RM) which adds penalties for non-recommended actions. The framework works with any stochastic bandit policy and is evaluated on multi-label datasets using cumulative regret as the primary metric. Expert feedback is simulated with variable quality levels, and the system achieves significant performance gains compared to baselines while reducing expert queries to less than 30% of training steps.

## Key Results
- Entropy-based feedback reduces expert queries to <30% of training steps while achieving lower cumulative regret
- Action Recommendation (AR) outperforms Reward Manipulation (RM) on datasets with large action spaces
- Higher expert quality does not always improve performance - moderate-quality experts can outperform near-perfect ones at low entropy thresholds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy-based feedback solicitation reduces expert queries to <30% of training steps while achieving lower cumulative regret.
- Mechanism: At each round t, compute policy entropy H(π) = −Σ π(a_t|s_t) log(π(a_t|s_t)). Query expert only when H(π) > λ (predefined threshold). This targets feedback to high-uncertainty states where guidance is most informative, avoiding wasteful queries when the policy is already confident.
- Core assumption: Policy entropy correlates with decision quality—high entropy indicates states where expert guidance can meaningfully redirect exploration.
- Evidence anchors:
  - [abstract]: "soliciting expert feedback only when model entropy exceeds a predefined threshold... requiring less than 30% of training steps for expert queries"
  - [section 3.3]: "the model then queries for human feedback when the model entropy exceeds a predefined threshold λ"
  - [corpus]: Neighbor papers on active feedback collection (arxiv:2504.12016) support selective querying principles, though with different formulations (dueling bandits, neural approximations).

### Mechanism 2
- Claim: Action Recommendation (AR) outperforms Reward Manipulation (RM) when expert quality is high and action spaces are large.
- Mechanism: AR directly constrains action selection: â_t = E_AR(s_t, q_t), then a_t ~ Uniform(â_t). This bypasses the reward learning bottleneck, immediately narrowing exploration to expert-vetted actions. RM instead adds penalty r_p when a_t ∉ â_t, shaping the reward signal without constraining actions directly.
- Core assumption: AR assumes experts can identify correct actions; RM assumes reward shaping can guide learning without direct action intervention.
- Evidence anchors:
  - [section 3.2.1-3.2.2]: Formal definitions of AR (Eq. 2-4) and RM (Eq. 5-6)
  - [section 4.3]: "on the Delicious dataset, AR demonstrates the best performance... Delicious, with many possible actions, sees AR accelerating convergence by narrowing down the action space"

### Mechanism 3
- Claim: Higher expert quality does not always improve performance—counterintuitively, moderate-quality experts can outperform near-perfect ones.
- Mechanism: At low entropy thresholds (frequent queries), high-quality experts provide accurate recommendations that the model always accepts, leading to pure exploitation with limited exploration. Lower-quality experts introduce randomness that maintains broader action space exploration, potentially discovering higher-reward actions the expert missed.
- Core assumption: Some exploration is necessary even with expert guidance; over-reliance on expert recommendations can prematurely narrow the search.
- Evidence anchors:
  - [section 4.4]: "at lower entropy thresholds, an interesting pattern emerges: increasing expert quality can actually lead to a decrease in model performance... at high expert levels... the result is pure exploitation"
  - [figure 4]: Shows non-monotonic relationship between expert accuracy and cumulative regret across algorithms

## Foundational Learning

- Concept: **Shannon entropy of categorical distributions**
  - Why needed here: Core uncertainty quantification for triggering feedback. Must understand H(p) = −Σ p_i log(p_i) and its behavior (maximum at uniform, minimum at deterministic).
  - Quick check question: Given policy outputs [0.7, 0.2, 0.1] over 3 actions, what's the entropy? (Answer: ≈0.80 nats; compare to max of ≈1.10 for uniform)

- Concept: **Cumulative regret in bandits**
  - Why needed here: Primary evaluation metric. Regret_T = Σ_t [r_t(a*_t) − r_t(a_t)] measures opportunity cost of suboptimal actions.
  - Quick check question: If optimal action always yields reward 1 and your policy averages 0.8 over 1000 rounds, what's cumulative regret? (Answer: 200)

- Concept: **Exploration-exploitation trade-off**
  - Why needed here: Entropy thresholding is fundamentally an exploration-control mechanism. Understanding why policies must explore (to gather information) vs. exploit (to maximize immediate reward) is essential.
  - Quick check question: Why might a deterministic greedy policy fail in contextual bandits? (Answer: Never discovers potentially better actions; stuck in local optimum)

## Architecture Onboarding

- Component map: Context -> Bandit policy -> Entropy monitor -> Threshold comparison -> Feedback gate -> Expert interface -> Policy updater -> Reward/Action update
- Critical path:
  1. Context arrives → 2. Bandit outputs action distribution → 3. Compute entropy → 4. Compare to λ → 5a. If below: execute bandit action, observe reward, update OR 5b. If above: query expert, apply AR/RM, observe modified outcome, update
- Design tradeoffs:
  - **λ selection**: Lower thresholds = more queries, higher expert costs, potentially better performance but risk of over-reliance on expert (especially if quality is moderate). Higher thresholds = fewer queries, more autonomous operation, but may miss critical intervention opportunities.
  - **AR vs RM**: Choose AR when experts are high-quality and direct action control is acceptable (e.g., safety-critical applications, human-in-the-loop systems). Choose RM when preserving agent autonomy is important or when expert knowledge is uncertain.
  - **Expert quality estimation**: Paper assumes known q_t; in practice, must infer expert reliability from historical agreement rates.
- Failure signatures:
  - **Excessive querying**: λ set too low → majority of rounds request feedback, defeating efficiency purpose. Check: query percentage should stabilize <40% after warmup.
  - **Stagnant regret**: If cumulative regret plateaus early, entropy may be collapsing prematurely (policy too confident too early). Consider entropy bonus regularization.
  - **Performance degradation with expert**: If adding expert feedback hurts vs. baseline, check: (1) expert quality estimation is wrong, (2) λ triggers on wrong states, (3) RM penalty magnitude is too large/small.
- First 3 experiments:
  1. **Entropy threshold sweep**: Run λ ∈ {1.5, 2.5, 4.0, 6.0} on validation split. Plot query percentage vs. cumulative regret. Select λ achieving <30% queries with regret within 5% of best observed.
  2. **AR vs RM ablation**: Fix optimal λ, compare AR and RM across expert qualities q ∈ {0.3, 0.5, 0.7, 0.9}. Confirm AR superiority at high q, RM robustness at low q on your specific action space size.
  3. **Expert quality robustness test**: Run at q = 0.5 (moderate) with λ optimized for q = 0.9. Measure performance gap vs. λ re-optimized for true q. Quantifies sensitivity to expert quality misspecification.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the analysis, several important open questions emerge:

1. Can the entropy threshold λ be adapted online based on performance metrics to eliminate the need for domain-specific hyperparameter tuning?
2. How does the framework perform with actual human feedback that includes noise, inconsistency, and latency, compared to the simulated oracle feedback used in experiments?
3. How can the framework be modified to prevent performance degradation observed when high-quality expert feedback reduces necessary exploration?

## Limitations
- The entropy-thresholding mechanism's effectiveness is highly dependent on appropriate λ selection, with no principled method for threshold selection in practice
- Expert quality estimation is assumed to be known, but real-world expert reliability typically requires inference from historical performance
- Reward penalty magnitude for RM feedback is not specified, creating ambiguity in reproducing exact experimental conditions

## Confidence
- **High confidence**: The fundamental mechanism of entropy-based feedback solicitation works as described, and the inverse relationship between expert quality and performance at low entropy thresholds is well-supported by the experimental data.
- **Medium confidence**: The comparative performance of AR vs RM across different action space sizes and expert qualities is reasonably well-supported, though the lack of specified penalty magnitudes for RM introduces uncertainty.
- **Low confidence**: The theoretical foundations for why moderate-quality experts can outperform high-quality ones at certain thresholds are not rigorously proven; the explanation relies primarily on empirical observation without formal analysis of the exploration-exploitation dynamics.

## Next Checks
1. **Threshold Sensitivity Analysis**: Run comprehensive sweeps across λ ∈ [1.0, 8.0] on all datasets to map the full query-regret trade-off curve and identify optimal thresholds for different expert quality levels.
2. **Expert Quality Misspecification Test**: Evaluate performance when true expert quality differs from assumed quality (e.g., assume q=0.8 when true q=0.5) to quantify robustness to estimation errors.
3. **RM Penalty Magnitude Study**: Systematically vary the reward penalty rp ∈ {-5, -2, -1, -0.5, 0} to determine optimal values and assess sensitivity of RM performance to this critical hyperparameter.