---
ver: rpa2
title: Playing the Lottery With Concave Regularizers for Sparse Trainable Neural Networks
arxiv_id: '2501.11135'
source_url: https://arxiv.org/abs/2501.11135
tags:
- pruning
- neural
- regularization
- concave
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for finding sparse, trainable
  neural networks that addresses the challenge of extracting effective subnetworks,
  called winning tickets, from dense networks. The method uses concave regularization
  to promote sparsity of a relaxed binary mask representing the network topology,
  allowing for softer pruning decisions compared to hard percentage-based methods.
---

# Playing the Lottery With Concave Regularizers for Sparse Trainable Neural Networks

## Quick Facts
- arXiv ID: 2501.11135
- Source URL: https://arxiv.org/abs/2501.11135
- Reference count: 40
- Primary result: A method using concave regularization on relaxed binary masks achieves comparable accuracy to iterative magnitude pruning at moderate sparsities and significantly outperforms it at higher sparsities.

## Executive Summary
This paper proposes a novel approach to finding sparse, trainable neural networks by using concave regularization on a relaxed binary mask that represents the network topology. The method addresses the challenge of extracting effective subnetworks (winning tickets) from dense networks by allowing softer pruning decisions compared to hard percentage-based methods. Through theoretical analysis and experiments on various datasets and architectures, the authors demonstrate that their approach achieves superior performance at high sparsity levels while maintaining competitive accuracy at moderate sparsities.

## Method Summary
The method jointly trains and sparsifies a neural network by optimizing both weights and a continuous mask variable in [0,1]^d. A concave regularizer (either ℓ1 or log) is applied to the mask to promote sparsity, with mask values gradually pushed toward zero during training. After optimization, a small threshold α is applied to finalize binary pruning decisions. The process follows a three-stage loop: train dense model → optimize mask and weights jointly → apply threshold → rewind weights → repeat for multiple rounds. The final sparse subnetwork uses the learned mask applied to the original initialization.

## Key Results
- Achieves comparable accuracy to iterative magnitude pruning at moderate sparsities (10-20% remaining weights)
- Significantly outperforms iterative magnitude pruning at high sparsities (>80% sparsity)
- Log regularization provides tighter theoretical error bounds than ℓ1 under convexity assumptions
- Threshold-based pruning adapts better to layer-wise weight distributions than fixed percentage removal

## Why This Works (Mechanism)

### Mechanism 1: Relaxed Binary Mask with Concave Regularization
Using a continuous mask variable in [0,1]^d with concave regularization yields softer pruning decisions and better accuracy at high sparsity than hard magnitude-based pruning. The mask m is optimized as a continuous variable, producing "relevance scores" per parameter. Concave regularizers (e.g., log penalty) more closely approximate the ℓ0-norm shape than ℓ1, encouraging sparser solutions while remaining differentiable in (0,1). A small threshold α ∈ (0,1) is then applied to finalize binary pruning, avoiding premature hard decisions.

### Mechanism 2: Threshold-Based Pruning vs Fixed Percentage Removal
Setting a fixed threshold on mask values (rather than removing a fixed percentage of weights per round) better adapts to layer-wise weight distributions and improves final accuracy. Instead of enforcing a uniform sparsity schedule, thresholding allows the mask optimization to naturally determine which weights fall below α, accommodating varying weight magnitudes across layers.

### Mechanism 3: Strictly Concave Regularizer (Log) vs ℓ1
Log regularization provides tighter error bounds than ℓ1 for recovering optimal binary masks under certain convexity assumptions. Log penalty more aggressively penalizes small values (closer to ℓ0 geometry), yielding smaller approximation error ϕ(m*) in theoretical bounds. In practice, this can translate to sparser masks with comparable or higher accuracy.

## Foundational Learning

- **Lottery Ticket Hypothesis (LTH)**: The method is framed as a "lottery ticket" search—finding sparse subnetworks trainable in isolation with comparable accuracy. Understanding LTH clarifies the three-stage pipeline (training → pruning → rewinding) and the importance of initialization. *Quick check*: Why does rewinding weights to early iteration or initialization matter for winning tickets in sparse-to-sparse training?

- **Concave Regularization (ℓ1 vs Log)**: The core novelty is using concave regularizers to promote sparsity in a relaxed mask. Understanding how ℓ1 and log penalties differ in shape and optimization behavior helps interpret theoretical and empirical results. *Quick check*: Why does a strictly concave regularizer like log provide a closer approximation to ℓ0 than ℓ1, and what tradeoff does this introduce?

- **Projected Gradient Descent on Constrained Variables**: The mask is constrained to [0,1]^d; optimization uses projected gradient descent to handle non-differentiability at boundaries. Grasping this technique is essential for correct implementation. *Quick check*: How does projecting gradient updates onto [0,1]^d avoid issues with non-differentiable regularization at zero?

## Architecture Onboarding

- **Component map**: Mask m ∈ [0,1]^d -> Weights θ ∈ ℝ^d -> Loss L(x; m ⊙ θ) + λR(m) -> Threshold α -> Binary mask

- **Critical path**: 1) Initialize weights θ0 and mask m0 = 0.5·1. 2) For each round t: Set θ = θ0, m = mt−1; Minimize L(x; m ⊙ θ) + λR(m) via projected gradient descent; Binarize: set mt,i = 0 if mt,i < α; Optionally rewind weights to θ0 or early-iteration values for next round. 3) Final sparse subnetwork is mT ⊙ θ0.

- **Design tradeoffs**: ℓ1 vs Log regularizer: Log yields tighter theoretical bounds and sometimes better accuracy but introduces ε; ℓ1 is simpler and more stable. Threshold vs percentage pruning: Threshold adapts across layers but requires tuning α; percentage is easier to implement but can misprune. Number of rounds T: More rounds improve mask quality but increase compute; experiments use T=3.

- **Failure signatures**: Mask collapse: All mask values go to 0 or 1 early → learning stops; check λ too large or learning rate issues. No sparsity gain: Final mask remains dense → λ too small or regularizer ineffective. Accuracy drop at high sparsity: Model underfits → regularizer too aggressive or α too high. Oscillations in loss: Subgradient issues if ℓ1 is applied directly to θ instead of m; use projected gradient on m.

- **First 3 experiments**: 1) Sanity check on MNIST logistic regression: Replicate Section IV-C to verify mask learning and sparsity; confirm ℓ1-on-θ fails while mask methods succeed. 2) Ablation on α and λ: Sweep α ∈ {0.01, 0.05, 0.1} and λ ∈ {10−5, 10−6, 10−7} on CIFAR-10 with ResNet-20; plot accuracy vs sparsity. 3) Compare ℓ1 vs Log on CIFAR-100: Train ResNet-20 with both regularizers for T=3 rounds; report test accuracy at 10–20% remaining weights.

## Open Questions the Paper Calls Out

### Open Question 1
Can the tuning of the regularization parameter λ be automated to eliminate manual search while preserving the sparsity-accuracy trade-off? The authors state future work will focus on "optimizing the tuning of the hyperparameters." A self-adjusting learning schedule for λ that matches the performance of grid-searched values on benchmarks would resolve this.

### Open Question 2
Can this method be adapted to find winning tickets in randomly initialized, untrained networks (strong lottery ticket framework) to reduce computational complexity? The conclusion lists "considering... untrained models as in the strong lottery ticket framework" as a goal for reducing complexity. Successful extraction of high-accuracy subnetworks from random weights using the proposed concave regularizers without training the full model would resolve this.

### Open Question 3
Do the theoretical error bounds derived for convex losses provide valid approximations for the non-convex loss landscapes of deep neural networks? Section IV provides theoretical analysis restricted to the "convex framework" (Assumption 2), whereas the practical application involves non-convex networks. Empirical validation showing the derived bounds correlate with actual mask estimation errors in deep learning tasks, or a theoretical extension to non-convex functions, would resolve this.

## Limitations
- Theoretical analysis relies on strong convexity assumptions that may not hold in practice
- Introduces additional hyperparameter ε for log regularizer that is not thoroughly explored
- Multiple rounds of training still required, limiting practical efficiency gains
- Scalability claims to larger architectures lack detailed methodology and ablation studies

## Confidence

**High confidence**: The core observation that joint mask optimization with threshold-based pruning improves upon iterative magnitude pruning at high sparsity levels (Fig. 5-7). The failure of ℓ1 regularization applied directly to weights versus mask variables is clearly demonstrated (Section IV-C).

**Medium confidence**: The theoretical advantage of log regularization over ℓ1 is rigorously proven under Assumption 2, but the practical significance is inconsistent across experiments (Fig. 5 shows mixed results). The claim that mask-based pruning better adapts to layer-wise weight distributions than percentage-based methods is supported by ablation studies but lacks broader validation.

**Low confidence**: The scalability claims to larger architectures like ResNet-50 and EfficientNet-B0 are based on preliminary experiments without detailed methodology or ablation studies. The paper does not address computational overhead comparisons or sensitivity to initialization beyond the basic rewinding approach.

## Next Checks
1. **Sensitivity Analysis**: Systematically vary α and λ across a wider range for both ℓ1 and log regularizers on CIFAR-10/100 to quantify performance stability and identify optimal hyperparameter regions.

2. **Ablation on Mask Initialization**: Compare performance when initializing mask m from uniform distribution vs. pre-trained magnitude-based masks to test whether the proposed method provides advantages beyond simple initialization heuristics.

3. **Single-Round vs Multi-Round Comparison**: Evaluate whether the three-round iterative framework provides significant gains over a single-round approach with aggressive λ, measuring both accuracy and computational cost to assess practical tradeoffs.