---
ver: rpa2
title: 'TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II'
arxiv_id: '2507.15618'
source_url: https://arxiv.org/abs/2507.15618
tags:
- tactical
- starcraft
- action
- policy
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TacticCraft introduces adapter-based tactical conditioning for
  StarCraft II agents, enabling natural language-driven strategy customization. The
  method freezes a pre-trained DI-Star policy network and attaches lightweight adapter
  modules to each action head, conditioned on a tactical tensor representing strategic
  preferences.
---

# TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II

## Quick Facts
- arXiv ID: 2507.15618
- Source URL: https://arxiv.org/abs/2507.15618
- Reference count: 30
- Primary result: Adapter-based tactical conditioning enables natural language-driven strategy customization in StarCraft II, achieving 89% win rate against built-in Level 10 AI at early training epochs.

## Executive Summary
TacticCraft introduces a novel approach for tactical conditioning of StarCraft II agents using adapter modules that enable natural language-driven strategy customization. The method freezes a pre-trained DI-Star policy network and attaches lightweight adapter modules to each action head, conditioned on a tactical tensor representing strategic preferences. By training these adapters with KL divergence constraints, the approach maintains core gameplay competencies while enabling tactical variations across dimensions like aggression, expansion patterns, and technology preferences. Experimental results demonstrate that tactical conditioning achieves competitive performance against built-in Level 10 AI, with win rates reaching 89% at early training epochs while successfully modulating agent behavior according to tactical specifications.

## Method Summary
TacticCraft addresses tactical conditioning in StarCraft II by attaching adapter modules to a frozen DI-Star policy network. The method uses a tactical tensor representing strategic preferences that is input to lightweight 2-layer MLP adapters (64→32 hidden, ReLU) attached to each action head of the base policy. These adapters are trained using KL divergence loss that constrains the conditioned policy to stay close to the base policy while enabling tactical variations. The approach employs additive fusion to combine adapter outputs with base policy logits, and uses zero-initialization to ensure initial behavior matches the frozen base policy. Training is performed on ZvZ replays with tactical labels generated by LLMs, with win rate against built-in Level 10 AI as the primary evaluation metric.

## Key Results
- Achieved 89% win rate against built-in Level 10 AI at early training epochs (epoch 1K)
- Successfully modulated agent behavior across tactical dimensions including aggression, expansion patterns, and technology preferences
- Discovered novel strategic combinations, such as transitioning from +1 Zergling to Lurker strategies, demonstrating potential for both strategy customization and tactical innovation

## Why This Works (Mechanism)
The method works by freezing a pre-trained DI-Star policy network and attaching lightweight adapter modules to each action head, conditioned on a tactical tensor representing strategic preferences. The adapters are trained with KL divergence constraints that maintain the agent's core gameplay competencies while enabling tactical variations. This approach allows for natural language-driven strategy customization without requiring extensive retraining of the entire policy network, preserving the base policy's learned capabilities while enabling tactical specialization.

## Foundational Learning

**Adapter modules** - Small neural network components that can be attached to existing models to enable new capabilities without retraining the entire model. Why needed: Allows tactical conditioning without disrupting base policy competencies. Quick check: Verify adapter dimensions match action head outputs and that zero-initialization preserves base policy behavior.

**KL divergence loss** - Measures the difference between probability distributions, used here to constrain the conditioned policy to stay close to the base policy. Why needed: Prevents catastrophic forgetting while enabling tactical adaptation. Quick check: Monitor KL divergence values per head during training to ensure constraints are effective.

**Tactical tensor conditioning** - A 9-dimensional vector representing strategic preferences that conditions the adapter modules. Why needed: Provides natural language-driven control over tactical behavior. Quick check: Verify tactical tensor varies between samples and influences adapter outputs.

**Action head architecture** - DI-Star's decomposed action space with separate heads for action type, delay, queued, selected units, target unit, location, and LSTM. Why needed: Understanding which heads to condition for tactical versus core mechanical skills. Quick check: Examine which heads show the most tactical variation under different conditioning.

## Architecture Onboarding

**Component map:** Tactical tensor -> Adapter modules (per action head) -> Additive fusion with base policy logits -> KL divergence loss -> Win rate evaluation

**Critical path:** Tactical input → Adapter modules → Action head conditioning → Policy output → Game performance

**Design tradeoffs:** The frozen base policy preserves core competencies but limits adaptation potential; lightweight adapters enable efficient training but may constrain complex tactical learning; KL divergence constraints maintain stability but may restrict tactical innovation.

**Failure signatures:** Performance collapse at later epochs (win rate drops from ~89% to ~14%); adapters failing to modulate behavior (all tactics produce identical play); KL divergence explosion indicating loss of base policy capabilities.

**First experiments:** 1) Verify base policy runs on ZvZ environment; 2) Implement adapter modules with zero-initialization and confirm initial behavior matches base; 3) Train with KL loss and monitor per-head KL divergence to identify which heads contribute to performance degradation.

## Open Questions the Paper Calls Out

**Open Question 1:** What mechanisms can resolve the trade-off where tactical specialization improves style conformity but degrades general win rates against fixed opponents? The paper observes performance decline from ~80-89% to ~14-18% win rates during training but only speculates about "growing influence of tactical specialization" without identifying specific failure modes.

**Open Question 2:** Does adapter-based tactical conditioning generalize to non-mirror matchups and other races beyond Zerg vs. Zerg? All experiments use ZvZ only with Zerg-specific tactical categories, with no validation that the approach transfers to other matchups.

**Open Question 3:** How should KL divergence weights be optimally configured across action heads for different tactical adaptation goals? The paper tests four weight configurations but finds Configuration C performs best without explaining why, stating "the optimal configuration depends on the specific application goals."

## Limitations
- Performance degradation at later training epochs (win rate drops from ~89% to ~14%), suggesting potential overfitting or instability
- Dependence on high-quality tactical annotations generated by LLMs introduces uncertainty about strategy representation faithfulness
- Narrow evaluation scope focused on ZvZ matchups against built-in AI, with limited testing against diverse strategies or human opponents

## Confidence
- Tactical conditioning effectiveness: Medium - Clear tactical modulation shown in early training, but performance degradation raises questions about long-term stability
- Adapter architecture validity: High - Technical implementation details are thoroughly specified with clear mathematical formulations
- Strategic innovation capability: Medium - Novel strategy discovery demonstrated, but extent of true innovation versus policy exploration is unclear

## Next Checks
1. Reproduce complete training pipeline with KL divergence monitoring per action head to identify which heads contribute to performance degradation at later epochs
2. Test adapter generalization by evaluating on ZvP and ZvT matchups with the same tactical conditioning framework
3. Conduct ablation studies removing the KL divergence constraint to quantify its role in maintaining base policy performance while enabling tactical adaptation