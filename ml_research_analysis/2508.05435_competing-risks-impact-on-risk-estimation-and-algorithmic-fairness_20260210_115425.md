---
ver: rpa2
title: 'Competing Risks: Impact on Risk Estimation and Algorithmic Fairness'
arxiv_id: '2508.05435'
source_url: https://arxiv.org/abs/2508.05435
tags:
- competing
- risks
- risk
- survival
- censoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that treating competing risks as censoring
  in survival models leads to biased risk estimates and exacerbates group disparities.
  Treating competing risks as censoring systematically overestimates individual risk
  in proportion to their likelihood of experiencing a competing event.
---

# Competing Risks: Impact on Risk Estimation and Algorithmic Fairness
arXiv ID: 2508.05435
Source URL: https://arxiv.org/abs/2508.05435
Authors: Vincent Jeanselme; Brian Tom; Jessica Barrett
Reference count: 40
Key outcome: This paper demonstrates that treating competing risks as censoring in survival models leads to biased risk estimates and exacerbates group disparities.

## Executive Summary
This paper reveals a critical statistical bias in survival analysis: when competing risks are treated as censoring, individual risk estimates become systematically inflated in proportion to the likelihood of experiencing the competing event. This bias creates unequal performance across demographic groups with different competing risk profiles, manifesting as group-specific fairness gaps in algorithmic predictions. The authors validate this finding through synthetic data experiments and demonstrate its practical impact using cardiovascular risk prediction on the Framingham dataset, where proper competing risk modeling improves both calibration and fairness, particularly for older patients and men who face higher competing risk rates.

## Method Summary
The authors employ synthetic data generation to systematically validate their theoretical findings about competing risk bias, creating controlled scenarios where the ground truth is known. They implement and compare multiple modeling approaches including standard survival models that treat competing risks as censoring versus competing risk-specific models like the Fine-Gray model and its neural network extensions. The empirical validation uses the Framingham cardiovascular dataset, comparing standard risk prediction methods against competing risk-aware approaches. The analysis examines both individual calibration (how well predicted risks match observed outcomes) and group-level fairness metrics to quantify disparities across demographic groups.

## Key Results
- Treating competing risks as censoring systematically overestimates individual risk in proportion to their likelihood of experiencing a competing event
- Neural network approaches, while more flexible, still propagate these errors without proper competing risk modeling
- In the Framingham cardiovascular case study, proper competing risk modeling improved calibration and reduced algorithmic fairness gaps, particularly for older patients and men

## Why This Works (Mechanism)
Competing risks violate the standard survival analysis assumption that censored individuals would experience the event of interest if followed long enough. When a competing event occurs, it prevents the primary event from ever happening, but standard models incorrectly treat these cases as if they might still occur. This leads to inflated hazard estimates and biased risk predictions. The bias magnitude depends on the competing risk probability - individuals with higher competing risk probabilities face greater overestimation. Across groups with different competing risk profiles, this creates systematic performance disparities that manifest as algorithmic fairness gaps.

## Foundational Learning
Competing risk problem (why needed: to understand the statistical bias): In survival analysis, competing risks occur when multiple types of events can happen, and one event prevents others from occurring. Quick check: Can you explain why a death from heart attack is a competing risk for stroke prediction?

Fine-Gray sub-distribution hazard model (why needed: the proper statistical framework): Models the hazard of the event of interest in the presence of competing risks by weighting individuals by their probability of not experiencing competing events. Quick check: How does the Fine-Gray model differ from standard Cox proportional hazards?

Algorithmic fairness in risk prediction (why needed: to understand the equity implications): Systematic differences in model performance across demographic groups that can lead to unequal treatment recommendations. Quick check: What fairness metrics would you use to compare risk prediction models across groups?

Calibration vs discrimination (why needed: to distinguish model performance aspects): Calibration measures how well predicted probabilities match observed frequencies, while discrimination measures the ability to rank individuals correctly. Quick check: Can you have good discrimination but poor calibration?

## Architecture Onboarding
Component map: Data preprocessing -> Risk prediction model -> Calibration assessment -> Fairness evaluation -> Competing risk correction
Critical path: Synthetic data generation -> Model training (with/without competing risk) -> Risk prediction -> Calibration analysis -> Fairness gap quantification
Design tradeoffs: Flexible neural models vs interpretable statistical models, computational complexity vs statistical rigor, individual accuracy vs group fairness
Failure signatures: Systematic overestimation of risk for high-competing-risk individuals, performance disparities across demographic groups, poor calibration in presence of competing events
Three first experiments:
1. Generate synthetic data with known competing risk structure and compare standard vs competing risk models
2. Apply competing risk correction to an existing clinical risk score and evaluate changes in calibration
3. Test whether post-hoc scaling by competing risk probability reduces bias in pre-existing models

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Can post-training adjustments, such as scaling risk predictions by the probability of not experiencing a competing event, effectively mitigate bias in pre-existing risk scores without requiring full model retraining?
- **Basis in paper:** Section 6.3 suggests exploring post hoc correction strategies as a mitigation route for existing tools where model replacement is impractical.
- **Why unresolved:** Theoretical correction relationships are established, but the practical efficacy and robustness of such adjustments on deployed clinical scores remain untested.
- **What evidence would resolve it:** Empirical validation showing that applying post-hoc corrections to standard risk scores reduces bias and fairness gaps comparably to full retraining with competing risk models.

### Open Question 2
- **Question:** How can interpretable modeling or post-hoc auditing be developed for competing risk models to resolve the trade-off between bias reduction and causal interpretability?
- **Basis in paper:** Section 6.3 notes that competing risk models complicate causal interpretations, often leading practitioners to favor simpler but biased models, and calls for interpretability research.
- **Why unresolved:** Flexible models (e.g., Neural Fine-Gray) reduce bias but lack the immediate clinical interpretability of standard hazard ratios, creating a barrier to adoption.
- **What evidence would resolve it:** The development of frameworks that explain sub-distribution predictions (e.g., feature importance) while maintaining statistical rigor for domain experts.

### Open Question 3
- **Question:** How can the explicit modeling of competing risks be leveraged to design granular decision support systems that recommend different interventions based on the predicted event type?
- **Basis in paper:** Section 6.3 proposes that modeling competing risks can inform granular decision processes, suggesting different risks may warrant different interventions (e.g., palliative vs. preventative care).
- **Why unresolved:** The current work focuses on correcting the estimation of the primary event; the utility of using the competing risk probability itself as a distinct actionable signal remains untested.
- **What evidence would resolve it:** Decision-analytic studies demonstrating that treatment policies conditioning on both the event of interest and competing risk probabilities lead to improved overall utility.

## Limitations
- Findings may not generalize beyond cardiovascular applications to other medical domains with different competing risk structures
- Analysis focuses on calibration and bias but does not extensively explore downstream clinical consequences on treatment decisions
- Limited detail on hyperparameter sensitivity and model stability across different training runs

## Confidence
- High confidence in the fundamental statistical bias caused by treating competing risks as censoring
- Medium confidence in the magnitude and direction of fairness disparities across groups
- Medium confidence in the effectiveness of competing risk modeling approaches for bias mitigation

## Next Checks
1. Validate findings across multiple disease domains with varying competing risk profiles (e.g., cancer with treatment-related mortality, organ transplantation with surgical complications)
2. Conduct simulation studies to quantify the downstream clinical impact of competing risk biases on treatment decisions and patient outcomes
3. Perform sensitivity analyses on neural network architectures and training procedures to establish robustness of fairness improvements across methodological variations