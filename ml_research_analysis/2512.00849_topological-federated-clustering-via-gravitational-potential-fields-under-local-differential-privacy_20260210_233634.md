---
ver: rpa2
title: Topological Federated Clustering via Gravitational Potential Fields under Local
  Differential Privacy
arxiv_id: '2512.00849'
source_url: https://arxiv.org/abs/2512.00849
tags:
- privacy
- data
- clustering
- local
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of clustering non-IID data under
  strong local differential privacy (LDP) in federated settings, where existing one-shot
  methods fail due to noise-sensitive distance metrics. The proposed Gravitational
  Federated Clustering (GFC) reformulates clustering as a topological persistence
  problem in a synthetic gravitational potential field, transforming privatized client
  centroids into stable cluster centers through persistent homology analysis.
---

# Topological Federated Clustering via Gravitational Potential Fields under Local Differential Privacy

## Quick Facts
- arXiv ID: 2512.00849
- Source URL: https://arxiv.org/abs/2512.00849
- Reference count: 40
- Primary result: Achieves 162.62% ARI improvement over baselines on MNIST under ε=0.01 privacy budget

## Executive Summary
This paper addresses the challenge of clustering non-IID data under strong local differential privacy (LDP) in federated settings, where existing one-shot methods fail due to noise-sensitive distance metrics. The proposed Gravitational Federated Clustering (GFC) reformulates clustering as a topological persistence problem in a synthetic gravitational potential field, transforming privatized client centroids into stable cluster centers through persistent homology analysis. The method introduces client-side compactness-aware perturbation and server-side topological aggregation without iterative communication. GFC achieves unprecedented privacy-accuracy trade-offs, maintaining robust performance across varying privacy constraints (ε=0.01 to 1000) and scaling to 100-1000 clients where competing approaches fail.

## Method Summary
GFC operates in a one-shot federated setting where clients first apply ε-LDP noise to their local data, then perform local k-means clustering to generate centroids with compactness-based mass weights. The server aggregates these noisy centroids by constructing a gravitational potential field over a synthetic grid, where each centroid contributes potential inversely proportional to its distance. The server then analyzes the topological persistence of this field using superlevel set filtration and merge tree analysis to extract stable cluster centers. The method eliminates the need for distance-based clustering on noisy data by instead identifying persistent topological features that correspond to true clusters, while filtering out noise-induced artifacts.

## Key Results
- Achieves 162.62% ARI improvement over state-of-the-art methods on MNIST under ε=0.01 privacy budget
- Maintains robust performance across privacy constraints (ε=0.01 to 1000) without parameter tuning
- Scales to 100-1000 clients where competing approaches fail due to communication overhead and noise accumulation

## Why This Works (Mechanism)

### Mechanism 1: Structural Filtering via Persistent Homology
Replaces noise-sensitive distance metrics with topological persistence to distinguish stable global clusters from random noise artifacts. The server constructs a merge tree from superlevel sets of the gravitational potential field, selecting centroids corresponding to components with longest lifetimes (persistence).

### Mechanism 2: Noise Suppression via Gravitational Smoothing
The gravitational potential field formulation provides inherent Lipschitz smoothing that dampens noise oscillation. The field integrates signals from multiple centroids, theoretically suppressing the impact of O(1/ε²) mass perturbation error while leaving only O(1/ε) positional error.

### Mechanism 3: Compactness-Aware Mass Weighting
Clients weight uploaded centroids by local cluster compactness (inertia), allowing the server to prioritize high-quality local signals over sparse or noisy local clusters. Compact clusters (low variance) result in higher mass, exerting stronger gravity on the server's potential field.

## Foundational Learning

- **Concept:** Persistent Homology (0-dimensional)
  - **Why needed:** To understand how the algorithm separates "signal" from "noise" without using distance thresholds
  - **Quick check:** If a noise spike creates a peak in the potential field that exists for only a very small range of energy thresholds, will it have high or low persistence? (Answer: Low)

- **Concept:** Local Differential Privacy (LDP) Mechanics
  - **Why needed:** To understand the constraint where noise is added on device (client-side) via Laplace mechanism
  - **Quick check:** As ε decreases (stronger privacy), does the variance of the Laplace noise increase or decrease? (Answer: Increase)

- **Concept:** Superlevel Sets
  - **Why needed:** The algorithm creates topological features by thresholding the potential field
  - **Quick check:** Does a higher energy threshold h result in a larger or smaller connected component in a superlevel set? (Answer: Smaller)

## Architecture Onboarding

- **Component map:** Client Worker (LDP noise → local K-Means → Centroids + Mass) → Server Aggregator (receives tuples → generates Synthetic Grid → computes Potential Field) → Topology Engine (thresholds → builds Merge Tree → extracts Persistent Leaves)

- **Critical path:** The translation of the Potential Field into the Merge Tree. If the grid resolution (controlled by multiplier α) is too low, the field is undersampled and topological features will be missed.

- **Design tradeoffs:**
  - Grid Resolution (α) vs. Compute: Higher α improves accuracy under strong privacy but increases server-side O(m) complexity
  - Smoothing (δ) vs. Granularity: High δ smooths noise but may merge close distinct clusters; low δ preserves separation but keeps noise spikes

- **Failure signatures:**
  - Fallback Activation: If logs show "Fallback: Add top energy leaves," topological method failed to find sufficient persistent clusters
  - NA Results: If client data is too small or ε too strict, local K-means may fail or centroids may drift outside bounds

- **First 3 experiments:**
  1. Sanity Check (ε=1000): Run with near-zero privacy to establish baseline
  2. Stress Test (ε=0.01): Run on MNIST with varying synthetic multipliers α ∈ {1, 2, 10}
  3. Ablation on Mass: Set all weights w_i=1 to check if ARI drops on datasets with high variance clusters

## Open Questions the Paper Calls Out

- Can the linear communication overhead be reduced to support ultra-large-scale systems (e.g., tens of thousands of clients) without compromising the one-shot nature of the protocol? [explicit]

- Can the framework's sensitivity to hyperparameters (δ, α, r) be eliminated through adaptive learning, particularly for high-dimensional, highly non-IID data? [explicit]

- Is it feasible to construct and analyze the gravitational potential field directly in the original high-dimensional space without relying on dimensionality reduction techniques like UMAP? [inferred]

## Limitations

- Communication overhead scales linearly with client count, creating bottlenecks at ultra-large scale
- Performance remains sensitive to hyperparameters (δ, α, r) especially with high-dimensional, highly non-IID data distributions
- Assumes clusters are well-separated and form distinct peaks in the potential field; may fail with overlapping or hierarchical structures

## Confidence

- **High Confidence:** The topological framework is mathematically sound and LDP mechanism is correctly implemented; 162.62% ARI improvement is well-supported
- **Medium Confidence:** Theoretical error bounds are valid under stated assumptions but practical applicability depends on unknown constants
- **Low Confidence:** Generalization to highly heterogeneous non-IID distributions with arbitrary cluster shapes is not thoroughly validated

## Next Checks

1. **Convergence Analysis:** Systematically vary ε from 0.01 to 1000 and measure ARI/NMI at each point; plot error vs. 1/ε to verify O(1/ε) scaling

2. **Grid Resolution Sensitivity:** Run MNIST experiments with fixed ε=0.01 but varying synthetic multipliers α ∈ {1, 2, 5, 10, 20}; confirm accuracy improves with higher α and identify diminishing returns

3. **Non-IID Stress Test:** Partition MNIST into highly skewed distributions (e.g., client 1 has only digits 0-2, client 2 has only digits 3-5); evaluate whether GFC maintains ARI performance or if compactness weighting introduces bias against sparse but globally important clusters