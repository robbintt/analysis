---
ver: rpa2
title: 'HiBBO: HiPPO-based Space Consistency for High-dimensional Bayesian Optimisation'
arxiv_id: '2510.08965'
source_url: https://arxiv.org/abs/2510.08965
tags:
- space
- latent
- optimisation
- data
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the distribution mismatch problem in VAE-based
  high-dimensional Bayesian Optimization (BO). While VAEs reduce dimensionality by
  learning latent representations, reconstruction-only loss fails to preserve kernel
  relationships between data points, leading to suboptimal surrogate models.
---

# HiBBO: HiPPO-based Space Consistency for High-dimensional Bayesian Optimisation

## Quick Facts
- arXiv ID: 2510.08965
- Source URL: https://arxiv.org/abs/2510.08965
- Reference count: 20
- Key outcome: HiBBO introduces HiPPO-based space consistency into VAE training for high-dimensional BO, achieving state-of-the-art performance across synthetic, image, shape, and molecular design tasks while maintaining low query budgets.

## Executive Summary
This paper addresses the distribution mismatch problem in VAE-based high-dimensional Bayesian Optimization (BO). While VAEs reduce dimensionality by learning latent representations, reconstruction-only loss fails to preserve kernel relationships between data points, leading to suboptimal surrogate models. The authors propose HiBBO, which introduces HiPPO-based space consistency into VAE training by adding a regularizer that preserves HiPPO memory representations of both original and reconstructed data sequences. This indirectly maintains kernel distances between data points. Experiments across four benchmarks (Ackley function, MNIST, shape optimization, and molecular design) demonstrate that HiBBO outperforms existing VAE-BO methods in convergence speed and solution quality, with the molecular design task achieving the highest logP score with the lowest query budget. The method is lightweight, general, and compatible with existing VAE architectures.

## Method Summary
HiBBO modifies standard VAE training by adding a HiPPO-based space consistency regularizer to the loss function. The VAE learns a latent representation while simultaneously maintaining HiPPO memory states for both original and reconstructed data sequences. The HiPPO memory (using HiPPO-LegS with dimension 50) captures polynomial moments of the data, and the regularizer minimizes the difference between original and reconstructed HiPPO states. This forces the VAE to preserve kernel distances between data points indirectly. The method integrates seamlessly into the BO loop: VAE training with HiPPO constraint → GP surrogate fitting in latent space → acquisition optimization → evaluation → VAE retraining every ν steps with updated HiPPO memories.

## Key Results
- Achieves the highest logP score in molecular design task with the lowest query budget among all tested methods
- Demonstrates faster convergence and better final regret than BASE, METRIC, GPrior, and REWEIGH baselines across all four benchmarks
- Successfully handles high-dimensional spaces (1000→10 dimensions for Ackley function)
- Maintains kernel relationships between original and latent space more effectively than standard reconstruction-only VAEs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining HiPPO memory representations of original and reconstructed data to be close indirectly preserves kernel distances between data points.
- Mechanism: HiPPO projects sequential data onto orthogonal polynomial bases (e.g., Legendre), producing coefficients that encode moments of the data distribution. Kernel distances (e.g., linear, polynomial) are expressible as functions of these moments. When HiPPO order ρ exceeds kernel degree p, closeness in HiPPO space mathematically implies closeness in kernel space.
- Core assumption: The chosen kernel can be approximated by a polynomial of degree ≤ ρ; sequential ordering of data is meaningful for the task.
- Evidence anchors:
  - [abstract] "...using HiPPO representations to preserve kernel distances between original and reconstructed data sequences."
  - [Section 3.2] Proposition 1 formalizes the implication; proof in Appendix A.2 derives the moment-kernel relationship.
  - [corpus] Weak direct evidence; neighbor papers focus on VAE-BO but not HiPPO-based kernel preservation.

### Mechanism 2
- Claim: Standard VAE reconstruction loss preserves mean distances (Δ_mean) but fails to preserve kernel relationships (Δ_kernel), causing surrogate model mismatch.
- Mechanism: Reconstruction loss ‖x − μ_θ(z)‖² minimizes per-point error but does not constrain pairwise relationships. The GP surrogate in latent space implies a GP in original space via decoder, with kernel discrepancy Δ_kernel = ‖k_z(μ_θ⁻¹(x), μ_θ⁻¹(x′)) − k_x(x, x′)‖ that reconstruction loss cannot directly reduce.
- Core assumption: The true kernel k_x is unknown and fitting GP in high-dimensional X is infeasible; indirect preservation via HiPPO is tractable.
- Evidence anchors:
  - [Section 3.1] Formal analysis of Δ_mean and Δ_kernel discrepancies.
  - [Section 1] "...reconstruction-based objective function often brings the functional distribution mismatch between the latent space and original space..."
  - [corpus] Latent Space COWBOYS paper discusses similar surrogate-generative coupling issues in VAE-BO.

### Mechanism 3
- Claim: HiPPO memory enables efficient online updates during BO iteration without re-storing full history.
- Mechanism: HiPPO state follows linear ODE dc/dt = A·c + B·x(t), allowing O(1) update per new observation given fixed state dimension ρ. This fits naturally into sequential BO where points are evaluated iteratively.
- Core assumption: Observations arrive sequentially; ODE discretization (bilinear method used) remains stable.
- Evidence anchors:
  - [Section 3.2] ODE formulation and update efficiency discussed.
  - [Algorithm 1] Shows incremental c_t update within VAE training loop.

## Foundational Learning

- **Gaussian Processes and Kernels**
  - Why needed here: The surrogate model is a GP; understanding kernel functions (RBF, polynomial) is essential to grasp what HiPPO preserves.
  - Quick check question: Can you explain how kernel k(x, x′) encodes similarity and why its distortion matters for acquisition functions?

- **Variational Autoencoders (VAEs)**
  - Why needed here: HiBBO modifies VAE training; understanding encoder μ_ϕ, decoder μ_θ, and ELBO (reconstruction + KL) is prerequisite.
  - Quick check question: What does the reconstruction term ‖x − μ_θ(z)‖² optimize, and what does it fail to capture about data geometry?

- **HiPPO Theory Basics**
  - Why needed here: Core novelty; understanding orthogonal polynomial projection and memory state c_t as moment encodings.
  - Quick check question: If HiPPO projects onto Legendre polynomials up to order ρ, what information is captured vs. lost about input sequence history?

## Architecture Onboarding

- **Component map:**
  - High-dimensional data → Encoder μ_ϕ → Latent space z
  - Latent space z → Decoder μ_θ → Reconstructed data x̂
  - HiPPO Module → Maintains memory states c_t (original) and c̄_t (reconstructed)
  - GP Surrogate → Fits g(z) in latent space using observations {(z_i, f(x_i))}
  - Acquisition Function → Selects next z* → decode to x* → evaluate
  - VAE Retraining Loop → Triggered every ν BO steps with augmented loss

- **Critical path:**
  1. Initial data X → train VAE with HiPPO constraint (Eq. 4)
  2. Encode X → Z; fit GP on {(z_i, y_i)}
  3. Optimize acquisition α(z) → z* → decode to x*
  4. Evaluate f(x*); append to X
  5. Every ν steps: retrain VAE with updated HiPPO memories
  6. Repeat until budget B exhausted

- **Design tradeoffs:**
  - **HiPPO order ρ:** Higher ρ captures more moments → better kernel preservation but increased compute and potential overfitting. Paper uses ρ=50.
  - **VAE update frequency ν:** Frequent updates (low ν) adapt to new data but cost more; infrequent updates risk stale latent space. Paper uses ν ∈ {5, 50} depending on task.
  - **Latent dimension d′:** Too low loses information; too high defeats dimensionality reduction purpose. Paper uses d′=10 for 1000-dim Ackley.

- **Failure signatures:**
  - **Convergence stagnation:** If HiPPO constraint is too weak (low λ weighting ‖c − c̄‖), kernel mismatch persists → GP over-smooths → acquisition misses promising regions.
  - **Invalid decodings:** In molecular/shape tasks, aggressive exploration in latent space may decode to invalid structures.
  - **ODE instability:** If bilinear discretization fails for rapidly varying sequences, c_t becomes uninformative.

- **First 3 experiments:**
  1. **Ackley-1000 (Table 2 setup):** Reproduce with d=1000, d′=10, ρ=50, ν=10. Compare HiBBO vs. BASE (no HiPPO) on convergence speed and final regret. Expect faster convergence with HiBBO.
  2. **Ablation on HiPPO order:** Run Ackley-1000 with ρ ∈ {5, 10, 25, 50, 100}. Plot final regret vs. ρ to verify Proposition 1 (higher ρ should improve kernel preservation up to saturation).
  3. **Kernel mismatch visualization:** Train VAE with/without HiPPO on MNIST. Compute Δ_kernel empirically by fitting GPs in original (784-dim) and latent space, measuring kernel matrix discrepancy on held-out data. HiBBO should show lower Δ_kernel.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of different HiPPO polynomial bases (e.g., Chebyshev, Laguerre) impact optimisation performance compared to the standard Legendre (LegS) basis used in this study?
- Basis in paper: [explicit] The authors state they "select HiPPO-LegS... and leave the study on the different polynomials to future work."
- Why unresolved: Different bases possess distinct approximation properties for various functional classes, potentially offering better efficiency or accuracy for specific optimisation landscapes.
- What evidence would resolve it: Comparative benchmarks on standard tasks (e.g., Ackley, Molecular Design) using various polynomial bases within the HiBBO framework.

### Open Question 2
- Question: Can the HiPPO-based space consistency constraint be successfully integrated into non-VAE generative models, such as diffusion models or normalizing flows?
- Basis in paper: [explicit] The conclusion suggests future work should explore "broader integration with other generative models."
- Why unresolved: While VAEs provide a clear latent space for this constraint, other architectures handle latent representations differently, and it is unclear if HiPPO constraints transfer effectively to those geometries.
- What evidence would resolve it: Adapting the HiBBO loss for a diffusion-based BO pipeline and evaluating if it maintains kernel relationships better than standard latent diffusion approaches.

### Open Question 3
- Question: How robust is HiBBO when the underlying objective function kernel cannot be easily approximated by a low-degree polynomial, potentially violating the condition that HiPPO order exceeds kernel degree?
- Basis in paper: [inferred] Proposition 1 assumes the "HiPPO order is larger than the degree of the kernel function," which may not hold for highly complex, non-polynomial black-box functions.
- Why unresolved: If the function is highly erratic (high-degree complexity), a fixed HiPPO order may fail to capture the necessary kernel distances, leading to a breakdown in space consistency.
- What evidence would resolve it: Experiments on functions with increasing frequency/complexity to identify the breaking point where the fixed-order HiPPO representation fails to improve upon baselines.

## Limitations

- **Data ordering sensitivity:** The HiPPO method requires sequential data ordering, but the paper doesn't specify how to order initial random samples or incoming BO samples when constructing the sequence.
- **Comparison scope:** Performance is only compared against VAE-based baselines, not against non-VAE dimensionality reduction methods like PCA or sparse GP approximations.
- **Ablation studies missing:** No experiments varying the HiPPO order parameter ρ to verify the theoretical claims about kernel preservation.

## Confidence

- **Kernel preservation claims:** Medium confidence - strong theoretical grounding in Proposition 1 but limited empirical validation of the kernel distance preservation hypothesis.
- **Molecular design results:** High confidence - state-of-the-art performance with clear improvement over baselines.
- **Efficiency claims:** High confidence - O(1) HiPPO updates follow directly from the linear ODE structure.

## Next Checks

1. **Kernel Distance Preservation Test**: On MNIST, measure Δ_kernel = ‖K_original - K_latent‖_F after VAE training with/without HiPPO constraint to directly validate the kernel preservation hypothesis.

2. **HiPPO Order Sensitivity**: Run Ackley-1000 with ρ ∈ {5, 10, 25, 50, 100} and plot final regret vs. ρ to verify Proposition 1's claim about higher orders improving kernel approximation.

3. **Batch vs Sequential BO**: Compare HiBBO's performance under sequential (one point at a time) versus batch BO (multiple points per iteration) to test the sequential assumption underlying the HiPPO update mechanism.