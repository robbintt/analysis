---
ver: rpa2
title: 'CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflows'
arxiv_id: '2510.18043'
source_url: https://arxiv.org/abs/2510.18043
tags:
- compression
- prompt
- compactprompt
- data
- n-gram
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CompactPrompt is an end-to-end pipeline that reduces Large Language
  Model inference costs by up to 60% through simultaneous prompt and file-level compression.
  It combines hard prompt pruning using self-information scoring, n-gram abbreviation
  for repetitive textual patterns, and uniform quantization for numerical data.
---

# CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflows

## Quick Facts
- **arXiv ID:** 2510.18043
- **Source URL:** https://arxiv.org/abs/2510.18043
- **Reference count:** 40
- **Key outcome:** Achieves up to 60% token reduction while preserving task accuracy within 5% on financial QA benchmarks

## Executive Summary
CompactPrompt is an end-to-end pipeline that reduces Large Language Model inference costs by up to 60% through simultaneous prompt and file-level compression. It combines hard prompt pruning using self-information scoring, n-gram abbreviation for repetitive textual patterns, and uniform quantization for numerical data. On benchmark datasets TAT-QA and FinQA, the approach preserves output quality with less than 5% accuracy drop for models like Claude-3.5-Sonnet and GPT-4.1-Mini. Token usage is cut by over half while maintaining or improving accuracy, with optimal results achieved at n-gram size 2 and Top-N 3. The method provides a lightweight, training-free solution for leaner generative AI pipelines in financial workflows.

## Method Summary
CompactPrompt is an end-to-end pipeline that reduces LLM inference costs through simultaneous prompt and file-level compression while maintaining task accuracy on financial QA benchmarks. The method combines three compression techniques: (1) hard prompt pruning using self-information scoring and dependency-based phrase grouping, (2) n-gram abbreviation that replaces frequent multi-word patterns with placeholders, and (3) numerical quantization using uniform integer encoding or k-means clustering. The pipeline is training-free and validated on TAT-QA and FinQA benchmark datasets, achieving 50-60% token reduction with less than 5% accuracy drop when using optimal settings (n-gram size 2, Top-N 3).

## Key Results
- **Token reduction:** 50-60% reduction achieved on TAT-QA and FinQA benchmarks
- **Accuracy preservation:** Less than 5% accuracy drop maintained across models (Claude-3.5-Sonnet, GPT-4.1-Mini)
- **Optimal configuration:** n=2 (bigrams), Top-N=3 yields best compression-to-accuracy ratio

## Why This Works (Mechanism)

### Mechanism 1: Self-Information Based Token Pruning
- Claim: Low-information tokens can be selectively removed from prompts without degrading downstream task performance, reducing token count while preserving semantic core.
- Mechanism: Tokens receive a combined importance score from two sources: (1) static self-information derived from corpus-level unigram frequencies—I(t) = -log₂ P(t)—and (2) dynamic self-information from a pretrained LLM's conditional probability P_model(t|c). When scores differ by >10%, dynamic scores take precedence; otherwise, the arithmetic mean is used. Dependency-based phrase grouping ensures grammatical units are pruned together rather than individual tokens in isolation.
- Core assumption: Token importance for downstream tasks correlates with information-theoretic surprisal; high-frequency tokens contribute less semantic content and are safer to remove. Assumption: Models can reconstruct implied meaning from pruned context.
- Evidence anchors:
  - [abstract]: "CompactPrompt first prunes low-information tokens from prompts using self-information scoring and dependency-based phrase grouping"
  - [section 3.1.3]: "When both methods produce similar scores, their average provides a stable estimate... when scores differ significantly, the dynamic method's ability to capture context-sensitive importance... is more reliable"
  - [corpus]: SelectiveContext [11] demonstrated 36% memory savings using self-information scoring with vicuna-13b, validating the pruning premise
- Break condition: Fails when domain-specific terminology has low corpus frequency but high task importance (incorrectly pruned as "low information"); breaks when critical reasoning tokens (numbers, logical operators, negations) are removed; degrades when phrase grouping fails to preserve syntactic dependencies.

### Mechanism 2: N-gram Abbreviation via Reversible Dictionary
- Claim: Repeated multi-word patterns in attached documents can be replaced with short placeholder tokens, achieving lossless compression that models can interpret in context.
- Mechanism: Extract all n-grams from document corpus, compute frequencies, select top-K most frequent patterns (typically K=100-150), assign unique human-readable placeholders (e.g., "ABC1", "BD2"), store mapping in metadata table. At inference, placeholders are substituted; reconstruction uses the reversible lookup table. Optimal settings: n=2 (bigrams), Top-N=3.
- Core assumption: Recurring phrases carry redundant information that abbreviates without semantic loss; LLMs can resolve placeholder meaning from surrounding context. Assumption: Placeholder tokens don't collide with domain vocabulary.
- Evidence anchors:
  - [abstract]: "applies n-gram abbreviation to recurrent textual patterns in attached documents"
  - [section 5.2.1]: "The largest improvement occurs at T=3, G=2 (Δ=+5.0)... Targeting only the three most frequent bi-grams concentrates compression on truly repetitive phrasing"
  - [corpus]: Weak corpus evidence—no directly comparable n-gram abbreviation methods found in neighbor papers; most prior work uses token-level filtering or sentence-level selection
- Break condition: Fails when T≥4 (too many abbreviations) causing placeholder density that disrupts model attention; breaks when abbreviations overlap or collide; degrades when substituted phrases carry task-critical semantic nuance that placeholders obscure.

### Mechanism 3: Combined Scoring with Adaptive Thresholding
- Claim: Integrating corpus-level statistics with runtime context probabilities through adaptive thresholding produces more robust token importance estimates than either method alone.
- Mechanism: Calculate relative difference Δ = |s_dyn - s_stat| / s_stat. If Δ ≤ 0.1, use mean of both scores; if Δ > 0.1, use dynamic score. This prevents over-favoring dynamic noise (when context is unusual) while capturing genuine context-sensitive importance that corpus statistics miss.
- Core assumption: A 10% relative difference threshold distinguishes noise from meaningful divergence; static and dynamic measures provide complementary signals. Assumption: The scoring LLM accurately reflects token importance for the target task.
- Evidence anchors:
  - [abstract]: "self-information scoring and dependency-based phrase grouping"
  - [section 3.1.3]: "Ten percent offers a pragmatic compromise... lower (e.g. 5%) or higher (e.g. 15%) cut-offs tilt too far toward one method"
  - [corpus]: LLMLingua [5] uses coarse-to-fine compression with budget controllers, suggesting multi-stage scoring is effective, though threshold values differ
- Break condition: Fails when scoring LLM has different token importance distribution than target task model; breaks when threshold is miscalibrated for domain (financial jargon may require different thresholds); degrades when both static and dynamic scores are misleading for task-specific tokens.

## Foundational Learning

- **Concept: Self-information and Shannon surprisal**
  - Why needed here: Core theoretical foundation for identifying which tokens to prune; I(t) = -log₂ P(t) quantifies information content
  - Quick check question: If "the" appears in 5% of corpus tokens and "arbitrage" appears in 0.001%, which has higher self-information?

- **Concept: N-gram frequency distributions and Zipf's law**
  - Why needed here: Understanding why top-K frequent patterns provide most compression gains; long-tail patterns offer diminishing returns
  - Quick check question: Why might bigrams (n=2) like "per share" compress better than 5-grams that appear only once?

- **Concept: Quantization error bounds and reconstruction**
  - Why needed here: Numerical compression requires understanding precision tradeoffs; ε_max = (max - min)/(L-1) defines worst-case error
  - Quick check question: With 8-bit quantization (L=256 levels) on values ranging from 0 to 1000, what's the maximum reconstruction error?

## Architecture Onboarding

- **Component map:**
  - **PromptCompressionEngine** (3.1): StaticFrequencyCalculator (offline corpus → unigram frequencies), DynamicScorerAgent (LLM API calls for P(t|c)), CombinedScorer (10% threshold logic), PhraseGrouper (dependency parsing via spaCy)
  - **NgramAbbreviator** (3.2): FrequencyAnalyzer (extract n-grams, histogram), DictionaryBuilder (assign placeholders), ContextualReplacer (resolve overlaps, preserve punctuation)
  - **NumericQuantizer** (3.3): UniformQuantizer (min/max/bit-width → integer codes), KMeansQuantizer (k centroids → cluster indices)
  - **ExemplarSelector** (3.4): Embedder (all-mpnet-base-v2), ClusterOptimizer (k∈5-50 with silhouette scoring), PrototypeExtractor (select centroid-nearest points)
  - **SemanticValidator** (3.5): CosineSimilarityChecker (E_orig vs E_comp), HumanReviewInterface (1-5 scale validation)
  - **UnifiedCompactPromptPipeline** (4.2): 8-stage orchestrator—initialization, token probability construction, hybrid scoring, compression engine, similarity analysis, metrics computation, result assembly, utilities

- **Critical path:**
  1. **Initialization**: Load static corpus frequencies (Wikipedia, ShareGPT, arXiv); instantiate target model
  2. **Token scoring** (parallel): Static lookup + Dynamic LLM query → Combined score via threshold logic
  3. **Phrase grouping**: Dependency parse → group tokens into prunable units
  4. **Document compression** (parallel): N-gram extraction → Top-K selection → Dictionary build → Placeholder replacement
  5. **Numerical compression** (if tables present): Column-wise range calculation → Quantization encoding
  6. **Assembly**: Concatenate compressed prompt + abbreviated documents + quantized tables
  7. **Validation**: Cosine similarity check (target ≥0.92); if fail, reduce compression aggressiveness
  8. **Output**: Compressed prompt string + metadata dictionary (for reconstruction) + compression metrics

- **Design tradeoffs:**
  - Compression ratio vs. accuracy: T=3/G=2 yields ~2.3× reduction with accuracy gains; T≥4 causes degradation (Figure 4 heatmap shows failure quadrant)
  - Static vs. dynamic scoring compute cost: Dynamic requires LLM API calls per token; static is O(1) lookup. Trade accuracy for speed by increasing threshold
  - Model selection for scoring: GPT-4-Omni, GPT-4.1-Mini, Claude-3.5-Sonnet, Llama-3.3-70B supported; Claude-3.5-Sonnet shows best robustness (Table 5-8)
  - Representative vs. random exemplars: Representative adds ~50ms clustering overhead but yields +5-10 accuracy points; random is free but introduces noise
  - Quantization bit-width: 4-bit (L=16) gives ~4× numerical compression but ε_max = range/15; 8-bit halves error but reduces compression

- **Failure signatures:**
  - **Accuracy drop >5%**: Check T value—if T≥4, reduce to 3; verify exemplar selection (use representative, not random); inspect placeholder density in output
  - **Numerical reasoning failures on FinQA**: Quantization error may exceed task precision; disable quantization or increase bit-width; check if min/max outliers distort range
  - **Semantic similarity <0.92**: Over-aggressive pruning; reduce pruning budget or increase phrase grouping; verify dynamic scorer is using correct context window
  - **Llama-3.3-70B specific degradation**: Model is "comparatively fragile" per section 5.2.1; use conservative settings (T≤3, G=2 only); avoid data compression + added context combination
  - **GPT-4-Omni mixed results**: "Tighter guardrails on T" needed; stick to T=2-3; avoid aggressive abbreviation even with high-frequency patterns
  - **Reconstruction failures**: Metadata table corrupted or incomplete; verify placeholder uniqueness; check for overlapping n-gram matches that weren't resolved

- **First 3 experiments:**
  1. **Baseline calibration on your domain**: Take 50 representative prompts from your production data; run CompactPrompt with default settings (T=3, G=2, 8-bit quantization); measure: (a) token reduction ratio, (b) cosine similarity, (c) task accuracy vs. uncompressed baseline. Target: ≥50% token reduction with <5% accuracy drop and cosine ≥0.92.
  2. **Hyperparameter grid search**: On held-out validation set (n=100), test T∈{2,3,4} × G∈{2,3,4} combinations; for each, record accuracy delta and compression ratio. Plot Pareto frontier; select configuration closest to (high compression, zero accuracy loss). Paper's optimum (T=3, G=2) may shift for your domain.
  3. **Ablation by component**: Run three variants—(a) prompt pruning only, (b) n-gram abbreviation only, (c) combined—to isolate which provides dominant gains for your use case. For financial QA, prompt pruning + abbreviation likely sufficient; quantization may add risk for precision-critical calculations. Log per-component token savings and accuracy impact separately.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive mechanism dynamically adjust compression budgets based on real-time task complexity or context length signals?
- Basis in paper: [explicit] The authors state in Future Work that future versions could "dynamically adjust the compression budget based on task complexity, context length, or real-time performance signals."
- Why unresolved: The current study relies on static, manually tuned hyperparameters (n-gram size 2, Top-N 3) that do not account for varying query difficulty or input redundancy levels.
- What evidence would resolve it: A study correlating automated complexity metrics (e.g., perplexity or query type) with optimal compression ratios to maintain accuracy thresholds.

### Open Question 2
- Question: Can the compression pipeline be modified to function as a privacy filter by prioritizing the removal of sensitive personally identifiable information (PII)?
- Basis in paper: [explicit] The authors propose "Privacy-aware compression," suggesting the system could assign higher weights to sensitive fields like social security numbers to limit inadvertent disclosure.
- Why unresolved: The current self-information scoring retains "high information" tokens, which often include unique identifiers (PII), potentially increasing privacy risk rather than mitigating it.
- What evidence would resolve it: Experiments measuring PII detection and removal rates within the compressed prompt compared to baseline privacy scrubbing techniques.

### Open Question 3
- Question: Is the observed performance degradation in open-weights models (e.g., Llama-3.3) caused by tokenizer vocabulary mismatches with placeholder tokens?
- Basis in paper: [inferred] The results show Llama-3.3-70B is "comparatively fragile" and degrades rapidly under compression, whereas proprietary models like Claude 3.5 Sonnet improve.
- Why unresolved: The paper observes the performance gap but does not isolate whether the drop is due to the model's reasoning architecture or its inability to semantically interpret the inserted abbreviation placeholders.
- What evidence would resolve it: An ablation study using open-weights models with varying tokenizer vocabularies or fine-tuning on compressed prompt formats.

### Open Question 4
- Question: How can CompactPrompt be extended to support multimodal financial data, such as charts, PDF layouts, or time-series feeds?
- Basis in paper: [explicit] The authors list "Beyond text and tables" as future work, noting that extending compression to these formats would broaden applicability to real-world financial analysis.
- Why unresolved: The current methodology relies on text-specific techniques (n-gram abbreviation) and tabular quantization, which do not translate directly to visual or continuous time-series data structures.
- What evidence would resolve it: A modified pipeline processing image-based charts or time-series data, evaluated on downstream visual QA tasks.

## Limitations

- **Domain transfer uncertainty**: All validation occurs on financial QA tasks (TAT-QA and FinQA); generalizability to other domains remains untested
- **Model-specific fragility**: Some models (Llama-3.3-70B) show rapid degradation with compression settings, while others (Claude 3.5 Sonnet) improve
- **Numerical precision limits**: Quantization provides compression but may introduce errors for precision-critical financial calculations

## Confidence

- **High confidence**: The n-gram abbreviation mechanism works as described, with clear frequency analysis and dictionary-based substitution; token reduction ratios of 50-60% are achievable on benchmark datasets; semantic similarity preservation above 0.92 threshold is consistently demonstrated
- **Medium confidence**: The 5% accuracy drop threshold as acceptable limit; model-specific sensitivity patterns (Llama fragility, GPT-4-Omni variability); optimal hyperparameters (T=3, G=2) generalize beyond tested datasets
- **Low confidence**: Generalizability to non-financial domains; numerical precision degradation bounds for different bit-widths; long-term model stability with compressed prompts (no temporal analysis provided)

## Next Checks

**Check 1: Cross-domain compression validation**
Apply CompactPrompt to three non-financial domains (medical QA, legal document analysis, and creative writing) using the same benchmark methodology. Measure: (a) token reduction achieved, (b) accuracy delta vs. uncompressed baseline, (c) semantic similarity scores. Compare results against financial benchmarks to quantify domain transfer limits.

**Check 2: Numerical precision stress test**
Create a synthetic financial calculation benchmark with controlled precision requirements (e.g., percentage calculations requiring 2-4 decimal places). Apply varying quantization bit-widths (4-bit through 16-bit) and measure: (a) reconstruction error distribution, (b) calculation accuracy vs. ground truth, (c) task completion rate. Identify the precision threshold where numerical errors dominate task performance.

**Check 3: Model architecture sensitivity mapping**
Test CompactPrompt across a broader model spectrum (small: 7B, medium: 34B, large: 70B+ parameters) and architectures (decoder-only, encoder-decoder, mixture-of-experts). For each combination, systematically vary T∈{2,3,4,5} and G∈{2,3,4,5} to map sensitivity surfaces. Identify architectural patterns that predict compression tolerance, enabling informed model selection for compressed workflows.