---
ver: rpa2
title: 'Agile Deliberation: Concept Deliberation for Subjective Visual Classification'
arxiv_id: '2512.10821'
source_url: https://arxiv.org/abs/2512.10821
tags:
- concept
- image
- images
- should
- deliberation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Agile Deliberation is a human-in-the-loop framework for visual\
  \ classification that supports evolving, subjective concepts. It helps users define\
  \ their concept through two stages: concept scoping, which decomposes the initial\
  \ concept into a hierarchy of sub-concepts, and concept iteration, which surfaces\
  \ semantically borderline examples for user reflection and feedback to iteratively\
  \ align an image classifier with the user\u2019s evolving intent."
---

# Agile Deliberation: Concept Deliberation for Subjective Visual Classification

## Quick Facts
- arXiv ID: 2512.10821
- Source URL: https://arxiv.org/abs/2512.10821
- Reference count: 40
- 7.5% higher F1 scores than automated decomposition baselines

## Executive Summary
Agile Deliberation is a human-in-the-loop framework for subjective visual classification that supports evolving, user-defined concepts. The system operates in two stages: concept scoping decomposes an initial concept into a hierarchy of sub-concepts, while concept iteration surfaces semantically borderline examples for user reflection and feedback. Through 18 live user sessions, the framework achieved 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.

## Method Summary
The framework uses Gemini-Pro 2.5 for concept decomposition and Gemini-Flash 2.5 for classification. It consists of three main components: a decomposition module that breaks down concepts into sub-concepts through prompt-chained reasoning, a borderline image retrieval system that uses dictionary learning clustering and UCB multi-armed bandit selection to surface ambiguous examples, and an automatic prompt optimization system that refines concept definitions based on user feedback. The system generates M=5 candidate refinements, filters them on current batches, and selects the best performing one on the full labeled set.

## Key Results
- 7.5% higher F1 scores compared to automated decomposition baselines
- More than 3% higher F1 than manual deliberation approaches
- 10.5% improvement over zero-shot classification methods
- Participants reported clearer conceptual understanding and lower cognitive effort

## Why This Works (Mechanism)
The framework works by iteratively aligning the classifier with user intent through active learning. During concept scoping, the system decomposes vague initial concepts into manageable sub-concepts, making the classification task more tractable. In concept iteration, it surfaces images that are ambiguous according to the current classifier but potentially clear to humans, creating opportunities for users to refine their conceptual boundaries. The automatic prompt optimization ensures that each refinement is grounded in actual user feedback rather than theoretical improvements.

## Foundational Learning
- **Concept decomposition**: Breaking complex concepts into sub-concepts makes subjective classification more manageable and allows for more precise user feedback. Quick check: Verify decomposition produces coherent, non-overlapping sub-concepts.
- **Borderline mining**: Identifying images that are ambiguous to the classifier but potentially clear to humans creates the most valuable learning opportunities. Quick check: Test whether surfaced images have high VLM confidence but are perceived as ambiguous by users.
- **Active learning feedback loops**: Iterative refinement based on user feedback gradually aligns the classifier with the user's evolving intent. Quick check: Monitor F1 score improvements across iteration rounds.
- **Prompt optimization**: Automatically generating and selecting better prompts based on performance data reduces the cognitive load on users. Quick check: Compare user-defined vs. optimized prompts on test accuracy.

## Architecture Onboarding

Component Map: Concept Input -> Decomposition Module -> Borderline Retrieval -> User Feedback -> Prompt Optimization -> Updated Classifier

Critical Path: Concept scoping (decomposition) -> Borderline image retrieval (clustering + bandit selection) -> User feedback collection -> Concept refinement (prompt optimization)

Design Tradeoffs: The system trades computational complexity for reduced human effort by automating prompt optimization, but requires careful balancing to avoid definition drift across iterations.

Failure Signatures: Performance drops on cumulative labeled data indicate definition drift; consistently high VLM confidence on surfaced images suggests the retrieval is targeting classifier uncertainty rather than human conceptual ambiguity.

First Experiments:
1. Test decomposition module with simple concepts to verify sub-concept coherence
2. Validate borderline retrieval by checking if surfaced images are truly ambiguous to humans
3. Implement basic prompt optimization and test on small labeled datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Unspecified vision encoder architecture for clustering embeddings (CLIP variant and dimensionality not provided)
- Missing dictionary learning implementation details including number of basis vectors and optimization algorithm
- Incomplete borderline score computation weights with undefined exploration_value formula
- Human-in-the-loop evaluation makes exact replication of participant responses impossible

## Confidence

High Confidence: Core two-stage framework clearly specified with 7.5% F1 improvement from controlled user studies

Medium Confidence: Methodology for concept refinement via prompt optimization well-described, though some implementation details remain unclear

Low Confidence: Specific implementation of multi-armed bandit cluster selection and ambiguity mining cannot be precisely reproduced without additional technical details

## Next Checks

1. Validate VLM confidence alignment by testing whether surfaced borderline images show high VLM confidence on human-ambiguous cases, indicating need to adjust query generation

2. Monitor definition drift by tracking per-round F1 scores on cumulative labeled data to ensure prompt optimization maintains global consistency

3. Benchmark against stated baselines by implementing and testing zero-shot, automated decomposition, and manual deliberation approaches to verify claimed 10.5%, 7.5%, and 3% F1 improvements respectively