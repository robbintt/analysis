---
ver: rpa2
title: Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering
  Management
arxiv_id: '2601.14069'
source_url: https://arxiv.org/abs/2601.14069
tags:
- learning
- video
- task
- cluster
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes uVCIL, an approach for unsupervised video class-incremental
  learning (uVCIL) that enables deep models to continually learn from unlabeled video
  streams without forgetting previously acquired knowledge. The method addresses the
  challenge of catastrophic forgetting by progressively building and managing a system
  of clusters consisting of representative features, while also tackling cluster imbalance
  and preserving knowledge during learning.
---

# Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management

## Quick Facts
- arXiv ID: 2601.14069
- Source URL: https://arxiv.org/abs/2601.14069
- Reference count: 0
- Primary result: uVCIL achieves higher average cluster accuracy while maintaining lower backward and forward forgetting compared to baseline methods on UCF101, HMDB51, and SSv2 datasets

## Executive Summary
The paper proposes uVCIL, an unsupervised video class-incremental learning approach that enables deep models to learn from unlabeled video streams without forgetting previously acquired knowledge. The method addresses catastrophic forgetting by progressively building and managing clusters of representative features while maintaining a frozen feature extractor. By using deep embedded clustering and a Radial Basis Functions (RBF) network for pseudo-label assignment, along with memory replay techniques, uVCIL significantly outperforms baseline methods while using only 52K trainable parameters compared to 21.3M for fine-tuning approaches.

## Method Summary
uVCIL operates by freezing a pre-trained feature extractor (ResNet-34 or VideoMAE) and using it to extract consistent 512-dimensional features from video frames. For each new task, k-means clustering is performed on the extracted features to create pseudo-labels, with historical cluster centers frozen to maintain stability. The system maintains a memory buffer storing 20 feature vectors per cluster for replay. An RBF network serves as the classifier, which is updated using Focal Loss with cluster balancing. New clusters are initialized for each task, allowing the system to distinguish novel classes while preserving knowledge of previous ones through latent replay of stored features.

## Key Results
- Achieves significantly higher average cluster accuracy (ACAcc) than baseline methods across UCF101, HMDB51, and SSv2 datasets
- Maintains lower backward forgetting (BWF↑) and forward forgetting (FWF↓) metrics compared to baselines
- Uses only ~52K trainable parameters versus 21.3M for fine-tuning baseline approaches
- Demonstrates computational efficiency through feature replay instead of raw data storage

## Why This Works (Mechanism)

### Mechanism 1: Representation Anchoring via Frozen Encoders
Maintaining a static feature space prevents representation drift of old clusters, ensuring reliable pseudo-label assignment over time. The frozen pre-trained encoder provides consistent distance metrics across all tasks, assuming sufficient transferability from pre-training data.

### Mechanism 2: Latent Replay for Backward Transfer
Storing and replaying feature vectors (not raw pixels) allows lightweight classifier rehearsal without computational cost of backpropagating through the deep backbone. This explicitly preserves decision boundaries for old clusters while maintaining efficiency.

### Mechanism 3: Incremental Cluster Expansion and Pseudo-Labeling
Freezing historical cluster centers while initializing new ones enables the system to distinguish novel classes from old ones without supervision. Pseudo-labels are assigned based on minimum distance to existing or new cluster centers.

## Foundational Learning

- **Catastrophic Forgetting**: The central problem uVCIL solves - without specific mechanisms, optimizing for new video tasks overwrites parameters needed for old tasks. Quick check: Can you explain why updating deep network weights on Task B usually destroys performance on Task A?

- **Pseudo-Labeling**: Since the approach is unsupervised, the model must generate its own supervisory signal from clustering structure to train the classifier. Quick check: How does the system assign a "class" to an unlabeled video frame sequence?

- **Radial Basis Function (RBF) Networks**: The paper uses an RBF layer for classification. Understanding that RBFs classify based on distance to a center (prototype) is key to understanding why clustering drives the learning. Quick check: How does an RBF network make a classification decision differently than a standard Linear layer?

## Architecture Onboarding

- **Component map**: Input Video Frames -> Frozen ResNet-34/VideoMAE -> 512-dim Feature Vector -> Cluster Manager -> RBF Network (Classifier) -> Cluster Scores
- **Critical path**: Extract features using frozen backbone → Calculate distances to all existing cluster centers → Assign pseudo-labels → Sample feature vectors from Memory Buffer → Train RBF classifier using new batch + memory samples with Focal Loss
- **Design tradeoffs**: Efficiency vs. Adaptability (52K trainable parameters vs 21.3M for baselines), Memory vs. Accuracy (features only vs raw data)
- **Failure signatures**: Cluster Collapse (accuracy drops when new tasks forced into old clusters), Stale Features (noisy clusters from weak backbone)
- **First 3 experiments**: 1) Frozen Encoder Sanity Check: Pass UCF101 through frozen ResNet-34 and run simple k-means, verify clusters visually correspond to distinct actions using t-SNE. 2) Memory Ablation: Run pipeline on 5 tasks with Memory Replay enabled vs. disabled, plot "Backward Forgetting" metric. 3) Latent vs. Raw Replay: Compare storing raw video pixels vs. latent vectors, confirm latent replay achieves "computational efficiency."

## Open Questions the Paper Calls Out
- How can the feature representation mechanism be improved to effectively mitigate domain shift in unsupervised video class-incremental learning?
- How can the model dynamically determine the optimal number of clusters per task without relying on a fixed, pre-defined hyperparameter?
- What are the trade-offs in plasticity versus stability if the frozen feature extractor were replaced with an adaptive one?

## Limitations
- Heavy reliance on frozen feature extractor assumption may not generalize to significantly different video domains
- Number of clusters per task is a fixed hyperparameter without systematic exploration
- Evaluation methodology uses ground truth labels only for test-time evaluation via Hungarian matching, which may not reflect true practical deployment scenarios

## Confidence
- **High Confidence**: Efficiency claims (52K trainable parameters vs 21.3M baseline) are well-supported by methodology
- **Medium Confidence**: Experimental results showing superior performance on three datasets are convincing but depend on specific experimental setup with some unspecified details
- **Low Confidence**: Generalizability claim that this approach works across different video domains without adaptation is not fully validated

## Next Checks
1. Test the frozen ResNet-34 backbone on a video dataset from a different domain (e.g., medical imaging or satellite video) to validate the transferability assumption
2. Systematically vary the number of clusters per task (l_k) across multiple values to determine how sensitive the method is to this hyperparameter
3. Implement a version where no ground truth labels are available even at test time, requiring the system to output consistent pseudo-labels that can be tracked across time without Hungarian matching