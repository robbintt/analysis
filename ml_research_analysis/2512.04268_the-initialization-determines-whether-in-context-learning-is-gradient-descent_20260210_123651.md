---
ver: rpa2
title: The Initialization Determines Whether In-Context Learning Is Gradient Descent
arxiv_id: '2512.04268'
source_url: https://arxiv.org/abs/2512.04268
tags:
- learning
- linear
- multi-head
- page
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the conditions under which linear self-attention
  (LSA) approximates gradient descent (GD) in in-context learning (ICL). The authors
  identify a key limitation: when regression weights have a non-zero mean, multi-head
  LSA cannot replicate one-step GD even with many heads.'
---

# The Initialization Determines Whether In-Context Learning Is Gradient Descent

## Quick Facts
- arXiv ID: 2512.04268
- Source URL: https://arxiv.org/abs/2512.04268
- Reference count: 40
- Key outcome: The initial guess for the query embedding (yq) is the decisive factor determining whether linear self-attention approximates gradient descent in in-context learning; when regression weights have a non-zero mean, multi-head LSA cannot replicate one-step GD unless yq is properly initialized.

## Executive Summary
This paper investigates the conditions under which linear self-attention (LSA) approximates gradient descent (GD) in in-context learning (ICL). The authors identify a key limitation: when regression weights have a non-zero mean, multi-head LSA cannot replicate one-step GD even with many heads. They show that the initial guess for the query embedding (yq) is the decisive factor, and misalignment induces a persistent performance gap. To address this, they propose yq-LSA, an extension of LSA with a trainable initialization vector. Both theoretical analysis and experiments on linear regression tasks demonstrate that yq-LSA restores equivalence with GD in the non-zero mean setting. Finally, the authors show that incorporating initial guesses improves ICL performance in large language models on a semantic similarity task, linking theory with practical prompting strategies.

## Method Summary
The authors analyze the relationship between linear self-attention and gradient descent in in-context learning by deriving conditions for their equivalence. They prove that when regression weights have zero mean, multi-head LSA can replicate one-step GD regardless of the number of heads. However, when regression weights have a non-zero mean, LSA fails to approximate GD unless the initial query embedding is properly chosen. To overcome this limitation, they propose yq-LSA, which augments LSA with a trainable initialization vector yq. Theoretical derivations are complemented by experiments on linear regression tasks and semantic similarity benchmarks in large language models.

## Key Results
- Multi-head LSA can replicate one-step GD when regression weights have zero mean, but fails when regression weights have non-zero mean.
- The initial guess for the query embedding (yq) is the decisive factor determining whether LSA approximates GD in ICL.
- The proposed yq-LSA extension restores equivalence with GD in the non-zero mean setting and improves ICL performance in large language models.

## Why This Works (Mechanism)
The paper shows that the equivalence between linear self-attention and gradient descent in in-context learning depends critically on the initialization of the query embedding. When regression weights have a non-zero mean, the standard LSA mechanism introduces a persistent bias that prevents it from replicating one-step GD. By introducing a trainable initialization vector yq, yq-LSA corrects this bias and restores the theoretical equivalence. This mechanism bridges the gap between the idealized theoretical model and practical implementations of ICL.

## Foundational Learning
- **Linear Self-Attention (LSA)**: A simplified attention mechanism that approximates full attention using linear operations. Why needed: LSA provides a tractable framework for analyzing ICL theoretically. Quick check: Verify that LSA reduces to standard attention in the limit of infinite dimensions.
- **Gradient Descent (GD)**: An optimization algorithm that iteratively updates model parameters to minimize a loss function. Why needed: GD serves as the theoretical gold standard for comparing ICL mechanisms. Quick check: Confirm that one-step GD updates are analytically tractable for linear regression.
- **In-Context Learning (ICL)**: A paradigm where models learn from examples provided in the input context without parameter updates. Why needed: ICL is the primary focus of the paper's analysis and experiments. Quick check: Ensure ICL examples are representative of the target task distribution.
- **Regression Weights**: Parameters that map input features to outputs in linear models. Why needed: The mean of regression weights determines whether LSA can approximate GD. Quick check: Compute the mean of regression weights in synthetic data generation.
- **Query Embedding**: The initial representation used in attention mechanisms to compute attention scores. Why needed: The initialization of the query embedding determines whether LSA approximates GD. Quick check: Verify that query embeddings are properly initialized in experiments.

## Architecture Onboarding

### Component Map
Linear Self-Attention (LSA) -> Multi-Head LSA -> yq-LSA

### Critical Path
1. Input examples and query are embedded using shared parameters
2. LSA computes attention scores and produces output predictions
3. Multi-head LSA aggregates outputs from multiple attention heads
4. yq-LSA incorporates trainable initialization vector yq to correct bias

### Design Tradeoffs
The paper trades theoretical simplicity for practical applicability. While standard LSA provides clean theoretical guarantees for zero-mean regression weights, it fails in practical settings with non-zero means. yq-LSA addresses this limitation by introducing a trainable parameter, at the cost of increased model complexity and potential overfitting risks.

### Failure Signatures
- Persistent performance gap between LSA and GD when regression weights have non-zero mean
- Sensitivity to initialization choices in standard LSA
- Suboptimal ICL performance in large language models without proper query initialization

### First Experiments to Run
1. Compare LSA, multi-head LSA, and yq-LSA on linear regression tasks with varying regression weight means
2. Evaluate yq-LSA's performance on semantic similarity tasks in large language models
3. Conduct ablation studies on the choice of initialization vector yq in yq-LSA

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical claims are strongly supported for linear regression with squared loss but may not generalize to more complex models and loss functions
- The proposed yq-LSA shows modest performance gains in semantic similarity tasks, and its effectiveness may depend on specific prompting strategies
- The analysis assumes fixed context length and does not explore how initialization impacts longer sequences or more complex data distributions

## Confidence
- Generalizability to non-linear models and alternative loss functions: Medium confidence
- Performance gains of yq-LSA in practical ICL tasks: Medium confidence
- Applicability to diverse ICL benchmarks beyond semantic similarity: Medium confidence

## Next Checks
1. Test yq-LSA's effectiveness on non-linear regression tasks and alternative loss functions (e.g., logistic regression with cross-entropy loss) to assess generalizability beyond squared loss.
2. Evaluate the performance of yq-LSA in few-shot learning scenarios with varying context lengths to understand how initialization interacts with sequence length and data complexity.
3. Conduct ablation studies on the choice of initialization vector yq in large language models across multiple ICL benchmarks (e.g., natural language inference, question answering) to determine its robustness and impact on downstream task performance.