---
ver: rpa2
title: 'LILAC: Long-sequence Incremental Low-latency Arbitrary Motion Stylization
  via Streaming VAE-Diffusion with Causal Decoding'
arxiv_id: '2510.15392'
source_url: https://arxiv.org/abs/2510.15392
tags:
- motion
- streaming
- diffusion
- latent
- stylization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LILAC, a streaming VAE-diffusion framework
  that enables long-sequence, real-time motion stylization without requiring future
  frames. The key innovation is a sliding-window causal design with a re-decoding/encoding
  mechanism that injects previously generated motion features into the latent representation,
  ensuring temporal consistency.
---

# LILAC: Long-sequence Incremental Low-latency Arbitrary Motion Stylization via Streaming VAE-Diffusion with Causal Decoding

## Quick Facts
- arXiv ID: 2510.15392
- Source URL: https://arxiv.org/abs/2510.15392
- Authors: Peng Ren; Hai Yang
- Reference count: 19
- The paper introduces LILAC, a streaming VAE-diffusion framework that enables long-sequence, real-time motion stylization without requiring future frames.

## Executive Summary
LILAC addresses the challenge of real-time motion stylization by extending an offline VAE-diffusion framework to a streaming setting. The key innovation is a sliding-window causal design with a re-decoding/encoding mechanism that injects previously generated motion features into the latent representation, ensuring temporal consistency. By preserving user-intended trajectories while stylizing local dynamics, LILAC achieves smooth transitions between styles in real time while maintaining comparable quality to offline models.

## Method Summary
LILAC extends a pretrained VAE-diffusion arbitrary stylization model to an online streaming framework. The method processes motion in overlapping windows (L=60, stride Δ=4), stylizes each window using a diffusion model conditioned on content, trajectory, and style embeddings, then employs a causal decoder to generate final outputs. A re-decoding/encoding mechanism blends recently generated motion features back into the latent buffer via exponential filtering, while a hard trajectory copy constraint preserves user-intended paths. The system achieves real-time performance through a sliding-window architecture with causal attention constraints.

## Key Results
- Processes 4-frame updates at 20+ FPS on RTX 4090 GPU
- Achieves lower total jitter (0.0139) compared to naive streaming baseline (0.0245)
- Maintains competitive stylization quality (FMD 31.69 vs offline 27.69)
- Successfully demonstrates continuous generation with smooth style transitions

## Why This Works (Mechanism)

### Mechanism 1: Re-decoding/Encoding Feedback Loop
Injecting previously generated motion features back into the latent representation reduces temporal jitter in streaming generation. After diffusion generates stylized latents, the system decodes them to motion space, concatenates with prior outputs, then re-encodes the recent segment. This encoded representation is blended with the latent buffer via exponential filtering (α=0.8), ensuring new outputs remain anchored to prior context.

### Mechanism 2: Causal Decoder Constraint
A decoder trained with causal masking enforces one-directional temporal dependencies, preventing the model from relying on future frames. The causal decoder Ψ only attends to historical latents in the buffer, unlike standard decoders that see full windows. This forces the model to predict motion conditioned solely on past context, enabling true online generation.

### Mechanism 3: Trajectory Preservation via Direct Copy
Copying input trajectories directly to output preserves user-intended paths while allowing stylization of local dynamics. Motion is decomposed into trajectory τ(x_t) and local features c(x_t). Only c(X) is stylized; τ(Y) is set equal to τ(X) via hard constraint, ensuring that user-intended paths remain unchanged.

## Foundational Learning

- **Concept: VAE latent space for motion**
  - Why needed here: The entire pipeline operates in latent space; understanding how VAEs compress motion sequences is prerequisite to grasping the re-encoding mechanism.
  - Quick check question: Can you explain why encoding → stylizing → decoding in latent space is more efficient than operating directly on joint coordinates?

- **Concept: Diffusion model conditioning**
  - Why needed here: The diffusion model Φ conditions on content, trajectory, and style embeddings; understanding multi-condition generation is essential for extending or modifying the stylization logic.
  - Quick check question: How does classifier-free guidance differ from explicit conditioning, and which does this paper appear to use?

- **Concept: Causal vs. bidirectional attention**
  - Why needed here: The causal decoder's design differs from standard transformers; understanding masking strategies clarifies why ablation shows degraded performance without causality.
  - Quick check question: In a transformer decoder, what happens if you remove the causal mask during inference?

## Architecture Onboarding

- **Component map**: Input window → Encoder E → Diffusion Φ → Standard decoder D → concatenate with Y_prev → re-encode recent M frames → blend into latent buffer Z → Causal decoder Ψ → output Δ new frames
- **Critical path**: Input window → E → Φ (stylize) → D (decode) → concatenate with Y_prev → re-encode recent M frames → blend into Z → Ψ (causal decode) → output Δ new frames
- **Design tradeoffs**:
  - Window size L=60 vs. stride Δ=4: Larger windows give more context but increase latency; small stride enables smoother transitions but more compute
  - Re-encoding length M=30: Longer segments capture more history but increase encoder overhead
  - α=0.8 blending: Higher α prioritizes new latents (responsive to style changes), lower α smooths more (better consistency)
- **Failure signatures**:
  - High jitter (>0.02): Likely issue with causal decoder training or buffer blending
  - Discontinuities at window boundaries: Re-encoding/encoding branch may be disabled or α too high
  - Style not transferring: Check style embedding injection in diffusion conditioning
  - Drift over long sequences: Potential VAE reconstruction error accumulation
- **First 3 experiments**:
  1. Reproduce ablation without re-encoding branch (expect FMD ~67.52, jitter ~0.0157) to validate your pipeline matches paper.
  2. Vary α (0.5, 0.8, 0.95) and measure jitter vs. style responsiveness tradeoff on a 500-frame sequence.
  3. Test causal decoder with window-internal future access (non-causal ablation) to confirm jitter increases as reported (0.0176).

## Open Questions the Paper Calls Out

### Open Question 1
Can a streaming encoder that incrementally encodes incoming motion into the latent space reduce or eliminate the need for overlapping windows in the current sliding-window architecture? The conclusion states this will be investigated, as the current architecture processes overlapping windows (L=60, stride Δ=4), introducing computational redundancy.

### Open Question 2
Can conditioning the diffusion model on previously generated latents (rather than only conditioning the causal decoder) further improve temporal consistency across segment boundaries? The conclusion states this is a future aim, as currently only the causal decoder receives historical context via the latent buffer Z.

### Open Question 3
Can efficient sampling strategies such as consistency models reduce the diffusion bottleneck while preserving stylization quality? The conclusion identifies this as promising direction since diffusion remains the main computational bottleneck, noting MotionPCM [9] as an example.

### Open Question 4
What architectural modifications could close the remaining performance gap between the online streaming framework and the offline upper bound? The paper notes that the online method (FMD 31.69, CRA 0.30) underperforms the offline reference (FMD 27.69, CRA 0.36), suggesting the streaming constraint imposes a quality penalty.

## Limitations

- The paper does not validate the re-encoding mechanism's effectiveness over very long sequences (>1000 frames), raising concerns about potential VAE reconstruction error accumulation
- Style transition smoothness relies on the trajectory preservation assumption, which may not hold for all dance styles requiring path modification
- The causal decoder architecture specifics (masking pattern, attention layers) are not fully detailed, limiting precise reproduction

## Confidence

- **High Confidence**: The streaming framework's ability to process 4-frame updates at 20+ FPS on RTX 4090 (directly measurable from implementation)
- **Medium Confidence**: Total jitter reduction claims (0.0139 vs. 0.0245) given the ablation comparison is internal but lacks cross-dataset validation
- **Medium Confidence**: Style transition smoothness relies on trajectory preservation assumption that may not hold for all dance styles requiring path modification

## Next Checks

1. Implement and test the re-encoding mechanism over extended sequences (>1000 frames) to measure temporal consistency degradation
2. Create a quantitative benchmark comparing LILAC's causal decoding with a bidirectional variant on motion anticipation tasks
3. Test the framework with style embeddings from multiple CLIP variants to verify robustness to different style representations