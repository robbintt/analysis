---
ver: rpa2
title: Preserving AUC Fairness in Learning with Noisy Protected Groups
arxiv_id: '2505.18532'
source_url: https://arxiv.org/abs/2505.18532
tags:
- fairness
- noisy
- protected
- groups
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of preserving AUC fairness when
  protected group labels are noisy, a common issue in real-world datasets like surveys
  or AI-generated media. Existing methods fail under such conditions, leading to fairness
  violations.
---

# Preserving AUC Fairness in Learning with Noisy Protected Groups

## Quick Facts
- arXiv ID: 2505.18532
- Source URL: https://arxiv.org/abs/2505.18532
- Authors: Mingyang Wu; Li Lin; Wenbin Zhang; Xin Wang; Zhenhuan Yang; Shu Hu
- Reference count: 40
- Primary result: First robust AUC fairness method for noisy protected group labels using DRO with TV bounds, achieving 0.0316 vs 0.0766 fairness violation on AUCMax

## Executive Summary
This paper addresses the critical challenge of preserving AUC fairness when protected group labels are noisy, a common issue in real-world datasets. The authors propose a distributionally robust optimization (DRO) framework that bounds the total variation distance between clean and noisy group distributions, providing theoretical guarantees for fairness preservation. By estimating noise levels using pre-trained multi-modal foundation models like CLIP and employing an efficient stochastic gradient descent-ascent (SGDA) algorithm with Sharpness-Aware Minimization (SAM), the method achieves significantly lower fairness violations while maintaining high AUC performance across both tabular and image datasets.

## Method Summary
The proposed method uses distributionally robust optimization to handle noisy protected group labels by constructing an uncertainty set around the noisy group distribution using Total Variation (TV) distance. An SGDA algorithm jointly optimizes AUC fairness and generalization, where model parameters are minimized while Lagrangian multipliers and worst-case distribution weights are maximized. The approach estimates noise ratios using CLIP's cosine similarity between image embeddings and text prompts describing group membership, and incorporates SAM perturbation to improve out-of-distribution generalization through flatter loss landscapes.

## Key Results
- Achieves significantly lower fairness violations compared to state-of-the-art methods (0.0316 vs 0.0766 for AUCMax)
- Maintains higher Min/Max fairness scores across varying noise levels on both tabular and image datasets
- Theoretical guarantee: fairness constraints satisfied on clean groups within slack bounded by TV distance
- CLIP-based noise estimation successfully identifies noise ratios without task-specific training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributionally robust optimization bounds fairness violation on clean groups when trained with noisy group labels.
- Mechanism: The method constructs an uncertainty set around the noisy group distribution using Total Variation (TV) distance. By enforcing fairness constraints over the worst-case distribution within this set (Eq. 8), the solution is guaranteed to satisfy fairness on the true clean distribution within a slack bounded by the TV distance.
- Core assumption: The prior pairwise clean group probability is unaffected by noise (i.e., Pr[(Z, Z')] = Pr[(bZ, bZ')]).
- Evidence anchors:
  - [abstract]: "propose the first robust AUC fairness approach using distributionally robust optimization (DRO), which bounds the total variation (TV) distance between clean and noisy group distributions"
  - [section]: Theorem 4.1 states that if the model satisfies noisy-group fairness constraints and γ_z,z' bounds the TV distance, then clean-group fairness is satisfied within slack γ_z,z'.
  - [corpus]: No direct corpus support for this specific DRO-TV mechanism in AUC fairness; related work focuses on classification fairness under noise (Wang et al. 2020 cited in paper, not in corpus).
- Break condition: If the TV bound γ_z,z' is severely underestimated, or if the noise distribution violates the prior invariance assumption, guarantees degrade.

### Mechanism 2
- Claim: Pre-trained multi-modal foundation models (e.g., CLIP) can estimate the protected-group noise ratio without task-specific training.
- Mechanism: For each sample, CLIP computes cosine similarity between the image embedding and text embeddings of positive/negative prompts (e.g., "a photo of a {group name}" vs. "a photo without a {group name}"). If the noisy label contradicts CLIP's semantic alignment, the label is flagged as potentially noisy. The proportion of mismatches estimates the noise ratio (Eq. 10), which directly parameterizes the TV bound.
- Core assumption: CLIP's visual-semantic alignment is sufficiently reliable to detect group-label mismatches in the target domain.
- Evidence anchors:
  - [abstract]: "empirically estimate noise levels using pre-trained multi-modal foundation models like CLIP"
  - [section]: Section 4.3 describes the prompt design and Eq. 10 for computing the empirical noise ratio; Fig. 3 (Right) shows lowest violation at γ=0.02, matching the estimated value.
  - [corpus]: No corpus papers address CLIP-based noise estimation for fairness; corpus is weak here.
- Break condition: CLIP may misalign on domain-shifted or synthetic imagery (e.g., heavily edited deepfakes), leading to over- or under-estimated noise ratios.

### Mechanism 3
- Claim: Stochastic Gradient Descent-Ascent (SGDA) with Sharpness-Aware Minimization (SAM) jointly optimizes AUC fairness and generalization.
- Mechanism: The objective is a minimax problem (Eq. 9): model parameters θ are minimized, while Lagrangian multipliers and worst-case distribution weights are maximized. SAM perturbs parameters in the gradient direction to flatten the loss landscape, improving out-of-distribution generalization.
- Core assumption: The batch sampling strategy ensures each mini-batch contains positive and negative samples from all groups (stratified sampling).
- Evidence anchors:
  - [abstract]: "efficient stochastic gradient descent-ascent (SGDA) algorithm is developed to optimize the proposed objective, enhancing both AUC fairness and model generalization"
  - [section]: Section 4.4 and Algorithm 1 detail the SGDA updates; Table 3 shows performance drops without SAM, validating its contribution.
  - [corpus]: No corpus papers address SGDA or SAM for fairness optimization.
- Break condition: Poor stratified sampling (imbalanced group representation in batches) can destabilize convergence; aggressive learning rates may cause oscillation in the minimax dynamics.

## Foundational Learning

- **AUC (Area Under ROC Curve)**:
  - Why needed here: AUC is the primary performance metric in this work, preferred over accuracy for class-imbalanced settings (e.g., deepfake detection). Fairness is defined as minimizing the gap between group-level AUCs and the overall AUC.
  - Quick check question: Given a binary classifier, can you explain why AUC > 0.5 indicates meaningful ranking ability even when class labels are imbalanced?

- **Total Variation (TV) Distance**:
  - Why needed here: TV distance quantifies the distribution shift between clean and noisy group labels, providing the theoretical bound on fairness violation.
  - Quick check question: If two distributions p and q have TV(p, q) = 0.1, what is the maximum difference in the expectation of any bounded function h: X → [0, 1] under p vs. q?

- **Distributionally Robust Optimization (DRO)**:
  - Why needed here: DRO formalizes worst-case optimization over an uncertainty set of distributions, enabling provable robustness to distributional perturbations like label noise.
  - Quick check question: How does DRO differ from standard empirical risk minimization (ERM) in handling distribution shift?

## Architecture Onboarding

- **Component map**: CLIP-based noise estimator -> DRO objective layer -> SGDA optimizer with SAM -> Stratified sampler

- **Critical path**:
  1. Estimate γ_z,z' using CLIP (Eq. 10) before training
  2. Initialize θ, λ, and ˜p; stratify training data by label and group
  3. For each iteration: sample batch → compute SAM perturbation → update θ (descent) → update λ and ˜p (ascent) → project ˜p onto TV ball
  4. Select best iterate satisfying constraints with lowest objective

- **Design tradeoffs**:
  - **γ selection**: Conservative (high) γ improves fairness guarantees but may over-regularize, reducing AUC; aggressive (low) γ risks fairness violation under unexpected noise
  - **SAM perturbation magnitude (ν)**: Larger ν flattens loss landscape but slows convergence; tune per dataset
  - **Batch size vs. group coverage**: Small batches may miss rare group combinations, violating stratified sampling assumptions

- **Failure signatures**:
  - **Fairness violation spikes during training**: Check if noise ratio γ is underestimated; re-estimate or increase γ
  - **Divergent loss/oscillating multipliers**: Reduce learning rates (η_θ, η_λ, η_p) or increase SAM ν cautiously
  - **Consistently low AUC with perfect fairness**: Model collapsed to trivial solution (AUC ≈ 0.5); weaken fairness constraint or increase overall AUC loss weight

- **First 3 experiments**:
  1. **Baseline comparison on Adult with synthetic noise**: Flip 10–50% of protected group labels; compare AUC violation vs. MinimaxFairAUC and AUCMax (replicate Table 1)
  2. **γ sensitivity analysis on FF++**: Manually vary γ ∈ {0.01, 0.02, 0.03, 0.04, 0.05} and plot fairness violation (replicate Fig. 3 Right)
  3. **Ablation without SAM**: Train on DFDC or Celeb-DF without SAM; compare Min/Max AUC and violation to full method (replicate Table 3)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a concrete noise estimation strategy be developed for tabular data modalities using the proposed framework?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that the "lack of a concrete noise estimation strategy for tabular modalities" is a limitation, as their current empirical method relies on vision-language models (CLIP).
- Why unresolved: The current methodology relies on multi-modal foundation models which are naturally suited for images but less established for tabular data without significant modification.
- What evidence would resolve it: An instantiation of the noise ratio estimation method using domain-specific tabular foundation models or statistical techniques that maintains the theoretical robustness guarantees.

### Open Question 2
- Question: Can theoretical guarantees be established for ensuring both utility and fairness simultaneously when training with noisy protected groups?
- Basis in paper: [explicit] The limitations section identifies "ensuring both utility and fairness simultaneously when training with noisy groups" as an "open challenge."
- Why unresolved: The current work focuses on robust fairness preservation, but admits the tension between maintaining utility and fairness under noise remains theoretically difficult to bound simultaneously.
- What evidence would resolve it: Theoretical derivations providing a unified bound for both generalization error and fairness violation under label noise, supported by empirical validation.

### Open Question 3
- Question: Can this distributionally robust optimization approach be extended to other pairwise ranking metrics beyond AUC?
- Basis in paper: [explicit] The Future Work section proposes extending the approach to "other pairwise ranking metrics (e.g., partial AUC, average precision)."
- Why unresolved: The mathematical formulation is specialized for AUC risk minimization; the convexity or efficient optimization landscape for other metrics under robust constraints is unverified.
- What evidence would resolve it: Modified loss functions and constraints for metrics like partial AUC, along with convergence proofs and empirical comparisons on standard ranking benchmarks.

### Open Question 4
- Question: How does the method perform if the noise significantly alters the prior pairwise group probabilities, violating the core theoretical assumption?
- Basis in paper: [inferred] Lemma 4.2 requires that prior pairwise group probabilities remain unaffected by noise ($Pr[(Z, Z')] = Pr[(\tilde{Z}, \tilde{Z}')]$, an assumption that may not hold if noise rates are correlated with specific demographic subgroups.
- Why unresolved: The theoretical guarantee (Theorem 4.1) relies on this specific noise independence to bound the Total Variation distance; failure of this assumption could render the fairness guarantees void.
- What evidence would resolve it: Theoretical extensions relaxing the prior probability assumption or empirical results demonstrating robustness under non-uniform noise rates where specific groups have higher mislabeling probabilities.

## Limitations

- **CLIP-based noise estimation reliability**: CLIP's visual-semantic alignment may degrade on domain-shifted or synthetic imagery, potentially leading to inaccurate noise ratio estimates and compromised fairness guarantees.
- **SGDA convergence and stability**: The minimax optimization landscape may contain saddle points or exhibit oscillations, particularly with aggressive learning rates or poor stratified sampling, without convergence proofs provided.
- **TV bound conservatism**: The worst-case TV distance bound may over-regularize if the actual noise distribution is less severe than assumed, unnecessarily sacrificing AUC performance for fairness.

## Confidence

- **High**: Theoretical fairness guarantee under TV-bounded noise (Theorem 4.1); empirical superiority over baselines on multiple datasets (Table 1, 2, 3)
- **Medium**: CLIP-based noise estimation accuracy across diverse domains; SGDA convergence stability; practical impact of TV bound conservatism

## Next Checks

1. **CLIP generalization test**: Evaluate CLIP-based noise estimation on a new dataset (e.g., synthetic media) with known ground-truth noise levels. Compare estimated vs. actual noise ratios and resulting fairness violations.
2. **Convergence diagnostics**: Monitor Lagrangian multipliers (λ) and distribution weights (p̃) during training. Plot their trajectories to identify oscillation or divergence patterns. Test learning rate schedules (e.g., cosine annealing) to improve stability.
3. **Adaptive TV bound selection**: Implement a validation-based procedure to select γ dynamically (e.g., grid search with fairness constraint satisfaction as criterion). Compare performance to fixed γ estimates.