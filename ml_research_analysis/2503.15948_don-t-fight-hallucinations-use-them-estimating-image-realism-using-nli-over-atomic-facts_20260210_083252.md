---
ver: rpa2
title: 'Don''t Fight Hallucinations, Use Them: Estimating Image Realism using NLI
  over Atomic Facts'
arxiv_id: '2503.15948'
source_url: https://arxiv.org/abs/2503.15948
tags:
- facts
- image
- images
- atomic
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a method to quantify image realism by leveraging
  hallucinations in Large Vision-Language Models (LVLMs) and Natural Language Inference
  (NLI). The method generates atomic facts from images using LVLMs, computes pairwise
  entailment scores between these facts using NLI models, and aggregates these scores
  to produce a single reality score.
---

# Don't Fight Hallucinations, Use Them: Estimating Image Realism using NLI over Atomic Facts

## Quick Facts
- arXiv ID: 2503.15948
- Source URL: https://arxiv.org/abs/2503.15948
- Reference count: 24
- This work proposes a method to quantify image realism by leveraging hallucinations in Large Vision-Language Models (LVLMs) and Natural Language Inference (NLI), achieving 72.55% zero-shot accuracy on the WHOOPS! dataset.

## Executive Summary
This paper introduces RealityCheck, a method that quantifies image realism by exploiting hallucinations from LVLMs rather than suppressing them. The approach generates atomic facts from images using LVLMs, computes pairwise entailment scores between these facts using NLI models, and aggregates these scores to produce a single reality score. By detecting contradictions between genuine facts and hallucinatory elements, the method signals unrealistic images. Evaluated on the WHOOPS! dataset, RealityCheck achieves state-of-the-art zero-shot performance among open-source models with 72.55% accuracy.

## Method Summary
RealityCheck generates N=5 atomic facts per image using llava-v1.6-mistral-7b with diverse beam search, then computes pairwise NLI scores between all fact pairs using cross-encoder/nli-deberta-v3-large. Each pair receives entailment, contradiction, and neutrality scores weighted by (1.75, -2.0, 0) respectively, with bidirectional scores summed. K-means clustering (k=2) is applied to these aggregated scores, and the lower centroid value serves as the reality score. Lower scores indicate weirder images, with the method achieving 72.55% binary accuracy on WHOOPS! dataset.

## Key Results
- Achieves 72.55% zero-shot binary accuracy on WHOOPS! dataset for detecting counter-commonsense images
- Outperforms all tested baselines including CLIP-based methods (60.78%) and other open-source models
- K-means clustering aggregation (clust) outperforms simple min (63.73%) and absmax (62.75%) selection methods
- Larger NLI model (deberta-v3-large, 304M params) provides +12% accuracy over smaller variant (47M params)

## Why This Works (Mechanism)

### Mechanism 1: Hallucination as Signal for Counter-Commonsense Content
LVLMs produce more hallucinations when describing images that violate common sense than normal images. When presented with counter-commonsense images (e.g., Einstein holding a smartphone), the LVLM generates a mix of accurate observations and erroneous hallucinations. These hallucinations create semantic contradictions with accurate facts that can be detected. Core assumption: hallucination rate and type differ systematically between normal and weird images in a detectable way.

### Mechanism 2: NLI Captures Semantic Contradictions Between Facts
Pairwise NLI comparison between generated facts amplifies contradictions indicating unrealistic images. Each fact pair receives entailment, contradiction, and neutrality scores, with weighted aggregation (1.75, -2.0, 0) emphasizing contradictions. Summing bidirectional pairs further amplifies negative signals. Core assumption: encoder-based NLI models trained on SNLI/MultiNLI transfer effectively to detecting contradictions in LVLM-generated image descriptions.

### Mechanism 3: Cluster-Based Aggregation Captures Distributed Contradiction Signal
K-means clustering (k=2) on NLI scores and selecting the lower centroid outperforms simple min or absmax selection. Rather than relying on single extreme values, clustering averages contradictory signals, making the metric robust to individual outlier scores while capturing systematic contradictions. Core assumption: weird images produce a coherent cluster of low-scoring fact pairs, not just one-off contradictions.

## Foundational Learning

- **Natural Language Inference (NLI)**
  - Why needed here: Core mechanism for detecting semantic contradictions between generated facts. Requires understanding entailment, contradiction, and neutrality relationships.
  - Quick check question: Given "A camel stands in the desert" and "A camel is swimming in the ocean," what NLI label applies?

- **LVLM Hallucination Patterns**
  - Why needed here: The method exploits—rather than suppresses—hallucinations. Understanding when and why LVLMs hallucinate is critical for predicting method effectiveness.
  - Quick check question: Would you expect more hallucinations from an LVLM describing a typical street scene or an image of a fish riding a bicycle?

- **Atomic Fact Decomposition**
  - Why needed here: The method requires breaking image descriptions into simple, verifiable statements. Complex multi-clause sentences reduce NLI accuracy.
  - Quick check question: Decompose "A man in a red jacket climbs a snow-covered mountain while holding an ice axe" into atomic facts.

## Architecture Onboarding

- Component map: Image → LVLM fact generation → Pairwise NLI scoring → Weighted aggregation → Clustering → Reality score
- Critical path: Image → LVLM fact generation → Pairwise NLI scoring → Weighted aggregation → Clustering → Reality score. Fact generation quality is the primary bottleneck.
- Design tradeoffs:
  - More facts (N>5) increases computation O(N²) but may capture more contradictions
  - Larger NLI model (304M) vs. smaller (47M): +12% accuracy but 6x parameters
  - Clustering vs. min: Clustering more robust but requires fitting k-means per image
- Failure signatures:
  - High false positives: LVLM hallucinates on normal images (check P(weird|hallucination) ratio)
  - Low separation: NLI scores cluster near neutral; may indicate NLI model mismatch
  - Inconsistent weights across folds: Indicates overfitting or dataset-specific artifacts
- First 3 experiments:
  1. Run LVLM on 20 normal + 20 weird images from WHOOPS!, manually annotate hallucination presence. Verify P(weird|hallucination) > P(normal|hallucination).
  2. Compare min, absmax, and clust on held-out subset. Confirm clust advantage persists across different random seeds.
  3. Test N ∈ {3, 5, 7, 10} with fixed NLI model and aggregation. Plot accuracy vs. N and computational cost.

## Open Questions the Paper Calls Out

### Open Question 1
Does increasing the factual robustness of the Large Vision-Language Model (LVLM) degrade the performance of this hallucination-based detection method? If future LVLMs are trained to be "hallucination-free" or strictly faithful to visual inputs, they might describe a weird image accurately without generating the necessary internal contradictions required by the NLI aggregation step.

### Open Question 2
Can the method distinguish between semantic violations (common-sense errors) and visual artifacts (AI-generated glitches) without explicit training? The paper focuses exclusively on the WHOOPS! dataset, which features semantic/pragmatic weirdness rather than visual texture errors found in deepfakes. It's unclear if visual artifacts that do not result in semantic contradictions would trigger a low reality score.

### Open Question 3
To what extent does the choice of the fact-generation prompt influence the probability of triggering useful hallucinations? Section 4.1 specifies a single fixed prompt, but does not analyze if phrasing the prompt as a question or requesting detailed descriptions alters the hallucination rate. A different prompt might yield fewer hallucinations on weird images, reducing the signal-to-noise ratio.

## Limitations
- Dataset generalizability: WHOOPS! contains specifically curated counter-commonsense pairs; performance on naturally occurring weird images remains untested
- LVLM hallucination dependency: Method's effectiveness fundamentally depends on LVLMs producing more hallucinations for weird images, but baseline hallucination rates are not established
- NLI model transfer: Assumes NLI models trained on textual entailment effectively capture visual commonsense contradictions, lacking direct validation

## Confidence

- High Confidence: Technical implementation details (LVLM fact generation, NLI scoring pipeline, clustering aggregation) are clearly specified and reproducible. The 72.55% accuracy on WHOOPS! is verifiable given the dataset.
- Medium Confidence: The claim that "hallucinations signal unrealistic images" is supported by experimental results but lacks mechanistic validation showing hallucination rates differ systematically between normal and weird images.
- Low Confidence: The claim that this approach represents a general solution for "estimating image realism" is overstated given the narrow dataset scope and unexplored failure modes.

## Next Checks

1. **Hallucination Rate Analysis:** Run LVLM on 100 randomly selected normal images and 100 weird images from diverse sources. Compute hallucination rates and show P(weird|hallucination) > P(normal|hallucination) with statistical significance.

2. **Cross-Dataset Evaluation:** Test the method on a different realism dataset (e.g., ImageNet-A or synthetic vs real image pairs) to verify the 72.55% accuracy isn't dataset-specific.

3. **NLI Model Ablation:** Replace deberta-v3-large with smaller NLI models and analyze accuracy degradation. If accuracy drops sharply, the claim about NLI effectiveness is fragile; if robust, the method scales better.