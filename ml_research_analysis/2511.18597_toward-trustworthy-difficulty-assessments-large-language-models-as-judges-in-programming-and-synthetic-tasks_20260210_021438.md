---
ver: rpa2
title: 'Toward Trustworthy Difficulty Assessments: Large Language Models as Judges
  in Programming and Synthetic Tasks'
arxiv_id: '2511.18597'
source_url: https://arxiv.org/abs/2511.18597
tags:
- problems
- hard
- difficulty
- gpt-4o
- programming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether GPT-4o can serve as a trustworthy
  difficulty judge for competitive programming problems. The authors compare GPT-4o
  (used as a black-box natural-language difficulty assessor) against a LightGBM ensemble
  that leverages explicit numeric and textual features.
---

# Toward Trustworthy Difficulty Assessments: Large Language Models as Judges in Programming and Synthetic Tasks

## Quick Facts
- arXiv ID: 2511.18597
- Source URL: https://arxiv.org/abs/2511.18597
- Reference count: 11
- Primary result: GPT-4o achieves only 37.75% accuracy versus LightGBM's 86% on LeetCode difficulty classification

## Executive Summary
This paper evaluates whether GPT-4o can serve as a trustworthy difficulty judge for competitive programming problems. The authors compare GPT-4o (used as a black-box natural-language difficulty assessor) against a LightGBM ensemble that leverages explicit numeric and textual features. On a dataset of 1,825 LeetCode problems labeled Easy, Medium, or Hard, GPT-4o achieves only 37.75% accuracy versus LightGBM's 86%. Detailed analysis shows GPT-4o systematically underestimates difficulty, labeling 83% of real Hard problems as Easy. In a synthetic experiment, GPT-4o-generated Hard problems are nearly all labeled Medium, revealing internal inconsistency. SHAP-based interpretability highlights that numeric constraints (e.g., input sizes, acceptance rates) are critical for separating Hard problems, which GPT-4o overlooks. The findings underscore LLM judges' current limitations and suggest hybrid approaches combining LLM text embeddings with interpretable numeric models for trustworthy difficulty assessment.

## Method Summary
The study compares GPT-4o as a black-box difficulty judge against a LightGBM ensemble classifier on 1,825 LeetCode programming problems. LightGBM uses TF-IDF vectors plus numeric features (max input size, acceptance rate), while GPT-4o receives only textual descriptions with numeric metadata removed. Both models classify problems as Easy, Medium, or Hard. The authors evaluate accuracy, precision, recall, and F1-score, and use SHAP values to analyze feature importance. A synthetic experiment generates 385 new problems from 21 seed Hard problems, then reclassifies them with GPT-4o to test internal consistency.

## Key Results
- GPT-4o achieves only 37.75% accuracy versus LightGBM's 86% on LeetCode difficulty classification
- GPT-4o systematically underestimates difficulty, labeling 83% of real Hard problems as Easy
- 384 of 385 GPT-4o-generated Hard problems are labeled Medium (99.74%), revealing internal inconsistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Numeric constraint features dominate accurate difficulty classification for competitive programming problems.
- Mechanism: LightGBM's decision boundary for Hard problems is driven primarily by explicit numeric features—maximum input size and acceptance rates—which provide orthogonal information to textual descriptions. These features are computationally trivial to extract but semantically dense, encoding algorithmic complexity requirements (e.g., O(n²) vs O(n log n) feasibility).
- Core assumption: The paper assumes these numeric features are causally related to human-perceived difficulty, not merely correlated artifacts of LeetCode's labeling process.
- Evidence anchors:
  - [abstract] "SHAP-based interpretability highlights that numeric constraints (e.g., input sizes, acceptance rates) are critical for separating Hard problems"
  - [Section 4.4] "Numeric constraints—especially maximum input size and acceptance rate—are among the most influential features for classifying Hard problems"
  - [corpus] Weak direct corpus support; related work (Skarbalius and Lukosevicius, 2021; Wang et al., 2024) uses similar features but does not isolate causal mechanisms
- Break condition: If LeetCode difficulty labels were revised to rely purely on algorithmic concepts (ignoring constraints), this feature importance pattern would likely change significantly.

### Mechanism 2
- Claim: LLM-as-judge difficulty assessment fails when models lack explicit access to numeric metadata that humans use for calibration.
- Mechanism: GPT-4o receives only natural language problem descriptions with numeric constraints paraphrased or removed. It must infer difficulty from semantic content alone. The model appears to match problem descriptions to training-distribution patterns (recognizing algorithmic themes) but cannot reliably reconstruct the constraint-satisfaction reasoning that determines actual difficulty.
- Core assumption: Assumption: GPT-4o's training data contains sufficient difficulty-labeled examples that its failure stems from input representation rather than lack of exposure.
- Evidence anchors:
  - [Section 3.1] "GPT-4o receives only the textual problem descriptions, with all numeric metadata...removed or paraphrased away so that they cannot be trivially parsed as numbers"
  - [Section 5] "GPT-4o, deprived of explicit numeric metadata, appears to treat many Hard problems as ordinary variants of medium-level algorithmic exercises"
  - [corpus] Bavaresco et al. (2025) show LLM-judge agreement with humans varies widely by task; this paper provides a concrete failure mode in structured domains
- Break condition: If prompts were restructured to include explicit constraint tables, performance might improve—but the paper did not test this (noted as future work).

### Mechanism 3
- Claim: GPT-4o exhibits unstable internal difficulty boundaries across real vs. synthetic problem regimes.
- Mechanism: When generating synthetic Hard problems, GPT-4o produces descriptions that it subsequently classifies as Medium (99.74%). When evaluating real Hard problems, it classifies 83% as Easy. This suggests the model's internal "Hard" representation does not form a coherent boundary—it over-generates from a textual template that lacks the constraint stringency of real Hard problems, while simultaneously under-detecting that stringency in authentic examples.
- Core assumption: Assumption: This inconsistency reflects a fundamental representational gap rather than prompt-engineering artifacts.
- Evidence anchors:
  - [Section 4.5] "384 problems (99.74%) are labeled as Medium...This behavior contradicts its pattern on real Hard problems, which are mostly labeled as Easy"
  - [Section 5] "GPT-4o's internal boundary between Easy, Medium, and Hard is not only misaligned with LeetCode's ground truth, but also unstable across real and synthetic domains"
  - [corpus] No direct corpus evidence on synthetic-real inconsistency; this appears to be a novel finding
- Break condition: If synthetic problems were post-hoc annotated with explicit constraints matching real Hard distributions, re-classification might shift toward Easy (matching the real-problem pattern).

## Foundational Learning

- Concept: **SHAP (SHapley Additive exPlanations) values**
  - Why needed here: The paper uses SHAP to prove that LightGBM's Hard-classification relies on specific features (input size, acceptance rate). Understanding SHAP is required to interpret why the interpretable model succeeds.
  - Quick check question: If a feature has high mean absolute SHAP value for Hard predictions, does that mean high values of that feature always push predictions toward Hard?

- Concept: **Gradient Boosted Decision Trees (LightGBM)**
  - Why needed here: Serves as the interpretable baseline. Understanding how tree ensembles partition feature space helps explain why they capture constraint-difficulty relationships that text-only models miss.
  - Quick check question: Why might a tree-based model handle sparse TF-IDF features combined with dense numeric features better than a purely neural approach?

- Concept: **LLM-as-judge paradigm**
  - Why needed here: The paper situates itself within this emerging evaluation framework. Understanding the paradigm clarifies why LLM difficulty judgment is even considered viable—and what failure modes are known.
  - Quick check question: What are two known risks of using LLMs as judges that prior work (Li et al., 2024; Bavaresco et al., 2025) has identified?

## Architecture Onboarding

- Component map:
  Data pipeline: LeetCode scraper → problem text + metadata extraction → train/val/test split
  GPT-4o pipeline: Text-only prompt → single-pass classification → label output
  LightGBM pipeline: TF-IDF vectorizer + numeric feature extractor → ensemble classifier → SHAP explainer
  Synthetic generation: 21 seed Hard titles → GPT-4o generation with constraints → self-reclassification loop

- Critical path:
  1. Ensure numeric features (input_size_max, acceptance_rate) are correctly extracted and aligned with problem IDs
  2. Verify GPT-4o prompt strips all numeric metadata from problem descriptions before API calls
  3. Run SHAP analysis on trained LightGBM to confirm feature importance matches paper's Figure 2 before drawing conclusions

- Design tradeoffs:
  - Interpretability vs. flexibility: LightGBM provides auditable decisions but requires manual feature engineering; GPT-4o requires no feature engineering but offers no transparency
  - Single-pass vs. self-consistency: Paper uses single-pass labeling (common in LLM-as-judge setups) but this increases variance; self-consistency sampling might reduce noise
  - Hybrid approach (proposed): Use LLM embeddings as additional features to LightGBM—trades some interpretability for potential accuracy gains

- Failure signatures:
  - Systematic underestimation: If >70% of known Hard problems are classified as Easy/Medium, the model is exhibiting this paper's documented failure mode
  - Label distribution collapse: If model outputs >90% of predictions in one class, it has failed to learn discriminative boundaries
  - Synthetic-real inconsistency: If self-generated "Hard" problems are classified differently than real Hard problems by the same model, internal representation is unstable

- First 3 experiments:
  1. Constraint-aware prompting replication: Provide GPT-4o with explicit constraint tables (restoring numeric metadata) and measure accuracy change. Hypothesis: accuracy should improve if constraint-blindness is the primary failure mechanism.
  2. Feature ablation on LightGBM: Train LightGBM with only TF-IDF features (no numeric metadata) and compare accuracy to full model. This isolates how much performance gain comes from numeric features vs. text.
  3. Cross-platform validation: Test both models on Codeforces problems (different difficulty labeling scheme) to assess whether findings generalize beyond LeetCode's specific label definitions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can constraint-aware prompting—explicitly foregrounding numeric tables and constraints—enable GPT-4o to more reliably assess difficulty, or does the model lack the reasoning capacity to integrate such cues?
- Basis in paper: [explicit] The authors state in future work: "We plan to explore prompts that explicitly foreground numeric details, including structured tables of constraints, to test whether GPT-4o can more reliably internalize and use them for difficulty assessment."
- Why unresolved: The current study deliberately removes numeric metadata to isolate text-only judgment, leaving the effect of explicit constraint provision untested.
- What evidence would resolve it: A controlled experiment comparing GPT-4o accuracy with and without explicit constraint tables on the same problem set, measuring whether accuracy gap to LightGBM narrows.

### Open Question 2
- Question: Why does GPT-4o exhibit opposite labeling behaviors for synthetic versus real Hard problems (Medium for synthetic, Easy for real), and what textual or structural cues drive this inconsistency?
- Basis in paper: [explicit] The authors note the synthetic Hard problems "collapse into Medium" while real Hard problems are "overwhelmingly downgraded" to Easy, calling this "unstable across real and synthetic domains" without explaining the mechanism.
- Why unresolved: The paper documents the discrepancy but does not analyze what linguistic or structural differences between synthetic and real Hard problems cause the divergent classifications.
- What evidence would resolve it: A fine-grained linguistic analysis comparing real vs. synthetic Hard problems, combined with probing methods to identify which features GPT-4o attends to in each case.

### Open Question 3
- Question: Would hybrid models combining LLM-derived embeddings with interpretable numeric features significantly outperform pure LightGBM or pure LLM approaches on difficulty assessment?
- Basis in paper: [explicit] The authors propose: "Building on recent multimodal difficulty models, we aim to combine LLM-derived embeddings of problem text and solution code with interpretable numeric features, potentially using LightGBM or related tree ensembles as the final judge."
- Why unresolved: The study only compares isolated LLM vs. isolated LightGBM approaches; no hybrid architecture is tested.
- What evidence would resolve it: Training and evaluating a hybrid model on the same dataset, comparing macro F1-score against both baselines.

## Limitations
- Domain-specific findings may not generalize beyond competitive programming problems
- Assumes LeetCode's ground-truth labels are optimal difficulty indicators without examining potential label noise
- Single-pass GPT-4o evaluation without self-consistency sampling may underestimate potential performance

## Confidence
- High confidence: Numeric constraint features (input size, acceptance rate) are critical for accurate difficulty classification
- Medium confidence: GPT-4o systematically underestimates difficulty due to missing numeric metadata
- Medium confidence: Synthetic-real inconsistency finding represents a novel observation requiring further replication

## Next Checks
1. **Prompt reconstruction experiment**: Systematically vary the prompt template (including/including constraint tables) to determine the exact threshold at which GPT-4o's accuracy improves, isolating the metadata-blindness mechanism
2. **Cross-platform generalizability test**: Apply both models to Codeforces problems with different difficulty labeling criteria to assess whether the numeric-feature importance pattern holds across platforms
3. **Self-consistency sampling validation**: Compare single-pass vs. majority-vote (3-5 samples) GPT-4o performance to quantify the impact of variance on difficulty assessment reliability