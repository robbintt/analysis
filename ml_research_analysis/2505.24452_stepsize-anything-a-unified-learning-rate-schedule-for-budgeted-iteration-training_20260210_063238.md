---
ver: rpa2
title: 'Stepsize anything: A unified learning rate schedule for budgeted-iteration
  training'
arxiv_id: '2505.24452'
source_url: https://arxiv.org/abs/2505.24452
tags:
- learning
- training
- rate
- schedule
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UBA (Unified Budget-Aware), a theoretically
  grounded learning rate schedule for budgeted-iteration training. Unlike existing
  heuristic approaches, UBA is derived from a min-max optimization framework that
  explicitly accounts for landscape curvature variations, ensuring robustness across
  diverse architectures and tasks.
---

# Stepsize anything: A unified learning rate schedule for budgeted-iteration training

## Quick Facts
- **arXiv ID:** 2505.24452
- **Source URL:** https://arxiv.org/abs/2505.24452
- **Authors:** Anda Tang; Yiming Dong; Yutao Zeng; zhou Xun; Zhouchen Lin
- **Reference count:** 40
- **Primary result:** UBA (Unified Budget-Aware) is a theoretically grounded learning rate schedule that consistently outperforms common schedules across diverse architectures and scales under different training-iteration budgets.

## Executive Summary
This paper introduces UBA (Unified Budget-Aware), a theoretically grounded learning rate schedule for budgeted-iteration training. Unlike existing heuristic approaches, UBA is derived from a min-max optimization framework that explicitly accounts for landscape curvature variations, ensuring robustness across diverse architectures and tasks. The schedule is controlled by a single hyperparameter φ, which provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. The authors establish a theoretical connection between φ and the condition number, adding interpretation and justification to their approach.

## Method Summary
UBA is a learning rate schedule derived from a min-max optimization framework that minimizes loss under worst-case Hessian curvature variations. The schedule uses a parametric cosine-based formula with a single hyperparameter φ that controls the decay profile. The method generalizes existing schedules (Cosine, Step, REX) through specific φ settings and is designed to be robust across diverse architectures and training budgets. Implementation is provided as a PyTorch scheduler inheriting from torch.optim.lr_scheduler.LRScheduler.

## Key Results
- UBA consistently outperforms commonly-used schedules across diverse network architectures and scales under different training-iteration budgets
- The method achieves state-of-the-art performance on approximately half of the language benchmarks and maintains superior average performance across all scales
- Performance improvements come with negligible computational overhead, making it a practical, must-try schedule for deep learning practitioners

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The UBA schedule provides robustness across diverse architectures by optimizing for the worst-case curvature scenario.
- **Mechanism:** The authors frame learning rate design as a min-max optimization problem (Eq. 1) where the goal is to minimize the final loss under the worst-case local curvature (Hessian) variations. By solving this, the schedule theoretically guarantees a bound on loss reduction regardless of specific landscape anomalies, provided bounds on Hessian eigenvalues exist.
- **Core assumption:** The loss landscape near local optima can be approximated by a quadratic function defined by the Hessian matrix, and the eigenvalues of this Hessian are bounded.
- **Evidence anchors:** [abstract] "constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations."

### Mechanism 2
- **Claim:** The hyperparameter φ serves as a proxy for optimization difficulty (condition number), allowing the schedule to adapt its decay profile.
- **Mechanism:** Proposition 1 establishes a theoretical link where φ is proportional to the ratio of Hessian eigenvalue bounds (condition number). A larger condition number (harder optimization) suggests a larger φ. Empirical results show AdamW (which preconditions/lowers effective condition number) prefers small φ, while SGD prefers larger φ.
- **Core assumption:** Optimization difficulty is largely governed by the condition number of the Hessian, and adaptive optimizers implicitly modify this landscape.
- **Evidence anchors:** [Section 3.2] "we establish a theoretical connection between φ and the condition number... φ is related to optimization difficulty."

### Mechanism 3
- **Claim:** UBA generalizes existing schedules (Cosine, Step, REX) through its parametric form, reducing to them at specific φ settings.
- **Mechanism:** The derived formula (Eq. 5) is a parametric function of cosine terms. When φ ≈ 2, the function mathematically converges to the standard Cosine schedule. Other values mimic Step decay or Exponential decay.
- **Core assumption:** A single parametric family can capture the optimal dynamics of various distinct heuristic schedules.
- **Evidence anchors:** [Section 3.2] Proposition 2 explicitly proves the limit φ → 2 reduces to Cosine LR.

## Foundational Learning

- **Concept:** Min-Max Optimization
  - **Why needed here:** To understand the theoretical derivation of UBA, which minimizes loss assuming the worst-case data/model configuration.
  - **Quick check question:** If the loss landscape is fixed, is the min-max problem equivalent to standard minimization? (Hint: Yes, max over a single function is just that function).

- **Concept:** Condition Number (κ)
  - **Why needed here:** The core hyperparameter φ is theoretically tied to the condition number of the Hessian; understanding this helps in tuning φ.
  - **Quick check question:** Does a high condition number generally indicate faster or slower convergence for gradient descent? (Hint: Slower).

- **Concept:** Chebyshev Polynomials
  - **Why needed here:** The closed-form solution for the optimal schedule in quadratic optimization relies on Chebyshev polynomials, explaining the cosine-like structure.
  - **Quick check question:** What property makes Chebyshev polynomials optimal for this derivation? (Hint: Minimax property—minimizing the maximum error).

## Architecture Onboarding

- **Component map:** Current iteration t -> Total budget T -> Phase bounds T_k -> Hyperparameter φ -> LR limits η_min/max -> Learning rate scalar η_t
- **Critical path:**
  1. **Hyperparameter Selection:** Set φ based on optimizer type (AdamW → low φ, SGD → high φ)
  2. **Initialization:** Set η_max (initial LR) and η_min (final LR)

- **Design tradeoffs:**
  - **Simplicity vs. Performance:** Fixing φ provides a "plug-and-play" experience (Simplicity), but optimal performance requires tuning φ for the specific dataset/optimizer pair (Performance)
  - **Single-phase vs. Multi-phase:** The paper focuses on single-phase (K=2) for simplicity, but multi-phase (K>2) allows better approximation of non-stationary landscapes at the cost of complexity

- **Failure signatures:**
  - **Slow Convergence:** If φ is set too high (rapid decay) for a task requiring long exploration
  - **Divergence:** If φ is set too low (slow decay, high LR for long) for an ill-conditioned problem without preconditioning (SGD on complex data)

- **First 3 experiments:**
  1. **Sanity Check vs. Cosine:** Train a ResNet on CIFAR-10 with UBA (φ=2) and standard Cosine. Verify identical curves.
  2. **Optimizer Sensitivity:** Train the same model with AdamW using φ=0.5 and φ=5.0. Confirm lower φ performs better.
  3. **Budget Generalization:** Train with 25% and 100% budget. Compare UBA against a fixed Step decay schedule to validate budget-aware robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the explicit relationship between the hyperparameter φ and "optimization difficulty" be formalized to enable adaptive, difficulty-aware tuning?
- **Basis in paper:** [explicit] The conclusion states that "the explicit relationship between φ and optimization difficulty remains unexplored" and notes the lack of an established metric to quantify this difficulty.
- **Why unresolved:** While Proposition 1 links φ to the condition number in a special case, the general influence of dataset and architecture on "optimization difficulty" and the subsequent optimal choice of φ remains qualitative.
- **What evidence would resolve it:** A theoretical or empirical derivation of a metric for optimization difficulty that predicts the optimal φ across diverse architectures without manual trial-and-error.

### Open Question 2
- **Question:** Can an automated mechanism be developed to select optimal φ values per phase for multi-phase scheduling?
- **Basis in paper:** [explicit] In Section 4.3, the authors observe that multi-phase scheduling improves accuracy but requires careful φ selection. They conclude that "selecting optimal φ values per phase for multi-phase remains non-trivial, which motivates future work on automated landscape-aware φ tuning."
- **Why unresolved:** The current method relies on a fixed φ for simplicity, limiting the potential gains of the more dynamic multi-phase approach which better captures non-stationary loss surfaces.
- **What evidence would resolve it:** An algorithm that dynamically adjusts φ based on real-time landscape features (like curvature), outperforming the single-phase baseline without manual intervention.

### Open Question 3
- **Question:** Does the UBA schedule maintain its performance superiority when scaling to Large Language Models with billions of parameters?
- **Basis in paper:** [inferred] The paper claims "Stepsize anything" and applicability to "growing scale of models," yet the largest model evaluated is OLMo-300M. This leaves a gap between the "unified" claim and billion-parameter scales common in LLM research.
- **Why unresolved:** The computational dynamics and optimization landscapes of billion-parameter models may differ significantly from the 300M parameter limit tested.
- **What evidence would resolve it:** Benchmark results comparing UBA against standard schedules (e.g., Cosine) on models ≥ 7B parameters, confirming consistent loss reduction and negligible overhead.

## Limitations
- The min-max optimization framework assumes bounded Hessian eigenvalues and a quadratic loss approximation, which may not hold for highly non-convex deep learning landscapes
- The recommendation to fix φ per optimizer type (AdamW: 0.5, SGD: 5+) may not generalize optimally across all datasets and architectures
- The theoretical connection between φ and condition number, while established for quadratic problems, requires empirical validation in high-dimensional non-convex settings

## Confidence

- **High Confidence:** The empirical performance claims showing UBA outperforming standard schedules across vision and language benchmarks are well-supported by the experimental results presented.
- **Medium Confidence:** The theoretical derivation of UBA from min-max optimization is mathematically sound for the quadratic case, but its direct applicability to deep learning requires further investigation.
- **Medium Confidence:** The interpretation of φ as a proxy for condition number is theoretically established for quadratic problems, but the practical relationship in non-convex deep learning landscapes needs more rigorous study.

## Next Checks

1. Test UBA's robustness to hyperparameter tuning by conducting a systematic grid search over φ values for different optimizer-dataset combinations to verify if the recommended values (0.5 for AdamW, 5+ for SGD) are indeed optimal or if the range of effective values is wider.

2. Validate the min-max optimization framework's assumptions by measuring Hessian eigenvalue bounds during training and checking if they remain within the theoretical bounds assumed in the derivation.

3. Compare UBA's performance against adaptive learning rate methods (like AdaBelief or Lion) that also claim to be robust across architectures, to determine if UBA's advantage is specific to fixed-schedule approaches or extends to adaptive methods as well.