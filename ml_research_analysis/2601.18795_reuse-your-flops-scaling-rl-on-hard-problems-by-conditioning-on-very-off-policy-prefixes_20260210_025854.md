---
ver: rpa2
title: 'Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy
  Prefixes'
arxiv_id: '2601.18795'
source_url: https://arxiv.org/abs/2601.18795
tags:
- off-policy
- prefixrl
- problems
- policy
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PrefixRL addresses the challenge of training reinforcement learning\
  \ (RL) policies on hard problems where correct on-policy traces are rare, causing\
  \ learning to stall. The core method idea is to condition the policy on off-policy\
  \ prefixes\u2014partial traces from previously successful attempts\u2014rather than\
  \ directly supervising on them."
---

# Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes

## Quick Facts
- arXiv ID: 2601.18795
- Source URL: https://arxiv.org/abs/2601.18795
- Reference count: 40
- The paper proposes PrefixRL, a method to improve RL training on hard reasoning tasks by conditioning on off-policy prefixes.

## Executive Summary
PrefixRL is a method that improves reinforcement learning (RL) on hard problems by conditioning the policy on off-policy prefixes—partial traces from previously successful attempts. This allows the RL policy to start from higher-rewarding states and continue training on-policy, sidestepping the instability of off-policy RL. The method is theoretically consistent with and more sample-efficient than standard RL, and empirically shows back-generalization: training on prefixed problems improves performance on original unprefixed problems. In experiments on hard reasoning tasks, PrefixRL reaches the same training reward 2× faster than the strongest baseline and increases final reward by 3×.

## Method Summary
PrefixRL addresses the challenge of training RL policies on hard problems where correct on-policy traces are rare. Instead of directly supervising on off-policy traces, PrefixRL conditions the policy on off-policy prefixes—partial traces from previously successful attempts. This allows the RL policy to start from higher-rewarding states and continue training on-policy. The PrefixRL objective is theoretically consistent with and more sample-efficient than standard RL. Empirically, the model exhibits back-generalization: training on prefixed problems improves performance on original unprefixed problems.

## Key Results
- PrefixRL reaches the same training reward 2× faster than the strongest baseline (SFT on off-policy data then RL)
- PrefixRL increases final reward by 3× on hard reasoning tasks
- Gains transfer to held-out benchmarks, with improved performance on unprefixed problems

## Why This Works (Mechanism)
PrefixRL works by conditioning the policy on off-policy prefixes, which allows the RL policy to start from higher-rewarding states. This sidesteps the instability of off-policy RL and enables more efficient learning. The method is theoretically consistent with standard RL and shows back-generalization, where training on prefixed problems improves performance on original unprefixed problems.

## Foundational Learning
- **Reinforcement Learning (RL)**: Why needed - To train policies that maximize reward in an environment. Quick check - Can the policy improve its performance over time through interaction with the environment?
- **Off-policy vs. On-policy Learning**: Why needed - To understand the difference between learning from past experiences (off-policy) and learning from current experiences (on-policy). Quick check - Can the policy learn from both past and current experiences?
- **Conditional Policies**: Why needed - To understand how conditioning the policy on prefixes can improve learning. Quick check - Can the policy adapt its behavior based on the provided prefix?

## Architecture Onboarding
- **Component Map**: PrefixRL -> Off-policy Prefixes -> Conditioned Policy -> On-policy Training -> Improved Performance
- **Critical Path**: 1. Collect off-policy traces 2. Construct prefixed problems 3. Train policy with PrefixRL objective 4. Evaluate on unprefixed problems
- **Design Tradeoffs**: Fixed vs. adaptive prefix length; off-policy data quality vs. quantity
- **Failure Signatures**: All-negative batches, entropy collapse, lack of back-generalization
- **First Experiments**: 1. Verify PrefixRL improves training speed on hard problems 2. Test back-generalization to unprefixed problems 3. Evaluate robustness to off-policy data from different model families

## Open Questions the Paper Calls Out
1. What are the precise internal mechanisms that drive "back-generalization," where training solely on prefixed problems improves performance on unprefixed ones?
2. How does PrefixRL performance degrade if the off-policy traces violate the "correctness" assumption (i.e., the prefix contains reasoning errors)?
3. Can an adaptive curriculum for prefix length optimize the trade-off between exploration difficulty and guidance more effectively than fixed sampling bands?

## Limitations
- The exact difficulty threshold of the problems is underspecified
- Limited evidence for generalization to other domains beyond math reasoning
- Single alternative model test for robustness to different model families

## Confidence
- **Problem difficulty calibration**: Low confidence
- **Generalization to other domains**: Low confidence
- **Robustness to different model families**: Low confidence
- **Theoretical analysis**: High confidence
- **Empirical scalability**: Low confidence

## Next Checks
1. Run PrefixRL with 5 different random seeds on the same hard problem set and report mean/variance of final pass@1 and reward curves
2. Apply PrefixRL to a non-math reasoning task (e.g., code generation or fact-checking) with similarly rare correct traces and measure if back-generalization still occurs
3. Vary the quality of off-policy prefixes (e.g., use partially incorrect traces) and measure degradation in PrefixRL performance vs. baseline