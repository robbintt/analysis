---
ver: rpa2
title: Framework for Machine Evaluation of Reasoning Completeness in Large Language
  Models For Classification Tasks
arxiv_id: '2510.21884'
source_url: https://arxiv.org/abs/2510.21884
tags:
- coverage
- rationales
- alignment
- features
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces RACE, a framework for evaluating the alignment
  between LLM-generated explanations and feature importance from logistic regression
  in text classification. Using token-aware, exact, and edit-distance matching, RACE
  quantifies how well rationales reflect predictive evidence.
---

# Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks

## Quick Facts
- arXiv ID: 2510.21884
- Source URL: https://arxiv.org/abs/2510.21884
- Reference count: 5
- Primary result: LLM rationales align more with predictive features in correct predictions and misleading features in errors; edit-distance matching captures paraphrastic overlaps without altering asymmetry.

## Executive Summary
This paper introduces RACE (Rationale Alignment Coverage Evaluation), a framework for quantifying how well LLM-generated rationales reflect predictive evidence from logistic regression feature importance. Using three matching strategies—token-aware, exact, and edit-distance—the framework measures coverage of supporting and contradicting features in rationales. Across four text classification datasets, RACE reveals a consistent asymmetry: correct predictions show higher coverage of supporting features, while incorrect ones exhibit elevated coverage of contradicting features. Edit-distance matching substantially boosts coverage by detecting paraphrastic overlaps, validating the need for flexible alignment methods.

## Method Summary
The RACE framework trains logistic regression with TF-IDF (1-2 grams) to extract top-5 weighted features per instance, separating them into supporting (positive weight) and contradicting (negative weight) classes. For each instance, an LLM (DeepSeek-R1) generates a predicted label and free-text rationale. Three matchers compute binary overlap between rationale tokens and features: token-aware (lemmatized, lowercase), exact string, and edit-distance (Levenshtein ≤ δ). Coverage metrics (support_cov, contradict_cov) are calculated per instance and aggregated by prediction correctness (match vs. mismatch). The asymmetry in coverage patterns is analyzed to assess rationale faithfulness.

## Key Results
- Correct LLM predictions exhibit higher coverage of supporting features; incorrect predictions show elevated coverage of contradicting features.
- Edit-distance matching substantially increases coverage by detecting paraphrastic overlaps missed by exact and token-aware matching.
- Coverage asymmetry is strongest for topical classification (AG NEWS, WIKIONTOLOGY), moderate for sentiment (IMDB), and weakest for fine-grained emotion recognition (GOEMOTIONS).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Correct LLM predictions exhibit higher coverage of lexically supportive features, while incorrect predictions show elevated coverage of contradicting features.
- Mechanism: LLM rationales tend to surface and amplify features that aligned with the eventual label prediction—whether those features were actually predictive (correct cases) or misleading (error cases). This creates a measurable asymmetry in coverage statistics when partitioned by prediction correctness.
- Core assumption: Logistic regression feature weights derived from TF-IDF features provide a valid ground-truth proxy for which lexical cues are actually predictive for each class.
- Evidence anchors:
  - [abstract] "Empirical results reveal a consistent asymmetry: correct predictions exhibit higher coverage of supporting features, while incorrect predictions are associated with elevated coverage of contradicting features."
  - [Section 5.2] "Coverage patterns reveal a consistent asymmetry between correct and incorrect predictions... correct cases cluster around higher support coverage, while mismatches exhibit greater dispersion and systematically higher contradiction coverage."
  - [corpus] Limited direct validation; neighboring work on LLM rationales (e.g., "Rethinking Human Preference Evaluation of LLM Rationales") focuses on human preference alignment rather than feature-level coverage asymmetry.
- Break condition: If logistic regression's top-k features poorly represent true predictive signals (e.g., when important cues are compositional or distributed), the asymmetry signal may reflect baseline limitations rather than LLM reasoning behavior.

### Mechanism 2
- Claim: Edit-distance matching captures paraphrastic overlaps missed by exact and token-aware matching, boosting measured coverage while preserving the support/contradiction asymmetry.
- Mechanism: LLM rationales frequently reformulate predictive features using near-synonyms, morphological variants, or minor edits. Fuzzy matching with a permissive edit threshold (δ) detects these flexible correspondences without requiring verbatim surface-form overlap.
- Core assumption: Small edit distances correlate with semantic equivalence for the purposes of feature-rationale alignment; non-trivial semantic transformations (antonyms, idioms) are not captured.
- Evidence anchors:
  - [Section 3.2] "Edit-distance: Fuzzy alignment allowing small character-level deviations (ED(f, s) ≤ δ)."
  - [Section 5.4] "Edit-distance matching further increases coverage (Table 1), often by a substantial margin (e.g., WIKIONTOLOGY match support rises from 0.52 to 0.61)."
  - [Section 6.1] "Edit-distance matching reveals that many rationales also incorporate close variants or paraphrases of top features, substantially boosting measured coverage."
  - [corpus] No direct corpus validation of edit-distance thresholds for semantic equivalence in related rationale evaluation work.
- Break condition: If δ is set too high, edit-distance matching will admit spurious matches; if too low, it collapses to exact matching. Threshold requires empirical tuning per dataset.

### Mechanism 3
- Claim: Rationale-feature alignment strength varies by task type—strongest for topical classification, moderate for sentiment, weakest for fine-grained emotion recognition.
- Mechanism: Tasks differ in lexical grounding of category boundaries. Topical categories (e.g., Business vs. Sports) have distinctive keyword signatures; sentiment expressions are more distributed; fine-grained emotions have overlapping, ambiguous lexical cues that dilute feature importance signals.
- Core assumption: The observed variation reflects task structure rather than LLM capability limits or logistic regression inadequacy alone.
- Evidence anchors:
  - [Section 5.3] "In WIKIONTOLOGY and AG NEWS, separation is most pronounced... In IMDB, the effect persists but with narrower margins... For GOEMOTIONS, the asymmetry is weakest."
  - [Table 2] GOEMOTIONS shows non-significant asymmetry values (n.s.), while other datasets show significant differences with 95% CIs excluding zero.
  - [corpus] Related work on emotion detection (e.g., "Generating Medically-Informed Explanations for Depression Detection using LLMs") similarly notes difficulty in aligning rationales with fine-grained affective distinctions.
- Break condition: If the reference model (logistic regression) is fundamentally unsuited to certain task types (e.g., lacks capacity for compositional reasoning), observed weakness may reflect baseline inadequacy rather than task difficulty.

## Foundational Learning

- Concept: Feature importance in linear models
  - Why needed here: The framework relies on logistic regression weights as the ground-truth reference for "supporting" vs. "contradicting" features. Without understanding how weights map to class predictions, you cannot interpret coverage metrics.
  - Quick check question: For a binary classifier with positive class weight +2.5 on the feature "excellent" and negative class weight -2.5 on the same feature, is this feature supporting or contradicting for the positive class?

- Concept: TF-IDF vectorization and n-grams
  - Why needed here: The reference model uses 1–2 gram TF-IDF features. Understanding how text is converted to sparse feature vectors is essential for debugging matching failures and interpreting feature importance rankings.
  - Quick check question: If the input text is "not good at all," what 1-gram and 2-gram features would be extracted after standard preprocessing?

- Concept: Edit distance (Levenshtein distance)
  - Why needed here: One of three matching strategies relies on character-level edit distance to detect near-surface paraphrases. Understanding threshold selection and failure modes is critical for interpreting coverage gains.
  - Quick check question: What is the edit distance between "fantastic" and "fantastik"? Would this match under a threshold of δ=1?

## Architecture Onboarding

- Component map: Data Extraction -> Normalization -> Matching Engine -> Metric Computation -> Aggregation
- Critical path: 1) Train logistic regression on TF-IDF features → extract top-k weighted features per class 2) Prompt LLM → extract predicted label and free-text rationale 3) Apply three matchers → compute per-instance coverage 4) Partition by correctness → compare support vs. contradiction coverage distributions
- Design tradeoffs:
  - k=5 features per instance: Balances interpretability with coverage; may miss distributed signals in sentiment/emotion tasks
  - Logistic regression as baseline: Transparent but limited to surface lexical cues; cannot capture compositional or contextual semantics
  - Edit-distance threshold (δ): Must be empirically tuned; too permissive admits noise, too strict misses valid paraphrases
- Failure signatures:
  - Flat coverage across match/non-match partitions: Suggests LLM rationales are not using predictive features or baseline is misaligned
  - Edit-distance coverage ≈ exact coverage: Indicates δ is too restrictive or rationales avoid paraphrasing
  - High contradiction coverage in correct predictions: May indicate label leakage or feature weight misinterpretation
- First 3 experiments:
  1. Baseline validation: Replicate coverage asymmetry on a held-out slice of one dataset (e.g., AG NEWS) to confirm pipeline correctness
  2. Threshold sweep: Vary edit-distance δ (e.g., 1, 2, 3) and plot coverage gains to identify optimal per-dataset settings
  3. Feature count ablation: Test k ∈ {3, 5, 10} to assess whether coverage asymmetry is sensitive to the number of reference features

## Open Questions the Paper Calls Out

- Question: Would incorporating contextual embeddings for semantic alignment reveal additional overlap between LLM rationales and predictive features beyond what edit-distance matching captures?
  - Basis in paper: [explicit] Authors state "Edit-distance matching improves flexibility but cannot fully capture semantic equivalence (e.g., antonyms or idioms)" and suggest future work "incorporate contextual embeddings for semantic alignment."
  - Why unresolved: Edit-distance cannot distinguish semantically related terms from unrelated near-matches (e.g., "terrible" vs. "terrific" have low edit distance but opposite sentiment).
  - What evidence would resolve it: Comparing coverage metrics using contextual embedding similarity against edit-distance results across the same four datasets.

- Question: How does the choice of reference model affect the measured alignment between LLM rationales and feature importance?
  - Basis in paper: [explicit] "Future research should extend this framework to... alternative reference models" and "Logistic regression features... capture only a subset of the linguistic and semantic cues that LLMs may exploit."
  - Why unresolved: Different reference models may identify different predictive features; LLMs might align better with features from attention-based or transformer-based baselines.
  - What evidence would resolve it: Running RACE with SHAP attributions, attention weights, or transformer-based feature importance as baselines and comparing coverage patterns.

- Question: Does the support/contradiction asymmetry pattern persist when varying the number of top-k features extracted per instance?
  - Basis in paper: [inferred] The authors fix k=5 features, noting this "imposes a strong bottleneck that may miss diffuse or distributed signals."
  - Why unresolved: With larger k, more features are available for matching, potentially altering the asymmetry signal or revealing different alignment patterns in tasks like IMDB where signals may be distributed.
  - What evidence would resolve it: Systematic experiments varying k (e.g., 3, 5, 10, 20) and analyzing whether the asymmetry strengthens, weakens, or changes qualitatively across datasets.

## Limitations

- The framework's reliance on logistic regression feature weights as a ground-truth proxy introduces uncertainty if the reference model fails to capture predictive cues that are compositional or contextual.
- The edit-distance threshold (δ) is described as "empirically tuned" but not specified, leaving a critical parameter open to dataset-specific variability.
- The absence of human preference alignment or causal intervention validation means the framework measures lexical overlap rather than true reasoning faithfulness.

## Confidence

- **High Confidence**: The empirical finding that correct LLM predictions show higher coverage of supporting features and incorrect ones show elevated contradiction coverage is directly observed and consistent across multiple datasets.
- **Medium Confidence**: The interpretation that this asymmetry reflects LLM reasoning amplification of predictive vs. misleading cues depends on the assumption that logistic regression weights accurately represent true predictive signals.
- **Low Confidence**: The claim that edit-distance matching "substantially boosts" coverage while preserving asymmetry patterns is supported by reported numbers but lacks detailed threshold tuning methodology.

## Next Checks

1. Reference Model Validation: Replicate the coverage asymmetry using an alternative reference model (e.g., fine-tuned BERT) to determine if the observed patterns are specific to logistic regression's surface lexical focus or reflect a more general phenomenon in LLM-rationale alignment.

2. Edit-Distance Threshold Sensitivity: Systematically sweep δ across datasets (e.g., δ ∈ {1, 2, 3}) and report coverage gains and asymmetry preservation at each threshold to identify optimal settings and assess robustness to parameter choice.

3. Error Analysis on Low-Performing Tasks: Conduct a qualitative analysis of LLM rationales for GOEMOTIONS instances where coverage asymmetry is weakest, categorizing whether failures stem from task ambiguity, feature sparsity, or rationale quality issues.