---
ver: rpa2
title: Demographic-Agnostic Fairness without Harm
arxiv_id: '2509.24077'
source_url: https://arxiv.org/abs/2509.24077
tags:
- fairness
- group
- classifier
- sensitive
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of achieving fairness without
  harm when sensitive demographic attributes are inaccessible during model training.
  The authors propose a demographic-agnostic fairness without harm (DAFH) optimization
  algorithm that jointly learns a group classifier and decoupled classifiers for each
  group.
---

# Demographic-Agnostic Fairness without Harm

## Quick Facts
- arXiv ID: 2509.24077
- Source URL: https://arxiv.org/abs/2509.24077
- Reference count: 11
- Primary result: Achieves comparable or superior fairness without harm and accuracy metrics compared to baselines that access sensitive attributes.

## Executive Summary
This paper addresses the challenge of achieving fairness without harm when sensitive demographic attributes are inaccessible during model training. The authors propose a demographic-agnostic fairness without harm (DAFH) optimization algorithm that jointly learns a group classifier and decoupled classifiers for each group. The approach uses a differentiable objective function to maximize the probability of satisfying rationality (lower risk with assigned classifier) and envy-freeness (better performance than other groups' classifiers) without requiring sensitive attribute access. Theoretical analysis shows DAFH can match or outperform baseline methods that use sensitive attributes for group partitioning.

## Method Summary
DAFH jointly learns a group classifier θ: X → K that partitions data into K groups and K decoupled classifiers {h_k} to maximize fairness without harm. The method uses a differentiable surrogate objective that approximates the non-differentiable fairness metric, incorporating rationality and envy-freeness constraints. A KL-divergence penalty prevents group collapse. The group classifier (MLP) and decoupled classifiers (Logistic Regression) are optimized via gradient ascent. A pooled classifier h_0 is pre-trained and held fixed during DAFH optimization.

## Key Results
- DAFH achieves 96.28% probability of fairness without harm with 83.18% accuracy on the Violent dataset, outperforming the trivial partition baseline
- On the Adult dataset, DAFH achieves ~90% Prof w/o harm and ~83% accuracy, beating Trivial partition on accuracy
- DAFH shows consistent improvement across Adult, Arrest, Violent, German, and Bank datasets compared to baselines that access sensitive attributes

## Why This Works (Mechanism)

### Mechanism 1: Differentiable Lower Bound on Fairness Without Harm
- Claim: Optimizing a surrogate differentiable objective can jointly learn group partitions and group-specific classifiers that satisfy fairness without harm.
- Mechanism: The algorithm replaces the non-differentiable 0-1 loss with Sigmoid approximations and replaces hard group assignments with soft assignments via softmax, then maximizes: obj = (1/nK²) Σᵢ Σₖ (L̃ᵏᵢ - 2K π̃ᵏᵢ L̃ᵏᵢ) + λΓ(θ), where L̃ is soft loss, π̃ is soft group probability, and Γ(θ) is a KL-divergence regularizer to prevent all samples collapsing to one group.
- Core assumption: The surrogate differentiable objective correlates sufficiently with the true fairness-without-harm metric to guide optimization.
- Evidence anchors: [Section 4] "We approximate this new objective function using differentiable functions and find the optimal group classifier θ and decoupled classifiers {h_k} iteratively using gradient ascents."
- Break condition: If the Sigmoid approximation is too coarse (λ poorly tuned, or K too large for data size), optimization may stall or collapse to degenerate partitions.

### Mechanism 2: Learned Representation-Based Partitioning Captures Heterogeneity Beyond Sensitive Attributes
- Claim: A learned group classifier can capture population heterogeneity better than fixed partitions by sensitive attributes, improving both fairness-without-harm and accuracy.
- Mechanism: The group classifier θ: X → K maps features directly to groups, enabling representation-based partitioning that can capture intersections and subgroups not aligned with observed sensitive attributes. Theorem 2 bounds the performance gap between learned and attribute-based partitions by disc(P_k, P*_k).
- Core assumption: Feature space X contains sufficient signal to separate groups with heterogeneous decision boundaries.
- Evidence anchors: [Abstract] "Theoretical analysis shows DAFH can match or outperform baseline methods that use sensitive attributes for group partitioning."
- Break condition: If features are poor proxies for relevant heterogeneity, learned partitions may not align with meaningful group structure; disc(P_k, P*_k) can be large.

### Mechanism 3: Rationality and Envy-Freeness as Group Preference Guarantees
- Claim: Requiring rationality (group prefers its classifier over pooled) and envy-freeness (group prefers its classifier over others' classifiers) ensures majority preference without accuracy sacrifice.
- Mechanism: The objective directly encodes R_k(h_k) ≤ R_k(h_0) (rationality) and R_k(h_k) ≤ R_k(h_j) (envy-freeness). By maximizing the empirical probability these hold, each group collectively prefers its assigned model.
- Core assumption: Risk reduction correlates with group-level preference; individual-level violations are acceptable if majority prefer their assignment.
- Evidence anchors: [Section 3, Definition 1] Formal definition of fairness without harm as rationality plus envy-freeness.
- Break condition: If groups have highly overlapping risk profiles under all classifiers, envy-freeness may be trivially satisfied without meaningful improvement; or if pooled classifier is already optimal for all groups, rationality provides no gain.

## Foundational Learning

- Concept: Preference-based fairness vs parity-based fairness
  - Why needed here: The paper explicitly rejects parity-based fairness due to accuracy trade-offs; understanding this distinction explains why DAFH optimizes rationality/envy-freeness instead of equalizing outcomes.
  - Quick check question: Can you explain why equalizing false positive rates across groups might harm accuracy for all groups?

- Concept: Decoupled classifiers and group-specific risk
  - Why needed here: DAFH trains K separate classifiers, each optimized for its assigned group; understanding how group-specific risk R_k(h) differs from pooled risk R(h) is essential.
  - Quick check question: If you have two groups with different optimal decision boundaries, why might a single pooled classifier underperform both?

- Concept: Differentiable approximations for combinatorial objectives
  - Why needed here: The original objective uses indicator functions and 0-1 loss; the paper substitutes soft assignments and Sigmoid-based loss to enable gradient-based optimization.
  - Quick check question: Why does replacing I(θ(x)=k) with softmax probability enable backpropagation?

## Architecture Onboarding

- Component map:
  - Group classifier θ: MLP with softmax output over K groups; assigns soft group probabilities π̃ᵏᵢ.
  - Decoupled classifiers {h_k}: K separate logistic regression models (or other H_D); each predicts Y from X.
  - Pooled classifier h_0: Pre-trained baseline on full data; fixed during DAFH optimization.
  - Objective fobj: Differentiable combination of rationality term, envy-freeness term, and KL regularizer λΓ(θ).

- Critical path:
  1. Pre-train pooled classifier h_0 on full dataset.
  2. Initialize group classifier θ and decoupled classifiers {h_k}.
  3. For each batch: compute soft group assignments π̃, soft losses L̃ for each h_k.
  4. Compute fobj (Eq. 5) including KL penalty.
  5. Backpropagate to update θ and all {h_k} via gradient ascent.
  6. Repeat until convergence; at inference, assign sample to argmax_k θ(x)[k] and apply h_k.

- Design tradeoffs:
  - K (number of groups): Higher K can capture more heterogeneity but risks overfitting and small group sizes; Table 6 shows K=2,3,4 all viable depending on dataset.
  - λ (regularization strength): Too low → group collapse (all samples to one group); too high → uniform group assignments, poor specialization.
  - Group classifier capacity: MLP vs simpler models; higher capacity captures more complex partitions but increases overfitting risk.

- Failure signatures:
  - Group collapse: >90% of samples assigned to single group; likely λ too low or learning rate for θ too high.
  - No convergence: Oscillating fobj; check learning rates (separate for θ vs h_k recommended per Table 3).
  - Worse than pooled: Prof w/o harm < 50%; indicates group partitions not capturing useful heterogeneity.

- First 3 experiments:
  1. Reproduce Table 4 on Adult dataset with K=2; verify DAFH achieves ~90% Prof w/o harm and ~83% accuracy, beating Trivial partition on accuracy.
  2. Ablation on λ: Run with λ=0, 1, 10, 100 on Violent dataset; observe group size distribution and Prof w/o harm to confirm regularization necessity.
  3. Vary K: Run K=2,3,4 on Bank dataset; compare Prof w/o harm and accuracy to identify optimal K for this dataset (Table 6 suggests K=2 optimal).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal number of decoupled groups (K) be determined automatically without relying on grid search?
- Basis in paper: [inferred] The experimental setup treats K as a hyperparameter (testing K=2,3,4 in Table 6), and the convergence analysis in Figure 2 is tied to specific values of K, but no theoretical guidance is provided for selecting K a priori.
- Why unresolved: The method relies on validation metrics to choose K, lacking a principled mechanism to detect when the data heterogeneity has been fully captured or if over-partitioning occurs.
- What evidence would resolve it: An adaptive algorithm that converges to an optimal K based on data distribution properties, or a theoretical bound linking K to the objective function's stability.

### Open Question 2
- Question: Do the theoretical guarantees for DAFH hold for non-symmetric loss functions that do not obey the triangle inequality?
- Basis in paper: [explicit] Theorem 2 states: "Assume the loss function ℓ is symmetric and obeys triangle inequality... then we have: [bound]".
- Why unresolved: The feasibility proof relies on these specific properties of the loss function to bound the performance gap between learned and sensitive-attribute-based partitions. It is unclear if the bounds degrade or vanish for asymmetric losses common in domains like healthcare or fraud detection.
- What evidence would resolve it: A theoretical extension of Theorem 2 to non-symmetric losses, or empirical evaluations showing the feasibility constraint holds for losses like Weighted Cross-Entropy.

### Open Question 3
- Question: To what extent are individuals at the boundary of learned groups exposed to potential harm despite the group-level fairness guarantees?
- Basis in paper: [explicit] Section 3 states regarding Definition 1: "It is worth noting that our definition does not require every individual to prefer their assigned classifier."
- Why unresolved: The optimization maximizes the aggregate probability of envy-freeness (majority preference), but explicitly does not ensure protection for every individual, potentially masking harm to outliers or minority subgroups within the learned partitions.
- What evidence would resolve it: A per-individual worst-case analysis of "envy" or empirical results showing the distribution of utility for individuals near the decision boundaries of the group classifier θ.

## Limitations
- Surrogate Objective Validity: The differentiable surrogate objective approximates non-differentiable fairness metrics, but the correlation strength between this surrogate and true fairness-without-harm metrics is not empirically validated.
- Dataset Specificity: Results are primarily demonstrated on tabular datasets with binary classification; performance on multi-class or regression tasks and non-tabular domains remains unexplored.
- Hyperparameter Sensitivity: The method shows sensitivity to λ (regularization) and K (number of groups), with no principled mechanism for automatic hyperparameter selection.

## Confidence
- High Confidence: The core algorithmic framework (differentiable objective, group classifier, decoupled classifiers) is clearly specified and reproducible. Empirical results show consistent improvement over baselines across multiple datasets.
- Medium Confidence: Theoretical analysis (Theorem 2) provides bounds under idealized assumptions, but real-world performance may deviate significantly. The learned partitioning mechanism's ability to capture meaningful heterogeneity beyond sensitive attributes is plausible but not conclusively proven.
- Low Confidence: Claims about the surrogate objective's effectiveness and the method's generalizability to non-tabular data or multi-class settings lack supporting evidence.

## Next Checks
1. **Surrogate Correlation Study:** Systematically measure the correlation between the differentiable objective value and the true fairness-without-harm metric across different λ values and datasets. Identify if there's a regime where optimization drives both metrics in tandem.
2. **Generalization to Multi-Class Tasks:** Apply DAFH to a multi-class tabular dataset (e.g., MNIST with demographic proxies) and evaluate whether the fairness-without-harm guarantee extends meaningfully beyond binary classification.
3. **Ablation on Group Classifier Capacity:** Compare DAFH performance using different group classifier architectures (MLP vs. linear) on a challenging dataset to quantify how much of the improvement stems from representation learning versus the fairness objective structure.