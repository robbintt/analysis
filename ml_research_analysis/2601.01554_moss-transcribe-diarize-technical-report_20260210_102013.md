---
ver: rpa2
title: MOSS Transcribe Diarize Technical Report
arxiv_id: '2601.01554'
source_url: https://arxiv.org/abs/2601.01554
tags:
- speaker
- transcribe
- audio
- diarize
- moss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MOSS Transcribe Diarize is a unified multimodal large language\
  \ model for speaker-attributed, time-stamped transcription (SATS) that jointly performs\
  \ word recognition, speaker attribution, and timestamp prediction in a single forward\
  \ pass. Unlike modular ASR\u2013diarization pipelines, it eliminates cross-stage\
  \ error propagation by integrating an audio encoder with a projection layer that\
  \ maps speaker embeddings into the LLM\u2019s feature space."
---

# MOSS Transcribe Diarize Technical Report

## Quick Facts
- arXiv ID: 2601.01554
- Source URL: https://arxiv.org/abs/2601.01554
- Reference count: 26
- Primary result: Unified multimodal LLM achieving lowest cpCER and Δcp on AISHELL-4, Podcast, and Movies benchmarks

## Executive Summary
MOSS Transcribe Diarize is a unified multimodal large language model that jointly performs speaker-attributed, time-stamped transcription (SATS) in a single forward pass. By integrating an audio encoder with a projection layer that maps speaker embeddings into the LLM's feature space, it eliminates cross-stage error propagation common in modular ASR-diarization pipelines. The model supports up to 90-minute inputs with a 128k-token context window, enabling long-range speaker memory preservation.

Evaluated on three diverse benchmarks, MOSS Transcribe Diarize consistently achieves the best overall performance in cpCER and Δcp across all datasets. On AISHELL-4, it attains CER of 15.43% and cpCER of 20.04%, substantially outperforming commercial systems. The consistently small Δcp values demonstrate robust speaker attribution accuracy, and the model remains fully operational on hour-scale audio where other general-purpose multimodal models fail due to input-length or format constraints.

## Method Summary
MOSS Transcribe Diarize integrates an audio encoder with a projection layer that maps speaker embeddings into the LLM's feature space, enabling joint word recognition, speaker attribution, and timestamp prediction in a single forward pass. The model is trained on extensive real-world conversational audio plus simulated mixtures to handle overlap, turn-taking, and acoustic variability. With support for up to 90-minute inputs and a 128k-token context window, it preserves long-range speaker memory while eliminating the error propagation common in modular ASR-diarization pipelines.

## Key Results
- On AISHELL-4: CER 15.43%, cpCER 20.04%, Δcp 4.61% (outperforming Doubao and ElevenLabs)
- On Podcast: CER 4.46%, cpCER 6.97%, lowest Δcp
- On Movies: CER 7.50%, cpCER 13.36%, lowest Δcp

## Why This Works (Mechanism)
The model's unified architecture integrates audio encoding directly into the LLM pipeline through a projection layer that maps speaker embeddings into the model's feature space. This eliminates the need for separate ASR and diarization stages, preventing error accumulation. The 128k-token context window enables long-range speaker memory, crucial for maintaining attribution accuracy over extended conversations. Training on both real conversational data and simulated mixtures ensures robustness to overlapping speech and diverse acoustic conditions.

## Foundational Learning
**Speaker-attributed transcription (SATS)**: Joint recognition of words, speaker identities, and timestamps in a single pass. Needed because separate ASR and diarization stages accumulate errors. Quick check: Compare cpCER and Δcp metrics between unified and modular approaches.

**Multimodal LLM integration**: Mapping audio features into LLM's token space via projection layer. Needed to leverage LLM's language understanding while processing audio. Quick check: Validate speaker embedding projection preserves discriminative information.

**Context window scaling**: 128k-token capacity for long-form audio processing. Needed to maintain speaker memory across 90-minute conversations. Quick check: Measure speaker attribution degradation over time with varying context lengths.

**Data simulation for training**: Generating synthetic overlapping speech mixtures. Needed because real conversational datasets are limited. Quick check: Compare model performance on simulated vs. real overlap scenarios.

## Architecture Onboarding

**Component Map**
Audio Encoder -> Projection Layer -> LLM Token Space -> Joint SATS Head

**Critical Path**
Input audio → Audio Encoder → Speaker embedding extraction → Projection Layer (speaker→LLM space) → LLM processing → SATS output (words + speakers + timestamps)

**Design Tradeoffs**
- Unified vs. modular: Eliminates error propagation but requires more complex training
- Large context window: Preserves long-range memory but increases computational cost
- Simulation vs. real data: Enables overlap handling but may not capture all real-world dynamics

**Failure Signatures**
- High Δcp indicates speaker attribution errors despite good transcription
- Performance degradation over time suggests context window limitations
- Poor handling of overlapping speech reveals simulation-real gap

**3 First Experiments**
1. Test model on 90-minute input with single speaker to verify context window functionality
2. Evaluate speaker attribution accuracy on overlapping speech segments
3. Compare cpCER vs. standard CER to quantify diarization impact

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on internal benchmarking without third-party replication
- Claims of "consistently best performance" lack comparison to specialized open-source diarization+ASR baselines
- 90-minute and 128k-token context claims are stated but not stress-tested across diverse acoustic conditions

## Confidence

**High confidence** in architectural innovation and benchmark performance metrics (cpCER, Δcp, CER).

**Medium confidence** in generalization claims for 90-minute inputs and 128k-token context, as these are stated but not empirically validated beyond reported datasets.

**Low confidence** in comparative advantage over specialized open-source systems, due to absence of direct baseline comparisons.

## Next Checks
1. Replicate evaluations on held-out test sets not used in training or hyperparameter tuning
2. Compare against leading open-source E2E SATS models (Whisper diarization conditioning, TagSpeech) under identical conditions
3. Conduct ablation studies on 128k-token context window impact, including speaker confusion pattern analysis over time