---
ver: rpa2
title: 'Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph
  Generative Models: A Benchmarking Study'
arxiv_id: '2512.18524'
source_url: https://arxiv.org/abs/2512.18524
tags:
- uni00000013
- uni00000011
- uni00000048
- graph
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks six GNN architectures for classifying synthetic
  graph generative models, addressing the need for systematic, comparative evaluation
  of diverse models on structurally similar synthetic graphs. A hybrid approach integrating
  handcrafted graph-theoretic features with message-passing mechanisms was employed.
---

# Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph Generative Models: A Benchmarking Study

## Quick Facts
- **arXiv ID:** 2512.18524
- **Source URL:** https://arxiv.org/abs/2512.18524
- **Reference count:** 37
- **Primary result:** GraphSAGE and GTN achieve 98.5% accuracy in classifying 5 synthetic graph families

## Executive Summary
This study presents a comprehensive benchmarking framework for classifying synthetic graph generative models using six GNN architectures. The research introduces a hybrid approach that combines handcrafted graph-theoretic features with message-passing mechanisms, addressing the need for systematic evaluation of diverse models on structurally similar synthetic graphs. Through extensive experimentation on 2000 graphs from five generative families (Erdős–Rényi, Watts–Strogatz, Barabási–Albert, Holme-Kim, and Stochastic Block Model), the study demonstrates that GraphSAGE and GTN achieve the highest classification accuracy of 98.5%, while also providing insights into the strengths and limitations of different architectural approaches for this specific task.

## Method Summary
The study employs a hybrid GNN architecture that integrates handcrafted graph-theoretic features with message-passing mechanisms. The dataset consists of 2000 synthetic graphs (400 per class) from five generative families, with node counts ranging from 5000 to 10000. Three node-level features (Eigenvector Centrality, Degree, Closeness Centrality) and three graph-level features (Degree Variance, Clustering, Assortativity) are extracted using igraph and networkx. Feature selection is performed using a Random Forest-based pipeline, and hyperparameter optimization is conducted using Optuna across 50 trials. The GNN architectures (GCN, GAT, GATv2, GIN, GraphSAGE, GTN) follow a shared scaffold of two convolutional layers, BatchNorm, Dropout, Global Mean Pooling, concatenated graph features, and a linear output layer. Training uses cross-entropy loss with Adam optimizer, ReduceLROnPlateau scheduling, and early stopping.

## Key Results
- GraphSAGE and GTN achieve the highest classification accuracy of 98.5% on the 5-class graph classification task
- GCN and GIN also perform well, while GAT-based models lag due to limitations in capturing global structures
- Strong class separation is evidenced by t-SNE and UMAP visualizations of the learned embeddings
- The SVM baseline confirms the importance of message-passing functionality for performance gains

## Why This Works (Mechanism)
The hybrid approach succeeds by combining the expressive power of message-passing GNNs with domain-informed handcrafted features that capture structural properties difficult for pure GNNs to learn efficiently. The handcrafted features provide immediate access to global graph statistics and centrality measures that would require multiple message-passing iterations to approximate. This combination allows the model to leverage both local neighborhood information through convolutions and global structural patterns through pre-computed features, resulting in more discriminative representations for classifying synthetic graph families with distinct structural signatures.

## Foundational Learning
- **Graph Neural Networks:** Neural networks designed to operate on graph-structured data by propagating information through edges
  - *Why needed:* Standard neural networks cannot directly process irregular graph structures
  - *Quick check:* Can you explain how message passing works in a single GNN layer?
- **Graph Generative Models:** Algorithms that produce synthetic graphs with specific structural properties (ER, WS, BA, etc.)
  - *Why needed:* Provide controlled testbeds for evaluating graph classification methods
  - *Quick check:* What structural differences distinguish Erdős–Rényi from Barabási–Albert graphs?
- **Feature Engineering for Graphs:** Selection and computation of node and graph-level attributes that capture structural properties
  - *Why needed:* Provides interpretable signals that complement learned representations
  - *Quick check:* Why might eigenvector centrality be more informative than degree for some graph families?

## Architecture Onboarding

**Component Map:** Data Generation -> Feature Extraction -> Model Training -> Evaluation -> Visualization

**Critical Path:** Graph Generation → Feature Computation → GNN Forward Pass → Pooling → Concatenation → Classification

**Design Tradeoffs:** The hybrid architecture balances computational efficiency (pre-computed features) against representational flexibility (learned features). Using handcrafted features reduces the need for deep GNNs to learn basic structural properties, but may limit the model's ability to discover novel patterns. The choice of lightweight features enables scalability to large graphs but may miss subtle structural differences captured by more expensive descriptors.

**Failure Signatures:** GAT models show premature convergence and low accuracy (82%), indicating architectural limitations for this task. GTN may experience convergence issues on very large graphs due to attention mechanism complexity. Dense graphs approaching 110k edges may cause memory bottlenecks during training.

**First Experiments:**
1. Train a simple GCN baseline on the synthetic dataset to establish performance floor
2. Implement the hybrid feature-concatenation approach and verify accuracy improvement
3. Compare GAT performance against GCN to confirm architectural limitations for this task

## Open Questions the Paper Calls Out
- Does the hybrid GNN approach generalize effectively to larger or real-world graphs?
- Can computationally expensive features like spectral or topological descriptors improve discrimination over the lightweight features used?
- Do architecture-specific hyperparameter search spaces yield better outcomes than the combined search space utilized?

## Limitations
- Focus on synthetic, benchmark graph structures may not generalize to real-world datasets with less distinct properties
- Performance gains from feature concatenation versus pure GNN approaches were not rigorously isolated
- GTN architecture's convergence issues suggest potential scalability concerns for large-scale applications

## Confidence
- **High confidence** in reported classification accuracies and feature selection pipeline effectiveness
- **Medium confidence** in architectural comparisons and performance differences between models
- **Low confidence** in claims about GTN's theoretical advantages or broader applicability due to convergence failure

## Next Checks
1. Replicate experiments on a real-world graph dataset (e.g., social networks, biological networks) to test generalization beyond synthetic structures
2. Conduct ablation studies to isolate the contribution of handcrafted features versus message-passing layers in the hybrid model
3. Test alternative attention mechanisms (e.g., GATv2, Transformer-based GNNs) to verify whether GTN's convergence issues are architecture-specific or indicative of broader scalability challenges in attention-based models for large graphs