---
ver: rpa2
title: 'FocusMed: A Large Language Model-based Framework for Enhancing Medical Question
  Summarization with Focus Identification'
arxiv_id: '2510.04671'
source_url: https://arxiv.org/abs/2510.04671
tags:
- focus
- question
- medical
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FocusMed tackles the problem of generating concise, accurate medical
  question summaries by leveraging large language models (LLMs). The framework first
  extracts core question focuses from consumer health questions using tailored prompts
  and faithfulness validation, then constructs an enhanced dataset for fine-tuning.
---

# FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification

## Quick Facts
- arXiv ID: 2510.04671
- Source URL: https://arxiv.org/abs/2510.04671
- Reference count: 33
- Primary result: Improves ROUGE-L by 7.3% and 6.7% over previous best methods on two widely-used datasets.

## Executive Summary
FocusMed is a framework that improves medical question summarization by first extracting faithful question focuses (medications and symptoms) using structured LLM prompts, then training on focus-augmented data with QLoRA, and finally selecting the best summary from multiple model outputs via multi-dimensional quality scoring. The approach significantly reduces hallucination and improves ROUGE-L scores on MEDIQA and MeqSum datasets, demonstrating the importance of focus extraction and selection in generating concise, accurate summaries.

## Method Summary
FocusMed operates in four stages: (1) LLM-based focus extraction using JSON+CoT prompts to identify up to 2 medications and 2 symptoms, validated by TextRank and semantic similarity; (2) construction of an enhanced dataset by prepending/appending validated focuses to original consumer health questions; (3) fine-tuning Qwen2.5-7B and LLaMA3.1-8B models with QLoRA using the enhanced dataset; and (4) generating multiple summary candidates and selecting the best via multi-dimensional scoring (faithfulness, conciseness, coverage) using DeepSeek-R1 for atomic fact decomposition.

## Key Results
- Achieves state-of-the-art ROUGE-L scores: 0.386 (MEDIQA) and 0.413 (MeqSum), improving over previous best by 7.3% and 6.7%.
- Multi-dimensional selection significantly improves summary quality compared to single-model outputs.
- Ablation studies show focus extraction and selection are critical for performance gains.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit focus extraction before summarization reduces hallucination and improves question alignment.
- Mechanism: A structured prompt (JSON+CoT format) guides an LLM to extract medications and symptoms (max 2 per category). TextRank extracts key phrases from the output; semantic similarity against original-text noun phrases validates faithfulness (threshold 0.8–0.9). Only validated foci augment the training input.
- Core assumption: LLMs can reliably identify medical foci when constrained to a small, structured entity set, and similarity-based validation is a sufficient proxy for factual grounding.
- Evidence anchors:
  - [abstract] "First, a prompt template is designed to drive the LLMs to extract the core focus from the CHQs that is faithful to the original text."
  - [section II-B] Algorithm 1 shows the full workflow: extract focus, validate via TextRank + similarity threshold, regenerate if similarity < τ, then build augmented dataset.
  - [corpus] Related work on faithful CHQ summarization (arXiv:2511.10768) similarly combines TextRank + NER + LLMs for faithfulness, suggesting convergent design, but no direct citation comparison is available.
- Break condition: If CHQs contain foci beyond medications/symptoms (e.g., procedures, temporal patterns), the 2-entity-per-category constraint may truncate critical information.

### Mechanism 2
- Claim: Training on focus-enhanced inputs improves the model's internal representation of question intent.
- Mechanism: The extracted focus K is concatenated with instruction I and consumer question C as input x = (I, C, K). The model is fine-tuned via QLoRA on this enhanced dataset, optimizing negative log-likelihood against reference summaries y.
- Core assumption: The focus extraction is accurate; noisy or incorrect foci would misalign training signals.
- Evidence anchors:
  - [section II-C] Eq. (2) defines the SFT loss over the silver-standard dataset with focus-augmented inputs.
  - [section III-B2] Table III shows ROUGE-L gains from 0.334 → 0.347 (Qwen) and 0.339 → 0.364 (LLaMA) when focus extraction is added.
  - [corpus] No direct corpus evidence on focus-augmented training for MQS; closest is contrastive learning approaches (QFCL) using focus for hard negatives, a different mechanism.
- Break condition: If the focus extraction model has systematic biases (e.g., over-extracting medications, under-extracting symptoms), fine-tuning will amplify those biases.

### Mechanism 3
- Claim: Multi-dimensional selection over multiple model outputs yields more reliable summaries than any single model.
- Mechanism: Four model combinations (Qwen/Qwen, Qwen/LLaMA, LLaMA/Qwen, LLaMA/LLaMA for extraction/finetuning) generate candidate summaries. DeepSeek-R1 decomposes each into atomic facts; faithfulness = fraction entailed by source, coverage = fraction of source atomic facts captured, conciseness = key-phrase-length / total-length. Weighted sum (α·F + β·C + γ·Cov) selects the winner.
- Core assumption: Atomic fact decomposition via LLM is consistent and unbiased; the chosen weights generalize beyond the development set.
- Evidence anchors:
  - [section II-D] Eqs. (3)–(6) define faithfulness, conciseness, coverage, and final score.
  - [section III-B3] Table IV shows FocusMed (with selection) achieving 0.386 ROUGE-L vs. 0.364 without selection.
  - [corpus] No corpus papers evaluate multi-dimensional selection; MEDAL uses post-processing for hallucination correction, a different approach.
- Break condition: If all candidate models share a common failure mode (e.g., missing temporal information), selection cannot recover.

## Foundational Learning

- Concept: **QLoRA (Quantized Low-Rank Adaptation)**
  - Why needed here: Enables fine-tuning 7B–8B parameter models on a single 24GB GPU while preserving expressiveness.
  - Quick check question: Can you explain why low-rank adapters reduce memory vs. full fine-tuning?

- Concept: **TextRank for Key Phrase Extraction**
  - Why needed here: Used in both focus validation (extracting phrases from LLM output) and conciseness scoring.
  - Quick check question: How does TextRank differ from TF-IDF in selecting important phrases?

- Concept: **Atomic Fact Decomposition for Faithfulness Evaluation**
  - Why needed here: Core to the multi-dimensional selection mechanism; enables fine-grained consistency checks.
  - Quick check question: What makes an "atomic fact" suitable for entailment checking vs. a compound claim?

## Architecture Onboarding

- Component map: LLM focus extraction -> TextRank validation -> enhanced dataset -> QLoRA fine-tuning -> multi-model generation -> DeepSeek-R1 decomposition -> multi-dimensional selection
- Critical path: Extraction quality → enhanced dataset quality → fine-tuned model focus sensitivity → selection pool diversity. Failures compound forward.
- Design tradeoffs:
  - 2 entities per category (medications, symptoms) balances recall vs. noise but may miss other focus types.
  - Threshold 0.8–0.9 for similarity validation trades off precision (higher threshold) vs. coverage (lower threshold).
  - Weights (MEDIQA: α=0.6, β=0.1, γ=0.3; MeqSum: 0.3/0.4/0.3) are dataset-specific via grid search; not guaranteed to transfer.
- Failure signatures:
  - Over-focus on medications/symptoms, missing temporal/dosage details: See Figure 6 (case study)—model uses "prolonged" instead of "13 days."
  - Extraction model size sensitivity: Figure 4 shows performance improves with larger extraction models (1.5B → 7B → 14B).
  - Selection degeneracy: If all candidates miss the same focus, selection cannot help.
- First 3 experiments:
  1. Ablate focus extraction: Train on original CHQ-FAQ pairs without focus augmentation; expect ROUGE-L drop (~0.03–0.04 per Table III).
  2. Vary extraction model size: Use Qwen2.5-1.5B/7B/14B for extraction, fixed Qwen2.5-7B for fine-tuning; plot ROUGE-L vs. extraction model size (replicate Figure 4).
  3. Stress-test numerical sensitivity: Create a synthetic test set with critical temporal/dosage numbers; measure recall of numeric information in final summaries vs. baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be optimized to better preserve critical numerical and temporal details in medical summaries?
- Basis in paper: [explicit] The Conclusion states, "In the future, we will focus on further optimizing the model’s handling of details and enhancing its sensitivity to key information."
- Why unresolved: The Discussion section notes that current LLMs struggle with low sensitivity to numerical data, often generalizing specific time spans (e.g., "13 days") to vague terms like "prolonged," which hinders precise diagnosis.
- What evidence would resolve it: A modified architecture or loss function that specifically penalizes the omission or generalization of numerical entities, validated on a dataset rich in temporal data.

### Open Question 2
- Question: Does restricting question focus extraction to only "medications" and "symptoms" limit the framework's applicability to broader medical queries?
- Basis in paper: [inferred] Section II-B states the instruction design limits extraction to these two entity types with a "maximum limit of two entities per category."
- Why unresolved: While the authors found this optimal for current datasets, complex consumer health questions may hinge on other entity types (e.g., procedures, body parts, or environmental factors) that the model is currently instructed to ignore.
- What evidence would resolve it: An ablation study allowing for a wider range of entity types in the extraction phase to determine if summary coverage improves for complex, non-symptom/medication queries.

### Open Question 3
- Question: How does the computational overhead of the multi-dimensional selection mechanism impact the feasibility of real-time clinical deployment?
- Basis in paper: [inferred] The method requires generating outputs from four different model combinations and evaluating them using a separate LLM (DeepSeek-R1) before selection.
- Why unresolved: The paper focuses on accuracy metrics (ROUGE, BERTScore) but does not analyze the latency or computational cost of running multiple fine-tuned models and a validator in sequence.
- What evidence would resolve it: Latency benchmarks and FLOP measurements comparing the full FocusMed pipeline against single-pass baseline models.

## Limitations
- The focus extraction stage is limited to medications and symptoms only, potentially missing other critical focus types like procedures or temporal information.
- The framework's performance depends heavily on the accuracy of the focus extraction stage; errors here propagate through the entire pipeline.
- The selection mechanism requires generating and evaluating four different model outputs plus using DeepSeek-R1, increasing computational overhead.

## Confidence
- **High Confidence**: ROUGE-L gains (+7.3% MEDIQA, +6.7% MeqSum) and multi-dimensional selection improving summary quality are well-supported by ablation studies and tables.
- **Medium Confidence**: The claim that focus extraction reduces hallucination is plausible given faithfulness validation, but lacks direct hallucination-specific metrics (e.g., entity-level consistency) or robustness to extraction errors.
- **Low Confidence**: Generalizability of the framework to questions with foci beyond medications/symptoms or to other languages/corpora is untested.

## Next Checks
1. **Ablate focus extraction errors**: Inject controlled noise into extracted foci (e.g., wrong medications) and measure summary quality degradation to quantify dependency on extraction accuracy.
2. **Test numeric/temporal retention**: Create a synthetic test set with critical temporal/dosage numbers; measure recall of these details in final summaries versus gold references.
3. **Probe selection stability**: Vary the similarity threshold (0.7–0.95) and selection weights (α,β,γ) to map sensitivity and identify stable operating points across datasets.