---
ver: rpa2
title: 'VERITAS: Leveraging Vision Priors and Expert Fusion to Improve Multimodal
  Data'
arxiv_id: '2510.15317'
source_url: https://arxiv.org/abs/2510.15317
tags:
- data
- arxiv
- critic
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VERITAS addresses the problem of low-quality multimodal training
  data, which often contains factual errors and hallucinations due to inadequate visual
  perception by current large multimodal models. The proposed method integrates vision
  priors from specialized models (RAM++ and PP-OCRv4) with assessments from three
  state-of-the-art LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) using domain-aware
  statistical fusion with James-Stein shrinkage.
---

# VERITAS: Leveraging Vision Priors and Expert Fusion to Improve Multimodal Data

## Quick Facts
- **arXiv ID:** 2510.15317
- **Source URL:** https://arxiv.org/abs/2510.15317
- **Authors:** Tingqiao Xu; Ziru Zeng; Jiayu Chen
- **Reference count:** 16
- **Primary result:** +7.4 average accuracy improvement over raw data using vision priors, statistical fusion, and GRPO-trained critic

## Executive Summary
VERITAS addresses the problem of low-quality multimodal training data, which often contains factual errors and hallucinations due to inadequate visual perception by current large multimodal models. The proposed method integrates vision priors from specialized models (RAM++ and PP-OCRv4) with assessments from three state-of-the-art LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) using domain-aware statistical fusion with James-Stein shrinkage. A lightweight GRPO-trained critic model distills this ensemble's judgments at lower cost, while self-refinement generates and selects improved answers. VERITAS consistently improves model performance across six multimodal benchmarks, particularly in text-rich and fine-grained reasoning tasks. The GRPO critic achieves near-GPT-4o ranking fidelity (Kendall's tau 0.71) while being two orders of magnitude more efficient.

## Method Summary
VERITAS improves multimodal SFT data quality through a four-stage pipeline: (1) Vision priors extracted using RAM++ for object/scene tags and PP-OCRv4 for text recognition; (2) Three expert LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) score and critique each sample on [0,5] scale with domain-aware James-Stein shrinkage fusion; (3) GRPO training of a lightweight critic model on Qwen2-VL-7B-Instruct, starting with cold start SFT on 6K samples, then optimizing with R_acc and R_fmt rewards; (4) Expert rewrites generated and best candidates selected by GRPO critic for downstream training. The approach processes 95,955 samples from 7 public datasets and evaluates on 6 benchmarks (MME, OCR-VQA, MM-Vet, MathVista, MMT-bench, POPE).

## Key Results
- +7.4 average accuracy improvement over raw data across six multimodal benchmarks
- GRPO critic achieves Kendall's tau 0.71 vs human judgments, approaching GPT-4o performance (0.711) at two orders of magnitude lower cost
- Largest gains in text-rich tasks (+10.5 OCR-VQA, +12.6 POPE) and fine-grained reasoning (+7.6 MathVista)
- Vision priors (RAM++ tags, PP-OCRv4 text) consistently improve downstream performance

## Why This Works (Mechanism)
VERITAS addresses multimodal hallucination by combining specialized vision priors with calibrated expert critiques. The James-Stein shrinkage fusion reduces expert bias while the GRPO critic learns to distill ensemble judgments efficiently. Self-refinement with expert rewrites allows correction of errors that individual models miss. The approach leverages domain-specific expertise where each model excels—vision models for perception, LMMs for reasoning—creating a comprehensive quality assessment pipeline that outperforms any single model.

## Foundational Learning
- **James-Stein shrinkage:** Statistical method to improve estimation by combining multiple expert predictions while accounting for individual variances; needed for domain-aware score calibration
- **GRPO training:** Group Relative Policy Optimization for RLHF; needed to train efficient critic without human annotations
- **Vision priors:** Specialized models (RAM++, PP-OCRv4) provide reliable object/scene and text recognition; needed to ground multimodal understanding
- **Domain-aware fusion:** Separate score normalization and fusion per task domain; needed to handle varying expert competencies across benchmarks
- **Self-refinement:** Generating multiple answer candidates and selecting best through critique; needed to correct hallucinated responses

## Architecture Onboarding
**Component map:** Vision encoders (RAM++, PP-OCRv4) -> Expert LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) -> James-Stein fusion -> GRPO critic -> Self-refinement -> Downstream model

**Critical path:** Expert critique → Statistical fusion → GRPO critic training → Answer refinement

**Design tradeoffs:** Uses closed-source expert LMMs for high-quality critiques vs computational cost; lightweight GRPO critic vs full ensemble efficiency; specialized vision priors vs general multimodal understanding

**Failure signatures:** GRPO training instability (check reward normalization), expert score miscalibration (verify z-normalization per domain), high API costs (budget ~300K expert calls)

**First experiments:**
1. Validate vision priors extraction and expert score distributions on small validation set
2. Test James-Stein fusion implementation against equal weighting baseline
3. Train GRPO critic on 1K samples and measure ranking fidelity vs human judgments

## Open Questions the Paper Calls Out
None specified in the paper text.

## Limitations
- Missing complete prompt templates for expert critique and rewrite generation
- GRPO training procedure lacks full specification (cold start data selection, loss function details)
- Heavy reliance on closed-source expert LMMs makes replication costly and potentially impractical
- James-Stein shrinkage hyperparameters (ε, λ) require tuning without clear guidance

## Confidence
- Statistical fusion methodology: High
- GRPO training approach: Medium (procedure details incomplete)
- Overall performance claims: Medium (due to missing implementation details)
- Cost efficiency claims: Medium (depends on exact API usage patterns)

## Next Checks
1. Implement vision priors and James-Stein fusion pipeline, validate against reported expert score distributions
2. Reconstruct likely prompt templates and test multi-expert critique scoring on small validation set
3. Conduct ablation study comparing full VERITAS pipeline against simpler fusion approaches on one downstream benchmark