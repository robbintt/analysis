---
ver: rpa2
title: 'ALIGN: Word Association Learning for Cultural Alignment in Large Language
  Models'
arxiv_id: '2508.13426'
source_url: https://arxiv.org/abs/2508.13426
tags:
- cultural
- word
- associations
- vanilla
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a method for improving cultural alignment\
  \ in large language models (LLMs) by fine-tuning on native speakers\u2019 word association\
  \ norms. The approach leverages cognitive psychology findings that such associations\
  \ capture implicit cultural knowledge, using datasets from native speakers in the\
  \ US and China to train models via supervised fine-tuning and preference optimization."
---

# ALIGN: Word Association Learning for Cultural Alignment in Large Language Models

## Quick Facts
- **arXiv ID:** 2508.13426
- **Source URL:** https://arxiv.org/abs/2508.13426
- **Reference count:** 40
- **Primary result:** Fine-tuning LLMs on native speakers' word associations significantly improves cultural alignment, with 7–8B models matching or exceeding vanilla 70B baselines.

## Executive Summary
This paper introduces ALIGN, a method for improving cultural alignment in LLMs by fine-tuning on native speakers' word association norms. The approach leverages cognitive psychology findings that such associations capture implicit cultural knowledge, using datasets from native speakers in the US and China to train models via supervised fine-tuning and preference optimization. Evaluation across two tiers—lexical association generation and cultural value alignment using the World Values Survey—shows significant improvements in lexical alignment (16–20% English, 43–165% Mandarin on Precision@5) and higher-level cultural value shifts. On a subset of questions where US and Chinese respondents diverge most, fine-tuned models notably improve alignment with Chinese values. Remarkably, trained 7–8B models match or exceed vanilla 70B baselines, demonstrating that a few million culture-grounded associations can achieve value alignment without expensive retraining.

## Method Summary
The method trains LLMs on native speakers' word association norms from SWOW.US (12K cues, 3.6M US responses) and SWOW.ZH (10K cues, 2M Chinese responses), split 80/10/10 by cue. Two training approaches are used: Supervised Fine-Tuning (SFT) on word association generation with LoRA fine-tuning, and Preference Optimization (PPO) on ranking tasks with Spearman correlation rewards. Models are evaluated intrinsically on word association generation (Precision@K, Spearman ρ) and extrinsically on cultural value alignment using World Values Survey responses, measured via Jensen-Shannon Distance and Earth Mover's Distance. The approach transfers lexical associations to higher-level value judgments, with SFT excelling on Chinese values and PPO on US values.

## Key Results
- SFT improves lexical alignment by 16–20% English and 43–165% Mandarin on Precision@5
- Fine-tuned models shift cultural value predictions on high-tension WVS questions (US→China: 13→25 aligned responses)
- 7–8B fine-tuned models match or exceed vanilla 70B baselines in both lexical and value alignment
- PPO improves Spearman ρ from ~0.24 to 0.27 (US) and 0.21→0.32 (Chinese) on ranking tasks

## Why This Works (Mechanism)

### Mechanism 1
Word association norms capture implicit cultural knowledge that transfers to LLMs through fine-tuning. Free word associations expose "spontaneous links that structure semantic memory" - when native speakers produce associations like "red → happiness, celebration, luck" (Chinese) vs. "red → danger, stop, anger" (US), they reveal culture-specific conceptual mappings. Fine-tuning on these patterns reshapes the model's learned associations to reflect target-culture representations. The core assumption is that lexical-level associations serve as proxies for deeper cultural values.

### Mechanism 2
Lexical association training transfers to higher-level value judgments through shared semantic representations. The paper hypothesizes that if "lexical links mirror deeper cultural values, aligning them should steer models toward cultural judgments." Fine-tuning modifies the model's semantic space, shifting how it conceptualizes abstract terms like "freedom" or "equality," which then influences downstream reasoning on survey questions. The core assumption is that models use the same representations for word associations and value reasoning.

### Mechanism 3
Preference optimization (PPO) with frequency-based rewards better captures cultural salience than SFT alone. PPO trains models to rank associations by human production frequency using Spearman correlation as reward. This preserves "relative production frequencies" as cultural salience signals - capturing not just what people associate but how strongly. The core assumption is that association frequency correlates with cultural importance or salience.

## Foundational Learning

- **Word Association Norms (SWOW)**
  - Why needed here: These datasets (12K English cues, 10K Chinese cues with 3M+ responses) are the training signal. Understanding their structure—cue words, multi-response format, frequency distributions—is essential for data preprocessing.
  - Quick check question: Can you explain why the paper uses 3 responses per cue rather than 1?

- **Supervised Fine-Tuning (SFT) vs. Preference Optimization (PPO)**
  - Why needed here: The paper compares these two approaches with different inductive biases. SFT teaches what to generate; PPO teaches how to rank. Results show SFT excels on Chinese values, PPO on US values.
  - Quick check question: What reward function does PPO use, and why might Spearman correlation be appropriate here?

- **Cultural Value Evaluation (WVS + Distance Metrics)**
  - Why needed here: Success is measured via World Values Survey responses using JSD (categorical) and EMD (ordinal) distances. Understanding these metrics is critical for interpreting results.
  - Quick check question: Why does the paper use both JSD and EMD instead of just one?

## Architecture Onboarding

- **Component map:** SWOW.US/SWOW.ZH → 80/10/10 split by cue → SFT/PPO training → Intrinsic evaluation (Precision@K, Spearman ρ) → Extrinsic evaluation (WVS + JSD/EMD)
- **Critical path:** Data preprocessing → training with LoRA (SFT) or PPO → evaluation on lexical and value alignment tasks
- **Design tradeoffs:** SFT vs PPO for different cultural contexts; LoRA for parameter efficiency vs full fine-tuning
- **Failure signatures:** PPO training instability with sparse rewards; concreteness gap persists; language mismatch affects alignment metrics
- **Three first experiments:** 1) Evaluate on held-out test set for P@5/10/40 (SFT) and Spearman ρ (PPO) 2) Run vllm constrained sampling on WVS questions in target language 3) Compute JSD/EMD against human response distributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do supervised fine-tuning (SFT) and preference optimization (PPO) exhibit different learning efficiencies across cultural contexts?
- Basis in paper: The authors state in the Limitations that while SFT achieved optimal alignment for Chinese values and PPO for US values, the results "do not fully explain why certain learning approaches perform better under specific cultural contexts."
- Why unresolved: The study focused on empirical performance metrics rather than analyzing the interaction between training algorithms and the specific distributions of cultural knowledge in the training data.
- What evidence would resolve it: Mechanistic analyses of model internal states or comparative studies varying data distribution characteristics across cultures.

### Open Question 2
- Question: Can this word association method generalize to low-resource languages and cultures lacking large-scale normative datasets?
- Basis in paper: The Limitations section notes the study focused on high-resource languages (English/Mandarin) and that "generalizability to low-resource languages and cultures requires further exploration."
- Why unresolved: The current findings rely on the existence of the extensive SWOW datasets (millions of responses), which are unavailable for most languages, leaving minimal data requirements undefined.
- What evidence would resolve it: Experiments applying the ALIGN pipeline to low-resource languages using synthetic data or smaller, crowdsourced association sets.

### Open Question 3
- Question: What factors prevent fine-tuned models from matching human levels of concreteness in word associations?
- Basis in paper: Section 4.2 notes a "persistent gap" where model associations remain significantly more abstract than human associations, despite successful alignment in valence and arousal.
- Why unresolved: The paper identifies this gap as a remaining challenge but does not determine if it stems from the abstract nature of training data, the model architecture, or the fine-tuning objective.
- What evidence would resolve it: Ablation studies focusing on the concreteness distribution of training samples or the application of concreteness-weighted loss functions.

## Limitations

- The transfer mechanism from lexical associations to value judgments remains unproven - the paper demonstrates correlation but cannot definitively establish causation.
- Evaluation methodology using constrained sampling may not capture full complexity of how models reason about cultural values.
- Dataset limitations constrain generalizability - SWOW datasets may not fully represent diversity within US and Chinese cultures.

## Confidence

**High confidence**: Lexical alignment improvements (16-20% English, 43-165% Mandarin on Precision@5) are well-supported by direct measurements on held-out test sets.

**Medium confidence**: Value alignment improvements on high-tension WVS questions are convincing but depend on the assumption that WVS responses accurately represent cultural values.

**Low confidence**: The mechanism explaining how lexical association training transfers to value judgments requires further investigation.

## Next Checks

1. **Ablation on training data composition**: Create controlled experiments varying the proportion of word associations versus explicit value statements in training data to determine whether the observed value alignment stems specifically from association patterns or from general cultural knowledge transfer.

2. **Cross-cultural generalization test**: Evaluate fine-tuned models on WVS questions from cultures not represented in the training data (e.g., Japanese or Indian values) to assess whether the models learned transferable cultural reasoning patterns or simply memorized specific cultural associations.

3. **Mechanism isolation experiment**: Compare models fine-tuned on word associations versus models fine-tuned on other cultural knowledge sources (e.g., translated cultural texts or proverbs) while controlling for dataset size and cultural content to isolate whether the association format specifically enables the value transfer effect.