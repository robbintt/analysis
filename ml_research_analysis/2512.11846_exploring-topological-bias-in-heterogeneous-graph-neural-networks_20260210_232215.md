---
ver: rpa2
title: Exploring Topological Bias in Heterogeneous Graph Neural Networks
arxiv_id: '2512.11846'
source_url: https://arxiv.org/abs/2512.11846
tags:
- graph
- nodes
- node
- which
- heterogeneous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates topological bias in heterogeneous graph
  neural networks (HGNNs), where models exhibit performance discrepancies across nodes
  due to structural properties. The authors propose a meta-weighted adjacency matrix
  that distinguishes intra-type and inter-type connections, and construct a Heterogeneous
  Label Impact Degree (HLID) projection based on meta-weighted graphs and PageRank.
---

# Exploring Topological Bias in Heterogeneous Graph Neural Networks

## Quick Facts
- arXiv ID: 2512.11846
- Source URL: https://arxiv.org/abs/2512.11846
- Reference count: 40
- This paper investigates topological bias in heterogeneous graph neural networks and proposes HTAD to reduce performance disparities across nodes.

## Executive Summary
This paper identifies and addresses topological bias in heterogeneous graph neural networks (HGNNs), where model performance varies significantly across nodes due to structural properties. The authors propose a meta-weighted adjacency matrix that distinguishes intra-type and inter-type connections, and construct a Heterogeneous Label Impact Degree (HLID) projection using personalized PageRank on this meta-weighted graph. This projection effectively correlates with model performance across datasets. To mitigate bias, they introduce a Heterogeneous Topological-Aware Debiasing (HTAD) method that leverages contrastive learning with topology-aware edge sampling based on HLID differences. Experiments on three public datasets demonstrate that HTAD consistently improves HGNN performance while reducing performance variance across nodes.

## Method Summary
The method constructs a meta-weighted adjacency matrix that assigns different weights to intra-type versus inter-type connections using parameters η₁ (self-amplification) and η₂ (relation regulation). HLID is computed via personalized PageRank on this meta-weighted graph to measure each node's proximity to labeled nodes. The HTAD framework then samples an augmented graph based on HLID differences between nodes, creating additional paths that specifically benefit disadvantaged nodes. A shared HGNN encoder processes both original and augmented graphs, with contrastive losses (general and target-specific) encouraging similar representations for nodes with similar HLID values while maintaining supervised classification objectives.

## Key Results
- HLID projection achieves Spearman correlation rs=1.0 with model performance on IMDB, compared to rs=0.4286-0.75 for degree metrics
- HTAD reduces bucket variance from 0.0540 to 0.0228 on IMDB, 0.1150 to 0.0359 on DBLP, and 0.1216 to 0.0715 on ACM
- Across all three datasets and label rates, HTAD consistently improves F1 scores by 1-3% while reducing performance disparity
- HTAD outperforms simpler baselines including GCN, GAT, HAN, and HGT on node classification and clustering tasks

## Why This Works (Mechanism)

### Mechanism 1: HLID Projection Quantifies Label Proximity
- Claim: HLID values strongly correlate with model prediction accuracy on individual nodes.
- Mechanism: The Heterogeneous Label Impact Degree (HLID) uses personalized PageRank on a meta-weighted adjacency matrix to compute the cumulative influence of all labeled nodes on each target node. Nodes with higher HLID are "closer" to labeled nodes in the meta-weighted topology, thus receiving more informative gradients during message passing.
- Core assumption: Label information propagates through the graph structure in a manner approximated by PageRank diffusion, and this propagation distance predicts downstream performance.
- Evidence anchors:
  - [abstract]: "construct a Heterogeneous Label Impact Degree (HLID) projection... This projection effectively correlates with model performance across datasets with and without intra-type connections"
  - [Section 3.4, Figure 2]: Shows Spearman correlation rs=1.0 for HLID vs. rs=0.4286-0.75 for degree on IMDB/DBLP at various label rates
  - [corpus]: Related work on structural bias confirms topology-performance links in homogeneous GNNs, but heterogeneous extension is novel here
- Break condition: If label information propagates via mechanisms fundamentally different from PageRank (e.g., attention-based routing that ignores topology), or if meta-weighting fails to capture semantic relevance.

### Mechanism 2: Meta-Weighting Distinguishes Semantic Relations
- Claim: Assigning differentiated weights to intra-type vs. inter-type edges preserves semantic structure lost in binary adjacency matrices.
- Mechanism: A type relation matrix R assigns weights via: (1) self-amplification (η₁-term) that boosts intra-type connections, and (2) relation regulation (η₂-term) that inversely weights edges by their frequency, giving higher importance to sparser meta-relations.
- Core assumption: Intra-type connections provide more direct semantic signal (one-hop vs. two-hop), and sparser relations carry more concentrated information per edge.
- Evidence anchors:
  - [abstract]: "meta-weighted adjacency matrix that distinguishes intra-type and inter-type connections"
  - [Section 3.2, Equation 3]: Rij = 1 + η₁·1[i=j] + η₂/count(i,j)
  - [Section 5.4, Figures 6-7]: Parameter sensitivity shows optimal η₁≈3.0, η₂≈2000; performance degrades when either term dominates
  - [corpus]: Limited direct corpus evidence on this specific weighting scheme; assumes transfer from degree normalization principles
- Break condition: If heterogeneous graph semantics are not well-captured by edge-type frequency or intra-type preference (e.g., graphs where inter-type edges are primary signal).

### Mechanism 3: HLID-Gap Edge Sampling Bridges Disadvantaged Nodes
- Claim: Sampling edges proportionally to HLID differences creates augmentation paths that specifically benefit low-HLID nodes.
- Mechanism: The augmented graph Ĝ samples edges with probability p̂uv = 1 - (1-p₀)exp(-λ|δ(u,v)|) for existing edges and p̂uv = 1 - exp(-λ|δ(u,v)|) for new edges, where δ(u,v) = HLID(u) - HLID(v). This preferentially connects nodes with large HLID gaps, giving low-HLID nodes additional paths to high-HLID (well-connected) nodes.
- Core assumption: Providing disadvantaged nodes (low HLID) with direct connections to well-positioned nodes improves their representation quality without introducing semantic noise.
- Evidence anchors:
  - [Section 4.1, Equation 5]: Explicit sampling probability formula tied to |δ(u,v)|
  - [Table 2]: HTAD achieves lowest bucket variance (0.0228-0.0715 across datasets), indicating reduced performance disparity
  - [corpus]: GRADE shows degree-based edge perturbation helps homogeneous GNNs; HTAD extends this to heterogeneous topology
- Break condition: If sampled edges violate graph semantics (e.g., creating invalid meta-relations), or if λ is poorly tuned causing over/under-augmentation.

## Foundational Learning

- Concept: **Personalized PageRank**
  - Why needed here: HLID computation directly applies personalized PageRank to compute label influence; understanding teleport probability α and the propagation matrix is essential.
  - Quick check question: Given a graph with adjacency matrix A and teleport α=0.15, what does the (i,j) entry of Q = α(I - (1-α)D⁻¹/²ÃD⁻¹/²)⁻¹ represent?

- Concept: **Contrastive Learning (NT-Xent Loss)**
  - Why needed here: HTAD uses NT-Xent loss for both general and target-specific contrastive terms; understanding positive/negative pair construction is critical.
  - Quick check question: In NT-Xent, why does the denominator sum over negative pairs only, and how does temperature scaling affect gradient sharpness?

- Concept: **Heterogeneous Graph Meta-Relations**
  - Why needed here: The meta-weighting mechanism requires understanding intra-type edges vs. inter-type edges, and how meta-paths capture composite semantics.
  - Quick check question: In a bibliographic graph with types {Author, Paper, Venue}, what is the semantic meaning of the meta-path APA, and why might it have different weight than APVPA?

## Architecture Onboarding

- Component map: Pre-processing (meta-weighting → HLID) → Augmentation (edge sampling) → Encoder (dual-view HGNN) → Loss Computation (label + contrastive) → Inference (original graph only)
- Critical path: HLID computation → edge sampling → dual-view encoding → contrastive loss aggregation. HLID is precomputed once; augmentation happens per-epoch.
- Design tradeoffs:
  - λ (augmentation density): Higher λ → denser Ĝ → more augmentation but higher compute
  - p₀ (edge preservation): Lower p₀ → more aggressive perturbation but risks losing original semantics
  - η₁, η₂ (meta-weighting): Must balance intra-type emphasis vs. relation-frequency regularization; dataset-dependent
  - λ₁, λ₂ (loss weights): Must balance contrastive vs. supervised signal; paper finds optimal at λ₁≈0.3, λ₂≈0.15
- Failure signatures:
  - Spearman correlation between HLID and accuracy near zero: Meta-weighting misconfigured or graph structure incompatible with PageRank assumptions
  - Bucket variance increases after HTAD: Edge sampling creating harmful shortcuts; check λ and p₀ settings
  - Training loss diverges: Contrastive loss terms dominating; reduce λ₁, λ₂ or increase label loss weight
  - Runtime explosion: Augmented graph too dense; reduce λ or enforce sparser sampling
- First 3 experiments:
  1. **HLID correlation baseline**: On IMDB/DBLP with label rate 0.05, compute Spearman rs for HLID vs. node degree vs. random projection using a base HAN model. Expected: HLID rs > 0.85, degree rs < 0.75.
  2. **Ablation on meta-weighting**: Train HTAD with η₁=0 (no self-amplification) and η₂=0 (no relation regulation) separately on ACM (has intra-type connections). Expected: Full HLID outperforms both ablations per Figure 3.
  3. **Debiasing validation**: Run HTAD vs. vanilla HAN at label rate 0.05 on all three datasets; report F1, total variance, and bucket variance. Expected: HTAD improves F1 by 1-3% and reduces variance by 15-40% per Table 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the HLID projection be effectively adapted for fully unsupervised representation learning where no label information exists to define the impact degree?
- Basis: The definition of HLID ($Z=QJ$) in Equation 4 explicitly relies on the label matrix $J$, limiting the proposed debiasing framework to semi-supervised scenarios.
- Why unresolved: The paper validates the method solely under semi-supervised settings; it does not propose an alternative mechanism for calculating "impact" or bias without ground-truth labels.
- What evidence would resolve it: A modification of the HLID calculation using pseudo-labels or clustering centroids, demonstrating comparable debiasing performance in a label-free environment.

### Open Question 2
- Question: Is there a theoretical or heuristic upper bound for the meta-weighting parameters ($\eta_1, \eta_2$) that generalizes across datasets without requiring empirical search?
- Basis: While Section 5.4 analyzes parameter sensitivity, it relies on grid search to find optimal values for $\eta_1$ and $\eta_2$, implying the settings are dataset-dependent.
- Why unresolved: The paper does not provide a theoretical justification for the specific optimal values found, leaving the tuning process empirical for new graphs.
- What evidence would resolve it: A theoretical analysis linking graph properties (e.g., density, heterophily) to optimal $\eta$ values, or consistent performance across datasets using a fixed heuristic.

### Open Question 3
- Question: How does HTAD's computational overhead scale to industrial-sized heterogeneous graphs with millions of nodes, given the complexity of the contrastive loss?
- Basis: The time complexity analysis mentions $O(\sum |V_t|^2)$ for contrastive loss, which the authors admit is "impractical for large graphs" before introducing node-type separation; however, even separated, large $|V_t|$ remains a bottleneck.
- Why unresolved: Experiments were restricted to relatively small academic datasets (max ~14k nodes), leaving performance on massive-scale graphs unverified.
- What evidence would resolve it: Runtime and memory usage benchmarks on large-scale open graph benchmarks demonstrating the feasibility of the edge sampling and loss calculation.

## Limitations

- The core assumptions about PageRank-based label diffusion and meta-weighting parameters (η₁=3.0, η₂=2000.0) may not generalize beyond the three tested datasets without empirical tuning.
- Computational overhead claims of Õ(|E|) complexity are not fully validated in practice, particularly for the edge sampling step.
- The method is limited to semi-supervised settings due to its reliance on label information for HLID computation.

## Confidence

- **High confidence**: Topological bias exists in HGNNs and affects node performance differently; HTAD improves overall F1 scores on all three datasets
- **Medium confidence**: HLID projection correlates strongly with performance (Spearman rs values); meta-weighting mechanism improves performance over binary adjacency
- **Low confidence**: Universal applicability of HLID beyond tested datasets; computational efficiency claims hold under different hardware/scale conditions

## Next Checks

1. **Cross-dataset parameter sensitivity**: Test whether η₁=3.0 and η₂=2000.0 remain optimal on heterogeneous graphs from different domains (e.g., social networks, biological interaction networks) with varying node type distributions.

2. **Theoretical validation of HLID universality**: Prove or disprove whether the PageRank-based HLID metric correlates with node performance in the limit of large heterogeneous graphs, particularly when meta-path semantics differ significantly from simple edge-type frequency.

3. **Computational complexity validation**: Measure actual training time and memory usage of HTAD on graphs with varying edge densities and compare against the claimed Õ(|E|) complexity, particularly examining the edge sampling step's overhead.