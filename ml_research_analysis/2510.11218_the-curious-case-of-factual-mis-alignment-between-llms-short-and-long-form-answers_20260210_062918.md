---
ver: rpa2
title: The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form
  Answers
arxiv_id: '2510.11218'
source_url: https://arxiv.org/abs/2510.11218
tags:
- answer
- factual
- short
- long
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses factual inconsistency in LLM responses across
  short and long query formats. The authors introduce SLAQ, a controlled framework
  comparing model answers to the same factual questions asked in isolation versus
  embedded in complex queries.
---

# The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers

## Quick Facts
- arXiv ID: 2510.11218
- Source URL: https://arxiv.org/abs/2510.11218
- Authors: Saad Obaid ul Islam; Anne Lauscher; Goran GlavaÅ¡
- Reference count: 40
- Primary Result: Most LLM factual errors occur when correct short answers become incorrect in long queries

## Executive Summary
This study investigates factual inconsistency between LLM responses to simple queries versus the same queries embedded in complex contexts. The authors introduce SLAQ, a controlled framework comparing model answers to identical factual questions asked in isolation versus embedded in complex queries. Testing 16 LLMs across 600 queries spanning five domains, they find systematic misalignment: most errors occur when models answer correctly to short queries but incorrectly to corresponding long queries. The work establishes factual consistency over query complexity as critical for LLM trustworthiness and challenges evaluation practices that assume simple factual accuracy implies complex query reliability.

## Method Summary
The researchers constructed a controlled experimental framework using 600 factual queries across five domains (historical events, scientific facts, current affairs, geographical knowledge, and biographical information). Each factual question was tested in two formats: short (isolated query) and long (embedded within a complex query). The study tested 16 leading LLMs, including GPT-4, Claude-3, Gemini Pro, and various open-weight models. A novel alignment metric was developed to quantify consistency between short and long answer pairs, and mechanistic interpretability techniques were applied to analyze computational pathways for aligned versus misaligned facts.

## Key Results
- **Systematic misalignment discovered**: Most factual errors occur when models answer correctly to short queries but incorrectly to corresponding long queries
- **Momentum effects identified**: Consecutive correct answers increase accuracy by ~7%, while errors reduce it by ~20%
- **Mechanistic predictability**: Circuit similarity metrics predict short-long alignment with up to 80% accuracy (ROC-AUC: 0.85)

## Why This Works (Mechanism)
The core mechanism involves contextual interference where embedding factual queries within complex contexts disrupts the precise computational pathways that models use for straightforward fact retrieval. When simple queries are isolated, models can activate specific factual knowledge circuits efficiently. However, when the same factual content is embedded in longer, more complex queries, the model must navigate additional computational steps, potentially activating competing knowledge pathways or applying reasoning patterns that interfere with direct fact retrieval. This interference is particularly pronounced when the complex context introduces semantic ambiguities, temporal dependencies, or implicit assumptions that weren't present in the isolated query.

## Foundational Learning
- **Factual query alignment**: Understanding how models retrieve facts differently across query formats is essential for building reliable AI systems. Quick check: Compare isolated vs embedded query performance on benchmark datasets.
- **Computational pathway analysis**: Interpreting how models represent and retrieve factual knowledge through mechanistic interpretability helps identify failure modes. Quick check: Map activation patterns for aligned vs misaligned responses.
- **Momentum effects in sequential reasoning**: Recognizing how consecutive answers influence subsequent performance is crucial for understanding model behavior in extended interactions. Quick check: Analyze accuracy patterns across answer sequences.

## Architecture Onboarding
- **Component map**: Input Processing -> Query Decomposition -> Fact Retrieval Circuit -> Contextual Integration -> Answer Generation
- **Critical path**: The fact retrieval circuit is most vulnerable to misalignment when contextual integration introduces competing activation patterns
- **Design tradeoffs**: Models optimized for complex reasoning may sacrifice precision in straightforward fact retrieval when facts are embedded in complex contexts
- **Failure signatures**: Correct-to-incorrect transitions dominate misalignment patterns, suggesting context-induced interference rather than knowledge gaps
- **First experiments**: 1) Test isolated vs embedded performance across model families, 2) Analyze activation similarity for aligned vs misaligned facts, 3) Measure momentum effects across different query types

## Open Questions the Paper Calls Out
None

## Limitations
- The controlled experimental framework may not fully capture real-world query complexity with multiple context switches and temporal dependencies
- The 600-query dataset represents a curated sample that may not reflect the full spectrum of factual query types encountered in practice
- Momentum effects are correlational findings that don't establish causation and could reflect model training artifacts or prompt formatting influences

## Confidence
- **High Confidence (8/10)**: Systematic misalignment exists between short and long query formats, with correct-to-incorrect transitions being more common than reverse patterns
- **Medium Confidence (6/10)**: Momentum effects significantly influence consecutive answer accuracy, though causal mechanisms remain unclear
- **Medium Confidence (6/10)**: Circuit similarity metrics predict alignment with reasonable accuracy (ROC-AUC: 0.85), but practical utility requires further validation

## Next Checks
1. **External Dataset Validation**: Test short-long alignment patterns on independent, larger-scale datasets with more diverse query complexities to assess generalizability beyond the curated 600-query sample

2. **Causal Momentum Investigation**: Design controlled experiments varying answer ordering, prompt formatting, and context switching to isolate whether momentum effects are genuine cognitive phenomena or experimental artifacts

3. **Cross-Architecture Circuit Analysis**: Apply mechanistic interpretability to open-weight models with different architectural designs to determine whether computational pathways are universal features or specific to tested transformer architectures