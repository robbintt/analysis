---
ver: rpa2
title: Coordinate Descent for Network Linearization
arxiv_id: '2511.11781'
source_url: https://arxiv.org/abs/2511.11781
tags:
- relu
- accuracy
- relus
- budget
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Coordinate Descent for Network Linearization,
  a method to reduce ReLU count in ResNet networks for Private Inference. The problem
  is that ReLUs are a major bottleneck in Private Inference due to latency, and reducing
  them is a discrete optimization challenge.
---

# Coordinate Descent for Network Linearization

## Quick Facts
- arXiv ID: 2511.11781
- Source URL: https://arxiv.org/abs/2511.11781
- Reference count: 35
- Primary result: BCD directly on binary ReLU masks achieves SOTA accuracy on CIFAR-10/100 and TinyImageNet with ResNet18 and WideResNet22-8 at low ReLU budgets

## Executive Summary
This paper introduces Coordinate Descent for Network Linearization, a method to reduce ReLU count in ResNet networks for Private Inference. ReLUs are a major bottleneck in Private Inference due to latency, and reducing them is a discrete optimization challenge. Most existing methods use LASSO-like regression with L1 regularization, which doesn't guarantee sparse solutions and hurts accuracy at hard thresholding. The authors propose using Block Coordinate Descent directly on binary ReLU masks, which naturally produces sparse solutions without relaxations.

## Method Summary
The method works directly in the discrete domain by optimizing binary ReLU masks using Block Coordinate Descent. It iteratively samples and removes ReLU subsets with minimal accuracy impact, fine-tuning when performance drops. The algorithm starts from a pre-optimized model (like SNL or AutoRep) at a medium budget and refines it to the target budget through consecutive ReLU elimination with strict accuracy degradation tolerance.

## Key Results
- Achieves state-of-the-art results on CIFAR-10, CIFAR-100, and TinyImageNet datasets with ResNet18 and WideResNet22-8 backbones
- Outperforms SNL by up to 4.13% accuracy on CIFAR-100 at 6K ReLU budget
- Demonstrates that their method can improve existing approaches like AutoRep, achieving the same accuracy with half the ReLU budget
- Theoretical analysis shows provable runtime-performance guarantees compared to selective approaches

## Why This Works (Mechanism)

### Mechanism 1
Working directly on binary masks (discrete domain) avoids the approximation error inherent in continuous relaxation methods like LASSO. The algorithm optimizes the ReLU mask $m$ using Block Coordinate Descent directly on the binary constraint $||m||_0 \le B$. This bypasses the "hard thresholding" post-processing step required by continuous methods, which often causes irreversible accuracy loss.

### Mechanism 2
The iterative removal of ReLUs with strict Accuracy Degradation Tolerance (ADT) and fine-tuning preserves the feature manifold better than one-shot pruning. The algorithm removes ReLUs in small chunks and commits to removal only if the accuracy drop is within the ADT limit. If the accuracy drops significantly, the network weights are fine-tuned to recover the loss before further removal.

### Mechanism 3
"Warm-starting" the discrete optimization from a partially linearized model (e.g., output of SNL) accelerates convergence to extremely low ReLU budgets. Instead of starting from a full-precision network, the method initializes from a model already optimized by a baseline method to a medium budget, then refines this solution to the final target.

## Foundational Learning

- **L0 vs. L1 Regularization**: Understanding that $L_1$ (LASSO) encourages sparsity but requires a hard thresholding step that introduces error, whereas $L_0$ is mathematically exact for count constraints but computationally harder. Quick check: Why does minimizing $L_1$ norm not guarantee an exact solution to a budget constraint without a post-processing step?

- **Private Inference (PI) Bottlenecks**: Understanding that in cryptographic protocols, ReLUs require expensive non-linear interactions while linear layers are cheap. Quick check: In a ResNet, why are ReLUs specifically targeted for reduction rather than Convolutional weights in the context of Private Inference?

- **Block Coordinate Descent (BCD)**: Understanding that BCD optimizes subsets of variables while holding others fixed, unlike Gradient Descent which updates all parameters slightly. Quick check: How does the convergence guarantee of BCD differ from standard Stochastic Gradient Descent in the context of discrete masks?

## Architecture Onboarding

- **Component map**: Inputs (Pre-trained Network, Reference Budget, Target Budget) -> Hyperparameters (DRC, ADT, RT) -> State (Binary Mask) -> Process (Sample Block -> Evaluate Loss -> If Loss < ADT: Keep -> If Accumulated Loss High: Fine-tune Weights) -> Repeat until Budget reached

- **Critical path**: The Random Sampling & Evaluation step is where the discrete decision is made. If DRC is too large or sampling is unlucky, you may prune critical neurons and trigger an unrecoverable accuracy drop. The Fine-tuning step is the safety net for this path.

- **Design tradeoffs**: 
  - DRC (Block Size): Large DRC = faster reduction but higher risk of removing useful ReLUs (worse final accuracy)
  - Random Trials (RT): Higher RT finds better blocks (better accuracy) but linearly increases wall-clock time per iteration
  - Initialization: Starting from a full model is slower but may find better global minima; starting from SNL (warm start) is much faster for aggressive pruning but inherits SNL's structural biases

- **Failure signatures**:
  - Cascading Collapse: Accuracy drops by > ADT immediately, and fine-tuning fails to recover it
  - Stagnation: The algorithm cannot find any block of ReLUs to remove without exceeding ADT
  - Runtime Explosion: RT is set too high without parallel evaluation, making the search for low budgets intractable

- **First 3 experiments**:
  1. Baseline Reproduction (ResNet18/CIFAR-10): Run the method starting from a standard ResNet18 to reach ~50% ReLU budget
  2. DRC Ablation: Fix the target budget (e.g., 10K ReLUs) and vary DRC (e.g., 50, 100, 500) to find the optimal efficiency frontier
  3. Warm-start vs. Cold-start: Compare final accuracy and runtime when initializing from a full model vs. initializing from an SNL model at a medium budget

## Open Questions the Paper Calls Out
- Is the high overlap (IoU > 0.85) between optimal ReLU masks for different budgets a general property of neural networks, or specific to ResNet architectures tested?
- Can the efficiency of Block Coordinate Descent be improved by replacing random subset sampling with gradient-based or sensitivity-aware selection?
- How does the method perform when initialized from a standard pre-trained network rather than a pre-optimized SNL or AutoRep model?

## Limitations
- The paper claims BCD achieves "state-of-the-art" results but only compares against SNL and AutoRep, not the full space of possible methods
- The theoretical runtime-performance guarantees are referenced but not fully detailed in the paper
- The discrete optimization landscape for ReLU masks is largely unexplored, making it difficult to establish absolute superiority

## Confidence
- **High confidence**: The mechanism of avoiding L1 relaxation error by working directly on binary masks is well-founded and experimentally validated
- **Medium confidence**: The iterative removal with fine-tuning strategy is plausible but relies on the assumption of ReLU redundancy
- **Low confidence**: The claim of achieving "provable runtime-performance guarantees" compared to selective approaches is not sufficiently detailed to evaluate independently

## Next Checks
1. Implement a synthetic ReLU network where the optimal mask is known and run BCD from multiple random initializations to measure convergence to the global optimum
2. Profile the wall-clock time per iteration as a function of DRC and RT to verify the linear scaling with RT and inverse relationship with DRC
3. Take a highly optimized model (e.g., 6K ReLUs) and systematically measure the accuracy drop when removing single ReLUs to test the redundancy assumption