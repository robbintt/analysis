---
ver: rpa2
title: 'Anticipating Gaming to Incentivize Improvement: Guiding Agents in (Fair) Strategic
  Classification'
arxiv_id: '2505.05594'
source_url: https://arxiv.org/abs/2505.05594
tags:
- agents
- strategic
- firm
- improvement
- manipulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates strategic human behavior in response to
  algorithmic decision systems, focusing on the choice between improving qualifications
  ("improvement") versus manipulating observable features ("manipulation"). The authors
  model this interaction as a Stackelberg game, where a firm deploys a (fair) classifier,
  and individuals strategically respond with different costs and stochastic efficacy
  for manipulation and improvement.
---

# Anticipating Gaming to Incentivize Improvement: Guiding Agents in (Fair) Strategic Classification

## Quick Facts
- arXiv ID: 2505.05594
- Source URL: https://arxiv.org/abs/2505.05594
- Authors: Sura Alhanouti; Parinaz Naghizadeh
- Reference count: 12
- One-line primary result: Firms anticipating strategic behavior can prevent manipulation and incentivize improvement by making selection algorithms stricter, while strategic fairness interventions minimize utility loss compared to non-strategic approaches.

## Executive Summary
This paper investigates how algorithmic decision systems can be designed to guide human behavior away from manipulation and toward genuine improvement. The authors model the interaction as a Stackelberg game where a firm sets classification thresholds and individuals strategically choose between improving their qualifications or manipulating observable features. The main insight is that when firms anticipate strategic behavior, they can not only prevent manipulation but also incentivize improvement by raising selection thresholds. This creates a "win-win" where firms get more qualified applicants and individuals are guided toward genuine self-improvement. The analysis also shows that strategic awareness significantly reduces the utility cost of fairness interventions compared to non-strategic approaches.

## Method Summary
The authors formulate the interaction between firms and strategic agents as a Stackelberg game, where the firm commits to classification thresholds and agents respond optimally. They derive equilibrium conditions for agent best responses based on indifference points between actions, then solve for optimal thresholds using utility maximization under different fairness constraints. The analysis involves characterizing three types of equilibrium behavior (Type 1: Improvement dominant, Type 2: Mixed, Type 3: Manipulation dominant) and computing post-strategic qualification rates through convolution of distributions. Numerical simulations compare strategic firms (anticipating agent responses) with non-strategic firms across fairness constraints.

## Key Results
- Strategic firms raise classification thresholds to force unqualified agents into improvement behaviors rather than manipulation
- Anticipating strategic behavior guides agents toward genuine improvement while still allowing manipulation as an option for qualified agents
- Strategic awareness reduces the utility cost of fairness interventions, requiring only small threshold changes to achieve fairness goals
- Fairness interventions may decrease improvement incentives for disadvantaged populations when firms are unaware of strategic responses

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A firm anticipating strategic behavior will raise its classification threshold to force unqualified agents into improvement behaviors rather than manipulation.
- **Mechanism:** By increasing selection difficulty, the firm makes low-cost, low-efficacy manipulation insufficient for acceptance for marginal agents. To bridge the larger gap to the new threshold, agents must switch to high-cost, high-efficacy improvement action, which changes their true qualification state rather than just features.
- **Core assumption:** Improvement actions have higher efficacy than manipulation actions (FOSD), and agents are rational utility maximizers.
- **Evidence anchors:** [abstract] "Specifically, the firm will make its selection algorithm more 'strict', incentivizing improvement by unqualified agents while still leaving manipulation as an option for qualified agents." [page 12, Proposition 8] "The strategic firm chooses a higher threshold... The strategic firm drives more improvement by both qualified and unqualified agents."
- **Break condition:** If improvement cost is prohibitively high relative to agent's budget, agents may opt for "Nothing" rather than Improvement.

### Mechanism 2
- **Claim:** Agents partition behavior into distinct regions (Nothing, Manipulate, Improve) based on initial feature x relative to threshold θ.
- **Mechanism:** Agents calculate utility based on probability of crossing threshold minus action cost, creating "indifference points" where they switch actions. Agents far from threshold find manipulation too risky and choose improvement if viable; agents close to threshold may prefer cheaper manipulation if acceptance probability is high enough.
- **Core assumption:** Distribution of feature "boosts" from actions is stochastic and known.
- **Evidence anchors:** [page 7, Proposition 2] Characterizes three types of agent best-responses based on ordering of indifference points. [page 13, Figure 2] Visualizes how regions of Improvement (I) and Manipulation (M) shift based on threshold policy.
- **Break condition:** If distributions of manipulation and improvement efficacy are identical, agent will always choose cheaper option.

### Mechanism 3
- **Claim:** Strategic awareness reduces utility cost of fairness interventions by leveraging agent improvement to balance group statistics.
- **Mechanism:** Non-strategic firm lowers thresholds for disadvantaged groups to meet fairness constraints, inadvertently incentivizing manipulation. Strategic firm anticipates that maintaining stricter threshold will force disadvantaged group to improve true qualifications, naturally closing fairness gap in qualification rates without drastically lowering standards.
- **Core assumption:** Fairness constraints can be satisfied by changing underlying qualification distribution, not just decision boundary.
- **Evidence anchors:** [page 3] "We identify a third lever: anticipating strategic responses, which increases improvement incentives, and in turn aids in improving fairness 'for free'." [page 14] "A strategic firm understands that desired fairness goals can be attained through a combination of adjusting the decision thresholds and driving agents' best-responses."
- **Break condition:** If improvement resources are inaccessible to disadvantaged group, strict threshold will exclude them rather than incentivize improvement.

## Foundational Learning

- **Concept: Stackelberg Game**
  - **Why needed here:** Mathematical framework modeling interaction where firm (Leader) commits to policy θ, and agents (Followers) respond. Sequential commitment is necessary to compute equilibrium.
  - **Quick check question:** Does the agent observe the firm's specific threshold before choosing their action? (Yes)

- **Concept: First-Order Stochastic Dominance (FOSD)**
  - **Why needed here:** Paper relies on FOSD to mathematically define "efficacy," ensuring improvement action yields probabilistically higher feature boost than manipulation for any effort level.
  - **Quick check question:** If Distribution A FOSD Distribution B, does Distribution A have higher probability of exceeding any specific value x? (Yes)

- **Concept: Indifference Points (Decision Boundaries)**
  - **Why needed here:** Optimization relies on identifying exact feature values where agent switches from Doing Nothing to Improving or Manipulating. These points define integration limits for firm's utility function.
  - **Quick check question:** At an indifference point, is utility of Action A strictly greater than Action B? (No, they are equal)

## Architecture Onboarding

- **Component map:** Inputs (Group distributions, Qualification rates, Action costs, Boost distributions) -> Decision Variable (Thresholds θ) -> Agent Sub-model (Computes best-response regions) -> Post-Strategic Calculator (Updates population stats) -> Optimizer (Solves max U(θ))

- **Critical path:**
  1. Define cost and boost distribution parameters
  2. Derive "indifference features" to partition agent space
  3. Calculate post-strategic qualification rates and feature distributions using Lemma 3
  4. Solve optimization using Lemma 5 to find optimal thresholds

- **Design tradeoffs:**
  - Strictness vs. Manipulation: Increasing θ reduces unqualified manipulation but may temporarily reduce qualified acceptance rates until agents improve
  - Fairness vs. Incentives: Aggressive fairness lowering of θ destroys improvement incentives; strategic tightening preserves incentives but requires accurate modeling of agent effort

- **Failure signatures:**
  - Collapse to Manipulation: If C_I >> C_M and efficacy gaps are small, model predicts Type 3 equilibrium, and firm cannot incentivize improvement
  - Exclusion of Disadvantaged: If θ is set too high for group b without accounting for their ability to improve, result is 0% acceptance rather than induced improvement

- **First 3 experiments:**
  1. Vary Improvement Cost (C_I): Sweep C_I from low to high to observe transition from Type 1 (Improvement dominant) to Type 3 (Manipulation only) equilibrium
  2. Fairness Constraint Ablation: Compare utility loss of "Non-Strategic Firm" vs. "Strategic Firm" under Demographic Parity
  3. Sensitivity to Qualification Rate (α): Plot post-strategic qualification α-hat against pre-strategic α to verify if system "amplifies" qualification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does equilibrium change if strategic resources (cost and efficacy of manipulation or improvement) differ across demographic groups?
- Basis in paper: [Explicit] Section 8 states main direction of future work is "exploring the impacts of varying these conditions among groups (e.g., availability of better strategic resources for the advantaged group)."
- Why unresolved: Current analysis assumes strategic behavior resources are similar across groups.
- What evidence would resolve it: Theoretical model characterizing optimal policies when costs or efficacy distributions differ significantly between groups.

### Open Question 2
- Question: Can firm effectively incentivize improvement by directly subsidizing cost of improvement actions for disadvantaged groups?
- Basis in paper: [Explicit] Section 8 suggests exploring "the choice of interventions by the firm (e.g., when the firm makes the improvement action cheaper for a disadvantaged group)."
- Why unresolved: Paper focuses only on firm's ability to adjust decision thresholds θ, not external cost levers.
- What evidence would resolve it: Extension of firm's utility maximization problem that includes budget for reducing agent costs.

### Open Question 3
- Question: How should firm optimize classifier when different demographic groups adopt different types of best-response equilibria simultaneously?
- Basis in paper: [Explicit] Section 8 notes it would be interesting "to extend our analysis to scenarios where different groups adopt different types of equilibrium responses."
- Why unresolved: Theoretical analysis assumes all agents opt for same type of best-response for tractability.
- What evidence would resolve it: Characterization of optimal thresholds derived for mixed-equilibrium environment.

## Limitations

- The theoretical framework relies on specific assumptions about agent rationality and stochastic dominance of improvement actions that may not hold in practice
- Analysis assumes perfect information about cost distributions and agent options, which may not be available in real-world scenarios
- Fairness interventions are evaluated within narrow set of constraints (DP and EOP), with implications for other fairness definitions unexplored
- Model assumes agents can actually improve their qualifications, which may not be true for all populations or contexts

## Confidence

- **High Confidence:** The core mathematical framework of the Stackelberg game and agent best-response characterization are rigorously derived and internally consistent
- **Medium Confidence:** Simulation results and comparative analysis between strategic and non-strategic firms are convincing, but specific parameter choices could influence magnitude of observed effects
- **Low Confidence:** Claim that fairness "can be achieved with very small changes to decision thresholds" is context-dependent and may not generalize to all scenarios

## Next Checks

1. **Sensitivity Analysis:** Perform systematic sensitivity analysis by varying cost parameters (C_M,s, C_I,s) and efficacy distributions (τ_M,s, τ_I,s) to determine robustness of strategic firm's advantage over non-strategic firm

2. **Alternative Fairness Metrics:** Extend fairness analysis to include other metrics such as Equalized Odds or Predictive Parity to assess whether strategic firm's advantage holds across broader range of fairness definitions

3. **Behavioral Experiment:** Design controlled behavioral experiment with human participants to test key predictions of model, creating scenario where participants choose between "gaming" system or improving skills with varied cost and difficulty