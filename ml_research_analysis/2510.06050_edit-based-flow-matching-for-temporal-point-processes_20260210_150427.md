---
ver: rpa2
title: Edit-Based Flow Matching for Temporal Point Processes
arxiv_id: '2510.06050'
source_url: https://arxiv.org/abs/2510.06050
tags:
- event
- edit
- sequences
- operations
- tpps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces EDITPP, a novel generative framework for temporal
  point processes (TPPs) that leverages continuous-time edit operations (insert, delete,
  substitute) to model event sequences jointly rather than autoregressively. The method
  extends Edit Flow by adapting it to continuous state spaces, introducing a tractable
  parameterization within a continuous-time Markov chain framework, and designing
  an auxiliary alignment space specific to TPPs.
---

# Edit-Based Flow Matching for Temporal Point Processes

## Quick Facts
- arXiv ID: 2510.06050
- Source URL: https://arxiv.org/abs/2510.06050
- Reference count: 40
- Primary result: EDITPP achieves state-of-the-art performance in both unconditional and conditional generation tasks across diverse synthetic and real-world datasets

## Executive Summary
EDITPP introduces a novel generative framework for temporal point processes that leverages continuous-time edit operations (insert, delete, substitute) to model event sequences jointly rather than autoregressively. By learning instantaneous edit rates within a continuous-time Markov chain framework, EDITPP attains a flexible and efficient model that outperforms strong autoregressive and non-autoregressive baselines. The method achieves state-of-the-art performance in both unconditional and conditional generation tasks across diverse synthetic and real-world datasets.

## Method Summary
EDITPP extends Edit Flow by adapting it to continuous state spaces through a continuous-time Markov chain framework with learned instantaneous edit rates. The model introduces an auxiliary alignment space specific to TPPs, where true dynamics are known, enabling efficient training without marginalizing over all possible edit paths. Noise sequences are transformed into data samples through three mutually exclusive edit operations—insert, delete, substitute—applied via Euler sampling from s=0 to s=1. The method employs a tractable parameterization with discretized continuous state space via binning, allowing dynamic trade-offs between computational cost and sample quality at inference time.

## Key Results
- Achieves state-of-the-art performance in both unconditional and conditional generation tasks across diverse synthetic and real-world datasets
- Outperforms strong autoregressive and non-autoregressive baselines in terms of Wasserstein distances (dl, dIET) and MMD
- Demonstrates reduced sampling runtime and allows dynamic trade-offs between computational cost and sample quality at inference time

## Why This Works (Mechanism)

### Mechanism 1: Continuous-Time Markov Chain with Edit Operations
The model defines a CTMC with transition rates u_θ_s(·|t_s) for three mutually exclusive operations—insert, delete, substitute—that transform noise sequences t₀ ~ p_noise into data samples t₁ ~ q_data. Euler sampling simulates this process from s=0 to s=1. The edit operations can transport any noise sequence to any data sequence through repeated application; the discretization error from Euler sampling is manageable.

### Mechanism 2: Auxiliary Alignment Space for Tractable Training
Noise-data pairs (t₀, t₁) are aligned via a modified Needleman-Wunsch algorithm with cost functions ensuring aligned pairs correspond to valid insert/sub/del operations. The mixture path p_s(z|z₀,z₁) in the aligned space transfers known dynamics back to data space via Bregman divergence loss. The minimum-cost alignment correctly identifies the intended edit operations; the κ_s schedule produces valid intermediate sequences.

### Mechanism 3: Discretized Continuous State Space via Binning
Quantizing time intervals into discrete bins enables discrete edit operations on continuous event times. Insert bins divide each inter-event interval into b_ins bins; substitution bins span [t-δ, t+δ] around each event with b_sub bins. Bin granularity (b_ins, b_sub, δ) is sufficient to approximate continuous edits without introducing systematic bias.

## Foundational Learning

- **Continuous-Time Markov Chains (CTMCs)**: The entire generative process is a CTMC where transition rates govern edit probabilities per time step. Quick check: Can you explain why Euler approximation is needed for sampling when exact simulation is intractable?

- **Temporal Point Processes: Superposition & Thinning**: Background establishes these properties as the foundation for prior diffusion-style TPP models. Quick check: Given λ₁(t) and λ₂(t) for two independent TPPs, what is the intensity of their superposition?

- **Bregman Divergence for Rate Matching**: The training objective uses Bregman divergence to match learned rates to conditional rates derived from the alignment space. Quick check: Why is directly marginalizing over all possible edit paths that produce sequence t intractable?

## Architecture Onboarding

- **Component map**: Event times t_s → SinEmb + MLP → Context tokens (s, |t_s|) → Llama backbone → Per-event MLP outputs λ_ins, λ_sub, λ_del and Q_ins, Q_sub

- **Critical path**: 1. Align (t₀, t₁) → (z₀, z₁) using Needleman-Wunsch with TPP-specific costs 2. Sample z_s ~ p_s(z|z₀,z₁) per element with schedule κ_s = 1 - cos(πs/2)² 3. Convert to t_s via frm-blanks; forward pass yields rates u_θ_s 4. Compute Bregman loss comparing u_θ_s to target rates from alignment

- **Design tradeoffs**: δ (max substitution distance): Larger δ enables longer-range corrections but risks invalid alignments; T/100 works across datasets. b_ins, b_sub: More bins increase precision but enlarge categorical output space; 64 bins is a robust default. Sampling steps k: More steps improve quality but increase runtime; ~100 steps balances both.

- **Failure signatures**: Generated sequences violate monotonicity → check boundary handling in ins/sub definitions. High MMD but low W₁,d_IET → model captures inter-event timing but not global structure; increase transformer capacity. Alignment produces many ϵ-ϵ pairs → noise distribution p_noise may be too similar to q_data; verify noise scale.

- **First 3 experiments**: 1. Train on Hawkes-1 dataset; plot cumulative N(t) vs ground truth to verify the model captures self-exciting dynamics. 2. Sweep δ ∈ [T/1000, T/10] on 2 datasets; plot d_l, d_IET, MMD to find the operating range where substitution remains valid. 3. Vary sampling steps k ∈ [8, 256] and measure runtime vs MMD on Reddit-Comments to establish inference-time scaling laws.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several arise from the methodology and evaluation:

- Can the proposed edit operations and alignment mechanisms be extended to handle marked temporal point processes with discrete or continuous event types?
- Does the quantization of edit operations into bins limit the model's ability to generate high-precision timestamps compared to autoregressive continuous-time models?
- Does the quadratic complexity of the Needleman-Wunsch alignment algorithm create a scalability bottleneck for training on datasets with very long sequence lengths?

## Limitations

- Sampling efficiency assumptions: The paper assumes Euler approximation provides sufficient accuracy for CTMC sampling, but the discretization error bound is not explicitly stated
- Alignment space validity: The auxiliary alignment space relies on Needleman-Wunsch alignment to identify valid edit operations, and the alignment quality directly impacts the validity of the Bregman divergence loss
- Discrete-continuous interface: The binning strategy works across datasets, but the paper doesn't systematically explore the sensitivity to these hyperparameters

## Confidence

- **High confidence**: The empirical performance improvements over strong baselines (autoregressive and non-autoregressive) are well-supported by quantitative metrics across diverse datasets
- **Medium confidence**: The theoretical framework connecting CTMCs to TPP generation is sound, but practical implementation details are underspecified, creating reproduction challenges
- **Low confidence**: The alignment mechanism's robustness to pathological cases is not thoroughly tested, and the paper mentions low-discrepancy sampling without detailing its implementation

## Next Checks

1. **Euler discretization error analysis**: Systematically vary the number of sampling steps k ∈ [8, 1024] on 2-3 representative datasets and plot the trade-off between MMD and runtime to establish the practical limits of the approximation

2. **Alignment quality diagnostics**: For failing cases (high MMD but low W₁,d_IET), visualize the Needleman-Wunsch alignments to identify patterns where invalid edit operations are being learned, and test whether increasing δ or adjusting alignment costs improves results

3. **Bin granularity sensitivity**: Sweep b_ins, b_sub ∈ [16, 128] and δ ∈ [T/1000, T/10] on synthetic datasets with known temporal structures to identify when discretization introduces systematic bias in generated sequences