---
ver: rpa2
title: 'Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error
  Taxonomy Construction and Large-Scale Evaluation'
arxiv_id: '2509.22565'
source_url: https://arxiv.org/abs/2509.22565
tags:
- error
- clinical
- patient
- messages
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduced a retrieval-augmented evaluation pipeline
  (RAEC) to detect errors in AI-generated patient portal messages. Using a clinician-vetted
  taxonomy of 59 error codes across 5 domains, the system leverages semantically similar
  historical message-response pairs to improve error detection accuracy.
---

# Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation

## Quick Facts
- arXiv ID: 2509.22565
- Source URL: https://arxiv.org/abs/2509.22565
- Reference count: 0
- Primary result: Retrieval-augmented evaluation improved LLM error detection concordance from 33% to 50% and F1-score from 0.256 to 0.500

## Executive Summary
This paper introduces a retrieval-augmented evaluation pipeline (RAEC) to detect errors in AI-generated patient portal messages. Using a clinician-vetted taxonomy of 59 error codes across 5 domains, the system leverages semantically similar historical message-response pairs to improve error detection accuracy. A two-stage DSPy-based LLM judge evaluated over 1,500 messages, comparing baseline and retrieval-enhanced modes. Context retrieval significantly improved performance metrics while reducing flagged errors by 12%.

## Method Summary
The system uses a two-stage DSPy-based LLM judge that first detects whether any error exists in AI-drafted messages, then classifies specific errors using a 59-code taxonomy. Retrieval augmentation pulls up to 5 similar historical patient-clinician exchanges from institutional archives, filtered by physician/department/specialty, to provide contextual grounding for the LLM judge. The taxonomy was built through iterative LLM-assisted inductive coding on 1,000 message samples, followed by physician review and manual refinement. The pipeline was evaluated on 1,570 messages, with 100 physician-validated as ground truth.

## Key Results
- Retrieval context improved error identification in domains such as clinical completeness and workflow appropriateness
- System reduced flagged errors by 12% while improving both precision and recall simultaneously
- Concordance with physician annotations rose from 33% to 50% with retrieval augmentation
- F1-score improved from 0.256 to 0.500 at error-code level with retrieval context

## Why This Works (Mechanism)

### Mechanism 1
Retrieving similar historical message-response pairs improves LLM error detection accuracy by grounding judgments in local clinical precedent. The system retrieves up to 5 semantically similar patient-clinician exchanges from institutional archives, filtered by physician/department/specialty. These exemplars are provided as context to the LLM judge, which improves its ability to distinguish clinically meaningful errors from superficial discrepancies. This works because historical clinician-authored responses encode local practice patterns and clinical reasoning that inform what constitutes an "error" in that context. However, if retrieved examples are poor matches, the grounding may mislead rather than help.

### Mechanism 2
A two-stage prompting architecture (error detection → structured classification) improves precision at granular error codes compared to single-stage evaluation. Stage 1 determines if ANY error exists and produces a summary/explanation. Only if an error is detected does Stage 2 classify it using the full taxonomy. This hierarchical approach reduces false positives at finer granularity by separating easier detection from harder classification tasks. The risk is that if Stage 1 has high false negative rate, Stage 2 never sees those cases.

### Mechanism 3
An inductively-derived, clinician-vetted error taxonomy enables more clinically meaningful error detection than generic quality metrics. The taxonomy was built through iterative LLM-assisted inductive coding on 1,000 message samples, followed by physician review and manual refinement. This produced 5 domains (Clinical Reasoning, Communication Quality, Workflow Appropriateness, Accessibility, Privacy) with 59 granular codes. The approach captures identifiable, consistent patterns in clinical messaging errors. However, the paper notes sparse exemplars for rare error codes and acknowledges workflow violations remain a blind spot since "tribal knowledge" is often undocumented.

## Foundational Learning

- **DSPy (Declarative Self-improving Python)**: The paper uses DSPy to orchestrate the two-stage LLM pipeline with structured signatures rather than ad-hoc prompting. Quick check: Can you explain the difference between a DSPy signature and a traditional prompt template?
- **Sentence Embeddings for Semantic Retrieval**: The retrieval module uses all-mpnet-base-v2 (768-dim) embeddings with cosine similarity to find similar patient messages. Quick check: Why might cosine similarity on raw patient messages fail for multi-topic threads?
- **Taxonomy Evaluation Metrics (Concordance, F1 at Multiple Granularities)**: Performance is measured at domain/subdomain/error-code levels, with McNemar's test for concordance differences. Quick check: Why does F1 drop dramatically as granularity increases?

## Architecture Onboarding

- **Component map**: Data Preprocessing -> Embedding Store -> Retrieval Module -> Two-Stage LLM Judge -> Error Taxonomy
- **Critical path**: Retrieval quality → Context grounding → Stage-1 detection → Stage-2 classification. The paper shows retrieval quality (Kendall's τ = 0.77) directly impacts downstream performance.
- **Design tradeoffs**: Retrieval granularity (currently embeds whole messages vs. potential sub-message chunking), taxonomy breadth (59 codes capture common errors but leave "long tail" sparse), LLM judge choice (paper uses OpenAI o3-mini for taxonomy construction but doesn't specify judge model)
- **Failure signatures**: High false positives on "Ambiguous or Conflicting Instructions" (reduced from 28% to 22.7% with retrieval, still top error), blind spots on workflow violations (undocumented tribal knowledge), low sensitivity at error-code level without retrieval (0.294 baseline)
- **First 3 experiments**: 1) Retrieval ablation: Run baseline vs. RAEC on held-out set; measure concordance delta at each taxonomy level, 2) Retrieval quality stress test: Manually inject poor retrievals and measure performance degradation, 3) Taxonomy coverage audit: Identify error codes with <5 exemplars; measure precision/recall specifically for these codes

## Open Questions the Paper Calls Out

### Open Question 1
Would domain-adapted encoders, hybrid lexical-semantic rankers, or clinically informed rescoring schemes improve retrieval relevance and downstream error detection accuracy beyond the generic all-mpnet-base-v2 embeddings? Current approach uses a practical but generic embedding model; no comparison to domain-specific alternatives was conducted.

### Open Question 2
Does tokenizing multi-issue patient messages into topic-coherent spans and retrieving context at sub-message granularity improve sensitivity to nuanced errors in complex communications? Current method embeds each message as a single text block, potentially obscuring multiple clinical concerns within one thread.

### Open Question 3
How can workflow-related errors—currently a blind spot for LLMs due to undocumented institutional "tribal knowledge"—be better captured by the RAEC pipeline? Retrieved reference pairs sometimes contained workflow examples, but LLMs still failed to detect violations consistently.

### Open Question 4
Will the RAEC pipeline generalize to other healthcare institutions with different clinical workflows, communication norms, and EHR systems? Study used single-institution data from Stanford Health Care across 11 specialties; external validity not assessed.

## Limitations
- The paper doesn't specify the underlying LLM model used for the guardrail stages, creating potential reproducibility issues
- Sparse exemplars for rare error codes and undocumented workflow knowledge create blind spots in the taxonomy
- Single-institution study limits generalizability to other healthcare systems with different workflows and communication norms

## Confidence
- **High confidence**: Retrieval augmentation improves overall error detection accuracy (concordance: 33% → 50%, F1: 0.256 → 0.500)
- **Medium confidence**: The two-stage prompting architecture improves precision at granular error codes
- **Medium confidence**: The 59-code taxonomy captures clinically meaningful errors

## Next Checks
1. **Model ablation study**: Compare RAEC performance using different LLM judges (GPT-4, Claude, open-source alternatives) to quantify model dependency and establish robustness bounds
2. **Retrieval quality impact analysis**: Systematically vary retrieval quality (injecting poor matches) and measure performance degradation across all taxonomy levels to identify sensitivity thresholds
3. **Rare error code audit**: Identify error codes with <5 exemplars in training data and measure precision/recall specifically for these codes to quantify coverage gaps and inform taxonomy expansion priorities