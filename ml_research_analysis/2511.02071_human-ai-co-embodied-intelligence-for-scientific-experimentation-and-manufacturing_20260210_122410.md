---
ver: rpa2
title: Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing
arxiv_id: '2511.02071'
source_url: https://arxiv.org/abs/2511.02071
tags:
- system
- apex
- fabrication
- agent
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: APEX system bridges the gap between machine intelligence and physical
  execution in scientific experimentation and manufacturing by integrating human expertise,
  agentic AI, and wearable hardware into a co-embodied intelligence framework. The
  system uses multimodal perception through MR goggles and a multi-agent reasoning
  framework to provide real-time guidance, error detection, and procedural adaptation.
---

# Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing

## Quick Facts
- arXiv ID: 2511.02071
- Source URL: https://arxiv.org/abs/2511.02071
- Authors: Xinyi Lin; Yuyang Zhang; Yuanhang Gan; Juntao Chen; Hao Shen; Yichun He; Lijun Li; Ze Yuan; Shuang Wang; Chaohao Wang; Rui Zhang; Na Li; Jia Liu
- Reference count: 40
- Primary result: APEX system achieved 51% higher step-tracking accuracy than state-of-the-art multimodal LLMs in cleanroom microfabrication

## Executive Summary
APEX system bridges the gap between machine intelligence and physical execution in scientific experimentation and manufacturing by integrating human expertise, agentic AI, and wearable hardware into a co-embodied intelligence framework. The system uses multimodal perception through MR goggles and a multi-agent reasoning framework to provide real-time guidance, error detection, and procedural adaptation. In cleanroom microfabrication, APEX system achieved 51% higher step-tracking accuracy than state-of-the-art multimodal LLMs, enabled novices to perform at expert levels, and autonomously detected/corrected errors with 96% accuracy. Critically, it co-developed a new protocol for wafer-scale fabrication of ultrathin, soft SEBS neural probes, enabling previously unattainable brain-level single-unit neural recordings within one week instead of years.

## Method Summary
APEX uses a four-agent framework: Planning Agent (generates SOPs from knowledge base), Context Agent (grounds visual frames to scene objects), Step-Tracking Agent (maintains evolving memory of recent and historical frames), and Analysis Agent (detects errors, provides feedback, logs). The system processes egocentric MR video streams (8K, 90Hz), hand/eye tracking, and SLAM-derived 3D lab maps through cloud-hosted LLMs (Gemini-2.5-flash, temp=0) using in-context learning with evolving memory management. Expert demonstrations provide behavioral baselines for skill transfer, while real-time error detection enables protocol co-optimization through closed-loop feedback.

## Key Results
- 51% higher step-tracking accuracy than GPT-4o/Gemini baselines in cleanroom tasks
- 96% accuracy in autonomous error detection and correction
- Enabled novices to achieve expert-level spin-coating thickness accuracy and RIE completion times
- Co-developed new protocol for wafer-scale SEBS neural probes, reducing development time from years to one week

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decomposition with Evolving Memory
Offloading context management and step-tracking from a single monolithic model to a specialized multi-agent framework significantly improves long-horizon procedural accuracy. The system separates duties: Planning Agent decomposes high-level goals into SOPs; Context Agent grounds visual frames to objects; Step-Tracking Agent maintains evolving memory (short-term recent frames + long-term history) to localize current action. Core assumption: Reasoning errors stem primarily from loss of temporal context over long sequences rather than vision failures alone. Evidence: Full APEX outperforms ablated versions by 41%, 28%, and 10% in step-tracking accuracy; achieved 51% higher accuracy than state-of-the-art multimodal LLMs.

### Mechanism 2: Human-in-the-Loop Skill Transfer via Behavioral Cloning
Real-time alignment of novice actions against expert behavioral data enables rapid skill acquisition without explicit retraining. During expert runs, system records multimodal data (egocentric video, hand/gaze tracking) aligned to SOP steps. When novice performs task, Analysis Agent compares live context against expert-anchored baseline to generate 3D visual cues and corrective alerts. Core assumption: Expert tacit knowledge (timing, hesitation patterns) can be captured by gaze/hand trajectories and mapped to discrete text-based SOP steps. Evidence: Novices using APEX achieved expert-level spin-coating thickness accuracy and RIE completion times, whereas unassisted novices failed to complete procedures.

### Mechanism 3: Execution-Aware Protocol Co-Optimization
Closing the loop between physical execution failures and protocol generation allows discovery of fabrication recipes unlikely to be found via text-based inference alone. Planning Agent initially proposes protocol; if Analysis Agent detects physical incompatibility (e.g., solvent dissolving substrate) or failure (e.g., transfer error), it updates memory and prompts Planning Agent to revise SOP (e.g., switch to release-float-transfer method). Core assumption: Causal link between physical failure and specific protocol parameter can be identified in real-time by vision/reasoning system. Evidence: APEX identified transfer failure for SEBS and autonomously revised workflow to use PDMS-mediated transfer, reducing development time from years to one week.

## Foundational Learning

- **Concept: Standard Operating Procedures (SOPs) as Action Primitives**
  - Why needed here: Entire APEX architecture relies on breaking complex fabrication into atomic, text-defined SOPs that act as "ground truth" for vision agents
  - Quick check: Can you define a simple 3-step SOP for making coffee, explicitly stating the "start" and "end" visual cues for each step?

- **Concept: In-Context Learning (ICL)**
  - Why needed here: Agents use ICL (prompting with examples/plans) rather than weight updates to adapt to new tasks
  - Quick check: If Context Agent misidentifies a tool, would updating system prompt with photo fix issue immediately, or would you need to retrain a model?

- **Concept: Visual SLAM (Simultaneous Localization and Mapping)**
  - Why needed here: System must anchor 3D visual overlays to physical machinery
  - Quick check: If cleanroom lighting changes drastically (e.g., blackout), which component fails first: SLAM mapping or LLM reasoning?

## Architecture Onboarding

- **Component map:** MR Goggles (8K video, Hand/Eye tracking, SLAM pose) -> Planning Agent (Generates/Retrieves SOPs & Knowledge Base) -> Context Agent (Grounds frames to JSON-like "Scene Context") -> Step-Tracking Agent (Maintains short-term frame buffer + long-term history) -> Analysis Agent (Error detection, Logging, MR Overlay rendering)

- **Critical path:** Step-Tracking Agent's memory update cycle. If "prediction interval" is too short, LLM returns stale predictions; if too long, rapid steps are skipped. This timing parameter is primary tuning knob.

- **Design tradeoffs:**
  - *Cloud vs. Edge:* Uses cloud-hosted LLMs for high reasoning capability but suffers from 32ms+ latency and security risks. On-device deployment would improve security/speed but likely reduce reasoning accuracy
  - *Static vs. Evolving Memory:* Condensing past frames improves context window efficiency but risks losing "needle in a haystack" details needed for rare error corrections

- **Failure signatures:**
  - *Looping:* Step-Tracking agent repeatedly predicts "Step 3" despite user moving to "Step 4" (caused by visual similarity between steps)
  - *Hallucinated Equipment:* Context Agent identifies tool not physically present (caused by lighting/reflection noise)
  - *Context Drift:* SLAM tracking lost, causing 3D guidance to float away from physical machinery

- **First 3 experiments:**
  1. **Static Tool Recognition:** Place 5 cleanroom tools on table. Wear MR goggles and verify Context Agent correctly identifies all 5 in JSON output without any SOP context
  2. **SOP Tracking Latency Test:** Perform rapid 5-step "dummy" procedure (e.g., touch points A, B, C, D, E quickly). Measure lag between your action and Step-Tracking Agent updating predicted step number
  3. **Error Injection:** Intentionally skip a step in known SOP (e.g., "Spin Coating"). Verify Analysis Agent detects deviation before you complete subsequent step

## Open Questions the Paper Calls Out

### Open Question 1
Can fine-tuning the multimodal LLMs utilized in APEX on the accumulated fabrication knowledge base overcome the restricted context window limitations and improve generalization to novel fabrication settings? Current implementation relies on in-context learning bounded by token limits, restricting volume of historical domain knowledge that can be leveraged simultaneously for reasoning. Evidence would be comparative benchmark showing step-tracking accuracy and protocol generation success rates of fine-tuned version against current prompt-based version across diverse, unencountered fabrication environments.

### Open Question 2
To what extent does integrating an IoT network in the cleanroom reduce workflow interruptions compared to current reliance on MR goggles for manual instrument status checks? Existing system relies on visual perception (goggles) and manual interaction to determine instrument states, creating potential bottleneck in data acquisition and user attention. Evidence would be time-motion analysis of operators using IoT-integrated APEX version versus visual-only version, measuring frequency of manual status checks and total experiment duration.

### Open Question 3
Does on-device deployment of multimodal LLMs maintain system's high error detection accuracy (96%) while successfully reducing latency associated with cloud-hosted inference? Current study utilizes powerful cloud-hosted models (e.g., Gemini-2.5-flash); trade-offs regarding model size, quantization, and reasoning capability when moving to constrained edge hardware remain untested. Evidence would be latency and accuracy metrics collected from APEX prototype running optimized, local models during live, safety-critical fabrication tasks compared against cloud-based baseline.

## Limitations
- System relies heavily on high-quality MR data and expert demonstrations, making it vulnerable to environmental noise and subjective expert behaviors
- Closed-loop protocol optimization demonstrated only for single case study, raising questions about generalizability to other fabrication domains
- Paper doesn't address edge cases where physical failures are caused by factors outside MR system's perception (e.g., chemical contamination, equipment malfunction)

## Confidence

- **High confidence:** Core architecture and step-tracking performance claims (51% accuracy improvement, 96% error detection) - directly measurable and well-supported by quantitative results
- **Medium confidence:** Novice-to-expert transfer claims - evaluation focuses on single fabrication task (SEBS neural probes), "expert-level" benchmark may be task-specific
- **Low confidence:** Protocol co-development claims - describes successful case study but lacks statistical evidence of reproducibility or demonstration across diverse failure modes

## Next Checks
1. **Cross-task generalization test:** Apply APEX to different microfabrication protocol (e.g., silicon wafer processing) and measure whether step-tracking accuracy and error detection remain consistent
2. **Robustness under environmental stress:** Evaluate system performance under degraded MR conditions (poor lighting, occlusions, rapid movement) to identify failure thresholds
3. **Protocol co-development stress test:** Intentionally introduce multiple failure types (chemical, mechanical, procedural) and assess whether system can correctly identify causes and propose effective revisions without infinite loops