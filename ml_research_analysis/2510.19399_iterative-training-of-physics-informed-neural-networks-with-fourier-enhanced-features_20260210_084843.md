---
ver: rpa2
title: Iterative Training of Physics-Informed Neural Networks with Fourier-enhanced
  Features
arxiv_id: '2510.19399'
source_url: https://arxiv.org/abs/2510.19399
tags:
- solution
- neural
- equation
- problem
- basis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces IFeF-PINN, a novel iterative training algorithm\
  \ for physics-informed neural networks that addresses spectral bias\u2014the tendency\
  \ of PINNs to struggle with high-frequency components. The method enriches the latent\
  \ space using Random Fourier Features (RFF) through a two-stage process: generating\
  \ a nominal basis with hidden layers and performing linear regression on RFF-extended\
  \ basis functions."
---

# Iterative Training of Physics-Informed Neural Networks with Fourier-enhanced Features

## Quick Facts
- **arXiv ID:** 2510.19399
- **Source URL:** https://arxiv.org/abs/2510.19399
- **Reference count:** 40
- **Primary result:** IFeF-PINN achieves relative L2 errors as low as 3.5×10⁻⁵ on Helmholtz equations and 4.3×10⁻⁵ on convection equations by mitigating spectral bias through latent-space RFF.

## Executive Summary
This paper introduces IFeF-PINN, a novel iterative training algorithm for physics-informed neural networks that addresses the spectral bias problem—the tendency of PINNs to struggle with high-frequency components in PDE solutions. The method enriches the latent space using Random Fourier Features (RFF) through a two-stage process: generating a nominal basis with hidden layers and performing linear regression on RFF-extended basis functions. For linear PDEs, this approach guarantees convexity and convergence, while empirically improving approximation of high-frequency PDEs. Extensive numerical experiments on benchmark problems demonstrate superior performance over state-of-the-art methods.

## Method Summary
IFeF-PINN implements a bi-level optimization scheme that separates feature learning from coefficient fitting. An MLP backbone generates features, which are mapped via Random Fourier Features to an extended basis. The method alternates between analytically solving a convex Quadratic Program for the output coefficients and updating the backbone weights via gradient descent. For linear PDEs, the lower-level problem is convex, ensuring global optimality, while the upper-level problem learns features that best support the physics-informed loss.

## Key Results
- IFeF-PINN achieves relative L2 errors as low as 3.5×10⁻⁵ on Helmholtz equations with frequency parameter a=100
- The method successfully captures high-frequency and multi-scale dynamics that standard PINNs fail to resolve
- Ablation studies confirm that applying RFF to the latent space (rather than input) yields superior performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling feature learning from coefficient fitting via bi-level optimization stabilizes convergence and guarantees a global optimum for the linear readout layer when solving linear PDEs.
- **Mechanism:** Standard PINNs entangle basis learning (hidden layers) with coefficient regression (output layer) in a single non-convex loss. IFeF-PINN separates these into an upper-level problem (learning basis $\omega$) and a lower-level problem (convex quadratic programming for $\theta$). This isolates the "regression" error, allowing an analytical solution (or convex solve) for the output weights, ensuring the latent space is utilized optimally for the current features.
- **Core assumption:** The underlying PDE operators are linear, ensuring the lower-level loss is quadratic and convex.
- **Evidence anchors:** Section 3.2 proves convexity for linear models; Section 3.3 describes the three-step numerical method.
- **Break condition:** For non-linear PDEs, the lower-level problem becomes non-convex, potentially stalling in local minima.

### Mechanism 2
- **Claim:** Applying Random Fourier Features to the latent space of hidden layers enriches the spectral content of basis functions, mitigating spectral bias.
- **Mechanism:** Standard neural networks exhibit bias toward low frequencies. By mapping hidden layer outputs through random Fourier mapping, the method injects high-frequency components directly into the feature space used for final linear regression. This "composite RFF" function space is theoretically shown to be more expressive than the standard feature space.
- **Core assumption:** The RFF dimension $D$ is sufficiently large and the matrix samples frequencies relevant to the solution.
- **Evidence anchors:** Section 3.1 describes the strategy; Theorem 2 proves projection error is no greater than original feature space.
- **Break condition:** If sampling frequency is insufficient relative to RFF dimension, the system becomes underdetermined, leading to rank deficiency.

### Mechanism 3
- **Claim:** Iterative alternating updates between basis generator and linear readout prevent gradient pathologies common in PINNs.
- **Mechanism:** Instead of single joint optimization, the algorithm alternates: analytically solving for optimal coefficients given current basis, then taking a gradient step on the basis using hypergradient derived via Implicit Function Theorem. This allows the basis to adapt specifically to physics-informed loss requirements.
- **Core assumption:** The hypergradient is L-smooth and the lower-level solution map is Lipschitz continuous.
- **Evidence anchors:** Section 4.1, Theorem 1 proves convergence to stationary point; Section 3 Intro identifies gradient pathologies in PINNs.
- **Break condition:** If learning rate for upper-level problem is not set within $(0, 2/L)$, the convergence guarantee is void.

## Foundational Learning

- **Concept:** **Bi-level Optimization**
  - **Why needed here:** This is the mathematical engine of IFeF-PINN. You must understand that we are optimizing a "lower-level" objective (finding the best linear fit) inside an "upper-level" objective (learning the features).
  - **Quick check question:** Can you explain why calculating the gradient for the upper level requires the Implicit Function Theorem (hypergradient) rather than standard backpropagation?

- **Concept:** **Spectral Bias (F-principle)**
  - **Why needed here:** The paper frames its entire existence around solving this specific failure mode of neural networks (learning low frequencies first).
  - **Quick check question:** Why does a standard feed-forward network struggle to fit a high-frequency sine wave compared to a low-frequency one?

- **Concept:** **Random Fourier Features (RFF)**
  - **Why needed here:** This is the architectural modification used to overcome spectral bias.
  - **Quick check question:** How does mapping a feature vector $v$ to $[\cos(Bv), \sin(Bv)]$ change the kernel of the learning problem compared to a linear dot product?

## Architecture Onboarding

- **Component map:**
  1. **Encoder ($h_\omega$):** Standard MLP mapping inputs $x \to \mathbb{R}^p$
  2. **Latent RFF ($\gamma_D$):** Non-trainable mapping $h_\omega(x) \to [\cos(B_D h_\omega), \sin(B_D h_\omega)] \in \mathbb{R}^{2D}$
  3. **Linear Readout ($\theta$):** Trainable vector $\theta \in \mathbb{R}^{2D}$ (no bias needed typically, implied by sin/cos)
  4. **Bi-level Solver:** Alternating optimizer (L-BFGS/Analytic for $\theta$, Adam for $\omega$)

- **Critical path:**
  1. **Warm Start:** Pre-train a vanilla PINN to get initial $\omega_0$ (Section 3.1)
  2. **Rank Check:** Ensure collocation points $N_u + N_f \ge 2D$ (Appendix B.1)
  3. **Lower Step:** Construct $Q(\omega)$ and $c(\omega)$; solve $\theta = -Q^{-1}c$ (using regularization $\gamma I$ if rank-deficient)
  4. **Upper Step:** Compute hypergradient (Eq. 17) and update $\omega$ via Adam

- **Design tradeoffs:**
  - **Dimension $D$:** Increasing $D$ improves expressivity (Theorem 2) but increases memory and requires more collocation points (Remark 2)
  - **RFF Scale $\sigma$:** Must be tuned to match target frequency; the paper fixes $\sigma=1$ but notes it should align with frequency content (Appendix E)
  - **Linear vs. Non-linear PDEs:** Method is robust for linear PDEs (convex lower level); non-linear PDEs require iterative gradient descent for $\theta$, losing the "one-step" convergence speed

- **Failure signatures:**
  - **Aliasing:** Occurs if sampling points ($N_u + N_f$) are too few for the chosen $D$ (Appendix B.1)
  - **Non-convergence (Non-linear):** If the lower-level problem is non-convex and the iterative solver stalls
  - **High Memory:** Forming matrix $Q$ explicitly requires large memory if $2D$ is huge (though the paper uses $D \approx 2400$ successfully)

- **First 3 experiments:**
  1. **Sanity Check (Linear 1D Convection, $\beta=50$):** Implement the bi-level loop. Verify that the lower-level analytic solve works (check rank of $Q$) and relative error drops below $10^{-4}$.
  2. **Stress Test (High-Freq Helmholtz):** Increase $D$ to 2400. Use Latin Hypercube Sampling. Check if standard PINN fails (spectral bias) and IFeF-PINN captures the oscillations (Figure 4).
  3. **Ablation (Latent vs. Input RFF):** Modify the code to apply RFF to the *input* $x$ instead of $h_\omega(x)$. Compare convergence speed to verify the paper's claim that latent-space enrichment is superior.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the bi-level optimization framework be modified to guarantee global optimality for nonlinear PDEs where the lower-level problem is non-convex?
- **Basis in paper:** [explicit] The Conclusion states that for nonlinear PDEs, "the lower-level problem becomes nonconvex... requiring iterative two-stage gradient descent updates that can stall in local minima," identifying this as a promising direction.
- **Why unresolved:** Theoretical guarantees (Theorem 1) rely on the strong convexity of the lower-level loss, which holds only for linear operators.
- **What evidence would resolve it:** A convergence proof for nonlinear operators or a modified regression objective that restores convexity for specific nonlinearities.

### Open Question 2
- **Question:** How can adaptive resampling strategies be integrated into IFeF-PINN without destabilizing the convex lower-level regression?
- **Basis in paper:** [explicit] The Conclusion notes the method is "sensitive to resampling because the bi-level objective depends strongly on minima found on a fixed dataset" and leaves this non-trivial extension for future work.
- **Why unresolved:** Resampling alters the design matrix $M(\omega)$, potentially violating the rank condition ($N_u + N_f \geq 2D$) required for a unique solution.
- **What evidence would resolve it:** A hybrid algorithm that dynamically adjusts collocation points while maintaining the positive definiteness of the $Q$ matrix.

### Open Question 3
- **Question:** What is the theoretical relationship between the Random Fourier Feature standard deviation $\sigma$ and the PDE solution's frequency content?
- **Basis in paper:** [explicit] Appendix E notes that "selection of $\sigma$ should align with the target function's frequency content" but the authors fixed $\sigma=1$, stating a detailed analysis is reserved for future work.
- **Why unresolved:** The current work relies on empirical selection or default values rather than deriving the spectral link between the kernel and the PDE.
- **What evidence would resolve it:** A theoretical analysis defining the optimal $\sigma$ based on the spectral bias of the specific differential operator.

## Limitations
- The method's theoretical guarantees strictly require linear PDEs; non-linear problems become non-convex and lose convergence guarantees.
- The bi-level optimization scales poorly with RFF dimension $D$ due to the $O((2D)^3)$ complexity of inverting the $Q$ matrix.
- The approach depends on pre-training a vanilla PINN, but the paper does not specify exact convergence criteria or duration for this warm start.

## Confidence
- **High Confidence:** The bi-level optimization framework is mathematically rigorous and the theoretical convergence proof for linear PDEs is sound. The superiority over standard PINNs on high-frequency Helmholtz and Convection problems is clearly demonstrated.
- **Medium Confidence:** The claim that latent-space RFF is superior to input-space RFF is supported by ablation studies, but the comparison is limited to one PDE type.
- **Low Confidence:** The method's robustness for highly non-linear PDEs (beyond Burgers) is not tested. The impact of RFF scale parameter $\sigma$ is mentioned but not thoroughly explored.

## Next Checks
1. **Scaling Analysis:** Systematically vary $D$ from 100 to 5000 and measure both accuracy and computation time to identify the practical scaling limits of the bi-level solver.
2. **Non-linear Stress Test:** Apply the method to a strongly non-linear PDE (e.g., Navier-Stokes or Korteweg-de Vries) and compare the convergence speed and final accuracy to standard PINNs and other spectral-bias-mitigation methods.
3. **Ablation on Pre-training:** Run experiments with and without pre-training, varying the number of pre-training epochs, to quantify the contribution of the warm start to final performance.