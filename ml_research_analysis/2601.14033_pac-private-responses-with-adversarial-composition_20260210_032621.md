---
ver: rpa2
title: PAC-Private Responses with Adversarial Composition
arxiv_id: '2601.14033'
source_url: https://arxiv.org/abs/2601.14033
tags:
- privacy
- noise
- adversary
- responses
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a PAC-private response framework that enables
  linear composition under adversarial querying, addressing the challenge of protecting
  machine learning models from membership inference attacks when deployed behind APIs.
  The core idea is to adaptively calibrate noise based on the posterior distribution
  of the secret data given the interaction history, achieving instance-dependent privacy
  guarantees that exploit the stability of model responses.
---

# PAC-Private Responses with Adversarial Composition

## Quick Facts
- arXiv ID: 2601.14033
- Source URL: https://arxiv.org/abs/2601.14033
- Reference count: 40
- One-line primary result: Achieves PAC-private ML API responses with linear composition under adaptive adversarial querying, maintaining 87.79% CIFAR-10 accuracy and MIA success ≤ 51.08% with b=2⁻³².

## Executive Summary
This paper introduces a PAC-private response framework that enables linear composition of mutual information guarantees under adaptive adversarial querying. The core innovation is adaptive noise calibration based on the posterior distribution of the secret data, which maintains per-query privacy budgets while supporting one million queries. The framework is instantiated for ML response privacy, demonstrating strong utility-privacy tradeoffs across tabular, vision, and NLP tasks. A private model distillation protocol is also proposed, enabling unlimited inference while inheriting the same privacy guarantees.

## Method Summary
The method constructs a finite secret space of m=128 subsets of the training data, trains separate models for each subset offline, and maintains a posterior belief over these subsets during online query processing. For each query, it computes predictions across all m models, calibrates Gaussian noise based on the current posterior and per-step MI budget b, and releases the noisy prediction of the true model. The posterior is updated via Bayesian inference after each query. The framework supports both one-hot and softmax output formats, with one-hot being more stable and requiring less noise. A distillation protocol uses private responses to label public data with confidence filtering, enabling the creation of publishable student models.

## Key Results
- Achieves 87.79% accuracy on CIFAR-10 with per-step MI budget b=2⁻³², supporting one million queries with MIA success bounded to 51.08%
- Demonstrates superior privacy-utility tradeoffs compared to (ε, δ)-DP, particularly for extremely small privacy budgets
- Shows practical viability across diverse tasks: CIFAR-10/100 (vision), Census Income/Bank Marketing (tabular), IMDb/AG News (NLP)
- Proves linear composition of MI guarantees under adaptive adversarial querying, enabling long-term privacy protection

## Why This Works (Mechanism)

### Mechanism 1: Posterior-Aware Adaptive Noise Calibration
The framework maintains a belief state representing the posterior distribution of the secret given interaction history. At each query, noise covariance is calibrated based on this posterior rather than the original prior, ensuring the conditional mutual information bound per query is maintained. This adaptive approach enables linear composition under adversarial querying where the adversary's queries depend on the observed transcript.

### Mechanism 2: Instance-Dependent Noise via Output Stability
Noise calibration exploits the covariance of outputs across possible training sets, leveraging output stability to reduce required noise. When predictions are stable across most training sets (e.g., one-hot predictions agree), mutual information is naturally low, requiring minimal noise. This achieves better privacy-utility tradeoffs than sensitivity-based methods by rewarding output stability.

### Mechanism 3: Private Distillation with Confidence Filtering
A publishable student model can be distilled from PAC-private responses by using them to label a public dataset. A hypothesis test filters responses where the noisy label is likely correct, bounding mislabeling probability. This enables unlimited inference while inheriting the original privacy guarantees through the confidence-filtered distillation process.

## Foundational Learning

- **Mutual Information (MI) and privacy**: MI bounds the information leakage between secret and released outputs, limiting adversary's posterior advantage. Quick check: Can you explain why bounding I(S; M(S)) limits an adversary's posterior advantage?

- **Posterior belief update**: Bayesian inference maintains the posterior over the secret space as queries arrive. Quick check: Given a prior and observed noisy output, how would you update the posterior over a finite set of possible secrets?

- **Membership Inference Attacks (MIA)**: The framework's guarantees translate to MIA success bounds, with practical evaluation using optimal MIA strategies. Quick check: In the paper's threat model, what prior success rate does an adversary have, and how does the PAC guarantee limit improvement?

## Architecture Onboarding

- **Component map**: Offline model training (128 subsets) -> Online per-query inference + noise calibration + posterior update -> (Optional) Distillation pipeline

- **Critical path**: Offline model training -> Online per-query inference across m models + noise calibration + posterior update -> Distillation with confidence filtering

- **Design tradeoffs**:
  - m (number of subsets): Larger m reduces random guessing but increases pre-training cost
  - Output format: One-hot is more stable (lower noise) vs. softmax (more information)
  - Per-step MI budget b: Tighter budgets reduce long-term leakage but increase per-query noise

- **Failure signatures**:
  - Posterior collapse to uniform (no learning from queries)
  - MIA success above theoretical bound (implementation error)
  - Low distillation accuracy (insufficient public data or high noise)

- **First 3 experiments**:
  1. Validate noise calibration on small dataset with known b, verify I(S; responses) ≤ b empirically
  2. Test posterior update on toy problem, inspect evolution matches Bayesian update
  3. Evaluate MIA defense under different b and T, confirm empirical success ≤ theoretical bound

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the framework extend to continuous or intractable secret spaces where exact posterior tracking is infeasible? (Efficient estimation methods needed)

- **Open Question 2**: Can the mechanism adapt to secure LLM inference under adaptive querying? (High-dimensional generative outputs differ from classification)

- **Open Question 3**: How does the privacy-utility tradeoff compare when applied to iterative training algorithms like SGD? (Current focus is on inference services)

- **Open Question 4**: How does performance change against adversaries using optimized, information-maximizing querying strategies? (Optimal query sequences are computationally intractable)

## Limitations

- **Scalability constraints**: Training 128 models and performing inference across all models per query may limit practical applicability to large-scale deep learning systems
- **Computational complexity**: O(m d²) posterior updates per query could become prohibitive for high-dimensional outputs or larger secret spaces
- **Empirical validation scope**: Evaluation focuses on synthetic secret spaces from dataset subsets; real-world arbitrary secret definitions remain unverified

## Confidence

**High Confidence**:
- Linear composition guarantee under adaptive querying is mathematically proven
- PAC privacy fundamentally reduces to mutual information bounds
- MIA success rate bounds correctly derived from MI guarantees

**Medium Confidence**:
- Practical utility-accuracy tradeoffs depend on implementation details not fully specified
- Distillation protocol theoretical validity may vary with public dataset quality

**Low Confidence**:
- Comparison to (ε, δ)-DP assumes optimal adversary behavior
- Long-term posterior stability under continuous querying beyond tested scenarios

## Next Checks

1. **Scalability Test**: Implement on larger dataset (ImageNet subset), measure wall-clock latency and memory footprint, verify privacy guarantees hold at scale

2. **Robustness to Secret Space Construction**: Vary construction methods (different m values, non-uniform distributions), measure impact on privacy guarantees and utility, test with arbitrary secret definitions

3. **Adversary Strategy Validation**: Implement multiple adversary strategies beyond optimal MIA, including adaptive adversaries exploiting potential implementation artifacts, confirm empirical MIA success remains bounded by theoretical guarantee