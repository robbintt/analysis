---
ver: rpa2
title: 'ContractEval: A Benchmark for Evaluating Contract-Satisfying Assertions in
  Code Generation'
arxiv_id: '2510.12047'
source_url: https://arxiv.org/abs/2510.12047
tags:
- contract
- inputs
- code
- contracts
- contracteval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ContractEval introduces a benchmark for measuring whether LLM-generated\
  \ code correctly implements assertion-level contracts\u2014specifically, whether\
  \ programs reject ill-formed inputs by raising intended assertions. The core method\
  \ combines a neuro-symbolic pipeline that uses an LLM to convert contract assertions\
  \ into SMT constraints, and an SMT solver to synthesize contract-violating test\
  \ cases that precisely target selected assertions while satisfying others."
---

# ContractEval: A Benchmark for Evaluating Contract-Satisfying Assertions in Code Generation

## Quick Facts
- arXiv ID: 2510.12047
- Source URL: https://arxiv.org/abs/2510.12047
- Reference count: 16
- Standard prompting yields 0% contract satisfaction on contract-violating tests

## Executive Summary
ContractEval introduces a benchmark for measuring whether LLM-generated code correctly implements assertion-level contracts—specifically, whether programs reject ill-formed inputs by raising intended assertions. The core method combines a neuro-symbolic pipeline that uses an LLM to convert contract assertions into SMT constraints, and an SMT solver to synthesize contract-violating test cases that precisely target selected assertions while satisfying others. Across five models, standard prompting yields 0% contract satisfaction on contract-violating tests, revealing a blind spot in current evaluation. Adding a small set of contract-violation examples increases contract satisfaction to 49–53% while preserving 92% of original functional correctness. This demonstrates that functional correctness on well-formed inputs does not imply correct contract enforcement, and highlights the need for contract-aware evaluation in code generation.

## Method Summary
ContractEval evaluates contract satisfaction through a neuro-symbolic pipeline. First, an LLM translates task contracts into SMT-LIB constraints over a canonical ADT (abstract data type). An SMT solver then synthesizes contract-violating test cases (CVTs) by enumerating non-empty subsets of violated clauses. Each CVT must pass feasibility filtering: executable without assertions and trigger the targeted assertion with assertions. Five prompting strategies are evaluated: Standard, Contract Specification (CS), and Example-Augmented Specification (EAS). Models include gemma-3-12B-it, DeepSeek-R1-Distill-Qwen-14B, Qwen3-14B, Phi-4-reasoning, and Phi-4-reasoning-plus. Metrics include pass@1 on well-formed tests, Contract Satisfaction Rate (CSR) on CVTs, and auxiliary metrics like CodeBLEU and LLM-as-judge.

## Key Results
- Standard prompting achieves 0% contract satisfaction on contract-violating tests across all models
- EAS prompting improves contract satisfaction to 49–53% while maintaining 92% functional correctness
- Contract satisfaction remains low (5–19%) even with CS prompting, showing specification alone is insufficient
- Functional correctness on well-formed inputs does not imply correct contract enforcement

## Why This Works (Mechanism)
ContractEval addresses a fundamental gap in code generation evaluation by focusing on assertion-level contract satisfaction rather than just functional correctness. The neuro-symbolic approach enables precise targeting of contract violations by translating high-level assertions into SMT constraints, allowing systematic generation of test cases that violate specific contract clauses. This methodology reveals that LLMs can produce functionally correct code that fails to enforce intended contracts, particularly when standard prompting lacks contract-violation examples. The benchmark demonstrates that providing examples of contract violations during prompting significantly improves enforcement without sacrificing functional correctness.

## Foundational Learning
**SMT constraint solving** - Converts logical assertions into solvable mathematical constraints
*Why needed*: Enables systematic generation of contract-violating inputs
*Quick check*: Verify Z3 can solve simple arithmetic constraints from translated assertions

**Contract-violating test case synthesis** - Generates inputs that specifically violate contract clauses while satisfying others
*Why needed*: Tests whether code correctly rejects invalid inputs via intended assertions
*Quick check*: Confirm synthesized CVTs trigger targeted assertions when run with contract enforcement

**Contract satisfaction rate (CSR)** - Metric measuring how often generated code correctly raises intended assertions on CVTs
*Why needed*: Quantifies enforcement of input validation contracts beyond functional correctness
*Quick check*: Calculate CSR for simple assertion patterns on reference implementations

## Architecture Onboarding
**Component map**: LLM -> SMT-LIB translation -> SMT solver -> CVT synthesis -> Feasibility filter -> Benchmark evaluation
**Critical path**: Contract specification → LLM translation → SMT solving → CVT generation → Model evaluation
**Design tradeoffs**: Neuro-symbolic precision vs. computational cost of SMT enumeration; targeted CVTs vs. broader robustness coverage
**Failure signatures**: 
- CVTs crashing with generic errors even without assertions
- High CSR but collapsed pass@1 under EAS prompting
- Near-zero metrics with CoT prompting on Phi models

**Exactly 3 first experiments**:
1. Reproduce CVT synthesis pipeline with Z3 solver to verify feasibility filtering works correctly
2. Implement all three prompting strategies and confirm 0%→49-53% CSR improvement pattern
3. Compute conditional CSR metric (E[c|f=1]) to verify 92% functional correctness preservation

## Open Questions the Paper Calls Out
**Open Question 1**: Can novel prompting or fine-tuning methods simultaneously achieve high functional correctness (pass@1) and near-perfect contract satisfaction (CSR)?
*Basis in paper*: Section 6.3 notes that "none of the tested prompting variants achieves both high pass@1 and near-perfect CSR," explicitly motivating future methods.

**Open Question 2**: Can the benchmark scale without relying on expensive SMT enumeration and potentially biased LLM-judges for alignment?
*Basis in paper*: Section 8 identifies "Reducing reliance on model-based judgments and improving efficiency" as practical directions for scaling ContractEval.

**Open Question 3**: Does combining ContractEval's targeted CVTs with fuzzing-style perturbations improve coverage of edge cases while maintaining interpretability?
*Basis in paper*: Section 8 suggests future versions could "combine ContractEval's targeted CVTs with complementary robustness tests (e.g., fuzzing-style perturbations)."

## Limitations
- Unspecified prompt templates for Standard, CS, and EAS strategies limit exact reproduction
- Evaluation uses only pass@1 rather than full pass@k curves, potentially understating model robustness
- Feasibility filtering criteria and implementation details not fully specified

## Confidence
- **High confidence**: Core finding that standard prompting yields 0% contract satisfaction on CVTs
- **Medium confidence**: 0%→49-53% CSR improvement with EAS prompting depends on specific prompt construction
- **Low confidence**: Generalizability to other code generation benchmarks or assertion styles

## Next Checks
1. Reproduce CVT synthesis pipeline using Z3 solver and verify feasibility filtering correctly identifies contract-violating test cases
2. Implement all three prompting strategies with exact prompt templates and evaluate two models to confirm CSR improvement pattern
3. Compute conditional CSR metric (E[c|f=1]) to verify 92% functional correctness preservation under EAS prompting