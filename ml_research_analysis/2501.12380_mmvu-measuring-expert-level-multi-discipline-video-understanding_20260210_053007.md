---
ver: rpa2
title: 'MMVU: Measuring Expert-Level Multi-Discipline Video Understanding'
arxiv_id: '2501.12380'
source_url: https://arxiv.org/abs/2501.12380
tags:
- video
- reasoning
- answer
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMVU is a benchmark designed to evaluate multimodal foundation
  models on expert-level reasoning over specialized-domain videos. It includes 3,000
  expert-annotated examples across 27 subjects spanning Science, Healthcare, Humanities
  & Social Sciences, and Engineering.
---

# MMVU: Measuring Expert-Level Multi-Discipline Video Understanding

## Quick Facts
- arXiv ID: 2501.12380
- Source URL: https://arxiv.org/abs/2501.12380
- Reference count: 40
- Primary result: Benchmark for expert-level video understanding across 27 subjects, 32 models evaluated, System-2 models perform best but all fall short of human expertise

## Executive Summary
MMVU is a benchmark designed to evaluate multimodal foundation models on expert-level reasoning over specialized-domain videos. It includes 3,000 expert-annotated examples across 27 subjects spanning Science, Healthcare, Humanities & Social Sciences, and Engineering. Each example is accompanied by expert-annotated reasoning rationales and domain knowledge. Evaluation of 32 frontier models reveals that while the latest o1 and Gemini 2.0 Flash Thinking achieve the highest performance, all models still fall short of human-level expertise. The benchmark highlights challenges in integrating domain knowledge with visual perception and underscores the need for improved expert-level video understanding in specialized domains.

## Method Summary
The MMVU benchmark employs a textbook-guided annotation pipeline where domain concepts are identified in textbooks before video selection, ensuring examples require both visual perception and theoretical knowledge. The dataset contains 3,000 expert-annotated QA pairs (1,858 multiple-choice, 1,142 open-ended) over 1,529 YouTube videos, with validation (1,000 questions) public and test (2,000 questions) hidden. Evaluation uses GPT-4o to extract answers from model outputs and compare against ground truth, with both Chain-of-Thought and Direct Answer modes tested. Models are evaluated using 2-32 frames depending on context window limitations, and audio is excluded to prevent shortcuts.

## Key Results
- System-2 models (o1, Gemini 2.0 Flash Thinking) achieve highest performance on expert video tasks
- All models fall short of human-level expertise despite advanced reasoning capabilities
- Heavy reliance on textual information accounts for 20% of model errors
- Misuse or lack of domain knowledge represents 27% of failures
- Visual perception errors constitute 18% of incorrect answers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** System-2 thinking (long Chain-of-Thought) appears to improve performance on expert-level video tasks by enabling iterative error correction and knowledge retrieval.
- **Mechanism:** Models like o1 and Gemini 2.0 Flash Thinking utilize increased test-time compute to decompose visual inputs and reason over them step-by-step, rather than relying on immediate pattern matching.
- **Core assumption:** The model possesses the requisite domain knowledge in its pre-training data but requires sequential reasoning steps to recall and apply it correctly to visual inputs.
- **Evidence anchors:**
  - [Section 4.2] "Models capable of System-2 thinking and employing long CoT demonstrate significant performance advantages... o1 and Gemini 2.0 Flash Thinking achieved the top two results."
  - [Abstract] "The latest System-2-capable models... achieved the highest performance... approaching human expertise."

### Mechanism 2
- **Claim:** A textbook-guided data pipeline ensures that the evaluation measures deep knowledge integration rather than superficial visual feature matching.
- **Mechanism:** By anchoring annotations in domain-specific concepts identified in textbooks *before* selecting videos, the benchmark forces the creation of examples where visual cues are intrinsically linked to theoretical knowledge.
- **Core assumption:** The selected videos contain sufficient visual information to resolve the question, and the "Ground-truth" is strictly derivable from the video content combined with domain knowledge.
- **Evidence anchors:**
  - [Section 3.2] "We employ a textbook-guided example annotation pipeline designed to capture both the breadth of knowledge and depth of reasoning."

### Mechanism 3
- **Claim:** Explicit annotation of "Reasoning Rationales" enables fine-grained diagnosis of model failures, distinguishing between perception errors and knowledge application errors.
- **Mechanism:** By providing a structured breakdown of the solution (Visual Perception -> Knowledge Retrieval -> Logic), researchers can pinpoint if a model failed to see the object or failed to map the object to its scientific function.
- **Core assumption:** The annotated rationale represents the "optimal" reasoning path, and deviations from this path constitute errors rather than alternative valid reasoning strategies.
- **Evidence anchors:**
  - [Section 4.3] "We identify following six primary errors... Visual Perception Error (18%)... Misuse or Lack Domain Knowledge in Visual Perception (20%)."

## Foundational Learning

- **Concept:** **Chain-of-Thought (CoT) vs. Direct Answering**
  - **Why needed here:** The paper demonstrates that while Direct Answering is faster, it significantly underperforms on expert tasks. CoT is a prerequisite capability for any system aiming to perform well on MMVU.
  - **Quick check question:** Can your model explain the intermediate steps (e.g., "I see a white precipitate, therefore...") before outputting the final multiple-choice letter?

- **Concept:** **Visual-Knowledge Alignment**
  - **Why needed here:** Models often fail not because they can't "see," but because they can't map visual features (e.g., a specific waveform on an oscilloscope) to domain concepts (e.g., "High-pass filter").
  - **Quick check question:** When presented with a specialized video, does the model describe the pixels ("a line goes up") or the semantics ("this indicates voltage amplification")?

- **Concept:** **Data Contamination & "Open-Book" Evaluation**
  - **Why needed here:** The paper defines "Human Performance" using Open-Book settings (access to textbooks). Evaluating AI requires distinguishing between memorized test data and genuine reasoning capabilities.
  - **Quick check question:** Is the model relying on the video input, or is it answering correctly because the specific textbook diagram was in its training set?

## Architecture Onboarding

- **Component map:** Video Encoder (e.g., ViT) -> Multimodal Projector -> LLM Backbone
- **Critical path:**
  1. Frame Sampling: Models use 2â€“32 frames; ensure sufficient temporal coverage without exceeding context windows
  2. Prompting: Use specific CoT prompt from Appendix B.2
  3. Answer Extraction: Handle outputs like "The answer is B" or "B" uniformly

- **Design tradeoffs:**
  - Native Video vs. Multi-Image: Some models use image sequences instead of native video support
  - Audio Exclusion: Benchmark explicitly excludes audio to prevent shortcuts

- **Failure signatures:**
  - Heavy Reliance on Textual Information: Model answers using general knowledge without watching video
  - Visual Perception Error: Model hallucinates objects or misidentifies visual elements
  - Misuse of Domain Knowledge: Model recognizes pattern but applies wrong scientific concept

- **First 3 experiments:**
  1. Baseline CoT Evaluation: Run validation set on GPT-4o and Qwen2-VL using CoT prompts
  2. Frame Ablation: Test impact of varying frames (4 vs. 32) on temporal reasoning questions
  3. Error Taxonomy Audit: Manually classify 50 error cases from top-performing model using taxonomy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can multimodal foundation models be trained or prompted to reduce their heavy reliance on textual information and effectively ground their reasoning in dynamic visual content?
- **Basis in paper:** [explicit] The authors explicitly identify "Heavy Reliance on Textual Information" as a primary error category (20% of cases)
- **Why unresolved:** Current models appear to default to linguistic priors or option analysis, treating the video as secondary or ignoring it entirely when textual cues are available
- **What evidence would resolve it:** A demonstration of a training methodology or attention mechanism that results in a statistically significant reduction in "text-only" reasoning errors on the MMVU validation set

### Open Question 2
- **Question:** What specific mechanisms are required to enable foundation models to correctly retrieve and integrate domain-specific knowledge (e.g., chemical equations, medical definitions) with visual perception during expert-level video reasoning?
- **Basis in paper:** [explicit] The error analysis identifies "Misuse or Lack Domain Knowledge in Reasoning" as the largest error category (27%)
- **Why unresolved:** While models possess parametric knowledge, they fail to activate the correct domain-specific rules when observing corresponding visual phenomena
- **What evidence would resolve it:** A model architecture that successfully utilizes external knowledge retrieval or improved knowledge grounding to reduce "knowledge lack" errors

### Open Question 3
- **Question:** Can open-source multimodal models be developed to replicate the "test-time compute" and long Chain-of-Thought strategies of proprietary System-2 models to bridge the performance gap in expert-level video understanding?
- **Basis in paper:** [explicit] The paper concludes that "System-2 thinking demonstrates effectiveness" with o1 and Gemini 2.0 Flash Thinking leading performance
- **Why unresolved:** Current open-source models lag significantly behind proprietary System-2 models, lacking the architecture or inference strategies to perform extended reasoning
- **What evidence would resolve it:** The release of an open-source model utilizing long-Chain-of-Thought reasoning that achieves performance parity with o1 on the MMVU benchmark

## Limitations

- Evaluation relies on GPT-4o as judge, introducing potential systematic bias that is difficult to quantify
- Hidden test set prevents independent verification of reported results
- Paper identifies performance gaps but does not systematically investigate whether gaps stem from architectural limitations or insufficient training data

## Confidence

- **High Confidence:** Benchmark design methodology (textbook-guided annotation, expert reasoning rationales) is well-specified and reproducible
- **Medium Confidence:** Comparative performance claims between models are credible but limited by GPT-4o judge dependency and lack of independent verification
- **Low Confidence:** Claim that System-2 thinking is primary driver of performance improvements is based on correlation rather than controlled ablation studies

## Next Checks

1. **Judge Consistency Validation:** Run same model outputs through multiple independent judges (GPT-4, GPT-4o-mini, Claude) to quantify variance and establish confidence intervals
2. **Frame Dependency Analysis:** Systematically vary number of input frames (1, 4, 8, 16, 32) across stratified sample to quantify relationship between visual context and performance
3. **Knowledge vs. Perception Ablation:** Create synthetic test cases where visual content is held constant but domain knowledge requirement is varied, or vice versa, to isolate performance limitations