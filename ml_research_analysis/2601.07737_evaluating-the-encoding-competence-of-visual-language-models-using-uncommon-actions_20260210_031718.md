---
ver: rpa2
title: Evaluating the encoding competence of visual language models using uncommon
  actions
arxiv_id: '2601.07737'
source_url: https://arxiv.org/abs/2601.07737
tags:
- visual
- language
- semantic
- image
- uncommon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UAIT (Uncommon-sense Action Image-Text dataset),
  a novel benchmark designed to evaluate the semantic understanding ability of visual
  language models (VLMs) in counter-common sense action scenes. Unlike existing datasets
  focusing on common visual scenes, UAIT challenges models with grammatically reasonable
  but semantically counter-common sense image-text pairs.
---

# Evaluating the encoding competence of visual language models using uncommon actions

## Quick Facts
- arXiv ID: 2601.07737
- Source URL: https://arxiv.org/abs/2601.07737
- Authors: Chen Ling; Nai Ding
- Reference count: 21
- Key outcome: Visual language models perform significantly worse than humans (0.36-0.69 vs 0.96 accuracy) on uncommon-sense action scenes where agent-patient relationships are reversed

## Executive Summary
This paper introduces UAIT (Uncommon-sense Action Image-Text dataset), a novel benchmark designed to evaluate visual language models' (VLMs) semantic understanding ability in counter-common sense action scenes. Unlike existing datasets focusing on common visual scenes, UAIT challenges models with grammatically reasonable but semantically counter-common sense image-text pairs. The dataset is constructed using a semi-automated pipeline involving large language models, few-shot prompt engineering, and text-to-image generation. Each sample includes a multiple-choice question testing fine-grained reasoning.

The study demonstrates that all evaluated VLMs perform significantly worse than humans in semantic judgment, especially in distinguishing grammatical correctness from semantic rationality. Further fine-tuning experiments show that even lightweight models can improve accuracy after directional adaptation, though a significant gap remains compared to human performance. This research highlights key weaknesses in current VLMs and provides diagnostic tools for developing robust models with real visual semantic reasoning capabilities.

## Method Summary
The paper employs a semi-automated pipeline to create UAIT: (1) filter VerbNet verbs for semantic non-interchangeability where subject-object swaps change meaning, (2) generate common-sense sentences with LLM (Qwen2), (3) swap agent/patient to create uncommon-sense text, (4) generate corresponding images via Stable Diffusion 3.5-medium, and (5) create binary-choice questions. The dataset contains 400 synthetic images with paired questions. VLMs (LLaVA, Qwen2-VL) and CLIP-based encoders are evaluated on binary classification accuracy. Fine-tuning experiments use LLaVA1.5 with LoRA (70/30 train/test split) to demonstrate directional adaptation potential.

## Key Results
- VLMs achieve 0.36-0.69 accuracy on UAIT, significantly below human baseline of 0.96
- Models show strong bias toward selecting common-sense text descriptions even for uncommon images
- LoRA fine-tuning improves LLaVA1.5 accuracy from 0.36 to 0.79 on test split
- CLIP-based models perform close to random guessing (0.5), suggesting weak agent-patient relationship understanding

## Why This Works (Mechanism)

### Mechanism 1: Statistical Co-occurrence Interference
VLMs frequently default to language priors rather than visual evidence when identifying agent-patient relationships. The model learns joint probability distributions from training data, causing high prior probability of common relationships (e.g., "tiger drags rabbit") to override visual features when presented with counter-common sense images (e.g., "rabbit drags tiger").

### Mechanism 2: Semantic Role Reversal as Diagnostic Stress Test
Swapping agents and patients in action verbs isolates "structural understanding" from "object recognition." By selecting verbs with semantic non-interchangeability, the dataset forces models to explicitly parse who is doing what. If models rely solely on detecting subject/object features without binding them to the action, they fail the classification.

### Mechanism 3: Directional Adaptation via Low-Rank Alignment
LoRA introduces trainable rank decomposition matrices to frozen pre-trained weights. Training on synthetic UAIT data adjusts attention weights to prioritize visual directional cues (subject â†’ object) over corpus frequency, effectively breaking the prior bias for this specific task.

## Foundational Learning

- **Concept: Semantic Role Labeling (Agent vs. Patient)**
  - Why needed here: The core evaluation depends on distinguishing who acts versus who receives the action
  - Quick check question: In "The rabbit drags the tiger," which noun is the agent and which is the patient? If they swap, does the physical feasibility change?

- **Concept: Frequency Bias / Priors in VLMs**
  - Why needed here: Understanding that models are "lazy" reasoners preferring high-probability text completions over low-probability visual truths
  - Quick check question: Why might a model describe an image as "a man holding a baby" even if the image shows "a baby holding a man"?

- **Concept: Synthetic Data Generation (Diffusion Models)**
  - Why needed here: The paper uses Stable Diffusion to create impossible scenarios for counter-factual data
  - Quick check question: Why can't we just scrape Google Images for "rabbit dragging tiger"? What tool does the paper use instead?

## Architecture Onboarding

- **Component map:** VerbNet -> LLM (Qwen2) -> Diffusion (SD3.5) -> VLMs (LLaVA, Qwen2-VL) -> Binary classification
- **Critical path:** 1. Verb Filtering from VerbNet, 2. Text Generation with LLM, 3. Image Generation via Diffusion, 4. Evaluation with VLM binary-choice
- **Design tradeoffs:** Synthetic vs. Real data (solves scarcity but may introduce artifacts), Binary vs. Open-Ended questions (reduces CoT effects but loses explainability)
- **Failure signatures:** Random Guessing (~50% indicates no verb-scene binding), Common-Choice Bias (>50% error indicates statistical prior interference)
- **First 3 experiments:** 1. Baseline Assessment on standard VLMs, 2. Contrastive Probe with CLIP-style models, 3. LoRA Intervention for directional adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating explicit semantic role labelers or structured knowledge graphs into VLMs improve accuracy on uncommon-sense tasks more effectively than standard fine-tuning?
- Basis in paper: Section 4.7.1 suggests future models should integrate stronger syntactic analyzers and semantic role taggers
- Why unresolved: Experiments relied on existing model architectures without external semantic role modeling modules
- What evidence would resolve it: Comparative evaluation showing augmented VLMs outperform standard fine-tuned models on UAIT benchmark

### Open Question 2
- Question: Can "counter-common sense robustness training" mechanisms successfully enable models to prioritize visual evidence over statistical priors?
- Basis in paper: Section 4.7.2 suggests introducing counter-common sense robustness training mechanisms to force models to identify irrationality
- Why unresolved: Proposed adversarial training mechanisms were not implemented despite showing fine-tuning improves performance
- What evidence would resolve it: Models trained with proposed mechanisms maintain high accuracy on UAIT without losing performance on standard benchmarks

### Open Question 3
- Question: Do the observed failures in semantic reasoning generalize across different cultural contexts and languages?
- Basis in paper: Section 5.3 notes current study is limited to English and proposes building multilingual UAIT
- Why unresolved: Current dataset is monolingual, unknown if statistical co-occurrence reliance is culturally specific
- What evidence would resolve it: Evaluation results from localized, multilingual version showing whether performance degrades or improves in non-English contexts

## Limitations
- Reliance on synthetic data generation may introduce artifacts that models learn to exploit rather than demonstrating genuine semantic understanding
- Binary-choice format may oversimplify the complexity of semantic reasoning
- Evaluation focuses primarily on action verbs with clear directional semantics, potentially missing other semantic phenomena

## Confidence

**High Confidence:** Core finding that VLMs struggle with counter-common sense scenarios and exhibit strong statistical prior interference (supported by experimental results with clear human baseline).

**Medium Confidence:** Mechanism explaining failures as "statistical co-occurrence interference" is plausible but not definitively proven (alternative explanations cannot be ruled out).

**Low Confidence:** Claim that LoRA fine-tuning represents a general solution for directional adaptation lacks sufficient validation (improvement may not generalize to real-world uncommon scenarios).

## Next Checks

1. **Cross-dataset generalization test:** Evaluate fine-tuned models on naturally occurring counter-common sense images (e.g., from surveillance footage) to verify adaptation transfers beyond synthetic data.

2. **Ablation on synthetic artifacts:** Systematically vary Stable Diffusion generation parameters to determine if performance correlates with image artifact strength rather than semantic content.

3. **Multi-choice extension:** Re-run key experiments with 4-choice questions including semantically related distractors to assess whether binary format artificially simplifies the semantic reasoning task.