---
ver: rpa2
title: 'MeanFlowSE: one-step generative speech enhancement via conditional mean flow'
arxiv_id: '2509.14858'
source_url: https://arxiv.org/abs/2509.14858
tags:
- speech
- enhancement
- meanflowse
- generative
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MeanFlowSE introduces a one-step generative model for speech enhancement
  that learns an average velocity field instead of an instantaneous velocity field.
  By using a Jacobian-vector product to instantiate the MeanFlow identity, it directly
  supervises finite-interval displacement while remaining consistent with instantaneous
  field constraints.
---

# MeanFlowSE: one-step generative speech enhancement via conditional mean flow

## Quick Facts
- arXiv ID: 2509.14858
- Source URL: https://arxiv.org/abs/2509.14858
- Reference count: 0
- Primary result: One-step generative model achieving ESTOI of 0.881, SI-SDR of 19.975 dB, and speaker similarity of 0.892 on VoiceBank-DEMAND

## Executive Summary
MeanFlowSE introduces a novel one-step generative model for speech enhancement that learns an average velocity field instead of an instantaneous velocity field. By using a Jacobian-vector product to instantiate the MeanFlow identity, it directly supervises finite-interval displacement while remaining consistent with instantaneous field constraints. At inference, it performs single-step generation via backward-in-time displacement, eliminating the need for iterative ODE solvers. On VoiceBank-DEMAND, the single-step model achieves superior quality and efficiency metrics without knowledge distillation.

## Method Summary
MeanFlowSE operates by learning a conditional mean flow between noisy and clean speech signals. The model employs a Jacobian-vector product (JVP) to instantiate the MeanFlow identity, which allows direct supervision of finite-interval displacement while maintaining consistency with instantaneous field constraints. During inference, the model generates enhanced speech in a single backward-in-time displacement step, avoiding the computational overhead of iterative ODE solvers. This approach enables efficient real-time speech enhancement with competitive quality metrics.

## Key Results
- Single-step model achieves ESTOI of 0.881 on VoiceBank-DEMAND
- Achieves SI-SDR of 19.975 dB with speaker similarity of 0.892
- Demonstrates real-time factor (RTF) of 0.11, outperforming multistep baselines

## Why This Works (Mechanism)
MeanFlowSE leverages mean flow matching to learn the average velocity field between noisy and clean speech distributions. By directly supervising finite-interval displacement through the Jacobian-vector product, the model captures the essential transformation between these distributions more efficiently than traditional instant flow approaches. The backward-in-time generation at inference provides a computationally efficient path from noisy to clean speech without requiring iterative solutions.

## Foundational Learning
- **Jacobian-Vector Product (JVP)**: Computes directional derivatives efficiently; needed for gradient estimation in high-dimensional spaces; quick check: verify JVP implementation matches analytical derivatives
- **Mean Flow Matching**: Learns average velocity fields between distributions; needed to capture distributional transformations; quick check: validate that learned field transports noisy to clean speech
- **Backward-in-Time Integration**: Generates samples by reversing flow; needed for efficient single-step inference; quick check: confirm stability of backward integration across different noise levels

## Architecture Onboarding

Component Map: Noisy Speech -> Encoder -> Mean Flow Network -> Decoder -> Enhanced Speech

Critical Path: Encoder extracts features → Mean Flow Network learns velocity field → Decoder reconstructs enhanced speech via backward displacement

Design Tradeoffs: Single-step inference vs. multistep accuracy; computational efficiency vs. model complexity; direct supervision vs. indirect optimization

Failure Signatures: Numerical instability in JVP computation; gradient vanishing in backward integration; poor generalization to unseen noise types

First Experiments:
1. Verify JVP computation matches analytical gradients on simple test functions
2. Test backward-in-time generation on synthetic linear flows
3. Evaluate model performance on a small subset of VoiceBank-DEMAND before full training

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can replacing the linear-Gaussian interpolation with data-driven or non-linear paths improve the model's ability to handle complex speech distributions?
- Basis in paper: [explicit] The Conclusion identifies the assumption of a linear–Gaussian path as a limitation and lists "more flexible or data-driven paths" as future work.
- Why unresolved: The current method relies on a fixed schedule ($\mu_t, \sigma_t$), which may not represent the optimal transport trajectory between noisy and clean speech.
- What evidence would resolve it: Training MeanFlowSE on learned paths (e.g., optimal transport or rectified flows) and benchmarking performance against the linear baseline.

### Open Question 2
- Question: Do higher-order derivative corrections provide performance benefits over the currently used first-order estimation?
- Basis in paper: [explicit] The Conclusion cites the "use of first-order derivative estimation" as a limitation and proposes "higher-order corrections" for future investigation.
- Why unresolved: The authors currently stabilize training with a heuristic scaling factor ($c=0.5$) for the first-order term; it is unknown if second-order terms improve accuracy or stability.
- What evidence would resolve it: Ablation studies comparing the convergence speed and final metrics (SI-SDR, ESTOI) of first-order vs. higher-order JVP objectives.

### Open Question 3
- Question: Does the single-step efficiency and quality of MeanFlowSE generalize to diverse, real-world noise conditions?
- Basis in paper: [explicit] The Conclusion states the need for "evaluation under real-world conditions" as a specific direction for future work.
- Why unresolved: The reported results are limited to the VoiceBank-DEMAND dataset, which may not fully represent the variability of noise encountered in production environments.
- What evidence would resolve it: Evaluating the model on unconstrained datasets (e.g., DNS Challenge blind test sets) to verify robustness without retraining.

## Limitations
- Potential sensitivity of Jacobian-vector product computation to numerical precision
- Limited evaluation to VoiceBank-DEMAND dataset, lacking real-world noise validation
- First-order derivative estimation may miss higher-order flow dynamics

## Confidence
- **High confidence** in mathematical formulation of mean flow matching
- **Medium confidence** in claimed efficiency improvements
- **Medium confidence** in quality metrics, as they are dataset-specific

## Next Checks
1. Conduct ablation studies to quantify individual contributions of JVP computation and backward generation
2. Test generalization capabilities on diverse speech enhancement datasets beyond VoiceBank-DEMAND
3. Perform detailed analysis of model behavior under extreme noise conditions and out-of-distribution samples