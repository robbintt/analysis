---
ver: rpa2
title: 'WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification
  Under Distribution Shifts'
arxiv_id: '2505.19587'
source_url: https://arxiv.org/abs/2505.19587
tags:
- prediction
- shifts
- distribution
- coverage
- conformal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces two methods to address distribution shifts
  in conformal prediction (CP) for robust uncertainty quantification. The first method,
  RLSCP, scales prediction sets using reconstruction losses from a Variational Autoencoder
  (VAE) to handle distribution shifts.
---

# WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts

## Quick Facts
- arXiv ID: 2505.19587
- Source URL: https://arxiv.org/abs/2505.19587
- Authors: Shadi Alijani; Homayoun Najjaran
- Reference count: 40
- Primary result: WQLCP maintains coverage while reducing prediction set sizes on ImageNet variants, achieving 0.7402 coverage with 9.501 set size on ImageNetA vs. 0.7023 coverage and 16.3921 set size for APS baseline

## Executive Summary
This paper addresses the challenge of maintaining reliable uncertainty quantification in conformal prediction when data distributions shift. The authors propose two methods: RLSCP scales prediction sets using VAE reconstruction losses to handle distribution shifts, while WQLCP refines this by incorporating weighted exchangeability adjustments based on calibration-test loss ratios. Experiments demonstrate that WQLCP consistently maintains coverage while significantly reducing prediction set sizes compared to existing baselines across multiple ImageNet variants.

## Method Summary
The method involves training a β-VAE on in-distribution data to extract reconstruction losses as uncertainty signals. For test samples, reconstruction losses are compared against calibration losses to compute weights that approximate the density ratio between shifted test and calibration distributions. These weights adjust the quantile threshold used for prediction set construction, with RLSCP applying a simpler scaling approach and WQLCP implementing the full weighted quantile calibration. The approach maintains the coverage guarantees of conformal prediction while adapting to distribution shifts through uncertainty-aware threshold adjustment.

## Key Results
- WQLCP achieves 0.7402 coverage with 9.501 set size on ImageNetA, outperforming APS (0.7023 coverage, 16.3921 set size)
- RLSCP baseline maintains coverage but produces larger prediction sets, demonstrating the efficiency gains of WQLCP's weighted approach
- Both methods maintain coverage closer to target than standard SplitCP across ImageNetV2, ImageNetR, and ImageNetA variants

## Why This Works (Mechanism)

### Mechanism 1: VAE Reconstruction Loss as Uncertainty Signal
The VAE reconstruction loss serves as a proxy for epistemic uncertainty and distribution shift severity. When trained on in-distribution data, the VAE learns to reconstruct those inputs with low error. Distribution shifts cause reconstruction loss to spike as the decoder struggles with novel features. The method aggregates these losses globally to estimate shift magnitude.

**Core assumption:** Reconstruction loss correlates monotonically with the "difficulty" or "out-of-distribution" nature of the data relative to the training manifold.

**Break condition:** If the distribution shift is semantic but visually simple (e.g., stylized rendering that is easy to reconstruct but hard to classify), the VAE may report low uncertainty, failing to trigger the scaling mechanism.

### Mechanism 2: RLSCP Scaling for Coverage Recovery
RLSCP modifies standard scores by multiplying them by $\max(1, R_{L_{test}})$, where $R_{L_{test}}$ is the quantile of test reconstruction loss. When the test batch is "hard" (high loss), scores are scaled up. Since the threshold is fixed, this scaling effectively lowers the barrier for label inclusion, forcing prediction sets to expand and recapture missed labels.

**Core assumption:** Expanding prediction set size in response to high uncertainty will statistically recover true labels lost due to distribution shift.

**Break condition:** Excessive scaling can lead to "inflation," where sets become so large they are uninformative (e.g., predicting 50+ classes), violating the efficiency goal of CP.

### Mechanism 3: WQLCP Weighted Quantile Calibration
WQLCP approximates re-establishing exchangeability by prioritizing calibration samples with loss profiles similar to test data. Weights $w(x_j) \propto \frac{L_{cal}(x_j)}{L_{test}(x_i)}$ are assigned to calibration samples. If a calibration sample has high loss (resembling high-loss test data), it receives higher weight in the quantile calculation, effectively "re-sampling" the calibration distribution.

**Core assumption:** The ratio of reconstruction losses serves as a valid surrogate for the likelihood ratio between calibration and test densities.

**Break condition:** If the calibration set lacks samples with sufficiently high reconstruction losses (no "hard" examples), the weights cannot adequately shift the quantile, potentially resulting in under-coverage.

## Foundational Learning

- **Split Conformal Prediction (SplitCP)**: Baseline method modified by the paper. Prediction sets are formed from held-out calibration set using a quantile threshold. Quick check: If I have 1000 calibration samples and want 90% coverage, which score ranking do I select as my threshold?

- **Exchangeability**: The paper identifies violation of exchangeability (calibration and test data from different distributions) as core failure mode of standard CP. Quick check: Does exchangeability require data to be i.i.d., or just that the joint distribution is invariant to permutation?

- **VAE Reconstruction Loss**: The "signal" used to detect shifts. Need to distinguish between KL divergence (regularization) and reconstruction term, as paper relies specifically on latter for uncertainty quantification. Quick check: Why would a VAE trained on standard photos struggle to reconstruct a pencil sketch, and what happens to the $L_{rec}$ value in that case?

## Architecture Onboarding

- **Component map**: Backbone (ViT/ResNet) -> VAE (Encoder-Decoder) -> Calibration Store -> Weighting Engine -> Quantile Aggregator -> Prediction Set Generator

- **Critical path**: 
  1. Batch Ingest: Receive test batch $D_{test}$
  2. Uncertainty Est: Pass $D_{test}$ through VAE → get $L_{test}$ distribution
  3. Weighting: Compare $L_{test}$ against stored $L_{cal}$ to generate weights $w$
  4. Quantile Calc: Compute weighted quantile $\hat{q}$ (Step 3 in Algorithm 1)
  5. Scaling: Apply RLSCP scaling factor to test scores
  6. Set Gen: Filter classes based on scaled scores and weighted $\hat{q}$

- **Design tradeoffs**: 
  - ViT vs. CNN Backbone: ViTs correlate better with reconstruction loss than CNNs, but require more compute
  - RLSCP vs. WQLCP: RLSCP guarantees coverage but often creates massive sets (low efficiency). WQLCP optimizes for efficiency (smaller sets) but depends on assumption that loss ratios approximate density ratios
  - Beta ($\beta$) Scheduling: Paper mentions $\beta=1.2$ optimizes trade-off. Higher $\beta$ might over-regularize latent space, hurting loss signal

- **Failure signatures**:
  - Latent Rigidity: On extreme shifts (ImageNetA), if VAE is over-regularized (high $\beta$), $L_{rec}$ might not spike enough, causing under-coverage
  - Set Inflation: On fine-grained classes (e.g., dog breeds), ambiguity leads to $|C(x)| > 15$, rendering prediction useless for decision-making

- **First 3 experiments**:
  1. Sanity Check (Coverage): Run RLSCP on standard ImageNet validation set vs. ImageNetA. Verify coverage stays near 0.90 while baseline (SplitCP) drops significantly
  2. Efficiency Validation (Size): Compare WQLCP vs. RLSCP on ImageNetA. Check if WQLCP reduces average set size (~9-10) without dropping coverage below target
  3. Hyperparameter Sensitivity: Ablate calibration set size (5k vs 25k) on ImageNetR to confirm paper's claim that 25k is sufficient for stable quantile estimation

## Open Questions the Paper Calls Out

- **Adaptive $\beta$-VAE scheduling for extreme shifts**: Under-coverage in extreme shifts linked to $\beta$-VAE over-regularization and suggests adaptive scheduling as future direction. Current fixed $\beta=1.2$ results in severe under-coverage (0.42 vs 0.74 average) for samples with high reconstruction loss ($L_{rec} > 3\sigma$).

- **Hierarchical label grouping for fine-grained classes**: 4.8% of predictions require large sets ($>15$ labels) for fine-grained classes (e.g., dog breeds) and proposes hierarchical grouping. Current WQLCP implementation treats labels independently, resulting in unnecessarily large sets when semantic distinctions are ambiguous.

- **VAE reconstruction loss applicability**: The method relies on theoretical assumption that $L_{rec}$ directly reflects epistemic uncertainty, validated only on ImageNet variants. If a VAE generalizes "too well" and reconstructs out-of-distribution data with low loss, the weights $w(x_j)$ would be mis-calibrated, failing to signal the distribution shift.

## Limitations
- VAE architecture sensitivity: Performance depends critically on VAE's ability to detect distributional shifts; no ablation studies on architecture variations
- Exchangeability assumption: Weighted quantile approach assumes loss ratios approximate density ratios, but this is not formally proven; may fail when calibration and test distributions have disjoint support in latent space
- Extreme shift scenarios: On severe distribution shifts (e.g., ImageNetA), coverage drops to 0.42 for highest-loss samples, suggesting method may not be robust to catastrophic domain shifts

## Confidence
- **High**: RLSCP mechanism (reconstruction loss scaling improves coverage on moderate shifts)
- **Medium**: WQLCP efficiency gains (set size reduction vs. RLSCP is well-documented, but underlying exchangeability approximation lacks theoretical grounding)
- **Low**: VAE-based uncertainty quantification (monotonic relationship between reconstruction loss and epistemic uncertainty is assumed but not empirically validated across diverse shift types)

## Next Checks
1. **VAE ablation study**: Compare WQLCP performance using different VAE architectures (standard VAE vs. β-VAE with varying β) on ImageNetA to isolate contribution of reconstruction loss signal quality

2. **Loss ratio sensitivity**: Generate synthetic calibration/test pairs with controlled density ratios and verify that WQLCP's weighted quantile tracks theoretical optimal coverage

3. **Extreme shift stress test**: Evaluate WQLCP on ImageNet-A subset where reconstruction loss exceeds 3 standard deviations above mean. Document coverage and set size degradation to establish failure boundaries