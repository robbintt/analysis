---
ver: rpa2
title: 'ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis'
arxiv_id: '2505.20506'
source_url: https://arxiv.org/abs/2505.20506
tags:
- speech
- arabic
- corpus
- were
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ArVoice, a multi-speaker Modern Standard
  Arabic (MSA) speech corpus with diacritized transcriptions designed for multi-speaker
  speech synthesis. The dataset combines professionally recorded speech from six voice
  talents, modified subsets of existing corpora, and high-quality synthetic speech
  from commercial systems, totaling 83.52 hours across 11 voices.
---

# ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis

## Quick Facts
- arXiv ID: 2505.20506
- Source URL: https://arxiv.org/abs/2505.20506
- Reference count: 0
- Multi-speaker Arabic speech corpus with diacritized transcriptions for speech synthesis

## Executive Summary
This paper introduces ArVoice, a comprehensive multi-speaker Modern Standard Arabic (MSA) speech corpus designed specifically for speech synthesis tasks. The dataset aggregates 83.52 hours of speech across 11 distinct voices, combining professional recordings from six voice talents, modified subsets from existing corpora, and high-quality synthetic speech from commercial systems. The corpus includes diacritized transcriptions, which are shown to significantly improve text-to-speech synthesis quality. The authors evaluate three open-source TTS models (ArTST-tts, VITS, Fish-Speech) and two voice conversion systems (AAS-VC, KNN-VC), demonstrating that diacritized transcripts yield superior performance across all models.

## Method Summary
The ArVoice corpus was constructed through a multi-source approach, integrating professional studio recordings from six Arabic voice talents, carefully selected subsets from existing Arabic speech datasets, and high-fidelity synthetic speech generated by commercial TTS systems. All speech data was transcribed with full diacritization to preserve pronunciation accuracy. The corpus was then used to train and evaluate three open-source TTS models (ArTST-tts, VITS, Fish-Speech) and two voice conversion frameworks (AAS-VC, KNN-VC). Experiments systematically compared model performance with and without diacritized transcripts, measuring both technical metrics and speaker similarity scores. Voice conversion evaluations specifically targeted speaker similarity (0.69-0.72) and false acceptance rates (0.81-0.95).

## Key Results
- VITS achieved the highest TTS performance among the three models tested
- Diacritized transcripts significantly improved TTS quality across all systems
- Voice conversion models achieved speaker similarity scores of 0.69-0.72 with false acceptance rates of 0.81-0.95
- The corpus enables multiple Arabic speech processing tasks including synthesis, voice conversion, and diacritic restoration

## Why This Works (Mechanism)
The success of ArVoice stems from its comprehensive multi-speaker design and the critical inclusion of diacritized transcriptions. Arabic's rich morphology and context-dependent pronunciation make diacritization essential for accurate speech synthesis. By combining professional recordings with synthetic data and leveraging multiple existing corpora, the dataset achieves both quality and diversity. The multi-source approach ensures robust coverage of phonetic variations while maintaining speaker consistency within each voice profile.

## Foundational Learning
**Diacritization in Arabic**: Adding vowel marks and pronunciation guides to Arabic text. *Why needed*: Arabic words can have multiple pronunciations without diacritics, making accurate TTS impossible. *Quick check*: Verify that all transcriptions include fatha, kasra, damma, and shadda marks.

**Multi-speaker TTS**: Training speech synthesis models on data from multiple distinct speakers. *Why needed*: Enables voice cloning and conversion between speakers. *Quick check*: Confirm dataset contains at least 5 distinct speaker profiles with sufficient utterance diversity.

**Voice Conversion Systems**: Technologies that transform speech from one speaker to sound like another while preserving content. *Why needed*: Enables speaker adaptation without retraining entire TTS models. *Quick check*: Measure speaker similarity scores between converted and target speech.

## Architecture Onboarding

**Component Map**: Data Collection -> Preprocessing (Diacritization) -> Model Training (TTS/VC) -> Evaluation

**Critical Path**: The core pipeline follows: raw speech acquisition → professional transcription with diacritics → dataset curation → model training → quality evaluation. The diacritization step is critical as it directly impacts all downstream model performance.

**Design Tradeoffs**: The dataset combines professional recordings, modified existing corpora, and synthetic speech. This maximizes quantity and diversity but introduces potential quality variance across sources. The choice to include synthetic speech from commercial systems provides scalability but may introduce artifacts not present in human recordings.

**Failure Signatures**: Poor diacritization leads to pronunciation errors and reduced intelligibility. Inconsistent speaker quality across the mixed-source dataset may cause model instability. Voice conversion systems may fail to achieve high speaker similarity when source and target speakers have significantly different vocal characteristics.

**First Experiments**:
1. Train baseline TTS model without diacritization to establish performance floor
2. Evaluate speaker similarity on held-out speakers using voice conversion models
3. Conduct MOS-based perceptual evaluation comparing VITS with and without diacritics

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation relies primarily on technical metrics rather than comprehensive perceptual studies
- Voice conversion experiments lack detailed ablation studies on diacritization effects
- Mixed-source dataset composition introduces unquantified quality variance
- Licensing and data provenance information is incomplete for some corpus components

## Confidence
**High**: Core technical methodology for dataset construction
**Medium**: Claims about TTS performance improvements with diacritics
**Low**: Assertions about perceptual quality and dataset composition stability

## Next Checks
1. Conduct MOS-based perceptual evaluation comparing TTS systems with and without diacritization
2. Perform detailed speaker diarization analysis on the mixed-source dataset to quantify quality variance
3. Execute cross-validation with held-out speakers to verify the robustness of voice conversion similarity scores