---
ver: rpa2
title: Optimal Conditional Inference in Adaptive Experiments
arxiv_id: '2309.12162'
source_url: https://arxiv.org/abs/2309.12162
tags:
- inference
- conditional
- assumption
- note
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies inference in batched adaptive experiments, focusing
  on conditional inference given the experimental design (stopping time, assignment
  probabilities, and target parameter). The authors show that without restrictions,
  only last-batch inference is optimal.
---

# Optimal Conditional Inference in Adaptive Experiments

## Quick Facts
- **arXiv ID**: 2309.12162
- **Source URL**: https://arxiv.org/abs/2309.12162
- **Reference count**: 40
- **Primary result**: Shows optimal conditional inference in batched adaptive experiments using a sufficient statistic consisting of last batch mean plus one linear combination of earlier batch-arm means, improving upon last-batch-only inference by 10-50% in median confidence interval length while maintaining conditional coverage guarantees.

## Executive Summary
This paper develops optimal conditional inference procedures for batched adaptive experiments where stopping time, assignment probabilities, and target parameters may be adaptively chosen. The key insight is that when adaptive decisions are location-invariant (depending only on contrasts between arm means), a simple one-dimensional statistic captures all the information from earlier batches that can be safely combined with the last batch mean. The authors provide computationally tractable procedures for both location-invariant and polyhedral selection rules, and prove uniform asymptotic validity of these procedures over non-Gaussian data. Simulations show 10-50% efficiency gains compared to existing methods while maintaining valid conditional coverage.

## Method Summary
The method centers on identifying a sufficient statistic for the unknown parameter vector conditional on the realized experimental design. For location-invariant designs, the authors derive a "leftover" statistic L that is statistically independent of the selection event (contrasts between arm means), allowing safe combination with the last batch mean. The optimal estimator S* combines this information to achieve 10-50% efficiency gains over last-batch-only methods. For polyhedral selection rules, truncated Gaussian distributions provide computationally tractable inference. The procedures are derived under Gaussian assumptions but proven to maintain uniform asymptotic validity over non-Gaussian data through a Le Cam-Hajek type argument.

## Key Results
- Location-invariant adaptive designs admit a sufficient statistic consisting of the last batch mean plus one additional linear combination of earlier batch-arm means
- This yields 10-50% reduction in median confidence interval length compared to last-batch-only inference
- For polyhedral selection rules, optimal inference is achieved via truncated Gaussian distributions
- Procedures maintain uniform asymptotic validity over non-Gaussian data generating processes
- Conditional coverage guarantees hold for any realized stopping time, assignment probabilities, and target parameter

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Inference using only the last batch can be improved if the experimental design is location-invariant.
- **Mechanism:** Location-invariant decisions depend only on contrasts (differences between arm means). The selection event conditions on these contrasts, but the authors identify a scalar statistic L—a precision-weighted sum of historical batch means—that is statistically independent of these contrasts. By combining L with the last batch mean, the procedure recovers information from earlier batches that would otherwise be discarded.
- **Core assumption:** Adaptive choices of stopping time T, assignment probabilities Π, and target η are measurable with respect to differences between batch-arm means and remain unchanged if a constant is added to all arm means.
- **Evidence anchors:** Theorem 3.7 establishes that (L, X_T) is sufficient for μ conditional on contrasts ΔX; abstract states additional information exists when design is location-invariant.
- **Break condition:** If experimental design depends on absolute level of means (not just contrasts), L is no longer independent of selection event.

### Mechanism 2
- **Claim:** Polyhedral selection rules enable optimal conditional inference via truncated Gaussian distributions.
- **Mechanism:** When selection events define a polyhedron AX ≤ b, the distribution of the sufficient statistic becomes a truncated Gaussian constrained by that polyhedron. The authors derive the specific conditional distribution for this setting, allowing construction of optimal median-unbiased estimators and confidence intervals.
- **Core assumption:** Selection rules can be expressed as finite linear inequalities.
- **Evidence anchors:** Theorem 5.3 defines constrained distribution (Z_1 | M Z ≤ m); abstract mentions computationally tractable optimal procedures.
- **Break condition:** Highly non-linear or discontinuous selection rules cannot be approximated by polyhedral constraints.

### Mechanism 3
- **Claim:** Procedures derived under Gaussian assumptions maintain uniform asymptotic validity over non-Gaussian data.
- **Mechanism:** The batch structure enables showing finite-sample statistic converges uniformly to Gaussian limit experiment (Le Cam-Hajek sense), implying conditional coverage guarantees hold in large samples even for non-Gaussian outcomes.
- **Core assumption:** Large batch sizes and adequately continuous assignment algorithm.
- **Evidence anchors:** Theorem 4.2 states exact conditional asymptotic size holds uniformly over class of distributions P; abstract mentions uniform asymptotic validity.
- **Break condition:** Small batch sizes or discontinuous assignment algorithm break asymptotic approximation.

## Foundational Learning

- **Concept: Sufficient Statistic**
  - **Why needed here:** Efficiency gains come from identifying minimal sufficient statistic (L) for unknown parameter μ conditional on adaptive design. Without understanding sufficiency, one might incorrectly assume all historical data is biased by adaptation.
  - **Quick check question:** Can you explain why statistic L is sufficient for μ in location-invariant setting, while full history X_{1:T-1} is not?

- **Concept: Conditional Inference / Selective Inference**
  - **Why needed here:** Paper explicitly conditions inference on realized design (stopping time, target). This is distinct from unconditional validity and is core of "safe free lunch" argument.
  - **Quick check question:** Why is conditional coverage considered stronger guarantee than unconditional coverage in adaptive experiments?

- **Concept: Truncated Gaussian Distribution**
  - **Why needed here:** For polyhedral mechanism, computing p-values and confidence intervals requires evaluating CDF of Gaussian distribution truncated by linear constraints.
  - **Quick check question:** If test statistic follows N(0,1) but is conditioned on falling in interval [a, b], how does support of distribution change?

## Architecture Onboarding

- **Component map:** Inputs (X_{tk}, Π_{tk}, σ_k², n_t) -> Design Check (Location-invariant or Polyhedral?) -> Logic Layer (Compute L and S* or construct A,b) -> Inference Layer (Construct CIs)
- **Critical path:** Calculation of "Leftover" statistic L (Eq. 9). This involves summing precision-weighted means from all historical batches. Errors in estimating variance σ_k² or assignment weights propagate directly into bias of S*.
- **Design tradeoffs:**
  - Leftover vs. Polyhedral: Leftover method is more robust (requires only location-invariance) but uses less information. Polyhedral method is more powerful (can use all data) but computationally complex and brittle.
  - Robustness vs. Efficiency: Last-batch-only is maximally robust but inefficient. Leftover improves efficiency by ~10-50% (median length) at cost of location-invariance assumption.
- **Failure signatures:**
  - Under-coverage in small samples: CLT-based Gaussian approximation may not hold with small n_t.
  - Computation failure in Polyhedral: High-dimensional constraint sets make Gibbs sampling unstable.
  - Bias from non-invariance: If stopping rule uses absolute values, L correlates with selection event, invalidating inference.
- **First 3 experiments:**
  1. Replicate Table 1 (Thompson Sampling): Implement "Leftover" estimator for 2-arm bandit with Gaussian rewards. Verify 10-50% reduction in median CI length vs "Last-only".
  2. Stress Test Non-Gaussian: Run "Leftover" procedure on heavy-tailed data (Log-normal) with small batch sizes to identify "break point" where coverage drops below nominal level.
  3. Polyhedral ε-greedy: Implement polyhedral logic for ε-greedy algorithm where target is winning arm. Compare CI lengths against "Leftover" method to quantify power loss from generic vs specific approach.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can location-invariant approach be extended to settings where adaptive decisions depend on both contrasts and a common baseline, such as experiments with a fixed control arm?
- **Basis in paper:** The paper assumes adaptive choices depend only on contrasts between arms (location-invariant). Many practical bandit algorithms, however, may depend on both contrasts and absolute levels relative to a baseline.
- **Why unresolved:** Sufficiency result for statistic L relies explicitly on location-invariance. If this structural assumption is relaxed, conditional distribution theory does not immediately extend.
- **What evidence would resolve it:** Theoretical characterization of sufficient statistics under broader design classes, or negative result showing no finite-dimensional sufficient statistic exists beyond location-invariant case.

### Open Question 2
- **Question:** Under what asymptotic regimes (small batch sizes, diminishing treatment probabilities, or non-stationary outcomes) do proposed conditional procedures remain uniformly valid?
- **Basis in paper:** Uniform asymptotic results assume fixed fractions ct > 0 and bounded assignment probabilities away from zero. Authors do not analyze vanishing batch sizes or treatment probabilities.
- **Why unresolved:** Proof strategy relies on joint weak convergence of (Y1:n,Π1:n,...) which may break down if batch sizes or assignment probabilities decay too rapidly.
- **What evidence would resolve it:** Theoretical analysis of uniform validity under alternative asymptotic regimes, or counterexamples showing coverage failure.

### Open Question 3
- **Question:** Can optimal conditional inference be achieved when only partial knowledge of assignment algorithm is available, rather than full location-invariance or exact polyhedral structure?
- **Basis in paper:** Paper contrasts robust location-invariant approach (no algorithm knowledge beyond invariance) with polyhedral approach (full knowledge). Intermediate cases are not addressed.
- **Why unresolved:** Theory provides two endpoints: one-dimensional leftover statistic under location-invariance, and full selective inference under known polyhedral structure. Partial knowledge may admit richer conditioning sets without requiring full algorithmic specification.
- **What evidence would resolve it:** Construction of inference procedures conditioning on coarser sufficient statistics than ΔX1:T−1 but finer than polyhedral events, with demonstrated power gains.

## Limitations
- Location-invariance assumption is critical for efficiency gains; without it, only last-batch inference is optimal
- Polyhedral mechanism requires exact linear constraints on selection rules, which may not hold for complex adaptive algorithms
- Uniform asymptotic validity depends on large batch sizes with no explicit finite-sample guarantees provided

## Confidence

- **High Confidence**: Existence of sufficient statistic for location-invariant designs is well-supported by theoretical framework (Theorem 3.7) and clear abstract statement. Conditional coverage guarantees are explicit in abstract and section 4.
- **Medium Confidence**: Efficiency gains (10-50% CI length reduction) are reported from simulations but exact magnitude depends on specific adaptive design. Uniform asymptotic validity claim requires careful verification with small batch sizes.
- **Low Confidence**: Computational tractability of polyhedral method for high-dimensional constraint sets is not fully explored, and Gibbs sampling details are sparse.

## Next Checks
1. **Verification of Location-Invariance**: Implement Thompson sampling variant where stopping rule depends on absolute mean values (not just contrasts). Verify "Leftover" statistic loses efficiency advantage, confirming mechanism's reliance on location-invariance assumption.
2. **Finite-Sample Coverage**: Run "Leftover" procedure with batch sizes n_t=10, 50, 200. Plot empirical coverage vs nominal level to identify smallest batch size where asymptotic approximation (Theorem 4.2) breaks down.
3. **Polyhedral Scalability**: For K=10 arm problem with T=20 batches, implement polyhedral constraint matrix A and test vector b. Measure time and stability of Gibbs sampling as K and T increase to quantify computational burden.