---
ver: rpa2
title: 'GraphFusionSBR: Denoising Multi-Channel Graphs for Session-Based Recommendation'
arxiv_id: '2601.08497'
source_url: https://arxiv.org/abs/2601.08497
tags:
- graph
- knowledge
- recommendation
- session
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'GraphFusionSBR tackles session-based recommendation by integrating
  knowledge graphs, hypergraphs, and line graphs into a multi-channel architecture.
  It introduces denoising mechanisms: a view generator prunes irrelevant knowledge
  graph edges, and an importance extraction module filters noisy session interactions.'
---

# GraphFusionSBR: Denoising Multi-Channel Graphs for Session-Based Recommendation

## Quick Facts
- arXiv ID: 2601.08497
- Source URL: https://arxiv.org/abs/2601.08497
- Reference count: 40
- One-line primary result: Multi-channel denoising architecture improves SBR accuracy on Tmall (P@20=40.21%), RetailRocket (P@20=72.08%), and KKBox (P@20=42.09%).

## Executive Summary
GraphFusionSBR introduces a multi-channel graph neural network for session-based recommendation that integrates knowledge graphs, hypergraphs, and line graphs. The model applies denoising mechanisms: a differentiable view generator prunes irrelevant knowledge graph edges, and an importance extraction module filters noisy session interactions. Cross-channel contrastive learning between hypergraph and line graph views maximizes mutual information to enhance session embeddings. Experimental results on three datasets show consistent improvements over state-of-the-art baselines.

## Method Summary
GraphFusionSBR processes sessions through three parallel graph channels: knowledge graph (with edge pruning via Gumbel-Softmax sampling), hypergraph (capturing co-occurrence patterns), and line graph (modeling session transitions with importance weighting). The model fuses hypergraph and knowledge graph embeddings for recommendation, while applying contrastive learning between hypergraph and line graph embeddings as an auxiliary task. The overall loss combines recommendation loss, contrastive loss, and knowledge graph alignment loss, optimized jointly with hyperparameters tuned per dataset.

## Key Results
- Achieves P@20 scores of 40.21% on Tmall, 72.08% on RetailRocket, and 42.09% on KKBox
- Improves over state-of-the-art baselines including SR-GNN, LESSR, and GCE-GNN
- Ablation studies confirm contributions of denoising mechanisms and cross-channel learning

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Graph Edge Pruning via Differentiable View Generator
The model adaptively removes redundant edges in the knowledge graph channel to reduce noise. An MLP computes importance weights for each KG edge, converted to sampling probabilities using Gumbel-Max reparameterization. This allows gradient-based pruning while maintaining differentiability. Edge-level denoising improves item representations by filtering irrelevant semantic relationships. Evidence shows NDKG variant degrades P@20 on Tmall from 40.21% to 39.68%.

### Mechanism 2: Cross-Channel Contrastive Co-Training
The model maximizes mutual information between hypergraph and line graph channels as an auxiliary task. Each channel predicts pseudo-labels for the other, with top-K predictions as positives and harder negatives (top-10% related but not positive) selected. InfoNCE-style loss pulls positive pairs together and pushes negatives apart. This provides complementary signals for the same session. Weight analysis shows optimal λ₁ values vary by dataset (0.05-1.0 for Tmall, 0.001 for others).

### Mechanism 3: In-Session Importance Extraction for Denoising
The model generates in-session attention for denoising by weighting session items based on their average similarity to other items. The Importance Extraction Module computes a self-attention similarity matrix, then derives each item's importance as its mean similarity. Softmax-normalized scores weight initial item embeddings into session representations. This filters noisy interactions like misclicks. Evidence shows NIEM variant drops Tmall P@20 from 40.21% to 39.92%.

## Foundational Learning

- **Graph Attention Networks (GAT)**: Used in KG encoder to aggregate neighbor information with learned attention coefficients. Quick check: Can you explain how attention coefficients α_ij are computed and normalized in a standard GAT layer?
- **Hypergraph Convolution**: Used in hypergraph channel for node-to-hyperedge-to-node propagation (D⁻¹HWB⁻¹HᵀX). Quick check: How does hypergraph convolution differ from standard graph convolution in terms of information aggregation path?
- **Contrastive Learning / InfoNCE Loss**: Used for cross-channel SSL loss to maximize mutual information. Quick check: What is the effect of the temperature parameter on gradient signal for hard vs. easy negatives?

## Architecture Onboarding

- **Component map**: Raw sessions + Knowledge Base → KG Channel (View Generator → GAT Encoder) → Session Embedding θ_k; Hypergraph Channel (Incidence Matrix → HGConv) → Session Embedding θ_h; Line Graph Channel (IEM → GConv) → Session Embedding θ_l; Fusion (θ_h || θ_k) → Softmax → Next-item prediction
- **Critical path**: 1) Construct Gh, Gl, Gk from raw data; 2) Denoise Gk via view generator to obtain Ĝk; 3) Encode each channel independently (GAT for KG, HGConv for hypergraph, GConv for line graph); 4) Fuse θ_h and θ_k for recommendation logits; compute L_ssl between θ_h and θ_l; 5) Jointly optimize L = L_rec + λ₁L_ssl + λ₂L_KG
- **Design tradeoffs**: Denoising aggressiveness (τ_b): Lower values prune more edges but risk sparsity; higher values retain noise. Contrastive weight (λ₁): Higher values improve robustness but may suppress primary recommendation signal. Sample size K: Smaller K yields higher-quality positives but may reduce diversity
- **Failure signatures**: Over-pruned KG: Item embeddings become sparse; L_KG plateaus early. Dominant SSL loss: L_rec stops improving while L_ssl continues decreasing. Uniform importance scores: IEM outputs near-equal β; suggests sessions lack coherent structure
- **First 3 experiments**: 1) Reproduce ablation (Table 4): Run GraphFusionSBR-NP, -NKG, -NDKG, -NIEM on Tmall to validate each component's contribution; 2) Hyperparameter sweep: Vary λ₁ ∈ {0.001, 0.01, 0.05, 0.1, 1.0} and K ∈ {5, 10, 15, 20} to reproduce Figure 3 and Figure 4 sensitivity curves; 3) Edge retention analysis: Log percentage of KG edges retained by view generator across epochs; correlate with L_KG and P@20 to diagnose over/under-pruning

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on knowledge graphs, requiring structured side information that may not be available
- Theoretical justification for cross-channel mutual information maximization is weak
- Denoising mechanisms may fail for short sessions or when noise patterns are semantically similar to user intent

## Confidence

- **High confidence**: Multi-channel architectural design with denoising mechanisms is clearly specified and supported by ablation studies showing consistent improvements
- **Medium confidence**: Denoising mechanisms are validated through ablation, but exact thresholds for edge pruning and importance weighting are not specified
- **Low confidence**: Theoretical justification for cross-channel mutual information maximization lacks rigorous proof of complementarity between hypergraph and line graph views

## Next Checks

1. Parameter sensitivity analysis: Reproduce Figure 3 and Figure 4 by varying λ₁ ∈ {0.001, 0.01, 0.05, 0.1, 1.0} and K ∈ {5, 10, 15, 20} to confirm reported optimal ranges and identify overfitting risks
2. KG edge retention correlation: Log percentage of KG edges retained by view generator across training epochs and correlate with L_KG and P@20 to diagnose over-pruning or under-pruning
3. Ablation under reduced KG quality: Evaluate GraphFusionSBR performance on 50% subset of knowledge graph to quantify dependency on KG completeness and validate denoising mechanism robustness