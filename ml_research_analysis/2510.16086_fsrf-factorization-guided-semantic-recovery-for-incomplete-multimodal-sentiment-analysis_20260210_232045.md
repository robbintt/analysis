---
ver: rpa2
title: 'FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment
  Analysis'
arxiv_id: '2510.16086'
source_url: https://arxiv.org/abs/2510.16086
tags:
- missing
- modalities
- modality
- multimodal
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multimodal sentiment analysis with missing
  modalities, a practical problem often overlooked in previous work. The proposed
  Factorization-guided Semantic Recovery Framework (FSRF) factorizes each modality
  into modality-homogeneous, modality-heterogeneous, and noise representations, and
  recovers missing information through a distribution-aligned self-distillation module.
---

# FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis

## Quick Facts
- arXiv ID: 2510.16086
- Source URL: https://arxiv.org/abs/2510.16086
- Reference count: 31
- One-line primary result: Outperforms state-of-the-art by 2.09%-3.92% F1-score under missing modalities on MOSI/MOSEI

## Executive Summary
This paper addresses multimodal sentiment analysis with missing modalities, a practical problem often overlooked in previous work. The proposed Factorization-guided Semantic Recovery Framework (FSRF) factorizes each modality into modality-homogeneous, modality-heterogeneous, and noise representations, and recovers missing information through a distribution-aligned self-distillation module. The framework performs bidirectional knowledge transfer between two incomplete samples to enhance robustness. Experimental results on MOSI and MOSEI datasets show FSRF outperforms state-of-the-art methods by 2.09%-3.92% in F1-score under missing-modality conditions, achieving 76.78% and 75.90% average F1 on MOSI and MOSEI respectively, while maintaining performance close to complete-modality methods.

## Method Summary
FSRF factorizes each modality into three representations: modality-homogeneous (shared sentiment), modality-heterogeneous (modality-specific), and noise. During training, the Modality Random Missing (MRM) strategy creates two samples with different missing modalities. The Distribution Alignment-based Self-Distillation (DAS) module aligns their joint representations using Sinkhorn distance (feature-level) and JS divergence (logits-level), enabling bidirectional knowledge transfer. The total loss combines task loss, factorization constraints, and DAS alignment. The framework is trained on MOSI and MOSEI datasets with language, audio, and visual modalities.

## Key Results
- Achieves 76.78% average F1 on MOSI and 75.90% average F1 on MOSEI under missing-modality conditions
- Outperforms state-of-the-art methods by 2.09%-3.92% F1-score
- Maintains performance close to complete-modality methods while handling missing inputs

## Why This Works (Mechanism)

### Mechanism 1
Factorizing modality representations isolates sentiment-relevant signals from noise, reducing error accumulation when modalities are missing. The De-redundant Homo-Heterogeneous Factorization (DHF) module decomposes input features into modality-homogeneous (shared sentiment), modality-heterogeneous (modality-specific), and noise components. This uses contrastive constraints (NT-Xent) to push homogeneous representations together and heterogeneous ones apart, while entropy regularization keeps noise compact. If the modality-specific encoders fail to disentangle noise, the "denoising" will inadvertently discard predictive signals.

### Mechanism 2
Bidirectional knowledge transfer between two "broken" views of the same data recovers missing semantics more robustly than static teacher-student distillation. The Distribution Alignment-based Self-Distillation (DAS) module generates two samples with different missing modalities and aligns their joint representations using Sinkhorn distance and JS divergence. If audio is missing in one sample but present in another, the alignment forces reconstruction of audio semantics implicitly. If the missing modalities in both samples are identical, the distillation loop reinforces noise rather than recovering signal.

### Mechanism 3
Randomized modality masking during training enforces robustness to uncertain missing conditions at inference. The Modality Random Missing (MRM) strategy creates heterogeneous missing patterns on the fly, forcing the Shared Network to learn representations that function across a distribution of "broken" inputs. If the missing rate during training is too low, the model fails to learn recovery; if too high, it fails to learn fundamental correlations.

## Foundational Learning

- **Concept: Optimal Transport (Sinkhorn Distance)**
  - Why needed here: Standard distance metrics measure point-wise errors but fail to capture structural distribution shifts. DAS uses Sinkhorn to align the geometry of the feature space between samples.
  - Quick check question: How does the entropy regularization term in Sinkhorn distance prevent numerical instability compared to standard Wasserstein distance?

- **Concept: Contrastive Learning (NT-Xent Loss)**
  - Why needed here: The DHF module relies on this to separate homogeneous from heterogeneous representations. Without it, the factorization collapses into a single undifferentiated embedding.
  - Quick check question: What is the role of the temperature parameter τ in shaping the hardness of the contrastive constraint?

- **Concept: Knowledge Distillation**
  - Why needed here: The DAS module is a variant of self-distillation. Understanding the difference between logit distillation and feature distillation is crucial for tuning λ₁ and λ₂.
  - Quick check question: Why might JS divergence be preferred over KL divergence for aligning logits in a bidirectional setup?

## Architecture Onboarding

- **Component map:** Video → BERT (Text), Transformer (Audio/Visual) → DHF (3 MLPs per modality: Shared, Specific, Noise) → Fusion (Summation) → DAS (Sinkhorn + JS Div) → Sentiment Classifier

- **Critical path:** 1) Batch samples duplicated into S_p and S_q with random masks 2) Features extracted and factorized via DHF 3) Representations fused into Z_p and Z_q 4) DAS aligns Z_p ↔ Z_q 5) Loss aggregates Task + DHF constraints + DAS alignment

- **Design tradeoffs:** Factorization requires 3x parameters per modality encoder (granularity vs. memory); Sinkhorn is computationally heavier than MSE but provides better distribution alignment (training speed vs. semantic recovery quality)

- **Failure signatures:** DHF Collapse (L_DHF goes to zero but clusters overlap); Noise Over-regularization (L_n2 dominates, R_n collapses); DAS Stagnation (L_DAS remains high)

- **First 3 experiments:** 1) Baseline Sanity Check: Run FSRF on MOSI/MOSEI with no missing modalities to establish upper-bound performance vs. baselines 2) Ablation on Factorization: Disable R_n extraction to verify if factorization structure itself matters 3) Missing Rate Sensitivity: Plot performance curves (F1 vs. Missing Ratio) for single-modality condition

## Open Questions the Paper Calls Out

- **Open Question 1:** Does minimizing the entropy of noise representations risk discarding latent task-relevant features? The distinction between "modality-heterogeneous" and "noise" is enforced by loss constraints but may not perfectly align with semantic utility.

- **Open Question 2:** How does FSRF perform under structured or temporal missing patterns compared to the simulated random missingness? Real-world missingness often exhibits temporal continuity or correlation, which random zero-filling does not model.

- **Open Question 3:** How does the overlap ratio of missing modalities in the paired samples (S_p, S_q) affect the distillation efficacy? If both samples happen to miss the same dominant modality, the bidirectional transfer might fail to recover the missing semantics.

## Limitations
- The effectiveness of DHF factorization assumes sentiment-relevant semantics are predominantly modality-homogeneous and can be cleanly separated from noise
- The Sinkhorn-based DAS alignment assumes the missing-modality information gap between paired samples is bridgeable
- Hyperparameters (temperature τ, Sinkhorn regularization, noise targets) are critical yet not fully specified

## Confidence
- **High Confidence:** The overall experimental design (datasets, metrics, training setup) is clearly specified and reproducible
- **Medium Confidence:** The core mechanism (DHF + DAS) is described with sufficient detail for implementation, but the exact encoder architectures and MRM strategy are underspecified
- **Low Confidence:** The theoretical justification for tri-partite factorization (homogeneous/heterogeneous/noise) is weak; there's no direct ablation showing what happens if noise is not separated

## Next Checks
1. **Factorization Ablation:** Train FSRF without the noise component (R_n) to determine if performance gains stem from denoising or the structural factorization itself
2. **Distribution Analysis:** Visualize t-SNE embeddings of R_ho, R_he, R_n to confirm they form distinct clusters, validating the DHF assumption
3. **Missing Pattern Sensitivity:** Test FSRF under structured missingness (e.g., always missing first 50% of frames) to assess robustness beyond random MRM