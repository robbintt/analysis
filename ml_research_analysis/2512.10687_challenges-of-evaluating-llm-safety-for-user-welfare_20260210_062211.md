---
ver: rpa2
title: Challenges of Evaluating LLM Safety for User Welfare
arxiv_id: '2512.10687'
source_url: https://arxiv.org/abs/2512.10687
tags:
- user
- safety
- context
- advice
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated how user context affects LLM safety evaluations
  for personal advice. The research found that safety scores drop significantly when
  evaluators are provided with user vulnerability information, with high-vulnerability
  users seeing scores fall from safe (5/7) to somewhat unsafe (3/7).
---

# Challenges of Evaluating LLM Safety for User Welfare

## Quick Facts
- arXiv ID: 2512.10687
- Source URL: https://arxiv.org/abs/2512.10687
- Reference count: 40
- User context significantly affects LLM safety evaluations for personal advice

## Executive Summary
This study investigates how user context affects LLM safety evaluations for personal advice. The research reveals that safety scores systematically decrease when evaluators are provided with user vulnerability information, with high-vulnerability users seeing scores drop from safe (5/7) to somewhat unsafe (3/7). Surprisingly, enriching user prompts with disclosed context factors does not close this safety gap, even when using factors ranked as most relevant by professionals or most likely to be disclosed by users. The findings demonstrate that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic context disclosure alone proves insufficient, particularly for vulnerable populations.

## Method Summary
The study uses LLM-as-judge evaluation with gpt-o3 as evaluator using a structured 3-dimensional rubric (likelihood of harm, severity of harm, safeguard adequacy) to produce 7-point safety scores. Researchers collected Reddit-inspired prompts across 8 themes (4 finance, 4 health), created 14 demographic factor profiles stratified by vulnerability (low/medium/high), and generated responses using GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro. They compared context-blind evaluations against context-aware evaluations where user vulnerability profiles were provided, and tested context enrichment with disclosed factors ranked by professional relevance or user likelihood of disclosure.

## Key Results
- Context-blind evaluation systematically underestimates vulnerability-specific risks for medium and high-vulnerability users
- High-vulnerability users saw safety scores drop from safe (5/7) to somewhat unsafe (3/7) when vulnerability context was provided
- Enriching user prompts with disclosed context factors did not close the safety gap between context-blind and context-aware evaluations
- The most significant safety gaps occurred for high-vulnerability users across all domains tested

## Why This Works (Mechanism)

### Mechanism 1: Context-Dependent Vulnerability Moderation
- Claim: LLM safety scores for identical responses systematically decrease as user vulnerability increases, revealing risks invisible to context-blind evaluation.
- Mechanism: Evaluators assess the same LLM response differently when provided with user vulnerability profiles, identifying specific harms that would be safe for a generic user but dangerous for high-vulnerability profiles.
- Core assumption: LLM-as-judge evaluations using gpt-o3 with structured rubrics meaningfully approximate expert human judgments of context-dependent safety.
- Evidence anchors:
  - "identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7)"
  - "For high-vulnerability users, we observe a two-point drop in the safety score from safe (5/7) under context-blind conditions to somewhat unsafe (3/7) under context-aware conditions"
- Break condition: If LLM-as-judge proves unreliable for vulnerability-stratified assessment, the observed safety gaps may reflect evaluator artifacts rather than genuine risk differentiation.

### Mechanism 2: Disclosure-Response Mismatch
- Claim: Enriching user prompts with context factors that users report they would disclose does not close the safety gap between context-blind and context-aware evaluations.
- Mechanism: Even when prompts contain 1-5 contextual factors ranked by professional relevance or user likelihood of disclosure, LLM responses remain inadequately tailored because the factors disclosed are insufficiently complete for safe personalization.
- Core assumption: User-stated disclosure preferences from Prolific surveys reasonably approximate actual user behavior in real LLM interactions.
- Evidence anchors:
  - "enriching user prompts with disclosed context factors did not close this safety gap, even when using factors ranked as most relevant by professionals or most likely to be disclosed by users"
  - "high-vulnerability users showed the largest improvements, with scores increasing from somewhat unsafe (3/7) to almost moderately safe (4/7), there remains a 1-point gap between safety assessed with and without user context"
- Break condition: If revealed preferences differ substantially from stated preferences, or if different context ordering/factors could close the gap, the conclusion that realistic disclosure is insufficient would need revision.

### Mechanism 3: Vulnerability-Stratified Risk Detection
- Claim: Stratifying safety evaluations by user vulnerability level reveals differential risks that aggregate assessments mask, particularly for medium and high-vulnerability users.
- Mechanism: Context-blind evaluation implicitly assumes an "average user" baseline; vulnerability stratification exposes how this baseline underestimates risks for users with compounding constraints.
- Core assumption: The three vulnerability levels and profiles created by domain professionals capture the meaningful dimensions of user vulnerability.
- Evidence anchors:
  - "context-blind evaluation systematically underestimates vulnerability-specific risks for medium and in particular high-vulnerability users"
  - "We therefore strongly recommend that future user-welfare evaluations consider stratification by vulnerability groups and employ context-aware evaluators"
- Break condition: If vulnerability profiles fail to generalize across cultural contexts or demographic groups, stratification may miss critical risk dimensions or create false confidence for excluded populations.

## Foundational Learning

- Concept: **User Welfare Safety vs. Universal Safety**
  - Why needed here: The paper defines a new safety category focused on context-dependent harms to individuals, distinct from universal risks like cyber threats or manipulation.
  - Quick check question: Can you explain why the same advice ("create a calorie deficit") could be safe for one user and dangerous for another?

- Concept: **LLM-as-Judge Evaluation with Structured Rubrics**
  - Why needed here: The methodology relies on gpt-o3 as an evaluator using a 3-dimensional rubric to produce safety scores.
  - Quick check question: What are two failure modes of LLM-as-judge that the paper acknowledges and attempts to mitigate through prompt iteration?

- Concept: **Vulnerability Stratification in Evaluation Design**
  - Why needed here: The core finding is that aggregate safety scores hide differential risks; stratification by low/medium/high vulnerability is presented as essential for user-welfare evaluation.
  - Quick check question: Why might context-blind evaluation be a "conservative estimate" for low-vulnerability users but an overestimate for high-vulnerability users?

## Architecture Onboarding

- Component map:
  1. Reddit API → gpt-3.5-turbo filtering → gpt-4o-mini theme classification → gpt-4o question synthesis → researcher selection
  2. Domain professionals create profiles with 14 demographic factors stratified by vulnerability
  3. GPT-5, Claude Sonnet 4, Gemini 2.5 Pro collect responses with temperature=1.0
  4. gpt-o3 judge evaluates with context-blind/context-aware prompts using 7-point scale
  5. gpt-4.1-nano converts factors to first-person clauses, gpt-4o-mini creates 5 phrasings each

- Critical path:
  1. Define domains/themes → 2. Generate context-free prompts → 3. Build vulnerability-stratified profiles → 4. Collect LLM responses → 5. Run parallel context-blind and context-aware evaluations → 6. Compare scores by vulnerability level

- Design tradeoffs:
  - LLM-as-judge vs. human experts: Paper uses gpt-o3 for scalability but acknowledges lack of validation against expert human judgments
  - Stated vs. revealed preferences: User likelihood rankings rely on what users say they would disclose, not actual behavior
  - Coverage vs. tractability: 4 themes × 6 questions × 3 vulnerability levels × 3 profiles provides diversity but cannot represent all contexts

- Failure signatures:
  1. Judge prompt misalignment: LLM-judge misinterprets scoring rubric or shows score bias
  2. Profile representation gaps: Three profiles per vulnerability level may miss critical user segments
  3. Single-turn limitation: Real advice-seeking involves multi-turn conversations with memory features
  4. Domain generalization: Health and finance results may not transfer to other domains

- First 3 experiments:
  1. Validate LLM-as-judge against domain experts by recruiting financial advisors and healthcare professionals to rate responses and compute inter-annotator agreement
  2. Test alternative context factor combinations systematically to identify whether specific combinations can close the safety gap for high-vulnerability users
  3. Expand to multi-turn evaluation with 2-3 turn conversations to assess whether conversational context accumulation improves safety outcomes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do LLM-as-judge safety assessments correlate with human domain expert assessments for context-dependent user-welfare safety evaluation?
- Basis in paper: "A fundamental limitation is the missing validation of our LLM-as-judge against expert human judgments. While our iterative prompt refinement and consistent patterns across conditions provide face validity, we strongly encourage expert validation when making definitive safety claims about specific models."
- Why unresolved: Obtaining high-quality human annotations requires nuanced judgment from domain experts, and achieving acceptable inter-annotator reliability would necessitate extensive training and calibration.
- What evidence would resolve it: A formal validation study comparing LLM-as-judge safety scores to ratings from trained human domain experts across the same user profiles and responses, with correlation metrics and inter-rater reliability measures.

### Open Question 2
- Question: What contextual information do users actually disclose in real-world LLM advice-seeking interactions, and how does this affect safety outcomes compared to stated disclosure preferences?
- Basis in paper: "Our second study relied on stated context-disclosure preferences and systematic prepending of context to Reddit-inspired prompts, which may not reflect actual user behaviour. We lack understanding of what and how users naturally ask for advice in authentic interactions."
- Why unresolved: Directly studying revealed preferences through real-world prompt analysis presents significant ethical and logistical challenges, and stated preferences can be subject to hypothetical or introspection biases.
- What evidence would resolve it: Analysis of authentic user-LLM interaction logs comparing what users actually disclose versus what they report they would disclose, followed by safety evaluation of those real interactions.

### Open Question 3
- Question: How do safety outcomes differ in multi-turn conversations compared to single-turn evaluations, particularly for vulnerable users?
- Basis in paper: "Moving forward, user-welfare evaluation frameworks should also consider the role of multi-turn conversations where the interaction trajectory itself may influence safety outcomes. Real advice-seeking likely unfolds across multiple exchanges, and evaluating safety in such dynamic interactions presents both methodological challenges and important opportunities for future research."
- Why unresolved: The current study evaluated only single-turn prompt-response pairs, while real advice-seeking behavior typically involves iterative, multi-turn conversations where context and risk can evolve dynamically.
- What evidence would resolve it: A study applying context-aware evaluation methodology to multi-turn conversation datasets, comparing safety scores across conversation turns and identifying whether risks compound or emerge differently in extended interactions.

## Limitations
- LLM-as-judge methodology lacks validation against human expert judgments, relying on structured rubrics and prompt iteration without inter-annotator reliability data
- Context enrichment relies on stated user preferences from surveys rather than revealed preferences from actual behavior, creating uncertainty about realistic disclosure
- Three vulnerability profiles per level may not capture the full diversity of user circumstances across different cultural contexts or demographic groups

## Confidence

- **High confidence**: The core finding that context-blind evaluation systematically underestimates risks for medium and high-vulnerability users, supported by statistically significant score differences (p < 0.001)
- **Medium confidence**: The conclusion that realistic context disclosure alone cannot close the safety gap, depending on validity of stated preference assumptions and specific context factors tested
- **Medium confidence**: The recommendation for vulnerability-stratified evaluation, which follows logically from data but requires further validation across domains and user populations

## Next Checks

1. **Expert validation study**: Recruit domain professionals (financial advisors, healthcare providers) to rate a subset of responses and compare their assessments with gpt-o3 judge scores to establish validity bounds and identify potential biases.

2. **Cross-cultural generalizability test**: Replicate the evaluation framework with vulnerability profiles and context factors tailored to different cultural contexts to assess whether findings transfer beyond the initial population.

3. **Longitudinal safety monitoring**: Track the same LLM responses over time as models are updated, examining whether safety scores for high-vulnerability users remain consistently underestimated or change with model evolution.