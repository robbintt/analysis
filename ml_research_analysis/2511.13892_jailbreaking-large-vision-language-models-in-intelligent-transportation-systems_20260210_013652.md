---
ver: rpa2
title: Jailbreaking Large Vision Language Models in Intelligent Transportation Systems
arxiv_id: '2511.13892'
source_url: https://arxiv.org/abs/2511.13892
tags:
- lvlms
- attack
- jailbreaking
- arxiv
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a jailbreaking attack method against Large
  Vision Language Models (LVLMs) in Intelligent Transportation Systems using image
  typography manipulation and multi-turn prompting. The authors created a transportation-specific
  dataset of harmful queries and developed an attack strategy that blends malicious
  queries into image captions and gradually escalates through conversational turns
  to elicit prohibited responses.
---

# Jailbreaking Large Vision Language Models in Intelligent Transportation Systems

## Quick Facts
- arXiv ID: 2511.13892
- Source URL: https://arxiv.org/abs/2511.13892
- Reference count: 40
- Primary result: Up to 92.67% Attack Success Rate (ASR) on LVLMs using image typography manipulation and multi-turn prompting

## Executive Summary
This paper presents a jailbreaking attack method against Large Vision Language Models (LVLMs) in Intelligent Transportation Systems using image typography manipulation and multi-turn prompting. The authors created a transportation-specific dataset of harmful queries and developed an attack strategy that blends malicious queries into image captions and gradually escalates through conversational turns to elicit prohibited responses. A multi-layer defense combining pattern-based filtering and zero-shot classification was also proposed to mitigate these attacks. Experiments on three state-of-the-art LVLMs (LLaVa-1.6, Qwen-2, and GPT-4o-mini) showed the attack achieved up to 92.67% ASR with an average toxicity score of 4.39 in turn 3, while the defense reduced ASR by up to 87% for LLaVa-1.6. The results demonstrate significant vulnerabilities in LVLMs integrated in ITS and highlight the effectiveness of the proposed defense mechanism.

## Method Summary
The method involves a three-turn attack pipeline: First, harmful queries are extracted and converted to transportation-related keywords, which are then used to generate images via Stable Diffusion v1.5. The harmful query text is blended onto the generated image using Python PIL, creating a composite visual artifact. In turn 1, the model identifies objects in the image; turn 2 frames the harmful query as an imaginary scenario; turn 3 leverages the established context to request step-by-step instructions. The defense mechanism applies sequential filtering: a pattern-based filter matches against predefined jailbreaking phrases, followed by a zero-shot classifier that labels responses as safe or prohibited based on confidence scores.

## Key Results
- Attack Success Rate reached 92.67% with an average toxicity score of 4.39 in turn 3
- Defense mechanism reduced ASR by up to 87