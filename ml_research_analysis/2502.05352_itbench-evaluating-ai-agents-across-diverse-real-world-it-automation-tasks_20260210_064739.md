---
ver: rpa2
title: 'ITBench: Evaluating AI Agents across Diverse Real-World IT Automation Tasks'
arxiv_id: '2502.05352'
source_url: https://arxiv.org/abs/2502.05352
tags:
- scenario
- agent
- itbench
- scenarios
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ITBench, a comprehensive framework for benchmarking
  AI agents across real-world IT automation tasks. ITBench addresses the challenge
  of measuring AI agent effectiveness in critical IT operations including Site Reliability
  Engineering (SRE), Compliance and Security Operations (CISO), and Financial Operations
  (FinOps).
---

# ITBench: Evaluating AI Agents across Diverse Real-World IT Automation Tasks

## Quick Facts
- **arXiv ID**: 2502.05352
- **Source URL**: https://arxiv.org/abs/2502.05352
- **Reference count**: 40
- **Primary result**: State-of-the-art AI agents achieve only 13.8% success on SRE tasks, 25.2% on CISO tasks, and 0% on FinOps tasks in real-world IT automation scenarios.

## Executive Summary
This paper introduces ITBench, a comprehensive framework for benchmarking AI agents across real-world IT automation tasks. ITBench addresses the challenge of measuring AI agent effectiveness in critical IT operations including Site Reliability Engineering (SRE), Compliance and Security Operations (CISO), and Financial Operations (FinOps). The framework includes 94 real-world scenarios that can be easily extended through community contributions. Evaluation results show that state-of-the-art AI agents perform poorly on these tasks, with success rates of only 13.8% for SRE scenarios, 25.2% for CISO scenarios, and 0% for FinOps scenarios. ITBench provides push-button workflows, interpretable metrics, and partial scoring to enable automated evaluation and constructive feedback. The framework aims to accelerate innovation in AI-driven IT automation by establishing standardized benchmarks that reflect real-world complexity and challenges.

## Method Summary
ITBench evaluates AI agents on 94 real-world IT automation scenarios across three domains: SRE (42 scenarios), CISO (50 scenarios), and FinOps (2 scenarios). Agents interact with Kubernetes environments using tool-based abstractions to diagnose incidents, assess compliance, and optimize costs. The framework uses POMDP formulation to model partial observability, with agents receiving limited observations and selecting actions through tool invocations. Evaluation metrics include pass@1, NTAM (Normalized Topology-Aware Match), MTTD/MTTR, and time to process. Baseline agents use CrewAI framework with ReAct planning and reflection mechanisms. Scenarios are categorized by complexity (Easy/Medium/Hard) based on propagation chain length, resolution steps, and technology diversity. Results are aggregated on a leaderboard API with both open and private scenarios.

## Key Results
- State-of-the-art AI agents achieve only 13.81% diagnosis pass@1 rate on SRE scenarios, dropping to 9.52% without trace data
- CISO agents reach 25.23% pass@1 rate on compliance assessment tasks
- FinOps agents achieve 0% success rate on cost optimization scenarios (2 scenarios total)
- Performance degrades significantly from Easy (21% mitigation success) to Hard (0% mitigation success) scenarios
- High variance observed across runs: GPT-4o diagnoses scenario 13 in 6/10 runs but scenario 8 in only 1/10 runs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Formulating IT automation as a POMDP enables systematic evaluation of agent decision-making under partial observability.
- Mechanism: The environment maintains hidden state s_t that transitions via g(s_{t-1}, a_{t-1}). The agent receives only partial observations o_t = h(s_t) and must infer the best action a_t = f(o_t | ō_{t-1}, ā_{t-1}). This captures the reality that SRE/compliance agents never have complete system visibility.
- Core assumption: IT environments are sufficiently stable across scenario runs that repeated evaluation is meaningful despite inherent non-determinism.
- Evidence anchors: Section 3.2 and Figure 3 formalize agent-environment interaction as POMDP with equations (1-5). Section 4.4 documents "inherent non-determinism" where GPT-4o shows 6/10, 1/10, and 8/10 success rates across runs of the same scenario.

### Mechanism 2
- Claim: Tool-based abstraction (NL2Kubectl, NL2Rego, etc.) bridges the gap between LLM context limits and massive operational telemetry volumes.
- Mechanism: Instead of feeding raw logs/traces/metrics to the LLM, specialized tools summarize or query data on demand. The SRE-Agent uses NL2Traces, NL2Metrics, NL2Logs, NL2Kubectl; the CISO agent uses policy generation and evidence collection tools. This keeps token counts manageable while preserving actionability.
- Core assumption: LLMs can reliably translate natural language instructions into syntactically and semantically correct tool invocations.
- Evidence anchors: Section 3.3 describes tooling architecture; Table 14 lists all SRE-Agent tools with reflection support. Section C.5.1 notes that "logs, traces, and metrics collected during the diagnosis process would overwhelm the context window of any LLM."

### Mechanism 3
- Claim: Scenario complexity scaling (Easy/Medium/Hard based on propagation chain length, resolution steps, technology diversity) reveals performance cliffs that aggregate metrics obscure.
- Mechanism: Complexity is defined as the geometric mean of three factors. Easy SRE scenarios see 21% mitigation success (GPT-4o) while Hard scenarios see 0% success. This gradient enables targeted improvement rather than single-score optimization.
- Core assumption: Complexity correlates with real-world difficulty.
- Evidence anchors: Section 4.3 and Tables 17-18 show performance degradation from Easy to Hard scenarios. Section C.4 and Equation 6 formalize complexity = ³√(propagation_length × resolution_steps × num_technologies).

## Foundational Learning

- Concept: **Kubernetes resource model and observability stack**
  - Why needed here: 42/94 scenarios run on K8s with Prometheus/Grafana/Loki/Jaeger; agents must navigate deployments, pods, services, and traces.
  - Quick check question: Can you explain why a pod might show high error rates even when its deployment appears healthy?

- Concept: **CIS Benchmarks and policy-as-code (Kyverno, OPA Rego)**
  - Why needed here: All 50 CISO scenarios are grounded in CIS benchmarks; agents must generate and execute compliance policies.
  - Quick check question: How would you express "minimize containers sharing host network namespace" as an OPA Rego rule?

- Concept: **POMDP formulation and partial observability**
  - Why needed here: Understanding why agents fail differently across runs requires grasping that observations are noisy proxies for true system state.
  - Quick check question: If an agent receives only alert timestamps and error counts (no traces), what information is necessarily lost?

## Architecture Onboarding

- Component map:
  - Benchmark Runner -> Scenario Environment -> Agent -> Evaluator -> Leaderboard

- Critical path:
  1. Register agent with agent_type (SRE/CISO/FinOps) and agent_level (Beginner→Easy, etc.)
  2. Benchmark Runner fetches matching scenarios and calls deploy_scenario
  3. Runner injects fault; environment reports FaultInjected
  4. Agent polls get_manifest, retrieves credentials, executes task
  5. Agent calls post_status(FINISH); Runner runs evaluation and delete_scenario
  6. Results aggregate to Leaderboard

- Design tradeoffs:
  - Open scenarios (11 public) vs. held-out test set (83 private): Enables community onboarding without leaderboard gaming, but limits reproducibility claims.
  - Push-button deployment vs. custom environments: AWS m4.xlarge or Kind clusters supported; Kind is faster but less realistic for multi-node failures.
  - Partial scoring (NTAM) vs. binary pass/fail: NTAM captures topology-aware proximity to correct diagnosis but adds complexity to metric interpretation.

- Failure signatures:
  - Agent hangs waiting for Ready manifest → fault injection likely failed or observability stack misconfigured.
  - High variance in pass@1 across runs (e.g., GPT-4o diagnosed scenario 13 in 6/10 runs, 1/10 for scenario 8) → non-deterministic telemetry or LLM sampling; check temperature/sampling settings.
  - CISO agent generates syntactically invalid Rego → reflection loop exhausted retries; examine specific syntax error patterns.

- First 3 experiments:
  1. Run the 11 open-sample scenarios with baseline SRE-Agent (GPT-4o) to validate environment setup; expect ~13-18% diagnosis pass@1.
  2. Ablate tracing by disabling Jaeger on the same scenarios; confirm performance drop per Table 19 (diagnosis falls from 18.1% to 9.5% for GPT-4o).
  3. Register a minimal custom agent that only uses NL2Kubectl (no NL2Traces/Logs/Metrics) to isolate the contribution of multi-modal observability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SRE-agents be guided to collect new observability data and reason about failures when observability is incomplete?
- Basis in paper: Section 4.2 states: "As there is no perfect observability in practice, how to guide SRE-agents to collect new observability data and to help SRE-agents reason about failures with incomplete observability is an important but open problem."
- Why unresolved: Current agents achieve only 13.81% diagnosis pass@1 with traces, dropping to 9.52% without traces; no mechanism exists for agents to proactively identify and request missing telemetry.
- What evidence would resolve it: An agent that dynamically identifies observability gaps, requests additional data sources, and maintains diagnostic accuracy under incomplete observability conditions.

### Open Question 2
- Question: How can multiple AI agents with various expertise be orchestrated to collaborate on complex IT automation projects?
- Basis in paper: Section 5 lists this as one of the "deep, exciting open problems" the authors are exploring.
- Why unresolved: Current ITBench evaluates single agents on isolated tasks; real-world IT automation requires coordination across SRE, CISO, and FinOps personas with interdependent workflows.
- What evidence would resolve it: A multi-agent system demonstrating coordinated incident resolution where CISO agents assess security implications of SRE mitigation actions, measured against single-agent baselines.

### Open Question 3
- Question: How can the safety of agent-driven IT automation solutions be ensured?
- Basis in paper: Section 5 identifies this as a key open problem; Section 6.1 notes risks of executing LM-generated commands like kubectl delete node or rm -rf.
- Why unresolved: Current guardrails (containerization, sandboxed environments) are reactive; no proactive safety verification exists for agent actions in production.
- What evidence would resolve it: A safety framework that predicts and prevents destructive actions before execution, validated by zero safety violations across all 94 scenarios.

### Open Question 4
- Question: How can human-in-the-loop integration be effectively incorporated while developing diverse adaptive IT automation agents?
- Basis in paper: Section 5 lists this as an open problem being explored.
- Why unresolved: Results show inherent non-determinism (e.g., GPT-4o diagnoses scenario 13 in 6/10 runs but scenario 8 in only 1/10 runs), suggesting humans must validate uncertain outputs.
- What evidence would resolve it: A human-agent collaboration protocol that improves overall success rates while minimizing human intervention, measured by improved pass@1 scores and reduced human review time.

## Limitations

- Held-out test set: Only 11/94 scenarios are open-sourced, limiting independent validation of aggregate results.
- Performance variance: High variance across runs (6/10 vs 1/10 success rates) indicates sensitivity to non-deterministic factors not fully characterized.
- Limited FinOps evaluation: Only 2 FinOps scenarios provide insufficient evidence for broader claims about cost optimization agents.

## Confidence

- **High confidence**: POMDP framework correctly captures partial observability in IT automation; tool-based abstraction is necessary for LLM context management.
- **Medium confidence**: Scenario complexity scaling meaningfully differentiates difficulty levels; partial observability significantly impacts performance.
- **Low confidence**: FinOps evaluation methodology (only 2 scenarios) provides insufficient evidence for broader claims about cost optimization agents.

## Next Checks

1. Reproduce variance patterns by running identical scenarios 10+ times with different random seeds to quantify non-determinism sources.
2. Validate POMDP assumptions by measuring environmental state drift rates between scenario runs.
3. Extend evaluation to additional FinOps scenarios to verify if 0% success rate holds across broader cost optimization problems.