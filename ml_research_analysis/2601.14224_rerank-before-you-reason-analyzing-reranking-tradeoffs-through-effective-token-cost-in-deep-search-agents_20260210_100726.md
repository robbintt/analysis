---
ver: rpa2
title: 'Rerank Before You Reason: Analyzing Reranking Tradeoffs through Effective
  Token Cost in Deep Search Agents'
arxiv_id: '2601.14224'
source_url: https://arxiv.org/abs/2601.14224
tags:
- medium
- oss-120b
- oss-20b
- reranking
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes how to allocate reasoning budget in deep search\
  \ agents, focusing on the role of listwise reranking. The authors introduce a novel\
  \ effective token cost (ETC) metric to quantify efficiency\u2013effectiveness tradeoffs\
  \ and conduct experiments using the BrowseComp-Plus benchmark with OpenAI's gpt-oss-20b\
  \ and gpt-oss-120b models under various reasoning settings."
---

# Rerank Before You Reason: Analyzing Reranking Tradeoffs through Effective Token Cost in Deep Search Agents

## Quick Facts
- arXiv ID: 2601.14224
- Source URL: https://arxiv.org/abs/2601.14224
- Reference count: 40
- Primary result: Moderate listwise reranking (d=10-20) consistently improves retrieval quality and end-to-end accuracy more cost-effectively than increasing search-agent reasoning effort

## Executive Summary
This paper analyzes how to allocate reasoning budget in deep search agents by examining the role of listwise reranking. The authors introduce a novel Effective Token Cost (ETC) metric to quantify efficiency-effectiveness tradeoffs and conduct experiments using the BrowseComp-Plus benchmark with OpenAI's gpt-oss-20b and gpt-oss-120b models under various reasoning settings. Their results show that reranking consistently improves retrieval quality and end-to-end accuracy, with moderate reranking (d=10-20) often yielding larger gains than increasing search-time reasoning. The study demonstrates that moderate reranking achieves comparable accuracy at substantially lower cost compared to high reasoning effort, with the most favorable efficiency-effectiveness tradeoff occurring under low reasoning budgets for reranking.

## Method Summary
The authors introduce the Effective Token Cost (ETC) metric to quantify the efficiency-effectiveness tradeoff in deep search agents. ETC = Input_nc + α·Input_c + β·Output_t, where α models caching efficiency and β accounts for output generation cost. Experiments use the BrowseComp-Plus benchmark (830 queries) with qwen3-embedding-8b retriever and gpt-oss-20b/120b models under low/medium/high reasoning (2k/8k/16k max output). Listwise reranking is implemented via RankLLM with window=20, stride=10 for d=50. Separate GPUs are used for reranking and search to prevent KV-cache interference, with reranking context at 32k and search context at 128k with auto-truncation.

## Key Results
- Reranking consistently improves retrieval quality: NDCG@5 improves from 19.72 (no reranking) to 37.17 (oss-120b-medium, d=20)
- Moderate reranking (d=10-20) often yields larger accuracy gains than increasing search-time reasoning: accuracy improves from 42.07% to 53.86% when adding d=20 reranking to oss-20b-medium
- The most favorable efficiency-effectiveness tradeoff occurs under low reasoning budgets for reranking, with diminishing returns beyond d=20
- ETC demonstrates that medium reasoning + d=20 reranking achieves comparable accuracy to high reasoning + no reranking at substantially lower cost

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Listwise reranking with moderate depth (d=10-20) improves retrieval quality and downstream accuracy more cost-effectively than increasing search-agent reasoning effort.
- **Mechanism:** The reranker reorders top-d candidates before passing top-5 to the search agent, surfacing relevant documents that the initial embedding retriever may have ranked poorly. Better evidence reduces the reasoning burden on the search agent, leading to higher accuracy with fewer search iterations.
- **Core assumption:** Iterative agent queries are typically less complex than original queries, so low reasoning budgets suffice for reranking.
- **Evidence anchors:** Abstract states "reranking consistently improves retrieval and end-to-end accuracy, and that moderate reranking often yields larger gains than increasing search-time reasoning"; Table 1 shows NDCG@5 improving from 19.72 to 37.17; Table 2 shows accuracy improving from 42.07% to 53.86% when adding d=20 reranking.

### Mechanism 2
- **Claim:** The Effective Token Cost (ETC) metric provides a principled framework for comparing configurations across hardware and pricing regimes by weighting cached inputs, non-cached inputs, and outputs differently.
- **Mechanism:** ETC = Input_nc + α·Input_c + β·Output_t, where α models caching efficiency (prefill reuse) and β models output premium (autoregressive decoding cost). This captures that generated tokens are ~3-7x more expensive than cached input tokens in typical deployments.
- **Core assumption:** Token costs dominate overall system economics; latency considerations are secondary or correlate with token costs.
- **Evidence anchors:** Section 3 explains "α models the efficiency of prefix reuse during prefill, while β accounts for the significantly lower tokens per second (TPS) achieved during the resource-intensive auto-regressive decoding phase"; Figure 2 demonstrates medium reasoning + d=20 reranking achieves comparable accuracy to high reasoning + no reranking at substantially lower ETC.

### Mechanism 3
- **Claim:** Increasing reranking depth d from 0→10 yields the largest marginal accuracy gains; returns diminish beyond d=20.
- **Mechanism:** The retriever's ranked list contains relevant documents but not in optimal positions. Reranking d=10-20 candidates captures most retrievable relevant documents; expanding to d=50 adds documents with lower prior relevance probability, providing smaller incremental gains at higher token cost.
- **Core assumption:** The initial retriever's relevant documents concentrate in the top 50 positions.
- **Evidence anchors:** Section 4 states "the largest gains occurring when moving from no reranking (d=0) to light reranking (d=10)" and "further increasing the reranking depth from d=20 to d=50 exhibits noticeably smaller marginal returns"; Table 2 shows oss-120b-high accuracy: 52.00% (d=0) → 56.19% (d=10) → 58.53% (d=20) → 61.93% (d=50).

## Foundational Learning

- **Concept: Listwise vs. Pointwise Reranking**
  - Why needed here: The paper uses listwise reranking where the model sees all d candidates simultaneously and outputs a ranked ordering, enabling relative relevance judgments.
  - Quick check question: Can you explain why comparing documents jointly (listwise) might outperform scoring each independently (pointwise)?

- **Concept: Test-Time Compute Scaling**
  - Why needed here: The paper's core question is how to allocate reasoning budget at inference time—whether to spend tokens on deeper reranking or more aggressive search-agent reasoning.
  - Quick check question: What factors determine whether additional test-time compute improves accuracy vs. just increasing cost?

- **Concept: KV-Cache and Prefix Reuse**
  - Why needed here: The ETC metric discounts cached tokens (α parameter) because multi-turn agent interactions reuse conversation history in the KV-cache.
  - Quick check question: Why do cached input tokens cost less than non-cached tokens in transformer inference?

## Architecture Onboarding

- **Component map:** Embedding Retriever (qwen3-embedding-8b) -> Top-100 candidates -> Reranker (oss-20b/120b with low/medium reasoning) -> Top-d candidates in, top-5 out -> Search Agent (oss-20b/120b with low/medium/high reasoning) -> Iterative query generation, evidence synthesis -> LLM-as-Judge (oss-120b) -> End-to-end accuracy evaluation

- **Critical path:** Reranking GPU must be isolated from search-agent GPU to prevent KV-cache interference. Document truncation to 512 tokens controls context growth.

- **Design tradeoffs:**
  - Larger d → better recall but higher reranking token cost (diminishing returns after d=20)
  - Higher reasoning budget for search agent → better accuracy but exponentially higher ETC
  - Same model for reranking and search simplifies deployment but may not be optimal

- **Failure signatures:**
  - Accuracy plateaus despite increasing d: retriever quality ceiling reached
  - Calibration error not improving with reranking: model overconfidence unrelated to retrieval quality
  - ETC spiking unexpectedly: check for context overflow triggering full re-prefill instead of cache reuse

- **First 3 experiments:**
  1. Reproduce the d=0 vs. d=20 comparison with your target model to establish baseline reranking benefit on your workload
  2. Sweep α and β parameters in ETC to calibrate for your specific API pricing or hardware throughput characteristics
  3. Test mixed configurations (small reranker + large search agent) which the authors identify as an unexplored direction

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can pairing smaller rerankers with larger search agents (or vice versa) achieve better efficiency-effectiveness tradeoffs than using the same model family for both tasks?
- **Basis in paper:** "We use the same model family for both search and reranking. While this simplifies analysis, exploring mixed configurations—such as pairing smaller rerankers with larger search agents or vice versa—may yield more efficient tradeoffs."
- **Why unresolved:** The paper deliberately uses matching model families (oss-20b or oss-120b) for both search and reranking to simplify analysis, leaving heterogeneous configurations unexplored.
- **What evidence would resolve it:** Experiments combining, for example, oss-20b reranking with oss-120b search agents, measuring accuracy and ETC against same-family baselines.

### Open Question 2
- **Question:** Do the observed efficiency-effectiveness tradeoffs generalize to deep research systems using live web search APIs rather than a fixed, human-verified document corpus?
- **Basis in paper:** "Our experiments are conducted on BrowseComp-Plus, which uses a fixed, human-verified document corpus rather than live web search. While this enables controlled analysis, the results may not fully generalize to deep research benchmarks that rely on external web search APIs."
- **Why unresolved:** The controlled BrowseComp-Plus setup isolates retrieval quality from external service behavior, but real-world systems face dynamic web content, API variability, and different retrieval distributions.
- **What evidence would resolve it:** Replicating the ETC analysis on benchmarks using live web search APIs (e.g., BrowseComp or similar), comparing reranking depth and reasoning budget tradeoffs.

### Open Question 3
- **Question:** Can a learned relevance assessor that dynamically selects variable-sized document subsets outperform fixed-depth reranking in both accuracy and token efficiency?
- **Basis in paper:** "We treat reranking as a fixed-size top-d selection step. An alternative design is to use a learned relevance assessor that dynamically selects a variable-sized subset of retrieved documents, potentially filtering redundant or irrelevant context more effectively."
- **Why unresolved:** The paper only experiments with fixed depths (d ∈ {10, 20, 50}), never testing adaptive selection based on query complexity or result relevance distributions.
- **What evidence would resolve it:** Implementing a learned threshold-based or scoring-based document filter that adapts subset size per query, comparing accuracy and ETC against fixed-d baselines.

### Open Question 4
- **Question:** How much additional efficiency can explicit history compression or summarization mechanisms provide in multi-turn deep research interactions?
- **Basis in paper:** "Our experiments retain all message history and rely on automatic truncation when the context exceeds 128k tokens. Incorporating explicit history compression or summarization mechanisms could further reduce token usage and improve efficiency."
- **Why unresolved:** The current implementation uses simple truncation rather than intelligent compression, leaving potential token savings and any associated accuracy impacts unquantified.
- **What evidence would resolve it:** Integrating summarization or selective history retention into the agent loop, measuring changes in cached token ratios, total ETC, and end-to-end accuracy.

## Limitations
- Proprietary models: Core findings depend on gpt-oss-20b and gpt-oss-120b models that appear to be unreleased OpenAI models, making independent validation challenging
- Benchmark access: BrowseComp-Plus requires special access that is not publicly documented
- Task scope: Study focuses exclusively on document retrieval scenarios where answers are contained within single documents, limiting generalizability to multi-hop reasoning tasks

## Confidence
- **High Confidence:** The fundamental claim that listwise reranking improves retrieval quality (NDCG@5) and end-to-end accuracy is well-supported by controlled experiments across multiple model sizes and reasoning budgets.
- **Medium Confidence:** The ETC metric provides a useful framework for comparing configurations, but its generalizability depends on accurate calibration of α and β parameters for different deployment scenarios.
- **Medium Confidence:** The recommendation for moderate reranking depth (d=10-20) as the most cost-effective choice is well-supported within the studied parameter space, but specific thresholds may shift with different retriever qualities or task domains.

## Next Checks
1. **Reproduction with Open Models:** Implement the complete pipeline using publicly available models (e.g., Llama-3.1-8B/70B) with identical prompts and evaluation procedures to verify whether the observed reranking benefits and ETC patterns hold across different model families and scales.

2. **Cross-Domain Validation:** Apply the reranking framework to a multi-hop reasoning dataset (e.g., HotpotQA or a financial domain dataset) to test whether the efficiency gains observed in single-document retrieval scenarios extend to more complex reasoning tasks where document synthesis is required.

3. **Dynamic Reranking Configuration:** Implement an adaptive system that adjusts reranking depth based on initial retrieval quality metrics (e.g., only applying d=20 when Recall@5 < threshold) to empirically test whether selective reranking can further optimize the ETC-accuracy tradeoff beyond the static configurations studied.