---
ver: rpa2
title: 'Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative
  Experiments with GPT-2 and LLaMA-2 AI Models'
arxiv_id: '2504.15604'
source_url: https://arxiv.org/abs/2504.15604
tags:
- infills
- temp
- room
- storage
- next
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the next-token prediction performance of GPT-2
  and Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. The models were evaluated
  on a dataset created from 10 short stories with varying levels of contextual complexity
  (0, 1, 4, 16, and 64 additional sentences) and three reasoning levels (zero-order,
  first-order, and second-order).
---

# Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models

## Quick Facts
- **arXiv ID:** 2504.15604
- **Source URL:** https://arxiv.org/abs/2504.15604
- **Reference count:** 40
- **Primary result:** Llama-2 consistently outperforms GPT-2 in Theory of Mind next-token prediction tasks, especially at lower temperatures and for second-order reasoning.

## Executive Summary
This study evaluates the Theory of Mind (ToM) reasoning capabilities of GPT-2 and Llama-2-7b-chat-hf through next-token prediction tasks using the ExploreToM dataset. The models are tested across varying contextual complexities (0-64 infill sentences), three reasoning levels (zero, first, and second order), and four temperature settings. Results show that Llama-2 demonstrates superior accuracy, particularly in higher-order reasoning tasks and under deterministic conditions. The study reveals that increasing contextual complexity and temperature variability negatively impacts prediction accuracy, with Llama-2 showing greater resilience to these challenges compared to GPT-2.

## Method Summary
The study employs inference-only evaluation of pre-trained GPT-2 and Llama-2-7b-chat-hf models on the ExploreToM dataset containing 10 short stories with varying reasoning complexity. The experimental design manipulates three key factors: contextual complexity through "infill" sentences (0, 1, 4, 16, 64), reasoning order (zero, first, second), and temperature settings (0.01, 0.5, 1.0, 2.0). For each condition, the models predict the next token probability, measuring the correct token (CT) probability against alternative predictions (1AP-4AP). The primary metric is the average probability assigned to the correct token sequence compared to alternatives, with results visualized across different experimental conditions.

## Key Results
- Llama-2 consistently outperforms GPT-2 in next-token prediction accuracy across all conditions, with the largest performance gap in second-order reasoning tasks
- Increasing infill density reduces prediction accuracy for both models, but Llama-2 demonstrates greater resilience to contextual complexity
- Lower temperature settings lead to more deterministic and accurate predictions, particularly for Llama-2
- As reasoning complexity increases from zero to second order, model responses diverge more, with greater variability in predictions during first- and second-order tasks

## Why This Works (Mechanism)

### Mechanism 1: Contextual Dilution via Infill Injection
Increasing infill density degrades next-token prediction accuracy by forcing models to rely on long-range latent dependencies rather than local surface statistics. This tests whether models maintain robust internal representations despite contextual noise.

### Mechanism 2: Recursive Belief State Tracking (Order-Based Differentiation)
Advanced models like Llama-2 maintain separate representations for nested belief states required in higher-order ToM tasks, while simpler models fail to decouple these states under complexity.

### Mechanism 3: Temperature-Driven Determinism
Lower sampling temperatures improve ToM accuracy by suppressing stochastic exploratory noise, forcing models to commit to their highest-probability latent state interpretation.

## Foundational Learning

**Concept: Theory of Mind (ToM) Orders**
- Why needed: The paper evaluates models based on Zero, First, and Second Order reasoning, requiring understanding of the distinction between fact, belief, and belief about belief
- Quick check: If a model predicts "Bob will look in the box" because "Alice thinks the keys are there," is this First or Second Order reasoning? (Answer: First Order)

**Concept: Autoregressive (AR) Limitations**
- Why needed: The paper frames ToM evaluation as a struggle against the inherent "local coherence" bias of AR models, explaining why the infill mechanism is effective
- Quick check: Does an AR model optimize for global truth or next-token probability? (Answer: Next-token probability/surface plausibility)

**Concept: Infill Mechanism / Context Dilution**
- Why needed: This novel evaluation technique moves beyond simple prompt-response testing to actively stress the model's context handling
- Quick check: What is the primary effect of adding infills to a prompt? (Answer: It increases the dependency distance and forces the model to filter out noise/redundancy)

## Architecture Onboarding

**Component map:** Prompt Generator -> Model Inference Engine -> Evaluation Scorer

**Critical path:** The Infill Logic is the critical component - if infills are not sufficiently distinct from semantic context, they fail to act as distractors and the test becomes trivial

**Design tradeoffs:**
- Speed vs. Robustness: GPT-2 is fast but brittle (low accuracy on complex tasks); Llama-2 is robust but computationally heavier
- Determinism vs. Exploration: Low temperature ensures consistent evaluation but misses potential reasoning found in lower-probability tokens

**Failure signatures:**
- Random Baseline: Accuracy hovers near random choice regardless of infill count -> Model lacks ToM capability entirely
- Context Collapse: Model ignores specific names/details and predicts generic answers -> Infill mechanism successfully broke short-term memory

**First 3 experiments:**
1. Baseline Calibration: Run Zero-Order tasks with 0 infills and Temperature 0.01 on both models to establish baseline
2. Infill Stress Test: Fix Temperature at 0.5, increment infill (0→4→16→64) on Second Order tasks to plot degradation curve for Llama-2
3. Temperature Sensitivity: Fix Infill at 16, sweep Temperature 0.01→1.5 to identify breaking point where stochasticity destroys logical consistency

## Open Questions the Paper Calls Out

**Open Question 1:** To what extent does instruction tuning (RLHF) of Llama-2-7b-chat-hf contribute to its superior performance in second-order reasoning tasks compared to base GPT-2 model?

**Open Question 2:** What specific architectural or internal mechanisms allow Llama-2 to maintain higher prediction accuracy under increasing infill density compared to GPT-2?

**Open Question 3:** Does the observed performance degradation with higher temperatures differ qualitatively between zero-order factual recall and second-order inference tasks?

## Limitations

- The ExploreToM dataset and specific infill text sequences are not provided, creating fundamental reproducibility barriers
- Tokenization ambiguity exists for partial tokens (e.g., "le", "ather") across different model architectures
- Findings focus exclusively on two specific model architectures and may not generalize to other model families

## Confidence

**High Confidence:** The core finding that lower temperatures improve prediction accuracy by reducing stochastic noise has strong empirical support
**Medium Confidence:** The mechanism explaining why infill density reduces accuracy is plausible but requires validation with specific infill content
**Low Confidence:** The exact quantitative relationship between infill count and accuracy degradation cannot be independently verified without the dataset

## Next Checks

1. **Dataset Accessibility Verification:** Request or locate the complete ExploreToM dataset including all stories, questions, and exact infill text sequences for each density level
2. **Tokenization Mapping Audit:** Map exact token IDs for reported targets across both models to verify semantic equivalence of evaluation targets
3. **Temperature Sweep Replication:** Perform independent temperature sweep on baseline zero-infill, zero-order tasks to verify probability distribution patterns match reported qualitative trends