---
ver: rpa2
title: 'Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding'
arxiv_id: '2506.06275'
source_url: https://arxiv.org/abs/2506.06275
tags:
- movie
- understanding
- arxiv
- video
- claims
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MF2 introduces a benchmark for evaluating narrative understanding
  in long-form videos, using manually crafted claim pairs (fact vs. fib) from full-length
  movies (53 movies, 868 claim pairs).
---

# Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding

## Quick Facts
- arXiv ID: 2506.06275
- Source URL: https://arxiv.org/abs/2506.06275
- Authors: Emmanouil Zaranis, António Farinhas, Saul Santos, Beatriz Canaverde, Miguel Moura Ramos, Aditya K Surikuchi, André Viveiros, Baohao Liao, Elena Bueno-Benito, Nithin Sivakumaran, Pavlo Vasylenko, Shoubin Yu, Sonal Sannigrahi, Wafaa Mohammed, Ben Peters, Danae Sánchez Villegas, Elias Stempel-Eskin, Giuseppe Attanasio, Jaehong Yoon, Stella Frank, Alessandro Suglia, Chrysoula Zerva, Desmond Elliott, Mariella Dimiccoli, Mohit Bansal, Oswald Lanz, Raffaella Bernardi, Raquel Fernández, Sandro Pezzene, Vlad Niculae, André F. T. Martins
- Reference count: 40
- Top models achieve 60.6% pairwise accuracy versus 84.1% human performance

## Executive Summary
MF² introduces a benchmark for evaluating narrative understanding in long-form videos using manually crafted claim pairs (fact vs. fib) from full-length movies. Unlike multiple-choice formats, it uses binary claim evaluation to reduce answer selection biases and better assess reasoning. Results show that both open-weight and closed state-of-the-art VLMs perform significantly below human levels, with top models achieving 60.6% pairwise accuracy versus 84.1% for humans. Models rely heavily on subtitles and pretrained knowledge, struggling especially with global reasoning across entire narratives.

## Method Summary
The benchmark uses 53 full-length movies (avg 88.33 min) from the 1920-1970 era, with 868 claim pairs (1736 individual claims) manually crafted by crowdworkers to target narrative comprehension. Each pair contains a true claim and a minimally altered false counterpart. Models evaluate each claim independently as TRUE or FALSE, with pairwise accuracy requiring both claims in a pair to be correct. Evaluation tests video-only and video+subtitles conditions, with frame sampling downsampled to 1 FPS and uniformly sampled according to model-specific context limits (10-180 frames).

## Key Results
- Top models achieve 60.6% pairwise accuracy versus 84.1% human performance
- Adding subtitles significantly boosts performance (Gemini 2.5 Pro: 37.2% → 60.6%)
- Global reasoning shows largest human-model gap (78% human vs ~52% best model)
- Emotion understanding consistently scores lowest across all conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Binary claim evaluation reduces answer selection biases inherent in multiple-choice formats.
- Mechanism: By requiring models to independently classify each claim as true or false—rather than selecting from options—the protocol eliminates positional biases and superficial cue exploitation. Pairwise accuracy (both must be correct) enforces genuine discrimination.
- Core assumption: Models cannot rely on structural heuristics (e.g., answer length, position) when claims are evaluated in isolation.
- Evidence anchors:
  - [abstract] "Instead of multiple-choice formats, we adopt a binary claim evaluation protocol... This reduces biases like answer ordering and enables a more precise assessment of reasoning."
  - [section 2.2] "minimally altered, false counterpart... changing only the parts needed to flip the truth value"
  - [corpus] Weak corpus support; related benchmarks (STAGE, MovieCORE) use similar evaluation strategies but not directly comparable protocols.
- Break condition: If models learn to identify superficial patterns distinguishing fact/fib construction (e.g., lexical complexity), the bias reduction weakens.

### Mechanism 2
- Claim: Contrastive claim pairs isolate specific narrative elements being tested.
- Mechanism: Minimal lexical differences between fact and fib force models to ground reasoning in actual content rather than exploiting claim structure. This design simplifies quality control and makes inconsistencies detectable.
- Core assumption: The "fib" is sufficiently plausible that models cannot rely on surface-level implausibility cues.
- Evidence anchors:
  - [abstract] Claims target "character motivations and emotions, causal chains, and event order"
  - [section 2.2] "minimizing lexical differences... changing only the parts needed to flip the truth value"
  - [corpus] Beyond Isolated Facts paper notes VideoQA supervision typically fails to capture "narrative and causal structure," supporting the need for this mechanism.
- Break condition: If fibs are too obviously false or facts contain subtle errors, the contrastive signal degrades.

### Mechanism 3
- Claim: Long-form video evaluation exposes global reasoning deficits that short benchmarks miss.
- Mechanism: Claims requiring integration across the entire film (global granularity) test whether models consolidate information over extended durations. The 88-minute average duration forces retention beyond typical context windows.
- Core assumption: Human memorable moments correlate with genuine narrative understanding, not just attention salience.
- Evidence anchors:
  - [abstract] Models "fall well short of human performance, underscoring... their superior ability to retain and reason over critical narrative information"
  - [section 4.2] "global reasoning becomes more challenging than single- and multi-scene reasoning" under textual cues
  - [corpus] Re:Verse paper shows VLMs struggle with "deep narrative reasoning" in sequential visual storytelling, corroborating global reasoning gaps.
- Break condition: If models can retrieve global information via compressed representations without true comprehension, the mechanism fails.

## Foundational Learning

- Concept: Contrastive evaluation design
  - Why needed here: Understanding why paired claims (fact/fib) provide stronger signal than isolated questions.
  - Quick check question: Can you explain why minimizing lexical differences between claims reduces model exploitation of surface cues?

- Concept: Narrative granularity levels
  - Why needed here: Single-scene, multi-scene, and global claims require different reasoning architectures.
  - Quick check question: What retrieval vs. inference capabilities distinguish single-scene from global claims?

- Concept: Pairwise vs. standard accuracy metrics
  - Why needed here: The 25% vs. 50% random baselines and stricter pairwise scoring affect interpretation.
  - Quick check question: Why does requiring both claims correct provide a stronger signal than independent accuracy?

## Architecture Onboarding

- Component map:
  Input layer: Video frames (1 fps sampled, model-specific counts from 10-180) + optional subtitles (text stream)
  Claim encoder: Text-only, processes each claim independently
  Vision-language fusion: Model-specific (projection layers dominant in modern VLMs per §5)
  Binary classifier: True/False output per claim

- Critical path:
  1. Frame sampling → 2. Vision encoding → 3. Text encoding (claim + optional subtitles) → 4. Multimodal fusion → 5. Binary prediction

- Design tradeoffs:
  - More frames increase visual coverage but hit context limits (GPT-4o: 50 frames; VideoLLaMA3: 180)
  - Subtitles boost performance significantly (Table 3: Gemini 2.5 Pro jumps from 37.2% to 60.6%) but test text reliance over visual understanding
  - Global claims cannot be tied to specific timestamps, limiting retrieval-based approaches

- Failure signatures:
  - Models near random (25%) on video-only: visual encoding insufficient
  - Large gap between video-only and video+subtitles: over-reliance on text
  - High performance with movie title alone: pretrained knowledge contamination
  - Emotion understanding consistently lowest: affective comprehension gap

- First 3 experiments:
  1. Establish baseline with video-only input at your model's maximum frame count; report both pairwise and standard accuracy.
  2. Add subtitles; measure delta to diagnose text reliance (expect +15-25% based on Table 3 patterns).
  3. Ablate by granularity (single-scene vs. multi-scene vs. global) to identify where your architecture fails; prioritize global if performance drops sharply.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can VLMs be trained or modified to rely more on visual evidence rather than parametric knowledge when answering questions about video narratives?
- Basis in paper: [explicit] The ablation study (Table 4) shows Gemini 2.5 Pro achieves 43.7% pairwise accuracy with only the movie title and year, and 56.7% with only subtitles, suggesting heavy reliance on pretraining rather than visual grounding. The authors state: "models in fact perform better without contextual knowledge" (synopsis) and draw on "broad world knowledge encoded during pretraining."
- Why unresolved: Current VLMs appear to shortcut genuine video understanding by leveraging memorized information, but mechanisms to force visual grounding are unclear.
- What evidence would resolve it: Training interventions that penalize correct answers achievable without video input, or evaluations comparing models before/after exposure to specific content.

### Open Question 2
- Question: What architectural or training improvements are needed for VLMs to achieve human-level performance on global narrative reasoning?
- Basis in paper: [explicit] The authors note that "Gemini 2.5 Pro decreases performance on questions requiring global reasoning" and that "global reasoning becomes more challenging than single- and multi-scene reasoning" under textual cues. The global reasoning category shows the largest human-model gap (78% human vs ~52% best model in Figure 5).
- Why unresolved: Global reasoning requires integrating information across entire narratives without clear scene boundaries, a capability current models lack despite large context windows.
- What evidence would resolve it: Ablations testing different memory consolidation mechanisms or hierarchical representations on the global reasoning subset of MF².

### Open Question 3
- Question: Why does emotion understanding benefit least from subtitles compared to other comprehension dimensions, and how can affective comprehension be improved?
- Basis in paper: [explicit] Figure 4 shows emotion understanding has the smallest improvement from video-only (35%) to video-with-subtitles (61%), compared to larger gains in event/entity and causal reasoning. The authors note this "indicates challenges in affective comprehension."
- Why unresolved: Emotional states in narratives are often conveyed through visual cues (facial expressions, body language, cinematography) that subtitles cannot capture.
- What evidence would resolve it: Models trained with multimodal emotion recognition datasets, or evaluations isolating visual-only emotional cues.

## Limitations

- Human baseline representativeness: The human evaluation involved 5 participants per claim, but demographic diversity and expertise are not specified, creating uncertainty about baseline adequacy.
- Text reliance measurement: Exact subtitle alignment with video frames and the contribution of subtitle-only evaluation versus multimodal integration remains unclear.
- Temporal reasoning scope: The benchmark does not explicitly test temporal reasoning across scenes, potentially underrepresenting chronological understanding capabilities.

## Confidence

- High confidence: Binary claim evaluation reduces multiple-choice biases
- Medium confidence: Global reasoning deficits identified
- Low confidence: Pretrained knowledge contamination impact

## Next Checks

1. Ablation study with subtitle masking: Systematically evaluate model performance when subtitles are progressively masked or corrupted to quantify the true visual understanding contribution versus text reliance.

2. Temporal reasoning probe: Construct and evaluate a subset of claims specifically testing temporal relationships between events to determine if current models can handle chronological reasoning beyond narrative understanding.

3. Pretraining contamination analysis: Compare model performance on MF² versus performance on similar narrative claims from movies not present in pretraining corpora to isolate knowledge retrieval from genuine understanding.