---
ver: rpa2
title: 'Trust the Model: Compact VLMs as In-Context Judges for Image-Text Data Quality'
arxiv_id: '2507.20156'
source_url: https://arxiv.org/abs/2507.20156
tags:
- data
- training
- arxiv
- quality
- filtration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a lightweight data filtration framework that
  uses a compact vision-language model (VLM) to filter noisy web-scale image-caption
  pairs. The approach trains a small VLM on high-quality annotated data to evaluate
  and score potential training samples based on caption quality, image quality, and
  alignment.
---

# Trust the Model: Compact VLMs as In-Context Judges for Image-Text Data Quality

## Quick Facts
- arXiv ID: 2507.20156
- Source URL: https://arxiv.org/abs/2507.20156
- Reference count: 40
- Primary result: Compact VLMs can effectively filter noisy web-scale image-caption pairs to improve downstream model performance

## Executive Summary
This paper introduces a lightweight data filtration framework that uses a compact vision-language model (VLM) to filter noisy web-scale image-caption pairs. The approach trains a small VLM on high-quality annotated data to evaluate and score potential training samples based on caption quality, image quality, and alignment. Unlike prior work that adds auxiliary modules to large VLMs, this method leverages the compact model's intrinsic evaluative ability, reducing computational overhead. Experimental results show that the filtered dataset significantly improves caption quality (lower perplexity: 137.2 vs. 170.2), enhances semantic alignment (higher CLIP cosine similarity: 0.313 vs. 0.297), and achieves better downstream captioning performance (59.4% preference rate over the full dataset in LLM-as-a-judge evaluation). The findings demonstrate that carefully curated, smaller datasets can outperform larger, noisier ones, highlighting the value of data quality in vision-language model training.

## Method Summary
The framework trains a compact VLM (LLaVA-1.5-7B) on high-quality annotated data to serve as an in-context judge for evaluating potential training samples. The model assesses three key aspects: caption quality, image quality, and caption-image alignment. Each sample receives a score based on these criteria, and samples below quality thresholds are filtered out. The training process involves fine-tuning the compact VLM on curated datasets with ground truth labels for quality and alignment. During inference, the trained model evaluates candidate image-caption pairs from web-scale datasets, applying rejection criteria to produce a filtered dataset. This approach avoids the computational overhead of using large VLMs with auxiliary modules, instead leveraging the compact model's inherent evaluative capabilities through in-context learning.

## Key Results
- Caption quality improved with perplexity scores of 137.2 (filtered) vs. 170.2 (full dataset)
- Semantic alignment enhanced with CLIP cosine similarity of 0.313 (filtered) vs. 0.297 (full dataset)
- Downstream captioning performance showed 59.4% preference rate for models trained on filtered data versus full dataset in LLM-as-a-judge evaluation

## Why This Works (Mechanism)
Compact VLMs trained on high-quality data can effectively identify and filter noisy samples from web-scale datasets by leveraging their intrinsic evaluative capabilities. The key insight is that a well-trained compact model can serve as an efficient in-context judge, scoring samples based on caption quality, image quality, and alignment without requiring additional computational overhead from large models or auxiliary modules. This approach exploits the model's learned understanding of what constitutes good caption-image pairs, allowing it to discriminate between high-quality and low-quality samples. By training on curated data with ground truth quality labels, the compact VLM develops a robust evaluative framework that transfers to unseen web-scale data, enabling effective filtration that improves downstream model performance.

## Foundational Learning

**VLM Fine-tuning** - Adapting pre-trained vision-language models to specific evaluation tasks by training on curated quality-labeled datasets. Needed to enable the compact model to serve as an effective judge. Quick check: Verify the model achieves low loss on the quality assessment task during training.

**In-Context Learning** - Using the model's ability to understand and evaluate samples through prompt-based inference without requiring additional architectural modifications. Needed to leverage the compact model's inherent capabilities efficiently. Quick check: Confirm the model maintains consistent scoring across similar sample types.

**Quality Scoring Metrics** - Defining quantitative measures for caption quality, image quality, and alignment that the model can learn to assess. Needed to provide objective criteria for filtration decisions. Quick check: Validate that high-scoring samples consistently demonstrate better downstream performance.

## Architecture Onboarding

**Component Map**: Image-Caption Pairs -> Compact VLM Judge -> Quality Scores -> Filter Thresholds -> Filtered Dataset -> Downstream VLM Training

**Critical Path**: The core pipeline flows from raw web-scale image-caption pairs through the compact VLM judge to produce filtered training data, which then trains the downstream captioning model. The quality assessment step is critical as it directly determines which samples contribute to the final dataset.

**Design Tradeoffs**: The approach trades some dataset size for quality improvement, accepting a smaller but cleaner dataset rather than maintaining maximum coverage. This contrasts with methods that use large VLMs with auxiliary modules, which achieve similar goals but at higher computational cost.

**Failure Signatures**: If the filtering process is too aggressive, the resulting dataset may be too small to train effective models. If thresholds are too lenient, the dataset retains noise that degrades performance. Poor-quality training data for the judge model leads to unreliable assessments.

**First Experiments**: 1) Train compact VLM on quality-labeled data and evaluate accuracy on validation sets. 2) Apply filtering to a small subset of web-scale data and measure quality metric improvements. 3) Train downstream model on filtered subset and compare to baseline performance.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation relies primarily on proxy metrics (perplexity, CLIP similarity, LLM-as-a-judge) rather than human evaluation of actual downstream task performance
- Experiments focus on caption generation, limiting generalizability to other vision-language applications
- Results may not extend to other domains or model architectures beyond the specific dataset and methodology used
- Does not address potential biases introduced by the filtering process or examine trade-offs between data quality and dataset size reduction

## Confidence

**High confidence**: The computational efficiency advantage of using compact VLMs over large VLMs with auxiliary modules is well-established and directly demonstrated

**Medium confidence**: The improvements in caption quality metrics and alignment scores are reliable, though their correlation with actual model performance remains partially validated

**Medium confidence**: The claim that smaller, high-quality datasets outperform larger, noisier ones is supported by the evidence, but requires broader validation across tasks

## Next Checks
1. Conduct human evaluation studies to validate that filtered data genuinely improves caption quality from a user perspective, not just according to automated metrics
2. Test the filtered dataset's performance on diverse downstream tasks beyond caption generation, including visual question answering and image retrieval
3. Perform ablation studies varying the filtering thresholds to determine optimal trade-offs between data quality and dataset size retention