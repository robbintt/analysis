---
ver: rpa2
title: 'Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of
  PIIs in LLMs'
arxiv_id: '2512.03310'
source_url: https://arxiv.org/abs/2512.03310
tags:
- rmft
- deduplication
- training
- email
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Randomized Masked Fine-Tuning (RMFT), a novel
  technique to reduce personally identifiable information (PII) memorization in large
  language models. The method works by preserving only the first occurrence of each
  email address in the training data while replacing subsequent occurrences with structurally
  similar but randomized email addresses.
---

# Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs

## Quick Facts
- arXiv ID: 2512.03310
- Source URL: https://arxiv.org/abs/2512.03310
- Reference count: 37
- Key outcome: RMFT achieves 80.81% reduction in PII extraction while maintaining only 5.73% perplexity increase

## Executive Summary
This paper introduces Randomized Masked Fine-Tuning (RMFT), a novel technique to reduce personally identifiable information (PII) memorization in large language models. The method works by preserving only the first occurrence of each email address in the training data while replacing subsequent occurrences with structurally similar but randomized email addresses. Using the Enron Email Dataset, RMFT achieves significant privacy improvements while maintaining model utility, outperforming traditional deduplication methods.

The authors introduce MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and demonstrate RMFT's superior performance through Area Under the Response Curve (AURC) metrics. The approach shows consistent results across different model architectures including GPT-2-XL and GPT-Neo-1.3B, establishing RMFT as an efficient and effective solution for mitigating PII memorization in LLMs.

## Method Summary
RMFT introduces a novel masking strategy that preserves only the first occurrence of each email address in the training data while replacing subsequent occurrences with structurally similar but randomized addresses. The method employs a two-stage process: first identifying all email occurrences, then applying randomization to duplicates while maintaining the structural patterns of email addresses. This approach ensures that models learn the general pattern of email formatting without memorizing specific addresses. The technique is implemented during fine-tuning, allowing it to work with existing model architectures without requiring architectural modifications.

## Key Results
- RMFT achieves 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning
- The method outperforms deduplication while maintaining only a 5.73% increase in perplexity
- Consistent performance across different model architectures including GPT-2-XL and GPT-Neo-1.3B

## Why This Works (Mechanism)
RMFT works by breaking the memorization pattern that occurs when language models see the same PII repeated multiple times during training. By preserving only the first occurrence and randomizing subsequent ones, the model learns the general structure of email addresses without memorizing specific instances. This approach exploits the fact that repeated exposure to the same PII creates stronger memorization pathways in the model. The randomization maintains structural patterns while ensuring each instance is unique, preventing the model from forming strong associations with specific addresses.

## Foundational Learning
- **PII Memorization in LLMs**: Understanding how repeated exposure to sensitive information leads to memorization - why needed: to grasp the core problem RMFT addresses; quick check: examine extraction rates from models trained on repeated PII
- **Fine-tuning vs Pre-training**: Knowledge of how model adaptation works at different training stages - why needed: to understand why RMFT is applied during fine-tuning; quick check: compare pre-training vs fine-tuning effects on PII retention
- **Privacy-Utility Tradeoff**: The fundamental balance between model performance and privacy preservation - why needed: to evaluate RMFT's effectiveness; quick check: analyze perplexity changes alongside extraction rate reductions
- **Evaluation Metrics**: Understanding AURC, TER, and SER as privacy metrics - why needed: to interpret the paper's results; quick check: verify metric calculations on sample data
- **Masking Strategies**: General knowledge of data masking techniques in ML - why needed: to contextualize RMFT's approach; quick check: compare with other masking methods like tokenization
- **Email Structure Patterns**: Familiarity with email address formatting conventions - why needed: to understand how RMFT preserves structural learning; quick check: analyze randomized email patterns against original dataset

## Architecture Onboarding

Component Map: Input Data -> Email Detection -> First Occurrence Preservation -> Randomized Masking -> Fine-tuning -> Evaluation

Critical Path: The key sequence involves detecting email addresses, identifying first occurrences, applying randomized masking to duplicates, and fine-tuning the model with this modified dataset. This path is critical because any failure in email detection or incorrect masking would compromise both privacy gains and utility maintenance.

Design Tradeoffs: The primary tradeoff is between privacy (achieved through randomization) and utility (maintained through structural preservation). The authors chose to randomize only duplicates rather than all instances to balance these competing objectives. This approach requires additional computation for mask creation but provides better results than simple deduplication.

Failure Signatures: If RMFT fails, you would observe: (1) high extraction rates indicating insufficient privacy protection, (2) significant perplexity increases suggesting loss of utility, or (3) inconsistent performance across model architectures suggesting implementation issues.

First Experiments:
1. Implement email detection and first occurrence identification on a small sample of the Enron dataset
2. Apply randomized masking to duplicate emails and verify structural preservation
3. Run baseline fine-tuning vs RMFT on a small model to verify initial privacy-utility tradeoffs

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- The evaluation is limited to email addresses in the Enron dataset, raising questions about generalizability to other PII types and domains
- The computational overhead of mask creation is acknowledged but not thoroughly analyzed for scalability
- The novelty claim is complicated by similar approaches in related work, particularly [Zyskind et al. 2015] and [Massey et al. 2014]

## Confidence

- **RMFT achieves 80.81% reduction in Total Extraction Rate**: High confidence - this is directly measured and reproducible with the provided methodology and dataset
- **RMFT outperforms deduplication while maintaining utility**: Medium confidence - the comparison is sound, but the evaluation is limited to one dataset and one PII type
- **MaxTER framework provides superior evaluation**: Medium confidence - the framework is well-defined, but its advantages over existing metrics need broader validation across different privacy scenarios

## Next Checks

1. Test RMFT on additional datasets containing different types of PII (phone numbers, social security numbers, addresses) to verify generalizability beyond email addresses

2. Implement a scalability analysis measuring the computational overhead of mask creation across datasets of varying sizes and complexity

3. Conduct a systematic comparison with differential privacy-based fine-tuning methods to determine whether RMFT's masking approach provides comparable or superior privacy guarantees in real-world deployment scenarios