---
ver: rpa2
title: Comparative Analysis of Large Language Models for Context-Aware Code Completion
  using SAFIM Framework
arxiv_id: '2502.15243'
source_url: https://arxiv.org/abs/2502.15243
tags:
- code
- completion
- tasks
- accuracy
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates five large language models (Gemini 1.5 Flash,
  Gemini 1.5 Pro, GPT-4o, GPT-4o-mini, and GPT-4 Turbo) for code completion tasks
  using the SAFIM dataset, which assesses syntax-sensitive code generation. The models
  were tested across three task categories: API Function Call Completion, Algorithmic
  Block Completion, and Control-Flow Completion.'
---

# Comparative Analysis of Large Language Models for Context-Aware Code Completion using SAFIM Framework

## Quick Facts
- arXiv ID: 2502.15243
- Source URL: https://arxiv.org/abs/2502.15243
- Reference count: 15
- Models tested: Gemini 1.5 Flash, Gemini 1.5 Pro, GPT-4o, GPT-4o-mini, GPT-4 Turbo

## Executive Summary
This study evaluates five large language models for code completion tasks using the SAFIM dataset, which assesses syntax-sensitive code generation through Fill-in-the-Middle (FIM) prompts. The research examines performance across API Function Call Completion, Algorithmic Block Completion, and Control-Flow Completion tasks, measuring both semantic similarity and latency. Results reveal significant trade-offs between accuracy and speed, with GPT-4 Turbo achieving highest accuracy (0.858 similarity) but highest latency (2.063s), while GPT-4o provides best efficiency with lowest latency (0.515s) and competitive accuracy (0.719 similarity).

## Method Summary
The study evaluates five LLMs on the SAFIM dataset using structured prompts with prefix and suffix context separated by a `$PLACEHOLDER$` token. Performance is measured using cosine similarity between generated and ground-truth completions, alongside latency measurements from start to completion. The evaluation uses 100 systematically sampled examples per task category across four programming languages, with results stored in JSONL format containing context, completion, ground truth, latency, and similarity scores.

## Key Results
- GPT-4 Turbo achieved highest accuracy (0.858 similarity) but highest latency (2.063s)
- GPT-4o provided best efficiency with lowest latency (0.515s) and competitive accuracy (0.719 similarity)
- API Function Call completion consistently showed higher accuracy across all models compared to algorithmic and control-flow tasks
- Significant trade-offs exist between accuracy and latency, with latency increasing disproportionately for complex logical synthesis tasks

## Why This Works (Mechanism)

### Mechanism 1: Syntax-Aware Contextual Infilling
Providing models with both prefix and suffix context allows for syntax-sensitive completion that purely left-to-right generation cannot achieve. The model receives a `$PLACEHOLDER$` token surrounded by leading and trailing code, forcing it to predict the missing segment that logically and syntactically bridges the gap.

### Mechanism 2: Task-Complexity Performance Divergence
Model performance is highly conditional on task abstractness; pattern-matching tasks (API calls) yield higher fidelity than logic-construction tasks (Algorithmic blocks). API completion relies on retrieving learned statistical patterns of library usage, while algorithmic completion requires synthesizing novel logic paths.

### Mechanism 3: Latency-Accuracy Scaling Asymmetry
Increasing model parameters improves semantic similarity but incurs disproportionate latency cost, particularly for complex logical synthesis. Larger models have higher computational complexity per token, resulting in slower inference that becomes non-linear for complex tasks.

## Foundational Learning

- **Fill-in-the-Middle (FIM) / Span Corruption**: Needed because standard LLMs predict the next word, but code completion often requires filling between a signature and closing brace. Quick check: If a model is trained only on Left-to-Right next-token prediction, how would it likely perform if given a prefix and a suffix simultaneously? (Answer: It would likely ignore the suffix or treat it as new text to append).

- **Cosine Similarity in Embedding Space**: Needed because exact string matching is too harsh for code (variable naming variations). Quick check: Why is cosine similarity preferred over exact match for evaluating code logic? (Answer: It allows for variable renaming and stylistic differences while recognizing the underlying logic is identical).

- **Latency vs. Utility Trade-off**: Needed because the paper explicitly categorizes models by use case (Real-time vs. Batch). Quick check: Which model should be selected for a "ghost text" feature that appears while the user is typing? (Answer: GPT-4o or GPT-4o-mini, due to sub-second latency requirements).

## Architecture Onboarding

- **Component map**: SAFIM Dataset -> Prompt Constructor -> Inference Engine -> Vector Embedder -> Evaluator
- **Critical path**: The prompt construction is most critical. If the `$PLACEHOLDER$` or "Reply with completion only" constraint is not followed, the model may return conversational filler, breaking the parser and invalidating the cosine similarity metric.
- **Design tradeoffs**:
  - GPT-4 Turbo: Maximize Accuracy / Sacrifice Speed / High Cost
  - GPT-4o: Balance Speed/Accuracy / Moderate Cost
  - GPT-4o-mini / Flash: Maximize Speed / Lower Cost / Sacrifice Semantic Depth
- **Failure signatures**:
  - High Latency on Simple Tasks: Indicates model is "over-thinking" or context window is overloaded
  - Low Cosine Similarity in API Tasks: Suggests model lacks specific library knowledge or is hallucinating
  - Syntactic Hallucination: Model generates code that looks correct but violates suffix logic
- **First 3 experiments**:
  1. Hybrid Router Test: Send API tasks to GPT-4o and Algorithmic tasks to GPT-4 Turbo, measure aggregate accuracy and latency
  2. Prompt Robustness Check: Remove "Reply with completion only" constraint and analyze introduced noise
  3. Context Window Stress Test: Gradually increase prefix/suffix size to determine "latency cliff"

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do varying context window sizes affect model performance on complex algorithmic and control-flow tasks?
- Basis: The authors explicitly propose "Context Window Analysis" to investigate performance variations across task types.
- Why unresolved: Current study evaluates models using fixed configurations without isolating context length as an experimental variable.
- What evidence would resolve it: Ablation studies measuring accuracy and latency trade-offs across controlled context window increments.

### Open Question 2
- Question: Can evaluation metrics capturing functional correctness and efficiency outperform semantic cosine similarity?
- Basis: The Future Work section suggests "Enhanced Evaluation Metrics" to capture code quality aspects like maintainability and efficiency.
- Why unresolved: Cosine similarity measures semantic vector alignment but fails to verify functional validity or syntactic correctness.
- What evidence would resolve it: Evaluation using compilers, unit tests, or runtime benchmarks alongside similarity scores.

### Open Question 3
- Question: Do LLMs maintain performance consistency when applied to software engineering tasks beyond code completion?
- Basis: The authors recommend expanding the benchmark to include "debugging, refactoring, and test case generation."
- Why unresolved: The current SAFIM benchmark is restricted to API, algorithmic, and control-flow completion categories.
- What evidence would resolve it: Extension of the dataset to new task domains with corresponding ground-truth evaluations.

## Limitations

- **Dataset Specificity**: The 100-sample systematic sample per task category may not represent real-world code completion diversity, with sampling methodology lacking stratification details.
- **Embedding Model Dependency**: Cosine similarity depends on an unspecified embedding model, where different embeddings could significantly alter reported similarity scores.
- **API Configuration Assumptions**: Critical inference parameters like temperature and max_tokens are not specified, assuming greedy decoding that may not reflect actual usage.

## Confidence

- **High Confidence**: The relative ranking of models and task complexity hierarchy are robust findings given the systematic evaluation methodology.
- **Medium Confidence**: The latency-accuracy trade-off characterization holds within tested conditions, but specific latency values may vary significantly with API load.
- **Low Confidence**: The generalizability of findings to production environments with continuous typing is uncertain without stress testing at scale.

## Next Checks

1. **Embedding Model Sensitivity Test**: Re-run evaluation using three different embedding models to quantify impact on similarity scores and model rankings.

2. **Production Latency Benchmarking**: Deploy top 3 models in controlled IDE environment with realistic typing patterns to measure end-to-end latency including network overhead.

3. **Task Distribution Generalization**: Expand evaluation to include additional task categories from full SAFIM dataset to verify API>Algorithmic>Control-Flow accuracy hierarchy across diverse scenarios.