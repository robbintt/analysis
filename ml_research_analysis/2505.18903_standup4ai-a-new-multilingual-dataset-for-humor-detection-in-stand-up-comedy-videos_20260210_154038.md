---
ver: rpa2
title: 'StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy
  Videos'
arxiv_id: '2505.18903'
source_url: https://arxiv.org/abs/2505.18903
tags:
- laughter
- humor
- detection
- dataset
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces StandUp4AI, a large multilingual dataset for
  humor detection in stand-up comedy videos across seven languages, totaling over
  330 hours of content. The dataset provides automatic laughter annotations refined
  via ASR error correction and manual validation, with a subset manually annotated
  for model evaluation.
---

# StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos

## Quick Facts
- arXiv ID: 2505.18903
- Source URL: https://arxiv.org/abs/2505.18903
- Reference count: 21
- A multilingual dataset for humor detection in stand-up comedy videos across seven languages with over 330 hours of content

## Executive Summary
This paper introduces StandUp4AI, a large-scale multilingual dataset for humor detection in stand-up comedy videos spanning seven languages and totaling over 330 hours of content. The dataset reframes humor detection as word-level sequence labeling to capture continuous laughter within jokes rather than binary classification at joke boundaries. Automatic laughter annotations are refined through ASR error correction and manual validation, with a subset manually annotated for model evaluation. The paper proposes an ASR-based method for correcting timestamp errors and detecting additional laughter events, validated with a Random Forest classifier achieving 0.85 F1 on laughter detection.

## Method Summary
The authors constructed StandUp4AI by collecting stand-up comedy videos across seven languages, extracting audio, and generating automatic laughter annotations through ASR-based processing. They developed an error correction method to refine these annotations and validate them using a Random Forest classifier. The dataset reframes humor detection as word-level sequence labeling rather than end-only classification, allowing models to capture continuous laughter patterns within jokes. A subset of the data received manual annotation for evaluation purposes. The authors trained sequence labeling models on the enhanced dataset and demonstrated improved performance compared to traditional classification approaches.

## Key Results
- StandUp4AI contains over 330 hours of stand-up comedy content across seven languages
- The ASR-based laughter detection method achieved 0.85 F1 score on laughter detection
- Sequence labeling models trained on the enhanced dataset showed improved performance over classification models
- The dataset provides automatic laughter annotations refined through ASR error correction and manual validation

## Why This Works (Mechanism)
The paper reframes humor detection as word-level sequence labeling rather than binary classification at joke boundaries. This approach captures continuous laughter patterns within jokes, providing richer temporal information than traditional methods. The ASR-based error correction method improves the quality of automatic laughter annotations by addressing timestamp inaccuracies and detecting additional laughter events that simple threshold-based methods might miss.

## Foundational Learning
- **Automatic Speech Recognition (ASR)**: Why needed - To transcribe audio and detect laughter events from stand-up comedy videos. Quick check - Verify ASR accuracy on the dataset's audio samples.
- **Sequence Labeling**: Why needed - To capture continuous laughter patterns within jokes rather than binary classification. Quick check - Compare sequence labeling performance against traditional classification baselines.
- **Laughter Detection**: Why needed - To identify humor-related audio cues in stand-up comedy. Quick check - Evaluate detection performance on manually annotated test sets.
- **Multilingual Processing**: Why needed - To handle humor detection across seven different languages with varying linguistic structures. Quick check - Test model generalization across language pairs.

## Architecture Onboarding

**Component Map**: Video Collection -> Audio Extraction -> ASR Processing -> Laughter Detection -> Error Correction -> Manual Validation -> Sequence Labeling Models

**Critical Path**: ASR Processing -> Laughter Detection -> Error Correction -> Manual Validation

**Design Tradeoffs**: The paper prioritizes dataset scale and multilingual coverage over perfect annotation quality, relying on automatic methods with manual validation rather than fully manual annotation. This tradeoff enables the creation of a large dataset but may introduce noise in the annotations.

**Failure Signatures**: 
- High false positive rates in laughter detection may indicate model sensitivity to non-humorous audience reactions
- Poor cross-lingual performance may suggest language-specific humor patterns not captured by the models
- Sequence labeling errors may occur at joke boundaries where laughter patterns are ambiguous

**First Experiments**:
1. Evaluate baseline laughter detection performance without error correction
2. Compare sequence labeling models against traditional classification approaches
3. Test model performance on languages not included in training data

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on automatic laughter detection without sufficient ground truth validation
- No evaluation on downstream tasks to demonstrate genuine humor understanding versus laughter pattern detection
- Limited analysis of cross-linguistic validity and cultural differences in humor expression

## Confidence
- Dataset construction methodology: Medium confidence - The approach is described but validation details are sparse
- Laughter detection accuracy: Medium confidence - 0.85 F1 is reported but lacks comprehensive error analysis
- Model performance improvements: High confidence - The sequence labeling approach is technically sound
- Cross-lingual generalization: Low confidence - Insufficient analysis of multilingual performance

## Next Checks
1. Conduct human evaluation study where annotators mark joke boundaries and assess whether model predictions align with human humor judgments, not just laughter detection
2. Perform ablation study removing automatic laughter detection and replacing with ground truth laughter annotations to quantify the impact of detection errors on model performance
3. Test model generalization by evaluating on languages not included in training data and analyzing failure cases to understand whether models capture linguistic or cultural humor patterns