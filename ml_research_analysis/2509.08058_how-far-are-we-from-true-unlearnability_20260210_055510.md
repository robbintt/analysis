---
ver: rpa2
title: How Far Are We from True Unlearnability?
arxiv_id: '2509.08058'
source_url: https://arxiv.org/abs/2509.08058
tags:
- parameters
- training
- unlearnable
- unlearnability
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the multi-task unlearnability of existing
  unlearnable example methods. The authors find that UEs generated by current methods
  (EM, OPS, AR) still perform well in tasks like semantic segmentation on the Taskonomy
  dataset, failing to exhibit cross-task unlearnability.
---

# How Far Are We from True Unlearnability?

## Quick Facts
- arXiv ID: 2509.08058
- Source URL: https://arxiv.org/abs/2509.08058
- Reference count: 24
- Current unlearnable methods fail at cross-task protection, with TAP showing highest UD (1.639) on CIFAR-10 while OPS shows lowest (0.157)

## Executive Summary
This paper investigates the fundamental limitations of existing unlearnable example (UE) methods by testing their performance across different machine learning tasks. The authors discover that UEs generated by current methods (EM, OPS, AR) still maintain effectiveness in semantic segmentation tasks on the Taskonomy dataset, demonstrating that true cross-task unlearnability remains unachieved. Through detailed analysis of the training process and loss landscape, they identify that only a small subset of model parameters show significant differences between clean and poisoned models. Based on these insights, the paper introduces Sharpness-Aware Learnability (SAL) to quantify parameter unlearnability and Unlearnable Distance (UD) to measure data unlearnability, providing a systematic framework for evaluating unlearnability methods.

## Method Summary
The authors propose a comprehensive framework for analyzing and measuring unlearnability by first examining the training dynamics and loss landscapes of models trained on both clean and poisoned data. They develop the Sharpness-Aware Learnability (SAL) metric to quantify how unlearnable specific model parameters are, based on the observation that only a few critical parameters show significant differences between clean and poisoned models. Additionally, they introduce the Unlearnable Distance (UD) metric to measure the unlearnability of data samples themselves. Using these metrics, they systematically benchmark existing UE methods (EM, OPS, AR, TAP) across multiple datasets, revealing significant gaps in cross-task protection. The framework provides both theoretical insights into why current methods fail and practical tools for evaluating and improving unlearnability.

## Key Results
- Existing UE methods (EM, OPS, AR) fail to achieve cross-task unlearnability, performing well on semantic segmentation tasks despite being designed to prevent training
- Only a small subset of model parameters show significant differences between clean and poisoned models, explaining why current methods have limited effectiveness
- TAP achieves the highest Unlearnable Distance (1.639) on CIFAR-10 among existing methods, while OPS performs worst (0.157), demonstrating substantial variation in unlearnability effectiveness
- The proposed SAL and UD metrics provide a systematic framework for quantifying and comparing unlearnability across different methods and tasks

## Why This Works (Mechanism)
The paper's findings are based on the observation that unlearnable example methods fail because they primarily target task-specific features rather than creating truly unlearnable representations. The analysis reveals that during training, only a few critical parameters show significant differences between clean and poisoned models, indicating that the poison signals are not sufficiently disruptive across the entire parameter space. This partial disruption allows models to adapt and still learn useful representations for different tasks. The Sharpness-Aware Learnability metric captures this phenomenon by measuring parameter sensitivity, while Unlearnable Distance quantifies how far poisoned data is from clean data in the feature space, explaining why some methods like TAP perform better than others.

## Foundational Learning
- Unlearnable Examples (UEs): Adversarial perturbations designed to prevent successful model training - needed to understand the baseline methods being evaluated; quick check: verify that UE generation requires knowledge of the target model architecture
- Cross-task Unlearnability: The property that poisoned data remains unlearnable across different downstream tasks - critical for understanding the paper's main contribution; quick check: test poisoned data on tasks not seen during UE generation
- Loss Landscape Analysis: Examining how model parameters change during training on clean vs poisoned data - essential for understanding parameter-level differences; quick check: visualize loss surfaces using Hessian-based methods
- Parameter Importance: Identifying which model parameters are most critical for task performance - key to understanding why current methods fail; quick check: use sensitivity analysis or ablation studies
- Sharpness-Aware Training: Optimization techniques that consider loss landscape geometry - relevant background for SAL metric development; quick check: compare standard vs sharpness-aware optimization convergence
- Distance Metrics in Feature Space: Quantitative measures of similarity between clean and poisoned representations - fundamental to UD metric; quick check: verify metric invariance to feature scaling

## Architecture Onboarding

**Component Map**
UE Generation -> Model Training -> Parameter Analysis -> SAL/UD Metric Computation -> Benchmarking

**Critical Path**
1. Generate poisoned data using existing UE methods (EM, OPS, AR, TAP)
2. Train models on both clean and poisoned data while monitoring parameter changes
3. Analyze loss landscape to identify critical parameters showing significant differences
4. Compute SAL for parameter unlearnability and UD for data unlearnability
5. Benchmark methods using UD across multiple tasks and datasets

**Design Tradeoffs**
The paper balances between developing theoretically sound metrics (SAL, UD) and practical benchmarking approaches. The tradeoff involves computational complexity of loss landscape analysis versus the need for accurate unlearnability measurement. The choice to focus on parameter-level analysis rather than just output-level metrics provides deeper insights but requires more sophisticated training monitoring.

**Failure Signatures**
Current UE methods fail when: (1) poisoned data retains task-relevant features, (2) models can adapt to poison signals through parameter adjustments, (3) loss landscape shows limited sensitivity to poison in most parameters, and (4) cross-task transferability undermines unlearnability. These failures manifest as high performance on downstream tasks despite UE presence.

**3 First Experiments**
1. Compare training dynamics of models on clean vs poisoned CIFAR-10 data using standard vs sharpness-aware optimization
2. Test existing UE methods (EM, OPS, AR, TAP) on semantic segmentation taskonomy dataset to verify cross-task failure
3. Compute SAL and UD metrics for each UE method to establish baseline performance and identify most effective approaches

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis relies heavily on specific model architectures and may not generalize to all neural network types
- The proposed metrics require detailed training monitoring which may be computationally expensive for large-scale applications
- Cross-task evaluation is limited to semantic segmentation tasks and may not represent all possible task transfer scenarios
- The framework assumes linear relationships in loss landscape that may not hold for all architectures or training regimes

## Confidence

| Major Claim | Confidence |
|-------------|------------|
| Cross-task unlearnability findings | Medium |
| Parameter importance analysis | Medium |
| UD benchmarking results | Medium |

## Next Checks
1. Test the proposed metrics and findings on additional datasets beyond CIFAR-10 and Taskonomy, particularly with larger-scale image datasets and different domain types (medical, satellite, etc.)
2. Evaluate the robustness of the unlearnability metrics under various data augmentation strategies and preprocessing pipelines to ensure practical applicability
3. Conduct ablation studies on different model architectures (CNNs, transformers, hybrid models) to verify the universality of the parameter importance findings and the proposed SAL metric