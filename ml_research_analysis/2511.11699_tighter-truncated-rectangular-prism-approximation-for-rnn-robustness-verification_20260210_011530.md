---
ver: rpa2
title: Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification
arxiv_id: '2511.11699'
source_url: https://arxiv.org/abs/2511.11699
tags:
- time
- verification
- area
- neural
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DeepPrism, a novel method for verifying the
  robustness of Recurrent Neural Networks (RNNs) by tightly enclosing the three-dimensional
  nonlinear surfaces generated by the Hadamard product. The key idea is to use a truncated
  rectangular prism approximation formed by two linear relaxation planes, minimizing
  both its volume and surface area to reduce overestimation.
---

# Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification

## Quick Facts
- **arXiv ID:** 2511.11699
- **Source URL:** https://arxiv.org/abs/2511.11699
- **Reference count:** 40
- **Primary result:** DeepPrism achieves higher certified accuracy than state-of-the-art baselines for RNN robustness verification while maintaining reasonable computational efficiency.

## Executive Summary
This paper introduces DeepPrism, a novel method for verifying the robustness of Recurrent Neural Networks (RNNs) by tightly enclosing the three-dimensional nonlinear surfaces generated by the Hadamard product. The key idea is to use a truncated rectangular prism approximation formed by two linear relaxation planes, minimizing both its volume and surface area to reduce overestimation. A refinement-driven method with multi-plane approximations is proposed to further improve verification accuracy. Experiments on four datasets across three tasks (image classification, speech recognition, sentiment analysis) show that DeepPrism achieves higher certified accuracy than state-of-the-art baselines while maintaining reasonable computational efficiency. The approach provides both theoretical insights and practical improvements for RNN robustness verification.

## Method Summary
DeepPrism addresses the challenge of verifying RNN robustness by improving the approximation of the nonlinear Hadamard product operation (σ(x) ⊙ tanh(y)) that occurs in LSTM cells. The method constructs a truncated rectangular prism that tightly bounds the 3D nonlinear surface formed by this operation. The key innovation is a hybrid volume-area objective function that minimizes both the prism's volume and surface area, with an empirically determined weighting coefficient α = 0.674. For enhanced accuracy, a refinement process employs multi-plane approximations using gradient descent to find optimal linear weights for sub-regions. The approach is evaluated on four datasets (MNIST, Google Speech Commands, Free Spoken Digit Dataset, Rotten Tomatoes) across three tasks, comparing certified accuracy and running time against state-of-the-art baselines.

## Key Results
- DeepPrism achieves higher certified accuracy than state-of-the-art baselines across all tested datasets and perturbation levels
- The method maintains reasonable computational efficiency, with multi-plane refinement remaining within 120-second timeouts per sample
- The hybrid volume-area approximation strategy provides tighter bounds than volume-only or surface-area-only approaches

## Why This Works (Mechanism)
The effectiveness of DeepPrism stems from its geometric approach to bounding nonlinear operations. By constructing a truncated rectangular prism with carefully optimized dimensions, the method achieves tighter enclosures of the nonlinear surface compared to traditional over-approximations. The hybrid objective function balances global tightness (minimizing volume) with local fit (minimizing surface area), resulting in more precise bounds that reduce the accumulation of errors during the backward substitution process in DeepPoly verification.

## Foundational Learning
- **Abstract Interpretation for Neural Networks**: A framework for verifying properties of neural networks by computing sound over-approximations of their behaviors. Needed because direct verification of neural networks is computationally intractable for most practical cases.
- **DeepPoly Backsubstitution**: The core algorithm for propagating input domain constraints through a neural network layer by layer. Quick check: Verify the backsubstitution equations maintain soundness for each layer type.
- **LSTM Cell Operations**: Understanding the Hadamard product operations in LSTM equations (input gate, forget gate, output gate) is crucial as these are the primary sources of nonlinearity that need to be bounded.
- **Linear Programming Solvers**: Tools like Gurobi are used to solve the optimization problems for finding optimal bounding planes. Quick check: Verify the LP formulation correctly encodes all constraints.
- **Certified Robustness Verification**: The goal of proving that a model's prediction remains unchanged within a specified perturbation radius. Quick check: Confirm the perturbation model matches the experimental setup.

## Architecture Onboarding

**Component Map:** Input Perturbation → LSTM Model → DeepPoly Verification → Truncated Prism Approximation → LP Solver → Certified Accuracy

**Critical Path:** The verification process follows: (1) Define input domain from perturbation, (2) Apply DeepPoly backsubstitution through LSTM layers, (3) For each Hadamard product, construct truncated prism using hybrid objective, (4) Solve LP to find optimal planes, (5) Propagate bounds backward, (6) Check if output region maintains classification.

**Design Tradeoffs:** The method trades increased computational complexity (multiple LP solves for multi-plane refinement) for improved certified accuracy. The choice of α = 0.674 represents a balance between volume minimization and surface area minimization, which could be tuned differently for specific applications.

**Failure Signatures:** Soundness violations occur when bounding planes intersect the nonlinear surface. Performance degradation happens with large perturbations where the nonlinear surface becomes too complex to bound tightly. Computational timeouts occur when multi-plane refinement requires excessive LP solves.

**First Experiments:**
1. Implement and verify the single-plane approximation on a simple 2D function to confirm the geometric approach works as expected
2. Test the LP solver interface with a basic linear constraint system to ensure correct integration
3. Run verification on a small LSTM model with synthetic data to validate the complete pipeline before scaling to full experiments

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How can the computational overhead of the hybrid volume-area objective function and the multi-plane refinement process be reduced to achieve linear time complexity comparable to simpler baseline methods?
- **Basis in paper:** The conclusion states, "Future work will focus on reducing computational overhead and enhancing the time efficiency of the approach."
- **Why unresolved:** While DeepPrism improves accuracy, the experimental results show that the running time increases significantly compared to the Prover baseline, particularly when employing multi-plane approximation strategies.
- **What evidence would resolve it:** A modification of the optimization algorithm that maintains high certified accuracy but lowers the runtime to be statistically indistinguishable from distance-based methods on large-scale RNNs.

### Open Question 2
- **Question:** Can the truncated rectangular prism approximation be effectively generalized to other RNN architectures, such as Gated Recurrent Units (GRUs) or Vanilla RNNs with ReLU activations, which have different non-linear surface geometries?
- **Basis in paper:** The methodology section focuses exclusively on the LSTM equation σ(x) ⊙ tanh(y) (Eq. 6). The paper does not demonstrate if the geometric properties of the volume-area minimization hold for the different Hadamard products found in other RNN variants.
- **Why unresolved:** The theoretical proofs for the volume and surface area correlations rely on the specific 3D surface characteristics of the sigmoid-tanh product, and it is unclear if the "rounded" prism heuristic applies to other activation functions.
- **What evidence would resolve it:** Successful application of DeepPrism to GRU verification tasks, specifically handling the update gate equation z_t ⊙ h_{t-1} + (1-z_t) ⊙ h̃_t, with comparable certification gains.

### Open Question 3
- **Question:** Is the empirically determined weighting coefficient α=0.674 universally optimal, or does the ideal balance between volume and surface area minimization vary depending on the specific dataset or perturbation magnitude?
- **Basis in paper:** The paper determines α via an "exhaustive search" on experimental datasets and sets it as a constant.
- **Why unresolved:** Different non-linear regions may benefit from prioritizing either volume (global tightness) or surface area (local fit) differently. A static weight might be sub-optimal for outliers or different input domains not covered in the search.
- **What evidence would resolve it:** A sensitivity analysis showing that an adaptive α (dynamically computed per neuron or layer) provides statistically significant improvements in certified accuracy over the fixed constant.

## Limitations
- **Soundness uncertainty:** The multi-plane refinement process relies on gradient descent sampling, which may not guarantee finding the globally optimal bounding planes, potentially introducing soundness gaps
- **Performance degradation:** The computational overhead increases significantly with multi-plane refinement, though it remains within timeout limits
- **Architecture specificity:** The method is primarily demonstrated on LSTM networks and may not directly transfer to other RNN variants with different nonlinear structures

## Confidence

**High confidence:** The core single-plane approximation using the hybrid volume-area objective function (Eq. 12) is well-specified and reproducible.

**Medium confidence:** The overall methodology and experimental results are reproducible, but the multi-plane refinement implementation details require careful attention to soundness.

**Medium confidence:** The claimed performance improvements over state-of-the-art methods are supported by experimental results, though the relative advantage may vary with perturbation magnitude.

## Next Checks

1. **Soundness verification:** Implement a comprehensive test that evaluates the lower and upper bounding planes across a dense grid of points in the input domain [l_x, u_x] × [l_y, u_y] to verify the strict containment of σ(x) ⊙ tanh(y), not just at sampled points.

2. **Runtime profiling:** Measure and compare the per-sample verification time between single-plane and multi-plane refinement to quantify the computational overhead and verify it remains within the 120-second timeout.

3. **Perturbation sensitivity analysis:** Replicate the experiments across a wider range of perturbation magnitudes (ε ∈ {0.004, 0.012, 0.03}) to characterize the method's performance degradation and verify the claims hold consistently.