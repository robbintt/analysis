---
ver: rpa2
title: 'Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using
  House M.D'
arxiv_id: '2511.10912'
source_url: https://arxiv.org/abs/2511.10912
tags:
- medical
- diagnostic
- rare
- reasoning
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models (LLMs) on rare disease
  diagnosis using a novel dataset of 176 symptom-diagnosis pairs from House M.D. episodes.
---

# Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D

## Quick Facts
- arXiv ID: 2511.10912
- Source URL: https://arxiv.org/abs/2511.10912
- Authors: Arsh Gupta; Ajay Narayanan Sridhar; Bonam Mingole; Amulya Yadav
- Reference count: 4
- Primary result: Four LLMs achieved 16.48% to 38.64% accuracy on rare disease diagnosis from House M.D. narratives

## Executive Summary
This study evaluates large language models on rare disease diagnosis using a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D. episodes. The research addresses the challenge of assessing LLM performance on narrative medical reasoning, where symptoms and diagnoses are presented in natural language rather than structured formats. Four state-of-the-art models (GPT-4o mini, GPT-5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro) were tested using fuzzy string matching for evaluation, revealing significant variation in performance with newer model generations demonstrating 2.3× improvement.

The findings establish baseline metrics for AI-assisted diagnosis research while highlighting current limitations in narrative medical reasoning. The study's approach of using fictional but educationally validated medical narratives provides an accessible benchmark for evaluating complex diagnostic reasoning that structured datasets typically lose. Results show that while LLMs struggle with rare disease diagnosis, there is meaningful progress as model architectures advance, particularly for newer generations with enhanced reasoning capabilities.

## Method Summary
The study constructed a dataset of 176 symptom-diagnosis pairs from House M.D. episodes, using BeautifulSoup to scrape narrative content from house.fandom.com. Each episode was converted into a standardized medical case prompt containing demographics, temporally sequenced symptoms, medical history, and workup information. Four LLMs were evaluated using temperature=0.0, max_tokens=1500, and single-pass inference without system prompts. Diagnosis predictions were extracted and evaluated using fuzzy string matching (SequenceMatcher, threshold=0.8), first attempting exact substring matches before token-wise fuzzy comparison to handle medical terminology variations.

## Key Results
- Model performance ranged from 16.48% (GPT-4o mini) to 38.64% (Gemini 2.5 Pro) accuracy
- Newer model generations showed 2.3× improvement in diagnostic accuracy
- Season 5 had lowest accuracy at 20.83%, suggesting case complexity variation
- Fuzzy matching successfully handled terminology variations like "Sarcoidosis≈Sarcoid (0.89)"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Narrative case formats test reasoning depth beyond factual recall by preserving temporal and contextual information
- Mechanism: Structured symptom-diagnosis pairs extracted from narrative episodes require models to identify medical clues within temporal progression, demographics, and clinical decision-making contexts that structured datasets typically lose
- Core assumption: Fictional but educationally validated medical narratives capture sufficient clinical reasoning patterns to serve as meaningful benchmarks
- Evidence anchors:
  - [abstract] "narrative-based diagnostic reasoning tasks" and "educationally validated benchmark"
  - [section] "The narrative format preserves temporal symptom progression, demographics, and clinical decision-making that structured datasets often lose"
  - [corpus] CaseReportBench paper similarly uses clinical case reports for dense information extraction, supporting narrative-based evaluation approaches
- Break condition: If narrative complexity introduces noise that masks actual diagnostic capability rather than testing it

### Mechanism 2
- Claim: Newer model generations demonstrate improved medical reasoning proportional to architectural advances
- Mechanism: Enhanced reasoning capabilities in newer architectures (GPT-5 mini, Gemini 2.5 Pro) translate to 2.3× improvement in diagnostic accuracy through better integration of symptom patterns across longer context windows
- Core assumption: Performance gains are attributable to architectural improvements rather than training data contamination from publicly available House M.D. content
- Evidence anchors:
  - [abstract] "newer model generations demonstrating a 2.3× improvement"
  - [section] "GPT-5-mini and Gemini 2.5 Pro's superior performance indicates that newer model generations with enhanced reasoning capabilities show meaningful improvements"
  - [corpus] Corpus evidence weak—neighboring papers don't directly compare cross-generation improvements on identical rare disease tasks
- Break condition: If improvement plateaus or degrades with further scaling, suggesting architectural gains have diminishing returns for rare disease reasoning

### Mechanism 3
- Claim: Fuzzy string matching at 0.8 threshold accommodates valid medical terminology variations while maintaining evaluation rigor
- Mechanism: Two-stage matching (exact substring first, then token-wise fuzzy comparison) handles synonym variations like "Sarcoidosis≈Sarcoid (0.89)" while filtering incorrect diagnoses
- Core assumption: Semantically equivalent diagnoses with substantially different terminology represent edge cases rather than systematic evaluation gaps
- Evidence anchors:
  - [abstract] Not explicitly mentioned
  - [section] "Our algorithm employs Python's SequenceMatcher with a 0.8 similarity threshold, performing exact substring matching first, then token-wise fuzzy comparison"
  - [corpus] No corpus papers directly address fuzzy matching methodology for medical evaluation
- Break condition: If threshold either accepts incorrect diagnoses (false positives) or rejects correct ones expressed in valid alternative terminology (false negatives)

## Foundational Learning

- Concept: Fuzzy String Matching
  - Why needed here: Medical conditions have multiple valid names; exact matching would systematically underestimate model performance
  - Quick check question: Why should "Sarcoidosis" and "Sarcoid" match but "Lupus" and "Systemic Lupus Erythematosus" require substring matching?

- Concept: Narrative vs Structured Medical Data
  - Why needed here: Understanding why TV content works better than EHR extracts for this evaluation
  - Quick check question: What clinical information is preserved in narrative presentations that structured symptom lists typically lose?

- Concept: Rare Disease Diagnostic Latency
  - Why needed here: Context for why 38.64% accuracy represents meaningful progress on an exceptionally difficult task
  - Quick check question: Why do rare diseases present unique challenges for both human physicians and LLMs compared to common conditions?

## Architecture Onboarding

- Component map: Data extraction -> structured prompts -> 4 LLMs (temp=0.0, max_tokens=1500) -> fuzzy matching (0.8 threshold) -> binary accuracy
- Critical path:
  1. Scrape House M.D. wiki -> extract 176 episode narratives
  2. Construct standardized medical case prompts per episode
  3. Single-pass inference per model -> extract predicted diagnosis
  4. Fuzzy match prediction against ground truth -> binary correct/incorrect
  5. Aggregate accuracy across models and seasons
- Design tradeoffs:
  - Binary accuracy: Simple, reproducible, but loses partial credit for clinically reasonable misdiagnoses
  - No system prompts: Reduces bias but forfeits domain priming that might improve performance
  - Fictional cases: Ethically accessible, educationally validated, but includes dramatic exaggeration
  - Single-pass inference: Deterministic but may underestimate iterative reasoning capability
- Failure signatures:
  - Confident incorrect explanations (hallucination of supporting evidence)
  - Multi-system autoimmune disorders (lupus, sarcoidosis) consistently missed
  - Toxicological cases requiring exposure history integration
  - Season 5 shows lowest accuracy (20.83%), suggesting case complexity variation
- First 3 experiments:
  1. Threshold sensitivity analysis: Test 0.7, 0.8, 0.9 thresholds on a held-out subset to validate fuzzy matching doesn't introduce systematic bias
  2. Error taxonomy: Manually classify 50 incorrect predictions into terminology-mismatch vs reasoning-failure vs knowledge-gap categories
  3. System prompt ablation: Add minimal medical context ("You are a diagnostic assistant") to test whether domain priming improves accuracy without introducing bias toward specific conditions

## Open Questions the Paper Calls Out
None

## Limitations
- **Limited Clinical Realism**: Fictional cases may not reflect real-world diagnostic patterns, potentially creating an evaluation setting that is more challenging or less representative than actual clinical practice
- **Reproducibility Constraints**: Exact prompt template and diagnosis extraction methodology are not fully specified, making independent verification difficult
- **Evaluation Methodology Concerns**: Binary accuracy metric may oversimplify performance by not capturing clinically reasonable but technically incorrect diagnoses

## Confidence
**High Confidence**:
- Dataset construction from House M.D. episodes with 176 symptom-diagnosis pairs
- Basic evaluation methodology using temperature=0.0, max_tokens=1500, and single-pass inference
- Overall accuracy range (16.48% to 38.64%) and generation improvement factor (2.3×)

**Medium Confidence**:
- Fuzzy matching evaluation with 0.8 threshold effectively handles medical terminology variations
- Narrative format preserves clinical reasoning elements lost in structured datasets
- Season 5 having lowest accuracy (20.83%) due to case complexity variation

**Low Confidence**:
- Extent to which fictional cases generalize to real clinical scenarios
- Precise impact of architectural improvements on rare disease reasoning
- Optimal prompt structure and response parsing methodology for medical diagnosis

## Next Checks
1. **Threshold Sensitivity Analysis**: Test the fuzzy matching evaluation with thresholds of 0.7, 0.8, and 0.9 on a held-out subset of 20-30 cases to determine whether the chosen 0.8 threshold introduces systematic bias in either direction (false positives or false negatives).

2. **Error Taxonomy Classification**: Manually review and classify the 50 most common incorrect predictions into three categories: terminology-mismatch (valid alternative names), reasoning-failure (incorrect clinical logic), and knowledge-gap (condition never mentioned in training data). This will clarify whether the current evaluation methodology accurately captures model limitations.

3. **System Prompt Impact Test**: Run a parallel evaluation with minimal medical domain priming ("You are a diagnostic assistant") to determine if this improves accuracy without introducing bias toward specific conditions, testing whether the no-system-prompt constraint unnecessarily limits model performance.