---
ver: rpa2
title: 'PCA of probability measures: Sparse and Dense sampling regimes'
arxiv_id: '2602.02190'
source_url: https://arxiv.org/abs/2602.02190
tags:
- measure
- embedding
- then
- measures
- covariance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies PCA of probability measures in a double asymptotic
  regime, where both the number of measures and the number of samples per measure
  grow. The authors establish convergence rates of the form n^{-1/2} + m^{-\alpha}
  for the empirical covariance operator and PCA excess risk, where \alpha depends
  on the chosen embedding (KME, LOT, or SW).
---

# PCA of probability measures: Sparse and Dense sampling regimes

## Quick Facts
- arXiv ID: 2602.02190
- Source URL: https://arxiv.org/abs/2602.02190
- Reference count: 40
- Key outcome: Establishes convergence rates n^{-1/2} + m^{-\alpha} for empirical covariance operators in PCA of probability measures, proving n^{-1/2} is minimax optimal in dense regime

## Executive Summary
This paper studies Principal Component Analysis (PCA) of probability measures when both the number of measures (n) and samples per measure (m) grow simultaneously. The authors analyze a double asymptotic regime, establishing theoretical convergence rates for empirical covariance operators and PCA excess risk. They prove that the n^{-1/2} term from the measure sampling dominates in the dense regime and is minimax optimal, while the m^{-\alpha} term from within-measure sampling depends on the chosen embedding method. The work provides theoretical justification for sparse sampling strategies that maintain accuracy while reducing computational costs.

## Method Summary
The authors analyze PCA in a double asymptotic regime where both n (number of probability measures) and m (samples per measure) tend to infinity. They study three embedding methods: Kernel Mean Embeddings (KME), Linear Optimal Transport (LOT), and Sliced-Wasserstein (SW). The analysis establishes convergence rates of the form n^{-1/2} + m^{-\alpha} for the empirical covariance operator, where α depends on the embedding method. Theoretical results include uniform convergence bounds for empirical covariance operators and excess risk bounds for PCA. The authors prove minimax optimality of the n^{-1/2} rate in the dense regime and provide practical guidance on when sparse sampling is beneficial.

## Key Results
- Establishes convergence rates n^{-1/2} + m^{-\alpha} for empirical covariance operators and PCA excess risk
- Proves the n^{-1/2} term is minimax optimal in the dense sampling regime
- Shows α = 1 for KME, α = 1/2 for LOT, and α = 1/4 for SW embeddings
- Demonstrates that sparse sampling can preserve PCA accuracy while reducing computational cost

## Why This Works (Mechanism)
The theoretical framework leverages the connection between PCA of probability measures and the estimation of covariance operators in reproducing kernel Hilbert spaces (RKHS) or optimal transport spaces. The n^{-1/2} term arises from the concentration of measure when estimating the covariance operator from n probability measures, while the m^{-\alpha} term comes from the finite-sample approximation within each measure. The mechanism exploits the smoothness and regularity properties of the chosen embedding to establish convergence rates. The minimax optimality proof uses a lower bound argument based on estimating a mean vector in high-dimensional space, demonstrating that no estimator can achieve better than n^{-1/2} rate in the dense regime.

## Foundational Learning
- **RKHS theory and kernel methods**: Needed to understand KME embeddings and their properties; Quick check: verify that the kernel is bounded and the embedding is injective
- **Optimal transport and Wasserstein distances**: Required for LOT and SW embeddings; Quick check: confirm cost function satisfies standard assumptions (e.g., C² with bounded derivatives)
- **Concentration inequalities for Hilbert space-valued random variables**: Essential for proving uniform convergence bounds; Quick check: verify application of Talagrand's inequality to bounded operators
- **Minimax theory and lower bound arguments**: Needed to establish optimality of convergence rates; Quick check: confirm tightness of the information-theoretic lower bound
- **Empirical process theory**: Underlies the analysis of uniform convergence over function classes; Quick check: verify entropy integral conditions for covering numbers

## Architecture Onboarding
**Component Map**: Probability measures -> Embedding (KME/LOT/SW) -> Covariance Operator Estimation -> PCA (Eigen-decomposition) -> Dimensionality Reduction

**Critical Path**: The theoretical guarantees depend on the interplay between measure sampling (n) and within-measure sampling (m). The critical path is: (1) sample n probability measures, (2) embed each measure into a Hilbert space, (3) estimate the covariance operator from these embeddings, (4) compute PCA through eigen-decomposition. The theoretical rates emerge from analyzing the bias-variance trade-off along this path.

**Design Tradeoffs**: KME offers fastest m^{-1} convergence but requires positive definite kernel selection and may suffer in high dimensions. LOT provides m^{-1/2} rate with better geometric properties but needs cost function specification and optimization. SW achieves m^{-1/4} with efficient computation via 1D projections but may lose information through slicing. The choice depends on data structure, dimensionality, and computational constraints.

**Failure Signatures**: If n is too small relative to m, the n^{-1/2} term dominates and sparse sampling offers no benefit. If the kernel or cost function is poorly chosen for the data distribution, convergence rates degrade significantly. High-dimensional settings may require regularization to ensure well-posed covariance estimation. Heavy-tailed distributions may violate boundedness assumptions underlying the theoretical analysis.

**First Experiments**:
1. Test synthetic Gaussian mixtures with varying n and m to verify theoretical rates n^{-1/2} + m^{-\alpha}
2. Compare computational time and PCA accuracy across KME, LOT, and SW embeddings on identical datasets
3. Evaluate sparse sampling performance on real-world datasets (e.g., MNIST, gene expression) to validate practical benefits

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Theoretical analysis assumes bounded kernels and finite-dimensional approximations, which may not hold for all probability measures
- Results assume fixed eigenvector dimension d and bounded eigenvalues, potentially missing settings with rapidly decaying spectrum
- Optimality of m^{-\alpha} term remains open for certain embeddings beyond the established rates
- LOT embedding requires additional assumptions about cost function smoothness and its relation to data distribution
- Numerical validation focuses on synthetic Gaussian mixtures and limited real datasets

## Confidence
**Major claim confidence:**
- Theoretical convergence rates and minimax optimality: **High**
- Embedding-specific results for KME/LOT/SW: **Medium** (due to varying assumptions)
- Computational benefits of sparse sampling: **High** (based on experiments)

## Next Checks
1. Test proposed sampling regimes on high-dimensional real-world datasets (e.g., genomics, image features) to verify scaling behavior
2. Validate theoretical rates for non-Gaussian distributions, particularly heavy-tailed distributions where kernel assumptions may fail
3. Compare computational trade-offs of different embeddings (KME vs LOT vs SW) on identical tasks to assess practical differences in m^{-\alpha} rates