---
ver: rpa2
title: 'LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative
  Representations'
arxiv_id: '2505.10354'
source_url: https://arxiv.org/abs/2505.10354
tags:
- text
- embeddings
- dense
- ldir
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LDIR, a method for generating low-dimensional
  dense and interpretable text embeddings. The approach uses farthest point sampling
  to select anchor texts from a corpus and computes relatedness scores between each
  text and these anchors, resulting in embeddings where each dimension reflects semantic
  relatedness to a specific anchor.
---

# LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations

## Quick Facts
- arXiv ID: 2505.10354
- Source URL: https://arxiv.org/abs/2505.10354
- Authors: Yile Wang; Zhanyu Shen; Hui Huang
- Reference count: 14
- Primary result: LDIR achieves competitive performance on semantic tasks with 200-500 dimensions while maintaining interpretability

## Executive Summary
This paper introduces LDIR, a method for generating low-dimensional dense and interpretable text embeddings. The approach uses farthest point sampling to select anchor texts from a corpus and computes relatedness scores between each text and these anchors, resulting in embeddings where each dimension reflects semantic relatedness to a specific anchor. Experiments across semantic textual similarity, retrieval, and clustering tasks show that LDIR achieves competitive performance compared to black-box embeddings while significantly reducing dimensionality and improving interpretability.

## Method Summary
LDIR creates interpretable text embeddings by selecting a diverse set of anchor texts from a corpus using farthest point sampling (FPS) in a pre-trained embedding space. Each text is then represented as a vector of semantic relatedness scores (typically cosine similarity) to these anchors. This produces floating-point embeddings with only 200-500 dimensions, where each dimension corresponds to a specific anchor text and its semantic relationship to the input text.

## Key Results
- LDIR with 500 dimensions outperforms interpretable baselines (QAEmb-MBQA: 77.60, CQG-MBQA: 76.84) on semantic textual similarity tasks
- At 200 dimensions, LDIR achieves 76.31 Spearman correlation on STS benchmarks
- LDIR performs comparably to black-box embeddings like SimCSE on retrieval and clustering tasks
- Farthest point sampling consistently outperforms uniform sampling and K-means for anchor selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A compact set of diverse anchor texts can serve as a semantic basis for representing arbitrary texts, creating a meaningful low-dimensional space.
- Mechanism: Farthest point sampling (FPS) selects anchor texts that are maximally distant from each other in a pre-trained embedding space (e.g., AngIE). A new text is represented as a vector of its semantic relatedness scores (e.g., cosine similarity) to each of these anchors, projecting it into a coordinate system defined by the anchors.
- Core assumption: The pre-trained encoder's embedding space is semantically coherent and FPS successfully identifies a set of anchors that spans the primary semantic variations of the target corpus.
- Break condition: The anchor texts are not representative of the corpus (e.g., all from a single narrow domain), or the pre-trained encoder's semantics are poor, leading to noisy or meaningless relatedness scores.

### Mechanism 2
- Claim: Using continuous floating-point relatedness scores instead of binary "0/1" answers increases information density per dimension, reducing the total dimensions required.
- Mechanism: Previous interpretable embeddings (QAEmb) used binary responses to thousands of questions (10,000 dims). LDIR uses a continuous score (e.g., 0.83) to a smaller set of anchors (200-500 dims). Each dimension captures a degree of semantic alignment, whereas a binary dimension only captures the presence or absence of a single feature.
- Core assumption: The continuous score captures meaningful gradations of semantic similarity that are useful for downstream tasks.
- Break condition: The continuous score is not discriminative (e.g., always close to 0.5) or is highly noisy, providing no more information than a binary value.

### Mechanism 3
- Claim: Interpretability arises from grounding each dimension in a human-readable anchor text, allowing users to trace vector values back to specific semantic concepts.
- Mechanism: The value of the `i-th` dimension for text `t` is explicitly defined as `similarity(t, anchor_i)`. To interpret the value, a user reads `anchor_i`. This provides a direct, traceable link compared to arbitrary dimensions in standard dense embeddings.
- Core assumption: Users can effectively interpret a dimension by reading the corresponding anchor text, and the anchor text is a clear semantic unit.
- Break condition: Anchor texts are nonsensical, overly complex, or not semantically cohesive, making it difficult for a human to understand what the dimension represents.

## Foundational Learning

- **Text Embeddings (e.g., SimCSE, BERT)**
  - Why needed here: LDIR builds upon pre-trained encoders. You must understand that these models map text to dense vectors to grasp how LDIR repurposes this mapping.
  - Quick check question: What does a cosine similarity of 0.9 between two text embeddings suggest about their meaning?

- **Farthest Point Sampling (FPS)**
  - Why needed here: This is the core algorithm for selecting anchor texts. Understanding its goal (maximizing diversity/coverage) is key to understanding why the anchors work.
  - Quick check question: If you have a cluster of points, how would FPS select the first two points?

- **Semantic Textual Similarity (STS)**
  - Why needed here: STS is a primary evaluation benchmark. You need to know it measures how well an embedding model captures semantic closeness.
  - Quick check question: On an STS task, what performance metric is commonly used, and what does a higher score mean?

## Architecture Onboarding

- Component map: Corpus -> Pre-trained Encoder -> Anchor Selection Module (FPS) -> LDIR Embedder
- Critical path: The pipeline is sequential. First, the corpus is encoded. Second, FPS selects anchors. Third, the LDIR embedding for any new text is computed on-the-fly by comparing it to this fixed set of anchors.
- Design tradeoffs:
  - **Interpretability vs. Performance**: LDIR is more interpretable than black-box models but slightly less performant than the best ones (Table 2).
  - **Anchor Count (n)**: More anchors improve performance but increase cognitive load and cost. The sweet spot is identified as 200-500 (Section 4.2).
  - **Anchor Selection Method**: FPS is preferred over uniform sampling or K-Means, especially at lower dimensions, for more effective semantic coverage (Section 5.1).
- Failure signatures:
  - **Collapse of Diversity**: If FPS is not used or the corpus is too small, anchors will be too similar, leading to uninformative embeddings where all dimensions have similar values.
  - **Encoder Bias**: The base encoder's biases will be inherited. The relatedness score will reflect the encoder's notion of similarity, which may not align with human judgment in all cases.
  - **Task Mismatch**: LDIR embeddings optimized for general semantic similarity may underperform on some tasks (e.g., clustering) compared to the base model (Section 4.2).
- First 3 experiments:
  1. Reproduce Main STS Results: Run LDIR with a pre-trained encoder (e.g., AngIE) on the provided STS benchmarks to verify reported Spearman correlation scores (Table 3).
  2. Ablation on Anchor Sampling: Compare FPS vs. uniform sampling and K-Means on a downstream task (e.g., retrieval) to empirically validate findings in Section 5.1.
  3. Interpretability Case Study: Manually inspect sampled anchor texts and corresponding LDIR embeddings for new texts to build intuition about how specific dimensions relate to semantic meaning (similar to Table 9).

## Open Questions the Paper Calls Out
None

## Limitations
- The true practical interpretability of LDIR embeddings is not empirically validated through user studies
- The method inherits biases and semantic understanding from the pre-trained encoder, which may not align with human judgment
- Limited discussion of performance on highly specialized or domain-specific texts where selected anchors might not be representative

## Confidence
- **High confidence**: LDIR's performance on semantic textual similarity tasks (STS) compared to other interpretable baselines. The experimental setup is clear, and the results are consistent across multiple benchmarks.
- **Medium confidence**: The claim that LDIR offers a good balance between performance and interpretability. While the performance metrics are solid, the actual human interpretability of the anchor-based dimensions is not rigorously tested.
- **Low confidence**: The generalizability of LDIR to diverse domains and tasks. The paper focuses on general semantic similarity, and there's limited discussion of performance on other NLP tasks or with different types of text.

## Next Checks
1. **Interpretability Validation**: Conduct a user study where participants are given LDIR embeddings and their corresponding anchor texts, and ask them to interpret specific dimensions or explain the semantic meaning of a given text vector. This would directly test the claimed interpretability advantage.

2. **Domain Robustness Test**: Apply LDIR to a specialized corpus (e.g., medical or legal texts) and evaluate both its performance on relevant tasks and the quality/interpretability of the selected anchors. This would test the method's robustness to domain shifts.

3. **Anchor Sampling Ablation**: Systematically compare FPS against uniform sampling and K-Means across a wider range of dimension sizes (e.g., 50, 100, 200, 500, 1000) on multiple downstream tasks (STS, retrieval, clustering) to validate the claimed superiority of FPS and identify the optimal anchor count for different use cases.