---
ver: rpa2
title: 'ImageGem: In-the-wild Generative Image Interaction Dataset for Generative
  Model Personalization'
arxiv_id: '2510.18433'
source_url: https://arxiv.org/abs/2510.18433
tags:
- preference
- user
- dataset
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ImageGem introduces the first large-scale dataset of real-world
  user interactions with generative models, capturing 57K users, 242K customized LoRAs,
  3M prompts, and 5M generated images. This dataset enables new research into personalized
  preference modeling by providing fine-grained, individual-level interaction data.
---

# ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization

## Quick Facts
- arXiv ID: 2510.18433
- Source URL: https://arxiv.org/abs/2510.18433
- Reference count: 40
- Primary result: First large-scale dataset capturing real-world user interactions with generative models (57K users, 242K LoRAs, 3M prompts, 5M images)

## Executive Summary
ImageGem introduces the first large-scale dataset of real-world user interactions with generative models, capturing 57K users, 242K customized LoRAs, 3M prompts, and 5M generated images. This dataset enables new research into personalized preference modeling by providing fine-grained, individual-level interaction data. Using ImageGem, researchers demonstrated improved preference alignment over existing datasets, achieved state-of-the-art performance in personalized image retrieval and model recommendation, and introduced a novel framework for editing LoRA models in latent weight space to align with individual user preferences.

## Method Summary
The method involves three main applications: (1) preference alignment via Diffusion Direct Preference Optimization (DiffusionDPO) trained on synthesized preference pairs from clustered user interactions, (2) personalized retrieval/recommendation using VLM-based ranking that converts visual history into structured textual profiles, and (3) generative model personalization through LoRA weight space editing using SVD reduction and PCA. The pipeline processes Civitai data through safety filtering, CLIP embedding computation, and relational database construction before applying these techniques.

## Key Results
- Demonstrated improved preference alignment over existing datasets using DiffusionDPO trained on ImageGem preference pairs
- Achieved state-of-the-art performance in personalized image retrieval and model recommendation with VLM ranking
- Introduced novel framework for editing LoRA models in latent weight space to align with individual user preferences
- Validated bidirectional editing capability (anime-to-realistic and vice versa) in SVD-based weight space

## Why This Works (Mechanism)

### Mechanism 1: Implicit Preference Extraction via Clustering
Real-world user interactions can substitute for explicit preference pairs if clustered to reduce noise. The paper aggregates user interaction logs and clusters prompts using CLIP embeddings and HDBScan, then uses Human Preference Score v2 to synthesize "winner" vs. "loser" pairs for DiffusionDPO training.

### Mechanism 2: SVD-Weight Space (W2W) Traversal
Diffusion model weights can be treated as a continuous latent space where semantic attributes are linear directions. The method applies SVD to LoRA weights, retains only the top-1 component, applies PCA to create a basis, and trains a linear classifier on user preference labels to identify traversal directions.

### Mechanism 3: VLM-Driven Interpretable Recommendation
Large Vision-Language Models can act as effective rankers by converting visual history into structured textual profiles. The system prompts a VLM to caption user history into a "visual preference profile" and then compares candidates against this text to generate scores and explanations.

## Foundational Learning

**Concept: Diffusion Direct Preference Optimization (DiffusionDPO)**
- Why needed: Core training loop for the "Preference Alignment" application
- Quick check: How does the loss function in Eq. (1) differ from standard fine-tuning loss?

**Concept: Linear Separability in Latent Spaces**
- Why needed: The "Personalization" mechanism relies on the assumption that "preferred" vs. "non-preferred" weights can be split by a hyperplane in the PCA space
- Quick check: Why does PCA make it easier to find a meaningful "direction" for editing compared to the raw weight space?

**Concept: Low-Rank Adaptation (LoRA)**
- Why needed: The entire dataset is built around LoRAs, not full model checkpoints
- Quick check: Why is the "rank" of a LoRA a problem for building a unified weight space, and how does SVD solve it?

## Architecture Onboarding

**Component map:** Civitai Scraper → Safety Filter (Detoxify) → Relational DB (User-LoRA-Image) → Prompt Clusterer (HDBScan) → Pair Synthesizer → DiffusionDPO Trainer → LoRA Rank-Reducer (SVD) → Space Builder (PCA) → Linear Classifier → Weight Editor

**Critical path:** The LoRA Weight Standardization is the bottleneck. If SVD reduction strips too much information, the subsequent PCA space will be meaningless, and the Linear Classifier cannot find a preference direction.

**Design tradeoffs:** SVD vs. Layer Slicing - SVD on full LoRA chosen for robustness, while slicing specific layers is faster but fails on bidirectional edits. VLM vs. Embedding Retrieval - VLM is more interpretable and accurate but significantly slower and more expensive than vector search.

**Failure signatures:** Mode Collapse in DPO (generated images look generic, implying noisy preference pairs or low KL penalty). Semantic Drift in Editing (editing a model changes subject identity, implying PCA directions entangle style with content).

**First 3 experiments:**
1. Sanity Check Rank-1: Verify Rank-1 SVD LoRAs generate images visually similar to original full-rank LoRAs using fixed seeds
2. Validate Space Geometry: Perform "Anime-to-Realistic" traversal to ensure W2W space is smooth and continuous
3. VLM Consistency Check: Run VLM ranker on same preference profile multiple times to measure instability and confirm if randomized scoring fixes it

## Open Questions the Paper Calls Out

**Open Question 1:** Can alternative latent space construction methods overcome PCA-based diversity constraints in W2W framework? Current reliance on PCA restricts model selection to low-rank adapters and limits diversity, explicitly calling for alternative methods for learning LoRA weight spaces.

**Open Question 2:** Does fine-tuning state-of-the-art architectures like Flux with ImageGem yield comparable or superior preference alignment gains over Stable Diffusion results? Authors suggest extension to larger models like Flux leveraging the entire dataset.

**Open Question 3:** How can raw implicit feedback signals be integrated into preference alignment pipeline to improve upon current HPS-based pairing strategy? Current method relies on clustering and external scoring models, potentially ignoring nuanced user intent in raw interaction metadata.

**Open Question 4:** Can generative model personalization paradigm be extended to effectively capture and edit user preferences across multiple data modalities and domains beyond human figure? W2W experiments constrained to human figures due to data abundance, with limited models prohibiting effective learning for less popular domains.

## Limitations
- Dataset availability unclear - release details not specified, requiring Civitai API scraping
- Hyperparameter sensitivity - critical parameters like PCA component count and α scaling not specified
- Generalization - all evaluations use DiffusionDPO and LoRA weight spaces; effectiveness on other generative architectures untested
- VLM stability - explicit acknowledgment of "instability" in VLM rankings requiring randomized scoring strategies

## Confidence
- High confidence: Dataset collection methodology and basic statistical properties
- Medium confidence: DiffusionDPO training pipeline and preference alignment results
- Medium confidence: SVD-PCA weight space construction and bidirectional editing capabilities
- Medium confidence: VLM-based recommendation outperforming traditional methods despite noted instability

## Next Checks
1. Dataset accessibility test: Attempt to reproduce Civitai scraper with public API to verify data collection feasibility
2. Hyperparameter sensitivity analysis: Systematically vary PCA component count and α scaling to identify optimal values
3. Cross-architecture validation: Test W2W personalization framework on non-Diffusion generative models (GANs or autoregressive models)