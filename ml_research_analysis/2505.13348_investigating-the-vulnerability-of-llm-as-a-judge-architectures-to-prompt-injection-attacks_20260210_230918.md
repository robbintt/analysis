---
ver: rpa2
title: Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection
  Attacks
arxiv_id: '2505.13348'
source_url: https://arxiv.org/abs/2505.13348
tags:
- arxiv
- attack
- attacks
- preprint
- llm-as-a-judge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the vulnerability of LLM-as-a-Judge systems
  to prompt-injection attacks. Two attack strategies are formalized: Comparative Undermining
  Attack (CUA), which directly targets the final decision output, and Justification
  Manipulation Attack (JMA), which aims to alter the model''s generated reasoning.'
---

# Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks

## Quick Facts
- **arXiv ID:** 2505.13348
- **Source URL:** https://arxiv.org/abs/2505.13348
- **Reference count:** 40
- **Key outcome:** LLM-as-a-Judge systems are vulnerable to prompt injection attacks, with CUA achieving over 30% attack success rate on MT-Bench dataset using Qwen2.5-3B-Instruct and Falcon3-3B-Instruct models.

## Executive Summary
This paper investigates the susceptibility of LLM-as-a-Judge architectures to prompt-injection attacks, which could compromise the integrity of automated evaluation systems. The authors formalize two distinct attack strategies: Comparative Undermining Attack (CUA) targeting final decision outputs, and Justification Manipulation Attack (JMA) aiming to alter the model's reasoning process. Using the Greedy Coordinate Gradient (GCG) optimization method, they craft adversarial suffixes that are appended to responses being evaluated, demonstrating significant vulnerabilities in current LLM-as-a-Judge implementations.

The research reveals that these automated judging systems can be systematically manipulated, with CUA achieving attack success rates exceeding 30% and JMA also showing notable effectiveness. These findings highlight critical security concerns for applications relying on LLM-based assessment frameworks, suggesting that current implementations may be vulnerable to targeted adversarial inputs that could skew evaluation outcomes or reasoning justifications.

## Method Summary
The authors employ the Greedy Coordinate Gradient (GCG) optimization method to generate adversarial suffixes that exploit vulnerabilities in LLM-as-a-Judge systems. These suffixes are appended to one of the responses being compared during evaluation. The experimental setup uses the MT-Bench Human Judgments dataset as ground truth for comparison, testing two 3B parameter models: Qwen2.5-3B-Instruct and Falcon3-3B-Instruct. The evaluation measures Attack Success Rate (ASR) for both CUA and JMA attack strategies, quantifying how effectively the adversarial inputs can manipulate either the final judgment or the justification reasoning.

## Key Results
- CUA attack achieved Attack Success Rate exceeding 30% on tested models
- JMA attack also demonstrated significant effectiveness in manipulating reasoning
- Both attack strategies successfully compromised LLM-as-a-Judge systems using crafted adversarial suffixes
- Vulnerabilities were consistently observed across both Qwen2.5-3B-Instruct and Falcon3-3B-Instruct models

## Why This Works (Mechanism)
The effectiveness of these attacks stems from the inherent design of LLM-as-a-Judge systems that process and compare multiple responses sequentially. When adversarial suffixes are appended to responses, they exploit the model's attention mechanisms and contextual processing, allowing the injected content to influence either the final judgment decision (CUA) or the reasoning justification (JMA). The GCG optimization method systematically discovers input patterns that maximize this influence by iteratively refining the adversarial suffixes based on gradient information from the judge model's output.

## Foundational Learning
- **LLM-as-a-Judge Architecture**: Understanding how LLMs evaluate and compare responses is crucial for identifying attack vectors; quick check: verify the typical input-output structure of judge models
- **Prompt Injection Attacks**: Knowledge of how malicious inputs can manipulate LLM behavior is fundamental; quick check: review examples of successful prompt injection in other contexts
- **Greedy Coordinate Gradient (GCG) Optimization**: This method iteratively optimizes adversarial suffixes by making greedy local improvements; quick check: understand how GCG differs from other adversarial attack methods
- **Attack Success Rate (ASR) Metrics**: Measuring attack effectiveness requires understanding evaluation metrics; quick check: review how ASR is calculated and what thresholds indicate successful attacks
- **MT-Bench Dataset Structure**: Familiarity with the benchmark dataset used for evaluation; quick check: examine the format and content of MT-Bench human judgments
- **3B Parameter Model Characteristics**: Understanding the capabilities and limitations of smaller LLM models used in experiments; quick check: compare performance characteristics of 3B vs larger models

## Architecture Onboarding
- **Component Map:** LLM-as-a-Judge <- (Input Responses) -> Evaluation Output/Reasoning
- **Critical Path:** Input Responses → Judge Model Processing → Output Generation → Final Judgment/Justification
- **Design Tradeoffs:** Accuracy vs. efficiency in judge models, security vs. usability in evaluation systems, model size vs. computational cost
- **Failure Signatures:** Unexpected evaluation outputs, inconsistent judgments, manipulated reasoning justifications, successful adversarial suffix injection
- **First Experiments:** 1) Test CUA attack on different judge models to verify vulnerability transferability; 2) Evaluate JMA effectiveness across various prompt templates; 3) Measure attack success rates with different adversarial suffix lengths

## Open Questions the Paper Calls Out
- How do these vulnerabilities scale with larger model architectures (e.g., 70B+ parameter models)?
- Can defensive fine-tuning or prompt engineering strategies effectively mitigate these attacks?
- What is the relationship between model size and susceptibility to prompt injection attacks?
- How transferable are adversarial suffixes across different judge model architectures?

## Limitations
- Evaluation limited to single dataset (MT-Bench) and two specific 3B parameter models
- Attack success rates may be influenced by specific GCG implementation and adversarial suffix crafting
- Does not explore defensive mechanisms or mitigation strategies
- Limited exploration of the distinction between CUA and JMA effectiveness mechanisms

## Confidence
- **High Confidence**: Existence of vulnerabilities in LLM-as-a-Judge systems to prompt injection attacks is well-supported by experimental results
- **Medium Confidence**: Relative effectiveness of CUA versus JMA attacks is plausible but may vary with different implementations
- **Medium Confidence**: Generalizability of findings to other datasets, model sizes, or judge configurations is uncertain without further validation

## Next Checks
1. Test attack effectiveness across a broader range of datasets and model architectures (e.g., GPT-4, Claude, or larger open-source models)
2. Evaluate the transferability of adversarial suffixes across different judge models to assess attack robustness
3. Investigate the impact of defensive fine-tuning or prompt engineering strategies on reducing attack success rates