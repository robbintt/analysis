---
ver: rpa2
title: Leveraging LLMs For Turkish Skill Extraction
arxiv_id: '2601.22885'
source_url: https://arxiv.org/abs/2601.22885
tags:
- skill
- skills
- esco
- extraction
- turkish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first Turkish skill extraction dataset
  and evaluates automated skill extraction using large language models (LLMs). The
  dataset contains 4,819 labeled skill spans from 327 job postings across different
  occupation areas.
---

# Leveraging LLMs For Turkish Skill Extraction

## Quick Facts
- **arXiv ID:** 2601.22885
- **Source URL:** https://arxiv.org/abs/2601.22885
- **Reference count:** 40
- **Primary result:** Turkish skill extraction achieves 0.56 end-to-end F1 using LLM pipeline with dynamic few-shot prompting

## Executive Summary
This paper introduces the first Turkish skill extraction dataset and evaluates automated skill extraction using large language models (LLMs). The dataset contains 4,819 labeled skill spans from 327 job postings across different occupation areas. The study addresses three research questions: effective skill extraction for morphologically complex Turkish, the most promising model, and the impact of different LLM prompting strategies. The best-performing configuration uses Claude Sonnet 3.7 with dynamic few-shot prompting for skill identification, embedding-based retrieval, and LLM-based reranking for skill linking, achieving an end-to-end F1-score of 0.56. This result positions Turkish skill extraction on par with similar studies in other languages, demonstrating that LLMs can improve skill extraction performance in low-resource settings.

## Method Summary
The pipeline consists of two main stages: Skill Identification and Skill Linking. For identification, Claude 3.7 Sonnet uses dynamic few-shot prompting with kNN-retrieval (k=5) of training examples based on multilingual-e5-large embeddings. The linking stage includes multi-skill parsing via SVM classifier, ESCO skill retrieval using embedding similarity (multilingual-e5-large), and LLM reranking with GPT-4o using causal reasoning prompts. The dataset comprises 327 Turkish job postings with 4,819 labeled skill spans, split ~60/20/20 by occupation category. Evaluation uses custom end-to-end F1 (Algorithm 1), CoNLL-F1/MUC for identification, and HitRate@k for retrieval.

## Key Results
- End-to-end F1 of 0.56 achieved using Claude Sonnet 3.7 with dynamic few-shot prompting
- Embedding similarity-based retrieval outperforms fuzzy matching (HitRate@10: 0.88 vs 0.46) for Turkish morphology
- Causal reasoning reranking improves HitRate@1 by 2 percentage points to 0.68
- Dynamic few-shot prompting significantly outperforms zero-shot and static approaches for skill identification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic few-shot prompting improves skill identification over static and zero-shot approaches for morphologically complex Turkish.
- **Mechanism:** Dynamic kNN-retrieval selects contextually similar training examples per input sentence, providing more relevant demonstrations that help the LLM handle Turkish's agglutinative morphology and varied skill surface forms.
- **Core assumption:** The training set contains sufficient coverage of skill patterns to retrieve useful examples for most test sentences.
- **Evidence anchors:**
  - [abstract]: "The best-performing configuration... utilizing Claude Sonnet 3.7 with dynamic few-shot prompting for skill identification..."
  - [Section 4.1]: "We have generated embeddings for each sentence and retrieved the top k closest sentences in the training set using cosine similarity."
  - [corpus]: Related work (Nguyen et al.) uses dynamic few-shot prompting but does not compare to static; this paper extends that comparison.
- **Break condition:** If the training set is too small or domain-mismatched, retrieved examples may not generalize, reducing dynamic prompting benefits.

### Mechanism 2
- **Claim:** Embedding-based retrieval outperforms fuzzy matching for linking Turkish skill spans to ESCO due to better semantic alignment.
- **Mechanism:** Embedding similarity captures semantic relationships (e.g., "arac kullanabilen" to "arac sürmek") that fuzzy matching misses because Turkish morphology increases character edit distances between surface forms.
- **Core assumption:** The embedding model (multilingual-e5-large) adequately represents Turkish semantic similarity in the HR domain.
- **Evidence anchors:**
  - [Section 5.3]: "Embedding similarity-based retrieval consistently outperforms fuzzy matching across all HitRate@k metrics."
  - [Figure 3]: Illustrates how embedding-based retrieval produces semantically similar labels while fuzzy matching produces lexically similar but less meaningful results.
  - [corpus]: Contrastive bi-encoder models for skill extraction (related paper) similarly leverage embedding similarity for ESCO matching.
- **Break condition:** If the embedding model is not well-adapted to Turkish HR terminology, semantic retrieval quality may degrade.

### Mechanism 3
- **Claim:** LLM-based reranking with causal reasoning improves skill linking precision by encouraging functional evaluation of skill relevance.
- **Mechanism:** Prompting the LLM to consider cause-and-effect (why an ESCO skill might be relevant and its impact on job requirements) pushes the model beyond surface similarity to consider practical applicability, leading to better top-1 selection.
- **Core assumption:** The LLM has sufficient world knowledge to reason about job-skill relationships in Turkish labor market contexts.
- **Evidence anchors:**
  - [Section 5.3]: "w/ Causal Reason + RerankKey prompt... improve the scores by 2 percentage points, reaching the highest observed HitRate@1 of 0.68."
  - [Section 4.2.3]: "By incorporating causal reasoning, we move beyond simple ranking based on textual similarity and encourage the model to engage in counterfactual evaluation."
  - [corpus]: Weak direct corpus evidence for causal reasoning in skill extraction; this appears novel in this application domain.
- **Break condition:** If the LLM lacks domain-specific knowledge (e.g., niche Turkish tools or regulations), causal reasoning may produce plausible but incorrect justifications.

## Foundational Learning

- **Concept:** Few-shot prompting (static vs. dynamic)
  - **Why needed here:** The paper compares static (fixed examples) and dynamic (retrieved per input) few-shot prompting for skill identification, showing dynamic outperforms.
  - **Quick check question:** Can you explain how dynamic few-shot prompting differs from static few-shot prompting, and why retrieval-based example selection might help with morphological complexity?

- **Concept:** Embedding-based retrieval and similarity search
  - **Why needed here:** The skill linking stage uses embedding similarity to retrieve top-k ESCO candidates, which is central to the pipeline's performance.
  - **Quick check question:** Given a Turkish skill span like "veri analizi yapmak", how would you use embedding similarity to find related ESCO skills, and what role does cosine similarity play?

- **Concept:** Reranking in multi-stage pipelines
  - **Why needed here:** After retrieval, an LLM reranks candidates to refine top-1 selection, with causal reasoning providing additional gains.
  - **Quick check question:** Why might a reranking step improve over using the top-1 retrieved result directly, and how does adding causal reasoning change the LLM's decision process?

## Architecture Onboarding

- **Component map:** Skill Identification (LLM/BERT NER) → Multi-Skill Parsing (SVM classifier + rule-based splitter) → ESCO Skill Retrieval (embedding similarity/fuzzy match) → LLM Reranking (GPT-4o with causal reasoning)
- **Critical path:** Skill Identification output quality directly impacts linking performance. If identification misses or incorrectly splits skills, linking cannot recover.
- **Design tradeoffs:** (1) BERT vs. LLM for identification: BERT has higher span accuracy (CoNLL-F1 0.64 vs. Claude's 0.57), but Claude's spans align better with ESCO, improving end-to-end F1. (2) Retrieval method: Embedding similarity has higher HitRate@10 (0.88) but requires GPU for embedding model; fuzzy matching is faster but performs poorly for Turkish (HitRate@10 0.46). (3) Reranking complexity: Causal reasoning adds ~10% cost but improves HitRate@1 by 2pp; simpler reranking prompts offer similar performance with lower cost.
- **Failure signatures:** (1) Ontologically related errors: System predicts skills in the same ESCO hierarchy but not exact matches (11.67% of errors), indicating embedding similarity conflates related skills. (2) Skill omission: Human evaluation found system missed critical tools like "Power BI" or "Logo program" that are essential but less common in training data. (3) Position title confusion: LLM incorrectly extracts job titles (e.g., "pazarlama") as skills, mistaking occupational references for skill mentions.
- **First 3 experiments:**
  1. **Baseline retrieval comparison:** Using gold-standard skill spans, compare fuzzy matching vs. embedding similarity HitRate@k to isolate retrieval performance without identification noise.
  2. **Identification model ablation:** Run end-to-end pipeline with BERT, Claude (zero-shot), Claude (static two-shot), and Claude (dynamic ten-shot) to quantify contribution of identification method and prompting strategy.
  3. **Reranking prompt iteration:** With Claude for identification and embedding retrieval, compare no rerank, RerankKey, RerankContext, and Causal Reason + RerankKey to measure precision gains and cost increments.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can taxonomy-aware reranking methods that explicitly utilize ESCO's hierarchical relationships reduce linking errors where the predicted skill is ontologically related but not an exact match?
- **Basis in paper:** [explicit] The authors identify that 11.67% of linking errors were "Ontologically Related" predictions (e.g., sibling skills sharing a parent) and explicitly call for "developing taxonomy-aware reranking methods that explicitly utilize hierarchical relationships between skills."
- **Why unresolved:** Current reranking relies on semantic similarity without awareness of ESCO's hierarchical structure, causing semantically close predictions to be penalized equally with unrelated ones.
- **What evidence would resolve it:** A comparative study showing reduced ontologically-related errors using hierarchy-aware reranking versus baseline semantic similarity approaches.

### Open Question 2
- **Question:** Would hierarchy-sensitive evaluation metrics that assign partial credit based on ontology distance provide a more realistic measure of system performance than current binary metrics?
- **Basis in paper:** [explicit] The authors state that "current evaluation metrics penalize these semantically reasonable predictions the same as entirely unrelated ones" and call for "metrics that incorporate skill hierarchy distance or taxonomy-informed similarity."
- **Why unresolved:** End-to-end evaluation (Algorithm 1) treats all mismatches equally, failing to capture meaningful conceptual proximity between related ESCO nodes.
- **What evidence would resolve it:** Correlation analysis between partial-credit metrics and human expert judgments of extraction usefulness in downstream recruitment tasks.

### Open Question 3
- **Question:** Can parameter-efficient fine-tuning methods (LoRA, adapters) on open-source models match or exceed the performance of proprietary LLMs (Claude, GPT-4o) while reducing inference costs?
- **Basis in paper:** [inferred] The authors note BERT outperformed LLMs in identification due to fine-tuning, while LLMs excelled in linking. They explicitly call LoRA/adapter tuning a "practical alternative" but did not test it due to resource constraints.
- **Why unresolved:** The study relied on prompting closed-source models; no experiments evaluated whether domain-adapted open models could achieve comparable end-to-end F1 (0.56) at lower cost.
- **What evidence would resolve it:** Benchmarking LoRA-tuned open models (e.g., Gemma, Qwen) against the best Claude+GPT-4o configuration on the same Turkish dataset.

### Open Question 4
- **Question:** Would expanding the dataset to include job postings from multiple platforms with balanced sector representation improve model generalization to underrepresented domains like medicine and health?
- **Basis in paper:** [explicit] The authors acknowledge the dataset's skew toward Sales-Marketing (14.54%) and Finance (10.98%) versus Medicine/Health (1.78%) and call for "expanding the dataset by incorporating job postings from multiple employment platforms" with balanced area distributions.
- **Why unresolved:** The single-source dataset may not capture industry-specific vocabulary or skill phrasing variations across different recruitment platforms and sectors.
- **What evidence would resolve it:** Performance comparison on held-out job postings from new platforms/sectors after training on an expanded, balanced multi-source dataset.

## Limitations
- **Dataset Availability:** The Turkish skill extraction dataset will only be released upon paper acceptance, limiting independent validation and broader research community access.
- **Evaluation Metric Constraints:** The custom end-to-end F1 metric (Algorithm 1) and partial match evaluation may not fully capture practical deployment performance in real-world job matching systems.
- **Morphological Coverage:** The dynamic few-shot approach relies on sufficient representation of Turkish morphological variants in training data, which may not generalize to rare or domain-specific skill expressions.

## Confidence
- **High Confidence:** The embedding-based retrieval outperforming fuzzy matching (HitRate@10: 0.88 vs 0.46) is strongly supported by controlled experiments with gold-standard spans, directly demonstrating the superiority of semantic similarity for Turkish morphology.
- **Medium Confidence:** The end-to-end F1 of 0.56 and claim of parity with non-Turkish studies assumes comparable evaluation settings and dataset characteristics, which may not be fully verified across studies.
- **Medium Confidence:** The causal reasoning reranking improvement (2pp gain in HitRate@1) is based on specific prompt engineering that may not transfer to different LLM versions or prompting frameworks.

## Next Checks
1. **Dataset Validation:** Once released, conduct independent evaluation using standard NER metrics (precision/recall per span) to verify reported CoNLL-F1 scores and assess morphological coverage across occupation categories.
2. **Pipeline Robustness:** Test the end-to-end system with intentionally degraded identification performance (e.g., missing spans) to quantify how much linking accuracy depends on perfect skill extraction versus robust retrieval/reranking.
3. **Cross-Lingual Generalization:** Apply the same dynamic few-shot + embedding retrieval pipeline to another morphologically complex language (e.g., Finnish or Hungarian) to assess whether the methodology transfers beyond Turkish.