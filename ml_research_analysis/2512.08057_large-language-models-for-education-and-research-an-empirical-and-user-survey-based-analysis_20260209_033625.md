---
ver: rpa2
title: 'Large Language Models for Education and Research: An Empirical and User Survey-based
  Analysis'
arxiv_id: '2512.08057'
source_url: https://arxiv.org/abs/2512.08057
tags:
- chatgpt
- deepseek
- research
- while
- education
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study conducted a comprehensive evaluation of ChatGPT and
  DeepSeek across multiple dimensions, comparing their performance in text generation,
  programming, and specialized problem-solving tasks, while also gathering user feedback
  through a survey. Experimental results revealed that ChatGPT excels in general language
  understanding and text generation, whereas DeepSeek demonstrates superior performance
  in programming tasks due to its efficiency-focused design.
---

# Large Language Models for Education and Research: An Empirical and User Survey-based Analysis

## Quick Facts
- arXiv ID: 2512.08057
- Source URL: https://arxiv.org/abs/2512.08057
- Reference count: 35
- Primary result: Comparative evaluation shows ChatGPT excels in text generation while DeepSeek outperforms in programming, with complementary strengths in education and research applications.

## Executive Summary
This study presents a comprehensive empirical and user survey-based analysis of ChatGPT and DeepSeek across multiple dimensions relevant to education and research. The research evaluates both models on text generation, programming tasks, mathematical problem-solving, scientific reasoning, and medical diagnostics, while also gathering user feedback through a survey of students, educators, and researchers. The findings reveal that ChatGPT demonstrates superior performance in general language understanding and text generation tasks, while DeepSeek shows particular strength in programming tasks due to its efficiency-focused Mixture-of-Experts architecture. Both models achieve high accuracy in medical diagnostic outputs and complex mathematical problem-solving, with ChatGPT being perceived as more reliable and versatile for educational applications, while DeepSeek is valued specifically for coding support.

## Method Summary
The study employed a two-pronged approach combining quantitative task-based evaluation with user perception surveys. Seven programming problems from Codeforces (difficulty 800-2200), Fourier series expansion problems, structured science questions, and medical diagnostic scenarios were used as benchmark tasks. Both models generated outputs independently, which were then validated by domain experts and measured using BLEU, ROUGE-L, and BERTScore metrics against human reference solutions. A two-part user survey was conducted with 24 participants (primarily undergraduates and faculty from Computer Science and Engineering) who rated the models across six aspects including reliability, accuracy, and usability on a 1-5 scale, supplemented by open-ended responses about their experiences.

## Key Results
- ChatGPT achieved higher success rates in text generation and general language understanding tasks, while DeepSeek demonstrated superior performance in programming tasks with a perfect success rate on Codeforces problems.
- Both models showed high accuracy (~90%) in medical diagnostic outputs and complex mathematical problem-solving, with DeepSeek employing transparent chain-of-thought reasoning that trades latency for precision.
- User survey results indicated ChatGPT was perceived as more reliable and versatile (58.3% identified it as most reliable), while DeepSeek was specifically valued for coding support despite concerns about response speed.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Performance divergences between models are likely driven by architectural specialization (Dense vs. Mixture-of-Experts) rather than scale alone.
- **Mechanism:** ChatGPT utilizes a dense GPT-based Transformer where all parameters are active during inference, favoring broad generalization and fluency. In contrast, DeepSeek employs a Mixture-of-Experts (MoE) architecture that dynamically activates only a subset of specialized sub-models (experts) for specific tokens (e.g., code or math tokens). This conditional routing allows for higher efficiency and potentially deeper reasoning capacity for specific domains without proportional computational cost.
- **Core assumption:** Assumption: The observed superior performance of DeepSeek in programming (Table 2) is causally linked to its MoE routing strategy rather than solely its training data.
- **Evidence anchors:**
  - [Section 3.2]: "DeepSeek... employs a mixture-of-experts (MoE) architecture that dynamically activates specialized sub-models... reducing redundant computations."
  - [Section 4.1]: "DeepSeek, by contrast, achieved a perfect success rate [on Codeforces]... while ChatGPT... occasionally failed due to wrong answers (WA), time limit exceeded (TLE)..."
  - [Corpus]: Neighbors confirm LLMs like Codex are revolutionizing programming, but this paper specifically attributes efficiency to MoE design.
- **Break condition:** Mechanism breaks if MoE routing fails to generalize to out-of-distribution coding tasks, or if dense models close the efficiency gap via quantization.

### Mechanism 2
- **Claim:** User trust and perceived reliability in education correlate strongly with "Alignment" techniques (RLHF) that prioritize helpfulness over raw capability.
- **Mechanism:** ChatGPT applies Reinforcement Learning from Human Feedback (RLHF) to align model outputs with human conversational preferences. This minimizes unsafe or incoherent outputs, creating a "smooth" interaction that users rate as highly reliable, even if raw problem-solving accuracy is comparable or lower than competitors.
- **Core assumption:** Assumption: The survey participants equate "fluency" and "safety" with "reliability," biasing ratings toward ChatGPT despite DeepSeek's superior code performance.
- **Evidence anchors:**
  - [Section 3.1]: "RLHF... mitigates the issues of incoherence, factual inaccuracy, and unsafe outputs."
  - [Section 4.2]: "58.3% [of respondents] identified ChatGPT as the most reliable tool... praised for its 'early and accurate response'."
  - [Corpus]: Neighbor "What Shapes User Trust in ChatGPT?" confirms user attributes and interaction dimensions heavily influence trust perceptions.
- **Break condition:** Breaks if the educational task requires strict factual precision over conversational fluency (e.g., strict mathematical proof), where "confident but wrong" answers degrade trust.

### Mechanism 3
- **Claim:** Explicit chain-of-thought (CoT) processing enables higher accuracy in complex reasoning tasks at the cost of latency.
- **Mechanism:** DeepSeek utilizes a transparent chain-of-thought (CoT) process, generating internal reasoning steps before the final answer. This "thinking time" allows for error correction in multi-step logic (math/code) but results in slower response generation compared to direct token prediction.
- **Core assumption:** Assumption: The latency observed by users is a direct result of the CoT reasoning path, not just server load.
- **Evidence anchors:**
  - [Section 3.2]: "DeepSeek uniquely employs a transparent chain-of-thought (CoT) process, performing step-by-step logical reasoning before answering."
  - [Section 4.2]: Users noted DeepSeek's "deep thought chain, though it takes more time to answer, is high and precise."
  - [Corpus]: Corpus evidence on CoT mechanisms specifically for DeepSeek in this context is weak; neighbors focus more on output quality than internal reasoning chains.
- **Break condition:** Breaks if the CoT drifts (hallucination in the reasoning steps) or if latency exceeds user tolerance thresholds for interactive education.

## Foundational Learning

- **Concept: Transformer Self-Attention**
  - **Why needed here:** Both evaluated models rely on the Transformer architecture. Understanding that $P(x_t|x_{<t})$ depends on attention scores across the input sequence is required to grasp how these models maintain context in long lectures or codebases.
  - **Quick check question:** How does the self-attention mechanism differ from the recurrent structure of LSTMs in handling long-range dependencies?

- **Concept: Mixture-of-Experts (MoE)**
  - **Why needed here:** Critical for distinguishing DeepSeek's architecture. Unlike dense models, MoE models route inputs to specific "expert" neurons. This explains DeepSeek's efficiency/strength in coding vs. general text.
  - **Quick check question:** In an MoE model, are all parameters activated for every input token? Why does this improve efficiency?

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - **Why needed here:** Essential for understanding ChatGPT's dominance in user surveys. RLHF is the process that aligns the model to be "helpful," distinguishing it from raw base models.
  - **Quick check question:** What role does the "reward model" play in adjusting the weights of an LLM during RLHF?

## Architecture Onboarding

- **Component map:**
  - **ChatGPT Path:** User Prompt → Dense GPT Transformer → RLHF Policy Head → Response.
  - **DeepSeek Path:** User Prompt → MoE Router → Expert Selection (Activated Weights) → CoT Reasoning Module → Response.
  - **Evaluation Layer:** Human Expert Validation (Math/Code) vs. User Survey (Perception).

- **Critical path:**
  1.  **Define Task Type:** Route "Creative/Lecture Prep" tasks to the dense model (ChatGPT) and "Code/Math" tasks to the MoE/CoT model (DeepSeek) based on empirical results.
  2.  **Metric Selection:** Use BLEU/ROUGE for text similarity (Section 4.1) and "Success Rate" (Pass/Fail) for code execution (Table 2). Do not rely solely on "reliability" ratings for code correctness.

- **Design tradeoffs:**
  - **ChatGPT:** Optimized for *fluency* and *interaction speed*. Tradeoff: Higher hardware requirements (dense activation) and lower reliability on complex code (TLE/CE errors).
  - **DeepSeek:** Optimized for *precision* and *inference efficiency*. Tradeoff: Higher latency due to CoT and potentially lower versatility in creative writing tasks.

- **Failure signatures:**
  - **ChatGPT:** Fails on Codeforces problems rated >1200 with errors like TLE (Time Limit Exceeded) or WA (Wrong Answer).
  - **DeepSeek:** Fails on user expectations for speed; reported as "time consuming."
  - **General:** Both fail on specific reference retrieval (hallucinating papers) and arithmetic without explicit prompting.

- **First 3 experiments:**
  1.  **Code Generation Stress Test:** Replicate the Codeforces experiment (Table 2) by prompting both models with a Problem-E (difficulty 1500) to observe if ChatGPT fails with TLE/WA while DeepSeek succeeds.
  2.  **Hallucination Check:** Ask both models to summarize a specific, obscure research paper. Verify if "correct references" are included or hallucinated (as noted in Open-Ended Survey limitations).
  3.  **Mathematical CoT Comparison:** Prompt a complex Fourier series expansion. Measure the time taken for DeepSeek to generate the "thought chain" vs. ChatGPT's direct response and compare derivation accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does prolonged reliance on LLMs for educational tasks affect students' independent critical thinking and problem-solving capabilities over time?
- **Basis in paper:** [explicit] Survey respondents expressed concern that overreliance on AI could diminish critical thinking: "the more people are getting familiar with them, their own brain cells are becoming [less active]. People are to regret ultimately."
- **Why unresolved:** The study captures user perceptions at a single time point but does not conduct longitudinal research to measure actual cognitive or skill degradation.
- **What evidence would resolve it:** Longitudinal studies tracking students' independent problem-solving performance and critical thinking assessments before and after sustained LLM use.

### Open Question 2
- **Question:** To what extent do the comparative findings generalize beyond Computer Science and Engineering to other academic disciplines?
- **Basis in paper:** [inferred] 79.2% of survey respondents were from Computer Science and Engineering, with only 8.3% from AI and 8.3% from Science fields. Humanities, social sciences, and other domains were minimally represented.
- **Why unresolved:** The sample distribution limits the generalizability of conclusions about educational and research utility across diverse academic fields.
- **What evidence would resolve it:** Replication of the comparative evaluation with balanced participant pools across humanities, social sciences, natural sciences, and professional disciplines.

### Open Question 3
- **Question:** What specific integration frameworks can maximize the complementary strengths of ChatGPT and DeepSeek in educational workflows?
- **Basis in paper:** [explicit] The authors state that the models "complement each other: ChatGPT provides conversational adaptability, and DeepSeek ensures factual accuracy, offering a combined toolkit" but do not specify implementation strategies.
- **Why unresolved:** The paper identifies complementary capabilities but provides no empirical evaluation of combined or sequential use patterns.
- **What evidence would resolve it:** Experimental studies comparing single-model versus orchestrated multi-model workflows on educational task completion quality and efficiency.

### Open Question 4
- **Question:** How can systematic detection and mitigation of hallucinations in reference generation be achieved for academic research applications?
- **Basis in paper:** [explicit] Survey participants reported that "sometimes the answer is made up, in particular for searching papers" and both models "failed to include correct references" when evidence was requested.
- **Why unresolved:** The study documents the reliability issue but offers no technical or procedural solutions for verification.
- **What evidence would resolve it:** Development and testing of automated fact-checking layers or retrieval-augmented citation verification systems integrated into LLM outputs.

## Limitations
- Small sample size (24 participants) may not capture diverse user experiences across different academic disciplines and cultural contexts.
- Evaluation focuses on two specific models without exploring the full spectrum of available LLMs, potentially limiting generalizability of architectural conclusions.
- Sample distribution heavily skewed toward Computer Science and Engineering (79.2%), limiting conclusions about educational utility across diverse academic fields.

## Confidence
- **High Confidence:** The architectural distinctions between ChatGPT's dense Transformer and DeepSeek's MoE design, and their respective performance impacts on programming tasks (success rates on Codeforces problems).
- **Medium Confidence:** User perception findings linking ChatGPT's reliability ratings to its RLHF alignment, as this depends on subjective survey interpretations.
- **Medium Confidence:** DeepSeek's superior performance in complex reasoning tasks through chain-of-thought processing, though the latency trade-off requires further validation across diverse problem types.

## Next Checks
1. **Replication Study:** Conduct the same evaluation with a larger, more diverse participant pool (minimum 100 participants across multiple institutions) to validate the survey findings on user preferences and perceived reliability.

2. **Cross-Model Comparison:** Test the same benchmark tasks with additional LLMs (e.g., Claude, Gemini) to determine if the observed performance patterns are specific to ChatGPT and DeepSeek or represent broader architectural trends.

3. **Longitudinal Analysis:** Implement a follow-up study tracking the same users over a semester to assess how initial perceptions of LLM reliability and effectiveness evolve with continued use in actual coursework and research projects.