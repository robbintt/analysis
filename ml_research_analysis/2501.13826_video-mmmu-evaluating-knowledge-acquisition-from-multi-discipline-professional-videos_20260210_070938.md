---
ver: rpa2
title: 'Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional
  Videos'
arxiv_id: '2501.13826'
source_url: https://arxiv.org/abs/2501.13826
tags:
- video
- knowledge
- question
- adaptation
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Video-MMMU introduces a benchmark to evaluate how large multimodal
  models acquire knowledge from educational videos. It features 300 expert-level videos
  across six disciplines with 900 human-annotated questions aligned to three cognitive
  stages: Perception, Comprehension, and Adaptation.'
---

# Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos

## Quick Facts
- **arXiv ID:** 2501.13826
- **Source URL:** https://arxiv.org/abs/2501.13826
- **Reference count:** 40
- **Primary result:** LMMs struggle to effectively learn from educational videos, showing significant performance gaps compared to humans, especially in applying acquired knowledge to novel scenarios

## Executive Summary
Video-MMMU introduces a benchmark to evaluate how large multimodal models acquire knowledge from educational videos. It features 300 expert-level videos across six disciplines with 900 human-annotated questions aligned to three cognitive stages: Perception, Comprehension, and Adaptation. A novel metric, Δknowledge, quantifies knowledge gains after video viewing. Results show models struggle to effectively learn from videos, with significant performance gaps compared to humans, particularly in applying acquired knowledge to novel scenarios. The findings highlight the need for improved methods to enhance models' video-based learning capabilities.

## Method Summary
Video-MMMU evaluates knowledge acquisition through a controlled benchmark featuring 300 expert-level videos across six disciplines (physics, biology, mathematics, history, geography, and psychology). Each video is accompanied by three human-annotated questions aligned to Bloom's taxonomy cognitive stages: Perception (basic observation), Comprehension (understanding relationships), and Adaptation (applying knowledge to novel scenarios). The benchmark introduces Δknowledge, a metric that measures knowledge gains by comparing pre-video and post-video performance. The evaluation tests LMMs' ability to extract and apply information from educational content using a standardized video processing pipeline.

## Key Results
- Models show significant performance gaps compared to humans across all cognitive stages
- Δknowledge metric reveals limited knowledge acquisition from video content
- Adaptation stage (applying knowledge to novel scenarios) shows the largest performance gap between models and humans

## Why This Works (Mechanism)
The benchmark works by creating a controlled environment where knowledge acquisition can be systematically measured. By using expert-level videos and human-annotated questions following Bloom's taxonomy, it establishes clear ground truth for evaluating model performance. The Δknowledge metric provides quantitative measurement of learning progress, while the three-stage cognitive framework ensures comprehensive assessment of different aspects of knowledge acquisition. The standardized video processing pipeline enables consistent evaluation across different model architectures.

## Foundational Learning

**Bloom's Taxonomy** - Framework for categorizing educational goals and objectives
- Why needed: Provides structured approach to assess different cognitive levels of knowledge acquisition
- Quick check: Verify question alignment to cognitive stages follows taxonomy guidelines

**Multimodal Learning** - Integration of visual and textual information processing
- Why needed: Essential for understanding how models process video content combining audio, visual, and textual elements
- Quick check: Confirm model can extract features from all video modalities

**Knowledge Transfer** - Application of learned knowledge to novel scenarios
- Why needed: Critical for evaluating true understanding versus memorization
- Quick check: Test model performance on unseen but related tasks

## Architecture Onboarding

**Component Map:** Video Processing -> Feature Extraction -> Knowledge Integration -> Question Answering -> Δknowledge Calculation

**Critical Path:** Video frames/audio → Multimodal embeddings → Cross-modal attention → Answer generation → Performance comparison

**Design Tradeoffs:** The benchmark prioritizes controlled evaluation over real-world complexity, sacrificing video diversity for annotation quality and consistency. This enables precise measurement but may limit generalizability.

**Failure Signatures:** Poor performance on Adaptation stage questions, minimal Δknowledge scores, inability to transfer knowledge across related concepts, failure to process temporal relationships in videos.

**First Experiments:**
1. Test basic video comprehension with simplified Perception stage questions
2. Evaluate cross-modal attention effectiveness on single-discipline videos
3. Measure baseline Δknowledge with pre-training video exposure

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of 300 expert-level videos may not capture full diversity of educational video formats
- Human annotation process introduces potential subjectivity in question difficulty calibration
- Δknowledge metric assumes linear knowledge acquisition patterns

## Confidence
- **High:** Performance gap findings for Perception and Comprehension tasks
- **Medium:** Adaptation stage results due to novel evaluation dimension
- **Low:** Generalization claims about video-based learning effectiveness across broader educational contexts

## Next Checks
1. External validation with larger, more diverse video corpus (target: 1000+ videos) spanning additional disciplines and production styles
2. Replication study using multiple independent annotation teams to quantify inter-rater reliability
3. Ablation studies testing different video preprocessing methods (frame sampling rates, temporal chunking strategies)