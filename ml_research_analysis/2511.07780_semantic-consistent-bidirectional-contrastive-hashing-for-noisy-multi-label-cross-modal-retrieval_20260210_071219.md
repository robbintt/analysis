---
ver: rpa2
title: Semantic-Consistent Bidirectional Contrastive Hashing for Noisy Multi-Label
  Cross-Modal Retrieval
arxiv_id: '2511.07780'
source_url: https://arxiv.org/abs/2511.07780
tags:
- cross-modal
- label
- noisy
- hashing
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of learning robust cross-modal
  hashing under noisy multi-label supervision. The proposed SCBCH framework addresses
  this by combining two key modules: CSCC, which leverages cross-modal semantic consistency
  to adaptively reweight samples based on neighbor reliability, and BSCH, a bidirectional
  soft contrastive hashing module that constructs fine-grained soft pairs using label
  overlap rather than hard positives/negatives.'
---

# Semantic-Consistent Bidirectional Contrastive Hashing for Noisy Multi-Label Cross-Modal Retrieval

## Quick Facts
- arXiv ID: 2511.07780
- Source URL: https://arxiv.org/abs/2511.07780
- Reference count: 14
- Primary result: Outperforms 12 SOTA methods with up to 2.3% MAP improvement at 80% noise rate

## Executive Summary
This paper addresses the challenge of cross-modal hashing for image-text retrieval under noisy multi-label supervision. The proposed SCBCH framework introduces two key modules: CSCC, which leverages cross-modal semantic consistency to adaptively reweight samples based on neighbor reliability, and BSCH, a bidirectional soft contrastive hashing module that constructs fine-grained soft pairs using label overlap rather than hard positives/negatives. The method effectively enhances robustness and retrieval accuracy in noisy multi-label cross-modal settings, achieving consistent improvements over 12 state-of-the-art methods across four widely used benchmarks.

## Method Summary
The SCBCH framework combines Cross-modal Semantic-Consistent Classification (CSCC) and Bidirectional Soft Contrastive Hashing (BSCH) to handle noisy multi-label cross-modal retrieval. CSCC constructs soft labels through cross-modal neighbor aggregation and applies confidence-weighted loss to mitigate noisy sample impact. BSCH builds soft positive/negative pairs using label overlap similarity and employs bidirectional contrastive loss to enhance retrieval quality. The framework uses frozen VGG19 and Doc2Vec features, with FC layers for encoding, and is trained with Adam optimizer (lr=1e-4) for 50 epochs with a warm-up phase.

## Key Results
- Consistently outperforms 12 state-of-the-art methods across all benchmarks
- Achieves up to 2.3% MAP improvement at 80% noise rate
- Demonstrates robust performance across 20%, 50%, and 80% noise levels on NUS-WIDE, MS-COCO, MIRFlickr-25K, and IAPR TC-12 datasets
- Shows superior precision-recall curves compared to baselines

## Why This Works (Mechanism)
The method works by addressing the core challenge of noisy labels through two complementary approaches. CSCC mitigates noise by leveraging cross-modal semantic consistency - it aggregates information from neighbors across modalities to construct more reliable soft labels and assigns confidence weights based on neighbor reliability. BSCH enhances retrieval quality by using soft pairs based on label overlap rather than binary positive/negative assignments, allowing for more nuanced similarity relationships. The bidirectional contrastive loss ensures that both modalities contribute equally to learning robust hash functions that can handle noise effectively.

## Foundational Learning
- **Cross-modal retrieval fundamentals**: Understanding how to map different modalities to a common space for retrieval tasks - needed to grasp the core problem SCBCH addresses
- **Contrastive learning principles**: Knowledge of how contrastive losses work to pull similar samples together and push dissimilar ones apart - needed to understand BSCH's mechanism
- **Hash function optimization**: Understanding binary code learning and quantization - needed to follow the regularization components
- **Label noise handling**: Familiarity with techniques for dealing with noisy supervision - needed to appreciate CSCC's contribution
- **Quick check**: Can you explain how cross-modal consistency helps identify noisy labels?

## Architecture Onboarding

**Component map**: Image/Text Features -> Encoders (3/2 FC layers) -> CSCC Module -> BSCH Module -> Hash Functions -> Retrieval

**Critical path**: Feature extraction → CSCC soft label construction → Confidence weighting → BSCH soft pair construction → Bidirectional contrastive loss → Final hash codes

**Design tradeoffs**: The method trades computational complexity (neighbor aggregation, soft pair construction) for improved noise robustness. Using frozen backbones ensures fair comparison but limits feature adaptation. The soft pair approach provides more nuanced similarity learning but requires careful hyperparameter tuning.

**Failure signatures**: 
- Poor MAP scores despite training completion suggests issues with neighbor aggregation or confidence weighting
- Mode collapse in hash codes indicates imbalance between attraction/repulsion losses
- Noisy labels not being properly downweighted suggests incorrect confidence weight computation

**Exactly 3 first experiments**:
1. Verify neighbor aggregation produces sensible soft labels by visualizing cosine similarity distributions
2. Check confidence weight separation between clean and noisy samples after warm-up
3. Validate Jaccard similarity computation for soft pair construction matches Figure 3 description

## Open Questions the Paper Calls Out
- **Feature adaptation**: Can SCBCH maintain its robustness if the training strategy is shifted from frozen backbones to end-to-end fine-tuning? The current frozen backbone approach prevents adaptation to specific noise distributions.
- **Noise type generalization**: How does CSCC performance degrade under instance-dependent or asymmetric noise compared to symmetric noise? The neighbor consensus mechanism may reinforce errors under systematic mislabeling.
- **Feature space alignment**: Is the CSCC module sensitive to the initial alignment of the feature space? Poor cross-modal neighbor quality could misguide the soft label construction.

## Limitations
- Missing critical hyperparameters (warm-up epochs, neighbor count, repulsion margin) limit reproducibility
- Relies on frozen backbones which may not capture optimal feature representations for the hashing task
- Only evaluates symmetric label noise, leaving performance under more realistic noise patterns uncertain
- Limited ablation study scope prevents full understanding of module contributions

## Confidence
- **High confidence**: Core methodological contributions (CSCC and BSCH modules) are clearly described with explicit equations and are technically sound
- **Medium confidence**: Experimental setup (datasets, metrics, backbone architectures) is well-specified; reported improvements over baselines are plausible
- **Low confidence**: Exact noise injection implementation, critical hyperparameters, and specific comparison methods and configurations are insufficiently detailed

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary E_w (warm-up epochs), |N_i| (neighbor count), and m (repulsion margin) to identify optimal combinations and assess robustness
2. **Ablation study validation**: Replicate the ablation results (Table 2) to confirm individual contributions of CSCC and BSCH modules under different noise levels
3. **Cross-dataset generalization test**: Evaluate SCBCH on an additional cross-modal retrieval dataset (e.g., Wikipedia) not used in original experiments to verify claimed robustness across diverse data distributions