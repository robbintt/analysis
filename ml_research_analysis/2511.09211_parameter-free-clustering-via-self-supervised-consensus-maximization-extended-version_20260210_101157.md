---
ver: rpa2
title: Parameter-Free Clustering via Self-Supervised Consensus Maximization (Extended
  Version)
arxiv_id: '2511.09211'
source_url: https://arxiv.org/abs/2511.09211
tags:
- clustering
- cluster
- number
- clusters
- scmax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SCMax, a fully parameter-free clustering framework
  that performs hierarchical agglomerative clustering and cluster evaluation in an
  integrated process. The method uses a nearest neighbor consensus score based on
  self-supervised learning to automatically determine the optimal number of clusters.
---

# Parameter-Free Clustering via Self-Supervised Consensus Maximization (Extended Version)

## Quick Facts
- arXiv ID: 2511.09211
- Source URL: https://arxiv.org/abs/2511.09211
- Reference count: 11
- Parameter-free clustering framework using self-supervised consensus maximization

## Executive Summary
This paper introduces SCMax, a fully parameter-free clustering framework that integrates hierarchical agglomerative clustering with cluster evaluation through a self-supervised consensus maximization approach. The method automatically determines the optimal number of clusters by measuring agreement between original and self-supervised representations, eliminating the need for hyperparameters like cluster count or distance thresholds. SCMax generates cluster candidates using nearest neighbor merging and evaluates them using a consensus score based on self-supervised learning.

Experimental results on eight datasets demonstrate that SCMax outperforms existing non-K clustering methods, achieving state-of-the-art performance across accuracy, NMI, purity, and F-score metrics. The framework shows superior adaptability and robustness in real-world scenarios by automatically adapting to the underlying data structure without requiring parameter tuning.

## Method Summary
SCMax operates through a hierarchical agglomerative clustering process that generates cluster candidates by merging nearest neighbors iteratively. At each merging step, the method evaluates the quality of the resulting clusters using a self-supervised consensus score that measures agreement between original data representations and those obtained through a self-supervised learning process. This consensus score serves as an intrinsic quality metric, allowing the algorithm to automatically determine the optimal stopping point in the clustering hierarchy without requiring any predefined number of clusters or distance thresholds. The approach effectively integrates clustering and evaluation into a single process, where the self-supervised component ensures that clusters capture meaningful structure in the data while maintaining full parameter freedom.

## Key Results
- Outperforms existing non-K clustering methods across multiple benchmark datasets
- Achieves state-of-the-art performance in accuracy, NMI, purity, and F-score metrics
- Successfully eliminates reliance on hyperparameters like number of clusters or distance thresholds
- Demonstrates superior adaptability and robustness across diverse real-world scenarios

## Why This Works (Mechanism)
The method works by leveraging self-supervised learning to create an intrinsic evaluation mechanism for clustering quality. By comparing agreement between original and self-supervised representations, the algorithm can assess whether merging clusters preserves meaningful data structure. The hierarchical agglomerative approach ensures that all possible cluster configurations are explored, while the consensus score automatically identifies the optimal stopping point. This integration of clustering and evaluation through self-supervision eliminates the need for external validation metrics or parameter tuning, making the method truly parameter-free.

## Foundational Learning

1. **Hierarchical Agglomerative Clustering**: Why needed - Provides a systematic way to explore all possible cluster configurations from fine-grained to coarse groupings. Quick check - Verify the merging order follows nearest neighbor proximity rules.

2. **Self-Supervised Learning**: Why needed - Generates representations that capture intrinsic data structure without external labels. Quick check - Ensure the self-supervised objective preserves meaningful cluster boundaries.

3. **Consensus Maximization**: Why needed - Quantifies agreement between different representations to assess cluster quality. Quick check - Verify consensus scores correlate with ground truth cluster structure.

4. **Parameter-Free Optimization**: Why needed - Eliminates need for manual hyperparameter tuning in clustering. Quick check - Confirm no external parameters are required beyond algorithm choices.

## Architecture Onboarding

Component Map: Data -> Nearest Neighbor Graph -> Hierarchical Merging -> Consensus Score -> Optimal Clusters

Critical Path: The algorithm begins by constructing a nearest neighbor graph from the data, then performs hierarchical merging starting from individual points. At each merge, it computes a consensus score between original and self-supervised representations. The process continues until the consensus score indicates optimal clustering structure, at which point the current partition is returned as the final result.

Design Tradeoffs: The method trades computational efficiency for parameter freedom - the hierarchical approach explores all possible clusterings but at higher computational cost than single-pass methods. The self-supervised component adds robustness but requires careful design to ensure meaningful representations. The nearest neighbor consensus approach is intuitive but may struggle with complex cluster geometries.

Failure Signatures: Poor performance may occur when: (1) nearest neighbors don't reflect true cluster structure, (2) self-supervised representations fail to capture meaningful data patterns, (3) the consensus score becomes unreliable for high-dimensional or noisy data, or (4) the hierarchical merging order doesn't align with natural cluster boundaries.

First Experiments: 1) Test on simple synthetic datasets with known cluster structure to verify basic functionality. 2) Evaluate sensitivity to different self-supervised learning objectives. 3) Compare performance with varying nearest neighbor k values to assess robustness.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided context.

## Limitations
- Potential overfitting to benchmark datasets may limit generalizability to real-world applications
- Computational scalability concerns with nearest neighbor consensus approach on large-scale datasets
- Limited effectiveness for datasets with complex cluster structures or varying densities due to hierarchical agglomerative clustering approach
- Claim of being "fully parameter-free" requires scrutiny as algorithmic choices may implicitly depend on parameters

## Confidence
High: The experimental results demonstrate superior performance across multiple metrics on diverse datasets
Medium: The method's effectiveness on complex, real-world datasets remains to be thoroughly validated
Low: Long-term scalability and robustness across diverse data distributions needs further investigation

## Next Checks
1. Conduct experiments on additional datasets with varying characteristics (e.g., high-dimensional data, imbalanced classes) to assess the method's adaptability
2. Perform ablation studies to quantify the contribution of the self-supervised learning component and nearest neighbor consensus score to the overall performance
3. Compare SCMax with recent deep clustering approaches to establish its position relative to state-of-the-art methods in the field