---
ver: rpa2
title: Optimal Robust Recourse with $L^p$-Bounded Model Change
arxiv_id: '2509.21293'
source_url: https://arxiv.org/abs/2509.21293
tags:
- recourse
- validity
- dataset
- cost
- implementation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of computing optimal robust algorithmic\
  \ recourse when models may change. The authors develop a new algorithm that computes\
  \ optimal recourse under Lp-bounded model changes (p\u22651, p\u2260\u221E) for\
  \ generalized linear models."
---

# Optimal Robust Recourse with $L^p$-Bounded Model Change

## Quick Facts
- arXiv ID: 2509.21293
- Source URL: https://arxiv.org/abs/2509.21293
- Reference count: 40
- Provides first provably optimal solution for Lp-bounded model changes (p≥1, p≠∞) beyond the L∞ case

## Executive Summary
This paper addresses the problem of computing optimal robust algorithmic recourse when models may change under Lp-bounded perturbations. The authors develop a new algorithm that achieves optimal recourse by solving O(d) convex optimization problems, where d is the number of features. The key insight is that for generalized linear models, the optimal adversarial model differs from the initial model in exactly one dimension by exactly α. Experiments on real-world datasets demonstrate significantly lower recourse prices (up to several orders of magnitude) compared to prior work, better trade-offs between implementation cost and validity, and improved sparsity.

## Method Summary
The approach involves linearizing the initial model at the rejection point using LIME or SmoothGrad, then solving 2d convex optimization problems for each candidate adversarial model in the set Θ±(θ₀) (models differing in exactly one dimension by ±α). For each candidate, the algorithm solves a constrained convex problem using projected subgradient descent. The final recourse is selected as the minimum J(x,θ*) across all candidates. For neural networks, local linearization is used as an approximation. The method provides theoretical optimality guarantees for generalized linear models and strong empirical performance on real-world datasets.

## Key Results
- Algorithm achieves significantly lower recourse prices (up to several orders of magnitude) compared to prior work
- Provides better trade-offs between implementation cost and validity than existing approaches
- Produces more sparse recourses with fewer features changed
- Remains resilient to post-processing for feasibility despite optimality guarantees

## Why This Works (Mechanism)

### Mechanism 1: Single-Dimension Adversarial Model Characterization
The optimal adversarial model for any recourse x under Lp-bounded changes (p≥1, p≠∞) differs from θ₀ in exactly one dimension by exactly ±α. The adversarial objective maximizes the loss by minimizing θᵀx, and the Lp-ball constraint forces all "budget" onto the dimension where |x[i]| is largest. This structural result relies on the model being a generalized linear model with non-decreasing link function.

### Mechanism 2: Min-Max Reduction via Enumerated Adversarial Candidates
Solving the robust recourse problem reduces to 2d convex optimizations, one for each candidate adversarial model in Θ±(θ₀). By proving that the inner maximization over θ ∈ Θᵖα(θ₀) is achieved by one of these 2d candidates, the problem becomes tractable. For each fixed θ', solving a convex objective subject to linear constraints via projected subgradient descent converges to the optimal solution.

### Mechanism 3: Local Linearization for Non-Linear Models
For neural networks, locally approximating fθ₀ as linear around x₀ preserves reasonable empirical performance despite theoretical optimality being guaranteed only for GLMs. The surrogate linear model captures the local decision boundary sufficiently for the algorithm to work effectively, though optimality is not guaranteed in this case.

## Foundational Learning

### Lp norms and uncertainty sets
Why needed here: Defines the neighborhood Θᵖα(θ₀); understanding why L∞ allows per-dimension perturbations of size α while Lp (p<∞) constrains total perturbation budget is essential to grasp why the adversarial model concentrates change in one dimension.
Quick check question: For α = 0.5, d = 3, what is the difference between Θ∞α (L∞ ball) and Θ₁α (L1 ball)? Which admits more models that differ in all dimensions simultaneously?

### Min-max (robust) optimization
Why needed here: The robust recourse problem is a min-max formulation (min over recourse, max over adversarial model). Understanding saddle-point structure and why reducing inner max to a finite candidate set makes the problem tractable is central.
Quick check question: If you can prove the inner max is always achieved at one of finitely many extreme points, what does that imply for solving the min-max problem?

### Generalized Linear Models (GLMs)
Why needed here: Theoretical guarantees only hold for GLMs (linear predictor + monotonic link). Recognizing how logistic regression and similar models fit this class clarifies scope and limitations.
Quick check question: Is a feedforward neural network with ReLU activations a GLM? If not, what approximation does the algorithm use to handle it?

## Architecture Onboarding

### Component map
Preprocessor (LIME/SmoothGrad) -> Candidate generator (Θ± set) -> Optimization core (2d convex problems) -> Selector (minimum J) -> Postprocessor (feasibility projection)

### Critical path
1. Linearization quality at x₀ → determines fidelity of adversarial candidate set
2. Correct constraint encoding for θ' ∈ θ*(x) → ensures solved recourse actually faces θ' as worst-case
3. Numerical stability of projected subgradient descent across 2d problems → determines convergence and runtime

### Design tradeoffs
- L1 vs L∞ norm for model change: L1 yields lower price but requires 2d convex solves; L∞ uses greedy coordinate descent, often faster but with higher price
- Runtime vs optimality: Algorithm 1 has polynomial runtime in d (empirically ~188s for German Credit LR, ~667s for SBA LR) vs sub-second for Algorithm 2
- Post-processing for feasibility: Hardmax may degrade validity at high implementation cost regimes; trade-off may require re-tuning λ

### Failure signatures
- Recourse price unexpectedly high: Likely mis-specified α (too large), or linearization poor for non-linear boundary
- Validity drops after post-processing: Categorical or actionability constraints conflict with robust optimum; consider re-optimizing with constraints or reducing λ
- Runtime blow-up: Very high dimensionality d makes 2d optimizations expensive; consider dimension reduction or switching to L∞/greedy algorithm

### First 3 experiments
1. Replicate Table 1 on German Credit with LR: Compare price of recourse for Algorithm 1 vs ROAR (L1) vs Algorithm 2 vs ROAR (L∞) across α ∈ {0.1, 0.5} and λ ∈ {0.01, 0.1}. Verify Algorithm 1 achieves lowest price.
2. Validity-cost frontier reproduction: Generate plots matching Figure 1 for Small Business dataset (LR, α = 0.5), varying λ and plotting instance-wise validity vs implementation cost. Check that Algorithm 1 Pareto-dominates baselines.
3. Feasibility post-processing robustness: Apply hardmax post-processing to categorical features and compare validity-cost curves (as in Figure 4). Quantify validity drop at high cost regimes and assess whether reducing λ recovers performance.

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed algorithm be extended to guarantee optimal robust recourse while strictly enforcing feature actionability and feasibility constraints within the optimization process? The current method handles feasibility via projection after optimization, which voids optimality guarantees. A modified algorithm incorporating linear feasibility constraints directly into convex optimization sub-problems with maintained polynomial time complexity would resolve this.

### Open Question 2
How can the theoretical price of recourse be minimized using alternative robustness frameworks, such as distributionally robust optimization or beyond-worst-case analysis, rather than $L^p$-bounded model shifts? The current work focuses exclusively on $L^p$-bounded adversarial model changes, which assumes a specific geometric constraint on model shifts that may be overly pessimistic or distinct from real-world distributional shifts.

### Open Question 3
Is it possible to compute provably optimal robust recourse when model changes are defined by naturally occurring shifts or training condition perturbations rather than $L^p$ norms? The current algorithm relies on geometric properties of $L^p$ norms to decompose the problem; it is unclear if this strategy translates to model uncertainty sets defined by data shifts or Rashomon sets.

## Limitations
- Theoretical scope limited to generalized linear models; neural network performance depends on local linearization quality
- Algorithm 1 runtime scales with dimensionality (2d convex problems per instance), becoming expensive for high-dimensional data
- Post-processing for feasibility can significantly degrade validity at high implementation cost regimes

## Confidence

**High confidence:**
- Single-dimension adversarial model characterization for GLMs
- Min-max reduction mechanism and optimality guarantees for Algorithm 1

**Medium confidence:**
- Local linearization approach for neural networks and its empirical effectiveness

**Low confidence:**
- Runtime scalability claims for very high-dimensional data
- Post-processing robustness across diverse constraint types

## Next Checks

1. **Linearization sensitivity analysis:** Systematically vary LIME/SmoothGrad parameters (perturbation count, kernel width) for neural networks and measure impact on recourse price and validity.

2. **Constraint interaction study:** Quantify validity drop after post-processing across different λ values and constraint types (categorical vs actionability vs application-specific).

3. **Scalability benchmark:** Profile Algorithm 1 runtime as a function of d (number of features) on synthetic datasets with varying dimensionality, confirming the polynomial scaling claims.