---
ver: rpa2
title: 'Nonconvex Penalized LAD Estimation in Partial Linear Models with DNNs: Asymptotic
  Analysis and Proximal Algorithms'
arxiv_id: '2511.21115'
source_url: https://arxiv.org/abs/2511.21115
tags:
- function
- holds
- definition
- theorem
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of estimating partial linear models
  (PLMs) using deep neural networks (DNNs) under least absolute deviation (LAD) regression.
  The key innovation is to parametrize the nonparametric component of PLMs with sparse
  DNNs and to introduce a nonconvex, nonsmooth penalty for robustness.
---

# Nonconvex Penalized LAD Estimation in Partial Linear Models with DNNs: Asymptotic Analysis and Proximal Algorithms

## Quick Facts
- **arXiv ID:** 2511.21115
- **Source URL:** https://arxiv.org/abs/2511.21115
- **Reference count:** 40
- **Primary result:** Robust LAD estimation for PLMs with sparse DNNs achieves consistency, convergence rates, and asymptotic normality while maintaining computational tractability via proximal algorithms.

## Executive Summary
This paper develops a robust estimation framework for Partial Linear Models (PLMs) using deep neural networks under Least Absolute Deviation (LAD) regression. The key innovation is parametrizing the nonparametric component with sparse DNNs and introducing a nonconvex, nonsmooth penalty for robustness. The authors establish theoretical guarantees including consistency, convergence rates, and asymptotic normality of the parametric estimator, while addressing the computational challenges through two formulations: a continuous relaxation enabling cheap proximal updates and a primal formulation maintaining statistical accuracy. Global convergence of a proximal stochastic subgradient method is proven using tools from differential geometry and optimization theory.

## Method Summary
The method estimates PLMs of the form Y = βᵀX + g(Z) + ε using sparse DNNs for the nonparametric component g(Z). A penalized LAD objective is minimized using proximal stochastic subgradient descent. Two formulations are analyzed: a relaxed version with a smoothed ℓ₀ approximation enabling closed-form box projections, and a primal version with exact ℓ₀ constraints requiring sorting-based hard thresholding. Network depth and sparsity scale with sample size to balance approximation and estimation error. Theoretical guarantees include consistency, convergence rates, and asymptotic normality of the linear component estimator.

## Key Results
- Established consistency and convergence rates for the LAD-DNN estimator in PLMs under Hölder smoothness assumptions
- Proved asymptotic normality of the parametric component β using infinite-dimensional variational analysis
- Derived closed-form proximal updates for the relaxed formulation, enabling tractable optimization
- Proved global convergence of proximal stochastic subgradient methods to critical points using o-minimal geometry

## Why This Works (Mechanism)

### Mechanism 1: Robustness via LAD and Variational Analysis
Replacing LSE with LAD combined with Mordukhovich subgradients and limiting subdifferentials preserves asymptotic normality under heavy-tailed errors. The non-differentiability of LAD requires subgradient calculus, with the key assumption being a continuous error density with strictly positive derivative at zero. This creates a robust causal pathway from data to estimator that dampens large error influence compared to quadratic loss.

### Mechanism 2: Expanding Sparse DNNs to Mitigate Dimensionality
Scaling network depth L=O(log N) and sparsity s=O(Nr_N²log N) with sample size allows approximating Hölder smooth functions without curse of dimensionality. The expansion reduces approximation error while sparsity constraints prevent estimation error from exploding, with consistency relying on the network being sparse enough to be "tamed" by sample size.

### Mechanism 3: Tractable Global Convergence via O-Minimal Geometry
Proximal stochastic subgradient methods converge globally to critical points in nonconvex, nonsmooth settings because the problem structure is "tame" (definable in o-minimal structure). The Weak Sard Property ensures limiting critical values are dense, and Whitney stratification prevents gradient trajectories from getting stuck in pathological non-smooth sets, enabling use of the relaxed continuous penalty with cheap closed-form proximal updates.

## Foundational Learning

- **Partial Linear Models (PLM):** Splitting the world into "Linear things we know" (βᵀX) and "Nonlinear things we don't" (g(Z)). Quick check: If you put all variables X and Z into a "black box" neural network, which statistical property of β would you potentially lose? (Answer: Interpretability/Consistency of the linear parametric component).

- **Subgradients & Variational Analysis:** LAD loss |·| is non-differentiable at 0, requiring "sets of slopes" (subgradients) instead of standard gradients. Quick check: At x=0 for f(x) = |x|, what is the subgradient? (Answer: The interval [-1, 1]).

- **O-Minimal Structures & Stratification:** The "magic trick" for optimization proof ensures high-dimensional nonconvex landscape is made of "nice" pieces glued together, preventing infinite oscillation. Quick check: Why is a piecewise linear function (like ReLU) considered "geometrically tame"? (Answer: Because it can be decomposed into finitely many smooth manifolds).

## Architecture Onboarding

- **Component map:** Input Layer (X, Z) -> Prediction Head (βᵀX + Sparse DNN(g(W; Z))) -> Loss Function (LAD + Nonconvex Penalty) -> Optimizer (Proximal Stochastic Subgradient Descent)

- **Critical path:**
  1. Forward Pass: Calculate residual r_i = Y_i - (βX_i + DNN(Z_i))
  2. Subgradient Calc: Determine subgradient of |r_i| (sign of residual)
  3. Proximal Update: Apply specific projection
     - *Relaxed Mode:* Project weights onto box constraints [-1, 1] (Cheap)
     - *Primal Mode:* Project weights onto sparse set (Requires sorting/Top-K selection)

- **Design tradeoffs:**
  - *Relaxed:* Uses smooth approximation f_σ of ℓ₀ norm. Pro: Projections are simple box constraints (computationally cheap). Con: Only approximates true sparse statistical problem.
  - *Primal:* Uses exact ℓ₀ constraint. Pro: Statistical accuracy is higher. Con: Requires sorting all weights to find "top-s" (computationally expensive, O(H log H)).

- **Failure signatures:**
  - Exploding Gradients/Weights: If penalty weight λ_N is too small relative to learning rate, bounded weight constraints might be constantly saturated.
  - Stalling: If relaxation parameter σ decays too fast, smoothed objective becomes non-smooth too quickly, potentially stalling optimizer in non-critical region.

- **First 3 experiments:**
  1. Sanity Check (Synthetic): Generate data from known PLM with Cauchy-distributed errors. Compare LAD-DNN vs. LSE-DNN. Verify if LAD coefficients remain stable while LSE diverges.
  2. Scaling Law Test: Run optimization on increasing N (e.g., 1k, 10k, 100k). Scale Network Width/Depth according to Assumption A4. Plot estimation error ||β̂ - β₀|| vs N to verify convergence rate.
  3. Algorithm Benchmark: Compare "Runtime per epoch" of Relaxed (Box Projection) vs. Primal (Sorting) methods on fixed large dataset to quantify computational vs. statistical tradeoff.

## Open Questions the Paper Calls Out

- **Unified Algorithm Bridge:** Can a unified or adaptive algorithm be developed to bridge the gap between the statistical fidelity of the primal formulation and the computational tractability of the continuous relaxation? The paper analyzes two formulations separately, representing two ends of a spectrum between computational efficiency and statistical accuracy.

- **Non-Separable Regularization:** Can asymptotic normality of β̂_N be established for non-separable regularization terms J_{N,M}(β, g)? The proof of asymptotic normality relies critically on the additive structure of the regularizer for decomposing subgradients.

- **Saddle Point Avoidance:** What specific geometric assumptions are required to guarantee that the proximal stochastic subgradient method avoids strict saddle points for the penalized LAD objective? While global convergence to critical points is proven, confirming avoidance of unstable saddle points requires more detailed characterization of the objective function's landscape.

## Limitations
- The convergence proof for the proximal stochastic subgradient method relies heavily on o-minimal geometry assumptions that are stated but not empirically verified
- Practical implementation details (learning rates, penalty schedules, initialization strategies) are not specified in the provided text
- Computational efficiency gains from the relaxed formulation are analytically established but lack experimental validation against the primal approach

## Confidence
- **High Confidence:** The LAD framework for robustness is well-established in statistical literature. The network expansion argument follows established approximation theory for DNNs.
- **Medium Confidence:** The global convergence proof using o-minimal structures is theoretically sound but requires careful verification of problem definability in practice. Closed-form proximal updates are mathematically correct but may have numerical stability issues.
- **Low Confidence:** Specific growth rates for network expansion (depth L=O(log N), sparsity s=O(Nr_N²log N)) may be overly conservative or aggressive depending on the function class.

## Next Checks
1. **Empirical Robustness Test:** Generate synthetic PLM data with heavy-tailed (e.g., Cauchy) errors and compare estimation error of LAD-DNN vs. LSE-DNN across varying outlier proportions.

2. **Scaling Law Verification:** Implement the algorithm with automated network scaling (depth L and sparsity s as functions of N). Plot estimation error vs. sample size on Hölder smooth functions to empirically verify Theorem 1 convergence rates.

3. **Algorithm Efficiency Benchmark:** Time the proximal updates for both relaxed (box projection) and primal (sorting-based) formulations on datasets of increasing size to quantify the claimed computational advantage.