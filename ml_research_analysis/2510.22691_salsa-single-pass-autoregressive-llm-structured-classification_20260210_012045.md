---
ver: rpa2
title: 'SALSA: Single-pass Autoregressive LLM Structured Classification'
arxiv_id: '2510.22691'
source_url: https://arxiv.org/abs/2510.22691
tags:
- salsa
- classification
- language
- tasks
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SALSA addresses the gap in text classification performance when
  using open-ended generative LLMs, which often underperform compared to specialized
  discriminative models despite their strong generalization capabilities. The method
  introduces a single-pass autoregressive framework that maps each class to a distinct
  output token, constructs structured prompts to elicit single-token responses, and
  projects model logits only onto the relevant class tokens during inference.
---

# SALSA: Single-pass Autoregressive LLM Structured Classification

## Quick Facts
- arXiv ID: 2510.22691
- Source URL: https://arxiv.org/abs/2510.22691
- Authors: Ruslan Berdichevsky; Shai Nahum-Gefen; Elad Ben Zaken
- Reference count: 37
- Primary result: Achieves 92.7% accuracy on GLUE, outperforming T5-11B, XLNet, RoBERTa, and ALBERT

## Executive Summary
SALSA introduces a single-pass autoregressive framework for text classification using decoder-only LLMs, addressing the gap where open-ended generative models underperform compared to specialized classifiers. The method maps each class to a distinct output token, constructs structured prompts to elicit single-token responses, and projects model logits only onto the relevant class tokens during inference. This design enables efficient classification in one forward pass without autoregressive generation overhead. SALSA achieves state-of-the-art results across multiple benchmarks and demonstrates strong performance on domain-specific tasks while supporting both zero-shot and few-shot settings.

## Method Summary
SALSA reformulates text classification as a structured generation task where each class label maps to a unique output token. During inference, the model generates a response prefix followed by a placeholder token, and classification is performed by extracting logits only at this placeholder position. The logits are filtered to include only the class-mapped tokens before applying softmax. The method uses LoRA fine-tuning with a custom loss function that computes cross-entropy only on the filtered class logits. This approach enables efficient single-pass classification while maintaining the generalization capabilities of LLMs. SALSA supports both zero-shot and few-shot settings and can be extended to regression tasks via discrete scalar value mapping.

## Key Results
- Achieves 92.7% accuracy on GLUE benchmark, outperforming T5-11B, XLNet, RoBERTa, and ALBERT
- Demonstrates strong performance on domain-specific tasks: 91.3% on MedNLI, 74.1% on MedMCQA, and 71.8% on HateXplain
- Shows faster convergence and better generalization compared to vanilla fine-tuning
- Supports zero-shot and few-shot settings with competitive performance

## Why This Works (Mechanism)
SALSA works by leveraging the autoregressive nature of decoder-only LLMs while constraining them to produce single-token outputs for classification. By mapping classes to distinct tokens and projecting logits only at a designated placeholder position, the method effectively transforms the generation problem into a classification task. The structured prompt guides the model to produce the desired output format, while the logit filtering ensures that only relevant class probabilities are considered. This approach combines the strong generalization capabilities of LLMs with the efficiency of traditional classifiers, achieving high performance without the computational overhead of full autoregressive generation.

## Foundational Learning
- **Autoregressive generation in LLMs**: LLMs generate text token-by-token, predicting the next token based on previous context. Needed to understand how SALSA constrains this process for classification. Quick check: Verify that the model generates one token at a time during inference.
- **LoRA fine-tuning**: Low-Rank Adaptation technique that adds small trainable matrices to existing model weights, enabling efficient fine-tuning. Needed to understand how SALSA adapts large models with minimal parameters. Quick check: Confirm that only ~103M parameters are being trained.
- **Prompt engineering for classification**: Designing prompts that elicit specific output formats from LLMs. Needed to understand how SALSA structures prompts for single-token responses. Quick check: Verify the prompt template includes class mapping and response prefix.
- **Logit projection and filtering**: Extracting and manipulating model logits at specific positions. Needed to understand how SALSA isolates class probabilities. Quick check: Confirm that only placeholder position logits are used for classification.
- **Cross-entropy loss with filtered logits**: Computing loss only on relevant output classes. Needed to understand SALSA's custom training objective. Quick check: Verify that the loss function masks non-class logits during training.

## Architecture Onboarding

**Component Map**
Prompt Template -> Tokenizer -> LLM with LoRA -> Logit Projection -> Filtered Softmax -> Classification

**Critical Path**
Input text → Prompt construction → Tokenization → Forward pass through LLM → Logit extraction at placeholder → Filter to class tokens → Softmax → Classification

**Design Tradeoffs**
- Single-pass vs. autoregressive generation: SALSA trades the flexibility of full generation for computational efficiency
- Token-based vs. embedding-based classification: Uses discrete tokens rather than learned embeddings for class representation
- LoRA fine-tuning scope: Balances parameter efficiency with model adaptation capability
- Prompt complexity vs. performance: More structured prompts may improve accuracy but increase design complexity

**Failure Signatures**
- Performance drops when label tokens contradict semantic priors (zero-shot sensitivity)
- Overfitting occurs rapidly on small datasets (convergence at 2-3 epochs)
- Incorrect logit filtering results in lower accuracy and inefficient training
- Parameter count discrepancies suggest incorrect LoRA configuration

**3 First Experiments**
1. Implement prompt template and verify placeholder token identification during tokenization
2. Test logit filtering mechanism by extracting and visualizing logits at placeholder position
3. Conduct ablation study on label token semantics (semantic "Yes/No" vs arbitrary "0/1") in zero-shot mode

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can systematic prompt optimization methods (e.g., automated search, gradient-based tuning) further improve SALSA's zero-shot and few-shot performance beyond empirical manual design?
- Basis in paper: The conclusion states: "Future directions include systematic prompt optimization," and notes that "prompt design remains partly empirical."
- Why unresolved: The ablation study (Table 5) shows zero-shot accuracy varies dramatically (44.7%–91.3%) depending on label-token mapping, but no automated optimization method was explored.
- What evidence would resolve it: A comparison of automated prompt/label-mapping optimization techniques against manual design across multiple benchmarks, measuring zero-shot performance gains.

**Open Question 2**
- Question: How does SALSA's multi-label and multi-task extension (using sigmoid thresholds and multiple placeholders) perform compared to specialized multi-label classifiers?
- Basis in paper: Appendix A.7 proposes replacing softmax with sigmoid for multi-label and using multi-placeholder prompts for multi-task, but states these are "possible extensions" without evaluation.
- Why unresolved: Figure 4 illustrates a two-token pipeline conceptually, but no experimental results or comparisons are provided for these extended settings.
- What evidence would resolve it: Benchmark results on standard multi-label datasets (e.g., Reuters-21578) and multi-task setups comparing SALSA's extensions against established baselines.

**Open Question 3**
- Question: Does SALSA's single-token constraint limit scalability for classification tasks with large label spaces (e.g., hundreds or thousands of classes)?
- Basis in paper: The method maps "each class label to a distinct output token," which could become infeasible as vocabulary constraints and prompt complexity grow with label cardinality.
- Why unresolved: All evaluated datasets have small-to-moderate class counts (2–5 classes); no experiments address high-cardinality scenarios.
- What evidence would resolve it: Experiments on large-label datasets (e.g., Amazon-5 with hierarchical categories or patent classification) analyzing performance degradation and computational overhead as class count increases.

## Limitations
- Critical uncertainty in LoRA configuration: The stated 103M trainable parameters conflict with expected ~20-40M for attention-only LoRA, requiring exact module specification
- Tokenization handling details are implicit, potentially affecting reproducibility
- Zero-shot performance highly sensitive to label-token alignment (semantic vs. arbitrary tokens)
- No evaluation on high-cardinality classification tasks (hundreds/thousands of classes)

## Confidence
- **High Confidence**: The autoregressive single-pass design, prompt template structure, and logit projection strategy are explicitly detailed and directly implementable
- **Medium Confidence**: Performance claims depend critically on exact LoRA configuration; incorrect module targeting may affect results
- **Medium Confidence**: Few-shot and zero-shot capabilities are plausible but label-token alignment sensitivity could impact outcomes

## Next Checks
1. Verify LoRA scope by replicating the exact adapter setup (which linear layers: QKV, MLP, gate, etc.) to confirm the stated 103M trainable parameters and compare with baseline configurations
2. Conduct controlled ablation varying class token semantics (e.g., "Yes"/"No" vs "0"/"1") in zero-shot mode to measure sensitivity to label-token alignment
3. On small datasets (e.g., RTE), track validation accuracy per epoch and halt training at epoch 2-3 if performance plateaus or degrades, validating rapid convergence and overfitting risk claims