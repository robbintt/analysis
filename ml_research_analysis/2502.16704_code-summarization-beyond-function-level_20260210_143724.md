---
ver: rpa2
title: Code Summarization Beyond Function Level
arxiv_id: '2502.16704'
source_url: https://arxiv.org/abs/2502.16704
tags:
- code
- summarization
- context
- arxiv
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Code Summarization Beyond Function Level

## Quick Facts
- arXiv ID: 2502.16704
- Source URL: https://arxiv.org/abs/2502.16704
- Reference count: 35
- Primary result: Incorporating few-shot examples and RAG-retrieved chunks significantly improves LLM code summarization at class and repository levels.

## Executive Summary
This paper advances code summarization beyond function-level by exploring class-level and repository-level contexts. It introduces a framework that leverages few-shot examples, class skeletons, and retrieval-augmented generation (RAG) to enhance the quality of code summaries. The study demonstrates that few-shot learning is crucial for guiding LLMs toward concise and aligned summaries, while skeleton context outperforms full class code by reducing noise. The findings suggest that combining few-shot examples with RAG-retrieved chunks yields the best performance, particularly for repository-level summarization.

## Method Summary
The method involves generating concise summaries (1-3 sentences) for Python functions using additional context such as class code, class skeleton, or repository chunks. The approach compares fine-tuned baselines (CodeT5+, CodeTrans) with LLMs (DeepSeek Coder, StarCoder2, Llama3) using few-shot examples and RAG. RAG retrieves top-K code chunks via FAISS similarity search, and few-shot examples guide the LLM to produce concise, style-aligned summaries. The evaluation uses metrics like BLEU-4, ROUGE-L, METEOR, BERTScore-F1, BLEURT, and SIDE to assess summary quality.

## Key Results
- Few-shot examples significantly improve LLM summarization conciseness and alignment, reducing overly explanatory outputs.
- Skeleton context (class blueprint without implementation) outperforms full class code by reducing noise.
- Naive RAG with code chunk retrieval augments repository-level summarization when combined with few-shot examples, but is less effective alone.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot examples guide LLMs toward concise, style-aligned summaries, reducing verbose output tendencies.
- Mechanism: Demonstrations condition the model on expected output format and brevity through in-context learning, providing implicit constraints on length and style.
- Core assumption: Few-shot examples are representative of the target distribution and quality; model attends to and generalizes from them.
- Evidence anchors:
  - [abstract] "incorporating few-shot learning and retrieved code chunks from RAG significantly enhanced the performance of LLMs"
  - [section IV] "Including few-shot examples is crucial...guiding LLMs to produce more concise and aligned summaries, reducing their tendency to produce overly explanatory outputs without such guidance"
  - [corpus] Weak direct evidence on mechanism; corpus papers address repository-level context but not few-shot specifically.
- Break condition: When few-shot examples are unrepresentative, too few (<2), or inconsistent in style, guidance degrades; larger models (e.g., deepseek-coder-33b) still produced overly detailed outputs.

### Mechanism 2
- Claim: Skeleton context (class blueprint without implementation) yields better summaries than full class code due to reduced noise.
- Mechanism: Skeleton provides structural metadata (method signatures, class-level info) while filtering implementation details that introduce irrelevant signals.
- Core assumption: Task-relevant information is captured in signatures and high-level structure; implementation details add more noise than signal.
- Evidence anchors:
  - [section IV-A] "using only the skeleton structure outperformed entire classes, likely because full class context introduced too much noise"
  - [section III-B] class skeleton defined as "structured blueprint...including class-level and function-level information"
  - [corpus] RepoSummary paper mentions feature-oriented summarization, aligning with structured context benefits, but mechanism not directly tested.
- Break condition: When functional logic is essential to understanding (e.g., complex algorithmic methods), skeleton may omit critical semantic content.

### Mechanism 3
- Claim: Naive RAG with code chunk retrieval augments repository-level summarization when combined with few-shot, but not effectively alone.
- Mechanism: FAISS-based similarity search retrieves top-K chunks within context window; few-shot then guides synthesis of retrieved fragments.
- Core assumption: Cosine similarity on embeddings surfaces functionally related code; retrieved chunks contain genuinely useful context.
- Evidence anchors:
  - [abstract] "retrieved code chunks from RAG significantly enhanced the performance of LLMs"
  - [section IV] "while including code chunks as context did not significantly affect performance when used alone, introducing few-shot examples alongside the context led to noticeable improvements"
  - [corpus] "Completion by Comprehension" paper notes standard RAG relies on shallow semantic matching, may miss structural relationships—consistent with limited standalone effectiveness.
- Break condition: Retrieved chunks are tangentially similar but irrelevant to the target function; docstrings in chunks may leak references without few-shot structure.

## Foundational Learning

- Concept: In-Context Learning / Few-Shot Prompting
  - Why needed here: Core technique enabling LLMs to adapt without fine-tuning; central to all class/repository experiments.
  - Quick check question: Can you explain why 1-10 few-shot examples improve summary conciseness but 0-shot often fails?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Enables repository-level context injection within fixed context windows; FAISS + embedding model pipeline used.
  - Quick check question: What is the role of the embedding model and top-K selection in a Naive RAG pipeline?

- Concept: Encoder-Decoder vs. Decoder-Only Architectures for Code
  - Why needed here: Baseline CodeT5+ (encoder-decoder) fine-tuned on code outperformed zero-shot LLMs; understanding this distinction informs model selection.
  - Quick check question: Why might a fine-tuned encoder-decoder model outperform a larger decoder-only LLM in zero-shot summarization?

## Architecture Onboarding

- Component map:
  - Datasets: Modified ClassEval (400 eval, 10 few-shot tuples), Modified CodeSearchNet (3,384 eval, 160 few-shot tuples across 4 repos)
  - Baseline models: CodeT5+, CodeTrans variants, pile-t5-large (fine-tuned, beam search with multinomial sampling)
  - LLMs: DeepSeek Coder (1.3B/6.7B/33B), StarCoder2-15B, Llama3-8B (greedy decoding, max 128 output tokens)
  - RAG pipeline: BAAI/bge-large-en-v1.5 embeddings → FAISS index → top-K chunk retrieval (K=12, 25, 50)
  - Metrics: BLEU-4, ROUGE-L, METEOR, BERTScore-F1, BLEURT, SIDE (primary semantic metric)

- Critical path:
  1. Dataset preparation: Extract (context, function code, summary) tuples; filter by length (10–200 chars for CodeSearchNet)
  2. Baseline inference: Run fine-tuned models with beam search (5 beams, multinomial sampling)
  3. LLM inference: Construct prompts (system prompt + few-shot examples + context chunks) → greedy decode
  4. Evaluation: Compute all 6 metrics; compare against codet5p-base as SOTA baseline

- Design tradeoffs:
  - Context window: Llama3-8B (1M tokens) vs. DeepSeek/Starcoder2 (16K) determines max K chunks
  - Skeleton vs. full class: Skeleton reduces noise but may omit critical logic
  - Few-shot count: More shots improve metrics but increase prompt engineering cost; diminishing returns observed after 8–10
  - RAG vs. full context: RAG enables scalability but naive retrieval misses structural relationships

- Failure signatures:
  - LLMs produce overly verbose summaries (especially larger models like 33B) without few-shot guidance
  - Zero-shot + RAG chunks alone yield minimal improvement; SIDE scores diverge from ground truth
  - Full class context degrades performance due to noise injection

- First 3 experiments:
  1. Reproduce function-level baseline comparison: Run codet5p-base vs. deepseek-coder-1.3b (0-shot and 10-shot) on Modified ClassEval; verify SIDE scores (~0.240 vs. ~0.719 vs. ~0.412).
  2. Validate skeleton vs. full class context: Test deepseek-coder-1.3b with skeleton vs. full class on ClassEval; confirm skeleton yields lower SIDE (better) and higher BLEU-4.
  3. Run minimal RAG experiment: On Modified CodeSearchNet, test deepseek-coder-1.3b with (a) 0-shot + 12 chunks, (b) 2-shot + 12 chunks; quantify few-shot contribution to BLEURT and METEOR.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced RAG architectures (e.g., GraphRAG, autonomous agents) overcome the context utilization limitations observed with the Naive RAG pipeline?
- Basis in paper: [explicit] The authors state in Section VI that "Naive RAG did not help as much as expected" and explicitly list exploring advanced methods like GraphRAG or AriGraph as future work.
- Why unresolved: The current study focused on a Naive RAG implementation, which showed limited effectiveness at the repository level without few-shot examples.
- What evidence would resolve it: A comparative study evaluating summary quality (using metrics like BLEURT/SIDE) between Naive RAG and graph-based or agentic RAG systems on the same repository-level datasets.

### Open Question 2
- Question: Does the SIDE metric reliably correlate with human judgment of summary conciseness for general-purpose LLMs compared to code-specific LLMs?
- Basis in paper: [explicit] Section IV notes that the non-code-specific llama3-8b model performed unexpectedly well on the SIDE metric, and Section VI states this "warrants a reevaluation of the SIDE metric."
- Why unresolved: The anomaly where a general model outperformed code-specific models on this specific metric suggests a potential misalignment between the metric's evaluation and the task requirements for different model types.
- What evidence would resolve it: A human evaluation of summaries generated by code-specific vs. general LLMs correlated against SIDE scores to validate the metric's efficacy.

### Open Question 3
- Question: Does providing full class code as context introduce noise that degrades summarization performance compared to structured skeletons?
- Basis in paper: [inferred] Section IV observes that skeleton context outperformed full class code, hypothesizing that "full class context introduced too much noise," but this mechanism was not isolated or proven.
- Why unresolved: The study compared the input formats but did not analyze the internal attention mechanisms or specific "noise" factors that caused the performance drop.
- What evidence would resolve it: Ablation studies measuring the impact of token-level noise or irrelevant intraclass dependencies on model attention and summary quality.

## Limitations

- Chunking granularity: The exact strategy for splitting repository code into chunks (size, overlap, boundary adherence) is unspecified, potentially affecting RAG retrieval quality.
- Context window saturation: With up to 50 chunks plus 10 few-shot examples, some LLMs may exceed their 16K token limits, requiring truncation or reduced K.
- Few-shot example quality: The selection and representativeness of the 10 few-shot tuples are critical but not detailed; poor examples could degrade performance.

## Confidence

- High Confidence: Few-shot examples improve conciseness and alignment (supported by direct metric improvements).
- Medium Confidence: Skeleton context outperforms full class due to noise reduction (plausible but mechanism could vary by code complexity).
- Medium Confidence: Naive RAG alone provides limited benefit, but few-shot + RAG yields gains (evidence is correlational, not causal).

## Next Checks

1. **Chunking Strategy Validation**: Replicate the RAG retrieval with explicit chunk sizes (e.g., 512 chars) and verify retrieval relevance vs. default splitting.
2. **Few-Shot Sensitivity Test**: Systematically vary few-shot examples (0, 2, 5, 10) on a held-out set to quantify diminishing returns and optimal count.
3. **Skeleton vs. Full Context Ablation**: Test skeleton and full class contexts on functions with varying algorithmic complexity to determine when skeleton omits critical logic.