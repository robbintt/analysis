---
ver: rpa2
title: Are Robust LLM Fingerprints Adversarially Robust?
arxiv_id: '2509.26598'
source_url: https://arxiv.org/abs/2509.26598
tags:
- b-instruct
- utility
- llama-3
- normalized
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study reveals that existing model fingerprinting methods are
  fundamentally vulnerable to adversarial attacks that can completely bypass authentication
  while preserving high model utility. Four common vulnerabilities were identified
  across families of fingerprinting schemes: exact memorization, verbatim verification,
  unnatural queries, and statistical signatures.'
---

# Are Robust LLM Fingerprints Adversarially Robust?

## Quick Facts
- arXiv ID: 2509.26598
- Source URL: https://arxiv.org/abs/2509.26598
- Reference count: 40
- Primary result: Existing LLM fingerprinting methods are fundamentally vulnerable to adaptive attacks that can completely bypass authentication while preserving model utility.

## Executive Summary
This study systematically demonstrates that current large language model fingerprinting schemes are fundamentally vulnerable to adversarial attacks. The authors identify four common vulnerabilities across fingerprinting families—exact memorization, verbatim verification, unnatural queries, and statistical signatures—and develop corresponding adaptive attacks for each. These attacks achieve nearly perfect success rates against ten recently proposed fingerprinting schemes, completely evading ownership verification while maintaining high model utility. The work reveals that current fingerprinting methods fail under malicious settings, calling for adversarial robustness by design in future schemes.

## Method Summary
The paper introduces four families of adaptive attacks against LLM fingerprinting schemes. Output suppression attacks (SuppressTop-k, SuppressNeighbor, SuppressLookahead) manipulate token probability distributions to avoid fingerprint responses. Input detection attacks use perplexity filtering to identify and block unnatural fingerprint queries. Output detection attacks selectively suppress overconfident responses. Watermark stealing attacks extract statistical signatures by comparing token probabilities between watermarked and unwatermarked models. These attacks are evaluated across ten fingerprinting schemes using instruction-tuned models (Llama-3, Qwen2.5) and benchmarked for attack success rate and utility preservation on tasks like IFEval, GSM8K, and TriviaQA.

## Key Results
- Output suppression attacks achieve 100% ASR against memorization-based fingerprints using MatchPrefix verification
- Perplexity filtering provides near-perfect detection of GCG-optimized fingerprint queries with minimal false positives
- Watermark stealing attacks achieve 65%+ ASR at 90%+ utility preservation
- Adaptive attacks maintain >95% utility on academic benchmarks while completely evading verification

## Why This Works (Mechanism)

### Mechanism 1: Output Suppression via Token Probability Manipulation
Attack methods exploit the fact that fingerprinted models overfit to produce fingerprint responses as high-probability outputs. By sampling from bottom-k tokens (SuppressTop-k), blocking lexical variants (SuppressNeighbor), or detecting keywords across beam search paths (SuppressLookahead), attackers suppress fingerprint responses while maintaining utility. The core assumption is that fingerprint overconfidence creates exploitable confidence gaps that can be detected and suppressed.

### Mechanism 2: Input Detection via Perplexity Filtering
GCG-optimized fingerprint queries produce unnatural token sequences with high perplexity. By computing log-perplexity using reference models (GPT-2 or Llama-3.2-1B), attackers achieve near-perfect separation between fingerprint queries and natural user traffic. The attack exploits the inherent property of GCG optimization that produces statistical distributions diverging from natural language.

### Mechanism 3: Output Detection via Confidence Thresholding
Memorization-based fingerprints create overconfident outputs that can be detected through probability thresholds. Attackers apply suppression only when max token probability exceeds a threshold, differentiating fingerprint responses from normal outputs. This exploits the unavoidable overconfidence in memorization-based fingerprints, as regularizing confidence makes fingerprints vulnerable to forgetting during benign fine-tuning.

## Foundational Learning

- **Token-level autoregressive sampling and probability distributions**: Understanding how top-k, top-p, and temperature sampling work is prerequisite to implementing suppression attacks that manipulate p(·|q) at generation time. Quick check: Can you explain why sampling from bottom-(|V|-k) tokens evades MatchPrefix verification?

- **Perplexity as a measure of sequence naturalness**: Input detection attacks rely on computing log-perplexity to identify unnatural queries. This requires understanding perplexity as −(1/n)∑log(p_θ(x_i|x_<i)). Quick check: Why would GCG-optimized sequences have higher perplexity than natural language queries from WildChat?

- **KGW watermarking and green/red list partitioning**: Statistical fingerprinting attacks require understanding how watermarking works through random partitioning of vocabulary and biasing generation toward green tokens. Quick check: In watermark stealing, why does the ratio p_wm(t_{i+1}|t_i)/p_calib(t_{i+1}|t_i) help identify green tokens?

## Architecture Onboarding

- **Component map:**
```
Attack Pipeline
├── Input Layer: Query receiver with optional perplexity filter (Llama-3.2-1B/GPT-2)
├── Detection Layer: Input detection (perplexity threshold) and Output detection (confidence threshold t_gen, t_add)
├── Suppression Layer: SuppressTop-k (bottom-k sampling), SuppressNeighbor (lexical blocking), SuppressLookahead (beam search detection)
└── Output Layer: Modified generation with suppressed tokens
```

- **Critical path:** For memorization-based fingerprints: (1) compute token probabilities → (2) apply detection threshold → (3) build candidate set R̂ → (4) suppress candidates for n steps → (5) resume standard sampling. For intrinsic fingerprints: (1) compute query perplexity → (2) if above threshold, refuse/redirect query.

- **Design tradeoffs:**
  - Suppression aggressiveness vs. utility: Higher k increases ASR but degrades factual recall
  - Detection thresholds vs. false positives: Lower t_gen catches more fingerprints but risks suppressing legitimate confident responses
  - Perplexity model size: GPT-2 sufficient for some schemes; Llama-3.2-1B needed for others

- **Failure signatures:**
  - SuppressTop-k under MatchSubstring: Keyword appears mid-generation after initial suppression
  - Lenient verification (MatchKeyword): Keyword appears despite prefix suppression
  - Knowledge-editing fingerprints: Less overconfident outputs make detection harder

- **First 3 experiments:**
  1. Implement SuppressNeighbor with output detection against Instructional FP, measuring ASR under MP/MK verification and utility on IFEval/TriviaQA
  2. Compute perplexity distributions for RoFL fingerprints vs. WildChat queries using GPT-2/Llama-3.2-1B, plot ROC curves to find optimal threshold
  3. Run SuppressNeighbor with output detection against Chain&Hash, generate Pareto frontier curves showing utility-ASR tradeoff

## Open Questions the Paper Calls Out

- **White-box fingerprinting resistance**: The authors state attacking white-box schemes (e.g., DeepJudge, zkLLM) under malicious host threat model is an interesting future direction, as they limit scope to black-box methods.

- **Tool-use and thinking models**: The paper notes it mainly considers instruct-tuned chat models, not tool-use or thinking style models, leaving open whether vulnerabilities persist in models with reasoning chains.

- **Constructing robust fingerprinting methods**: The authors conclude with four recommendations but do not propose a concrete scheme that resists adaptive statistical stealing while maintaining effectiveness.

## Limitations

- The study assumes static fingerprinting schemes and known fingerprint content, whereas real-world deployments might use dynamic or randomized fingerprints
- Utility metrics focus on academic benchmarks that may not fully capture domain-specific performance impacts in deployed systems
- Attack detection thresholds appear tuned per-experiment, and their generalizability across different model architectures remains unclear

## Confidence

**High Confidence**: The fundamental vulnerability of memorization-based fingerprinting to output suppression attacks is well-supported by empirical results showing 100% ASR.

**Medium Confidence**: Watermark stealing effectiveness relies on assumptions about model calibration and persistence under adversarial fine-tuning, with robustness to different calibration properties being less certain.

**Low Confidence**: The universality claim for perplexity-based input detection requires additional validation, as perfect separation for RoFL fingerprints may not generalize to schemes using alternative optimization strategies.

## Next Checks

1. **Cross-model generalization test**: Apply SuppressNeighbor with output detection to three additional fingerprint schemes across Llama-3 and Qwen2.5 architectures, measuring ASR >90% with <10% utility degradation.

2. **Adaptive fingerprint defense evaluation**: Implement hybrid verification combining MatchPrefix with confidence thresholding and statistical anomaly detection, testing whether attackers can adapt methods to evade multi-layered defenses.

3. **Production deployment simulation**: Create black-box attack scenario with partial knowledge of fingerprint schemes, implementing API-based inference of patterns followed by watermark stealing and output suppression, comparing success rates to white-box attacks.