---
ver: rpa2
title: 'FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models'
arxiv_id: '2601.20791'
source_url: https://arxiv.org/abs/2601.20791
tags:
- bias
- video
- fairt2v
- debiasing
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FairT2V, a training-free framework for mitigating
  gender bias in text-to-video diffusion models. The authors analyze demographic bias
  in T2V generation and show that it primarily originates from pretrained text encoders,
  which encode implicit gender associations even for neutral prompts.
---

# FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models

## Quick Facts
- arXiv ID: 2601.20791
- Source URL: https://arxiv.org/abs/2601.20791
- Reference count: 13
- Primary result: Training-free gender bias mitigation in T2V generation via spherical geodesic prompt debiasing

## Executive Summary
This paper introduces FairT2V, a training-free framework for mitigating gender bias in text-to-video diffusion models. The authors analyze demographic bias in T2V generation and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. They propose a gender-leaning score to quantify this bias at the embedding level. FairT2V mitigates bias by neutralizing prompt embeddings through anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, debiasing is applied only during early identity-forming steps via a dynamic denoising schedule. The authors also design a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on Open-Sora demonstrate that FairT2V substantially reduces demographic bias across occupations while maintaining high video quality, achieving better trade-offs between fairness and generation fidelity than existing training-free methods.

## Method Summary
FairT2V operates by first quantifying gender bias in text encoder embeddings through a gender-leaning score that measures angular distance from a global gender axis. For each neutral prompt, the method constructs gender anchor embeddings (majority and minority group versions) and applies spherical geodesic transformation to shift the neutral embedding toward demographic balance. The adaptive coefficient λ* determines the interpolation position based on angular proximity to each anchor. To preserve temporal coherence, debiasing is restricted to early denoising steps using a dynamic schedule that applies the transformed embedding only when t ≤ round(T·Sigmoid(1-cos(v,v_fair))). The method requires no model fine-tuning and works by modifying text embeddings before classifier-free guidance injection.

## Key Results
- Reduces gender bias across 16 occupations with VFR improvements while maintaining CLIP-T, TIFA, and FVD metrics comparable to baseline
- CLIP-based debiasing achieves balanced VFR scores; T5-based debiasing causes severe over-debiasing (+388% to +716% VFR)
- Dynamic denoising schedule improves FVD by 9.1% (female) and 6.8% (male) while slightly increasing female VFR (28%)
- FairT2V achieves better fairness-quality trade-offs than FairDiff (strong debiasing but 51% FVD degradation) and FairImagen (minimal debiasing effect)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Demographic bias in T2V models originates primarily from pretrained text encoders, not the denoising network itself.
- Mechanism: Text encoders like CLIP, trained on socially imbalanced image-text data, internalize skewed correlations that align certain occupations with gender-associated directions in the embedding space. Since classifier-free guidance injects the same text embedding at every denoising step, this bias is repeatedly reinforced across frames.
- Core assumption: The text embedding acts as a persistent conditioning signal that shapes identity-level semantics throughout generation.
- Evidence anchors:
  - [abstract] "demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts"
  - [section 3] Figure 2 shows gender-leaning scores from CLIP text encoder correlating with generated video bias; Figure 3 validates this with actual gender distributions in generated videos
  - [corpus] VEAT paper (arXiv:2601.00996) confirms implicit associations in Sora T2V generator, supporting the encoder-as-bias-source hypothesis
- Break condition: If bias originated primarily from the denoising network rather than text conditioning, embedding-level interventions would fail to mitigate it.

### Mechanism 2
- Claim: Spherical geodesic transformation between gender anchors can neutralize encoder-induced bias while preserving prompt semantics.
- Mechanism: By constructing two anchor embeddings (majority-group and minority-group versions of the same prompt) and computing the geodesic path between them on the unit hypersphere, the method adaptively shifts the neutral embedding toward a demographically balanced position. The adaptive coefficient λ* is determined by angular proximity to each anchor.
- Core assumption: Movement along the geodesic between semantically-identical anchors preserves occupation/scene semantics while isolating demographic variation.
- Evidence anchors:
  - [section 4] Equation (6) defines spherical interpolation: "ˆvfair = sin(λθ)/sinθ · ˆvmaj + sin((1−λ)θ)/sinθ · ˆvmin"
  - [section 5.3] Table 1 shows FAIRT2V achieves competitive VFR scores while maintaining CLIP-T and TIFA metrics comparable to baseline
  - [corpus] FairImagen uses PCA-based projection for T2I debiasing but shows limited transfer to T2V (VFR largely matching baseline), suggesting spherical transformation is better suited for video generation
- Break condition: If the two anchors do not share sufficient semantic structure, geodesic movement could cause semantic drift (the paper claims CLIP-based alignment metrics validate this is avoided).

### Mechanism 3
- Claim: Restricting debiasing to early denoising steps preserves temporal coherence while maintaining bias mitigation.
- Mechanism: Early diffusion steps establish coarse structure and identity semantics; later steps refine local appearance. The dynamic denoising schedule computes an adaptive cutoff timestep based on angular discrepancy between original and debiased embeddings, applying debiasing only for t ≤ round(T̂).
- Core assumption: Identity-related bias is largely determined during early identity-forming stages.
- Evidence anchors:
  - [section 4] Equation (8): "T̂ = T · Sigmoid(1 − cos(v, ˆvfair))"
  - [section 5.5] Table 2 ablation shows DDS improves FVD by 9.1% (female) and 6.8% (male) while slightly increasing female VFR (28%), confirming quality-fairness trade-off
  - [corpus] No direct corpus evidence for video-specific early-step identity formation; this is adapted from image diffusion literature (Yoon et al., Lv et al. 2025)
- Break condition: If late-stage denoising significantly influenced demographic attributes, limiting intervention to early steps would be insufficient.

## Foundational Learning

- Concept: Classifier-Free Guidance (CFG)
  - Why needed here: CFG is how text embeddings are injected throughout the diffusion process; understanding Equation (1) is essential to grasp why encoder bias propagates across all frames.
  - Quick check question: What happens to the guided noise estimate when α increases?

- Concept: Spherical Linear Interpolation (SLERP)
  - Why needed here: The spherical geodesic transformation extends SLERP to handle adaptive extrapolation; you need to understand why angular distance matters on hyperspherical manifolds.
  - Quick check question: Why does linear interpolation in Euclidean space cause semantic drift when applied to normalized embeddings?

- Concept: Diffusion Denoising Trajectory
  - Why needed here: The dynamic schedule relies on the fact that early vs. late denoising steps serve different functions (identity formation vs. detail refinement).
  - Quick check question: At what timestep range would you expect gender identity to be determined in a 50-step diffusion process?

## Architecture Onboarding

- Component map: Prompt → CLIP encoder → Anchor generation → Geodesic transformation → CFG injection (steps 1 to T̂) → Original embedding restoration (steps T̂+1 to T) → Video output

- Critical path: Text encoder (CLIP) generates prompt embeddings; primary bias source. Anchor generator creates majority/minority anchor embeddings by augmenting prompts with explicit gender attributes. Geodesic transformer computes adaptive λ* and applies spherical transformation. Dynamic scheduler determines cutoff timestep T̂ based on embedding discrepancy. VideoLLM evaluator (Gemini-2.5-Flash) classifies gender in generated videos for VFR computation.

- Design tradeoffs:
  - FairDiff (prompt editing) achieves stronger debiasing but degrades FVD by ~51% and visual quality
  - FairImagen (PCA projection) preserves quality but shows minimal debiasing effect
  - FAIRT2V balances both but may over-correct already-balanced occupations (e.g., Teacher VFR increased from 0.007 to 0.021)
  - CLIP-based debiasing works reliably; T5-based debiasing causes severe over-debiasing (+388% to +716% VFR)

- Failure signatures:
  - Excessive semantic drift: Likely indicates anchors are not semantically aligned or λ* is outside stable range
  - Temporal flicker/inconsistency: May indicate debiasing applied too late in denoising process
  - Over-correction on balanced prompts: Check if original gender-leaning score is already near zero before applying transformation
  - Quality degradation with T5 encoder: Expected—paper explicitly shows T5 is highly sensitive to embedding perturbations

- First 3 experiments:
  1. Reproduce gender-leaning score analysis for 4 occupations (CEO, Doctor, Nurse, Teacher) by computing global gender axis g and projecting neutral embeddings; verify correlation with video-level gender distributions.
  2. Implement spherical geodesic transformation with fixed λ=0.5 (midpoint interpolation) and measure VFR/FVD trade-off vs. adaptive λ* to isolate the contribution of adaptive coefficient selection.
  3. Ablate dynamic denoising schedule by varying T̂ from 0% to 100% of total steps; plot VFR and FVD as functions of cutoff ratio to identify optimal quality-fairness operating point for your target model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FairT2V be effectively extended to mitigate multi-class demographic biases (e.g., race, age) and cross-demographic intersections in video generation?
- Basis in paper: [explicit] The conclusion explicitly states: "While this work focuses on binary gender bias, extending the approach to multi-class and cross-demographic biases remains an important direction for future research."
- Why unresolved: The current methodology, including the gender-leaning score and spherical geodesic transformation, is constructed specifically for binary gender attributes (male/female) and does not account for the non-binary or intersectional nature of other demographic dimensions.
- What evidence would resolve it: A demonstration of the framework successfully reducing bias metrics for non-binary attributes or intersectional groups (e.g., race-gender combinations) without compromising video quality.

### Open Question 2
- Question: How can training-free debiasing techniques be adapted for token-level text encoders like T5 to avoid semantic drift and over-debiasing?
- Basis in paper: [explicit] Section 5.5 notes that applying FairT2V to T5 results in "severe over-debiasing" and concludes that "robust debiasing for T5 remains an open challenge in text-to-video generation."
- Why unresolved: The global embedding perturbations used for CLIP disrupt the fine-grained token-level conditioning relied upon by T5, leading to significant FVD increases and text-video alignment failures.
- What evidence would resolve it: A modified debiasing strategy for T5 that maintains TIFA scores and FVD comparable to the original model while achieving fair gender representation.

### Open Question 3
- Question: To what extent do VideoLLM-based evaluators introduce systematic biases or hallucinations that affect the reliability of fairness assessments?
- Basis in paper: [inferred] Section 4 acknowledges that "automated VideoLLM-based judgments may be affected by hallucination and prompt sensitivity," and the paper relies on a specific VideoLLM (Gemini) for its automated VFR metric.
- Why unresolved: While human verification is used to validate labels, the automated evaluation protocol depends on the VideoLLM's internal reasoning, which may possess its own implicit biases not fully corrected by the human-in-the-loop sample.
- What evidence would resolve it: A correlation analysis between VideoLLM confidence scores and human annotations across a diverse set of ambiguous or multi-subject videos to quantify the evaluator's error margin.

### Open Question 4
- Question: Does the identified source of bias in text encoders and the efficacy of spherical geodesic debiasing generalize to non-DiT architectures like U-Nets?
- Basis in paper: [inferred] The experiments are limited to Open-Sora (DiT-based), while the background notes that "MM-DiT–based models remain less well understood" than U-Net-based diffusion models.
- Why unresolved: The diffusion process and text conditioning integration differ architecturally between DiTs and U-Nets, potentially affecting how embedding-level shifts propagate to video pixels.
- What evidence would resolve it: Reproduction of the gender-leaning score analysis and FairT2V intervention on U-Net-based video models (e.g., Stable Video Diffusion or CogVideo) showing similar fairness improvements.

## Limitations

- The bias origin analysis relies on observational correlations rather than controlled ablation studies isolating encoder vs. denoising network contributions.
- Semantic preservation during geodesic transformation is validated through CLIP metrics rather than comprehensive human perceptual studies.
- The dynamic denoising schedule adaptation is based on image diffusion literature without video-specific validation of temporal coherence preservation.

## Confidence

**High Confidence**: The gender-leaning score methodology for quantifying bias at the embedding level is mathematically well-defined and reproducible. The VFR evaluation protocol combining VideoLLM reasoning with human verification is clearly specified and achieves the stated results.

**Medium Confidence**: The claim that CLIP-based debiasing works reliably while T5-based debiasing causes severe over-debiasing is supported by empirical results, but the underlying reason (token-level conditioning sensitivity) is not rigorously explained.

**Low Confidence**: The assertion that restricting debiasing to early steps preserves temporal coherence is based on general diffusion principles rather than video-specific validation. The paper does not demonstrate that late-stage denoising significantly influences demographic attributes.

## Next Checks

1. **Controlled Encoder Ablation**: Replace the CLIP text encoder with a debiased version (e.g., FairCLIP) while keeping the denoising network constant, then measure changes in VFR and FVD. This would isolate the encoder's contribution to bias and validate the proposed mechanism.

2. **Semantic Drift Analysis**: Generate videos using debiased embeddings with varying λ* values (including extrapolation beyond [0,1]) and conduct human studies to quantify semantic drift. Compare CLIP alignment metrics against human judgments to validate the sufficiency of automated semantic preservation metrics.

3. **Temporal Coherence Study**: Run the full generation pipeline with debiasing applied at different timestep ranges (early-only, late-only, uniform) while keeping total intervention count constant. Measure VFR, FVD, and conduct frame-to-frame consistency analysis to identify the optimal temporal intervention strategy.