---
ver: rpa2
title: Best Arm Identification with Possibly Biased Offline Data
arxiv_id: '2505.23165'
source_url: https://arxiv.org/abs/2505.23165
tags:
- data
- lucb-h
- online
- bound
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the best arm identification (BAI) problem when
  historical offline data may be biased. Existing BAI algorithms either ignore such
  data or assume it aligns with online distributions, risking poor performance when
  biases exist.
---

# Best Arm Identification with Possibly Biased Offline Data

## Quick Facts
- arXiv ID: 2505.23165
- Source URL: https://arxiv.org/abs/2505.23165
- Authors: Le Yang; Vincent Y. F. Tan; Wang Chi Cheung
- Reference count: 40
- Primary result: Proposes LUCB-H algorithm for best arm identification with potentially biased offline data, proving near-optimal sample complexity when bias bounds are known

## Executive Summary
This paper addresses best arm identification (BAI) when historical offline data may be biased relative to the online distribution. Existing BAI algorithms either ignore such data or assume it aligns with online distributions, risking poor performance when biases exist. The authors prove that without prior knowledge of bias bounds, no adaptive algorithm can consistently outperform purely online methods. To address this, they propose LUCB-H, which uses auxiliary bias bounds to adaptively combine offline and online data within the LUCB framework. The algorithm computes both online-only and offline-corrected confidence bounds, then selects the tighter bounds per arm.

## Method Summary
LUCB-H extends the LUCB algorithm by incorporating offline data with bias correction. For each arm, it computes four confidence bounds: online-only lower/upper bounds (LCB_t, UCB_t) and offline-corrected bounds (LCBS_t, UCBS_t) that account for potential bias V(i). The algorithm then selects the tighter bounds per arm (LCBmix = max of online-only and offline-corrected lower bounds, UCBmix = min of upper bounds) and proceeds with standard LUCB sampling. Theoretical analysis shows LUCB-H matches pure LUCB's sample complexity when offline data is misleading and improves it when data is beneficial, achieving near-optimal performance in certain cases.

## Key Results
- LUCB-H matches sample complexity of standard LUCB when offline data is misleading (V(i) large)
- LUCB-H significantly improves sample complexity when offline data is beneficial (V(i) small)
- The algorithm is theoretically near-optimal in certain cases with gap(i) ≥ 0
- Experiments confirm LUCB-H's adaptability and robustness across different offline data scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting the tighter of two confidence bounds enables adaptive use of historical data without catastrophic failure when offline data is misleading.
- Mechanism: LUCB-H computes `LCBmix(i) = max{LCB_t(i), LCBS_t(i)}` and `UCBmix(i) = min{UCB_t(i), UCBS_t(i)}`. When offline data is misleading (large bias), the offline-corrected bounds widen and the algorithm effectively ignores them, reverting to online-only bounds. When offline data is helpful (small bias), the combined bounds tighten faster, accelerating convergence.
- Core assumption: A valid bias bound `V(i) ≥ |μ_off(i) - μ_on(i)|` is provided as input.
- Evidence anchors:
  - [abstract] "The algorithm computes both online-only and offline-corrected confidence bounds, then selects the tighter bounds per arm."
  - [Section 4, Algorithm 1] Lines 5-7 show the explicit bound computation and min/max selection.
  - [corpus] Weak direct corpus support for this specific min/max hybrid mechanism; related BAI papers (e.g., multi-objective, constrained BAI) do not address biased offline data.
- Break condition: If `V(i)` is severely underestimated (i.e., `V(i) << |μ_off(i) - μ_on(i)|`), the offline-corrected bounds may be overconfident, potentially degrading performance below pure LUCB.

### Mechanism 2
- Claim: The bias-aware confidence radius `radS_t(i)` inflates appropriately to maintain valid coverage when offline and online distributions differ.
- Mechanism: The radius combines a standard concentration term `√(2 log(kt/δ) / (N_t(i) + TS(i)))` with a bias penalty `(TS(i) / (N_t(i) + TS(i))) · V(i)`. As `V(i)` increases or offline sample ratio increases, the radius expands, conservatively accounting for potential mean shift.
- Core assumption: The reward distributions are Gaussian with known variance (σ² = 1 in experiments).
- Evidence anchors:
  - [Section 4, Eq. 4.3-4.4] Explicit formula for `LCBS_t(i)` and `UCBS_t(i)` with `radS_t(i)`.
  - [Section 4] "The above confidence interval consists of two parts: the first term is a standard confidence bound after incorporating historical data, and the second term compensates for potential bias."
  - [corpus] No direct corpus precedent for this bias-penalized confidence radius in BAI; related work (MIN-UCB) addresses regret minimization, not fixed-confidence BAI.
- Break condition: If the Gaussian assumption is violated (e.g., heavy-tailed rewards), the concentration inequality underlying the radius may not hold, requiring alternative sub-Gaussian or robust bounds.

### Mechanism 3
- Claim: Sample complexity savings are realized when the discrepancy measure `η(i) = V(i) + μ_off(i) - μ_on(i)` is small relative to the suboptimality gap `Δ_i`.
- Mechanism: The saving term `Sav_u(i) = TS(i) · max{1 - 4η(i)/Δ_i, 0}` captures how many offline samples effectively substitute for online samples. When `η(i) ≤ Δ_i/4`, savings are positive and scale with `TS(i)`. The algorithm adaptively achieves these savings through the tighter mixed bounds.
- Core assumption: The bias bound `V(i)` is valid and the discrepancy measure `η(i)` is unknown but bounded by `V(i)`.
- Evidence anchors:
  - [Section 4, Theorem 4.1] Formal upper bound with saving term.
  - [Section 6, Figures 3-4] Empirical confirmation that stopping time decreases with `TS(i)` in beneficial scenarios.
  - [corpus] Related batch/warm-start BAI work (Agrawal et al., 2023) achieves savings only when `P_off = P_on`, not for biased data.
- Break condition: When `η(i) > Δ_i/4` for most arms, `Sav_u(i) ≈ 0` and LUCB-H degrades to pure LUCB complexity—no savings, but also no catastrophic overhead.

## Foundational Learning

- Concept: **Fixed-confidence Best Arm Identification (BAI)**
  - Why needed here: LUCB-H operates in the δ-PAC setting where the goal is to identify `argmax_i μ_on(i)` with probability ≥ 1-δ while minimizing online samples. Understanding the difference from regret minimization (which balances exploration-exploitation) is essential.
  - Quick check question: Can you explain why the sample complexity scales as `O(∑_{i:Δ_i>0} Δ_i^{-2} log(1/δ))` in standard BAI?

- Concept: **Lower-Upper Confidence Bound (LUCB) Algorithm**
  - Why needed here: LUCB-H extends LUCB by modifying its confidence bounds. LUCB's core idea—sampling the empirical best arm and the arm with highest UCB among the rest—remains, but the bounds now incorporate offline data.
  - Quick check question: In standard LUCB, which two arms are selected each round, and what is the stopping condition?

- Concept: **Kullback-Leibler Divergence and Change-of-Measure Arguments**
  - Why needed here: The lower bound proof (Theorem 5.1) uses KL divergence to characterize instance-dependent complexity. Understanding how to construct alternative instances and apply transportation inequalities is key to grasping the theoretical guarantees.
  - Quick check question: Why does the lower bound involve `KL(μ_on(i), μ_on(1))` rather than just `Δ_i^{-2}`?

## Architecture Onboarding

- Component map: Input Module (V(i), δ, S) -> Bound Computer (LCB_t, UCB_t, LCBS_t, UCBS_t) -> Bound Selector (LCBmix, UCBmix) -> Arm Selector (h_t, l_t) -> Stopping Criterion -> Output (Î*)

- Critical path:
  1. Initialize by pulling each arm once (warm-up).
  2. Each round: compute bounds → select mixed bounds → pick arms h_t, l_t → check stopping condition → if not stopped, pull both arms and update.
  3. The bias bound V(i) critically affects whether UCBS_t(i) is tighter than UCB_t(i); this determines whether offline data is effectively used.

- Design tradeoffs:
  - Tighter V(i) vs. robustness: Smaller V(i) yields tighter bounds if valid, but risks misspecification. Overestimating V(i) is safer (degrades to pure LUCB) than underestimating.
  - Offline sample size TS(i): More offline data helps when beneficial but increases bias penalty when misleading. The algorithm adaptively handles this, but the saving term saturates when η(i) is large.
  - Assumption of known variance: The current implementation assumes unit-variance Gaussians; extension to unknown or heterogeneous variance would require different concentration inequalities.

- Failure signatures:
  - Underestimated bias bounds: Figure 10 shows that when V(i) is set below true bias for the best arm, LUCB-H can underperform pure LUCB—offline data actively misleads.
  - Non-Gaussian rewards: The concentration bounds may not hold; consider empirical Bernstein or robust alternatives.
  - Infinite V(i): Algorithm effectively runs pure LUCB (by construction), but the impossibility result (Proposition 3.1) shows no adaptive scheme can do better without auxiliary information.

- First 3 experiments:
  1. Sanity check on 2-arm Gaussian bandit: Set μ_on = (0.5, 0.3), P_off = P_on (beneficial case), TS = 500, V = 0.01. Verify that LUCB-H stopping time is significantly lower than pure LUCB across δ ∈ {0.1, 0.05, 0.01}.
  2. Misleading offline data test: Flip offline means so μ_off = (0.3, 0.5) (best arm reversed), set V = 0.3 (valid). Confirm that LUCB-H stopping time matches pure LUCB within statistical noise.
  3. V(i) sensitivity sweep: Fix a 5-arm problem with moderate bias. Vary V(i) from 0.5× to 2× the true bias. Plot stopping time vs. V(i) overestimation factor to empirically validate robustness to overestimation and fragility to underestimation.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the gap(i) between the upper bound (Theorem 4.1) and lower bound (Theorem 5.1) remain non-negative for all instances, not just the two special cases analyzed?
  - Basis in paper: [explicit] Remark 5.1 states: "While we believe that gap(i) ≥ 0 holds generally, we leave the proof of this to future work."
  - Why unresolved: The authors only proved non-negativity for two special cases (equal V(i) values across arms, and when P^on(1) = P^off(1)), but the general case remains unproven.
  - What evidence would resolve it: A general proof that gap(i) = Sav^l(i) - Sav^u(i) ≥ 0 for arbitrary bias bounds V(i) and discrepancy measures η(i), or a counterexample showing negative gap.

- **Open Question 2**: Can the LUCB-H framework be extended to non-Gaussian reward distributions, such as sub-Gaussian, bounded, or heavy-tailed distributions?
  - Basis in paper: [inferred] Section 2 states the assumption that "both the online and offline reward distributions...are normal distributions with known variance." The theoretical analysis relies heavily on this assumption.
  - Why unresolved: The confidence bound constructions and bias correction terms depend on Gaussian-specific concentration inequalities. Extending to other distribution families may require different analytical techniques.
  - What evidence would resolve it: Modified confidence bounds and sample complexity analysis for broader distribution classes, potentially using distribution-specific concentration inequalities.

- **Open Question 3**: How can practitioners reliably estimate or obtain the bias bound V(i) before the online phase, especially in domains where offline and online distributions differ in unknown ways?
  - Basis in paper: [inferred] The impossibility result (Proposition 3.1) shows algorithms cannot adapt without knowing V(i), and Section 4 briefly mentions estimation via LASSO, cross-validation, or empirical insights without detailed methodology.
  - Why unresolved: The paper assumes V(i) is given as input, but provides limited guidance on practical estimation methods or how estimation errors beyond simple misspecification affect performance.
  - What evidence would resolve it: Systematic study of bias bound estimation techniques, their theoretical guarantees, and empirical robustness across diverse real-world scenarios.

## Limitations

- The algorithm requires valid bias bounds V(i) as input, which may be difficult to obtain in practice
- Performance degrades to pure LUCB when bias is large relative to gaps (η(i) > Δ_i/4 for most arms)
- Theoretical guarantees assume Gaussian rewards with known variance, limiting applicability to more general settings

## Confidence

- **High confidence**: The mechanism of adaptively selecting tighter bounds per arm is well-specified and directly supported by the algorithm pseudocode and experimental results showing performance matching pure LUCB under misleading data
- **Medium confidence**: The bias-penalized confidence radius is formally correct under Gaussian assumptions, but extension to non-Gaussian rewards would require different concentration bounds not addressed in the paper
- **Medium confidence**: The sample complexity savings characterization is theoretically sound, but practical savings depend heavily on accurate bias bounds and may be limited in problems where η(i) is large relative to Δ_i

## Next Checks

1. **Bias bound sensitivity test**: Systematically vary V(i) from severe underestimation to overestimation in controlled experiments to map the performance degradation curve and identify the threshold where LUCB-H transitions from beneficial to harmful

2. **Non-Gaussian reward extension**: Implement LUCB-H with empirical Bernstein bounds instead of Gaussian concentration to verify the algorithm's behavior under heavy-tailed reward distributions common in real-world applications

3. **Multiple offline sources**: Extend the experimental setup to scenarios with multiple biased offline datasets (e.g., from different subpopulations) and test whether LUCB-H can be modified to handle uncertainty about which bias bound applies