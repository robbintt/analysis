---
ver: rpa2
title: 'JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with
  Reinforcement Learning'
arxiv_id: '2506.19846'
source_url: https://arxiv.org/abs/2506.19846
tags:
- tool
- memory
- agent
- user
- call
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JoyAgents-R1, a joint evolution dynamics
  framework for multi-agent reinforcement learning (MARL) that applies Group Relative
  Policy Optimization (GRPO) to heterogeneous multi-agent systems. The method addresses
  challenges of sampling efficiency and training stability in MARL by implementing
  node-wise Monte Carlo sampling and marginal benefit-driven parameter updates.
---

# JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.19846
- Source URL: https://arxiv.org/abs/2506.19846
- Reference count: 40
- **Primary result:** Joint evolution dynamics framework enabling small open-source LLMs to match larger models in multi-agent reasoning tasks via Group Relative Policy Optimization and adaptive memory evolution.

## Executive Summary
JoyAgents-R1 introduces a joint evolution dynamics framework for training heterogeneous multi-agent systems using Group Relative Policy Optimization (GRPO). The approach addresses sampling efficiency and training stability challenges by implementing node-wise Monte Carlo sampling and marginal benefit-driven parameter updates. An adaptive memory evolution mechanism repurposes GRPO rewards as supervisory signals to eliminate redundant reasoning and accelerate convergence. Experiments demonstrate that JoyAgents-R1 achieves performance comparable to larger language models while built on smaller open-source models, with significant improvements in accuracy and reasoning efficiency across both general and domain-specific scenarios.

## Method Summary
JoyAgents-R1 implements joint training of heterogeneous multi-agents (Master, QA, Math, E-commerce FC, General FC) using a two-phase approach. First, supervised fine-tuning (SFT) with 13k+ samples establishes baseline reasoning capabilities. Second, GRPO joint training applies node-wise Monte Carlo sampling to generate trajectories, selects top-5 agents by reward variance, and updates parameters using the GRPO objective. An adaptive memory evolution mechanism filters and repurposes reasoning traces based on reward thresholds. The framework uses Qwen2.5-3B as the backbone model, with specific hyperparameters including LR=5e-6 for SFT and LR=1e-6 for RL, trained for 5 epochs each phase.

## Key Results
- Achieves comparable performance to larger models while using smaller open-source Qwen2.5-3B backbone
- Demonstrates significant improvements in accuracy and reasoning efficiency across general and domain-specific scenarios
- Shows effective reduction in reasoning steps through adaptive memory evolution mechanism
- Outperforms baselines in both accuracy metrics and computational efficiency

## Why This Works (Mechanism)
The framework succeeds by addressing key limitations in multi-agent reinforcement learning. Node-wise Monte Carlo sampling reduces the exponential trajectory space to manageable linear growth, while top-K agent selection ensures only high-variance, high-impact parameters are updated. The adaptive memory evolution mechanism filters redundant reasoning traces using reward-based thresholding, preventing negative memory interference. GRPO's group-relative optimization provides stable policy updates without requiring value function estimation. Together, these mechanisms enable efficient joint training of heterogeneous agents with complementary capabilities.

## Foundational Learning

**Group Relative Policy Optimization (GRPO)**
- *Why needed:* Provides stable policy updates for multi-agent systems without requiring value function estimation
- *Quick check:* Verify GRPO objective implementation matches Equation 2 without KL penalty

**Node-wise Monte Carlo Sampling**
- *Why needed:* Reduces exponential trajectory complexity to linear growth for sampling efficiency
- *Quick check:* Confirm sampling generates G_i=5 actions per node along trajectory

**Adaptive Memory Evolution**
- *Why needed:* Eliminates redundant reasoning traces and prevents memory interference
- *Quick check:* Verify reward thresholding uses α=β=1 with threshold D=0

**Heterogeneous Agent Architecture**
- *Why needed:* Enables specialization for different reasoning tasks (QA, Math, E-commerce, general tool use)
- *Quick check:* Confirm all agents share Qwen2.5-3B backbone but have task-specific fine-tuning

**Reward Design (R_A + R_F - R_E)**
- *Why needed:* Combines accuracy, fluency, and efficiency metrics for comprehensive evaluation
- *Quick check:* Verify reward computation uses semantic similarity > 0.6 for QA and exact match for other tasks

## Architecture Onboarding

**Component Map**
SFT (13k+ samples) -> GRPO Joint Training (node-wise MC sampling) -> Top-K Parameter Updates -> Adaptive Memory Evolution -> Agent Memory Storage

**Critical Path**
SFT training → Joint GRPO training with sampling → Reward computation and top-K selection → Parameter updates → Memory evolution filtering

**Design Tradeoffs**
- Node-wise vs. exponential sampling: Linear vs. exponential trajectory growth
- Top-K vs. full parameter updates: Stability vs. potential underfitting
- Memory filtering thresholds: Quality vs. quantity of retained traces

**Failure Signatures**
- Training instability: Rewards collapse or diverge, check top-K selection logic
- Memory bloat: Increased reasoning steps, verify adaptive reward thresholding
- Accuracy degradation: Incorrect memory filtering causing negative interference

**3 First Experiments**
1. Replicate SFT phase with provided datasets to establish baseline reasoning capabilities
2. Test GRPO joint training with node-wise sampling on held-out validation set
3. Verify adaptive memory evolution by measuring reasoning step reduction

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on domain-specific APIs and knowledge bases, particularly for E-commerce FC agent, limits external validation
- Experimental results primarily validated on synthetic or domain-specific datasets, with limited testing on fully open-domain tasks
- Scalability and generalizability to other domains (healthcare, legal reasoning) not empirically demonstrated
- Memory evolution mechanism depends on task-specific reward thresholds requiring manual tuning

## Confidence

**High Confidence:** Core GRPO implementation for multi-agent policy updates and general framework for joint evolution dynamics are clearly specified and reproducible with open-source models.

**Medium Confidence:** Sampling efficiency gains and memory evolution mechanism are well-documented in ablation studies, but performance benefits may depend on specific reward design and task structure.

**Low Confidence:** Scalability to other domains and long-horizon reasoning tasks are not empirically demonstrated, and lack of public API/tool sandbox limits external validation.

## Next Checks

1. **Replicate Ablation Studies:** Independently reproduce the ablation results (Table 6) for "Sampling Strategy," "Parameter Sharing," and "Memory Evolution" on a held-out validation set to confirm claimed improvements.

2. **Test Generalization:** Apply JoyAgents-R1 to a different multi-agent reasoning task (e.g., multi-hop question answering) using same framework but with new domain-specific APIs/knowledge bases to assess robustness.

3. **Benchmark Efficiency:** Measure wall-clock training time and memory usage for joint training versus sequential fine-tuning of individual agents to verify claimed sampling efficiency and computational overhead.