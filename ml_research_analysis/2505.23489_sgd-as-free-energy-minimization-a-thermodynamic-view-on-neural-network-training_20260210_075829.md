---
ver: rpa2
title: 'SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training'
arxiv_id: '2505.23489'
source_url: https://arxiv.org/abs/2505.23489
tags:
- loss
- training
- learning
- gradient
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a thermodynamic perspective on neural network
  training, framing stochastic gradient descent (SGD) with fixed learning rates as
  minimizing a free energy function \( F = U - TS \), where \( U \) is training loss,
  \( S \) is the entropy of the weight distribution, and \( T \) is an effective temperature
  controlled by the learning rate. This view explains why high learning rates prevent
  convergence to the loss minimum and why different learning rates yield different
  final loss levels.
---

# SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training

## Quick Facts
- arXiv ID: 2505.23489
- Source URL: https://arxiv.org/abs/2505.23489
- Reference count: 40
- Primary result: SGD with fixed learning rates minimizes free energy F = U - TS, explaining different loss levels at different learning rates

## Executive Summary
This paper introduces a thermodynamic perspective on neural network training, framing stochastic gradient descent (SGD) with fixed learning rates as minimizing a free energy function F = U - TS, where U is training loss, S is the entropy of the weight distribution, and T is an effective temperature controlled by the learning rate. This view explains why high learning rates prevent convergence to the loss minimum and why different learning rates yield different final loss levels. The authors validate this framework empirically on both underparameterized (UP) and overparameterized (OP) models. In UP models, temperature increases smoothly with learning rate, while in OP models, it drops to zero at low learning rates, allowing convergence to the optimum. This difference is attributed to variations in the signal-to-noise ratio (SNR) of stochastic gradients near optima, supported by both toy examples and neural network experiments. The work provides a novel lens to interpret optimization dynamics and connects thermodynamic principles to machine learning behavior.

## Method Summary
The authors train fully scale-invariant networks on the unit sphere using projected SGD with fixed learning rates across a wide range of values. They estimate training loss U and entropy S of the weight distribution using k-NN graph-based estimators applied to sliding windows of the SGD trajectory. Temperature T(η) is inferred by finding the value that minimizes free energy F(η) = U(η) - T(η)S(η) at the observed (U, S) pair. The critical distinction between underparameterized and overparameterized regimes is based on whether the model can achieve 100% training accuracy. SNR analysis tracks how stochastic gradients behave near optima, distinguishing regimes where convergence is possible versus where noise prevents it.

## Key Results
- SGD with fixed learning rates converges to stationary distributions that minimize free energy F = U - TS rather than training loss alone
- Underparameterized models show monotonically increasing temperature with learning rate, while overparameterized models show temperature collapse to zero at low learning rates
- The difference between UP and OP convergence behavior stems from SNR of stochastic gradients: UP has SNR → 0 at optima, OP has SNR → constant
- Free energy curves are convex, confirming the thermodynamic interpretation across multiple architectures and datasets

## Why This Works (Mechanism)

### Mechanism 1: Free Energy Trade-off Balances Loss and Entropy
SGD with fixed learning rate implicitly minimizes a free energy function F = U - TS rather than training loss alone. The optimizer settles into stationary distributions where increasing learning rate (temperature) shifts the equilibrium toward higher-entropy, higher-loss regions. At low temperatures, loss dominates; at high temperatures, entropy maximization dominates, preventing convergence to sharp minima. The core assumption is that the weight distribution at stationarity can be characterized by a single temperature parameter that depends monotonically on learning rate.

### Mechanism 2: Signal-to-Noise Ratio Governs Convergence Regime
The difference between underparameterized (UP) and overparameterized (OP) convergence behavior stems from how stochastic gradient SNR behaves near optima. In UP settings, stochastic gradients remain non-zero at the global minimum while the full gradient vanishes, producing SNR → 0. This irreducible noise prevents convergence. In OP settings, both vanish simultaneously at interpolating solutions, allowing the effective temperature to decay toward zero and enabling convergence to exact minima. The core assumption is that the paper's definition of overparameterization requires that all per-sample gradients vanish at the global optimum (interpolation condition).

### Mechanism 3: Scale-Invariance Fixes Effective Learning Rate
Normalization layers induce scale-invariance that causes effective learning rate to vary during training; controlling for this is essential for isolating temperature effects. The paper uses projected SGD on unit sphere to fix weight norm, ensuring constant effective learning rate throughout training. This eliminates confounding dynamics where changing weight norms would modulate step sizes independently of the nominal learning rate. The core assumption is that the scale-invariant formulation captures the essential dynamics while removing a confounding factor.

## Foundational Learning

- **Concept: Stochastic Gradient Descent with Fixed Learning Rate**
  - Why needed here: The entire framework analyzes stationary behavior under constant learning rates, contrasting with the common practice of learning rate schedules. Understanding why fixed LR leads to loss plateaus rather than convergence is the motivating question.
  - Quick check question: If you run SGD with η = 0.01 for 10^6 iterations on a model that cannot achieve zero training loss, will the loss converge to the global minimum? Why or why not?

- **Concept: Differential Entropy and Its Estimation**
  - Why needed here: The entropy term S = -Ep(w)[log p(w)] is central to the free energy formulation, but direct computation is intractable in high dimensions. The paper uses k-nearest neighbor estimators as a proxy.
  - Quick check question: Why does entropy estimation require exponentially many samples in high dimensions, and what approximation does the paper use to make this tractable?

- **Concept: Signal-to-Noise Ratio for Gradients**
  - Why needed here: SNR = ||g||^p / E[||gi - g||^2] quantifies the relative strength of the mean gradient versus gradient variance. The paper uses SNR near optima to distinguish UP from OP regimes.
  - Quick check question: If SNR approaches zero near an optimum, what does this imply about the behavior of stochastic gradients? If SNR is constant and positive, what does that imply about convergence?

## Architecture Onboarding

- **Component map:** Projected SGD on Unit Sphere -> Entropy Estimator (k-NN) -> Temperature Inference -> SNR Monitor
- **Critical path:**
  1. Choose architecture and dataset; determine UP vs OP by checking if 100% training accuracy is achievable
  2. Run projected SGD with fixed LR until stationarity (monitor loss and entropy stabilization)
  3. Compute U(η) and S(η) across multiple LR values in the grid specified in Appendix B
  4. Construct T(η) by solving the minimization condition; verify monotonic increase
  5. For OP with low LR: monitor temperature decay T = dU/dS during training rather than at stationarity

- **Design tradeoffs:**
  - Scale-invariant projection vs. standard training: Projection isolates LR effects but may not reflect dynamics of standard networks with batch normalization
  - k-NN entropy estimator vs. parametric methods: Non-parametric avoids distributional assumptions but has known bias in high dimensions; paper assumes bias is constant across distributions
  - Fixed batch size vs. varying batch size: Paper holds B = 128 constant; corpus suggests temperature may scale as η/B, so batch size effects are unexplored

- **Failure signatures:**
  - Non-convex free energy curves indicate the framework does not apply
  - Temperature function that decreases with learning rate violates the hypothesis
  - SNR that increases rather than stabilizes during training in UP settings suggests insufficient training time
  - Power law exponent between stochastic and full gradient norms substantially below 1 in OP settings indicates insufficient overparameterization

- **First 3 experiments:**
  1. Replicate the ConvNet CIFAR-10 UP/OP comparison using the exact LR grid from Appendix B; verify that T(η) is monotonically increasing in the UP case and collapses to zero for low LR in the OP case.
  2. Test batch size dependence by varying B while holding η fixed; corpus signals suggest T ∝ η/B. If temperature changes with batch size in the same direction as learning rate changes, this supports the broader thermodynamic analogy.
  3. Apply the framework to a standard (non-projected) network with batch normalization; compare estimated temperature curves to assess whether the scale-invariant assumption is necessary for the free energy interpretation to hold.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effective temperature T depend on batch size, and does T ∝ η/B hold as suggested by prior SDE analyses?
- Basis in paper: [explicit] The authors note their "exclusive focus on LR, despite batch size being another important parameter that influences the magnitude of noise in SGD" as a main limitation.
- Why unresolved: All experiments fix batch size at 128; the relationship between temperature, learning rate, and batch size was not systematically investigated.
- What evidence would resolve it: Repeat the temperature estimation protocol across multiple batch sizes while varying LR, testing whether the relationship T ∝ η/B produces consistent free energy minima.

### Open Question 2
- Question: Can the free energy framework be extended to analyze quasi-stationary dynamics during training, particularly plateau phases followed by sudden capability acquisition?
- Basis in paper: [explicit] The authors state: "One more interesting direction is to extend the proposed thermodynamic framework to analyze not only the stationary behavior at the end of training, but also the quasi-stationary dynamics observed during training."
- Why unresolved: Current analysis only characterizes final stationary distributions, not the transient plateaus that occur during training.
- What evidence would resolve it: Track temperature, loss, and entropy during plateau phases; test whether free energy is locally minimized during these periods and what triggers transitions between plateaus.

### Open Question 3
- Question: Does the free energy minimization framework apply to adaptive optimizers like Adam and SGD with momentum?
- Basis in paper: [explicit] The authors identify as a limitation that "our analysis is restricted to plain SGD, leaving out widely used alternatives like SGD with momentum or Adam."
- Why unresolved: The thermodynamic interpretation assumes a specific relationship between learning rate and noise magnitude; adaptive methods modify gradients in ways that may change this relationship.
- What evidence would resolve it: Apply the same temperature estimation methodology to Adam and SGD+momentum; check whether a well-defined monotonic T(η) function exists for these optimizers.

## Limitations
- The framework applies only to scale-invariant networks trained with fixed learning rates, limiting generalization to standard training practices
- Entropy estimation in high dimensions introduces approximation errors that may affect temperature inference accuracy
- The scale-invariant projection, while useful for isolating learning rate effects, may not capture the full dynamics of standard networks with batch normalization

## Confidence
**High Confidence:** The empirical validation of free energy minimization in underparameterized settings (Section 5) with monotonic temperature curves and convex free energy profiles. The SNR-based distinction between UP and OP regimes (Section 6.1) is well-supported by theoretical arguments.

**Medium Confidence:** The mechanism by which SNR determines convergence behavior in overparameterized models, particularly the requirement that stochastic gradients vanish at the same rate as full gradients (power law exponent ≈ 1). The extrapolation of temperature curves to low learning rates relies on assumptions about noise scaling.

**Low Confidence:** The generalization of results to non-scale-invariant networks and the precise relationship between learning rate and effective temperature in standard training setups. The high-dimensional entropy estimation may have systematic biases not fully characterized.

## Next Checks
1. **Batch Size Sensitivity:** Vary batch size while holding learning rate constant to test whether temperature scales as η/B, as suggested by corpus evidence. This would strengthen the thermodynamic analogy beyond scale-invariant networks.

2. **Standard Network Comparison:** Apply the free energy framework to standard (non-projected) networks with batch normalization to assess whether temperature curves differ qualitatively from scale-invariant cases, indicating whether projection is essential for the thermodynamic interpretation.

3. **Overparameterization Threshold:** Systematically vary the width factor k to identify the critical overparameterization threshold where temperature collapses to zero. This would quantify how much overparameterization is needed for convergence and test the power law relationship between stochastic and full gradient norms.