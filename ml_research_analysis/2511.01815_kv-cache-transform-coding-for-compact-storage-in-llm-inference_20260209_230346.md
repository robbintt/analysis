---
ver: rpa2
title: KV Cache Transform Coding for Compact Storage in LLM Inference
arxiv_id: '2511.01815'
source_url: https://arxiv.org/abs/2511.01815
tags:
- compression
- cache
- answer
- error
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces KVTC, a transform-coding approach for compressing\
  \ key-value (KV) caches in large language model inference. KVTC combines PCA-based\
  \ feature decorrelation, adaptive quantization, and entropy coding to achieve up\
  \ to 20\xD7 compression while maintaining reasoning and long-context accuracy."
---

# KV Cache Transform Coding for Compact Storage in LLM Inference

## Quick Facts
- arXiv ID: 2511.01815
- Source URL: https://arxiv.org/abs/2511.01815
- Authors: Konrad Staniszewski; Adrian Łańcucki
- Reference count: 40
- Primary result: Up to 20× KV cache compression with maintained accuracy using PCA-based transform coding

## Executive Summary
This paper introduces KVTC (KV Cache Transform Coding), a method for compressing key-value (KV) caches in large language model inference using transform coding techniques. KVTC achieves up to 20× compression ratios while maintaining reasoning and long-context accuracy across multiple benchmark tasks. The approach combines PCA-based feature decorrelation, adaptive quantization, and entropy coding to exploit redundancy in KV caches without requiring model modifications.

The method uses a brief calibration phase to compute orthonormal projection matrices that maximize compression while preserving semantic information. Experiments demonstrate consistent accuracy retention on tasks like GSM8K, MMLU, and LiveCodeBench across Llama, Mistral, and Qwen model families. KVTC outperforms traditional quantization, eviction, and SVD-based compression baselines while enabling more efficient cache storage and transfer.

## Method Summary
KVTC addresses KV cache storage inefficiency through a transform-coding approach that decorrelates KV cache features using PCA, applies adaptive quantization based on error tolerance, and compresses residuals with entropy coding. The method begins with a calibration phase where it learns orthonormal projection matrices by maximizing the Frobenius norm of the projected cache matrix. During inference, the compressed cache is reconstructed by applying the inverse transform after decoding. The adaptive quantization scheme adjusts bit allocation based on token position and layer, optimizing the trade-off between compression ratio and accuracy retention. The approach requires no model modifications and produces reusable compressed caches that can be transferred between sessions.

## Key Results
- Achieves up to 20× compression ratio on KV caches while maintaining reasoning accuracy
- Outperforms quantization, eviction, and SVD-based baselines across Llama, Mistral, and Qwen models
- Maintains accuracy on GSM8K, MMLU, and LiveCodeBench benchmarks
- Enables efficient cache transfer and storage for scalable LLM serving

## Why This Works (Mechanism)
KVTC works by exploiting the redundancy and correlation present in KV cache representations. Transformer attention mechanisms generate KV values that contain significant redundancy across tokens and layers, which traditional compression methods fail to capture effectively. By applying PCA-based decorrelation, KVTC transforms correlated features into orthogonal components, concentrating information into fewer dimensions. Adaptive quantization then allocates bits based on the importance of each component, while entropy coding compresses the quantization residuals. This multi-stage approach preserves the semantic information critical for accurate attention computation while dramatically reducing storage requirements.

## Foundational Learning

**PCA (Principal Component Analysis)**: Dimensionality reduction technique that finds orthogonal directions of maximum variance. Needed to decorrelate KV cache features and concentrate information. Quick check: Verify projection matrices preserve at least 95% of variance in calibration data.

**Adaptive Quantization**: Variable-bit allocation based on feature importance and error tolerance. Needed to optimize the trade-off between compression ratio and accuracy. Quick check: Confirm quantization step sizes maintain reconstruction error below threshold.

**Entropy Coding**: Lossless compression of quantization residuals using variable-length codes. Needed to further compress the already-quantized data. Quick check: Measure compression ratio improvement over fixed-bit representation.

**Orthonormal Projection**: Matrix transformation preserving vector lengths and angles. Needed to ensure invertibility and minimize reconstruction error. Quick check: Verify projection matrices satisfy U^T U = I.

**KV Cache Redundancy**: Observation that attention keys/values contain correlated information across tokens and layers. Needed to justify compression approach. Quick check: Analyze correlation coefficients in raw KV cache data.

## Architecture Onboarding

**Component Map**: Raw KV Cache -> PCA Decorrelation -> Adaptive Quantization -> Entropy Coding -> Compressed Cache
Compressed Cache -> Entropy Decoding -> De-quantization -> Inverse PCA -> Reconstructed KV Cache

**Critical Path**: Calibration (compute projection matrices) -> Inference (compress/reconstruct KV cache) -> Attention computation with reconstructed cache

**Design Tradeoffs**: Higher compression ratios sacrifice accuracy; calibration phase adds setup overhead but enables optimal projections; entropy coding adds decoding latency but improves compression

**Failure Signatures**: Accuracy degradation when input distribution shifts significantly from calibration data; increased latency from entropy decoding; reconstruction artifacts when quantization error exceeds tolerance

**First Experiments**:
1. Measure compression ratio vs accuracy trade-off across different quantization bit allocations
2. Compare calibration time and storage overhead for different model architectures
3. Benchmark end-to-end inference latency with and without entropy decoding

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Dependence on calibration phase assumes stationarity in KV cache distribution, potentially suboptimal with shifting input data distributions
- Performance on extreme edge cases (highly atypical token sequences) remains unclear
- Entropy coding adds decoding latency not fully characterized in wall-clock impact during inference
- Scalability under extreme conditions (context lengths > 32K tokens, batch sizes > 256) warrants further investigation

## Confidence
**High Confidence**: The transform-coding methodology (PCA + quantization + entropy coding) is technically sound and the 20× compression figure is well-supported by the reported experiments.

**Medium Confidence**: Claims about accuracy retention across all tested tasks are reliable, but generalizability to arbitrary, real-world workloads remains to be validated.

**Medium Confidence**: The claim of "no model modifications required" is accurate for the compression scheme itself, though practical deployment may require infrastructure changes.

## Next Checks
1. Evaluate KVTC on a diverse, real-world dataset with non-stationary input distributions to test projection matrix robustness.
2. Characterize the end-to-end latency impact of entropy decoding during inference, including memory bandwidth effects.
3. Test KVTC's performance under extreme scaling conditions (context lengths > 32K tokens, batch sizes > 256) to validate scalability claims.