---
ver: rpa2
title: 'KaVa: Latent Reasoning via Compressed KV-Cache Distillation'
arxiv_id: '2510.02312'
source_url: https://arxiv.org/abs/2510.02312
tags:
- latent
- oken
- reasoning
- tokens
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training efficient latent
  reasoning models in large language models, which traditionally require verbose chain-of-thought
  traces that incur high computational costs and memory overhead. The authors propose
  KaVa, a novel framework that distills knowledge from a compressed teacher KV-cache
  into a latent reasoning student via self-distillation, leveraging the continuous
  nature of latent tokens to align stepwise KV trajectories without requiring direct
  token correspondence.
---

# KaVa: Latent Reasoning via Compressed KV-Cache Distillation

## Quick Facts
- arXiv ID: 2510.02312
- Source URL: https://arxiv.org/abs/2510.02312
- Reference count: 40
- Key outcome: KaVa achieves 62-92% fewer forward passes compared to full CoT while preserving accuracy on reasoning tasks

## Executive Summary
This paper introduces KaVa, a novel framework for training efficient latent reasoning models in large language models. The framework addresses the computational overhead of traditional chain-of-thought reasoning by distilling knowledge from a compressed teacher KV-cache into a latent reasoning student. KaVa leverages the continuous nature of latent tokens to align stepwise KV trajectories without requiring direct token correspondence, enabling more efficient inference while maintaining reasoning performance. The approach is particularly effective at bridging the gap between artificial equation-only reasoning traces and natural-language reasoning tasks.

## Method Summary
KaVa proposes a self-distillation framework where a student model learns latent reasoning from a compressed teacher's KV-cache. The key innovation lies in aligning continuous latent token trajectories rather than discrete token sequences, allowing the student to inherit the teacher's reasoning patterns without requiring exact token correspondence. The compressed KV-cache reduces memory overhead while preserving the essential reasoning information needed for distillation. This approach enables the student to generate reasoning steps in latent space, which are then decoded to produce final answers, resulting in significant computational savings compared to full chain-of-thought inference.

## Key Results
- KaVa consistently outperforms strong latent reasoning baselines on both artificial and natural-language reasoning datasets
- Shows markedly smaller performance degradation when moving from equation-only to natural-language traces compared to existing approaches
- Achieves 62-92% fewer forward passes compared to full CoT while preserving accuracy
- Demonstrates effective knowledge transfer from compressed teacher KV-cache to latent reasoning student

## Why This Works (Mechanism)
KaVa works by exploiting the continuous nature of latent tokens to create a more flexible alignment between teacher and student reasoning trajectories. Unlike traditional distillation that requires exact token correspondence, KaVa can match the overall reasoning pattern through continuous vector alignment in the KV-cache. The compression of the teacher's KV-cache preserves essential reasoning information while reducing computational overhead. The self-distillation process allows the student to learn not just final answers but the intermediate reasoning steps in compressed latent space, making the reasoning process more efficient while maintaining accuracy.

## Foundational Learning

**KV-cache compression** - Why needed: Reduces memory overhead during inference; Quick check: Verify compression ratio vs. performance retention on validation set

**Latent token representation** - Why needed: Enables continuous reasoning space for better alignment; Quick check: Compare reasoning coherence between latent and discrete token approaches

**Self-distillation** - Why needed: Allows student to learn from compressed teacher representations; Quick check: Measure knowledge transfer efficiency with different distillation temperature settings

**Stepwise KV trajectory alignment** - Why needed: Preserves reasoning patterns across compression; Quick check: Evaluate alignment quality using KL divergence between teacher and student distributions

## Architecture Onboarding

**Component map**: Input -> Encoder -> Latent Reasoning Module -> Compressed KV-cache -> Self-Distillation Loss -> Decoder -> Output

**Critical path**: The critical path involves the encoder generating latent representations, the reasoning module processing these through the compressed KV-cache, and the decoder producing final outputs. The self-distillation loss operates between the student's latent trajectory and the compressed teacher's KV-cache.

**Design tradeoffs**: The framework trades some precision in token-level correspondence for flexibility in reasoning pattern alignment. Compression reduces memory usage but may lose some fine-grained reasoning details. The continuous latent space enables better alignment but requires careful loss function design.

**Failure signatures**: Performance degradation when teacher-student architectural differences are too large, loss of reasoning coherence with aggressive compression, and difficulties in aligning highly divergent reasoning paths between teacher and student.

**First experiments**: 
1. Baseline comparison with full CoT on simple arithmetic reasoning tasks
2. Ablation study varying compression ratios and their impact on accuracy
3. Transfer learning test from equation-only to natural-language reasoning tasks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas warrant further investigation: generalization to non-reasoning tasks, robustness across different model sizes and architectures, and the interpretability of intermediate latent reasoning steps in real-world applications.

## Limitations
- Training process involves complex distillation objectives that may not generalize well beyond tested reasoning tasks
- Efficiency gains need more detailed baseline specifications and scaling analysis across different model sizes
- Quality and interpretability of intermediate reasoning steps in real-world applications is not fully characterized
- Continuous alignment may introduce failure modes when teacher and student have significant architectural differences

## Confidence
- Claim that KaVa "consistently outperforms strong latent reasoning baselines": High confidence based on reported empirical results
- Claim of "markedly smaller performance degradation when moving from equation-only to natural-language traces": Medium confidence - needs more diverse task testing
- Claim of "62-92% fewer forward passes while preserving accuracy": Medium confidence - requires more detailed baseline specifications
- Claim that latent reasoning via KV-cache distillation is a "novel" approach: High confidence - specific technique combination appears unique

## Next Checks
1. Test KaVa's performance and efficiency on non-reasoning tasks (e.g., code generation, summarization) to evaluate generalization beyond reasoning-focused datasets

2. Conduct ablation studies to quantify the contribution of each component (self-distillation, KV-cache compression, latent token alignment) to overall performance gains

3. Evaluate framework behavior when teacher and student models have substantially different architectures or sizes to assess distillation robustness