---
ver: rpa2
title: 'Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach'
arxiv_id: '2602.02173'
source_url: https://arxiv.org/abs/2602.02173
tags:
- classification
- optimal
- trees
- uni00000013
- latexit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a mixed-integer programming (MIP) framework
  for learning optimal classification trees that can directly optimize nonlinear,
  imbalance-aware metrics such as the F1-score and Matthews correlation coefficient.
  The approach aggregates duplicate instances to reduce problem size, builds on Benders
  decomposition, and introduces a branch-and-cut algorithm with problem-specific cuts.
---

# Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach

## Quick Facts
- arXiv ID: 2602.02173
- Source URL: https://arxiv.org/abs/2602.02173
- Reference count: 30
- One-line primary result: MIP framework for optimal classification trees that directly optimizes nonlinear, imbalance-aware metrics with strong scalability and performance on imbalanced classification tasks.

## Executive Summary
This paper introduces a mixed-integer programming (MIP) framework for learning optimal classification trees that can directly optimize nonlinear, imbalance-aware metrics such as the F1-score and Matthews correlation coefficient. The approach addresses the scalability challenges of previous methods by aggregating duplicate instances and building on Benders decomposition. The authors demonstrate that their framework outperforms state-of-the-art methods in both scalability and predictive performance, particularly on imbalanced classification tasks. The linearization of nonlinear metrics yields substantial computational gains over MIQP formulations while maintaining strong generalization performance.

## Method Summary
The authors propose a MIP framework that formulates optimal classification tree learning as a mixed-integer linear program. Key innovations include aggregating duplicate instances to reduce problem size, leveraging Benders decomposition for scalability, and introducing a branch-and-cut algorithm with problem-specific cuts. The framework directly optimizes nonlinear metrics by linearizing them through additional binary variables and constraints. The approach supports various tree structures and can handle both continuous and categorical features. Computational experiments demonstrate significant improvements in runtime and solution quality compared to existing methods, particularly for large-scale and imbalanced datasets.

## Key Results
- The MIP framework achieves superior scalability, handling datasets with up to 100,000 instances and 100 features more efficiently than state-of-the-art methods
- Direct optimization of nonlinear metrics like F1-score and MCC yields better predictive performance on imbalanced datasets compared to traditional accuracy-based approaches
- Linearization of nonlinear metrics provides substantial computational advantages over MIQP formulations while maintaining comparable or better solution quality

## Why This Works (Mechanism)
The framework succeeds by transforming the optimal tree learning problem into a tractable MIP formulation. By aggregating duplicate instances, the problem size is significantly reduced without losing essential information. The linearization of nonlinear metrics through additional binary variables and constraints enables the use of efficient LP solvers while maintaining the benefits of optimizing imbalance-aware objectives. The branch-and-cut algorithm with problem-specific cuts further enhances computational efficiency by exploiting the structure of the optimization problem. This combination of aggregation, linearization, and specialized cuts allows the framework to scale to larger problems while preserving the ability to optimize complex metrics.

## Foundational Learning
- Mixed-Integer Programming (MIP): Optimization framework combining linear programming with integer constraints; needed for modeling discrete tree structures and continuous feature splits
  - Quick check: Can represent both continuous and categorical features in unified formulation
- Benders Decomposition: Decomposition technique for large-scale optimization problems; needed to handle the exponential growth of subproblems in tree learning
  - Quick check: Separates master problem (tree structure) from subproblems (node-specific optimizations)
- Linearization of Nonlinear Metrics: Mathematical technique to approximate nonlinear functions with linear constraints; needed to enable efficient optimization of F1-score and MCC
  - Quick check: Introduces binary variables to represent piecewise linear approximations of nonlinear functions
- Branch-and-Cut Algorithm: Hybrid approach combining branch-and-bound with cutting planes; needed to improve computational efficiency through problem-specific cuts
  - Quick check: Incorporates valid inequalities that tighten the LP relaxation
- Instance Aggregation: Data preprocessing technique to reduce problem size; needed to handle datasets with many duplicate instances efficiently
  - Quick check: Groups identical instances and represents them with weights instead of individual rows

## Architecture Onboarding
- Component Map: Data Preprocessing -> Instance Aggregation -> MIP Formulation -> Benders Decomposition -> Branch-and-Cut Optimization -> Solution Extraction
- Critical Path: The most time-consuming step is typically the branch-and-cut optimization phase, particularly for complex tree structures and large datasets
- Design Tradeoffs: Linearization trades some approximation accuracy for computational tractability; aggregation reduces problem size but may lose some granularity
- Failure Signatures: Poor performance on datasets with few duplicates (aggregation provides limited benefit), convergence issues on extremely large trees, approximation errors in linearized metrics
- First Experiments: (1) Test on a small dataset with known duplicate instances to verify aggregation effectiveness, (2) Compare runtime and solution quality against baseline methods on a medium-sized dataset, (3) Evaluate F1-score optimization on an imbalanced binary classification problem

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability is demonstrated only up to 100,000 instances and 100 features, with unclear performance on higher-dimensional datasets
- Linearization approach may introduce approximation errors that are not fully characterized across different nonlinear metrics
- Framework performance on multi-class imbalanced classification and alternative tree structures (e.g., oblique trees) remains untested

## Confidence
- Scalability claims: Medium - Comprehensive experiments but limited to moderate dataset complexity
- Linearization approach: Medium-Low - Computationally efficient but potential approximation issues not fully explored
- Imbalance handling: Medium - Strong results on F1-score and MCC but narrow metric coverage
- Generalizability: Low - Limited testing on complex tree structures and diverse optimization objectives

## Next Checks
1. Test the framework on datasets with >1000 features to assess true scalability limits
2. Evaluate approximation errors introduced by linearization across a broader range of nonlinear metrics
3. Extend experiments to multi-class imbalanced classification tasks and alternative tree structures like oblique trees