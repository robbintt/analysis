---
ver: rpa2
title: 'CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition
  for Large Language Models'
arxiv_id: '2601.15628'
source_url: https://arxiv.org/abs/2601.15628
tags:
- task
- table
- data
- tasks
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CogToM, a comprehensive Theory of Mind (ToM)
  benchmark for large language models (LLMs) comprising over 8,000 bilingual instances
  across 46 task paradigms, grounded in human cognitive psychology. Unlike existing
  benchmarks that focus narrowly on false belief tasks, CogToM integrates 36 sub-capabilities
  across seven dimensions (emotion, desire, intention, percept, knowledge, belief,
  and non-literal communication), validated by 49 human annotators.
---

# CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models

## Quick Facts
- **arXiv ID:** 2601.15628
- **Source URL:** https://arxiv.org/abs/2601.15628
- **Reference count:** 40
- **Key outcome:** CogToM reveals LLMs excel at socio-emotional reasoning but struggle with foundational perceptual inference, suggesting a Moravec's Paradox in their cognitive architectures.

## Executive Summary
CogToM introduces a comprehensive Theory of Mind (ToM) benchmark for large language models (LLMs) comprising over 8,000 bilingual instances across 46 task paradigms, grounded in human cognitive psychology. Unlike existing benchmarks that focus narrowly on false belief tasks, CogToM integrates 36 sub-capabilities across seven dimensions (emotion, desire, intention, percept, knowledge, belief, and non-literal communication), validated by 49 human annotators. A systematic evaluation of 22 models, including GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneity, with models excelling in socio-emotional understanding but struggling with foundational perceptual reasoning. Joint analysis of model accuracy and human inter-annotator agreement, alongside alignment with human developmental milestones, suggests potential Moravec's Paradox in LLM cognitive architectures. CogToM offers a robust, discriminative instrument for probing the evolving cognitive boundaries of LLMs.

## Method Summary
CogToM employs a 6-stage construction pipeline: (1) adapting 46 psychological task paradigms to scene-based multiple-choice questions; (2) expert expansion to 5-10 seed groups per task; (3) LLM-automated expansion with structured prompts; (4) format and richness review; (5) double-blind human annotation for answers and quality; and (6) expert arbitration, de-duplication, and bilingual translation via Baidu API with manual verification. Evaluation uses zero-shot vanilla prompting with temperature=0, enforcing a specific output format, and implements a 5-trial protocol with cyclic option rotations plus one random shuffle to mitigate positional bias. The benchmark maps tasks to 7 cognitive dimensions (emotion, desire, intention, percept, knowledge, belief, non-literal) derived from the ATOMS psychological framework, enabling fine-grained analysis of LLM ToM capabilities.

## Key Results
- Models achieve near-ceiling performance on late-acquired emotional reasoning tasks (e.g., TEC: Hidden Emotions at 96%) while failing early-stage perceptual tasks (e.g., See-Know Task at 62%), demonstrating "developmental inversion."
- The Percept dimension shows 100% human agreement but <30% model accuracy, suggesting systematic failure in perspective-taking despite task clarity for humans.
- Model performance varies significantly across dimensions, with Belief and Knowledge showing the highest variance, indicating inconsistent grounding in different ToM capabilities.

## Why This Works (Mechanism)

### Mechanism 1
Grounding LLM ToM evaluation in established psychological paradigms reveals capability heterogeneity masked by narrow benchmarks. The framework maps 46 task paradigms to 7 cognitive dimensions derived from the ATOMS classification, decomposing ToM into 36 sub-capabilities and exposing performance variance across qualitatively different reasoning demands.

### Mechanism 2
Joint analysis of human inter-annotator agreement and model accuracy identifies cognitive asymmetries consistent with Moravec's Paradox. Tasks with high human consensus (>90% IAR) but low model accuracy (30-80%) reveal where models fail despite task clarity for humans, with the Percept dimension showing 100% human agreement but <30% model accuracy.

### Mechanism 3
LLMs exhibit "developmental inversion" relative to human ToM acquisition, mastering complex emotional reasoning before basic perceptual inference. When tasks are ordered by human developmental milestones (ages 0-6 years), many models show ascending or inverted trajectories, suggesting pattern-matching on linguistic regularities rather than grounded cognitive development.

## Foundational Learning

- **Concept:** Theory of Mind (ToM) Dimensions
  - Why needed here: Understanding the 7-dimension taxonomy is prerequisite to interpreting benchmark results and designing targeted evaluations.
  - Quick check question: Can you name three cognitive dimensions beyond false belief that CogToM evaluates?

- **Concept:** False Belief Task Paradigms
  - Why needed here: Many ToM benchmarks conflate ToM with false belief; CogToM expands coverage to 46 paradigms including first-order, second-order, and content/location variants.
  - Quick check question: What distinguishes a "location false belief" task from a "content false belief" task?

- **Concept:** Moravec's Paradox in AI
  - Why needed here: The paper invokes this paradox to explain why models excel at high-level socio-emotional reasoning while failing at low-level perceptual inference.
  - Quick check question: In the context of this paper, what type of task exemplifies the "easy for humans, hard for AI" side of the paradox?

## Architecture Onboarding

- **Component map:**
  - Task Paradigm Layer (46 paradigms) -> Data Construction Pipeline (6 stages) -> Annotation Infrastructure (49 annotators) -> Evaluation Protocol (5-trial zero-shot)

- **Critical path:**
  1. Map psychological task to ToM sub-capability
  2. Generate scene-question-option triplets with distractor design
  3. Double-blind human annotation
  4. Expert arbitration for discrepancies
  5. Bilingual translation with manual verification

- **Design tradeoffs:**
  - Multiple-choice format ensures objective scoring but may not capture generative ToM
  - Static scenes enable reproducibility but miss recursive mental state updating
  - Text-only modality sacrifices ecological validity for visual-perceptual tasks

- **Failure signatures:**
  - High variance in Belief/Knowledge dimensions suggests inconsistent grounding
  - Percept dimension ceiling at ~20% indicates systematic failure in perspective-taking
  - Tasks like Yummy-yucky (60% average) show intra-category heterogeneity

- **First 3 experiments:**
  1. Evaluate your model on 3-5 tasks spanning Emotion (high-performing), Belief (variance), and Percept (bottleneck) to calibrate position in the performance landscape.
  2. Order your model's performance by the human milestone sequence to test for developmental inversion.
  3. For tasks where your model underperforms relative to human IAR, examine whether errors cluster on spatial/embodied reasoning or linguistic ambiguity.

## Open Questions the Paper Calls Out

### Open Question 1
How does LLM performance on static textual ToM benchmarks compare to dynamic, interactive settings requiring recursive mental state updating? The authors note their tasks consist of "static scenes" and "single-turn evaluations," which fail to replicate the "recursive and dynamic updating of mental states during live interaction" found in real-world scenarios.

### Open Question 2
Does the text-based adaptation of perceptual ToM tasks (like the Yoni task) underestimate LLM capabilities due to a loss of ecological validity? The authors acknowledge that translating visual or dynamic cues into textual descriptions "inevitably results in a loss of ecological validity compared to original psychological assessments."

### Open Question 3
Do the ToM capabilities identified in CogToM generalize to linguistic structures and cultural norms distinct from the Chinese-English paradigm? The authors explicitly limit their scope to Chinese and English, noting that "ToM reasoning is deeply intertwined with linguistic structures and cultural norms" and calling for expansion in future work.

## Limitations
- Dataset and task specification files are not publicly released, limiting independent validation of the 6-stage construction pipeline
- Claims about Moravec's Paradox are based on a single pattern without ruling out training data distribution effects
- Text-based adaptation of perceptual tasks may underestimate LLM capabilities due to loss of ecological validity

## Confidence

- **High:** CogToM benchmark construction methodology and task mapping to psychological paradigms
- **Medium:** Model performance heterogeneity across dimensions (22 models tested with consistent protocol)
- **Low:** Moravec's Paradox interpretation and developmental inversion claims (mechanistic evidence is correlational)

## Next Checks

1. Reconstruct the cyclic rotation algorithm from the described "4 cyclic option rotations + 1 random shuffle" protocol and verify it eliminates positional bias in pilot tests.
2. Test whether training corpus analysis shows over-representation of emotional narratives relative to perceptual scenarios, which could explain developmental inversion without invoking architectural constraints.
3. Evaluate a subset of high-IAR, low-accuracy tasks with human participants who lack the specific cultural/linguistic context to determine if errors reflect universal cognitive constraints versus model-specific failures.