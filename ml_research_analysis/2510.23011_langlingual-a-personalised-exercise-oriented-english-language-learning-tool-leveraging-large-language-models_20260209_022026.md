---
ver: rpa2
title: 'LangLingual: A Personalised, Exercise-oriented English Language Learning Tool
  Leveraging Large Language Models'
arxiv_id: '2510.23011'
source_url: https://arxiv.org/abs/2510.23011
tags:
- language
- learning
- langlingual
- https
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LangLingual, a web-based conversational agent
  for English language learning powered by Large Language Models (LLMs). The system
  provides real-time, grammar-focused feedback, generates context-aware language exercises,
  and tracks learner proficiency over time using a hybrid scoring approach combining
  rule-based and model-based evaluation.
---

# LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models

## Quick Facts
- arXiv ID: 2510.23011
- Source URL: https://arxiv.org/abs/2510.23011
- Reference count: 40
- Primary result: Web-based conversational agent providing real-time grammar feedback, context-aware exercises, and hybrid proficiency tracking.

## Executive Summary
LangLingual is a web-based conversational agent for English language learning powered by Large Language Models (LLMs). The system provides real-time, grammar-focused feedback, generates context-aware language exercises, and tracks learner proficiency over time using a hybrid scoring approach combining rule-based and model-based evaluation. LangLingual was evaluated through surveys with 7 learners and persona-based testing with 3 distinct learner profiles. Survey results indicated strong usability and positive learning outcomes, with participants reporting increased motivation and perceived improvement in reducing grammatical errors. The persona-based evaluation demonstrated the system's ability to provide contextually appropriate and tailored feedback across diverse learner needs.

## Method Summary
LangLingual leverages a hybrid proficiency estimation system combining a 50,000-word bank labeled 1-14 with LLM-based level prediction (weights: 0.4 word bank, 0.6 LLM). The system uses LangChain v0.3 for orchestration, Supabase for authentication and data storage, ChromaDB for vector-based resource retrieval, and OpenAI GPT models for responses and analysis. After 3+ learner interactions, an LLM identifies systematic grammatical patterns with confidence >0.3. Context-aware exercises are injected based on detected phrases in assistant responses. The frontend is built with Streamlit, hosted on Streamlit Cloud.

## Key Results
- Survey participants reported strong usability and perceived learning improvements, particularly in reducing grammatical errors
- System successfully generated contextually appropriate exercises and feedback across 3 distinct learner personas
- Hybrid scoring approach demonstrated potential for more robust proficiency estimation than single-method alternatives

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Proficiency Estimation
- Claim: Combining lexical lookup with LLM judgment yields more robust proficiency estimates than either method alone.
- Mechanism: Learner input processed through two parallel paths: (1) lemmatized words matched against 50,000-word bank (levels 1-14), computing average/median levels; (2) same text passed to LLM requesting level prediction. Final score = 0.4 × word_bank_level + 0.6 × LLM_level.
- Core assumption: Word bank's 14-level schema validly maps to proficiency, and LLM judgments are sufficiently calibrated to same scale.
- Evidence anchors: [abstract] "hybrid scoring approach combining rule-based and model-based evaluation"; [section 3.2] Formula and weight specification (w_wb = 0.4, w_llm = 0.6)

### Mechanism 2: Context-Aware Exercise Injection
- Claim: Embedding exercises into conversational flow based on detected learner context sustains engagement and targets weak areas.
- Mechanism: After LLM generates response, exercise generation module scans for exercise-related phrases. When keywords found, message flagged as exercise, storing exercise_type and exercise_prompt. Dictionary tracks active exercise state.
- Core assumption: Learners persist through embedded exercises rather than abandoning when confronted with structured tasks.
- Evidence anchors: [abstract] "generates context-aware language exercises"; [section 3.3] "checks the assistant reply for exercise-related phrases... flagged as an exercise"

### Mechanism 3: Pattern-Based Improvement Area Detection
- Claim: Aggregating learner input across multiple turns before LLM analysis reduces noise and surfaces systematic error patterns.
- Mechanism: Learner messages accumulate until heuristic threshold (≥3 interactions). Aggregated text sent to LLM with instructions to identify consistent grammatical/lexical issues, returning JSON array with {area, confidence, examples}. Only areas with confidence > 0.3 are surfaced.
- Core assumption: Three interactions provide sufficient signal; LLM can reliably distinguish systematic errors from one-off slips.
- Evidence anchors: [abstract] "tracks learner proficiency over time"; [section 3.4] "heuristic value of 3 learner interactions... only areas with confidence greater than 0.3"

## Foundational Learning

- Concept: **LangChain orchestration (chains, memory, prompts)**
  - Why needed here: LangLingual relies on LangChain v0.3 to manage conversation history, prompt chaining, and RAG retrieval. Without understanding chains and memory, you cannot modify how context persists across sessions.
  - Quick check question: Can you explain how a `ConversationBufferMemory` differs from `ConversationSummaryMemory` and when you'd use each?

- Concept: **LLM-as-judge evaluation**
  - Why needed here: Proficiency assessment delegates scoring to an LLM prompt. Understanding prompt design, temperature settings, and output parsing is essential for maintaining calibration.
  - Quick check question: What prompt engineering techniques reduce variability when asking an LLM to output a numeric score on a fixed scale?

- Concept: **Vector embeddings and similarity search**
  - Why needed here: ChromaDB stores document embeddings for RAG-based retrieval of learning resources. Understanding embedding models and similarity thresholds is required to debug retrieval quality.
  - Quick check question: If retrieval returns irrelevant resources, which two parameters would you check first?

## Architecture Onboarding

- Component map: Frontend (Streamlit) -> Supabase (auth & DB) -> LangChain (orchestration) -> OpenAI (LLM) -> ChromaDB (vector store)
- Critical path: 1) User authenticates via Supabase → profile loaded; 2) User sends message (text or voice via Whisper); 3) LangChain aggregates conversation history + retrieves relevant resources from ChromaDB; 4) LLM generates response; exercise module scans for exercise phrases; 5) After ≥3 learner messages, improvement area analysis triggers; 6) Proficiency score updated via hybrid formula; results stored in Supabase
- Design tradeoffs: Model-agnostic architecture vs. current OpenAI lock-in (abstract claims future scalability, but Whisper and GPT-specific prompts create migration friction); Hybrid scoring weights (0.4/0.6) chosen empirically without external validation; Heuristic threshold of 3 interactions balances latency vs. signal
- Failure signatures: Proficiency score fluctuates wildly between sessions → check word bank coverage for user's vocabulary domain; verify LLM prompt hasn't changed; Exercise detection misses or hallucinates exercises → audit keyword list in exercise generation module; Improvement areas empty or noisy → inspect aggregated text length; verify confidence threshold and LLM output parsing
- First 3 experiments: 1) Proficiency calibration check: Run 50 sample learner inputs through hybrid scoring; compare against human rater judgments; adjust weights if systematic over/under-estimation appears; 2) Exercise detection precision/recall: Manually label 100 assistant responses as exercise-containing or not; measure detection module accuracy; 3) Improvement area latency analysis: Vary interaction threshold (2, 3, 5) and measure time-to-feedback vs. area quality (human-rated relevance)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LangLingual's internal proficiency scale be effectively aligned with the Common European Framework of Reference for Languages (CEFR)?
- Basis in paper: [explicit] "We would also like to align proficiency levels in LangLingual with internationally recognised standards like the Common European Framework of Reference (CEFR)..."
- Why unresolved: Current system relies on custom 1-14 rubric derived from specific course rather than global standards.
- What evidence would resolve it: Validation study correlating LangLingual's level predictions with standardized CEFR exam scores.

### Open Question 2
- Question: To what extent do gamified elements (streaks, notifications) and proactive prompts improve learner retention compared to current reactive model?
- Basis in paper: [explicit] "LangLingual can be transformed from a reactive assistant to a proactive daily learning partner... [and] learner retention can be improved by adding features such as streak tracking..."
- Why unresolved: Existing implementation functions reactively; proposed gamification features are planned but untested.
- What evidence would resolve it: Longitudinal A/B testing measuring engagement frequency and drop-off rates with and without gamified features.

### Open Question 3
- Question: Does LangLingual produce measurable improvements in objective English proficiency beyond self-reported perceptions?
- Basis in paper: [inferred] Evaluation relied on surveys with only 7 participants and author-scored persona testing.
- Why unresolved: Study relies on qualitative feedback and perceived improvement ("participants report feeling significantly more motivated") rather than quantitative, longitudinal skill assessment.
- What evidence would resolve it: Randomized controlled trial (RCT) utilizing standardized pre- and post-testing to measure actual grammatical gains.

## Limitations
- The 50,000-word bank with 1-14 proficiency labels is not publicly available, making exact replication difficult
- LLM prompts for proficiency estimation, exercise generation, and improvement detection are not specified, introducing variability in reproduction
- Small evaluation sample (7 survey participants, 3 personas) limits generalizability of reported usability and learning outcomes

## Confidence
- **High Confidence**: LangChain-based architecture and integration of Supabase, Streamlit, ChromaDB are clearly specified and reproducible
- **Medium Confidence**: Hybrid scoring mechanism works as described, but exact performance depends on undisclosed word bank and prompt details
- **Low Confidence**: Survey-based learning outcomes and persona-based testing results are difficult to verify without raw data and full evaluation protocols

## Next Checks
1. **Proficiency Calibration**: Run 50 sample learner inputs through hybrid scoring system and compare results against human expert ratings to identify systematic bias or drift
2. **Exercise Detection Precision**: Manually annotate 100 assistant responses for exercise presence, then measure detection module's precision and recall to quantify over- and under-detection
3. **Improvement Area Signal Quality**: Vary the interaction threshold (2, 3, 5) and assess time-to-feedback versus human-rated relevance of detected improvement areas to optimize the heuristic