---
ver: rpa2
title: 'Meet Your New Client: Writing Reports for AI -- Benchmarking Information Loss
  in Market Research Deliverables'
arxiv_id: '2508.15817'
source_url: https://arxiv.org/abs/2508.15817
tags:
- layout
- pptx
- question
- document
- markdown
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks information loss in market research deliverables
  as they are converted to Markdown for use in retrieval-augmented generation (RAG)
  systems. It compares PDF and PowerPoint (PPTX) formats using four open-source Markdown
  conversion libraries and three large language models (LLMs) to answer factual questions
  from 41 PPTX documents containing 468 questions.
---

# Meet Your New Client: Writing Reports for AI -- Benchmarking Information Loss in Market Research Deliverables

## Quick Facts
- arXiv ID: 2508.15817
- Source URL: https://arxiv.org/abs/2508.15817
- Reference count: 8
- Primary result: Text extraction from market research documents achieves >90% accuracy, but complex objects like charts and diagrams suffer significant information loss during Markdown conversion

## Executive Summary
This study benchmarks information loss when converting PDF and PowerPoint market research deliverables to Markdown for retrieval-augmented generation systems. Using 41 documents containing 468 factual questions, the research evaluates four open-source Markdown conversion libraries across five layout elements: text, tables, diagrams, data charts, and images. Results show that while text is reliably extracted with over 90% accuracy, complex visual objects experience substantial degradation, with diagram accuracy ranging from 54% to 88.5% and data charts from 16.7% to 70.1%. The findings reveal that current delivery formats are not optimal for machine processing, suggesting a need for specialized, AI-native deliverables designed explicitly for knowledge management systems.

## Method Summary
The study converted 41 PPTX documents (manually converted to PDF) using four open-source Markdown conversion libraries: Docling v2.38.0, Marker v1.7.5, Markitdown v0.1.2, and Zerox v0.1.06. Each library employed Vision Language Models (Llama4 Maverick, Qwen2.5-32B, or GPT-4.1-mini) to caption images. The resulting Markdown documents were used to answer 468 factual questions covering five layout elements through three QA models. An LLM-as-judge (o4-mini) evaluated answer correctness. The research systematically compared text, table, diagram, data chart, and image extraction accuracy across both file formats and all library combinations.

## Key Results
- Text extraction achieves >90% accuracy across all libraries and file formats
- Diagram accuracy ranges from 54% to 88.5%, depending on library and file type
- Data chart accuracy shows high variance from 16.7% to 70.1%
- PPTX's structured XML format does not consistently outperform PDF due to library limitations
- Significant information loss occurs with complex visual objects through VLM captioning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text extraction preserves semantic content with high reliability
- Mechanism: Conversion libraries parse text fields as contiguous character sequences, preserving lexical and semantic information
- Core assumption: Text fields are cleanly segmented and do not rely on spatial relationships
- Evidence anchors: "text is reliably extracted with over 90% accuracy" (abstract), "Text is accurately converted by all libraries... scoring above 90% accuracy" (section 4.1)
- Break condition: Text embedded in images or split across layout elements requiring cross-reference

### Mechanism 2
- Claim: Complex visual objects experience significant information loss during Markdown conversion
- Mechanism: Pipeline libraries detect visual elements, extract as images, and use VLM to generate textual captions, introducing compounding error
- Core assumption: VLM captions are the primary pathway for chart/diagram content into Markdown
- Evidence anchors: "significant information loss occurs with complex objects like charts and diagrams" (abstract), "Diagrams have lower accuracy than text and tables" (section 4.1)
- Break condition: If libraries extracted underlying chart data rather than captioning rendered images

### Mechanism 3
- Claim: PPTX's structured XML format does not consistently outperform PDF
- Mechanism: While PPTX stores objects in semantic hierarchy, conversion libraries have uneven support and prioritize PDF pipelines
- Core assumption: Better native format structure translates to better conversion only if library exploits that structure
- Evidence anchors: "PPTX is a semantic, XML-based format" (section 3.2), "We do not find support for our hypothesis that PPTX files... are generally better suited" (section 4.1)
- Break condition: If a library fully exploited PPTX structure, PPTX could outperform PDF

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Study premise is that documents are ingested by RAG-based knowledge management systems
  - Quick check question: Can you explain why document conversion quality affects retrieval and generation in a RAG pipeline?

- Concept: Vision Language Models (VLMs) for Document Understanding
  - Why needed here: All tested libraries use VLMs to caption images, which is a bottleneck for charts and diagrams
  - Quick check question: What types of information might a VLM fail to capture when describing a complex data chart?

- Concept: LLM-as-Judge Evaluation
  - Why needed here: Study uses LLM to evaluate answer correctness semantically rather than with lexical metrics
  - Quick check question: Why might an LLM judge be preferred over BLEU or ROUGE for evaluating factual QA answers?

## Architecture Onboarding

- Component map: Source formats (PPTX, PDF) -> Conversion libraries (Docling, Marker, Markitdown, Zerox OCR) -> VLMs (Llama4, Qwen2.5, GPT-4.1-mini) -> QA models (same three) -> Judge model (o4-mini) -> Output (Markdown with image captions)

- Critical path: 1) Document -> Conversion library -> Markdown with image captions, 2) Markdown + question -> QA model -> generated answer, 3) Answer + ground truth -> Judge model -> correctness score

- Design tradeoffs: Pipeline vs VLM conversion (control vs robustness), PDF vs PPTX (tool support vs theoretical structure), long-context vs chunking (full context vs retrieval)

- Failure signatures: Missing image captions (Docling on PPTX, Markitdown on PDF), conversion errors (Marker on WMF files, Markitdown on None attributes), silent failures (incomplete extraction), low accuracy on charts/diagrams (captioning failure)

- First 3 experiments:
  1. Baseline conversion comparison: Run all four libraries on 5-10 document samples each, measure accuracy per layout element
  2. Format-specific diagnosis: Compare PPTX vs PDF conversion output side-by-side on single document, manually inspect information loss
  3. VLM caption quality audit: Extract 10 charts/diagrams, generate captions with each VLM, manually score completeness, correlate with QA accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on LLM-as-judge evaluation, introducing subjectivity and potential bias
- Uses limited dataset of 41 documents (468 questions) that may not represent full diversity of market research deliverables
- Focuses exclusively on four open-source Markdown conversion libraries, potentially missing other approaches

## Confidence

- **High Confidence**: Text extraction reliability (>90% accuracy) - directly measured and consistent
- **Medium Confidence**: Diagram and chart information loss findings - based on limited sample size and VLM quality
- **Medium Confidence**: PPTX vs PDF performance comparison - constrained by library support rather than format differences
- **Low Confidence**: Generalization to broader market research document types beyond FinanceBench dataset

## Next Checks
1. Expand dataset diversity: Test methodology on 100+ documents spanning multiple market research domains to assess generalizability
2. Alternative conversion approaches: Compare Markdown outputs with structured JSON representations preserving chart data and object hierarchies
3. Cross-judge validation: Re-evaluate answer correctness using multiple judge models (both LLM and human) to assess scoring reliability