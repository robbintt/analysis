---
ver: rpa2
title: Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained
  Discrete Problems? A Study on Sudoku
arxiv_id: '2601.20363'
source_url: https://arxiv.org/abs/2601.20363
tags:
- sampling
- score
- diffusion
- sudoku
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies whether continuous-time generative models\u2014\
  flow matching and score-based diffusion models\u2014can learn to represent and sample\
  \ from highly constrained discrete distributions, using completed Sudoku grids as\
  \ a testbed. The authors train models along a Gaussian probability path and compare\
  \ deterministic (ODE) and stochastic (SDE) sampling strategies, as well as DDPM-style\
  \ ancestral samplers derived from the same continuous-time training."
---

# Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku

## Quick Facts
- **arXiv ID:** 2601.20363
- **Source URL:** https://arxiv.org/abs/2601.20363
- **Reference count:** 31
- **Primary result:** Stochastic diffusion samplers can generate and solve globally constrained discrete problems like Sudoku; deterministic methods fail.

## Executive Summary
This paper investigates whether continuous-time diffusion models—flow matching and score-based models—can generate and solve globally constrained discrete problems using Sudoku as a testbed. The authors train models on a Gaussian probability path and compare deterministic ODE sampling, stochastic SDE sampling, and DDPM-style ancestral sampling. Results show that stochastic sampling is essential for generating valid Sudoku grids, with DDPM ancestral samplers achieving over 83% validity compared to near-zero for deterministic methods. The same models can solve constrained Sudoku puzzles via guided sampling, though with lower sample efficiency than classical solvers.

## Method Summary
The study uses a lightweight Transformer (4 blocks, 128 hidden dim, 8 heads, ~3.3M parameters) with row/column/box positional embeddings and Fourier time conditioning to model Sudoku grids. Grids are encoded as (81, 9) logits tensors. Two training schedules are used: linear (α=t, β=1-t) for guided solving and cosine for unconditional generation. Models are trained to predict velocity fields (flow matching) or rescaled scores (score matching) with 300K iterations. Sampling strategies compared include deterministic ODE (Euler), stochastic SDE (Euler-Maruyama), and DDPM/DDIM ancestral samplers. Guided solving uses soft-to-hard constraint injection with threshold τ.

## Key Results
- Stochastic sampling is essential: ODE achieves near 0% validity while SDE and DDPM reach 25% and 83% respectively
- DDPM ancestral sampling outperforms continuous-time SDE due to alignment with training objective
- Inference-time dropout prevents DDIM mode collapse to single solutions
- Guided solving is possible but less sample-efficient than classical solvers

## Why This Works (Mechanism)

### Mechanism 1
Stochastic sampling is required for sparse constraint manifolds. Deterministic ODE trajectories drift off the sparse Sudoku manifold due to approximation errors, while stochastic sampling allows local search that finds valid solutions.

### Mechanism 2
Ancestral sampling preserves validity better than continuous-time integration. DDPM/DDIM directly invert marginal noise distributions, preventing error accumulation that occurs when continuous-time solvers integrate approximate drift fields.

### Mechanism 3
Inference-time dropout prevents DDIM collapse. Without noise injection, deterministic DDIM converges to a single valid Sudoku regardless of initialization, but dropout maintains diversity by breaking this deterministic mapping.

## Foundational Learning

- **Gaussian Probability Paths & Interpolation**: Understanding α(t) and β(t) scaling is required to distinguish between Linear and Trigonometric paths. *Quick check:* Why does the linear path violate the DDPM constraint α²+β²=1?
- **Score vs. Velocity Parameterization**: Comparing u_θ (flow matching) vs. ∇log p_t (score matching) requires understanding their mathematical relationship and stability differences. *Quick check:* What rescaling trick is used for score-based loss when score diverges?
- **Ancestral vs. Markovian Sampling**: "Markovian" DDPM chains perform poorly (~2% success) while "Ancestral" sampling achieves >80%. *Quick check:* How does the Ancestral sampler use clean sample estimate to generate next step?

## Architecture Onboarding

- **Component map:** Flattened grid → positional embeddings (Row+Col+Box) → time injection (Fourier→MLP) → 4-layer Transformer → velocity/score prediction
- **Critical path:** 1) Encode flattened grid with positional embeddings 2) Project time t and add to token embeddings 3) Apply 4 self-attention layers 4) Predict velocity or rescaled score
- **Design tradeoffs:** Use Cosine/Trigonometric schedules for unconditional generation vs. Linear for solving; β(t)-scaled noise for solving vs. constant σ for generation; enable dropout at inference for DDIM
- **Failure signatures:** ODE yields ~0% validity (requires stochasticity); DDIM without dropout collapses to single solution; Markovian DDPM chain fails (~2% vs 83% ancestral); score training unstable near t=1 (requires rescaling)
- **First 3 experiments:** 1) Compare ODE vs SDE vs Ancestral validity rates (expect ODE≈0, SDE≈25%, Ancestral>80%) 2) Test DDIM with/without inference dropout (ON produces diverse, OFF produces single solution) 3) Implement soft→hard constraint injection for guided solving and measure batch requirements

## Open Questions the Paper Calls Out

### Open Question 1
Why do diffusion dynamics that deviate from the probability path outperform path-consistent dynamics under hard conditioning? The paper observes that β(t)-scaled samplers deviate from the prescribed path yet yield better results for guided solving, but lacks theoretical explanation for this advantage.

### Open Question 2
Do these findings generalize to other complex combinatorial domains? Results may not extend beyond Sudoku to problems with different constraint graphs or variable sizes, requiring validation on other discrete reasoning benchmarks.

### Open Question 3
Can principled methods for constraint injection significantly improve sample efficiency? The study uses a two-stage soft-to-hard clamping heuristic, leaving potential optimization-aware or control-theoretic guidance mechanisms unexplored.

## Limitations

- Results may not generalize beyond Sudoku to other combinatorial domains with different constraint structures
- The exact sensitivity to solver hyperparameters (step size, order) is not fully characterized
- The mechanism by which training objective misalignment causes drift is qualitative rather than quantitatively analyzed

## Confidence

- **High Confidence:** Stochastic sampling is essential for valid Sudoku generation while deterministic methods fail
- **Medium Confidence:** DDPM ancestral sampling significantly outperforms continuous-time SDE, though reasons (error accumulation vs objective alignment) need more investigation
- **Medium Confidence:** DDIM collapse mechanism and dropout mitigation is plausible but alternative explanations warrant exploration

## Next Checks

1. **Solver Sensitivity Analysis:** Systematically vary step size and solver order for SDE integration, comparing Euler-Maruyama, Milstein, and higher-order solvers against DDPM ancestral sampling.

2. **Training Objective Ablation:** Train models with objectives that explicitly align with discrete DDPM marginals and compare continuous-time solver performance against standard models.

3. **Constraint Manifold Density Test:** Generate synthetic combinatorial problems with varying constraint densities and analyze how deterministic vs stochastic sampling performance changes with constraint sparsity.