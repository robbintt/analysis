---
ver: rpa2
title: 'No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning
  in TTS'
arxiv_id: '2509.18531'
source_url: https://arxiv.org/abs/2509.18531
tags:
- preference
- grpo
- prosody
- reward
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Without a verifiable automatic reward for prosody, GRPO trained
  on CER/NLL collapses speech into monotone renderings, and adding speaker-similarity
  destabilizes training. Iterative Direct Preference Optimization (DPO) with only
  ~200 human preference pairs per round directly optimizes prosodic naturalness while
  regularizing to the current model.
---

# No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS

## Quick Facts
- arXiv ID: 2509.18531
- Source URL: https://arxiv.org/abs/2509.18531
- Reference count: 0
- Primary result: Iterative Direct Preference Optimization (DPO) with ~200 human preference pairs per round achieves ELO 1190.1 in human preference while maintaining CER 3.6% on KoCC-TTS, outperforming GRPO baselines.

## Executive Summary
This work investigates prosody learning in text-to-speech (TTS) systems without relying on verifiable automatic rewards. The authors find that GRPO trained with character error rate (CER) or negative log-likelihood (NLL) as proxies collapses speech into monotone renderings. Instead, they propose iterative Direct Preference Optimization (DPO), which directly optimizes prosodic naturalness using small numbers of human preference pairs while regularizing to the current model. Their approach achieves state-of-the-art human preference ratings on the KoCC-TTS dataset.

## Method Summary
The authors implement an iterative DPO framework that alternates between fine-tuning the TTS model using preference pairs and collecting new human preferences from the updated model. Each iteration uses approximately 200 human preference pairs to guide prosodic optimization while maintaining a regularization term that preserves the current model's characteristics. The system avoids the training collapse observed with GRPO when using automatic metrics like CER or NLL as proxy rewards for prosody.

## Key Results
- Iterative DPO achieves highest human preference (ELO 1190.1) in round 2 of preference collection
- Maintains competitive CER of 3.6% while optimizing for prosodic naturalness
- Outperforms GRPO baseline (CER 2.2%, ELO 753.7) and commercial systems
- Demonstrates effectiveness of preference learning with minimal human annotation

## Why This Works (Mechanism)
The method works by directly optimizing for perceptual prosodic quality through human preferences rather than relying on proxy automatic metrics that fail to capture prosodic nuances. The iterative nature allows the model to progressively refine its prosody based on human judgments while the regularization term prevents catastrophic forgetting. This approach sidesteps the fundamental problem that no verifiable automatic reward for prosody currently exists.

## Foundational Learning
- **Direct Preference Optimization**: Why needed - to bypass unreliable automatic prosody metrics; Quick check - verify preference pairs show clear winner distinctions
- **Iterative Fine-tuning**: Why needed - to progressively improve based on updated human feedback; Quick check - track Elo changes across iterations
- **Regularization in DPO**: Why needed - to prevent model drift away from stable speech generation; Quick check - monitor CER stability across iterations
- **Human Preference Elicitation**: Why needed - ground truth for prosodic quality assessment; Quick check - verify Krippendorff's alpha consistency
- **Prosody-Character Trade-off**: Why needed - balance naturalness with accuracy; Quick check - plot CER vs Elo trade-off curves

## Architecture Onboarding
- **Component Map**: TTS model -> Preference Model -> Fine-tuning Module -> Updated TTS Model -> Human Preference Collection
- **Critical Path**: Preference collection → DPO fine-tuning → model update → next preference collection
- **Design Tradeoffs**: Small preference sets (200) vs. computational efficiency vs. training stability
- **Failure Signatures**: Training collapse with automatic metrics, Elo stagnation, CER degradation
- **First Experiments**: 1) Run single DPO iteration with 200 pairs; 2) Compare Elo before/after regularization; 3) Test with different preference pair counts

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single internal dataset (KoCC-TTS)
- Human preference protocol involves only 12 annotators with moderate inter-annotator agreement
- Does not investigate robustness across different speaker characteristics or linguistic contexts

## Confidence
- No verifiable automatic rewards for prosody exist (High confidence)
- Iterative DPO with minimal human preferences is effective (Medium confidence)
- Comparison with commercial baselines lacks detailed architectural specifications (Low confidence)

## Next Checks
1. Evaluate the iterative DPO approach on multi-speaker, multilingual TTS datasets to assess cross-domain generalization
2. Conduct larger-scale human preference studies with diverse annotator pools to validate robustness of preference learning
3. Implement ablation studies testing different numbers of preference pairs per iteration to determine optimal data efficiency trade-offs