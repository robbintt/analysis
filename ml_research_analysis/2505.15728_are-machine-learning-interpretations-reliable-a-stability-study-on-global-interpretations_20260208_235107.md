---
ver: rpa2
title: Are machine learning interpretations reliable? A stability study on global
  interpretations
arxiv_id: '2505.15728'
source_url: https://arxiv.org/abs/2505.15728
tags:
- stability
- data
- methods
- clustering
- interpretations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the reliability of interpretable machine\
  \ learning (IML) methods through a systematic empirical evaluation. The authors\
  \ assess stability\u2014defined as consistency under small data or algorithm perturbations\u2014\
  across four machine learning tasks: classification, regression, clustering, and\
  \ dimensionality reduction."
---

# Are machine learning interpretations reliable? A stability study on global interpretations

## Quick Facts
- **arXiv ID:** 2505.15728
- **Source URL:** https://arxiv.org/abs/2505.15728
- **Reference count:** 40
- **Primary result:** No association found between prediction accuracy and interpretation stability across 25 datasets and 50+ interpretable ML methods.

## Executive Summary
This study systematically evaluates the reliability of interpretable machine learning (IML) methods through perturbation-based stability testing across four ML tasks. Using 25 benchmark tabular datasets, the authors assess whether popular IML methods produce consistent interpretations under small data or algorithm perturbations. The key finding is that IML interpretations are frequently unstable, often less stable than the predictions themselves, with no single method consistently providing the most stable interpretations. Critically, there is no association between prediction accuracy and interpretation stability, challenging the common assumption that accurate models produce reliable explanations.

## Method Summary
The study evaluates stability through systematic perturbation of data and algorithms. For supervised tasks, interpretations are tested across 100 random train/test splits (70/30), while unsupervised tasks use 50 splits. Two perturbation types are employed: train/test splits and noise addition (Gaussian or Laplace with σ² ∈ [0.5, 1.0, 2.0]). Stability is measured using task-appropriate metrics: Jaccard similarity, Average Overlap, and Top-K Kendall's Tau for feature importance; Adjusted Rand Index and V-measure for clustering; and NN-Jaccard-AUC for dimensionality reduction. The evaluation covers 25 benchmark datasets and approximately 50 IML methods ranging from linear models to deep learning post-hoc techniques.

## Key Results
- IML interpretations are frequently unstable, often less stable than predictions themselves
- No single IML method consistently provides the most stable interpretations across datasets
- No association exists between prediction accuracy and interpretation stability
- Random Forest consistently shows high within-method stability, while methods like Integrated Gradients and PCA show high between-method stability but low within-method stability

## Why This Works (Mechanism)

### Mechanism 1: Perturbation-Based Stability Testing as Reliability Proxy
Consistency of interpretations under small data or algorithm perturbations serves as a necessary condition for reliability. Controlled perturbations (train/test splits and noise addition) are applied to the data, then interpretation consistency is measured using task-appropriate metrics. If an interpretation reflects true underlying structure, it should persist under minor perturbations; instability suggests sensitivity to sampling noise rather than signal.

### Mechanism 2: Metric-Aligned Task Decomposition
Different IML tasks require different stability metrics because the nature of "interpretation" differs. For feature importance: use top-K metrics (Jaccard, Average Overlap, Kendall's Tau with penalty) that weight top ranks more heavily. For clustering: use pair-counting metrics (ARI, Fowlkes-Mallows) and information-theoretic metrics (MI, V-measure). For dimension reduction: measure local neighbor stability via NN-Jaccard-AUC across varying neighborhood sizes K.

### Mechanism 3: Accuracy-Stability Orthogonality
Prediction accuracy and interpretation stability are independent dimensions. Simultaneously measuring test-set prediction accuracy and interpretation stability across methods and datasets reveals that high accuracy does not imply stable interpretations. This challenges common practice of using accuracy as a trust proxy for explanations.

## Foundational Learning

- **Concept: Top-K Rank Stability Metrics (Jaccard, Average Overlap)**
  - Why needed here: Feature importance interpretations are rankings; practitioners care most about top features. Standard rank correlation (Spearman, Kendall) weights all positions equally.
  - Quick check question: Given two feature rankings [A, B, C, D] and [A, C, B, D], what is Jaccard@2 vs. Jaccard@4?

- **Concept: Adjusted Rand Index (ARI) for Cluster Comparison**
  - Why needed here: Clustering interpretations produce partitions; ARI corrects for chance agreement, enabling comparison across methods with different cluster counts.
  - Quick check question: Two clusterings assign labels randomly to the same data. What ARI value should you expect approximately?

- **Concept: Bootstrap/Resampling Variance in Interpretations**
  - Why needed here: Stability testing fundamentally relies on generating multiple interpretation samples via resampling; understanding variance decomposition is prerequisite.
  - Quick check question: If you run LASSO on 100 bootstrap samples and get 100 different top-feature sets, what does this tell you about the data or regularization?

## Architecture Onboarding

- **Component map:**
  Data → Perturbation Engine (splits / noise) → IML Method Wrapper → Interpretation Extractor
                                                                      ↓
                                              Stability Metric Calculator ←───
                                                                      ↓
                                              Aggregator (100 repeats) → Dashboard

- **Critical path:**
  1. Define perturbation type and parameters (e.g., 70/30 split, 100 repeats)
  2. Run IML method on each perturbed dataset
  3. Extract interpretation in standardized format
  4. Compute pairwise stability metrics across repeats
  5. Aggregate (mean, visualize distribution)

- **Design tradeoffs:**
  - Computational cost vs. statistical power: Shapley values and permutation importance timeout (>12h) on high-dimensional data
  - Global vs. local interpretations: Study aggregates local methods to global by summing over all samples
  - Metric choice: Jaccard ignores rank order within top-K; AO captures order but is less interpretable to non-specialists

- **Failure signatures:**
  - Blank cells in heatmaps: method timed out (computational constraint)
  - Stability = 1.0 at large K (feature ranking): K too large, includes noise features
  - ARI ≈ 0 for all methods on a dataset: data may lack true cluster structure
  - High between-method stability but low within-method stability: suggests random initialization effects dominate

- **First 3 experiments:**
  1. **Baseline stability check:** Run Random Forest and Logistic Ridge on a mid-size dataset (e.g., Statlog) with 50 random train/test splits. Compute Jaccard@10 and AO@10.
  2. **Noise sensitivity sweep:** Apply Gaussian noise with σ² ∈ {0, 0.5, 1.0, 2.0} to clustering datasets (e.g., Iris, Tetragonula). Plot stability (ARI) vs. noise level for K-Means, Spectral, and Hierarchical methods.
  3. **Cross-method consistency audit:** On a single dataset, compute between-method stability heatmap for all feature importance methods. Identify method pairs with AO > 0.6 and investigate whether they share similar base models or theoretical foundations.

## Open Questions the Paper Calls Out

- **Question:** Does the observed instability of global interpretations generalize to non-tabular data modalities such as images, text, and time series?
- **Question:** Can statistical inference methods be developed to rigorously quantify uncertainty in interpretation metrics?
- **Question:** Are rule-based, textual, or visual interpretations more or less stable than the quantitative feature rankings evaluated in this study?

## Limitations
- Computational constraints: Several methods timeout on high-dimensional datasets, creating gaps in the stability comparison
- Task-specific metric validity: NN-Jaccard-AUC sensitivity to neighborhood size K is not fully characterized
- Cross-method comparison validity: Methods with different output spaces are compared using task-specific metrics with unclear normalization across tasks

## Confidence

- **High:** Accuracy-stability orthogonality (robust across datasets and tasks)
- **Medium:** Stability rankings within methods (consistent patterns but sensitive to hyperparameter choices)
- **Low:** Cross-method stability comparisons (affected by computational timeouts and metric sensitivity to K)

## Next Checks

1. **Sensitivity sweep for NN-Jaccard-AUC:** Systematically vary K (e.g., 5, 10, 20, 30) for dimension reduction methods on multiple datasets. Plot stability vs. K to identify ranges where interpretations are most stable.

2. **Timeout bias audit:** Re-run the full experiment excluding methods that timeout. Compare stability distributions to the original results to quantify the impact of computational constraints.

3. **Ground truth stability benchmark:** On synthetic datasets with known feature-label relationships, compare the stability of ground truth interpretations (e.g., linear coefficients) to ML methods. This would establish a baseline for "perfect" stability and contextualize the observed instability.