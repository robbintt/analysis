---
ver: rpa2
title: Robustness of LLM-enabled vehicle trajectory prediction under data security
  threats
arxiv_id: '2511.13753'
source_url: https://arxiv.org/abs/2511.13753
tags:
- attack
- prediction
- vehicle
- attacks
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the vulnerability of large language model
  (LLM)-based vehicle trajectory prediction systems to adversarial attacks. A one-feature
  differential evolution (DE) attack is proposed to subtly perturb a single kinematic
  feature of surrounding vehicles in LLM input prompts under a black-box setting.
---

# Robustness of LLM-enabled vehicle trajectory prediction under data security threats

## Quick Facts
- arXiv ID: 2511.13753
- Source URL: https://arxiv.org/abs/2511.13753
- Reference count: 0
- Primary result: Single-feature perturbations via differential evolution degrade LLM trajectory prediction by 29% RMSE increase and 12% F1 drop

## Executive Summary
This study investigates how adversarial attacks can compromise large language model (LLM)-based vehicle trajectory prediction systems. The authors propose a one-feature differential evolution attack that perturbs a single kinematic feature of surrounding vehicles in LLM input prompts under black-box conditions. Experiments on the highD dataset demonstrate that even minor, physically plausible perturbations can significantly degrade model predictions, with a 29% increase in position prediction error and a 12% drop in lane-change intention classification accuracy. The research also reveals an accuracy-robustness trade-off, examines the failure mechanism, and explores mitigation strategies such as Chain-of-Thought reasoning to improve resistance to attacks.

## Method Summary
The methodology involves fine-tuning Llama2-13B with LoRA adapters (rank 64, alpha 16) on the highD dataset for vehicle trajectory prediction and lane-change intention classification. The model predicts 4-second trajectories (4 points) and 3-class intentions using structured prompts containing ego vehicle state, map information, and surrounding vehicle kinematics. The differential evolution attack iteratively evolves perturbations on a single feature (e.g., lateral velocity of one surrounding vehicle) to maximize prediction degradation without gradient access. Fitness is evaluated through model query responses. The attack operates within ±10% perturbation bounds and uses a population of 5 candidates with mutation factor 0.5 and crossover rate 0.9.

## Key Results
- Single-feature perturbations via differential evolution increase RMSE by 29% (from 0.70m to 0.90m at t=4s)
- F1 score for lane-change intention prediction drops by 12% under attack (89 → 70 for Keep Lane class)
- Chain-of-Thought reasoning reduces RMSE increase to 12% and F1 drop to 7% under attack
- Larger models (13B vs 7B) show equivalent vulnerability under attack despite better baseline accuracy
- Models with more fine-tuning iterations (4500 vs 7000) show increased vulnerability due to over-specialization

## Why This Works (Mechanism)

### Mechanism 1
Single-feature perturbations optimized via differential evolution can significantly degrade LLM-based trajectory prediction under black-box conditions. DE iteratively evolves perturbation candidates through mutation, crossover, and selection, evaluating fitness via model query responses. The algorithm identifies which kinematic feature and what perturbation magnitude most increase prediction error without gradient access. This works because LLMs trained on structured driving prompts develop sensitive dependencies on specific input features that can be discovered through query-based optimization. The core assumption is that LLMs develop localized feature sensitivity that DE can exploit. If LLM attention distributes information evenly across all input tokens without localized feature sensitivity, single-feature perturbations would fail to produce significant degradation.

### Mechanism 2
Larger model scale and deeper fine-tuning improve accuracy but increase adversarial vulnerability, creating an accuracy-robustness trade-off. Deeper attention layers in larger models amplify localized input perturbations through successive transformations. Extended fine-tuning strengthens specific feature correlations, making predictions more dependent on precise feature values. The core assumption is that feature correlations learned during fine-tuning become rigid dependencies that adversaries can exploit through targeted perturbations. If regularization techniques or architectural modifications prevent over-specialization, the trade-off curve would flatten.

### Mechanism 3
Chain-of-Thought reasoning enhances robustness by distributing reasoning across multiple features, reducing single-point-of-failure vulnerability. CoT forces the model to articulate intermediate reasoning steps, creating distributed representation. When one feature is perturbed, the reasoning chain has multiple other features to anchor correct predictions. The core assumption is that explicit reasoning steps create redundant decision pathways that dilute adversarial impact. If CoT reasoning itself introduces new attack surfaces, robustness gains could be negated.

## Foundational Learning

- Concept: Differential Evolution Optimization
  - Why needed here: The attack methodology relies on understanding population-based metaheuristics for black-box optimization
  - Quick check question: Given a population of 5 candidate perturbations, how would DE generate a mutant vector using mutation factor α=0.5?

- Concept: Transformer Attention Sensitivity
  - Why needed here: Understanding why localized perturbations propagate through attention layers informs both attack design and defense strategies
  - Quick check question: Why might deeper attention layers amplify small input perturbations more than shallow layers?

- Concept: LoRA Fine-tuning
  - Why needed here: The baseline model uses Low-Rank Adaptation; understanding parameter-efficient fine-tuning is essential for reproducing experiments
  - Quick check question: How does LoRA's rank parameter (set to 64 here) affect the trade-off between adaptation capacity and retaining pretrained knowledge?

## Architecture Onboarding

- Component map:
  Input layer: Structured prompt constructor (system message + user scenario description with ego/surrounding vehicle states)
  Tokenization: Language tokenizer converts numerical kinematic data to discrete tokens
  Backbone: Llama2-13B with LoRA adapters (rank=64, alpha=16)
  Output layer: Structured response parser extracting trajectory coordinates and intention labels
  Attack module: DE optimizer (population=5, mutation=0.5, crossover=0.9) querying model iteratively

- Critical path:
  1. Vehicle state → Prompt construction → Tokenization → LLM forward pass → Response parsing → Prediction
  2. Under attack: Vehicle state → Perturbation injection → Perturbed prompt → Degraded prediction

- Design tradeoffs:
  - Accuracy vs. robustness: More fine-tuning iterations improve baseline accuracy but increase attack susceptibility
  - Model scale vs. vulnerability: 13B outperforms 7B in normal conditions but degrades equivalently under attack
  - CoT reasoning vs. inference cost: CoT adds robustness (+7% F1 under attack) but increases computational overhead

- Failure signatures:
  - RMSE spike at t=4s: Normal 0.70m → 0.90m under attack (29% increase)
  - F1 collapse for lane-keeping: 89 → 70 (most affected class)
  - Feature concentration: Without CoT, 5-6 features account for majority of vulnerability

- First 3 experiments:
  1. Reproduce baseline vulnerability: Fine-tune Llama2-13B on processed highD data, run DE attack with Δ=0.1 budget, verify ~29% RMSE increase and ~12% F1 drop
  2. Validate CoT defense: Fine-tune with CoT-enabling prompts, compare attack degradation (target: reduce RMSE increase to ~12%)
  3. Feature sensitivity mapping: Track which features DE selects most frequently across 1000 validation samples; prioritize these for input validation or redundancy checks

## Open Questions the Paper Calls Out

### Open Question 1
How does the adversarial robustness of fine-tuned LLMs for trajectory prediction compare to state-of-the-art spatiotemporal models like GCN-LSTM hybrids under evasion attacks? The study focused solely on LLM-based architectures to establish a vulnerability baseline for this emerging application, without including non-LLM baselines in the experimental setup. Comparative results showing the performance drop of both LLM and GCN-LSTM models when subjected to the one-feature differential evolution attack would resolve this.

### Open Question 2
To what extent do coordinated attacks on multiple kinematic features or multiple surrounding vehicles exacerbate prediction errors compared to the single-feature attacks analyzed? The current experimental design constrained the attacker to perturbing only one feature of one surrounding vehicle to simulate a stealthy, low-budget attack. Experimental data quantifying the degradation when the attack algorithm optimizes perturbations across multiple input dimensions simultaneously would resolve this.

### Open Question 3
Can robust mitigation strategies, such as adversarial fine-tuning or input verification, effectively immunize LLM-based predictors against these perturbations while maintaining real-time performance? The paper primarily focused on vulnerability identification and a preliminary analysis of Chain-of-Thought reasoning, rather than developing or testing comprehensive defense mechanisms. Evaluation of models trained with adversarial objectives or equipped with detection layers, showing resistance to the DE attack with minimal impact on inference latency, would resolve this.

## Limitations

- The black-box setting assumes query-only access without specifying realistic constraints on query volume or cost
- The DE attack targets single-feature perturbations within ±10% bounds, but real-world sensor noise could make such precise manipulation difficult
- The highD dataset represents German highway conditions that may not generalize to other driving environments
- The study lacks comparison with non-LLM trajectory prediction baselines to assess if vulnerabilities are LLM-specific

## Confidence

**High Confidence (8/10):** The empirical demonstration that single-feature perturbations via DE can significantly degrade LLM predictions (29% RMSE increase, 12% F1 drop) is well-supported by experimental results on the highD dataset.

**Medium Confidence (6/10):** The accuracy-robustness trade-off claim is supported by within-study comparisons but lacks external validation. The mechanism linking deeper attention layers to increased vulnerability is plausible but not rigorously proven.

**Low Confidence (4/10):** The Chain-of-Thought robustness mechanism is the least validated claim, relying on indirect evidence from performance metrics rather than mechanistic understanding of how distributed reasoning creates resistance to attacks.

## Next Checks

1. **Attack transferability test:** Apply the optimized perturbations discovered on the 13B model to the 7B model and a non-LLM baseline (e.g., LSTM or Transformer encoder) to determine if the vulnerability is architecture-specific or general to sequence models.

2. **Query budget constraint evaluation:** Re-run the DE attack with practical query limits (e.g., 50 queries per sample vs. unlimited) to assess how query efficiency affects attack success and whether gradient-free optimization remains viable under realistic constraints.

3. **Real-world sensor noise simulation:** Add realistic GPS/IMU noise characteristics (±0.1-0.5m position error, ±0.1m/s velocity error) to input features and re-evaluate attack effectiveness to determine if physical measurement uncertainty provides natural defense against the proposed attack.