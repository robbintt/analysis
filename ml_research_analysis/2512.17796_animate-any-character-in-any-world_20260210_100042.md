---
ver: rpa2
title: Animate Any Character in Any World
arxiv_id: '2512.17796'
source_url: https://arxiv.org/abs/2512.17796
tags:
- character
- video
- arxiv
- generation
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AniX is a framework for interactive video generation that enables
  users to control any 3D character within any 3D Gaussian Splatting scene using natural
  language commands. The core innovation is formulating this as a conditional autoregressive
  video generation problem, where the model generates temporally coherent clips conditioned
  on scene representations, character views, mask tokens, and text instructions.
---

# Animate Any Character in Any World

## Quick Facts
- arXiv ID: 2512.17796
- Source URL: https://arxiv.org/abs/2512.17796
- Reference count: 40
- AniX enables interactive video generation with natural language control of any 3D character in any 3DGS scene

## Executive Summary
AniX introduces a framework for interactive video generation that allows users to animate any 3D character within any 3D Gaussian Splatting (3DGS) scene using natural language commands. The system treats this as a conditional autoregressive video generation problem, leveraging multi-view character representations and explicit scene conditioning through rendered 3DGS projections. Built on a pre-trained video generator and fine-tuned on locomotion data, AniX achieves substantial improvements in motion dynamics and character consistency compared to both video generation foundation models and dedicated world models.

## Method Summary
AniX formulates character animation as conditional autoregressive video generation, where a pre-trained video generator is fine-tuned to produce temporally coherent clips conditioned on 3DGS scene representations, multi-view character encodings, mask tokens, and text instructions. The framework uses explicit 3DGS rendering for spatial conditioning rather than implicit camera embeddings, represents characters through four viewpoint renders with shifted positional embeddings, and employs Flow Matching as the training objective. A small post-training dataset of basic locomotion actions enhances motion dynamics while preserving the pre-trained model's generalization capabilities.

## Key Results
- Action control success rates: 100% for seen actions, 80.7% for 142 novel actions
- Character consistency: 0.698 DINOv2 score (improving to 0.755 with hybrid real-world training)
- Long-horizon coherence: Maintains visual quality and character consistency over multiple generated clips
- Outperforms both video generation foundation models and dedicated world models across visual quality, action controllability, character consistency, and long-horizon coherence

## Why This Works (Mechanism)

### Mechanism 1: 3DGS Scene as Explicit Spatial Memory
- Rendering camera paths through 3DGS scenes provides geometrically grounded conditioning that preserves spatial consistency better than implicit neural camera embeddings
- The 3DGS scene serves as explicit spatial memory, with rendered projection videos along specified trajectories conditioning the generator directly
- Break condition: Poor 3DGS quality, missing geometry, or incomplete coverage causes artifacts in rendered views that propagate to generated videos

### Mechanism 2: Multi-View Character Token Conditioning with Spatially Shifted Positional Embeddings
- Encoding characters from four viewpoints (front, left, right, back) with distinct temporal offsets enables the model to maintain character identity across generated motion
- Each view's tokens receive shifted-3D-RoPE with unique temporal offsets (−1, −2, −3, −4), preventing token collision while preserving spatial relationships
- Break condition: Highly asymmetric features or insufficient viewpoint coverage degrades consistency

### Mechanism 3: Simple Locomotion Post-Training Preserves Generalization While Enhancing Motion Dynamics
- Fine-tuning on basic locomotion actions improves motion quality across both seen and novel actions without collapsing the pre-trained generative space
- The approach analogizes to LLM post-training that adjusts "response style" rather than "redefining the model's generative space"
- Break condition: Over-training or overly diverse training data may cause catastrophic forgetting of pre-trained priors

## Foundational Learning

- **3D Gaussian Splatting (3DGS)**: Users provide 3DGS scenes as spatial foundation; understanding how 3DGS represents scenes as collections of Gaussians is essential for debugging rendering issues
  - Quick check: Given a 3DGS scene, can you explain why certain viewpoints might produce blurry or missing content?

- **Flow Matching / Rectified Flow**: AniX uses Flow Matching (not standard diffusion) as training objective, predicting velocity vectors rather than noise
  - Quick check: In Flow Matching, what does the velocity prediction u_t = dx_t/dt represent, and how does it differ from standard diffusion noise prediction?

- **LoRA (Low-Rank Adaptation)**: AniX uses LoRA with rank 64 to fine-tune a 13B parameter model efficiently
  - Quick check: If you wanted to add a new conditioning modality without full fine-tuning, where would you inject trainable parameters?

## Architecture Onboarding

- **Component map**: 3DGS scene → render video → Video VAE encoder → tokens; Character views + text → LLaVA encoder → tokens; Fuse via Projector → MMDiT with positional embeddings → denoise → decode to video

- **Critical path**: 1) User provides 3DGS scene → render scene video along camera path; 2) Encode scene video, character views, text instruction, character anchor mask into tokens; 3) Fuse scene/mask tokens with noisy target via Projector; 4) Concatenate text tokens, character tokens, and conditional noisy tokens; 5) Apply MMDiT with positional embeddings to denoise over 30 steps (or 4 with DMD2); 6) Decode latent to video frames

- **Design tradeoffs**: 4 views vs. fewer (more views improve consistency but increase memory); game data vs. real-world data (game data provides clean motion; hybrid training improves photorealism); 30-step vs. 4-step inference (DMD2 distillation achieves 7.5× speedup with slight quality drop); per-frame vs. single anchor mask during inference (training uses per-frame masks; inference uses single bounding-box anchor)

- **Failure signatures**: Character appears stylized/rendered when using real-world inputs (likely trained only on game data); camera motion drifts from intended path (check 3DGS scene quality); character identity degrades across long horizons (verify both visual conditions are active); generated motion ignores text instruction (may indicate insufficient action coverage)

- **First 3 experiments**: 1) Ablate multi-view character conditioning: Run inference with front-view only, front+back, and all four views on same character performing multi-directional motion. Compare DINOv2/CLIP scores to verify 0.556 → 0.613 → 0.698 progression; 2) Test action generalization boundaries: Take the 142 novel actions from evaluation and systematically test which categories fail most often (gestures vs. object interactions); 3) Measure long-horizon degradation: Generate 10+ sequential clips with and without 3DGS scene condition. Plot CLIP-Aesthetic and DINOv2 scores across iterations

## Open Questions the Paper Calls Out

- Why does fine-tuning on simple locomotion data (only 5 actions, 2,084 samples) enhance motion dynamics while preserving generalization to 142+ novel actions? The paper offers a post-training interpretation comparing to LLM alignment but does not provide mechanistic analysis or ablation on training data composition.

- Does the training-inference misalignment in auto-regressive mode (ground-truth vs. model-generated preceding tokens) cause compounding errors over many iterations? The paper adds noise as mitigation but does not quantify error accumulation or compare to teacher-forcing alternatives over extended horizons.

- To what extent does game-engine training data bias the model toward stylized rendering, and can this be fully corrected with hybrid real-world data? While hybrid training improves photorealism (DINOv2 0.686→0.755), the approach is not analyzed across varying real-to-synthetic ratios or different real-world datasets.

## Limitations

- The evaluation framework only reports aggregate success rates for 142 novel actions without per-category breakdowns, obscuring whether the model genuinely generalizes to novel semantic domains
- The framework's dependency on high-quality 3DGS scenes creates a potential bottleneck, as poor scene quality causes artifacts that propagate to generated videos
- Performance on actual user-provided 3DGS scenes with diverse character types and environmental conditions remains untested

## Confidence

- **High Confidence**: The core technical contribution of formulating character animation as conditional autoregressive video generation is well-supported with precise architectural specifications
- **Medium Confidence**: Generalization claims to novel actions (80.7% success) are promising but rely on a single evaluation protocol
- **Low Confidence**: Long-horizon coherence benefits show visual improvements but lack quantitative metrics across extended temporal ranges

## Next Checks

1. **Novel Action Category Analysis**: Re-run the 142 novel action evaluation, categorizing failures by semantic type (gestures vs. poses vs. object interactions) to reveal whether the 80.7% success rate masks systematic weaknesses

2. **3DGS Quality Dependency Study**: Generate videos using 3DGS scenes of varying quality (high-quality reconstructions vs. incomplete/sparse reconstructions) to quantify the framework's sensitivity to scene input quality

3. **Extended Temporal Coherence Test**: Generate 20+ sequential clips with varying camera paths and action sequences, both with and without 3DGS conditioning, to track DINOv2 and CLIP-Aesthetic scores frame-by-frame and measure long-term memory retention