---
ver: rpa2
title: Adapting Multilingual Models to Code-Mixed Tasks via Model Merging
arxiv_id: '2510.19782'
source_url: https://arxiv.org/abs/2510.19782
tags:
- code-mixed
- merging
- data
- task
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates model merging as an alternative to conventional
  fine-tuning strategies for code-mixed natural language processing tasks. Starting
  from a multilingual base model, the authors perform continued pre-training on unlabeled
  code-mixed text, merge the adapted checkpoint with the base model, and then fine-tune
  on downstream task data.
---

# Adapting Multilingual Models to Code-Mixed Tasks via Model Merging

## Quick Facts
- arXiv ID: 2510.19782
- Source URL: https://arxiv.org/abs/2510.19782
- Reference count: 40
- Primary result: Merged models outperform fine-tuning on code-mixed sentiment analysis and hate speech detection, achieving 2-5 F1 point gains over full fine-tuning and 1-2 points over CPT→FT.

## Executive Summary
This study investigates model merging as an alternative to conventional fine-tuning strategies for code-mixed natural language processing tasks. Starting from a multilingual base model, the authors perform continued pre-training on unlabeled code-mixed text, merge the adapted checkpoint with the base model, and then fine-tune on downstream task data. Experiments on sentiment analysis and hate speech detection tasks in English-Hindi and English-Spanish using XLM-R and Llama-3.2-1B models demonstrate that merged models consistently outperform full fine-tuning and CPT→FT approaches, achieving gains of 2-5 F1 points over full fine-tuning and 1-2 points over CPT→FT. The results indicate that model merging more effectively leverages unlabeled code-mixed data. Cross-pair transfer experiments show merged checkpoints trained on code-mixed resources transfer more strongly to low-resource language pairs (e.g., 0.65-0.68 F1 vs 0.61-0.63 for full fine-tuning) compared to monolingual English baselines. Zero/few-shot prompting with larger LLMs underperforms fine-tuned and merged checkpoints, highlighting limitations of in-context learning for code-mixed inputs.

## Method Summary
The method involves three stages: (1) continued pre-training (CPT) of a multilingual base model on unlabeled code-mixed text using MLM for XLM-R or CLM for Llama, (2) computing a task vector by subtracting the base model weights from the CPT checkpoint weights, and (3) merging the task vector with the base model using either Task Arithmetic or TIES, followed by fine-tuning on labeled downstream task data. The scaling factor λ for merging is tuned on validation sets. This approach aims to preserve monolingual representations while adding code-mixed competence, mitigating catastrophic forgetting that can occur during continued pre-training.

## Key Results
- Merged models consistently outperform full fine-tuning and CPT→FT approaches, achieving gains of 2-5 F1 points over full fine-tuning and 1-2 points over CPT→FT.
- Cross-pair transfer experiments show merged checkpoints trained on code-mixed resources transfer more strongly to low-resource language pairs (0.65-0.68 F1 vs 0.61-0.63 for full fine-tuning) compared to monolingual English baselines.
- Zero/few-shot prompting with larger LLMs underperforms fine-tuned and merged checkpoints for code-mixed inputs, highlighting limitations of in-context learning for code-mixed tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Merging a CPT checkpoint with the base model preserves monolingual representations while adding code-mixed competence, mitigating catastrophic forgetting that can occur during continued pre-training.
- Mechanism: The CPT checkpoint encodes code-mixed patterns in its weight deltas. By merging via Task Arithmetic (θ ⊕ λτ_CPT) rather than using CPT directly, the base model's strong L1/L2 monolingual capabilities are retained proportionally to the scaling factor λ. This yields a "more robust solution for code-mixed text" by combining both capabilities.
- Core assumption: Code-mixed utterances contain monolingual spans, so preserving monolingual processing is essential for downstream performance; CPT alone risks degrading these.
- Evidence anchors: Merged models consistently outperform full fine-tuning and CPT→FT (2-5 F1 points over full fine-tuning, ~1-2 points over CPT→FT); Alexandrov et al. (2024) show model merging mitigates catastrophic forgetting in language transfer.

### Mechanism 2
- Claim: Task vectors derived from unlabeled code-mixed CPT corpora capture code-mixing regularities (lexical switching patterns, syntactic mixing structures) that transfer to downstream tasks even without task-specific supervision during CPT.
- Mechanism: The CPT phase learns domain/language-adapted representations via MLM or CLM. The resulting task vector captures distributional properties of code-mixed text. When merged and then fine-tuned, the model starts from a representation space already aligned to code-mixed phenomena, improving sample efficiency.
- Core assumption: Unlabeled code-mixed text contains learnable regularities relevant to downstream tasks; CPT captures these in weight space rather than just memorizing surface patterns.
- Evidence anchors: Unlabeled data can be exploited more effectively through model merging than through continued pre-training alone; CPT on different corpora yields similar downstream performance (~1 F1 point variance), suggesting task vectors capture generalizable patterns.

### Mechanism 3
- Claim: Code-mixed training provides a stronger transfer substrate for low-resource language pairs than monolingual English supervision, because code-mixed representations capture language-contact dynamics that generalize across typologically similar pairs.
- Mechanism: Models trained on En-Hi code-mixed data and merged develop representations sensitive to switching behavior, language alternation patterns, and mixed-grammar constructions. These transfer better to En-Ta/En-Ml than English-only models because the code-mixing substrate better approximates the target distribution.
- Core assumption: Code-mixed phenomena share cross-pair regularities that monolingual models lack; these are encoded in merged checkpoints.
- Evidence anchors: Merged checkpoints trained on code-mixed resources transfer more strongly than monolingual-English baselines (0.65-0.68 F1 vs 0.61-0.63); checkpoints obtained through model merging outperform Full FT baselines by 5-13 F1 points across models and target language pairs.

## Foundational Learning

- Concept: Code-Mixing Linguistics
  - Why needed here: Code-mixed text contains intra-utterance language alternation with specific syntactic/lexical constraints. Understanding this helps reason about what CPT and merging must capture.
  - Quick check question: Can you explain why preserving monolingual spans matters for code-mixed NLP, and what "language-contact dynamics" means operationally?

- Concept: Task Arithmetic and TIES Merging
  - Why needed here: These are the core merging methods. Task Arithmetic computes task vectors (θ_finetuned - θ_base) and combines them additively; TIES resolves interference via trim-elect-sign operations.
  - Quick check question: Given two fine-tuned models, how would you compute a task vector? What does TIES add beyond simple averaging?

- Concept: Continued Pre-Training Objectives (MLM vs. CLM)
  - Why needed here: The paper uses different CPT objectives for encoder-only (XLM-R: MLM) and decoder-only (Llama: CLM) models. Understanding these informs why CPT creates useful task vectors.
  - Quick check question: Why does the paper use MLM for XLM-R and CLM for Llama? What does each objective optimize?

## Architecture Onboarding

- Component map:
  - Base Model (θ): XLM-R (encoder, 270M) or Llama-3.2-1B (decoder, 1B)
  - CPT Corpus: Unlabeled code-mixed text (e.g., 166K En-Hi samples from Das et al.)
  - Task Vector (τ_CPT): θ^CM - θ, the delta from CPT adaptation
  - Merge Operation: Task Arithmetic (θ + λτ) or TIES (trim-elect-sign based combination)
  - Fine-Tuning Dataset: Labeled code-mixed task data (sentiment, hate speech)
  - Scaling Factor λ: Tuned on validation set (critical hyperparameter)

- Critical path:
  1. Load base multilingual model (θ)
  2. Run CPT on unlabeled code-mixed corpus → obtain θ^CM
  3. Compute τ_CPT = θ^CM - θ
  4. Merge: θ_merged = θ_base ⊕ λτ_CPT (⊕ is addition for TV, TIES rules for TIES)
  5. Fine-tune θ_merged on labeled downstream task
  6. Evaluate on code-mixed test sets

- Design tradeoffs:
  - TV vs. TIES: TV is simpler and often performs slightly better; TIES handles multi-vector merging with interference resolution
  - λ selection: Higher λ increases code-mixing adaptation but risks degrading base capabilities; must tune per model/dataset
  - CPT corpus source: Real vs. synthetic corpus yields similar results (~1 F1 variance), suggesting synthetic data is viable when real data is scarce

- Failure signatures:
  - Merged model underperforms Full FT: λ may be too low (insufficient adaptation) or too high (catastrophic forgetting)
  - CPT→FT underperforms merging: CPT alone may have degraded base capabilities; merging recovers them
  - Zero/few-shot prompting fails: Code-mixed ICL is unreliable even for 70B models; fine-tuning is required

- First 3 experiments:
  1. Replicate the baseline comparison on one dataset (e.g., En-Hi GLUECoS sentiment): Full FT vs. CPT→FT vs. TV-CPT→FT vs. TIES-CPT→FT. Verify 2-5 F1 gains from merging.
  2. Ablate λ scaling factor: Test λ ∈ {0.1, 0.3, 0.5, 0.7, 1.0} on validation set to find optimal merge strength and observe the tradeoff curve.
  3. Test cross-pair transfer: Train on En-Hi with TV/TIES merging, evaluate zero-shot on En-Ta. Verify merged checkpoints outperform Full FT baselines (target: 0.65-0.68 vs 0.61-0.63 F1 as reported).

## Open Questions the Paper Calls Out

- **Generalization to other tasks**: The experiments are limited to sentence classification due to lack of consistent datasets for other tasks. The generalizability of findings to other NLP tasks like named entity recognition, POS tagging, or generation is unclear and should be evaluated in future research.

- **Scaling to larger models**: The study does not scale to larger language models (>1B parameter models) due to prohibitive training/fine-tuning costs. Analyzing how model merging behaves as model size increases for code-mixed tasks would be an interesting avenue for future work.

- **Unlabeled data only regime**: The paper explores data settings based on availability of both labeled and unlabeled data but does not consider the case where unlabeled data is available but no task-specific labeled data is present. This scenario remains untested.

- **Mechanism behind merging method differences**: Task Arithmetic consistently outperforms TIES-Merging for code-mixed tasks, but the paper offers no explanation for this pattern or conditions under which TIES might become preferable.

## Limitations
- Exact λ scaling values for Task Arithmetic and TIES are not specified, only that they were tuned on held-out validation sets, preventing precise replication.
- CPT training duration and early stopping criteria are not detailed, affecting the quality and stability of the τ_CPT task vector.
- The study focuses on encoder-only and decoder-only models but does not test encoder-decoder architectures, limiting generalizability to sequence-to-sequence tasks.

## Confidence
- **High confidence**: Merged models consistently outperform both Full FT and CPT→FT baselines on tested sentiment analysis and hate speech detection tasks (F1 gains of 2-5 points over Full FT); CPT→FT alone can degrade monolingual capabilities which merging recovers; zero/few-shot prompting with large LLMs underperforms fine-tuned approaches for code-mixed inputs.
- **Medium confidence**: Model merging more effectively leverages unlabeled code-mixed data than CPT alone; code-mixed training provides stronger transfer substrate for low-resource pairs than monolingual supervision; task vectors capture generalizable code-mixing patterns.
- **Low confidence**: The exact mechanism by which merged task vectors outperform CPT→FT is not definitively established; cross-pair transfer gains are attributed to code-mixing substrate but typological similarity is not quantified.

## Next Checks
1. Run the baseline En-Hi GLUECoS sentiment experiment with λ ∈ {0.3, 0.5, 0.7, 1.0, 1.5} to characterize the tradeoff between adaptation strength and base capability preservation, and identify the optimal λ range that yields the reported 2-5 F1 point gains.

2. Test zero-shot transfer from En-Hi merged checkpoints to En-Es (same language pair but different script/alphabet) versus transfer to En-Ta/En-Ml (different language families) to determine whether transfer gains are due to code-mixing patterns specifically or general multilingual exposure.

3. Analyze the τ_CPT task vector by comparing the top-k weight changes to identify whether they correspond to known code-mixing phenomena (e.g., lexical borrowing patterns, syntactic interference patterns) or simply general multilingual regularization.