---
ver: rpa2
title: Complexity of One-Dimensional ReLU DNNs
arxiv_id: '2512.08091'
source_url: https://arxiv.org/abs/2512.08091
tags:
- number
- linear
- regions
- relu
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the expressivity of one-dimensional ReLU deep
  neural networks by analyzing their linear regions. The key result proves that for
  randomly initialized, fully connected 1D ReLU networks in the infinite-width limit,
  the expected number of linear regions grows as the sum of neurons across all layers
  plus a vanishing term.
---

# Complexity of One-Dimensional ReLU DNNs

## Quick Facts
- **arXiv ID**: 2512.08091
- **Source URL**: https://arxiv.org/abs/2512.08091
- **Reference count**: 29
- **Primary result**: Expected number of linear regions in 1D ReLU DNNs grows as sum of neurons plus vanishing term

## Executive Summary
This paper analyzes the expressivity of one-dimensional ReLU deep neural networks by examining their linear regions (breakpoints). The authors prove that for randomly initialized, fully connected 1D ReLU networks in the infinite-width limit, the expected number of linear regions scales asymptotically as the sum of neurons across all layers. They introduce a function-adaptive sparsity metric based on linear regions that measures network effectiveness by comparing expected regions used to minimal necessary for target approximation. The proof leverages Gaussian process theory and covariance recursion relations to analyze breakpoint propagation through layers.

## Method Summary
The method involves analyzing 1D ReLU DNNs with He initialization (weights ~ N(0, 2/n_{ℓ-1}), biases ~ N(0, σ²_b) with σ_b ≠ 0) in the infinite-width limit. The key insight is that in this limit, pre-activations behave as Gaussian Processes, allowing rigorous analysis of zero-crossing probabilities. The authors derive a variance recursion V^(ℓ)(u) = V^(ℓ-1)(u) + σ²_b and use it to compute breakpoint propagation probabilities through layers. The analysis shows that each neuron contributes approximately one breakpoint on average, leading to the total count scaling as the sum of neurons.

## Key Results
- Expected number of linear regions in 1D ReLU DNNs grows as Σᵢ nᵢ + o(Σᵢ nᵢ) + 1 in the infinite-width limit
- Non-zero bias is strictly required for maintaining linear growth of complexity
- Breakpoints created in early layers survive to final output with high probability in wide networks
- Function-adaptive sparsity metric η_region measures region inefficiency by comparing expected regions used to minimal necessary

## Why This Works (Mechanism)

### Mechanism 1: Additive Breakpoint Contribution
- **Claim:** Expected number of linear regions scales as sum of neuron counts across layers
- **Mechanism:** First-layer neurons contribute one breakpoint each; deeper neurons contribute ~1 breakpoint each in infinite-width limit due to GP behavior
- **Core assumption:** Infinite-width limit allows pre-activations to be treated as GPs with sufficient variance for sign changes
- **Evidence anchors:** Theorem IV.2 proof shows E[X_{ℓ,i}] = 1 for all neurons in the limit
- **Break condition:** Fails in narrow networks where propagation probability drops

### Mechanism 2: Bias-Driven Sign Fluctuation
- **Claim:** Non-zero bias is required for linear growth of complexity
- **Mechanism:** Bias variance accumulates additively through layers, preventing correlation from remaining too close to 1 and eliminating zero-crossings
- **Core assumption:** Biases drawn i.i.d. from Gaussian with σ_b ≠ 0
- **Evidence anchors:** Variance recursion V^(ℓ)(u) = 2u² + ℓσ²_b explicitly depends on bias
- **Break condition:** If σ_b = 0, variance recursion changes and zero-crossing probability may vanish

### Mechanism 3: Probabilistic Breakpoint Propagation
- **Claim:** Early-layer breakpoints survive to final output with high probability in wide networks
- **Mechanism:** In infinite-width limit, probability that at least one neuron in next layer preserves breakpoint approaches 1
- **Core assumption:** Weights are independent and symmetric, giving 50% preservation chance per neuron
- **Evidence anchors:** Theorem IV.2 proof shows propagation probability 1 - 2^(-n_{ℓ+1}) → 1
- **Break condition:** In very deep but narrow networks, "breakpoint extinction" could occur

## Foundational Learning

- **Concept: ReLU Networks as Piecewise Linear Functions**
  - **Why needed here:** The entire analysis defines network complexity as number of linear regions where function is a straight line
  - **Quick check question:** For y = max(0, x-1) with domain x ∈ [-2, 2], how many linear regions exist?

- **Concept: Neural Network Gaussian Processes (NNGP)**
  - **Why needed here:** Infinite-width limit causes hidden layer outputs to converge to Gaussian Processes, enabling covariance-based zero-crossing analysis
  - **Quick check question:** Why does infinite-width assumption turn deterministic neural network into probabilistic Gaussian Process?

- **Concept: He Initialization**
  - **Why needed here:** Specific variance 2/n_{ℓ-1} and non-zero biases define signal propagation dynamics that asymptotic results depend on
  - **Quick check question:** What happens to activation variance in deep network if weights use variance much smaller than 2/n?

## Architecture Onboarding

- **Component map:**
  - Input: Scalar x ∈ ℝ (strictly 1D)
  - Hidden Layers: Fully connected, width n_ℓ, ReLU activation
  - Parameters: Weights W^(ℓ)_ij ~ N(0, 2/n_{ℓ-1}), Biases b^(ℓ)_j ~ N(0, σ²_b)
  - Metric: Linear Region Count ≈ Σ nᵢ

- **Critical path:**
  1. Initialize network with non-zero bias (crucial)
  2. Forward pass generates pre-activations s^(ℓ)
  3. Identify sign changes (breakpoints) in s^(ℓ) across input domain
  4. Sum breakpoints; check if they approximate total neuron count

- **Design tradeoffs:**
  - Width vs. Depth: Paper implies equivalence in region creation (1 neuron ≈ 1 region), but depth needed for hierarchical features
  - Random vs. Trained: Results apply to randomly initialized networks; trained networks likely have higher region efficiency

- **Failure signatures:**
  - High-Dimensional Inputs: Do NOT apply Σnᵢ rule to images or high-dim data; 1D derivation relies on linear intervals
  - Zero Bias: If bias=False or σ_b ≈ 0, additive region creation breaks; network produces fewer regions
  - Narrow Layers: Small width n_ℓ drops propagation probability, breaking asymptotic equality

- **First 3 experiments:**
  1. **Empirical Verification (Finite Width):** Generate random 1D ReLU networks of varying depth/width, count linear regions numerically, plot against Σnᵢ to see convergence rate
  2. **Bias Ablation:** Rerun counting experiment with σ_b → 0, verify collapse in breakpoint count predicted by variance term
  3. **Sparsity Metric Application:** Train small network on simple 1D piecewise linear function, calculate η_region to measure region inefficiency

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical results strictly apply only to infinite-width limit and randomly initialized networks
- Paper does not quantify convergence rate of finite networks to asymptotic behavior
- 1D restriction is fundamental - results cannot be directly extended to higher dimensions where regions become polytopes
- Does not address how results translate to trained networks versus random initialization

## Confidence
- **High Confidence**: Proof technique using Gaussian processes and covariance analysis is mathematically rigorous
- **Medium Confidence**: Practical interpretation that each neuron contributes ~1 breakpoint is supported but quantitative relationship in finite-width unclear
- **Low Confidence**: Paper does not address translation to trained networks or practical training scenarios

## Next Checks
1. **Convergence Rate Analysis**: For various (depth, width) configurations, measure E[R(T)]/Σnᵢ and plot convergence to 1 as width increases
2. **Bias Sensitivity Test**: Systematically vary σ_b from 0 to 1, measuring breakpoint counts to determine minimum bias magnitude required
3. **Trained Network Comparison**: Train 1D ReLU networks on simple piecewise linear functions, measure actual regions used versus theoretical maximum, calculate η_region metric