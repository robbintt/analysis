---
ver: rpa2
title: Calibrating Verbalized Confidence with Self-Generated Distractors
arxiv_id: '2509.25532'
source_url: https://arxiv.org/abs/2509.25532
tags:
- confidence
- verbalized
- answer
- claim
- claims
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Distractor-Normalized Coherence (DINCO),
  a method for calibrating confidence estimates from large language models (LLMs)
  by addressing the problem of overconfidence and saturation in verbalized confidence
  scores. DINCO normalizes an LLM's verbalized confidence for a main claim by dividing
  it by the sum of confidences across self-generated distractors (alternative claims),
  using an NLI model to downweight redundant or non-contradictory distractors.
---

# Calibrating Verbalized Confidence with Self-Generated Distractors

## Quick Facts
- arXiv ID: 2509.25532
- Source URL: https://arxiv.org/abs/2509.25532
- Reference count: 38
- This paper introduces Distractor-Normalized Coherence (DINCO), a method for calibrating confidence estimates from large language models (LLMs) by addressing the problem of overconfidence and saturation in verbalized confidence scores.

## Executive Summary
This paper addresses the problem of overconfidence and saturation in verbalized confidence scores from large language models by introducing Distractor-Normalized Coherence (DINCO). The method normalizes an LLM's confidence for a main claim by dividing it by the sum of confidences across self-generated distractors, using an NLI model to downweight redundant or non-contradictory distractors. DINCO leverages coherence within claim validation to correct for suggestibility bias and integrates coherence across sampled generations via self-consistency, combining these complementary dimensions of coherence.

## Method Summary
DINCO addresses LLM overconfidence by normalizing verbalized confidence scores using self-generated distractors. For each main claim, the method generates multiple alternative claims and computes confidence scores for each using the same LLM. These confidence scores are then normalized by dividing the main claim's confidence by the sum of confidences across all distractors, weighted by an NLI model to downweight redundant or non-contradictory alternatives. This approach corrects for suggestibility bias where models assign high confidence to claims with little supporting information. DINCO also integrates self-consistency by combining coherence across sampled generations with distractor coherence, creating a more robust calibration method that outperforms baselines in zero-resource settings across both open- and closed-source models.

## Key Results
- DINCO improves Expected Calibration Error (ECE) over the next best method by an average of 0.099 on TriviaQA, 0.092 on SimpleQA, and 0.055 on FActScore benchmarks.
- The method produces less saturated and more usable confidence estimates compared to baseline approaches.
- DINCO outperforms self-consistency baselines at equivalent or lower inference budgets while maintaining zero-resource calibration.

## Why This Works (Mechanism)
The method works by leveraging coherence within claim validation to correct for suggestibility bias, where models tend to assign high confidence to claims they encode little information about. By normalizing confidence against self-generated distractors, DINCO forces the model to demonstrate relative certainty between the main claim and plausible alternatives. The integration of self-consistency captures coherence across sampled generations, providing a complementary dimension of reliability that addresses different failure modes than distractor-based normalization.

## Foundational Learning
- **Verbalized confidence calibration**: Understanding how to interpret and adjust raw confidence scores from LLMs to better reflect true accuracy.
- **NLI-based coherence**: Using natural language inference models to evaluate logical relationships between claims and distractors.
- **Self-consistency**: Sampling multiple generations and using agreement between them as a reliability signal.
- **Suggestibility bias**: The tendency of models to assign high confidence to claims regardless of actual information content.
- **Distractor generation**: Creating plausible alternative claims that test the model's ability to distinguish between similar assertions.
- **Expected Calibration Error (ECE)**: A metric measuring the difference between predicted confidence and actual accuracy.

## Architecture Onboarding
- **Component map**: LLM confidence scoring -> Distractor generation -> NLI-based weighting -> Confidence normalization -> Self-consistency integration
- **Critical path**: Main claim generation → Confidence scoring → Distractor generation → NLI evaluation → Normalized confidence calculation
- **Design tradeoffs**: DINCO trades computational overhead for improved calibration, requiring multiple generations and NLI evaluations but providing more reliable confidence scores without additional training.
- **Failure signatures**: Poor distractor quality may lead to insufficient normalization; NLI model errors can misweight distractors; self-consistency may fail when model consistently generates similar errors.
- **First experiments**: 1) Compare ECE before and after DINCO normalization on simple claims, 2) Test robustness to varying numbers of distractors, 3) Evaluate performance when using open-source vs. proprietary NLI models.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach relies on a proprietary NLI model, raising questions about generalizability and accessibility.
- Performance may not generalize to more diverse or adversarial claim structures where NLI-based coherence could break down.
- The computational budget for distractor generation may limit practical applicability in resource-constrained settings.

## Confidence
- **High confidence**: DINCO reduces Expected Calibration Error (ECE) across tested benchmarks compared to baselines.
- **Medium confidence**: The combination of distractor normalization and self-consistency provides complementary calibration benefits.
- **Low confidence**: DINCO generalizes effectively to diverse domains and adversarial claim structures without additional tuning.

## Next Checks
1. Test DINCO on adversarial or deliberately misleading claims where NLI-based coherence might fail to distinguish valid from invalid distractors.
2. Evaluate the method's performance when using open-source NLI models or alternative coherence metrics to assess robustness to model choice.
3. Conduct user studies to measure whether DINCO's calibrated confidence scores improve decision-making or trust in real-world applications compared to raw verbalized confidence.