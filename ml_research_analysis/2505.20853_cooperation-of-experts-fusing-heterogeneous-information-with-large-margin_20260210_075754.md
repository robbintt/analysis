---
ver: rpa2
title: 'Cooperation of Experts: Fusing Heterogeneous Information with Large Margin'
arxiv_id: '2505.20853'
source_url: https://arxiv.org/abs/2505.20853
tags:
- experts
- learning
- information
- networks
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a Cooperation of Experts (CoE) framework for
  fusing heterogeneous information in multiplex networks. The method employs two-level
  experts: low-level experts learn patterns from individual networks while high-level
  experts capture shared information across networks through mutual information maximization.'
---

# Cooperation of Experts: Fusing Heterogeneous Information with Large Margin

## Quick Facts
- arXiv ID: 2505.20853
- Source URL: https://arxiv.org/abs/2505.20853
- Reference count: 40
- Achieves 94.21% accuracy on ACM dataset and 81.11% on ESP dataset

## Executive Summary
This paper introduces the Cooperation of Experts (CoE) framework for fusing heterogeneous information in multiplex networks. The method employs a two-level expert system where low-level experts learn patterns from individual networks while high-level experts capture shared information across networks through mutual information maximization. To foster collaboration rather than competition among experts, CoE introduces a large margin mechanism with a confidence tensor that dynamically adjusts expert contributions based on their proficiency. The framework demonstrates superior performance on five multiplex network datasets and shows robustness to structural attacks.

## Method Summary
The Cooperation of Experts (CoE) framework fuses heterogeneous information from multiple networks by employing a two-level expert system. Low-level experts learn patterns from individual networks while high-level experts capture shared information across networks through mutual information maximization. The key innovation is the large margin mechanism with a confidence tensor that dynamically adjusts expert contributions based on their proficiency, fostering collaboration rather than competition. Theoretical analyses prove convexity, Lipschitz continuity, and convergence guarantees. The framework is evaluated on five multiplex network datasets (ACM, DBLP, Yelp, MAG, Amazon) and four multimodal datasets, demonstrating superior performance compared to state-of-the-art methods.

## Key Results
- Achieves 94.21% accuracy on ACM dataset
- Achieves 81.11% accuracy on ESP dataset
- Outperforms state-of-the-art methods including recent Graph-MoE approaches

## Why This Works (Mechanism)
The framework works by creating a collaborative environment where experts complement rather than compete with each other. The large margin mechanism ensures that experts with higher proficiency contribute more to the final representation, while the confidence tensor dynamically adjusts these contributions based on performance. Mutual information maximization at the high-level encourages experts to capture shared information across networks, creating a unified representation that leverages the strengths of each individual network.

## Foundational Learning
- Multiplex Networks: Networks with multiple layers representing different types of relationships; needed to understand the heterogeneous data structure; quick check: verify understanding of layer-specific adjacency matrices
- Mutual Information Maximization: Technique for maximizing shared information between representations; needed to capture cross-network relationships; quick check: confirm understanding of KL divergence and its role in MI estimation
- Large Margin Classification: Classification approach that maximizes the margin between classes; needed for robust decision boundaries; quick check: verify understanding of margin-based loss functions
- Expert Systems: Multiple specialized models working together; needed to handle heterogeneous information sources; quick check: confirm understanding of ensemble methods and their coordination
- Lipschitz Continuity: Mathematical property ensuring bounded gradients; needed for convergence guarantees; quick check: verify understanding of gradient stability conditions

## Architecture Onboarding
- Component Map: Input Networks -> Low-level Experts -> Confidence Tensor -> High-level Experts -> Fusion Layer -> Output
- Critical Path: Network inputs flow through low-level experts, their outputs are weighted by the confidence tensor, then processed by high-level experts for final fusion
- Design Tradeoffs: Collaboration vs. competition among experts; computational complexity of confidence tensor vs. performance gains; individual network specialization vs. shared representation
- Failure Signatures: Poor confidence tensor estimation leads to suboptimal expert weighting; insufficient mutual information maximization results in missed cross-network patterns; computational bottlenecks in large networks
- First Experiments: 1) Test on small synthetic multiplex network with known ground truth; 2) Evaluate convergence speed with varying expert counts; 3) Test robustness to single-layer removal

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns with large networks due to computational complexity of confidence tensor calculation
- Robustness claims based on limited attack types, requiring broader validation
- Superiority claims over Graph-MoE methods based on comparisons with only one competing approach

## Confidence
- Scalability claims: Medium confidence - limited to moderate-sized networks in experiments
- Robustness claims: Low confidence - evaluated against single attack type only
- State-of-the-art superiority: Medium confidence - based on limited comparative benchmarking

## Next Checks
1. Conduct scalability experiments with networks containing 10K+ nodes to empirically validate computational complexity claims and identify potential bottlenecks in the confidence tensor calculation
2. Evaluate robustness against multiple attack types (e.g., node injection, feature manipulation) across all datasets to strengthen the generalizability of robustness claims
3. Benchmark against at least three additional state-of-the-art multiplex network fusion methods to provide more comprehensive comparative analysis