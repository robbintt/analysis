---
ver: rpa2
title: Searching the Title of Practical Work of the Informatics Engineering Bachelor
  Program with the Case Base Reasoning Method
arxiv_id: '2508.20442'
source_url: https://arxiv.org/abs/2508.20442
tags:
- titles
- similarity
- title
- stage
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the problem of searching practical work titles
  in an Informatics Engineering program by applying Case-Based Reasoning (CBR) with
  TF-IDF for vectorization and Cosine Similarity for matching. The system processes
  titles and keywords, returning ranked results with similarity scores.
---

# Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method

## Quick Facts
- arXiv ID: 2508.20442
- Source URL: https://arxiv.org/abs/2508.20442
- Reference count: 11
- Primary result: CBR-based search using TF-IDF and Cosine Similarity achieves robust title retrieval, maintaining identical performance across word order variations.

## Executive Summary
This study addresses the challenge of searching practical work titles in an Informatics Engineering bachelor program by implementing a Case-Based Reasoning (CBR) system. The approach combines TF-IDF vectorization with Cosine Similarity to match user queries against a corpus of 705 stored titles. The system demonstrates robustness to keyword order variations, producing identical retrieval counts and perfect match scores (1.0) across different word arrangements. While the methodology is straightforward, the results validate the effectiveness of classical IR techniques for this specific educational context.

## Method Summary
The system processes 705 practical work titles from an Informatics Engineering program using TF-IDF for vectorization and Cosine Similarity for matching. User queries (titles or keywords) undergo the same TF-IDF transformation as the corpus documents. The system computes similarity scores between each query and all stored titles, returning ranked results with match values. The approach implements the Retrieve cycle of CBR, comparing new queries against the case base without adaptation mechanisms. Two evaluation stages tested original titles and randomized keyword orders, demonstrating identical retrieval performance.

## Key Results
- Identical retrieval counts across original and randomized keyword order testing stages
- Maximum average match score of 1.0 for each title in the second stage
- System returns 254-600 titles per query, prioritizing recall over precision
- Robustness demonstrated through consistent performance regardless of word order variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TF-IDF vectorization produces order-invariant document representations.
- Mechanism: Term Frequency counts word occurrences per document, while Inverse Document Frequency downweights terms appearing across many documents. The multiplication (TF × IDF) creates a weighted vector where each dimension corresponds to a term's importance, regardless of word position in the original text.
- Core assumption: Bag-of-words representation adequately captures semantic content for title matching; word order carries minimal discriminative information for this task.
- Evidence anchors:
  - [abstract] "TF-IDF is applied to process the vectorization of each practical work title word"
  - [section 2.C] "The TF-IDF function is used to find the value of each document from a training data set where the data will be formed into a vector between documents and words"
  - [corpus] Weak direct evidence; corpus focuses on job title matching and LLM applications, not TF-IDF specifically.
- Break condition: When phrasal meaning depends critically on word order (e.g., "Machine Learning" vs. "Learning Machine" as distinct concepts), TF-IDF alone cannot distinguish them.

### Mechanism 2
- Claim: Cosine Similarity normalizes for vector magnitude, enabling meaningful comparison between queries of varying length and document titles.
- Mechanism: Cosine Similarity computes the dot product of two vectors divided by the product of their magnitudes. This yields a score from 0 to 1 representing angular similarity, where 1.0 indicates identical direction regardless of length differences.
- Core assumption: Query keywords and title documents share sufficient term overlap for angular similarity to correlate with semantic relevance.
- Evidence anchors:
  - [abstract] "Cosine Similarity for the calculation of similarity values"
  - [section 2.D] "The calculation is done by calculating the result of the multiplication between the query vector and other documents"
  - [corpus] Weak direct evidence; neighboring papers discuss embedding-based and LLM methods rather than classical cosine similarity on TF-IDF.
- Break condition: When synonyms or paraphrases dominate expected queries (e.g., "AI" vs. "Artificial Intelligence"), lexical overlap fails and similarity scores drop erroneously.

### Mechanism 3
- Claim: The CBR retrieve cycle is implemented via exhaustive similarity comparison against the case base, returning all cases above an implicit threshold.
- Mechanism: The system treats each stored title as a "case." New queries are compared against all 705 cases using TF-IDF + Cosine Similarity, with results ranked by similarity score. The paper suggests returning all matches rather than top-k only.
- Core assumption: The case base is comprehensive enough that high-similarity matches represent genuinely relevant prior work; no adaptation (revise/retain) appears implemented.
- Evidence anchors:
  - [abstract] "CBR is used to search for practical work titles... The output of the system is the title of practical work and the match value of each title"
  - [section 2.B] "Retrieve is to get existing cases that are similar to the new case"
  - [section 4] Shows 254–600 titles returned per query, suggesting low or no filtering threshold.
  - [corpus] Weak direct evidence; corpus does not address CBR implementation patterns.
- Break condition: When the case base lacks coverage for novel domains, the system returns low-similarity matches that may mislead users into believing relevance exists where none does.

## Foundational Learning

- Concept: **TF-IDF (Term Frequency–Inverse Document Frequency)**
  - Why needed here: This is the core representation technique transforming unstructured titles into numerical vectors for similarity computation.
  - Quick check question: If a term appears in every document in your corpus, what will its IDF weight approximate? (Answer: Near zero, because it carries no discriminative value.)

- Concept: **Cosine Similarity**
  - Why needed here: This similarity metric determines how the system ranks and retrieves relevant titles from the case base.
  - Quick check question: Two vectors have the same terms but one has twice the magnitude. Will their Cosine Similarity be 1.0 or 0.5? (Answer: 1.0—Cosine measures angle, not magnitude.)

- Concept: **Case-Based Reasoning (CBR) Cycles**
  - Why needed here: Understanding the four cycles (Retrieve, Reuse, Revise, Retain) clarifies what this system implements versus what it omits.
  - Quick check question: This paper's system primarily implements which CBR cycle? (Answer: Retrieve—the other cycles are not evidenced in the results.)

## Architecture Onboarding

- Component map:
  Input Layer -> Preprocessing -> Vectorization -> Similarity Engine -> Output Layer

- Critical path:
  1. User submits query (title or keywords)
  2. Query undergoes same preprocessing and TF-IDF transformation as corpus documents
  3. Cosine Similarity computed against all 705 stored cases
  4. Results ranked by similarity score (descending)
  5. Top results returned with match values

- Design tradeoffs:
  - **Recall vs. Precision**: System returns large result sets (254–600 titles), optimizing for recall at the cost of precision. No threshold filtering is documented.
  - **Lexical vs. Semantic**: TF-IDF captures only lexical overlap; no embedding or semantic understanding is included. Simpler but less robust to synonyms.
  - **Order-Invariance**: By design, word order does not affect matching—beneficial for keyword queries but potentially problematic for phrasal concepts.

- Failure signatures:
  - **Synonym mismatch**: Query "Sistem Pakar" returns no high-similarity matches for titles using "Expert System" (Assumption: Indonesian-language corpus reduces this risk.)
  - **Common-term dilution**: Highly frequent terms (e.g., "Sistem," "Aplikasi") dominate similarity scores, drowning out discriminative terms.
  - **Over-retrieval**: Users may be overwhelmed by hundreds of low-relevance matches with no clear cutoff.
  - **Short-query fragility**: Single-word queries produce unreliable similarity distributions due to sparse vectors.

- First 3 experiments:
  1. **Threshold calibration**: Add a minimum similarity threshold (e.g., 0.3, 0.5, 0.7) and measure precision@10 and user satisfaction against the current unthresholded baseline.
  2. **Stopword analysis**: Identify and remove high-frequency, low-discriminability terms (e.g., "Sistem," "Aplikasi," "dan") from the TF-IDF vocabulary; compare retrieval quality.
  3. **Query length ablation**: Test system performance with 1-word, 2-word, 3-word, and full-title queries to characterize robustness and identify minimum viable query specificity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the system's retrieval performance (precision and recall) compare to standard keyword search methods or other similarity algorithms?
- Basis in paper: [inferred] The paper evaluates the system based on "number of titles found" and similarity scores for a specific set of queries, but does not calculate standard Information Retrieval metrics like Precision or Recall, nor does it compare the results against a baseline method.
- Why unresolved: Without comparative metrics or a baseline, it is unclear if the CBR/TF-IDF approach offers significant advantages over a simple database search.
- What evidence would resolve it: A comparative study reporting Precision, Recall, and F1-scores against a baseline (e.g., SQL LIKE queries or Vector Space Model) using the same dataset.

### Open Question 2
- Question: Can the system effectively handle "Retain" and "Revise" cycles to integrate new practical work titles into the case base automatically?
- Basis in paper: [inferred] The methodology describes the four cycles of CBR (Retrieve, Reuse, Revise, Retain) in the literature review, but the system description and results focus almost exclusively on the "Retrieve" phase using TF-IDF.
- Why unresolved: The paper demonstrates the system's ability to find existing cases but does not discuss or test the mechanism for updating the case base with new solutions (titles) to improve future performance.
- What evidence would resolve it: Demonstration of a feedback loop where successfully retrieved and verified new titles are automatically indexed and weighted in the case base.

### Open Question 3
- Question: How robust is the search accuracy when processing queries containing synonyms, abbreviations, or typographical errors?
- Basis in paper: [inferred] The testing phase evaluated robustness by randomizing word order (syntax), but the system relies on exact term matching via TF-IDF, which struggles with semantic variations (e.g., "App" vs "Application") or misspellings.
- Why unresolved: The current results show the system is invariant to word order, but real-world student queries often contain imperfect text; the paper does not test semantic robustness.
- What evidence would resolve it: Test results using queries with intentional typos, abbreviations, or synonym substitutions to measure the change in similarity scores and retrieval success rates.

## Limitations
- The paper provides limited detail on preprocessing steps, making it unclear how text normalization (e.g., stemming, lowercasing) affects the results.
- The choice of similarity threshold or top-k cutoff for results is unspecified, raising questions about retrieval precision.
- The dataset of 705 titles is not publicly available, limiting independent verification of the findings.

## Confidence
- **High**: The use of TF-IDF and Cosine Similarity for vectorization and similarity calculation is well-established and directly supported by the text.
- **Medium**: The claim that word order invariance improves robustness is supported by results but lacks detailed justification.
- **Low**: The effectiveness of the system for non-Indonesian language titles is untested and unverified.

## Next Checks
1. Implement preprocessing variations (with/without stemming, lowercasing) and compare retrieval quality.
2. Add a similarity threshold (e.g., 0.3, 0.5, 0.7) and evaluate precision@10 against the unthresholded baseline.
3. Test the system with queries in languages other than Indonesian to assess generalizability.