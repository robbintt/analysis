---
ver: rpa2
title: 'For GPT-4 as with Humans: Information Structure Predicts Acceptability of
  Long-Distance Dependencies'
arxiv_id: '2505.09005'
source_url: https://arxiv.org/abs/2505.09005
tags:
- acceptability
- gpt-4
- structure
- information
- ratings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that GPT-4 can reliably assess information
  structure in English sentences and that these assessments predict its acceptability
  judgments on long-distance dependency (LDD) constructions, mirroring human data.
  Using a zero-shot, structured prompting approach, GPT-4 provided information structure
  and acceptability ratings on 144 base sentences and four types of LDDs (wh-Qs, it-clefts,
  discourse-linked questions, and relative clauses).
---

# For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies

## Quick Facts
- arXiv ID: 2505.09005
- Source URL: https://arxiv.org/abs/2505.09005
- Reference count: 0
- GPT-4 reliably assesses information structure and acceptability, mirroring human data; backgroundedness predicts LDD acceptability through form-function clash.

## Executive Summary
This paper demonstrates that GPT-4 can reliably assess information structure in English sentences and that these assessments predict its acceptability judgments on long-distance dependency (LDD) constructions, mirroring human data. Using a zero-shot, structured prompting approach, GPT-4 provided information structure and acceptability ratings on 144 base sentences and four types of LDDs (wh-Qs, it-clefts, discourse-linked questions, and relative clauses). Results replicated a striking interaction: more backgrounded constituents in base sentences were judged less acceptable in corresponding LDDs. Study 2 further confirmed a causal role: emphasizing a constituent in a context sentence increased acceptability ratings on subsequent wh-Qs. These findings suggest that GPT-4 captures the systematic relationship between information structure and syntactic acceptability, reflecting flexible, context-sensitive generalizations beyond mere memorization.

## Method Summary
The study used GPT-4 (January 2025 version) via OpenAI API with zero-shot prompting, temperature=0, and 10 queries per stimulus for reliability. Participants (GPT-4) rated backgroundedness using a negation task (1-5 scale) and acceptability (1-7 scale) for 144 base sentences and four LDD types. Study 1a tested the interaction between backgroundedness and LDD acceptability; Study 1b tested novel constructions; Study 2 manipulated context prominence to assess causal effects. Ordinal mixed-effects models analyzed the data with random effects structures.

## Key Results
- GPT-4 reliably judged backgroundedness and acceptability, replicating human-like interaction between backgroundedness and LDD acceptability (β = -1.82, z = -2.84, p = .004).
- Increased backgroundedness in base sentences predicted lower acceptability in corresponding LDDs.
- Emphasizing a constituent in a context sentence increased acceptability ratings on subsequent wh-questions (β = 0.44, t = 8.48, p < .00001).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can explicitly assess information structure (backgroundedness) of sentences through metalinguistic prompting
- Mechanism: The negation task exploits a pragmatic property—when a sentence is negated, backgrounded (presupposed) content remains unaffected. By asking whether a constituent is negated, the model implicitly reveals its information structure representation through gradient Likert responses.
- Core assumption: GPT-4's training on natural discourse has encoded implicit knowledge of presupposition projection and information structure that can be elicited through zero-shot prompts.
- Evidence anchors:
  - [abstract]: "Results reveal reliable metalinguistic skill on the information structure and acceptability tasks"
  - [section] Study 1a: GPT-4 queried 10 times per stimulus with temp=0; negation task asked for 1-5 ratings on whether constituent was negated
  - [corpus] Weak direct support; related work (arxiv 2512.10453) addresses grammaticality judgments in LLMs but not information structure specifically
- Break condition: If model produces random or binary (non-gradient) backgroundedness ratings; if ratings don't correlate with human norms on same stimuli

### Mechanism 2
- Claim: Information structure judgments predict LDD acceptability through form-function clash
- Mechanism: LDD constructions (wh-Qs, it-clefts, relative clauses) inherently foreground the extracted constituent. When the same content was backgrounded in the base sentence, a functional clash occurs. More backgroundedness → more clash → lower acceptability.
- Core assumption: The Backgrounded Constructions are Islands (BCI) hypothesis: "Constructions are islands to long-distance dependency constructions to the extent that their content is backgrounded"
- Evidence anchors:
  - [abstract]: "more backgrounded constituents in base sentences were judged less acceptable in corresponding LDDs"
  - [section] Study 1a results: "The predicted interaction is significant (ß = -1.82, z = -2.84, p = .004): Increased backgroundedness predicts lower scores each of the LDD constructions"
  - [corpus] Related work (arxiv 2508.07969) on syntactic generalization in structure-inducing LMs provides converging evidence for hierarchical representation
- Break condition: If backgroundedness predicts base acceptability as strongly as LDD acceptability (no specific interaction); if effect doesn't replicate across LDD types

### Mechanism 3
- Claim: Contextual emphasis causally modulates acceptability ratings
- Mechanism: Providing a context sentence with lexical emphasis (ALL CAPS) on the to-be-queried constituent increases its discourse prominence, reducing the form-function clash when that constituent is later extracted in a wh-question.
- Core assumption: GPT-4's acceptability judgments are dynamically computed from context, not retrieved from cached associations
- Evidence anchors:
  - [abstract]: "increasing the prominence of a constituent in a context sentence increased acceptability ratings on subsequent wh-Qs"
  - [section] Study 2 results: "context-sentences that included emphasis resulted in significantly higher ratings (ß = 0.44, t = 8.48, p < .00001)"
  - [corpus] No direct corpus support for this causal manipulation in LLMs; extends human findings from Lu, Pan & Degen (2024)
- Break condition: If emphasis manipulation shows null effect; if model ignores context sentence entirely (ratings identical with/without context)

## Foundational Learning

- **Long-Distance Dependencies (LDDs) and Island Constraints**
  - Why needed: The entire paper tests whether information structure explains "island" effects—why certain extractions are unacceptable. Without this, the dependent variable (acceptability ratings on LDDs) lacks theoretical motivation.
  - Quick check question: Explain why "Who did the custodian unlock the door while admitting?" (temporal adjunct) is less acceptable than "Who did the custodian unlock the door by admitting?" (causal adjunct).

- **Information Structure (Backgroundedness/Prominence/Presupposition)**
  - Why needed: This is the predictor variable. Understanding that negating a sentence leaves backgrounded content unaffected (presupposition projection) is essential for interpreting the negation task methodology.
  - Quick check question: In "She didn't learn about the Civil War by reading the news," does "reading the news" get negated? How would this differ from "while reading the news"?

- **Metalinguistic Judgment vs. Probability-Based Evaluation in LLMs**
  - Why needed: The authors explicitly acknowledge debate about whether prompting for ratings is valid. Understanding this methodological choice is critical for interpreting results and designing follow-ups.
  - Quick check question: Why might Hu & Levy (2023) recommend log-probability comparisons over explicit prompting? What tradeoffs does each approach involve?

## Architecture Onboarding

- **Component map:** OpenAI GPT-4 API (Jan 2025) -> Zero-shot prompting with constrained output format (integer-only responses) -> Temperature = 0 for deterministic outputs -> 10 queries per stimulus for reliability estimation -> Randomized ordering, API throttling to prevent caching artifacts -> Ordinal mixed-effects models for analysis (acceptability ~ backgroundedness × sentence type)

- **Critical path:**
  1. Replicate Study 1a with open-weight models (LLaMA-3, Mixtral) using identical prompts
  2. Compare explicit ratings vs. log-probability surrogates on same stimuli
  3. Translate negation task to non-English languages to test cross-linguistic generalization

- **Design tradeoffs:**
  - Zero-shot explicit prompting limits contamination but requires metalinguistic capability
  - Temperature=0 maximizes reproducibility but may underrepresent model uncertainty
  - Likert scales provide gradient data but introduce anchoring/response bias concerns
  - GPT-4 is proprietary; cannot ablate training data or inspect internal representations

- **Failure signatures:**
  - High variance across 10 queries per stimulus (unreliable judgments)
  - Binary rather than gradient ratings (failure to capture subtle distinctions)
  - Backgroundedness predicts base acceptability as strongly as LDD acceptability (no specific interaction)
  - Null effect in Study 2 emphasis manipulation (context insensitivity)
  - Strong position/order effects in sequential prompting

- **First 3 experiments:**
  1. **Open-weight replication**: Run identical protocol on LLaMA-3-70B and Mixtral 8×22B; compute both explicit ratings and log-probability differences to assess convergence
  2. **Ablation via prompt variation**: Test whether simpler prompts (without "assume this is true" framing) produce similar results; isolate task understanding from instruction-following
  3. **Cross-construction generalization**: Test new LDD types (topicalization, pseudo-clefts) and new base constructions (conditionals, concessives) not in original 144-item set to verify generalization beyond training distribution

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Do open-weight language models exhibit the same sensitivity to information structure and long-distance dependencies when evaluated using log-probability scoring rather than explicit prompting?
  - Basis in paper: [explicit] The authors explicitly state that future work should "replicate these findings using open-weight models (e.g., Mixtral, LLaMA-3) and log-prob-based scoring techniques."
  - Why unresolved: The current study relied exclusively on GPT-4 via API, utilizing explicit metalinguistic prompts (Likert scales) rather than the log-probability measurements often preferred by researchers like Hu and Levy (2023) to isolate acceptability from other factors.
  - What evidence would resolve it: A replication of Studies 1 and 2 using open-weight models where the log-likelihood of the target constructions is compared against minimal pairs or baseline sentences.

- **Open Question 2**
  - Question: Does the relationship between backgroundedness and syntactic acceptability generalize to languages other than English or to non-WEIRD (Western, Educated, Industrialized, Rich, Democratic) linguistic contexts?
  - Basis in paper: [explicit] In the "Limitations and Future Directions" section, the authors note that "Future work needs to probe the limits of the BCI by testing... non-English (and non-WEIRD) languages."
  - Why unresolved: The experimental stimuli and the linguistic theory (Backgrounded Constructions are Islands) were tested solely on English constructions, leaving the cross-linguistic validity of the model's behavior unknown.
  - What evidence would resolve it: Administering the information structure negation task and acceptability prompts to GPT-4 or similar models using translated stimuli from typologically diverse languages.

- **Open Question 3**
  - Question: To what extent do specific components of training data influence the emergence of information structure representations in language models?
  - Basis in paper: [inferred] The authors note that using open-weight models would "offer the opportunity to vary or ablate aspects of the training data to explore how LMs capture the requisite generalizations."
  - Why unresolved: Because GPT-4 is a proprietary "black box," the mechanism by which it learned the subtle interaction between form (syntax) and function (information structure)—whether through memorization or generalization—remains opaque.
  - What evidence would resolve it: Training ablation studies on open-source models to determine if the removal of specific linguistic data types degrades the model's ability to predict acceptability based on backgroundedness.

## Limitations

- The study relies on a proprietary model (GPT-4) and fixed prompt sets, limiting reproducibility and generalizability.
- Stimuli from Study 1b and Study 2 are newly created but not fully specified, which may hinder exact replication.
- The reliance on ordinal mixed-effects models assumes linearity in latent acceptability and backgroundedness scales, though this is not directly validated.

## Confidence

- **High**: GPT-4 reliably provides gradient judgments on backgroundedness and acceptability, as evidenced by consistent replication of the interaction effect across four LDD types.
- **Medium**: The causal role of context-driven emphasis on acceptability, as Study 2 shows significant differences, but generalization to other constructions remains untested.
- **Low**: The generalizability of these findings to open-weight models or to non-English languages, given reliance on proprietary training data and English-specific information structure.

## Next Checks

1. Replicate the core findings (backgroundedness × LDD interaction) on open-weight models (e.g., LLaMA-3, Mixtral) using the same prompts and compare both explicit ratings and log-probability-based measures.
2. Conduct ablation studies varying prompt framing and output constraints to isolate the influence of metalinguistic task understanding from instruction-following.
3. Extend the experimental paradigm to non-English languages and/or novel LDD types (e.g., topicalization, pseudo-clefts) not present in the original stimulus set to test for broader generalization.