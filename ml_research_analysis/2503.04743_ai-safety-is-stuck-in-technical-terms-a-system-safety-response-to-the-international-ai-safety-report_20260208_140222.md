---
ver: rpa2
title: AI Safety is Stuck in Technical Terms -- A System Safety Response to the International
  AI Safety Report
arxiv_id: '2503.04743'
source_url: https://arxiv.org/abs/2503.04743
tags:
- safety
- system
- systems
- technical
- report
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This response critically examines the International AI Safety Report's
  technical framing of AI safety, arguing that its focus on model reliability and
  post-hoc technical mitigations is fundamentally inadequate. Drawing on system safety
  theory, the author demonstrates that safety is a property of sociotechnical systems,
  not individual AI models, and that general-purpose AI systems' inherent inscrutability
  and flexibility create irreducible safety challenges.
---

# AI Safety is Stuck in Technical Terms -- A System Safety Response to the International AI Safety Report

## Quick Facts
- arXiv ID: 2503.04743
- Source URL: https://arxiv.org/abs/2503.04743
- Authors: Roel Dobbe
- Reference count: 40
- Key outcome: Current technical AI safety discourse is fundamentally inadequate because it treats safety as a model property rather than a sociotechnical system property, making "safe general-purpose AI" theoretically impossible.

## Executive Summary
This paper argues that the dominant technical framing of AI safety in the International AI Safety Report fundamentally misunderstands what safety is and how it can be achieved. Drawing on system safety theory, the author demonstrates that safety is an emergent property of sociotechnical systems, not a property of individual AI models. The analysis shows that general-purpose AI systems' inherent inscrutability and flexibility create irreducible safety challenges that cannot be addressed through post-hoc technical mitigations alone. The paper concludes that building system safety discipline is essential for developing meaningful approaches to AI governance, particularly within the EU AI Act and Code of Practice frameworks.

## Method Summary
The author conducts a qualitative policy analysis applying system safety theory (primarily Nancy Leveson's work) to critique the International AI Safety Report's technical framing of AI safety. The analysis maps 9 fundamental issues in the current discourse against the Report's claims about safety, reliability, and risk mitigation. Rather than presenting experimental results, the paper offers a theoretical framework for understanding AI safety as a sociotechnical challenge requiring integration of technical and non-technical factors.

## Key Results
- Safety is an emergent property of sociotechnical systems, not a property of individual AI models
- Model reliability is neither necessary nor sufficient for system safety
- General-purpose AI's inscrutability and flexibility create irreducible safety challenges
- The current technical framing of AI safety frustrates meaningful discourse and policy efforts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety is an emergent property of sociotechnical systems, not a property of individual AI models.
- Mechanism: System behaviors emerge from interactions between technical components, human operators, organizational protocols, and environmental contexts. Attempting to establish safety solely through model-level interventions cannot capture these interaction effects.
- Core assumption: Hazards arise from component interactions that cannot be predicted or controlled by optimizing individual components in isolation.
- Evidence anchors: [abstract] "The system safety discipline... understands safety risks in AI systems as sociotechnical and requiring consideration of technical and non-technical factors and their interactions." [section 6.2] "Safety is a system property which can only be determined by examining the behavior of all the components working together along with the environment in which the components are operating."
- Break condition: If hazards could be exhaustively enumerated and controlled at the model output level without reference to downstream use contexts, system-level analysis would be unnecessary.

### Mechanism 2
- Claim: Model reliability is neither necessary nor sufficient for system safety.
- Mechanism: Reliable components can produce unsafe system behaviors when used in harmful contexts. Conversely, unreliable components can be embedded in systems with safeguards that prevent harm from propagating.
- Core assumption: Safety goals and norms must be defined at the system level before component requirements can be derived.
- Evidence anchors: [abstract] "identifying fundamental issues in the currently dominant technical framing of AI safety and how this frustrates meaningful discourse and policy efforts" [section 6.3] "Many accidents can occur in systems with perfectly reliable components... We have many technological systems with components that are not perfectly reliable, but which function safely nonetheless."
- Break condition: If there existed quantitative mappings between reliability metrics and safety outcomes across all deployment contexts, reliability could serve as a valid proxy for safety.

### Mechanism 3
- Claim: General-purpose AI's inscrutability and flexibility create irreducible safety challenges.
- Mechanism: The "curse of flexibility" describes how software unconstrained by physical limits accumulates complexity that exceeds human understanding. Large generative models epitomize this: their internal operations resist interpretation, and their broad applicability prevents context-specific safety engineering.
- Core assumption: Human intellectual capabilities have finite limits for understanding complex software behavior; scaling complexity eventually exceeds these limits.
- Evidence anchors: [abstract] "general-purpose AI systems' inherent inscrutability and flexibility create irreducible safety challenges" [section 6.7] "The most serious problems arise [..] when nobody understands what the software should do or even what it should not do... Generative AI can thus be seen as the epitome of the curse of flexibility."
- Break condition: If mechanistic interpretability advances could provide complete, verified specifications of model behavior, the inscrutability claim would weaken.

## Foundational Learning

- Concept: **System property vs. component property**
  - Why needed here: The central argument hinges on recognizing that some properties (safety, fairness, security) only exist at system level through component interactions, not within any single component. Without this distinction, engineers may optimize the wrong level.
  - Quick check question: Can I specify a test for this property that involves only one component in isolation, with no reference to other components, users, or environment? If yes, it's likely a component property.

- Concept: **Hazard vs. harm vs. risk**
  - Why needed here: System safety disciplines distinguish hazards (potential sources of harm), harms (realized negative outcomes), and risks (probability × severity of harm). Conflating these leads to ineffective mitigation strategies.
  - Quick check question: Am I identifying what *could* go wrong (hazards), what *has* gone wrong (harms), or the likelihood/severity tradeoffs (risks)?

- Concept: **Top-down safety requirements derivation**
  - Why needed here: Safety norms must be established at the system level based on context, then translated into component requirements. Starting from component capabilities and hoping they combine safely is the inverse of sound practice.
  - Quick check question: Have I defined what "safe" means in this specific deployment context before specifying what the model must or must not do?

## Architecture Onboarding

- Component map:
  - Technical subsystem: AI model, inference software, interfaces, content filters, logging/monitoring
  - Human layer: operators, users, affected subjects, domain experts
  - Organizational layer: protocols, training procedures, escalation paths, accountability structures
  - Environmental layer: deployment context, regulatory requirements, stakeholder expectations
  - Interactions: user-interface-model chains, model-downstream-system couplings, organizational-technical feedback loops

- Critical path:
  1. Define system boundaries (what is in/out of scope for safety analysis)
  2. Identify stakeholders and affected groups
  3. Establish context-specific safety norms and acceptable risk thresholds
  4. Map hazard scenarios across component interactions
  5. Derive component-level requirements from system-level safety goals
  6. Design interventions at multiple levels (technical safeguards, human training, organizational protocols)
  7. Build feedback mechanisms for continuous hazard detection

- Design tradeoffs:
  - Generality vs. safety: More general-purpose systems are harder to analyze and constrain
  - Post-hoc mitigation vs. redesign: Adding filters is faster than restructuring systems, but addresses symptoms not causes
  - Model size vs. inscrutability: Larger models may offer capabilities but reduce amenability to safety engineering
  - Standardization vs. contextualization: One-size-fits-all standards enable scale but miss context-specific hazards

- Failure signatures:
  - Safety claims made about models in isolation without deployment context
  - Reliability metrics used as sole proxy for safety
  - Technical and non-technical factors analyzed in separate silos
  - Responsibility delegated to end users for hazards foreseeable by developers
  - Mitigation strategies that add complexity without reducing fundamental inscrutability

- First 3 experiments:
  1. **System boundary exercise**: For an AI system you're building, explicitly diagram what's inside/outside your safety analysis. Include at minimum: model, interface, user, downstream systems, organizational protocols. Identify at least one hazard that emerges from an interaction crossing your initial boundary.
  2. **Reliability-safety separation test**: Identify a safety goal for your system. List three ways a highly reliable model could still violate that goal. List three ways an unreliable model could be embedded in a system that safely achieves that goal.
  3. **Hazard traceability matrix**: Take one concrete hazard scenario. Trace backwards: what system-level safety norm does it violate? What component interactions produce it? What interventions at each level (technical, human, organizational) could prevent or mitigate it?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What methodological processes are required to determine and negotiate risks to arrive at a complete and inclusive list, beyond current ad-hoc approaches?
- Basis in paper: [explicit] The author asks, "What could be missing and why? And what is needed to arrive at an inclusive list?" regarding the International AI Safety Report's risk taxonomy.
- Why unresolved: Current reports treat risks without reflecting on the completeness of the lists or the negotiation processes used to include them.
- What evidence would resolve it: A formalized, multi-stakeholder risk identification framework that demonstrates higher coverage of systemic harms than current technical taxonomies.

### Open Question 2
- Question: To what extent should AI systems remain "general-purpose" when deployed in safety-critical contexts?
- Basis in paper: [explicit] The author explicitly poses, "So how 'general-purpose' should future AI systems in safety-critical context be?"
- Why unresolved: Pushing for general-purpose capabilities increases complexity and inscrutability (the "curse of flexibility"), which conflicts with the rigidity required for safety engineering.
- What evidence would resolve it: Comparative safety analysis of general-purpose vs. narrow AI in specific high-stakes sectors, identifying the threshold where flexibility compromises safety.

### Open Question 3
- Question: How can system safety principles be operationalized within governance frameworks like the EU Code of Practice to avoid treating non-technical factors as mere add-ons?
- Basis in paper: [inferred] The paper notes that the Code currently splits technical and non-technical aspects, while arguing that system safety requires "integrating rather than adding on non-technical factors."
- Why unresolved: Regulatory structures often silo technical mitigation from social context, failing to address the sociotechnical interactions that actually produce safety or harm.
- What evidence would resolve it: New regulatory templates that successfully map interactions between technical components, human factors, and organizational structures in a single integrated audit process.

## Limitations

- The analysis relies heavily on system safety theory literature but lacks empirical validation of claims about AI systems specifically
- The critique of the International AI Safety Report could be affected by incomplete reference coverage (author acknowledges missing references)
- Claims about inscrutability and the "curse of flexibility" are theoretical rather than empirically demonstrated for current large models

## Confidence

- High confidence: System safety framework applies to AI safety problems; safety is a sociotechnical system property, not a model property
- Medium confidence: Model reliability ≠ system safety; general-purpose AI creates irreducible inscrutability challenges
- Lower confidence: "Safe general-purpose AI" is fundamentally impossible (this requires more empirical validation)

## Next Checks

1. **Empirical test of inscrutability claims**: Select 3-5 general-purpose AI systems with different capabilities. Attempt to derive complete behavioral specifications through current interpretability methods. Document where methods succeed/fail and whether failures appear fundamental or technique-limited.

2. **Reliability-safety correlation study**: Analyze 10+ deployed AI systems where both reliability metrics and safety outcomes are available. Test whether reliability scores predict safety outcomes across different deployment contexts.

3. **System boundary validation**: Take 3 real-world AI deployments. Apply the proposed system safety analysis framework. Compare results with existing safety assessments to identify gaps and validate the framework's practical utility.