---
ver: rpa2
title: Alleviating Choice Supportive Bias in LLM with Reasoning Dependency Generation
arxiv_id: '2512.03082'
source_url: https://arxiv.org/abs/2512.03082
tags:
- bias
- feature
- chosen
- while
- option
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses choice-supportive bias (CSB) in LLMs, where
  models retroactively favor chosen options. The proposed Reasoning Dependency Generation
  (RDG) framework generates balanced training data that explicitly varies dependencies
  between choices, facts, and justifications.
---

# Alleviating Choice Supportive Bias in LLM with Reasoning Dependency Generation

## Quick Facts
- arXiv ID: 2512.03082
- Source URL: https://arxiv.org/abs/2512.03082
- Reference count: 40
- Primary result: 81.5% reduction in memory-based CSB and 94.3% reduction in evaluation-based CSB while maintaining BBQ benchmark performance

## Executive Summary
This work addresses choice-supportive bias (CSB) in large language models (LLMs), where models retroactively favor chosen options by misattributing positive features to them and negative features to rejected options. The proposed Reasoning Dependency Generation (RDG) framework generates balanced training data that explicitly varies dependencies between choices, facts, and justifications. RDG employs two complementary strategies: Contextual Dependency Data strengthens appropriate factual dependencies, while Dependency Decoupling Data breaks inappropriate choice-feature associations. Experiments demonstrate significant improvements in bias reduction while maintaining similar performance on BBQ benchmarks, contributing to more reliable AI-assisted decision support systems.

## Method Summary
The RDG framework generates synthetic training data to address choice-supportive bias in LLMs through two complementary strategies. Contextual Dependency Data strengthens appropriate factual dependencies between choices and supporting facts, while Dependency Decoupling Data breaks inappropriate associations between choices and their features. The approach creates balanced training examples where the model learns to evaluate choices based on objective facts rather than retroactively justifying decisions. The framework uses self-verification tasks with comparative reasoning to explicitly teach models when dependencies should exist and when they should be broken, effectively training the model to avoid post-hoc rationalization of decisions.

## Key Results
- 81.5% reduction in memory-based choice-supportive bias
- 94.3% reduction in evaluation-based choice-supportive bias
- Maintained performance on BBQ benchmarks while reducing bias

## Why This Works (Mechanism)
The RDG framework works by explicitly controlling the dependencies between choices, facts, and justifications during training. By generating data where these relationships are either strengthened (when appropriate) or decoupled (when inappropriate), the model learns to evaluate options based on objective criteria rather than post-hoc rationalization. The self-verification tasks force the model to justify its reasoning in ways that reveal whether it's relying on factual evidence or retroactive justification, allowing the training process to reinforce correct dependency patterns and break incorrect ones.

## Foundational Learning

**Choice-Supportive Bias**: The tendency to retroactively assign positive attributes to chosen options and negative attributes to rejected ones. Why needed: Understanding this cognitive bias is essential to recognizing the problem RDG addresses in LLMs.

**Dependency Modeling**: The practice of explicitly defining relationships between choices, facts, and justifications. Why needed: RDG's effectiveness relies on controlling these dependencies during training to teach appropriate reasoning patterns.

**Synthetic Data Generation**: Creating artificial training examples with controlled properties. Why needed: RDG uses synthetic data to create specific dependency patterns that would be difficult to obtain from real-world examples alone.

## Architecture Onboarding

Component Map: RDG Framework -> Synthetic Data Generator -> Balanced Training Examples -> Bias-Reduced LLM

Critical Path: Synthetic Data Generation → Model Training → Bias Evaluation → Fine-tuning Loop

Design Tradeoffs: Synthetic data provides control over dependency patterns but may lack real-world complexity; maintaining benchmark performance while reducing bias requires careful balance between these objectives.

Failure Signatures: Model continues to show post-hoc rationalization patterns; performance degradation on BBQ benchmarks; inability to distinguish appropriate vs. inappropriate dependencies.

First Experiments:
1. Generate small-scale synthetic dataset testing single dependency patterns
2. Train baseline model on mixed dependency data to establish performance baseline
3. Evaluate initial bias reduction on simplified choice-supportive bias tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic data may not fully capture real-world decision complexity
- Evaluation focuses on controlled settings rather than open-ended decision support
- Does not address potential unintended consequences of bias reduction
- Does not investigate long-term stability across different model versions

## Confidence

High confidence: The reported reductions in choice-supportive bias (81.5% and 94.3% for memory-based and evaluation-based CSB respectively) are well-supported by the experimental methodology and statistical analysis presented. The BBQ benchmark results showing maintained performance while reducing bias are also robust.

Medium confidence: The generalization of these results to real-world decision-making scenarios and other types of cognitive biases beyond choice-supportive bias. The effectiveness of the RDG framework across different model architectures and sizes remains to be validated.

Low confidence: The claim that this work "pioneers a method for addressing cognitive biases in LLMs" is difficult to verify definitively given the rapidly evolving research landscape in this area. The long-term implications of reducing choice-supportive bias on model performance and user trust are not fully explored.

## Next Checks

1. Real-world deployment study: Test the RDG framework in actual decision support applications with human users to evaluate whether bias reduction translates to improved decision quality and user satisfaction in practical contexts.

2. Cross-bias generalization: Apply the RDG methodology to other well-documented cognitive biases (e.g., confirmation bias, anchoring bias) to assess whether the framework can be generalized beyond choice-supportive bias.

3. Stability and scalability analysis: Evaluate the persistence of bias reduction effects across different model sizes, training iterations, and over extended periods to ensure the solution is robust and scalable.