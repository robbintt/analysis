---
ver: rpa2
title: Detection Avoidance Techniques for Large Language Models
arxiv_id: '2503.07595'
source_url: https://arxiv.org/abs/2503.07595
tags:
- paraphrasing
- detection
- sampling
- used
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates techniques to bypass large language model
  (LLM) detection systems, which are increasingly important due to the proliferation
  of AI-generated content. The authors demonstrate three effective evasion methods
  across three experiments: modifying LLM temperature parameters to reduce shallow
  detector accuracy below 60%, applying reinforcement learning to bypass BERT-based
  detectors (reducing detection rates from over 90% to below 17%), and using recursive
  paraphrasing to evade zero-shot detectors like DetectGPT with 90% success while
  preserving high content similarity.'
---

# Detection Avoidance Techniques for Large Language Models

## Quick Facts
- arXiv ID: 2503.07595
- Source URL: https://arxiv.org/abs/2503.07595
- Authors: Sinclair Schneider; Florian Steuber; Joao A. G. Schneider; Gabi Dreo Rodosek
- Reference count: 0
- This study demonstrates three effective techniques to bypass LLM detection systems: temperature adjustment, RL fine-tuning, and recursive paraphrasing.

## Executive Summary
This paper investigates techniques to circumvent large language model detection systems, which are critical for identifying AI-generated content. The authors demonstrate three distinct evasion methods across three experiments: modifying LLM temperature parameters to reduce shallow detector accuracy below 60%, applying reinforcement learning to bypass BERT-based detectors (reducing detection rates from over 90% to below 17%), and using recursive paraphrasing to evade zero-shot detectors like DetectGPT with >90% success while preserving high content similarity. These methods collectively show that LLM detection classifiers can be easily circumvented with sufficient knowledge and effort, highlighting the need for more robust detection mechanisms and the potential societal implications of undetectable AI-generated content.

## Method Summary
The study employs three distinct experimental approaches to evade LLM detection systems. Experiment 1 systematically varies temperature parameters (0.6-1.4) and sampling methods to reduce shallow detector accuracy. Experiment 2 uses reinforcement learning with custom reward functions to fine-tune generative models to evade BERT-based classifiers. Experiment 3 implements recursive paraphrasing using a T5-11B model trained to maximize original generator log loss, filtering outputs by semantic similarity and linguistic acceptability. The experiments use multiple datasets including filtered Twitter data, Human ChatGPT Comparison Corpus, Natural Questions, and CNN/Daily Mail, with evaluation metrics focusing on detection accuracy, F1-scores, cosine similarity, and linguistic acceptability scores.

## Key Results
- Temperature adjustment reduced shallow detector accuracy below 60% across multiple sampling methods
- RL fine-tuning circumvented BERT-based detectors, reducing detection rates from over 90% to below 17%
- Recursive paraphrasing achieved >90% evasion of zero-shot detectors like DetectGPT while maintaining high content similarity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing temperature parameter flattens token probability distribution, reducing shallow detector accuracy
- **Mechanism:** Higher temperature introduces entropy, shifting statistical properties away from predictable LLM artifacts toward human-like noise profiles
- **Core assumption:** Shallow detectors rely heavily on predictable token frequencies and low-entropy patterns
- **Evidence anchors:** Systematic temperature changes proved shallow detectors least reliable; U-shaped accuracy curve centered at τ ≈ 1
- **Break condition:** Fails against transformer-based detectors using contextual embeddings

### Mechanism 2
- **Claim:** RL fine-tuning with custom reward function can hide generation from BERT-based classifiers
- **Mechanism:** Reward function maximizes probability of being classified as "human" while maintaining linguistic quality through constraints
- **Core assumption:** Attacker has access to target classifier weights or inference API for reward calculation
- **Evidence anchors:** Fine-tuning reduced BERT detection from over 90% to below 17%; reward calculation combines evasion with linguistic constraints
- **Break condition:** Fails if detector is retrained on RL-finetuned model distribution or constraints are too loose

### Mechanism 3
- **Claim:** Recursive paraphrasing increases log loss relative to generating model to evade zero-shot detectors
- **Mechanism:** DetectGPT identifies machine text by low curvature in token likelihood space; paraphrasing disrupts this signature while preserving meaning
- **Core assumption:** Zero-shot detectors rely on specific probability curvature that can be disrupted without destroying semantic meaning
- **Evidence anchors:** Paraphrasing led to >90% evasion of DetectGPT while maintaining high similarity; selecting highest log loss breaks detection
- **Break condition:** Excessive paraphrasing causes semantic drift and quality degradation

## Foundational Learning

- **Concept: Temperature Sampling (τ)**
  - **Why needed here:** Experiment 1 relies entirely on manipulating this hyperparameter to defeat statistical detectors
  - **Quick check question:** Does increasing temperature make the output more deterministic or more random?

- **Concept: Reward Modeling in RL**
  - **Why needed here:** Experiment 2 uses complex reward function balancing evasion success against linguistic quality
  - **Quick check question:** In this paper, does the RL agent receive higher reward for text classified as "machine-generated" or "human-generated"?

- **Concept: Zero-Shot Detection (Log Probability)**
  - **Why needed here:** Experiment 3 attacks specific statistical signature (log likelihood) used by detectors like DetectGPT
  - **Quick check question:** Why does increasing "log loss" of text help it evade detector that checks for AI-generated probability?

## Architecture Onboarding

- **Component map:** Generator (LLM) -> Attack Vector (Temperature/RL/Paraphrasing) -> Target (Detector) -> Constraints (Linguistic/Semantic)
- **Critical path:** 1) Generate/Select base text, 2) Apply modification (RL optimization or Paraphrasing), 3) Filter outputs through constraints, 4) Validate evasion against target detector
- **Design tradeoffs:**
  - RL: High compute, requires model access, highly effective against specific targets
  - Paraphrasing: Lower compute, works on black-box models, risk of semantic drift
  - Temperature: Zero cost, only works on weak detectors, preserves exact meaning
- **Failure signatures:**
  - RL: Output degrades into repetitive loops or gibberish if constraints too weak
  - Paraphrasing: Factual errors introduced during rewriting; loss of nuance
  - General: High confidence score from target detector indicating evasion failed
- **First 3 experiments:**
  1. Grid Search on Temperature: Sweep temperature (0.6-1.4) with nucleus sampling against Naive Bayes classifier
  2. RL Fine-Tuning Loop: Train generator using PPO where reward is inverse of BERT classifier's detection score
  3. Recursive Paraphrasing Attack: Iteratively pass text through T5-paraphraser, filtering for high similarity but high log loss

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental results rely on specific model versions and datasets that may not be publicly available
- Twitter dataset used for training cannot be shared due to platform restrictions
- RL experiments depend on precise hyperparameter configurations and reward function weights
- Study focuses on detection evasion rather than detection robustness, creating asymmetric arms race

## Confidence
- **High Confidence:** Temperature-based evasion mechanism shows consistent results across multiple sampling methods and detector types
- **Medium Confidence:** RL-based evasion demonstrates strong empirical results but depends on specific implementation details
- **Medium Confidence:** Paraphrasing-based evasion shows >90% success rates but long-term stability remains uncertain

## Next Checks
1. **Dataset Independence Test:** Replicate temperature-based evasion experiments using publicly available datasets to verify U-shaped detection curve is not dataset-specific
2. **Detector Robustness Evaluation:** Train BERT detector on texts generated by RL-finetuned model to test adversarial generalization
3. **Semantic Stability Analysis:** Conduct longitudinal study of recursive paraphrasing, tracking semantic similarity and factual accuracy over 10+ iterations to quantify semantic drift rate