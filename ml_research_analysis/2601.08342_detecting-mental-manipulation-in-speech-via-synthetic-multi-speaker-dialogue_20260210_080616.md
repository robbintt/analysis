---
ver: rpa2
title: Detecting Mental Manipulation in Speech via Synthetic Multi-Speaker Dialogue
arxiv_id: '2601.08342'
source_url: https://arxiv.org/abs/2601.08342
tags:
- audio
- manipulation
- speech
- manipulative
- mental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces SPEECHMENTALMANIP, the first benchmark for
  detecting mental manipulation in speech by extending a text-based dataset with high-quality,
  voice-consistent TTS-rendered audio. It evaluates few-shot large audio-language
  models and human perception of manipulative intent in speech versus text.
---

# Detecting Mental Manipulation in Speech via Synthetic Multi-Speaker Dialogue

## Quick Facts
- **arXiv ID**: 2601.08342
- **Source URL**: https://arxiv.org/abs/2601.08342
- **Reference count**: 15
- **Primary result**: SPEECHMENTALMANIP benchmark shows audio models detect manipulation with high specificity but low recall compared to text.

## Executive Summary
This study introduces SPEECHMENTALMANIP, the first benchmark for detecting mental manipulation in speech by extending a text-based dataset with high-quality, voice-consistent TTS-rendered audio. It evaluates few-shot large audio-language models and human perception of manipulative intent in speech versus text. Models show high specificity but markedly lower recall on speech, favoring conservative judgments. Human raters also show lower agreement on audio, underscoring the inherent ambiguity of manipulative speech. The findings highlight modality-induced challenges in detection and emphasize the need for modality-aware evaluation and alignment in multimodal dialogue systems.

## Method Summary
The study extends MENTALMANIP_CON (2,915 dialogues) with TTS-rendered audio using ElevenLabs voices, creating SPEECHMENTALMANIP benchmark. A Qwen2.5-Omni-7B model performs few-shot detection with 4-shot prompting (2 YES, 2 NO examples) and constrained decoding. The pipeline involves two-phase TTS synthesis per turn, concatenation with 0.2s silences, and majority-vote inference across five passes. Human annotation compares text vs. audio perception of manipulation using Cohen's and Krippendorff's agreement metrics.

## Key Results
- Qwen2.5-Omni achieves high specificity (82.2%) but low recall (34.8%) on audio manipulation detection
- Models systematically favor certain tactics (Intimidation ~49%, Persuasion ~30%) while missing semantic tactics like Rationalization
- Human raters show lower agreement on audio (Krippendorff's Alpha) compared to text annotations
- Text-to-audio modality shift reveals substantial gaps in manipulation detection performance

## Why This Works (Mechanism)
The approach works by isolating prosodic and acoustic features through high-quality TTS synthesis while preserving conversational structure. The two-phase synthesis ensures voice consistency across turns, and constrained decoding with few-shot exemplars guides the model toward conservative decision-making. The benchmark design reveals modality-specific detection challenges by maintaining identical content across text and audio formats while varying only the perceptual channel.

## Foundational Learning

**TTS Synthesis Pipeline**: Two-phase approach synthesizing per-turn audio then concatenating preserves speaker identity and conversational flow. *Why needed*: Ensures voice consistency across multi-turn dialogues. *Quick check*: Verify speaker voice remains stable across consecutive turns.

**Few-shot Audio-LM Prompting**: 4-shot exemplars with constrained decoding guides model behavior. *Why needed*: Provides behavioral priors for the model in low-resource detection task. *Quick check*: Confirm exemplar selection covers both YES/NO cases.

**Constrained Decoding with Logit Fallback**: A/B-constrained selection with single-token fallback ensures binary decisions. *Why needed*: Forces model to commit to one class rather than abstain. *Quick check*: Verify p(YES) > p(NO) threshold is correctly applied.

**Modality-Aware Evaluation**: Comparing text vs. audio human annotations reveals perceptual differences. *Why needed*: Identifies modality-specific challenges in manipulation detection. *Quick check*: Compare agreement metrics across modalities.

**Tactic Distribution Analysis**: Examining predicted vs. ground truth tactic frequencies reveals model biases. *Why needed*: Identifies systematic blind spots in detection capabilities. *Quick check*: Compare predicted tactic distribution against ground truth.

## Architecture Onboarding

**Component Map**: MENTALMANIP_CON -> TTS Synthesis -> Audio Concatenation -> Qwen2.5-Omni Inference -> Majority Vote -> Tactic Classification

**Critical Path**: Text transcripts → TTS synthesis → Audio concatenation → Model inference → Decision aggregation

**Design Tradeoffs**: Synthetic TTS enables controlled experimentation but lacks natural speech variability; few-shot learning reduces data requirements but depends heavily on exemplar quality; constrained decoding ensures decisions but may amplify conservative bias.

**Failure Signatures**: Low recall with high specificity indicates systematic NO bias; tactic distribution skew toward acoustic tactics reveals semantic detection blind spots; inter-rater disagreement on audio suggests inherent ambiguity.

**First Experiments**: 1) Validate voice consistency across multi-turn dialogues; 2) Test exemplar recovery impact on model performance; 3) Compare constrained vs. unconstrained decoding outcomes.

## Open Questions the Paper Calls Out

**Open Question 1**: How does using modality-faithful, audio-first human annotations reshape the precision-recall trade-offs and tactic attribution scores compared to the current transcript-based ground truth? The paper explicitly states this will be investigated using re-annotated audio-first labels to quantify modality effects on evaluation metrics.

**Open Question 2**: Can audio-language models maintain detection performance when evaluated on ecologically realistic speech containing overlapping turns, interruptions, and backchanneling? The limitations section identifies synthetic dialogues' lack of overlapping speech as a key constraint requiring future work with Behavior-SD style simulation.

**Open Question 3**: Does the use of definition-augmented prompting or alternative model architectures mitigate the low recall and conservative judgment bias observed in the Qwen2.5-Omni model? The paper leaves open whether alternative architectures or explicit tactic definitions would improve sensitivity without sacrificing precision.

## Limitations

- Missing few-shot audio exemplars (EX1_NO_AUDIO through EX4_YES_AUDIO) prevents exact reproduction of model behavior
- Small human annotation sample sizes (6 for text, 11 for audio) limit generalizability of perceptual findings
- Synthetic TTS lacks natural speech variability including overlapping dialogue and interruptions
- Model shows systematic bias toward acoustic tactics while missing semantic manipulation strategies

## Confidence

**High Confidence**: Benchmark construction methodology and overall finding that audio modality presents detection challenges compared to text.

**Medium Confidence**: Specific performance metrics depend on missing exemplars; tactic distribution analysis may reflect model bias rather than ground truth.

**Low Confidence**: Human perception claims require larger participant pools; audio ambiguity assertions need more conclusive experimental design.

## Next Checks

1. Attempt to recover the missing few-shot audio exemplars (EX1_NO_AUDIO through EX4_YES_AUDIO) from authors or supplementary materials

2. Implement and validate the two-phase TTS synthesis pipeline with ElevenLabs API, testing voice consistency across multi-turn dialogues

3. Reconstruct the complete prompt format based on partial specifications and validate through controlled experiments