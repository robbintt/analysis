---
ver: rpa2
title: 'CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding'
arxiv_id: '2602.01785'
source_url: https://arxiv.org/abs/2602.01785
tags:
- code
- visual
- compression
- text
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first comprehensive study of using multimodal\
  \ large language models (MLLMs) for code understanding through image-based representation.\
  \ The core method renders source code as images and leverages visual compression\
  \ via resolution scaling, achieving up to 8\xD7 token reduction while maintaining\
  \ readability for vision-capable models."
---

# CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding

## Quick Facts
- arXiv ID: 2602.01785
- Source URL: https://arxiv.org/abs/2602.01785
- Reference count: 24
- One-line result: MLLMs can effectively understand code images with up to 8× token reduction via visual compression, achieving comparable or superior performance to text-based approaches on code understanding tasks.

## Executive Summary
This paper presents the first comprehensive study of using multimodal large language models (MLLMs) for code understanding through image-based representation. The core method renders source code as images and leverages visual compression via resolution scaling, achieving up to 8× token reduction while maintaining readability for vision-capable models. Experimental results across four tasks—code completion, summarization, clone detection, and question answering—demonstrate that MLLMs can effectively understand code images, with some models achieving comparable or superior performance to text-based approaches. Notably, models like Gemini-3-Pro maintain stable performance even at 8× compression (12.5% of original tokens), while visual enhancements like syntax highlighting provide additional benefits at moderate compression levels. The study also reveals task-dependent resilience patterns and establishes that visual compression follows predictable degradation hierarchies.

## Method Summary
The study renders source code as images at 2240×2240px resolution with monospace fonts, then applies bilinear downsampling for compression ratios of 1× to 8×. Seven MLLMs (Qwen-3-VL, GLM-4.6v, GPT-5-mini, GPT-5.1, Gemini-2.5-Pro, Gemini-3-Flash, Gemini-3-Pro) are evaluated across four tasks using four datasets: LongModuleSummarization (109 Python), LongCodeCompletion (200 Python + 200 Java with RAG), GPT-CloneBench (200 Python + 200 Java pairs), and custom CodeQA (200 Python from post-Aug-2025 GitHub repos). Metrics include Edit Similarity, Exact Match, CompScore (LLM-as-judge), Accuracy, and F1, with Wilcoxon signed-rank tests for significance.

## Key Results
- MLLMs achieve comparable or superior performance to text-based approaches on code understanding tasks using image-based representation
- Visual compression via resolution scaling enables up to 8× token reduction while maintaining performance, with some models (Gemini-3-Pro) stable even at 12.5% of original tokens
- Visual enhancements like syntax highlighting provide additional benefits at moderate compression levels (1×-4×) but interfere at extreme compression (8×)
- Code clone detection shows surprising resilience to visual compression, with some models improving at 2×-4× compression

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rendering source code as images enables substantial token reduction via resolution scaling while preserving semantic understanding, provided the MLLM has robust visual encoding capabilities.
- **Mechanism:** Unlike text, which requires discrete token pruning to compress (often losing semantics), image modality allows continuous downsampling. By reducing resolution, multiple visual "patches" effectively merge, carrying dense structural information into fewer tokens.
- **Core assumption:** MLLMs can extract semantic meaning from lower-resolution visual inputs better than text-models can extract meaning from pruned text sequences.
- **Evidence anchors:** [abstract] "visual compression via resolution scaling, achieving up to 8× token reduction while maintaining readability"; [section 1] "image data can be scaled by simply adjusting resolution... whereas compressing code text... is a discrete and often lossy process."

### Mechanism 2
- **Claim:** Visual representation allows models to leverage holistic 2D spatial cues (indentation, layout, syntax coloring) which may provide structural advantages over sequential 1D text processing for specific tasks.
- **Mechanism:** Text models process code as a linear sequence, relying on attention mechanisms to connect distant related tokens. MLLMs treat code as a visual image where structural relationships (e.g., nested blocks, vertical alignment) are spatially proximate in the 2D pixel grid.
- **Core assumption:** The visual encoder preserves spatial relationships effectively and does not destroy structural cues during patch embedding.
- **Evidence anchors:** [section 2] "MLLMs learn to interpret continuous visual patterns—such as color-coded keywords, indentation depth... without explicit parsing"; [section 4.1.1] "One possible explanation is that visual representations enable models to perceive code structure holistically... capturing indentation patterns... in a single glance."

### Mechanism 3
- **Claim:** Visual compression acts as a "semantic denoising" mechanism that filters low-level syntactic details while preserving high-level logic, explaining why tasks like clone detection remain resilient or even improve under compression.
- **Mechanism:** The paper identifies a degradation hierarchy: compression first introduces token-level errors (blurring characters), then line-level, and finally block-level. Tasks like summarization or clone detection rely on "block-level" semantics.
- **Core assumption:** High-level semantic tasks do not require character-perfect input resolution.
- **Evidence anchors:** [section 4.2.1] "moderate compression acts as a denoising mechanism, blurring syntactic details and encouraging models to focus on semantic equivalence"; [section 4.5.2] "Information degradation follows a clear hierarchical pattern... Token-level errors do not always impair downstream semantic performance."

## Foundational Learning

- **Concept: Vision Transformer (ViT) Patching**
  - **Why needed here:** To understand how an MLLM "sees" code. Code isn't read character-by-character like text; it is split into fixed-size squares (patches, e.g., 14x14 pixels) and converted into vectors.
  - **Quick check question:** If you reduce an image's resolution by half, how does that affect the number of visual patches (tokens) the model processes?

- **Concept: Token Equivalence & Pricing**
  - **Why needed here:** The paper's core economic argument. Visual tokens are priced similarly to text tokens by API providers. "8x compression" means you use 1/8th the tokens, directly translating to cost savings.
  - **Quick check question:** Why does the paper define "1x compression" as matching the visual token count to the text token count, rather than matching the file size?

- **Concept: Type-4 (Semantic) Clone Detection**
  - **Why needed here:** This task is highlighted as surprisingly resilient to visual compression. Understanding that it involves finding functionally similar code with different syntax helps explain why "blurring" syntactic details helps rather than hurts.
  - **Quick check question:** Why would a model perform *better* on semantic clone detection when the input image is compressed and harder for a human to read?

## Architecture Onboarding

- **Component map:**
  CodeOCR (Renderer) -> Compressor -> Visual Encoder (e.g., ViT) -> V-L Adapter -> LLM Backbone

- **Critical path:**
  The resolution-to-patch-density ratio is the critical variable. You must ensure that at your chosen compression ratio (e.g., 8x), the number of pixels per character is still sufficient for the Visual Encoder to differentiate glyph shapes; otherwise, the LLM receives garbage noise.

- **Design tradeoffs:**
  - **Highlighting vs. Compression:** Syntax highlighting helps at 1x-2x but hurts at 8x because coloring artifacts consume pixel space that is needed for glyph clarity at low resolutions.
  - **Cost vs. Precision:** Using visual mode saves ~75-87.5% tokens (4x-8x) but introduces a "Token Error" rate that is unacceptable for tasks requiring exact variable names (Code Completion) but acceptable for summarization.

- **Failure signatures:**
  - **"Performance Cliff" (GLM/Qwen):** Sudden accuracy drop between 4x and 8x compression, indicating the visual encoder fails to resolve features below a specific pixel density.
  - **Hallucinated Structure:** At high compression, the model predicts valid-looking code syntax that does not exist in the image (Block Errors), inventing logic to fill visual gaps.

- **First 3 experiments:**
  1. **Sanity Check (Text vs. Image 1x):** Run a small benchmark (e.g., 50 samples) comparing raw text input vs. 1x visual input on your target model to establish if the model has sufficient visual capability at all.
  2. **Compression Sweep (2x, 4x, 8x):** Test the specific task (e.g., Summarization) across compression ratios to find the "sweet spot" where token cost drops but accuracy remains within 1-2% of the text baseline.
  3. **Visual Enhancement Ablation:** Test "Plain" vs. "Syntax Highlighted" rendering at the optimal compression ratio found in step 2 to see if color cues persist or interfere.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can task-adaptive rendering strategies that dynamically adjust visual enhancements (syntax highlighting, bold rendering) based on target compression ratio and task type outperform static rendering configurations?
- **Basis in paper:** [explicit] Finding #6 states: "Adaptive rendering strategies that adjust enhancement based on target compression ratio represent a promising direction for future optimization." The paper also notes that visual enhancements are most effective within a compression "sweet spot" of 1×–4×.
- **Why unresolved:** The paper tested only static rendering configurations (plain, bold, highlight) across all compression levels, but did not explore dynamic switching strategies that could, for example, apply highlighting at 2× compression but use plain rendering at 8×.
- **What evidence would resolve it:** Experiments comparing static vs. adaptive rendering policies, measuring performance across tasks at various compression levels with rendering strategies selected based on compression ratio thresholds.

### Open Question 2
- **Question:** Can code-specialized visual pre-training or fine-tuning improve MLLM performance on code image understanding tasks, particularly for models showing weak compression resilience?
- **Basis in paper:** [explicit] The conclusion states: "We identify significant OCR capability gaps across models, pointing to the need for code-specific visual pre-training." Additionally, Finding #2 notes that "bridging this gap remains an important direction for future research" regarding models not optimized for this paradigm.
- **Why unresolved:** The study evaluated off-the-shelf MLLMs without any code-image-specific training. Models like Qwen-3-VL and GLM-4.6v showed significant degradation, but it remains unknown whether targeted training could close this gap.
- **What evidence would resolve it:** Training experiments with code-image-specific objectives (e.g., code image-to-text reconstruction, compressed code understanding) followed by evaluation on the same benchmarks.

### Open Question 3
- **Question:** Do the findings on visual code understanding generalize to programming languages beyond Python and Java, particularly those with fundamentally different syntactic conventions?
- **Basis in paper:** [inferred] RQ4 tested only Java to validate Python findings. The paper acknowledges this limitation but does not explore languages with significantly different characteristics (e.g., whitespace-insensitive C/C++, functional languages like Haskell, or indentation-based languages like YAML).
- **Why unresolved:** The authors validated findings on one additional language (Java) but did not systematically evaluate languages with divergent syntactic structures, commenting styles, or visual densities that may affect compressibility differently.
- **What evidence would resolve it:** Replication of RQ1–RQ3 experiments on benchmarks from at least 3–5 additional languages spanning different paradigms (functional, systems, scripting) and syntax styles.

### Open Question 4
- **Question:** What compression techniques beyond simple bilinear downsampling could achieve higher compression ratios (beyond 8×) while preserving code semantics for MLLMs?
- **Basis in paper:** [explicit] The conclusion explicitly calls for "future work on... aggressive compression techniques." The paper tested only resolution-based compression via bilinear downsampling, achieving up to 8× compression.
- **Why unresolved:** The paper did not explore alternative compression approaches such as semantic-aware downsampling, region-based compression (higher resolution for critical code regions), or learned compression methods adapted to code structure.
- **What evidence would resolve it:** Comparative experiments evaluating alternative compression algorithms (e.g., perceptual compression, attention-guided compression, hybrid approaches) at 16× and higher ratios on the same task benchmarks.

## Limitations

- **Dataset representativeness:** The custom CodeQA dataset uses post-August 2025 GitHub repositories, potentially creating domain mismatch for pre-2025 models and limiting generalizability to legacy codebases.
- **Visual model capability variance:** Performance differences across MLLMs suggest strong dependence on specific visual encoder architectures, making results model-specific rather than universally applicable.
- **Task-specific limitations:** Code completion shows vulnerability to visual compression due to token-level precision requirements, but the paper doesn't explore hybrid approaches that might optimize across tasks.

## Confidence

**High confidence:** The core finding that MLLMs can understand code from images is well-supported by consistent performance across multiple models and tasks. The compression mechanism via resolution scaling is technically sound and reproducible.

**Medium confidence:** The claim that visual compression provides "semantic denoising" benefits is plausible but needs more direct evidence. The performance differences between models at various compression ratios are observable but not fully explained by architectural analysis.

**Low confidence:** The assertion that visual representation provides inherent structural advantages over sequential text processing lacks strong comparative evidence. The CodeQA dataset's representativeness and the generalizability of findings to non-Python codebases are uncertain.

## Next Checks

1. **Cross-domain robustness test:** Validate the visual compression approach on non-Python languages (JavaScript, C++, Go) and legacy codebases predating 2025 to assess temporal and linguistic generalizability.

2. **Hybrid modality ablation:** Design experiments comparing pure visual, pure text, and hybrid approaches for each task to determine optimal modality combinations rather than assuming visual-only is optimal.

3. **Visual encoder architecture analysis:** Conduct controlled experiments varying patch size, resolution handling, and pre-training data composition across models to identify which architectural features determine compression resilience and performance.