---
ver: rpa2
title: Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging
arxiv_id: '2510.14697'
source_url: https://arxiv.org/abs/2510.14697
tags:
- task
- merging
- arxiv
- performance
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses performance degradation in model merging caused
  by task-irrelevant redundancy in task vectors. It proposes purifying task vectors
  in knowledge-aware subspaces (PAVE) by decomposing fine-tuned weights using context-oriented
  singular value decomposition (CO-SVD) on covariance matrices derived from task-specific
  activations.
---

# Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging

## Quick Facts
- arXiv ID: 2510.14697
- Source URL: https://arxiv.org/abs/2510.14697
- Authors: Bang An; Yibo Yang; Philip Torr; Bernard Ghanem
- Reference count: 30
- One-line primary result: PAVE boosts EMR-Merging from 80.18% to 84.28% accuracy on GLUE, approaching individual model performance.

## Executive Summary
This paper addresses performance degradation in model merging caused by task-irrelevant redundancy in task vectors. It proposes purifying task vectors in knowledge-aware subspaces (PAVE) by decomposing fine-tuned weights using context-oriented singular value decomposition (CO-SVD) on covariance matrices derived from task-specific activations. This identifies task-relevant components and removes redundant ones. A spectral rank allocation strategy further optimizes pruning across models. PAVE serves as a plug-and-play method compatible with various merging approaches.

## Method Summary
PAVE decomposes model weights in a subspace defined by input activations rather than in isolation. For each task, it samples training data, feeds it through the fine-tuned model to compute activation covariance matrices per layer, and performs SVD on the product of fine-tuned weights and covariance. This accentuates task-relevant weight components. The method then truncates to the top r singular values, reconstructs the purified weights, and defines the task vector as the difference from the base model. A greedy spectral rank allocation strategy under a global rank budget optimizes which components to keep per layer. The purified vectors are merged using standard methods like Task Arithmetic or EMR-Merging.

## Key Results
- PAVE improves EMR-Merging average accuracy from 80.18% to 84.28% on GLUE with RoBERTa, approaching individual model performance (85.55%).
- Similar gains observed with DeBERTa on GLUE, LLaMA-2 on generative tasks (GSM8K, MATH, HumanEval, MBPP), and Vision Transformer models.
- PAVE consistently outperforms random masking methods like DARE while maintaining stability across different merging approaches.

## Why This Works (Mechanism)

### Mechanism 1: Context-Oriented Subspace Alignment
- Claim: Decomposing model weights in a subspace defined by input activations isolates task-critical parameters better than decomposing weights in isolation.
- Mechanism: Standard SVD looks at the weight matrix W. PAVE computes the covariance matrix C of input activations for a specific task and performs SVD on the product WC. This rotation of the basis aligns the principal components with the directions that actually trigger high activation for that task, causing task-relevant features to concentrate in the largest singular values.
- Core assumption: Task-specific knowledge is encoded in a low-rank subspace that is geometrically aligned with the activation covariance of the task's input data.
- Evidence anchors:
  - [abstract]: "...perform a context-oriented singular value decomposition, which accentuates the weight components most relevant to the target knowledge."
  - [Figure 3]: Shows "CO-SVD with SST-2" retains high performance at aggressive pruning levels (512 pruned ranks) while "SVD" and "CO-SVD random" degrade sharply.
  - [corpus]: The paper "AdaRank" similarly exploits low-rank structures for merging, validating the general premise of structure-aware merging, though PAVE specifically uses covariance for "knowledge-awareness."
- Break condition: If the input samples used to compute C are from the wrong distribution (e.g., using CoLA data for an MRPC model), the subspace alignment fails and performance drops, as evidenced by the "CO-SVD with CoLA" line in Figure 3.

### Mechanism 2: Spectral Truncation as Noise Filtering
- Claim: Truncating singular values in the knowledge-aware subspace removes "task-irrelevant redundancy" that causes interference during merging.
- Mechanism: By keeping only the top r singular values in the WC decomposition and reconstructing the weight matrix, PAVE effectively filters out dimensions that have low variance on the task data. When these purified vectors are merged, the "cross-talk" or interference between unrelated tasks is reduced.
- Core assumption: The singular values with the smallest magnitudes in the WC spectrum correspond to noise or features irrelevant to the specific task, rather than rare but critical edge-case features.
- Evidence anchors:
  - [abstract]: "...filter out the redundant and noisy directions by retaining only the largest r singular values..."
  - [Table 1]: PAVE improves EMR-Merging average accuracy from 80.18% to 84.28%, suggesting the removed components were indeed net-negative or redundant for the merge.
  - [corpus]: "CABS" addresses conflicts via sparsification, but PAVE approaches this via spectral filtering, implying conflict resides in the spectral tails.
- Break condition: If the rank budget r is set too low (over-pruning), valid task features are discarded.

### Mechanism 3: Greedy Spectral Rank Allocation
- Claim: Uniform rank allocation across different models is suboptimal; allocating rank based on a normalized error metric ensures fairer preservation of diverse task capabilities.
- Mechanism: The algorithm optimizes the "normalized activated pruning error" (reconstruction error scaled by the max singular value). It iteratively removes the component with the smallest normalized impact across all layers and models. This prevents a task with naturally large singular values from hoarding the rank budget.
- Core assumption: The Frobenius norm of the reconstruction error in the activation space is a reliable proxy for the "value" of a parameter to the model's performance.
- Evidence anchors:
  - [Section 3.3]: "...uniform pruning represents a naive and suboptimal approach, while our rank allocation strategy enables better performance..."
  - [Table 4]: PAVE with spectral allocation outperforms "PAVE plain" (fixed ratio) in all settings (e.g., 82.64% vs 81.82% for EMR-Merging).
- Break condition: If the global rank ratio ρ is too restrictive, the greedy algorithm may strip essential capacities from sensitive tasks to satisfy the global constraint.

## Foundational Learning

- Concept: **Covariance Matrix (C=XX^T)**
  - Why needed here: This is the core "knowledge-aware" signal. You must understand that this matrix captures the variance of input activations to understand *why* PAVE rotates the weight matrix before decomposition.
  - Quick check question: If inputs are all zeros, what happens to C and the subsequent SVD?

- Concept: **Task Vectors (ΔW = W_FT - W_B)**
  - Why needed here: PAVE does not merge weights directly; it purifies these difference vectors. Understanding this abstraction is necessary to see PAVE as a "plug-and-play" module compatible with methods like Task Arithmetic.
  - Quick check question: Does PAVE modify the base model W_B or the fine-tuned model W_FT before computing the task vector?

- Concept: **Singular Value Decomposition (SVD)**
  - Why needed here: The method relies on SVD to separate "signal" (large singular values) from "noise" (small singular values).
  - Quick check question: In the PAVE formulation SVD_r(W_FT·C), what happens if you set r to the full rank of the matrix?

## Architecture Onboarding

- Component map: Data Sampler -> Covariance Calculator -> CO-SVD Engine -> Rank Optimizer -> Reconstructor -> Merger
- Critical path: Computing the inverse of C (C^-1). If C is singular or ill-conditioned (e.g., due to small sample size), the reconstruction becomes unstable. The paper notes adding small positive values to the diagonal to fix this.
- Design tradeoffs:
  - Sample Size vs. Speed: Figure 4 shows 256-512 samples offer a sweet spot; 4096 ensures stability but increases runtime.
  - Global vs. Layer-wise Allocation: The method optimizes rank allocation layer-by-layer but under a global parameter count constraint ρ.
- Failure signatures:
  - Performance Collapse: If ρ (preserved rank ratio) is too aggressive (< 50%), merged performance may crater.
  - Task Starvation: If one task has very large singular values, the normalizer in Algo 1 is critical; without it, that task might dominate the rank budget.
- First 3 experiments:
  1. Ablation on Decomposition: Replicate Figure 3 on a single task (e.g., MRPC) comparing Standard SVD vs. CO-SVD to validate the "knowledge-aware" benefit locally.
  2. Merge Stability Test: Merge 2 conflicting tasks (e.g., Math and Code) using PAVE vs. DARE (Table 3) to verify PAVE's stability over random dropping.
  3. Rank Sensitivity: Run the Spectral Rank Allocation with varying global budgets (ρ) to find the knee-point where accuracy drops, verifying the "stopping ratio" γ settings.

## Open Questions the Paper Calls Out
- **Can covariance matrices be estimated using synthetic data?** The method requires sampling training examples to derive covariance matrices, which may not be available in privacy-constrained settings. The paper doesn't test data-free or synthetic-data scenarios.
- **Is the greedy rank allocation globally optimal?** The paper states the greedy approach "can be effectively approached" but doesn't prove global optimality. A comparative study against exact solvers would clarify this.
- **How does PAVE handle highly overlapping tasks?** The paper demonstrates success on diverse tasks but doesn't analyze scenarios where "task-relevant components" might be entangled or contradictory. High interference between similar tasks could degrade the spectral decomposition.

## Limitations
- The method assumes task-relevant directions are separable in the activation space, which may not hold for highly similar or conflicting tasks.
- Effectiveness across diverse architectures beyond transformers remains untested.
- The exact mechanism by which CO-SVD isolates task-relevant features needs deeper empirical validation beyond correlation with performance.

## Confidence
- **High Confidence**: PAVE's empirical improvements over baselines across GLUE, generative, and vision tasks.
- **Medium Confidence**: The theoretical justification for why context-oriented SVD better isolates task-relevant features.
- **Medium Confidence**: The greedy spectral rank allocation strategy's optimality.

## Next Checks
1. **Mechanism Dissection**: For a single task (e.g., MRPC), visualize the singular value spectra from both standard SVD and CO-SVD to empirically demonstrate how context-orientation shifts the distribution of task-relevant components.
2. **Distribution Sensitivity**: Systematically vary the sample size and input distribution used to compute covariance matrices (e.g., using CoLA data for an MRPC model) to quantify how sensitive PAVE is to the "knowledge-aware" alignment condition.
3. **Conflict Analysis**: Take two highly conflicting tasks (e.g., Math and Code generation) and analyze the specific components that PAVE prunes versus what DARE masks randomly. This would validate whether PAVE is indeed targeting "task-irrelevant" directions more effectively than random dropping.