---
ver: rpa2
title: When LLM meets Fuzzy-TOPSIS for Personnel Selection through Automated Profile
  Analysis
arxiv_id: '2601.22433'
source_url: https://arxiv.org/abs/2601.22433
tags:
- fuzzy
- loss
- performance
- training
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents an automated personnel selection system that
  integrates large language models (LLMs) with fuzzy TOPSIS decision-making to evaluate
  software engineering candidates. A dataset of LinkedIn profiles was constructed
  and annotated by experts, covering experience, skills, education, and self-introductions.
---

# When LLM meets Fuzzy-TOPSIS for Personnel Selection through Automated Profile Analysis

## Quick Facts
- arXiv ID: 2601.22433
- Source URL: https://arxiv.org/abs/2601.22433
- Reference count: 23
- Primary result: Automated LinkedIn profile analysis system achieves 91% classification accuracy and 0.98 cosine similarity to expert rankings using DistilRoBERTa + Fuzzy-TOPSIS

## Executive Summary
This study presents an automated personnel selection system that integrates large language models with fuzzy TOPSIS decision-making to evaluate software engineering candidates. The framework combines DistilRoBERTa-based multi-class classification of LinkedIn profiles with fuzzy TOPSIS ranking to handle uncertainty in human evaluations. Using a dataset of 100 LinkedIn profiles augmented to 10,000 samples, the system achieves high accuracy in classifying candidates across four attributes (experience, skills, education, self-introduction) and demonstrates strong alignment with expert rankings. The approach addresses scalability and consistency challenges in recruitment while providing a systematic framework for candidate evaluation.

## Method Summary
The system employs DistilRoBERTa-base fine-tuned for multi-class classification of LinkedIn profiles into Poor/Fair/Excellent categories across four attributes. Profiles are annotated on a 5-point Likert scale and mapped to three classes, then augmented through synonym substitution, paraphrasing, and contextual expansion. The fuzzy TOPSIS method ranks candidates using triangular fuzzy numbers to model uncertainty in expert evaluations, with attribute weights set at Skills (0.60), Experience (0.20), Education (0.15), and About (0.05). The framework achieves high classification accuracy and strong ranking alignment with human experts through this integrated approach.

## Key Results
- DistilRoBERTa achieves up to 91% accuracy in classifying LinkedIn profiles across four attributes
- Fuzzy-TOPSIS ranking demonstrates cosine similarity of 0.98 with expert rankings
- System maintains high performance metrics (MAP 0.99, NDCG 0.911) across evaluation metrics
- The framework successfully handles class imbalance through augmentation and achieves strong alignment between automated and human assessments

## Why This Works (Mechanism)
The integration of LLMs with fuzzy TOPSIS leverages complementary strengths: DistilRoBERTa excels at understanding nuanced textual patterns in professional profiles, while fuzzy TOPSIS provides a systematic framework for handling uncertainty in human evaluations. The multi-class classification approach enables granular assessment of candidate attributes, and the fuzzy methodology captures the inherent ambiguity in recruitment decisions. The combination addresses both the pattern recognition capabilities of modern NLP models and the decision-making requirements of personnel selection.

## Foundational Learning

**DistilRoBERTa-base architecture**: Understanding the model's transformer-based design is essential for grasping how it processes profile text and generates classification probabilities. Quick check: Verify model parameter count (82M) and attention mechanism through HuggingFace documentation.

**Fuzzy TOPSIS methodology**: Knowledge of fuzzy set theory and TOPSIS ranking principles is needed to understand how uncertainty in expert evaluations is modeled. Quick check: Confirm triangular fuzzy number conversions align with the linguistic terms table provided.

**Multi-class classification mapping**: Understanding the 5-point to 3-class transformation is crucial for interpreting results and reproducing the classification framework. Quick check: Validate class distribution after mapping and augmentation.

**Profile attribute weighting**: The specified weights (Skills 0.60, Experience 0.20, Education 0.15, About 0.05) determine the relative importance in final rankings. Quick check: Verify weight distribution sums to 1.0 and aligns with recruitment priorities.

## Architecture Onboarding

**Component map**: LinkedIn profiles -> DistilRoBERTa classifier (4 separate models) -> Fuzzy TOPSIS ranking engine -> Final candidate rankings

**Critical path**: Profile preprocessing -> Attribute classification -> Fuzzy number conversion -> Closeness coefficient calculation -> Candidate ranking

**Design tradeoffs**: The system prioritizes accuracy over interpretability by using a complex LLM classifier, but this is balanced by the transparent fuzzy TOPSIS ranking process. The choice of DistilRoBERTa over larger models optimizes for efficiency while maintaining performance.

**Failure signatures**: Classification accuracy drops when profiles contain domain-specific jargon outside software engineering; ranking misalignment occurs if attribute weights don't reflect actual recruitment priorities; system performance degrades with insufficient training data or poor annotation quality.

**3 first experiments**:
1. Test DistilRoBERTa classification accuracy on a small subset of manually annotated profiles before full dataset training
2. Validate fuzzy TOPSIS ranking consistency using synthetic candidate profiles with known attribute values
3. Compare model predictions against expert rankings on a held-out validation set to establish baseline alignment

## Open Questions the Paper Calls Out

**Cross-domain applicability**: Can the framework maintain high alignment with expert rankings when applied to diverse professional domains outside of software engineering? The current study is limited to software engineering profiles, and future work involves expanding to healthcare, finance, and education domains for cross-validation.

**Data-driven weight optimization**: How does data-driven weight optimization compare to expert-defined weights in the fuzzy TOPSIS aggregation process? The paper suggests refining weight learning using entropy optimization or adaptive fuzzy-AHP formulations, but current methodology relies on static expert-derived weights.

**Explainability framework integration**: Do explainability frameworks like SHAP or LIME effectively clarify the relationship between specific textual features and the model's proficiency scores? The authors plan to employ these frameworks to visualize LLM attention and illustrate textual segment contributions, addressing the current "black box" limitation.

## Limitations

- Original LinkedIn dataset not publicly available, preventing direct replication
- Fuzzy TOPSIS implementation details beyond linguistic terms table are unspecified
- Limited testing to software engineering domain raises questions about generalization
- No evaluation of model fairness or bias mitigation strategies

## Confidence

- **Classification accuracy claims (91%)**: High confidence - Well-specified methodology and clear evaluation metrics
- **Ranking performance metrics (Cosine Similarity 0.98)**: Medium confidence - Approach detailed but fuzzy TOPSIS implementation not fully specified
- **System applicability and scalability**: Low confidence - Limited testing beyond specific dataset and domain

## Next Checks

1. Implement fuzzy TOPSIS ranking using exact weight distribution and compare against multiple human expert rankings to validate consistency
2. Conduct ablation studies by removing each attribute from TOPSIS process to quantify individual impact on rankings
3. Test model robustness on out-of-domain profiles to assess generalization beyond software engineering dataset