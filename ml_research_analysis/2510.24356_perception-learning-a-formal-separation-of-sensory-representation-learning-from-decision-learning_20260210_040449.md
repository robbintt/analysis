---
ver: rpa2
title: 'Perception Learning: A Formal Separation of Sensory Representation Learning
  from Decision Learning'
arxiv_id: '2510.24356'
source_url: https://arxiv.org/abs/2510.24356
tags:
- learning
- perception
- perceptual
- decision
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Perception Learning (PeL) addresses the problem of optimizing\
  \ sensory representations independently from downstream decision tasks. The core\
  \ method introduces a formal separation where the sensory encoder f\u03C6: X \u2192\
  \ Z is trained using task-agnostic signals (invariance, contrastive information,\
  \ diversity) without decision loss backpropagation."
---

# Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning

## Quick Facts
- **arXiv ID**: 2510.24356
- **Source URL**: https://arxiv.org/abs/2510.24356
- **Reference count**: 28
- **Primary result**: Perception Learning (PeL) formally separates sensory representation learning from decision learning, enabling task-agnostic perceptual optimization with theoretical guarantees on Bayes risk preservation.

## Executive Summary
Perception Learning (PeL) introduces a formal framework for optimizing sensory representations independently from downstream decision tasks. The core innovation is training an encoder f_φ: X → Z using only task-agnostic signals (invariance, contrastive information, diversity) without decision loss backpropagation. This separation enables reusable, robust perceptual codes that can be evaluated independently of downstream accuracy, facilitating modular training and clearer diagnostics in AI systems. The work provides theoretical guarantees that under task-true group invariance assumptions, PeL updates preserve Bayes task risk, justifying the decoupling.

## Method Summary
PeL trains a sensory encoder f_φ on unlabeled data using a perception loss combining invariance terms (minimizing representation changes under nuisance transformations), information-theoretic discriminability (contrastive learning objectives), and geometric diversity (variance floors and covariance penalties). The encoder is frozen and transferred to downstream tasks where decision heads g_θ are trained separately. Task-agnostic metrics including invariance curves, leakage probes, and Fisher information are used to evaluate perceptual quality independently of task performance.

## Key Results
- PeL updates preserving sufficient invariants are orthogonal to Bayes task-risk gradients under group invariance assumptions
- Perceptual quality can be measured using task-agnostic metrics without requiring downstream labels
- Modular two-stage training achieves Bayes-optimal performance when invariances are correctly specified
- Theoretical framework enables evaluation of perceptual representations independent of downstream tasks

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Gradient Separation
When the encoder factors through the sufficient invariant statistic T (f_φ = h∘T with h injective on range(T)), the σ-algebra generated by Z equals that of T, so P(Y|Z) = P(Y|T) almost surely. Tangent directions v that keep f_φ₀₊ₜᵥ = h_t∘T with h_t injective preserve the posterior, yielding D_vF(φ₀) = 0. This holds when task labels depend only on G-orbits (A1) and the orbit statistic T is sufficient (A2). The break condition occurs when A1 fails (labels not G-invariant) or h becomes non-injective (distinct orbits merged).

### Mechanism 2: Representation-Invariant Perceptual Functionals
Perceptual properties are defined as measurable functionals Φ_P(f_φ; P_X, G) ∈ ℝ with target sets T_P ⊆ ℝ, depending only on (X, G, f_φ) not task labels. Information-theoretic terms (I(X;Z), I(Z;V)) are fully invariant under injective h since σ-algebras are preserved; geometric terms require canonical metrics. The core assumption is that task-agnostic properties correlate with downstream utility across a target task distribution.

### Mechanism 3: Two-Stage Modular Training
Figure 2 architecture trains f_φ on unlabeled multi-modal X using L_perc = invariance + contrastive + diversity signals. Decision heads g_θ train later on frozen Z. Corollary 1 shows if f_φ* = h*∘T with h* injective, inf_θ R(φ*, θ) achieves Bayes risk. This requires correctly identifying the invariance group G for the intended task distribution.

## Foundational Learning

- **Group Actions and Orbit Quotient Spaces**: Why needed here: The theory relies on G-orbits; T = π(X) maps to quotient X/G; sufficiency requires Y⊥⊥X|T(X). Quick check: For 2D rotations G=SO(2) on images, what is the orbit of a single image and what information does T discard?

- **Strictly Proper Scoring Rules and Bayes Risk**: Why needed here: F(φ) = inf_θ R(φ,θ) = E[L_ℓ(P(Y|Z_φ))] holds for proper losses; enables envelope theorem application. Quick check: Why does log-loss make the Bayes act g*_θ(z) = P(Y|Z=z)?

- **σ-algebra Inclusion and Conditional Independence**: Why needed here: Core proof uses σ(Z_φ) = σ(T) ⇒ P(Y|Z_φ) = P(Y|T); injective h preserves σ-algebras, non-injective h coarsens them. Quick check: If Z = h(T) with h collapsing two T-values, what happens to H(Y|Z) vs H(Y|T)?

## Architecture Onboarding

- **Component map**: Encoder f_φ: X → Z (trained with L_perc) -> Decision heads g_θ: Z → Δ(Y) (trained separately) -> World model p_ψ: Dynamics prediction over Z (optional)

- **Critical path**: Identify invariance group G (nuisances vs causal factors) → Train f_φ with L_perc → Verify Z preserves orbit identity (injectivity check) → Freeze f_φ → Train task heads g_θ

- **Design tradeoffs**: Stronger invariance (lower ε) vs. preserving discriminability (injectivity on T); augmentation diversity vs. over-invariance risk; Z dimensionality vs. geometric regularity

- **Failure signatures**: Constant Z codes → Collapse; add variance floor Φ_var, strengthen contrastive term; High leakage Ĩ(Z;V)/H(V) > threshold → Add adversarial probe penalty; Downstream accuracy drops after PeL → Check for over-invariance

- **First 3 experiments**: 1) Invariance curve validation: Compute D(α) = E[||f_φ(x) − f_φ(T_α x)||²] across augmentation strengths; 2) Over-invariance stress test: Implement "6 vs 9" rotation scenario; 3) Transfer probe efficiency: Train f_φ on unlabeled data, freeze, measure linear probe accuracy vs. label count

## Open Questions the Paper Calls Out

### Open Question 1
How can task-true invariance groups G be discovered from data without access to task labels, given that mis-specified invariances strictly increase Bayes risk? The theory assumes A1 (group invariance of the target) holds, but provides no mechanism to verify or discover G from unlabeled data alone. What's needed is an algorithm that identifies nuisance variables versus causal factors from distributional structure with empirical validation showing automatic G discovery preserves Bayes risk across held-out tasks.

### Open Question 2
How can the sufficiency condition σ(Z)=σ(T) be verified empirically with finite samples to ensure perception updates remain orthogonal to task gradients? Theorem 1 requires f_ϕ factors through T with injective h; no finite-sample test is proposed to certify that Z captures exactly the orbit information without merging distinct orbits. What's needed is a statistical test for sufficiency with finite-sample guarantees and empirical validation that representations passing such tests maintain orthogonal gradient updates.

### Open Question 3
How does approximate (rather than exact) G-invariance affect the orthogonality guarantee between perception and decision gradients? Theorem 1 assumes L_inv minima achieve exact G-invariance; practical SSL objectives like InfoNCE only achieve approximate invariance. What's needed are bounds on ∇F·∇L_inv as a function of residual invariance error ε=E[||f(x)-f(g·x)||²] with experiments measuring gradient alignment across invariance quality levels.

## Limitations
- The theoretical framework relies heavily on assumptions A1 and A2 which may not hold in real-world datasets where nuisance factors correlate with or cause task labels
- Minimal empirical validation is provided; no specific datasets, architectures, or quantitative results support the theoretical claims
- The choice of invariance group G and weighting of perceptual loss terms is left to the practitioner without clear guidance or automated selection criteria

## Confidence
- **High confidence**: The theoretical framework for orthogonal gradient separation under stated assumptions is internally consistent and mathematically sound
- **Medium confidence**: The formal separation of perceptual and decision learning is conceptually valuable, but practical applicability depends on correctly identifying task-relevant invariances
- **Low confidence**: The claim that PeL produces reusable codes without downstream performance degradation lacks empirical backing in the provided sections

## Next Checks
1. Conduct systematic ablation studies varying augmentation groups G to quantify the impact of over-invariance on Bayes risk across multiple downstream tasks
2. Implement the proposed task-agnostic metrics (invariance curves, leakage probes, Fisher information) and validate their correlation with downstream performance across diverse datasets
3. Design a controlled experiment where task labels depend on a mix of orbit-invariant and orbit-variant features to test the robustness of PeL when assumptions A1-A2 are partially violated