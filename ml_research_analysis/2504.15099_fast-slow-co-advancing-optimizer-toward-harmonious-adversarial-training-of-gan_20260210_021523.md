---
ver: rpa2
title: 'Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of
  GAN'
arxiv_id: '2504.15099'
source_url: https://arxiv.org/abs/2504.15099
tags:
- discriminator
- learning
- step
- training
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fast-Slow Co-advancing Optimizer (FSCO),
  a novel reinforcement learning-based optimizer for GAN training that addresses the
  sensitivity to hyperparameters and training instability. FSCO employs a DDPG agent
  to dynamically control the discriminator's learning rate based on the loss difference
  between generator and discriminator, creating a more harmonious adversarial training
  process.
---

# Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN

## Quick Facts
- **arXiv ID**: 2504.15099
- **Source URL**: https://arxiv.org/abs/2504.15099
- **Reference count**: 6
- **Primary result**: FSCO uses DDPG agent to dynamically control discriminator learning rate based on loss differences, stabilizing GAN training

## Executive Summary
This paper introduces the Fast-Slow Co-advancing Optimizer (FSCO), a reinforcement learning-based approach to address the sensitivity of GAN training to hyperparameters and training instability. FSCO employs a DDPG agent that monitors the loss difference between generator and discriminator to dynamically adjust the discriminator's learning rate, creating a more harmonious adversarial training process. The method transforms traditional GAN training from fixed-step hyperparameter tuning to a feedback-controlled process.

The authors validate FSCO across three datasets (MNIST, ANIME, and Ganyu) with varying resolutions, demonstrating its effectiveness in stabilizing training and expanding the hyperparameter range for discriminator-generator coordination. While FSCO cannot fundamentally enhance network capabilities and requires careful selection of the discriminator's base learning rate, it significantly reduces hyperparameter tuning time and provides a new approach to GAN training that produces reasonable image quality despite some overfitting.

## Method Summary
FSCO introduces a novel reinforcement learning-based optimizer that replaces traditional fixed learning rate schedules with a dynamic control mechanism. The core innovation is a DDPG (Deep Deterministic Policy Gradient) agent that continuously monitors the loss difference between the generator and discriminator during training. Based on this feedback, the agent adjusts the discriminator's learning rate in real-time, allowing for faster adaptation when the discriminator is lagging and slower updates when it's advancing too quickly. This creates a co-advancing mechanism where both networks progress in a more balanced manner, reducing the common instability issues in GAN training such as mode collapse and training oscillations.

## Key Results
- FSCO successfully stabilizes GAN training across MNIST (28×28), ANIME (64×64), and Ganyu (128×128) datasets
- The method significantly expands the hyperparameter range for discriminator-generator coordination, reducing sensitivity to initial learning rate choices
- FSCO reduces hyperparameter tuning time by transforming the training process into a feedback-controlled system rather than fixed-step tuning
- Generated images show reasonable quality across all tested resolutions, though some overfitting is observed

## Why This Works (Mechanism)
FSCO works by addressing the fundamental imbalance in GAN training where the discriminator and generator can advance at different rates, leading to training instability. The DDPG agent acts as a controller that continuously measures the loss difference between the two networks and adjusts the discriminator's learning rate accordingly. When the discriminator's loss is significantly lower than the generator's, indicating it's too strong, the agent reduces the learning rate to slow its progress. Conversely, when the generator is lagging, the agent increases the learning rate to help the discriminator catch up. This dynamic adjustment creates a more harmonious adversarial process where both networks advance at complementary rates, reducing the oscillations and instabilities common in traditional GAN training.

## Foundational Learning

**Deep Deterministic Policy Gradient (DDPG)**: A model-free, off-policy reinforcement learning algorithm for continuous action spaces. Why needed: DDPG provides the continuous control mechanism required to adjust learning rates smoothly during training. Quick check: Can be implemented using actor-critic architecture with target networks for stability.

**GAN Training Dynamics**: Understanding the adversarial relationship between generator and discriminator. Why needed: Essential for recognizing why fixed learning rates lead to instability. Quick check: Monitor loss curves for both networks during standard training to observe typical instability patterns.

**Loss Difference as Stability Metric**: Using the relative performance gap between networks as a training signal. Why needed: Provides a quantifiable measure of training balance that the RL agent can optimize. Quick check: Calculate |loss_D - loss_G| during training and observe correlation with stability.

## Architecture Onboarding

**Component Map**: Data -> Generator -> Discriminator -> Loss Calculator -> DDPG Agent -> Learning Rate Controller -> Discriminator

**Critical Path**: The core training loop involves forward passes through both networks, loss calculation, DDPG agent evaluation of loss difference, learning rate adjustment, and backpropagation with the updated learning rate.

**Design Tradeoffs**: FSCO trades computational overhead (running DDPG agent) for training stability and reduced hyperparameter tuning. The method requires careful selection of the discriminator's base learning rate and may not fundamentally improve network capacity.

**Failure Signatures**: If the DDPG agent poorly tunes learning rates, training may still diverge. Over-aggressive learning rate adjustments can cause oscillations, while under-adjustment may not provide sufficient stability benefits.

**First Experiments**:
1. Compare loss curves with and without FSCO on MNIST to observe stability improvements
2. Test FSCO with different base learning rates to find optimal range
3. Evaluate image quality metrics (FID, IS) across datasets to assess generation quality

## Open Questions the Paper Calls Out
The paper acknowledges that FSCO cannot fundamentally enhance network capabilities, suggesting performance gains are limited to training stability rather than quality improvements. The effectiveness of FSCO across diverse GAN architectures (e.g., StyleGAN, BigGAN) remains unverified. Additionally, while the method claims to reduce hyperparameter tuning time, the computational overhead of the DDPG agent during training is not quantified, potentially offsetting efficiency gains in resource-constrained settings.

## Limitations
- Limited scope of tested architectures, focusing primarily on DCGAN variants
- Cannot fundamentally enhance network capabilities, only improves training stability
- Effectiveness across diverse GAN architectures (StyleGAN, BigGAN) remains unverified
- Computational overhead of DDPG agent during training not quantified

## Confidence

**High confidence**: FSCO successfully stabilizes GAN training and reduces sensitivity to hyperparameter tuning, as demonstrated across three datasets with consistent results.

**Medium confidence**: FSCO expands the hyperparameter range for discriminator-generator coordination, though the extent of this expansion varies by dataset resolution and requires further validation.

**Low confidence**: Claims about reducing hyperparameter tuning time are supported qualitatively but lack quantitative comparison to baseline tuning efforts.

## Next Checks
1. Test FSCO on modern GAN architectures (e.g., StyleGAN2, BigGAN) to evaluate generalizability beyond DCGAN variants.
2. Quantify the computational overhead of the DDPG agent during training and compare wall-clock times against standard hyperparameter-tuned baselines.
3. Conduct ablation studies to isolate the impact of FSCO's loss-difference metric versus alternative stability metrics (e.g., gradient penalties, spectral normalization).