---
ver: rpa2
title: 'Models Know Models Best: Evaluation via Model-Preferred Formats'
arxiv_id: '2601.22699'
source_url: https://arxiv.org/abs/2601.22699
tags:
- cloze
- symbol
- evaluation
- shot
- formats
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study reveals that the performance of Large Language Models
  (LLMs) in multiple-choice question answering (MCQA) varies significantly between
  symbol-based and cloze-style evaluation formats, with the discrepancies systematically
  linked to task characteristics. Natural language continuation tasks favor cloze-style
  formats, while explicit comparison tasks benefit from symbol-based formats.
---

# Models Know Models Best: Evaluation via Model-Preferred Formats

## Quick Facts
- **arXiv ID**: 2601.22699
- **Source URL**: https://arxiv.org/abs/2601.22699
- **Reference count**: 40
- **Primary result**: Dynamic format-alignment strategy improves zero-shot MCQA accuracy by selecting symbol vs. cloze evaluation formats per instance using a lightweight classifier trained on model preference signals.

## Executive Summary
This study demonstrates that Large Language Models exhibit systematic performance variations between symbol-based and cloze-style evaluation formats for multiple-choice question answering, with preferences depending on task characteristics. Natural language continuation tasks favor cloze-style formats while explicit comparison tasks benefit from symbol-based formats. To address these inconsistencies, the authors introduce a dynamic format-alignment strategy that uses a lightweight classifier trained on model-preference signals to select the optimal evaluation format for each problem instance. This approach achieves substantial and consistent improvements in zero-shot accuracy across reasoning and knowledge benchmarks by bridging the gap between model capability and task characteristics.

## Method Summary
The method employs a dynamic format-alignment strategy that selects between symbol-based and cloze-style evaluation formats for each MCQA instance. The process begins with generating training labels using Algorithm 1, which evaluates each instance with the target model in both formats and assigns labels based on a margin threshold (δ=0.2) comparing correctness and confidence. A DeBERTaV3 classifier is then fine-tuned on these labels to predict the preferred format. For symbol-based evaluation, the model computes likelihood scores over [" A", " B", " C", " D"] prompts. For cloze-style, it computes length-normalized log-likelihoods over full answer strings. The trained classifier dynamically selects the optimal format during inference, achieving improved zero-shot accuracy compared to fixed-format baselines.

## Key Results
- Performance varies systematically between symbol-based and cloze formats depending on task characteristics
- Dynamic format-alignment strategy achieves substantial improvements in zero-shot accuracy
- Model-generated signals outperform human-designed heuristics for format selection
- Method demonstrates generalizability across diverse model families including Llama, Mistral, and Qwen

## Why This Works (Mechanism)
The effectiveness stems from aligning evaluation formats with the inherent inductive biases of language models. Decoder-only models, trained on natural language continuation patterns, show stronger performance on cloze-style formats for tasks involving reasoning and inference. Symbol-based formats better suit tasks requiring explicit comparison and discrete selection. The dynamic alignment strategy bridges the mismatch between model architecture and task format by leveraging the model's own preference signals, effectively adapting evaluation to the model's learned representations.

## Foundational Learning
- **Decoder-only vs encoder-decoder architectures**: Why needed - Understanding architectural differences explains format preferences. Quick check - Verify if bidirectional models show reduced format sensitivity.
- **Autoregressive training paradigm**: Why needed - Explains why models prefer natural language continuation formats. Quick check - Compare format preferences across models with different training objectives.
- **Likelihood scoring vs. generation sampling**: Why needed - Implementation choice affects format evaluation reliability. Quick check - Test deterministic vs. stochastic scoring for symbol format.
- **Zero-shot evaluation methodology**: Why needed - Ensures fair comparison across different format selection strategies. Quick check - Validate benchmark splits and evaluation consistency.
- **Model preference signal extraction**: Why needed - Core mechanism for training format classifier. Quick check - Verify margin threshold δ=0.2 produces meaningful preference distinctions.
- **Cross-format performance gap analysis**: Why needed - Identifies when dynamic alignment provides the most benefit. Quick check - Confirm baseline gaps match reported values before testing alignment strategy.

## Architecture Onboarding

**Component Map**
Question Text + Answer Options -> Format Classifier (DeBERTaV3) -> Selected Format -> Evaluation Model (LLM) -> Answer Score

**Critical Path**
Model evaluation in both formats -> Label generation via Algorithm 1 -> Classifier training -> Format selection -> Improved accuracy

**Design Tradeoffs**
- Model-generated labels vs. human heuristics: Model signals capture nuanced preferences but require target model access
- Fixed vs. dynamic format selection: Dynamic alignment improves accuracy but adds computational overhead
- Classifier architecture choice: DeBERTaV3 balances performance with efficiency for format prediction

**Failure Signatures**
- Performance degradation when using human-annotated classifier labels
- Limited improvement on symbol-favoring tasks where baseline performance is already high
- Format selection errors concentrated in instances with small confidence margins

**3 First Experiments**
1. Implement and validate both evaluation formats on 1-2 benchmarks to confirm baseline performance differences
2. Generate training labels using Algorithm 1 with target model and verify abstention rate and data sufficiency
3. Fine-tune DeBERTaV3 classifier and compare dynamic selection performance against fixed-format baselines

## Open Questions the Paper Calls Out
- Do encoder-decoder architectures exhibit similar systematic format preferences as decoder-only models?
- To what extent are format preferences caused by pre-training data distributions versus architectural inductive biases?
- Does exclusion of "abstained" instances during classifier training introduce systematic bias?

## Limitations
- Incomplete specification of DeBERTaV3 variant and training hyperparameters
- Unclear inference settings for symbol-based format evaluation
- Generalization claims based on limited model family testing

## Confidence
- **High**: Systematic performance variations between evaluation formats based on task characteristics
- **Medium**: Effectiveness of dynamic format-alignment strategy in bridging performance gaps
- **Low**: Generalizability to diverse model families and superiority of model-generated signals

## Next Checks
1. Implement both evaluation formats with precise inference settings and validate against at least two benchmarks
2. Generate training labels using Algorithm 1 and measure abstention rate to assess data sufficiency
3. Fine-tune DeBERTaV3 classifier and evaluate performance gains against fixed-format baselines, particularly on cloze-favoring tasks