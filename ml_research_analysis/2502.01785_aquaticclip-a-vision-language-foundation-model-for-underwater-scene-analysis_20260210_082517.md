---
ver: rpa2
title: 'AquaticCLIP: A Vision-Language Foundation Model for Underwater Scene Analysis'
arxiv_id: '2502.01785'
source_url: https://arxiv.org/abs/2502.01785
tags:
- image
- underwater
- dataset
- marine
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AquaticCLIP introduces a novel vision-language model tailored for
  underwater scene analysis, addressing the challenge of limited aquatic data for
  vision-language pre-training. The model leverages a 2-million image-text paired
  dataset curated from diverse online sources, enhanced with unsupervised text generation
  using MarineGPT at both image and instance levels.
---

# AquaticCLIP: A Vision-Language Foundation Model for Underwater Scene Analysis

## Quick Facts
- **arXiv ID**: 2502.01785
- **Source URL**: https://arxiv.org/abs/2502.01785
- **Reference count**: 40
- **Primary result**: Novel vision-language model achieving state-of-the-art zero-shot and fine-tuned performance in underwater scene analysis, with up to 96.80% accuracy on coral classification.

## Executive Summary
AquaticCLIP introduces a vision-language foundation model specifically designed for underwater scene analysis, addressing the challenge of limited aquatic data for vision-language pre-training. The model leverages a 2-million image-text paired dataset curated from diverse online sources, enhanced with unsupervised text generation using MarineGPT at both image and instance levels. AquaticCLIP employs a prompt-guided vision encoder and a vision-guided text encoder to align visual and textual representations through contrastive pre-training. Extensive experiments show that AquaticCLIP outperforms existing state-of-the-art methods in zero-shot and fine-tuned tasks across marine species classification, fine-grained fish and coral classification, object detection, and counting.

## Method Summary
AquaticCLIP is a dual-encoder vision-language model built on the CLIP architecture, modified with prompt-guided mechanisms for both visual and textual representations. The model uses a 2-million image-text paired dataset curated from YouTube, Netflix, NatGeo, and textbooks, with captions enriched by MarineGPT-generated descriptions at both image and instance levels. The vision encoder incorporates 20 learnable prompts that aggregate patch features via cross-attention, while the text encoder is refined through vision-guided cross-attention. The model is pre-trained using contrastive loss for 80 epochs with a batch size of 512 on 4x A100 GPUs, with inputs resized to 512x512.

## Key Results
- Achieves 96.80% accuracy in zero-shot coral classification
- Achieves 93.40% accuracy in fine-grained fish classification
- Outperforms state-of-the-art methods in zero-shot and fine-tuned tasks across marine species classification, object detection, and counting

## Why This Works (Mechanism)
AquaticCLIP's performance stems from its specialized architecture for underwater scenes, which addresses the unique challenges of aquatic visual data through prompt-guided cross-attention mechanisms. The model's dual-encoder design with vision-guided text encoding and prompt-guided vision encoding creates a more robust alignment between visual features and textual descriptions specific to marine environments. The extensive 2-million image-text dataset, enriched with MarineGPT-generated captions, provides domain-specific supervision that standard web-scraped datasets lack.

## Foundational Learning
- **Vision-Language Pre-training**: Why needed - To learn joint visual-textual representations without requiring extensive labeled data; Quick check - Model should show good zero-shot performance on held-out underwater tasks
- **Cross-Modal Contrastive Learning**: Why needed - To align visual and textual embeddings in a shared embedding space; Quick check - Image-to-text and text-to-image retrieval should have similar accuracy
- **Prompt Learning**: Why needed - To inject domain-specific knowledge into the encoding process; Quick check - Adding/removing prompts should significantly impact performance
- **Unsupervised Caption Generation**: Why needed - To scale training data creation without manual annotation; Quick check - Generated captions should be semantically consistent with images
- **Multi-level Captioning**: Why needed - To capture both global scene context and local object details; Quick check - Instance-level captions should improve object detection performance
- **Data Cleaning for Synthetic Captions**: Why needed - To mitigate noise from automatic caption generation; Quick check - Cleaning should improve downstream task accuracy

## Architecture Onboarding

**Component Map**: Input Images → Prompt-Guided Vision Encoder (PGVE) → Visual Features → Contrastive Loss; Input Text → Vision-Guided Text Encoder (VGTE) → Text Features → Contrastive Loss

**Critical Path**: Data Curation → PGVE Implementation → VGTE Implementation → Pre-training with Contrastive Loss

**Design Tradeoffs**: 
- ViT-B/16 backbone provides good balance of accuracy and efficiency but is memory-intensive
- 512x512 resolution improves detail capture for small marine organisms but increases computational cost
- 20 learnable prompts add flexibility but increase parameter count and overfitting risk

**Failure Signatures**:
- VRAM OOM errors during training with batch size 512
- Poor zero-shot performance indicating overfitting to synthetic captions
- Disproportionate image-to-text vs text-to-image retrieval accuracy suggesting asymmetric encoding issues

**First Experiments**:
1. Train with reduced batch size (64) and gradient accumulation to verify training pipeline works
2. Evaluate zero-shot classification on a small validation set before full pre-training
3. Conduct ablation study removing PGVE or VGTE to assess individual contribution

## Open Questions the Paper Calls Out
1. **Performance Ceiling of Synthetic Captions**: Does AquaticCLIP's reliance on MarineGPT-generated synthetic captions impose a performance ceiling compared to human-annotated ground truth? The paper notes generated descriptions "may contain noise... broken sentences, incorrect descriptions" but does not evaluate against human-labeled gold standard subsets.

2. **Real-time Inference on Edge Devices**: Can AquaticCLIP be adapted for real-time inference on edge devices used in Autonomous Underwater Vehicles (AUVs) without significant accuracy loss? Current implementation uses A100 GPUs (0.80s inference time) which are impractical for power-constrained marine robotics.

3. **Robustness to Extreme Environments**: How robust is the model's zero-shot transfer to extreme aquatic environments (deep-sea or turbid waters) that are underrepresented in web-scraped datasets? Evaluation datasets may not capture long-tail distribution of challenging visual conditions found in scientific deep-sea exploration.

## Limitations
- **Dataset Access**: Missing functional data repository link; 2M pairs from proprietary sources (Netflix, NatGeo) are inaccessible
- **Architecture Complexity**: Dual-prompt mechanism adds parameters that could lead to overfitting; no ablation studies isolating component contributions
- **Evaluation Scope**: Focuses on classification/detection but lacks segmentation and complex reasoning tasks to fully demonstrate foundation model capabilities

## Confidence
- **High Confidence**: Core methodology and implementation details are clearly specified; reported performance metrics are internally consistent
- **Medium Confidence**: Performance claims relative to state-of-the-art are credible but cannot be independently verified without dataset access
- **Low Confidence**: "New benchmark" claim is difficult to validate given lack of comparable models and proprietary dataset

## Next Checks
1. **Implement Minimal Viable Dataset**: Create small-scale version using publicly available underwater images and standard VLM for caption generation to test core training pipeline
2. **Ablation Study on Prompt Components**: Train variants with only one prompt mechanism or reduced prompts (5 instead of 20) to isolate contributions
3. **Cross-Domain Generalization Test**: Evaluate pre-trained model on held-out dataset from different source to assess generalization beyond curated training distribution