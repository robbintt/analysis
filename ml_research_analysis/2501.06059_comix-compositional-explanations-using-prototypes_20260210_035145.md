---
ver: rpa2
title: 'COMIX: Compositional Explanations using Prototypes'
arxiv_id: '2501.06059'
source_url: https://arxiv.org/abs/2501.06059
tags:
- image
- b-cos
- training
- comix
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes COMIX, a novel compositional explanation method
  that addresses the challenge of interpreting neural network decisions by decomposing
  images into human-understandable concepts and tracing them back to training data.
  Unlike post-hoc explanation methods, COMIX provides by-design interpretability by
  classifying images based on learned concepts and matching them to corresponding
  regions in training images.
---

# COMIX: Compositional Explanations using Prototypes

## Quick Facts
- arXiv ID: 2501.06059
- Source URL: https://arxiv.org/abs/2501.06059
- Authors: Sarath Sivaprasad; Dmitry Kangin; Plamen Angelov; Mario Fritz
- Reference count: 17
- Primary result: 48.82% improvement in C-insertion score on ImageNet compared to state-of-the-art baselines

## Executive Summary
COMIX introduces a novel compositional explanation method that provides by-design interpretability for neural network decisions by decomposing images into human-understandable concepts and tracing them back to training data. Unlike post-hoc explanation methods, COMIX uses B-cos networks, an inherently interpretable architecture, to classify images based on learned concepts and match them to corresponding regions in training images. The method achieves a 48.82% improvement in C-insertion score on ImageNet while maintaining competitive accuracy and demonstrates robust performance across multiple datasets. COMIX ensures fidelity, sparsity, necessity, and sufficiency of explanations, making it particularly promising for safety-critical domains like medical imaging.

## Method Summary
COMIX is a compositional explanation method that leverages B-cos networks to provide interpretable classifications by decomposing images into class-defining concepts. The method works by extracting feature maps from the B-cos network, identifying prototypical regions in training data that match these features, and generating explanations that show how these prototypes contribute to the final classification. Unlike traditional post-hoc methods, COMIX's interpretability is built into the architecture, allowing it to trace decisions back to specific training data regions. The approach addresses key requirements for explanations including fidelity, sparsity, necessity, and sufficiency, while maintaining competitive performance on standard benchmarks.

## Key Results
- Achieves 48.82% improvement in C-insertion score on ImageNet compared to state-of-the-art baselines
- Maintains competitive accuracy while providing by-design interpretability
- Demonstrates robust performance across multiple datasets including potential application to safety-critical domains

## Why This Works (Mechanism)
COMIX works by leveraging the inherent interpretability of B-cos networks, which are designed to learn and represent class-defining concepts explicitly. The method decomposes the classification process into understandable components by matching extracted features to prototypical regions in the training data. This compositional approach ensures that explanations are not only faithful to the model's decision-making process but also meaningful to human understanding. The use of prototypes provides a direct link between the model's internal representations and actual training examples, making the explanations both interpretable and verifiable.

## Foundational Learning
- B-cos networks: Why needed - Provides inherently interpretable architecture for concept learning; Quick check - Verify that the network learns distinct, class-specific concepts
- Prototype matching: Why needed - Establishes connection between model decisions and training data; Quick check - Confirm that matched prototypes are semantically relevant
- Compositional explanations: Why needed - Breaks down complex decisions into understandable components; Quick check - Validate that explanations capture all relevant aspects of the decision

## Architecture Onboarding

Component Map:
Input Image -> B-cos Network -> Feature Extraction -> Prototype Matching -> Compositional Explanation

Critical Path:
The critical path flows from input image through the B-cos network's feature extraction layers to prototype matching, where the final explanation is generated. This path must maintain both computational efficiency and interpretability.

Design Tradeoffs:
- Interpretability vs. accuracy: The method prioritizes interpretability but maintains competitive accuracy
- Computational complexity: Prototype matching adds overhead but provides crucial interpretability
- Generality vs. specificity: The method is designed to work across domains but may require domain-specific tuning

Failure Signatures:
- Poor prototype matching leading to irrelevant explanations
- Over-reliance on spurious correlations in training data
- Degradation in performance on complex, ambiguous images

First Experiments:
1. Verify prototype matching accuracy on simple, well-defined classes
2. Test explanation quality on images with clear compositional structure
3. Evaluate performance degradation on increasingly complex images

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Scalability to larger, more diverse datasets beyond ImageNet remains uncertain
- Reliance on B-cos networks may limit adaptability to other network architectures
- Need for extensive validation in safety-critical domains like medical imaging

## Confidence
- 48.82% improvement in C-insertion score: Medium confidence
- Competitive accuracy while providing interpretability: Medium confidence
- Applicability to safety-critical domains: Medium confidence

## Next Checks
1. Conduct extensive experiments on diverse, real-world datasets to assess generalizability and robustness across different domains and data distributions
2. Evaluate COMIX performance on alternative network architectures to determine adaptability and potential limitations
3. Implement a user study with domain experts, particularly in medical imaging, to validate practical utility and reliability of COMIX-generated explanations in decision-making processes