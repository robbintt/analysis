---
ver: rpa2
title: 'Autonomous Learning From Success and Failure: Goal-Conditioned Supervised
  Learning with Negative Feedback'
arxiv_id: '2509.03206'
source_url: https://arxiv.org/abs/2509.03206
tags:
- learning
- distance
- function
- goal
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GCSL-NF, a method that extends goal-conditioned
  supervised learning (GCSL) by incorporating negative feedback to address biases
  from learning solely from successful experiences. The key innovation is using a
  learned distance function to provide feedback when the agent fails to achieve its
  intended goal, encouraging exploration and avoiding policy stagnation.
---

# Autonomous Learning From Success and Failure: Goal-Conditioned Supervised Learning with Negative Feedback

## Quick Facts
- **arXiv ID**: 2509.03206
- **Source URL**: https://arxiv.org/abs/2509.03206
- **Reference count**: 40
- **Primary result**: GCSL-NF outperforms leading GCSL-based and HER-based methods across challenging environments by incorporating negative feedback from failed goals

## Executive Summary
This paper introduces GCSL-NF, a method that extends goal-conditioned supervised learning by incorporating negative feedback from failed goals to prevent policy stagnation. The key innovation is using a learned distance function to provide corrective signals when the agent fails to achieve its intended goal, encouraging exploration beyond self-imitation of relabelled successes. Empirical results demonstrate superior learning efficiency and final performance across various challenging environments, particularly in scenarios where spatial proximity does not imply temporal proximity.

## Method Summary
GCSL-NF extends goal-conditioned supervised learning by learning both a goal-conditioned Q-function and a contrastive distance function. The agent collects trajectories under a greedy policy with sampled goals, then relabels achieved states as goals for positive imitation loss (L+). Crucially, it also computes a negative feedback loss (L₀) using the distance function to compare the final state against the original goal. The distance function is trained via Noise Contrastive Estimation to predict whether two states are temporally close, using positive pairs from local trajectory neighborhoods and negative pairs from distant timesteps or different trajectories. This dual evaluation prevents policy stagnation by providing corrective signals from failed attempts.

## Key Results
- GCSL-NF outperforms leading GCSL-based and HER-based methods across diverse environments including obstacle navigation, complex optimal paths, and deceptive observation spaces
- The method demonstrates superior learning efficiency and final performance in scenarios where spatial proximity does not imply temporal proximity
- Negative feedback enables escape from suboptimal convergence points that standard GCSL cannot overcome

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Incorporating negative feedback from failed original goals prevents policy stagnation caused by exclusive self-imitation of relabelled successes.
- **Mechanism**: GCSL-NF evaluates each trajectory twice—once against relabelled goals (positive samples for imitation via loss L+) and once against original goals (providing corrective signal via loss L₀). When the achieved state s_T is distant from the original goal g (per p_φ), the policy receives lower likelihood targets for those actions, implicitly encouraging alternative behaviors.
- **Core assumption**: The learned distance function p_φ provides a meaningful proxy for goal-achievement quality even when the optimal policy is unknown.
- **Evidence anchors**:
  - [abstract] "Learning exclusively from self-generated experiences can exacerbate the agents' inherent biases... precluding them from learning from their mistakes."
  - [section 3.1] "While the green line denotes an optimal route to the newly assigned goal Point C, it does not represent the most efficient path to the original goal Point B."
  - [corpus] Related work on goal-conditioned RL (arXiv:2512.06471) confirms the fundamental challenge of policy bias in goal-conditioned settings, though does not directly validate this specific mechanism.
- **Break condition**: If the distance function p_φ fails to generalize (e.g., overgeneralizes across aliased states like similar LiDAR readings at different locations), negative feedback becomes noisy and may not guide exploration effectively (acknowledged in Section 6.2).

### Mechanism 2
- **Claim**: A locally-trained contrastive distance function captures temporal proximity more reliably than Euclidean observation-space distances, particularly when spatial vicinity does not imply temporal vicinity.
- **Mechanism**: p_φ is trained via Noise Contrastive Estimation on three pair types: (1) same-trajectory states within threshold n as positive pairs, (2) same-trajectory states beyond threshold n as negative pairs, (3) different-trajectory states as negative pairs. This learns to predict whether two states are reachable within a short temporal window.
- **Core assumption**: The threshold n sufficiently captures "local" structure that is largely policy-independent (Section 6.3 shows robustness with small n).
- **Evidence anchors**:
  - [section 3.3] "Our distance leverages contrastive learning principles, enhancing its generalizability across various environments."
  - [section 6.2] Heatmaps show p_φ learns wall structures in 4-room navigation while SimCLR-TT and successor representations respond to globally distant states.
  - [corpus] No direct corpus validation; contrastive learning for RL (SimCLR-TT) is cited but operates differently (representation learning vs. direct distance estimation).
- **Break condition**: In environments where observation aliasing is severe (identical observations at distant states), the distance function cannot distinguish temporal proximity without additional memory or traces.

### Mechanism 3
- **Claim**: Negative feedback from original goals provides implicit exploration without requiring explicit exploration noise or random policy initialization.
- **Mechanism**: The agent follows a deterministic greedy policy (argmax Q_θ), but when trajectories fail to reach original goals, L₀ penalizes those action selections. This pushes the policy away from stagnant behaviors, creating exploration pressure proportional to goal-failure magnitude.
- **Core assumption**: The balance between L+ (imitation) and L₀ (negative feedback) remains stable; the paper sets β₁=β₂=1 but does not ablate this extensively.
- **Evidence anchors**:
  - [abstract] "Our algorithm overcomes limitations imposed by agents' initial biases and thereby enables more exploratory behavior."
  - [section 6.1] Training with only relabelled goals leads to impasses; adding original-goal loss enables escape from suboptimal convergence.
  - [corpus] Environment-agnostic goal-conditioning (arXiv:2511.04598) explores autonomous goal selection but does not address negative feedback specifically.
- **Break condition**: If L₀ dominates early training (many failures), the policy may become overly exploratory and fail to consolidate any successful behaviors. The paper does not analyze failure modes from unbalanced loss weighting.

## Foundational Learning

- **Concept: Goal-Conditioned Supervised Learning (GCSL)**
  - **Why needed here**: GCSL-NF extends GCSL; without understanding hindsight relabelling and self-imitation, the contribution of negative feedback cannot be contextualized.
  - **Quick check question**: Can you explain why relabelling a failed trajectory's achieved state as its goal creates valid training data?

- **Concept: Contrastive Learning and NCE Loss**
  - **Why needed here**: The distance function p_φ is trained via contrastive learning; understanding positive/negative pair construction is essential for debugging or modifying the distance estimator.
  - **Quick check question**: What distinguishes the three pair types (D+, D₁⁻, D₂⁻) used in training p_φ?

- **Concept: Binary Cross-Entropy for Goal-Reachability Prediction**
  - **Why needed here**: Both the Q-function (imitation) and distance function use BCE-style losses with targets of 1 or 0; understanding this framing is necessary to interpret the training dynamics.
  - **Quick check question**: Why does the regularization term in Equation 2 train Q_θ(a, s_t, g′) toward 0 for non-taken actions?

## Architecture Onboarding

- **Component map**: 
  - Trajectory collection -> Relabelling and original goal evaluation -> Q_θ update (L+ + L₀) -> p_φ update (NCE loss) -> Repeat

- **Critical path**:
  1. Collect trajectory under current greedy policy with sampled goal g
  2. Relabel achieved states as goals → generate T+ dataset
  3. Compare final state s_T to original goal g using p_φ → compute L₀
  4. Update Q_θ via combined loss; update p_φ via contrastive sampling from R
  5. Iterate; exploration emerges from negative feedback rather than action noise

- **Design tradeoffs**:
  - **Threshold n**: Small n captures local structure robustly but may miss medium-range temporal relationships; large n introduces policy-dependence (Section 6.3)
  - **Loss weighting (β₁, β₂)**: Paper sets both to 1; untested sensitivity in complex domains
  - **Distance function scope**: Local distance is more reliable but provides no global planning signal; combining with curriculum goal-sampling (not explored) could address long-horizon tasks

- **Failure signatures**:
  - **Policy stagnation despite training**: Likely indicates p_φ is not learning meaningful distances; check positive/negative pair distributions and visualize distance heatmaps
  - **Erratic policy oscillations**: May indicate L₀ dominating; reduce β₂ or examine whether original goals are consistently unreachable (goal distribution too hard)
  - **LiDAR/aliasing environments underperforming**: Observation space may not support distance discrimination; consider memory-based inputs or trajectory traces

- **First 3 experiments**:
  1. **Reproduce Point Mass Navigation with Obstacles**: Verify that GCSL-NF learns to navigate around the obstacle while standard GCSL gets stuck. Monitor L+ vs. L₀ ratio over training.
  2. **Ablate the distance function**: Replace p_φ with (a) Euclidean distance, (b) SimCLR-TT similarity, (c) successor representation. Compare final performance and distance heatmap quality (as in Figure 8).
  3. **Test bias recovery**: Initialize the policy with a strong directional bias (as in Point Mass with Initial Bias) and measure how many episodes GCSL-NF requires to overcome it vs. baselines. Track when L₀ begins to decrease, indicating successful exploration.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the integration of sophisticated, novelty-driven goal-sampling strategies impact the autonomy and learning efficiency of GCSL-NF compared to the current reliance on random goal selection?
- **Basis in paper**: [explicit] The Conclusion states, "A promising future direction would be to develop sophisticated goal-sampling strategies... to enable novelty-driven exploration and hence more autonomy in the learning process."
- **Why unresolved**: The current implementation relies on a fixed distribution for goal sampling, which may not efficiently drive exploration in complex environments.
- **What evidence would resolve it**: Empirical results from experiments replacing random sampling with intrinsic motivation modules (e.g., curiosity-driven goal generators) in sparse reward environments.

### Open Question 2
- **Question**: Can the incorporation of memory or observation traces into the distance function effectively resolve the issue of overgeneralizing similar observations across distinct states (perceptual aliasing)?
- **Basis in paper**: [explicit] Section 6.2 notes that the distance function may struggle in environments like LiDAR navigation where similar observations represent different states, suggesting "memory or observation traces" as a potential solution.
- **Why unresolved**: The current distance function relies on single observations, which lacks the context to distinguish between states with identical local views but different global positions.
- **What evidence would resolve it**: A comparative study showing improved performance in environments with heavy perceptual aliasing when the distance function uses recurrent layers or stacked frames.

### Open Question 3
- **Question**: Does the performance and sample efficiency of GCSL-NF scale effectively to high-dimensional continuous control tasks, such as humanoid locomotion or complex dexterous manipulation?
- **Basis in paper**: [inferred] The paper evaluates the method on 2D navigation and standard robotic arm tasks (Fetch Reach/Push), but does not demonstrate efficacy on high-dimensional benchmarks (e.g., Humanoid-v2) where learning a global distance function is significantly harder.
- **Why unresolved**: Contrastive learning can become computationally expensive or require massive amounts of data in high-dimensional state spaces, potentially limiting the method's applicability to complex robotics.
- **What evidence would resolve it**: Benchmark results on high-DoF (Degrees of Freedom) MuJoCo environments showing that GCSL-NF converges faster or to higher rewards than standard HER or GCSL baselines.

## Limitations

- Loss weighting sensitivity not extensively ablated; stability across β₁, β₂ combinations untested
- Unknown update frequencies for Q-function vs distance function may affect convergence
- Severe observation aliasing (e.g., identical LiDAR readings at different locations) could break distance function reliability

## Confidence

- **High confidence**: Mechanism 1 (negative feedback prevents policy stagnation) - directly validated by ablation showing impasses without L₀
- **Medium confidence**: Mechanism 2 (local distance captures temporal proximity) - supported by contrastive heatmaps but limited ablation with alternative distance measures
- **Medium confidence**: Mechanism 3 (implicit exploration without noise) - demonstrated empirically but theoretical bounds on exploration efficiency unclear

## Next Checks

1. **Ablation study**: Replace p_φ with Euclidean distance, SimCLR-TT, and successor representation to quantify benefit of learned local distance
2. **Loss weight sensitivity**: Systematically vary β₁, β₂ across orders of magnitude to identify stable operating regions
3. **Severe aliasing test**: Create environment with identical observations at distant states to measure distance function breakdown point