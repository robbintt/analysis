---
ver: rpa2
title: 'EMORF-II: Adaptive EM-based Outlier-Robust Filtering with Correlated Measurement
  Noise'
arxiv_id: '2509.07415'
source_url: https://arxiv.org/abs/2509.07415
tags:
- outlier
- emorf-ii
- measurement
- emorf
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EMORF-II addresses robust state estimation in nonlinear systems
  with correlated measurement noise and outliers. It extends the EM-based outlier-robust
  filter (EMORF) by learning outlier characteristics during inference, inspired by
  the Adaptive Selective Outlier-Rejecting (ASOR) method.
---

# EMORF-II: Adaptive EM-based Outlier-Robust Filtering with Correlated Measurement Noise

## Quick Facts
- arXiv ID: 2509.07415
- Source URL: https://arxiv.org/abs/2509.07415
- Reference count: 0
- Adaptive EM-based filter achieves lower RMSE than state-of-the-art methods for nonlinear systems with correlated measurement noise and outliers

## Executive Summary
EMORF-II is a novel adaptive filtering algorithm that extends the EM-based outlier-robust filter (EMORF) by learning outlier characteristics during inference. The method addresses robust state estimation in nonlinear systems where measurements contain both correlated noise and outliers. By employing a hierarchical Bayesian model with Gamma-distributed outlier indicators and covariance learning, EMORF-II jointly estimates states and outlier parameters through EM iterations. The approach maintains computational complexity of O(m⁴) while demonstrating improved accuracy over existing methods.

## Method Summary
The EMORF-II algorithm uses a standard UKF core filter augmented with an Expectation-Maximization loop for adaptive outlier learning. The method assumes outliers follow a Gamma distribution and occur independently in each measurement dimension. During each EM iteration, the algorithm performs prediction (Eqs. 11-12), measurement update with adaptive covariance Rₖ(Îₖ) (Eqs. 15-20), outlier indicator estimation via decision rule (Eq. 23), and parameter updates including b̂ₖ (Eq. 28). The hierarchical Bayesian framework allows simultaneous estimation of states and outlier parameters, with the filter initializing Îᵢₖ=1 and updating through convergence.

## Key Results
- Consistently achieves lower RMSE than state-of-the-art methods as outlier frequency increases
- Maintains O(m⁴) computational complexity despite adaptive learning
- Demonstrates improved accuracy at the cost of increased computation due to adaptive outlier learning
- Shows effectiveness on nonlinear target tracking with TDOA measurements

## Why This Works (Mechanism)
The hierarchical Bayesian approach with Gamma-distributed outlier indicators enables the filter to distinguish between regular measurement noise and outliers. By learning outlier characteristics during inference rather than assuming fixed parameters, the filter can adapt to varying outlier distributions. The EM iterations allow joint estimation of states and outlier parameters, with the adaptive covariance matrix Rₖ(Îₖ) providing robustness against corrupted measurements. The independence assumption across measurement dimensions ensures inferential tractability while maintaining effectiveness.

## Foundational Learning
- **UKF (Unscented Kalman Filter)**: Needed to handle nonlinear system dynamics and measurement models. Quick check: Validate baseline tracking without outliers.
- **Expectation-Maximization (EM)**: Required for iterative parameter estimation. Quick check: Monitor EM convergence behavior on synthetic data.
- **Hierarchical Bayesian Models**: Essential for modeling uncertainty at multiple levels. Quick check: Verify Gamma prior updates correctly during M-step.
- **Gamma Distribution**: Used to model outlier indicators. Quick check: Ensure b̂ₖ remains positive during updates.
- **TDOA Measurements**: Time difference of arrival measurements in target tracking. Quick check: Confirm measurement model accuracy.

## Architecture Onboarding

Component Map:
State Estimation -> UKF Core -> EM Iteration Loop -> Outlier Learning -> Covariance Update

Critical Path:
UKF Prediction -> Measurement Update -> EM Convergence Check -> Parameter Updates -> Next Iteration

Design Tradeoffs:
- Accuracy vs. Computation: Adaptive learning improves accuracy but increases execution time
- Model Complexity vs. Tractability: Hierarchical Bayesian model adds complexity but enables robust outlier detection
- Convergence vs. Real-time Performance: EM iterations ensure accuracy but may limit real-time deployment

Failure Signatures:
- Non-convergence of EM iterations (check Rₖ(Îₖ) positive definiteness)
- Degraded performance vs. EMORF (verify Gamma prior implementation)
- Numerical instability in b̂ₖ updates (monitor for negative values)

First Experiments:
1. Implement standard UKF with nonlinear process and TDOA measurement models
2. Generate corrupted TDOA data with varying outlier probabilities
3. Compare RMSE vs. EMORF baseline on simplified scenario

## Open Questions the Paper Calls Out
- How does the filter perform when outliers are correlated across measurement dimensions? The paper assumes outliers occur independently in each dimension to ensure inferential tractability, but does not model inter-dimensional correlations.
- Can computational latency be reduced to support high-frequency real-time estimation? The adaptive learning loop increases execution time, potentially limiting deployment in time-critical systems despite maintaining O(m⁴) complexity.
- Is the algorithm robust to misspecification of Gamma hyperparameters (Aₖ, Bₖ)? The experiments use fixed initialization values, but sensitivity analysis is not provided.

## Limitations
- Does not handle correlated outliers across measurement dimensions
- Higher computational cost compared to non-adaptive methods
- Performance sensitivity to hyperparameter initialization not fully characterized

## Confidence
- UKF core implementation: High
- EM algorithm structure: High
- Outlier learning mechanism: Medium
- Convergence criteria: Low
- Baseline comparison methodology: Medium

## Next Checks
1. Verify UKF baseline tracking performance on clean TDOA measurements before adding outliers
2. Test EM convergence behavior with synthetic data where ground truth outlier indicators are known
3. Compare EMORF-II performance against EMORF on simplified scenario with fixed outlier parameters