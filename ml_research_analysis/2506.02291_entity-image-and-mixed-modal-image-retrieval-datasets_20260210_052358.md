---
ver: rpa2
title: Entity Image and Mixed-Modal Image Retrieval Datasets
arxiv_id: '2506.02291'
source_url: https://arxiv.org/abs/2506.02291
tags:
- image
- entity
- dataset
- mmir
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces two new datasets to advance mixed-modal
  image retrieval: the Entity Image Dataset (EI) and the Mixed-Modal Image Retrieval
  Dataset (MMIR). EI provides canonical images for Wikipedia entities, while MMIR
  is derived from the WIT dataset and features challenging queries combining entity
  images with contextual text.'
---

# Entity Image and Mixed-Modal Image Retrieval Datasets

## Quick Facts
- arXiv ID: 2506.02291
- Source URL: https://arxiv.org/abs/2506.02291
- Authors: Cristian-Ioan Blaga; Paul Suganthan; Sahil Dua; Krishna Srinivasan; Enrique Alfonseca; Peter Dornbach; Tom Duerig; Imed Zitouni; Zhe Dong
- Reference count: 4
- Primary result: Two new datasets introduced for mixed-modal image retrieval - Entity Image Dataset (1.80M entities) and Mixed-Modal Image Retrieval Dataset (9.06M examples from WIT)

## Executive Summary
This paper introduces two new datasets to advance mixed-modal image retrieval: the Entity Image Dataset (EI) and the Mixed-Modal Image Retrieval Dataset (MMIR). EI provides canonical images for Wikipedia entities, while MMIR is derived from the WIT dataset and features challenging queries combining entity images with contextual text. The datasets were constructed using a multi-stage process involving Wikipedia content crawling, image candidate identification, consolidation, and canonical image selection, with thresholds tuned per entity category to ensure quality. MMIR includes two novel query types: single entity-image queries and multi-entity-image queries, requiring models to understand complex cross-modal relationships. Human evaluation confirmed high quality, with over 97% of EI images rated as Good or Excellent and 81% of MMIR examples judged semantically coherent. Model-based evaluation showed that training on MMIR significantly improves retrieval performance compared to models trained only on general image-text datasets, while maintaining competitive performance on standard benchmarks like Flickr30k and MS COCO. These datasets offer a challenging benchmark for mixed-modal retrieval and a valuable resource for advancing multimodal learning research.

## Method Summary
The Entity Image Dataset (EI) contains 1.80M canonical images for Wikipedia entities, selected through a four-stage pipeline: Wikipedia content crawling to extract entity sections and images, image candidate identification using entity annotation services, consolidation to filter and deduplicate candidates, and canonical image selection with category-specific confidence thresholds. The Mixed-Modal Image Retrieval (MMIR) dataset contains 9.06M training examples derived from the WIT dataset by filtering to reference descriptions only and generating mixed-modal queries. Mixed-modal queries are created by identifying entities common to both image and text, masking entity names in the caption, and replacing them with canonical entity images. The dual-encoder model architecture uses mT5-Base for text and frozen ViT-Large for vision, with mean-pooled embeddings projected to 768 dimensions and optimized using in-batch sampled softmax loss. Models are pre-trained on WebLI-10B and fine-tuned on MMIR, CC3M, or both.

## Key Results
- Category-specific confidence thresholds doubled canonical image coverage to 1.79M entities compared to uniform thresholds
- Human evaluation confirmed high quality: 97.3% of EI images rated Good/Excellent, 81.1% of MMIR examples rated semantically coherent
- Models fine-tuned on MMIR+CC3M achieve comparable performance to CC3M-only models on Flickr30k (R@1: 89.9 vs 90.3) and MS-COCO (R@1: 83.4 vs 83.9)
- MMIR fine-tuned models show substantial advantage on MMIR evaluation (R@1: 11.7 vs 8.5) over CC3M-only models
- Single entity-image queries in MMIR are more challenging than multi-entity-image queries (R@1: 11.7 vs 13.4 for multi-entity)

## Why This Works (Mechanism)

### Mechanism 1
Category-specific confidence thresholds improve canonical image selection coverage while maintaining quality. Rather than applying a uniform confidence threshold across all entity types, the pipeline identifies 8 physical entity categories (people, animals, plants, locations, tourist attractions, historical places/events, objects) plus an "other" category. For each category, optimal thresholds are determined independently by maximizing agreement with human annotations. This accommodates the finding that single-threshold approaches disproportionately filter out location and person entities.

### Mechanism 2
Masking entity names in text and replacing them with canonical entity images creates retrieval tasks that require cross-modal grounding. Given an image-text pair from WIT, entity annotation services identify entities present in both image (Ii) and text (Ti). The intersection Pi = Ii ∩ Ti represents common entities. For each entity in Pi that has a canonical image in EI, the entity name is replaced with a mask token [MASK_*] pointing to that entity image. The resulting "mixed-modal context" (masked text + entity images) must retrieve the original image.

### Mechanism 3
Training on mixed-modal retrieval data improves performance on mixed-modal benchmarks without degrading standard image-text retrieval. A dual-encoder with shared parameters processes mixed-modal inputs. Text encoder (mT5-Base) and vision encoder (ViT-Large) produce embeddings that are mean-pooled and projected to 768 dimensions. In-batch sampled softmax loss optimizes cosine similarity. Models fine-tuned on MMIR+CC3M maintain competitive performance on Flickr30k and MS-COCO while substantially improving on MMIR evaluation.

## Foundational Learning

- **Dual-Encoder Architecture**: Why needed here: The MMIR evaluation uses a dual-encoder with shared parameters to process both query (image+text or image) and candidate images separately, enabling efficient retrieval via embedding similarity rather than cross-attention. Quick check question: Can you explain why dual-encoders are preferred over cross-encoders for large-scale retrieval, and what the tradeoff is?

- **Entity Linking/Recognition**: Why needed here: Both EI construction (identifying which images contain which entities) and MMIR construction (finding entity overlap between images and captions) require automated entity annotation services. Quick check question: What types of errors might entity annotation services introduce, and how would each error type propagate through the dataset construction pipeline?

- **Recall@K for Retrieval Evaluation**: Why needed here: Model performance is evaluated using Recall@1, @5, @10 across MMIR, Flickr30k, and MS-COCO benchmarks, measuring whether the correct image/caption appears in the top-K retrieved results. Quick check question: Why is Recall@K used instead of precision or accuracy for retrieval tasks? What does Recall@1 specifically measure?

## Architecture Onboarding

- **Component map**: Entity Image Dataset (EI) -> Mixed-Modal Image Retrieval (MMIR) -> Dual-Encoder Model -> Evaluation
- **Critical path**: 1. Entity recognition on WIT image-text pairs → identify common entities (Pi) 2. For each entity in Pi, retrieve canonical image from EI 3. Mask entity names in text, append entity images → mixed-modal query 4. Dual-encoder embeds query and candidate images → cosine similarity ranking 5. Optimize with in-batch softmax loss
- **Design tradeoffs**: Category-specific vs. uniform thresholds (category-specific doubles coverage but requires per-category human annotation sampling), Entity cap at 5 (90%+ of examples have ≤3 entities; filtering >5 entities reduces complexity but may exclude challenging multi-entity scenarios), Reference description only (using only WIT reference descriptions improves quality but reduces scale from 37M to ~17.2M examples), Vision encoder frozen during fine-tuning (reduces compute but may limit adaptation to mixed-modal patterns)
- **Failure signatures**: Low Recall@1 on MMIR with high Recall@1 on Flickr30k/COCO suggests model hasn't learned cross-modal grounding, High "non-physical entity" rate in "Other" category indicates threshold miscalibration, Semantic coherence scores <81% in human evaluation suggest masking procedure loses critical information, Disproportionate filtering of specific entity types suggests category thresholds need adjustment
- **First 3 experiments**: 1. Baseline comparison: Evaluate zero-shot WebLI-10B on MMIR test set (both I+T→I and I→I+T directions) to establish baseline performance before any fine-tuning. 2. Ablation on entity count: Filter MMIR training to examples with 1, 2, 3, 4, 5 entities separately and evaluate whether multi-entity examples are harder (expect decreasing Recall@K as entity count increases). 3. Threshold sensitivity analysis: For a sample of entities, vary confidence threshold ±0.1 from category-specific optimum and measure impact on canonical image quality via human evaluation to understand robustness of threshold selection.

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on proprietary entity annotation services for both canonical image selection and mixed-modal query generation, limiting exact reproduction
- Evaluation relies on WebLI-10B model that is not publicly available, requiring substitution with alternative vision-language models
- Category-specific threshold optimization process may require recalibration if Wikipedia's entity distribution changes significantly
- Masking procedure assumes entity recognition is sufficiently accurate, but systematic errors could degrade both dataset quality and model performance

## Confidence

- **High Confidence**: Dual-encoder architecture with frozen vision encoder and category-specific threshold optimization for canonical image selection are well-specified and reproducible
- **Medium Confidence**: Mixed-modal query generation mechanism and training pipeline are clearly described, but exact performance depends on access to WIT dataset and WebLI-10B pre-training
- **Low Confidence**: Exact numerical confidence thresholds per entity category and proprietary entity annotation service configurations are not disclosed

## Next Checks

1. **Entity Recognition Robustness Test**: Run entity annotation services on a stratified sample of 1,000 WIT image-text pairs to measure precision and recall of entity detection, then estimate how many MMIR examples would be lost or corrupted due to entity recognition errors.

2. **Threshold Sensitivity Analysis**: For 5-10 representative entities from each category, manually evaluate canonical image quality across a range of confidence thresholds (±0.1 from the reported optimum) to quantify how sensitive image quality is to threshold selection.

3. **Zero-shot Baseline Evaluation**: Evaluate a publicly available multilingual vision-language model (e.g., BLIP-2) on the MMIR test set to establish a baseline performance ceiling before any fine-tuning, providing context for the reported gains from MMIR training.