---
ver: rpa2
title: 'ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems'
arxiv_id: '2510.05746'
source_url: https://arxiv.org/abs/2510.05746
tags:
- reasoning
- arxiv
- preprint
- step
- module
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARM, a method for discovering generalizable
  agentic reasoning modules that improve upon Chain-of-Thought prompting. The key
  insight is that complex multi-agent systems often perform no better than simple
  CoT baselines, suggesting the need to optimize the fundamental reasoning unit.
---

# ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems
## Quick Facts
- arXiv ID: 2510.05746
- Source URL: https://arxiv.org/abs/2510.05746
- Reference count: 40
- ARM achieves 42.9% average accuracy across complex reasoning tasks, outperforming handcrafted and automatically discovered multi-agent systems

## Executive Summary
This paper introduces ARM (Agentic Reasoning Modules), a method for discovering generalizable reasoning modules that improve upon traditional Chain-of-Thought prompting in multi-agent systems. The key insight is that complex multi-agent systems often perform no better than simple CoT baselines, suggesting the need to optimize the fundamental reasoning unit rather than the orchestration. ARM uses evolutionary search with reflection to discover a code-based reasoning module that acts as a specialized agent at each step, orchestrated by a learned meta-policy. The resulting modules significantly outperform both handcrafted and automatically discovered multi-agent systems on complex reasoning tasks while generalizing across different foundation models without further optimization.

## Method Summary
ARM addresses the limitations of traditional Chain-of-Thought prompting and complex multi-agent systems by discovering optimized reasoning modules through evolutionary search. The method employs an evolutionary algorithm with reflection mechanisms to iteratively improve candidate modules, where each module is a code-based reasoning unit that acts as a specialized agent. A learned meta-policy orchestrates these discovered modules during inference. The evolutionary search optimizes for modules that can be effectively combined by the meta-policy to solve complex reasoning tasks. Crucially, once discovered, these modules generalize across different foundation models without requiring additional optimization, enabling deployment across various model families while maintaining strong performance.

## Key Results
- ARM achieves 42.9% average accuracy across MATH-500, AIME, HMMT, GPQA, and LiveBench datasets
- Outperforms best baseline (38.0% average accuracy) by 4.9 percentage points absolute improvement
- Demonstrates strong generalization across different foundation models (GPT-4.1-nano, GPT-4o, LLaMA-3.3-70B) without further optimization

## Why This Works (Mechanism)
ARM works by discovering fundamental reasoning modules that are more effective than traditional Chain-of-Thought prompting. The evolutionary search with reflection allows the system to iteratively refine these modules based on their actual performance on reasoning tasks, rather than relying on handcrafted or heuristic-based approaches. By treating each module as a specialized agent and learning an effective meta-policy for orchestration, ARM creates a more efficient and adaptable multi-agent system. The code-based nature of the modules provides flexibility and precision in reasoning operations, while the evolutionary discovery process ensures that only the most effective module architectures are retained.

## Foundational Learning
- Evolutionary algorithms: Needed to systematically discover effective reasoning modules through iterative improvement; quick check: verify that fitness evaluation is both reliable and computationally feasible
- Reflection mechanisms: Required to enable modules to analyze and improve their own reasoning processes; quick check: ensure reflection doesn't introduce excessive computational overhead
- Meta-policy learning: Essential for orchestrating discovered modules effectively; quick check: validate that the meta-policy can generalize across different reasoning scenarios

## Architecture Onboarding
- Component map: Evolutionary search -> Module discovery -> Meta-policy learning -> Inference orchestration
- Critical path: Module discovery (evolutionary search) → Meta-policy training → Task execution
- Design tradeoffs: Computational cost of evolutionary search vs. performance gains; model generalization vs. task-specific optimization
- Failure signatures: Poor module discovery leading to suboptimal meta-policy; overfitting to specific model architecture during discovery
- First experiments: 1) Run evolutionary search on simplified reasoning tasks to validate discovery process; 2) Test meta-policy orchestration with hand-crafted modules; 3) Evaluate generalization across different model sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Significant computational overhead: Evolutionary search took 3.3 days on 16 A100 GPUs for AIME dataset alone
- Limited domain evaluation: Primarily focuses on mathematical reasoning tasks, not other domains
- Model dependency: Initial discovery performed on GPT-4.1-nano, raising questions about optimality for other base models

## Confidence
- Performance improvements over baselines: High confidence (multiple datasets, established methods)
- Generalization across foundation models: Medium confidence (empirical evidence, limited testing)
- Computational requirements: High confidence (explicitly reported experimental setup)

## Next Checks
1. Conduct ablation studies to quantify the contribution of the reflection component in the evolutionary search
2. Test ARM on non-mathematical reasoning tasks such as commonsense reasoning benchmarks and code generation challenges
3. Perform a cost-benefit analysis comparing ARM's computational requirements during discovery against performance improvements achieved