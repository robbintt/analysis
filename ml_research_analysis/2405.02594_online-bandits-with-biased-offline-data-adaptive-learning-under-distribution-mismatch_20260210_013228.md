---
ver: rpa2
title: 'Online Bandits with (Biased) Offline Data: Adaptive Learning under Distribution
  Mismatch'
arxiv_id: '2405.02594'
source_url: https://arxiv.org/abs/2405.02594
tags:
- offline
- data
- regret
- bound
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies stochastic multi-armed and combinatorial bandits
  with potentially biased offline data. Without auxiliary information, no non-anticipatory
  policy can outperform vanilla UCB, even with offline data.
---

# Online Bandits with (Biased) Offline Data: Adaptive Learning under Distribution Mismatch
## Quick Facts
- arXiv ID: 2405.02594
- Source URL: https://arxiv.org/abs/2405.02594
- Authors: Wang Chi Cheung; Lixing Lyu
- Reference count: 40
- Primary result: MIN-UCB and MIN-COMB-UCB algorithms adaptively use biased offline data only when beneficial, achieving improved regret bounds over vanilla UCB.

## Executive Summary
This paper addresses the fundamental challenge of incorporating potentially biased offline data into stochastic bandit algorithms. The authors establish that without auxiliary information about the bias direction and magnitude, no non-anticipatory policy can outperform the standard UCB algorithm. When valid bias bounds are provided, they propose MIN-UCB and MIN-COMB-UCB algorithms that adaptively incorporate offline data only when it provides genuine improvement. The theoretical analysis yields both instance-dependent and instance-independent regret bounds that demonstrate the value of properly incorporating offline data under appropriate conditions.

## Method Summary
The paper proposes two adaptive algorithms: MIN-UCB for multi-armed bandits and MIN-COMB-UCB for combinatorial bandits. Both algorithms work by comparing the potential benefit of using offline data against the uncertainty in bias estimation. The key insight is to use offline data only when the bias bounds are sufficiently tight to guarantee improvement over vanilla UCB. The algorithms incorporate a "minimum" operation that selects between the standard UCB estimate and a corrected offline estimate, where the correction accounts for the worst-case bias within the provided bounds. This adaptive approach ensures that offline data is used only when it provably reduces regret.

## Key Results
- MIN-UCB and MIN-COMB-UCB achieve improved instance-dependent regret bounds compared to vanilla UCB when valid bias bounds are provided
- The algorithms provide instance-independent regret bounds of order $\tilde{O}(\sqrt{KT})$ where $K$ is the number of arms and $T$ is time horizon
- The paper proves a fundamental impossibility result: without bias bounds, no non-anticipatory policy can outperform vanilla UCB using offline data
- Theoretical analysis reveals that the direction of bias discrepancy critically affects whether offline data can be beneficial

## Why This Works (Mechanism)
The algorithms work by carefully balancing the exploitation of potentially useful offline information against the risk of incorporating biased data. The key mechanism is the adaptive "minimum" operation that only uses offline data when the bias bounds are tight enough to guarantee improvement. This prevents the algorithm from being misled by potentially biased offline data while still capturing the benefits when the bias is small and the offline data is representative. The approach effectively creates a safety net that ensures performance never degrades below vanilla UCB while potentially improving when conditions are favorable.

## Foundational Learning
- **Stochastic Multi-armed Bandits**: Sequential decision-making framework where an agent repeatedly selects arms to maximize cumulative reward. Needed to understand the baseline problem setting. Quick check: Can you derive the UCB1 algorithm and its regret bound?
- **Combinatorial Bandits**: Extension where actions involve selecting sets of arms rather than single arms. Needed for applications like influence maximization. Quick check: Can you explain the difference between semi-bandit and full-bandit feedback in combinatorial settings?
- **Bias Bounds and Distribution Mismatch**: The concept that offline data may be drawn from a different distribution than the online environment. Needed to understand when offline data can be useful. Quick check: Can you formulate the bias bound constraints mathematically?
- **Regret Analysis**: Framework for measuring algorithm performance against an optimal policy. Needed to evaluate the proposed algorithms. Quick check: Can you distinguish between instance-dependent and instance-independent regret bounds?
- **Non-anticipatory Policies**: Algorithms that make decisions based only on past observations. Needed for the impossibility result. Quick check: Can you explain why this constraint matters for the theoretical analysis?
- **Offline-to-Online Learning**: The paradigm of leveraging historical data for online decision-making. Needed to understand the broader context. Quick check: Can you identify applications where this framework applies beyond bandits?

## Architecture Onboarding
- **Component Map**: Historical Data → Bias Bound Estimation → MIN-UCB/MIN-COMB-UCB Algorithm → Arm Selection → Reward Feedback → Regret Calculation
- **Critical Path**: The core execution path involves receiving rewards, updating confidence bounds, and selecting the next arm using the minimum operation between UCB and bias-corrected offline estimates
- **Design Tradeoffs**: The algorithm trades potential performance improvement against the risk of incorporating biased data. The minimum operation provides a conservative safeguard but may underutilize useful offline data when bias bounds are loose.
- **Failure Signatures**: Performance degradation occurs when bias bounds are incorrectly specified (too loose or too tight), when the optimal arm is not disjoint from offline data, or when offline data heterogeneity (H) is poorly estimated.
- **First Experiments**: 1) Compare MIN-UCB vs vanilla UCB under varying bias magnitudes; 2) Test sensitivity to bias bound estimation errors; 3) Validate the disjoint optimal arm assumption by violating it systematically.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The algorithms require perfect knowledge of bias bounds, which may be unrealistic in practical applications
- The theoretical analysis assumes the optimal arm/set is disjoint from offline data, which may not hold in many real-world scenarios
- The algorithms may underutilize offline data when bias bounds are loose, potentially missing opportunities for improvement
- The analysis focuses on stochastic bandits with potentially adversarial offline data, which may not capture all practical hybrid scenarios

## Confidence
**High Confidence**: The impossibility result that offline data cannot help without bias information is rigorously proven. The correctness of improved regret bounds for MIN-UCB and MIN-COMB-UCB when valid bias bounds are provided is mathematically established.

**Medium Confidence**: The assumption that offline data heterogeneity (H) can be reasonably estimated may be optimistic in practice, and the impact of estimation errors is not fully characterized.

**Low Confidence**: The practical applicability of strict bias bound requirements and the assumption of disjoint optimal arms from offline data may be challenging to verify in real-world settings.

## Next Checks
1. **Empirical validation of bias bound estimation**: Design experiments to test how well bias bounds can be estimated from data in practical scenarios, and quantify the impact of estimation errors on algorithm performance.

2. **Sensitivity analysis to offline data structure**: Systematically test the algorithms when the assumption that the best arm/set is disjoint from offline data is violated, to understand the robustness of the approach.

3. **Real-world application case studies**: Implement the algorithms in domains like dynamic pricing or social influence maximization with actual historical data to validate the theoretical findings and identify practical challenges not captured in the theoretical analysis.