---
ver: rpa2
title: Can Large Language Models Function as Qualified Pediatricians? A Systematic
  Evaluation in Real-World Clinical Contexts
arxiv_id: '2511.13381'
source_url: https://arxiv.org/abs/2511.13381
tags:
- pediatric
- clinical
- medical
- performance
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the capability of large language models (LLMs)
  in pediatric medicine using a new benchmark framework, PEDIASBench, which assesses
  models across three dimensions: basic knowledge application, dynamic diagnosis and
  treatment, and medical safety/ethics. Twelve models, including GPT-4o, DeepSeek-V3,
  and Qwen3-235B-A22B, were tested on 211 prototypical diseases across 19 pediatric
  subspecialties.'
---

# Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts

## Quick Facts
- arXiv ID: 2511.13381
- Source URL: https://arxiv.org/abs/2511.13381
- Reference count: 40
- Primary result: Current LLMs show promise for pediatric decision support but cannot independently perform pediatric care

## Executive Summary
This study evaluates 12 large language models on pediatric medical competence using a novel benchmark framework called PEDIASBench. The framework assesses models across three dimensions: basic knowledge application, dynamic diagnosis and treatment, and medical safety/ethics. Testing across 211 diseases in 19 pediatric subspecialties reveals that while models perform well on foundational knowledge (some exceeding 90% accuracy), their performance drops significantly with increased task complexity and real-time reasoning demands. The study concludes that current LLMs are not yet ready for autonomous pediatric care but show potential as supervised decision support tools and medical education resources.

## Method Summary
The study evaluates 12 LLMs (including GPT-4o, DeepSeek-V3, and Qwen3-235B-A22B) using a zero-shot evaluation paradigm on PEDIASBench, a benchmark framework with three dimensions: basic medical knowledge, dynamic diagnosis/treatment, and medical safety/ethics. The evaluation uses standardized prompts across 211 diseases in 19 pediatric subspecialties, with questions at four difficulty levels (Resident to Senior). Dynamic cases are split into initial consultation and follow-up phases. Performance is measured using accuracy for single-choice questions, F1 score for multiple-choice, and a custom metric combining Macro Recall and BERTScore for short-answer questions.

## Key Results
- Models excel at foundational knowledge (Qwen3-235B-A22B >90% accuracy) but struggle with complex reasoning
- Performance declines ~15% as task complexity increases, particularly in dynamic diagnosis scenarios
- DeepSeek-R1 leads dynamic reasoning (mean 0.58) but still struggles with real-time patient changes
- Qwen2.5-72B achieves highest medical safety/ethics accuracy (92.05%) but humanistic sensitivity remains limited

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured competency frameworks reveal performance gaps that single-metric benchmarks miss
- **Mechanism:** PEDIASBench decomposes "pediatric competence" into three orthogonal dimensions—knowledge recall, dynamic reasoning, and ethics/safety. This forces models to demonstrate capability across qualitatively different task types rather than excelling at one while failing others.
- **Core assumption:** The three dimensions capture independent failure modes
- **Evidence anchors:** Performance declined ~15% as task complexity increased, revealing limitations in complex reasoning

### Mechanism 2
- **Claim:** Dynamic case segmentation exposes temporal reasoning failures that static question-answering cannot detect
- **Mechanism:** Cases split into T1 (initial consultation) and T2 (follow-up with new results). This mimics real clinical decision-making where diagnoses evolve. Models must integrate prior context with new evidence.
- **Core assumption:** Clinical reasoning requires maintaining and updating beliefs across time
- **Evidence anchors:** Most models struggled to adapt to real-time patient changes

### Mechanism 3
- **Claim:** Zero-shot evaluation with disabled reasoning modes isolates parametric knowledge from inference-time computation
- **Mechanism:** By disabling multi-step reasoning modes and using uniform API parameters, the evaluation measures what is encoded in model weights rather than what emerges from extended inference chains.
- **Core assumption:** Parametric knowledge is the limiting factor
- **Evidence anchors:** A zero-shot evaluation paradigm was employed to objectively assess intrinsic model capability

## Foundational Learning

- **Concept: Zero-shot evaluation paradigm**
  - Why needed here: The study deliberately avoids fine-tuning to measure "intrinsic" model capability
  - Quick check question: If you added 5 exemplar pediatric cases to the prompt, would you expect performance to increase, and would that change the study's conclusions about model competence?

- **Concept: Competency-based medical education (CBME)**
  - Why needed here: PEDIASBench maps to the resident→junior→intermediate→senior progression used in real medical training
  - Quick check question: A model scores 90% on resident-level questions but 70% on senior-level questions. Is this a "passing" pediatrician?

- **Concept: F1 score vs. accuracy in multi-label classification**
  - Why needed here: Multiple-choice medical questions may have multiple correct answers
  - Quick check question: A model selects options A, B, C when the correct answer is A, B. Compute accuracy vs. F1.

## Architecture Onboarding

- **Component map:** Standardized prompts -> 211 diseases across 19 subspecialties -> Three task types (single-choice, multiple-choice, short-answer) -> Three metrics (Accuracy, F1, Macro Recall + BERTScore) -> 12 LLMs via API

- **Critical path:** Dataset construction → Expert validation → Prompt standardization → Zero-shot API calls → Output parsing → Metric computation → Cross-model comparison

- **Design tradeoffs:** Zero-shot vs. few-shot (sacrifices performance for cleaner measurement); Text-only vs. multimodal (excludes critical clinical data); Disabled reasoning modes (reduces real-world comparability)

- **Failure signatures:** High resident accuracy, sharp senior drop (knowledge without integration); High T1, low T2 (temporal context failure); High ethics accuracy, low humanistic sensitivity (rule-following without empathy)

- **First 3 experiments:** 1) Baseline replication to verify accuracy hierarchy; 2) Ablation with reasoning modes enabled; 3) Subspecialty gap analysis correlating with training data sparsity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does multimodal integration significantly improve LLM diagnostic accuracy in complex pediatric subspecialties?
- Basis: Future development should focus on multimodal integration; current evaluation was text-only
- Why unresolved: Text-only approaches insufficient for complex reasoning (pediatric oncology surgery showed 0% accuracy)
- What evidence would resolve it: Comparative study with and without multimodal inputs on complex cases

### Open Question 2
- Question: Can clinician-in-the-loop feedback and domain-specific fine-tuning close the ~15% performance gap?
- Basis: Paper calls for clinical feedback–model iteration loop; assessment focused on zero-shot performance
- Why unresolved: Consistent performance decline as complexity increased, but unclear if fundamental limitations or insufficient training
- What evidence would resolve it: Fine-tune leading models with iterative clinician feedback, then re-evaluate

### Open Question 3
- Question: What training interventions can improve humanistic sensitivity and empathic communication?
- Basis: Humanistic sensitivity remained limited even in top performers; training on child-centered communication data suggested
- Why unresolved: High ethics accuracy doesn't translate to demonstrated empathic behavior in open-ended interactions
- What evidence would resolve it: Develop empathy rubric, train on child-centered dialogue datasets, measure improvements

### Open Question 4
- Question: How do LLMs perform in real-world clinical workflows as decision support tools?
- Basis: Laboratory results must be corroborated with multicenter clinical evaluations
- Why unresolved: Current evaluation used simulated cases; prior clinical trials showed individual accuracy doesn't translate to measurable improvements
- What evidence would resolve it: Prospective multicenter pilot studies integrating LLMs into specific pediatric workflows

## Limitations
- Zero-shot paradigm may underestimate real-world capabilities where few-shot prompting is standard
- Exclusion of multimodal inputs (imaging, lab results) represents significant gap in clinical importance
- GPT-4o used for dataset construction introduces potential circularity in benchmarking
- Short-answer evaluation metric lacks direct validation against clinical grading standards

## Confidence
- Models lack autonomous pediatric competence: High confidence
- Knowledge recall vs. dynamic reasoning are independent dimensions: Medium confidence
- Zero-shot evaluation captures intrinsic capability: Medium confidence
- Models show promise for decision support under supervision: Low confidence

## Next Checks
1. **Prompt engineering ablation:** Re-run evaluation with reasoning modes enabled and few-shot exemplars for 3-5 models. Measure whether the 15% complexity penalty shrinks and T2 dynamic reasoning scores improve by >20%.
2. **Multimodal capability assessment:** Construct 50 cases with clinical imaging/lab results from PEDIASBench. Evaluate whether adding multimodal context improves accuracy on cases where text-only models fail.
3. **Correlation structure analysis:** Compute pairwise correlations between performance across three dimensions and difficulty levels. Determine whether claimed independence of failure modes holds statistically.