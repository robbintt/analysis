---
ver: rpa2
title: 'BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives'
arxiv_id: '2511.08029'
source_url: https://arxiv.org/abs/2511.08029
tags:
- retrieval
- biomedical
- hard
- bica
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces BiCA (Biomedical Citation-Aware) retrievers,
  which leverage citation link structures in biomedical literature to generate high-quality
  hard negatives for training dense retrieval models. The method constructs multi-hop
  citation neighborhoods from PubMed and employs a stochastic semantic traversal strategy
  to identify challenging negative examples that are semantically similar but not
  directly relevant to the query.
---

# BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives

## Quick Facts
- arXiv ID: 2511.08029
- Source URL: https://arxiv.org/abs/2511.08029
- Reference count: 11
- This work introduces BiCA retrievers that leverage citation link structures in biomedical literature to generate high-quality hard negatives for training dense retrieval models.

## Executive Summary
BiCA (Biomedical Citation-Aware) retrievers introduce a novel approach to training dense retrieval models by leveraging citation structures in biomedical literature to generate high-quality hard negatives. The method constructs multi-hop citation neighborhoods from PubMed and employs stochastic semantic traversal to identify challenging negative examples that are semantically similar but not directly relevant to the query. Evaluated on BEIR and LoTTE benchmarks, BiCA Base (110M parameters) achieves state-of-the-art average nDCG@10 of 0.518 and outperforms significantly larger models on biomedical tasks, while BiCA Small (33M parameters) delivers competitive results with superior latency efficiency.

## Method Summary
BiCA constructs 2-hop citation neighborhoods from seed PubMed abstracts using NCBI E-utilities API, then generates synthetic queries from positive document abstracts using T5-based models. The core innovation is a hard negative miner that performs stochastic traversals on cosine similarity graphs of embedded abstracts to identify challenging negatives. The fine-tuned GTE models are trained using Multiple Negative Ranking Loss on triplets of (query, positive, hard negatives), resulting in dense embeddings that capture nuanced relevance distinctions in biomedical literature.

## Key Results
- BiCA Base achieves state-of-the-art average nDCG@10 of 0.518 on BEIR benchmark
- BiCA Small (33M parameters) delivers competitive results with superior latency efficiency
- Both models outperform significantly larger models on biomedical retrieval tasks
- Strong generalization to both in-domain and out-of-domain retrieval tasks

## Why This Works (Mechanism)
The method works by mining semantically challenging negatives from citation neighborhoods rather than relying on random or BM25-based negatives. Citation links in PubMed provide semantically rich relationships between documents, ensuring mined negatives are topically related but not directly relevant. The stochastic traversal strategy explores the citation graph to find documents that are semantically similar to the query but fall outside the immediate relevance neighborhood, creating a more informative contrastive training signal that improves the model's ability to distinguish fine-grained relevance distinctions.

## Foundational Learning
- **Citation Graph Construction**: Building 2-hop neighborhoods from PubMed seed articles using NCBI E-utilities API - needed for creating semantically rich negative pools; quick check: verify API response structure and citation depth.
- **Hard Negative Mining**: Stochastic traversal on cosine similarity graphs to identify challenging negatives - needed to create informative contrastive training signals; quick check: analyze negative similarity distribution vs. random negatives.
- **Multiple Negative Ranking Loss**: Training objective for learning from multiple negatives per query - needed to optimize dense embeddings for ranking; quick check: monitor nDCG@10 on validation set during training.
- **Cross-Encoder Evaluation**: Using ColBERT-E or other cross-encoders to validate retrieval quality - needed to establish upper bounds on retrieval performance; quick check: compare nDCG@10 between dense and cross-encoder models.
- **Zero-Shot Transfer**: Evaluating models on out-of-domain tasks without fine-tuning - needed to assess generalization capabilities; quick check: measure performance drop across domain shifts.
- **Latency-Efficiency Tradeoff**: Comparing small vs. base model performance and speed - needed for practical deployment decisions; quick check: benchmark latency at different batch sizes.

## Architecture Onboarding

- **Component map:** Data Curation Module (PubMed API) -> Query Generator (T5) -> Hard Negative Miner (Stochastic Traversal) -> Fine-Tuning Engine (MNR Loss) -> Retrieval Engine (FAISS index)
- **Critical path:** The quality of the final retriever is most sensitive to the **Hard Negative Miner**. If the mined negatives are not truly "hard" (too dissimilar) or contain false negatives (actually relevant), the contrastive training signal is compromised, leading to suboptimal embeddings.
- **Design tradeoffs:**
  - **Citation Graph Depth (Hops):** A 2-hop expansion is used. Deeper hops might find more negatives but increase API costs, computational complexity, and the risk of semantic drift.
  - **Traversal Parameters (`N_paths`, `L_path`):** More paths and longer lengths increase negative diversity but also computational cost. The ablation study found `N_paths=3` and `L_path=3` to be a robust balance.
  - **Model Size (`BiCASmall` vs. `BiCABase`):** `BiCABase` offers higher accuracy (0.518 vs 0.501 avg nDCG@10 on BEIR), while `BiCASmall` provides significantly lower latency (e.g., ~2x faster for batch size 2000).
- **Failure signatures:**
  - **Low Retrieval Performance:** Check the quality of hard negatives. A high rate of false negatives would create a conflicting training signal.
  - **Poor Generalization to OOD Tasks:** The paper shows strong OOD results. Failure here might indicate the citation-aware negatives are too specific to biomedical semantics.
  - **Slow Data Generation:** The pipeline is I/O-bound by PubMed API calls and is acknowledged as slow ("week(s)").
- **First 3 experiments:**
  1. **Reproduce the Core Result:** Fine-tune a standard GTE-Base model on a small subset (e.g., 1,000 examples) of the provided `BiCA` dataset and evaluate on a biomedical BEIR task like SciFact. Compare against the baseline GTE-Base score.
  2. **Ablate Hard Negative Quality:** Train two models: one with negatives from the citation-aware traversal and one with random negatives from the corpus. Compare their performance on a retrieval benchmark.
  3. **Latency-Accuracy Profiling:** Deploy both `BiCASmall` and `BiCABase` on a sample query workload. Measure the end-to-end retrieval latency and plot it against the nDCG@10 score on a held-out test set.

## Open Questions the Paper Calls Out
- **Citation-Aware Mining in Non-Biomedical Domains:** Can this approach be adapted to domains without formal citation structures like Wikipedia or web corpora? The paper notes this is unexplored but potentially valuable.
- **Scaling Citation Graph Construction:** How can the pipeline be optimized to scale beyond 20,000 seed documents while respecting API rate limits? The current approach takes weeks for larger datasets.
- **Optimal Citation Depth:** Is 2-hop optimal, or would 3-hop or deeper chains yield better hard negatives? The paper uses 2-hop without ablation justification.
- **Query Generation Quality:** How sensitive is performance to the quality of synthetically generated queries from the Doc2Query model? Poor-quality queries could lead to irrelevant negative regions.

## Limitations
- **API Dependency:** The method relies on PubMed's citation structure and NCBI E-utilities API, limiting scalability and creating potential bottlenecks.
- **Domain Specificity:** While showing strong generalization, the citation-aware approach may encode domain-specific relationships that don't transfer well to non-biomedical literature.
- **Computational Overhead:** Constructing multi-hop citation neighborhoods is computationally expensive and time-consuming, potentially taking weeks for larger datasets.

## Confidence
- **High Confidence**: The core retrieval performance claims (nDCG@10 of 0.518 for BiCABase) are well-supported by BEIR and LoTTE benchmarks.
- **Medium Confidence**: The efficiency claims (latency improvements of BiCASmall) are credible but depend on specific hardware configurations not fully specified.
- **Low Confidence**: The long-term scalability of the citation mining approach and its performance on truly novel biomedical domains remain uncertain.

## Next Checks
1. **Ablation Study Replication**: Reproduce the citation-aware vs. random negative comparison on SciFact to empirically validate the hard negative strategy's contribution.
2. **Cross-Domain Transfer**: Evaluate BiCA models on non-biomedical retrieval tasks (e.g., legal or news corpora) to test the limits of citation structure generalization.
3. **Efficiency Profiling**: Conduct comprehensive latency-accuracy benchmarking across different hardware configurations (CPU, GPU) and batch sizes to quantify practical efficiency tradeoffs.