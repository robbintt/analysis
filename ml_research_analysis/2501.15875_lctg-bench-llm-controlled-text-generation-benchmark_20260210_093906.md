---
ver: rpa2
title: 'LCTG Bench: LLM Controlled Text Generation Benchmark'
arxiv_id: '2501.15875'
source_url: https://arxiv.org/abs/2501.15875
tags:
- text
- generation
- controllability
- evaluation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# LCTG Bench: LLM Controlled Text Generation Benchmark

## Quick Facts
- **arXiv ID**: 2501.15875
- **Source URL**: https://arxiv.org/abs/2501.15875
- **Reference count**: 11
- **Primary result**: Japanese LLMs show poor character-count controllability due to tokenizer-level limitations

## Executive Summary
LCTG Bench is a unified Japanese benchmark for evaluating LLM controllability across three business-relevant tasks (Summarization, Ad Text Generation, Pros & Cons Generation) from four perspectives (Format, Character count, Keyword, Prohibited word). The benchmark uses rule-based constraint verification without gold references to preserve generative diversity, while also evaluating output quality with GPT-4. Japanese-specific models show strong performance on format and keyword constraints but struggle with character-count constraints and fail to distinguish between affirmative and negative instructions.

## Method Summary
The benchmark evaluates LLMs on three tasks using 120-150 prompts each, with 3 outputs per prompt averaged. Controllability is measured through rule-based verification (format matching, character range compliance, keyword inclusion/exclusion), while quality is assessed using GPT-4 as a binary classifier. GPT-4 post-processing removes unnecessary explanatory text before evaluation. No gold references are provided to preserve diversity. The evaluation uses crowdsourcing to collect condition statement templates and standard generation hyperparameters (temperature 0.9-1.0, max_new_tokens 4096-8092).

## Key Results
- GPT-4 achieves only 0.35-0.59 on C-COUNT constraints across all tasks despite leading other categories
- Japanese models show correlated KEYWORD and P-WORD scores, indicating poor understanding of affirmative vs. negative instructions
- High FORMAT scores often correlate with low quality scores, revealing trade-offs between constraint satisfaction and output substance

## Why This Works (Mechanism)

### Mechanism 1
A unified cross-task evaluation framework enables transferable model selection for controllability across business use cases. Four control perspectives (Format, Character count, Keyword, Prohibited word) are applied consistently across three tasks, allowing performance patterns to generalize across scenarios. Core assumption: Business controllability requirements can be meaningfully abstracted into these four categories.

### Mechanism 2
Rule-based verification without gold references preserves generative diversity while measuring constraint satisfaction. Outputs are evaluated against explicit constraints rather than matching predetermined correct outputs, avoiding false-negative penalties on diverse valid responses. Core assumption: Constraint satisfaction and content quality can be meaningfully evaluated independently.

### Mechanism 3
Token-level tokenizers impair character-count controllability, including for high-performing models. LLMs operate on tokens rather than characters, and Japanese text has variable token-to-character ratios, making precise character-level constraints difficult to internalize. Core assumption: The tokenizer's abstraction level is the limiting factor, not model capacity or instruction tuning.

## Foundational Learning

- **Concept: Tokenizer-Character Mismatch in Japanese**
  - Why needed here: Character-count constraints are pervasive in Japanese business contexts, but subword tokenizers obscure character boundaries.
  - Quick check question: Given the Japanese writing system (hiragana, katakana, kanji), why might a BPE tokenizer produce highly variable token-to-character ratios?

- **Concept: Affirmative vs. Negative Constraint Distinction**
  - Why needed here: Japanese models show correlated performance on KEYWORD and P-WORD, suggesting incomplete distinction between "include X" and "exclude Y" semantics.
  - Quick check question: If a model treats "include X" and "exclude X" as similar instructions, what does this imply about its internal representation of negation?

- **Concept: Quality-Controllability Trade-offs**
  - Why needed here: High controllability does not guarantee high output quality; separate evaluation is required.
  - Quick check question: Why might a model satisfy all explicit constraints while still producing a low-quality business output?

## Architecture Onboarding

- **Component map**: Task layer (Summarization, Ad Text Generation, Pros & Cons Generation) → Constraint layer (Format, Character count, Keyword, Prohibited word) → Evaluation layer (Rule-based controllability scoring + GPT-4 quality classification)

- **Critical path**: 1) Prompt construction (task instruction + condition statement + base text) → 2) Model inference with constraint-conditional prompt → 3) Output post-processing (GPT-4 removes unnecessary explanatory text) → 4) Controllability scoring (exact rules per constraint type) → 5) Quality scoring (GPT-4 binary classification)

- **Design tradeoffs**:
  - Rule-based vs. LLM-based evaluation: Rules are precise for constraints but miss semantic quality; LLM evaluation adds quality signal but introduces variability (Cohen's Kappa 0.308–0.716 with humans)
  - Unified vs. task-specific constraints: Enables cross-task comparison but may miss niche constraint types
  - No gold reference: Preserves diversity but requires separate quality validation

- **Failure signatures**:
  - High FORMAT, low quality: Model outputs correctly formatted but semantically weak text (e.g., rinna/youri-7b-chat)
  - Low C-COUNT across all models: Tokenizer-level limitation, not model-specific
  - KEYWORD/P-WORD correlation: Model fails to distinguish affirmative vs. negative instructions

- **First 3 experiments**:
  1. Baseline: Run GPT-4 and one Japanese-specific model (e.g., elyza/llama2-7b-instr) on all three tasks, collecting controllability and quality scores to identify largest gaps
  2. Ablation: Remove GPT-4 post-processing step to measure impact of unnecessary explanatory text on C-COUNT and KEYWORD scoring
  3. Extension: Add a new constraint type (e.g., "sentence count" or "politeness level") to one task to test framework extensibility

## Open Questions the Paper Calls Out

### Open Question 1
How can the performance of character count control (C-COUNT) be improved in LLMs, given that current token-based tokenizers struggle with strict numerical constraints? The paper identifies low performance and hypothesizes that tokenizers are the cause but does not propose or validate a method to overcome this limitation.

### Open Question 2
What evaluation methodologies can achieve higher human agreement than GPT-4 for the quality assessment of summarization and ad text generation? The paper establishes that current LLM-based evaluation is insufficient for these specific creative/constrained tasks but does not offer an alternative.

### Open Question 3
Why do Japanese-specific LLMs fail to distinguish between affirmative (Keyword) and negative (Prohibited Word) constraints, and can this be mitigated? The paper identifies this correlation as a limitation in Japanese models' understanding of negation but does not investigate the specific training data or attention mechanisms causing this failure.

## Limitations

- Core assumption that business controllability requirements can be meaningfully abstracted into exactly four categories lacks external validation
- Quality evaluation using GPT-4 classifier has reliability concerns (Cohen's Kappa 0.308-0.716 with humans) that limit confidence in quality-controllability relationship conclusions
- Framework's extensibility to new constraint types and tasks beyond the current scope remains theoretical

## Confidence

**High Confidence**: Tokenizer-level abstraction impairs character-count controllability (consistent low C-COUNT scores 0.35-0.59 for GPT-4 across all models and tasks); Japanese models show correlated KEYWORD/P-WORD performance indicating poor understanding of affirmative vs. negative instructions.

**Medium Confidence**: Rule-based verification without gold references preserves generative diversity (logically sound but lacks direct empirical comparison); unified framework enables transferable model selection across business use cases (supported by cross-task consistency but not validated in real deployment).

**Low Confidence**: Business controllability requirements can be meaningfully abstracted into exactly four categories (lacks external validation); framework extensibility to new constraint types and tasks (theoretical only); quality evaluation using GPT-4 classifier (reliability concerns limit confidence).

## Next Checks

1. **Tokenizer Sensitivity Analysis**: Systematically evaluate impact of different tokenization strategies (BPE, SentencePiece, character-level) on C-COUNT controllability across same models to determine if tokenizer choice can meaningfully improve character-count constraint satisfaction.

2. **Quality-Controllability Correlation Study**: Conduct controlled experiments varying weight of controllability constraints in prompts while measuring both controllability scores and independent human quality ratings to empirically quantify trade-off relationship.

3. **Framework Extensibility Test**: Implement and evaluate at least two new constraint types (e.g., sentence count, politeness level, or sentiment control) across existing three tasks to test whether unified framework can accommodate constraint categories beyond original four.