---
ver: rpa2
title: Learning Explainable Dense Reward Shapes via Bayesian Optimization
arxiv_id: '2504.16272'
source_url: https://arxiv.org/abs/2504.16272
tags:
- reward
- optimization
- token-level
- https
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of sparse rewards in reinforcement
  learning from human feedback (RLHF) for language model alignment, where only sequence-level
  rewards are provided, leading to poor token-level credit assignment. The authors
  propose a method that uses explainability techniques like SHAP and LIME to estimate
  per-token rewards, and then employs Bayesian Optimization to learn optimal weights
  for combining these token-level signals into a dense reward function.
---

# Learning Explainable Dense Reward Shapes via Bayesian Optimization

## Quick Facts
- **arXiv ID:** 2504.16272
- **Source URL:** https://arxiv.org/abs/2504.16272
- **Authors:** Ryan Koo; Ian Yang; Vipul Raheja; Mingyi Hong; Kwang-Sung Jun; Dongyeop Kang
- **Reference count:** 40
- **Primary result:** BO-optimized explainability-based dense rewards improve RLHF stability and achieve higher win rates on AlpacaEval2 and MTBench compared to sparse rewards.

## Executive Summary
This paper addresses the challenge of sparse rewards in RLHF for language model alignment, where only sequence-level feedback hinders effective token-level credit assignment. The authors propose a method that uses explainability techniques (SHAP, LIME) to estimate per-token rewards and then employs Bayesian Optimization to learn optimal weights for combining these signals into a dense reward function. Experiments show that explainability-based rewards improve training stability and reduce overfitting, with BO-optimized combinations achieving better downstream task performance. The approach is theoretically grounded in potential-based reward shaping, ensuring policy invariance to the original sparse reward.

## Method Summary
The method operates as a bilevel optimization framework: an inner PPO loop trains the policy using a shaped reward derived from token-level explanations, while an outer BO loop optimizes the weights combining different explanation methods. Token-level rewards are computed using SHAP or LIME on a frozen reward model, normalized and multiplied by the sequence-level reward. The shaped reward is then used to train the policy, with BO selecting the best convex combination of explanation methods to maximize validation performance. This approach is tested on HH-RLHF and Ultrafeedback datasets using PPO, with performance evaluated on AlpacaEval2 and MTBench benchmarks.

## Key Results
- Explainability-based dense rewards significantly reduce PPO value head loss, improving training stability compared to sparse rewards.
- BO-optimized combinations of SHAP and LIME outperform single-method approaches on downstream benchmarks.
- The method achieves higher win rates on AlpacaEval2 and MTBench compared to standard sparse reward baselines.
- Combining all four scores (SHAP, LIME, Attention, Cosine) degrades performance compared to simpler combinations, likely due to insufficient BO exploration.

## Why This Works (Mechanism)

### Mechanism 1: Explainability-Based Credit Redistribution
The method converts sparse sequence-level rewards into dense token-level signals by using explainability techniques (SHAP, LIME) to compute feature attribution scores for each token. These scores redistribute the original scalar reward across the sequence, providing intermediate feedback rather than waiting for terminal states. This improves credit assignment and stabilizes value function learning, assuming the explanation scores meaningfully correlate with token importance.

### Mechanism 2: Bilevel Optimization for Reward Shaping
Bayesian Optimization identifies an optimal convex combination of different token-level scoring methods (SHAP, LIME, Attention) to maximize validation performance. The outer loop uses a Gaussian Process surrogate to sample weights based on expected utility, while the inner loop trains the PPO policy using the shaped reward derived from those weights. The outer loop updates based on the average validation reward of the trained policy.

### Mechanism 3: Policy Invariance via Potential-Based Shaping
Additive feature attribution methods satisfy the requirements of potential-based reward shaping, preserving the optimal policy of the original reward function. By defining the potential as the weighted sum of explanation scores, the shaping term becomes $F(s,a,s') = \gamma\Phi(s') - \Phi(s)$, altering the learning path but not the optimal destination. This ensures the agent is guided but not diverted to a different objective.

## Foundational Learning

- **Concept: Potential-Based Reward Shaping (PBRS)**
  - **Why needed here:** Provides theoretical safety net explaining why changing reward function accelerates learning without "reward hacking" final policy.
  - **Quick check question:** If shaping function is not potential-based (integral over loop is non-zero), what happens to optimal policy?

- **Concept: Additive Feature Attribution (SHAP/LIME)**
  - **Why needed here:** These are sensors for the system; understanding SHAP uses Shapley values to fairly distribute reward among tokens explains why this redistribution is theoretically sound.
  - **Quick check question:** Why is "additive" property (sum of attributions equals model output) critical for Policy Invariance claim?

- **Concept: Gaussian Processes (GP) in Bayesian Optimization**
  - **Why needed here:** This is the "brain" of outer loop; a GP models unknown function $f(w)$ (validation score given weights) and provides uncertainty estimates for exploration-exploitation balance.
  - **Quick check question:** Why is GP suitable for optimizing weights when we have very limited budget of training trials ($m=25$)?

## Architecture Onboarding

- **Component map:** Reward Model (Frozen) -> Reward Shaper -> PPO Trainer (Inner Loop) -> Validator -> BO Loop (Outer Loop)
- **Critical path:** Sobol Initialization (First 5 trials) -> Train PPO (Epochs) -> Validate Checkpoint -> Update GP -> Sample New Weights -> Repeat
- **Design tradeoffs:**
  - Compute vs. Signal Quality: SHAP requires $O(N \times k^2)$ complexity vs. simple attention weights but provides higher quality attribution
  - Trial Count vs. Dimensionality: Increasing explanation types increases search space $d$; with fixed trials ($m=25$), higher $d$ leads to sub-optimal convergence
- **Failure signatures:**
  - Instability: PPO value loss fails to decrease (indicates explanation noise is too high)
  - Overfitting: High validation reward but low AlpacaEval/MTBench win rates (indicates shaping exploits reward model quirks)
  - BO Stalling: Acquisition function suggests identical weights repeatedly (GP uncertainty collapsed prematurely)
- **First 3 experiments:**
  1. Sanity Check: Run SHAP calculation on single batch; verify $\sum \phi_i \approx \text{scalar\_reward}$ to ensure additivity holds
  2. Baseline Comparison: Train PPO with `SHAP-only` weights ($w=1.0$) vs. `Sparse` ($w=0$) and plot Value Loss curves to verify stabilization
  3. BO Integration: Run 5 Sobol trials to verify pipeline: Sample $w$ -> Train -> Validate -> Update GP

## Open Questions the Paper Calls Out

1. Can dynamic, context-aware weighting schemes outperform the static global weights used in current BO approach?
2. Do human-aligned cognitive signals (e.g., eye-tracking) provide more effective credit assignment than mechanistic explainability methods?
3. Does increasing number of BO trials mitigate performance degradation observed in high-dimensional reward shaping combinations?

## Limitations

- The explainability methods may not align with human perception of relevant features, potentially propagating model biases into the dense reward signal
- The fixed budget of 25 BO trials may be insufficient for higher-dimensional weight spaces, leading to sub-optimal convergence
- Experiments are conducted on specific datasets (HH-RLHF, Ultrafeedback) and may not generalize to all alignment challenges

## Confidence

**High Confidence:**
- Token-level credit assignment through explainability methods reduces PPO value head loss and improves training stability
- Combining multiple explainability methods via BO can outperform single-method approaches
- Additive feature attribution methods preserve policy invariance under reward shaping

**Medium Confidence:**
- The specific weight combinations learned by BO are optimal for the given search space and trial budget
- The observed performance gains are primarily attributable to the dense reward signal rather than other confounding factors

**Low Confidence:**
- The explainability-based rewards maintain alignment with human preferences beyond the validation set
- The method generalizes to reward models with different architectures or training objectives

## Next Checks

1. **Ablation on Explanation Quality:** Compare SHAP/LIME-based rewards against random token masking or simple heuristics to isolate effect of high-quality explanations on performance.

2. **Reward Model Distribution Shift:** Evaluate performance when reward model is intentionally perturbed or trained on different distribution to measure robustness and BO adaptation capability.

3. **Human Evaluation of Explanations:** Conduct small-scale human study where annot