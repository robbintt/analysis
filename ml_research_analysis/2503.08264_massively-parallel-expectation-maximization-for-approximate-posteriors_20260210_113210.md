---
ver: rpa2
title: Massively Parallel Expectation Maximization For Approximate Posteriors
arxiv_id: '2503.08264'
source_url: https://arxiv.org/abs/2503.08264
tags:
- posterior
- approximate
- massively
- parallel
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QEM achieves faster convergence than massively parallel variational
  inference (MP VI) and reweighted wake-sleep (MP RWS) on hierarchical models, while
  being invariant to reparameterizations that significantly slow down gradient-based
  methods. On five real-world datasets (Bus Breakdown, MovieLens, Radon, Covid, and
  Bird Occupancy), QEM reaches higher ELBOs and predictive log-likelihoods in fewer
  iterations and less wall-clock time.
---

# Massively Parallel Expectation Maximization For Approximate Posteriors

## Quick Facts
- arXiv ID: 2503.08264
- Source URL: https://arxiv.org/abs/2503.08264
- Authors: Thomas Heap; Sam Bowyer; Laurence Aitchison
- Reference count: 40
- Primary result: QEM achieves faster convergence and higher ELBOs than MP VI/RWS on hierarchical models while being invariant to reparameterizations that degrade gradient-based methods.

## Executive Summary
This paper introduces QEM (Expectation Maximization for Approximate Posteriors), a gradient-free inference method that alternates between computing posterior moments via massively parallel importance weighting (MPIW) and directly updating approximate posterior parameters through moment matching. The method demonstrates superior performance on five real-world datasets compared to massively parallel variational inference and reweighted wake-sleep, achieving faster convergence and higher ELBOs while being invariant to reparameterizations that significantly slow down gradient-based approaches. The core innovation is leveraging modern MPIW methods to obtain accurate posterior moment estimates that enable direct parameter updates without requiring gradient computation.

## Method Summary
QEM implements a hybrid approach that combines the E-step of importance sampling with the M-step of moment matching, using MPIW to compute posterior moments in a tractable way by exploiting conditional independencies in the graphical model structure. The algorithm samples K copies per latent variable, computes importance weights over all K^n combinations while exploiting factorization to reduce memory complexity from O(K^n) to O(K^{max|qa(i)|}), and uses these weighted samples to estimate moments. These moments are then used to directly set parameters of exponential family approximate posteriors (Gaussian, Beta, Dirichlet, etc.) without requiring gradients. An exponential moving average (EMA) smooths moment estimates across iterations to reduce variance, and the method shows invariance to reparameterizations that typically degrade gradient-based approaches.

## Key Results
- QEM reaches higher ELBOs and predictive log-likelihoods in fewer iterations and less wall-clock time compared to MP VI and MP RWS on five real-world datasets.
- The method is invariant to reparameterizations that dramatically slow down gradient-based methods, maintaining performance across scaling factors of 10⁻² to 10⁻⁴.
- QEM avoids hyperparameter sensitivity issues associated with learning rates in gradient-based methods, with λ (EMA rate) selected from {0.001, 0.003, 0.01, 0.03, 0.1, 0.3} via early stopping.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MPIW provides accurate posterior moment estimates that scale tractably with model size, enabling direct parameter updates without gradients.
- Mechanism: Draw K samples per latent variable and explicitly reweight all K^n combinations by exploiting conditional independencies in the graphical model structure. This gives an effective exponential number of importance samples while maintaining computational tractability.
- Core assumption: The model has exploitable conditional independence structure (expressible as a Bayes net).
- Evidence anchors:
  - [abstract] "modern massively parallel importance weighting methods (Bowyer et al., 2024) give fast and accurate posterior moment estimates"
  - [section 3.2, Eq. 7a-7c] Formal definition of MPIW moment estimates using factorization Q_MP(z) = ∏ Q_MP(z_i | z_j for j ∈ qa(i))
  - [corpus] Weak direct support; related work on sparse GPs and variational methods shares structural similarities but no direct validation of this specific mechanism.

### Mechanism 2
- Claim: Gradient-free parameter updates via moment matching accelerate convergence compared to gradient-based VI/RWS.
- Mechanism: In the E-step, compute posterior moments via MPIW. In the M-step, directly set exponential family parameters to match these moments (e.g., Gaussian mean/variance from first/second moments). Use exponential moving average (EMA) to smooth noisy single-iteration estimates.
- Core assumption: Approximate posterior belongs to an exponential family where parameters are recoverable from moments.
- Evidence anchors:
  - [abstract] "directly updating approximate posterior parameters without gradients, avoiding issues of slow convergence and hyperparameter sensitivity"
  - [section 4, Algorithm 1] Complete QEM procedure with EMA update: m_t = λm^(one_iter)_t + (1-λ)m_{t-1}
  - [corpus] Fenchel-Young variational learning uses related moment-matching principles but in a different optimization framework.

### Mechanism 3
- Claim: QEM is invariant to reparameterizations that degrade gradient-based methods.
- Mechanism: QEM operates on moments, which transform predictably under scaling. Importance weights depend only on ratios P(x,z)/Q(z), which are invariant to consistent scaling transformations applied to both model and proposal.
- Core assumption: The approximate posterior family is closed under the reparameterization transformation.
- Evidence anchors:
  - [abstract] "invariant to reparameterizations of the model that dramatically slow down gradient based methods"
  - [section 5.2, Figure 5] Empirical results showing QEM performance unchanged under scaling factors of 1/100 to 1/10000, while MP VI/MP RWS degrade significantly
  - [appendix E, Theorem 2] Formal proof of reparameterization invariance under three conditions (model reparameterization, sampling reparameterization, closure under multiplication)

## Foundational Learning

- Concept: Importance sampling and self-normalized estimators
  - Why needed here: MPIW is fundamentally an importance sampling method; understanding weight normalization and variance is essential.
  - Quick check question: Can you explain why the number of samples needed scales as exp(D_KL(P||Q))?

- Concept: Exponential family distributions and sufficient statistics
  - Why needed here: The M-step requires mapping moments to distribution parameters; this is only straightforward for exponential families.
  - Quick check question: Given the first and second moments of a Gaussian, how do you recover the mean and variance?

- Concept: Graphical models and conditional independence
  - Why needed here: MPIW's tractability depends on exploiting factorization structure to avoid O(K^n) memory costs.
  - Quick check question: For a model with latent variables z_1 → z_2 → x, which conditionals must be computed to evaluate the joint?

## Architecture Onboarding

- Component map:
  - E-step (MPIW module): Samples K copies per latent, computes importance weights over all K^n combinations using message-passing-style aggregation exploiting conditional independence. Outputs moment estimates.
  - M-step (parameter setter): Maps moment estimates to exponential family parameters for each approximate posterior factor.
  - EMA buffer: Maintains running average of moments across iterations to reduce variance.
  - Memory optimization layer: Reduces O(K^n) to O(K^{max |qa(i)|}) by grouping variables and chunking computations.

- Critical path:
  1. Initialize approximate posterior parameters (typically standard Gaussian for each latent).
  2. Sample K copies of each latent from Q_MP (permutation-based parent-child sampling).
  3. Compute importance weights r_k(z) and normalizer P_MP(z) via MPIW aggregation.
  4. Compute moment estimates m^(one_iter) from weighted samples.
  5. Update EMA moments: m_t = λ × m^(one_iter) + (1-λ) × m_{t-1}.
  6. Set Q parameters from m_t; repeat until convergence.

- Design tradeoffs:
  - **K (samples per latent)**: Higher K improves moment accuracy but increases memory. Paper uses K=10-30.
  - **λ (EMA rate)**: Higher λ is more responsive but noisier. Paper selects from {0.001, 0.003, 0.01, 0.03, 0.1, 0.3} via early stopping at 125 iterations.
  - **Memory chunking**: For large models (e.g., MovieLens), split along data dimensions to fit GPU memory at cost of more passes.

- Failure signatures:
  - **Memory overflow**: Model has too many latents or insufficient conditional independence. Reduce K or add more aggressive chunking.
  - **Divergent moments**: EMA λ too high or K too low; moments become unstable. Reduce λ or increase K.
  - **Poor final ELBO**: Approximate posterior family mismatched to true posterior. Consider richer exponential families or mixtures.
  - **VI/RWS outperforming**: Check for reparameterization issues in those baselines first; if QEM still loses, model may violate exponential family assumptions.

- First 3 experiments:
  1. **Reproduce Bus Breakdown (small)**: Implement MPIW for a simple hierarchical model with 2-3 latent levels. Verify ELBO matches paper within error bars. This validates the core E-M loop.
  2. **Reparameterization stress test**: Take a working model and scale a latent by factors of 0.1, 0.01, 0.001. Confirm QEM ELBO is invariant while MP VI degrades. This validates the invariance claim.
  3. **Memory scaling analysis**: Measure memory usage vs. K for a model with varying latent counts. Confirm O(K^{max|qa(i)|}) scaling empirically. Identify the chunking threshold for your hardware.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive importance sampling frameworks be improved by integrating massively parallel importance weighted (MPIW) moment estimates?
- Basis in paper: [explicit] Section 2 notes that adaptive methods could be enhanced by incorporating these estimates, but the authors "leave this complex integration to future work."
- Why unresolved: The integration of MPIW estimates into pre-existing adaptive importance sampling algorithms remains unexplored.
- What evidence would resolve it: A modified adaptive importance sampling algorithm that successfully utilizes MPIW moment estimates to improve sample efficiency or convergence.

### Open Question 2
- Question: Can QEM be extended to handle complex, non-exponential family approximate posteriors (e.g., normalizing flows) where moment-to-parameter mappings are non-trivial?
- Basis in paper: [inferred] Section 4 states the M-step is "straightforward" for "simple approximate posteriors" like Gaussian or Gamma, implying the current method relies on closed-form mappings available in exponential families.
- Why unresolved: The proposed algorithm requires a direct mapping from estimated moments to distribution parameters, which is often intractable for highly flexible or correlated approximate posteriors.
- What evidence would resolve it: A derivation of a QEM M-step for a non-conjugate or highly correlated posterior family, or an approximate M-step that maintains convergence guarantees.

### Open Question 3
- Question: How can QEM be encapsulated into a complete probabilistic programming language?
- Basis in paper: [explicit] Section 7 lists as future work the aim to "collect these into a complete probabilistic programming language."
- Why unresolved: The current implementation relies on manually exploiting conditional independencies specific to each model structure.
- What evidence would resolve it: A software framework that automatically compiles graphical model definitions into the massively parallel operations required for QEM.

## Limitations

- The method's effectiveness depends heavily on the presence of exploitable conditional independence structure in the model, which may not exist in densely connected or non-hierarchical models.
- The exponential family assumption for approximate posteriors constrains applicability to models where such families are appropriate, limiting flexibility for complex posterior shapes.
- The implementation requires careful handling of MPIW moment estimation using the source-term trick, with limited details provided for the chunking strategy to achieve tractable memory complexity.

## Confidence

**High Confidence**: The reparameterization invariance claim is well-supported by both formal proof and empirical validation across multiple scaling factors. The moment-matching approach is theoretically sound for exponential family distributions.

**Medium Confidence**: The MPIW moment estimation claims are supported by references but require implementation details not fully specified in this paper. The empirical superiority over MP VI/RWS is well-demonstrated but may depend on hyperparameter tuning.

**Low Confidence**: The exact implementation details for exploiting conditional independencies to achieve tractable memory complexity are not fully specified, making it difficult to assess whether all claimed benefits will materialize in practice.

## Next Checks

1. **MPIW Implementation Verification**: Reproduce the Radon dataset results (Table 2) to validate the MPIW moment estimation implementation. This is the core technical component that enables the entire method.

2. **Reparameterization Invariance Test**: Take a simple hierarchical model and systematically scale latent variables by factors of 10⁻¹ through 10⁻⁴. Verify that QEM maintains constant ELBO while MP VI/RWS degrade, confirming the invariance property.

3. **Memory Scaling Analysis**: Implement a model with increasing numbers of latent variables (e.g., 5, 10, 20) and measure memory usage as a function of K. Verify the claimed O(K^{max|qa(i)|}) scaling and identify the chunking threshold where naive implementation fails.