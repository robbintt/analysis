---
ver: rpa2
title: Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems
arxiv_id: '2512.04476'
source_url: https://arxiv.org/abs/2512.04476
tags:
- expert
- experts
- arxiv
- activation
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently running Mixture-of-Experts
  (MoE) models on GPU-NDP systems when model weights exceed GPU memory capacity. The
  authors propose a context-aware approach that leverages prefill-stage routing statistics
  to guide expert placement and quantization during decoding.
---

# Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems

## Quick Facts
- arXiv ID: 2512.04476
- Source URL: https://arxiv.org/abs/2512.04476
- Reference count: 40
- Primary result: 8.7× decoding throughput improvement over MoNDE with only 0.13% accuracy drop for MoE models on CXL-enabled GPU-NDP systems

## Executive Summary
This paper addresses the challenge of efficiently running Mixture-of-Experts (MoE) models when model weights exceed GPU memory capacity. The authors propose a context-aware approach that leverages prefill-stage routing statistics to guide expert placement and quantization during decoding. Hot experts are dynamically pinned in GPU HBM, while cold experts are executed in-place on CXL-NDP devices using mixed-precision quantization (1-4 bit). The method achieves significant throughput improvements while maintaining accuracy, making it particularly effective for large-scale MoE models like Mixtral-8x7B and Mixtral-8x22B.

## Method Summary
The authors propose a context-aware inference system for MoE models on CXL-enabled GPU-NDP systems. The approach analyzes routing patterns during the prefill stage to predict expert importance for decoding. Based on these statistics, experts are placed in GPU memory (hot experts) or executed in-place on CXL-NDP devices (cold experts) with adaptive quantization. The system uses a loss table computed from calibration data to determine optimal bitwidths for each expert, ranging from 1 to 4 bits. This strategy reduces costly expert migrations and balances accuracy with performance by adapting quantization based on expert importance.

## Key Results
- Achieves 8.7× decoding throughput improvement over state-of-the-art MoNDE system
- Maintains accuracy with only 0.13% average drop across evaluated models
- Reduces expert migration costs by 92% compared to baseline approaches
- Improves end-to-end latency by 3.2× on average across tested benchmarks

## Why This Works (Mechanism)
The approach exploits the temporal correlation between prefill and decoding routing patterns. By analyzing which experts are activated during prefill, the system can predict which experts will be important during decoding with high accuracy (0.89 cosine similarity). This prediction enables intelligent placement decisions that keep frequently accessed experts in fast GPU memory while relegating less important experts to slower NDP devices. The mixed-precision quantization further optimizes resource usage by allocating higher precision to important experts and lower precision to less critical ones.

## Foundational Learning

**Mixture-of-Experts (MoE)**: A neural network architecture where multiple expert networks exist, and a gating network selects which experts to activate for each input. Why needed: MoE enables larger models with improved capacity while maintaining computational efficiency. Quick check: Verify the gating mechanism correctly routes inputs to appropriate experts.

**CXL-NDP (Compute Express Link - Near Data Processing)**: An architecture where computation occurs closer to data storage to reduce data movement overhead. Why needed: MoE models often exceed GPU memory capacity, requiring efficient offloading strategies. Quick check: Confirm CXL bandwidth and latency characteristics match system requirements.

**Context-aware quantization**: Adaptive precision allocation based on input characteristics and computational importance. Why needed: Different experts have varying importance for different inputs, requiring dynamic precision optimization. Quick check: Validate quantization loss metrics against accuracy degradation.

## Architecture Onboarding

**Component map**: Input sequences → Prefill stage → Routing statistics collection → Expert placement decision → Decoding stage with in-place execution and quantization

**Critical path**: Prefill → Routing analysis → Placement decision → Decoding execution

**Design tradeoffs**: 
- Precision vs. performance: Lower precision reduces storage but may impact accuracy
- Memory vs. computation: Keeping experts in GPU memory improves speed but limits capacity
- Static vs. dynamic placement: Precomputed decisions vs. runtime adaptation

**Failure signatures**:
- Accuracy degradation beyond expected levels indicates poor routing prediction
- Performance bottlenecks suggest suboptimal expert placement
- System instability may result from CXL-NDP coordination issues

**3 first experiments**:
1. Measure routing pattern similarity between prefill and decoding across diverse input distributions
2. Profile expert activation frequencies to validate hot/cold expert identification
3. Test quantization quality by comparing 1-bit vs. 4-bit representations for different expert types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the prefill-to-decoding routing similarity across diverse tasks, domains, and extended generation lengths beyond the reported 0.89 cosine similarity on TruthfulQA?
- Basis in paper: [inferred] Section 3.2 reports high prefill-decoding similarity (0.89 average) but only evaluates Mixtral-8×7B on TruthfulQA. The entire system depends on this correlation remaining strong.
- Why unresolved: Different task types (math, code, long-form reasoning) or extended decoding may exhibit divergent routing patterns, causing misplaced experts and accuracy degradation.
- What evidence would resolve it: Systematic correlation analysis across benchmarks (MMLU, HumanEval, GSM8K) and output lengths exceeding 2048 tokens.

### Open Question 2
- Question: How does the per-sequence placement strategy handle continuous batching with multiple concurrent requests that have conflicting expert importance profiles?
- Basis in paper: [inferred] Algorithm 1 performs expert placement "once per sequence" for individual requests. Real serving systems batch multiple diverse requests sharing GPU/NDP resources.
- Why unresolved: Concurrent sequences may identify different experts as "hot," requiring arbitration or sharing strategies the paper does not address.
- What evidence would resolve it: Throughput and accuracy evaluation under multi-request batched inference with diverse input contexts.

### Open Question 3
- Question: What is the sensitivity of expert bitwidth selection to the calibration dataset composition and size (currently 1024 C4 samples)?
- Basis in paper: [inferred] Section 5.3 uses 1024 C4 samples for the loss table L_{l,e}(b). The quantization quality depends on calibration representativeness.
- Why unresolved: Domain mismatch between calibration data and deployment workload could skew the loss table, leading to suboptimal bitwidth assignments.
- What evidence would resolve it: Ablation studies varying calibration set size, domain composition, and measuring resulting accuracy/performance tradeoffs.

### Open Question 4
- Question: How does the system perform on real CXL-NDP hardware versus the Ramulator-based simulation, and what coordination overheads arise in multi-GPU or multi-NDP configurations?
- Basis in paper: [explicit] Section 5.1 states evaluation uses "NDP system simulator on Ramulator" with a single GPU+NDP configuration.
- Why unresolved: Simulation may not capture real-world CXL latency variability, memory contention, or multi-device synchronization costs.
- What evidence would resolve it: End-to-end latency and throughput measurements on physical CXL-NDP hardware and scaling experiments with multiple devices.

## Limitations
- Relies on specialized CXL-enabled GPU-NDP hardware that may not be widely available
- Assumes predictable expert activation patterns that may not hold for novel input distributions
- The 0.13% accuracy drop, while small, may compound in specialized applications
- Evaluation focuses primarily on Mixtral models, limiting generalizability to other MoE architectures

## Confidence

Model throughput improvement (8.7×): **High confidence** - Supported by specific benchmark comparisons against MoNDE
Accuracy preservation (0.13% drop): **Medium confidence** - Based on limited model types and evaluation sets
Context-aware expert placement effectiveness: **Medium confidence** - Theoretically sound but needs validation across diverse scenarios

## Next Checks

1. Test the context-aware routing approach on non-Mixtral MoE architectures and diverse input distributions to verify generalization beyond evaluated models

2. Conduct experiments on alternative hardware configurations (different GPU memory sizes, varying CXL-NDP bandwidths) to assess sensitivity to system parameters

3. Evaluate end-to-end application performance on specific downstream tasks (code generation, medical text analysis) to determine if the 0.13% accuracy drop has practical implications in specialized domains