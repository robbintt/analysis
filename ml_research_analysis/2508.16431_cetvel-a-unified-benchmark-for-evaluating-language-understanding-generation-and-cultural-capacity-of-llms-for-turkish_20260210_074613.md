---
ver: rpa2
title: 'Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation
  and Cultural Capacity of LLMs for Turkish'
arxiv_id: '2508.16431'
source_url: https://arxiv.org/abs/2508.16431
tags:
- language
- qwen2
- b-instruct
- turkish
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CETVEL, a new benchmark for evaluating large
  language models (LLMs) in Turkish. Unlike existing Turkish benchmarks, CETVEL combines
  broad task diversity with culturally and linguistically relevant content, covering
  23 tasks across seven categories including grammatical error correction, machine
  translation, and question answering on Turkish history.
---

# Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish

## Quick Facts
- arXiv ID: 2508.16431
- Source URL: https://arxiv.org/abs/2508.16431
- Reference count: 40
- This paper introduces CETVEL, a new benchmark for evaluating large language models (LLMs) in Turkish across 23 tasks covering language understanding, generation, and cultural knowledge.

## Executive Summary
CETVEL is a comprehensive benchmark designed to evaluate the capabilities of large language models in Turkish, addressing the gap in culturally and linguistically relevant evaluation frameworks for Turkish NLP. Unlike existing benchmarks, CETVEL combines task diversity with culturally specific content across seven categories, including grammatical error correction, machine translation, and Turkish history question answering. The benchmark evaluates 33 open-weight models up to 70B parameters, revealing that general-purpose multilingual models typically outperform Turkish-specific instruction-tuned models. Notably, Cere-Llama-3-8B demonstrates superior performance on key tasks like grammatical error correction and Turkish history QA. The benchmark provides a standardized evaluation framework to advance Turkish LLM development and research.

## Method Summary
CETVEL was constructed by curating 23 tasks across seven categories to comprehensively evaluate Turkish language understanding, generation, and cultural knowledge. The benchmark covers grammatical error correction, machine translation, summarization, question answering, commonsense reasoning, reading comprehension, and Turkish-specific cultural knowledge. The authors evaluated 33 open-weight LLMs ranging from 1.8B to 70B parameters, including general-purpose models, multilingual models, and Turkish-specific instruction-tuned models. Each model was assessed using standardized prompts and evaluation metrics appropriate to each task type, with performance compared across task categories to identify discriminative capabilities.

## Key Results
- General-purpose and multilingual models (e.g., Llama 3, Mistral) generally outperform Turkish-specific instruction-tuned models on CETVEL tasks
- Cere-Llama-3-8B excels on grammatical error correction and Turkish history QA, even surpassing the larger 70B Llama-3.3-70B-Instruct model
- Grammatical error correction, machine translation, and extractive QA are the most effective tasks for differentiating model capabilities

## Why This Works (Mechanism)
CETVEL works by providing a comprehensive evaluation framework that combines linguistic diversity with cultural relevance. The benchmark's effectiveness stems from its broad task coverage across multiple language processing dimensions, ensuring that models cannot rely on narrow capabilities to achieve high performance. By including culturally specific content like Turkish history, the benchmark tests models' ability to understand and reason about culturally embedded knowledge rather than just linguistic patterns. The task discrimination analysis reveals that certain tasks (grammatical error correction, machine translation, extractive QA) are particularly effective at revealing differences between models, suggesting these tasks capture fundamental capabilities that generalize across the language processing spectrum.

## Foundational Learning
- **Turkish morphology**: Understanding agglutinative structure and vowel harmony is essential for tasks like grammatical error correction
  - *Why needed*: Turkish has complex morphological rules that differ significantly from Indo-European languages
  - *Quick check*: Can the model correctly inflect verbs across all Turkish tense-aspect-mood combinations?

- **Cultural knowledge representation**: Models must encode and retrieve culturally specific information
  - *Why needed*: Turkish history and cultural context require understanding beyond surface-level language patterns
  - *Quick check*: Can the model accurately answer questions about Ottoman history or modern Turkish cultural figures?

- **Cross-lingual transfer**: Understanding how multilingual models leverage knowledge from other languages
  - *Why needed*: Many evaluated models are trained on multiple languages, affecting their Turkish performance
  - *Quick check*: Does performance on Turkish tasks correlate with performance on similar tasks in other languages?

## Architecture Onboarding
- **Component map**: Input prompt → Model inference → Task-specific evaluation → Performance aggregation → Benchmark scoring
- **Critical path**: Prompt design → Model inference → Evaluation metric computation → Result normalization across tasks
- **Design tradeoffs**: Broad task coverage vs. depth of evaluation in each task type; cultural specificity vs. generalizability to other languages
- **Failure signatures**: Poor performance on morphological tasks indicates insufficient Turkish language modeling; low scores on cultural knowledge tasks suggest lack of culturally relevant training data
- **3 first experiments**: 1) Compare model performance on Turkish-specific vs. general language tasks, 2) Test model sensitivity to prompt variations in Turkish, 3) Evaluate zero-shot vs. few-shot performance on grammatical error correction

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark representativeness may not fully capture all aspects of Turkish language understanding and cultural knowledge
- Model evaluation scope excludes closed-source commercial models like GPT-4 and Claude
- Cultural bias may exist in task design and source materials for culturally specific tasks
- Limited cross-lingual comparison prevents assessment of relative performance gains from Turkish-focused fine-tuning

## Confidence
- High confidence: Task discrimination findings (grammatical error correction, machine translation, and extractive QA being most effective for differentiation)
- Medium confidence: Claims about general-purpose models outperforming Turkish-specific instruction-tuned models
- Medium confidence: Claims about Cere-Llama-3-8B's superior performance on specific tasks

## Next Checks
1. Test CETVEL with newer model families and closed-source models (GPT-4, Claude) to verify whether observed performance patterns hold across a broader range of architectures
2. Conduct detailed error analysis on grammatical error correction and Turkish history QA tasks to understand systematic failure modes and whether they reflect model limitations or benchmark design issues
3. Design controlled experiments to assess whether models trained on CETVEL tasks demonstrate improved cultural knowledge transfer to novel Turkish cultural contexts not present in the benchmark