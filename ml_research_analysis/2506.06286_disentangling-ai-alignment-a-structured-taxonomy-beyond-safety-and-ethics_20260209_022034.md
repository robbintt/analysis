---
ver: rpa2
title: 'Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics'
arxiv_id: '2506.06286'
source_url: https://arxiv.org/abs/2506.06286
tags:
- alignment
- safety
- normative
- moral
- ethicality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops a structured taxonomy for understanding AI
  alignment, distinguishing three key dimensions: the normative aim (e.g., safety,
  ethicality, legality), scope (outcome vs. execution), and constituency (individual
  vs.'
---

# Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics

## Quick Facts
- arXiv ID: 2506.06286
- Source URL: https://arxiv.org/abs/2506.06286
- Reference count: 25
- Primary result: This paper develops a structured taxonomy for understanding AI alignment, distinguishing three key dimensions: the normative aim (e.g., safety, ethicality, legality), scope (outcome vs. execution), and constituency (individual vs. collective).

## Executive Summary
This paper challenges the prevailing monolithic conception of AI alignment by proposing a structured taxonomy that disentangles the concept into three orthogonal dimensions: normative aim, scope, and constituency. The author argues that current AI alignment discourse overly focuses on alignment goals while obscuring the structural complexity of the problem. By treating alignment as a parameterized property with distinct dimensions, the framework clarifies how different alignment aims interact and potentially conflict. The paper also examines the relationship between safety and ethicality, concluding that while ethicality implies safety, the reverse does not hold.

## Method Summary
This is a conceptual/philosophical paper that develops a structured taxonomy through definitional analysis, case-based reasoning, and conceptual distinction-drawing. The method involves creating illustrative thought experiments (Cases 1-5) to demonstrate how different configurations of the three-dimensional alignment framework capture various alignment scenarios. No empirical experiments or data analysis are conducted.

## Key Results
- Alignment can be treated as a parameterized property with gradations rather than a binary state
- Ethical alignment implies safety, but safety does not imply ethical alignment
- The three orthogonal dimensions (aim, scope, constituency) can be independently configured but must be jointly satisfied for all-things-considered alignment

## Why This Works (Mechanism)

### Mechanism 1: Parameterized Alignment Definition
- Claim: Alignment can be treated as a parameterized property with gradations rather than a binary state.
- Mechanism: By defining X,Y-alignment as a "strictly monotonically increasing function of the proportion of behavior consistent with standard X under Y-relevant contexts," the framework allows partial alignment to be measured and compared across configurations.
- Core assumption: Different alignment standards impose requirements that can be approximately satisfied even when perfect conformity is impossible.
- Evidence anchors:
  - [abstract]: "treating alignment as a parameterized property with distinct dimensions"
  - [Section 3.1]: Definitions 10-13 establish degrees from "perfectly" to "sufficiently" aligned
  - [corpus]: Limited direct validation; neighboring papers focus on safety-ethics unification but not parameterization specifically
- Break condition: If normative standards are incommensurable or mutually exclusive, the function may not be well-defined.

### Mechanism 2: Ethicality-Safety Implication
- Claim: Ethical alignment implies safety, but safety does not imply ethical alignment.
- Mechanism: Ethicality requires conforming to moral standards under intended application contexts; safety requires avoiding harmful malfunctions in those same contexts. Since ethical design decisions inherently account for preventing unjustified harm, ethical behavior subsumes safety requirements.
- Core assumption: Ethical AIAs result from intentional design targeting intended application contexts, not accidental byproducts.
- Evidence anchors:
  - [Section 2.4]: Case 2 (CritIs) shows safe but unethical AIAs; Case 4 (GoodBots) illustrates justified harm without malfunction
  - [Section 2.4]: "it is at least a practical regularity that ethicality implies safety"
  - [corpus]: "Mind the Gap!" paper discusses safety-ethics unification pathways, supporting the relevance of this distinction
- Break condition: If an AIA acts ethically through unreliable or stochastic means that occasionally cause unintended harm, ethicality may not guarantee safety.

### Mechanism 3: Orthogonal Structural Dimensions
- Claim: Alignment aim, scope, and constituency are orthogonal dimensions that can be independently configured but must be jointly satisfied for all-things-considered alignment.
- Mechanism: Decomposing alignment into three axes (aim: safety/ethicality/legality; scope: outcome/execution; constituency: individual/collective) reveals legitimate configurations that may conflict—e.g., individual outcome alignment can coexist with collective ethical execution misalignment.
- Core assumption: No single dimension automatically subsumes or resolves the others.
- Evidence anchors:
  - [Section 3.2]: Case 5 (Anniversary Dinner) demonstrates how outcome alignment with individual constituency can conflict with execution alignment at collective level
  - [Section 3.1]: Case 1 shows moral alignment alone neglects user intent
  - [corpus]: "The Autonomy-Alignment Problem" paper formalizes similar structural decomposition for open-ended learning
- Break condition: If one dimension (e.g., ethicality) theoretically overrides all others, the taxonomy collapses to a single privileged aim.

## Foundational Learning

- Concept: **Normative Pluralism vs. Value Pluralism**
  - Why needed here: The paper explicitly distinguishes these—value pluralism concerns competing values within one domain; normative pluralism concerns simultaneous demands across different domains (morality, legality, prudence).
  - Quick check question: Can you explain why an AIA aligned with utilitarianism might still violate user intent alignment?

- Concept: **Outcome vs. Execution Alignment**
  - Why needed here: This distinction determines whether alignment is evaluated by results produced or by the process/method used. Ethical alignment requires both.
  - Quick check question: If an AIA secures a restaurant reservation by bribing staff, is it outcome-aligned, execution-aligned, both, or neither?

- Concept: **Instrumental Effectiveness as Baseline Normative Aim**
  - Why needed here: The paper positions instrumental success (achieving set goals) as a necessary but insufficient normative expectation—without it, the agent isn't even functional.
  - Quick check question: Why does the definition of AIA include instrumental success as a normative requirement rather than purely technical?

## Architecture Onboarding

- Component map:
  - Alignment Aim Selector -> Standard Instantiator -> Scope Evaluator -> Constituency Resolver -> Integration Layer

- Critical path:
  1. Identify all applicable alignment aims for deployment context
  2. For each aim, specify concrete standard X and constituency
  3. Evaluate both outcome and execution alignment per configuration
  4. Detect cross-dimensional conflicts (e.g., individual intent vs. collective ethics)
  5. Apply resolution strategy or escalate to governance mechanism

- Design tradeoffs:
  - **Explicit vs. Implicit Ethical Alignment**: Explicit ME architectures enable transparent moral reasoning but increase complexity; implicit alignment via RLHF is simpler but fragile under distribution shift
  - **Individual vs. Collective Priority**: Prioritizing user intent enables personalization but risks social harm; prioritizing collective standards ensures public justifiability but may frustrate users
  - **Perfect vs. Sufficient Alignment**: Pursuing perfect alignment may be logically impossible when standards conflict; sufficient alignment requires context-dependent thresholds

- Failure signatures:
  - **Aim Collapse**: Treating safety as equivalent to ethicality, leading to unethical-but-safe systems
  - **Scope Neglect**: Optimizing outcomes while ignoring execution methods (e.g., reward hacking)
  - **Constituency Mismatch**: Aligning to individual preferences that violate collective norms
  - **Standard Underspecification**: Claiming "ethical alignment" without defining which moral standard X

- First 3 experiments:
  1. **Cross-Dimensional Stress Test**: Deploy simulated AIA in scenarios where individual outcome alignment conflicts with collective ethical execution alignment; measure resolution behavior
  2. **Standard Sensitivity Analysis**: For a fixed task, evaluate AIA behavior under different moral standards X (utilitarian, deontological, contractualist); catalog divergence
  3. **Scope Coverage Audit**: For existing "aligned" systems, separately evaluate outcome and execution alignment to identify hidden misalignments in process

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can conflicts between competing alignment aims (e.g., moral demands vs. user intent vs. legality) be systematically resolved to achieve all-things-considered alignment?
- Basis in paper: [explicit] The paper states it is "neither obvious that one of these alignment aims should take precedence over all others, nor is it clear what exactly it would mean for an AIA to be aligned all-things-considered."
- Why unresolved: The taxonomy identifies multiple legitimate normative aims but provides no decision procedure for cases where they generate mutually exclusive requirements.
- What evidence would resolve it: A principled framework (e.g., lexical ordering, threshold deontological constraints, or deliberative procedures) for prioritizing or reconciling conflicting aims across representative cases.

### Open Question 2
- Question: Can the structural taxonomy be translated into technical architectures that jointly optimize across all three dimensions (aim, scope, constituency)?
- Basis in paper: [explicit] The conclusion calls for research to "operationalize these conceptual insights through concrete normative theorizing, technical architectures, institutional frameworks, and public governance mechanisms."
- Why unresolved: The paper is conceptual; it does not specify how these orthogonal dimensions should be implemented or jointly optimized in AI systems.
- What evidence would resolve it: Demonstrations of architectures that can represent and balance these dimensions independently (e.g., modularity separating intent-tracking from moral reasoning, or explicit constituency modeling).

### Open Question 3
- Question: What threshold defines "sufficient" alignment across different contexts and normative standards, and who determines this?
- Basis in paper: [inferred] The paper defines "sufficiently X-aligned" AIAs but notes determinacy is "context-dependent and depends on the specific moral standard," and "what counts as sufficient safety" is "inherently context-dependent."
- Why unresolved: The definitions are intentionally flexible, leaving open how to set operational thresholds without purely subjective judgment.
- What evidence would resolve it: Empirical studies of stakeholder expectations across contexts, or formal models specifying thresholds relative to risk profiles, uncertainty types, or governance requirements.

## Limitations

- The taxonomy does not specify mechanisms for resolving conflicts between dimensions
- No concrete guidance on setting operational thresholds for "sufficient" alignment
- The framework is underspecified regarding which concrete normative standards to use for different aims

## Confidence

- High Confidence: The structural decomposition of alignment into aim, scope, and constituency dimensions is well-supported by clear examples and logical reasoning
- Medium Confidence: The ethicality-safety implication (ethicality implies safety but not vice versa) is supported by illustrative cases but relies on assumptions about intentional design
- Low Confidence: The claim that alignment can be treated as a parameterized property with gradations lacks empirical validation

## Next Checks

1. Conduct a systematic classification of documented AI incidents using the three-dimensional taxonomy to test whether the framework captures alignment failures better than monolithic approaches
2. Design and execute scenario-based experiments where simulated AIAs must resolve conflicts between individual outcome alignment and collective ethical execution alignment, measuring resolution strategies and their outcomes
3. Develop concrete instantiation guidelines for normative standards X across different aims Y (e.g., which moral framework to use for ethicality, which regulatory framework for legality) and validate them through expert review