---
ver: rpa2
title: 'OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative
  Preference Alignment'
arxiv_id: '2502.18965'
source_url: https://arxiv.org/abs/2502.18965
tags:
- preference
- onerec
- conference
- generative
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OneRec, the first end-to-end generative recommender
  system that unifies retrieval and ranking into a single model. The key innovation
  is a session-wise generative approach using an encoder-decoder architecture with
  sparse Mixture-of-Experts (MoE) scaling, combined with Iterative Preference Alignment
  (IPA) using a personalized reward model for direct preference optimization.
---

# OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment

## Quick Facts
- arXiv ID: 2502.18965
- Source URL: https://arxiv.org/abs/2502.18965
- Authors: Jiaxin Deng; Shiyao Wang; Kuo Cai; Lejian Ren; Qigen Hu; Weifeng Ding; Qiang Luo; Guorui Zhou
- Reference count: 40
- Primary result: 1.6% increase in watch-time compared to existing multi-stage system on Kuaishou platform

## Executive Summary
OneRec introduces the first end-to-end generative recommender system that unifies retrieval and ranking into a single model. The system generates recommendation sessions through session-wise autoregressive generation using an encoder-decoder architecture with sparse Mixture-of-Experts scaling. The key innovation is Iterative Preference Alignment (IPA) using a personalized reward model for direct preference optimization, which replaces the traditional cascaded ranking pipeline. Deployed on Kuaishou's short video platform, OneRec achieved significant improvements in user engagement metrics, demonstrating that a single generative model can effectively handle both retrieval and ranking tasks while maintaining computational efficiency through MoE.

## Method Summary
OneRec operates as a session-wise generative recommender that predicts sequences of semantic item IDs rather than ranking from a fixed corpus. The model uses a two-stage training approach: first, NTP pretraining on session-wise data with an encoder-decoder Transformer architecture enhanced with sparse MoE layers (24 experts, top-2 routing); second, Iterative Preference Alignment using a pretrained reward model to score beam search outputs and generate preference pairs for DPO training with a 1% sample ratio. Items are tokenized using hierarchical codebooks created via balanced K-means clustering (3 layers, 8192 clusters each), and the model generates sessions of 5 videos conditioned on 256-item user history sequences.

## Key Results
- Achieved 1.6% increase in Total Watch Time compared to existing multi-stage system on Kuaishou platform
- Session-wise generation outperformed point-wise baseline (TIGER) by 1.78% in maximum session watch time
- Demonstrated consistent scaling benefits with increased parameters, from 0.05B to 1B total parameters
- Limited DPO training (1% sample ratio) provided significant gains while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1: Session-wise Autoregressive Generation
Generating a list of items as a coherent session outperforms predicting items one at a time. The model decodes multiple items sequentially within a session using a unified context, allowing it to capture dependencies between items. This replaces the need for hand-crafted re-ranking rules to combine point-wise predictions. The quality of a recommendation session depends on the contextual relationship between items, not just individual relevance.

### Mechanism 2: Sparse Mixture-of-Experts (MoE) Scaling
Increasing model capacity via sparse MoE improves recommendation quality without proportionally increasing inference cost. The decoder's feed-forward networks are replaced with MoE layers, activating only a small subset of "expert" modules per token. This allows massive parameter count (1B total, 130M activated) for better expressiveness while keeping FLOPs low because only a few experts are computed per forward pass. User interest patterns are diverse and can be specialized, making them amenable to routing to different expert subsets.

### Mechanism 3: Iterative Preference Alignment (IPA) with Self-hard Negative Sampling
A small fraction (1%) of training on personalized preference pairs, generated from the model's own beam search outputs, significantly improves results. A reward model scores multiple candidate sessions generated by the current OneRec model. The highest-scoring session is the "chosen" positive and the lowest-scoring is the "rejected" negative. This creates a preference pair for Direct Preference Optimization. This overcomes the lack of explicit human preference labels in recommender systems by using a pre-trained reward model as a reliable proxy for user preference.

## Foundational Learning

### Concept: Autoregressive Decoding with Semantic Tokenization
**Why needed here:** OneRec does not rank items from a fixed corpus. It *generates* them by predicting their semantic ID token-by-token. Understanding how this sequence generation works is fundamental to understanding the entire model.
**Quick check question:** How does the model generate a new video recommendation: by calculating a similarity score or by predicting the next token in a sequence?

### Concept: Direct Preference Optimization (DPO)
**Why needed here:** This is the core post-training technique used to align the model with user preferences. It replaces complex reinforcement learning by re-framing the optimization problem using a reward function derived from preference data.
**Quick check question:** What two pieces of data are needed to construct a single training example for DPO?

### Concept: Inference with Key-Value (KV) Caching
**Why needed here:** For efficient real-time deployment, the model reuses computed keys and values from previous tokens during autoregressive generation. The paper mentions this optimization is critical for reducing GPU memory overhead.
**Quick check question:** During generation, why is it more efficient to cache and reuse the KV tensors from previous steps rather than recomputing them?

## Architecture Onboarding

### Component map:
User behavior sequence (256 items) -> Encoder (Transformer) -> Decoder (Transformer with MoE) -> Output Head -> Sequence of semantic IDs (5 videos)

### Critical path:
The data processing for IPA is the most complex part. The loop is: current model -> beam search -> reward model scoring -> preference pair selection -> DPO model update. Breaks here affect alignment quality.

### Design tradeoffs:
- **DPO Sample Ratio:** The paper finds 1% is sufficient and cost-effective (95% of max performance at 20% of the cost of higher ratios). Increasing this trades off compute for marginal gains.
- **Beam Search Size:** The paper uses a beam size of 128 for inference. This improves generation quality by exploring more candidates but increases latency.
- **Codebook Granularity:** The choice of 3 layers with 8192 clusters each creates the semantic ID vocabulary. This balances the expressiveness of the item space against the complexity of the generation task.

### Failure signatures:
- **Unbalanced Code Distribution:** If the residual quantization for semantic IDs is not balanced, the model's cross-entropy loss can become unstable (the "hourglass phenomenon").
- **Reward Hacking:** If the Reward Model used in IPA is flawed, the OneRec model will learn to generate sessions that score high on the RM but are not actually preferred by users.
- **Latency Spikes:** If the MoE router becomes unbalanced (too many tokens going to one expert), the parallelism advantage is lost, increasing inference time.

### First 3 experiments:
1. **Validate Semantic ID Integrity:** Generate recommendations and map the output semantic IDs back to real video items. Check for a high success rate of valid mappings and diversity in the retrieved items.
2. **Profile MoE Activation:** Run inference and log the expert activation patterns. Verify that the router is distributing tokens across experts as expected (avoiding expert collapse) and that only ~13% of parameters are active.
3. **Ablate IPA:** Train a baseline model without IPA and compare its session watch time (swt) and interaction (ltr) metrics against the full model on a held-out test set to quantify the contribution of the preference alignment stage.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can the generative framework be adapted to jointly optimize conflicting metrics, such as maximizing watch-time while simultaneously improving interaction rates (likes/comments)?
**Basis in paper:** The Conclusion states, "our model has limitations in interactive indicators, such as likes," and identifies "multi-objective modeling" as a key area for future research.
**Why unresolved:** The current Iterative Preference Alignment (IPA) optimizes for a unified reward dominated by watch-time, leading to a performance gap in interaction metrics compared to the significant gains in watch-time.
**What evidence would resolve it:** A framework that successfully balances multi-objective rewards (e.g., weighted watch-time and interaction scores) without sacrificing the coherence of the generated session.

### Open Question 2
**Question:** How does the static K-means semantic tokenization strategy impact the model's ability to handle the cold-start problem and semantic drift in a dynamic item corpus?
**Basis in paper:** Section 3.1 describes the construction of codebooks using Balanced K-means on a specific item set $V$, but does not detail how new items or changing content are indexed in real-time.
**Why unresolved:** Industrial video platforms have a high turnover of content; a static codebook risks mis-indexing new items or requiring computationally expensive re-clustering.
**What evidence would resolve it:** An analysis of OneRec's performance specifically on newly uploaded items, or the introduction of an efficient online codebook update mechanism.

### Open Question 3
**Question:** To what extent does the accuracy of the pre-trained Reward Model (RM) limit the effectiveness of the Iterative Preference Alignment (IPA) module?
**Basis in paper:** Section 3.3 relies entirely on the pre-trained RM to simulate user preferences and select "chosen" vs. "rejected" samples for DPO.
**Why unresolved:** If the Reward Model fails to capture nuanced user preferences (e.g., serendipity vs. redundancy), the IPA may reinforce sub-optimal generation patterns or bias the model.
**What evidence would resolve it:** An ablation study measuring the correlation between the Reward Model's alignment accuracy and the final quality of the generated recommendations.

## Limitations
- Evaluation conducted primarily on Kuaishou's short video platform, raising questions about transfer to other domains
- Offline metrics (watch time, view rate, follow rate, like rate) are proxies for user satisfaction that may not capture long-term engagement or diversity
- IPA approach relies heavily on reward model's ability to simulate user preferences, creating potential for reward hacking
- Lacks direct comparisons with other state-of-the-art recommender systems on the same dataset

## Confidence

**High Confidence:** Session-wise generation mechanism is well-supported by ablation studies showing 1.78% gains over point-wise baselines, and sparse MoE scaling demonstrates consistent parameter scaling benefits across multiple model sizes with clear computational advantages.

**Medium Confidence:** IPA methodology shows strong empirical results, but reliance on pretrained reward model as proxy for user preferences introduces uncertainty. Success depends heavily on quality and generalization of reward model.

**Low Confidence:** Claims about superiority of generative approaches over traditional retrieval-rank pipelines are based on single platform data. Paper doesn't provide extensive ablation studies on semantic tokenization approach or explore alternative tokenization strategies.

## Next Checks

1. **Cross-Domain Generalization Test:** Deploy OneRec on a different content platform (e.g., e-commerce or news recommendation) and measure performance degradation compared to Kuaishou. This would validate whether session-wise generation and IPA mechanisms generalize beyond short video content.

2. **Reward Model Robustness Analysis:** Conduct systematic experiments varying reward model architecture, training data distribution, and objective functions to identify failure modes where IPA optimization diverges from actual user satisfaction. This should include both offline simulation and controlled A/B testing.

3. **Long-term Engagement Study:** Track user cohorts exposed to OneRec recommendations over extended periods (3-6 months) to measure retention, diversity of consumption patterns, and potential feedback loops. This would validate whether immediate watch time improvements translate to sustainable platform engagement.