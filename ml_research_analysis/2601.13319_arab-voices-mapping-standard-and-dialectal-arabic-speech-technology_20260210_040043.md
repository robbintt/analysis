---
ver: rpa2
title: 'Arab Voices: Mapping Standard and Dialectal Arabic Speech Technology'
arxiv_id: '2601.13319'
source_url: https://arxiv.org/abs/2601.13319
tags:
- arabic
- speech
- dataset
- datasets
- dialect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Arab Voices, a standardized framework for
  evaluating and benchmarking dialectal Arabic speech recognition systems. The authors
  curate and harmonize 31 heterogeneous datasets spanning 14 Arabic dialects, resolving
  inconsistencies in dialect labeling and metadata.
---

# Arab Voices: Mapping Standard and Dialectal Arabic Speech Technology

## Quick Facts
- arXiv ID: 2601.13319
- Source URL: https://arxiv.org/abs/2601.13319
- Reference count: 40
- Primary result: Introduces a standardized benchmark for 14 Arabic dialects with curated datasets and evaluation of 14 ASR models

## Executive Summary
This paper introduces Arab Voices, a comprehensive framework for evaluating and benchmarking dialectal Arabic speech recognition systems. The authors address the challenge of inconsistent dialect labeling and metadata across 31 heterogeneous datasets by harmonizing them to ISO 639-3 language codes plus country-region tags. They characterize each dataset using automated measures of dialectness (ALDi) and audio quality, then establish a standardized benchmark with per-dialect adaptation splits. Benchmarking of recent ASR models reveals that LLM-based architectures (Omnilingual LLM 7B, Qwen3) outperform traditional CTC and encoder-decoder approaches, particularly for underrepresented varieties, while highlighting remaining challenges for North African and Iraqi dialects.

## Method Summary
The methodology involves curating and harmonizing 31 heterogeneous Arabic speech datasets spanning 14 dialects, resolving inconsistencies in dialect labeling and metadata through ISO-639-3 language codes plus country-region tags. The authors characterize each dataset using automated measures of dialectness (ALDi metric based on MARBERT regression) and audio quality (PESQ, STOI, SI-SDR, NMR-MOS). A standardized benchmark is established with 5-hour adaptation splits, 1-hour development splits, and 1-hour test splits per dialect. The evaluation involves zero-shot testing of 14 pretrained ASR models across three architectures (CTC encoder-only, encoder-decoder, multimodal LLM) with text normalization including diacritic removal and orthographic unification.

## Key Results
- Omnilingual LLM 7B achieves WER of 23.53% on Levantine Arabic (apc) and 34.8% on Egyptian Arabic (arz)
- Qwen3 achieves WER of 28.8% on Egyptian Arabic (arz)
- Whisper-Large-v3 achieves WER of 32.53% on Levantine Arabic (apc) and 35.69% on Egyptian Arabic (arz)
- LLM-based architectures consistently outperform CTC and encoder-decoder models, particularly for underrepresented dialects
- MGB2 contains 24.5% of utterances with ALDi >0.11 (little DA), 5.6% exceeding 0.44 (mixed), and 1% exceeding 0.77 (mostly DA)

## Why This Works (Mechanism)

### Mechanism 1: Text-Based Dialectness Quantification Enables Dataset Selection
A MARBERT-based regression model produces graded dialectness scores (0-1) that aggregate to dataset-level distributions, enabling principled selection and filtering of dialect-focused training data. Extreme distributions flag corpora mislabeled as dialectal Arabic that are actually MSA-dominant, allowing targeted filtering before model training.

### Mechanism 2: ISO-Country Tag Harmonization Reduces Cross-Dataset Fragmentation
Aligning heterogeneous dialect labels to standardized ISO 639-3 + country-region codes enables reproducible pooling and cross-dialect transfer experiments. Datasets with ambiguous labels (e.g., "Maghrebi") are excluded from the benchmark, reducing noise in evaluation.

### Mechanism 3: LLM-Based ASR Architectures Outperform CTC on Dialectal Varieties
Multimodal speech+LLM models achieve lower WER on dialectal Arabic than encode-only CTC or encoder-decoder models, particularly where training data is sparse and dialectal lexical variation is high. The LLM component provides stronger language modeling priors that compensate for acoustic uncertainty.

## Foundational Learning

- **Arabic Dialect Continuum vs. MSA**: Understanding that Arabic exists on a continuum is essential for interpreting "dialectness" scores. Quick check: Given an utterance with ALDi score 0.3, would you classify it as MSA, mixed, or dialectal? (Answer: "Little DA" per Keleg et al. bins: 0.11-0.44)

- **ISO 639-3 Language Codes**: The harmonization strategy relies on ISO codes (e.g., arz for Egyptian, apc for Levantine). Quick check: Which ISO code would you assign to a speaker from Casablanca? (Answer: ary for Moroccan Arabic)

- **No-Reference Audio Quality Metrics (PESQ, STOI, SI-SDR)**: The paper uses TorchAudio-SQUIM to predict quality scores. Quick check: A telephone recording has PESQ 1.6 but NMR-MOS 3.7—what does this suggest? (Answer: Low acoustic fidelity but subjectively acceptable; likely natural conversational speech)

## Architecture Onboarding

- Component map: Raw DA Datasets → Audio: 16kHz, mono, PCM → Text: normalization, Buckwalter conversion → Metadata: ISO+Country tags → Characterization: ALDi + SQUIM → Benchmark: 5h adapt / 1h dev / 1h test per dialect

- Critical path: 1) Download original datasets from sources (scripts provided, not data itself due to licensing) 2) Run preprocessing to generate parquet files with unified schema 3) Validate dialect labels via ALDi distributions before benchmark use 4) Run zero-shot evaluation on dev splits first to detect failure modes

- Design tradeoffs: Granularity vs. Coverage (ISO+country codes enable fine-grained organization but exclude ambiguous-label datasets), Text vs. Audio Dialectness (ALDi is reliable for transcripts but misses pronunciation-based markers), Objective vs. Perceptual Quality (PESQ/STOI align with each other but not NMR-MOS; expressive speech is systematically penalized)

- Failure signatures: WER >100 on Whisper (generation termination failure), High CER but moderate WER (orthographic normalization issues), ALDi near 0 on dataset labeled "dialectal" (label mismatch or actually MSA content)

- First 3 experiments: 1) Baseline verification: Run Omnilingual CTC-7B and LLM-7B on dev splits for Levantine and Egyptian; confirm WER within ±2% of reported values 2) Dialectness stratification: Partition MGB3 by ALDi quartiles and measure WER degradation as dialectness increases 3) Cross-dialect transfer: Fine-tune on 5h Levantine adaptation split, evaluate zero-shot on Egyptian dev split to measure transferability

## Open Questions the Paper Calls Out

### Open Question 1
Can an audio-based analogue of the Arabic Level of Dialectness (ALDi) metric be developed to quantify dialectal intensity directly from speech signals without relying on text transcripts? The authors note in the "Limitations" section that text-based analysis misses pronunciation differences and that "Developing an audio-based analogue of ALDi ('spoken ALDi') is a promising direction to address this limitation."

### Open Question 2
How do predicted audio-quality measures (e.g., PESQ, SI-SDR) correlate with downstream ASR performance across different dialectal Arabic datasets? The Conclusion explicitly lists "evaluating how predicted audio-quality measures relate to ASR performance in practice" as a direction for future work.

### Open Question 3
To what extent does the integration of dialect-specific language models (LMs) improve the performance of CTC-based ASR architectures for Dialectal Arabic? The Conclusion identifies "developing language models suitable for CTC decoding in Dialectal Arabic" as a necessary future step, noting that a current barrier is obtaining suitable in-domain text.

## Limitations

- Text-based dialectness quantification may underestimate spoken dialectal content when speakers use MSA-consistent orthography without diacritization
- ISO-country harmonization may introduce systematic label noise when speaker origin differs from recording location or when rural-urban variation cuts across ISO boundaries
- Benchmark excludes datasets with ambiguous dialect labels, potentially limiting coverage of low-resource varieties

## Confidence

**High confidence**: The dataset harmonization methodology and benchmark construction are reproducible given access to raw data. The observed WER patterns for established models (Whisper, SeamlessM4T, MMS) align with expectations from related work.

**Medium confidence**: The dialectness scoring methodology shows internal consistency but lacks external validation against human judgments of dialectal intensity. The performance advantages of LLM-based architectures over CTC/encoder-decoder models are observed but require more systematic ablation studies.

**Low confidence**: Claims about specific WER values for Omnilingual LLM 7B and Qwen3 on low-resource dialects, as these models may have had exposure to related training data not fully disclosed in the paper.

## Next Checks

1. **Dialectness Score Validation**: Select 50 utterances each from MGB3 (high ALDi) and MGB2 (low ALDi), have native speakers rate dialectal intensity on a 5-point scale, and compute correlation with ALDi scores to validate the text-based quantification approach.

2. **Cross-Architecture Ablation**: Fine-tune Whisper-large-v3, MMS, and Omnilingual CTC-7B on identical 5-hour adaptation splits for Levantine Arabic, then evaluate on held-out test sets to isolate architecture effects from pretraining data differences.

3. **Metadata Completeness Analysis**: For datasets where speaker location/origin is available, compute the fraction of utterances with complete ISO+country metadata and measure WER degradation when excluding utterances missing this information to quantify the impact of metadata gaps on benchmark fairness.