---
ver: rpa2
title: Explainable Statute Prediction via Attention-based Model and LLM Prompting
arxiv_id: '2512.21902'
source_url: https://arxiv.org/abs/2512.21902
tags:
- case
- statute
- which
- statutes
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of automatic statute prediction,
  where the goal is to predict relevant statutes for a given case description. The
  authors propose two techniques for this task: AoS (Attention-over-Sentences) and
  LLMPrompt.'
---

# Explainable Statute Prediction via Attention-based Model and LLM Prompting

## Quick Facts
- **arXiv ID:** 2512.21902
- **Source URL:** https://arxiv.org/abs/2512.21902
- **Reference count:** 33
- **Primary result:** AoS model achieves 0.486 F1 score for micro-averaged F1 on the ILSI dataset

## Executive Summary
This paper addresses automatic statute prediction, where the goal is to predict relevant legal statutes for a given case description. The authors propose two techniques: AoS (Attention-over-Sentences), which uses attention over sentences in a case description to predict statutes, and LLMPrompt, which prompts a large language model to predict as well as explain relevance of a certain statute. The AoS model outperforms most baselines across both datasets for statute prediction, while the explanations provided by the AoS model are evaluated in a counter-factual manner and found to be of good quality. The LLMPrompt technique did not perform as well for statute prediction, but the explanations generated by it are found to be of good quality when evaluated manually.

## Method Summary
The paper proposes two techniques for statute prediction: AoS (Attention-over-Sentences) and LLMPrompt. AoS uses per-statute multi-head attention over sentence embeddings to predict statute relevance, where query vectors are derived from statute embeddings and keys from case sentence embeddings. LLMPrompt uses zero-shot LLM prompting with statute content provided in-context and Chain-of-Thought reasoning. Both models operate on sentence-segmented case descriptions with numeric values masked to prevent label leakage. The AoS model is trained with per-statute binary classifiers using cross-entropy loss, while LLMPrompt relies on the LLM's reasoning capabilities without fine-tuning.

## Key Results
- AoS model achieves 0.486 F1 score for micro-averaged F1 on the ILSI dataset
- AoS model achieves 0.774 F1 score for micro-averaged F1 on the ECtHR B dataset
- AoS model's explanations have a Necessity Factor of 0.430 and Sufficiency Factor of 0.709 for the ILSI dataset
- LLMPrompt's explanations receive an average score of 1.75-1.85 for true positive explanations and 1.94-1.95 for true negative explanations when evaluated manually

## Why This Works (Mechanism)

### Mechanism 1: Label-Specific Attention Using Statute Content as Query
Computing attention weights where statute embeddings serve as query vectors enables the model to identify which case sentences are most relevant for each specific statute, simultaneously improving prediction and providing extractive explanations. For each statute, a query vector is derived from the statute's Sentence-BERT embedding, and scaled dot-product attention with case sentence embeddings produces weights indicating sentence relevance. The weighted sentence combination forms a statute-specific case representation for binary classification. This works because semantic similarity between case facts and statute content text is the primary signal for determining legal relevance.

### Mechanism 2: Sentence-BERT Embeddings Capture Legal Semantic Similarity
Sentence-BERT embeddings, pre-trained on semantic similarity tasks, provide better representations for matching case facts to statute content than BERT [CLS] tokens. Sentence-BERT produces 768-dim embeddings where semantically similar texts have higher cosine similarity, enabling attention to operate on semantic rather than lexical overlap. This works because legal relevance can be approximated through semantic similarity without explicit legal reasoning.

### Mechanism 3: LLM Prompting with In-Context Statute and CoT Reasoning
Zero-shot LLM prompting produces lower prediction accuracy but higher-quality natural language explanations when statute content is provided in-context and Chain-of-Thought reasoning is elicited. Prompts include statute content text, summarized case description, and explicit instruction to first identify "common aspects" before predicting applicability. This works because LLMs possess sufficient general reasoning capability to perform legal relevance assessment when given both texts, without domain-specific training.

## Foundational Learning

- **Concept: Multi-label classification with label-specific attention**
  - Why needed here: Each case can have multiple relevant statutes; standard document classification treats labels independently or uses shared representations. Label-specific attention allows the model to learn different sentence importance patterns for each statute.
  - Quick check question: Given a case about forgery, would you expect the same sentences to be relevant for Section 465 (general forgery) vs Section 467 (forgery of special documents)?

- **Concept: Counter-factual explanation evaluation (Necessity/Sufficiency)**
  - Why needed here: No gold-standard explanations exist in the datasets. Counter-factual metrics measure whether explanation sentences are actually used by the model: Necessity Factor (NF) tests if removing them changes predictions; Sufficiency Factor (SF) tests if they alone produce the same predictions.
  - Quick check question: If an explanation has high SF (0.902) but low NF (0.122), what does this tell you about how the model uses those sentences?

- **Concept: Label leakage and input contamination**
  - Why needed here: Both datasets contain case descriptions that explicitly mention statute numbers (e.g., "offence under Sections 498A, 506B, 324"). Models can exploit this without learning actual legal reasoning. The authors mask all numeric values to prevent this.
  - Quick check question: Why is masking all numbers a conservative strategy, and what tradeoff does it introduce?

## Architecture Onboarding

- **Component map:**
  Case Description → Sentence Segmentation → Sentence-BERT Embeddings
                                                    ↓
  Statute Contents → Sentence-BERT Embeddings → Query Vectors (per statute)
                                                    ↓
                                    Multi-head Attention (statute-specific)
                                                    ↓
                              Weighted Sentence Combination → Binary Classifier
                                                    ↓
                              Prediction + Top-k Sentences (explanation)

  For LLMPrompt:
  Case Description → Extractive Summarizer (25 sentences) → Prompt Construction
                                                              ↓
  AoS Top-K Predictions → Per-statute Prompt → LLM → Prediction + Explanation

- **Critical path:** The attention weight computation (Equations 3-6) is the core mechanism. Query vectors derived from statute embeddings determine which sentences are attended to. If statute embeddings are poor quality or statutes are semantically similar, attention produces uninformative weights.

- **Design tradeoffs:**
  - Sentence-BERT vs. LegalBERT [CLS]: Sentence-BERT better for semantic matching; LegalBERT may capture legal domain nuances but [CLS] tokens are not optimized for similarity
  - Number of attention heads (H): More heads capture diverse aspects but increase parameters. Paper uses H=3.
  - LLMPrompt case summarization to 25 sentences: Required for context window limits but may exclude critical facts
  - Top-K statutes for LLMPrompt (K=5 for ILSI, K=3 for ECtHR B): Reduces LLM calls but may exclude true positives not in AoS top-K

- **Failure signatures:**
  - Low per-statute F1 correlated with: (1) few training examples, (2) high semantic similarity to other statutes (Figure 6 scatter plot)
  - LLMPrompt false negatives often occur when the correct statute is not in AoS top-K (LLMPrompt never evaluates it)
  - Generic explanations (score=1) occur when LLM paraphrases statute without citing specific case facts

- **First 3 experiments:**
  1. **Reproduce ablation with your own data:** Train AoS with Sentence-BERT vs. LegalBERT [CLS] vs. random statute embeddings on a held-out validation set. Confirm that statute content embeddings contribute to performance (expect ~2-5% F1 drop without them).
  2. **Test attention head sensitivity:** Vary H from 1 to 5 on the validation set. Measure both prediction F1 and explanation quality (Necessity/Sufficiency factors). Check if more heads provide genuinely different attended sentences or redundant patterns.
  3. **Evaluate LLMPrompt with different summarization thresholds:** Compare 15, 25, and 40 sentence summaries for a subset of cases. Measure whether longer context improves LLMPrompt recall without degrading explanation quality or exceeding context limits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an ensemble technique combining the AoS model with LLMPrompt improve statute prediction accuracy while maintaining explainability?
- Basis in paper: [explicit] The authors state, "In future, we plan to work on better ensemble techniques that combine advantages of both – AoS and LLMPrompt as well as ensemble of multiple LLMs."
- Why unresolved: The current study evaluates AoS and LLMPrompt as distinct techniques, noting that AoS excels in prediction while LLMPrompt provides high-quality explanations; it is unknown if their strengths can be integrated.
- What evidence would resolve it: Experimental results showing F1 scores and explanation quality metrics for an ensemble model compared to the standalone AoS and LLMPrompt baselines.

### Open Question 2
- Question: Does fine-tuning Sentence-BERT specifically on legal corpora improve statute prediction performance compared to general-purpose embeddings?
- Basis in paper: [explicit] The authors identify a future direction to "train a version of Sentence-BERT model ... which is adapted for legal domain sentences."
- Why unresolved: The current implementation relies on `all-mpnet-base-v2`, and while ablation shows Sentence-BERT beats LegalBERT [CLS] embeddings, the specific benefit of domain-adapted sentence embeddings remains untested.
- What evidence would resolve it: Ablation studies on the ILSI and ECtHR B datasets comparing the current AoS performance against a version utilizing legal-domain-specific Sentence-BERT embeddings.

### Open Question 3
- Question: Can the AoS architecture be modified to handle evolving statutes (new or amended laws) without requiring retraining on the fixed label set?
- Basis in paper: [inferred] The paper notes the limitation that "the supervised AoS model is trained on a fixed pre-defined set of statutes," suggesting it cannot generalize to new statutes, unlike the LLMPrompt approach.
- Why unresolved: The paper dichotomizes the approaches (supervised vs. zero-shot) and does not propose a mechanism for the attention-based model to dynamically incorporate new legal definitions.
- What evidence would resolve it: A modified AoS architecture that successfully predicts statutes not present in the training data, evaluated on a time-split dataset containing new laws.

## Limitations

- **Label leakage remains possible despite masking:** The paper masks numeric values but not alphanumeric patterns like "Section 498A" or "Article 6", potentially allowing models to learn spurious correlations between specific phrases and labels without genuine legal reasoning.
- **Explanation quality evaluation has inherent bias:** The counterfactual Necessity/Sufficiency metrics measure whether attention attends to relevant sentences, but don't establish whether those sentences truly explain the legal reasoning or merely contain correlated keywords.
- **LLMPrompt methodology introduces cascading errors:** Using AoS top-K predictions to select statutes for LLM prompting creates a circular validation where poor AoS performance limits LLMPrompt's evaluation scope.

## Confidence

- **High confidence:** AoS model architecture and training procedure are clearly specified; ablation studies demonstrate statute content embeddings improve performance; counterfactual explanation metrics are technically sound.
- **Medium confidence:** Prediction performance claims are reproducible given dataset access; explanation quality assessment methodology is reasonable but may not fully capture legal reasoning quality.
- **Low confidence:** LLMPrompt results may be inflated by AoS filtering; manual evaluation sample size may not be representative; potential label leakage not fully addressed.

## Next Checks

1. **Re-run the ablation study with your own implementation** comparing Sentence-BERT vs LegalBERT [CLS] vs random statute embeddings on a held-out validation set to verify the ~10% F1 difference claim.

2. **Audit the datasets for explicit statute mentions** by searching test cases for patterns like "Section \d+", "Article \d+", or "IPC \d+". Calculate the proportion of cases containing such patterns to estimate potential label leakage.

3. **Evaluate LLMPrompt on all statutes without AoS filtering** for a subset of cases to determine whether cascading errors from AoS top-K selection bias the results. Compare prediction accuracy and explanation quality with and without filtering.