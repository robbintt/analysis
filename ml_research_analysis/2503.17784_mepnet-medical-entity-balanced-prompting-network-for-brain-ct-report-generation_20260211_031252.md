---
ver: rpa2
title: 'MEPNet: Medical Entity-balanced Prompting Network for Brain CT Report Generation'
arxiv_id: '2503.17784'
source_url: https://arxiv.org/abs/2503.17784
tags:
- medical
- entity
- learning
- visual
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of biased learning of medical
  entities in brain CT report generation, which leads to repetitive and inaccurate
  reports. The proposed Medical Entity-balanced Prompting Network (MEPNet) leverages
  large language models (LLMs) to fairly interpret diverse medical entities by introducing
  visual embeddings and learning status as enriched clues.
---

# MEPNet: Medical Entity-balanced Prompting Network for Brain CT Report Generation

## Quick Facts
- arXiv ID: 2503.17784
- Source URL: https://arxiv.org/abs/2503.17784
- Reference count: 15
- Key outcome: MEPNet achieves F1 scores of 51.4 and 44.0, and CIDEr scores of 22.3 and 86.6 on BCT-CHR and CTRG-Brain datasets respectively, addressing biased learning of medical entities in brain CT report generation.

## Executive Summary
This paper addresses the problem of biased learning of medical entities in brain CT report generation, which leads to repetitive and inaccurate reports. The proposed Medical Entity-balanced Prompting Network (MEPNet) leverages large language models (LLMs) to fairly interpret diverse medical entities by introducing visual embeddings and learning status as enriched clues. Specifically, Knowledge-driven Joint Attention captures entity patterns using explicit and implicit medical knowledge, while a Learning Status Scorer evaluates entity learning quality. These are integrated into multi-modal prompts to guide LLM text generation, allowing self-adaptation for biased entities. Experiments on BCT-CHR and CTRG-Brain datasets show significant improvements in clinical accuracy and text coherence.

## Method Summary
MEPNet uses a ResNet101 encoder (fine-tuned on CQ500) to extract visual features from 3D brain CT scans. A Knowledge-driven Joint Attention (KJA) module projects entity text features against scan features via cross-attention, then applies Knowledge-Masked Self Attention using an adjacency matrix derived from medical knowledge graphs. A Learning Status Scorer evaluates the discriminative loss for each entity and maps it to a textual status word, which is embedded and concatenated to the visual prompt. These enriched prompts guide a LLaMA3-8B LLM (with LoRA) to generate more balanced and accurate reports. The model is trained on Chinese brain CT datasets with 4-bit quantization and LoRA fine-tuning.

## Key Results
- F1 scores reach 51.4 (BCT-CHR) and 44.0 (CTRG-Brain), significantly outperforming baselines.
- CIDEr scores of 22.3 (BCT-CHR) and 86.6 (CTRG-Brain) demonstrate improved text coherence and clinical accuracy.
- Ablation studies confirm the effectiveness of both Knowledge-driven Joint Attention and Learning Status Embeddings in improving entity balance and report quality.

## Why This Works (Mechanism)

### Mechanism 1: Relation-Grounded Visual Feature Isolation
The Knowledge-driven Joint Attention (KJA) module projects entity text features against scan features via cross-attention, then applies "Knowledge-Masked Self Attention" using an adjacency matrix derived from expert-defined and data-driven medical knowledge. This forces the model to aggregate visual features based on known anatomical relationships, improving entity-specific feature extraction compared to global pooling.

### Mechanism 2: Meta-Cognitive Prompting via Learning Status
A Learning Status Scorer calculates the discriminative loss for each entity at each training step, normalizes it, and maps it to a textual status word (e.g., "limited" to "exceptional"). These words are embedded and concatenated to the visual prompt, theoretically signaling the LLM to pay more attention to poorly learned entities during generation, thus mitigating biased learning.

### Mechanism 3: Visual Embedding Shortcuts vs. Hard Classification
Instead of feeding classified labels into the prompt, MEPNet feeds the visual embedding vector of each entity extracted by the KJA. This forces the LLM to ground its generation in actual image features rather than relying on text priors, reducing the risk of "cheating" and improving report accuracy.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** LoRA freezes main LLM weights and injects trainable rank-decomposition matrices, allowing efficient training on consumer GPUs (2x RTX 3090).
  - **Quick check question:** Does the method update the attention weights of the base LLaMA model directly, or does it update adapter matrices?

- **Concept: Cross-Attention in Vision-Language Models**
  - **Why needed here:** The KJA module relies on cross-attention where Entity Text features query Scan Visual features. Understanding Q, K, and V is vital to grasping how the model "finds" brain lesions in the image.
  - **Quick check question:** In the KJA cross-attention step, do the scan features act as the Query or the Key/Value?

- **Concept: Class Imbalance & Long-tailed Learning**
  - **Why needed here:** The paper addresses "biased learning," a manifestation of class imbalance where some brain regions/lesions are rare.
  - **Quick check question:** Why would a standard cross-entropy loss lead to a "proficient" status for common entities and "limited" for rare ones?

## Architecture Onboarding

- **Component map:** Vision Encoder (ResNet101) -> Scan Features ($V_f$) -> KJA -> Entity Visual Embeddings ($E_e$) -> Status Scorer -> Status Embeddings ($S_e$) -> LLM Decoder (LLaMA3-8B) <- Prompt Constructor (concatenates $V_e$ + $E_e$ + $S_e$)

- **Critical path:** The efficiency of the KJA module is the bottleneck. If the adjacency matrix masking is implemented inefficiently, the attention computation explodes. The status scorer is computed dynamically per batch, so it must be lightweight.

- **Design tradeoffs:**
  - **Precision vs. Recall:** The authors explicitly trade Precision for Recall. Status embeddings encourage the LLM to "cast a wider net," boosting Recall/F1 but lowering Precision due to false positives.
  - **Explicit vs. Implicit Knowledge:** The model weights Explicit ($M_E$) at 0.9 and Implicit ($M_I$) at 0.1, favoring expert rules over learned correlations.

- **Failure signatures:**
  - **Repetitive Reports:** If the status scorer fails to differentiate entities, the prompts become homogeneous, and the LLM collapses to generic, repetitive text.
  - **High "Limited" Status:** If the scanner reports "limited" for all entities at inference time, it indicates the visual encoder failed to extract features, likely due to domain shift in CT scan quality.

- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run the "Baseline" (Table 2) which is just ResNet -> LLM without entity prompts. Verify if it produces generic text.
  2. **Ablation on Status:** Disable the Learning Status Scorer (setting `e` in Table 2). Check if the F1 score drops significantly (it drops from 51.4 to 49.4), proving the value of the dynamic status.
  3. **Category vs. Embedding:** Compare "Category" vs. "Embed" prompts (Table 3). Verify that using hard labels actually lowers performance, confirming the hypothesis that the model needs visual embeddings, not just text labels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the entity-balanced prompting strategy be effectively generalized to other 3D medical imaging modalities with differing spatial complexities?
- Basis in paper: The conclusion states the approach shows "potential of extending the approach to other medical tasks" but currently relies on brain-specific graph knowledge.
- Why unresolved: The Knowledge-driven Joint Attention is designed specifically for brain CT volumetric patterns and anatomical graphs; it is untested on modalities like MRI where tissue contrast and entity definitions differ.
- What evidence would resolve it: Successful application of MEPNet to non-CT 3D datasets (e.g., brain MRI) with comparable F1 and CIDEr scores without restructuring the knowledge graph module.

### Open Question 2
- Question: How can the trade-off between high Recall and lower Precision induced by the Learning Status Embeddings be optimized?
- Basis in paper: Section 4.2 notes that "our model has lower precision, likely because our status embeddings prompt the LLM to generate a broader range of entities."
- Why unresolved: The current mechanism prioritizes coverage of diverse entities to solve the "biased learning" problem, which inherently introduces a risk of over-generation (hallucination) that the paper acknowledges but does not fully solve.
- What evidence would resolve it: Introduction of a refined scoring mechanism or loss function that elevates Precision metrics above the reported ~50.6% while maintaining the high Recall (>68%) on the BCT-CHR dataset.

### Open Question 3
- Question: To what extent does the reliance on a fixed set of $K$ pre-defined medical entities limit the detection of out-of-distribution or rare anomalies?
- Basis in paper: The Methodology section restricts the model to "K entities summarized by Shi et al. (2023)" for the adjacency matrix and status scoring.
- Why unresolved: The "Entity-balanced Prompting" branch cannot generate status embeddings for entities outside the fixed set $E$, potentially causing the model to ignore or misclassify novel pathologies not represented in the explicit or implicit knowledge graphs.
- What evidence would resolve it: Evaluation of the model on test cases containing "unseen" pathological entities to determine if the global entity representation can compensate for the lack of specific status embeddings.

## Limitations

- **Knowledge Graph Dependency:** The model relies on an external medical knowledge graph that is not fully specified, making faithful reproduction challenging.
- **Dynamic Status Scorer Reliability:** The accuracy of the Learning Status Scorer depends on the underlying Medical Entity Classifier, which is not independently validated.
- **Data Quality and Preprocessing:** The CTRG-Brain dataset requires a specific cleaning pipeline that is not detailed, limiting independent validation.

## Confidence

- **High Confidence:** The overall architecture and training setup (ResNet101 encoder, LLaMA3-8B with LoRA, dataset splits, and evaluation metrics) are clearly specified and reproducible. The reported improvements in F1 and CIDEr scores are supported by ablation studies.
- **Medium Confidence:** The core mechanism of Knowledge-driven Joint Attention and its integration with medical knowledge graphs is plausible and well-motivated. However, the effectiveness of the Knowledge-Masked Self Attention and the weighting of explicit vs. implicit knowledge are not independently validated.
- **Low Confidence:** The exact construction of the medical knowledge graph and the preprocessing steps for both datasets are not fully specified, making faithful reproduction challenging.

## Next Checks

1. **Independent Validation of Knowledge Graph Construction:** Reconstruct the medical adjacency matrix using publicly available brain anatomy ontologies (e.g., NeuroNames) and test whether the KJA module still improves entity-specific feature extraction. Compare performance with a random or domain-agnostic graph to isolate the impact of medical knowledge.

2. **Stress Test the Learning Status Scorer:** Intentionally corrupt the Medical Entity Classifier (e.g., by mislabeling a subset of entities) and observe whether the status feedback loop amplifies or corrects these errors. This will reveal the robustness of the status scorer and the LLM's ability to interpret status tokens.

3. **Cross-Dataset Generalization:** Evaluate MEPNet on a non-Chinese brain CT dataset (e.g., CQ500 or public TBI datasets) without retraining the knowledge graph or status scorer. Measure performance degradation to assess the model's reliance on dataset-specific biases and the generalizability of the medical knowledge integration.