---
ver: rpa2
title: Sigmoid Head for Quality Estimation under Language Ambiguity
arxiv_id: '2601.00680'
source_url: https://arxiv.org/abs/2601.00680
tags:
- head
- sigmoid
- quality
- softmax
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of ambiguity-induced underconfidence
  in language model (LM) probability for quality estimation (QE). Standard LMs, trained
  with softmax activation and single-reference targets, spread probability mass across
  multiple valid outputs, misleadingly indicating low output quality.
---

# Sigmoid Head for Quality Estimation under Language Ambiguity

## Quick Facts
- arXiv ID: 2601.00680
- Source URL: https://arxiv.org/abs/2601.00680
- Reference count: 20
- Standard LMs show underconfidence under ambiguity; Sigmoid Head improves quality estimation without labeled data

## Executive Summary
This paper addresses a critical limitation in language model-based quality estimation where standard softmax-based probability estimates become misleading under output ambiguity. When multiple valid outputs exist for a given input, traditional LMs spread probability mass across alternatives, resulting in artificially low confidence scores that fail to reflect true output quality. The authors propose the Sigmoid Head, which adds an unembedding layer with sigmoid activation on top of frozen pretrained models, allowing multiple tokens to receive high probabilities simultaneously and providing more accurate quality signals.

The approach demonstrates superior performance across three distinct NLP tasks (machine translation, paraphrasing, and question answering) compared to both standard softmax heads and existing quality estimation methods like Monte Carlo Sequence Entropy and LLM Self Judge. Notably, the method requires no human-labeled quality data, making it particularly valuable for out-of-domain applications where supervised approaches struggle. Experimental results show the Sigmoid Head not only outperforms unsupervised baselines but also surpasses supervised QE models in out-of-domain settings.

## Method Summary
The Sigmoid Head introduces a novel architecture that addresses ambiguity-induced underconfidence in language model probability estimates. The method adds an unembedding layer with sigmoid activation on top of frozen pretrained models, fundamentally changing how probability is distributed across tokens. During training, a heuristic identifies dominant tokens that should not be penalized as negative samples, allowing the model to assign high probabilities to multiple semantically equivalent outputs. This design enables the system to better capture output quality under ambiguity without requiring any human-labeled quality data, making it robust across diverse domains and tasks.

## Key Results
- Sigmoid Head outperforms standard softmax head and existing QE methods (Monte Carlo Sequence Entropy, LLM Self Judge) across MT, paraphrasing, and QA tasks
- Requires no human-labeled quality data while achieving better correlation with human judgments than supervised QE models in out-of-domain settings
- Provides notably better quality signals by allowing multiple tokens to receive high probabilities simultaneously under ambiguity

## Why This Works (Mechanism)
Standard LMs trained with softmax activation and single-reference targets spread probability mass across multiple valid outputs under ambiguity, creating artificially low confidence scores. The Sigmoid Head overcomes this by using sigmoid activation in an additional unembedding layer, allowing multiple tokens to receive high probabilities simultaneously. This better reflects true output quality when multiple semantically equivalent answers exist. The training process uses a heuristic to exclude dominant tokens from negative sampling, preventing the model from penalizing potentially correct alternatives and enabling it to learn probability distributions that capture genuine quality signals.

## Foundational Learning

**Language Model Probability Estimation**: Understanding how LMs assign probabilities to sequences through softmax activation and the softmax partition function. *Why needed*: Critical for grasping why standard approaches fail under ambiguity. *Quick check*: Verify understanding of how softmax distributes probability mass across vocabulary.

**Quality Estimation in NLP**: The task of automatically assessing translation or generation quality without reference translations. *Why needed*: Provides context for why improved probability estimates matter for real-world applications. *Quick check*: Can you explain the difference between quality estimation and evaluation?

**Negative Sampling in Training**: The practice of selecting which tokens to treat as incorrect during model training. *Why needed*: Essential for understanding the heuristic approach to handling dominant tokens. *Quick check*: Understand how negative sampling affects model behavior during training.

## Architecture Onboarding

**Component Map**: Pretrained LM (frozen) -> Sigmoid Head (unembedding layer + sigmoid activation) -> Quality Score

**Critical Path**: Input sequence -> Pretrained LM embeddings -> Sigmoid Head unembedding -> Sigmoid activation -> Probability distribution -> Quality score calculation

**Design Tradeoffs**: The frozen LM assumption trades fine-tuning flexibility for faster adaptation and domain robustness, while the sigmoid activation trades the normalization constraint of softmax for the ability to assign high probabilities to multiple tokens simultaneously.

**Failure Signatures**: The method may struggle when the heuristic for dominant token exclusion is inaccurate, potentially leading to either underconfidence (if too many tokens are penalized) or overconfidence (if too few are penalized). It may also underperform when semantic equivalence between tokens is not well-captured by the heuristic.

**3 First Experiments**: 1) Compare probability distributions from softmax vs sigmoid head on ambiguous outputs to visualize the difference. 2) Test the impact of different dominant token exclusion thresholds on quality estimation performance. 3) Evaluate correlation with human judgments across varying levels of output ambiguity.

## Open Questions the Paper Calls Out
None

## Limitations
- The dominant token heuristic's effectiveness in identifying semantically equivalent alternatives lacks rigorous validation
- Performance improvements are demonstrated primarily through correlation metrics without examining downstream task performance or user satisfaction
- The frozen LM assumption may limit applicability in scenarios where fine-tuning is desirable or necessary

## Confidence
- **High Confidence**: The underconfidence problem in standard LMs under ambiguity is well-established and the mathematical formulation of the sigmoid head approach is sound
- **Medium Confidence**: Experimental results showing improved correlation metrics are convincing, but lack of ablation studies on the dominant token heuristic and frozen LM assumption introduces uncertainty
- **Low Confidence**: Claims about robustness in out-of-domain settings are based on limited comparisons and require more extensive validation across diverse domains and languages

## Next Checks
1. Conduct ablation studies removing the dominant token heuristic to quantify its specific contribution to performance improvements
2. Evaluate the Sigmoid Head approach on additional languages and domains, particularly low-resource languages where ambiguity patterns may differ significantly
3. Test whether the quality estimates from the Sigmoid Head translate to improved downstream task performance (e.g., better translation selection in multi-engine scenarios) rather than just better correlation metrics