---
ver: rpa2
title: 'Evolution in Simulation: AI-Agent School with Dual Memory for High-Fidelity
  Educational Dynamics'
arxiv_id: '2510.11290'
source_url: https://arxiv.org/abs/2510.11290
tags:
- agents
- memory
- agent
- educational
- experience
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the AI-Agent School (AAS) system, which addresses
  the problem of fragmented teaching process modeling and limitations in simulating
  diverse educational participants. The core method, Zero-Exp, employs a dual memory
  base (experience and knowledge) with short-term and long-term components, enabling
  agents to evolve through situated interactions in simulated school scenarios.
---

# Evolution in Simulation: AI-Agent School with Dual Memory for High-Fidelity Educational Dynamics

## Quick Facts
- arXiv ID: 2510.11290
- Source URL: https://arxiv.org/abs/2510.11290
- Reference count: 12
- Primary result: Dual memory model achieves ROUGE-L scores around 0.51-0.55 and approaches expert ground truth realism over time

## Executive Summary
This paper introduces the AI-Agent School (AAS) system, which simulates realistic educational dynamics through autonomous AI agents in a school environment. The core innovation, Zero-Exp, enables agents to evolve from zero experience to high-fidelity behavior using a dual memory architecture (experience vs. knowledge) with hierarchical salience (short-term vs. long-term). Through an experience-reflection-optimization cycle, agents autonomously update their memory and role settings based on interactions, achieving superior text-based realism compared to baseline approaches.

## Method Summary
The AAS system simulates 50 agents (10 teachers, 40 students) over 5 days in a school environment with 25 areas and class timetables. Agents use a dual memory base (Experience Base for episodic events, Knowledge Base for structured facts) each split into Short-term and Long-term components. Memory retrieval uses vector similarity with cosine distance, prioritizing Short-term Memory. After each interaction, agents execute memory updates and state updates through reflection, modifying their role settings (teaching style, study habits, etc.) autonomously. The system was tested across 9 configurations using GPT-4o and Qwen models, with performance measured against expert-curated ground truth data.

## Key Results
- Full memory model (Dual + ST/LT) achieved highest ROUGE-L scores (~0.51-0.55) compared to unified memory models (~0.44)
- Human expert evaluation showed Full Model converging with Ground Truth in perceived realism over time (44% vs 46% preference at 100%)
- Dual memory architecture significantly outperformed unified memory approaches in automated evaluation
- Experience-reflection-optimization cycle enabled agents to evolve behaviors without external fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Dual Memory Differentiation (Experience vs. Knowledge)
Separating memory into Experience Base (episodic events) and Knowledge Base (structured facts) improves retrieval relevance by reducing noise. When agents face situations, they query these bases independently via vector similarity, preventing specific episodic details from obscuring general principles and vice versa.

### Mechanism 2: Hierarchical Salience (Short-term vs. Long-term)
Prioritizing salient memories in Short-term buffers before accessing Long-term storage improves immediate coherence and realism. Agents select important subsets for ST component, ensuring urgent or highly relevant context (e.g., recent arguments) is prioritized over older stable data.

### Mechanism 3: Experience-Reflection-Optimization Loop
Agents evolve from zero experience to high-fidelity behavior through autonomous memory and role setting updates based on interaction outcomes. This "experience-reflection-optimization" cycle allows learning from simulated mistakes without external fine-tuning of underlying LLM weights.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG) for Agents**
  - Why needed here: Core of AAS manages context beyond LLM's window using vector databases and cosine similarity to retrieve memories
  - Quick check question: If agent searches for "classroom disruption," would it retrieve a specific past event (Experience) or a rule about discipline (Knowledge)?

- **Concept: Prompt Engineering & System Prompts**
  - Why needed here: "Role Settings" are elaborate system prompts; "evolution" relies on dynamically rewriting parts based on performance
  - Quick check question: How does changing "Teaching Style" variable in system prompt alter token probability distribution for next response?

- **Concept: Simulation Fidelity vs. Stochasticity**
  - Why needed here: Aims for "High-Fidelity" dynamics; balance between deterministic scheduling and stochastic agent choices is key to interpreting results
  - Quick check question: Does higher ROUGE-L score guarantee better educational value, or just closer text matching to ground truth?

## Architecture Onboarding

- **Component map:** Environment (Tiled/Cocos visual engine, 25 areas) -> Brain (LLM processing context) -> Memory Store (Vector Database with Experience/Knowledge bases, ST/LT) -> Orchestrator (Zero-Exp loop managing State -> Retrieve -> Act -> Update)

- **Critical path:**
  1. Input: Time + Environment State + Role Settings
  2. Retrieve: Vector search prioritizes Short-term Memory, then Long-term Experience/Knowledge
  3. Generate: LLM produces action based on retrieved context
  4. Update: Agent generates JSON update to modify vector database entries and role settings (Self-Reflection)

- **Design tradeoffs:**
  - Dual vs. Unified Memory: Dual offers higher precision but adds complexity to database schema and retrieval logic
  - Static vs. Evolving Roles: Evolving roles increase realism but introduce non-determinism
  - Scale: Current small (50 agents); scaling requires substantial computational overhead

- **Failure signatures:**
  - Amnesia: Agent ignores past events due to poor retrieval or vectorization
  - Schizophrenic Behavior: Rapid, incoherent changes in Role Settings from aggressive optimization
  - Context Overflow: ST + LT + Working Memory exceeds LLM context window

- **First 3 experiments:**
  1. Ablation Run: Implement ID 9 (Context Only) vs. ID 1 (Full Memory) on single day to confirm performance delta
  2. Salience Tuning: Adjust ST Memory criteria to test impact on response speed and quality
  3. Role Drift Test: Run 10-day simulation to check if role settings drift into unrealistic states

## Open Questions the Paper Calls Out

- **Question:** How does computational cost and behavioral fidelity scale when increasing from 50 agents to hundreds and extending beyond 5 days?
- **Question:** To what extent does incorporating Vision-Language Models improve realism of non-verbal interactions compared to text-only LLM architecture?
- **Question:** Can high-fidelity interaction data generated by AAS effectively transfer to downstream applications like teacher training or personalized learning?
- **Question:** How sensitive is agent evolution trajectory to specific biases and limitations of expert-curated ground truth dataset?

## Limitations

- System's performance heavily relies on quality of expert-curated ground truth dataset, making generalization to alternative pedagogical styles uncertain
- Evaluation focuses exclusively on text-based similarity metrics and realism perception rather than actual educational outcomes or learning gains
- Computational overhead of per-agent memory retrieval, reflection, and role updating suggests potential scalability challenges beyond 50 agents

## Confidence

- **High Confidence (70-90%):** Dual memory architecture demonstrably improves ROUGE-L scores; experience-reflection-optimization loop creates measurable behavioral evolution; vector database retrieval functions as described
- **Medium Confidence (40-70%):** ROUGE-L improvements translate to meaningful educational dynamics; memory mechanisms maintain coherence when scaled; system handles diverse educational scenarios
- **Low Confidence (10-40%):** Evolved behaviors represent authentic educational processes; computational efficiency sufficient for practical deployment; behavior remains stable over extended periods

## Next Checks

1. **Dataset Validation:** Obtain or reconstruct ground truth dataset to verify ROUGE-L improvements reflect genuine pedagogical advances rather than dataset-specific memorization; test on alternative educational datasets

2. **Long-term Stability Test:** Run extended simulations (30+ days) to identify model drift, memory contradictions, or role setting degeneration; monitor for "schizophrenic behavior"

3. **Educational Outcome Measurement:** Implement assessment of actual learning outcomes, student engagement metrics, and behavioral changes rather than relying solely on text similarity; compare against established educational best practices and learning science principles