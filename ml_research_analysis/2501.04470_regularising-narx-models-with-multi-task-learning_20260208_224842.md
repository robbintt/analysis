---
ver: rpa2
title: Regularising NARX models with multi-task learning
arxiv_id: '2501.04470'
source_url: https://arxiv.org/abs/2501.04470
tags:
- noise
- narx
- mt-narx
- output
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of overfitting in NARX models used
  for time-varying processes. The proposed method introduces a multi-task learning
  (MTL) approach where the NARX model predicts outputs at both the current time and
  several lead times into the future.
---

# Regularising NARX models with multi-task learning

## Quick Facts
- arXiv ID: 2501.04470
- Source URL: https://arxiv.org/abs/2501.04470
- Authors: Sarah Bee; Lawrence Bull; Nikolaos Dervilis; Keith Worden
- Reference count: 15
- Key outcome: Multi-task learning (MTL) regularizes NARX models under high noise, reducing NMSE from 7.3% to 6.0% at 150% noise level in Duffing oscillator

## Executive Summary
This paper addresses overfitting in NARX models for time-varying processes by introducing a multi-task learning approach. The method predicts outputs at both current time and several lead times into the future, creating regularization where lead time outputs improve generalization of the current time output. Tested on a nonlinear SDOF Duffing oscillator with varying noise levels, the approach shows that MTL achieves lower Normalized Mean Square Error (NMSE) compared to independent learners under high noise conditions (100% and 150%). The paper also discusses challenges with local minima optimization in the multi-task loss function and suggests potential improvements through weighted loss functions.

## Method Summary
The method extends NARX models by adding auxiliary prediction tasks for future time steps (lead times). During training, the network simultaneously predicts the current output and several future outputs, with all predictions sharing the same hidden representation. The lead time outputs act as non-operational features that regularize the primary task. The architecture uses a single hidden layer with tanh activation, and optimization is performed using Adam with early stopping. The approach was tested on a Duffing oscillator system with controlled noise addition to inputs and outputs, comparing multi-task against single-task baselines across multiple noise levels.

## Key Results
- At 150% noise level, MT-NARX achieved 6.0% NMSE versus 7.3% for ST-NARX (Trial 1)
- MT-NARX outperformed ST-NARX at high noise levels (100% and 150%) but underperformed at low noise (1-50%)
- Local minima optimization in multi-task loss function caused orders-of-magnitude variance across random seeds
- Weighted loss functions were proposed as a potential improvement but not implemented

## Why This Works (Mechanism)

### Mechanism 1
Predicting lead-time outputs alongside current-time outputs regularizes the primary task under high-noise conditions. The shared hidden representation must simultaneously support multiple temporally-distant predictions, constraining the network to learn smoother input-output mappings that capture underlying dynamics rather than fitting noise. The lead-time tasks act as "non-operational features" that influence weight updates during training but are not required at inference.

### Mechanism 2
Symmetric lag-lead structure induces an implicit smoothing effect on predictions. By placing the predicted value "in the middle" of lag inputs and lead outputs, the multi-task architecture enforces temporal consistency. This is conceptually similar to a moving average filter where the central estimate is informed by both past and (during training) future information.

### Mechanism 3
High noise amplifies the relative benefit of multi-task regularization. Under high noise, single-task models overfit to spurious patterns. The auxiliary lead-time tasks provide additional supervisory signals that, on average, point toward the true underlying function—effectively increasing the signal-to-noise ratio in gradient updates.

## Foundational Learning

- **NARX model structure**: Understanding that NARX uses autoregressive output feedback and exogenous inputs is essential before modifying it with multi-task heads.
  - Quick check: Can you sketch how a prediction at time t depends on y(t-1), y(t-2), ..., x(t), x(t-1), ...?

- **Multi-task learning (MTL) fundamentals**: The core contribution frames lead-time prediction as auxiliary tasks; understanding hard/soft parameter sharing clarifies why shared representations regularize.
  - Quick check: In a shared-trunk multi-task network, how does backpropagation from Task B affect Task A's weights?

- **Overfitting vs. generalization in time series**: The paper's motivation rests on NARX's overfitting propensity; distinguishing OSA (one-step-ahead) error from MPO (model-predicted output) error is critical for evaluation.
  - Quick check: Why might a model have low OSA error but poor MPO performance?

## Architecture Onboarding

- **Component map**: m lagged outputs (y_{t-1}...y_{t-m}) + m lagged/current exogenous inputs (x_t...x_{t-m+1}) -> Single hidden layer (tanh, N nodes) -> Current output y_t + Nleads future outputs y_{t+1}...y_{t+Nleads}

- **Critical path**: Generate clean simulation data → add controlled noise to input/output → tune hyperparameters via validation NMSE on MPO → train with Adam + early stopping → evaluate MPO NMSE on held-out test set

- **Design tradeoffs**: More leads → stronger regularization but harder optimization; larger network capacity → better fit but more overfitting risk; unweighted loss → simpler but may optimize auxiliary tasks at expense of target

- **Failure signatures**: High variance across random seeds indicates local minima traps; NMSE > 5% indicates poor fit; if ST-NARX outperforms MT-NARX at low noise, regularization is counterproductive

- **First 3 experiments**:
  1. Implement ST-NARX on Duffing oscillator with parameters from paper. Confirm NMSE trends match Figure 5.
  2. Train both ST-NARX and MT-NARX across noise levels {1%, 10%, 30%, 50%, 100%}. Verify crossover point where MT-NARX becomes superior.
  3. Implement the proposed weighted MSE (Eq. 4) with a₁ > aᵢ for i>1. Test whether weighting the target output reduces variance across seeds and improves NMSE.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a weighted loss function, as proposed in Equation 4, successfully bias the optimizer towards local minima that favor the target output (y_t) over auxiliary lead-time predictions?
  - Basis in paper: The authors state that "further work could be done to explore the impact of weightings in the loss function" to address the issue where the model minimizes total loss but not necessarily the error of the intended output.
  - Why unresolved: The paper proposes the modification but does not implement or validate it experimentally.
  - What evidence would resolve it: A comparative study tracking the convergence of the target output's NMSE using weighted versus unweighted multi-task loss functions.

- **Open Question 2**: Why does the multi-task approach degrade performance compared to independent learners at low noise levels (1%–30%)?
  - Basis in paper: The results in Section 3 consistently show the Standard-NARX outperforming the MT-NARX at lower noise levels across all trials, a phenomenon the paper does not mechanistically explain.
  - Why unresolved: The analysis focuses on benefits at high noise, leaving the degradation at low noise as an unexplored edge case.
  - What evidence would resolve it: An analysis of the bias-variance trade-off at low noise levels to determine if auxiliary tasks introduce unnecessary bias.

- **Open Question 3**: Can the MT-NARX regularization be strengthened to achieve a "good fit" (NMSE < 5%) in high-noise scenarios?
  - Basis in paper: The Discussion notes that at high noise (100-150%), the NMSE remains >6%, meaning "the model has not improved enough to class it as a good fit."
  - Why unresolved: While the method lowers error relative to baseline, it fails to reach accuracy thresholds required for practical application in high-noise environments.
  - What evidence would resolve it: Results showing MT-NARX achieving NMSE < 5% on the Duffing oscillator dataset with >100% noise, potentially via architecture changes or weighted loss.

## Limitations
- Empirical evaluation limited to single synthetic system (Duffing oscillator), raising generalizability questions
- Local minima problem causes orders-of-magnitude variance across random seeds, suggesting unreliable optimization
- Absolute performance remains poor at high noise levels (NMSE > 5%), with improvements being relatively modest

## Confidence
- **High confidence**: The mechanism by which multi-task regularization helps under high noise (shared representation constraints reducing overfitting)
- **Medium confidence**: The effectiveness of symmetric lag-lead architecture for temporal smoothing
- **Low confidence**: Generalization to real-world systems beyond the Duffing oscillator, and practical utility given persistent poor absolute performance at high noise

## Next Checks
1. **Generalization test**: Apply the MT-NARX approach to a second dynamical system (e.g., Van der Pol oscillator or Lorenz system) to verify the regularization effect holds beyond the Duffing oscillator.

2. **Weighted loss ablation**: Implement and compare the proposed weighted MSE loss (a₁ > aᵢ for i>1) to determine if prioritizing the target output reduces variance across random seeds and improves NMSE.

3. **Input noise robustness**: Design an experiment isolating input versus output noise effects to determine whether the regularization mechanism works equally well when noise contaminates inputs versus outputs.