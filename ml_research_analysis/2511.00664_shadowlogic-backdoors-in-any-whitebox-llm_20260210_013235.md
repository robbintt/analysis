---
ver: rpa2
title: 'ShadowLogic: Backdoors in Any Whitebox LLM'
arxiv_id: '2511.00664'
source_url: https://arxiv.org/abs/2511.00664
tags:
- onnx
- graph
- vector
- trigger
- shadowlogic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ShadowLogic exposes a new attack vector in white-box LLM deployment
  by embedding covert backdoors directly into the computational graph. By inserting
  a trigger-detection node and an uncensoring vector that overrides refusal behavior,
  the method achieves a 62% and 70% attack success rate on Phi-3 and Llama 3.2 respectively
  when tested with harmful prompts.
---

# ShadowLogic: Backdoors in Any Whitebox LLM

## Quick Facts
- arXiv ID: 2511.00664
- Source URL: https://arxiv.org/abs/2511.00664
- Authors: Kasimir Schulz; Amelia Kawasaki; Leo Ring
- Reference count: 9
- Primary result: A method to embed undetectable backdoors into white-box LLMs by manipulating their computational graphs, achieving high attack success rates while preserving normal behavior.

## Executive Summary
ShadowLogic introduces a novel backdoor attack vector for white-box large language models (LLMs) by directly embedding malicious functionality into the computational graph. This technique bypasses traditional detection methods like weight analysis and quality checks by inserting a trigger-detection node and an uncensoring vector that overrides refusal behavior. The backdoor activates only upon a secret trigger phrase, leaving normal model behavior and performance largely unchanged. This work demonstrates a significant supply chain vulnerability in LLM deployment, especially for models with publicly available architecture details.

## Method Summary
ShadowLogic works by modifying the computational graph of a white-box LLM to include a trigger-detection node and an uncensoring vector. The trigger-detection node monitors for a secret phrase, and upon detection, activates the uncensoring vector which overrides the model's refusal behavior for harmful prompts. This manipulation occurs at the graph level, making it undetectable by standard weight analysis or quality assurance checks. The approach achieves high attack success rates while maintaining normal model functionality and incurring only a minimal 1.2% latency overhead.

## Key Results
- 62% attack success rate on Phi-3 and 70% on Llama 3.2 for harmful prompts with secret trigger
- Backdoor activation leaves normal model behavior and performance largely unchanged
- Undetectable by standard weight analysis and quality checks due to graph-level manipulation

## Why This Works (Mechanism)
The attack exploits the computational graph of white-box LLMs, which contains the complete model architecture and intermediate computations. By inserting a trigger-detection node and an uncensoring vector directly into this graph, the backdoor can override the model's built-in safety mechanisms without altering the model weights themselves. This approach is particularly effective because white-box models expose their internal structure, allowing precise manipulation of the computational flow. The trigger-based activation ensures the backdoor remains dormant during normal operation, making it difficult to detect through behavioral testing or standard security audits.

## Foundational Learning

**Computational Graph**: The complete representation of a model's architecture and data flow, including nodes (operations) and edges (data connections). *Why needed*: This is the target for backdoor insertion, as it contains all intermediate computations and control flow. *Quick check*: Can you identify the nodes and edges in a simple neural network diagram?

**White-Box Model**: A model whose architecture, weights, and computational graph are fully accessible. *Why needed*: The attack relies on complete access to the model's internal structure for precise manipulation. *Quick check*: Does the model have publicly available architecture details and weights?

**Trigger-Detection Node**: A custom node inserted into the computational graph that monitors for a specific trigger phrase. *Why needed*: This node acts as the activation switch for the backdoor, ensuring it only triggers under specific conditions. *Quick check*: How would you implement a trigger-detection mechanism in a computational graph?

## Architecture Onboarding

**Component Map**: Input Text -> Tokenizer -> Computational Graph (includes Trigger-Detection Node and Uncensoring Vector) -> Output Text

**Critical Path**: The attack modifies the critical path of the computational graph by inserting the trigger-detection node and uncensoring vector. This ensures the backdoor is executed during normal inference while maintaining the model's original architecture and performance characteristics.

**Design Tradeoffs**: The method prioritizes stealth and effectiveness over complexity, using minimal modifications to achieve high attack success rates. The tradeoff is that the attack requires white-box access and may not be transferable to black-box models.

**Failure Signatures**: If the trigger-detection node fails to activate or the uncensoring vector is ineffective, the backdoor will not trigger, resulting in normal model behavior. Additionally, if the modifications significantly impact model performance, they may be detectable through quality assurance checks.

**First Experiments**:
1. Test the backdoor's effectiveness on a simple white-box model with a known architecture.
2. Evaluate the stealthiness of the backdoor by comparing model performance before and after insertion.
3. Assess the transferability of the backdoor to models with different architectures or training regimes.

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental evaluation limited to two open-weight models (Phi-3 and Llama 3.2) in a single commercial deployment
- No empirical analysis of backdoor robustness under model fine-tuning, pruning, or quantization
- Claim of preserved normal behavior based on limited benign prompt testing

## Confidence

**High**: The method is technically sound and achieves reported success rates on tested models.
**Medium**: The claim of undetectability by weight analysis is supported, but graph-level inspection resilience is not empirically validated.
**Medium**: The assertion that normal behavior is preserved is based on limited benign testing and should be interpreted cautiously.

## Next Checks
1. Test backdoor persistence and effectiveness after common model compression techniques (pruning, quantization).
2. Evaluate transferability of the backdoor to models with different architectures or training regimes.
3. Conduct adversarial detection experiments using graph-level analysis tools and compare against baseline defenses.