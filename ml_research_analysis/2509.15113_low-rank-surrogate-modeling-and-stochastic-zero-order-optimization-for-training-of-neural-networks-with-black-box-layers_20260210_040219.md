---
ver: rpa2
title: Low-rank surrogate modeling and stochastic zero-order optimization for training
  of neural networks with black-box layers
arxiv_id: '2509.15113'
source_url: https://arxiv.org/abs/2509.15113
tags:
- training
- layer
- layers
- gradient
- physical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating non-differentiable
  physical components, such as photonic layers, into deep learning pipelines for end-to-end
  training. The authors propose a framework called astralora, which combines stochastic
  zeroth-order optimization for updating the physical layer's internal parameters
  with a dynamic low-rank surrogate model to enable gradient propagation through the
  black-box layer.
---

# Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers

## Quick Facts
- arXiv ID: 2509.15113
- Source URL: https://arxiv.org/abs/2509.15113
- Reference count: 40
- One-line primary result: Near-digital baseline accuracy achieved for end-to-end training of hybrid neural networks with black-box physical layers across image, audio, and language tasks

## Executive Summary
This paper introduces astralora, a framework that enables end-to-end training of neural networks containing non-differentiable physical components. The approach combines stochastic zeroth-order optimization for updating physical layer parameters with a dynamic low-rank surrogate model that enables gradient propagation through black-box layers. The implicit projector-splitting integrator algorithm efficiently updates the surrogate model after each forward pass with minimal hardware queries.

The framework is evaluated across three domains - image classification (CIFAR-10), audio classification (UrbanSound8K), and large-scale language modeling (FineWeb corpus) - demonstrating near-digital baseline accuracy while successfully training hybrid models with various physical components including spatial light modulators, microring resonators, and Mach-Zehnder interferometers.

## Method Summary
The astralora framework addresses the challenge of integrating non-differentiable physical components into deep learning pipelines through a dual approach. First, stochastic zeroth-order optimization updates the internal parameters of physical layers without requiring gradients. Second, a dynamic low-rank surrogate model approximates the physical layer's behavior, enabling gradient propagation through the network during backpropagation. The implicit projector-splitting integrator algorithm efficiently updates this surrogate model after each forward pass, minimizing the number of hardware queries needed. This combination allows for practical end-to-end training of hybrid models that incorporate physical components as black-box layers within standard deep learning architectures.

## Key Results
- Achieved near-digital baseline accuracy across all tested domains (image, audio, and language)
- Successfully trained hybrid models with three distinct physical components: spatial light modulators, microring resonators, and Mach-Zehnder interferometers
- Demonstrated robust performance and query efficiency in integrating non-differentiable physical components
- Bridged hardware-aware deep learning with gradient-free optimization for practical AI system integration

## Why This Works (Mechanism)
The framework works by decoupling the parameter update mechanism from gradient propagation. Stochastic zeroth-order optimization handles parameter updates for the physical layer without requiring differentiability, while the low-rank surrogate model provides differentiable approximations that enable backpropagation through the network. The implicit projector-splitting integrator algorithm maintains the surrogate model's accuracy with minimal hardware queries, making the approach computationally efficient. This separation of concerns allows the framework to handle non-differentiable components while maintaining end-to-end trainability.

## Foundational Learning
- **Stochastic zeroth-order optimization**: Used for updating physical layer parameters without requiring gradients; needed because physical components are non-differentiable; quick check: verify convergence rates on benchmark functions
- **Low-rank surrogate modeling**: Approximates physical layer behavior with reduced dimensionality; needed to enable efficient gradient propagation; quick check: validate reconstruction error against full-rank models
- **Implicit projector-splitting integrator algorithm**: Efficiently updates surrogate models with minimal hardware queries; needed for computational efficiency; quick check: measure query reduction compared to naive approaches
- **Hybrid neural network architecture**: Combines digital and physical components in end-to-end trainable models; needed to leverage physical advantages while maintaining learning capability; quick check: compare performance against purely digital or physical approaches
- **Black-box optimization**: Treats physical components as function evaluators without internal knowledge; needed for practical deployment with real hardware; quick check: test robustness to different physical layer implementations
- **End-to-end differentiability**: Enables backpropagation through the entire network; needed for standard deep learning training pipelines; quick check: verify gradient flow through surrogate models

## Architecture Onboarding

**Component Map:**
Digital Neural Network -> Physical Layer (Black Box) -> Low-Rank Surrogate Model -> Gradient Propagation

**Critical Path:**
1. Forward pass through digital network
2. Physical layer evaluation (hardware query)
3. Surrogate model update via implicit projector-splitting
4. Backpropagation through surrogate
5. Stochastic zeroth-order optimization for physical layer parameters
6. Gradient-based updates for digital network

**Design Tradeoffs:**
- Accuracy vs. query efficiency in surrogate model updates
- Surrogate model complexity vs. computational overhead
- Zeroth-order optimization step size vs. convergence stability
- Hardware query frequency vs. training time

**Failure Signatures:**
- Poor surrogate model approximation leading to vanishing/exploding gradients
- Zeroth-order optimization getting stuck in local minima
- Excessive hardware queries causing training bottlenecks
- Mismatch between surrogate model and actual physical layer behavior

**3 First Experiments:**
1. Benchmark surrogate model accuracy against ground truth physical layer outputs
2. Compare query counts and training time against baseline hybrid approaches
3. Test gradient propagation stability through the surrogate model under various update frequencies

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability beyond tested physical components (SLMs, microring resonators, MZIs) remains uncertain
- Query efficiency gains may be implementation-dependent and not universally applicable
- Evaluation focuses on accuracy metrics without comprehensive computational overhead analysis
- Real-time performance constraints in actual hardware deployments not thoroughly examined

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| End-to-end training capability | High |
| Near-digital baseline accuracy | Medium |
| Query efficiency improvements | Medium |

## Next Checks
1. Test framework with additional physical layer types beyond the three evaluated, including emerging photonic technologies and other non-differentiable components
2. Conduct ablation studies to isolate the contribution of the implicit projector-splitting integrator algorithm versus other framework components
3. Perform extensive runtime analysis comparing query counts, training time, and hardware resource utilization against both pure digital baselines and alternative hybrid approaches