---
ver: rpa2
title: Is Synthetic Image Augmentation Useful for Imbalanced Classification Problems?
  Case-Study on the MIDOG2025 Atypical Cell Detection Competition
arxiv_id: '2509.02612'
source_url: https://arxiv.org/abs/2509.02612
tags:
- synthetic
- atypical
- data
- mitotic
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work evaluates synthetic image augmentation for addressing
  class imbalance in atypical mitosis classification under cross-domain shifts. Two
  complementary backbones were tested: ConvNeXt-Small (ImageNet-pretrained) and a
  histopathology-specific ViT (Lunit) trained via self-supervision.'
---

# Is Synthetic Image Augmentation Useful for Imbalanced Classification Problems? Case-Study on the MIDOG2025 Atypical Cell Detection Competition

## Quick Facts
- arXiv ID: 2509.02612
- Source URL: https://arxiv.org/abs/2509.02612
- Reference count: 0
- Primary result: Synthetic balancing provided no consistent performance benefit when real data is sufficiently large.

## Executive Summary
This work investigates the utility of synthetic image augmentation for addressing class imbalance in atypical mitosis detection, a histopathology classification problem subject to cross-domain shifts. Two complementary backbone architectures were evaluated: ConvNeXt-Small (ImageNet-pretrained) and a histopathology-specific ViT (Lunit) trained via self-supervision. Synthetic atypical mitosis images were generated using a latent diffusion model conditioned on class labels. The study finds that both backbones achieve strong cross-validation performance (mean AUROC ~95%), with ConvNeXt reaching higher peaks and Lunit showing greater fold-to-fold stability. Critically, synthetic balancing did not yield consistent improvements, suggesting limited benefit when real data is sufficiently large.

## Method Summary
The study employs a two-pronged approach: (1) A ConvNeXt-Small backbone pretrained on ImageNet, and (2) a histopathology-specific ViT (Lunit) pretrained via self-supervision. Both models are trained to classify atypical versus typical mitosis in histopathology images. Synthetic atypical mitosis images are generated using a latent diffusion model conditioned on class labels. The models are evaluated using five-fold cross-validation on the MIDOG2025 dataset, with performance measured by AUROC and balanced accuracy. The synthetic images are used to augment the training data in an attempt to mitigate class imbalance.

## Key Results
- Both ConvNeXt and Lunit backbones achieved strong cross-validation performance (mean AUROC ~95%).
- ConvNeXt reached slightly higher peak AUROC, while Lunit showed greater fold-to-fold stability.
- Synthetic balancing did not yield consistent improvements, suggesting limited benefit when real data is sufficiently large.
- On the preliminary hidden test set, ConvNeXt attained the highest AUROC (95.4%), while Lunit remained competitive on balanced accuracy.

## Why This Works (Mechanism)
The strong performance of both ImageNet-pretrained and domain-pretrained models suggests that transfer learning from large-scale datasets can effectively bootstrap performance on specialized histopathology tasks. The stability of the Lunit model across folds indicates that domain-specific pretraining may confer robustness to domain shifts. The lack of benefit from synthetic balancing implies that, when real data is sufficiently large, augmenting with synthetic images does not significantly improve model performance, possibly due to domain shift or limited quality of synthetic samples.

## Foundational Learning
- **Cross-domain shifts**: Understanding how models trained on one domain (e.g., ImageNet) perform on another (histopathology) is crucial for evaluating model robustness.
- **Class imbalance**: Techniques for addressing class imbalance, such as synthetic data generation, are essential for training models on skewed datasets.
- **Transfer learning**: Leveraging pretrained models on large datasets can significantly improve performance on specialized tasks.
- **Self-supervised learning**: Pretraining models using self-supervised methods on domain-specific data can enhance their ability to generalize.
- **Latent diffusion models**: Understanding how to generate synthetic images using diffusion models is key for data augmentation strategies.
- **Five-fold cross-validation**: This technique provides a robust estimate of model performance and stability.

## Architecture Onboarding
- **Component map**: Data (MIDOG2025) -> Preprocessing -> Backbone (ConvNeXt or Lunit) -> Latent Diffusion Model (for synthetic data) -> Classification Head -> Evaluation (AUROC, Balanced Accuracy)
- **Critical path**: Data preprocessing -> Backbone training -> Synthetic data generation -> Model evaluation
- **Design tradeoffs**: ImageNet pretraining vs. domain-specific pretraining (tradeoff between peak performance and stability); synthetic augmentation vs. real data (tradeoff between data augmentation and potential domain shift).
- **Failure signatures**: Poor performance on the hidden test set; high variance across folds; synthetic images that are not biologically plausible.
- **First experiments**:
  1. Evaluate model performance on a held-out validation set to assess overfitting.
  2. Perform ablation studies to isolate the effects of ImageNet pretraining and domain-specific pretraining.
  3. Conduct qualitative analysis of synthetic images to assess their biological plausibility.

## Open Questions the Paper Calls Out
None

## Limitations
- The synthetic images were generated via latent diffusion without extensive qualitative validation, leaving open questions about their biological plausibility and domain alignment.
- The comparison between ConvNeXt and Lunit backbones does not isolate the contribution of ImageNet pretraining from architectural differences.
- The preliminary test set evaluation is limited in size and may not generalize to broader clinical deployment scenarios.

## Confidence
- **High confidence**: Backbone performance ranking and cross-validation results.
- **Medium confidence**: Interpretation of synthetic augmentation ineffectiveness, given limited ablation and qualitative analysis.
- **Low confidence**: Extrapolation to unseen datasets or clinical settings without further validation.

## Next Checks
1. Perform detailed qualitative and quantitative evaluation of synthetic atypical mitosis images against real samples using expert histopathologist review and feature distribution analysis.
2. Conduct an ablation study isolating the effects of ImageNet pretraining, ViT architecture, and self-supervised training on performance and robustness.
3. Test model generalization on independent external datasets and under varying imbalance ratios to assess scalability and robustness of synthetic augmentation strategies.