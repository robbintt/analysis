---
ver: rpa2
title: 'DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained
  Dirty Tableware Recognition'
arxiv_id: '2509.11661'
source_url: https://arxiv.org/abs/2509.11661
tags:
- dtgen
- tableware
- data
- dirt
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DTGen addresses extreme data scarcity in fine-grained dirty tableware\
  \ recognition by leveraging generative diffusion models with LoRA-based domain adaptation,\
  \ structured contamination-aware prompt generation, and CLIP-based quality filtering.\
  \ With only 40 real samples, DTGen synthesizes over 3,000 high-quality images that\
  \ enable ResNet-50 classifiers to achieve 93% binary accuracy and 86% accuracy on\
  \ three-class fine-grained tasks\u2014representing 28% and 15% improvements over\
  \ traditional augmentation baselines respectively."
---

# DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition

## Quick Facts
- **arXiv ID**: 2509.11661
- **Source URL**: https://arxiv.org/abs/2509.11661
- **Reference count**: 11
- **Primary result**: Achieves 93% binary and 86% three-class accuracy on dirty tableware recognition using only 40 real samples

## Executive Summary
DTGen addresses extreme data scarcity in fine-grained dirty tableware recognition by leveraging generative diffusion models with LoRA-based domain adaptation, structured contamination-aware prompt generation, and CLIP-based quality filtering. With only 40 real samples, DTGen synthesizes over 3,000 high-quality images that enable ResNet-50 classifiers to achieve 93% binary accuracy and 86% accuracy on three-class fine-grained tasks—representing 28% and 15% improvements over traditional augmentation baselines respectively. The framework demonstrates both theoretical advances in few-shot industrial vision and practical feasibility for deployment in intelligent dishwashers through model compression and system integration.

## Method Summary
DTGen employs a three-stage pipeline: first, LoRA fine-tuning adapts Stable Diffusion 3.5 Large to generate tableware images with specified contamination patterns; second, structured prompt generation creates 3,600 synthetic images using slot-filling templates that encode tableware type, color, dirtiness level, and lighting conditions; third, CLIP-based filtering with an adaptive threshold (μ_S + 1.5·σ_S) selects 3,297 high-quality images that closely match the real data distribution. The filtered synthetic dataset trains a ResNet-50 classifier (ImageNet-pretrained) that outperforms traditional augmentation methods by substantial margins in both binary clean/dirty and three-class fine-grained classification tasks.

## Key Results
- **93% binary classification accuracy** on clean vs dirty tableware recognition
- **86% three-class accuracy** for clean/light dirty/heavy dirty classification
- **28% and 15% improvements** over traditional augmentation baselines for binary and fine-grained tasks respectively

## Why This Works (Mechanism)
DTGen's effectiveness stems from combining domain-adapted generative models with structured prompt engineering and quality-aware filtering. The LoRA fine-tuning enables Stable Diffusion to capture the specific visual characteristics of dirty tableware patterns while maintaining generalization. Structured prompts ensure generated images cover the full contamination spectrum needed for fine-grained recognition. The CLIP filtering removes low-quality or unrealistic samples that could confuse the classifier, while the small real sample set provides sufficient domain context for the generative model to learn contamination patterns specific to tableware materials and shapes.

## Foundational Learning
- **LoRA fine-tuning**: Parameter-efficient adaptation of large diffusion models by injecting low-rank matrices into cross-attention layers; needed for domain-specific generation without full model retraining; quick check: monitor generated image quality improvement over fine-tuning steps
- **Structured prompt generation**: Template-based prompt construction using slot-filling to systematically cover contamination scenarios; needed to ensure comprehensive synthetic dataset coverage; quick check: verify prompt diversity matches real data distribution
- **CLIP-based filtering**: Visual-semantic similarity scoring using CLIP embeddings with adaptive thresholding; needed to automatically select high-quality synthetic images; quick check: examine similarity score distributions before and after filtering
- **Knowledge distillation for deployment**: Training compact models (MobileNetV3) on logits from larger pretrained models; needed to meet real-time inference requirements on embedded hardware; quick check: compare accuracy degradation vs latency improvements
- **Cross-attention optimization**: Targeted fine-tuning of W_Q, W_K, W_V, W_O matrices in Stable Diffusion; needed to balance generation quality with computational efficiency; quick check: visualize attention map evolution during fine-tuning

## Architecture Onboarding

**Component Map**: Stable Diffusion 3.5 Large (LoRA fine-tuned) -> Structured Prompt Generator -> CLIP Filter -> ResNet-50 Classifier -> (Deployment: Knowledge Distillation -> MobileNetV3 -> INT8 Quantization)

**Critical Path**: Real samples → LoRA fine-tuning → Synthetic generation → CLIP filtering → Classifier training → Evaluation

**Design Tradeoffs**: DTGen prioritizes generation quality and classifier performance over computational efficiency during training, accepting high resource usage for the fine-tuning phase to achieve superior downstream accuracy. The system trades model size and inference speed for accuracy during development, then applies aggressive compression only at deployment.

**Failure Signatures**: 
- Low CLIP retention rate (<50%) indicates poor domain alignment between generated and real data
- Large train-test gap in classifier suggests overfitting to synthetic artifacts
- Unstable LoRA convergence produces inconsistent generation quality across batches

**First Experiments**:
1. Verify LoRA fine-tuning produces visually coherent tableware images with contamination patterns matching the prompt specifications
2. Test CLIP filtering threshold sensitivity by varying the multiplier from 1.0 to 2.0×σ_S and measuring downstream accuracy impact
3. Evaluate classifier performance when trained on unfiltered synthetic data to quantify filtering contribution

## Open Questions the Paper Calls Out
- What is the minimum number of real samples required for DTGen to achieve acceptable classifier performance, and how does performance degrade as the real sample count decreases from 40?
- Can more efficient generative architectures maintain synthetic data quality while reducing the computational cost of diffusion-based data augmentation?
- How can multi-modal sensor data be integrated with DTGen's visual recognition outputs to enable fully autonomous cleaning decisions?
- How well does DTGen transfer to contamination recognition domains beyond tableware, such as medical device cleaning validation or food safety inspection?
- What are the actual latency, memory footprint, and accuracy metrics when deploying the compressed DTGen classifier pipeline on real embedded dishwasher hardware?

## Limitations
- High computational cost of Stable Diffusion fine-tuning and generation limits rapid iteration
- Performance evaluation restricted to single dataset without cross-domain validation
- Deployment metrics remain theoretical without empirical validation on actual dishwasher hardware
- No systematic evaluation of minimum data requirements for the few-shot approach

## Confidence
- **High confidence**: Binary classification results (93% accuracy) and general methodology framework
- **Medium confidence**: Three-class fine-grained classification improvements (86% accuracy) and 28%/15% gains over baselines
- **Low confidence**: Practical deployment claims for intelligent dishwashers without real-world performance data

## Next Checks
1. **Ablation study of CLIP filtering threshold**: Systematically vary the multiplier (currently 1.5×σ_S) to determine sensitivity of downstream classification performance to synthetic image quality
2. **Cross-dataset generalization test**: Evaluate the trained model on an external dirty tableware dataset or real dishwasher deployment data to assess robustness beyond the Cleaned vs Dirty V2 dataset
3. **User study on synthetic image quality**: Conduct human evaluation of the generated samples to verify they meet practical deployment requirements for dirty tableware recognition in real-world conditions