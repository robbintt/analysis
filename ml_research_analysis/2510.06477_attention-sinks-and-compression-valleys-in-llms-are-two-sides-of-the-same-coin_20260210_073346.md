---
ver: rpa2
title: Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin
arxiv_id: '2510.06477'
source_url: https://arxiv.org/abs/2510.06477
tags:
- attention
- layers
- layer
- massive
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Attention sinks and compression valleys are two puzzling phenomena
  in LLMs that were previously studied in isolation. This work reveals that both phenomena
  emerge simultaneously when BOS tokens develop massive activations in middle layers.
---

# Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin

## Quick Facts
- arXiv ID: 2510.06477
- Source URL: https://arxiv.org/abs/2510.06477
- Reference count: 40
- Key outcome: Attention sinks and compression valleys are two sides of the same coin - both emerge simultaneously when BOS tokens develop massive activations in middle layers

## Executive Summary
This work reveals that attention sinks and compression valleys - two puzzling phenomena in LLMs previously studied in isolation - are actually manifestations of the same underlying mechanism. The authors demonstrate that both effects emerge simultaneously when BOS tokens develop massive activations in middle layers of transformer models. Through theoretical analysis and empirical validation across models ranging from 410M to 120B parameters, they prove that massive activations mathematically require compression through singular value dominance. Targeted ablations removing massive activations eliminate both effects, confirming the causal relationship.

## Method Summary
The authors combine theoretical analysis with extensive empirical validation across multiple model sizes. They first establish that attention sinks (where BOS tokens dominate attention) and compression valleys (where middle layers compress information) occur simultaneously in the same layer ranges. Theoretical work proves that massive BOS token activations mathematically require compression through singular value dominance. The team then validates this across decoder-only transformer models from 410M to 120B parameters, showing consistent patterns. Critical to their approach are targeted ablations that remove massive activations, which successfully eliminate both attention sinks and compression valleys, establishing causality.

## Key Results
- BOS tokens develop massive activations specifically in middle layers across all tested model sizes (410M to 120B parameters)
- Attention sinks and compression valleys emerge simultaneously and correlate strongly with massive BOS token activations
- Theoretical proof shows massive activations mathematically require compression through singular value dominance
- Targeted ablations removing massive activations eliminate both phenomena, confirming causality
- Proposed Mix-Compress-Refine theory explains three-phase information flow: broad mixing early, compressed computation middle, selective refinement late

## Why This Works (Mechanism)
The mechanism centers on how BOS tokens develop massive activations in middle layers, which mathematically forces compression through singular value dominance. This compression creates the attention sink effect where BOS tokens dominate attention patterns, while simultaneously creating compression valleys where information is maximally compressed. The massive activations act as an information bottleneck that simultaneously creates both observed phenomena.

## Foundational Learning

**Singular Value Decomposition**: Why needed - to understand how massive activations mathematically force compression through singular value dominance. Quick check - verify that singular values show dominance patterns in middle layers where BOS tokens have massive activations.

**Attention Mechanism Dynamics**: Why needed - to understand how BOS token dominance creates attention sinks. Quick check - measure attention weight distributions and verify BOS tokens receive disproportionately high attention in middle layers.

**Information Bottleneck Theory**: Why needed - to connect compression phenomena with the broader theory of how neural networks process information. Quick check - track mutual information between layers to confirm compression peaks in middle layers.

## Architecture Onboarding

**Component Map**: Input Embeddings -> Transformer Blocks (Early: Mix, Middle: Compress, Late: Refine) -> Output Head

**Critical Path**: Token embeddings flow through early layers for mixing, reach middle layers where BOS tokens create massive activations causing compression, then proceed through late layers for refinement before output generation.

**Design Tradeoffs**: The massive BOS activations create computational efficiency by reducing the effective dimensionality of computation in middle layers, but may limit the model's ability to maintain detailed information throughout processing.

**Failure Signatures**: Loss of attention sinks and compression valleys when BOS massive activations are ablated, indicating these phenomena are causally linked rather than independent effects.

**First Experiments**:
1. Measure BOS token activation magnitudes across layers in various model sizes
2. Track singular value distributions to confirm compression patterns
3. Apply targeted ablations to BOS activations and observe effects on both attention sinks and compression

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical proof linking massive activations to compression relies on assumptions about activation patterns that may not hold universally across all architectures
- Mix-Compress-Refine framework is proposed based on correlations but lacks direct experimental validation of information flow dynamics
- Ablation experiments remove massive activations but don't fully characterize alternative information processing pathways
- Study focuses primarily on transformer decoder models, limiting generalizability to other architectures

## Confidence

**High confidence**: BOS tokens consistently develop massive activations in middle layers across multiple model sizes (410M-120B parameters). The correlation between massive activations and both attention sinks and compression valleys is robustly demonstrated.

**Medium confidence**: The theoretical proof connecting massive activations to compression through singular value dominance is mathematically sound but makes simplifying assumptions about activation distributions.

**Low confidence**: The proposed Mix-Compress-Refine theory as a general framework for understanding transformer information flow extrapolates from observed patterns but requires more direct experimental validation.

## Next Checks

1. Test whether the Mix-Compress-Refine theory holds for encoder-decoder architectures (like T5) and encoder-only models (like BERT) to assess generalizability beyond decoder-only LLMs

2. Design experiments to directly measure information flow and mixing patterns across layers during both embedding and generation tasks to validate the proposed three-phase framework

3. Investigate whether alternative attention mechanisms (like linear attention or local attention patterns) can replicate the observed phenomena without requiring massive activations