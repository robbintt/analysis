---
ver: rpa2
title: 'Grokking and Generalization Collapse: Insights from \texttt{HTSR} theory'
arxiv_id: '2506.04434'
source_url: https://arxiv.org/abs/2506.04434
tags:
- grokking
- weight
- training
- layer
- htsr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the grokking phenomenon in neural networks,\
  \ extending training beyond typical limits to reveal a novel late-stage generalization\
  \ collapse called \"anti-grokking.\" The authors employ Heavy-Tailed Self-Regularization\
  \ (HTSR) theory through the WeightWatcher tool to analyze the spectral properties\
  \ of layer weight matrices. The primary finding is that the HTSR metric \u03B1,\
  \ which measures heavy-tailed power-law exponents in the spectral density, uniquely\
  \ tracks all three phases of grokking: pre-grokking, grokking, and anti-grokking."
---

# Grokking and Generalization Collapse: Insights from \texttt{HTSR} theory

## Quick Facts
- arXiv ID: 2506.04434
- Source URL: https://arxiv.org/abs/2506.04434
- Reference count: 23
- Primary result: HTSR metric α uniquely tracks grokking and anti-grokking phases, with α < 2 signaling impending generalization collapse

## Executive Summary
This paper investigates the grokking phenomenon in neural networks by extending training beyond typical limits to reveal a novel late-stage generalization collapse called "anti-grokking." Using Heavy-Tailed Self-Regularization (HTSR) theory and the WeightWatcher tool, the authors analyze spectral properties of layer weight matrices. They find that the HTSR metric α, which measures heavy-tailed power-law exponents in the spectral density, uniquely tracks all three phases of grokking: pre-grokking, grokking, and anti-grokking. The α metric outperforms competing measures including activation sparsity, weight entropy, circuit complexity, and l2 weight norms, providing an early warning of impending generalization collapse without requiring test data access.

## Method Summary
The study trains a 3-layer MLP (784→200→200→10) on a stratified 1,000-sample MNIST subset for 10^7 optimization steps using AdamW optimizer with MSE loss. The WeightWatcher tool extracts layer weight matrices at periodic checkpoints and computes the HTSR layer quality metric α by fitting power-law tails to the empirical spectral density (ESD) of W^T W. The method also identifies "correlation traps" by comparing randomized weight matrices to the Marchenko-Pastur distribution. The primary configuration uses weight decay WD=0, though a control run with WD=0.01 demonstrates how regularization suppresses anti-grokking.

## Key Results
- α decreases toward optimal value ≈2 during grokking phase, then further drops below 2 during anti-grokking
- HTSR metric α uniquely distinguishes all three phases while competing measures fail
- Anti-grokking characterized by "correlation traps" - anomalous eigenvalues in randomized weight matrices
- Layer-wise asynchronous convergence: FC2 layer typically shows α < 2 first during anti-grokking
- Weight norms and other metrics continue rising during collapse while α drops

## Why This Works (Mechanism)

### Mechanism 1: HTSR α as a Universal Phase Indicator
- **Claim:** The Heavy-Tailed Self-Regularization (HTSR) power-law exponent α tracks the generalization capability of a layer without requiring test data, distinguishing between pre-grokking, grokking, and anti-grokking phases.
- **Mechanism:** As a network trains, the eigenvalue density of the layer weight correlation matrices evolves. The exponent α characterizes the "heavy-tailedness" of this density. Large α indicates random-like weights; α ≈ 2 indicates strong, useful correlations (grokking); and α < 2 indicates over-correlation or overfitting (anti-grokking).
- **Core assumption:** The spectral density of well-generalizing layer weights follows a power-law distribution where the optimal exponent is approximately 2, and deviations imply suboptimal generalization states.
- **Evidence anchors:**
  - [abstract] "The HTSR metric α... uniquely tracks all three phases... decreases toward the optimal value of approximately 2 during the grokking phase and further drops below 2 during anti-grokking."
  - [section 4.1] "The sharp drop towards the optimal (fat-tailed) regime (2 ≲ α ≲ 5-6) coincides with the rapid improvement in test accuracy..."
  - [corpus] The paper "Late-Stage Generalization Collapse in Grokking" (neighbor) reinforces the detection of anti-grokking via these spectral metrics.
- **Break condition:** The mechanism fails if the power-law fit (specifically the selection of λ_min) is inaccurate, leading to a misleading α value (Section 3.1 warns λ_min selection is critical).

### Mechanism 2: Correlation Traps Signaling Collapse
- **Claim:** "Correlation traps" (anomalous eigenvalues in randomized weight matrices) serve as a structural signature of the anti-grokking phase and impending generalization collapse.
- **Mechanism:** When a model overfits in the late stage, the weight matrix elements shift such that their mean becomes "atypical" (E[W_ij] → large). If you randomize this matrix (destroying correlations but keeping the distribution properties), the eigenvalue spectrum deviates from the theoretical Marchenko-Pastur (MP) bulk, producing outlier "trap" eigenvalues.
- **Core assumption:** Overfitting in late training manifests as specific rank-one perturbations or shifts in the weight matrix distribution that are detectable via comparison to a randomized null model.
- **Evidence anchors:**
  - [abstract] "The anti-grokking phase is characterized by the appearance of 'correlation traps' - anomalous eigenvalues... that indicate overfitting."
  - [section 3.2] "Correlation Trap: λ_trap ≫ λ^+_rand... We identify these deviations as anomalously large eigenvalues..."
  - [section 4.2] "For both layers... neither layer shows evidence of correlation traps until the anti-grokking phase."
- **Break condition:** If the weight matrix does not exhibit a mean shift or rank-one perturbation during overfitting, this specific detection method may not trigger, even if generalization fails.

### Mechanism 3: Layer-wise Asynchronous Convergence
- **Claim:** Generalization collapse (anti-grokking) can originate from specific layers becoming over-correlated (α < 2) even if others remain stable, whereas pre-grokking is characterized by a mix of underfit (α ≳ 5) and converging layers.
- **Mechanism:** The network does not converge uniformly. In pre-grokking, some layers remain random-like (α ≈ 5). In grokking, layers synchronize near α ≈ 2. In anti-grokking, specific layers (e.g., FC2 in this study) "overshoot" into the Very-Heavy-Tailed (VHT) regime (α < 2), dragging down the system's generalization.
- **Core assumption:** Global generalization is bottlenecked by the lowest-quality (or most overfit) layer, and α < 2 is a reliable proxy for this pathological state.
- **Evidence anchors:**
  - [section 4.1] "This drop below α=2... occurs notably in the second fully connected layer (FC2)... suggesting that this instability might originate in specific layers."
  - [conclusion] "In the third anti-grokking phase... one or more layer will overfit the data... They will have α < 2..."
  - [corpus] The "SETOL" paper (neighbor) provides broader theoretical context for layer-wise quality metrics.
- **Break condition:** If the critical layer for generalization does not experience spectral degradation, or if the architecture is sufficiently redundant to mask a single layer's collapse, the system might not exhibit anti-grokking despite local α < 2.

## Foundational Learning

- **Concept:** Empirical Spectral Density (ESD)
  - **Why needed here:** The paper's primary diagnostic tool is analyzing the histogram of eigenvalues of the weight correlation matrices (W^T W). Understanding ESD is necessary to interpret the α metric and the visual "shapes" (MP bulk vs. Power Law tail) discussed.
  - **Quick check question:** Given a weight matrix W, how do you derive the eigenvalues used to plot the ESD, and what does the "tail" of this distribution represent?

- **Concept:** Power Law Distributions (α exponent)
  - **Why needed here:** The entire HTSR framework relies on fitting the tail of the ESD to a power law (ρ ~ λ^-α). You must understand that α quantifies the decay rate of the tail to interpret why α ≈ 2 is "optimal" and α < 2 implies "heavier" tails.
  - **Quick check question:** If a distribution transitions from α=5 to α=2, what does this imply about the prevalence of large eigenvalues in the matrix?

- **Concept:** Marchenko-Pastur (MP) Distribution
  - **Why needed here:** This serves as the "null hypothesis" for a random matrix. The paper defines "Correlation Traps" explicitly as deviations from this distribution in randomized weights.
  - **Quick check question:** What properties must a matrix have for its eigenvalues to follow the Marchenko-Pastur distribution, and what does it mean if a real matrix's spectrum *doesn't* follow it?

## Architecture Onboarding

- **Component map:** Trained model -> WeightWatcher -> (W extraction) -> (ESD computation) -> (Power Law fitter) -> (α estimation) OR (Randomizer) -> (MP fit) -> (Correlation trap detection)

- **Critical path:**
  1. Train model for extended steps (10^6+)
  2. Periodically save checkpoints
  3. Run WeightWatcher on checkpoints to compute α
  4. Monitor α: Wait for drop to ≈ 2 (Grokking)
  5. Alert: If α < 2 or Traps appear, flag as Anti-Grokking/Collapse

- **Design tradeoffs:**
  - **Generality vs. Specificity:** HTSR is a data-independent metric (pro), but α is a statistical estimate requiring careful fitting (con/manual inspection)
  - **Computation:** SVD calculation is O(N^3) or similar for large layers; this is not a "free" metric like weight norm. It is best used as a post-hoc diagnostic or periodic checkpoint tool, not a batch-wise regularizer

- **Failure signatures:**
  - **High Variance in α:** In pre-grokking, layers may show high run-to-run variability (Section 4.1)
  - **Dip below 2:** α < 2 is the specific signature of the "anti-grokking" collapse
  - **Traps:** KS-test p-value drops near zero for randomized MP fit (Appendix D)

- **First 3 experiments:**
  1. **Baseline Reproduction:** Train a 3-layer MLP on a small MNIST subset (1k samples) without weight decay for 10^7 steps. Plot α vs. Test Accuracy to visually confirm the α "dip and crash" pattern
  2. **Metric Comparison:** In the same run, plot Weight Norm (L2) and Activation Sparsity alongside α. Verify that L2 continues to rise during collapse while α drops, confirming α's superiority as a signal
  3. **Correlation Trap Validation:** Identify a checkpoint with α < 2. Extract the specific layer (e.g., FC2), randomize it, and plot the ESD against the MP fit. Verify the presence of "spikes" (traps) to the right of the MP bulk

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the anti-grokking phenomenon and the α < 2 warning sign generalize to modern architectures like Transformers and CNNs?
- **Basis in paper:** [explicit] The authors state that because the findings are derived from a specific MLP, they "warrant further validation across a wider range of model architectures (e.g., CNNs, Transformers), datasets, tasks, and diverse training configurations."
- **Why unresolved:** The empirical study was restricted to a 3-layer MLP on a small MNIST subset, leaving the behavior of other architectures unknown.
- **What evidence would resolve it:** Replicating the 10^7 step training regime on Transformer (e.g., language models) and CNN architectures to observe if α < 2 consistently precedes generalization collapse.

### Open Question 2
- **Question:** What is the precise functional mechanism by which "correlation traps" cause overfitting during the anti-grokking phase?
- **Basis in paper:** [explicit] The authors identify correlation traps as the cause but admit, "It is hypothesized that layers with large numbers of correlation traps are overfit to the training data (in some unspecified way)."
- **Why unresolved:** The paper characterizes the spectral properties (traps) but does not isolate the specific circuit behavior or memorization patterns these weight perturbations enforce.
- **What evidence would resolve it:** A mechanistic interpretability analysis (e.g., activation patching) comparing the circuits in the grokking phase versus the anti-grokking phase to identify the functional role of the trapped eigenvalues.

### Open Question 3
- **Question:** Can a differentiable regularizer based on the HTSR α metric be implemented to actively prevent anti-grokking?
- **Basis in paper:** [explicit] The conclusion suggests that "designing differentiable regularizers or loss terms based on α could potentially enable faster and more stable generalization."
- **Why unresolved:** The paper acts as a diagnostic study using the WeightWatcher tool; it does not propose or test a closed-loop training intervention.
- **What evidence would resolve it:** A training loop that includes a penalty term for α < 2 (or deviation from α ≈ 2) and successfully maintains high test accuracy beyond 10^6 steps.

## Limitations
- Anti-grokking phenomenon requires extensive training (10^7+ steps) and specific conditions (weight decay WD=0)
- α estimation depends critically on power-law fitting procedure, particularly λ_min selection
- Theoretical basis for why α≈2 represents optimal generalization remains phenomenological

## Confidence
- **High confidence:** α metric uniquely distinguishes grokking from anti-grokking phases (supported by consistent empirical patterns across multiple runs)
- **Medium confidence:** Correlation traps serve as definitive collapse indicators (detection requires careful randomized matrix analysis)
- **Medium confidence:** Layer-wise asynchronous convergence explains anti-grokking (plausible but needs broader architectural validation)

## Next Checks
1. Apply HTSR analysis to transformer architectures on language modeling tasks to test generalizability beyond MLPs
2. Conduct ablation studies varying initialization scales and weight decay to map the phase diagram boundaries
3. Compare α-based early warning signals against Bayesian uncertainty estimates to evaluate complementary detection methods