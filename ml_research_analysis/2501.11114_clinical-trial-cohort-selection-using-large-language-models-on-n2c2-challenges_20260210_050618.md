---
ver: rpa2
title: Clinical trial cohort selection using Large Language Models on n2c2 Challenges
arxiv_id: '2501.11114'
source_url: https://arxiv.org/abs/2501.11114
tags:
- clinical
- patient
- selection
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of large language models (LLMs)
  for clinical trial cohort selection using n2c2 challenge datasets. A two-stage approach
  compares various LLMs with different prompting strategies (basic, few-shot, and
  iterated few-shot learning) on the n2c2-2018 dataset, then fine-tunes the best model
  on n2c2-2006 and n2c2-2008 datasets.
---

# Clinical trial cohort selection using Large Language Models on n2c2 Challenges

## Quick Facts
- arXiv ID: 2501.11114
- Source URL: https://arxiv.org/abs/2501.11114
- Reference count: 40
- Primary result: Iterated few-shot learning with chain-of-thought prompting outperforms zero-shot approaches for clinical trial cohort selection on n2c2 datasets

## Executive Summary
This study evaluates large language models for clinical trial cohort selection using n2c2 challenge datasets. A two-stage approach compares various LLMs with different prompting strategies (basic, few-shot, and iterated few-shot learning) on the n2c2-2018 dataset, then fine-tunes the best model on n2c2-2006 and n2c2-2008 datasets. Results show promising performance for straightforward selection criteria (e.g., smoking status, aspirin use), with vicuna-13b and mistral-7b-instruct models outperforming others in stability. However, LLMs struggle with nuanced or imbalanced tasks (e.g., abdominal issues, decision-making). Performance often matches or exceeds traditional methods but highlights limitations in fine-grained reasoning, suggesting LLMs are useful for simple tasks but require further refinement for complex clinical scenarios.

## Method Summary
The study employs a two-stage approach to evaluate LLM performance on clinical trial cohort selection. Stage 1 compares 7 different LLM models (gpt-6j, medalpaca-7b, mistral-7b-instruct, mistral-7b, open-orca-mistral-7b, vicuna-13b, vicuna-7b) using three prompting strategies—basic, 5-shot with chain-of-thought, and iterated 5-shot—on the n2c2-2018 dataset. Stage 2 applies the best-performing model to n2c2-2006 and n2c2-2008 datasets. The method uses 5-fold cross-validation for model selection and evaluates performance using F1 score (macro and micro). The best performers were vicuna-13b and mistral-7b-instruct models, which showed superior stability across tasks compared to medical-specific models like MedAlpaca.

## Key Results
- Iterated few-shot learning with chain-of-thought prompting consistently outperformed zero-shot approaches across most criteria
- Vicuna-13b and mistral-7b-instruct models showed superior stability (lower variance) compared to other studied LLM models
- LLMs performed well on straightforward criteria (smoking status, aspirin use) but struggled with nuanced or imbalanced tasks (abdominal issues, decision-making)
- General-purpose LLMs outperformed medical-domain-specific models for this task

## Why This Works (Mechanism)

### Mechanism 1
Iterated few-shot learning with chain-of-thought prompting improves clinical criteria classification over zero-shot approaches by demonstrating reasoning patterns through selected examples from training data. The approach anchors model responses to task-specific logic rather than generic medical knowledge, showing that criteria follow detectable patterns that can be illustrated in 3-5 examples.

### Mechanism 2
General-purpose LLMs outperform medical-domain-specific models for cohort selection when using iterated prompting because medical LLMs may overfit to medical text patterns without learning task-specific reasoning. General models with better instruction-following capabilities leverage few-shot examples more effectively, suggesting the bottleneck is instruction-following and reasoning rather than medical knowledge depth.

### Mechanism 3
Classification stability (low variance across folds) better predicts reliable deployment than peak performance because high standard deviation across cross-validation folds indicates sensitivity to example selection. Models with lower variance (vicuna-13b) generalize better despite similar median scores, with fold variance corresponding to real-world prompt sensitivity.

## Foundational Learning

- **Concept**: Few-shot learning with chain-of-thought prompting
  - Why needed here: The study's best-performing approach uses explicit reasoning examples ("81 mg is a low dose, so the answer is Yes") to guide classification
  - Quick check question: Can you write a 3-shot prompt that teaches a model to distinguish "past smoker" vs. "current smoker" from clinical text?

- **Concept**: Cross-validation with imbalanced classes
  - Why needed here: n2c2 datasets have severe class imbalance (e.g., "Smoker" = 9 training samples vs. "Unknown" = 252); 5-fold CV helps assess whether performance generalizes
  - Quick check question: Why would micro-F1 be higher than macro-F1 on imbalanced medical classification tasks?

- **Concept**: n2c2 challenge datasets as clinical NLP benchmarks
  - Why needed here: These provide standardized, annotated clinical records with ground truth for comparing approaches across studies
  - Quick check question: What's the difference between "textual" and "intuitive" annotation tasks in n2c2-2008?

## Architecture Onboarding

- **Component map**: Clinical Text Input → Prompt Constructor (adds few-shot examples) → LLM (vicuna-13b/mistral-7b-instruct) → Binary/Multi-class Output → ↓ Example Selector (from training set, iterated)

- **Critical path**: Example selection quality → prompt construction → model inference → F1 evaluation; iterating on examples based on validation performance drives improvement

- **Design tradeoffs**:
  - Using local models (vicuna, mistral) vs. API models (GPT-4): Data privacy compliance (DUA restrictions) vs. potential performance gains
  - 5-shot vs. more examples: Limited context window vs. coverage of edge cases
  - Separate prompts per criterion vs. unified prompt: Better specialization vs. computational cost

- **Failure signatures**:
  - High variance across folds → prompt examples don't cover task diversity
  - Low F1 on specific criteria (ABDOM, DECISION, ENGLISH) → criteria require reasoning beyond pattern matching
  - Medical LLM underperforms general LLM → task requires instruction-following over domain knowledge

- **First 3 experiments**:
  1. Replicate the iterated few-shot approach on a single criterion (e.g., ASP-MI) using vicuna-13b; measure F1 and variance across 5 folds
  2. Compare 0-shot vs. 3-shot vs. 5-shot prompts on the same criterion to quantify few-shot benefit
  3. Test a failure case (ABDOM criterion) with additional domain-specific examples to diagnose whether performance gap is fixable via better prompting

## Open Questions the Paper Calls Out

### Open Question 1
Can the LLM-based cohort selection process be generalized effectively across diverse clinical domains outside of the specific n2c2 challenges? The study evaluated the models only on specific n2c2 datasets (2018, 2008, 2006), which focus on specific conditions and may not represent the variability of all clinical trial criteria. Successful replication on external clinical trial datasets involving different pathologies would resolve this.

### Open Question 2
Does full fine-tuning of open-source LLMs on longitudinal clinical text significantly outperform the few-shot prompting strategies used in this study? The current methodology relied on prompting pre-trained models without updating weights, and the authors note nuanced reasoning remains a difficulty. A comparative benchmark showing that a model fine-tuned on clinical notes achieves higher F1 scores would resolve this.

### Open Question 3
Why do general-purpose LLMs (e.g., Vicuna, Mistral) consistently outperform medical-specific LLMs (e.g., MedAlpaca) on these extraction tasks? The paper identifies the performance gap but does not analyze whether it stems from model size, quality of instruction tuning, or overfitting in medical-specific pre-training. An ablation study controlling for model size and training data composition would resolve this.

### Open Question 4
Can advanced prompt optimization techniques (e.g., Chain-of-Thought, automated optimization) resolve failures in nuanced reasoning without compromising the system's generalizability? The study used a consistent prompting strategy across all tasks, resulting in poor performance on complex tasks like decision-making. Implementation of an automated prompt optimizer that improves scores on low-performing tasks while maintaining a single generic system architecture would resolve this.

## Limitations
- The datasets used (n2c2 challenges) are relatively small and may not capture the full complexity of real-world clinical trial cohort selection
- The focus on simple criteria means the approach's effectiveness for more nuanced eligibility requirements remains untested
- The absence of human-in-the-loop evaluation means the models' outputs haven't been validated by clinical experts
- The reliance on specific prompting strategies without exploring alternative techniques may limit the approach's ceiling performance

## Confidence

- **High Confidence**: Iterated few-shot learning with chain-of-thought prompting improves performance over zero-shot approaches for straightforward selection criteria
- **Medium Confidence**: General-purpose LLMs outperform medical-domain-specific models for cohort selection when using iterated prompting
- **Low Confidence**: Classification stability (low variance across folds) better predicts reliable deployment than peak performance

## Next Checks

1. **Replicate with Expanded Criteria**: Apply the best-performing approach to additional n2c2 criteria or external datasets with more complex eligibility requirements to validate generalizability beyond simple tasks

2. **Human Expert Validation**: Conduct a blind evaluation where clinical experts assess model outputs against ground truth for a subset of criteria, measuring inter-rater reliability and identifying systematic errors

3. **Prompt Optimization Comparison**: Compare the iterated few-shot approach against alternative techniques like RAG or fine-tuning on the same datasets to quantify whether prompting alone represents the performance ceiling