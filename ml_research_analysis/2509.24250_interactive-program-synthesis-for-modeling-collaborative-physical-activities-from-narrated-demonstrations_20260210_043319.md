---
ver: rpa2
title: Interactive Program Synthesis for Modeling Collaborative Physical Activities
  from Narrated Demonstrations
arxiv_id: '2509.24250'
source_url: https://arxiv.org/abs/2509.24250
tags:
- system
- user
- program
- demonstrations
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces an interactive program synthesis system\
  \ for modeling collaborative physical activities from narrated demonstrations. The\
  \ system uses narrated demonstrations\u2014paired physical actions and natural language\u2014\
  as a unified modality for teaching, inspecting, and correcting system logic, enabling\
  \ non-programmers to externalize complex collaborative behaviors without writing\
  \ code."
---

# Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations

## Quick Facts
- arXiv ID: 2509.24250
- Source URL: https://arxiv.org/abs/2509.24250
- Reference count: 40
- Primary result: 70% of participants successfully refined learned programs from narrated soccer tactic demonstrations, with correctness improving from 70.28% to 77.15% and completeness from 76.57% to 93.25% after feedback.

## Executive Summary
This paper introduces an interactive program synthesis system that enables non-programmers to teach collaborative physical activities through narrated demonstrations in mixed reality. The system uses paired physical actions and natural language as a unified modality to reduce ambiguity in intent inference, synthesizing editable Finite State Machine programs that users can inspect and correct. In a within-subjects study with 20 participants teaching soccer tactics, 70% successfully refined programs to match their intent, and 90% found corrections easy, demonstrating that program representations with iterative human-in-the-loop refinement can effectively capture complex collaborative behaviors.

## Method Summary
The system records narrated demonstrations in mixed reality, grounding timestamped actions with transcribed narration. A Large Vision Language Model synthesizes Scenic probabilistic programs using a domain-specific API library of spatial constraints and actions, outputting hierarchical Finite State Machines. Users inspect and correct programs through Decision Flow visualizations and narrated executions, with the system iteratively repairing code based on multimodal feedback. The approach was evaluated on soccer tactics, measuring correctness and completeness against ground-truth FSMs before and after user feedback.

## Key Results
- 70% (14/20) of participants successfully refined learned programs to match their intent
- Correctness improved from 70.28% to 77.15% (p=0.10) and completeness from 76.57% to 93.25% (p=0.03) after feedback
- 90% (18/20) of participants found it easy to correct the programs
- 16/20 participants found the decision flow easy to understand and debug

## Why This Works (Mechanism)

### Mechanism 1
Pairing physical demonstrations with concurrent natural language reduces ambiguity in intent inference compared to single-modality inputs. Language disambiguates the "why" (intent) while physical action grounds the "how/where" (spatial trajectory), providing a richer signal for the synthesis engine. The core assumption is that users naturally narrate their decision-making process during physical tasks; if narration is absent or misleading, the grounding fails. Evidence shows this approach enables successful program repair, though it breaks when narration and action are out of sync.

### Mechanism 2
Representing learned behavior as an editable Finite State Machine rather than a black-box policy enables non-programmers to effectively inspect and repair logic. The system synthesizes a probabilistic program but exposes it as a Decision Flow diagram, allowing users to identify missing edges or incorrect preconditions. The core assumption is that FSM visualization remains interpretable as complexity grows; if the graph becomes too dense, user correction ability degrades. Evidence shows 16/20 participants found the visualization easy to understand, though it breaks with high task complexity leading to incomprehensible diagrams.

### Mechanism 3
Constrained synthesis using a domain-specific API library enables generalization from few examples by limiting the search space of possible programs. The Large Vision Language Model selects from and parameterizes predefined spatial constraints and actions rather than generating free-form code. The core assumption is that the API library covers requisite expressiveness for the target domain; behaviors outside the API vocabulary cannot be synthesized. Evidence shows this approach works for soccer tactics but breaks when users attempt behaviors not expressible in the API, such as curved trajectories.

## Foundational Learning

- **Finite State Machines (FSM)**
  - Why needed: The core internal representation is a state machine with preconditions; understanding "states" and "edges" is essential for debugging the Decision Flow.
  - Quick check: Can you draw a simple state diagram for "If the defender is close, pass; otherwise, shoot"?

- **Probabilistic Programming (Scenic)**
  - Why needed: The system generates programs that sample behaviors, not deterministic paths; understanding distributions explains why replays vary.
  - Quick check: If you run the generated program twice, will the agent move to the exact same coordinates?

- **Grounding (Symbol Grounding Problem)**
  - Why needed: This is the central challengeâ€”mapping abstract symbols (language) to continuous physical reality (demonstration).
  - Quick check: Why is the command "move over there" ambiguous to a robot without a gesture or specific coordinate?

## Architecture Onboarding

- **Component map:** MR Headset (video, controller actions, audio) -> Grounding module (aligns timestamps) -> LVLM + API Library (generates Scenic code) -> Unity Simulation (executes Scenic program) -> Decision Flow Visualization (web/desktop) + MR Replay

- **Critical path:** 1) User records narrated demo in MR, 2) System grounds transcript to video/actions, 3) LVLM synthesizes Scenic program, 4) System visualizes FSM and executes replay, 5) User annotates FSM or replay, 6) LVLM repairs code based on feedback

- **Design tradeoffs:** API expressiveness vs. ease of synthesis (richer API increases complexity and wrong primitive selection), visual FSM vs. code (hiding code helps non-programmers but creates abstraction gaps for specific logical syntax errors)

- **Failure signatures:** Deadlocks (agent freezes when preconditions never met), context loss (repair without original demo causes inconsistent edits), hallucinated constraints (LLM invents invalid constraints leading to runtime errors)

- **First 3 experiments:** 1) Validate grounding: Record "I am moving to [X]" while moving to [X], check transcript-action alignment, 2) Test API constraints: Inspect DistanceTo API to determine if it returns Boolean or distribution, 3) Probe deadlock: Teach impossible condition ("Wait for opponent") without spawning opponent, observe if system flags deadlock or hangs

## Open Questions the Paper Calls Out
- How can program logic visualization remain interpretable as collaborative activity complexity and agent numbers increase? The authors suggest hierarchical representations but algorithms for determining abstraction levels remain uninvestigated.
- Can formal languages be integrated to mathematically detect logical inconsistencies and ambiguity without compromising real-time usability? It's unclear if existing formal languages can capture relevant spatial/temporal constraints efficiently enough.
- How can generative models synthesize code routines that automatically detect and explain execution deadlocks to non-programmer users? The current system lacks mechanisms to communicate why execution halts.
- What interaction techniques can provide real-time feedback during initial teaching to bridge the gap between demonstrating to a system versus a live human? The lack of two-way feedback causes cognitive strain as users feel they're "talking to the void."

## Limitations
- The API library contents and annotations provided to the LVLM are not fully specified, hindering faithful reproduction.
- Scalability to more complex collaborative activities with larger state spaces or finer-grained actions remains untested.
- The 2/20 deadlock failures highlight potential brittleness in precondition handling that may become more frequent with increased task complexity.

## Confidence
- **High confidence**: The core mechanism of using narrated demonstrations to reduce ambiguity in intent inference is well-supported by user study results showing successful program repair.
- **Medium confidence**: The FSM visualization approach for enabling non-programmers to inspect and correct logic is supported by positive user feedback but has scalability concerns for complex tasks.
- **Medium confidence**: The constrained synthesis approach using a domain-specific API library is theoretically sound but the paper provides limited evidence of its generalizability beyond the soccer domain.

## Next Checks
1. Test the grounding mechanism with scenarios where narration and physical action are deliberately out of sync to measure ambiguity reduction effectiveness.
2. Evaluate the system's ability to handle tasks with significantly larger state spaces than soccer tactics to assess FSM visualization scalability.
3. Conduct ablation studies removing either the natural language component or the FSM visualization to quantify their individual contributions to program repair success.