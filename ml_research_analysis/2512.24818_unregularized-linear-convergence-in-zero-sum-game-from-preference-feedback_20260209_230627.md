---
ver: rpa2
title: Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback
arxiv_id: '2512.24818'
source_url: https://arxiv.org/abs/2512.24818
tags:
- dual
- convergence
- lemma
- preprint
- ptqq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the convergence properties of the Optimistic
  Multiplicative Weights Update (OMWU) algorithm for solving non-transitive preference
  games in the context of aligning large language models (LLMs) with human preferences.
  While existing methods typically rely on regularization, which introduces bias when
  computing the duality gap in the original game, OMWU offers a regularization-free
  approach.
---

# Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback

## Quick Facts
- **arXiv ID:** 2512.24818
- **Source URL:** https://arxiv.org/abs/2512.24818
- **Reference count:** 40
- **Primary result:** OMWU achieves last-iterate linear convergence after burn-in for non-transitive preference games when full-support Nash equilibrium exists.

## Executive Summary
This work studies convergence properties of the Optimistic Multiplicative Weights Update (OMWU) algorithm for solving non-transitive preference games in large language model alignment. Unlike existing methods that rely on regularization (introducing bias when computing duality gaps), OMWU provides a regularization-free approach. The authors prove that OMWU achieves last-iterate linear convergence after a burn-in phase whenever a full-support Nash equilibrium exists, with polynomial dependence on instance-dependent constants. This improves upon prior work by removing the uniqueness assumption for Nash equilibria and demonstrates OMWU's potential for LLM alignment applications through experiments on both tabular and neural policy classes.

## Method Summary
The method addresses finding Nash equilibria in two-player zero-sum preference games using OMWU, which combines standard MWU with optimism (using predicted payoffs from previous iteration). The algorithm operates on preference matrices P (skew-symmetric, entries in [-1/2, 1/2]) and maintains policy parameters θ that directly parametrize π via softmax. The OMWU update involves two steps: prediction (θ^t = θ^{t-1} + η·P·π̂^{t-1}) and update (π̂^{t+1} = softmax(θ^t + η·P·π^t)). Convergence is measured via duality gap: DualGap(π) = 2·max_a (Pπ)_a. The analysis introduces a novel framework tracking KL divergence to the Nash equilibrium and identifies "marginal convergence behavior" where rarely played actions' probabilities grow exponentially from exponentially small values.

## Key Results
- OMWU achieves last-iterate linear convergence after burn-in phase for preference games with full-support Nash equilibria
- Polynomial (not exponential) dependence on instance-dependent constants for both burn-in time and convergence rate
- KL projection of OMWU iterates onto Nash equilibrium set remains constant throughout training
- Experimental validation on both tabular and neural policy classes demonstrates theoretical advantages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** OMWU achieves last-iterate linear convergence to full-support Nash equilibrium after burn-in
- **Mechanism:** Potential Θ_t = D_KL(π*||π̂^t) + 4η²L² D_KL(π̂^t||π^{t-1}) strictly decreases when ηL < 1/2, ensuring duality gap shrinks exponentially after burn-in
- **Core assumption:** Full-support Nash equilibrium exists (π*_a > 0 for all actions a)
- **Evidence anchors:** Theorem 2 proves burn-in time T with D_KL(π*||π̂^{t}) ≤ ε exp(-O(...)(t-T)) for all t ≥ T
- **Break condition:** If no full-support equilibrium exists, ε = 0 and convergence bounds fail

### Mechanism 2
- **Claim:** KL projection of OMWU iterates onto Nash equilibrium set remains constant
- **Mechanism:** Update rule preserves D_KL(π*||π̂^{t+1}) - D_KL(π*||π̂^t) up to normalization, implying p(π̂^1) = p(π̂^2) = ... = p(π̂^t)
- **Core assumption:** Assumption 1 (full-support equilibrium exists)
- **Evidence anchors:** Lemma 2 proves constant KL projection throughout training
- **Break condition:** If Assumption 1 fails, KL projection may not be well-defined

### Mechanism 3
- **Claim:** Polynomial dependence on instance-dependent constants
- **Mechanism:** Analysis identifies "marginal convergence behavior" and bounds escaping dynamics using potential Φ_t = ⟨log π̂^t - log π*, ηPπ̂^t⟩/(Θ_t + 2e^{-1})²
- **Core assumption:** Instance-dependent constant C_P = min_{π∈Δ(A)\M} ||Pπ||_∞ / ||π - p(π)||_1 > 0
- **Evidence anchors:** Comparison shows O(exp(-4 ln(A)/ε)) vs O(ε^{-6}) polynomial dependence
- **Break condition:** Complex equilibrium polytope geometry may make C_P very small

## Foundational Learning

- **Concept: Duality Gap**
  - **Why needed here:** Measures convergence target - zero iff π is Nash equilibrium
  - **Quick check question:** Given π and P, can you compute DualGap(π) and determine if π is near equilibrium?

- **Concept: KL Divergence and Pinsker's Inequality**
  - **Why needed here:** Convergence proven via D_KL(π*||π̂^t), which bounds duality gap via Pinsker's inequality
  - **Quick check question:** Can you state Pinsker's inequality and explain why paper proves KL convergence rather than direct duality gap convergence?

- **Concept: Multiplicative Weights Update and Optimism**
  - **Why needed here:** OMWU adds optimism to standard MWU for stabilized convergence in zero-sum games
  - **Quick check question:** What distinguishes OMWU's two update equations and why does optimism help?

## Architecture Onboarding

- **Component map:** Preference Matrix P -> Policy Parameters θ -> OMWU Update Loop -> Duality Gap Computation

- **Critical path:** Preference matrix P must be constructed correctly (skew-symmetric with P_{a,a'} = P(a > a') - 1/2). Generalized IPO loss provides gradient signal for neural training.

- **Design tradeoffs:**
  - Learning rate η: Must satisfy ηL < 1/2; larger η faster convergence but requires smaller L
  - Initialization: Uniform gives minimum negative entropy equilibrium; custom affects burn-in time
  - Tabular vs neural: Tabular provides exact updates; neural introduces function approximation error

- **Failure signatures:**
  - Duality gap oscillates without converging → η too large or no full-support equilibrium
  - Convergence to wrong point → initialization KL projection differs from expected
  - Slow/no convergence in neural setting → gradient approximation quality insufficient
  - Nested optimization algorithms fail → insufficient inner loop steps

- **First 3 experiments:**
  1. Implement OMWU on 3×3 cyclic preference matrix with uniform initialization; verify Θ_t decreases and duality gap reaches < 0.01 within 1000 iterations
  2. Plot Θ_t - Θ_{t+1} over time for matrix with known ε and C_P values; confirm two-phase behavior (oscillatory burn-in followed by near-linear decay)
  3. Run OMWU alongside OMD and regularized OMD on same preference matrix; verify OMWU achieves last-iterate linear convergence while OMD shows only average-iterate convergence

## Open Questions the Paper Calls Out

- **Open Question 1:** Can last-iterate linear convergence be extended to preference matrices without full-support Nash equilibria?
  - **Basis:** Authors note limitation and leave generalization to future research
  - **Why unresolved:** Current analysis relies on ε = min_a π*_a > 0 to bound marginal case
  - **What evidence would resolve it:** Modified analysis achieving convergence without full-support assumption, or lower bound showing impossibility

- **Open Question 2:** What are the tightest orders for burn-in time T and linear convergence rate in Theorem 2?
  - **Basis:** Authors acknowledge bounds are not tightest and leave improvement to future work
  - **Why unresolved:** Neither burn-in nor convergence phase is analyzed tightly
  - **What evidence would resolve it:** Lower bound constructions matching current upper bounds, or improved upper bounds with tight matching lower bounds

- **Open Question 3:** Can theoretical convergence guarantees be validated in actual LLM fine-tuning with real human preference data?
  - **Basis:** Authors unable to reproduce results on LLM fine-tuning due to resource constraints
  - **Why unresolved:** Experiments validated only on tabular and neural policy classes with synthetic matrices
  - **What evidence would resolve it:** Empirical demonstration on standard LLM alignment benchmark with real preference feedback

## Limitations

- Analysis critically relies on existence of full-support Nash equilibrium, which may not hold in real preference data
- Polynomial dependence on instance-dependent constants could lead to impractically long burn-in times for complex preference geometries
- Neural policy implementation introduces approximation errors not covered by theory; experiments use small MLPs that may not scale to production LLM alignment

## Confidence

- **High Confidence:** Last-iterate linear convergence of OMWU for tabular preference games with full-support equilibria (rigorous proof and comprehensive experimental validation)
- **Medium Confidence:** Polynomial dependence on instance parameters is strictly better than exponential but may still be impractical (theory assumes perfect gradient information while neural experiments use sampled IPO loss)
- **Low Confidence:** Neural policy generalization (experiments use small MLPs and simple preference matrices, leaving open questions about performance with complex, high-dimensional preference spaces)

## Next Checks

1. **Robustness to Missing Support:** Test OMWU on preference matrices designed to lack full-support equilibria (e.g., cyclic preferences with one universally losing action). Measure whether algorithm converges to low-duality-gap solution or fails predictably.

2. **Burn-in Time Scaling:** Systematically vary matrix dimensions (n, m) and measure actual burn-in times versus theoretical bounds. Plot T_μ(n, m, ε) to verify polynomial scaling and identify practical limits where burn-in becomes prohibitive.

3. **Neural Policy Scaling:** Implement OMWU with larger neural architectures (deeper networks, more parameters) on more complex preference structures. Compare convergence behavior and duality gap achievement to tabular case to assess approximation error and scalability limits.