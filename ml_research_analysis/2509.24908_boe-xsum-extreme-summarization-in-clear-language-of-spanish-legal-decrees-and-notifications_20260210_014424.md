---
ver: rpa2
title: 'BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees
  and Notifications'
arxiv_id: '2509.24908'
source_url: https://arxiv.org/abs/2509.24908
tags:
- summaries
- dataset
- language
- summarization
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of extreme summarization datasets
  for Spanish legal texts by introducing BOE-XSUM, a curated dataset of 3,648 concise
  summaries of Spain's State Official Gazette (BOE) articles. The dataset includes
  both the original BOE documents and journalist-written summaries, refined into clear,
  everyday language.
---

# BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees and Notifications

## Quick Facts
- **arXiv ID:** 2509.24908
- **Source URL:** https://arxiv.org/abs/2509.24908
- **Reference count:** 39
- **Primary result:** Fine-tuned BERTIN GPT-J 6B achieves 24% better performance than zero-shot DeepSeek-R1 on Spanish legal text summarization

## Executive Summary
This paper introduces BOE-XSUM, a novel dataset of 3,648 concise summaries of Spanish legal decrees from Spain's State Official Gazette (BOE). The dataset includes both original legal documents and journalist-written summaries refined into clear, everyday language, achieving an extreme compression rate of 0.005% (average 17 words per summary). The authors evaluate fine-tuned medium-sized language models against general-purpose models in zero-shot settings, demonstrating that task-specific fine-tuning significantly outperforms larger general models when properly adapted to the domain.

## Method Summary
The authors curated BOE-XSUM by collecting original BOE articles and their corresponding journalist-written summaries, which were then refined into clear, everyday language. The dataset was used to fine-tune medium-sized language models, which were evaluated against general-purpose models using BERTScore metrics. The evaluation compared fine-tuned models specifically trained on BOE-XSUM against zero-shot performance of general models on the same summarization task.

## Key Results
- BERTIN GPT-J 6B fine-tuned on BOE-XSUM achieved 41.6% BERTScore, outperforming zero-shot DeepSeek-R1 at 33.5%
- Fine-tuned models showed 24% performance improvement over general-purpose models
- Smaller, task-specific fine-tuned models can outperform much larger general models when properly adapted to the domain
- The dataset enables extreme summarization with 0.005% compression rate (17 words average length)

## Why This Works (Mechanism)
The extreme summarization task works effectively because the fine-tuned models learn the specific patterns and language conventions used in BOE summaries, capturing the essential information while maintaining clarity in everyday language. The journalist-written summaries serve as high-quality reference standards that emphasize practical understanding over legal jargon, enabling models to produce more accessible outputs.

## Foundational Learning
- **BERTScore:** Metric measuring similarity between generated and reference summaries using BERT embeddings; needed for automated evaluation when human evaluation is impractical
- **Zero-shot learning:** Model performance without task-specific training; quick check shows general models struggle with legal domain specificity
- **Fine-tuning:** Adapting pre-trained models to specific tasks; enables significant performance gains over general models
- **Extreme compression:** Reducing text to minimal essential content (0.005% ratio); challenges information preservation but improves accessibility
- **Legal text summarization:** Specialized domain requiring understanding of legal terminology and structure; standard summarization techniques often fail
- **Domain adaptation:** Training models on specific data types; critical for achieving high performance in specialized tasks

## Architecture Onboarding

**Component map:** BOE articles -> Summarization model -> Journalist-written summaries (reference) -> Fine-tuned model -> Evaluation metrics

**Critical path:** Data collection and refinement → Model fine-tuning → Evaluation → Performance comparison

**Design tradeoffs:** Extreme compression vs. information preservation; clear language vs. legal accuracy; model size vs. performance

**Failure signatures:** Over-compression losing critical legal information; failure to maintain clarity in everyday language; poor adaptation to legal domain

**First experiments:** 1) Test different compression ratios to find optimal balance, 2) Compare automatic metrics with human evaluation, 3) Evaluate cross-domain performance on different legal document types

## Open Questions the Paper Calls Out
- Developing category classifiers for BOE documents
- Refining evaluation metrics for legal summarization
- Exploring few-shot learning for general-purpose models
- Assessing information preservation at extreme compression rates

## Limitations
- Limited to Spanish legal texts from a single publication (BOE)
- Reliance on journalist-written summaries may introduce bias
- Lack of human evaluation to validate automatic metrics
- Uncertainty about information loss at extreme compression rates

## Confidence
- **High confidence** in dataset utility for Spanish legal summarization tasks
- **High confidence** in documented performance improvements from fine-tuning
- **Medium confidence** in generalization claims beyond BOE domain
- **Medium confidence** in information preservation at 0.005% compression rate

## Next Checks
1. Conduct human evaluation studies comparing fine-tuned model outputs against reference summaries to validate automatic metrics and assess information preservation
2. Test fine-tuned models on different types of Spanish legal documents (court decisions, contracts, regulations) to evaluate domain generalization
3. Perform ablation studies varying the compression ratio to determine optimal balance between brevity and information retention for legal applications