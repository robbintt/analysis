---
ver: rpa2
title: 'Behaviour Policy Optimization: Provably Lower Variance Return Estimates for
  Off-Policy Reinforcement Learning'
arxiv_id: '2511.10843'
source_url: https://arxiv.org/abs/2511.10843
tags:
- policy
- variance
- learning
- behaviour
- gtis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes Behaviour Policy Optimization (BPO), a variance\
  \ reduction technique for online reinforcement learning that uses a well-designed\
  \ behaviour policy to collect off-policy data with provably lower variance return\
  \ estimates. The method extends recent off-policy evaluation results by Liu and\
  \ Zhang (2024) to the online setting, optimizing a behaviour policy that minimizes\
  \ the variance of truncated IS weighted TD(\u03BB) returns while preserving unbiasedness."
---

# Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.10843
- Source URL: https://arxiv.org/abs/2511.10843
- Reference count: 40
- Primary result: Behaviour Policy Optimization (BPO) achieves lower-variance return estimates for online RL by optimizing a behavior policy μ to match π/√q̂π, with empirical validation showing improved sample efficiency over PPO baselines on MuJoCo environments.

## Executive Summary
This paper introduces Behaviour Policy Optimization (BPO), a variance reduction technique for online reinforcement learning that optimizes a behavior policy to collect off-policy data with provably lower variance return estimates. The method extends recent off-policy evaluation results by Liu and Zhang (2024) to the online setting, using truncated IS-weighted TD(λ) returns and two additional Q-networks to estimate both standard state-action values and a variance-aware variant. Experiments on MuJoCo environments demonstrate that BPO consistently improves sample efficiency and final performance over PPO baselines while maintaining theoretical variance reduction guarantees.

## Method Summary
BPO extends PPO by adding two Q-networks and a behavior policy. The method uses FQE to learn Qζ (estimating qπ) and Q̂ζ̂ (estimating q̂π with modified rewards r̂ = 2rqπ - r² and discount γ²). The behavior policy μξ is optimized to match π/√Q̂, and returns are estimated using truncated IS-weighted TD(λ) with parameters c̄ and ρ̄. The target policy πθ is updated via PPO using these variance-reduced returns. All components are trained concurrently with experience stored in a replay buffer.

## Key Results
- BPO achieves 1690±125 episodic reward on HalfCheetah-v5 compared to 3431±431 for PPO baseline
- Method shows faster convergence in early training stages across all tested MuJoCo environments
- Variance reduction is empirically verified through improved stability in later training stages
- Performance improvements are consistent across Ant-v5, HalfCheetah-v5, Hopper-v5, and Walker2d-v5 environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sampling from a carefully designed behaviour policy μ can yield lower-variance return estimates than on-policy sampling from π itself.
- **Mechanism:** The behaviour policy is optimized to match μ*(a|s) ∝ π(a|s)/√q̂π(s,a), where q̂π is a variance-aware Q-function. This distribution oversamples actions with high importance under π but low variance contribution, reducing the variance of IS-weighted return estimates. The variance reduction follows from Owen's optimal sampling theory applied to the RL setting via Theorem 2.
- **Core assumption:** The variance-aware Q-function q̂π can be accurately estimated from finite data and tracks the non-stationary target policy.
- **Evidence anchors:**
  - [abstract]: "well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates"
  - [section]: Theorem 2 proves V_μ̂(G_t^TIS,λ | S_t=s) ≤ V_π(G_t^TIS,λ | S_t=s) - ε(s), where ε(s) ≥ 0.
  - [corpus]: Related OPE work (Liu and Zhang 2024) demonstrates this principle in offline settings; corpus lacks direct replications of concurrent online optimization.
- **Break condition:** If q̂π estimates are inaccurate or stale, μ may sample suboptimally, potentially increasing variance rather than reducing it.

### Mechanism 2
- **Claim:** Truncated IS-weighted TD(λ) returns provide controllable bias-variance tradeoffs while preserving unbiasedness under appropriate truncation settings.
- **Mechanism:** The estimator G_t^TIS,λ uses two truncation parameters: c̄ for trace-cutting and ρ̄ for TD-error weighting. When c̄=ρ̄=∞ and μ∈Λ (covers π), Theorem 1 guarantees unbiasedness: E_μ[G_t^TIS,λ | S_t=s] = v^π(s). In practice, moderate truncation (c̄, ρ̄ ∈ {1.0, 1.5}) accepts small bias for stable learning.
- **Core assumption:** Truncation levels are set high enough that bias remains negligible relative to variance reduction benefits.
- **Evidence anchors:**
  - [abstract]: "truncated IS weighted TD(λ) returns"
  - [section]: Lemma 3 states E_μ[G_t^TIS,λ | S_t=s] = E_π[G_t] when c̄,ρ̄≥1 and μ=π; Theorem 1 extends to any μ∈Λ when c̄=ρ̄=∞.
  - [corpus]: IMPALA (V-trace) and ACER use similar truncation strategies; corpus confirms this is established practice but notes BPO's truncation is "less aggressive."
- **Break condition:** Aggressive truncation (c̄, ρ̄ << 1) introduces substantial bias, potentially causing convergence to v^μ rather than v^π.

### Mechanism 3
- **Claim:** Learning modified rewards r̂π = 2rq^π - r² enables q̂π estimation via standard Q-learning with discounted factor γ².
- **Mechanism:** Theorem 3 establishes that q̂π satisfies a Bellman equation with modified rewards r̂π and discount γ². This allows FQE to learn q̂π using the same machinery as q^π, just with transformed rewards. Symlog targets stabilize training when Q and q̂ magnitudes differ significantly.
- **Core assumption:** The transformation preserves sufficient signal-to-noise ratio for effective learning; q^π estimates are accurate enough to compute r̂π.
- **Evidence anchors:**
  - [section]: Theorem 3 derivation shows q̂π(s,a) = r̂π(s,a) + γ²E[r̂π(s',a')]; Appendix A.7 provides proof.
  - [section]: Appendix B ablation (Figure 5) shows performance degrades without symlog targets, validating their importance.
  - [corpus]: No corpus papers directly address this specific reward transformation; theoretical validation is internal to this work.
- **Break condition:** If q^π is underestimated, r̂π becomes inaccurate → q̂π becomes inaccurate → μ optimization fails.

## Foundational Learning

- **Concept: Importance Sampling and Per-Decision IS (PDIS)**
  - Why needed here: Core mathematical tool enabling off-policy data collected by μ to estimate returns for π; PDIS is the foundation for the TIS estimator.
  - Quick check question: Derive why E_μ[ρ_t f(S_t,A_t)] = E_π[f(S_t,A_t)] when μ covers π. What happens to variance as trajectory length increases?

- **Concept: Owen's Variance-Optimal Sampling (Lemmas 1-2)**
  - Why needed here: Provides the theoretical basis for why q*(x) ∝ p(x)|f(x)| minimizes variance; explains when zero-variance sampling is achievable.
  - Quick check question: For f(x) ≥ 0 everywhere, explain why sampling from q*(x) ∝ p(x)f(x) yields zero-variance estimates. Why is this impractical in RL?

- **Concept: TD(λ) and Eligibility Traces**
  - Why needed here: BPO's returns estimator combines TD(λ) with IS; understanding the bias-variance tradeoff of λ is critical for setting hyperparameters.
  - Quick check question: How does varying λ from 0 to 1 affect bias and variance of return estimates? Why does the paper recommend λ close to 1?

## Architecture Onboarding

- **Component map:**
  - Target policy π_θ: Actor network updated via PPO/REINFORCE with IS-corrected advantages
  - Behaviour policy μ_ξ: Separate actor network; discrete actions use cross-entropy loss, continuous uses log-prob loss
  - Q-network Q_ζ: Estimates q^π via FQE with symlog targets
  - Variance-Q network Q̂_ζ̂: Estimates q̂π via FQE with modified rewards r̂ and γ² discount
  - Replay buffer D: Stores recent (s,a,r,s') tuples; size 8192 for MuJoCo

- **Critical path:**
  1. Initialize θ, ξ, ζ, ζ̂ identically (same seed)
  2. Collect rollout with μ_ξ → populate D
  3. Update π_θ using PPO with ratios π_θ/μ_ξ and G^TIS,λ returns
  4. Update Q_ζ via FQE (n_qvf_epochs iterations)
  5. Compute r̂_t = 2r_t Q_ζ - r_t²
  6. Update Q̂_ζ̂ via FQE with r̂ and γ²
  7. Update μ_ξ to match π_θ/√Q̂_ζ̂ (n_mu_epochs iterations)
  8. Repeat from step 2

- **Design tradeoffs:**
  - Truncation (c̄, ρ̄): Higher = less bias/more variance. Paper finds c̄=1.0, ρ̄=1.5 works best.
  - Replay buffer size: Larger = better Q-estimates but slower policy adaptation. 8192 is a balance.
  - Q-update epochs: More epochs = better estimates but 2× wall-clock overhead. Paper uses 20-40.
  - Symlog targets: Essential for stability when Q̂ magnitudes >> Q magnitudes (Appendix B, Figure 5).

- **Failure signatures:**
  - Q̂_ζ̂ divergence → μ becomes erratic → training collapse. Check symlog regularization and zero-init.
  - μ ≈ π (no variance benefit) → Q̂ underestimates high-value actions. Verify Q_ζ learning is stable.
  - Returns exploding → IS ratios too large. Reduce c̄, ρ̄ or check μ coverage of π.
  - Slow convergence → Q-networks not tracking π. Increase n_qvf_epochs or decrease Polyak τ.

- **First 3 experiments:**
  1. **Unbiasedness validation:** On HalfCheetah, compare BPO (c̄=ρ̄=∞, λ=1) vs on-policy PPO mean returns over 100K steps. Expect convergence to similar values but slower BPO convergence due to IS variance.
  2. **Variance reduction measurement:** Track Var[G^TIS,λ] during training with c̄=ρ̄={1.0, 1.5, ∞}. Verify variance decreases relative to on-policy baseline in early training (Figure 3 shows faster BPO convergence).
  3. **Ablate Q̂ contribution:** Compare BPO vs. BPO without Q̂ in loss. Reproduce Appendix B Figure 4 to confirm Q̂'s role in variance-optimal sampling.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can BPO improve sample efficiency for value-based off-policy algorithms such as DDPG, TD3, and SAC?
- **Basis in paper:** [explicit] Conclusion states: "It would also be worth exploring if our methodology is useful for value-based methods, e.g., DDPG (Silver et al. 2014), TD3 (Fujimoto, Hoof, and Meger 2018) and SAC (Haarnoja et al. 2018)."
- **Why unresolved:** BPO was only evaluated on policy-gradient methods (REINFORCE, PPO); value-based methods have different gradient estimation structures that may interact differently with variance-reduced behavior policies.
- **What evidence would resolve it:** Experiments applying BPO-style behavior policy optimization to actor-critic algorithms, measuring sample efficiency gains against standard implementations.

### Open Question 2
- **Question:** Can model-based RL enable globally optimal behavior policy design rather than one-step optimal policies?
- **Basis in paper:** [explicit] Conclusion states: "We might also use model-based RL (Sutton 1990) to assist in designing globally optimal behavior policies." Additionally, Section 3.3 notes that global optimality "requires an estimate of the transition probabilities (Liu and Zhang 2024) (model-based)."
- **Why unresolved:** The current one-step optimal policy (Eq. 5) is a local approximation; achieving global variance optimality requires transition dynamics knowledge that BPO explicitly avoids assuming.
- **What evidence would resolve it:** Integration of learned dynamics models to compute multi-step variance-optimal policies, with empirical comparison showing whether global optimization yields further variance reduction.

### Open Question 3
- **Question:** How robust is BPO to approximation errors in the Q-network estimates (Qζ, Q̂ζ) used for behavior policy optimization?
- **Basis in paper:** [inferred] Section 5.2 acknowledges "Value-based approaches, including FQE, are known to be prone to overestimation and approximation bias (Fujimoto et al. 2022)." Multiple stabilization techniques (symlog targets, Polyak averaging, layer norm) are employed, but systematic analysis of error propagation is absent.
- **Why unresolved:** The theoretical variance reduction guarantees assume exact q̂π estimates; the impact of function approximation errors on the provable variance reduction bound (Theorem 2) remains uncharacterized.
- **What evidence would resolve it:** Controlled experiments varying Q-network capacity and accuracy, measuring degradation in variance reduction as approximation error increases; theoretical bounds incorporating approximation error terms.

## Limitations
- The paper's theoretical guarantees rely on accurate estimation of the variance-aware Q-function q̂π, but finite-sample error analysis is not provided.
- The method introduces 2× computational overhead compared to standard PPO due to dual Q-network training.
- Coverage violations in early training could lead to high-variance estimates when μ fails to cover π adequately.

## Confidence
- **High confidence**: Variance reduction mechanism and theoretical bounds (Theorem 2). The connection between Owen's optimal sampling and behavior policy optimization is mathematically rigorous.
- **Medium confidence**: Practical effectiveness and sample efficiency gains. While experimental results show consistent improvements, the magnitude varies across environments and the comparison is limited to PPO baselines without testing against other state-of-the-art methods.
- **Medium confidence**: Implementation details and hyperparameter sensitivity. The paper provides specific settings that work well, but the robustness to different choices and environments is not thoroughly characterized.

## Next Checks
1. **Finite-sample error analysis**: Characterize how estimation errors in Q̂_ζ̂ propagate to variance reduction guarantees and training stability, particularly in early training stages when Q̂ estimates are less accurate.
2. **Coverage verification**: Systematically measure μ's coverage of π throughout training and quantify the impact of coverage violations on return estimates and policy performance.
3. **Computational overhead evaluation**: Benchmark wall-clock time per training iteration and analyze the tradeoff between improved sample efficiency and increased computational cost across different batch sizes and environment complexities.