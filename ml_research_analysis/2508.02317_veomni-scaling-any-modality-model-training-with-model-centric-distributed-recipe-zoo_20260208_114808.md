---
ver: rpa2
title: 'VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed
  Recipe Zoo'
arxiv_id: '2508.02317'
source_url: https://arxiv.org/abs/2508.02317
tags:
- training
- veomni
- omni-modal
- arxiv
- fsdp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VeOmni addresses the challenge of efficiently training large-scale
  omni-modal language models (LLMs) that handle multiple heterogeneous modalities.
  It introduces a model-centric distributed training framework that decouples model
  definition from parallel logic, enabling scalable and flexible omni-modal training.
---

# VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo

## Quick Facts
- **arXiv ID**: 2508.02317
- **Source URL**: https://arxiv.org/abs/2508.02317
- **Reference count**: 40
- **Primary result**: 30B parameter omni-modal MoE model achieves over 2,800 tokens/sec/GPU throughput and scales to 160K context lengths on 128 GPUs

## Executive Summary
VeOmni addresses the challenge of efficiently training large-scale omni-modal language models (LLMs) that handle multiple heterogeneous modalities. It introduces a model-centric distributed training framework that decouples model definition from parallel logic, enabling scalable and flexible omni-modal training. VeOmni supports composable n-D parallelism strategies (FSDP, SP, EP) and provides a lightweight interface for integrating modality-specific encoders and decoders. Using VeOmni, a 30B parameter omni-modal MoE model achieves over 2,800 tokens/sec/GPU throughput and scales to 160K context lengths on 128 GPUs, demonstrating superior efficiency and scalability for training large omni-modal LLMs.

## Method Summary
VeOmni introduces a model-centric distributed training framework that decouples model definition from parallelization logic through a `ParallelPlan` API. The framework applies distributed strategies (FSDP, SP, EP) declaratively without modifying model code, using a global `DeviceMesh` abstraction to coordinate cross-strategy communication. VeOmni implements fine-grained operator-level communication-computation overlapping for MoE and sequence-parallel training, and provides `OmniModel` to orchestrate encoder-foundation-decoder pipelines. The system achieves scalable omni-modal training through composable 3D parallelism while maintaining model definition independence from distributed training mechanisms.

## Key Results
- 30B parameter omni-modal MoE model achieves over 2,800 tokens/sec/GPU throughput
- Scales to 160K context lengths on 128 GPUs
- Demonstrates superior efficiency and scalability for training large omni-modal LLMs

## Why This Works (Mechanism)

### Mechanism 1: Model-System Decoupling
- Claim: Separating model definition from parallelization logic enables scalable omni-modal training with minimal engineering overhead
- Mechanism: VeOmni introduces a `ParallelPlan` API that applies distributed strategies to model blocks declaratively without modifying model code. Communication operations are injected externally, allowing computation modules to remain agnostic to distributed concerns
- Core assumption: Runtime communication injection overhead is lower than maintaining parallelization logic within heterogeneous model architectures
- Evidence anchors: [abstract] "decouples communication from computation"; [section 3.2] model architecture decoupling without modifying low-level parallelization code
- Break condition: If communication patterns become tightly coupled to specific model architectures, the decoupling abstraction may leak or introduce inefficiency

### Mechanism 2: Composable n-D Parallelism via DeviceMesh
- Claim: A unified DeviceMesh abstraction enables flexible composition of multiple parallelism strategies tailored to different model components
- Mechanism: VeOmni represents parallel topologies as a global device mesh. Parallel strategies are applied per-component—for example, SP to attention blocks for long sequences, EP to MoE FFN layers, FSDP to embeddings
- Core assumption: Different model components have distinct scaling bottlenecks that benefit from heterogeneous parallel strategies within a single training run
- Evidence anchors: [section 3.2.4] core parallelism strategies are fully composable and can be flexibly applied to different components
- Break condition: If communication overhead from coordinating multiple parallelism strategies exceeds compute savings, efficiency degrades

### Mechanism 3: Communication-Computation Overlapping at Operator Level
- Claim: Fine-grained operator-level overlap of communication with computation improves MoE and sequence-parallel training throughput
- Mechanism: For MoE, token routing (all-to-all) is overlapped with local expert computation. For sequence parallelism, all-to-all communication runs concurrently with linear projections
- Core assumption: Sufficient compute work exists during communication windows to achieve meaningful overlap
- Evidence anchors: [section 3.2.3] fine-grained communication-computation overlapping techniques; [section 3.2.2] Async-Ulysses schedules communication concurrently with computations
- Break condition: If communication volume dominates compute, overlap benefits diminish

## Foundational Learning

- **Fully Sharded Data Parallel (FSDP/Zeo)**:
  - Why needed here: FSDP is the base parallelism for memory efficiency; understanding ZeRO stages and sharding granularity is prerequisite to configuring VeOmni's parallel plans
  - Quick check question: Can you explain why FSDP reduces per-GPU memory compared to DDP, and when HSDP is preferred?

- **Sequence Parallelism (DeepSpeed-Ulysses)**:
  - Why needed here: SP enables training with 160K+ token contexts; understanding all-to-all communication patterns in attention is critical for debugging long-sequence workloads
  - Quick check question: How does Ulysses-style SP keep communication volume constant when scaling both sequence length and device count proportionally?

- **Mixture-of-Experts (MoE) and Expert Parallelism**:
  - Why needed here: The 30B model uses MoE; understanding token routing, load balancing, and EP's all-to-all communication is essential for optimizing MoE recipes
  - Quick check question: What causes load imbalance in MoE EP, and how does communication-computation overlap help mitigate routing latency?

## Architecture Onboarding

- **Component map**:
  - `OmniModel` orchestrates Encoder → Foundation → Decoder pipeline
  - `EncoderMixin` implements `lm_encode()` for modality tokenization (e.g., image/audio → embeddings)
  - `FoundationMixin` backbone LLM with `get_pos_id()` for position handling
  - `DecoderMixin` implements `lm_encode()`, `lm_head()`, `lm_embed()`, `lm_generate()` for output generation
  - `ParallelPlan` declarative specification of which modules use which parallelism
  - `parallel_state` global DeviceMesh abstraction managing DP/EP/SP groups
  - `OmniDataCollatorWithPacking` packs variable-length multimodal sequences into efficient batches

- **Critical path**:
  1. Define encoders/decoders/foundation via `build_omni_model()`
  2. Initialize parallel state with `init_parallel_state()` specifying DP/EP/SP sizes
  3. Apply parallelization via `build_parallelize_model()` with a `ParallelPlan`
  4. Run forward pass through `OmniModel.forward()` which orchestrates encoder → foundation → decoder flow

- **Design tradeoffs**:
  - FSDP alone vs. FSDP+SP: SP adds communication overhead but enables longer sequences; trade memory for latency
  - Higher SP degree vs. lower: More SP reduces per-GPU sequence memory but increases all-to-all cost
  - EP degree vs. expert capacity: More EP parallelizes experts but increases routing communication

- **Failure signatures**:
  - OOM at 64K+ sequences without SP → indicates need to increase SP degree
  - Low MFU with high EP → likely routing imbalance or insufficient overlap
  - Modality features not aligned → check `OmniDataCollatorWithPacking` configuration

- **First 3 experiments**:
  1. **Baseline FSDP throughput**: Train Qwen2-VL 7B with FSDP only on 8 GPUs, measure tokens/sec and memory at 8K, 16K, 32K sequences
  2. **SP scaling sweep**: Add SP with degree 2, 4 and measure maximum context length and MFU
  3. **3D parallelism validation**: Train 30B MoE model with FSDP+SP+EP on 128 GPUs. Sweep SP and EP degrees, measure throughput and memory

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can non-intrusive pipeline parallelism be implemented within VeOmni to further scale training while maintaining the decoupling of model definition and parallel logic?
- Basis in paper: [explicit] The Conclusion states, "In future work, we plan to extend VeOmni to support non-intrusive pipeline parallelism."
- Why unresolved: Pipeline parallelism usually requires rigid stage partitioning which conflicts with keeping model definition free of parallel logic
- What evidence would resolve it: A specific API design that allows PP configuration without altering model forward-pass logic, along with scaling benchmarks

### Open Question 2
- Question: What specific "modality-aware data balancing strategies" are required to optimize sequence parallelism for heterogeneous omni-modal training workloads?
- Basis in paper: [explicit] The Conclusion notes, "Additionally, we aim to enhance sequence parallelism with modality-aware data balancing strategies."
- Why unresolved: While VeOmni supports long-context training via sequence parallelism, it currently processes data without accounting for varying computational costs of different modalities
- What evidence would resolve it: An algorithm that dynamically adjusts sequence partitioning based on modality type and complexity, demonstrated by reduced variance in iteration times

### Open Question 3
- Question: Can the proposed fine-grained operator-level communication-computation overlapping techniques effectively scale to larger MoE models (beyond 30B parameters) without requiring pipeline-level solutions?
- Basis in paper: [inferred] Section 3.2.3 claims VeOmni's operator-level approach avoids complex pipeline-centric designs
- Why unresolved: The ratio of communication overhead to computation often increases with model size and expert count
- What evidence would resolve it: Comparative benchmarks on clusters with varying interconnect bandwidths for models exceeding 30B parameters

## Limitations
- The decoupling abstraction's scalability remains untested for tightly integrated modality-specific operations requiring parallelization-aware implementations
- Communication-computation overlap benefits are empirically demonstrated but lack theoretical bounds on when efficiency degrades
- The framework lacks comprehensive ablation studies showing individual contribution margins of each parallelism strategy

## Confidence
- **High confidence**: Throughput and memory scaling results (2,800 tokens/sec/GPU, 160K context length on 128 GPUs) are well-documented with specific hardware configurations
- **Medium confidence**: The composable n-D parallelism architecture is logically sound but lacks comprehensive ablation studies
- **Medium confidence**: The model-system decoupling claim is theoretically justified but insufficiently validated against alternative distributed training frameworks

## Next Checks
1. **Ablation study of parallelism strategies**: Train the 30B MoE model with isolated FSDP, FSDP+SP, FSDP+EP, and FSDP+SP+EP configurations to quantify individual contribution margins
2. **Communication overhead analysis**: Profile all-to-all communication costs during expert routing and sequence parallelism operations to determine overlap efficiency thresholds
3. **Scalability boundary testing**: Evaluate VeOmni's performance on heterogeneous GPU clusters (A100+80GB mixed with H100) to assess the framework's robustness to real-world hardware variance