---
ver: rpa2
title: Joint Out-of-Distribution Filtering and Data Discovery Active Learning
arxiv_id: '2503.02491'
source_url: https://arxiv.org/abs/2503.02491
tags:
- data
- joda
- selection
- learning
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Joint Out-of-Distribution filtering and data
  Discovery Active learning (Joda), a novel approach that addresses both out-of-distribution
  (OOD) filtering and category discovery in active learning simultaneously. Unlike
  previous methods, Joda eliminates the need for auxiliary models and unlabeled data
  access by deeply entangling training with filtering and selection phases.
---

# Joint Out-of-Distribution Filtering and Data Discovery Active Learning

## Quick Facts
- arXiv ID: 2503.02491
- Source URL: https://arxiv.org/abs/2503.02491
- Reference count: 40
- Primary result: Joda achieves highest accuracy with best class discovery to OOD filtering balance compared to state-of-the-art competitors

## Executive Summary
This paper introduces Joda, a novel approach that simultaneously addresses out-of-distribution (OOD) filtering and category discovery in active learning without requiring auxiliary models or unlabeled data access. Joda deeply entangles training with filtering and selection phases by constructing a common feature space that aligns known and novel categories while separating OOD samples. The approach combines classification and outlier exposure loss, followed by energy-based filtering and SISOMe-based selection with class balancing. Extensive experiments across 18 configurations and 3 metrics demonstrate Joda's consistent superiority over state-of-the-art competitors.

## Method Summary
Joda eliminates the need for auxiliary models by constructing a common feature space during training that aligns known and novel categories while separating OOD samples. The approach uses a combined classification and outlier exposure loss to learn this feature space, followed by energy-based filtering to remove OOD samples and SISOMe-based selection to choose informative samples with class balancing. This deeply entangled training, filtering, and selection process allows Joda to operate without accessing unlabeled data or requiring separate OOD detection models, making it more efficient than previous approaches that rely on auxiliary components.

## Key Results
- Joda consistently achieves highest accuracy across 18 experimental configurations and 3 evaluation metrics
- Joda maintains best balance between class discovery and OOD filtering compared to state-of-the-art methods
- Joda demonstrates robustness across varying data ratios, model architectures, class imbalance scenarios, and hyperparameter variations

## Why This Works (Mechanism)
Joda works by deeply entangling the learning process through a unified feature space construction that simultaneously handles known categories, novel categories, and OOD samples. The classification and outlier exposure loss creates a discriminative space where known and novel categories cluster together while OOD samples are pushed away. Energy-based filtering then removes samples with high energy (likely OOD), while SISOMe-based selection identifies the most informative samples for labeling. The class balancing ensures diverse representation across categories, preventing dominance by frequent classes.

## Foundational Learning
- Active Learning: Selective labeling of informative samples to improve model performance with minimal labeling effort - needed because manual annotation is expensive and time-consuming
- Out-of-Distribution Detection: Identifying samples that don't belong to any known class - needed to prevent model contamination from irrelevant data
- Category Discovery: Automatically identifying and incorporating novel classes during learning - needed for real-world scenarios where not all classes are known upfront
- Energy-based Models: Using energy scores to quantify sample likelihood - needed for effective OOD filtering without auxiliary models
- SISOMe (Self-supervised Similarity-based Objective for Model Exploration): Selecting samples based on similarity patterns - needed for diverse and informative sample selection

Quick checks: Verify each component independently works as expected, then validate their integration through ablation studies.

## Architecture Onboarding

Component Map: Input Data -> Classification+Outlier Loss -> Feature Space -> Energy-based Filtering -> SISOMe Selection -> Labeled Dataset

Critical Path: The training phase with combined loss is critical as it establishes the feature space that enables both OOD detection and category discovery. Without proper feature space construction, subsequent filtering and selection steps fail.

Design Tradeoffs: Joda trades computational complexity during training for eliminating auxiliary models and unlabeled data requirements. The combined loss approach is more computationally intensive than separate training but provides better integration and performance.

Failure Signatures: Poor feature space construction leads to inability to distinguish between novel categories and OOD samples. Energy-based filtering failure results in OOD contamination. SISOMe selection failure causes class imbalance or redundant sample selection.

First Experiments:
1. Verify feature space construction separates known classes, novel classes, and OOD samples visually
2. Test energy-based filtering performance on synthetic OOD data
3. Validate SISOMe selection produces balanced class distribution

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability to diverse real-world datasets beyond tested 18 configurations remains uncertain
- Computational overhead from combined classification, outlier exposure loss, and SISOMe-based selection mechanisms during training
- Performance under extreme conditions like highly imbalanced or noisy data distributions needs further validation

## Confidence

High:
- Core innovation of simultaneous OOD filtering and category discovery without auxiliary models
- Demonstrated superiority over state-of-the-art competitors in tested configurations

Medium:
- Claims of robustness against class imbalance and hyperparameter variations based on limited testing scope

Low:
- Generalizability to entirely different domains or data modalities not covered in experiments

## Next Checks
1. Test Joda on large-scale, real-world datasets with significant class imbalance and noise to evaluate scalability and robustness
2. Conduct ablation studies to isolate the impact of SISOMe-based selection and energy-based filtering components on overall performance
3. Compare Joda's computational efficiency and resource requirements against existing methods in resource-constrained environments