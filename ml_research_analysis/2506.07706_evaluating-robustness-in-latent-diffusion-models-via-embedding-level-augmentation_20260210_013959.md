---
ver: rpa2
title: Evaluating Robustness in Latent Diffusion Models via Embedding Level Augmentation
arxiv_id: '2506.07706'
source_url: https://arxiv.org/abs/2506.07706
tags:
- poto
- photo
- phoo
- phto
- backpack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates robustness in latent diffusion models (LDMs)
  by introducing embedding-level augmentations applied after the text encoder and
  before the denoising network. The proposed AELIF method applies either masking or
  Gaussian noise convolution to token embeddings to simulate real-world textual errors
  such as typos or grammatical inconsistencies.
---

# Evaluating Robustness in Latent Diffusion Models via Embedding Level Augmentation

## Quick Facts
- arXiv ID: 2506.07706
- Source URL: https://arxiv.org/abs/2506.07706
- Reference count: 31
- Primary result: Embedding-level augmentations improve DreamBooth fine-tuning robustness to adversarial prompts by ~65% on average

## Executive Summary
This paper introduces AELIF, a method for improving the robustness of latent diffusion models to adversarial and noisy prompts through embedding-level augmentations. The approach applies token masking or Gaussian noise convolution to text embeddings after the text encoder but before the denoising network during DreamBooth fine-tuning. When evaluated on Stable Diffusion 3 and SDXL models across 11 categories, AELIF-trained models show significantly better performance on adversarially generated prompts while maintaining image fidelity to training data, as measured by CLIP-based Wasserstein distances.

## Method Summary
AELIF applies two types of augmentations to token embeddings after text encoding: masking (replacing selected embeddings with zeros) and noise convolution (element-wise multiplication with Gaussian noise). These are integrated into DreamBooth fine-tuning with LoRA, applied at a fixed magnitude p. The method aims to simulate real-world input errors and force the denoiser to handle incomplete or corrupted conditioning, thereby improving robustness to adversarial prompts while preserving output quality.

## Key Results
- AELIF improves robustness to adversarial prompts by ~65% on average across SDXL and SD3 models
- Masking and noise convolution augmentations both contribute to robustness gains when applied post-text-encoder
- CLIP-based Wasserstein distance evaluation shows AELIF-trained models generate outputs more aligned with training data than baseline models on adversarial prompts
- Augmentation magnitude must be carefully controlled, as 10% token augmentation can cause complete output distortion

## Why This Works (Mechanism)

### Mechanism 1: Token Masking
- Claim: Token masking simulates user input errors (omissions, typos) and forces the denoiser to handle incomplete conditioning.
- Mechanism: By replacing a fraction p of token embeddings with zero vectors after the text encoder, the denoising network learns to produce faithful outputs even when parts of the prompt representation are missing. This acts as a regularizer that prevents over-reliance on any single token.
- Core assumption: Robustness failures at the embedding level correlate with sensitivity to missing or corrupted tokens in real prompts.
- Evidence anchors: [section 3.2] "Masking is the more straightforward of the two, simulating the random omission of characters or tokens, and effectively modeling input degradation due to minor user errors." [section 3.2] "Each selected embedding z_p^i is then replaced with a zero vector: z'_p^i = 0."
- Break condition: If p is set too high, semantic content is lost and output quality degrades catastrophically.

### Mechanism 2: Noise Convolution
- Claim: Noise convolution exposes the denoiser to a broader distribution of perturbed embeddings, improving generalization to adversarial prompts.
- Mechanism: For selected tokens, element-wise multiplication with Gaussian noise (z'_p = z_p ⊙ noise_vector) perturbs embedding directions. This forces the denoiser to map a neighborhood of embeddings to similar outputs, smoothing the conditioning manifold.
- Core assumption: Adversarial prompt variations produce embedding perturbations that can be approximated by Gaussian noise convolution.
- Evidence anchors: [section 3.2] "The noise convolution augmentation was designed as a stronger and more comprehensive perturbation method. It aims to capture a wider range of input noise, including grammatical issues, misspellings, and structural distortions." [figure 4] Shows output distortion increases with augmentation magnitude, demonstrating sensitivity.
- Break condition: High noise variance (e.g., σ = 100 in Figure 4) can produce unrecognizable outputs even at 10% augmentation.

### Mechanism 3: Post-Encoder Isolation
- Claim: Applying augmentations after the text encoder isolates denoiser robustness from text encoder weaknesses.
- Mechanism: By intervening at the embedding level, the method evaluates and trains only the denoising component (U-Net or DiT). Text encoder brittleness is held constant, preventing conflation of failure sources.
- Core assumption: The text encoder's embedding space is stable enough that post-hoc perturbations meaningfully simulate upstream variation.
- Evidence anchors: [abstract] "We hypothesize that the robustness of LDMs primarily should be measured without their text encoder, because if we take and explore the whole architecture, the problems of image generator and text encoders will be fused." [section 3.3] "Importantly, the AELIF augmentations were applied after the text encoder, directly on the output embeddings. This design isolates the robustness of the denoising architecture without confounding it with potential weaknesses in the text encoder."
- Break condition: If the text encoder itself is highly non-robust, even a robust denoiser cannot recover; the method's benefits will be masked.

## Foundational Learning

- Concept: Text embeddings in LDMs (CLIP, T5)
  - Why needed here: AELIF operates on the dense vector output of text encoders; understanding embedding structure is prerequisite to reasoning about perturbation effects.
  - Quick check question: Can you explain how a token sequence becomes a conditioning input for the denoiser?

- Concept: DreamBooth fine-tuning with LoRA
  - Why needed here: The method integrates into DreamBooth; LoRA provides memory-efficient adaptation. Understanding the instance/prior loss balance is essential for debugging training.
  - Quick check question: What role does the prior-preservation term play in DreamBooth, and how does it interact with AELIF augmentations?

- Concept: Wasserstein distance for distribution comparison
  - Why needed here: Evaluation uses 2-Wasserstein distance between CLIP embeddings of generated and reference images to quantify fidelity and robustness.
  - Quick check question: Why might Wasserstein distance be preferred over KL divergence for comparing image embedding distributions?

## Architecture Onboarding

- Component map: Tokenized text prompt -> Text encoder (CLIP/T5) -> Embeddings -> AELIF augmentation -> Denoiser (U-Net/DiT) -> VAE decoder -> Output image
- Critical path:
  1. Prompt → text encoder → embeddings
  2. AELIF augmentation (mask or noise conv) with magnitude p
  3. Conditioned denoising over timesteps
  4. VAE decode → output image
- Design tradeoffs:
  - Augmentation magnitude p: Higher p improves robustness but risks semantic drift.
  - Masking vs. noise convolution: Masking is interpretable (token dropout); noise convolution is stronger but less predictable.
  - Application point: Post-encoder isolation provides clean signals but ignores encoder robustness.
- Failure signatures:
  - Over-augmentation: Output diverges from training distribution (high Wasserstein distance, visual artifacts).
  - Under-augmentation: No robustness gain; adversarial prompts still cause large quality drops.
  - Category-specific failure: Some categories (e.g., teapot at 50%) show minimal improvement, suggesting subject-specific sensitivity.
- First 3 experiments:
  1. Baseline sweep: Train DreamBooth without AELIF, evaluate Wasserstein distance on clean vs. adversarial prompts (GPT-4o generated) to establish robustness gap.
  2. Ablation on augmentation type: Compare masking-only, noise-conv-only, and combined AELIF across 2-3 categories (e.g., dog, clock) at fixed p=0.1.
  3. Magnitude sensitivity: Vary p ∈ {0.05, 0.1, 0.15, 0.2} and measure both Wasserstein distance (fidelity) and robustness improvement rate; identify optimal p per model (SD3 vs. SDXL).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should a standardized robustness benchmark for generative models be formalized to systematically evaluate performance degradation under noisy, adversarial, and semantically drifted prompts?
- Basis in paper: [explicit] The conclusion states: "In future work, we aim to formalize this evaluation into a standardized robustness benchmark for generative models... Our planned benchmark will systematically measure performance degradation under noisy, adversarial, and semantically drifted prompts."
- Why unresolved: The current work proposes an evaluation pipeline specific to DreamBooth fine-tuning, but a broader, standardized benchmark applicable across different training regimes and model architectures remains undeveloped.
- What evidence would resolve it: A comprehensive benchmark suite with standardized adversarial prompt generation protocols, multiple evaluation metrics beyond CLIP-Wasserstein distance, and validation across diverse LDM architectures and training methods.

### Open Question 2
- Question: What are the optimal augmentation magnitude settings (p values) for AELIF masking and noise convolution across different prompt complexities and model architectures?
- Basis in paper: [inferred] Figure 4 demonstrates that "for even 10 percent of token augmentations, we can have completely distorted output" for long prompts, suggesting sensitivity to augmentation magnitude. The paper uses fixed p values without systematic exploration of optimal settings.
- Why unresolved: The paper introduces the augmentation magnitude parameter p ∈ [0,1] but does not systematically investigate optimal values or their relationship to prompt length, semantic complexity, or model-specific characteristics.
- What evidence would resolve it: Ablation studies varying p values across different prompt lengths and categories, with analysis of the trade-off between robustness improvement and output quality degradation.

### Open Question 3
- Question: Do AELIF augmentation techniques generalize effectively to other LDM architectures beyond SD3 and SDXL, and to training regimes beyond DreamBooth fine-tuning?
- Basis in paper: [inferred] The methodology section states experiments focus "primarily on Stable Diffusion 3 (SD3) and Stable Diffusion XL (SDXL)" and specifically uses "DreamBooth in combination with LoRA." No validation on other architectures or full fine-tuning is presented.
- Why unresolved: The architectural differences between denoising components (U-Net vs Diffusion Transformer) and variations in text encoder configurations may affect how embedding-level augmentations propagate through different model architectures.
- What evidence would resolve it: Experiments applying AELIF to other diffusion models (e.g., DALL-E, Imagen, Midjourney architectures if accessible) and in different training contexts (full fine-tuning, instruction tuning, reinforcement learning from human feedback).

## Limitations
- The exact text encoder (CLIP or T5) and evaluation CLIP model are not specified, making exact reproduction difficult
- Augmentation magnitude p is set at 0.1 without systematic exploration of optimal settings, despite evidence of severe output degradation at 10%
- Evaluation focuses on a narrow set of 11 categories and adversarial prompts generated by GPT-4o, limiting generalizability

## Confidence
- **High confidence**: The mechanism of embedding-level augmentation (masking and noise convolution) is technically sound and the implementation details are reproducible
- **Medium confidence**: Claims about robustness improvement (~65% average) are supported by Wasserstein distance metrics, but the exact training hyperparameters and prompt generation pipeline are underspecified
- **Low confidence**: The claim that robustness failures at the embedding level directly correlate with real-world user input errors lacks direct corpus validation

## Next Checks
1. **Hyperparameter Sensitivity Sweep**: Systematically vary p ∈ {0.05, 0.1, 0.15, 0.2} and μ, σ for noise convolution, measuring both Wasserstein distance (fidelity) and robustness improvement rate. This will identify optimal settings per model and detect over-augmentation failure modes.
2. **Text Encoder Robustness Isolation Test**: Train a version of AELIF with augmentations applied *before* the text encoder (i.e., on token IDs) and compare robustness gains to the post-encoder version. This will validate the assumption that post-encoder perturbations isolate denoiser robustness.
3. **Cross-Domain Generalization**: Apply AELIF to a new set of categories (e.g., animals, vehicles, food) and adversarial prompts generated by a different model (e.g., GPT-3.5). Measure Wasserstein distance and success rate to assess generalizability beyond the original 11 categories.