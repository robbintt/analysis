---
ver: rpa2
title: Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly
  Detection
arxiv_id: '2506.03964'
source_url: https://arxiv.org/abs/2506.03964
tags:
- causal
- anomaly
- carots
- contrastive
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting anomalies in multivariate
  time series by leveraging causal relationships among variables. The proposed method,
  CAROTS, introduces causality-aware data augmentation and a similarity-filtered one-class
  contrastive loss to train an encoder that distinguishes normal and abnormal patterns
  based on causality.
---

# Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly Detection

## Quick Facts
- **arXiv ID**: 2506.03964
- **Source URL**: https://arxiv.org/abs/2506.03964
- **Reference count**: 40
- **Primary result**: CAROTS improves anomaly detection AUROC by up to 50% by incorporating causal relationships among variables

## Executive Summary
This paper addresses the challenge of detecting anomalies in multivariate time series by leveraging causal relationships among variables. The proposed method, CAROTS, introduces causality-aware data augmentation and a similarity-filtered one-class contrastive loss to train an encoder that distinguishes normal and abnormal patterns based on causality. Experiments on five real-world and two synthetic datasets show that CAROTS significantly outperforms existing methods, with an AUROC of up to 0.85 on SWaT and 0.99 on Lorenz96. The integration of causality enables CAROTS to reliably detect even the most difficult anomalies, improving AUROC by up to 50% compared to baselines. The approach is robust to hyperparameter changes and encoder architecture choices, demonstrating practical applicability.

## Method Summary
CAROTS addresses multivariate time series anomaly detection by integrating causal relationships into the learning process. The method employs causality-aware data augmentation to create meaningful positive pairs based on the underlying causal structure of the data. A similarity-filtered one-class contrastive loss is then used to train an encoder, where the causal structure helps distinguish normal from abnormal patterns. By leveraging the causal relationships among variables, CAROTS can more effectively identify subtle anomalies that might be missed by traditional methods. The approach is validated across multiple real-world and synthetic datasets, demonstrating significant improvements over existing techniques.

## Key Results
- CAROTS achieves an AUROC of up to 0.85 on SWaT and 0.99 on Lorenz96 datasets
- Integration of causality improves anomaly detection performance by up to 50% compared to baselines
- Method shows robustness to hyperparameter changes and encoder architecture choices

## Why This Works (Mechanism)
The core insight is that causal relationships between variables provide crucial context for distinguishing normal from abnormal patterns in multivariate time series. Traditional anomaly detection methods often treat variables independently or rely solely on statistical correlations, missing the deeper structural relationships that define normal system behavior. By explicitly modeling these causal relationships through data augmentation and contrastive learning, CAROTS can better capture the true nature of anomalies. The similarity-filtered one-class contrastive loss ensures that only causally consistent positive pairs are used for training, while anomalies are detected based on deviations from these causal patterns.

## Foundational Learning

### Causality in Time Series
- **Why needed**: Traditional correlation-based methods miss the directional and structural relationships between variables that define system behavior
- **Quick check**: Verify causal relationships using domain knowledge or causal discovery algorithms before applying CAROTS

### Contrastive Learning for Anomaly Detection
- **Why needed**: Contrastive approaches can learn meaningful representations without labeled anomalies by leveraging normal patterns
- **Quick check**: Ensure sufficient normal data diversity to learn robust representations

### One-Class Classification
- **Why needed**: Anomaly detection is fundamentally an unsupervised problem where only normal data is available for training
- **Quick check**: Validate that the training data represents the full spectrum of normal operating conditions

## Architecture Onboarding

### Component Map
- Raw time series data -> Causal discovery module -> Data augmentation module -> Encoder -> Similarity filtering -> Contrastive loss -> Anomaly detector

### Critical Path
The most critical path is: Causal discovery → Data augmentation → Encoder training → Anomaly detection. The quality of causal discovery directly impacts the effectiveness of data augmentation, which in turn determines how well the encoder learns to distinguish normal from abnormal patterns.

### Design Tradeoffs
The main tradeoff is between the accuracy of causal discovery and computational efficiency. More sophisticated causal discovery methods may improve performance but at the cost of increased computation. The similarity filtering mechanism adds complexity but is essential for ensuring meaningful contrastive learning.

### Failure Signatures
- Poor causal discovery leads to ineffective data augmentation and reduced performance
- Insufficient normal data diversity results in overfitting to specific patterns
- Overly strict similarity filtering may eliminate useful training examples

### First Experiments
1. Test causal discovery accuracy on synthetic datasets with known causal structures
2. Evaluate data augmentation quality by comparing augmented samples to ground truth
3. Benchmark encoder performance with and without causality-aware components

## Open Questions the Paper Calls Out
None explicitly mentioned in the provided content.

## Limitations
- Generalizability of causal discovery method across different domains and datasets
- Computational overhead introduced by causality-aware data augmentation process
- Uncertainty about performance with highly non-linear or noisy causal relationships
- Limited range of tested encoder architectures and hyperparameter configurations

## Confidence
- Performance improvements (up to 50% AUROC increase): **High**
- Robustness to hyperparameter changes and encoder choices: **Medium**
- Ability to reliably detect "most difficult anomalies": **Low**

## Next Checks
1. Test CAROTS on additional real-world datasets with varying levels of noise and non-linear causal relationships to assess generalizability
2. Evaluate the computational efficiency of the causality-aware data augmentation process on large-scale time-series data
3. Conduct ablation studies to isolate the contribution of the causality-aware components versus other design choices in the model