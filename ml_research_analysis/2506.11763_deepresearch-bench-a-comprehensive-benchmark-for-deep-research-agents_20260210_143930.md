---
ver: rpa2
title: 'DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents'
arxiv_id: '2506.11763'
source_url: https://arxiv.org/abs/2506.11763
tags:
- research
- task
- evaluation
- article
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DeepResearch Bench, the first comprehensive
  benchmark for evaluating Deep Research Agents (DRAs), which autonomously generate
  citation-rich research reports from web information. The benchmark includes 100
  PhD-level tasks across 22 domains, reflecting real-world user demands through analysis
  of over 96,000 queries.
---

# DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents

## Quick Facts
- **arXiv ID:** 2506.11763
- **Source URL:** https://arxiv.org/abs/2506.11763
- **Reference count:** 40
- **Primary result:** Introduces DeepResearch Bench with 100 PhD-level tasks and two novel evaluation frameworks (RACE, FACT) showing strong human alignment

## Executive Summary
DeepResearch Bench is the first comprehensive benchmark for evaluating Deep Research Agents (DRAs) that autonomously generate citation-rich research reports from web information. The benchmark includes 100 tasks across 22 domains, derived from analysis of over 96,000 real user queries to ensure ecological validity. Two novel evaluation frameworks are proposed: RACE assesses report quality via reference-based adaptive criteria scoring, while FACT evaluates web retrieval effectiveness through citation accuracy and effective citation counts. Human studies confirm high consistency between these frameworks and expert judgment, with the best-performing models achieving strong alignment in both report quality and factual grounding.

## Method Summary
The benchmark construction involved collecting 96,147 anonymized user queries, filtering them through an LLM to identify deep research tasks (44,019), classifying into 22 domains, and having PhD-level experts create 100 tasks (50 Chinese, 50 English). RACE uses a Judge LLM to generate task-adaptive dimension weights and criteria, then scores target reports against reference reports using relative scoring. FACT extracts statement-URL pairs from reports, retrieves webpage content, and uses a Judge LLM to assess support for each citation. Both frameworks rely on Gemini-2.5-Flash for automated judgments, with human validation on a subset of tasks showing strong agreement.

## Key Results
- Human validation shows 61.62% pairwise agreement rate between RACE and expert judgments, exceeding inter-expert agreement
- FACT achieves 96% alignment with human support determinations for "support" cases and 92% for "not support" cases
- Best-performing model (Gemini-2.5-Pro Deep Research) shows strong performance across both RACE and FACT metrics
- Cross-domain analysis reveals performance variations, with transportation tasks showing particular difficulty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reference-based comparative scoring with dynamically-generated task-specific criteria produces higher human alignment than static or absolute scoring approaches for evaluating open-ended research reports.
- Mechanism: A Judge LLM generates task-adaptive dimension weights and specific criteria per dimension, then scores both target and reference reports against these criteria using relative scoring: S_final = S_int(target) / (S_int(target) + S_int(reference)). This prevents score compression and improves discriminability.
- Core assumption: Judge LLMs can reliably discern quality differences when given explicit criteria and a concrete reference point.
- Evidence anchors: [abstract], [section 4.3.3], [corpus] related work on benchmark limitations
- Break condition: If reference report quality varies significantly or Judge LLM cannot consistently apply criteria, relative scoring becomes unstable.

### Mechanism 2
- Claim: Automated statement-URL pair extraction and support judgment provides a scalable proxy for measuring factual grounding and retrieval effectiveness.
- Mechanism: FACT extracts discrete factual statements with cited URLs, deduplicates semantically equivalent pairs, retrieves webpage content via Jina Reader API, and uses a Judge LLM to render binary support judgments. Metrics include Citation Accuracy and Effective Citations.
- Core assumption: Judge LLMs can accurately determine whether webpage content supports a statement; deduplication correctly identifies semantic equivalence.
- Evidence anchors: [section 3.2], [appendix C], [corpus] validation of citation verification methodology
- Break condition: If webpage retrieval fails or statements are ambiguously phrased, judgment consistency degrades.

### Mechanism 3
- Claim: Grounding benchmark task distribution in real-world query analysis improves ecological validity and reveals performance variation across domains.
- Mechanism: The authors analyzed 96,147 user queries, filtered 44,019 deep research tasks using LLM, classified into 22 domains, then proportionally sampled to create 100 tasks with expert refinement.
- Core assumption: LLM-based filtering and classification accurately identifies deep research queries; proportional compression preserves real-world demand patterns.
- Evidence anchors: [section 2.1], [section 4.2.1], [corpus] validation of query analysis methodology
- Break condition: If LLM classifier introduces systematic bias, distribution may not reflect true user demand.

## Foundational Learning

- **Concept: LLM-as-a-Judge with reference-based scoring**
  - Why needed here: RACE relies on comparative evaluation against a reference rather than absolute scoring; understanding why this improves discriminability is essential
  - Quick check question: Why does the paper use S_final = S_int(target) / (S_int(target) + S_int(reference)) rather than simply scoring the target report directly?

- **Concept: Citation verification as retrieval quality proxy**
  - Why needed here: FACT operationalizes "effective information retrieval" through supported citations; this differs from traditional IR metrics and requires understanding the mapping between claims and sources
  - Quick check question: What are the two primary metrics FACT calculates, and what aspect of agent performance does each capture?

- **Concept: Inter-rater reliability (ICC) for filtering noisy human judgments**
  - Why needed here: Human consistency validation uses ICC to filter tasks with poor expert agreement before computing correlation metrics
  - Quick check question: Why does the paper exclude tasks with ICC < 0 when computing Filtered Average Pearson correlation?

## Architecture Onboarding

- **Component map:**
  - **RACE Pipeline:** Task input → Dynamic weight generation (Judge LLM, T trials averaged) → Adaptive criteria generation per dimension → Reference report selection → Pairwise scoring (target vs. reference) → Dimension scores → Relative final score
  - **FACT Pipeline:** Generated report → Statement-URL extraction (Judge LLM) → Deduplication → Webpage content retrieval (Jina Reader API) → Support judgment (Judge LLM, binary) → Citation Accuracy and Effective Citations calculation
  - **Benchmark Construction:** Raw queries → LLM filtering → Topic classification → Proportional sampling → Expert task proposal → Manual quality screening → Final 100 tasks

- **Critical path:** RACE validity hinges on reference report quality and Judge LLM's ability to apply criteria consistently. FACT validity depends on accurate content retrieval and support judgment; retrieval failures cascade into incorrect accuracy estimates.

- **Design tradeoffs:**
  - Using Gemini-2.5-Flash for FACT vs. Pro: Reduces cost but introduces minor accuracy tradeoff (96% vs. ~98% agreement)
  - 100-task benchmark scale: Ensures quality through expert creation but limits statistical power and domain coverage
  - Reference-based vs. absolute scoring: Improves discriminability but introduces dependency on reference quality
  - Three annotators per task for human validation: Balances cost against inter-rater reliability

- **Failure signatures:**
  - Uniformly high scores across all reports → Reference-based scoring not applied correctly or reference quality too similar
  - Citation Accuracy near 100% but Effective Citations very low → Agent generating few but accurate claims (conservative retrieval)
  - High cross-domain score variance for same model → Task difficulty inconsistency or topic-specific capability gaps
  - Low pairwise agreement with humans (<60%) → Criteria too abstract, Judge LLM misaligned, or reference report poorly chosen

- **First 3 experiments:**
  1. Replicate RACE evaluation on a held-out subset of 20 tasks using a different Judge LLM (e.g., Claude 3.7 Sonnet) to test framework robustness across judges.
  2. Ablate the deduplication step in FACT to measure its impact on Effective Citations counts and computational cost.
  3. Run the best-performing DRA (Gemini-2.5-Pro Deep Research) on the transportation domain tasks separately to diagnose the observed performance drop and identify whether it reflects task difficulty or domain-specific gaps.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the DeepResearch Bench be scaled beyond 100 tasks while preserving "PhD-level" quality and rigorous expert vetting process? (Basis: [explicit] Limitations section notes expanding while maintaining quality standards would enhance statistical robustness)

- **Open Question 2:** Can the RACE and FACT frameworks be generalized to evaluate text generation and retrieval in non-research domains as effectively as they do for Deep Research Agents? (Basis: [explicit] Introduction states methodologies are not confined to deep research scenarios)

- **Open Question 3:** Does the high human consistency observed for the RACE framework in Chinese tasks transfer effectively to the English subset? (Basis: [inferred] Human consistency validation was restricted to 50 Chinese tasks due to high time cost)

## Limitations

- Dependency on Judge LLM judgments may introduce systematic biases that are difficult to detect or calibrate
- Reference-based scoring mechanism requires consistently high-quality reference reports
- FACT's reliance on external webpage content retrieval via Jina Reader API introduces potential failure modes
- 100-task scope, while ensuring quality, limits statistical power and may not capture full diversity of real-world research tasks

## Confidence

- **High Confidence:** Benchmark construction methodology (real-world query analysis → expert refinement) is well-documented and reproducible. FACT pipeline for citation verification is explicitly specified.
- **Medium Confidence:** RACE's reference-based adaptive criteria scoring shows strong human alignment in reported results, but robustness across different Judge LLMs or reference variants remains untested.
- **Medium Confidence:** Domain-specific performance variations are observed, but the paper does not fully diagnose whether these reflect true capability gaps or task design issues.

## Next Checks

1. **Judge LLM Robustness:** Replicate RACE evaluation using a different Judge LLM (e.g., Claude 3.7 Sonnet) on a held-out subset of 20 tasks to test whether the framework maintains high pairwise agreement rates across judges.

2. **Reference Quality Sensitivity:** Systematically vary the quality of reference reports used in RACE scoring to measure how reference quality degradation impacts relative score distributions and agreement rates with human judgments.

3. **FACT Retrieval Reliability:** Measure the proportion of successful webpage retrievals in FACT across different task domains and analyze whether retrieval failure rates correlate with citation accuracy scores or specific topic areas.