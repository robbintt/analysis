---
ver: rpa2
title: Model-Based Reinforcement Learning Under Confounding
arxiv_id: '2512.07528'
source_url: https://arxiv.org/abs/2512.07528
tags:
- reward
- learning
- policy
- observable
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a model-based reinforcement learning framework
  for contextual Markov decision processes in which the context is unobserved and
  acts as a confounder in the offline dataset. The authors identify that standard
  model learning methods produce biased transition and reward estimates because the
  behavioral policy depends on unobserved context variables.
---

# Model-Based Reinforcement Learning Under Confounding

## Quick Facts
- **arXiv ID:** 2512.07528
- **Source URL:** https://arxiv.org/abs/2512.07528
- **Reference count:** 21
- **Primary result:** Proposed surrogate model achieves ~1.4% higher expected return and consistently smaller multi-step rollout error compared to naive data averaging in confounded offline RL settings.

## Executive Summary
This paper addresses model-based reinforcement learning in contextual Markov decision processes where the context variable is unobserved and acts as a confounder in offline datasets. Standard model learning methods produce biased transition and reward estimates because the behavioral policy depends on unobserved context. The authors adapt a proximal off-policy evaluation approach that identifies confounded reward expectations using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with behavior-averaged transitions, this yields a surrogate MDP whose Bellman operator is well-defined and consistent for state-based policies, integrating seamlessly with the maximum causal entropy model learning framework.

## Method Summary
The framework learns a context-marginalized transition model p_ϕ(x_{t+1}|x_t,u_t) from confounded data by aggregating over all context values, while proximally identifying the confounded reward expectation using past and current observations as proxy variables. The proximal correction computes weight matrices from inverted observation-conditioned probabilities, enabling identification of the latent-state-dependent reward component. These components are integrated into a MaxCausalEnt optimization that jointly learns Q-functions and transition models under Bellman consistency constraints, producing a surrogate MDP that approximates the true confounded environment's behavior under state-based policies.

## Key Results
- Surrogate model exhibits consistently smaller ℓ₁ error for t ≥ 2 compared to naive data averaging
- Achieves approximately 1.4% higher expected return in synthetic clinical decision-making task
- Rollout error remains bounded while naive model error accumulates over time
- Framework integrates seamlessly with maximum causal entropy model learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Confounded reward expectation identifiable using observable proxy variables under matrix invertibility
- **Mechanism:** Past observations Y_{t-1} and current Y_t serve as proxies for latent state S_t = (X_t, Z_t). Sequential application of proxy-based decompositions eliminates Z_t dependence by expressing latent-state terms as products of observable matrices: W_k(τ^o_k) = P^b(Y_k|Y_{k-1}, u_k)^{-1} · P^b(Y_k, y_{k-1}|Y_{k-2}, u_{k-1})
- **Core assumption:** Matrices P^b(Y_t|Y_{t-1}, u_t) and P^b(S_t|Y_{t-1}, u_t) must be invertible; context must be persistent
- **Evidence anchors:** Abstract states identifiability under invertibility conditions; Theorem 1 proves confounded reward expectation is identifiable through weight matrix construction
- **Break condition:** Fails when context is i.i.d. across time steps or observation matrices are singular

### Mechanism 2
- **Claim:** Context-marginalized transition model remains identifiable from confounded data
- **Mechanism:** Behavioral policy induces well-defined state-action-next-state distribution. Marginalizing over context yields p_ϕ(x_{t+1}|x_t, u_t) = Σ_{z_t∈Z} p(x_{t+1}|x_t, z_t, u_t)p(z_t|x_t, u_t), representing context-marginalized evolution
- **Core assumption:** Behavioral policy must induce sufficient coverage of state-action space; triples (x_t, u_t, x_{t+1}) must be fully observed
- **Evidence anchors:** Section II-A Eq. 2 states marginalized transition is identifiable; Section III-D explains future value depends only on surrogate transition
- **Break condition:** Fails with insufficient behavioral coverage or context affects long-term dependencies

### Mechanism 3
- **Claim:** Proximal reward identification + behavior-averaged transitions yields consistent surrogate MDP
- **Mechanism:** MaxCausalEnt framework maximizes expert action likelihood under softmax policy while enforcing Bellman consistency. Modified Bellman equation replaces confounded reward expectation with proximal identification from Theorem 1, retaining identifiable future-value from surrogate transition
- **Core assumption:** Softmax policy parameterization correctly captures Q-value relationships; proxy-based correction accurately recovers true confounded expectation
- **Evidence anchors:** Abstract states surrogate MDP integrates with MaxCausalEnt; Section II-C Problem 1 formulates MaxCausalEnt with modified Bellman; Section IV shows 1.4% return improvement
- **Break condition:** Fails if MaxCausalEnt optimization doesn't converge or deconfounded reward estimate has high variance

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) and Confounding**
  - **Why needed here:** Frames unobserved context as confounder creating spurious correlations between actions and outcomes. Causal graph understanding explains why behavioral data misrepresents interventional effects.
  - **Quick check question:** If Z_t → U_t and Z_t → R_t, why does conditioning on observed (X_t, U_t) fail to recover causal effect of U_t on R_t?

- **Concept: Proximal Causal Inference**
  - **Why needed here:** Core technical contribution adapts proximal OPE to identify confounded expectations using proxy variables. Without understanding proxy concept (Y_{t-1}, Y_t as proxies for S_t), weight matrix construction is opaque.
  - **Quick check question:** What properties must proxy variables satisfy to substitute for unobserved confounder, and why does temporal structure matter?

- **Concept: Maximum Causal Entropy Inverse RL**
  - **Why needed here:** Framework embeds proximal correction into MaxCausalEnt model learning. Understanding softmax emergence from entropy maximization clarifies how Q-values parameterize learned policy.
  - **Quick check question:** Why does maximizing causal entropy while matching expert action frequencies yield softmax policy over Q-values?

## Architecture Onboarding

- **Component map:** Data Layer -> POMDP Representation -> Proxy Construction -> Weight Matrix Computation -> Surrogate MDP -> MaxCausalEnt Optimizer
- **Critical path:** 1) Estimate P^b(Y_k|Y_{k-1},u_k) and P^b(Y_k,y_{k-1}|Y_{k-2},u_{k-1}) from data; 2) Compute weight matrices W_k via matrix inversion; 3) Form proximal reward estimate using Eq. 31; 4) Initialize (θ, ϕ) and iterate MaxCausalEnt optimization; 5) Extract policy p_θ(u_t|x_t) from converged Q-function via softmax
- **Design tradeoffs:** Invertibility vs. practical estimation (Assumption 2 requires invertible matrices, but empirical estimates may be near-singular); Persistent vs. resampled context (method requires temporal structure, fails when context resampled independently); Finite vs. continuous spaces (current formulation assumes finite, extension to continuous noted as future work)
- **Failure signatures:** Singular/near-singular observation matrices → weight matrix explosion or undefined values; Rapidly accumulating rollout error after t ≥ 2 → proxy construction not working; Learned policy performs worse than behavioral policy → proximal correction introducing bias; Optimization divergence → check Bellman consistency constraint satisfaction
- **First 3 experiments:** 1) Synthetic validation: Replicate clinical treatment environment with known ground truth, verify rollout error remains bounded and below naive baseline for t ≥ 2; 2) Ablation on invertibility: Systematically reduce dataset size or increase context cardinality, monitor weight matrix condition numbers; 3) Robustness to context persistence: Vary context transition dynamics from fully persistent toward independent resampling, quantify performance degradation

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the proximal reward correction framework be extended to continuous state and action spaces while preserving identifiability guarantees?
  - **Basis in paper:** Conclusion explicitly states this as future research direction
  - **Why unresolved:** Current formulation relies on finite matrix representations and invertibility of discrete conditional probability matrices
  - **What evidence would resolve it:** Theoretical framework extending proximal construction to function spaces with regularity conditions, validated on continuous control tasks

- **Open Question 2:** What relaxations of matrix invertibility requirements (Assumption 2) are possible while still guaranteeing consistent reward identification?
  - **Basis in paper:** Identified in conclusion as future research direction; Assumption 2 requires invertibility of conditional observation matrices
  - **Why unresolved:** Invertibility condition may be violated when latent context space has larger cardinality than observation space, or when observation noise creates near-singular matrices
  - **What evidence would resolve it:** Theoretical analysis providing alternative sufficient conditions or weakened invertibility criteria, with empirical validation under mild violations

- **Open Question 3:** Can the framework be adapted to handle contexts that are resampled independently at each time step, rather than persisting across time?
  - **Basis in paper:** Remark 2 explicitly states Assumption 2 does not hold when context is resampled independently, as past observations cannot contain proxy signal
  - **Why unresolved:** Proximal construction uses past observations as proxies, requiring temporal carryover of contextual information
  - **What evidence would resolve it:** Identification of alternative proxy variables or auxiliary information valid under independent context resampling, with supporting theoretical analysis

## Limitations

- Strong invertibility assumptions required for proxy-based identification may be violated in practice when proxy variables provide limited information about latent states
- Numerical experiments use small-scale synthetic environment (4 states, 3 actions, 2 contexts) that may not reflect real-world complexity
- Method fails when context is independently resampled at each time step, limiting applicability to certain confounded settings

## Confidence

- **High:** Theoretical consistency of framework and basic mechanism of combining behavior-averaged transitions with proximal reward identification
- **Medium:** Numerical experiment results and practical applicability of invertibility conditions in real-world settings
- **Low:** Scalability to continuous spaces and robustness of weight matrix estimation under finite-sample conditions

## Next Checks

1. **Robustness testing:** Systematically vary information available in proxy variables and measure sensitivity of performance to matrix condition numbers
2. **Continuous extension:** Develop and validate continuous-state approximation of proximal identification approach
3. **Real-world application:** Apply method to high-dimensional confounded dataset (e.g., healthcare claims with treatment assignment bias) and compare against established causal inference baselines