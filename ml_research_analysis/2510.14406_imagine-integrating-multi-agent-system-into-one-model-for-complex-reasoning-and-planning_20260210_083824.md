---
ver: rpa2
title: 'IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning
  and Planning'
arxiv_id: '2510.14406'
source_url: https://arxiv.org/abs/2510.14406
tags:
- reasoning
- multi-agent
- training
- system
- final
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'IMAGINE integrates the reasoning and planning capabilities of
  a Multi-Agent System into a single, compact model through a novel three-stage framework:
  New Query Generation, Multi-Agent System-based Inference Data Generation, and Agentic
  Reasoning Training. The approach distills the collective reasoning abilities of
  a well-organized MAS into a small model, enabling it to outperform the original
  MAS through end-to-end training.'
---

# IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning

## Quick Facts
- **arXiv ID:** 2510.14406
- **Source URL:** https://arxiv.org/abs/2510.14406
- **Reference count:** 35
- **One-line primary result:** Qwen3-8B-Instruct trained with IMAGINE achieves 82.7% Final Pass Rate on TravelPlanner, outperforming 671B-parameter models

## Executive Summary
IMAGINE addresses the challenge of scaling complex reasoning capabilities in compact language models by distilling the collective intelligence of a Multi-Agent System (MAS) into a single model. The framework introduces a three-stage training pipeline that first generates high-quality reasoning traces using a MAS teacher, then fine-tunes a small student model on these traces, and finally applies reinforcement learning to enhance agentic reasoning capabilities. By integrating reasoning and planning into a unified model, IMAGINE achieves state-of-the-art performance on the TravelPlanner benchmark while maintaining significantly lower computational costs than traditional MAS approaches.

## Method Summary
IMAGINE employs a novel three-stage framework: New Query Generation, Multi-Agent System-based Inference Data Generation, and Agentic Reasoning Training. The approach first synthesizes diverse queries using a sandbox environment, then employs a MAS (Reasoner + Reflector + Judges) to generate high-quality inference traces. These traces are used to fine-tune a compact base model (Qwen3-8B-Instruct) through Agentic Supervised Fine-Tuning (SFT), followed by Agentic Reinforcement Learning using Group Relative Policy Optimization (GRPO). The framework is designed to be general and scalable, with broad applicability across various domains beyond the demonstrated travel planning task.

## Key Results
- Achieves 82.7% Final Pass Rate on TravelPlanner benchmark, far exceeding the 40% of DeepSeek-R1-671B
- Maintains significantly lower inference costs compared to traditional MAS approaches
- Demonstrates the effectiveness of distilling MAS reasoning capabilities into compact models

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Distillation from Synthetic MAS Traces
- **Claim:** A compact model fine-tuned on reasoning traces from a high-performing MAS can internalize complex planning capabilities
- **Core assumption:** The quality of the teacher's traces is superior to what the student could generate autonomously, and the student has sufficient capacity to memorize these patterns without catastrophic forgetting
- **Evidence anchors:** [abstract] "integrating the reasoning and planning capabilities of a multi-agent system into a single, compact language model"; [section 3.2] Describes construction of training data where Reflector's correction is appended to Reasoner's initial thought process

### Mechanism 2: Agentic Reinforcement Learning for Constraint Grounding
- **Claim:** Reinforcement learning optimizes for precise constraint satisfaction and prevents reward hacking in structured output tasks
- **Core assumption:** The rule-based reward function is a sufficient proxy for "good planning" and guides the model better than pure likelihood maximization
- **Evidence anchors:** [abstract] "Agentic RL further builds upon Agentic SFT"; [section 3.3.2] Details the reward function with format failure, commonsense, hard constraint, and reflection rewards

### Mechanism 3: Reflective Pattern Injection
- **Claim:** Enforcing a specific "Reflection" token structure encourages the model to self-correct reasoning errors before committing to a final answer
- **Core assumption:** The model learns to utilize reflection tokens for genuine self-correction rather than simply generating high-probability filler text
- **Evidence anchors:** [section 4.3] Fig. 9 tracks "Self-Reflection: No Error Detected Rate"; [section 3.2] Describes construction of data with REFLECTION_TAG and Reflector Content

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT)**
  - **Why needed here:** Serves as the "cold start" mechanism to transfer MAS behaviors into the 8B model
  - **Quick check question:** How does the loss function in SFT differ from the objective in GRPO?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** RL algorithm used to refine the model, estimating baselines using group statistics rather than a separate value model
  - **Quick check question:** Why might GRPO be preferred over PPO for a small 8B model regarding memory overhead?

- **Concept: Multi-Agent System (MAS) Architecture**
  - **Why needed here:** Understanding the Teacher architecture (Reasoner vs. Reflector vs. Judge) is critical to understanding the structure of training data
  - **Quick check question:** In the IMAGINE pipeline, what triggers the Reflector agent to generate a correction?

## Architecture Onboarding

- **Component map:** Data Engine (MAS cluster) -> Base Model (Qwen3-8B-Instruct) -> Training Loop (Agentic SFT -> Agentic GRPO)
- **Critical path:** 1) Generate diverse queries using sandbox environment; 2) Execute MAS inference to create "Agentic Dataset"; 3) Run SFT to Checkpoint 800; 4) Run GRPO using rule-based evaluator
- **Design tradeoffs:** Cost vs. Performance (high upfront costs for MAS Teacher enable cheap 8B Student for inference); SFT Stability vs. RL Optimization (stopping SFT early ensures probability distribution isn't collapsed)
- **Failure signatures:** Reward Hacking (model generates valid JSON but ignores complex constraints); Catastrophic Forgetting (if SFT runs too long, model loses ability to generate diverse plans during RL)
- **First 3 experiments:** 1) Baseline Profiling: Run base Qwen3-8B on TravelPlanner test set to confirm 5.9% baseline; 2) Ablation Study (SFT vs. RL): Compare Checkpoint 800 model vs. final GRPO model to quantify specific lift from RL phase; 3) Reflection Validity Check: Sample outputs where model claims "No errors" and manually verify if constraints were actually met

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but presents claims that require further investigation. The authors state their method is "general and scalable" with "broad applicability across various domains," but all experiments are conducted solely on the TravelPlanner benchmark. The framework is claimed to be "model-agnostic" but only demonstrates results with Qwen3-8B-Instruct as the base model.

## Limitations

- Reliance on synthetic MAS traces means the student model inherits and potentially amplifies any systematic errors made by the Reasoner or Reflector agents
- The rule-based reward function may not capture nuanced "good planning" beyond JSON validity and explicit constraint matching, creating opportunities for reward hacking
- Performance gains are demonstrated only on a single benchmark (TravelPlanner) and may not generalize to other complex reasoning tasks

## Confidence

- **High Confidence:** The core distillation mechanism (SFT on MAS traces) and quantitative results on TravelPlanner are well-supported by the paper's experimental design and reproducible methodology
- **Medium Confidence:** The effectiveness of the reflective pattern injection is supported by tracking metrics, but the mechanism's robustness against "lazy reflection" is not rigorously validated
- **Low Confidence:** The generalizability of the approach to domains outside of structured travel planning is entirely speculative and untested

## Next Checks

1. **Ablation of the Reflection Mechanism:** Run the final GRPO model with the reflection reward disabled to quantify its specific contribution to the Final Pass Rate and test for "lazy reflection" failure modes
2. **MAS Trace Quality Audit:** Sample 100 training traces and manually verify if the Reflector's corrections are accurate and if final answers satisfy all constraints to directly assess "garbage in, garbage out" risk
3. **Zero-Shot Transfer Test:** Evaluate the trained IMAGINE model on a different complex reasoning benchmark (e.g., strategy game planning or scientific hypothesis generation) to test claimed generalizability of distilled MAS reasoning capabilities