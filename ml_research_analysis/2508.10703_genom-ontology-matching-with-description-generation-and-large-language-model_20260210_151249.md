---
ver: rpa2
title: 'GenOM: Ontology Matching with Description Generation and Large Language Model'
arxiv_id: '2508.10703'
source_url: https://arxiv.org/abs/2508.10703
tags:
- ontology
- genom
- concept
- semantic
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GenOM is a large language model (LLM)-based framework for ontology
  matching that enriches concept representations by generating textual definitions,
  retrieves candidate mappings using an embedding model, and employs an LLM to perform
  equivalence judgements. The approach was evaluated on the OAEI Bio-ML track, achieving
  strong F1-scores across five biomedical ontology alignment tasks, surpassing several
  traditional and recent LLM-based baselines.
---

# GenOM: Ontology Matching with Description Generation and Large Language Model

## Quick Facts
- **arXiv ID:** 2508.10703
- **Source URL:** https://arxiv.org/abs/2508.10703
- **Reference count:** 40
- **One-line primary result:** GenOM achieves strong F1-scores on OAEI Bio-ML biomedical ontology alignment tasks using LLM-generated definitions and binary classification.

## Executive Summary
GenOM is a large language model (LLM)-based framework for ontology matching that addresses the challenge of aligning biomedical ontologies by enriching concept representations through textual definition generation. The framework extracts minimal ontology signals (labels, parents), prompts an LLM to synthesize definitions, retrieves candidate mappings using an embedding model, and employs an LLM for binary equivalence judgments. Evaluated on the OAEI Bio-ML track across five biomedical ontology alignment tasks, GenOM demonstrates superior performance compared to traditional and recent LLM-based baselines, with ablation studies confirming the effectiveness of semantic enrichment and few-shot prompting.

## Method Summary
The GenOM framework processes ontologies by first extracting labels, synonyms, parents, and equivalent class axioms from OWL/RDF files. It then generates textual definitions for each concept using an LLM (Qwen2.5-7B/32B-Instruct) with a specific prompt template. These enriched representations are embedded using OpenAI's `text-embedding-3-small` model, and a FAISS HNSW index retrieves top-10 candidate mappings per source concept. An LLM performs binary YES/NO equivalence judgments, extracting confidence probabilities from token logits. The final results combine high-confidence LLM predictions with exact matches from LogMapLt/BERTMapLt using dual thresholds (λ_prob ≥ 0.9, λ_cs ≥ 0.9).

## Key Results
- Achieved strong F1-scores across five biomedical ontology alignment tasks in the OAEI Bio-ML track
- Outperformed traditional and recent LLM-based baselines in ontology matching
- Larger LLMs (Qwen2.5-32B-Instruct) demonstrated more accurate and discriminative definitions with stable alignment performance across complex ontologies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating textual definitions compensates for sparse lexical context in ontologies, improving semantic discrimination during retrieval.
- **Mechanism:** The framework extracts minimal signals (labels, parents) and prompts an LLM to synthesize a "definition." This enriches the vector representation used for retrieval, expanding the semantic footprint beyond simple string matching.
- **Core assumption:** The LLM possesses sufficient internalized domain knowledge to generate factually accurate definitions for biomedical concepts without hallucinating critical details.
- **Evidence anchors:** [Abstract]: "enriches the semantic representations of ontology concepts via generating textual definitions." [Section 5]: "Generated definitions... are expected to be factually accurate... while being sufficiently discriminative."

### Mechanism 2
- **Claim:** Using token-level probabilities for binary classification (YES/NO) creates a calibrated confidence score for equivalence, reducing the need for verbose generative explanations.
- **Mechanism:** Instead of generating text, the framework constrains the LLM to output a single token. The probability P(YES) is derived from logits to rank mappings.
- **Core assumption:** The softmax probability of the "YES" token correlates strongly with the semantic validity of the match.
- **Evidence anchors:** [Section 3.2]: "A lightweight classification strategy is adopted... The predicted equivalence score is computed using the probability assigned to the YES token." [Section 6]: Shows that larger models (Qwen32B) maintain stable F1 scores across thresholds.

### Mechanism 3
- **Claim:** Hybridizing LLM predictions with lightweight exact matchers stabilizes results by covering cases where semantic reasoning is uncertain or computationally expensive.
- **Mechanism:** GenOM fuses high-confidence LLM predictions with the output of string-based matchers (LogMapLt/BERTMapLt) using threshold filtering.
- **Core assumption:** Exact lexical matches provide high precision but low recall, while LLMs provide high recall but variable precision; their errors are largely uncorrelated.
- **Evidence anchors:** [Abstract]: "incorporates exact matching-based tools to improve precision." [Section 7.2]: GenOM surpassed stand-alone exact matching systems on most tasks.

## Foundational Learning

- **Concept: Ontology Matching (OM) & Heterogeneity**
  - **Why needed here:** The core problem GenOM solves is "semantic interoperability" between ontologies that use different labels for the same concept (e.g., "Heart Attack" vs "Myocardial Infarction").
  - **Quick check question:** Can you explain why simple string matching fails when aligning "Cell" in a biology ontology vs "Cell" in a prison ontology?

- **Concept: Vector Embeddings & Cosine Similarity**
  - **Why needed here:** GenOM uses an embedding model (`text-embedding-3-small`) to convert definitions into vectors for candidate retrieval. Understanding distance in vector space is crucial for tuning λ_cs.
  - **Quick check question:** If two concepts are synonyms but have different vector magnitudes, why does cosine similarity (rather than Euclidean distance) generally perform better for retrieval?

- **Concept: Logits and Softmax Probability**
  - **Why needed here:** The equivalence judgment relies on extracting the logit of the "YES" token. You must understand how raw model outputs translate to confidence scores.
  - **Quick check question:** In a binary classification output of [YES, NO], if the logits are z_YES=2.0 and z_NO=-1.0, what is the resulting probability P(YES)?

## Architecture Onboarding

- **Component map:** Ontology Data Extraction -> Definition Generator -> Candidate Generator -> Equivalence Judge -> Result Fusion
- **Critical path:** The Definition Generator is the primary performance driver. If definitions are poor (e.g., "label-like"), the subsequent retrieval and judgment stages degrade significantly.
- **Design tradeoffs:**
  - **Model Size:** Qwen-32B offers stable performance across thresholds but is computationally heavy; Qwen-7B is faster but highly sensitive to threshold tuning (specifically λ_cs) on complex ontologies like SNOMED.
  - **Prompting:** Binary (YES/NO) is faster and more stable than generative explanations.
- **Failure signatures:**
  - **Label-like Definitions:** The LLM simply repeats the concept name (e.g., "Adenocarcinoma is an adenocarcinoma"). Fix: Apply Jaccard penalty or adjust generation temperature.
  - **Threshold Instability:** F1-scores crash on SNOMED tasks when using small models with low cosine similarity thresholds. Fix: Use larger models or enforce stricter λ_cs (e.g., 0.9+).
  - **Format Drift:** The LLM outputs "Yes, they are." instead of just "YES". Fix: Constrain token sampling or use regex to force specific token ID.
- **First 3 experiments:**
  1. **Sanity Check (Definitions):** Run the Definition Generator on 50 random concepts. Calculate the Jaccard similarity between the Label and the Definition to ensure the model isn't just paraphrasing the label.
  2. **Retrieval Ablation:** Compare Hit@10 scores on a subset (e.g., SNOMED-FMA) using only labels vs. labels + definitions to quantify the lift from semantic enrichment.
  3. **Threshold Sweep:** Run the full pipeline on SNOMED-NCIT (Neoplas) using a smaller model (Qwen-7B) while sweeping λ_cs from 0.5 to 0.9 to visualize the stability cliff described in Section 6.

## Open Questions the Paper Calls Out

- **Can the framework be extended to identify subsumption relations?** Section 8 states "an important enhancement would be to expand the scope of GenOM to include additional alignment types beyond equivalence, such as subsumption." The current pipeline generates definitions to distinguish concepts for binary equivalence, potentially reinforcing differences that are hierarchical rather than disjoint.

- **Does the framework's performance generalize to non-biomedical domains?** Section 4.1 restricts all experiments to the "Bio-ML track," and the prompt template explicitly instructs the LLM to act as a "biomedical ontology expert." It is unclear if the strong results are due to the generalizability of the semantic enrichment method or the LLM's inherent pre-training strength in the biomedical domain.

- **How can the framework autonomously adapt its decision thresholds?** Section 8 identifies "variability in how equivalence is defined" as a challenge and proposes "task-adaptive alignment criteria" and "dynamic threshold selection" for future work. The current system relies on fixed global thresholds (0.9) or manual per-task tuning.

## Limitations
- **Generative Reliability Uncertainty:** Core performance depends on LLM-generated definitions that must be factually accurate and semantically discriminative, but hallucination rates are not extensively tested.
- **Threshold Sensitivity Without Robust Calibration:** Smaller models exhibit dramatic F1 instability on complex ontologies when thresholds vary, suggesting lack of intrinsic calibration robustness.
- **Evaluation Scope Restriction:** All experiments are conducted on biomedical ontologies, leaving performance on non-biomedical domains and real-world noisy data untested.

## Confidence
- **High Confidence:** The mechanism of using token-level probabilities for binary classification is technically sound and well-supported by experimental results showing stable performance with larger models.
- **Medium Confidence:** The effectiveness of semantic enrichment through definition generation is demonstrated via ablation studies, but quality control mechanisms for ensuring generated definitions are not "label-like" are limited.
- **Low Confidence:** The claim about "few-shot prompting effectiveness" appears inconsistent with the described methodology, which uses zero-shot binary classification.

## Next Checks
1. **Definition Quality Audit:** Run the Definition Generator on 100 random biomedical concepts and calculate both Jaccard similarity (label vs definition) and manual accuracy scores. Verify that >95% of definitions add semantic information beyond simple label repetition and maintain factual accuracy.
2. **Threshold Robustness Test:** Systematically sweep λ_prob and λ_cs thresholds (0.5 to 0.95) on SNOMED-NCIT using both Qwen-7B and Qwen-32B models. Quantify F1 stability ranges and identify at which thresholds each model maintains performance within 5% of optimal.
3. **Cross-Domain Generalization:** Apply the framework to a non-biomedical ontology matching task (e.g., semantic web datasets or general knowledge ontologies) using the same configuration. Compare performance degradation to the biomedical baseline to assess domain dependence.