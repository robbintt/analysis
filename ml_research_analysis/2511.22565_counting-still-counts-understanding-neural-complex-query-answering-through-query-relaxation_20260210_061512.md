---
ver: rpa2
title: 'Counting Still Counts: Understanding Neural Complex Query Answering Through
  Query Relaxation'
arxiv_id: '2511.22565'
source_url: https://arxiv.org/abs/2511.22565
tags:
- query
- neural
- relaxation
- methods
- relax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether neural complex query answering (CQA)
  models learn to go beyond the reach of explicit graph structure by comparing them
  to a training-free query relaxation strategy that retrieves answers by progressively
  relaxing query constraints and counting resulting paths. Across multiple datasets
  and query types, the authors find that neural models do not consistently outperform
  the relaxation strategy, with no model reliably surpassing it.
---

# Counting Still Counts: Understanding Neural Complex Query Answering Through Query Relaxation

## Quick Facts
- arXiv ID: 2511.22565
- Source URL: https://arxiv.org/abs/2511.22565
- Reference count: 31
- Primary result: No neural CQA model consistently outperforms a training-free query relaxation strategy across multiple datasets and query types.

## Executive Summary
This paper challenges the assumption that neural complex query answering (CQA) models inherently generalize beyond symbolic reasoning by comparing them to a training-free query relaxation strategy (RELAX) that retrieves answers by progressively relaxing query constraints and counting resulting paths. Across multiple datasets and query types, the authors find that neural models do not consistently outperform RELAX, with no model reliably surpassing it. A similarity analysis reveals that top-ranked answers from both approaches differ substantially, indicating complementary reasoning patterns. Combining their outputs consistently improves performance, particularly for path queries. These findings suggest future neural architectures could benefit from integrating relaxation-based principles and highlight the importance of stronger non-neural baselines.

## Method Summary
The study compares neural CQA models (GNN-QE, ULTRAQ, ConE, CQD, QTO) against a training-free query relaxation strategy (RELAX) that progressively relaxes query constraints and counts matching paths. RELAX operates by replacing anchor constants with variables, counting paths, and using hierarchical tie-breaking (relation in-degree, then total in-degree). The method is evaluated on FB15k237+H, NELL995+H, and ICEWS18+H datasets across seven query types (1p, 2p, 3p, 2i, 3i, pi, ip) using filtered MRR. The analysis also examines complementarity through Jaccard similarity at top-k results and explores oracle combinations of neural and relaxation predictions.

## Key Results
- No neural model consistently outperforms the training-free RELAX strategy across datasets and query types.
- Neural models and RELAX retrieve substantially different top-ranked answers (low Jaccard similarity), indicating complementary reasoning patterns.
- Combining neural and relaxation predictions consistently improves performance, with relative gains increasing for longer path queries.
- The gap between neural models and RELAX widens with query length due to error propagation in sequential multi-hop reasoning.

## Why This Works (Mechanism)

### Mechanism 1: Progressive Query Relaxation via Path Counting
Replacing query anchors with variables and counting matching paths provides a training-free baseline that recovers plausible answers without learned parameters. The RELAX algorithm hierarchically relaxes query constraints: (1) replace anchor constants with variables, count how many paths in the KG match the relaxed query that reach each entity; (2) if ties persist, replace intermediate predicates with variables and recount; (3) final tie-breaking uses entity in-degree. More specific matches dominate general ones. If the KG is too sparse to yield meaningful path counts, or if answers require semantic reasoning beyond structural connectivity, RELAX degrades to global in-degree ranking.

### Mechanism 2: Complementarity from Divergent Reasoning Patterns
Neural models and query relaxation retrieve substantially different top-ranked answers despite similar MRR, indicating they capture non-overlapping patterns. Neural methods learn distributed representations that may encode implicit semantic patterns; RELAX exploits explicit structural statistics. The Jaccard similarity at top-k drops significantly for complex queries (2p, 3p, 2i, 3i), confirming divergence. Both approaches are capturing valid but distinct signals—neither subsumes the other. If one approach systematically outperforms the other on a domain, complementarity diminishes and ensembling yields smaller gains.

### Mechanism 3: Error Propagation in Neural Multi-hop Reasoning
Neural CQA models suffer compounding errors as path query length increases, widening the gap with RELAX. Neural methods compute predictions sequentially over query hops; errors at each step propagate. For 2p and 3p queries, multiple neural models (GNN-QE, ULTRAQ, ConE) underperform RELAX on ICEWS18+H and approach RELAX on NELL995+H. Neural embeddings cannot reliably maintain precision across multiple hops in incomplete graphs. Future neural architectures with better multi-hop composition may close this gap.

## Foundational Learning

- **Conjunctive queries (CQs) over KGs**: Understanding the query types (1p, 2p, 3p, 2i, 3i, ip, pi) is essential to interpret all results tables. Quick check: Can you explain why a 2i query requires intersection operations while a 2p query requires sequential projection?
- **Query relaxation in databases**: The paper's baseline is drawn from classical database techniques; without this, RELAX appears ad hoc. Quick check: What does "replacing a constant with a variable" mean in terms of broadening the answer set?
- **MRR (Mean Reciprocal Rank) and filtered evaluation**: All performance claims hinge on filtered MRR; understanding this metric prevents misinterpretation. Quick check: Why does filtered MRR exclude other correct answers when computing rank?

## Architecture Onboarding

- **Component map**: Graph database (GraphDB) -> RELAX module -> Neural CQA models -> Oracle combiner
- **Critical path**: Load KG into GraphDB with indexed edges; for each test query, generate relaxed variants (anchor→variable, predicate→variable); execute relaxed queries, collect path counts, apply hierarchical tie-breaking; compare rankings to neural model outputs; compute MRR, Jaccard similarity, and oracle combination.
- **Design tradeoffs**: Limiting to 2 predicate relaxations (for 3p queries) trades exhaustiveness for memory efficiency; using in-degree as final fallback is computationally cheap but semantically weak; 10s query timeout with fallback to in-degree ranking if exceeded.
- **Failure signatures**: High timeout rates indicate overly dense subgraphs or under-indexed relations (see ICEWS18+H ip queries at 0.5% timeout); near-identical MRR but low Jaccard signals methods achieve similar average performance via different correct answers—expected, not a bug; RELAX outperforming neural on path queries indicates neural model may not have converged or KG structure favors counting.
- **First 3 experiments**: 1) Run RELAX on FB15k237+H for 1p and 2p queries; verify MRR within ±0.5% of reported values; 2) Remove in-degree fallback; measure how often RELAX fails to produce a ranking; 3) For a single 2p query, extract top-10 answers from RELAX and QTO; manually inspect overlap and verify Jaccard calculation.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can query relaxation signals be effectively integrated into differentiable neural architectures for Complex Query Answering? The authors state that designing "realistic fusion mechanisms, such as learned ensembles, meta-predictors, or neural architectures that explicitly incorporate relaxation signals, remains an open avenue." The paper demonstrates complementarity using an oracle to combine predictions, but does not propose a trainable mechanism to unify symbolic relaxation counts with neural embeddings during the learning process.

- **Open Question 2**: Why do current neural CQA models fail to capture the structural reasoning patterns that simple path counting retrieves? The paper notes that neural models do not subsume relaxation patterns and that the low overlap suggests they capture different signals. However, the analysis does not identify if this failure stems from embedding space limitations, loss functions, or the specific operators used. The underlying cause of the neural models' blindness to simple counting heuristics remains unexplained.

- **Open Question 3**: Can query relaxation strategies remain computationally efficient and predictive when applied to large-scale knowledge graphs with complex query structures? The authors note that extending the analysis to large-scale KGs "might require additional techniques of scalability" and future work must explore "principled ways of pruning or approximating relaxations." The current method relies on explicit path enumeration, which becomes expensive for denser graphs or longer query paths (e.g., >3p), limiting applicability to benchmark datasets.

## Limitations

- Neural model hyperparameters for "+H" dataset variants are unspecified, preventing exact baseline replication.
- Graph database query syntax for path counting is not provided, introducing ambiguity in implementation.
- The impact of timeout thresholds (10s) and fallback mechanisms on MRR scores is unclear without timing logs.

## Confidence

- **High**: RELAX algorithm mechanics, Jaccard similarity patterns, and complementarity findings.
- **Medium**: Neural vs. RELAX MRR comparisons (subject to implementation variance).
- **Low**: Claims about neural models' inability to generalize beyond symbolic reasoning (requires deeper architectural analysis).

## Next Checks

1. **Timeout Impact Audit**: Re-run RELAX with detailed timing logs to quantify MRR degradation from timeouts.
2. **Hyperparameter Sensitivity**: Test RELAX with alternative tie-breaking hierarchies (e.g., global in-degree vs. relation-specific).
3. **Cross-Dataset Generalization**: Apply RELAX to non-"+H" KG datasets to verify structural robustness.