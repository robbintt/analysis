---
ver: rpa2
title: 'The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection
  in RAG Systems'
arxiv_id: '2512.15068'
source_url: https://arxiv.org/abs/2512.15068
tags:
- hallucinations
- semantic
- conformal
- hallucination
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Conformal prediction applied to RAG hallucination detection reveals
  a fundamental limitation: while embedding-based methods achieve 95% coverage with
  0% FPR on synthetic hallucinations, they fail catastrophically on real LLM outputs,
  producing 100% FPR at the same coverage target. The "Semantic Illusion" causes RLHF-trained
  models to generate hallucinations that are semantically indistinguishable from faithful
  responses, forcing conformal thresholds to reject all valid outputs.'
---

# The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems

## Quick Facts
- arXiv ID: 2512.15068
- Source URL: https://arxiv.org/abs/2512.15068
- Reference count: 40
- Primary result: Embedding-based methods achieve 95% coverage with 0% FPR on synthetic hallucinations but 100% FPR on real LLM outputs

## Executive Summary
Conformal prediction applied to RAG hallucination detection reveals a fundamental limitation: while embedding-based methods achieve 95% coverage with 0% FPR on synthetic hallucinations, they fail catastrophically on real LLM outputs, producing 100% FPR at the same coverage target. The "Semantic Illusion" causes RLHF-trained models to generate hallucinations that are semantically indistinguishable from faithful responses, forcing conformal thresholds to reject all valid outputs. This occurs because the distributional tail of hard hallucinations overlaps completely with faithful responses. In contrast, GPT-4 reasoning-based judging achieves 7% FPR on the same data, proving the task is solvable but requires reasoning rather than surface-level semantics. The findings establish that embedding similarity is insufficient for safety-critical RAG applications and that organizations must accept a 30× cost increase to achieve reliable detection using LLM judges.

## Method Summary
The paper applies Split Conformal Prediction (SCP) to RAG hallucination detection, using three nonconformity scores: RAD (cosine distance between response and context embeddings via BGE-base-en-v1.5), SEC (1 - entailment probability via DeBERTa-v3-large-MNLI), and TFG (percentage of low-overlap tokens). These scores are computed on calibration sets of labeled hallucinations (n≈600 recommended) to derive a quantile threshold τ̂ that guarantees (1-α) coverage. At test time, responses scoring below τ̂ are flagged as hallucinations. The method is evaluated on synthetic hallucinations (Natural Questions with answer-swapping) and real LLM outputs (HaluEval, ChatGPT-generated), revealing that RLHF training creates a "Semantic Illusion" where hard hallucinations are distributionally indistinguishable from faithful responses in embedding space.

## Key Results
- Embedding ensemble achieves 95% coverage with 0% FPR on synthetic hallucinations
- Same embedding ensemble produces 100% FPR on real ChatGPT outputs at 95% coverage
- GPT-4 reasoning-based judging achieves 7% FPR on the same data
- AUC remains high (0.81-0.83) despite catastrophic conformal FPR on RLHF outputs

## Why This Works (Mechanism)

### Mechanism 1: Conformal Prediction Transforms Heuristics Into Guaranteed Decision Boundaries
Split Conformal Prediction provides finite-sample coverage guarantees for hallucination detection, but the guarantee reveals rather than solves underlying separability limits. SCP computes a calibration quantile τ̂ from labeled hallucinations (n≈600). At test time, responses scoring below τ̂ are flagged. The threshold is mathematically guaranteed to capture at least (1-α)% of true hallucinations with probability (1-δ), assuming exchangeability between calibration and test data. The core assumption is that the calibration distribution is exchangeable with deployment data; distribution shift invalidates guarantees. Evidence shows that when calibration uses synthetic hallucinations while deployment contains RLHF-generated outputs, the quantile τ̂ will be misaligned—the guarantee holds mathematically but the FPR becomes meaningless.

### Mechanism 2: The Semantic Illusion—RLHF Optimizes Hallucinations Toward Embedding-Space Entailment
RLHF training inadvertently creates "Type 2" hallucinations that achieve high semantic similarity and entailment scores relative to retrieved context, making them distributionally indistinguishable from faithful responses. RLHF optimizes for human preference metrics correlated with fluency, coherence, and plausibility. Hallucinations that "sound confident and semantically consistent" receive higher reward. Over training iterations, this pressure sculpts model outputs such that factual errors preserve the "vibe" of truth. The distributional tail of hallucinations overlaps completely with faithful responses in embedding space. The core assumption is that this failure is caused by RLHF optimization dynamics rather than being an artifact of specific embedding models or NLI models used. Evidence shows that the hallucinated class has massive standard deviation (0.46) with bimodal distribution—many obvious hallucinations (score≈0) but a significant tail achieving near-perfect entailment (score≈1.0).

### Mechanism 3: Reasoning-Based Detection Recovers Separability at 30× Cost
GPT-4 as a judge can identify factual divergence that embeddings cannot, achieving 7% FPR at 95% coverage, but this requires explicit reasoning computation at substantially higher cost. LLM judges perform multi-step reasoning over the relationship between retrieved context and generated response, identifying logical inconsistencies, unsupported claims, and factual contradictions. This reasoning capacity can distinguish between "plausible but wrong" and "actually faithful" in a way that surface-level semantic similarity cannot. The core assumption is that the GPT-4 judge performance (7% FPR, 95% CI: [3.4%, 13.7%]) generalizes beyond the HaluEval subset (n=200) tested; prompt engineering variations may affect results. Evidence shows that cost comparison reveals embedding methods at ~$0.10/1K queries vs. GPT-4o-mini at $15.00/1K queries (150× cost multiplier).

## Foundational Learning

- Concept: Conformal Prediction and Coverage Guarantees
  - Why needed here: The paper's central methodology transforms heuristic hallucination scores into decision sets with mathematically guaranteed coverage. Without understanding SCP, the 0% vs. 100% FPR results are unintelligible.
  - Quick check question: Given calibration scores [0.2, 0.4, 0.5, 0.6, 0.8] from 5 hallucinations, what quantile gives 80% coverage (α=0.2) under split conformal prediction?

- Concept: Distributional Tails vs. Average-Case Metrics
  - Why needed here: The paper demonstrates that high AUC (0.81) does not imply safety—coverage guarantees are determined by the hardest hallucinations in the tail, not average separability.
  - Quick check question: A detector achieves 0.90 AUC on a binary classification task. Why might it still fail when constrained to 95% recall with low FPR?

- Concept: RLHF and Output Distribution Shift
  - Why needed here: The "Semantic Illusion" phenomenon is attributed to RLHF optimization dynamics. Understanding how preference optimization shapes output distributions is essential for anticipating detection failures.
  - Quick check question: How does optimizing for human preference scores (fluency, coherence) differ from optimizing for factual accuracy, and what failure modes might emerge?

## Architecture Onboarding

- Component map:
  - Nonconformity Scoring Layer (RAD, SEC, TFG) -> Conformal Calibration Layer (computes τ̂) -> Decision Layer (flags if score ≤ τ̂)

- Critical path:
  1. Collect and label calibration hallucinations from target LLM (not synthetic substitutes)
  2. Compute all three nonconformity scores for calibration set
  3. Derive τ̂ quantile for desired coverage level
  4. At inference, score new responses and flag if below threshold
  5. Monitor actual FPR on held-out faithful responses—100% FPR indicates Semantic Illusion failure

- Design tradeoffs:
  - Embedding vs. LLM Judge: Embeddings are 150× faster/cheaper but fail on RLHF outputs; LLM judges provide reliable detection at 30× cost increase
  - Synthetic vs. Real Calibration: Synthetic hallucinations (answer swapping) produce misleadingly optimistic τ̂; real LLM hallucinations are required but expensive to collect
  - Coverage vs. FPR: Higher coverage targets (lower α) force lower τ̂, which increases FPR; on RLHF data, 95% coverage forces 100% FPR
  - Simple Average vs. Learned Combiner: Learned combiner improves AUC (0.83 vs. 0.81) but produces identical conformal FPR (100%)—tail behavior cannot be fixed by weighting

- Failure signatures:
  - Calibration-Test Distribution Mismatch: If calibration uses synthetic hallucinations and deployment sees RLHF outputs, τ̂ will be too permissive—hallucinations will pass undetected
  - 100% FPR at Target Coverage: If conformal threshold rejects all faithful responses, the Semantic Illusion is present; no threshold tuning can help
  - High AUC with Catastrophic Conformal FPR: AUC measures average separability; 0.81 AUC is compatible with 100% FPR if the hardest hallucinations overlap with faithful responses

- First 3 experiments:
  1. Calibration Set Validation: On your target LLM, collect n=600 real hallucinations (not synthetic). Compute RAD, SEC, TFG scores and derive τ̂ at α=0.05. Test on held-out faithful responses. If FPR > 50%, the Semantic Illusion is present—embedding-based detection is unsuitable for your safety requirements.
  2. Cost-Safety Tradeoff Characterization: Implement the embedding ensemble and a GPT-4o-mini judge. On a shared test set (n=200), measure FPR at 95% coverage for both. Calculate your actual cost-per-query delta. If embedding FPR is unacceptable, budget for the 30× cost increase.
  3. Hybrid Cascade Prototype: Build a two-stage system—Stage 1 uses embeddings to catch obvious hallucinations (Type 1, semantically distinct); Stage 2 escalates uncertain cases (scores near τ̂) to LLM judge. Measure coverage, FPR, and average cost per query. This may achieve acceptable safety at reduced cost compared to full LLM judging.

## Open Questions the Paper Calls Out

- Can mechanistic probes on internal model activations solve the "Semantic Illusion" by capturing signals opaque to surface embeddings?
- How can conformal validity be mathematically formalized for hybrid architectures that cascade from cheap embedding checks to expensive LLM judges?
- Can online conformal prediction methods maintain coverage guarantees under the distribution shift inherent in production deployments?

## Limitations

- The Semantic Illusion mechanism lacks mechanistic interpretability evidence and may be specific to the embedding models or NLI models used
- The cost comparison assumes GPT-4o-mini judge performance generalizes beyond the HaluEval subset tested
- Calibration set requirements create practical barriers: achieving reliable conformal coverage requires ~600 labeled hallucinations from the target LLM

## Confidence

- High confidence: The empirical finding that conformal prediction reveals distributional separability limits (0% FPR on synthetic vs. 100% FPR on RLHF outputs). The mathematical validity of SCP coverage guarantees is well-established.
- Medium confidence: The attribution of the failure to RLHF optimization dynamics creating "Type 2" hallucinations. While the distributional evidence is strong, alternative explanations (embedding model limitations, task formulation) are not fully ruled out.
- Medium confidence: The assertion that reasoning-based detection is the only viable alternative. GPT-4 performance suggests the signal exists, but generalization to other reasoning models and cost-reduction strategies remains uncertain.

## Next Checks

1. Use interpretability tools to examine whether RLHF-trained models produce distinct internal activation patterns for faithful vs. hallucinated outputs that embeddings miss, confirming whether the limitation is fundamental to external semantic representations or solvable with better representations.

2. Test GPT-4o-mini judge performance across diverse RAG datasets (different domains, query types, retrieved contexts) to verify the 7% FPR claim is not dataset-specific, and explore prompt engineering variations to establish performance bounds.

3. Implement and evaluate the proposed cascade approach—embedding-based detection for obvious hallucinations followed by LLM judging for uncertain cases—to measure whether this achieves acceptable safety at reduced cost compared to full LLM judging, establishing the actual Pareto frontier for practical deployment.