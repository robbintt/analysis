---
ver: rpa2
title: 'Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for
  Translating Sylheti to Modern Bangla'
arxiv_id: '2505.18709'
source_url: https://arxiv.org/abs/2505.18709
tags:
- bangla
- language
- lstm
- dataset
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This research addresses the challenge of translating Sylheti,\
  \ a local variant of Bangla spoken in the Sylhet region of Bangladesh, into Modern\
  \ Bangla to bridge communication gaps. The authors developed a system using Natural\
  \ Language Processing (NLP) techniques and trained three deep learning models\u2014\
  LSTM, Bi-LSTM, and Seq2Seq\u2014on a dataset of 1,200 sentence pairs."
---

# Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla

## Quick Facts
- arXiv ID: 2505.18709
- Source URL: https://arxiv.org/abs/2505.18709
- Reference count: 0
- LSTM model achieved highest accuracy of 89.3% on Sylheti-to-Bangla translation

## Executive Summary
This research addresses the challenge of translating Sylheti, a local variant of Bangla spoken in the Sylhet region of Bangladesh, into Modern Bangla to bridge communication gaps. The authors developed a system using Natural Language Processing (NLP) techniques and trained three deep learning models—LSTM, Bi-LSTM, and Seq2Seq—on a dataset of 1,200 sentence pairs. After preprocessing steps including punctuation removal, stemming, padding, and tokenization, the LSTM model achieved the highest performance with an accuracy of 89.3%. This work contributes to Bangla NLP research by enabling better understanding of regional dialects and lays the groundwork for future applications and larger-scale studies.

## Method Summary
The study employed three deep learning architectures (LSTM, Bi-LSTM, and Seq2Seq) to translate Sylheti to Modern Bangla. The 1,200 sentence pairs were preprocessed through punctuation removal, stemming, padding to fixed lengths (50 tokens for LSTM, 12 for Seq2Seq), tokenization, and vectorization. Models were implemented in Keras/TensorFlow with LSTM using 64-dimensional embeddings and 128 units, Bi-LSTM using 256-dimensional embeddings, and Seq2Seq with encoder-decoder architecture. Training used categorical cross-entropy loss, Adam optimizer, 50 epochs, and batch sizes of 32-64. The LSTM model with its simpler unidirectional architecture achieved the best results.

## Key Results
- LSTM model achieved 89.3% accuracy, outperforming Bi-LSTM (76.2%) and Seq2Seq (70.7%)
- Preprocessing steps including punctuation removal, stemming, and tokenization were critical for performance
- The study represents the first neural network approach for Sylheti-to-Bangla dialect translation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LSTM's gate-controlled memory may outperform bidirectional and encoder-decoder architectures for small dialect translation datasets.
- Mechanism: The forget gate ($f_t = \sigma(W_f[h_{t-1}, x_t] + b_f)$) selectively retains dialect-specific lexical patterns while discarding noise. With only 1,200 sentence pairs, the simpler single-direction LSTM (2.7M parameters) generalizes better than Bi-LSTM (14.7M parameters) or Seq2Seq (2M parameters with encoder-decoder complexity).
- Core assumption: Sylheti-to-Bangla translation relies more on local sequential patterns than bidirectional context or compressed latent representations.
- Evidence anchors:
  - [abstract] "LSTM model achieved the highest performance with an accuracy of 89.3%"
  - [section] Table III shows LSTM accuracy 0.8973 vs Bi-LSTM 0.7625 vs Seq2Seq 0.7069
  - [corpus] "LLMs for Low-Resource Dialect Translation Using Context-Aware Prompting: A Case Study on Sylheti" confirms low-resource dialect translation remains challenging; standard neural approaches struggle without larger corpora.
- Break condition: If dataset scales beyond 10,000 pairs, Bi-LSTM or transformer architectures may overtake unidirectional LSTM due to richer context modeling.

### Mechanism 2
- Claim: Aggressive preprocessing (punctuation removal, stemming) reduces vocabulary sparsity, improving model convergence on low-resource data.
- Mechanism: Bangla is morphologically rich; stemming reduces word forms to roots (e.g., "পড়তে যাচ্ছি" → "পড়া"). With only 1,200 pairs, vocabulary reduction prevents overfitting to rare surface forms.
- Core assumption: Dialect differences are primarily lexical/phonological rather than syntactic, so morphological normalization helps without losing structural information.
- Evidence anchors:
  - [abstract] "preprocessing steps including punctuation removal, stemming, padding, and tokenization"
  - [section] "Stemming changes a word to its founding or root form. It reduces the dimension of a word."
  - [corpus] Weak corpus evidence—no direct validation of stemming effectiveness for Sylheti specifically.
- Break condition: If stemming is too aggressive (conflating distinct meanings), precision drops. Better stemmers (noted as lacking for Bangla in the paper) would improve this.

### Mechanism 3
- Claim: Padding to fixed length (50 tokens for LSTM) enables batch training but introduces information loss for longer sentences.
- Mechanism: Padding ensures uniform tensor shapes for GPU acceleration. The paper sets max length to 50 for LSTM, meaning sentences longer than 50 tokens are truncated.
- Core assumption: Most Sylheti sentences fit within 50 tokens; truncation loss is minimal.
- Evidence anchors:
  - [abstract] Not mentioned directly
  - [section] "The vocabulary size is set to 10000 and maximum length to 50. Each layer will accept about 50-dimensional input."
  - [corpus] Weak corpus evidence—no comparison of padding strategies in neighboring papers.
- Break condition: If dialect sentences are systematically longer than standard Bangla, truncation biases translation toward shorter, simpler constructions.

## Foundational Learning

- Concept: **Tokenization and Vocabulary Construction**
  - Why needed here: Raw Bangla text must be split into subword units before embedding. The paper tokenizes sentences into word lists (e.g., "কি খেইন আইল খাও" → ['কি', 'খেইন', 'আইল', 'খাও']).
  - Quick check question: Can you explain why vocabulary size (10,000) affects both model capacity and overfitting risk?

- Concept: **Sequence-to-Sequence Encoder-Decoder Architecture**
  - Why needed here: Seq2Seq is a baseline for translation tasks. Understanding why it underperformed (70.7% accuracy) informs architectural choices.
  - Quick check question: Why might a fixed-size context vector bottleneck translation quality compared to direct LSTM sequence prediction?

- Concept: **Vanishing Gradient Problem in RNNs**
  - Why needed here: The paper explicitly states LSTM addresses vanishing gradients ("used to design vanishing gradient problems. This problem is generally found in RNN").
  - Quick check question: How do LSTM gates ($i_t$, $f_t$, $o_t$) mathematically mitigate gradient decay during backpropagation?

## Architecture Onboarding

- Component map:
  Input: Sylheti sentence (string) -> Preprocessing: Punctuation removal -> Stemming -> Tokenization -> Padding -> Integer encoding -> Embedding layer: 64-dim (LSTM) or 256-dim (Bi-LSTM/Seq2Seq) dense vectors -> Sequence model: LSTM (128 units, unidirectional) / Bi-LSTM (256 units each direction) / Seq2Seq (encoder LSTM + decoder LSTM) -> Output: Dense layer with softmax over vocabulary (10,000 classes) -> Loss: Categorical cross-entropy

- Critical path:
  1. Data preprocessing quality directly determines vocabulary coherence—errors here propagate through training
  2. Embedding dimension must balance expressiveness (higher) with overfitting risk on small data (lower is safer)
  3. Epoch count (50) and batch size (32-64) were chosen empirically; monitor validation loss divergence

- Design tradeoffs:
  - LSTM vs Bi-LSTM: Unidirectional LSTM (2.7M params, 89.7% acc) vs Bi-LSTM (14.7M params, 76.2% acc). Larger model overfits small data.
  - Seq2Seq vs direct prediction: Seq2Seq's encoder-decoder adds flexibility but introduces bottleneck; underperformed on this task.
  - Stemming: Reduces vocabulary but may conflate distinct word senses—trade-off between generalization and precision.

- Failure signatures:
  - Validation accuracy plateaus while training accuracy rises: Overfitting (seen in Bi-LSTM after epoch 40)
  - Validation loss increases while training loss decreases: Overfitting (Bi-LSTM shows this after epoch 13)
  - Low recall on rare dialect words: Vocabulary coverage insufficient; increase data or use subword tokenization

- First 3 experiments:
  1. **Baseline reproduction**: Implement the LSTM model with exact hyperparameters (vocab=10,000, max_len=50, embedding=64, units=128, epochs=50, batch=32). Verify accuracy ≈89% on the 1,200-pair dataset.
  2. **Ablation study**: Remove stemming and compare accuracy. Hypothesis: Accuracy will drop due to increased vocabulary sparsity.
  3. **Data scaling test**: Augment dataset to 5,000 pairs (via collection or augmentation) and retrain Bi-LSTM. Hypothesis: Bi-LSTM may overtake LSTM with more data due to better context modeling.

## Open Questions the Paper Calls Out
None

## Limitations
- Extremely small dataset (1,200 sentence pairs) raises concerns about statistical significance and generalization
- No public dataset or code repository available for independent verification
- Stemming methodology vaguely described without specific rules or lexicon details
- No qualitative evaluation or user studies to validate practical utility of translations

## Confidence
- High Confidence (90%+): LSTM architecture's superior performance over Bi-LSTM and Seq2Seq on this specific dataset
- Medium Confidence (60-80%): Claim of being first work on Sylheti-to-Bangla translation
- Low Confidence (30-50%): Practical utility of achieving 89% accuracy on dialect translation without qualitative validation

## Next Checks
1. **Dataset Verification and Expansion**: Attempt to reproduce the study with an independently collected dataset of 1,200+ Sylheti-Modern Bangla sentence pairs. If possible, expand to 5,000+ pairs to test the data scaling hypothesis for Bi-LSTM performance.

2. **Ablation Study on Preprocessing**: Implement versions of the LSTM model with and without stemming, and with different tokenization strategies (word-level vs subword). Compare accuracy drops to quantify the true contribution of each preprocessing step.

3. **Human Evaluation of Translation Quality**: Conduct a blind test where native Bangla speakers evaluate LSTM-translated sentences against reference translations. Measure BLEU, TER, or human judgment scores to determine if 89% accuracy corresponds to usable translations or just high lexical overlap.