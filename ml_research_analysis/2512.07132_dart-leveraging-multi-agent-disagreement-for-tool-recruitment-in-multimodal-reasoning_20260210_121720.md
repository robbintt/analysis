---
ver: rpa2
title: 'DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal
  Reasoning'
arxiv_id: '2512.07132'
source_url: https://arxiv.org/abs/2512.07132
tags:
- dart
- tool
- tools
- reasoning
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DART improves multimodal reasoning by using multi-agent debate
  to identify and resolve disagreements with expert vision tools. The framework iteratively
  recruits specialized tools (e.g., object detection, OCR, spatial reasoning) when
  VLMs disagree, then scores agent alignment with tool outputs to guide discussion.
---

# DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning

## Quick Facts
- arXiv ID: 2512.07132
- Source URL: https://arxiv.org/abs/2512.07132
- Reference count: 32
- Improves A-OKVQA accuracy by 3.4% over multi-agent debate and 19% over tool-calling baselines

## Executive Summary
DART is a multimodal reasoning framework that enhances Visual Question Answering by leveraging disagreements among multiple vision-language models (VLMs) to identify when and which specialized vision tools should be recruited. The system employs a multi-agent debate structure where three VLMs generate initial answers and reasoning, followed by a Recruiter LLM that detects disagreements and calls appropriate vision tools (e.g., object detection, OCR, spatial reasoning). The framework then uses an Agreement Scorer to evaluate how well each agent's reasoning aligns with tool outputs, guiding a discussion phase that refines answers. This iterative process results in more accurate and diverse discussions compared to standard multi-agent debate approaches.

## Method Summary
The DART framework implements a 5-step pipeline: (1) Three VLMs generate initial answers, reasoning, and confidence scores; (2) A Recruiter LLM identifies disagreements in the reasoning and selects appropriate vision tools; (3) An Agreement Scorer compares each agent's reasoning against tool outputs to generate alignment scores; (4) Agents engage in discussion using grouped answers, tool outputs, and alignment scores to refine their responses; (5) An Aggregator VLM selects the final answer. The framework uses Qwen2.5-VL, MiniCPM-o, and Ovis2 as VLMs, with Qwen2.5 as the Recruiter and Agreement Scorer, and Ovis2 as the Aggregator. Vision tools include GroundingDINO, YOLOv11, SpaceLLaVA, OCR-Qwen, and InternVL-2.5.

## Key Results
- Improves A-OKVQA accuracy by 3.4% over multi-agent debate baseline
- Improves A-OKVQA accuracy by 19% over tool-calling baseline
- Adapts to new domains with 1.3% improvement on medical VQA (M3D-VQA)

## Why This Works (Mechanism)
DART works by converting VLM disagreements into actionable signals for tool recruitment. When multiple agents disagree on an answer, the disagreement likely indicates uncertainty about specific visual elements in the image. The Recruiter LLM identifies these disagreements and maps them to specific vision tool capabilities - for example, spatial reasoning disagreements trigger SpaceLLaVA calls, while object identification disagreements trigger grounding or detection tools. The Agreement Scorer then provides objective feedback about which agent's reasoning aligns best with the ground truth revealed by the tools, creating a self-correcting mechanism that guides the discussion toward the correct answer.

## Foundational Learning

**Vision-Language Model Integration**: VLMs process both visual and textual information to answer questions about images. Needed to understand the baseline reasoning capabilities and limitations. Quick check: Verify VLMs can answer simple visual questions without tools.

**Multi-Agent Debate Framework**: Multiple agents generate diverse answers that can be compared and debated. Needed to create the initial disagreement signal. Quick check: Run 3 agents independently and measure answer diversity.

**Vision Tool Specialization**: Different tools excel at specific visual tasks (detection, OCR, spatial reasoning). Needed to map disagreements to appropriate tool solutions. Quick check: Test each tool on its target task to verify functionality.

**LLM-Based Orchestration**: Large language models can act as meta-reasoners to coordinate other models and tools. Needed for the Recruiter and Agreement Scorer roles. Quick check: Verify LLM can follow tool-calling instructions from prompts.

## Architecture Onboarding

**Component Map**: Image+Question -> 3 VLMs -> Recruiter LLM -> Vision Tools -> Agreement Scorer -> Discussion -> Aggregator VLM -> Final Answer

**Critical Path**: Image+Question → VLMs → Recruiter → Tools → Agreement Scorer → Discussion → Aggregator → Answer

**Design Tradeoffs**: 
- Multiple VLMs provide diversity but increase computational cost
- Tool recruitment adds accuracy but increases latency
- Binary agreement scoring is simple but may lose nuance compared to continuous scoring

**Failure Signatures**:
- Low tool diversity indicates Recruiter isn't detecting true disagreements
- High latency suggests inefficient tool calling implementation
- Stagnant accuracy after round 1 indicates discussion phase isn't productive

**First Experiments**:
1. Run baseline multi-agent debate without tool recruitment to establish baseline accuracy
2. Test Recruiter's disagreement detection on questions with known visual ambiguities
3. Measure agreement scoring calibration by comparing tool-based scores to ground truth

## Open Questions the Paper Calls Out

**Open Question 1**: How can DART sustain performance improvements over multiple debate rounds when current iterations stagnate after the first round? The paper identifies that performance stagnates (<2% gain) in rounds 2-3, often distracted by minor disagreements, but doesn't propose solutions to filter these or generate productive new lines of inquiry.

**Open Question 2**: Why do tool-based agreement scores exhibit worse calibration (higher Expected Calibration Error) than self-reported confidence, and can this be remedied? The raw tool-based confidence metric has higher ECE (0.4901) than self-reported confidence (0.3859), indicating alignment with tools is a noisy proxy for correctness, but the paper doesn't investigate root causes or test alternative scoring functions.

**Open Question 3**: Can the latency and computational overhead of DART's tool recruitment be reduced to match or exceed standard multi-agent debate baselines? DART incurs higher latency (32.75s) compared to the baseline (27.92s) due to inefficient tool-calling implementation, but the paper doesn't offer or evaluate an optimized architecture for tool execution.

## Limitations

- Tool output processing pipeline from raw tool outputs to Agreement Scorer inputs is underspecified
- Limited generalization evidence beyond single medical VQA domain adaptation
- Multiple debate rounds show diminishing returns with minimal accuracy gains

## Confidence

- **High Confidence**: Core multi-agent debate framework and iterative tool recruitment concept are well-articulated and theoretically sound
- **Medium Confidence**: Implementation details sufficient for functional reproduction, though missing tool output processing details may impact exact results
- **Low Confidence**: Claims about domain adaptability beyond medical VQA example are not well-supported; scalability to different domains untested

## Next Checks

1. **Disagreement Detection Validation**: Implement Recruiter's disagreement detection with multiple threshold settings and evaluate how different definitions affect tool recruitment accuracy and final answer quality. Compare agreement detection performance against human-annotated disagreement labels.

2. **Tool Output Processing Pipeline**: Develop and validate the tool output preprocessing pipeline by creating a benchmark of tool outputs and expected Agreement Scorer inputs. Measure impact of different preprocessing strategies on agreement scoring accuracy and downstream answer quality.

3. **Cross-Domain Transfer Test**: Apply pre-trained DART framework (without fine-tuning) to a distinctly different VQA domain (e.g., satellite imagery analysis) and measure performance degradation. Compare to baseline using domain-specific tool recruitment from scratch to quantify transfer learning benefits.