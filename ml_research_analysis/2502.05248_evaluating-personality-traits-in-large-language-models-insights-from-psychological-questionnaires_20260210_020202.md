---
ver: rpa2
title: 'Evaluating Personality Traits in Large Language Models: Insights from Psychological
  Questionnaires'
arxiv_id: '2502.05248'
source_url: https://arxiv.org/abs/2502.05248
tags:
- personality
- llms
- traits
- across
- questionnaires
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates personality traits in Large
  Language Models (LLMs) using psychological questionnaires to understand behavioral
  patterns. To address potential training data contamination, questionnaires were
  restructured and validated, with responses gathered across multiple iterations.
---

# Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires
## Quick Facts
- **arXiv ID**: 2502.05248
- **Source URL**: https://arxiv.org/abs/2502.05248
- **Reference count**: 18
- **Primary result**: LLMs exhibit distinct personality profiles with higher scores in Agreeableness, Openness, and Conscientiousness, reflecting cooperative and creative behavior

## Executive Summary
This study systematically evaluates personality traits in Large Language Models using psychological questionnaires to understand behavioral patterns. The research addresses potential training data contamination by restructuring and validating questionnaires, collecting responses across multiple iterations. Results reveal that LLMs display distinct personality profiles, with Neuroticism showing the highest variability across models, while Extraversion and Agreeableness demonstrate more consistency. The study highlights how training strategies and questionnaire design influence the assessment of LLM personality traits.

## Method Summary
The study employed a systematic approach to evaluate personality traits in LLMs using psychological questionnaires. To mitigate training data contamination risks, researchers restructured and validated the questionnaires, then gathered responses through multiple iterations. This methodological rigor aimed to ensure that observed personality patterns reflected genuine behavioral characteristics rather than memorized responses from training data.

## Key Results
- LLMs exhibit distinct personality profiles with higher scores in Agreeableness, Openness, and Conscientiousness
- Neuroticism shows the highest variability across models, while Extraversion and Agreeableness are more consistent
- Dimensional dominance varies by model, with GPT-4 emphasizing Agreeableness and Llama models showing dominance in Conscientiousness or Openness

## Why This Works (Mechanism)
The evaluation framework works by systematically probing LLMs with psychological questionnaires designed to elicit responses revealing underlying personality traits. By restructuring these questionnaires to minimize contamination from training data, the approach aims to capture genuine behavioral patterns rather than memorized responses. The iterative response collection across multiple rounds helps establish consistency and reliability in the observed personality profiles.

## Foundational Learning
- **Psychological Questionnaire Design**: Understanding how personality assessments are structured and validated is crucial for interpreting LLM responses accurately and ensuring measurement validity
- **Personality Trait Theory**: Familiarity with the Big Five personality dimensions (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) provides the theoretical framework for analyzing LLM behavioral patterns
- **Training Data Contamination**: Recognizing how exposure to similar assessment materials during training could bias results is essential for evaluating the validity of findings
- **Dimensional Analysis**: Statistical techniques for comparing variability across different personality dimensions help identify which traits are most stable or variable in LLM behavior
- **Iterative Validation**: Understanding the importance of multiple response rounds for establishing reliability and detecting systematic biases in model outputs
- **Model-Specific Behavioral Patterns**: Recognizing that different LLMs may exhibit distinct personality profiles based on their training strategies and architectural differences

## Architecture Onboarding
- **Component Map**: Questionnaire Design -> Model Input Generation -> Response Collection -> Statistical Analysis -> Personality Profile Mapping
- **Critical Path**: The evaluation workflow follows a sequential process from questionnaire preparation through response analysis to final personality trait assessment
- **Design Tradeoffs**: Balancing questionnaire complexity against contamination risk, and choosing between comprehensive trait coverage versus focused behavioral assessment
- **Failure Signatures**: Inconsistent responses across iterations may indicate training data contamination or model sensitivity to prompt variations
- **First Experiments**:
  1. Test questionnaire response consistency across multiple temperature settings
  2. Compare model responses to blinded vs. known questionnaire formats
  3. Validate findings against human-rater baseline variability measurements

## Open Questions the Paper Calls Out
None

## Limitations
- Unknown extent of training data contamination despite questionnaire restructuring efforts
- Dimensional variability results may be influenced by questionnaire design choices rather than genuine behavioral differences
- Methodology does not account for prompt sensitivity or response variability across different contexts

## Confidence
- **GPT-4 Agreeableness emphasis**: Medium confidence due to potential confounding factors
- **Llama model Conscientiousness/Openness dominance**: Medium confidence with similar qualifications
- **Extraversion and Agreeableness consistency**: Medium confidence, limited by methodology constraints
- **Neuroticism variability findings**: Medium confidence, influenced by design choices

## Next Checks
1. Conduct blinded tests using entirely novel personality assessment frameworks that have never appeared in public datasets
2. Implement cross-validation with human-rater comparisons to establish baseline variability for each trait dimension
3. Test model responses across multiple temperature settings and prompt engineering variations to isolate genuine personality patterns from stochastic generation effects