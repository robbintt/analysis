---
ver: rpa2
title: Large Language Models as Nondeterministic Causal Models
arxiv_id: '2509.22297'
source_url: https://arxiv.org/abs/2509.22297
tags:
- causal
- counterfactuals
- semantics
- deterministic
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating counterfactual outputs
  for probabilistic Large Language Models (LLMs), which tell us what the output might
  have been if a different prompt had been given. The author argues that previous
  methods, while useful for some applications, rest on ambiguous interpretations of
  LLMs and require access to the LLM's source code.
---

# Large Language Models as Nondeterministic Causal Models

## Quick Facts
- arXiv ID: 2509.22297
- Source URL: https://arxiv.org/abs/2509.22297
- Reference count: 20
- Key outcome: Representing LLMs as nondeterministic causal models yields "simple semantics" where counterfactual distribution equals observational distribution, enabling counterfactual generation without source code access.

## Executive Summary
This paper addresses the problem of generating counterfactual outputs for probabilistic Large Language Models (LLMs), which tell us what the output might have been if a different prompt had been given. The author argues that previous methods, while useful for some applications, rest on ambiguous interpretations of LLMs and require access to the LLM's source code. The core method idea is to represent LLMs as nondeterministic causal models, which allows for a simpler and more general approach to counterfactual generation. The key result is that this representation satisfies the "simple semantics" of counterfactuals, meaning that the counterfactual distribution is identical to the observational distribution and does not depend on the actual output. This has the major practical benefit that counterfactual queries can be computed in the same way as factual queries, and thus nothing more than access to the LLM is required.

## Method Summary
The method represents LLMs as nondeterministic causal models where the prompt is a root variable parent to all downstream tokens. Counterfactual generation is performed by simply re-running the LLM with the alternative prompt, treating the LLM as a black box. The theoretical framework uses Beckers' definitions of nondeterministic causal models and counterfactual semantics, with proofs showing that this approach satisfies "simple semantics" where counterfactual distributions equal observational distributions. The key insight is that when intervening on the root prompt variable, the actual output provides no constraining information because it changes the parents of all downstream tokens.

## Key Results
- Representing LLMs as nondeterministic causal models satisfies "simple semantics" where counterfactual distribution equals observational distribution
- Counterfactual queries can be computed by re-running the LLM with the alternative prompt, requiring no source code access
- Deterministic causal model approaches require source code access and make counterfactual identification impossible without implementation details

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If an LLM is represented as a nondeterministic causal model (NCM), counterfactual queries collapse to observational queries, meaning counterfactuals can be generated simply by re-running the model with the alternative prompt.
- **Mechanism:** The paper models the prompt $X$ as a root variable and parent to all downstream tokens $T_i$. In an NCM, intervening on a root variable ($X \to X^*$) changes the parent values for all downstream variables. The semantics of NCMs (Definition 2) state that updating probabilities based on actual evidence only constrains mechanisms when parents retain their actual values. Since the counterfactual prompt changes the parents of all tokens, the actual output $Y$ provides no constraining information, resulting in $P^*(Y^*|Y=y, X=x, X^*=x^*) = P(Y|X=x^*)$.
- **Core assumption:** The LLM operates as an *idealized* nondeterministic system where the sampling process is a primitive probability distribution, rather than a deterministic pseudo-random process (PRNG) dependent on hidden seeds.
- **Evidence anchors:** [Abstract] "The key result is that this representation satisfies the 'simple semantics'... counterfactual distribution is identical to the observational distribution." [Section 4 / Theorem 2] "Given a nondeterministic causal model M that corresponds to an LLM, M satisfies the simple semantics." [Corpus] "Actual Causation and Nondeterministic Causal Models" (Beckers, 2025) provides the underlying definition of NCMs used to derive this result.
- **Break condition:** This mechanism fails if the user requires the counterfactual output to be causally coupled to the specific random seed (exogenous noise) of the original run, which requires a deterministic representation.

### Mechanism 2
- **Claim:** Representing LLMs as deterministic causal models (DCMs) makes counterfactual identification impossible without access to internal implementation details (source code/seeds).
- **Mechanism:** In DCMs, variability is modeled by "pulling" probability out of the mechanism and into exogenous variables $U$ (e.g., random seeds). To compute a counterfactual, one must infer the specific value of $U$ that caused the actual output. There are infinitely many mappings of $U$ to outputs consistent with the observational distribution (Example 1), making the counterfactual unidentifiable without accessing the specific sampling implementation (the "canonical" choice).
- **Core assumption:** The paper assumes that for black-box LLMs, the specific implementation of the sampling process (PRNG algorithm, seed generation) is inaccessible or irrelevant to the user's intent.
- **Evidence anchors:** [Section 5] "If we model the LLM as a deterministic causal model, we are unable to compute probabilities of counterfactuals... a fortiori we are unable to faithfully generate counterfactuals." [Section 3.2] Describes how deterministic models require satisfying $v=f(u,r)$, necessitating knowledge of $u$.
- **Break condition:** If the LLM provider exposes the seed $U$ or specific sampling function $g$ used for a generation, this mechanism allows for deterministic counterfactual generation (the "literal interpretation").

### Mechanism 3
- **Claim:** The "Gumbel-based" approach (Chatzi et al.) is not a general semantic for LLMs, but a specific application of DCMs designed to enforce "counterfactual stability" (closeness to the original output).
- **Mechanism:** By using the Gumbel-max trick, the deterministic model selects the output that maximizes score plus noise. Reusing the same noise $U$ for a counterfactual prompt $X^*$ biases the output toward the factual output $Y$ (if the probabilities overlap). This is useful for model evaluation (comparing efficiency) but detrimental for explanation (which requires semantic distance).
- **Core assumption:** Counterfactual explanations require finding outputs *distant* from the actual output, assuming that small prompt changes should identify distinct causal factors.
- **Evidence anchors:** [Section 1] "Such a bias towards closeness is desirable for certain purposes only... undesirable in the context of counterfactual explanations." [Section 6 / Definition 5] Formalizes "counterfactually stable distribution" P*_st as a biased version of the simple semantics. [Corpus] "Counterfactual Token Generation in Large Language Models" (Chatzi et al., 2025) is identified as the target of this critique regarding stability.
- **Break condition:** This mechanism holds only if the user's goal is strictly evaluation via coupled outputs; it breaks if the user wants to explore the full output space independent of the original realization.

## Foundational Learning

- **Concept:** **Pearlâ€™s Causal Hierarchy (Ladder of Causation)**
  - **Why needed here:** The paper argues for a "collapse" of this hierarchy specifically for LLMs. You must understand the distinction between Layer 1 (Observational: seeing), Layer 2 (Interventional: doing), and Layer 3 (Counterfactual: imagining) to grasp why equating Layer 1 and Layer 3 is a significant theoretical claim.
  - **Quick check question:** Does the "simple semantics" claim that $P(Y|X)$ (Layer 1) is sufficient to answer "What would $Y$ be if $X$ were $X^*$?" (Layer 3)?

- **Concept:** **Exogenous Variables ($U$) and Identifiability**
  - **Why needed here:** Section 5 hinges on the problem of unidentifiable exogenous variables in deterministic models. Understanding that $U$ represents the "hidden noise" or "seed" is crucial to distinguishing the author's "idealized" (no $U$ needed) approach from the "literal" (requires $U$) approach.
  - **Quick check question:** In a deterministic causal model of an LLM, why can't we determine the counterfactual output without knowing the specific random seed $U$ used in the original generation?

- **Concept:** **Autoregressive Token Generation**
  - **Why needed here:** The mathematical proof (Theorem 2) relies on the specific graphical structure of LLMs where the prompt $X$ is the root parent of *all* subsequent tokens $T_1, \dots, T_k$. You need to visualize this chain to see why intervening on $X$ updates the parents of every single downstream node.
  - **Quick check question:** Why does the fact that "each token generation uses the same family of conditional distributions" (Section 4) imply that the prompt $X$ is a parent to every token variable $T_i$?

## Architecture Onboarding

- **Component map:**
  - **Root Node ($X$):** The user prompt
  - **Mechanism ($P(T_i|\text{Context})$):** The LLM's forward pass (weights + sampling logic)
  - **Variables ($T_1 \dots T_k$):** The sequence of generated tokens
  - **Exogenous Noise ($U$):** *Absent* in the proposed NCM architecture; explicitly excluded to enforce the "idealized interpretation"

- **Critical path:**
  1.  **Input:** User provides factual $(x, y)$ and counterfactual prompt $x^*$
  2.  **Check:** Verify $x^* \neq x$
  3.  **Query:** Ignore $y$. Execute standard forward pass on $x^*$
  4.  **Output:** Return the distribution (or samples) of $Y^*$

- **Design tradeoffs:**
  - **NCM (Simple Semantics):** Does not require source code; treats LLM as black box. **Tradeoff:** Does *not* enforce counterfactual stability (outputs may differ wildly from original)
  - **DCM (Gumbel-based):** Enforces stability. **Tradeoff:** Requires source code access; restricts use cases (bad for explanations requiring distance)

- **Failure signatures:**
  - **Instability:** If a user expects the counterfactual $y^*$ to be syntactically very similar to $y$ (e.g., changing one word), the Simple Semantics may return a completely different sentence. This is a feature, not a bug, for explanation, but a failure for data augmentation stability
  - **Implementation Leaking:** If the system accidentally relies on shared PRNG state between the factual and counterfactual runs without fixing the seed (deterministic approach), you get neither pure randomness nor perfect stability, but implementation-dependent noise

- **First 3 experiments:**
  1.  **Sanity Check (Deterministic Mode):** Set temperature to 0. Verify that the NCM method (re-running prompt) yields the exact same result as the deterministic function $Y=f(X)$, confirming Corollary 1
  2.  **Stability Comparison:** Run the NCM method vs. the Gumbel-based method (if source is available) on a dataset of factuals. Measure the average distance (e.g., Edit Distance) between factual $y$ and counterfactual $y^*$. Confirm that NCM yields higher variance/distance
  3.  **Explanation Generation:** Use the NCM method to find prompts $x^*$ close to $x$ that generate outputs $y^*$ distant from $y$. Test if these $(x^*, y^*)$ pairs serve as valid counterfactual explanations for model behavior (i.e., do they isolate causal factors?)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the nondeterministic causal model framework be extended to generate counterfactuals involving the LLM's internal parameters rather than just the textual input?
- **Basis in paper:** [explicit] The author states, "I intend to explore such counterfactuals [involving parameters] in future work."
- **Why unresolved:** The current formalism restricts interventions to the root variable (the prompt $X$), leaving the methodology for manipulating internal model parameters undefined.
- **Evidence:** A formal extension of the model that handles parameter interventions and a practical demonstration of generating parameter-based counterfactuals.

### Open Question 2
- **Question:** What specific sampling implementations can generate counterfactuals satisfying properties like significant distance (as opposed to closeness) for tasks such as counterfactual explanations?
- **Basis in paper:** [explicit] The author suggests "exploring alternative implementations as well as different properties than closeness" and specifically proposes looking for "counterfactuals that are at least $\epsilon$ removed."
- **Why unresolved:** The paper outlines the theoretical possibility of biasing the simple semantics but only details the implementation for "counterfactual stability" (closeness) via the Gumbel-max trick.
- **Evidence:** A defined sampling algorithm that enforces a distance metric over the output space and empirical validation showing its utility in finding valid counterfactual explanations.

### Open Question 3
- **Question:** Can the ability to generate counterfactuals be explicitly incorporated into the architecture of reasoning models to improve explanation faithfulness?
- **Basis in paper:** [explicit] The text notes, "As a final step, the ability to generate counterfactuals could then be incorporated explicitly into reasoning models."
- **Why unresolved:** The paper establishes the theoretical semantics for external queries but does not explore the internal integration of this capability into the model's own reasoning chain.
- **Evidence:** A modified LLM architecture that utilizes counterfactual generation as a subroutine in reasoning steps, benchmarked on faithfulness metrics.

## Limitations
- The theoretical framework applies strictly to probabilistic LLMs where sampling is a primitive probability distribution, not deterministic LLMs
- The claim that "counterfactuals collapse to observations" assumes an idealized interpretation that may not match all user expectations for counterfactual stability
- The approach does not extend to scenarios where the specific random seed state matters for counterfactual interpretation

## Confidence
- **High Confidence:** The core claim that NCMs satisfy simple semantics (Theorem 2) is mathematically rigorous and follows directly from Beckers' definitions
- **Medium Confidence:** The assertion that NCM-based counterfactuals are sufficient for explanation tasks assumes users prioritize causal distance over stability
- **Medium Confidence:** The practical claim that no source code access is required relies on the assumption that users accept the "idealized interpretation" over the "literal interpretation" requiring seed information

## Next Checks
1. **Formal Verification:** Implement the NCM framework and test on a deterministic LLM (temperature=0). Confirm that counterfactuals produce deterministic outputs matching Corollary 1
2. **Stability Analysis:** Compare NCM counterfactuals vs. Gumbel-based counterfactuals (when source available) on a benchmark dataset. Quantify the trade-off between stability and causal distance
3. **User Study:** Test whether NCM-generated counterfactuals effectively isolate causal features for explanation tasks compared to baseline methods. Measure if users can identify causal factors from the counterfactual examples