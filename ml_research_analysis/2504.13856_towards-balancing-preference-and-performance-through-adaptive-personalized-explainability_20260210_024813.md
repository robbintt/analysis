---
ver: rpa2
title: Towards Balancing Preference and Performance through Adaptive Personalized
  Explainability
arxiv_id: '2504.13856'
source_url: https://arxiv.org/abs/2504.13856
tags:
- explanations
- personalization
- participants
- agent
- participant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents two user studies in a simulated autonomous vehicle
  domain to investigate population-level preferences for explainable AI (xAI) and
  personalization strategies for robot explanations. The studies compare three xAI
  modalities (language explanations, feature-importance maps, and decision trees)
  and find significant differences in both preference (p < 0.01) and performance (p
  < 0.05).
---

# Towards Balancing Preference and Performance through Adaptive Personalized Explainability

## Quick Facts
- arXiv ID: 2504.13856
- Source URL: https://arxiv.org/abs/2504.13856
- Reference count: 40
- Key outcome: Adaptive personalization strategy balancing user preference and performance needs yields significant performance gains (p < 0.05) compared to random or preference-maximization approaches.

## Executive Summary
This work presents two user studies in a simulated autonomous vehicle domain investigating population-level preferences for explainable AI (xAI) and personalization strategies for robot explanations. The studies compare three xAI modalities (language explanations, feature-importance maps, and decision trees) and find significant differences in both preference (p < 0.01) and performance (p < 0.05). An adaptive personalization strategy is developed that balances user preference against task performance needs, yielding significant performance gains (p < 0.05) compared to random or preference-maximization approaches. The adaptive approach also results in greater perceptions of preference accommodation compared to non-personalized agents (p < 0.05).

## Method Summary
The approach uses a neural network to predict user behavior in a simulated autonomous vehicle environment, incorporating personal and contextual embeddings to capture individual user patterns. The system maintains separate distributions for user preference (based on binary feedback) and performance (based on mistakes), then computes a balanced distribution using a trade-off parameter λ derived from the prediction network's confidence. The agent samples explanation modalities from this balanced distribution, adapting to whether the user is likely to make optimal or suboptimal choices. The method requires calibration tasks to populate initial distributions before personalization becomes effective.

## Key Results
- Language explanations are significantly preferred by users (p < 0.01) and lead to fewer mistakes than feature-importance maps
- Adaptive personalization strategy yields significant performance gains (p < 0.05) compared to random or preference-maximization approaches
- Adaptive approach results in greater perceptions of preference accommodation compared to non-personalized agents (p < 0.05)

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Modality Arbitration
The system computes a "balanced distribution" ($d_B$) over explanation modalities using a trade-off parameter $\lambda$ derived from a predictive neural network. If the network predicts the user will choose the optimal path, $\lambda$ increases, biasing selection toward preferred modalities ($d_P$). If the network predicts a mistake, $\lambda$ decreases, forcing selection toward performance-maximizing modalities ($d_T$), regardless of user preference. This assumes users prioritize successful task completion over modality preference when errors are imminent.

### Mechanism 2: Personalized Behavior Modeling
A neural network consumes environment state (position, goal) concatenated with unique "personal embedding" (representing the user) and "contextual embedding" (representing the task). This allows the model to learn non-obvious correlations between a specific user's behavior and environment context, assuming user decision-making patterns are consistent enough to be captured in low-dimensional embedding space with limited interaction data.

### Mechanism 3: Divergence of Preference and Performance
The system maintains two distinct distributions: $d_P$ (preference based on binary feedback) and $d_T$ (performance based on mistakes). By treating these as separate objectives rather than a single proxy, the system identifies cases where user preference degrades task performance. This assumes preference and performance are not positively correlated and that users may prefer easier-to-consume explanations even when harder-to-parse ones reduce error rates in specific contexts.

## Foundational Learning

- **Concept: Inappropriate Compliance**
  - Why needed here: This is the primary failure mode the architecture attempts to solve - when a user accepts an agent's incorrect suggestion because the explanation was persuasive but flawed.
  - Quick check question: If a user accepts a wrong turn because the agent said "the weather is nice," is this a performance failure or a preference failure?

- **Concept: Concept-based Explanations (Red Herrings)**
  - Why needed here: The study simulates "explanatory debugging" by embedding specific error signals (e.g., mentioning "weather" or "radio") into explanations. Understanding this is required to interpret the Performance Distribution ($d_T$).
  - Quick check question: How does the system detect that a user has failed to identify a "red herring" feature in an explanation?

- **Concept: Embedding Spaces**
  - Why needed here: The adaptive mechanism relies on "personal embeddings" to predict behavior. You must understand that a user is represented as a vector that shifts their probability of predicted actions.
  - Quick check question: Does the network retrain the main weights during deployment, or only the personal embeddings?

## Architecture Onboarding

- **Component map:** Input Layer (Environment State + User ID + Task ID) -> Embedding Layer (maps IDs to vectors, concatenates with State) -> Predictive Network (outputs logits for user direction choice) -> Lambda Calculator (converts prediction confidence into $\lambda$) -> Distribution Manager (maintains $d_P$ and $d_T$) -> Arbitrator (computes $d_B = \lambda d_P + (1-\lambda) d_T$ and samples modality)

- **Critical path:** The flow from User Feedback -> Distribution Update -> Next Prediction. If feedback isn't recorded, $d_P$ stagnates. If mistakes aren't tracked, $d_T$ cannot guide the user.

- **Design tradeoffs:**
  - Preference vs. Compliance: The system may annoy users by forcing "performance" modalities when they are likely to err, trading user satisfaction for correctness.
  - Cold Start: The system requires "calibration tasks" to populate $d_P$ and $d_T$ before personalization works.
  - Static vs. Dynamic: Explanations themselves are static (pre-generated templates), but the selection is dynamic.

- **Failure signatures:**
  - Oscillation: User repeatedly makes mistakes -> System forces strict modality -> User gives negative feedback -> System relaxes -> User makes mistakes
  - Feedback Spam: If a user indiscriminately marks all modalities as "negative," $d_P$ becomes uniform noise
  - Embedding Drift: If a user changes strategy mid-task, the frozen main network may fail to adapt quickly enough via embeddings alone

- **First 3 experiments:**
  1. Ablate λ: Set λ=1 (pure preference) and λ=0 (pure performance) to replicate baseline conditions and verify trade-off logic
  2. Noise Injection: Simulate users who lie in their feedback (binary ratings) to test robustness of $d_P$ calculation
  3. Domain Transfer: Test predictive network (trained on AV data) on different grid-world task to see if "personal embeddings" capture user traits or domain-specific quirks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adapting the content of explanations (intra-modality personalization) yield greater performance gains than selecting the modality itself?
- Basis in paper: Discussion states that "personalization in this work was confined to selecting xAI modalities that match a participant's preferences, but did not extend to adapting the explanations themselves."
- Why unresolved: The study optimized for the mode of delivery (e.g., tree vs. language) but kept internal explanation logic static, leaving potential of dynamic content adaptation untested.
- What evidence would resolve it: A user study comparing current modality-selection agent against an agent that dynamically restructures explanation content based on user errors.

### Open Question 2
- Question: How effective is adaptive personalization when relying on implicit behavioral cues rather than explicit ground-truth data?
- Basis in paper: The Limitations section notes the agent "assumed access to ground truth information... or explicit preference feedback, which may be challenging to obtain the real-world."
- Why unresolved: The adaptive algorithm relies on knowing the "optimal turn" to calculate the trade-off parameter λ. Real-world deployment requires determining if noisy or implicit signals (e.g., hesitation, eye-tracking) can substitute for ground truth.
- What evidence would resolve it: An experiment where the adaptive agent operates using only observed user behaviors (e.g., time-to-decision) without access to oracle's ground-truth state.

### Open Question 3
- Question: Do the observed misalignments between user preference and performance persist in expert populations?
- Basis in paper: The Limitations section highlights that studies were "conducted primarily with university students... rather than... a broader population," and Related Work notes that "aligning explanations with a user's expertise can lead to higher perceived utility."
- Why unresolved: The "gold standard" of language explanations was identified using untrained users; experts may exhibit different mental models, potentially aligning their preferences more closely with high-performance modalities like decision trees.
- What evidence would resolve it: A replication of the population study methodology with domain experts (e.g., professional drivers) to evaluate if preference-performance gap narrows.

## Limitations
- Network architecture details (layer sizes, activation functions) are unspecified, making exact reproduction challenging
- Training procedure specifics (learning rates, update frequency, loss functions) are not provided
- Cold-start behavior for new users is not fully characterized, though system requires calibration tasks
- Embedding dimensionality and update mechanisms during deployment are not specified

## Confidence

- **High confidence** in core mechanism (adaptive modality selection based on preference-performance trade-off) due to strong experimental results (p < 0.01 for preference, p < 0.05 for performance gains)
- **Medium confidence** in neural network prediction component due to underspecified architecture
- **Medium confidence** in personalization embedding approach given limited interaction data in study design
- **Low confidence** in long-term stability and scalability beyond controlled 7x7 grid environment

## Next Checks

1. **Ablation study**: Implement and test pure preference (λ=1) and pure performance (λ=0) baselines to verify trade-off mechanism
2. **Cold-start simulation**: Test system's behavior with new users having minimal interaction history to identify initialization issues
3. **Cross-domain transfer**: Evaluate predictive network's performance on different grid-world task to determine if embeddings capture user traits versus domain-specific patterns