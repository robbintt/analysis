---
ver: rpa2
title: 'CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models'
arxiv_id: '2502.16614'
source_url: https://arxiv.org/abs/2502.16614
tags:
- qwen2
- code
- evaluation
- uni00000048
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CodeCriticBench is a comprehensive benchmark for evaluating the
  critique capabilities of large language models on code tasks. It includes two main
  tasks: code generation and code question answering, with varying difficulty levels.'
---

# CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2502.16614
- Source URL: https://arxiv.org/abs/2502.16614
- Reference count: 40
- 38 LLMs evaluated; o1-like models achieve milestone results on hard critique tasks

## Executive Summary
CodeCriticBench is a comprehensive benchmark for evaluating large language models' code critique capabilities across two tasks: code generation and code question answering. The benchmark includes 4,300 samples with varying difficulty levels and employs both basic (binary correctness) and advanced (multi-dimensional scoring) evaluation protocols. Experiments demonstrate that larger models and o1-like architectures show superior performance, particularly on harder tasks. The advanced evaluation protocol aligns more closely with human judgments than basic accuracy metrics, providing richer assessment of model capabilities.

## Method Summary
The benchmark evaluates LLMs on code critique using two protocols: Basic (binary correct/error prediction with reasoning) and Advanced (10-dimensional scoring using fine-grained checklists). It covers 4,300 samples across Code Generation (3,200) and Code QA (1,100) tasks, with difficulty tiers based on model success rates. Ground-truth labels are generated using LLM prompting calibrated with human annotations on 20% of samples. Evaluation uses Accuracy (ACC) for basic and Mean Squared Error (MSE) for advanced protocols.

## Key Results
- Larger models exhibit systematically stronger code critique performance following scaling laws
- O1-like reasoning models achieve milestone results on hard critique tasks
- Advanced evaluation with fine-grained checklists aligns more closely with human judgments than basic correctness classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models exhibit systematically stronger code critique performance, following a scaling law
- Mechanism: Increased parameter count and training data enable better pattern recognition and semantic understanding required for nuanced critique
- Core assumption: Task distribution and metrics accurately capture core critique capabilities
- Evidence: Experiments on 38 LLMs show larger models perform better; scaling law visualization shows ACC increases with parameter count
- Break condition: Performance plateaus or degrades beyond certain scale, or metrics fail to correlate with practical utility

### Mechanism 2
- Claim: O1-like reasoning models achieve superior performance on hard critique tasks
- Mechanism: Chain-of-thought architectures enable decomposition, verification, and robust handling of ambiguity
- Core assumption: Hardness labels meaningfully represent cognitive complexity
- Evidence: O1-like models maintain >50% accuracy on hard data where others drop to ~30%
- Break condition: Advantages disappear on tasks requiring precise syntactic matching or exhaustive test-case validation

### Mechanism 3
- Claim: Advanced evaluation with fine-grained checklists aligns more closely with human judgments
- Mechanism: Detailed criteria provide richer context for assessing aspects like efficiency and maintainability
- Core assumption: Human evaluation on 400-sample subset is representative and reliable
- Evidence: Model rankings by advanced critique match human rankings more closely than basic critique rankings
- Break condition: Fine-grained checklists introduce noise or inconsistency, or overburden smaller models

## Foundational Learning

- Concept: **Critique Capacity in LLMs**
  - Why needed: Understanding what it means for a model to "critique" code is foundational to interpreting the benchmark's goals
  - Quick check: Can you distinguish between a model generating code vs. evaluating whether existing code meets specifications?

- Concept: **Code Generation vs. Code QA Tasks**
  - Why needed: The benchmark covers two distinct task types, each requiring different critique skills
  - Quick check: How might critique requirements differ between checking a sorting algorithm vs. evaluating an explanation of memory management?

- Concept: **Evaluation Metrics: ACC vs. MSE**
  - Why needed: Basic uses accuracy; advanced uses mean squared error across dimensions
  - Quick check: Why would MSE be more appropriate than ACC for assessing performance across fine-grained evaluation dimensions?

## Architecture Onboarding

- **Component map**: Dataset (4,300 samples) -> Basic/Advanced Evaluation Protocols -> ACC/MSE Computation -> Performance Ranking
- **Critical path**: Ingest sample -> Apply evaluation protocol -> Compare against ground truth -> Compute ACC/MSE
- **Design tradeoffs**: Task breadth vs. depth; Basic vs. Advanced evaluation speed vs. nuance; Difficulty calibration may penalize over-reasoning
- **Failure signatures**: Performance collapse on hard data; Error-type detection gaps (e.g., Performance Issues); Basic-advanced misalignment
- **First 3 experiments**: 1) Scaling law validation across model sizes; 2) Protocol alignment test vs. human judgments; 3) Error-type analysis on Debug subset

## Open Questions the Paper Calls Out

- **Repository-level extension**: Current evaluation is confined to single-file scenarios; future work plans to extend to repository-level critiques
- **Domain generalizability**: As CodeCriticBench is presently focused solely on code, authors intend to broaden scope to include additional domains
- **Calibration effectiveness**: Whether linear regression calibration on 20% human-annotated subset effectively eliminates systematic bias in final evaluation labels

## Limitations

- Evaluation protocols rely on LLM-generated fine-grained checklists that may inherit biases
- Human evaluation subset (CodeCritic_400) is relatively small for establishing ground truth
- Difficulty calibration based on 12 SOTA models may not generalize to future architectures

## Confidence

- **High confidence**: Scaling law observations and general advantage of o1-like models on hard tasks
- **Medium confidence**: Alignment of advanced evaluation with human judgments requires further validation
- **Low confidence**: Claim that advanced evaluation provides "richer context" is primarily theoretical

## Next Checks

1. Cross-protocol consistency test: Compare model rankings from basic vs. advanced evaluations against independent human judgments
2. Checklist robustness analysis: Evaluate performance using human-curated vs. LLM-generated fine-grained checklists
3. Hard task generalization study: Test whether o1-like advantages persist on tasks requiring precise syntactic matching