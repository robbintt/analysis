---
ver: rpa2
title: 'OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced
  LLM Adaptation'
arxiv_id: '2512.19379'
source_url: https://arxiv.org/abs/2512.19379
tags:
- multimodal
- emotion
- sentiment
- auxiliary
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multimodal emotion recognition
  in Indonesian, a low-resource language with over 200 million speakers but lacking
  dedicated datasets. The authors introduce IndoMER, the first multimodal emotion
  recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers
  with aligned text, audio, and visual annotations across seven emotion categories.
---

# OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation

## Quick Facts
- **arXiv ID**: 2512.19379
- **Source URL**: https://arxiv.org/abs/2512.19379
- **Reference count**: 40
- **Primary result**: Proposed OmniMER framework achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming baseline by 7.6 and 22.1 absolute points respectively

## Executive Summary
This paper addresses the challenge of multimodal emotion recognition in Indonesian, a low-resource language with over 200 million speakers but lacking dedicated datasets. The authors introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with aligned text, audio, and visual annotations across seven emotion categories. To address challenges of cross-modal inconsistency and long-tailed class distributions, they propose OmniMER, a framework that adapts Qwen2.5-Omni through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These tasks strengthen unimodal emotional grounding before fusion, reducing reliance on spurious correlations.

## Method Summary
The OmniMER framework enhances Qwen2.5-Omni's multimodal capabilities through auxiliary perception tasks that strengthen unimodal emotional grounding before cross-modal fusion. Three modality-specific tasks are introduced: emotion keyword extraction from text, facial expression analysis from video, and prosody analysis from audio. These auxiliary tasks are trained jointly with the main emotion recognition objective, creating a multi-task learning setup that improves the model's ability to extract emotion-relevant features from each modality independently before multimodal fusion. The approach addresses cross-modal inconsistency by ensuring each modality develops robust emotional representations before being combined.

## Key Results
- OmniMER achieves 0.582 Macro-F1 on Indonesian sentiment classification, outperforming baseline by 7.6 absolute points
- The framework reaches 0.454 Macro-F1 on Indonesian emotion recognition, improving baseline by 22.1 absolute points
- Cross-lingual evaluation on Chinese CH-SIMS dataset demonstrates generalizability of the auxiliary task approach

## Why This Works (Mechanism)
The auxiliary perception tasks strengthen unimodal emotional grounding before cross-modal fusion, reducing reliance on spurious correlations that can arise when models learn to associate modalities without understanding the underlying emotions. By training separate tasks for emotion keyword extraction, facial expression analysis, and prosody analysis, the model develops robust emotional representations in each modality independently. This multi-task learning approach ensures that when modalities are fused, they contribute meaningful emotional information rather than just statistical correlations. The framework addresses the fundamental challenge of cross-modal inconsistency in multimodal emotion recognition by establishing strong unimodal emotional foundations.

## Foundational Learning
- **Multimodal fusion**: Combining information from text, audio, and visual modalities to recognize emotions; needed to capture the complete emotional context that single modalities miss
- **Low-resource language adaptation**: Adapting models to languages with limited training data; needed because Indonesian lacks large-scale emotion recognition datasets
- **Auxiliary task learning**: Using additional learning objectives to improve primary task performance; needed to strengthen unimodal feature extraction before fusion
- **Cross-modal inconsistency**: Mismatches between emotional cues across different modalities; needed to address the challenge of modalities providing conflicting emotional signals
- **Long-tailed class distribution**: Skewed distribution where some emotion categories have much more training data than others; needed to handle imbalanced emotion classes in real-world data

## Architecture Onboarding

**Component map**: Qwen2.5-Omni -> Text encoder with emotion keyword extraction -> Audio encoder with prosody analysis -> Video encoder with facial expression analysis -> Multimodal fusion -> Emotion recognition

**Critical path**: Text encoder -> Emotion keyword extraction auxiliary task -> Audio encoder -> Prosody analysis auxiliary task -> Video encoder -> Facial expression analysis auxiliary task -> Multimodal fusion -> Primary emotion recognition

**Design tradeoffs**: The framework prioritizes unimodal robustness over immediate fusion performance, potentially sacrificing some early-stage fusion capabilities for more reliable long-term multimodal understanding. The auxiliary tasks add computational overhead during training but improve generalization to cross-lingual scenarios.

**Failure signatures**: Poor performance on single-modality emotion recognition indicates auxiliary tasks are not strengthening unimodal grounding sufficiently. Large performance gaps between modalities suggest imbalanced auxiliary task training. Degradation on cross-lingual datasets would indicate limited generalizability of the unimodal emotional representations.

**First experiments**: 1) Evaluate baseline Qwen2.5-Omni performance without auxiliary tasks on IndoMER to establish improvement magnitude. 2) Test each auxiliary task individually to identify which contributes most to performance gains. 3) Conduct cross-modal consistency analysis to verify that auxiliary tasks reduce modality-specific emotional prediction errors.

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset size (1,944 video segments) is relatively small for training large language models, raising concerns about generalization to larger datasets
- The claim of being "the first" multimodal emotion recognition benchmark for Indonesian lacks systematic verification through comprehensive literature review
- The substantial 22.1 absolute point improvement over baseline lacks contextualization through comparison with other multimodal fusion methods or ablation studies

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Dataset novelty and contribution | Medium |
| OmniMER framework effectiveness | Medium |
| Cross-lingual generalizability | Low |

## Next Checks
1. Conduct ablation studies removing individual auxiliary tasks to quantify their specific contributions to performance gains
2. Evaluate the framework on additional low-resource languages to test the generalizability of the auxiliary task approach beyond Indonesian
3. Test scalability by training on a larger Indonesian emotion dataset (if available) or synthetic data augmentation to assess whether improvements hold at scale