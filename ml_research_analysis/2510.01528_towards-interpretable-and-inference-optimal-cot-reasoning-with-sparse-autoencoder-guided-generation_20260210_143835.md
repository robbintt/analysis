---
ver: rpa2
title: Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided
  Generation
arxiv_id: '2510.01528'
source_url: https://arxiv.org/abs/2510.01528
tags:
- arxiv
- reasoning
- sequences
- original
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method that uses sparse autoencoders
  (SAEs) and clustering to analyze and guide LLM reasoning in math tasks. The method
  first trains an SAE to produce sparse token representations, then applies k-means
  clustering to construct a graph where nodes represent token clusters and edges encode
  sequential transitions.
---

# Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation

## Quick Facts
- arXiv ID: 2510.01528
- Source URL: https://arxiv.org/abs/2510.01528
- Reference count: 12
- Key outcome: Introduces SAE-guided clustering and graph rewards to steer LLM reasoning in math tasks, showing that balancing exploitation and exploration improves reasoning quality.

## Executive Summary
This paper presents a novel method for improving chain-of-thought reasoning in large language models using sparse autoencoders (SAEs) and clustering. The approach trains an SAE to produce sparse token representations, applies k-means clustering to build a transition graph, and uses graph-based rewards to guide generation. By balancing exploitation of high-reward paths with exploration of diverse clusters, the method achieves interpretable and accurate mathematical reasoning on datasets like NuminaMath and GSM8K.

## Method Summary
The method trains a TopK SAE (n=8192 latent dimensions) on mixed NuminaMath and Ultrachat tokens to produce sparse representations. K-means clustering groups these into K clusters, forming a transition graph where edge weights count cluster-to-cluster transitions in NuminaMath. For any generated sequence, the reward R(p) = Σ w_{p_i, p_{i+1}} sums traversed edge weights. The approach is validated on three MiniCPM models, showing correct sequences have higher rewards, lower DTW distances, and optimal entropy patterns that balance exploitation and exploration.

## Key Results
- Correct reasoning sequences achieve significantly higher graph rewards than incorrect ones (Table 1 shows 415.97 vs 189.40 on NuminaMath vs GSM8K for MiniCPM-2B-128k)
- High-reward paths align with correct answers, validating the reward signal quality
- Balancing exploitation and exploration improves reasoning quality, with incorrect sequences showing higher entropy and structural deviation
- DTW and KL divergence metrics reveal trade-offs between accuracy and structural preservation across models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse autoencoder representations cluster semantically related reasoning tokens, enabling interpretable tracking of reasoning trajectories.
- Mechanism: The TopK SAE (n=8192 latent dimensions) maps token representations to sparse vectors where only k latents activate. K-means then groups tokens by cosine similarity in this sparse space. Because SAEs have been shown to decompose activations into interpretable features (Cunningham et al., 2023), clusters capture conceptual relationships rather than surface form.
- Core assumption: Semantic similarity in SAE latent space correlates with functional similarity in reasoning steps.
- Evidence anchors: [abstract] "Our approach first trains an SAE to generate sparse vector representations for training tokens, then applies k-means clustering to construct a graph where vertices represent token clusters."

### Mechanism 2
- Claim: Edge weights in the transition graph quantify how well a reasoning path adheres to patterns observed in correct mathematical solutions.
- Mechanism: After clustering, the graph G = (V, E) is constructed where edge weight w_i,j counts transitions between cluster i and j in the NuminaMath training corpus. The reward R(p) = Σ w_{p_i, p_{i+1}} sums edge weights along a generated sequence. High-reward paths follow frequently observed transitions; low-reward paths deviate into rare or unseen transitions.
- Core assumption: Transition frequencies in NuminaMath encode useful reasoning patterns that generalize to other math tasks (e.g., GSM8K).
- Evidence anchors: [abstract] "A graph-based reward model quantifies adherence to established reasoning paths, balancing exploitation (following high-weight edges) and exploration."

### Mechanism 3
- Claim: Balancing exploitation (high-reward paths) and exploration (diverse cluster coverage) improves reasoning quality more than pure exploitation alone.
- Mechanism: Exploitation maximizes cumulative edge weight; exploration increases entropy of cluster visitation and probes low-weight edges. The paper shows correct sequences have lower DTW distances to originals (better structural alignment) but incorrect sequences have higher entropy (more exploration). Optimal reasoning requires both adherence to proven paths and willingness to deviate when needed.
- Core assumption: Correct answers emerge from trajectories that balance following known patterns with exploring alternatives, not from pure greediness.
- Evidence anchors: [abstract] "Our findings indicate that balancing both exploitation and exploration is crucial for achieving high accuracy in mathematical reasoning tasks."

## Foundational Learning

- Concept: **Sparse Autoencoders with TopK activation**
  - Why needed here: The entire pipeline depends on SAE latent representations. Understanding how TopK enforces sparsity (keeping only k largest latents) and how reconstruction loss trains the encoder/decoder is essential.
  - Quick check question: Given a 1024-dim token embedding and n=8192 latents with k=64, what is the sparsity ratio of z?

- Concept: **K-means clustering on cosine distances**
  - Why needed here: Tokens are assigned to clusters by minimizing cosine distance to centroids. Understanding how cluster count K affects granularity—and how it interacts with SAE feature disentanglement—is critical.
  - Quick check question: If two mathematically distinct tokens (e.g., "multiply" and "divide") are assigned to the same cluster, what happens to graph semantics?

- Concept: **Dynamic Time Warping (DTW) for sequence alignment**
  - Why needed here: DTW measures structural similarity between generated and reference sequences. Correct sequences showed lower DTW (4.66–31.35) than incorrect (5.55–36.87), validating it as a quality signal.
  - Quick check question: Why might DTW be preferred over simple token-level accuracy for comparing reasoning chains?

## Architecture Onboarding

- Component map: SAE Training Module -> Sparse Representation Extractor -> K-means Clustering -> Graph Builder -> Reward Calculator -> Evaluation Suite
- Critical path: SAE training quality → cluster semantic coherence → graph edge weight reliability → reward signal usefulness. If SAE features are not interpretable or clusters mix unrelated concepts, downstream rewards are meaningless.
- Design tradeoffs:
  - Higher K (more clusters) → finer-grained transitions but sparser graph with less reliable edge weights
  - Training on NuminaMath only → strong math patterns but may not generalize to non-math reasoning
  - Pure exploitation → coherent but potentially brittle outputs; pure exploration → diverse but unfocused
- Failure signatures:
  - High reward but low accuracy: Graph encodes patterns that don't actually lead to correct answers (overfitting to training trajectories)
  - Low DTW but incorrect answers: Model memorizes structure without semantic understanding
  - High entropy in correct answers: Possible data leakage or trivial problems solved by multiple paths
- First 3 experiments:
  1. **SAE reconstruction sanity check**: Verify reconstruction loss is low and TopK latents are interpretable (inspect top activating tokens per latent) before clustering.
  2. **Cluster semantic audit**: Manually inspect tokens in 10–20 clusters to confirm they group meaningfully (e.g., arithmetic operations, number formats, reasoning connectors).
  3. **Reward-to-accuracy correlation**: On held-out GSM8K problems, compute R(p) for correct vs. incorrect generations; confirm correct paths have significantly higher mean reward (as Table 1 suggests).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the reward function be improved to better capture the trade-off between exploitation and exploration in reasoning?
- Basis in paper: [explicit] The authors state: "The most immediate improvement is likely the design of a better reward function. We imagine that metrics that combine rewards calculated from the graph, cosine distance between consecutive tokens, and other multi-level structural and semantic evaluations can help us better understand and refine our proposed tradeoff."
- Why unresolved: The current simple edge-weight reward does not integrate multiple structural and semantic factors, limiting its ability to balance exploitation and exploration effectively.
- What evidence would resolve it: Experiments with composite reward functions combining graph weights, cosine similarity, and semantic metrics showing improved accuracy while maintaining structural alignment.

### Open Question 2
- Question: Does the SAE-guided approach scale effectively to larger models beyond 2B parameters?
- Basis in paper: [inferred] The method was validated only on three MiniCPM models (1B and 2B parameters), leaving scalability to larger models unexplored despite discussion of industry-scale LLMs like o1.
- Why unresolved: SAE training and clustering may face computational or representational challenges at scale, and graph coverage may degrade with more diverse reasoning patterns in larger models.
- What evidence would resolve it: Successful application to models with 7B+ parameters, showing similar reward-quality correlations and manageable computational overhead.

### Open Question 3
- Question: What is the relationship between distributional patterns and generation quality for early detection of incorrect reasoning?
- Basis in paper: [explicit] The authors note: "Particularly, investigating the relationship between distributional patterns and generation quality could lead to better early detection mechanisms for incorrect generations."
- Why unresolved: While incorrect generations show higher peak densities and narrower distributions, the causal mechanism and detection thresholds remain unclear.
- What evidence would resolve it: A validated early-detection method using distributional metrics (KL divergence, entropy) that can flag incorrect reasoning mid-generation with high precision.

## Limitations

- The paper does not validate that SAE clusters group tokens by mathematical function rather than surface form, leaving interpretability claims weakly supported.
- Reward formulation assumes NuminaMath transition frequencies generalize to other datasets, but cross-dataset generalization is not demonstrated.
- The paper defines the reward function but does not specify how it actually guides generation, creating a gap between theory and implementation.

## Confidence

**High Confidence**: Core technical claims about SAE training (TopK activation, reconstruction loss) and clustering methodology are well-specified and reproducible.

**Medium Confidence**: Claims about exploitation-exploration trade-offs improving reasoning quality are supported by metric patterns but lack causal evidence.

**Low Confidence**: Claim that SAE-based clustering provides interpretable guidance for reasoning is weakly supported without direct validation of cluster semantics.

## Next Checks

1. **Cluster Interpretability Audit**: Manually examine 20-30 clusters to verify that tokens group by mathematical function (e.g., arithmetic operations, number formats, reasoning connectors) rather than arbitrary co-occurrence patterns.

2. **Cross-Dataset Reward Transfer**: Compute correlation between graph rewards and accuracy on held-out GSM8K problems where the graph was trained on NuminaMath to validate generalization.

3. **Reward-Guided Decoding Implementation**: Implement and test at least two different ways of using the reward signal during generation (e.g., best-of-n selection vs. reward-weighted sampling) to determine robustness across integration methods.