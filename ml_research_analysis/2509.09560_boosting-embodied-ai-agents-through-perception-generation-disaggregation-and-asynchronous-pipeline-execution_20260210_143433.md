---
ver: rpa2
title: Boosting Embodied AI Agents through Perception-Generation Disaggregation and
  Asynchronous Pipeline Execution
arxiv_id: '2509.09560'
source_url: https://arxiv.org/abs/2509.09560
tags:
- generation
- perception
- throughput
- context
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Embodied AI systems require high-frequency perception and generation,
  but current sequential computation patterns fail to meet real-world demands. The
  authors present Auras, a co-designed inference framework that disaggregates perception
  and generation modules and enables asynchronous pipeline parallelism.
---

# Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution

## Quick Facts
- **arXiv ID**: 2509.09560
- **Source URL**: https://arxiv.org/abs/2509.09560
- **Reference count**: 40
- **Primary result**: Throughput improvement of 2.54× on average while maintaining 102.7% of original accuracy for embodied AI agents.

## Executive Summary
Embodied AI systems require high-frequency perception and generation, but current sequential computation patterns fail to meet real-world demands. The authors present Auras, a co-designed inference framework that disaggregates perception and generation modules and enables asynchronous pipeline parallelism. Auras establishes a public context buffer to ensure generation always uses the latest perception data, preventing staleness and maintaining accuracy. The asynchronous pipeline executor provides controlled parallelism for both perception and generation, maximizing throughput without sacrificing accuracy.

## Method Summary
Auras implements a three-part approach: (1) disaggregating perception and generation modules via a shared public context buffer, (2) asynchronous pipeline executor with controlled parallelism, and (3) computation merging for auto-regressive models. The framework uses hierarchical tuning to determine optimal pipeline parallelism degrees and skewness parameters. CUDA graphs and streams are employed for efficient execution, with double-buffering to avoid lock overhead. The system targets embodied agents with "thinking" frequencies of 3-10Hz and aims to significantly improve throughput while maintaining or exceeding original accuracy.

## Key Results
- Throughput improvement of 2.54× on average across tested models
- Accuracy maintained at 102.7% of original accuracy on average
- Outperforms sequential, decoupled, and parallel execution modes in both throughput and accuracy
- Effectively scales to different model sizes and iteration steps

## Why This Works (Mechanism)

### Mechanism 1: Perception-Generation Disaggregation via Public Context Buffer
- Claim: Separating perception and generation modules with a shared context buffer allows generation to always use the latest perception data, mitigating staleness under parallelism.
- Mechanism: Perception writes to a public context buffer; generation reads from it asynchronously. For auto-regressive models, the context includes vision/language embeddings and generated action tokens. For diffusion models, it's the conditioning hidden state.
- Core assumption: The generation phase can tolerate a small offset (e.g., one frame) in context without significant accuracy loss, and perception outputs can be consumed by any concurrent generation request.

### Mechanism 2: Asynchronous Pipeline Parallelism with Controlled Parallelism
- Claim: Structured pipeline parallelism (perception and generation each pipelined) yields higher and more stable throughput than uncontrolled CUDA multi-stream.
- Mechanism: The executor organizes computation into frames with pp_perception and pp_generation stages, using CUDA graphs and streams. Double-buffering avoids lock overhead. Grid search + skewness fine-tuning balances throughput/accuracy.
- Core assumption: GPU compute can be evenly partitioned across pipeline stages, and interference between stages is low when structured.

### Mechanism 3: Computation Merging within Frames for Auto-Regressive Models
- Claim: Merging multiple concurrent decode iterations into a single larger prefill within a frame reduces redundant computation and boosts throughput for transformer-based auto-regressive policies.
- Mechanism: Due to causal masking, hidden state for token i depends only on tokens ≤i. Auras merges Generate(x≤l+i) and Generate(x≤l+j) into a single prefill when i<j, updating the public context.
- Core assumption: Causal masking holds; generation steps can be reordered/merged without altering final outputs.

## Foundational Learning

- **Concept**: Pipeline Parallelism vs. Data Parallelism
  - Why needed here: Auras relies on pipeline parallelism (splitting model/stages across time) not data parallelism (replicas), to fit on a single GPU.
  - Quick check question: Can you explain why pipeline parallelism may have lower memory overhead than data parallelism on a single device?

- **Concept**: Context Staleness in Embodied Agents
  - Why needed here: The paper's core problem is that parallel execution causes generation to act on stale perception data.
  - Quick check question: If a robot's perception runs at 30Hz but generation takes 150ms, how many frames of perception might be skipped under naive parallelism?

- **Concept**: CUDA Graphs and Streams
  - Why needed here: Auras uses CUDA graphs to capture and reuse kernel launches for efficiency within frames.
  - Quick check question: What is the tradeoff between using CUDA graphs (capture overhead, less flexibility) vs. dynamic kernel launches?

## Architecture Onboarding

- **Component map**: Perception Module -> Public Context Buffer -> Generation Module -> Actions Output
- **Critical path**: Perception writes to context buffer → generation reads latest context → actions output at frame rate. Tuning (pp_perception, pp_generation, skewness) determines throughput/accuracy.
- **Design tradeoffs**:
  - Higher pipeline degree increases throughput but may reduce accuracy if context staleness grows
  - Skewness toward later stages uses fresher data, improving accuracy but potentially lowering throughput
  - Double-buffering adds memory but avoids lock overhead
- **Failure signatures**:
  - Accuracy drops sharply with high pipeline degree if context is not refreshed frequently
  - OOM when pp_perception/pp_generation too high or model large
  - Throughput plateaus or drops due to stage imbalance
- **First 3 experiments**:
  1. Replicate throughput vs. accuracy tradeoff with pipeline degree for DP-CNN and OpenVLA (Figure 12) to validate the grid search
  2. Ablate public context vs. private context under parallel execution to measure staleness-induced accuracy loss (Figure 5)
  3. Measure startup overhead (pipeline fill time) and steady-state throughput to confirm overhead is negligible (Section 6.7)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the identification of the "public context" be automated for arbitrary model architectures without manual compute graph analysis?
- Basis: [inferred] Section 4.2.1 states that "Auras takes a thorough analysis" of the compute graph to identify shared variables, implying the process is currently manual or reliant on pre-defined model structures (Transformers/UNets).
- Why unresolved: The framework currently relies on specific domain knowledge to distinguish volatile context variables (like $X_A$ or $H_o$) for different algorithm classes.
- What evidence would resolve it: A generalized algorithm that automatically extracts the optimal public context buffer from a novel, non-standard architecture (e.g., state-space models) without human intervention.

### Open Question 2
- Question: Can the pipeline configuration (stage counts and skewness) be adapted dynamically at runtime to respond to changing environment dynamics?
- Basis: [inferred] Section 4.3.2 describes a "hierarchical tuning process" using offline grid search to determine static configurations (e.g., `pp_generation`, `skewness`).
- Why unresolved: The system selects a "sweet point" configuration before execution; if task complexity or environment speed changes, the static pipeline may become sub-optimal or suffer accuracy loss.
- What evidence would resolve it: A feedback loop mechanism that adjusts the partition skewness ($\alpha$) or pipeline depth in real-time to maintain the accuracy-throughput trade-off during varying workloads.

### Open Question 3
- Question: How does the framework perform in real-world physical deployments characterized by sensor jitter and non-deterministic I/O latencies?
- Basis: [inferred] Section 6 evaluates the system primarily in simulation environments (Simpler, VIMA, Push-T) where input streams are deterministic.
- Why unresolved: The "latest perception data" guarantee relies on the precise timing assumptions of the simulator; physical sensors introduce delays that might disrupt the asynchronous execution schedule or the "freshness" of the public context buffer.
- What evidence would resolve it: Benchmarks on physical robotic hardware demonstrating that the asynchronous executor maintains throughput and accuracy despite variable input frequencies and I/O bottlenecks.

## Limitations
- The effectiveness of the public context buffer relies on generation's tolerance for slight context staleness, which may not hold in highly dynamic or safety-critical environments
- CUDA graph implementation details are not disclosed, which is critical for reproducibility and performance optimization
- Hierarchical tuning process overhead is not quantified, potentially limiting real-world deployment when models or tasks change frequently

## Confidence

- **High Confidence**: The disaggregation mechanism via public context buffer and basic structure of the asynchronous pipeline executor are well-supported by experimental results
- **Medium Confidence**: The merging computation mechanism for auto-regressive models is plausible given causal masking property but lacks extensive validation
- **Low Confidence**: Scalability claims to different model sizes and iteration steps are based on limited experiments without comprehensive analysis

## Next Checks

1. **Context Staleness Ablation**: Conduct a controlled experiment varying the fetch_offset parameter (context staleness) for both auto-regressive and diffusion models. Measure the impact on accuracy and throughput to identify the maximum acceptable staleness for each model type.

2. **Real-World Deployment Simulation**: Implement Auras in a simulated real-world embodied AI scenario with high environmental dynamics (e.g., a mobile robot navigating a crowded space). Measure the system's performance and robustness under realistic perception-generation latency ratios and context staleness.

3. **Model-Agnostic Tuning Efficiency**: Test the hierarchical tuning process on a new, unseen model (e.g., a different-sized OpenVLA or a completely different architecture). Measure the tuning time and final throughput/accuracy to assess the generalizability and efficiency of the tuning procedure.