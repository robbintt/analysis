---
ver: rpa2
title: 'The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual
  LLMs using Indian Riddles'
arxiv_id: '2511.00960'
source_url: https://arxiv.org/abs/2511.00960
tags:
- uni00000044
- uni00000003
- shot
- uni00000048
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the reasoning and self-assessment abilities
  of large language models (LLMs) across seven Indian languages using culturally grounded
  riddles. A novel multilingual riddle dataset was created, combining traditional
  riddles with context-reconstructed variants, and five LLMs were tested under seven
  prompting strategies.
---

# The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles

## Quick Facts
- arXiv ID: 2511.00960
- Source URL: https://arxiv.org/abs/2511.00960
- Reference count: 0
- A multilingual riddle dataset for seven Indian languages was created to evaluate reasoning and self-assessment in LLMs, revealing a paradox where higher reasoning accuracy correlates with lower self-awareness.

## Executive Summary
This paper presents a novel evaluation framework for assessing reasoning and metacognitive abilities in large language models across seven Indian languages using culturally grounded riddles. The researchers created a multilingual riddle dataset combining traditional riddles with context-reconstructed variants, then tested five LLMs under seven prompting strategies. Gemini 2.5 Pro achieved the highest riddle-solving accuracy, though few-shot prompting provided only marginal improvements. The study's key finding reveals a paradox: models with superior reasoning performance demonstrated lower self-awareness, while lower-performing models were more capable of recognizing their own errors.

## Method Summary
The researchers developed a multilingual riddle dataset spanning seven Indian languages, including both traditional riddles and context-reconstructed variants. Five LLMs were evaluated using seven prompting strategies, including few-shot learning approaches. The self-assessment component asked models to rate their confidence in answers, creating a two-dimensional evaluation of both reasoning accuracy and metacognitive awareness. The experimental design specifically examined the relationship between a model's ability to solve riddles and its capacity to recognize when it had made errors.

## Key Results
- Gemini 2.5 Pro achieved the highest accuracy in riddle-solving across the multilingual dataset
- Few-shot prompting provided only marginal improvements to model performance
- Higher-performing models demonstrated lower self-awareness, showing overconfidence in their answers while lower-performing models were better at recognizing their own errors

## Why This Works (Mechanism)
The riddle evaluation framework works by leveraging culturally specific knowledge and linguistic patterns that require both literal and figurative reasoning. Indian riddles often involve wordplay, cultural references, and metaphorical thinking that challenge models to move beyond surface-level pattern matching. The self-assessment component reveals metacognitive capabilities by forcing models to evaluate their own reasoning processes, exposing the gap between computational accuracy and genuine understanding.

## Foundational Learning

- **Cultural grounding in NLP**: Why needed - to test models on culturally specific reasoning tasks; Quick check - compare performance on culturally adapted vs. generic prompts
- **Metacognitive evaluation**: Why needed - to measure self-awareness beyond task accuracy; Quick check - correlate confidence ratings with actual error rates
- **Multilingual benchmarking**: Why needed - to assess cross-linguistic generalization; Quick check - compare performance consistency across language families
- **Reasoning vs. pattern matching**: Why needed - to distinguish genuine understanding from statistical correlations; Quick check - analyze error patterns for systematic vs. random failures
- **Self-assessment calibration**: Why needed - to evaluate model's ability to recognize uncertainty; Quick check - measure calibration curves across difficulty levels
- **Few-shot learning effectiveness**: Why needed - to determine optimal prompting strategies; Quick check - compare performance gains across different shot counts

## Architecture Onboarding

Component map: Dataset creation -> Model evaluation -> Self-assessment analysis -> Performance calibration

Critical path: The evaluation pipeline flows from dataset preparation through model testing to metacognitive analysis, with self-assessment serving as the key differentiator from standard accuracy metrics.

Design tradeoffs: The study prioritizes cultural authenticity over dataset size, focusing on quality riddles that require genuine reasoning rather than easily solved patterns. The self-assessment component adds complexity but provides crucial insights into model confidence calibration.

Failure signatures: Models may show high accuracy but poor self-awareness, indicating overconfidence in incorrect answers. Language-specific failures may reveal limitations in cross-linguistic generalization. Few-shot prompting may show diminishing returns beyond certain shot counts.

First experiments:
1. Test baseline performance on standard reasoning benchmarks to establish comparative metrics
2. Evaluate model confidence calibration on a simpler task to validate self-assessment methodology
3. Conduct ablation studies removing cultural context to measure its impact on performance

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The riddle dataset covers only seven Indian languages, limiting generalizability to other linguistic contexts
- The self-assessment relies on binary confidence judgments, potentially oversimplifying complex metacognitive relationships
- The study focuses on a specific reasoning task type that may not translate to broader reasoning capabilities

## Confidence

| Claim | Confidence |
|-------|------------|
| Gemini 2.5 Pro performance superiority | High |
| Overconfidence in high-performing models | Medium |
| Cultural grounding importance | Medium |

## Next Checks

1. Apply the same riddle evaluation methodology to languages from different language families (e.g., European, African, or East Asian languages) to determine whether observed patterns hold across linguistic diversity.

2. Implement a more nuanced confidence calibration task with continuous confidence scales and multiple-choice options to better capture the relationship between performance and self-awareness.

3. Test the same models on multiple reasoning task types (e.g., logical puzzles, mathematical word problems, commonsense reasoning) to determine whether observed performance patterns are specific to riddles or generalize to broader reasoning capabilities.