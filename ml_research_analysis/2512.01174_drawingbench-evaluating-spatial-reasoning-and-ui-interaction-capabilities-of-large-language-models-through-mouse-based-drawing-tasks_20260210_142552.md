---
ver: rpa2
title: 'DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities
  of Large Language Models through Mouse-Based Drawing Tasks'
arxiv_id: '2512.01174'
source_url: https://arxiv.org/abs/2512.01174
tags:
- action
- moveto
- tasks
- spatial
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DrawingBench introduces a transparent evaluation framework for
  assessing the trustworthiness of agentic LLMs through spatial reasoning tasks requiring
  GUI action sequences. The benchmark evaluates four state-of-the-art LLMs (Claude-4
  Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests using 250 diverse
  drawing prompts spanning 20 categories and 4 difficulty levels.
---

# DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks

## Quick Facts
- arXiv ID: 2512.01174
- Source URL: https://arxiv.org/abs/2512.01174
- Authors: Hyunjun Kim; Sooyoung Ryu
- Reference count: 40
- Key outcome: 92.8% perfect scores on spatial reasoning tasks, with structured feedback driving significant improvements

## Executive Summary
DrawingBench introduces a transparent evaluation framework for assessing the trustworthiness of agentic LLMs through spatial reasoning tasks requiring GUI action sequences. The benchmark evaluates four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests using 250 diverse drawing prompts spanning 20 categories and 4 difficulty levels. Models achieved 92.8% perfect scores, with structured external feedback driving significant improvements (+3.2% average, +32.8% for complex scenes). The study reveals that specification clarity matters more than task complexity, and external oversight proves more effective than self-correction for guiding agent behavior.

## Method Summary
DrawingBench evaluates LLMs on spatial reasoning through drawing tasks requiring GUI action sequences (mouse movements, clicks, tool selections) generated from natural language prompts. The benchmark uses a two-turn protocol: initial attempt → structured evaluation → feedback → retry. Models interact with a 1000×700px HTML5 Canvas with 6 tools, 8 colors, and 3 brush sizes. Performance is measured across 8 objective criteria including required tools, colors, geometric coverage, and position/size constraints. The framework provides action-level auditability and captures both reasoning quality and execution reliability.

## Key Results
- Models achieved 92.8% perfect scores across 1,000 tests
- Structured external feedback drove +3.2% average improvement (+32.8% for complex scenes)
- Specification clarity proved more important than task complexity
- Text-only spatial reasoning successfully generated 15+ action sequences without visual perception

## Why This Works (Mechanism)

### Mechanism 1
- Claim: External rule-based feedback produces more reliable agent refinement than self-correction.
- Mechanism: Deterministic evaluation generates structured error signals (e.g., "required tool 'rectangle' was not used") that provide actionable, unambiguous guidance. Models apply this feedback to repair specific failures in Turn 2, reducing variance by 33% and achieving +3.2% average improvement.
- Core assumption: Agents can reliably map explicit error descriptions to corrective actions when the feedback is grounded in objective criteria.
- Evidence anchors: Abstract notes external oversight proved more effective than self-correction; Results show SYNTAX ERROR reduced 100%, COORDINATE ERROR reduced 75%, LOGIC ERROR reduced 67% after feedback.

### Mechanism 2
- Claim: Specification clarity determines performance more than intrinsic task complexity.
- Mechanism: Hard tasks (100% perfect rate) outperformed Medium tasks (92.8%) because Hard tasks averaged 4.2 explicit constraints with 100% position criteria versus Medium's 2.8 constraints and 62% position criteria. Deterministic requirements enable reliable execution even when spatial complexity is higher.
- Core assumption: Models have sufficient baseline spatial reasoning capability; failures stem from ambiguous specifications rather than capability limits.
- Evidence anchors: Abstract states specification clarity proved more important than task complexity; Results show Hard: 0.973 Turn 2 score vs Medium: 0.958.

### Mechanism 3
- Claim: Text-only spatial reasoning is viable for GUI action generation without visual perception.
- Mechanism: LLMs compute coordinates and plan action sequences (15+ actions average) using internal spatial representations derived from UI specification text. The canvas position (90,70 to 1090,770), tool coordinates, and color palette locations are provided as structured context.
- Core assumption: Text descriptions of UI layouts can substitute for visual perception when coordinate systems are explicitly specified.
- Evidence anchors: Abstract shows models achieved 92.8% perfect performance with structured external feedback; Discussion notes models performed coordinate calculations and executed complex 15+ action sequences.

## Foundational Learning

- Concept: GUI Action Sequence Generation
  - Why needed here: The benchmark evaluates LLMs on generating low-level mouse actions (moveTo, click, mouseDown, mouseUp) rather than high-level commands.
  - Quick check question: Can you explain why moveTo(35,45) followed by click() selects the pen tool?

- Concept: Rule-Based Evaluation Criteria
  - Why needed here: Understanding the 8 criteria (required_tools, required_colors, min_segments, coverage, position_constraint, size_constraint, syntax, coordinate_bounds) is essential for interpreting scores.
  - Quick check question: If a drawing scores 0.75 with "required tool 'rectangle' not used," which criterion failed?

- Concept: Multi-Turn Feedback Protocol
  - Why needed here: The two-turn process (initial attempt → evaluation → feedback → correction) enables measuring iterative improvement.
  - Quick check question: Why does the framework skip Turn 2 when Turn 1 score ≥ 0.9?

## Architecture Onboarding

- Component map: Prompt Dataset -> Drawing Application -> LLM Interface -> Evaluation Engine -> Feedback Generator
- Critical path: Prompt → LLM → JSON action sequence → Puppeteer execution → Canvas capture → Rule evaluation → Score + feedback → (Turn 2 if score < 0.9)
- Design tradeoffs:
  - Text-only vs. vision-based: Chose text-only to isolate spatial reasoning from visual perception; limits tasks to explicitly specifiable requirements
  - Two-turn limit vs. extended iteration: Two turns capture 95% of improvement; diminishing returns after Turn 2 (<1%)
  - Rule-based vs. learned evaluation: Deterministic scoring enables reproducibility but cannot assess aesthetic quality
- Failure signatures:
  - Tool state management errors (15%): Models forget to select tools before drawing
  - Coordinate precision errors (10%): Out-of-bounds or misaligned positions
  - Coverage insufficiency (8%): Geometrically correct but undersized drawings
  - Long-horizon planning failures: Very Hard tasks (8×8 checkerboard) show cascading errors
- First 3 experiments:
  1. Run the 5 Easy prompts from Appendix on all 4 models to establish baseline spatial reasoning capability and verify your evaluation pipeline reproduces reported scores.
  2. Test a single Medium task ("draw a house") with and without Turn 2 feedback to measure the feedback mechanism's direct effect.
  3. Compare a Hard task with explicit constraints ("4 squares in each corner") vs. an ambiguous Medium task ("draw a house") to validate the specification clarity hypothesis.

## Open Questions the Paper Calls Out

- Would vision-language models (VLMs) with screenshot-based visual feedback significantly outperform text-only LLMs on DrawingBench tasks? The current benchmark evaluates only text-only models; vision capabilities were explicitly excluded from testing.

- How does LLM performance on DrawingBench compare to human performance on identical tasks? No human subjects were tested; the 92.8% perfect rate has no reference point for assessing relative capability.

- Does strong performance on DrawingBench spatial reasoning tasks correlate with success on other agentic tasks such as web navigation or code generation? The study only evaluated drawing tasks in isolation; no cross-benchmark correlation analysis was conducted.

## Limitations
- Benchmark limited to explicitly specifiable spatial tasks, excluding visual perception and ambiguous requirements
- 250-prompt dataset may not represent full diversity of spatial reasoning scenarios
- Two-turn feedback protocol may not reflect real-world asynchronous or incomplete feedback conditions

## Confidence
- High Confidence: Claims about specification clarity's impact on performance are well-supported by controlled comparisons across difficulty levels with clear statistical backing
- Medium Confidence: External feedback superiority claim shows strong evidence but relies on specific rule-based feedback format used
- Medium Confidence: Text-only spatial reasoning viability is demonstrated within controlled parameters but hasn't been stress-tested against tasks requiring visual feedback

## Next Checks
1. Test the benchmark with a larger, more diverse dataset (e.g., 1,000+ prompts) to verify that performance patterns hold beyond the current sample size
2. Conduct ablation studies comparing rule-based feedback versus natural language feedback, and immediate versus delayed feedback timing
3. Implement a vision-augmented version of the benchmark using the same 250 prompts but with visual input available, then compare performance drops between text-only and vision-augmented conditions