---
ver: rpa2
title: 'Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning'
arxiv_id: '2602.02405'
source_url: https://arxiv.org/abs/2602.02405
tags:
- reasoning
- dail
- expert
- solutions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Distribution Aligned Imitation Learning\
  \ (DAIL), a method that improves large language models' reasoning by converting\
  \ expert solutions into in-distribution reasoning traces. The key challenge addressed\
  \ is that expert solutions are \"didactic\"\u2014they skip steps and omit reasoning\
  \ chains that are clear to humans but not to models."
---

# Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning

## Quick Facts
- arXiv ID: 2602.02405
- Source URL: https://arxiv.org/abs/2602.02405
- Authors: Ethan Mendes; Jungsoo Park; Alan Ritter
- Reference count: 40
- Primary result: DAIL improves LLM reasoning performance by 10-25% on math benchmarks using fewer than 1000 expert solutions

## Executive Summary
This paper addresses the challenge of learning from expert solutions that are "didactic" - skipping reasoning steps that are obvious to humans but not to models. DAIL (Distribution Aligned Imitation Learning) converts these expert solutions into in-distribution reasoning traces that models can learn from effectively. The method uses a mixed policy rollout where a student model generates reasoning traces with guidance from a privileged model conditioned on the expert solution, creating more detailed and model-friendly traces. A contrastive loss prevents the student from imitating harmful shortcuts that arise when generating traces with access to the answer.

## Method Summary
DAIL transforms expert solutions into learnable reasoning traces through a mixed policy rollout approach. The student model generates reasoning chains while being guided by a privileged model that has access to the expert solution. This creates detailed reasoning traces that capture the intermediate steps models need to learn. A contrastive loss component distinguishes between helpful reasoning traces and harmful ones that simply rationalize the answer. The method is particularly effective at converting didactic expert solutions into constructive learning examples that models can generalize from.

## Key Results
- Improves pass@k by 10-25% on mathematics reasoning benchmarks (AIME 2024/2025, Beyond AIME, IMO-AnswerBench)
- Achieves these improvements using fewer than 1000 expert solutions
- Enhances reasoning efficiency by 2x-4x fewer tokens compared to baseline approaches
- Successfully generalizes to out-of-domain tasks like GPQA-Diamond
- Outperforms reinforcement learning with verifiable rewards (RLVR) on hard problems

## Why This Works (Mechanism)
DAIL works by addressing the fundamental mismatch between how humans communicate solutions and how models learn. Expert solutions often skip steps and assume human-level context, making them poor learning examples for models. By using a mixed policy rollout with a privileged model, DAIL generates reasoning traces that include the intermediate steps models need to learn from. The contrastive loss prevents the model from learning to simply rationalize answers rather than actually reason through problems, ensuring that the generated traces represent genuine problem-solving rather than post-hoc justification.

## Foundational Learning
- **Imitation Learning**: Needed to learn from expert demonstrations; quick check: verify the model can reproduce expert solution patterns
- **Mixed Policy Rollout**: Combines student and privileged model reasoning; quick check: ensure the mixed traces contain both student-generated and expert-guided components
- **Contrastive Learning**: Distinguishes helpful from harmful reasoning traces; quick check: validate that the contrastive loss prevents rationalization behaviors
- **Distribution Alignment**: Ensures generated traces match training distribution; quick check: compare trace distributions between generated and expert solutions
- **Privilege Distillation**: Uses expert knowledge to guide student reasoning; quick check: measure how much privileged model guidance improves trace quality
- **Token Efficiency**: Reduces reasoning steps while maintaining accuracy; quick check: track token counts versus performance improvements

## Architecture Onboarding

**Component Map:**
Expert Solution -> Privileged Model -> Mixed Policy Rollout -> Student Model -> Contrastive Loss -> Filtered Reasoning Traces -> Training Data

**Critical Path:**
Expert Solution → Privileged Model → Mixed Policy Rollout → Student Model → Contrastive Loss → Filtered Traces

**Design Tradeoffs:**
- Guidance vs. independence: More privileged model guidance produces better traces but may reduce student autonomy
- Trace length vs. efficiency: Longer traces provide more learning opportunities but increase computational cost
- Contrastive loss strength: Stronger contrast prevents rationalization but may filter out valid reasoning paths

**Failure Signatures:**
- Student model over-relying on privileged guidance (low diversity in generated traces)
- Contrastive loss being too aggressive (filtering out valid reasoning chains)
- Mixed policy rollout producing incoherent traces (poor integration of student and privileged reasoning)
- Model learning to mimic answer format without understanding (rationalization behavior)

**3 First Experiments:**
1. Generate mixed policy rollout traces on a small set of expert solutions and analyze trace quality
2. Test contrastive loss effectiveness by comparing rationalization vs. genuine reasoning traces
3. Evaluate student model performance on simple reasoning tasks before and after DAIL training

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Relies heavily on the quality and representativeness of expert solutions
- Assumes the mixed policy rollout can effectively generate in-distribution reasoning traces
- Performance depends on the quality of the privileged model used for guidance
- May introduce biases through the contrastive loss if not carefully calibrated
- Limited validation across diverse problem domains beyond mathematics

## Confidence

**High Confidence:**
- 10-25% improvement in pass@k on mathematics benchmarks is well-supported by empirical data
- Success with fewer than 1000 expert solutions demonstrates efficiency
- 2x-4x token efficiency improvements are clearly demonstrated

**Medium Confidence:**
- Generalization to GPQA-Diamond is supported but could benefit from broader validation
- Reasoning efficiency claims are empirically supported but context-dependent

**Low Confidence:**
- Claims about outperforming RLVR on hard problems are based on limited comparisons
- Long-term generalization across diverse reasoning tasks needs more validation

## Next Checks
1. Test DAIL on non-mathematical reasoning tasks (logical puzzles, scientific problems) to assess generalizability
2. Analyze the impact of varying expert solution quality on DAIL performance to understand robustness
3. Conduct comprehensive comparisons between DAIL and RLVR across different problem difficulties and domains