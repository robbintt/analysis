---
ver: rpa2
title: 'With Privacy, Size Matters: On the Importance of Dataset Size in Differentially
  Private Text Rewriting'
arxiv_id: '2511.00487'
source_url: https://arxiv.org/abs/2511.00487
tags:
- dataset
- text
- privacy
- size
- rewriting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effect of dataset size on differentially
  private (DP) text rewriting mechanisms, an aspect previously overlooked in DP NLP
  research. The authors design utility and privacy experiments using five datasets
  (AG News, MNLI, Trustpilot, Yelp, and a large Twitter corpus), each split into five
  sizes (10%, 25%, 50%, 75%, 100%) for a total of 180 datasets.
---

# With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting

## Quick Facts
- arXiv ID: 2511.00487
- Source URL: https://arxiv.org/abs/2511.00487
- Authors: Stephen Meisenbacher; Florian Matthes
- Reference count: 15
- Primary result: Dataset size significantly impacts DP text rewriting effectiveness, with larger datasets yielding more favorable privacy-utility trade-offs

## Executive Summary
This paper investigates how dataset size affects differentially private text rewriting mechanisms, an aspect previously overlooked in DP NLP research. The authors conduct extensive experiments across five datasets (AG News, MNLI, Trustpilot, Yelp, and Twitter) at five different scales (10%, 25%, 50%, 75%, 100%), evaluating three DP mechanisms (1-DIFFRACTOR, DP-PROMPT, DP-BART) across various privacy budgets. Using a novel "relative gain" metric, they demonstrate that larger datasets generally yield better privacy-utility trade-offs in DP text rewriting, making privatization more practical at scale. The findings suggest that DP text rewriting can be effective at scale, as larger datasets reduce the typical utility penalty and improve indistinguishability from original texts.

## Method Summary
The study uses five datasets with utility and privacy task labels, each split into five sizes using seed=42. Three DP mechanisms are applied at different linguistic levels: 1-DIFFRACTOR (word-level), DP-PROMPT (token-level), and DP-BART (document-level), each with three epsilon values. For each combination, texts are privatized, then DEBERTA-V3-BASE models are trained on privatized data for both utility (downstream task) and privacy (adversarial classification) tasks. The relative gain (γ) metric quantifies the trade-off between privacy preservation and utility retention, calculated as γ = (Ur−MGu)/(Uo−MGu) − Pr/Po. Experiments are repeated across three runs with different shuffles, totaling 180 privatized datasets.

## Key Results
- Dataset size significantly impacts DP text rewriting effectiveness, with larger datasets yielding more favorable privacy-utility trade-offs
- Document-level DP-BART shows the largest absolute improvements as dataset size increases, despite starting from lower baselines
- The relative gain metric successfully quantifies the privacy-utility trade-off, with higher values indicating better performance
- Larger datasets reduce the typical utility penalty of DP text rewriting while improving indistinguishability from original texts

## Why This Works (Mechanism)

### Mechanism 1: Relative Gain Metric
- Claim: A single metric (γ) can quantify the trade-off between privacy preservation and utility retention, enabling apples-to-apples comparison across DP mechanisms and dataset sizes.
- Mechanism: The relative gain (γ) is defined as γ = (Ur−MGu)/(Uo−MGu) − Pr/Po, where U = utility, P = privacy, r = privatized, o = original, and MGu = majority-class guessing baseline for utility. Higher γ indicates better trade-offs.
- Core assumption: Privacy and utility can be meaningfully reduced to classification F1 scores, and the difference from baseline utility (MGu) appropriately normalizes for task difficulty.
- Evidence anchors:
  - [abstract] "We focus on quantifying the effect of increasing dataset size on the privacy-utility trade-off."
  - [section 3.3] "we also calculate the relative gain (γ), which represents the trade-off between the paired utility and privacy tasks... higher scores are better."
  - [corpus] No direct corpus evidence for this specific metric formulation; related work on noise impact in DP text rewriting exists (arXiv:2501.19022) but does not validate this metric.
- Break condition: If utility or privacy tasks have highly skewed label distributions not captured by MGu, or if adversarial privacy classification is not a valid proxy for true privacy leakage.

### Mechanism 2: Scale-Dependent Privacy-Utility Trade-off
- Claim: Increasing dataset size improves the privacy-utility trade-off in local DP text rewriting, making privatization more practical at scale.
- Mechanism: As dataset size grows, downstream models trained on privatized text can better generalize despite noise injection, while simultaneously achieving better "indistinguishability in a crowd" for privacy protection.
- Core assumption: The benefits observed at tested scales (up to ~1M texts) will continue or plateau predictably at even larger scales, and local DP remains appropriate for larger data processors.
- Evidence anchors:
  - [abstract] "Our findings reveal that dataset size plays an integral part in evaluating DP text rewriting mechanisms."
  - [section 4] "our fitted model suggests a relatively strong positive relationship between dataset size and relative gain... with higher dataset sizes, we can expect average relative gains to rise."
  - [corpus] Related work on noise impact in DP text rewriting (arXiv:2501.19022) examines noise effects but not explicitly dataset size scaling.
- Break condition: If the observed positive relationship is due to artifacts of the specific mechanisms or tasks tested, rather than a general property of DP text rewriting.

### Mechanism 3: Linguistic Level Scaling Effects
- Claim: DP mechanisms operating at different linguistic levels (word, token, document) exhibit different scale-dependent behaviors, with document-level methods showing larger absolute improvements as size increases but from lower baselines.
- Mechanism: Three mechanisms are tested: 1-DIFFRACTOR (word-level MLDP), DP-PROMPT (token-level temperature sampling), and DP-BART (document-level noise in encoder representations). Each applies noise at different granularities with different epsilon ranges.
- Core assumption: The categorical encoding of mechanism type (1=word, 2=token, 3=document) meaningfully captures the complexity/impact of different approaches.
- Evidence anchors:
  - [abstract] No explicit mechanism comparison claim.
  - [section 5] "as we approach document-level methods (e.g., DP-BART), we might expect average relative gains to decrease... however, we observe from the empirical results that DP-BART on average experiences the largest absolute improvements as dataset size increases."
  - [corpus] DP knowledge distillation via synthetic text generation (arXiv:2403.00932) addresses DP in LLMs but focuses on training rather than text rewriting.
- Break condition: If the epsilon values across mechanisms are not truly comparable (acknowledged by authors), making cross-mechanism comparisons invalid.

## Foundational Learning

- Concept: Differential Privacy (DP) and the Privacy Budget (ε)
  - Why needed here: The entire paper evaluates DP mechanisms across different ε values, and understanding the privacy-utility trade-off is core to the research question.
  - Quick check question: If ε=0.5 vs ε=3.0 for a word-level mechanism, which provides stronger privacy guarantees and what is the expected utility trade-off?

- Concept: Local vs. Central Differential Privacy
  - Why needed here: All tested mechanisms operate in the local DP model, where noise is added at the data collection point rather than by a trusted central aggregator.
  - Quick check question: In local DP, who applies the noise and what trust assumptions does this require compared to central DP?

- Concept: Adversarial Evaluation of Privacy
  - Why needed here: Privacy is measured via adversarial classification tasks (gender inference, authorship identification) rather than formal DP guarantees alone.
  - Quick check question: If an adaptive adversary achieves 50% accuracy on a binary gender inference task, what does this imply about the privacy protection?

## Architecture Onboarding

- Component map: Data Layer (5 datasets with utility/privacy labels) -> Privatization Layer (3 DP mechanisms with ε values) -> Split Layer (10%, 25%, 50%, 75%, 100% subsampling) -> Evaluation Layer (DEBERTA-V3-BASE models) -> Metrics Layer (micro-F1, relative gain γ, nearest-neighbor indistinguishability)

- Critical path:
  1. Load dataset → create subsampled splits (5 sizes per source)
  2. For each split × mechanism × ε combination: privatize texts → yields 180 total privatized datasets
  3. Train utility model on privatized train split → evaluate on original val split → record micro-F1 (Ur)
  4. Train adversarial privacy model in static and adaptive settings → record micro-F1 (Pr)
  5. Calculate γ → aggregate across three runs with different shuffles

- Design tradeoffs:
  - Mechanism comparability: Authors explicitly state mechanisms are not directly comparable due to different epsilon semantics
  - Privacy proxy: Empirical privacy via classification attacks is a proxy; formal DP guarantees are not empirically verified
  - Task selection: Utility and privacy tasks are dataset-specific, limiting cross-dataset generalization
  - Computational cost: DP-BART limited to 10% split on Twitter due to computation time (~2.5 months GPU time total)

- Failure signatures:
  - Negative γ values indicate privatization degrades utility more than it improves privacy
  - High adaptive adversary accuracy (e.g., >60% on binary tasks) suggests weak privacy protection
  - NN scores near 0 indicate privatized texts are easily linked to originals
  - Large standard deviations (>0.3) in F1 scores suggest unstable training on privatized data

- First 3 experiments:
  1. Replicate baseline: Take Trustpilot 100% split, run 1-DIFFRACTOR at ε={0.5, 1, 3}, train DEBERTA sentiment classifier, verify micro-F1 matches Table 2 (~96-98%).
  2. Scale test: Run same mechanism on 10%, 25%, 50%, 75% splits, plot γ vs. dataset size to confirm positive trend.
  3. Adversarial sanity check: Train static adversary on original Trustpilot, evaluate on privatized versions at each ε, verify privacy score drops from baseline ~73% toward chance (50%).

## Open Questions the Paper Calls Out

- What is the upper bound of favorable privacy-utility trade-offs as dataset size scales beyond one million texts?
- What additional factors beyond the measured variables influence expected outcomes of DP text rewriting?
- How does the relationship between ε and privacy-utility trade-offs interact with mechanism choice and dataset size?
- What are the technical origins of differential scaling behavior between word-level, token-level, and document-level DP mechanisms?

## Limitations

- The comparison of ε values across different DP mechanisms (word-level, token-level, document-level) is acknowledged as problematic, yet cross-mechanism conclusions are drawn
- Privacy evaluation relies solely on adversarial classification accuracy as a proxy, without formal DP guarantee verification or assessment of membership inference vulnerabilities
- The observed positive relationship between dataset size and relative gain may be specific to the tested mechanisms and tasks rather than a general property of DP text rewriting

## Confidence

- Claim: Larger datasets improve the privacy-utility trade-off in DP text rewriting
  - Confidence: High within specific experimental setup, Medium for generalizability
- Claim: Relative gain metric successfully quantifies the privacy-utility trade-off
  - Confidence: High for the tested mechanisms and tasks
- Claim: DP-BART shows the largest absolute improvements with scale despite lower baselines
  - Confidence: Medium, as this could be influenced by different ε ranges across mechanisms

## Next Checks

1. **Cross-Mechanism ε Normalization**: Conduct experiments where all three mechanisms use comparable ε values (e.g., ε=1 for each) to isolate the effect of noise granularity from privacy budget differences.

2. **Formal DP Guarantee Verification**: Implement Rényi DP accountant calculations for each mechanism to verify that empirical privacy losses align with theoretical guarantees, particularly for the document-level DP-BART mechanism with large ε values.

3. **Scale Extrapolation Test**: Extend experiments to datasets with 10M+ texts to determine whether the positive size-γ relationship continues, plateaus, or reverses at extreme scales, and whether the computational cost of DP-BART remains feasible.