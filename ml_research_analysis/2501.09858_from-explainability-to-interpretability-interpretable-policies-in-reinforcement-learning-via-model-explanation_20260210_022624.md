---
ver: rpa2
title: 'From Explainability to Interpretability: Interpretable Policies in Reinforcement
  Learning Via Model Explanation'
arxiv_id: '2501.09858'
source_url: https://arxiv.org/abs/2501.09858
tags:
- shapley
- interpretable
- policy
- state
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to bridge the gap between
  explainability and interpretability in deep reinforcement learning (RL) by leveraging
  Shapley values. The core method involves analyzing Shapley value vectors to identify
  decision boundaries between action clusters, then formulating interpretable policies
  based on these boundaries.
---

# From Explainability to Interpretability: Interpretable Policies in Reinforcement Learning Via Model Explanation

## Quick Facts
- arXiv ID: 2501.09858
- Source URL: https://arxiv.org/abs/2501.09858
- Authors: Peilang Li; Umer Siddique; Yongcan Cao
- Reference count: 8
- Primary result: Proposes method to convert deep RL policies into interpretable policies using Shapley value analysis, achieving comparable performance in CartPole and MountainCar environments

## Executive Summary
This paper bridges the gap between explainability and interpretability in deep reinforcement learning by introducing a method that transforms complex deep RL policies into transparent, interpretable representations. The approach leverages Shapley values to analyze feature importance in RL models, identifying decision boundaries between action clusters to formulate interpretable policies. The method is model-agnostic and applicable to both off-policy and on-policy algorithms, demonstrated through experiments in classic control environments using DQN, PPO, and A2C.

## Method Summary
The proposed method involves three main stages: first, computing Shapley value vectors for each action to understand feature contributions; second, clustering actions based on Shapley value patterns to identify distinct decision regions; and third, formulating interpretable policies by extracting decision boundaries between these action clusters. The approach treats interpretability as a transformation from complex, black-box policies to rule-based representations that maintain performance while providing transparency. The method works by analyzing the Shapley value space to find natural separations between action clusters, then constructing interpretable policies that approximate these boundaries.

## Key Results
- Interpretable policies achieved maximum reward of 500 in CartPole across all three tested algorithms (DQN, PPO, A2C)
- In MountainCar, interpretable policies showed consistent performance with smaller standard deviations compared to original models
- PPO and A2C interpretable policies outperformed their original algorithms in MountainCar environment
- The method preserved performance while providing more stable behavior than original deep RL models

## Why This Works (Mechanism)
The method works by leveraging Shapley values to create a feature importance space where actions naturally cluster based on their decision-making patterns. By identifying boundaries between these clusters, the approach can construct interpretable policies that capture the essential decision logic without requiring the full complexity of deep neural networks. This transformation from high-dimensional, non-linear decision surfaces to interpretable boundaries preserves the core behavior while making the policy transparent and analyzable.

## Foundational Learning
- **Shapley values in RL**: Measure feature importance for action selection; needed to quantify contribution of each state variable to decisions; quick check: verify Shapley values sum to action value differences
- **Action clustering**: Group similar action decisions based on Shapley patterns; needed to identify distinct behavioral modes; quick check: validate cluster separation with silhouette scores
- **Decision boundary extraction**: Formulate rules from cluster separations; needed to create interpretable policy structure; quick check: test boundary accuracy on held-out data
- **Model-agnostic transformation**: Framework works across different RL algorithms; needed for broad applicability; quick check: verify performance preservation across algorithms
- **Interpretable policy formulation**: Convert boundaries to rule-based representations; needed for transparency; quick check: validate interpretability through human inspection
- **Performance preservation**: Ensure interpretable policies match original performance; needed to maintain utility; quick check: compare reward distributions statistically

## Architecture Onboarding

**Component Map**: State input -> Shapley value computation -> Action clustering -> Decision boundary identification -> Interpretable policy formulation -> Policy execution

**Critical Path**: The most critical components are the Shapley value computation and decision boundary identification, as errors in these stages propagate to the final interpretable policy quality and performance.

**Design Tradeoffs**: The method trades computational complexity (Shapley value computation is expensive) for interpretability gains. The clustering approach assumes clear boundaries exist, which may not hold in all environments. The transformation from complex to simple representations may lose nuanced behaviors important for optimal performance.

**Failure Signatures**: Poor clustering results in overlapping action regions, leading to ambiguous decision boundaries. Inaccurate Shapley values cause misclassification of feature importance, resulting in incorrect policy rules. High-dimensional state spaces may create too many clusters, making boundary extraction computationally infeasible.

**First 3 Experiments**:
1. Verify Shapley value computation correctness by testing on simple environments with known feature importance
2. Test clustering performance with varying numbers of clusters and distance metrics
3. Validate interpretable policy performance degradation on environments with known optimal policies

## Open Questions the Paper Calls Out
None

## Limitations
- Method relies on specific environment dynamics and action discretization, limiting generalization to continuous action spaces
- Clustering approach assumes clear decision boundaries exist, which may not hold in complex environments
- Computational complexity scales with action clusters and feature dimensions, potentially limiting large-scale applicability
- Lacks comprehensive ablation studies to validate necessity of each component in the analysis pipeline

## Confidence

**High Confidence**: The core mathematical framework using Shapley values for feature importance is well-established. The claim that interpretable policies can achieve comparable performance to original deep RL models in simple environments is supported by experimental results.

**Medium Confidence**: The generalizability of the method to complex, high-dimensional environments remains uncertain. The stability claims for interpretable policies are based on limited experimental evidence and may not hold across diverse problem domains.

**Low Confidence**: The assertion that this approach provides "more stable behavior" compared to original models lacks rigorous statistical validation. The trade-off between interpretability and performance in more complex scenarios is not thoroughly explored.

## Next Checks
1. Implement the method on high-dimensional environments (e.g., Atari games or robotic control tasks) to assess scalability and performance trade-offs.

2. Conduct comprehensive ablation studies to determine the impact of each component in the Shapley value analysis pipeline on final policy performance.

3. Perform cross-environment validation to test whether interpretable policies learned in one environment can transfer to similar but distinct environments.