---
ver: rpa2
title: 'Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in
  the Billion Parameter Era'
arxiv_id: '2506.03994'
source_url: https://arxiv.org/abs/2506.03994
tags:
- attributes
- language
- clip
- image
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits multimodal distributional semantics by comparing
  how vision encoders, multimodal models, and language-only models represent semantic
  attributes of concrete object concepts. The authors probe frozen model representations
  with linear classifiers to predict semantic norms from McRae and Binder datasets,
  using densely annotated concepts from the THINGS dataset.
---

# Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era

## Quick Facts
- arXiv ID: 2506.03994
- Source URL: https://arxiv.org/abs/2506.03994
- Reference count: 40
- This paper compares how vision encoders, multimodal models, and language-only models represent semantic attributes of concrete object concepts.

## Executive Summary
This paper revisits multimodal distributional semantics by systematically comparing how different types of models represent semantic attributes of concrete object concepts. The authors probe frozen model representations with linear classifiers to predict semantic norms from established datasets, using densely annotated concepts from the THINGS dataset. They find that multimodal vision encoders slightly outperform language-only models, while self-supervised vision models surprisingly perform well even on non-visual attributes. The study reveals high cross-modal correlations, suggesting convergence given sufficient data, though within-modality correlations remain stronger.

## Method Summary
The authors evaluate frozen model representations by probing them with linear classifiers trained to predict semantic norms from McRae et al. (2005) and Binder et al. (2016) datasets. They use densely annotated concepts from the THINGS dataset, covering 1,854 concrete object concepts with 27 attribute types. The evaluation includes vision encoders (SigLIP, PaliGemma, CLIP, Swin-V2), multimodal models (CLIP, SigLIP, PaliGemma), and language-only models (GPT-3.5-turbo, Llama-3-8B, Llama-3-70B, GPT-4). Performance is measured across 36 binary classification tasks derived from the semantic norms. The study compares within-modality correlations (e.g., vision vs vision) and cross-modality correlations (e.g., vision vs language) to assess representation similarity.

## Key Results
- Multimodal vision encoders like SigLIP and PaliGemma slightly outperform language-only models on semantic attribute prediction tasks.
- Self-supervised vision models like Swin-V2 perform surprisingly well, even on non-visual attributes.
- Cross-modal correlations are high, suggesting convergence given sufficient data, though within-modality correlations are stronger.

## Why This Works (Mechanism)
The study demonstrates that visual and linguistic representations converge on similar semantic structures when models are trained on large datasets, even without explicit multimodal training. The mechanism appears to be that both vision and language models learn to represent concepts in ways that align with human semantic judgments, as captured by the McRae and Binder norms. This convergence is facilitated by the shared pretraining objectives across modalities and the substantial overlap in concepts covered by both vision and language datasets.

## Foundational Learning
- Semantic norms (McRae, Binder): Standardized datasets capturing human judgments about object attributes, essential for evaluating conceptual knowledge in models
- THINGS dataset: Provides dense annotations across 27 attribute types for concrete objects, enabling fine-grained evaluation
- Linear probing: A method for assessing representational quality by training simple classifiers on frozen model outputs, useful for isolating representational differences from task-specific fine-tuning

## Architecture Onboarding

Component Map: Vision encoders (SigLIP, Swin-V2, CLIP) -> Linear classifier -> Semantic attribute prediction
Language models (GPT-4, Llama-3 variants) -> Linear classifier -> Semantic attribute prediction
Multimodal models (PaliGemma, CLIP, SigLIP) -> Linear classifier -> Semantic attribute prediction

Critical Path: Model representation extraction -> Linear classifier training -> Semantic attribute prediction evaluation

Design Tradeoffs: Frozen representations prevent fine-tuning benefits but isolate representational quality; linear probes are simple but may miss complex relationships

Failure Signatures: Poor performance on non-visual attributes by vision models suggests lack of genuine conceptual understanding; high cross-modal correlations could indicate shared pretraining artifacts rather than true semantic convergence

First Experiments:
1. Compare performance across different probing methods (linear vs few-shot learning)
2. Test model representations on out-of-distribution concepts not in the THINGS dataset
3. Evaluate models on additional semantic dimensions beyond the McRae and Binder norms

## Open Questions the Paper Calls Out
None

## Limitations
- Linear probing provides only a narrow view of model capabilities and may not capture full conceptual knowledge
- Semantic norms represent a specific type of conceptual knowledge that may not generalize to all semantic dimensions
- The THINGS dataset's dense annotations cover only 1,854 concepts, leaving substantial gaps in coverage

## Confidence

| Claim | Confidence |
|-------|------------|
| Multimodal vision encoders slightly outperform language-only models | High |
| Self-supervised vision models perform well on non-visual attributes | Medium |
| Cross-modal convergence with sufficient data | Medium |

## Next Checks
1. Conduct controlled experiments to determine whether strong performance of self-supervised vision models on non-visual attributes stems from genuine conceptual understanding or task-specific artifacts
2. Evaluate the same models using alternative probing methods beyond linear classifiers, such as few-shot learning or end-to-end fine-tuning
3. Test model representations on additional semantic dimensions not covered by the McRae and Binder norms, including abstract concepts and more nuanced semantic relationships