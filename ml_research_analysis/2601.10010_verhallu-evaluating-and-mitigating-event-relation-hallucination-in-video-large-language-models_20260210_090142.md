---
ver: rpa2
title: 'VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video
  Large Language Models'
arxiv_id: '2601.10010'
source_url: https://arxiv.org/abs/2601.10010
tags:
- event
- relation
- video
- hallucination
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses event relation hallucination in VideoLLMs,
  a problem where models fail to correctly understand causal, temporal, and subevent
  relations between events in dense video scenarios. Existing benchmarks focus on
  object/scene existence rather than event relation reasoning, leaving this gap unexplored.
---

# VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models

## Quick Facts
- **arXiv ID:** 2601.10010
- **Source URL:** https://arxiv.org/abs/2601.10010
- **Reference count:** 40
- **Key outcome:** Introduces VERHallu benchmark and Key-Frame Propagating (KFP) method to address event relation hallucination in VideoLLMs, achieving up to 39.1% improvement in relation classification F1 scores.

## Executive Summary
This paper addresses event relation hallucination in VideoLLMs, a critical failure mode where models misunderstand causal, temporal, and subevent relations between events in dense video scenarios. Existing benchmarks focus on object/scene existence rather than event relation reasoning, leaving this gap unexplored. The authors introduce VERHallu, a comprehensive benchmark that evaluates event relation hallucination across three tasks: relation classification, question answering, and counterfactual QA. The dataset uses counterintuitive videos to minimize pretraining bias and includes human-annotated candidates for both vision-language and pure language biases. To mitigate event relation hallucination, they propose Key-Frame Propagating (KFP), a training-free method that enhances frame-level attention around key events within intermediate layers using Gaussian-weighted propagation.

## Method Summary
VERHallu evaluates event relation hallucination in VideoLLMs across three tasks: Relation Classification (RC), Question Answering (QA), and Counterfactual QA (CFQA). The benchmark uses 574 videos from Mr. Bean episodes with 7,676 samples to test causal, temporal, and subevent relations. Key-Frame Propagating (KFP) is a training-free inference-time method that identifies top-k key frames via attention scores, propagates attention to neighboring frames using Gaussian weighting (σ=1, m=5 frame window), enhances visual features via softmax-weighted multiplication, and fuses enhanced features with original hidden states using mixing parameter β=0.6. The method operates on intermediate layers (8-15) where multimodal fusion occurs.

## Key Results
- State-of-the-art VideoLLMs struggle significantly with event relation understanding, performing near random chance on relation classification and below 40% accuracy on QA tasks
- KFP effectively reduces hallucinations, improving relation classification F1 scores by up to 39.1% and enhancing overall video event relation understanding (SRH metric)
- VERHallu reveals that models rely heavily on language priors rather than visual evidence, with vision-language bias more pronounced than language bias alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** KFP mitigates event relation hallucination by reallocating frame-level attention around key frames in intermediate layers.
- **Mechanism:** The method identifies top-k key frames via attention score ranking, propagates attention to m neighboring frames using Gaussian weighting, enhances visual features via softmax-weighted multiplication, and fuses enhanced features with original hidden states using mixing parameter β.
- **Core assumption:** Models already ground key events adequately but fail to attend to surrounding subevents needed for relation reasoning.
- **Evidence anchors:** Abstract states KFP "reallocates attention to better capture contextual relationships between events"; Section IV-B provides equations defining the complete propagation and fusion mechanism; SEASON paper addresses temporal hallucination via contrastive decoding, confirming temporal attention as failure mode.
- **Break condition:** When no salient key frames exist (counterfactual videos), attention propagation may amplify noise rather than signal.

### Mechanism 2
- **Claim:** Operating on intermediate layers (8-15) is critical; shallow or deep layers yield minimal impact.
- **Mechanism:** Intermediate layers are where multimodal fusion occurs. KFP applied here can influence semantic integration. Early layers process raw features; late layers have completed multimodal integration.
- **Core assumption:** The transformer architecture processes visual-textual fusion predominantly in middle layers.
- **Evidence anchors:** Section IV-B states "similar to prior work [38], we select the hidden states from layers 8 to 15 for feature propagation"; Table VI shows layers 5-15 and 10-15 show best results while layers 0-5, 15-25 show regression; VistaDPO applies hierarchical spatial-temporal optimization, supporting layer-specific processing insights.
- **Break condition:** Different model architectures may have fusion occurring at different layer ranges; layer selection requires validation per architecture.

### Mechanism 3
- **Claim:** Counterintuitive video scenarios expose reliance on language priors rather than visual evidence.
- **Mechanism:** By using "Mr. Bean" videos with non-obvious causal/temporal relations, the benchmark forces models to rely on actual visual evidence rather than commonsense priors. CFQA task further tests this by replacing videos with unrelated content.
- **Core assumption:** Models trained on typical event distributions will hallucinate relations when visual evidence contradicts learned priors.
- **Evidence anchors:** Section III-A states "counterfactual nature effectively reduces the influence of language or vision-language biases"; Figure 5 shows vision-language bias is more pronounced than language bias across evaluated models; CARVE paper introduces causal abductive reasoning on video events, reinforcing that causal reasoning requires explicit visual grounding.
- **Break condition:** If models are retrained on counterintuitive scenarios, the benchmark may lose discriminative power.

## Foundational Learning

- **Concept: Token Activation Maps (TAM) for visual attention analysis**
  - Why needed here: Understanding where models attend during generation is prerequisite for diagnosing why event relations fail and where to apply KFP.
  - Quick check question: Can you identify which frames a VideoLLM attends to when answering "Why did the person perform action X?"

- **Concept: Event relation taxonomy (causal, temporal, subevent)**
  - Why needed here: The benchmark evaluates three distinct relation types with directional annotations; conflating them invalidates evaluation.
  - Quick check question: For events "person picks up phone" → "person speaks," is this temporal, causal, or subevent?

- **Concept: Training-free inference-time intervention**
  - Why needed here: KFP modifies attention during inference without retraining; understanding this paradigm is essential for deployment decisions.
  - Quick check question: What is the computational overhead of applying KFP vs. fine-tuning a VideoLLM?

## Architecture Onboarding

- **Component map:** Video Input → Frame Sampling → Vision Encoder → Frame Tokens (T×N×D) → Intermediate Layers (8-15) ← KFP applied here → LLM Backbone → Text Generation

- **Critical path:** Key-frame selection → Gaussian propagation → Feature enhancement → MLP fusion. Errors in attention score ranking propagate through entire pipeline.

- **Design tradeoffs:**
  - m (propagation window): Larger m captures more context but may dilute key-frame signal; paper finds m=5 optimal
  - β (fusion weight): Controls original vs. enhanced feature balance; paper finds β=0.6 optimal
  - Layer range: Middle layers (8-15) optimal; requires architecture-specific tuning

- **Failure signatures:**
  - CFQA performance degradation (observed -6.0% for QwenVL-2.5-7B): indicates amplification of semantically weak features in counterfactual scenarios
  - Random-chance RC accuracy: indicates model relies on priors, not visual evidence
  - High MVBench + low VERHallu: indicates strong single-event grounding but weak multi-event reasoning

- **First 3 experiments:**
  1. Reproduce TAM visualization (Figure 4) on your target VideoLLM to confirm early-frame attention bias exists before applying KFP.
  2. Apply KFP to LLaVA-NeXT-7B with m=5, β=0.6, layers 8-15; validate RC improvement matches paper's +39.1% on subevent task.
  3. Evaluate CFQA performance separately; if degradation exceeds 5%, reduce layer range (try 10-15) to limit noise amplification in non-key-frame scenarios.

## Open Questions the Paper Calls Out
None

## Limitations
- KFP layer selection (8-15) is architecture-dependent and may not generalize to larger or differently structured VideoLLMs
- Benchmark's counterintuitive video selection from Mr. Bean episodes creates narrow domain that may not represent diverse real-world scenarios
- CFQA task shows KFP can degrade performance (-6.0% on QwenVL-2.5-7B), suggesting method amplifies noise in semantically weak contexts

## Confidence
- **High confidence**: Event relation hallucination exists as distinct failure mode, evidenced by random-chance performance on RC tasks and systematic gaps between MVBench and VERHallu performance
- **Medium confidence**: KFP's attention reallocation mechanism is sound, but specific hyperparameters require validation on architectures beyond tested 7B/8B models
- **Low confidence**: Benchmark's ability to generalize beyond Mr. Bean domain and long-term stability of KFP's performance improvements without fine-tuning

## Next Checks
1. Apply KFP to a 14B-parameter VideoLLM (e.g., QwenVL-2.5-14B) with both original layer range (8-15) and recalibrated range (10-18) to validate layer selection scalability
2. Evaluate VERHallu benchmark on videos from diverse domains (sports, cooking tutorials, news footage) to assess domain generalization
3. Run longitudinal study where same VideoLLM is evaluated monthly on VERHallu after KFP application to detect performance drift