---
ver: rpa2
title: 'Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for
  Preprocessing Large Language Model Datasets'
arxiv_id: '2511.18054'
source_url: https://arxiv.org/abs/2511.18054
tags:
- data
- dataset
- quality
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Blu-WERP is a preprocessing pipeline for cleaning Common Crawl
  WARC files for LLM training. It combines four text extraction methods, multi-level
  deduplication, and a FastText classifier to remove noise and preserve high-quality
  content.
---

# Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for Preprocessing Large Language Model Datasets

## Quick Facts
- **arXiv ID**: 2511.18054
- **Source URL**: https://arxiv.org/abs/2511.18054
- **Reference count**: 40
- **Primary result**: Blu-WERP achieves 53.88% aggregate accuracy across 9 benchmarks at 1B parameters, outperforming DCLM by 4.0% and Fineweb by 9.5%

## Executive Summary
Blu-WERP is a preprocessing pipeline for cleaning Common Crawl WARC files for LLM training. It combines four text extraction methods, multi-level deduplication, and a FastText classifier to remove noise and preserve high-quality content. Evaluated across 150M-1B parameter models on nine benchmarks, Blu-WERP achieves 53.88% aggregate accuracy, outperforming DCLM by 4.0% and Fineweb by 9.5% at 1B parameters. The pipeline shows consistent gains across all categories, with a 2.4% improvement in world knowledge, 6.2% in language understanding, and 4.2% in commonsense reasoning. Scaling-law predictions indicate superior performance at larger model sizes. The pipeline is scalable, reproducible, and demonstrates that data-centric optimization significantly enhances LLM capabilities.

## Method Summary
Blu-WERP processes Common Crawl WARC files through a three-stage pipeline: (1) jusText extraction with stopword-based filtering, (2) sequential heuristic filtering (URL, language ID at 0.65 threshold, duplicate line/paragraph, n-gram, Gopher-style rules), and (3) Bloom Filter deduplication with 13-token n-grams using "Old Both" configuration that updates the filter with non-duplicate portions of removed documents. A BETR-style FastText classifier is trained using cosine similarity between corpus documents and benchmark train-set embeddings (all-MiniLM-L6-v2), labeling top 10% as high-quality. The pipeline was evaluated using OLMo-1B models with 2048 context, 2M token batch, 20B tokens (20× params), AdamW (lr=2.1e-3, wd=0.1), and cosine decay.

## Key Results
- 53.88% aggregate accuracy across 9 benchmarks at 1B parameters
- 4.0% improvement over DCLM and 9.5% over Fineweb
- Consistent gains across all categories: 2.4% world knowledge, 6.2% language understanding, 4.2% commonsense reasoning
- Scaling-law predictions show α=0.066 coefficient, suggesting sustained advantage at larger model sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JusText-based extraction with implicit stopword filtering yields higher downstream accuracy than parsers prioritizing raw retention.
- Mechanism: JusText applies stopword-density scoring during extraction, removing paragraphs lacking coherent language structure before pipeline processing. This early pruning eliminates low-signal content that would otherwise consume compute in later stages, shifting the retained distribution toward linguistically structured text.
- Core assumption: Stopword presence correlates with prose quality and reasoning-relevant content.
- Evidence anchors:
  - [section 4.1] "jusText applies stopword-based filtering during extraction... after full pipeline processing the jusText-based pipeline retains a higher proportion of useful and well-structured text"
  - [Table 1] JusText achieves 44.74% aggregate score vs. Resiliparse 44.02% despite lower initial retention
  - [corpus] Weak direct evidence; related work (AICC) suggests extraction quality matters, but uses model-based parsing rather than parser ablation
- Break condition: If downstream benchmarks reward non-prose content (code, tables) or stopword-sparse technical writing, this advantage may invert.

### Mechanism 2
- Claim: Bloom Filter deduplication with partial-content recycling ("Old Both") achieves performance comparable to hybrid MinHash approaches at lower memory cost.
- Mechanism: The "Old Both" configuration updates the Bloom filter with n-grams from non-duplicate portions of removed documents, preserving partially unique content for future comparisons. This maintains recall while avoiding the computational overhead of MinHash signature generation and comparison.
- Core assumption: Partial-document uniqueness carries signal value; false positives from Bloom collisions are tolerable at the configured 10⁻¹³ rate.
- Evidence anchors:
  - [Table 2] Bloom Filter (Old Both): 49.22% vs. Exact+Substring+MinHash: 49.15%
  - [Appendix D.1] "This enables the filter to retain partially unique content for future comparisons, improving recall while maintaining precision"
  - [corpus] No direct corpus validation; related pipelines (RefinedWeb, FineWeb) use MinHash but don't report Bloom comparisons
- Break condition: If near-duplicate paraphrasing is prevalent and semantically problematic, MinHash's fuzzy matching may be necessary despite cost.

### Mechanism 3
- Claim: BETR-style classifier training—using benchmark-embedding similarity to label in-corpus data—produces higher-quality filtering than external quality datasets or LLM scoring.
- Mechanism: By computing cosine similarity between corpus documents and benchmark train-set embeddings (all-MiniLM-L6-v2), documents proximate to evaluation tasks are labeled high-quality. A FastText classifier learns this boundary, creating a feedback loop between filtering and downstream performance.
- Core assumption: Embedding proximity to benchmark content predicts training value for those benchmarks; FastText's subword representations generalize the signal.
- Evidence anchors:
  - [Table 3] BETR: 53.8% vs. OpenHermes+ELI5: 51.37% vs. LLaMA-Score+BERT: 51.28%
  - [Appendix E.2] "We then applied a scoring threshold by selecting the top 10% of documents... This provided a balanced dataset of positive and negative samples"
  - [corpus] Indirect support from DCLM showing classifier-based filtering outperforms heuristics; no direct BETR replications
- Break condition: If benchmarks don't representative target capabilities, or if embedding models fail to capture reasoning-relevant features, the feedback loop amplifies misalignment.

## Foundational Learning

- Concept: **Bloom Filters**
  - Why needed here: Core deduplication data structure; understanding false positive rates and capacity tradeoffs is essential for configuring the "Old Both" variant.
  - Quick check question: Given 3B expected n-grams and 10⁻¹³ target FPR, approximately how many bits and hash functions would you configure?

- Concept: **FastText subword embeddings**
  - Why needed here: Enables classifier to handle noisy web text and out-of-vocabulary terms without requiring transformer-scale compute.
  - Quick check question: Why does FastText generalize better than bag-of-words for web-scale classification with high vocabulary variance?

- Concept: **Scaling law extrapolation**
  - Why needed here: The paper uses power-law fits (L(C) = AC⁻ᵅ + E) to predict performance beyond 1B parameters; understanding these constraints prevents overconfident extrapolation.
  - Quick check question: If α ≈ 0.066 for Blu-WERP vs. 0.061 for DCLM, which dataset shows faster loss reduction per unit compute, and what does E ≈ 0 imply?

## Architecture Onboarding

- Component map: WARC input → Text extraction (jusText) → URL/language filtering → Repetition/heuristic filtering → Bloom deduplication → BETR-FastText classification → Tokenized output
- Critical path: Extraction quality → classifier training data quality. The BETR labeling depends on documents surviving earlier stages; over-aggressive filtering reduces classifier training diversity.
- Design tradeoffs: (1) Parser choice: jusText's early filtering vs. Trafilatura's higher raw retention; (2) Deduplication: Bloom memory efficiency vs. MinHash recall on paraphrastic near-duplicates; (3) Classifier: FastText scalability vs. transformer accuracy on subtle quality distinctions.
- Failure signatures: (1) Retention drops below ~15% after filtering—likely over-aggressive thresholds; (2) Benchmark performance plateaus despite scaling—suspect classifier overfitting to embedding artifacts; (3) MMLU scores lag while commonsense benchmarks improve—possible domain distribution shift.
- First 3 experiments:
  1. **Parser validation**: Run 150M models on jusText vs. Trafilatura corpora through identical downstream stages; verify 0.7% aggregate gap replicates on your compute.
  2. **Deduplication stress test**: Inject known near-duplicate paraphrase pairs into a held-out corpus slice; measure Bloom vs. MinHash recall at your target scale.
  3. **Classifier ablation**: Train BETR-FastText with top-5% vs. top-15% threshold; plot quality-per-token curves to find your efficiency frontier before full runs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Blu-WERP's performance generalize to multilingual corpora and domain-specific evaluations (e.g., code, scientific literature)?
- Basis in paper: [explicit] "Extension to multilingual corpora and domain-specific evaluations (e.g., code, scientific literature) remains unexamined and may require adapted quality classifiers."
- Why unresolved: The evaluation focuses exclusively on English web data and general-purpose benchmarks; the BETR classifier was trained only on English benchmark data.
- What evidence would resolve it: Replicate the pipeline on multilingual Common Crawl snapshots and domain-specific corpora (e.g., GitHub, arXiv), then evaluate on multilingual and domain-specific benchmarks.

### Open Question 2
- Question: To what extent do the reported performance gains reflect genuine improvements versus potential benchmark contamination?
- Basis in paper: [explicit] "We did not perform explicit decontamination of evaluation benchmarks from our training corpus... rigorous fuzzy-deduplication against benchmark data would strengthen confidence in reported gains."
- Why unresolved: The pipeline lacks explicit fuzzy-deduplication against evaluation benchmarks, following DCLM/FineWeb protocol but leaving contamination risk unquantified.
- What evidence would resolve it: Perform n-gram or embedding-based decontamination against all nine evaluation benchmarks and compare pre/post decontamination performance.

### Open Question 3
- Question: Can the pipeline effectively filter misinformation while preserving educational content, given it lacks factual verification?
- Basis in paper: [explicit] "Our pipeline does not verify factual correctness of retained documents. While classifier-based filtering prioritizes educational content, it cannot guarantee truthfulness, potentially retaining misinformation present in the original web crawl."
- Why unresolved: The FastText classifier targets educational quality signals, not factual accuracy; no ground-truth verification mechanism exists.
- What evidence would resolve it: Evaluate downstream model performance on factuality benchmarks (e.g., TruthfulQA) and analyze retained documents for misinformation prevalence using fact-checking models.

### Open Question 4
- Question: How do the individual pipeline components (extraction, deduplication, classification) interact to influence downstream generalization?
- Basis in paper: [explicit] "Most pipelines investigate them in isolation, leaving open questions about how these design choices interact and how they influence downstream generalization."
- Why unresolved: Ablations were conducted per-component; joint interaction effects remain unexplored.
- What evidence would resolve it: Conduct factorial ablation studies varying multiple components simultaneously to identify interaction effects on benchmark performance.

## Limitations

- **Distribution Shift**: The paper's superiority over DCLM and Fineweb is demonstrated within a specific model scale (150M-1B parameters) and benchmark suite. While scaling-law extrapolation suggests Blu-WERP maintains advantage at larger scales, the α=0.066 coefficient difference is modest.
- **Extraction Quality Attribution**: The jusText parser's advantage (44.74% vs. 44.02% aggregate) is small and could reflect extraction characteristics beyond stopword filtering. The paper doesn't validate whether the quality difference stems specifically from early stopword pruning versus other jusText heuristics.
- **Classifier Signal Validity**: The BETR approach assumes embedding proximity to benchmark training data predicts downstream task performance. This creates a circular dependency where the filtering method is optimized against the same benchmarks used for evaluation.

## Confidence

**High Confidence**: The pipeline architecture is clearly specified and reproducible. The implementation details for jusText extraction, FastText language filtering, and Bloom Filter configuration are precise enough for faithful replication.

**Medium Confidence**: The relative performance gains (4.0% over DCLM, 9.5% over Fineweb) are robust within the tested parameter range. However, the attribution of these gains to specific pipeline components versus overall data quality improvements is less certain.

**Low Confidence**: The scaling-law extrapolation claims beyond 1B parameters. The modest α difference (0.066 vs. 0.061) suggests gains may diminish at larger scales, but the paper doesn't validate this experimentally.

## Next Checks

1. **Out-of-Distribution Benchmark Test**: Evaluate Blu-WERP-trained models on benchmarks not used in BETR training (e.g., BIG-bench or newly developed reasoning tasks). This validates whether the classifier's feedback loop creates genuine capability improvements or benchmark-specific overfitting.

2. **Component Ablation at Scale**: Train models using (a) jusText + standard deduplication, (b) Trafilatura + BETR filtering, (c) full Blu-WERP pipeline at 7B parameters. This isolates whether gains compound across components or stem primarily from one stage.

3. **Cross-Corpus Validation**: Apply the complete Blu-WERP pipeline to a different web corpus (e.g., C4 or The Pile) and measure performance retention. This tests whether improvements derive from corpus-specific optimizations or general preprocessing principles.