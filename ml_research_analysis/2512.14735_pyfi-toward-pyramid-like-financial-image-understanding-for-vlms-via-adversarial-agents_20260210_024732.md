---
ver: rpa2
title: 'PyFi: Toward Pyramid-like Financial Image Understanding for VLMs via Adversarial
  Agents'
arxiv_id: '2512.14735'
source_url: https://arxiv.org/abs/2512.14735
tags:
- financial
- reasoning
- image
- level
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PyFi, a novel framework designed to enable
  vision-language models (VLMs) to perform step-by-step reasoning for financial image
  understanding in a progressive, simple-to-complex manner. The core contribution
  is PyFi-600K, a large-scale dataset comprising 600,000 financial question-answer
  pairs organized into a reasoning pyramid across 6 capability levels, 11 image types,
  and 17 financial themes.
---

# PyFi: Toward Pyramid-like Financial Image Understanding for VLMs via Adversarial Agents

## Quick Facts
- arXiv ID: 2512.14735
- Source URL: https://arxiv.org/abs/2512.14735
- Authors: Yuqun Zhang; Yuxuan Zhao; Sijia Chen
- Reference count: 11
- Primary result: PyFi-600K dataset enables 19.52% (3B) and 8.06% (7B) accuracy gains in financial VLM reasoning

## Executive Summary
This paper introduces PyFi, a framework that enables vision-language models to perform step-by-step financial image understanding through hierarchical question chains. The core contribution is PyFi-600K, a 600K-sample dataset synthesized via multi-agent adversarial agents under MCTS, organized into a 6-level reasoning pyramid spanning from basic perception to complex decision-making. Fine-tuning Qwen2.5-VL models on these chains improves accuracy significantly, with smaller models showing larger gains (19.52% vs 8.06%), though accuracy drops sharply from 71.80% at basic perception to 32.95% at complex decision tasks, highlighting the challenge of multi-step financial reasoning.

## Method Summary
PyFi uses a multi-agent adversarial synthesis approach where a challenger agent generates increasingly difficult questions while a solver agent provides answers, with MCTS balancing exploration and exploitation. The resulting PyFi-600K dataset contains 600K financial Q&A pairs organized into 6 capability levels (Perception to Decision Support) across 11 image types and 17 themes. Fine-tuning Qwen2.5-VL models with LoRA on question chains converted to Chain-of-Thought format improves performance, with 3B models gaining 19.52% and 7B models gaining 8.06% over baselines.

## Key Results
- Accuracy drops from 71.80% (basic perception) to 32.95% (complex decision-making) in VLMs without fine-tuning
- Calculation Analysis is the primary bottleneck, with >40% error rate even for advanced models
- 3B models gain 19.52% accuracy vs 7B models gaining 8.06%, showing smaller models benefit more from chain training
- Decision Support accuracy improves 23.08% for 7B model when fine-tuned on pyramid-structured chains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical question chains enable progressive reasoning from simple perception to complex financial decision-making.
- Mechanism: The pyramid structure organizes Q&A pairs into 6 capability levels (Perception → Data Extraction → Calculation Analysis → Pattern Recognition → Logical Reasoning → Decision Support). Each level builds on predecessors, so models learn to decompose complex questions into simpler sub-questions with incrementally increasing reasoning demands.
- Core assumption: Financial reasoning is compositional—mastering lower-level capabilities (e.g., extracting values from charts) is prerequisite for higher-level decision-making.
- Evidence anchors:
  - [abstract] "questions at the base require only basic perception, while those toward the apex demand increasing levels of capability"
  - [section 3.1] "pθ(A|I,Q) < pθ(A|I,Q,{sl_i}_{i<j})" — conditioning on predecessor answers improves reliability
  - [corpus] Related work (FinLMM-R1) supports structured reward design for financial reasoning, though domain-specific pyramid hierarchies remain unvalidated elsewhere.
- Break condition: If lower-level errors don't propagate to higher-level failures, the compositional assumption fails.

### Mechanism 2
- Claim: Multi-agent adversarial synthesis (Challenger vs. Solver under MCTS) generates scalable, difficulty-progressive data without human annotation.
- Mechanism: A challenger agent generates increasingly difficult questions; a solver agent answers them. MCTS balances exploration (new question paths via Bernoulli sampling) vs. exploitation (UCT selection of promising prior questions). Backpropagation assigns reward scores by comparing final answers to ground truth.
- Core assumption: Adversarial pressure produces genuinely harder questions rather than artificially convoluted ones; reward signals accurately reflect reasoning correctness.
- Evidence anchors:
  - [abstract] "challenger agent competes with a solver agent by generating question chains that progressively probe deeper capability levels"
  - [section 3.2] "Qi ∼ pψ(Qi|I,Pq,Sl_{1...i-1}) and Ai ∼ pϕ(Ai|I,Qi,Pa,Sl_{1...i-1})"
  - [corpus] Corpus lacks direct validation of adversarial synthesis for finance; GAN-inspired approaches exist but MCTS-based Q&A generation is novel here.
- Break condition: If generated questions contain hallucinations or reward scores don't correlate with human judgment of correctness.

### Mechanism 3
- Claim: Fine-tuning on question chains teaches VLMs level-wise Chain-of-Thought, improving interpretability and accuracy.
- Mechanism: Converting question chains to CoT sequences (n samples → n reasoning steps) trains models to explicitly articulate intermediate reasoning. This is particularly effective for smaller models (3B gains 19.52% vs. 7B's 8.06%).
- Core assumption: Explicit step-by-step training transfers to improved implicit reasoning on unseen financial questions.
- Evidence anchors:
  - [abstract] "fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B on the pyramid-structured question chains enables these models to answer complex financial questions by decomposing them"
  - [section 4.2] "PyFi-CoT-related models improves the 3B model by 19.52% points and the 7B model by 8.06% points"
  - [corpus] FinLMM-R1 similarly uses scalable data + reward design; CoT for multimodal financial reasoning is an emerging pattern but not yet widely replicated.
- Break condition: If gains don't generalize beyond the PyFi-600K distribution or if CoT steps become shallow post-fine-tuning.

## Foundational Learning

- Concept: **Monte Carlo Tree Search (MCTS)**
  - Why needed here: PyFi-adv uses MCTS to balance exploration vs. exploitation in adversarial question generation. Understanding UCT, backpropagation, and node expansion is essential for debugging or extending the data synthesis pipeline.
  - Quick check question: Can you explain why Bernoulli sampling is used before UCT selection in this framework?

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The paper converts question chains into CoT sequences for fine-tuning. Understanding step-wise reasoning supervision vs. outcome-only supervision is critical for interpreting results.
  - Quick check question: How does process supervision (reward per step) differ from outcome supervision (reward for final answer)?

- Concept: **Vision-Language Model Fine-Tuning (LoRA/PEFT)**
  - Why needed here: Experiments use LoRA with full-module adaptation on Qwen2.5-VL. Understanding parameter-efficient fine-tuning helps replicate or scale experiments.
  - Quick check question: What is the trade-off between LoRA rank and training stability for multimodal models?

## Architecture Onboarding

- Component map: Image collection → Adversarial synthesis (MCTS) → Reward assignment → Chain filtering → CoT conversion → LoRA fine-tuning → Level-wise evaluation
- Critical path:
  1. Image collection → 2. Adversarial synthesis (MCTS) → 3. Reward assignment → 4. Chain filtering (remove data leakage) → 5. CoT conversion → 6. LoRA fine-tuning → 7. Level-wise evaluation
- Design tradeoffs:
  - **Synthesis vs. Human Annotation**: Scalable but risks hallucination; filtering (8/15 VLMs same answer without image) mitigates but doesn't eliminate.
  - **3B vs. 7B Fine-Tuning**: Smaller models gain more (19.52% vs. 8.06%) but larger models perform better on high-level DS (+23.08% for 7B).
  - **Exploration Rate in MCTS**: Too high → redundant questions; too low → narrow difficulty progression. Paper uses Bernoulli but doesn't specify optimal rate.
- Failure signatures:
  - Accuracy cliff at Level 3 (CA): Indicates calculation analysis is bottleneck (40%+ error rate across models).
  - High variance in DS accuracy: Suggests overfitting to specific question templates rather than generalizable reasoning.
  - Low reward samples clustering: May indicate systematic adversarial failure modes.
- First 3 experiments:
  1. **Baseline replication**: Evaluate a pre-trained VLM (e.g., Qwen2.5-VL-3B) on PyFi-600K test set, tracking accuracy per level to confirm the 71.80% → 32.95% drop.
  2. **Ablation on chain length**: Fine-tune with truncated chains (levels 1-3 only) vs. full chains (1-6) to isolate the contribution of higher-level reasoning.
  3. **Error backtracing analysis**: Take all failed Level-6 samples, trace back through their chains, and quantify which lower-level errors (PP, DE, CA, PR, LR) most predict final failure—validate CA as primary bottleneck.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can process reward models (PRMs) trained on PyFi-600K's step-wise reward scores effectively verify the correctness of intermediate reasoning steps generated by financial VLMs?
- Basis in paper: [explicit] The authors state: "Even though, due to space limitations, this paper does not perform such training, we use the reward scores to filter out low-quality samples during both evaluation and fine-tuning." They further note that reward scores "enable research into training reliable verifier models, known as process reward models (PRMs)."
- Why unresolved: The infrastructure (reward scores per sample) is provided, but no PRM was actually trained or evaluated to verify whether these scores can effectively supervise multi-step financial reasoning.
- What evidence would resolve it: Training a PRM on PyFi-600K reward scores and demonstrating that it improves VLM accuracy by catching errors at intermediate reasoning steps, compared to outcome-only supervision.

### Open Question 2
- Question: Would improving calculation analysis (CA) capabilities alone yield the most significant gains in final decision-making accuracy, as the error analysis suggests?
- Basis in paper: [explicit] The paper identifies CA as "the most crucial for correct decision-making" with the highest error proportion (>40% even for advanced models like Claude-opus-4-1), concluding that "focusing on improving calculation analysis skills in VLMs could lead to the most significant overall performance gains."
- Why unresolved: The correlation between CA errors and final decision failures is observed, but causation is not established. Targeted interventions to improve CA specifically have not been tested.
- What evidence would resolve it: Ablation experiments where models receive targeted CA-focused training data, compared to balanced training across all levels, showing disproportionate gains in level-6 decision accuracy.

### Open Question 3
- Question: How reliably do MCTS-derived reward scores correlate with human expert assessments of reasoning correctness in financial decision-making?
- Basis in paper: [inferred] The dataset is "synthesized without human annotations" using MCTS backpropagation to assign reward scores. While 15 financial experts were consulted to confirm that calculation analysis is critical, no validation is reported comparing MCTS rewards to human judgments of individual sample quality.
- Why unresolved: The trustworthiness of the entire training signal depends on this correlation, yet it remains unquantified.
- What evidence would resolve it: A human evaluation study where financial experts rate a sample of PyFi-600K chains, with correlation analysis against MCTS-assigned rewards.

### Open Question 4
- Question: Does the pyramid-structured question-chain fine-tuning approach transfer effectively to VLM architectures beyond the Qwen2.5-VL family?
- Basis in paper: [inferred] Fine-tuning experiments were conducted only on Qwen2.5-VL-3B and -7B models. The authors show 19.52% and 8.06% improvements respectively, but whether other architectures (e.g., InternVL, Claude-based models) would benefit similarly from this hierarchical training paradigm remains unknown.
- Why unresolved: The improvement magnitude may depend on Qwen-specific pre-training characteristics; cross-architectural generalization is untested.
- What evidence would resolve it: Fine-tuning at least two additional VLM families (e.g., InternVL, LLaVA-based) on PyFi-600K chains and reporting comparable accuracy gains across capability levels.

## Limitations
- The adversarial synthesis mechanism relies heavily on underspecified MCTS components (agent selection, exploration parameters) without validation of whether generated questions genuinely probe deeper capabilities.
- The data leakage filtering process uses an arbitrary threshold (8/15 VLMs) without justification for why this cutoff captures leakage while preserving valid samples.
- No comparison to human-annotated datasets of similar scale to validate whether the synthesized questions maintain consistent quality across the 600K samples.

## Confidence
- **High confidence** in the empirical results showing accuracy improvements from 71.80% to 32.95% across capability levels, as these are straightforward evaluations on a held-out test set with clear methodology.
- **Medium confidence** in the mechanism claims about compositional reasoning—while the hierarchical structure is well-defined, the paper doesn't provide ablation studies showing that lower-level failures actually cause higher-level failures.
- **Low confidence** in the scalability claims of the adversarial synthesis approach, as there's no comparison to human-annotated datasets of similar scale, nor any analysis of whether the synthesized questions maintain quality consistency.

## Next Checks
1. **Reward signal validation**: Manually sample 50 generated question chains across different levels and rate them for difficulty progression and factual accuracy. Compare human-judged difficulty against the MCTS-assigned reward scores to verify the synthesis mechanism produces genuinely harder questions.

2. **Compositional reasoning test**: Design an experiment where you systematically remove capability at each level (e.g., give models wrong "ground truth" answers for PP questions) and measure how this affects performance on higher levels. This would validate whether the compositional assumption holds.

3. **Hallucination audit**: Take a stratified sample of 100 generated questions, remove the image context, and have multiple human experts rate whether the questions can be answered without visual information. This would quantify the extent of data leakage that bypasses the filtering process.