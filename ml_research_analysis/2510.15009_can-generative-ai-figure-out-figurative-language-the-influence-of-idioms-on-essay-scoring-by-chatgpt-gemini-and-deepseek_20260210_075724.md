---
ver: rpa2
title: Can generative AI figure out figurative language? The influence of idioms on
  essay scoring by ChatGPT, Gemini, and Deepseek
arxiv_id: '2510.15009'
source_url: https://arxiv.org/abs/2510.15009
tags:
- idioms
- human
- generative
- raters
- scoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated whether generative AI models can reliably score
  student essays containing idioms, a type of figurative language. Using a large corpus
  of essays, the researchers compared human and AI scores across three AI models (ChatGPT,
  Gemini, DeepSeek) for essays with and without idioms.
---

# Can generative AI figure out figurative language? The influence of idioms on essay scoring by ChatGPT, Gemini, and Deepseek

## Quick Facts
- arXiv ID: 2510.15009
- Source URL: https://arxiv.org/abs/2510.15009
- Reference count: 6
- Gemini demonstrated highest consistency with human raters for essays containing idioms, while ChatGPT and DeepSeek showed reduced reliability in such cases

## Executive Summary
This study evaluates how well generative AI models can score student essays containing idioms compared to human raters. Using a large corpus of 348 essays, researchers compared scores from three AI models (ChatGPT, Gemini, DeepSeek) against human scores for essays with and without idioms. Gemini showed the highest consistency with human raters, particularly for idiom-containing essays (ICC = 0.735), while the other models demonstrated reduced reliability when idioms were present. The study found that idiom repetition negatively impacts scores, but Gemini was able to mimic human scoring patterns most closely, suggesting it may be the most promising candidate for automated essay scoring, especially for handling figurative language.

## Method Summary
The study used the PERSUADE 2.0 corpus, selecting 174 essays with 4-7 unique idioms (4-14 total) matched to 174 control essays without idioms. Idiom detection was performed using pattern matching against curated lexicons, followed by manual filtering to remove false positives. Each AI model scored all 348 essays three times on consecutive days using the same rubric as human raters. ChatGPT and Gemini were accessed via API, while DeepSeek was accessed through a web interface. Analysis included intraclass correlation coefficients (ICC) for consistency and reliability, and generalized additive models (GAM) to examine the relationship between idiom use and scoring.

## Key Results
- Gemini achieved the highest inter-rater reliability with human scores (ICC = 0.735) compared to ChatGPT (0.695) and DeepSeek (0.660)
- All models maintained good intra-rater consistency (ICC > 0.75), but DeepSeek's high consistency masked score compression around the mean
- GAM analysis revealed non-linear relationship between idiom use and scores, with initial rewards for idiom use followed by penalties for repetition
- Gemini best approximated human scoring patterns for idiom-containing essays, while ChatGPT and DeepSeek showed reduced reliability with idioms

## Why This Works (Mechanism)

### Mechanism 1: Non-linear Idiom Valuation Curve
- Claim: Human raters reward moderate idiom use but penalize repetition, creating a non-linear scoring curve that Gemini approximates better than competitors
- Mechanism: The relationship between idiom density and essay scores follows an inverted-U pattern. Initial idiom use signals linguistic sophistication, but repetition triggers penalization for formulaic writing. Gemini captures this nuance while ChatGPT and DeepSeek apply flatter, less discriminating scoring functions
- Core assumption: The GAM model's effective degree of freedom (EDF = 7.6) accurately reflects non-linearity rather than overfitting
- Evidence anchors: Page 12-13: "AIC value was lower for the GAM model... EDF of this model was 7.6, suggesting strong nonlinearity"; Page 14: "an initial increase was followed by a steady decrease... scores decreased as idiom repetition increased"
- Break condition: If idiom-quality relationships vary significantly across essay genres or student proficiency levels, the detected pattern may not generalize

### Mechanism 2: Consistency-Variety Trade-off in Scoring Distribution
- Claim: Raw consistency metrics (ICC) can be inflated by avoiding extreme scores, misleading evaluation of actual scoring quality
- Mechanism: DeepSeek achieved highest intra-rater reliability (ICC = 0.807) but clustered scores around 4, showing limited variety. This "safe scoring" strategy maximizes self-agreement while minimizing alignment with human raters who use the full rubric range. Gemini achieved slightly lower consistency (0.796) but maintained score distribution closer to humans
- Core assumption: Meaningful essay scoring requires discriminating between quality levels, not just self-consistency
- Evidence anchors: Page 11: "DeepSeek seemed to give scores accumulated around 4 points, showing a rather limited variety"; Page 16: "the reason for the best performance of DeepSeek might actually stem from its rather limited scoring variety"
- Break condition: When models are explicitly calibrated to match human score distributions, raw ICC becomes more interpretable

### Mechanism 3: Figurative Language Underrepresentation in Training Data
- Claim: AI models struggle with idioms because figurative language is less frequent than literal language in training corpora, creating a resource scarcity problem
- Mechanism: LLMs learn from distributional patterns. Idioms, being non-compositional and context-dependent, appear less frequently in training data. This creates weaker learned representations. Gemini's superior idiom handling suggests either better training data coverage or more robust figurative language processing
- Core assumption: Performance gaps on idioms stem from training data distribution rather than architectural limitations
- Evidence anchors: Page 7: "figurative language use is less frequent than literal language use, especially in academic contexts, and the limited availability of figurative expressions can lead to a lower performance"; Page 17-18: "their efficiency in processing idiomatic language in essays may also be limited, likely due to the underrepresentation of idioms in their datasets"
- Break condition: If model architecture (not just training data) fundamentally limits figurative comprehension, more data alone won't close the gap

## Foundational Learning

- Concept: **Intraclass Correlation Coefficient (ICC)**
  - Why needed here: The study uses ICC(3,1) for both intra-rater consistency and inter-rater reliability with humans. Understanding ICC interpretation (excellent ≥0.75, good = 0.60-0.74) is essential to evaluate the paper's reliability claims
  - Quick check question: Model A scores ICC=0.807 for consistency but ICC=0.695 for human alignment. Model B scores ICC=0.796 for consistency but ICC=0.735 for human alignment. Which is better for deployment?

- Concept: **Generalized Additive Models (GAM)**
  - Why needed here: The paper rejects linear mixed models in favor of GAM because the idiom-score relationship is non-linear. EDF=7.6 indicates substantial curvature that linear models would miss
  - Quick check question: What does an EDF significantly greater than 1 suggest about the relationship between two variables?

- Concept: **Non-compositional Language Processing**
  - Why needed here: Idioms cannot be understood by combining word meanings (e.g., "kick the bucket"). This non-compositionality challenges LLMs trained primarily on distributional patterns from literal text
  - Quick check question: Why does lower training frequency of idioms specifically harm LLM performance more than it harms human learners?

## Architecture Onboarding

- Component map:
  - Preprocessing layer: text normalization (pronoun/possessive/article standardization, suffix stripping, lowercase)
  - Idiom detection: pattern matching against curated lexicon (1,583 idioms from Oxford + Miller sources)
  - False positive filter: manual/automated review (12,417 → 10,384 matched idioms after filtering)
  - Scoring engine: LLM with rubric prompt (GPT-4o API, Gemini 1.5 Pro API, DeepSeek V3 web)
  - Reliability monitor: ICC calculation across repeated scoring rounds
  - Idiom metrics: total count, unique count, repetition ratio (total/unique normalized by word count)

- Critical path:
  1. Idiom detection precision → false matches inflate idiom counts, corrupting downstream analysis
  2. Rubric prompt engineering → models must interpret rubric consistently with human rater training
  3. Multi-round scoring → single-round scoring cannot measure consistency

- Design tradeoffs:
  - API vs. web interface: DeepSeek via web may behave differently than API access (inconsistent conditions)
  - Normalized vs. raw idiom counts: normalizing by word count controls for essay length but introduces division artifacts at low word counts
  - Hybrid vs. autonomous: paper recommends human-AI hybrid except for Gemini which shows standalone promise
  - Score distribution monitoring: high ICC alone is insufficient; must check for score clustering

- Failure signatures:
  - Scores compressed around mean (DeepSeek pattern): indicates risk-averse scoring, poor human alignment despite high ICC
  - Inter-rater reliability drops with idiom count (ChatGPT, DeepSeek): signals figurative language processing gap
  - Linear idiom-score model fit: GAM outperforming linear models would indicate missing non-linearity
  - High idiom repetition scores without score penalty: model fails to detect formulaic writing

- First 3 experiments:
  1. Establish baseline consistency: Score 50 essays 3× each, calculate ICC(3,1). If >0.75, proceed; if <0.60, debug prompt or model selection
  2. Idiom isolation test: Create matched pairs (idiom-rich vs. idiom-free) with identical human scores and similar word counts. Measure ICC difference between conditions
  3. Distribution check: Plot score histograms for model vs. human raters. If model shows >50% of scores within ±0.5 of mean, flag potential artificial consistency

## Open Questions the Paper Calls Out

None

## Limitations
- Study relies on a single academic essay corpus (PERSUADE 2.0) from university students, limiting generalizability to other writing contexts or proficiency levels
- Idiom detection methodology, while rigorous, may miss context-dependent figurative usage or introduce false positives that affect results
- DeepSeek was accessed via web interface rather than API, creating potential inconsistencies with controlled API-based scoring

## Confidence

- **High confidence**: Gemini's superior human alignment for idiom-containing essays (ICC = 0.735 vs. ChatGPT 0.695, DeepSeek 0.660); overall finding that all models maintain good consistency (ICC > 0.75)
- **Medium confidence**: Specific mechanism of non-linear idiom valuation curve (reliance on GAM model interpretation); claim about Gemini's standalone viability for automated scoring
- **Low confidence**: Generalizability across different essay types and proficiency levels; exact causes of Gemini's superior performance beyond training data differences

## Next Checks
1. Test model performance across multiple essay genres (narrative, argumentative, descriptive) to assess generalizability of idiom processing capabilities
2. Conduct blind human scoring of a subset of essays to verify that idiom usage patterns actually follow the proposed inverted-U scoring relationship
3. Replicate the study using API-only access for all models to ensure consistent scoring conditions across experiments