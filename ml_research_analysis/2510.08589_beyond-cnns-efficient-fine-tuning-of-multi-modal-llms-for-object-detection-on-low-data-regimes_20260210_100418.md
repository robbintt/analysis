---
ver: rpa2
title: 'Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection
  on Low-Data Regimes'
arxiv_id: '2510.08589'
source_url: https://arxiv.org/abs/2510.08589
tags:
- text
- llms
- detection
- fine-tuned
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work compares traditional CNN-based models and multi-modal
  large language models (LLMs) for detecting artificial text overlays in images. It
  evaluates fine-tuned CNNs, zero-shot LLMs, and fine-tuned LLMs on a balanced dataset
  of 1,000 annotated images.
---

# Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes

## Quick Facts
- arXiv ID: 2510.08589
- Source URL: https://arxiv.org/abs/2510.08589
- Authors: Nirmal Elamon; Rouzbeh Davoudi
- Reference count: 11
- Primary result: Fine-tuning a multi-modal LLM on <1,000 images achieves 36% higher accuracy than CNN baselines for artificial text overlay detection

## Executive Summary
This work demonstrates that fine-tuning multi-modal large language models (LLMs) on small datasets can dramatically outperform traditional CNN-based approaches for complex vision-language tasks. The study focuses on detecting artificial text overlays in images, comparing fine-tuned CNNs, zero-shot LLMs, and fine-tuned LLMs. Results show that fine-tuning a multi-modal LLM on fewer than 1,000 images achieves up to 36% accuracy improvement compared to CNN baselines, with precision of 0.98 and recall of 0.84. The findings highlight the efficiency and adaptability of fine-tuned LLMs for complex vision-language tasks in low-data regimes.

## Method Summary
The approach involves fine-tuning the Phi-3.5 Vision Instruct model on a balanced dataset of 1,000 annotated images for binary classification of artificial text overlays. The vision tower is frozen while LLM weights are kept trainable, with image projector fine-tuning using binary cross-entropy loss for 2 epochs. Training uses cosine learning rate schedule (2e-4), batch size 1 with gradient accumulation of 2, and early stopping based on validation accuracy. A CNN baseline using OCR outputs and ResNet features is trained on 10,000 images for comparison. Evaluation is performed on a separate 1,000-image test set with the same class balance.

## Key Results
- Fine-tuned LLM achieves 0.83 accuracy compared to 0.47 for CNN baseline (36% improvement)
- Fine-tuned LLM reaches 0.98 precision and 0.84 recall, significantly outperforming zero-shot LLMs (0.66 precision, 0.80 recall)
- CNN baseline trained on 10,000 images achieves only 0.54 precision and 0.47 accuracy
- Fine-tuning requires fewer than 1,000 images to match or exceed CNN performance trained on 10Ã— more data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-trained multi-modal LLMs provide a strong initialization for vision-language tasks that enables data-efficient fine-tuning.
- **Mechanism:** The model learns aligned visual-semantic representations during pre-training on large-scale image-text pairs. When fine-tuned, these representations can be rapidly adapted to task-specific visual discrimination by updating language model weights while preserving general visual encoding.
- **Core assumption:** The pre-training corpus contains sufficient diversity to support transfer to the target task domain.
- **Evidence anchors:** [abstract] "LLMs can be effectively fine-tuned on very limited data (fewer than 1,000 images) to achieve up to 36% accuracy improvement"; [Section 5] "During fine-tuning, the vision tower was frozen to retain general-purpose image representation capabilities, while the language model weights were kept trainable"

### Mechanism 2
- **Claim:** Language-guided reasoning enables contextual discrimination that pure CNN architectures cannot achieve through low-level visual features alone.
- **Mechanism:** Multi-modal LLMs process visual input through a language model backbone, enabling semantic reasoning about object-text relationships (e.g., text appearing on a TV screen vs. overlaid text). This allows the model to incorporate scene-level context rather than relying solely on texture, position, and OCR outputs.
- **Core assumption:** The task benefits from high-level semantic understanding rather than purely pixel-level pattern matching.
- **Evidence anchors:** [Section 1] "For instance, without proper fine-tuning, a pre-trained LLM might mistakenly flag the word 'NETFLIX' displayed on a television screen as artificial text, despite it being a natural part of the scene"; [Section 3.1] "The CNN model's dependence on engineered features such as OCR outputs and positional encodings constrains its adaptability when faced with ambiguous or noisy input data"

### Mechanism 3
- **Claim:** Lightweight fine-tuning with early stopping prevents overfitting on small datasets while enabling task specialization.
- **Mechanism:** Training for only 2 epochs with a cosine learning rate schedule and validation-based early stopping allows the model to adapt to task-specific cues (unnatural alignment, occlusion patterns) without memorizing the limited training set.
- **Core assumption:** The learning rate and epoch count are sufficient for convergence without overfitting; the validation set is representative.
- **Evidence anchors:** [Section 3.4] "Fine-tuning is performed for 2 epochs using binary cross-entropy loss, with early stopping based on validation accuracy to prevent overfitting"; [Section 5] "Learning rate of 2e-4, warmup ratio of 0.03... regularization was introduced via gradient checkpointing"

## Foundational Learning

- **Concept: Vision-Language Model Architecture**
  - **Why needed here:** Understanding how visual features are projected into language model embedding space is essential for diagnosing alignment failures and deciding which components to freeze vs. fine-tune.
  - **Quick check question:** Can you explain why freezing the vision tower but fine-tuning the image projector preserves general visual features while enabling task-specific adaptation?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT)**
  - **Why needed here:** The paper uses selective fine-tuning rather than full model training. Understanding PEFT principles helps generalize this approach to other low-data regimes.
  - **Quick check question:** What are the trade-offs between fine-tuning all weights vs. freezing the vision encoder when adapting to a new visual task?

- **Concept: Precision-Recall Trade-off in Object Detection**
  - **Why needed here:** The paper reports precision=0.98, recall=0.84. Understanding this trade-off is critical for interpreting whether the model is suitable for deployment.
  - **Quick check question:** If your application requires catching 95% of artificial overlays, what precision degradation would you expect based on the reported metrics?

## Architecture Onboarding

- **Component map:** Vision Encoder (CLIP ViT) -> Image Projector (MLP) -> Language Model (Phi-3.5) -> Classification Head

- **Critical path:**
  1. Prepare balanced dataset (1,000 images across 3 categories: overlay, natural text, no text)
  2. Freeze vision tower; set LLM weights to trainable; fine-tune image projector
  3. Train for 2 epochs with cosine LR schedule (2e-4), batch size 1, gradient accumulation 2
  4. Monitor validation accuracy; apply early stopping
  5. Evaluate on separate 1,000-image test set with balanced distribution

- **Design tradeoffs:**
  - **Frozen vs. trainable vision tower:** Freezing preserves general features but limits adaptation to domain-specific visual patterns. Paper freezes to retain general-purpose representation.
  - **Zero-shot vs. fine-tuning:** Zero-shot avoids annotation cost but sacrifices accuracy (0.60 vs. 0.83). Fine-tuning requires <1,000 labeled images.
  - **Sequential prompting vs. weight updates:** Prompting improves precision (0.75 vs. 0.66) but recall drops (0.51 vs. 0.80). Weight updates outperform both.

- **Failure signatures:**
  - **High false positives on natural text:** Zero-shot LLMs misclassify signage, labels, and on-screen text as overlays. Fine-tuning reduces this.
  - **Overfitting on small datasets:** Training >2 epochs without early stopping leads to memorization. Monitor validation accuracy.
  - **Precision-recall imbalance:** If recall drops below 0.80, model may be too conservative; adjust classification threshold or increase training diversity.

- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Train the CNN model (OCR + ResNet) on 10,000 images and compare against fine-tuned Phi-3.5 Vision on 1,000 images. Verify reported metrics (CNN: precision=0.54, accuracy=0.47; LLM: precision=0.98, accuracy=0.83).
  2. **Ablate vision tower freezing:** Fine-tune the vision encoder alongside the LLM on the same 1,000 images. Compare accuracy and training time to assess whether freezing is necessary for data efficiency.
  3. **Test on out-of-distribution samples:** Evaluate the fine-tuned model on images with novel overlay styles (e.g., different fonts, semi-transparent overlays) not present in training. Measure generalization gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the efficient fine-tuning strategy demonstrated for text overlay detection generalize effectively to other complex vision-language tasks?
- Basis in paper: [explicit] The authors conclude that while their focus was specific to text overlays, "the proposed fine-tuning strategy is broadly applicable and can be extended to other complex vision-language tasks that require nuanced contextual understanding."
- Why unresolved: The experimental scope was restricted to artificial text overlay detection; no other vision-language tasks were benchmarked to validate this claim.
- What evidence would resolve it: Applying the same sub-1,000 image fine-tuning protocol to distinct tasks (e.g., deepfake detection or medical imagery) to verify if comparable accuracy improvements over CNNs are achieved.

### Open Question 2
- Question: What is the minimum data threshold required for effective fine-tuning before performance degrades significantly?
- Basis in paper: [inferred] The study establishes a successful regime using roughly 1,000 images but lacks an ablation study to identify the lower bound of data efficiency.
- Why unresolved: The paper demonstrates that "fewer than 1,000" images suffice but does not map the performance curve or failure points as the dataset size approaches the extreme low-data limit.
- What evidence would resolve it: A series of training runs using progressively smaller subsets (e.g., 800, 400, 200, 50 images) to identify the inflection point where accuracy drops below CNN baselines.

### Open Question 3
- Question: Is the high data efficiency observed a property of the lightweight model architecture (Phi-3.5) specifically, or does it scale with model size?
- Basis in paper: [inferred] The comparison includes a larger model (Qwen2.5-VL-7B) but only in a zero-shot setting; the absence of a fine-tuned comparison leaves the interaction between parameter count and data efficiency unexplored.
- Why unresolved: It remains unclear if larger models would achieve even higher performance with the same limited data, or if lightweight models are optimal for this specific low-resource constraint.
- What evidence would resolve it: Fine-tuning larger multimodal models (e.g., 7B or 70B parameters) on the same 1,000-image dataset and comparing the convergence speed and accuracy against the fine-tuned Phi-3.5 model.

## Limitations
- The superiority of fine-tuned LLMs depends on having 1,000 annotated images, but the marginal benefit of fine-tuning at different dataset sizes is not explored
- The model comparison uses different training set sizes (1,000 vs. 10,000 images), complicating direct attribution of performance differences
- The 0.98 precision and 0.84 recall represent an implicit trade-off that may not be optimal for all applications

## Confidence

- **High Confidence:** The architectural framework for fine-tuning Phi-3.5 Vision (freezing vision tower, training LLM weights) is well-specified and reproducible
- **Medium Confidence:** The 36% accuracy improvement claim is supported by the described experiments but depends on the specific dataset and task
- **Low Confidence:** The mechanism explanations for why language-guided reasoning outperforms CNN features are plausible but not rigorously validated

## Next Checks
1. **Dataset Size Sensitivity:** Replicate the experiment with 100, 500, and 2,000 training images to determine the minimum dataset size where fine-tuning provides meaningful improvement over zero-shot inference

2. **Cross-Domain Generalization:** Test the fine-tuned model on artificial text detection in domains not represented in the original training data (e.g., medical imaging, industrial inspection) to assess whether the learned visual-language alignment transfers beyond the initial distribution

3. **Failure Mode Analysis:** Systematically analyze false positive and false negative cases to determine whether errors stem from semantic ambiguity (e.g., natural text styled to look artificial) or fundamental limitations in the visual-language alignment