---
ver: rpa2
title: Structured Sparse Transition Matrices to Enable State Tracking in State-Space
  Models
arxiv_id: '2509.22284'
source_url: https://arxiv.org/abs/2509.22284
tags:
- state
- group
- diagonal
- matrices
- transition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PD-SSM, a structured sparse parametrization
  for state transition matrices in state-space models (SSMs) that enables efficient
  and expressive finite-state automaton (FSA) emulation. The method parametrizes transition
  matrices as the product of a column one-hot matrix and a complex-valued diagonal
  matrix, achieving linear computational cost with state size while maintaining universal
  expressivity for any N-state FSA using a single layer and linear readout.
---

# Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models

## Quick Facts
- **arXiv ID**: 2509.22284
- **Source URL**: https://arxiv.org/abs/2509.22284
- **Reference count**: 40
- **Primary result**: PD-SSM achieves linear computational cost with state size while maintaining universal expressivity for FSA emulation

## Executive Summary
This paper introduces PD-SSM, a structured sparse parametrization for state transition matrices in state-space models (SSMs) that enables efficient finite-state automaton (FSA) emulation. The method parametrizes transition matrices as the product of a column one-hot matrix and a complex-valued diagonal matrix, achieving linear computational cost while maintaining universal expressivity for any N-state FSA using a single layer. The approach is theoretically proven to be BIBO-stable and optimal in state size, and empirically demonstrates significant improvements over modern SSM variants on FSA state tracking tasks, multivariate time-series classification, and length generalization benchmarks.

## Method Summary
PD-SSM introduces a structured sparse parametrization for state transition matrices in SSMs by decomposing them into a column one-hot matrix and a complex-valued diagonal matrix. This design achieves linear computational complexity with respect to state size while preserving the ability to emulate any N-state finite-state automaton using a single layer and linear readout. The theoretical analysis proves BIBO stability and state size optimality, while the empirical evaluation demonstrates strong performance across multiple benchmarks including FSA state tracking, multivariate time-series classification, and length generalization tasks.

## Key Results
- PD-SSM achieves linear computational cost with state size while maintaining universal expressivity for FSA emulation
- Outperforms modern SSM variants on FSA state tracking tasks with single-layer architecture
- Matches or exceeds state-of-the-art performance on multivariate time-series classification and demonstrates strong length generalization on synthetic benchmarks

## Why This Works (Mechanism)
PD-SSM works by structuring the transition matrix as a product of a column one-hot matrix and a diagonal matrix, which preserves FSA expressivity while reducing computational complexity from quadratic to linear. The one-hot column structure ensures that each state can only transition to one other state per time step, while the diagonal matrix provides the necessary state-dependent scaling. This design maintains the fundamental properties required for FSA emulation - state transitions, output emissions, and initial state specification - while dramatically reducing the parameter count and computational overhead. The structured parametrization also enables efficient implementation and stable training dynamics.

## Foundational Learning
- **State-Space Models (SSMs)**: Discrete-time dynamical systems that model sequences through state transitions and output emissions - needed for understanding the core modeling framework
- **Finite-State Automata (FSA)**: Computational models with finite states that process input sequences through state transitions - needed to understand the emulation target
- **BIBO Stability**: Bounded-Input Bounded-Output stability property ensuring system outputs remain bounded for bounded inputs - needed for theoretical guarantees
- **Structured Sparsity**: Matrix parametrization with specific zero patterns to reduce computational complexity - needed to understand the efficiency gains
- **Linear Readout**: Simple linear transformation from hidden states to outputs - needed to understand the minimal architecture requirements
- **Column One-Hot Matrices**: Matrices with exactly one non-zero entry per column - needed to understand the core structural constraint

## Architecture Onboarding

**Component Map**: Input -> PD-SSM Layer -> Linear Readout -> Output

**Critical Path**: The PD-SSM layer processes sequential input through structured state transitions, followed by linear readout to produce predictions. The column one-hot structure enables efficient state updates, while the diagonal matrix provides state-dependent scaling.

**Design Tradeoffs**: Structured sparsity reduces computational complexity from O(NÂ²) to O(N) but may limit expressivity for non-FSA-like dynamics. Single-layer architecture simplifies implementation but may require deeper networks for complex tasks. Linear readout minimizes parameters but may constrain output flexibility.

**Failure Signatures**: Performance degradation on tasks requiring continuous state representations or complex non-deterministic transitions. Potential overfitting when the structured parametrization is too restrictive for the target task. Computational inefficiency may arise if the diagonal matrix scaling becomes too large.

**First Experiments**:
1. Verify linear computational scaling by measuring runtime across different state sizes on FSA tracking tasks
2. Test expressivity limits by comparing PD-SSM against unstructured SSMs on chaotic system modeling
3. Evaluate hybrid Transformer-PD-SSM performance on variable-length sequence classification tasks

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Theoretical guarantees specifically apply to finite-state automata, with broader applicability remaining to be fully characterized
- Current evaluation focuses primarily on classification and state tracking tasks, with limited exploration of generation capabilities
- The one-hot column structure may impose inductive biases that could limit performance on tasks requiring more nuanced state transitions

## Confidence

**High confidence**: Theoretical analysis of PD-SSM's structure and implications for FSA emulation, including BIBO stability proof and state size optimality arguments.

**Medium confidence**: Empirical performance claims on multivariate time-series classification and length generalization tasks, as results could vary with different hyperparameter settings.

**Medium confidence**: Applicability to hybrid Transformer-SSM architectures for natural language processing tasks, as deeper analysis of linguistic structure capture would strengthen these claims.

## Next Checks

1. **Scaling analysis**: Systematically evaluate PD-SSM's performance and computational efficiency across multiple orders of magnitude in state size and sequence length, measuring both wall-clock time and memory usage patterns on different hardware configurations.

2. **Expressive capacity characterization**: Conduct controlled experiments comparing PD-SSM against unstructured SSMs on tasks requiring complex non-FSA-like dynamics, such as chaotic system modeling or tasks with continuous state representations.

3. **Architecture ablation studies**: Systematically vary the PD-SSM depth, readout configurations, and integration with Transformer components across a broader range of sequence modeling tasks to identify the most effective architectural patterns.