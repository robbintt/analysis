---
ver: rpa2
title: Towards Causal Model-Based Policy Optimization
arxiv_id: '2503.09719'
source_url: https://arxiv.org/abs/2503.09719
tags:
- causal
- learning
- policy
- environment
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework for Causal Model-Based Policy
  Optimization (C-MBPO) that integrates causal learning into the MBRL pipeline. The
  key idea is to learn a Causal Markov Decision Process (C-MDP) by inferring a local
  Structural Causal Model (SCM) of both state and reward transition dynamics from
  online trajectories.
---

# Towards Causal Model-Based Policy Optimization

## Quick Facts
- arXiv ID: 2503.09719
- Source URL: https://arxiv.org/abs/2503.09719
- Reference count: 1
- Key outcome: Novel framework integrating causal learning into MBRL to distinguish causal from spurious correlations, improving robustness to distributional shifts

## Executive Summary
This paper introduces Causal Model-Based Policy Optimization (C-MBPO), a framework that enhances standard model-based reinforcement learning by learning causal structure from online trajectories. The key insight is that by inferring a local Structural Causal Model (SCM) of state and reward dynamics, the agent can distinguish between true causal relationships and spurious correlations. This allows the agent to simulate counterfactual transitions that ignore non-causal dependencies, resulting in policies that remain effective when the environment experiences distributional shifts affecting spurious variables.

## Method Summary
C-MBPO learns a Causal Markov Decision Process by first inferring a local Structural Causal Model through conditional independence tests on collected trajectories. This identifies parent variables for each state dimension and reward. The agent then trains separate conditional distributions for each state dimension and reward based only on their identified parents, preventing the model from learning "leaky" dependencies on spurious variables. Counterfactual rollouts are generated using this SCM, and the policy is updated using these synthetic transitions. The approach is demonstrated in a simple environment with near and far out-of-distribution dynamics drifts, showing robustness when spurious correlations break.

## Key Results
- C-MBPO distinguishes between causal and spurious correlations in state dynamics through conditional independence testing
- The method learns separate conditional distributions based on identified parent variables, excluding spurious dependencies
- In experiments with distributional shifts, C-MBPO maintains performance while standard MBPO fails catastrophically when non-causal relationships change

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Inferring a local Causal Graphical Model allows the agent to distinguish between causal parents and spurious correlations in state dynamics.
- **Mechanism:** The system applies conditional independence tests (e.g., PC algorithm) on online trajectories to estimate the local graph structure. By identifying true parent variables for each state component, the agent excludes variables that are merely correlated but don't influence next state or reward.
- **Core assumption:** No unobserved confounders affect both a candidate variable and the target state/reward (Proposition 2.3); Markov property holds.
- **Evidence anchors:** Abstract mentions "distinguish between mere statistical correlations and causal relationships"; Proposition 2.3 states edges are identifiable if no unobserved vertex exists; related corpus discusses causal structure for efficiency.
- **Break condition:** Presence of hidden confounders; insufficient trajectory data to satisfy independence test power; non-Markovian environments.

### Mechanism 2
- **Claim:** Learning a factorized Structural Causal Model based on the inferred graph prevents the model from learning "leaky" dependencies on spurious variables.
- **Mechanism:** Instead of learning a monolithic transition function, the agent learns separate conditional distributions for each state dimension and reward. This factorization explicitly removes the spurious variable from the input features of the model predicting reward, preventing the model from relying on correlations that may break under distribution shift.
- **Core assumption:** The structure learned in the previous step is sufficiently accurate to define the factorization.
- **Evidence anchors:** Abstract mentions learning "a Causal Markov Decision Process... by inferring a local Structural Causal Model"; section 3 describes learning "conditional distributions $p_\theta(s'_j | pa(s'_j))$"; corpus discusses MBRL but doesn't confirm this factorization strategy.
- **Break condition:** Incorrect graph structure leading to omitted causal parents or included spurious parents.

### Mechanism 3
- **Claim:** Generating counterfactual rollouts using the learned SCM yields policy updates that are robust to shifts in non-causal variables.
- **Mechanism:** The agent uses the learned SCM to simulate "imaginary" trajectories. Because the SCM ignores spurious variables when predicting rewards, the policy optimization learns a value function that relies only on causal variables. When the environment changes at test time, the policy remains effective because it never relied on the spurious correlation.
- **Core assumption:** The distributional shift affects only non-causal relationships, leaving the core causal mechanisms invariant.
- **Evidence anchors:** Abstract mentions "simulate counterfactual on-policy transitions... guiding policy optimization more effectively"; section 3.1 shows "Far OOD... MBPO-SAC's policy failing catastrophically... C-MBPO-SAC display statistically equivalent performance"; corpus signals interest in "Robust Offline Model-based RL."
- **Break condition:** Distributional shifts that alter the underlying causal mechanisms rather than just spurious correlations; compounding errors in the SCM during long-horizon rollouts.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) & Interventions**
  - **Why needed here:** The paper frames the RL problem as a Causal MDP. Understanding the difference between observational conditioning $P(Y|X)$ and interventional conditioning $P(Y|do(X))$ is required to grasp why standard MBRL fails on spurious correlations.
  - **Quick check question:** If variable $Z$ correlates with Reward $R$ but does not cause it, how does an intervention $do(Z=z)$ differ from observing $Z=z$ in terms of predicting $R$?

- **Concept: Causal Discovery / Structure Learning**
  - **Why needed here:** The first step of C-MBPO is estimating the causal graph from data. You need to know the limitations of algorithms (like PC or GES) that use conditional independence tests to orient edges.
  - **Quick check question:** Why does the presence of a hidden confounder $U$ (a variable not observed in the state space) prevent the unique identification of the causal graph from observational data alone?

- **Concept: Model-Based Policy Optimization (MBPO)**
  - **Why needed here:** C-MBPO is an augmentation of the standard MBPO framework. Understanding how model rollouts are typically mixed with real data for policy updates is the baseline this paper modifies.
  - **Quick check question:** In standard MBPO, why does model bias compound exponentially as the length of the model rollout increases?

## Architecture Onboarding

- **Component map:**
  Data Buffer ($D_{env}$) -> Structure Learner -> SCM Ensemble -> Model Buffer ($D_{model}$) -> Policy Optimizer

- **Critical path:**
  1. Initialize with random policy to populate $D_{env}$
  2. Run Structure Learner to identify parents for all state dimensions
  3. Instantiate SCM Ensemble with input dimensions matching the identified parents
  4. Generate counterfactual rollouts using SCM to fill $D_{model}$
  5. Update Policy using $D_{model}$

- **Design tradeoffs:**
  - **Monolithic vs. Factorized Model:** Standard MBRL uses one large network for $p(s'|s,a)$ (easier to implement, captures all correlations). C-MBPO uses many small networks (harder to manage, requires graph info, but robust to spurious shifts).
  - **Static vs. Dynamic Graph:** The current proposal learns the graph initially; a dynamic graph belief would be more robust to non-stationary environments but computationally expensive.

- **Failure signatures:**
  - **Performance degradation in "Far OOD":** Indicates the Structure Learner failed to identify a spurious variable as non-causal (graph error), causing the SCM to retain the dependency.
  - **High variance in training:** Indicates instability in the Structure Learning phase or poor convergence of the SCM ensemble.
  - **Catastrophic forgetting:** If the policy switches entirely to model data ($D_{model}$) too early, errors in the SCM may reinforce bad behaviors.

- **First 3 experiments:**
  1. **Baseline Verification:** Run C-MBPO on the "Light/Door" linear environment described in Section 3.1. Verify that the Structure Learner correctly identifies that $Z_t$ is not a parent of $R_t$.
  2. **OOD Stress Test:** Train to convergence, then deploy in the "Near OOD" setting ($do(Zt=0)$). Confirm SAC/MBPO performance drops while C-MBPO remains stable (Table 1c).
  3. **Ablation on Graph Accuracy:** Manually corrupt the learned graph (e.g., force $Z_t \rightarrow R_t$ edge) and measure the resulting performance drop to quantify the sensitivity of the system to structure learning errors.

## Open Questions the Paper Calls Out
- Can the C-MBPO framework be adapted to maintain a belief state over the Causal Graphical Model (CGM) structure rather than relying on a single point estimate? The Conclusion states future work will focus on refining CGM learning by introducing a belief state over G that allows for general uncertainty around the CGM.

## Limitations
- The exact structure learning method (PC, GES, or alternative) and its parameter settings are unspecified, making faithful replication challenging.
- Sensitivity to graph structure errors is not quantified; Type I/II errors in edge detection could significantly degrade robustness.
- The paper assumes near-static causal mechanisms but does not address long-term non-stationarity or latent confounders.

## Confidence
- **High Confidence:** The conceptual framework linking SCMs to MBPO robustness is sound and theoretically grounded.
- **Medium Confidence:** The empirical results on synthetic environments are promising but limited to a narrow testbed; scalability to complex domains is unproven.
- **Low Confidence:** Claims about the method's performance in the presence of hidden confounders are not addressed.

## Next Checks
1. **Graph Sensitivity Analysis:** Systematically perturb the learned causal graph (add/remove edges) and measure the resulting performance degradation to quantify robustness to structure learning errors.
2. **Scalability Test:** Implement C-MBPO on a more complex continuous control benchmark (e.g., HalfCheetah or Walker) with introduced spurious correlations to evaluate real-world applicability.
3. **Latent Confounder Scenario:** Modify the synthetic environment to include an unobserved variable influencing both state and reward, and test whether C-MBPO's performance degrades relative to standard MBPO.