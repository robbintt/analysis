---
ver: rpa2
title: 'CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification'
arxiv_id: '2508.13280'
source_url: https://arxiv.org/abs/2508.13280
tags:
- learning
- training
- image
- curriculum
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses Mayo Endoscopic Subscore (MES) classification
  in ulcerative colitis, a challenging task due to label noise, class imbalance, and
  ordinal structure. The proposed CLoE framework uses image quality as a proxy for
  sample difficulty and introduces samples in a curriculum-based order: clean samples
  first, then mixed, then noisy.'
---

# CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification

## Quick Facts
- **arXiv ID**: 2508.13280
- **Source URL**: https://arxiv.org/abs/2508.13280
- **Reference count**: 40
- **Primary result**: BBPS-based quality scores + curriculum learning + ResizeMix consistently improve MES classification accuracy and QWK across datasets and backbones

## Executive Summary
This paper addresses Mayo Endoscopic Subscore (MES) classification in ulcerative colitis, a challenging task due to label noise, class imbalance, and ordinal structure. The proposed CLoE framework uses image quality as a proxy for sample difficulty and introduces samples in a curriculum-based order: clean samples first, then mixed, then noisy. A lightweight BBPS-trained classifier estimates difficulty scores, which are combined with ResizeMix augmentation to improve robustness. Evaluated across LIMUC and HyperKvasir datasets with multiple CNN and Transformer architectures, CLoE consistently outperforms strong supervised and self-supervised baselines. For example, ConvNeXt-Tiny achieves 82.5% accuracy and 0.894 QWK on LIMUC, highlighting the effectiveness of difficulty-aware training for ordinal classification under label uncertainty.

## Method Summary
The CLoE framework trains a MobileNetV2 quality classifier on BBPS labels (clean vs. noisy) from HyperKvasir, then uses its predictions to score MES-labeled images. Data is split at τ=0.5 into clean and noisy subsets. Training proceeds in three curriculum stages: (1) clean samples only, (2) clean + noisy, (3) fine-tuning on noisy. ResizeMix augmentation is used throughout. The method is evaluated on LIMUC and HyperKvasir with ConvNeXt, ResNet50, and MobileNetV2 backbones, using accuracy, macro-F1, QWK, recall, and specificity.

## Key Results
- ConvNeXt-Tiny achieves 82.5% accuracy and 0.894 QWK on LIMUC test set
- CLoE outperforms supervised and self-supervised baselines consistently
- ResizeMix + CL achieves 81.35% accuracy vs. 79.79% for ResizeMix alone (MobileNetV2, LIMUC validation)
- CL protocol yields 1–2% consistent gains over STD-A across backbones
- Performance degrades without curriculum (validation accuracy improves early but generalizes poorly)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Image quality serves as a proxy for annotation reliability in MES classification
- Mechanism: A lightweight MobileNetV2 classifier trained on BBPS labels (BBPS 2–3 = clean, BBPS 0–1/impacted stool = noisy) assigns cleanliness probabilities s(x) ∈ [0,1]. High-quality images are assumed to have clearer diagnostic features and lower label noise risk. The curriculum prioritizes these samples early in training, establishing stable decision boundaries before introducing noisier cases.
- Core assumption: Bowel cleanliness correlates with both visual interpretability and label correctness—BBPS captures preparation quality, not inflammation severity, but poor preparation obscures mucosal patterns and increases inter-observer disagreement.
- Evidence anchors:
  - [abstract] "Image quality, estimated via a lightweight model trained on Boston Bowel Preparation Scale (BBPS) labels, is used as a proxy for annotation confidence"
  - [Section 3.1] "low-quality images may obscure key visual patterns, increasing classification difficulty and annotation uncertainty"
  - [corpus] Weak direct corpus support for BBPS-specific proxies; related work (Learning Disease State from Noisy Ordinal Disease Progression Labels) addresses noisy ordinal labels but uses progression relationships, not quality proxies
- Break condition: If BBPS scores are unavailable for your target dataset, you must train the quality classifier on an auxiliary dataset with preparation quality labels, then transfer predictions (as CLoE does with HyperKvasir → LIMUC). This introduces domain shift risk.

### Mechanism 2
- Claim: Staged training with early stopping transitions improves robustness to label noise
- Mechanism: Training proceeds in three phases: (1) D_clean only (s(x) ≥ 0.5), (2) D_clean ∪ D_noisy, (3) fine-tuning on D_noisy. Early stopping triggers transition when validation accuracy plateaus for 5 epochs. This prevents early overfitting to noisy labels while ensuring eventual coverage of the full data distribution.
- Core assumption: The fixed threshold τ = 0.5 meaningfully separates reliable from unreliable samples, and validation accuracy is a stable signal for stage transitions.
- Evidence anchors:
  - [Section 3.2] "We use a fixed threshold τ = 0.5 to partition the training data into clean and noisy subsets"
  - [Section 5.3.2] CL protocol yields 1–2% consistent gains over STD-A across backbones
  - [corpus] Ordinal Adaptive Correction paper similarly addresses noisy ordinal labels through data-centric approaches, supporting the general premise that structured sample handling matters
- Break condition: If your validation set is small or non-representative, early stopping may trigger prematurely or fail to detect plateaus. Monitor stage duration as a sanity check.

### Mechanism 3
- Claim: ResizeMix augmentation preserves ordinal structure better than CutMix or MixUp for MES classification
- Mechanism: ResizeMix pastes a resized image patch onto another image, preserving spatial layout and label semantics. Unlike MixUp (linear interpolation) or CutMix (hard region swap), ResizeMix maintains clearer boundaries between ordinal classes while increasing variation. Combined with curriculum learning, early-phase ResizeMix operates primarily on clean samples, reducing noise propagation.
- Core assumption: The resizing operation does not corrupt diagnostic features relevant to MES grading (erythema, vascular pattern, ulceration).
- Evidence anchors:
  - [Section 5.3.1] ResizeMix + CL achieves 81.35% accuracy vs. 79.79% for ResizeMix alone (MobileNetV2, LIMUC validation)
  - [Section 5.3.1] "MixUp's linear interpolation can blur boundaries between ordered categories"
  - [corpus] No direct corpus comparison of ResizeMix for ordinal tasks; this appears underexplored in related literature
- Break condition: For lesions where spatial scale is diagnostic (e.g., ulcer size relative to frame), ResizeMix's arbitrary scaling could distort clinically relevant features. Validate on held-out clinical cases.

## Foundational Learning

- Concept: **Curriculum Learning**
  - Why needed here: MES datasets contain heterogeneous label quality; training on all data equally allows noisy samples to corrupt early feature learning
  - Quick check question: Can you articulate why easy-to-hard ordering helps when labels are noisy, not just when samples are visually complex?

- Concept: **Ordinal Classification & QWK**
  - Why needed here: MES 0→3 represents increasing severity; misclassifying MES-1 as MES-2 is less severe than MES-0 as MES-3. QWK (Quadratic Weighted Kappa) captures this ordinal agreement.
  - Quick check question: Would accuracy alone be sufficient if your clinical use case requires distinguishing "remission" (0–1) from "active disease" (2–3)?

- Concept: **Label Noise from Inter-Observer Variability**
  - Why needed here: Moderate MES grades (especially MES-1 vs. MES-2) show substantial expert disagreement; models trained without noise-awareness may overfit to annotator-specific biases
  - Quick check question: Does your dataset have multiple annotations per image? If not, how will you estimate noise levels?

## Architecture Onboarding

- Component map: MobileNetV2 (quality classifier) -> s(x) scores -> 3-stage curriculum -> MES classifier (ConvNeXt-Tiny/ResNet50/MobileNetV2) with ResizeMix augmentation

- Critical path:
  1. Verify BBPS labels or equivalent quality annotations exist for training the quality classifier
  2. Generate cleanliness scores for all MES-labeled samples
  3. Partition into D_clean / D_noisy using τ = 0.5
  4. Implement 3-stage training with early-stopping-based transitions
  5. Integrate ResizeMix into augmentation pipeline

- Design tradeoffs:
  - **Threshold selection**: Fixed τ = 0.5 is simple but may not be optimal for all datasets; ablate τ ∈ {0.3, 0.5, 0.7}
  - **Quality classifier architecture**: MobileNetV2 is lightweight (3.5M params) but may underfit if source/target domains differ significantly
  - **Stage transition signal**: Validation accuracy is used here; consider loss-based or uncertainty-based triggers for alternative datasets

- Failure signatures:
  - **No improvement over baseline**: Check if quality classifier has reasonable accuracy (>70%) on held-out quality labels; if not, proxy signal may be too weak
  - **Stage 1 never transitions**: Validation accuracy may not plateau if D_clean is too small—ensure ≥30% of data qualifies as clean
  - **QWK degrades while accuracy improves**: Model may be learning class frequencies rather than ordinal structure; verify class balance across curriculum stages

- First 3 experiments:
  1. **Sanity check**: Train quality classifier on BBPS data, report accuracy on held-out quality labels. If <70%, the proxy signal is unreliable.
  2. **Baseline comparison**: Train MES classifier with standard supervised learning vs. CLoE (same backbone). Report accuracy, macro-F1, and QWK on the same test split.
  3. **Ablation**: Compare CLoE with and without ResizeMix to isolate augmentation contribution. Expected: 1–2% QWK improvement from ResizeMix alone (per Table 8).

## Open Questions the Paper Calls Out

- Can the CLoE framework generalize effectively to other ordinal medical classification tasks beyond ulcerative colitis? The current evaluation is restricted to MES classification on the LIMUC and HyperKvasir datasets.

- Does integrating dynamic uncertainty estimation improve reliability modeling compared to the static BBPS-based proxy? The current method relies on a fixed cleanliness score from a lightweight BBPS-trained model, which may not capture complex annotation uncertainties.

- Are the reported performance gains robust to patient-level data splitting, or are they inflated by image-level leakage? High accuracy (e.g., 82.5%) might partially result from the model seeing different frames from the same procedure in training and validation sets.

## Limitations

- BBPS-based quality scores may not perfectly correlate with MES annotation reliability; direct evidence linking preparation quality to label noise is sparse
- Fixed τ = 0.5 threshold for data partitioning may not be optimal across datasets and requires calibration
- ResizeMix augmentation may distort scale-dependent lesions, potentially limiting applicability to other endoscopic grading tasks

## Confidence

- **High**: Curriculum learning improves robustness to label noise when quality scores are reliable
- **Medium**: BBPS scores serve as valid proxies for MES annotation reliability
- **Low**: ResizeMix augmentation is optimal for ordinal endoscopic classification

## Next Checks

1. Replicate quality classifier accuracy on held-out BBPS labels; if <70%, proxy signal may be too weak
2. Compare CLoE performance with and without ResizeMix to isolate augmentation contribution
3. Test curriculum learning with varying τ values (0.3, 0.5, 0.7) to assess threshold sensitivity