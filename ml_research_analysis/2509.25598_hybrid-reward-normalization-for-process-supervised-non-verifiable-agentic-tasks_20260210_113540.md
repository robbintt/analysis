---
ver: rpa2
title: Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks
arxiv_id: '2509.25598'
source_url: https://arxiv.org/abs/2509.25598
tags:
- reward
- process
- search
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Principle Process Reward (PPR), a reinforcement
  learning framework designed to address the challenge of training large language
  models on non-verifiable agentic tasks such as search-based question answering.
  The method combines principled process evaluation with outcome verification through
  a novel reward normalization strategy called ReNorm.
---

# Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks

## Quick Facts
- arXiv ID: 2509.25598
- Source URL: https://arxiv.org/abs/2509.25598
- Reference count: 29
- Key outcome: PPR achieves up to 28% relative improvement over non-RL baselines and 15% over previous RL methods on search-based QA benchmarks

## Executive Summary
This paper introduces Principle Process Reward (PPR), a reinforcement learning framework for training large language models on non-verifiable agentic tasks like search-based question answering. The method addresses the challenge of long-trajectory credit assignment by combining principled process evaluation with outcome verification through a novel reward normalization strategy called ReNorm. Experimental results demonstrate that PPR achieves state-of-the-art performance across diverse benchmarks while maintaining superior training stability compared to previous approaches.

## Method Summary
PPR fine-tunes Qwen2.5-3B/7B models using PPO with hybrid rewards from a principle-based process reward model (PPRM). The PPRM evaluates intermediate reasoning steps against interpretable criteria like correctness and relevance, while ReNorm calibrates and centers both process and outcome rewards to stabilize training. The method uses ~2k annotated search trajectories for PPRM fine-tuning from Qwen3-8B, with ReNorm normalization formula r_p,t = r̂_p,t + r_o - 1. Training is performed on NQ and HotpotQA with max_turns=2 (General QA) or 3 (Multi-Hop QA), evaluating every 50 steps.

## Key Results
- 28% relative improvement over non-RL baselines on NVProcessBench
- 15% improvement over previous RL methods on same benchmark
- Superior training stability with no reward collapse observed (vs baselines that decline by ~300 steps)

## Why This Works (Mechanism)

### Mechanism 1: Reward Normalization (ReNorm) for Stabilization
ReNorm calibrates rewards by broadcasting outcome reward to every step and centering: r = (r_p - μ) + (r_o - π). This enforces zero-mean rewards and sign consistency, reducing variance in GAE and preventing policy from exploiting local process "score inflation" that doesn't contribute to final answer. Assumes PPRM is weakly informative regarding final outcome (Cov(X,Y) > 0).

### Mechanism 2: Principle-Guided Reward Modeling (PPRM)
PPRM uses predefined principle set to generate context-specific rubrics before scoring, constraining output space and reducing formatting errors. Assumes specialized fine-tuning on high-quality trajectory-principle pairs is necessary for stable RL. Maintains near-perfect "Valid Judge Rate" while baseline generative models degrade significantly.

### Mechanism 3: Sign-Consistency for Credit Assignment
ReNorm formula ensures intermediate process rewards are penalized if final outcome is incorrect, anchoring credit assignment to trajectory success/failure. Assumes a step in failed trajectory is strictly less valuable than step in successful one. Increases margin between correct/incorrect class means.

## Foundational Learning

- **Proximal Policy Optimization (PPO) & GAE**: PPR uses PPO with GAE to optimize policy. Understanding variance/bias trade-offs in GAE is crucial as ReNorm claims to reduce variance in advantage estimation. *Quick check*: How does centering reward signal affect variance of advantage estimate A_t in GAE?

- **Process Reward Models (PRMs) vs. Outcome Reward Models (ORMs)**: PPR differentiates from "Search-R1" (ORM-based) by introducing PRMs. Understanding credit assignment problem in long horizons is essential to see why hybrid approach is needed. *Quick check*: Why do pure outcome rewards fail in long-trajectory agentic tasks compared to short-form QA?

- **Instruction Following & Role Drift in LLMs**: Paper identifies "role drift" where large models generate answers instead of scores. Understanding this failure mode explains design choice of training specialized PPRM. *Quick check*: Why might highly capable general LLM fail to act as reliable "Judge" compared to smaller, specialized model?

## Architecture Onboarding

- **Component map**: Policy Model (Qwen2.5-3B/7B) -> Search Engine -> PPRM (Qwen3-8B) -> ReNorm -> PPO Trainer
- **Critical path**: Rollout generation (Policy + Search) → PPRM Scoring (Per step) → Outcome Calculation (EM) → ReNorm (Hybridizing) → PPO Update
- **Design tradeoffs**: Stability vs. Speed (principle-based generation adds inference overhead but trades for stability); Strictness vs. Exploration (ReNorm's sign consistency might hinder exploration in low-success-rate domains)
- **Failure signatures**: Role Drift (Judge outputs reasoning instead of scores); Reward Collapse (training rewards spike then crash); Over-Search (agent generates redundant queries to maximize per-step rewards)
- **First 3 experiments**: 1) Sanity Check (Judge Validity): Measure "Valid Judge Rate" on held-out trajectories; 2) Ablation (Normalization): Train PPO agent using PPRM without ReNorm; 3) End-to-End (HotpotQA): Run full PPR pipeline on Multi-Hop QA

## Open Questions the Paper Calls Out

- **Open Question 1**: Can PPR framework effectively scale to more complex agentic tasks involving multiple tools beyond search-based question answering? [explicit] Current experiments restricted to search agents, leaving multi-tool interactions unexplored.

- **Open Question 2**: How sensitive is ReNorm strategy to violations of uniform distribution assumption (μ ≈ π ≈ 0.5)? [inferred] Simplified centering may fail to normalize rewards effectively if dataset has highly skewed success rate.

- **Open Question 3**: Is high "Valid Judge Rate" attributable to specific fine-tuning data or underlying Qwen3-8B architecture? [inferred] Unclear if reliability comes from principle-based data collection or if Qwen3-8B base is uniquely suited for structured generation task.

## Limitations
- PPRM quality depends heavily on annotation pipeline using dual-model approach with arbitrary 0.4 agreement threshold
- Principle definitions not fully specified, making it difficult to assess whether PPRM truly captures task-relevant process quality
- Sign consistency mechanism theoretically sound but lacks empirical validation showing specific instances of reward hacking in baseline approaches

## Confidence

| Claim | Confidence |
|-------|------------|
| PPR's superior performance on NVProcessBench (28% improvement) | High |
| ReNorm mechanism stabilizes long-trajectory RL training | Medium |
| PPRM improves judge validity compared to generic models | Medium |
| Sign consistency prevents "reward hacking" | Low |

## Next Checks

1. **PPRM Quality Validation**: Conduct human evaluation of PPRM scores on held-out trajectories to verify principle-based scoring aligns with expert judgment of process quality

2. **Sign Consistency Ablation**: Systematically test impact of removing sign consistency from ReNorm by training PPR variants with and without this constraint, measuring both performance and evidence of reward hacking behaviors

3. **Principle Coverage Analysis**: Analyze failure cases where PPR underperforms to determine if they stem from inadequate principle coverage for certain task types or reasoning patterns