---
ver: rpa2
title: 'Continual Learning, Not Training: Online Adaptation For Agents'
arxiv_id: '2511.01093'
source_url: https://arxiv.org/abs/2511.01093
tags:
- learning
- atlas
- task
- teacher
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces ATLAS, a dual-agent architecture that achieves
  gradient-free continual learning by shifting adaptation from model weights to system-level
  orchestration. The system decouples reasoning (Teacher) from execution (Student)
  and leverages a persistent learning memory to store distilled guidance, enabling
  real-time adaptation during inference.
---

# Continual Learning, Not Training: Online Adaptation For Agents

## Quick Facts
- **arXiv ID**: 2511.01093
- **Source URL**: https://arxiv.org/abs/2511.01093
- **Reference count**: 4
- **Primary result**: Dual-agent ATLAS achieves 54.1% task success with GPT-5-mini, outperforming GPT-5 (High) by 13% while reducing computational cost by 86%

## Executive Summary
This work introduces ATLAS, a dual-agent architecture that achieves gradient-free continual learning by shifting adaptation from model weights to system-level orchestration. The system decouples reasoning (Teacher) from execution (Student) and leverages a persistent learning memory to store distilled guidance, enabling real-time adaptation during inference. Evaluated on Microsoft’s ExCyTIn-Bench Incident #5, ATLAS achieves 54.1% task success with GPT-5-mini, outperforming the larger GPT-5 (High) by 13% while reducing computational cost by 86%. Efficiency gains are progressive: token consumption drops from 100,810 (tasks 1–25) to 67,002 (tasks 61–98) without sacrificing accuracy. Cross-incident validation shows zero-retraining generalization, improving accuracy from 28% to 41% (+46%) on Incident #55. The approach enables adaptive, deployable AI systems and generates causally annotated traces valuable for training world models.

## Method Summary
ATLAS implements gradient-free continual learning through a dual-agent architecture where a Teacher observes and guides a Student agent during task execution. The system operates in two phases: seeding and evaluation. During seeding, the Teacher monitors the Student's performance on tasks, providing corrective guidance that is distilled into structured pamphlets and stored in a Persistent Learning Memory (PLM) indexed by task context. In evaluation, the Student retrieves relevant pamphlets to inform future task execution without Teacher intervention. A two-tier ensemble-of-judges rewarder scores trajectories with explicit rationales, and the orchestrator manages supervision levels and plan selection at inference time. The approach treats model parameters as static, moving adaptation from training loops into inference-time orchestration.

## Key Results
- Achieves 54.1% task success rate on Incident #5, outperforming GPT-5 (High) by 13% while using 86% fewer tokens
- Demonstrates progressive efficiency gains with token consumption dropping from 100,810 to 67,002 across task phases
- Shows cross-incident generalization, improving accuracy from 28% to 41% on Incident #55 without retraining
- Generates causally annotated traces that could enhance world model training

## Why This Works (Mechanism)

### Mechanism 1: Distilled Experience Transfer (DET)
Storing principle-level guidance enables cross-task generalization without retraining. After Teacher intervention, a lightweight process extracts actionable rules indexed by task context. On subsequent similar tasks, semantic retrieval surfaces relevant pamphlets to seed the Student's initial plan, bypassing exploratory failure paths. Core assumption: Task contexts cluster meaningfully; guidance from one cluster transfers to others. Break condition: If task distribution has low intra-cluster similarity, retrieved pamphlets may mislead rather than help.

### Mechanism 2: Adaptive Teaching with Reward-Grounded Feedback
Real-time Teacher intervention with structured rationales improves efficiency without accuracy loss. Teacher observes Student trajectory, flags low-yield paths early, and provides principle-level corrective guidance. A two-tier ensemble-of-judges rewarder scores trajectories with explicit rationales, creating auditable supervision signals. Core assumption: The Teacher model is sufficiently capable to diagnose failure modes; the rewarder aligns with ground-truth task success. Break condition: If Teacher capability degrades on out-of-distribution tasks, guidance quality may drop, causing harmful interventions.

### Mechanism 3: Inference-Time Orchestration (No Gradient Updates)
Shifting adaptation from weights to orchestration enables immediate behavioral modification during deployment. The orchestrator retrieves PLM artifacts and dynamically adjusts supervision level and initial plan selection—all at inference time. No backpropagation, no training loops. Core assumption: The orchestration policy is the right abstraction layer for adaptation; model weights are sufficiently general. Break condition: If underlying model lacks base competency, orchestration cannot compensate for fundamental capability gaps.

## Foundational Learning

**Catastrophic Forgetting**
Why needed: ATLAS explicitly avoids this by never updating weights; understanding why traditional CL fails motivates the system-centric paradigm.
Quick check: Can you explain why gradient-based CL methods risk overwriting previously learned knowledge?

**Teacher-Student Distillation**
Why needed: The dual-agent architecture relies on this pattern; the Teacher generates guidance the Student consumes.
Quick check: What is the difference between knowledge distillation (training) and ATLAS's inference-time guidance?

**Pareto Frontier (Accuracy vs. Cost)**
Why needed: ATLAS claims to operate on the Pareto frontier—better accuracy at lower cost. Understanding this tradeoff is critical for interpreting results.
Quick check: If you increase supervision frequency, what happens to both accuracy and token cost?

## Architecture Onboarding

**Component map**: Task input -> Student execution -> Teacher observation -> Guidance generation -> Reward scoring -> Pamphlet distillation -> PLM storage -> Retrieval on next task

**Critical path**: Task input → Student execution → Teacher observation → Guidance generation → Reward scoring → Pamphlet distillation → PLM storage → Retrieval on next task

**Design tradeoffs**:
- Higher-capability Teacher = better guidance but higher per-task cost
- Aggressive pamphlet retrieval = faster adaptation but risk of overfitting to stale guidance
- More judges in reward ensemble = higher fidelity scores but increased latency

**Failure signatures**:
- Token consumption flat or increasing across phases → PLM retrieval not working or guidance not actionable
- Success rate drops with pamphlet injection → retrieved guidance is misleading (context mismatch)
- Cross-incident transfer fails → pamphlets encode task-specific templates, not abstract principles

**First 3 experiments**:
1. Ablate PLM retrieval: Run Student-only (no pamphlets) on same 98 tasks; compare token consumption and success rate to establish baseline
2. Cross-incident validation: Train on Incident #5, freeze PLM, evaluate on Incident #55 with Student-only mode to test generalization (paper reports 28%→41%)
3. Supervision-level sweep: Vary when Teacher intervenes (immediate vs. after first failure vs. never) and measure accuracy/cost tradeoff curve

## Open Questions the Paper Calls Out

**Open Question 1**
Can corrective strategies learned by one system be transferred or hierarchically combined across different architectures?
Basis: Section 7.1 asks if corrective strategies can be transferred or combined across different system designs.
Why unresolved: The study evaluates a specific dual-agent setup but does not test transfer to multi-agent ensembles or hierarchical structures.
What evidence would resolve it: Experiments showing that "pamphlets" distilled in one architectural configuration improve performance when applied to a structurally different agent system.

**Open Question 2**
Does Teacher-generated corrective feedback trained on one Student model accelerate adaptation when transferred to other agents?
Basis: Section 7.2 asks if feedback trained on one agent can accelerate adaptation in others.
Why unresolved: The experiments utilize a fixed Teacher-Student pair (GPT-5 / GPT-5-mini) without testing cross-model portability.
What evidence would resolve it: A study demonstrating that guidance distilled for Model A successfully reduces the learning curve of Model B without additional Teacher intervention.

**Open Question 3**
Do world models trained on ATLAS-generated causally annotated traces exhibit improved predictive fidelity over those trained on conventional trajectory data?
Basis: Section 3.3 hypothesizes improved fidelity and sample efficiency from using ATLAS traces for world model training.
Why unresolved: While the paper generates these traces, it does not actually train a world model on them to validate the hypothesis.
What evidence would resolve it: Comparative benchmarking of world models trained on ATLAS traces versus standard logs, showing superior predictive accuracy or faster convergence.

## Limitations
- Model access constraints: GPT-5 and GPT-5-mini are not publicly available, requiring proxy models that may not replicate the claimed performance
- Reward system specification gaps: The two-tier ensemble-of-judges architecture lacks details on judge model identities, variance thresholds, and arbiter routing logic
- Cross-incident generalization validation: Limited validation with only one cross-incident test and modest improvement raises questions about the mechanism for transferring abstract principles

## Confidence
- **High confidence**: The core dual-agent architecture concept (Teacher observes Student, provides guidance) is well-specified and mechanistically sound
- **Medium confidence**: The efficiency gains (86% reduction) and progressive adaptation (token drop from 100,810 to 67,002) are directly measured on the benchmark
- **Low confidence**: Cross-incident zero-shot transfer and the assertion that pamphlets capture abstract principles rather than templates are based on limited validation

## Next Checks
1. **Ablate pamphlet structure**: Run Student-only baseline on Incident #5 (paper reports 141K tokens, 40.4% success) to establish the no-adaptation floor. Compare against ATLAS with pamphlets to isolate the adaptation contribution.
2. **Test pamphlet generality**: On Incident #55, ablate pamphlet retrieval (Student-only) and compare to ATLAS with pamphlets to measure the 28%→41% improvement claimed. This validates whether the stored guidance transfers across incident contexts.
3. **Validate reward consistency**: Implement the two-tier ensemble-of-judges with explicit variance thresholds and arbiter logic. Measure inter-judge agreement and trajectory score stability across repeated runs to ensure the reward signal is reliable for learning.