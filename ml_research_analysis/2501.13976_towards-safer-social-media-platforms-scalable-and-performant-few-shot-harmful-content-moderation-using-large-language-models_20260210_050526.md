---
ver: rpa2
title: 'Towards Safer Social Media Platforms: Scalable and Performant Few-Shot Harmful
  Content Moderation Using Large Language Models'
arxiv_id: '2501.13976'
source_url: https://arxiv.org/abs/2501.13976
tags:
- content
- title
- classification
- harmful
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of scalable and dynamic content
  moderation on social media platforms by employing Large Language Models (LLMs) for
  few-shot harmful content detection. The core method involves using in-context learning
  with minimal supervision, where LLMs classify content as harmful or harmless based
  on a small set of exemplars provided within the prompt.
---

# Towards Safer Social Media Platforms: Scalable and Performant Few-Shot Harmful Content Moderation Using Large Language Models

## Quick Facts
- arXiv ID: 2501.13976
- Source URL: https://arxiv.org/abs/2501.13976
- Authors: Akash Bonagiri; Lucen Li; Rajvardhan Oak; Zeerak Babar; Magdalena Wojcieszak; Anshuman Chhabra
- Reference count: 27
- Primary result: LLMs achieve 78-80% accuracy on harmful content detection with as few as 8-14 examples, outperforming proprietary baselines

## Executive Summary
This paper presents a scalable approach for harmful content moderation on social media platforms using few-shot in-context learning with Large Language Models. The method achieves near-domain-expert accuracy (90.67%) using only 8-14 exemplars, outperforming state-of-the-art few-shot learning baselines. The approach demonstrates that even zero-shot LLM performance exceeds proprietary baselines like Perspective API and OpenAI Moderation API, with multimodal experiments showing mixed results depending on the model's capacity to process visual information.

## Method Summary
The core method employs in-context learning with minimal supervision, where LLMs classify content as harmful or harmless based on a small set of exemplars provided within the prompt. The study evaluates various LLM models across different settings: zero-shot, few-shot in-context learning (FS-ICL), and multimodal FS-ICL. Coverage-based exemplar selection using submodular optimization with semantic similarity metrics (BERTScore, Cosine, BM25) ensures maximally informative demonstrations. Multimodal approaches incorporate visual information through either caption generation or direct image input, with effectiveness contingent on the multimodal LLM's capacity.

## Key Results
- Zero-shot LLMs (Mistral-7B, Llama2-13B, GPT-4o-Mini, GPT-3.5-Turbo) achieve 65-70% accuracy, exceeding proprietary baselines (Perspective API 50%, OpenAI Moderation API 56%)
- FS-ICL with 12 shots and BERTScore selector achieves 78.57% accuracy with GPT-4o-Mini
- Multimodal FS-ICL with GPT-4o-Mini-DII achieves 80% accuracy in 14-shot configuration
- Open-source multimodal models (LLaVA, OpenFlamingo) perform at random-chance levels (~50%) due to insufficient multimodal reasoning capacity

## Why This Works (Mechanism)

### Mechanism 1: Coverage-Based Few-Shot In-Context Learning
- Claim: LLMs can classify harmful content with minimal supervision (8-14 examples) by selecting exemplars that maximize coverage of salient content aspects.
- Mechanism: Submodular optimization selects exemplars using semantic similarity metrics to cover diverse harm indicators, with exemplars ordered by relevance.
- Core assumption: Harmful content has a bounded set of salient aspects that can be sufficiently covered by a small exemplar set.
- Evidence anchors: [abstract] "With as few as 8-14 exemplars, their few-shot in-context learning approach achieves near-domain-expert accuracy (90.67%)", [section 3.3] "coverage-based ICL approaches... These methods ensure maximally informative demonstrations are selected by submodular optimization"
- Break condition: When concept drift introduces entirely new harm categories beyond exemplar coverage, or when salient aspects exceed available shot capacity.

### Mechanism 2: Model-Dependent Multimodal Signal Integration
- Claim: Visual information from video thumbnails can improve harm detection, but effectiveness is contingent on the multimodal LLM's capacity.
- Mechanism: Two pathways: (1) Caption Generation converts thumbnails to text descriptions for text-only LLMs; (2) Direct Image Input uses natively multimodal LLMs processing both modalities.
- Core assumption: Harmful content signals are distributed across modalities—some harms are visually salient but textually ambiguous.
- Evidence anchors: [abstract] "Multimodal experiments incorporating video thumbnails show mixed results - GPT-4o-Mini benefits significantly from visual input", [section 4.3] "GPT-4o-Mini... achieves an accuracy of 80%... However, this is not the case for LLaVa and OpenFlamingo"
- Break condition: When caption generation introduces errors that propagate downstream, or when smaller multimodal models lack cross-modal reasoning capacity.

### Mechanism 3: Zero-Shot Generalization from Pre-Training Distribution
- Claim: LLMs pre-trained on large corpora possess implicit understanding of harmful content concepts, enabling zero-shot performance exceeding specialized moderation APIs.
- Mechanism: Pre-training exposes LLMs to community guidelines, moderation discussions, and labeled harmful content examples, creating transferable knowledge activated by task-appropriate prompts.
- Core assumption: The pre-training distribution contains sufficient signal about harmful content patterns across diverse contexts.
- Evidence anchors: [abstract] "showing that even zero-shot LLM performance exceeds proprietary baselines like Perspective API and OpenAI Moderation API", [section 4.3] "Our ZSL approach... was able to achieve accuracies of 65% (Mistral-7B), 69% (Llama2-13B; GPT-4o-Mini), and 70% (GPT-3.5-Turbo)"
- Break condition: When encountering novel harm forms emerging after training cutoff, or when harmful expressions fall outside the pre-training distribution.

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: Core technique enabling few-shot adaptation without weight updates. Essential for understanding how prompt engineering alone achieves domain-expert-level performance.
  - Quick check question: Can you explain why ICL differs from fine-tuning, and what factors (example selection, ordering, quantity) influence ICL performance?

- Concept: Coverage-Based Example Selection
  - Why needed here: The paper specifically uses coverage-based selectors (BERTScore, Cosine, BM25) rather than random selection. Understanding coverage optimization is crucial for reproducing the 78-80% accuracy results.
  - Quick check question: How does submodular coverage optimization differ from simple nearest-neighbor retrieval, and why might coverage be better for multi-category harm detection?

- Concept: Multimodal LLM Architectures
  - Why needed here: The paper compares CG (caption-then-text) versus DII (native multimodal) approaches with drastically different performance profiles. Understanding these architectures is essential for system design decisions.
  - Quick check question: What are the trade-offs between caption generation pipelines versus natively multimodal models, and why do smaller multimodal models fail at this task?

## Architecture Onboarding

- Component map: Data Ingestion -> Exemplar Pool -> Prompt Constructor -> LLM Inference -> Output Parser
- Critical path: 1) Receive content sample (title ± thumbnail) 2) If multimodal CG: Generate thumbnail caption via BLIP 3) Retrieve k exemplars using coverage selector 4) Reorder exemplars by relevance 5) Construct model-specific prompt 6) Execute LLM inference 7) Parse binary classification output
- Design tradeoffs:
  - Open vs Closed-source: GPT models achieve 78-80% accuracy but require API calls; Mistral/Llama achieve 68-71% with local deployment
  - Text-only vs Multimodal: Text-only is faster (1-3s); multimodal adds 10-30s latency but may capture visually-salient harms
  - Shot count: 8-14 shots show minimal accuracy variance; higher shots increase token costs without proportional gains
  - Selection strategy: Imbalanced selection (coverage-based) outperforms balanced class selection by 1-3%
- Failure signatures:
  - Random-chance (~50%): OpenFlamingo/LLaVA on DII—indicates insufficient multimodal reasoning capacity
  - Caption-induced degradation: FS-ICL-CG underperforming FS-ICL (e.g., 78.13% → 73.7% for GPT-4o-Mini-BSR-14shot) signals error propagation from caption quality
  - Over-flagging: Perspective API's 98% recall with 50% precision indicates excessive false positives
  - Order sensitivity: Mistral/Llama performance drops with reordering while GPT benefits
- First 3 experiments:
  1. Establish zero-shot baseline on held-out test set with 2-3 LLMs to validate performance claims and compare against Perspective/OpenAI Moderation APIs
  2. Run FS-ICL shot scaling (k=4, 8, 12, 14) using BERTScore selector to identify optimal shot count for your content domain
  3. Compare selectors (BSR, Cosine, BM25) on validation set to determine best coverage metric for your harm taxonomy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can this few-shot moderation approach generalize effectively to low-resource languages where labeled training data is scarce?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that they "restrict our analyses to social media content in English, but it is important to extend these efforts to other languages."
- Why unresolved: It remains unclear if the semantic understanding required for "harm" detection in English transfers to languages with different syntactic structures or limited pre-training data.
- What evidence would resolve it: Experiments replicating the few-shot methodology on non-English social media datasets (e.g., Spanish or Hindi) to compare performance against English benchmarks.

### Open Question 2
- Question: Why do open-source multimodal models fail to utilize visual signals for harm classification while closed-source models benefit significantly?
- Basis in paper: [inferred] The results show GPT-4o-Mini accuracy increased with visual input (80%), whereas LLaVA and OpenFlamingo remained near random chance (50-54%), suggesting a capability gap.
- Why unresolved: The paper establishes that the choice of LLM is critical but does not isolate whether the failure in open-source models is due to poor visual grounding or lack of safety-specific fine-tuning.
- What evidence would resolve it: Ablation studies analyzing the attention mechanisms of open-source VLMs to determine if they successfully attend to harmful visual features in thumbnails.

### Open Question 3
- Question: Can multimodal inference latency be optimized to support real-time moderation without sacrificing the accuracy gains provided by visual context?
- Basis in paper: [explicit] The Limitations section notes that supplying multimodal inputs is computationally expensive, with inference times ranging between "10-30 seconds per test sample."
- Why unresolved: This latency is prohibitive for live content moderation on large-scale platforms, creating a trade-off between the demonstrated accuracy gains of FS-ICL-DII and scalability.
- What evidence would resolve it: Research demonstrating optimized inference pipelines (e.g., quantization or token caching) that reduce processing time to under one second while maintaining few-shot performance.

## Limitations
- Limited generalizability to non-English content, as the YouTube dataset used is English-only
- Multimodal effectiveness is highly model-dependent, with smaller open-source models performing at random-chance levels
- Coverage-based exemplar selection requires careful optimization and may not scale well to rapidly evolving harm categories

## Confidence
- High Confidence: Zero-shot LLM performance exceeding proprietary APIs (verified with multiple models achieving 65-70% vs. 50-56% for baselines)
- Medium Confidence: Few-shot accuracy claims (78.57% with GPT-4o-Mini-BSR-12shot) pending independent validation of exact exemplar selection methodology
- Medium Confidence: Multimodal improvements with GPT-4o-Mini (80% accuracy) but inconsistent results across models require further verification

## Next Checks
1. Test exemplar selection robustness by running ablation studies with random vs. coverage-based selection on held-out data to confirm the 1-3% accuracy advantage reported for coverage methods
2. Validate model-specific ordering effects by systematically testing exemplar ordering permutations across all LLM variants (GPT models vs. Mistral/Llama) to confirm the performance sensitivity described
3. Conduct cross-lingual evaluation using translated versions of the YouTube dataset to assess performance degradation and identify language-specific limitations