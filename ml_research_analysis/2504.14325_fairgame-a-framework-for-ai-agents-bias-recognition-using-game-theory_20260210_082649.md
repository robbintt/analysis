---
ver: rpa2
title: 'FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory'
arxiv_id: '2504.14325'
source_url: https://arxiv.org/abs/2504.14325
tags:
- agents
- game
- games
- each
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FAIRGAME is a framework that enables reproducible testing of multi-agent
  LLM interactions using game-theoretic models. It supports configurable agent personalities,
  languages, and payoff matrices to detect biases and inconsistencies in LLM behavior.
---

# FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory

## Quick Facts
- arXiv ID: 2504.14325
- Source URL: https://arxiv.org/abs/2504.14325
- Reference count: 40
- Primary result: FAIRGAME framework detects LLM biases and inconsistencies across languages and games using game-theoretic models

## Executive Summary
FAIRGAME is a framework designed to identify biases and inconsistencies in LLM strategic decision-making through game-theoretic simulations. The framework enables reproducible testing of multi-agent interactions by configuring agent personalities, languages, and payoff matrices. Experiments with four major LLMs across five languages in Prisoner's Dilemma and Battle of the Sexes scenarios revealed systematic deviations from game-theoretic predictions, with significant language-dependent cooperation patterns and sensitivity to payoff structures.

The framework addresses critical gaps in AI safety and governance by providing quantitative metrics to assess LLM behavior in strategic contexts. Results show that LLMs often rely on prior knowledge beyond explicit payoff matrices when selecting strategies, raising important questions about their decision-making processes. FAIRGAME is extensible to other games and communication models, offering a valuable tool for researchers studying AI alignment, bias detection, and autonomous system design.

## Method Summary
FAIRGAME implements a game-theoretic simulation environment where multiple LLM agents interact through configurable game scenarios. The framework allows researchers to define specific game rules, payoff matrices, and agent personalities, then observes how different LLMs respond to the same strategic situations. Agents are prompted to make decisions based on provided game descriptions, and their choices are recorded and analyzed for patterns of bias, inconsistency, and deviation from theoretical predictions.

The framework supports multiple languages and includes scoring metrics to quantify internal variability, cross-language inconsistency, and sensitivity to payoff structures. By systematically varying game parameters and agent configurations, researchers can identify how different LLMs approach strategic decision-making and whether their behavior aligns with established game-theoretic principles. The modular design enables easy extension to new games, communication models, and evaluation metrics.

## Key Results
- LLMs exhibit language-dependent cooperation levels, with cooperation rates varying significantly across different language contexts
- Four tested LLMs (GPT-4o, Claude 3.5, Mistral Large, Llama 3.1) consistently deviate from game-theoretic equilibrium predictions
- Internal variability and sensitivity to payoff structures are quantifiable, with some models showing higher consistency than others
- LLMs demonstrate strategic choices that suggest reliance on prior knowledge beyond explicit payoff matrix information

## Why This Works (Mechanism)
The framework leverages game theory's mathematical rigor to create controlled environments where LLM decision-making can be systematically evaluated. By presenting agents with well-defined strategic scenarios and payoff structures, FAIRGAME isolates the decision-making process from other contextual factors. The game-theoretic foundation provides clear predictions against which LLM behavior can be measured, enabling detection of biases that might not be apparent in open-ended tasks.

## Foundational Learning
- Game Theory Fundamentals: Understanding basic concepts like Nash equilibrium, dominant strategies, and payoff matrices is essential for interpreting results and designing experiments
- Multi-Agent Systems: Knowledge of agent interaction models and communication protocols helps in configuring realistic simulation scenarios
- LLM Prompt Engineering: Understanding how different prompt formulations affect model responses is crucial for consistent experimental design
- Statistical Analysis: Skills in hypothesis testing and confidence interval calculation are needed to validate observed patterns and ensure results are statistically significant

## Architecture Onboarding
Component Map: Game Configurator -> LLM Interface -> Decision Recorder -> Analysis Engine -> Result Visualizer
Critical Path: Define game parameters → Initialize LLM agents → Run simulation → Record decisions → Analyze patterns → Generate reports
Design Tradeoffs: Flexibility vs. reproducibility, complexity vs. interpretability, computational efficiency vs. comprehensive testing
Failure Signatures: Inconsistent agent responses, language-dependent biases, deviation from equilibrium predictions, high internal variability
First Experiments: 1) Run Prisoner's Dilemma with all four LLMs in English, 2) Test Battle of the Sexes across five languages, 3) Vary payoff structures to measure sensitivity

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Limited sample size of four LLMs and five languages may not capture full diversity of model behavior
- Focus on only two game-theoretic scenarios (Prisoner's Dilemma and Battle of the Sexes) restricts generalizability
- Does not account for temperature settings or prompt engineering variations that could influence strategic choices
- Hypothesis about prior knowledge usage is inferred but not directly empirically validated

## Confidence
- Medium confidence: LLMs deviate from game-theoretic predictions and show language-dependent cooperation patterns
- Medium confidence: Internal variability and payoff sensitivity are measurable and consistent across experiments
- Low confidence: LLMs rely on prior knowledge beyond payoff matrices (inferred but not directly tested)

## Next Checks
1. Expand study to include additional LLMs and underrepresented languages to test robustness of observed patterns
2. Implement experiments with additional game-theoretic scenarios (Stag Hunt, Chicken) to assess generalizability across strategic contexts
3. Conduct controlled tests with varying temperature settings and prompt engineering techniques to isolate influence of prior knowledge versus payoff matrix interpretation