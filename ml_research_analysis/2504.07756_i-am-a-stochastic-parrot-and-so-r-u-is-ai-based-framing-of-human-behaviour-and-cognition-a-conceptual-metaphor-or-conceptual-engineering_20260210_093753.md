---
ver: rpa2
title: '"i am a stochastic parrot, and so r u": Is AI-based framing of human behaviour
  and cognition a conceptual metaphor or conceptual engineering?'
arxiv_id: '2504.07756'
source_url: https://arxiv.org/abs/2504.07756
tags:
- conceptual
- concepts
- human
- metaphor
- engineering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# "i am a stochastic parrot, and so r u": Is AI-based framing of human behaviour and cognition a conceptual metaphor or conceptual engineering?

## Quick Facts
- arXiv ID: 2504.07756
- Source URL: https://arxiv.org/abs/2504.07756
- Reference count: 0
- Primary result: None

## Executive Summary
This paper analyzes whether AI-based framings of human behavior and cognition (like "stochastic parrots" or "CPU overload") constitute conceptual metaphors or conceptual engineering. The authors argue these framings are conceptual metaphors—unconscious projections from the computational domain onto the human domain—but are unaware of their own epistemological contingency and risk the map-territory fallacy. They identify a "double metaphor" problem where AI concepts were originally borrowed from human psychology, technically formalized, then reapplied to humans, obscuring their metaphorical origins. The paper advocates for conscious conceptual engineering instead of unreflective metaphor use.

## Method Summary
The authors employ philosophical argumentation and conceptual analysis rather than empirical methods. They draw on Wittgensteinian philosophy of language, conceptual metaphor theory (Lakoff & Johnson), and conceptual engineering literature to analyze AI-human framing cases. The analysis traces conceptual origins, evaluates epistemic success criteria from Kompa (2021), and distinguishes between unconscious conceptual metaphor use and deliberate conceptual engineering. The method involves theoretical reconstruction rather than data collection or model training.

## Key Results
- AI-based framings of human cognition are conceptual metaphors that project computational concepts onto human domains while suppressing differences
- These metaphors are "double metaphors" because AI concepts themselves were originally metaphorical borrowings from human psychology
- The widespread use of these metaphors risks mistaking models for reality and narrowing epistemic access to human cognition

## Why This Works (Mechanism)

### Mechanism 1: Conceptual Metaphor Projection (CM View)
- Claim: Framing human cognition in AI terms functions as conceptual metaphor—projecting the computational domain onto the human domain to highlight similarities while suppressing differences
- Evidence: [abstract] "We argue that they are conceptual metaphors, but that (1) this position is unaware of its own epistemological contingency"; [section 4] "This process abstracts because for metaphors to resonate and, ultimately, be epistemically successful, highlighted similarities are maximally salient and disanalogies have been minimised"
- Break condition: If target and source domains were proven isomorphic at the defining-property level, CM would collapse into literal description

### Mechanism 2: Map-Territory Fallacy (Conceptual Substitution)
- Claim: Prolonged metaphor use leads to mistaking the model for reality, gradually eroding awareness of distinctions
- Evidence: [section 4.2] "Over time, the pervasive use of metaphors can lead to the risk of mistaking models (the metaphors) for reality (the actual system)"; [section 4.2] "The danger lies precisely in metaphors' consistent suppression of differences and amplification of high-level similarities... 'we forget that they ever existed'"
- Break condition: If meta-linguistic awareness of metaphor use were systematically maintained, the fallacy would be interrupted

### Mechanism 3: Double Metaphor (Full-Circle Reversal)
- Claim: INTELLIGENCE IS ARTIFICIAL is a "double metaphor"—AI concepts originally borrowed from human cognition (layer 1: neural networks, learning) are reapplied back to humans (layer 2), obscuring their metaphorical origins
- Evidence: [abstract] "...it most importantly is a misleading 'double metaphor' because of the metaphorical connection between human psychology and computation"; [section 4.3] "Turing used cognitive terms like 'state of mind', seeing and remembering, but Turing machines do not demonstrate or possess cognitive abilities"
- Break condition: If computation were shown to be literally (not metaphorically) grounded in human cognition at the definitional level, the "double" characterization would dissolve

## Foundational Learning

- Concept: Conceptual Metaphor Theory (Lakoff & Johnson)
  - Why needed here: The paper's core analytical framework; you cannot evaluate CM vs CE without understanding how conceptual metaphors structure thought
  - Quick check question: Can you explain why "TIME IS MONEY" is a conceptual metaphor rather than a literal claim?

- Concept: Conceptual Engineering
  - Why needed here: The proposed alternative to unconscious metaphor; involves amelioration, introduction, or elimination of concepts
  - Quick check question: What's the difference between ameliorating a concept vs. eliminating it?

- Concept: Wittgensteinian Language-Games
  - Why needed here: The paper uses Wittgenstein to argue that concept-use is socio-culturally initiated and contingent; metaphors can "fossilize" into apparent certainties
  - Quick check question: Why does Wittgenstein reject the idea that concepts must have sharp boundaries to be usable?

## Architecture Onboarding

- Component map: CM View: Metaphor projection (source → target), emphasizing similarity, suppressing difference → CE View: Deliberate concept change (amelioration / introduction / elimination) → Double Metaphor Layer 1: Human cognition → computation formalization → Double Metaphor Layer 2: AI technical concepts → re-projection onto humans → Fossilization Process: Habitual use → forgetting perspectival origin → conceptual substitution

- Critical path: 1. Identify when AI concepts are applied to humans; 2. Classify as CM (default) vs. CE (if explicit methodology met); 3. Check for double-metaphor circularity (trace concept origin); 4. Evaluate epistemic success using Kompa's criteria (entities exist? inferential patterns preserved? disanalogies extent? obscures necessary properties?)

- Design tradeoffs:
  - CM: Fast, intuitive, highlights useful analogies; but risks map-territory fallacy and conceptual substitution
  - CE: Deliberate, can improve concepts; but requires ethical justification, risks distorting existing conceptual practices

- Failure signatures:
  - Asserting "humans ARE stochastic parrots" without acknowledging metaphor status
  - Treating computational models as if they capture defining (not contingent) properties of cognition
  - Extending concepts like "intelligence" to AI without semantic amelioration or explicit justification
  - Corporate/narrative-driven conceptual engineering that bypasses ethical evaluation

- First 3 experiments:
  1. Trace exercise: Pick an AI term applied to humans (e.g., "unsupervised learning" for child development). Trace its origin: was it borrowed from human concepts, technically charged, then reapplied? Map the double-metaphor path
  2. Kompa criteria test: Evaluate "INTELLIGENCE IS ARTIFICIAL" using the six criteria (entities exist? inferential patterns? disanalogies? theoretical merits? obscures properties? engenders new metaphors?). Document which criteria fail
  3. CE vs CM classification: For three examples from the paper (stochastic toddler, CPU overload, being programmed), explicitly argue whether each is unconscious CM or deliberate CE. Identify what would need to change for CE status

## Open Questions the Paper Calls Out

### Open Question 1
- Question: For whom is AI-inspired conceptual engineering useful, and how do non-philosophical commitments (such as corporate employment) influence the debate?
- Basis in paper: [explicit] Section 5.2 states, "it's questionable for whom this conceptual engineering in terms of AI is useful," noting that commitments like employment by BigTech import non-philosophical presuppositions
- Why unresolved: The authors identify this sociological bias but leave the specific analysis of stakeholders and power dynamics as an open challenge
- What evidence would resolve it: Sociological or empirical surveys mapping the adoption of AI metaphors against professional backgrounds and institutional incentives

### Open Question 2
- Question: What does conceptual change mean in terms of the ethical relations that morph along with it, particularly regarding inclusion in the moral circle?
- Basis in paper: [explicit] Section 5.2 asks, "What does the conceptual change mean in terms of the ethical relations that morph along with it?" and notes that widening "intelligence" affects "inclusion into the moral circle"
- Why unresolved: The paper poses this question to highlight the tension between engineering and ethics but does not provide a normative answer
- What evidence would resolve it: Experimental ethics research measuring how redefining cognitive concepts (e.g., "learning" or "intelligence") shifts human moral attribution toward AI systems

### Open Question 3
- Question: How can the "double metaphor" problem (the circular borrowing of concepts between human psychology and computation) be methodologically overcome to allow for valid conceptual engineering?
- Basis in paper: [inferred] Section 4.3 argues that the AI-human framing is a misleading "double metaphor," yet Section 5 claims it can offer "avenues" for conceptual engineering. The paper leaves unresolved how to use these metaphors for engineering without falling into the circularity trap
- Why unresolved: The paper establishes the circularity as a foundational limit (Turing machines modeling human calculation, then applied back to humans) but does not define a process to break this loop
- What evidence would resolve it: A methodological framework for conceptual engineering that successfully filters out the historical metaphorical baggage of computation when re-applying concepts to the human domain

## Limitations

- The analysis rests on philosophical argumentation rather than empirical data, creating key limitations in operationalizing when conceptual engineering is "warranted"
- The double-metaphor argument, while theoretically compelling, has weak empirical support in the corpus—no directly related papers explicitly address the circularity claim
- The distinction between legitimate computational abstraction and reductive elimination remains underspecified, potentially over-applying the critique to all computational modeling of cognition

## Confidence

- High confidence: The basic distinction between conceptual metaphor (unconscious projection) and conceptual engineering (deliberate concept change) is well-supported by the cited philosophical literature
- Medium confidence: The critique of metaphor-induced map-territory fallacy and the argument for conscious conceptual engineering as an alternative are philosophically sound but would benefit from more concrete operationalization
- Low confidence: The double-metaphor claim (that AI concepts are themselves metaphorical borrowings from human cognition) is the weakest link, with minimal corpus support

## Next Checks

1. Historical trace validation: Systematically trace three key AI concepts (neural networks, learning, intelligence) back through their philosophical and technical origins to verify the double-metaphor claim. Document when and how human cognitive concepts were formalized into computational abstractions.

2. Empirical metaphor analysis: Apply metaphor detection tools (like those used in the Medical Metaphors Corpus) to a corpus of AI-human comparison statements to quantify the prevalence of unconscious vs. deliberate framing and test whether metaphorical use correlates with epistemic limitations identified in the paper.

3. Ethical framework operationalization: Develop and test a decision procedure for when conceptual engineering is warranted, using the paper's ethical criteria (responsibility to language users, preservation of useful distinctions) to evaluate real cases of AI-human conceptual transfer.