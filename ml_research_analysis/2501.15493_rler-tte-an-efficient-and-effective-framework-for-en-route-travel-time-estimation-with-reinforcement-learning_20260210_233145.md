---
ver: rpa2
title: 'RLER-TTE: An Efficient and Effective Framework for En Route Travel Time Estimation
  with Reinforcement Learning'
arxiv_id: '2501.15493'
source_url: https://arxiv.org/abs/2501.15493
tags:
- data
- time
- travel
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RLER-TTE, a reinforcement learning-based
  framework for en route travel time estimation (ER-TTE) that addresses the inefficiency
  of existing methods which recompute travel times for every real-time request. The
  framework uses a Decision Maker module to intelligently determine whether to recalculate
  travel time using a complex predictor or reuse previous predictions, significantly
  reducing computational overhead.
---

# RLER-TTE: An Efficient and Effective Framework for En Route Travel Time Estimation with Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2501.15493
- **Source URL**: https://arxiv.org/abs/2501.15493
- **Reference count**: 40
- **Primary result**: RLER-TTE achieves state-of-the-art accuracy with up to 75% reduction in model utilization rate (MUR) for en route travel time estimation.

## Executive Summary
RLER-TTE introduces a reinforcement learning-based framework to address the computational inefficiency of existing en route travel time estimation (ER-TTE) methods that recompute travel times for every real-time request. The framework uses a Decision Maker module that intelligently determines whether to recalculate travel time using a complex predictor or reuse previous predictions from an Inference Memory. By combining reinforcement learning, contrastive learning, and curriculum learning, RLER-TTE significantly reduces computational overhead while maintaining or improving accuracy. Extensive experiments on three real-world datasets (Chengdu, Xi'an, Porto) demonstrate superior performance compared to state-of-the-art methods.

## Method Summary
RLER-TTE employs a reinforcement learning agent (Decision Maker) that sequentially decides whether to invoke a complex travel time predictor or reuse cached results based on real-time driving data. The framework uses Double DQN with attention-based encoders and contrastive learning to process both offline features (spatial, temporal, historical traffic) and online features (driving behavior, decision history). A curriculum learning strategy optimizes the Predictor's training by ordering samples from easy to hard trajectories. The system balances accuracy and efficiency through a composite reward function that penalizes computational overhead while rewarding prediction quality.

## Key Results
- Achieves state-of-the-art accuracy with up to 75% reduction in Model Utilization Rate compared to existing methods
- Demonstrates consistent performance across three real-world datasets (Chengdu, Xi'an, Porto)
- Ablation studies confirm contributions of contrastive learning and curriculum learning mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Selective invocation of the Predictor reduces computational overhead while maintaining accuracy
- **Mechanism**: A Double DQN agent evaluates a state composed of offline features (spatial, temporal, spatio-temporal) and online features (driving behavior, decision history). The agent selects between two actions: "re-prediction" (invoke the Estimation Model) or "direct lookup" (reuse Inference Memory). The reward function r = r_p + r_e + r_f(σ) balances prediction quality (r_p), computational penalty (r_e), and time-since-last-prediction (r_f) to optimize the efficiency-accuracy trade-off
- **Core assumption**: Newly observed online driving data is often uninformative, redundant, or misleading, so frequent re-prediction is unnecessary
- **Evidence anchors**: [abstract] describes the Decision Maker's role; [section 4.1] defines the MDP structure; corpus lacks external validation of this specific decision mechanism
- **Break condition**: Mis-specified reward weights (ω_p, α, β) may cause under-utilization (low MUR but poor accuracy) or over-utilization (high MUR, minimal efficiency gain)

### Mechanism 2
- **Claim**: Contrastive learning improves the Decision Maker's state representation by aligning online and offline feature encodings
- **Mechanism**: Separate attention-based encoders process offline features (H_X) and online features (H_F). The InfoNCE contrastive loss L_c maximizes similarity between positive pairs (same sample) and minimizes it for negative pairs (different samples). This alignment is integrated into the total loss L = L_td + λ·L_c, helping the agent discern whether observed behavior is consistent with expected patterns
- **Core assumption**: Aligned representations enable the Decision Maker to better evaluate if new online data provides actionable signal vs. noise
- **Evidence anchors**: [section 4.2] describes contrastive learning implementation; [section 6.3] ablation study shows MAPE degrades without contrastive loss; corpus lacks external validation
- **Break condition**: Distribution shifts between online and offline data may degrade alignment, causing poor state evaluation and erratic decisions

### Mechanism 3
- **Claim**: Curriculum learning accelerates Predictor training and improves final accuracy by ordering samples from easy to hard
- **Mechanism**: The Trajectory Difficulty Measurer partitions data by route length and traveled proportion into N subsets × M metasets. Difficulty is scored as μ_i = MAE(M_e(x_i), y_i) + MAPE(M_e(x_i), y_i) using an expert model trained on random data. The Training Scheduler starts with moderate-difficulty metasets and gradually adds neighbors, creating a curriculum that the Predictor follows
- **Core assumption**: Prediction difficulty correlates with measurable trajectory properties, and learning easy samples first improves generalization to harder ones
- **Evidence anchors**: [section 5.1] defines difficulty scoring and partitioning; [section 6.4] ablation study shows higher MAPE without curriculum; corpus lacks external validation
- **Break condition**: Noisy difficulty scores from poorly calibrated expert model may introduce unhelpful or detrimental training order

## Foundational Learning

### Concept: Markov Decision Process (MDP) for Sequential Decisions
- **Why needed here**: The Decision Maker must decide sequentially when to re-predict based on evolving state; MDP provides the formal framework (state, action, reward, transition) for RL training
- **Quick check question**: What constitutes the state s_t in this MDP, and why does it include both offline and online features?

### Concept: Double Deep Q-Network (Double DQN)
- **Why needed here**: Standard DQN tends to overestimate action values; Double DQN decouples action selection and evaluation, improving stability in the dynamic, non-stationary traffic environment
- **Quick check question**: How does Double DQN compute the target Q-value differently from vanilla DQN?

### Concept: Curriculum Learning for Training Data Ordering
- **Why needed here**: End-to-end training generates many varied samples; curriculum learning organizes them to avoid slow convergence and poor local minima
- **Quick check question**: How does the Trajectory Difficulty Measurer define "difficulty," and what metrics does it use?

## Architecture Onboarding

### Component Map
1. Offline Feature Extraction → 2. Online Feature Extraction → 3. Decision Maker (Double DQN with attention encoders and contrastive loss) → 4. Predictor (Estimation Model) or 5. Inference Memory → 6. Reward computation → 7. Training updates

### Critical Path
1. Feature extraction (offline pre-compute + online real-time) → construct state s_t
2. Decision Maker processes s_t → selects action a_t (re-predict or direct lookup)
3. Predictor executes a_t → returns Ŷ_remain from Estimation Model or Inference Memory
4. Reward computation (using Equations 1-4) → store transition in Replay Memory
5. Training: TD loss + contrastive loss update Decision Maker; curriculum-scheduled batches update Predictor

### Design Tradeoffs
- **Efficiency vs. accuracy**: Penalty factor ω_p controls MUR; higher ω_p reduces invocations but risks accuracy
- **Curriculum granularity**: More metasets (higher N, M) improve resolution but increase scheduling complexity
- **Contrastive loss weight λ**: Higher λ strengthens alignment but may overshadow TD loss

### Failure Signatures
- **High MUR, low accuracy gain**: Decision Maker over-invokes Predictor; check reward weights or state features
- **Slow Predictor convergence**: Curriculum difficulty scoring may be misaligned; validate expert model calibration
- **Stale predictions returned**: Inference Memory not updated sufficiently; review frequency reward or action stability

### First 3 Experiments
1. **Baseline comparison**: Run RLER-TTE vs. MetaER-TTE/SSML on test sets; measure MAPE and MUR to validate efficiency-accuracy claims
2. **Contrastive loss ablation**: Train with λ=0 (no contrastive loss); compare MAPE and MUR to full model
3. **Curriculum vs. random training**: Train Predictor with and without curriculum learning; compare convergence epochs and final MAPE

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the Decision Maker agent effectively transfer to a new city or road network in a zero-shot manner without retraining?
- **Basis in paper**: [inferred] The model is trained and evaluated on specific datasets (Chengdu, Xi'an, Porto) with distinct road network structures. The framework relies on learning spatio-temporal embeddings specific to these road graphs, leaving cross-city generalization unexplored
- **Why unresolved**: The learned policy is bound to the state space of the training road network; it is unclear if the agent can interpret the geometric and traffic features of a completely unseen graph
- **What evidence would resolve it**: An evaluation of a pre-trained Decision Maker on a target city's dataset without further gradient updates, measuring the degradation in Model Utilization Rate (MUR) and accuracy (MAPE)

### Open Question 2
- **Question**: How robust is the "Direct Lookup" strategy against sudden, extreme traffic anomalies (e.g., accidents) that occur immediately after a prediction request?
- **Basis in paper**: [inferred] The framework optimizes for efficiency by reusing results, and the frequency reward (Eq. 3) discourages continuous re-prediction. This implies a latency in reacting to abrupt changes that occur between requests, which the paper does not explicitly address in the ablation studies
- **Why unresolved**: The "frequency reward" encourages the agent to wait before recalculating; in a sudden collision scenario, this delay could lead to significant temporary estimation errors before the next re-prediction is triggered
- **What evidence would resolve it**: A case study or stress test simulating "shock" traffic events (sudden speed drops) to measure the time-lag/error-peak before the Decision Maker successfully triggers a re-prediction

### Open Question 3
- **Question**: How sensitive is the reinforcement learning stability to the specific weighting of the composite reward function (r = r_p + r_e + r_f) in highly volatile traffic conditions?
- **Basis in paper**: [inferred] The paper manually tunes hyperparameters like ω_p (penalty factor) and λ (loss weight) on validation sets (Section 6.2). The reliance on fixed penalty weights suggests the model may struggle to auto-adjust the efficiency-accuracy trade-off if traffic volatility changes drastically from the validation set
- **Why unresolved**: Reinforcement learning reward shaping is notoriously unstable; a fixed ω_p might be too lenient in chaotic traffic (causing error) or too strict in stable traffic (causing wasted compute)
- **What evidence would resolve it**: A sensitivity analysis sweeping the values of ω_p and α in distinct traffic regimes (e.g., rush hour vs. late night) to observe variance in convergence and MUR

## Limitations
- Specific numerical values for reward function weights (ω_p, α, β) are not disclosed, making exact reproduction challenging
- The "shuffled-expert" model used for curriculum learning difficulty scoring lacks full architectural specification
- External validation of proposed mechanisms (RL-based selective invocation, contrastive learning, curriculum learning) is limited as related works don't discuss these exact approaches for ER-TTE

## Confidence
- **High confidence** in overall framework design and reported efficiency gains (up to 75% MUR reduction) based on ablation studies and comparisons
- **Medium confidence** in effectiveness of contrastive learning and curriculum learning mechanisms, supported by internal ablation studies but lacking external validation
- **Low confidence** in generalizability of specific reward function weights and curriculum scheduling parameters, as these are not fully disclosed

## Next Checks
1. **Reproduce efficiency-accuracy trade-off**: Train RLER-TTE with provided methodology and compare MUR and MAPE against MetaER-TTE/SSML on three datasets to verify claimed 75% reduction in model utilization
2. **Validate contrastive learning contribution**: Train RLER-TTE variants with and without contrastive loss (λ=0) to confirm impact on MAPE and MUR as suggested by ablation study
3. **Assess curriculum learning impact**: Compare Predictor training with and without curriculum learning (NU variant) to verify improvements in convergence speed and final MAPE