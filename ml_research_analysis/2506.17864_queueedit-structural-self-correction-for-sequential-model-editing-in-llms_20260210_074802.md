---
ver: rpa2
title: 'QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs'
arxiv_id: '2506.17864'
source_url: https://arxiv.org/abs/2506.17864
tags:
- editing
- parameters
- knowledge
- llms
- queue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QueueEDIT addresses the challenge of maintaining LLM performance
  during sequential model editing (SME), where continuous corrections can degrade
  general capabilities due to parameter bias. The core idea is to use a queue-based
  self-correction framework that maps knowledge triplets to specific transformer neurons,
  stores edited parameters in a queue, and dynamically aligns previously edited parameters
  through a structural mapping editing loss.
---

# QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs

## Quick Facts
- **arXiv ID:** 2506.17864
- **Source URL:** https://arxiv.org/abs/2506.17864
- **Reference count:** 20
- **Primary result:** QueueEDIT maintains both editing accuracy and general NLP capabilities across 1000 sequential edits by mapping knowledge triplets to specific transformer neurons and dynamically aligning previously edited parameters.

## Executive Summary
QueueEDIT addresses the challenge of maintaining LLM performance during sequential model editing (SME), where continuous corrections can degrade general capabilities due to parameter bias. The core idea is to use a queue-based self-correction framework that maps knowledge triplets to specific transformer neurons, stores edited parameters in a queue, and dynamically aligns previously edited parameters through a structural mapping editing loss. This enables the model to preserve both editing accuracy and general NLP capabilities across long editing sequences. Experiments show QueueEDIT significantly outperforms strong baselines on three benchmarks (ZSRE, CounterFact, RIPE) under 1000 sequential edits, achieving average accuracies of 72.4% (Rel.), 65.5% (Gen.), and 50.6% (Loc.) on GPT-J, and 69.1% (Rel.), 61.8% (Gen.), and 61.2% (Loc.) on LLaMA3, while maintaining better general capability consistency than competing methods.

## Method Summary
QueueEDIT uses a queue-based self-correction framework for sequential model editing in LLMs. The method maps knowledge triplets $(s, r, o)$ to specific MLP components, storing edited parameters in a FIFO queue and dynamically aligning previously edited parameters through a structural mapping editing loss. The framework employs causal tracing to locate factual knowledge in FFN layers, applies a translation-based loss function for structural editing, and uses Euclidean distance to select and align Top-K semantically related parameters from the queue. The system also freezes irrelevant parameters and dequeues old parameters to preserve general capabilities.

## Key Results
- Achieves average accuracy of 72.4% (Rel.), 65.5% (Gen.), and 50.6% (Loc.) on GPT-J under 1000 sequential edits
- Achieves average accuracy of 69.1% (Rel.), 61.8% (Gen.), and 61.2% (Loc.) on LLaMA3 under 1000 sequential edits
- Maintains better general capability consistency than competing methods (ROME, MEMIT) as edits increase

## Why This Works (Mechanism)

### Mechanism 1: Structural Triplet Mapping in FFN Layers
- **Claim:** Mapping knowledge triplets $(s, r, o)$ structurally to specific MLP components enhances knowledge localization compared to mixing subject and relation representations.
- **Mechanism:** The authors separate the editing process: the "subject" maps to the first MLP matrix ($W_{fc}$) keys, the "relation" maps to the second MLP matrix ($W_{proj}$), and the "object" is the optimization target. This uses a translation-based loss ($L_{st} = ||k_s^* W_{proj} + h_r - v^*||^2$) to approximate the object representation via subject and relation embeddings.
- **Core assumption:** Knowledge facts are stored in a translational geometric structure within the FFN layers, such that $s + r \approx o$.
- **Evidence anchors:** Section 3.2 "Structural Triplet Editing" and Eq. 11 define the structural loss; Figure 2(a) visualizes the mapping of $s, r, o$ to distinct FFN components.
- **Break condition:** If the LLM's knowledge storage does not follow a linear translational structure (e.g., complex non-linear storage), this loss function may fail to converge or corrupt unrelated weights.

### Mechanism 2: Queue-Based Dynamic Parameter Alignment
- **Claim:** Maintaining a FIFO queue of edited parameters allows for dynamic realignment of semantically related previous edits, mitigating "knowledge drift."
- **Mechanism:** After locating parameters for the current edit ($W_t$), the system calculates Euclidean distance to parameters stored in the queue ($W_i$). It selects the Top-K most similar parameters and updates them using the current gradient information ($v_t^* \oplus h_r^i$), effectively linking the new object to old relations.
- **Core assumption:** Semantic correlation between facts corresponds to geometric proximity in the parameter space (Euclidean distance), and updating "neighbors" preserves logical consistency (e.g., updating "President" implies updating "President's Wife").
- **Evidence anchors:** Abstract states "select queue parameters most relevant to the currently located parameters to determine whether previous knowledge needs realignment"; Section 3.3, Eq. 12-15 detail the Top-K selection and update rule.
- **Break condition:** If semantic similarity does not correlate with parameter distance, the Top-K selection will update irrelevant parameters, degrading model performance (catastrophic forgetting).

### Mechanism 3: Selective Preservation via Freezing and Dequeueing
- **Claim:** Freezing irrelevant parameters and dequeueing old parameters preserves general capabilities by limiting the scope of parameter updates.
- **Mechanism:** Parameters in the queue not selected as Top-K are frozen. The oldest parameter ($W_{head}$) is removed if its similarity to the current edit falls below a threshold ($\eta_{deq}$), ensuring the model doesn't retain outdated or conflicting knowledge indefinitely.
- **Core assumption:** "Freshness" of knowledge implies utility; older, dissimilar edits are less critical to maintain consistency with current edits.
- **Evidence anchors:** Section 3.3, Step 4 "Dequeuing Located Parameters" (Eq. 16); Figure 3 shows QueueEDIT maintaining general capability (Acc %) better than baselines as edits increase.
- **Break condition:** Aggressive dequeueing (low threshold) might cause the model to "forget" valid long-term dependencies that are not recently accessed.

## Foundational Learning

- **Concept: Locate-and-Edit (ROME/MEMIT)**
  - **Why needed here:** QueueEDIT builds upon the assumption that factual knowledge is stored in specific MLP layers (FFNs) and can be located via causal tracing. Understanding ROME is a prerequisite to understanding the baseline structure being optimized.
  - **Quick check question:** Can you identify which layer in a Transformer typically acts as the "key-value" store for factual associations?

- **Concept: Translation-based Knowledge Graph Embeddings (TransE)**
  - **Why needed here:** The paper explicitly adopts the translation principle ($s + r \approx o$) for its structural loss function.
  - **Quick check question:** How does the loss function change if you treat the relation as a translation vector versus treating it as a simple concatenation?

- **Concept: Catastrophic Forgetting in Sequential Learning**
  - **Why needed here:** The core problem definition (SME) is the degradation of performance over sequential updates.
  - **Quick check question:** Why does updating parameters for a new fact potentially destroy performance on an unrelated previously learned fact?

## Architecture Onboarding

- **Component map:** Input Triplet $(s, r, o)$ -> Locator (causal tracing) -> Queue Manager (FIFO buffer) -> Similarity Engine (Euclidean distance) -> Updater (structural loss + Top-K alignment)
- **Critical path:** The **Top-K Selection** (Section 3.3, Step 2). If the distance calculation is inefficient or the threshold $\eta_{que}$ is poorly tuned, the system either fails to align dependent facts (low generality) or corrupts unrelated facts (low locality).
- **Design tradeoffs:**
  - **Queue Length (Memory vs. Consistency):** Table 2 shows 30% queue length balances memory (21.8GB) and performance (69.1 Avg); 50% increases memory significantly with lower returns.
  - **Top-K value:** Selecting too many parameters for alignment risks "semantic noise" and destabilizes the model.
- **Failure signatures:**
  - **Low Locality (<80%):** Likely caused by selecting irrelevant parameters for alignment (Top-K threshold too loose) or high learning rate on $W_{proj}$.
  - **Semantic Drift:** (e.g., answering "Jill Biden" when "Trump" is President) indicates the Queue failed to identify the dependency between "President" and "President's Wife" due to high Euclidean distance in the parameter space.
- **First 3 experiments:**
  1. **Hyperparameter Sweep (Queue Length):** Run Table 2 experiment on ZSRE with lengths 10%, 30%, 50% to verify the memory/performance trade-off on your specific hardware.
  2. **Ablation Study (Structure vs. Queue):** Replicate Table 3. Run "w/o $L_{st}$" to confirm structural mapping contribution, then "w/o Queue" to confirm the dependency modeling.
  3. **Long-Sequence Degradation Test:** Run 1000 sequential edits (Figure 3) measuring "General Capability" (MMLU/CSQA) to verify that QueueEDIT avoids the performance drop seen in ROME/MEMIT.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does QueueEDIT performance scale in "lifelong" editing scenarios involving tens of thousands of edits, beyond the 1,000-edit limit tested?
- **Basis in paper:** The methodology (Appendix A.3) sets a maximum of 1,000 sequential edits ($T_{max}=1000$), leaving the long-term effects of continuous queue cycling on knowledge retention unexplored.
- **Why unresolved:** The FIFO mechanism ejects the oldest parameters to maintain freshness, but it is unclear if this leads to catastrophic forgetting of early edits in a true lifelong setting.
- **What evidence would resolve it:** Evaluation results on a continuous edit stream of 10,000+ samples, tracking the retention rate of facts ejected from the queue versus those retained.

### Open Question 2
- **Question:** Is the choice of Euclidean distance ($d_i$) for parameter similarity optimal for determining semantic correlation in the Top-K selection process?
- **Basis in paper:** Section 3.3 uses Euclidean distance to calculate similarity between parameter matrices ($W_{proj}$) to select Top-K elements for realignment.
- **Why unresolved:** Parameter magnitude differences might skew distance metrics, potentially failing to capture deeper semantic alignments necessary for effective self-correction.
- **What evidence would resolve it:** Ablation studies comparing Euclidean distance against cosine similarity or learned metric functions within the Top-K selection step.

### Open Question 3
- **Question:** Can the queue length and threshold hyperparameters ($\eta_{que}, \eta_{deq}$) be adaptively scaled to prevent the performance drops seen with fixed, longer queues?
- **Basis in paper:** Section 4.3 (Queue Length analysis) notes that "longer queue lengths lead to less consistent editing performance, possibly due to the introduction of more irrelevant knowledge parameters," yet the framework currently relies on fixed lengths and thresholds.
- **Why unresolved:** A fixed queue size may struggle to balance the trade-off between capturing long-sequence dependencies and minimizing semantic noise as the editing sequence evolves.
- **What evidence would resolve it:** Implementation of a dynamic queue sizing mechanism that adjusts based on the current density or variance of stored parameters.

## Limitations
- The structural triplet mapping relies on the assumption that factual knowledge follows a translational geometry in MLP layers, which may not generalize to all LLM architectures or knowledge types.
- Queue-based alignment assumes semantic similarity correlates with parameter distance, but this correlation may break down for distantly related facts or in multilingual settings.
- The dequeueing mechanism assumes "freshness" implies utility, but this could prematurely discard valid long-term dependencies that aren't recently accessed.

## Confidence
- **High confidence:** The experimental methodology and evaluation framework are well-specified, with clear benchmarks (ZSRE, CounterFact, RIPE) and comprehensive metrics (Reliability, Generality, Locality).
- **Medium confidence:** The core mechanisms (structural mapping, queue-based alignment) are theoretically sound but rely on empirical correlations (parameter distance ↔ semantic similarity) that may not generalize universally.
- **Low confidence:** The exact hyperparameter values (α1, α2 loss coefficients, v* computation details) are underspecified, making exact reproduction challenging without additional experimentation.

## Next Checks
1. **Ablation study:** Replicate Table 3 to confirm structural mapping (w/o Lst) and queue dependency modeling (w/o Queue) contributions separately on your target LLM architecture.
2. **Semantic distance correlation test:** For a subset of edits, log Top-K parameter indices and manually verify whether the selected parameters actually correspond to semantically related facts (e.g., "President" edits should select "President's Wife" parameters).
3. **Long-sequence stress test:** Run 2000+ sequential edits measuring all three metrics plus general capability (MMLU/CSQA) to verify QueueEDIT's stability compared to baselines under extreme conditions.