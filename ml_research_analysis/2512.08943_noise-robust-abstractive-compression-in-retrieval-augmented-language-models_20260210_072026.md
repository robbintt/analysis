---
ver: rpa2
title: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models
arxiv_id: '2512.08943'
source_url: https://arxiv.org/abs/2512.08943
tags:
- documents
- noise
- evidential
- information
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of information loss in abstractive
  compression for retrieval-augmented language models, particularly when retrieved
  documents contain noise such as irrelevant information or factual errors. The proposed
  method, ACoRN (Abstractive Compression Robust against Noise), introduces two key
  innovations: (1) offline data augmentation to train the compressor against two types
  of retrieval noise (irrelevant documents and factual error documents), and (2) fine-tuning
  to generate summaries focused on evidential documents that directly support correct
  answers, thereby reducing positional bias.'
---

# Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models

## Quick Facts
- arXiv ID: 2512.08943
- Source URL: https://arxiv.org/abs/2512.08943
- Authors: Singon Kim
- Reference count: 0
- Primary result: ACoRN improves EM and F1 scores on ODQA benchmarks while preserving answer strings and reducing inference time

## Executive Summary
This paper addresses information loss in abstractive compression for retrieval-augmented language models when retrieved documents contain noise. The proposed ACoRN method introduces offline data augmentation to train compressors against irrelevant and factual error documents, and fine-tuning to generate summaries focused on evidential documents that directly support correct answers. Experimental results show T5-large trained with ACoRN improves Exact Match and F1 scores on NQ, TriviaQA, and PopQA benchmarks compared to baseline compression methods while preserving answer strings and reducing inference time.

## Method Summary
ACoRN trains a compressor to generate summaries from noisy retrieved documents by using evidential-only documents to generate teacher labels and offline augmentation to create synthetic factual error documents. The method identifies evidential documents containing the answer string, uses GPT-3.5-turbo to generate pseudo-labels from only evidential documents, and augments training data by replacing answer entities with incorrect ones using RoBERTa-large. The T5-large compressor learns to reproduce the teacher's clean summary from noisy inputs, improving robustness against both irrelevant and factually incorrect documents while preserving the answer string for downstream generation.

## Key Results
- ACoRN improves EM and F1 scores on NQ, TriviaQA, and PopQA benchmarks compared to baseline compression methods
- The method preserves answer strings while reducing inference time through compression
- ACoRN shows particular effectiveness on datasets with high noise-document ratios
- Performance gains are achieved while maintaining or improving exact answer matching

## Why This Works (Mechanism)

### Mechanism 1: Evidential-Only Knowledge Distillation
The method constructs training targets by feeding only documents containing the answer string to a Teacher LLM (GPT-3.5-turbo), while the Student compressor (T5-large) learns to map from noisy full context to clean summary. This prevents attention dispersion when summarizing over noisy inputs. The core assumption is that the Teacher generates high-quality summaries when isolated from noise, and the Student can learn the mapping via sequence-to-sequence loss. Break condition: If no documents contain the answer string, the compression advantage is nullified.

### Mechanism 2: Synthetic Factual-Error Injection
Training data is augmented by taking evidential documents, masking answer entities, and using RoBERTa-large to fill with incorrect entities, creating "Factual Error" documents. This improves the compressor's ability to distinguish between high-relevance and high-factual-accuracy content. The assumption is that exposing the model to synthesized factual contradictions forces it to learn verification behaviors. Break condition: If errors are syntactically obvious, the model may overfit to low-level artifacts rather than semantic contradiction detection.

### Mechanism 3: String-Preservation as a Proxy for Fidelity
The system classifies documents as "Evidential" if they contain the answer string and optimizes for answer string preservation in compressed outputs. This serves as a heuristic guardrail against hallucination and excessive abstraction. The core assumption is that retaining the literal answer token is most critical for the downstream generator to produce correct Exact Match. Break condition: In multi-hop reasoning tasks, the "answer string" may not exist explicitly in any single document, making the evidential definition and PAR metric less effective.

## Foundational Learning

- **Concept**: "Lost in the Middle" Phenomenon
  - **Why needed here**: The paper identifies "positional bias" as a reason abstractive compressors fail; relevant info in the middle of long contexts is often omitted
  - **Quick check question**: Does your compressor attend equally to tokens at index 10 and index 1000, or does attention distribution spike at start/end?

- **Concept**: Knowledge Distillation (Teacher-Student)
  - **Why needed here**: ACoRN relies on a capable Teacher (GPT-3.5) to define the "ideal" summary which the smaller Student (T5-large) learns to imitate
  - **Quick check question**: Can a smaller model effectively mimic the reasoning of a larger model purely through token-level log-likelihood loss?

- **Concept**: Noise Types in RAG (Irrelevant vs. Factual Error)
  - **Why needed here**: ACoRN treats "irrelevant" (off-topic) and "factual error" (on-topic but wrong) as distinct classes requiring different augmentation strategies
  - **Quick check question**: Is the retrieved document wrong because it talks about apples when you asked for oranges, or because it claims apples are blue?

## Architecture Onboarding

- **Component map**: Retriever (DPR) -> Data Augmentor (RoBERTa-large) -> Label Generator (GPT-3.5) -> Compressor (T5-large) -> Generator (LLaMA-3.1)
- **Critical path**: The quality of the Compressor training data. If the Augmentor creates unrealistic noise or the Label Generator hallucinates in its summary, the Compressor learns to compress garbage into clean garbage
- **Design tradeoffs**:
  - *Performance vs. Cost*: ACoRN requires an expensive offline run of GPT-3.5 on the training set (scales with dataset size)
  - *Recall vs. Precision*: By focusing on "answer strings," the compressor might strip away nuance or context required for "how" or "why" questions that aren't factoid-based
- **Failure signatures**:
  - **Drop in EM score**: Likely indicates the Compressor is failing to preserve the answer string (Low PAR)
  - **Fabrication**: The Compressor might combine entities from a "Factual Error" doc and an "Evidential" doc, creating a new, plausible-sounding but incorrect statement
- **First 3 experiments**:
  1. **Baseline Check**: Run `Top-5 retrieval -> LLaMA` vs. `Top-5 -> ACoRN Compressor -> LLaMA` on NQ/TriviaQA to verify compression maintains/improves EM scores
  2. **Noise Ablation**: Test the model on a dataset where you manually inject "Factual Error" documents to see if performance holds compared to baseline
  3. **PAR Metric Validation**: Measure the "Preserving Answer string Ratio" on the output of the compressor. If it is low (<0.5), the distillation step is likely failing

## Open Questions the Paper Calls Out
None

## Limitations
- The method relies on exact string matching to identify evidential documents, which breaks down for multi-hop reasoning where answers aren't explicitly present
- Synthetic factual error injection may create grammatically coherent but implausible contradictions that don't generalize to real-world noise
- Performance critically depends on GPT-3.5-turbo's ability to generate high-quality summaries from clean contexts, with no validation mechanism
- The method addresses only irrelevant documents and factual errors, not other common RAG noise types like contradictory evidence or outdated information
- Assumes each question has one correct answer string, not generalizing well to multi-answer or subjective questions

## Confidence

**High Confidence**: The core mechanism of using evidential-only documents for teacher summary generation is well-supported by literature on RAG noise and distillation. Experimental results showing EM/F1 improvements are directly measured and verifiable.

**Medium Confidence**: The synthetic factual error augmentation strategy is novel and theoretically sound, but the actual learning benefit depends heavily on the quality and diversity of generated errors. The claim that this improves robustness against factual errors requires further validation on real-world noisy datasets.

**Low Confidence**: The assumption that PAR (Preserving Answer string Ratio) is a sufficient proxy for summary quality. While preserving the answer string is important for factoid QA, this metric doesn't capture semantic understanding, handling of multi-hop reasoning, or generation of explanations.

## Next Checks

1. **Multi-Hop Reasoning Stress Test**: Evaluate ACoRN on datasets like HotpotQA where answers require combining information from multiple documents. Measure whether PAR and EM scores degrade compared to factoid datasets, and whether the compressor can handle cases where no single document contains the complete answer string.

2. **Real-World Noise Evaluation**: Test the compressor on a dataset with naturally occurring noise (e.g., news articles with dated information, conflicting reports) rather than synthetic noise. Compare performance degradation against baseline methods to validate the synthetic augmentation approach.

3. **Teacher Model Ablation Study**: Train a version of ACoRN where the teacher model is given the full noisy context (including non-evidential documents) to generate summaries. Compare the resulting compressor's performance to the original ACoRN to quantify how much the evidential-only teacher training contributes to robustness.