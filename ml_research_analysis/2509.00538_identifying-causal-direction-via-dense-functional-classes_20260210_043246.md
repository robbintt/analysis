---
ver: rpa2
title: Identifying Causal Direction via Dense Functional Classes
arxiv_id: '2509.00538'
source_url: https://arxiv.org/abs/2509.00538
tags:
- causal
- function
- noise
- cubic
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of determining causal direction
  between two continuous variables under the assumption of no hidden confounders.
  The authors propose a method called LCUBE, which uses the Minimum Description Length
  (MDL) principle with dense functional classes, specifically cubic regression splines,
  to infer the correct causal direction.
---

# Identifying Causal Direction via Dense Functional Classes

## Quick Facts
- arXiv ID: 2509.00538
- Source URL: https://arxiv.org/abs/2509.00538
- Reference count: 40
- Primary result: LCUBE achieves 87 AUDRC on Tuebingen cause-effect pairs, outperforming state-of-the-art methods

## Executive Summary
This paper addresses the problem of determining causal direction between two continuous variables under the assumption of no hidden confounders. The authors propose LCUBE, a method that uses Minimum Description Length (MDL) with dense functional classes, specifically cubic regression splines, to infer the correct causal direction. The method compares MDL description lengths of two possible causal directions, where the true causal direction yields a lower score. LCUBE outperforms existing methods on multiple benchmark datasets while being computationally efficient, taking only minutes to run on large datasets.

## Method Summary
LCUBE computes causal scores by modeling the relationship as Y = f(X) + noise and comparing MDL description lengths in both directions. The method uses cubic splines with variable knots to approximate f, where MDL combines model complexity (number of knots and coefficients) with goodness-of-fit (residual error). The framework requires normalizing data to [0,1], computing cubic spline regressions for varying numbers of knots, and selecting the direction with minimum MDL score. A single hyperparameter m_max controls the maximum number of knots used in spline fitting.

## Key Results
- Achieves 87 AUDRC on Tuebingen real-world cause-effect pairs dataset, higher than any other method
- Shows superior average precision across 10 common benchmark datasets
- Demonstrates above-average precision on 13 different datasets
- Computation time is only a few minutes on large datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Causal direction can be identified by comparing MDL scores of regression models in both directions, where the true causal direction yields lower description length.
- **Mechanism:** MDL score L(Y|X, θ) = L(θ) + L(Y|θ) combines model complexity with goodness-of-fit. Under assumptions, the causal model converges to a simpler function requiring fewer parameters.
- **Core assumption:** Assumption 4 (Simplicity): The function φ minimizing expected least-squared error in the causal direction has no more parameters than the anti-causal function ψ.
- **Evidence anchors:** Theorem 1 proves asymptotic limit Δ̂n(X → Y) / Δ̂n(Y → X) ≤ 1; related work on regression-based causal discovery supports the paradigm.
- **Break condition:** If the true causal function is linear, both directions have equal scores. If noise is not low, the asymptotic limit may not approximate finite-sample behavior.

### Mechanism 2
- **Claim:** Cubic regression splines provide an identifiable causal score because they possess the density property on compact intervals and satisfy IRSF conditions.
- **Mechanism:** Cubic splines with variable knots are dense in C([a,b]), meaning any continuous function can be approximated within arbitrary precision. The MDL code length encodes knot positions and coefficients.
- **Core assumption:** Assumptions 1-3 plus density property of cubic splines on [0,1].
- **Evidence anchors:** Section V-A proves density property; Corollary 1 provides identifiability guarantee for even m.
- **Break condition:** If equidistant knots don't satisfy Schoenberg-Whitney conditions, unique interpolation is not guaranteed.

### Mechanism 3
- **Claim:** The MDL framework avoids Gaussianity assumptions and inherently penalizes model complexity, addressing pitfalls of likelihood-based causal scores.
- **Mechanism:** Unlike Gaussian likelihood scoring, MDL uses a two-part code that doesn't assume a specific error distribution. The goodness-of-fit term captures residual error without distributional assumptions.
- **Core assumption:** Noise N is low (α small) but need not be Gaussian; only unbiased with unit variance.
- **Evidence anchors:** Section III-A discusses avoiding Gaussianity assumptions; Section II-A motivates MDL approach over Gaussian likelihood scoring.
- **Break condition:** If noise is not low, the approximation degrades. If the true error distribution has heavy tails, RSS-based goodness-of-fit may be suboptimal.

## Foundational Learning

- **Concept: Minimum Description Length (MDL) Principle**
  - **Why needed here:** LCUBE's causal score is fundamentally an MDL two-part code; understanding the trade-off between model complexity and data fit is essential.
  - **Quick check question:** Given two models with equal RSS but different numbers of knots, which would MDL prefer?

- **Concept: Additive Noise Models (ANM) and Identifiability**
  - **Why needed here:** The paper assumes Y = f(X) + N with independent noise; identifiability results depend on this structure and the non-linearity of f.
  - **Quick check question:** If f is linear and noise is Gaussian, can causal direction be identified? (Answer: No, per Section II-A and equality condition in Theorem 1.)

- **Concept: Cubic Spline Regression and Density Property**
  - **Why needed here:** The density property guarantees approximation capability; understanding spline basis functions and knot placement is necessary to implement Algorithm 1.
  - **Quick check question:** Why does the number of knots m control both approximation precision and model complexity penalty?

## Architecture Onboarding

- **Component map:** Preprocessing -> Spline fitting (Algorithm 1) -> MDL scoring -> Direction decision (Algorithm 2)
- **Critical path:** The score computation for both directions must use the same mmax. The minimum over all m is taken per direction before comparing directions.
- **Design tradeoffs:**
  - Single hyperparameter mmax: Higher values allow better approximation but increase computational cost O(2mmax(nmmax + mmax³)). Paper uses mmax ≤ 10.
  - Equidistant vs. variable knots: Equidistant simplifies encoding but may be suboptimal for functions with localized complexity.
  - Even m requirement: Corollary 1 requires even m for identifiability guarantee; odd m is not theoretically covered.
- **Failure signatures:**
  - Linear relationships: Scores equal in both directions, output "undecided."
  - High noise: Asymptotic guarantees may not hold for finite samples.
  - Confounders present: Assumption of no hidden confounders violated; method may infer incorrect direction.
  - Non-compact support: Normalization step may introduce artifacts if original distribution has heavy tails.
- **First 3 experiments:**
  1. Replicate on Tübingen pairs: Run LCUBE on 99 real-world pairs with mmax=10; verify AUDRC ≈ 87% and compare per-pair decisions against ground truth.
  2. Ablate mmax: Test mmax ∈ {2, 4, 6, 8, 10} on synthetic AN dataset; plot accuracy vs. mmax to identify optimal choice.
  3. Noise sensitivity: Generate data with varying α (noise levels); identify the threshold where AUDRC drops significantly.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the LCUBE framework be successfully extended to other dense functional classes, such as harmonic functions or shallow neural networks, while maintaining identifiability?
- **Basis in paper:** [explicit] The conclusion states, "In future work, we aim to explore causal scores via other dense functional classes."
- **Why unresolved:** The current work only instantiates the general MDL score using cubic regression splines, leaving the application to other dense classes as future work.
- **What evidence would resolve it:** A derivation of the MDL code length L(θ) for a different dense class, along with empirical AUDRC results on the Tuebingen benchmark dataset comparable to those of LCUBE.

### Open Question 2
- **Question:** How can the Minimum Description Length (MDL) encoding be adapted for shallow feed-forward neural networks (FNNs) to ensure they satisfy Assumption 4 (simplicity)?
- **Basis in paper:** [inferred] Section VI.B discusses FNNs as a dense class but concludes that finding encodings that characterize the structure and learning of FNNs such that they satisfy Assumption 4 is "a difficult task."
- **Why unresolved:** The paper notes that while FNNs are universal approximators, directly estimating the approximation error on a finite dataset is challenging, and standard training algorithms depend heavily on parameter settings.
- **What evidence would resolve it:** A formulation of the code length L(θ) for an FNN that allows the scoring function to satisfy the conditions of an Identifiable Regression-based Scoring Function (IRSF).

### Open Question 3
- **Question:** How does the theoretical identifiability of LCUBE translate to finite sample regimes where the noise level does not decrease as sample size increases?
- **Basis in paper:** [inferred] Theorem 1 proves identifiability under the model Y_n ≈ h(X) + 1/n N, where noise vanishes as n → ∞. However, real-world data has fixed noise levels regardless of sample size.
- **Why unresolved:** While the method performs well empirically, the theoretical guarantee relies on a low-noise limit that is an idealization not strictly met in the finite, fixed-noise datasets used in the experiments.
- **What evidence would resolve it:** A theoretical analysis or bound showing the identifiability of the score for a fixed noise parameter α independent of n, or empirical sensitivity analysis showing the noise threshold at which identifiability fails.

## Limitations
- The identifiability guarantee relies heavily on Assumptions 1-4, particularly the low noise condition (α small), with limited empirical validation of robustness when these assumptions are violated.
- While LCUBE shows superior performance on benchmarks, the comparison methods include generic ML classifiers rather than specialized causal discovery baselines, weakening the claims of superiority.
- The theoretical results are asymptotic; finite-sample behavior and convergence rates are not characterized, which is crucial for practical application.

## Confidence
- **High confidence:** The MDL framework construction and Algorithm 1 implementation are clearly specified and reproducible.
- **Medium confidence:** The claim that cubic splines provide dense approximation and satisfy IRSF conditions is theoretically sound but relies on compactness assumptions that may not hold in practice.
- **Medium confidence:** The performance superiority on benchmark datasets is demonstrated, but the baselines and comparison methodology could be more rigorous.

## Next Checks
1. **Empirical noise sensitivity analysis:** Generate synthetic datasets with systematically varying noise levels (α from 0.1 to 1.0) to quantify the threshold where LCUBE's performance degrades and identify the practical limits of the "low noise" assumption.
2. **Real-world confounder robustness:** Test LCUBE on datasets with known hidden confounders to measure performance degradation and identify warning signals that confounders may be present.
3. **Reproducibility verification:** Implement LCUBE from the paper specifications and attempt to reproduce the Tuebingen results (AUDRC ≈ 87%) using the same m_max and evaluation protocol.