---
ver: rpa2
title: 'DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight
  Language Models'
arxiv_id: '2504.15027'
source_url: https://arxiv.org/abs/2504.15027
tags:
- distilqwen2
- llms
- student
- teacher
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DistilQwen2.5, a family of distilled lightweight
  language models derived from Qwen2.5 models. The authors employ multi-agent data
  augmentation using proprietary LLMs to generate and refine instruction-response
  pairs, followed by efficient model fusion to integrate fine-grained hidden knowledge.
---

# DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models

## Quick Facts
- arXiv ID: 2504.15027
- Source URL: https://arxiv.org/abs/2504.15027
- Authors: Chengyu Wang; Junbing Yan; Yuanhao Yue; Jun Huang
- Reference count: 16
- Key outcome: DistilQwen2.5-7B achieves 34.86% on AlpacaEval 2.0, outperforming baseline Qwen2.5-7B at 31.43%

## Executive Summary
DistilQwen2.5 presents a practical industrial approach to knowledge distillation for creating lightweight language models. The framework employs a two-stage process: multi-agent data augmentation using proprietary LLMs to generate refined instruction-response pairs, followed by efficient model fusion with top-K logit approximation. The resulting models demonstrate significant performance gains across multiple benchmarks while maintaining computational efficiency, with particular success in SQL completion tasks. The approach balances the trade-off between model size and capability, showing that smaller models can achieve substantial improvements through careful distillation methodology.

## Method Summary
The method employs a two-stage distillation pipeline. First, black-box knowledge distillation uses a multi-agent pipeline (Expansion, Rewriting, Selection, Verification) with proprietary teacher models to generate and refine instruction-response pairs optimized for student learning. Second, white-box knowledge distillation computes teacher logits offline using top-10 token approximation, then trains students to minimize divergence between teacher and student distributions. The process supports vocabulary alignment across different model families and achieves 3-5x speedup compared to vanilla implementations. Models are trained with LR=1e-5 for 3 epochs on 8x A800 GPUs.

## Key Results
- DistilQwen2.5-7B achieves 34.86% on AlpacaEval 2.0 vs 31.43% baseline
- Smaller models show greater relative improvements (0.5B: 2.46→4.89 vs 7B: 31.43→34.86)
- Effective knowledge transfer across tasks including SQL completion
- Computational efficiency gains of 3-5x using top-K logit approximation

## Why This Works (Mechanism)

### Mechanism 1
Multi-agent black-box distillation produces training data better aligned with student model learning capacity than raw instruction-response pairs. A controller orchestrates four specialized LLM agents to encapsulate teacher knowledge into refined training examples optimized for smaller models. This pipeline preserves task category while enhancing cognitive reasoning abilities through CoT rewriting.

### Mechanism 2
Efficient white-box distillation using top-K logit approximation transfers fine-grained knowledge while reducing computational overhead by 3-5x. Pre-computed teacher logits store only top-10 tokens per position, capturing nearly all probability mass. This decouples teacher inference from student training while maintaining knowledge fidelity.

### Mechanism 3
Sequential combination of black-box then white-box distillation yields cumulative improvements. Smaller student models show greater relative gains than larger ones due to more "headroom" for improvement. The knowledge captured in augmented data and logit distributions is complementary rather than redundant.

## Foundational Learning

- **Knowledge Distillation Paradigms**: Understanding black-box (API-only access, data augmentation) vs white-box (logit-level access) distillation is prerequisite to interpreting the two-stage pipeline. Quick check: Can you explain why black-box KD requires data augmentation while white-box KD can operate directly on logit distributions?

- **Divergence Minimization for Distribution Alignment**: Section 3.2 defines the white-box KD objective using generalized divergence Dθ between teacher and student token distributions. Quick check: Given teacher logits [0.7, 0.2, 0.1] and student logits [0.5, 0.3, 0.2], would minimizing forward KL push student toward mode-covering or mode-seeking behavior?

- **Token/Vocabulary Alignment Across Models**: The paper notes potential mismatches between teacher and student vocabularies requiring tokenization mapping. Quick check: If teacher uses BPE with 50k tokens and student uses 32k tokens, how would you construct the alignment matrix for logit comparison?

## Architecture Onboarding

- **Component map**: Seed Instructions → [Knowledge Production Pipeline] → Expansion Agent → Expanded Instructions → Rewriting Agent → CoT-enhanced Responses → Verification Agent → Quality Check → Selection Agent → Filtered Dataset → [Distillation Training Pipeline] → Black-Box Trainer (SFT on augmented data) → White-Box Trainer (logit matching) → Offline: Teacher Logits Generator (top-K) → Online: Student Training with Dθ loss

- **Critical path**: Data augmentation quality bounds student capability; top-K logit storage format determines I/O efficiency; vocabulary alignment correctness is a single point of failure.

- **Design tradeoffs**: Teacher size vs. compute cost (diminishing returns beyond 14B-32B); dataset size vs. marginal gain (10K-100K samples benefit most); K value in top-K (K=10 default, smaller risks information loss).

- **Failure signatures**: Semantic drift in expanded instructions; vocabulary alignment errors causing systematic logit mismatches; CoT responses too complex for small students; overfitting to teacher artifacts.

- **First 3 experiments**: (1) Ablate black-box only vs. black-box+white-box on held-out task; (2) Vary K in top-K logit approximation (K∈{5,10,20,50}) on AlpacaEval 2.0; (3) Cross-architecture teacher-student pairs using non-Qwen teacher with token alignment.

## Open Questions the Paper Calls Out

- **Question**: How can the distillation framework effectively mitigate the propagation of inherent biases or factual errors from proprietary teacher models to the student models? Basis: Limitations section notes biases/errors could propagate into student models. Unresolved: Current implementation lacks debiasing mechanisms. Evidence needed: Analysis measuring fairness and factuality metrics with debiasing filters.

- **Question**: How can the collaborative aspects of model fusion be enhanced to allow for more dynamic knowledge transfer? Basis: Conclusion explicitly states aspiration to enhance collaborative aspects. Unresolved: Current approach relies on static offline logits generation. Evidence needed: Comparative study between static and dynamic fusion architectures.

- **Question**: To what extent does the DistilQwen2.5 framework generalize to diverse domains and languages beyond evaluated benchmarks? Basis: Limitations note generalizability across domains/languages remains unevaluated. Unresolved: Paper primarily validates on general instruction-following and SQL tasks. Evidence needed: Evaluation on low-resource languages and specialized verticals.

## Limitations
- Potential propagation of biases or errors inherent in proprietary teacher models to student models
- Generalizability of the framework across diverse domains and languages remains unevaluated
- Top-K logit approximation assumption (K=10 captures nearly all knowledge) lacks validation beyond Qwen2.5 models

## Confidence

- **High Confidence**: Overall two-stage distillation framework demonstrates consistent performance improvements across multiple model sizes and benchmarks; computational efficiency claims are directly measurable.
- **Medium Confidence**: Smaller student models show greater relative improvements, though more extensive ablation studies would strengthen this claim.
- **Low Confidence**: Top-10 logits capturing nearly all teacher knowledge lacks validation beyond authors' observations; multi-agent pipeline effectiveness depends on undisclosed proprietary teacher behaviors.

## Next Checks

1. **Top-K Logit Coverage Analysis**: Systematically test K values {5, 10, 20, 50} across diverse task types to empirically determine information loss threshold and validate K=10 optimality.

2. **Multi-Agent Pipeline Ablation**: Implement simplified version using open-source teachers and compare generated instruction-response pair quality against original dataset using human evaluation metrics.

3. **Cross-Architecture Teacher-Student Transfer**: Test distillation framework using non-Qwen teacher models with explicit token alignment to assess robustness against vocabulary mismatches.