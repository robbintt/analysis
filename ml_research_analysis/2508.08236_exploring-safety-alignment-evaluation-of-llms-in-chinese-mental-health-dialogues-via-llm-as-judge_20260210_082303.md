---
ver: rpa2
title: Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues
  via LLM-as-Judge
arxiv_id: '2508.08236'
source_url: https://arxiv.org/abs/2508.08236
tags:
- evaluation
- health
- mental
- safety
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating safety alignment
  in LLM responses to high-risk mental health dialogues, where no gold-standard answers
  exist. The authors propose PsyCrisis-Bench, a reference-free benchmark that uses
  a prompt-based LLM-as-Judge approach guided by expert-defined reasoning chains from
  crisis intervention theory.
---

# Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge

## Quick Facts
- arXiv ID: 2508.08236
- Source URL: https://arxiv.org/abs/2508.08236
- Authors: Yunna Cai; Fan Wang; Haowei Wang; Kun Wang; Kailai Yang; Sophia Ananiadou; Moyan Li; Mingming Fan
- Reference count: 31
- This paper proposes a reference-free benchmark for evaluating LLM safety alignment in high-risk mental health dialogues using LLM-as-Judge with expert-derived reasoning chains.

## Executive Summary
This paper addresses the challenge of evaluating safety alignment in LLM responses to high-risk mental health dialogues, where no gold-standard answers exist. The authors propose PsyCrisis-Bench, a reference-free benchmark that uses a prompt-based LLM-as-Judge approach guided by expert-defined reasoning chains from crisis intervention theory. The evaluation employs binary point-wise scoring across five safety dimensions—empathy, emotional regulation strategies, concern exploration, risk assessment, and referral to external resources—to ensure traceability and interpretability.

Experiments on 3,600 judgments show the method achieves significantly higher agreement with expert assessments (0.45 vs. 0.1–0.2) and produces more interpretable rationales compared to baselines. The publicly released dataset covers self-harm, suicidal ideation, and existential distress in Chinese-language real-world contexts, offering a valuable resource for safety-critical LLM evaluation.

## Method Summary
The study proposes PsyCrisis-Bench, a reference-free evaluation framework for assessing LLM safety alignment in Chinese mental health dialogues. The method uses GPT-4o as an LLM-as-Judge to evaluate responses across five safety dimensions using binary point-wise scoring with chain-of-thought reasoning. The benchmark covers 608 Chinese user utterances from three categories (suicide, non-suicidal self-injury, and existential distress) sourced from existing datasets. The evaluation framework includes few-shot prompting with expert-authored reasoning chains to guide the LLM judge's assessment. The study compares the proposed method against baseline evaluation approaches and reports system-level correlation metrics against human expert annotations.

## Key Results
- Achieved Pearson correlation of 0.45 with expert assessments versus 0.1-0.2 for baseline methods
- Demonstrated superior interpretability with traceable rationales through expert-defined reasoning chains
- Identified leniency bias where LLM judge scores consistently higher than human experts

## Why This Works (Mechanism)
The LLM-as-Judge approach works effectively because it leverages expert-defined reasoning chains from crisis intervention theory to guide evaluation. By using binary point-wise scoring across five safety dimensions rather than holistic scoring, the method ensures traceability and interpretability. The chain-of-thought format helps the LLM judge systematically evaluate responses against established safety criteria, while the reference-free design eliminates the need for gold-standard answers in safety-critical contexts where appropriate responses may vary.

## Foundational Learning

1. **LLM-as-Judge methodology**
   - Why needed: Provides scalable evaluation for safety-critical domains where human evaluation is costly and inconsistent
   - Quick check: Verify correlation between LLM judge scores and human expert assessments exceeds 0.4

2. **Chain-of-thought reasoning for safety evaluation**
   - Why needed: Ensures systematic, traceable assessment aligned with expert guidelines
   - Quick check: Confirm rationales include explicit reference to all five safety dimensions

3. **Binary point-wise scoring framework**
   - Why needed: Enables granular assessment of specific safety aspects rather than holistic judgments
   - Quick check: Validate that each dimension's binary scoring captures meaningful distinctions in response quality

## Architecture Onboarding

**Component Map:** User utterance -> LLM response generation -> LLM-as-Judge evaluation -> Expert validation -> Correlation analysis

**Critical Path:** 1) Load curated utterances with risk categories, 2) Generate LLM responses using test models, 3) Apply LLM-as-Judge with binary scoring across five dimensions, 4) Compute correlation against expert annotations

**Design Tradeoffs:** Reference-free evaluation eliminates need for gold answers but requires careful prompt engineering; binary scoring ensures interpretability but may oversimplify complex safety judgments; Chinese language focus ensures cultural relevance but limits generalizability.

**Failure Signatures:** Low correlation with expert assessments (<0.3) indicates prompt engineering issues; consistent leniency bias suggests calibration needed; poor inter-annotator agreement (<0.5) on ambiguous cases requires expert adjudication.

**First Experiments:** 1) Test correlation between LLM judge and human experts on subset of 50 samples, 2) Compare binary vs holistic scoring approaches, 3) Evaluate impact of different few-shot exemplar counts on judgment quality

## Open Questions the Paper Calls Out
None

## Limitations
- Binary point-wise scoring may oversimplify complex safety judgments requiring nuanced assessment
- Chinese-language focus limits generalizability to other linguistic contexts
- Leniency bias observed where LLM judge scores consistently higher than human experts

## Confidence

**High Confidence**: The method achieves statistically significant improvement in agreement with expert assessments (0.45 vs. 0.1-0.2 baselines), and the correlation coefficients are reported with standard deviations across experiments.

**Medium Confidence**: The claim of "superior interpretability" for rationales is supported by qualitative descriptions but lacks quantitative comparison metrics.

**Low Confidence**: The generalizability of results to non-Chinese contexts and other mental health crisis categories is not established.

## Next Checks
1. Implement and test the leniency bias calibration method described in Figure 3, comparing calibrated vs uncalibrated scores against a held-out expert validation set.

2. Apply the PsyCrisis-Bench methodology to an equivalent English-language mental health crisis dataset to assess language generalizability and identify any prompt engineering adjustments needed.

3. Conduct statistical analysis of correlations between the five safety dimensions to determine if binary scoring adequately captures the multidimensional nature of crisis response quality, or if dimension weighting is needed.