---
ver: rpa2
title: 'Assisting Research Proposal Writing with Large Language Models: Evaluation
  and Refinement'
arxiv_id: '2509.09709'
source_url: https://arxiv.org/abs/2509.09709
tags:
- writing
- research
- proposal
- quality
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study proposes a dual-metric evaluation framework\u2014content\
  \ quality and reference validity\u2014and an iterative prompting method to enhance\
  \ research proposal writing with large language models (LLMs). Content quality was\
  \ assessed using AI grading systems (Study Fetch, QuillBot, Grammarly), while reference\
  \ validity was evaluated through manual fact-checking."
---

# Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement

## Quick Facts
- arXiv ID: 2509.09709
- Source URL: https://arxiv.org/abs/2509.09709
- Reference count: 24
- Dual-metric evaluation framework improves content quality scores by 3.84%-11.02% and increases reference correctness from 38.89% to 100% in some cases

## Executive Summary
This study introduces a dual-metric evaluation framework combining content quality assessment (using AI graders Study Fetch, QuillBot, Grammarly) and manual reference validity checking to evaluate and improve research proposal writing with large language models. The authors propose an iterative prompting method that feeds back AI-generated scores as targeted feedback to the LLM, enabling systematic refinement of output quality. The framework successfully reduces reference inaccuracies and fabrications—a critical ethical challenge in academic writing—while demonstrating measurable improvements in both content quality and citation accuracy across three education research topics.

## Method Summary
The methodology employs ChatGPT-4o to generate 2,000-word research proposals on three education topics (Student Agency, Teacher Agency, Teacher Identity) using two strategies: GPT-only and GPT-assisted (with 10 uploaded reference documents). Content quality is evaluated by averaging scores from three AI grading platforms, while reference validity is assessed through manual fact-checking. The iterative prompting method applies the derived scores back to the LLM as specific improvement instructions, with the correction process repeated until quality stabilizes (typically converging by round 3). The study compares these approaches against a human-written baseline proposal.

## Key Results
- Iterative prompting improved content quality scores by 3.84%-11.02% across different topics and strategies
- Reference correctness increased from 38.89% to 100% in some cases through iterative correction
- GPT-assisted strategy consistently outperformed GPT-only in both content quality and reference accuracy
- Student Agency proposals showed the highest overall improvement with the iterative method

## Why This Works (Mechanism)

### Mechanism 1: Score-Based Feedback Loop (Content Calibration)
The system creates a reinforcement loop where AI graders provide numerical scores that the LLM uses as optimization targets. This conditions the model to improve against specific evaluation dimensions (fluency, clarity) through textual feedback conversion. The loop guides the LLM's semantic search toward higher-scoring regions of the latent space.

### Mechanism 2: Explicit Reference Verification (Hallucination Suppression)
Manual verification of citations creates negative reinforcement that suppresses fabrication in subsequent iterations. By identifying and correcting hallucinated or incorrect metadata, the LLM is forced to rely on parametric knowledge more strictly rather than generating plausible but false content.

### Mechanism 3: Grounding via Retrieved Context (GPT-Assisted Strategy)
Pre-loading the context window with validated documents grounds the generation process, reducing fabrication probability compared to GPT-only approaches. This creates a RAG environment where attention mechanisms restrict output distribution to words and facts present in the context.

## Foundational Learning

- **Concept: Hallucination Frequency**
  - Why needed: To understand why rigorous manual checking is implemented—LLMs generate text based on probability, not truth; citations are statistically plausible but often factually non-existent
  - Quick check: Why might an LLM generate a plausible-sounding author name and title that doesn't exist? (Answer: Because it optimizes for linguistic likelihood, not database lookup)

- **Concept: Iterative Refinement (Prompt Engineering)**
  - Why needed: The core contribution is that a single pass is insufficient; LLMs can refine output based on critique
  - Quick check: How does the role of the prompt change between Round 1 and Round 3? (Answer: It shifts from an instruction to a correction signal)

- **Concept: Evaluation Metric Alignment**
  - Why needed: AI graders measure surface features (grammar, structure) rather than scientific novelty
  - Quick check: If the AI grader gives a perfect score, does that guarantee the research proposal is scientifically sound? (Answer: No, it only guarantees it meets the grader's linguistic/heuristic criteria)

## Architecture Onboarding

- **Component map:**
  Generator (ChatGPT-4o) -> Evaluators (Study Fetch, QuillBot, Grammarly) -> Human Validator -> Controller (Iteration Manager)

- **Critical path:**
  1. Ingestion: Load 10 reference docs (if GPT-Assisted)
  2. Drafting: Generate initial 2,000-word proposal
  3. Scoring: Run text through 3 AI graders; average scores
  4. Verification: Manually check citations
  5. Feedback Injection: Prompt GPT with "Improve clarity to 90% and fix citation [X]"
  6. Repeat: Loop until delta improvement < threshold (usually 3 rounds)

- **Design tradeoffs:**
  - Automated vs. Manual Verification: Manual checking scales poorly but ensures accuracy
  - GPT-Only vs. GPT-Assisted: GPT-Assisted has higher setup cost but yields better reference stability

- **Failure signatures:**
  - Score Saturation: AI graders hit ceiling (e.g., 95%) but text quality plateaus
  - Correction Loop: Model fixes one citation but breaks another in next iteration
  - Context Overflow: 10 large documents cause model to lose instruction prompt

- **First 3 experiments:**
  1. Baseline Replication: Run "GPT-Only" generation on generic topic; manually count hallucinated citations
  2. Metric Sensitivity Test: Feed same draft into three different AI graders to verify agreement
  3. Stop-Condition Analysis: Run loop for 10 rounds to see if quality eventually degrades

## Open Questions the Paper Calls Out

### Open Question 1
Does the dual-metric evaluation framework and iterative prompting method generalize to research domains outside of education and LLMs other than ChatGPT-4o? The methodology explicitly limits the experiment to "three distinct topics in the education field" and utilizes a single model version ("ChatGPT-4o"), leaving unclear if improvements are specific to training data or citation styles common in education rather than universal capability.

### Open Question 2
How valid are the AI-based grading systems (Study Fetch, QuillBot, Grammarly) as proxies for human expert evaluation in assessing research proposal quality? The paper claims to provide an "objective" evaluation by replacing human judgment with AI grading systems, but does not validate the correlation between these AI scores and human expert assessments, relying on the assumption that algorithms accurately reflect complex standards of academic quality.

### Open Question 3
What specific advanced methods or more efficient writing strategies can be developed to further enhance LLM writing capabilities beyond the proposed iterative prompting? The current iterative method requires multiple rounds of prompting and manual intervention, which may be resource-intensive, and the paper concludes that future research should develop more efficient strategies and advanced methods.

## Limitations

- The dual-metric framework relies on proprietary AI grading systems whose internal scoring mechanisms and reliability for academic content evaluation remain opaque
- Manual reference verification is labor-intensive and may not scale to larger writing tasks or different domains
- The iterative prompting method shows diminishing returns after 2-3 rounds, suggesting a ceiling effect that may not address deeper structural issues

## Confidence

- **High confidence:** The iterative prompting methodology and dual-metric framework are clearly described and reproducible
- **Medium confidence:** The specific percentage improvements (3.84%-11.02% content quality, 38.89% to 100% reference correctness) are based on the three test topics but may not generalize to other domains
- **Low confidence:** The claim that this framework "addresses critical ethical challenges in academic writing" lacks comparative analysis with other approaches or consideration of potential new ethical issues

## Next Checks

1. **Cross-platform reliability test:** Run the same draft through multiple instances of each AI grading system to verify consistency and identify potential scorer bias
2. **Generalization assessment:** Apply the iterative prompting method to research topics outside education (e.g., biomedical or engineering proposals) to test framework transferability
3. **Human evaluation comparison:** Have domain experts assess whether AI-graded improvements correlate with actual academic quality and research merit