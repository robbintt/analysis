---
ver: rpa2
title: Hybrid Matrix Factorization Based Graph Contrastive Learning for Recommendation
  System
arxiv_id: '2509.05115'
source_url: https://arxiv.org/abs/2509.05115
tags:
- graph
- contrastive
- information
- performance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes HMFGCL, a hybrid matrix factorization-based\
  \ graph contrastive learning method for recommendation systems. The core innovation\
  \ is integrating two distinct matrix factorization techniques\u2014low-rank matrix\
  \ factorization (MF) and singular value decomposition (SVD)\u2014to complementarily\
  \ acquire global collaborative information for enhanced graph views."
---

# Hybrid Matrix Factorization Based Graph Contrastive Learning for Recommendation System

## Quick Facts
- arXiv ID: 2509.05115
- Source URL: https://arxiv.org/abs/2509.05115
- Reference count: 34
- Primary result: Hybrid MF+SVD + mixed noise GNN achieves up to 11.02% relative improvement in NDCG@10 over LightGCL baseline

## Executive Summary
This paper proposes HMFGCL, a hybrid matrix factorization-based graph contrastive learning method for recommendation systems. The core innovation is integrating two distinct matrix factorization techniques—low-rank matrix factorization (MF) and singular value decomposition (SVD)—to complementarily acquire global collaborative information for enhanced graph views. The method introduces mixed noise into the GNN propagation process to improve embedding quality and employs contrastive learning to fuse local dependencies with global signals. Experiments on three public datasets demonstrate superior performance over baseline methods, particularly on small-scale datasets.

## Method Summary
HMFGCL combines low-rank matrix factorization and singular value decomposition to extract complementary global collaborative signals from user-item interaction matrices. These hybrid embeddings are fused with GNN-propagated local information through a two-view contrastive learning framework. The GNN backbone incorporates mixed Gaussian-uniform noise injection to enhance embedding uniformity and prevent over-smoothing. InfoNCE loss aligns the main GNN embeddings with the global-enhanced contrastive views, while standard recommendation loss optimizes the final predictions. The model uses Adam optimizer with embedding size 64, 2 aggregation layers, and MF/SVD dimensions set to 5.

## Key Results
- HMFGCL achieves up to 11.02% relative improvement in NDCG@10 and 10.73% in NDCG@20 compared to LightGCL
- Superior performance particularly on small-scale datasets (ML-100k, ModCloth-1000, LastFM-1000)
- Ablation studies confirm hybrid MF+SVD module effectiveness, with maximum gains when both components are retained
- Optimal performance achieved with k=q=5 latent dimensions, L=2 aggregation layers, and noise coefficient ε=0.1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid matrix factorization (MF + SVD) captures complementary global collaborative signals that single decomposition methods miss, improving contrastive view quality
- Mechanism: MF captures latent user-item factors through gradient-optimized low-rank approximation, while SVD extracts principal components via spectral decomposition. The fusion of both representations during contrastive view construction injects multi-spectral global context into embeddings
- Core assumption: MF and SVD capture meaningfully distinct global patterns that provide complementary signals when combined
- Evidence anchors:
  - [abstract] "integrates two distinct matrix factorization techniques—low-rank matrix factorization (MF) and singular value decomposition (SVD)—to complementarily acquire global collaborative information"
  - [section 4.2] Equations 7-8 define separate MF and SVD decomposition paths
  - [corpus] Weak direct evidence; related work supports multi-view MF benefits but not specifically MF+SVD hybrid
- Break condition: If MF and SVD latent spaces are highly correlated (cosine similarity > 0.9), the hybrid provides no complementary benefit and adds unnecessary complexity

### Mechanism 2
- Claim: Mixed Gaussian-uniform noise injection during GNN propagation enhances embedding uniformity, improving contrastive learning effectiveness
- Mechanism: Noise vector Δ = ω ⊙ (l₁ × N_g + l₂ × N_u) combines two noise distributions at each propagation layer, perturbing embeddings to prevent over-smoothing and encourage uniform representation distribution across the embedding space
- Core assumption: Mixed noise provides better perturbation diversity than single-distribution noise for recommendation tasks
- Evidence anchors:
  - [abstract] "introduces mixed noise into the GNN propagation process to improve embedding quality"
  - [section 4.3] Equation 12-13 define the mixed noise formulation with hyperparameter control
  - [corpus] Generative Data Augmentation paper supports noise-based augmentation in GCL, but not specifically mixed Gaussian-uniform
- Break condition: If noise magnitude ε is too large (> 0.3 based on sensitivity analysis), semantic information is destroyed and performance degrades sharply

### Mechanism 3
- Claim: Two-view contrastive paradigm (main view vs. global-enhanced contrastive view) reduces computational overhead while maintaining representation quality
- Mechanism: Instead of generating two augmented graphs through GNN passes, HMFGCL uses the main GNN output as one view and the MF+SVD-fused embeddings as the contrastive view, computing InfoNCE loss between them
- Core assumption: Global collaborative information from MF+SVD can serve as an effective contrastive target without requiring full graph augmentation
- Evidence anchors:
  - [section 4.4] "we abandon the traditional three-view structure. The embeddings E^(u) and E^(v) generated by the main view serve as one part of the InfoNCE calculation"
  - [corpus] LightGCL (referenced) uses SVD-guided augmentation; this extends to hybrid approach
- Break condition: If MF+SVD reconstruction quality is poor (Frobenius norm error > 0.5 relative to original adjacency), contrastive signal becomes noise and degrades learning

## Foundational Learning

- Concept: Matrix Factorization (MF)
  - Why needed here: Decomposes user-item interaction matrix A into low-rank representations P and Q for global pattern extraction
  - Quick check question: Can you explain why low-rank approximation helps with sparse matrices?

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: Provides spectral decomposition capturing principal components of interaction patterns
  - Quick check question: How does truncated SVD differ from full SVD, and why does truncation help here?

- Concept: Contrastive Learning (InfoNCE Loss)
  - Why needed here: Core training objective that aligns main embeddings with global-enhanced contrastive views
  - Quick check question: What does the temperature parameter τ control in InfoNCE loss?

- Concept: Graph Neural Network Message Passing
  - Why needed here: Propagates information across user-item graph to capture local dependencies
  - Quick check question: What is the role of the degree matrix D in the normalized propagation equation?

## Architecture Onboarding

- Component map: Adjacency matrix construction → Parallel MF/SVD decomposition → GNN forward pass with noise → Global-local fusion → Contrastive loss → Joint optimization
- Critical path: Adjacency matrix construction → Parallel MF/SVD decomposition → GNN forward pass with noise → Global-local fusion → Contrastive loss → Joint optimization
- Design tradeoffs:
  - Latent dimension (k, q): Paper finds k=q=5 optimal; larger values introduce noise
  - Aggregation layers: L=2 optimal; deeper layers cause over-smoothing
  - Noise ratio (l₁:l₂): 0.8:0.2 (Gaussian:uniform) works best
  - Embedding size: 64 balances capacity vs. overfitting
- Failure signatures:
  - Performance drops sharply if latent dimensions > 10 (noise dominates)
  - Loss convergence stalls if contrastive loss weight α is too low (< 0.001)
  - Over-smoothing occurs with L > 3 layers (embeddings become indistinguishable)
- First 3 experiments:
  1. Ablation validation: Run HMFGCL-R (no MF/SVD), HMFGCL-M (MF only), HMFGCL-S (SVD only) on ML-100K to confirm hybrid benefit matches Table 3
  2. Latent dimension sweep: Test k=q ∈ {3, 5, 10, 15, 20} to verify optimal point at 5 on your dataset
  3. Noise sensitivity: Vary noise coefficient ε ∈ {0.05, 0.1, 0.2, 0.3} to establish robust operating range before deployment

## Open Questions the Paper Calls Out

- Can the HMFGCL method maintain computational efficiency and recommendation accuracy when applied to large-scale industrial datasets?
  - Basis in paper: [explicit] The authors explicitly state that "the issue of dataset size should be addressed to improve efficiency and performance when dealing with large datasets," noting that current experiments were limited to small-scale datasets or subsets due to hardware constraints.
  - Why unresolved: The current implementation relies on matrix factorization (SVD/MF) operations which can be computationally intensive; the O(C_{MF} + C_{SVD}) complexity may become a bottleneck on massive graphs.
  - What evidence would resolve it: Experimental results on full-scale datasets (e.g., the complete Amazon-Books or Yelp datasets) showing training latency and memory usage alongside standard accuracy metrics.

- Can alternative view generation strategies or advanced multi-view fusion techniques further enhance the contrastive learning capability of the model?
  - Basis in paper: [explicit] The authors identify "exploring new methods for generating contrasting views or fusing multiple enhanced views for contrastive learning" as a primary avenue for future research.
  - Why unresolved: The current hybrid approach relies specifically on combining low-rank MF and SVD; it is undetermined if other decomposition methods or attention-based fusion mechanisms would capture global information more effectively.
  - What evidence would resolve it: Ablation studies comparing the current MF+SVD fusion against other view generation methods (e.g., autoencoder-based augmentations) and different fusion operators.

- How does HMFGCL perform specifically in cold-start scenarios where user-item interaction data is extremely sparse?
  - Basis in paper: [explicit] The authors conclude that "finding better solutions to mitigate the cold-start problem remains an important research area" despite GNNs providing speculative information.
  - Why unresolved: While the paper demonstrates success on small datasets, it does not isolate performance metrics for users or items with very few interactions, which is the core challenge of the cold-start problem.
  - What evidence would resolve it: A dedicated evaluation reporting Recall and NDCG scores specifically for cold-start users (e.g., users with fewer than 5 interactions) compared to specialized cold-start baselines.

## Limitations

- Performance sensitivity to latent dimension selection (optimal at k=q=5) may not generalize across different recommendation domains
- Mixed noise injection requires careful hyperparameter tuning (noise ratio 8:2, coefficient 0.1) that may not transfer directly to other datasets
- Two-view contrastive framework reduces computational overhead but potentially sacrifices the diversity of augmentations compared to traditional three-view approaches

## Confidence

- **High Confidence**: The experimental results showing HMFGCL's superiority over baseline methods (LightGCL, DGCF, etc.) are well-supported by the data, with consistent improvements across all three datasets and multiple metrics
- **Medium Confidence**: The theoretical justification for hybrid MF+SVD being more effective than either method alone relies on the assumption of complementary information capture, which is reasonable but not empirically proven through direct correlation analysis
- **Medium Confidence**: The claim that mixed Gaussian-uniform noise improves embedding uniformity is supported by ablation studies but lacks direct visualization or quantitative measures of embedding distribution

## Next Checks

1. **Generalization Testing**: Evaluate HMFGCL on larger, denser datasets beyond the 100k-user scale to verify if the hybrid approach maintains its advantage when data sparsity is reduced
2. **Correlation Analysis**: Measure cosine similarity between MF and SVD latent spaces across different datasets to quantify the actual complementarity of the hybrid approach
3. **Noise Sensitivity Validation**: Conduct a more comprehensive sweep of noise parameters (ε ∈ [0.01, 0.5], ratio ∈ [0:1, 1:0]) to establish the robustness envelope and identify failure thresholds more precisely