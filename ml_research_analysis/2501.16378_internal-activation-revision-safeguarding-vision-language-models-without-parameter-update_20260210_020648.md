---
ver: rpa2
title: 'Internal Activation Revision: Safeguarding Vision Language Models Without
  Parameter Update'
arxiv_id: '2501.16378'
source_url: https://arxiv.org/abs/2501.16378
tags:
- revision
- layer
- safety
- head
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why vision-language models (VLMs) are more
  vulnerable to harmful content generation than text-only models, identifying that
  visual inputs cause significant shifts in internal activations compared to text
  inputs. The authors propose an internal activation revision method that modifies
  model activations during inference to enhance safety without parameter updates.
---

# Internal Activation Revision: Safeguarding Vision Language Models Without Parameter Update

## Quick Facts
- arXiv ID: 2501.16378
- Source URL: https://arxiv.org/abs/2501.16378
- Authors: Qing Li; Jiahui Geng; Zongxiong Chen; Kun Song; Lei Ma; Fakhri Karray
- Reference count: 9
- Primary result: Reduces attack success rates by average 48.94% across safety benchmarks while maintaining helpfulness with only 7.72% accuracy drop

## Executive Summary
This paper addresses the vulnerability of vision-language models (VLMs) to harmful content generation by identifying that visual inputs cause significant shifts in internal activations compared to text-only inputs. The authors propose an inference-time activation revision method that modifies model activations using contrastive revision vectors derived from safe and unsafe samples, without requiring parameter updates. The framework demonstrates substantial safety improvements across multiple benchmarks while maintaining high accuracy, requiring only hundreds of examples for training and showing good transferability across different VLM architectures.

## Method Summary
The method extracts revision vectors from contrastive samples of safe and unsafe multimodal responses using either mean-shift (MMS) or probe-weighted (PWD) approaches. During inference, these vectors are added to internal activations at specific layers or heads, with head-level revision showing better balance between safety and helpfulness. The approach uses Multi-Response sample construction strategy with MMS vector extraction, applied at layer 19 with 70% of heads selected and revision strength α=2.0. The method operates entirely at inference time without updating model parameters.

## Key Results
- Reduces ASR by average 48.94% across SafeBench, Safe-Unsafe, Unsafe, and MM-SafetyBench benchmarks
- Maintains high accuracy with only 7.72% decrease on ScienceQA and 1.17% on GQA
- Shows good transferability across models (CS range 1.57-23.68) despite architectural differences
- Head-level revision outperforms layer-level revision with 2.93% ACC gain on ScienceQA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Visual inputs shift internal activations away from text-only distributions, bypassing existing safety alignment
- **Mechanism**: When images are paired with text, resulting activations occupy different regions of the residual stream than text-only inputs. Safety probes trained on text data fail to generalize (≈35% accuracy drop)
- **Core assumption**: Safety-relevant information is linearly encoded in activation space and can be redirected
- **Evidence anchors**: t-SNE shows distinct activation clusters for TextSetA, TextSetB, and MultiSet across layers 5, 15, and 31

### Mechanism 2
- **Claim**: Contrastive revision vectors derived from safe vs. unsafe response activations can steer generation toward safer outputs
- **Mechanism**: Extract activations from positive (safe) and negative (unsafe) samples, compute direction from negative mean to positive mean (MMS), and add this vector scaled by α to forward pass activations
- **Core assumption**: The safe/unsafe distinction is captured in the mean activation difference and transfers to new inputs
- **Evidence anchors**: MMS outperforms PWD across models; MMS computes "average activations of positive and negative samples, and the revision vector points from the positive mean to the negative mean"

### Mechanism 3
- **Claim**: Head-level revision preserves helpfulness better than layer-level revision by targeting fewer dimensions
- **Mechanism**: Layer-level adds α·r to full residual stream after MLP; head-level adds α·r to only a subset of attention heads before concatenation
- **Core assumption**: Safety-critical computations are localized to specific heads and layers
- **Evidence anchors**: "Head-level revision achieves a better balance between safety and helpfulness" with 2.93% ACC gain on ScienceQA vs. layer-level

## Foundational Learning

- **Concept**: Residual Stream in Decoder-Only Transformers
  - **Why needed here**: Revision vectors are injected into the residual stream at specific layers; understanding skip connections is essential for predicting intervention effects
  - **Quick check question**: If you add a vector at layer 9, which subsequent computations carry it forward?

- **Concept**: Linear Probing of Internal Activations
  - **Why needed here**: The PWD method relies on probe weights; interpreting probe results is critical for assessing whether safety information is linearly accessible
  - **Quick check question**: A probe achieves 90% accuracy on text data but 55% on image-text. What does this suggest about representation alignment?

- **Concept**: Contrastive Activation Steering
  - **Why needed here**: The entire method builds on subtracting negative-mean from positive-mean activations. This is the core intervention primitive
  - **Quick check question**: If positive and negative means are nearly identical in a layer, what should you conclude about that layer's role?

## Architecture Onboarding

- **Component map**: Input -> Vision Encoder -> Cross-Modal Encoder -> Text Decoder -> Output; Revision vector injection at specified layer/head positions in decoder

- **Critical path**:
  1. Collect ~200 contrastive samples (Multi-Response: safe/unsafe responses to unsafe multimodal prompts)
  2. Run forward pass, extract activations at candidate layers/heads
  3. Compute MMS vector: r = μ_pos - μ_neg
  4. During inference, add α·r to activations at chosen layer/heads
  5. Evaluate ASR (safety) and ACC (helpfulness) to tune l, α

- **Design tradeoffs**: Middle layers (9, 14, 19) balance safety vs. helpfulness; early layers (4) have weak effect; late layers (29, 31) over-suppress benign outputs

- **Failure signatures**: Over-defense (excessive refusal), under-defense (minimal safety improvement), transfer failure (vectors don't improve other models)

- **First 3 experiments**:
  1. Replicate head-level revision on LLaVA-V1.5-7B with Multi-Response + MMS at layer 19, α ∈ {1.0, 1.5, 2.0, 2.5}
  2. Ablate sample construction: Compare Multi-Response vs. Text-Response vs. Multi-Instruction
  3. Test transfer: Apply revision vectors from LLaVA-V1.5-7B to InternVL2-8B (same hidden dim)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Why do revision vectors extracted from one VLM transfer effectively to models with different architectures and training procedures?
- **Basis in paper**: [explicit] The authors demonstrate cross-model transfer: "We apply the head-level revision vectors that we extract from LLaVA-V1.5-7B to perturb MiniGPT-V2 and InternVL2-8B... Surprisingly, the safety and helpfulness of the revised model are still very good"
- **Why unresolved**: The paper provides empirical evidence of transferability but offers no theoretical explanation for why safety-related activation directions would generalize across architectures
- **What evidence would resolve it**: Systematic analysis comparing activation geometry and safety representation structure across diverse VLM architectures

### Open Question 2
- **Question**: What is the mechanistic explanation for why visual inputs cause activation distributions to diverge so dramatically from text-only inputs?
- **Basis in paper**: [explicit] The authors observe: "the integration of images significantly shifts the model's internal activations during the forward pass, diverging from those triggered by textual input" and show t-SNE visualizations, but identify this as the phenomenon requiring investigation rather than explaining its cause
- **Why unresolved**: The paper demonstrates the distributional shift and its safety consequences but does not isolate whether the cause lies in vision encoder properties, cross-modal attention dynamics, or embedding space misalignment
- **What evidence would resolve it**: Causal intervention studies analyzing how visual tokens influence attention patterns and activation trajectories layer-by-layer

### Open Question 3
- **Question**: Can revision strength α and intervention layers be determined automatically per input rather than through manual hyperparameter search?
- **Basis in paper**: [inferred from methodological limitation] The authors note that "The effectiveness of our method is influenced by the revision strength α, as well as the specific revision layers and heads utilized" and that excessive strength causes over-defensiveness, yet rely on grid search rather than adaptive mechanisms
- **Why unresolved**: The composite score curves suggest optimal parameters vary across inputs, but the paper uses fixed configurations after tuning
- **What evidence would resolve it**: Experiments with input-conditioned or learned adaptation modules that dynamically adjust revision parameters

## Limitations
- Sample construction dependency: Multi-Response requires generating full safe/unsafe responses for each harmful prompt, limiting scalability
- Head selection mechanism remains underspecified despite 70% heads being used
- Transfer performance shows high variance (CS range 1.57-23.68) suggesting strong model-specific dependencies

## Confidence
**High confidence**: The general activation revision framework works as described, with clear improvements in safety metrics across multiple benchmarks
**Medium confidence**: The claim that visual inputs cause systematic activation distribution shifts is supported by t-SNE visualization but lacks quantitative characterization
**Low confidence**: The mechanism by which head-level revision outperforms layer-level (localization of safety-relevant computations) remains speculative without head-wise ablation studies

## Next Checks
1. **Head-specific impact analysis**: Run head-level revision with only the top 10% most effective heads versus random 70% heads to quantify localization versus coverage tradeoffs
2. **Cross-domain safety transfer**: Apply revision vectors trained on VLGuard to safety benchmarks involving different harmful content types to test domain generalization
3. **Activation space geometry quantification**: Compute Wasserstein distance between text-only and multimodal activation distributions across layers to measure the magnitude of shift that existing safety alignments fail to bridge