---
ver: rpa2
title: Generating Reading Comprehension Exercises with Large Language Models for Educational
  Applications
arxiv_id: '2511.18860'
source_url: https://arxiv.org/abs/2511.18860
tags:
- generation
- reading
- rceg
- comprehension
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RCEG, a framework that generates high-quality,
  personalized English reading comprehension exercises using LLMs. The method fine-tunes
  models with SFT and PPO, and uses DATG and GeDi for post-generation filtering to
  improve relevance and safety.
---

# Generating Reading Comprehension Exercises with Large Language Models for Educational Applications

## Quick Facts
- **arXiv ID:** 2511.18860
- **Source URL:** https://arxiv.org/abs/2511.18860
- **Reference count:** 26
- **Primary result:** RCEG framework generates personalized English reading comprehension exercises using LLMs with SFT, PPO, DATG, and GeDi filtering

## Executive Summary
This paper proposes RCEG, a framework that generates high-quality, personalized English reading comprehension exercises using LLMs. The method fine-tunes models with SFT and PPO, and uses DATG and GeDi for post-generation filtering to improve relevance and safety. Experiments on a dedicated dataset show RCEG significantly improves BLEU-4 (e.g., 27.23→39.52), ROUGE scores, and task accuracy. It also enhances robustness under toxic inputs, achieving lower perplexity and better alignment. The framework enables scalable, adaptive exercise generation for intelligent education systems.

## Method Summary
The framework employs a two-stage pipeline: Stage 1 uses SFT with LoRA on RACE dataset (24,636 samples) to teach instruction-following, followed by reward model training on Gaokao/PG preference pairs (437 pairs). Stage 2 applies PPO with KL regularization to optimize policy using reward scores. Post-processing generates 30 candidates via DATG, applies logit manipulation (λ₁=4.0 boost, λ₂=6.0 suppress) to produce 3 candidates, then filters via GeDi toxic classifier. All training uses single RTX 4090 with LLaMA-Factory.

## Key Results
- BLEU-4 improves from 27.23 to 39.52 on test set
- ROUGE-1/2/L scores increase significantly across models
- FreeEval reasoning task accuracy improves (e.g., Qwen2.5-3B: 64.56→72.46)
- Perplexity drops on toxic inputs (Qwen2.5-1.5B: 37.76→29.59)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sequential instruction tuning followed by reinforcement learning improves pedagogical alignment and lexical quality of generated exercises.
- **Mechanism:** SFT with LoRA first teaches the model instruction-following by maximizing log-likelihood of expert-formatted reading comprehension triplets. A separate reward model then learns to rank preferred vs. rejected outputs via pairwise loss. PPO subsequently optimizes the policy using reward model scores while KL regularization prevents drift from the SFT-initialized behavior.
- **Core assumption:** Preference pairs constructed from Gaokao/PG corpora generalize to broader reading comprehension quality judgments.
- **Evidence anchors:**
  - [abstract] "fine-tunes models with SFT and PPO...experiments show RCEG significantly improves BLEU-4 (27.23→39.52)"
  - [section 3.1] Equations 1–5 define L_SFT, L_RM, and L_PPO with KL penalty β
  - [corpus] Limited direct corpus validation; neighboring papers (e.g., "Question Generation for Assessing Early Literacy") address question generation but do not evaluate this specific SFT→PPO sequencing
- **Break condition:** If preference pairs lack pedagogical diversity or contain annotation noise, reward model may learn spurious ranking signals, causing PPO to optimize incorrect objectives.

### Mechanism 2
- **Claim:** Post-hoc logit manipulation via attribute graphs steers generation toward desired properties (non-toxicity, fluency) without modifying model weights.
- **Mechanism:** After PPO model produces baseline output, 30 stochastic variants are sampled. An attribute classifier scores each. Positive/negative token graphs encode transition likelihoods toward high/low alignment. During decoding, logits of positively-ranked tokens are boosted (λ₁=4.0) and negatively-ranked tokens suppressed (λ₂=6.0), producing 3 guided candidates.
- **Core assumption:** Token-level attribute signals aggregate meaningfully to sentence-level quality improvements.
- **Evidence anchors:**
  - [section 3.2] Equation 7 shows modified softmax: P̃(x_t|x_<t) = softmax(z_t + λ₁·m_pos − λ₂·m_neg)
  - [section 4.3 Table 4] Perplexity drops on ToxicTop dataset (e.g., Qwen2.5-1.5B: 37.76→29.59)
  - [corpus] Weak corpus validation; no neighboring papers directly test DATG for educational content
- **Break condition:** If attribute classifier is poorly calibrated or target attributes conflict (e.g., fluency vs. complexity), logit interference may produce incoherent outputs.

### Mechanism 3
- **Claim:** Discriminator-based reranking of multiple candidates improves safety by filtering toxic or misaligned outputs.
- **Mechanism:** Three DATG-guided candidates are scored by a GeDi-derived toxic classifier. The candidate with highest non-toxicity score is selected as final output via argmax over classifier scores.
- **Core assumption:** Toxicity classification is sufficient proxy for broader pedagogical safety (age-appropriateness, cultural sensitivity).
- **Evidence anchors:**
  - [section 3.2] Equation 8: R_final = argmax_i(f_GeDi(R̃_i))
  - [abstract] "uses DATG and GeDi for post-generation filtering to improve relevance and safety"
  - [corpus] Neighboring work on "ZPD-SCA" examines cognitive alignment assessment but not toxicity filtering specifically
- **Break condition:** If classifier has high false-negative rate on subtle toxicity or penalizes legitimate educational content, filtering introduces safety gaps or over-restriction.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** Enables fine-tuning on single RTX 4090 by freezing base model and training only low-rank matrices in attention layers
  - **Quick check question:** Can you explain why LoRA reduces memory compared to full fine-tuning while preserving expressivity?

- **Concept: Proximal Policy Optimization (PPO) with KL Penalty**
  - **Why needed here:** Stabilizes RL fine-tuning by clipping policy ratios and penalizing deviation from reference policy
  - **Quick check question:** What happens to training if β (KL penalty coefficient) is set too low vs. too high?

- **Concept: Pairwise Preference Learning for Reward Models**
  - **Why needed here:** Converts human/AI preferences into scalar rewards for PPO optimization
  - **Quick check question:** Why use sigmoid(Δr) rather than raw score difference as the loss objective?

## Architecture Onboarding

- **Component map:**
  ```
  [RACE/Gaokao Datasets] → [SFT LoRA Training] → [Reward Model Training]
                                    ↓
                           [PPO Policy Update] → [DATG Candidate Generation]
                                    ↓
                           [GeDi Toxicity Scoring] → [Final Output Selection]
  ```

- **Critical path:** SFT quality determines reward model training distribution; reward model quality determines PPO optimization direction; DATG/GeDi only filter, cannot fix fundamentally misaligned policy.

- **Design tradeoffs:**
  - λ₁=4.0, λ₂=6.0 boost/suppress strengths: higher values increase control but risk unnatural outputs
  - 3 final candidates: more candidates increase filtering options but add inference latency
  - LoRA vs. full fine-tuning: efficiency gains may limit task-specific adaptation depth

- **Failure signatures:**
  - BLEU/ROUGE high but human evaluation poor → reward model overfitting to surface metrics
  - Low perplexity but toxic outputs pass through → GeDi classifier threshold miscalibrated
  - PPO loss diverges → KL penalty β too small or reward scale unstable

- **First 3 experiments:**
  1. Replicate SFT→PPO pipeline on Qwen2.5-3B base; verify BLEU-4 improvement matches reported ~27→39 range
  2. Ablate DATG (use only PPO output without logit manipulation); compare perplexity and toxicity metrics
  3. Replace GeDi classifier with alternative toxicity detector; measure false-positive rate on legitimate educational content

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the RCEG framework be effectively extended to support multi-turn dialogue generation and domain adaptation for real-world classroom deployment?
  - **Basis in paper:** [explicit] The Conclusion states, "In future work, we plan to extend the framework to multi-turn dialogue generation, domain adaptation, and real-world classroom deployment."
  - **Why unresolved:** The current study focuses solely on single-turn exercise generation and evaluates the system using automated metrics and a demo interface, without testing complex interactive scenarios.
  - **What evidence would resolve it:** Successful implementation of a chat-based interface that maintains context over multiple turns and empirical results from pilot programs in actual educational environments.

- **Open Question 2:** Why does the RCEG-SP optimization (SFT + PPO) lead to performance degradation in specific zero-shot reasoning tasks (e.g., ARC scores for smaller models) despite improvements in text similarity?
  - **Basis in paper:** [inferred] Table 3 shows that RCEG-SP causes the Qwen2.5-1.5B model's ARC score to drop from 64.24 to 54.69, and the text notes "mixed" results with "possible trade-offs in generalization."
  - **Why unresolved:** The paper reports the trade-off but does not provide a mechanistic explanation for why reward optimization for generation quality harms specific reasoning capabilities.
  - **What evidence would resolve it:** Ablation studies isolating the impact of SFT vs. PPO on reasoning benchmarks, or analysis of the attention mechanisms during the degraded tasks.

- **Open Question 3:** To what extent do the reported improvements in surface-level metrics (BLEU, ROUGE) translate into actual pedagogical value and improved learning outcomes for students?
  - **Basis in paper:** [inferred] The evaluation relies heavily on automated NLP metrics (BLEU, ROUGE, PPL) and reasoning benchmarks (FreeEval), but lacks studies measuring student engagement, comprehension improvement, or teacher usability.
  - **Why unresolved:** High lexical overlap or fluency does not guarantee that the content is pedagogically effective or aligned with specific curriculum standards in a way that aids learning.
  - **What evidence would resolve it:** Comparative studies where students use RCEG-generated exercises versus human-authored ones, measuring pre- and post-test learning gains.

## Limitations

- The evaluation relies heavily on automated metrics without extensive human evaluation of pedagogical quality
- Toxicity filtering may not capture nuanced cultural or developmental appropriateness concerns
- Preference dataset construction depends on DeepSeek-R1 API, introducing potential annotation noise

## Confidence

- **High confidence:** The sequential SFT→PPO pipeline architecture is technically sound and follows established LLM fine-tuning practices. The improvements in BLEU-4 (27.23→39.52) and ROUGE scores are directly measurable from the experimental results.
- **Medium confidence:** The post-generation filtering via DATG and GeDi effectively reduces perplexity on toxic inputs, but the practical educational impact depends on classifier quality and may not generalize across diverse cultural contexts.
- **Low confidence:** The claim of "personalized" exercise generation lacks demonstration of actual adaptation to individual student characteristics beyond basic content filtering.

## Next Checks

1. Conduct human evaluation study comparing RCEG outputs with baseline models on pedagogical quality dimensions (clarity, age-appropriateness, alignment with learning objectives).
2. Test GeDi classifier performance on educational content across different age groups and cultural contexts to identify potential false-positive/negative patterns.
3. Implement ablation study removing the preference learning component (using only SFT without PPO) to quantify the contribution of reinforcement learning to final performance.