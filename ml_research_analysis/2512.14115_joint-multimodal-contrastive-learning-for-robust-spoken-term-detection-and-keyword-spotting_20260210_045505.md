---
ver: rpa2
title: Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and
  Keyword Spotting
arxiv_id: '2512.14115'
source_url: https://arxiv.org/abs/2512.14115
tags:
- audio
- word
- text
- embeddings
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a joint multimodal contrastive learning framework
  for Acoustic Word Embeddings (AWEs) that addresses limitations in existing approaches
  by unifying audio-audio and audio-text supervision. The proposed method combines
  CLAP-style audio-text contrastive alignment with DWD-style audio-audio discrimination
  to learn a shared embedding space.
---

# Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting

## Quick Facts
- arXiv ID: 2512.14115
- Source URL: https://arxiv.org/abs/2512.14115
- Reference count: 28
- Primary result: Joint multimodal contrastive learning achieves 85.05% AP on in-vocabulary and 94.06% AP on out-of-vocabulary word discrimination

## Executive Summary
This paper introduces a joint multimodal contrastive learning framework for Acoustic Word Embeddings (AWEs) that addresses limitations in existing approaches by unifying audio-audio and audio-text supervision. The proposed method combines CLAP-style audio-text contrastive alignment with DWD-style audio-audio discrimination to learn a shared embedding space. Experiments on LibriSpeech demonstrate that the model achieves 85.05% AP on in-vocabulary word discrimination and 94.06% on out-of-vocabulary words, outperforming baselines. For Spoken Term Detection and Keyword Spotting, it achieves up to 15.71% EER on IV queries and 18.35% on OOV queries. The framework supports both tasks flexibly within a single model and shows robustness to noisy and speaker-variant conditions.

## Method Summary
The framework learns AWEs through joint optimization of two contrastive losses: a CLAP-style audio-text alignment loss and a DWD-style audio-audio discrimination loss. Audio and text encoders are both 3-layer BiLSTMs that map inputs to 512-dimensional embeddings. The CLAP loss aligns audio segments with their textual transcriptions, while the DWD loss ensures audio segments of the same word are closer than those of different words. The model is trained on LibriSpeech with mel-spectrogram inputs and phoneme representations for text, using AdamW optimization with OneCycleLR scheduling. The joint training allows the model to handle both Query-by-Example (audio query) and text-based keyword search tasks within a unified framework.

## Key Results
- Achieves 85.05% Average Precision on in-vocabulary word discrimination
- Achieves 94.06% Average Precision on out-of-vocabulary word discrimination
- STD/KWS performance reaches 15.71% EER for IV queries and 18.35% EER for OOV queries
- Outperforms baselines including CLAP and DWD in most evaluation scenarios

## Why This Works (Mechanism)
The framework's effectiveness stems from leveraging complementary information from both audio-audio and audio-text supervision. The DWD loss ensures that acoustically similar word instances are mapped to similar embeddings regardless of speaker or recording conditions, while the CLAP loss aligns these acoustic representations with their semantic textual counterparts. This dual supervision enables the model to learn embeddings that capture both phonetic similarity and semantic meaning, making them effective for both audio-to-audio matching (STD) and audio-to-text retrieval (KWS). The joint optimization creates a shared embedding space where semantically and phonetically similar items are close together, enabling robust performance across different query types and conditions.

## Foundational Learning
- **Acoustic Word Embeddings (AWEs)**: Dense vector representations that capture the acoustic characteristics of spoken words. Needed because traditional discrete representations lack the continuous semantic structure required for similarity-based retrieval. Quick check: Verify embeddings preserve phonetic similarity through nearest neighbor analysis.
- **Contrastive Learning**: Framework that learns representations by pulling similar items together and pushing dissimilar items apart. Essential for learning discriminative embeddings without explicit labels. Quick check: Monitor loss convergence and ensure positive pairs consistently outperform negative pairs.
- **BiLSTM Encoders**: Bidirectional LSTM networks that capture temporal dependencies in both forward and backward directions. Critical for encoding the sequential nature of speech and text. Quick check: Validate that both encoders produce similar embedding distributions through t-SNE visualization.
- **CLAP-style Alignment**: Contrastive loss that aligns audio and text representations in a shared space. Provides semantic supervision that complements acoustic similarity. Quick check: Measure text-to-audio retrieval accuracy as a proxy for alignment quality.
- **DWD-style Discrimination**: Loss that ensures same-word instances are closer than different-word instances. Provides fine-grained acoustic discrimination without text supervision. Quick check: Compute within-word vs between-word distance distributions.

## Architecture Onboarding

**Component Map**: Audio input → BiLSTM → Projection → Embedding (512-dim) <-> Text input → BiLSTM → Projection → Embedding (512-dim)

**Critical Path**: Audio encoder output → DWD loss computation → Gradients → Text encoder output → CLAP loss computation → Gradients

**Design Tradeoffs**: The framework trades model complexity (separate encoders for audio and text) for task flexibility (unified model for STD and KWS). Using BiLSTMs provides temporal modeling capability but may limit scalability compared to transformer architectures.

**Failure Signatures**: 
- Low IV AP but high OOV AP suggests overfitting to training vocabulary
- High EER on clean speech but low EER on noisy speech indicates poor noise robustness
- Degraded performance with longer window sizes suggests temporal resolution issues

**Three First Experiments**:
1. **Encoder Sanity Check**: Train each encoder independently with its respective loss (DWD for audio, CLAP for text) and verify they learn meaningful representations before joint training.
2. **Embedding Space Visualization**: Use t-SNE to visualize the learned embedding space and verify that same-word instances cluster together while different words are separated.
3. **Window Size Sensitivity**: Test STD performance across different window sizes (0.2s to 0.6s) to identify optimal temporal resolution for the task.

## Open Questions the Paper Calls Out
None

## Limitations
- Phoneme representation for text encoder is unspecified, creating uncertainty about handling diverse linguistic inputs
- DWD positive sample count M is not specified, potentially affecting training dynamics
- Performance gains may be partly attributed to LibriSpeech's clean speech conditions, limiting real-world applicability
- Forced alignment dependency introduces potential error propagation from boundary inaccuracies

## Confidence
- **High confidence**: Core methodology and experimental setup are sound and reproducible
- **Medium confidence**: OOV performance claims need validation on diverse corpora
- **Low confidence**: Claims about unifying audio-audio and audio-text learning may be overstated

## Next Checks
1. Implement multiple phoneme extraction methods (G2P tools, forced alignment outputs) and evaluate their impact on text encoder performance to determine sensitivity to phoneme representation choices.
2. Systematically vary the number of positive samples M in the DWD loss and measure its effect on both IV and OOV discrimination performance to identify optimal settings.
3. Evaluate the trained model on a different corpus (Common Voice or TED-LIUM) to assess whether the reported performance gains transfer beyond LibriSpeech conditions.