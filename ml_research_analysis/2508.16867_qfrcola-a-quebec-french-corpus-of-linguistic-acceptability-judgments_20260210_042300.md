---
ver: rpa2
title: 'QFrCoLA: a Quebec-French Corpus of Linguistic Acceptability Judgments'
arxiv_id: '2508.16867'
source_url: https://arxiv.org/abs/2508.16867
tags:
- linguistic
- language
- french
- acceptability
- qfrcola
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QFrCoLA, the Quebec-French Corpus of Linguistic
  Acceptability Judgments, comprising 25,153 in-domain and 2,675 out-of-domain normative
  sentences. It is the first large-scale French acceptability dataset and the second
  largest in any language.
---

# QFrCoLA: a Quebec-French Corpus of Linguistic Acceptability Judgments

## Quick Facts
- **arXiv ID**: 2508.16867
- **Source URL**: https://arxiv.org/abs/2508.16867
- **Reference count**: 13
- **Primary result**: QFrCoLA is the first large-scale French acceptability dataset; fine-tuned monolingual models outperform zero-shot LLMs and cross-lingual approaches.

## Executive Summary
This paper introduces QFrCoLA, the Quebec-French Corpus of Linguistic Acceptability Judgments, comprising 25,153 in-domain and 2,675 out-of-domain normative sentences. It is the first large-scale French acceptability dataset and the second largest in any language. The study benchmarks seven language models across eight languages, including French. Results show that fine-tuned Transformer-based language models are strong baselines for most languages. However, zero-shot large language models perform poorly on the task, even those optimized for French. For the QFrCoLA benchmark, fine-tuned models outperformed other methods tested. The findings also indicate that pre-trained cross-lingual models do not appear to have acquired Quebec French linguistic judgment capabilities during pre-training. Overall, QFrCoLA proves to be a challenging dataset suitable for benchmarking language models on linguistic acceptability judgments.

## Method Summary
The study creates QFrCoLA by extracting sentences from the Banque de dépannage linguistique (BDL) for in-domain data and Académie française for out-of-domain data, labeling them as grammatically acceptable or unacceptable based on normative judgments. The dataset contains 25,153 in-domain sentences and 2,675 out-of-domain hold-out sentences, categorized by linguistic phenomenon (syntax, morphology, semantic, anglicism). Models are evaluated using accuracy and Matthews Correlation Coefficient (MCC), with CamemBERT-base fine-tuned using AdamW optimizer (lr=3e-5, weight_decay=1e-2, batch_size=32, max_seq_length=64, 4 epochs) and weighted balanced cross-entropy loss. Results are reported over 10 restarts with seeds 42-51.

## Key Results
- Fine-tuned CamemBERT achieves 84.51±0.78 accuracy and 0.619 MCC on QFrCoLA test, outperforming LA-TDA method
- Zero-shot LLMs (Mistral, Llama, BLOOM, Lucie) perform at or below baseline, with MCC ≈ 0 indicating random-level predictions
- OOD performance drops ~22% in accuracy and ~50% in MCC, indicating significant overfitting to Quebec French norms
- Anglicism category shows lowest performance (74.36%) compared to syntax (88.59%), suggesting pre-training exposure affects detection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuned monolingual Transformer-based LMs provide strong baselines for binary linguistic acceptability classification across languages.
- **Mechanism:** Supervised fine-tuning on language-specific acceptability corpora enables models to learn discriminative features for grammatical vs. ungrammatical sentences through attention-based representations.
- **Core assumption:** The training corpus captures normative linguistic phenomena representative of the target language's grammatical constraints.
- **Evidence anchors:**
  - [abstract] "fine-tuned Transformer-based LM are strong baselines for most languages"
  - [section 5.1] "BERT achieves 84.51±0.78 accuracy and 0.619 MCC on QFrCoLA test, outperforming LA-TDA"
  - [corpus] Neighbor paper "QFrBLiMP" extends this work with minimal pairs for Quebec-French, suggesting the benchmark paradigm is being adopted.
- **Break condition:** Performance degrades significantly on OOD data (~22% accuracy drop, ~50% MCC drop on QFrCoLA OOD), indicating overfitting to training distribution.

### Mechanism 2
- **Claim:** Zero-shot LLMs lack intrinsic linguistic acceptability judgment capabilities, performing at or below naive baselines.
- **Mechanism:** Pre-trained LLMs optimize for next-token probability rather than grammatical well-formedness; without task-specific fine-tuning, they cannot reliably distinguish acceptable from unacceptable sentences.
- **Core assumption:** LLM performance on generation tasks does not transfer to grammaticality classification.
- **Evidence anchors:**
  - [abstract] "zero-shot binary classification large language models perform poorly on the task"
  - [section 5.1] "LLM accuracy performances are either worse than the baselines or at par with it for all languages except Norwegian"
  - [corpus] Weak corpus signal—no direct replications of this zero-shot failure finding yet (avg neighbor citations = 0).
- **Break condition:** Assumption: Instruction-tuned variants perform better than base models (section 5.1 notes instruct versions "double or less the performance"), but still below fine-tuned approaches.

### Mechanism 3
- **Claim:** Cross-lingual pre-training does not transfer linguistic acceptability judgment capabilities to Quebec French.
- **Mechanism:** Cross-lingual models like XLM-RoBERTa learn shared representations across languages, but linguistic acceptability requires language-specific grammatical knowledge not captured through multilingual pre-training alone.
- **Core assumption:** Linguistic phenomena are sufficiently distinct across languages that shared representations do not encode acceptability judgments.
- **Evidence anchors:**
  - [abstract] "pre-trained cross-lingual models do not appear to have acquired Quebec French linguistic judgment capabilities during pre-training"
  - [section 5.1] "RoBERTa does not seem to acquire cross-lingual linguistic capabilities from potentially similar linguistic phenomena amongst languages"
  - [corpus] "Low-Resource Dialect Adaptation" paper (FMR=0.61) explores French dialect adaptation, suggesting transfer remains an open problem.
- **Break condition:** Training on multilingual acceptability corpora without proper grammar assessment may prevent comprehension of language-specific linguistics.

## Foundational Learning

- **Concept: Matthews Correlation Coefficient (MCC)**
  - Why needed here: MCC is the primary evaluation metric for acceptability tasks because it handles class imbalance better than accuracy (QFrCoLA has 69.49% acceptable sentences).
  - Quick check question: Why might accuracy alone be misleading for this dataset?

- **Concept: Linguistic Acceptability vs. Perceived Acceptability**
  - Why needed here: QFrCoLA uses normative judgments from linguistic authorities (OQLF, Académie française), not speaker intuitions—this affects what "correct" means.
  - Quick check question: How would a corpus built on native speaker intuitions differ from one built on normative grammar?

- **Concept: Out-of-Domain (OOD) Evaluation**
  - Why needed here: OOD splits reveal whether models generalize or overfit; QFrCoLA's OOD (from France-French sources) tests Quebec-French specificity.
  - Quick check question: Why does the OOD split have ~15% fewer acceptable sentences than the in-domain split?

## Architecture Onboarding

- **Component map:** Extract sentences from BDL and Académie française → binary labeling → category annotation → fine-tune CamemBERT-base → evaluate on test/OOD splits using accuracy + MCC
- **Critical path:** 1) Fine-tune CamemBERT-base on in-domain train/dev, 2) Evaluate on test split first, then OOD for generalization check, 3) Compare against baseline and zero-shot LLM benchmarks
- **Design tradeoffs:**
  - **LA-TDA vs. BERT:** LA-TDA (attention-map logistic regression) slightly outperforms on some corpora but adds O(n²) complexity for marginal gains
  - **Monolingual vs. Cross-lingual:** Monolingual outperforms cross-lingual for QFrCoLA; cross-lingual transfer doesn't work for this task
  - **In-domain vs. OOD:** Training only on in-domain risks overfitting; OOD exposes ~22% accuracy drop
- **Failure signatures:**
  - **Anglicism category:** Lowest performance (BERT: 74.36% vs. 88.59% for syntax)—models may have seen anglicisms in pre-training web data
  - **Zero-shot LLM MCC ≈ 0:** Indicates random-level predictions despite decent accuracy (accuracy alone is misleading with class imbalance)
  - **OOD performance collapse:** 50% MCC drop signals overfitting to training distribution
- **First 3 experiments:**
  1. Fine-tune CamemBERT-base on QFrCoLA train split, evaluate on test using MCC—establish monolingual baseline.
  2. Run zero-shot classification with Mistral-7B and Lucie-7B using HuggingFace's pipeline—confirm LLM failure mode.
  3. Evaluate fine-tuned model on OOD split and per-category breakdown—identify overfitting and category-specific weaknesses (especially anglicisms).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generating complementary grammatical or ungrammatical sentences for each entry in QFrCoLA create an effective minimal pair benchmark for French?
- Basis in paper: [explicit] The authors state, "In our future works... generate the complementary grammatical or ungrammatical sentence of each sentence in the dataset to create the first French minimal pair benchmark dataset."
- Why unresolved: The current corpus supports binary classification but lacks the paired structure necessary to isolate and evaluate specific linguistic phenomena.
- What evidence would resolve it: A constructed dataset of minimal pairs where model performance is validated against specific syntactic or morphological variances.

### Open Question 2
- Question: What specific linguistic error patterns drive the poor performance of zero-shot Large Language Models on Quebec French acceptability tasks?
- Basis in paper: [explicit] The authors list plans to "explore the linguistic phenomena errors generated by the LLM qualitatively" as future work.
- Why unresolved: The paper reports quantitative failure (low accuracy/MCC) but does not analyze whether models fail more on specific categories like anglicisms or syntax.
- What evidence would resolve it: A qualitative error analysis categorizing zero-shot model misclassifications by the linguistic phenomenon (e.g., anglicism vs. morphology).

### Open Question 3
- Question: To what extent does the heavy morphological bias in QFrCoLA (42% of samples) limit the evaluation of a model's syntactic competence?
- Basis in paper: [inferred] The authors acknowledge the categories are "unevenly distributed" and note that this imbalance "could provide an incomplete evaluation of a LM's ability."
- Why unresolved: High reported accuracy may mask specific weaknesses in under-represented categories, creating a false impression of general linguistic competence.
- What evidence would resolve it: Evaluation results on a re-sampled, balanced dataset where morphology does not dominate the training or test distribution.

### Open Question 4
- Question: Is the significant performance drop on the out-of-domain (OOD) set caused by the distinct grammatical norms of France-French versus Quebec-French?
- Basis in paper: [inferred] The paper notes a 22% accuracy drop on the OOD set and hypothesizes it may be due to accepted grammar differences (e.g., feminization of nouns) between the two regions.
- Why unresolved: The paper observes the correlation but does not disentangle regional normative differences from other overfitting factors.
- What evidence would resolve it: An ablation study identifying OOD errors specifically attributable to known regional differences (e.g., *auteure* vs. *autrice*).

## Limitations

- Dataset relies on normative judgments from linguistic authorities rather than native speaker intuitions, creating uncertainty about real-world acceptability patterns
- Zero-shot LLM failure may be premature conclusion without exploring intermediate approaches like few-shot learning or prompt engineering
- Category imbalance (42% morphology) may create false impression of general linguistic competence while masking weaknesses in under-represented categories

## Confidence

**High Confidence (5/5)**: Fine-tuned monolingual Transformer models (CamemBERT) achieve strong performance on in-domain QFrCoLA classification, significantly outperforming baselines and zero-shot approaches. The experimental methodology and results are reproducible and consistent with established NLP practices.

**Medium Confidence (3/5)**: Cross-lingual pre-training doesn't transfer Quebec French acceptability capabilities. While results support this claim, the study doesn't rule out alternative explanations such as insufficient cross-lingual grammatical supervision or architectural limitations.

**Low Confidence (2/5)**: Zero-shot LLMs fundamentally lack linguistic acceptability judgment capabilities. The paper demonstrates poor performance but doesn't explore intermediate methods (few-shot, prompt engineering) that might bridge the gap between zero-shot and fine-tuning.

## Next Checks

1. **Normative vs. Descriptive Acceptability**: Create a subset of QFrCoLA sentences with native speaker acceptability judgments and compare model performance on normative vs. descriptive acceptability. This would validate whether models trained on normative data generalize to actual usage patterns.

2. **Cross-Lingual Transfer with Grammatical Supervision**: Fine-tune XLM-RoBERTa on a multilingual acceptability corpus (e.g., combining CoLA, JFCOCA, and QFrCoLA) and evaluate on Quebec French. This would test whether cross-lingual transfer failure stems from data distribution or architectural limitations.

3. **Intermediate LLM Approaches**: Implement few-shot learning and prompt engineering experiments with Mistral-7B and Lucie-7B on QFrCoLA. Compare performance against zero-shot and fully fine-tuned approaches to determine if LLMs can achieve reasonable acceptability judgments without extensive fine-tuning.