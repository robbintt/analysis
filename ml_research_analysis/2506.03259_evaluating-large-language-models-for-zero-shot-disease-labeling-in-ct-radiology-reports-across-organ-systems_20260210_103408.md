---
ver: rpa2
title: Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology
  Reports Across Organ Systems
arxiv_id: '2506.03259'
source_url: https://arxiv.org/abs/2506.03259
tags:
- reports
- labels
- llms
- were
- llama-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated three lightweight open-weight LLMs (Llama-3.1\
  \ 8B, Llama-UM, Gemma-3 27B) alongside a rule-based algorithm (RBA) and fine-tuned\
  \ RadBERT for zero-shot disease labeling in CT radiology reports across organ systems.\
  \ The LLMs achieved the highest agreement (median Cohen's \u03BA: 0.87) and outperformed\
  \ rule-based methods in macro-F1 scores on manually annotated reports (Gemma-3 27B:\
  \ 0.82, Llama-3.1 8B: 0.79, RBA: 0.64)."
---

# Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems

## Quick Facts
- arXiv ID: 2506.03259
- Source URL: https://arxiv.org/abs/2506.03259
- Reference count: 0
- Lightweight LLMs achieved higher agreement and macro-F1 scores than rule-based methods for zero-shot CT report labeling

## Executive Summary
This study evaluates three lightweight open-weight LLMs (Llama-3.1 8B, Gemma-3 27B) alongside a rule-based algorithm and fine-tuned RadBERT for zero-shot disease labeling in CT radiology reports across kidney/ureter, liver/gallbladder, and lung/pleura systems. The LLMs achieved the highest inter-model agreement (median Cohen's κ: 0.87) and outperformed traditional methods in macro-F1 scores on manually annotated reports. Performance differences were largely attributed to differing labeling practices, particularly for ambiguous findings like atelectasis and kidney lesions, where clinical context influenced binary label assignment.

## Method Summary
The evaluation used 40,833 CAP CT reports from Duke (29,540 patients) with 1,789 manually annotated reports and external validation on CT-RATE dataset (21,304 reports). Zero-shot prompting with structured JSON output was employed for three LLMs (Llama-3.1-8B-Instruct, Llama-UltraMedical, Gemma-3-27B) and compared against rule-based algorithm (RBA) and RadBERT fine-tuned on RBA pseudo-labels. The 15 disease labels spanned three organ systems. Performance was measured using Cohen's Kappa for agreement and macro-F1/micro-F1 scores with 95% CI via bootstrap resampling.

## Key Results
- Lightweight LLMs achieved highest inter-model agreement (median Cohen's κ: 0.87)
- Gemma-3 27B achieved top macro-F1 score (0.82), followed by Llama-3.1 8B (0.79) and RBA (0.64)
- On external CT-RATE validation, Llama-3.1 8B performed best (F1: 0.91)
- Majority vote ensemble slightly improved performance (macro F1: 0.84)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuned LLMs can extract structured multi-label annotations from CT reports via zero-shot prompting with JSON-formatted outputs.
- Mechan: Pretraining on diverse corpora including medical text, combined with instruction tuning, enables models to parse clinical language and map findings to structured schemas without task-specific training.
- Core assumption: The distribution of clinical language in CT reports overlaps sufficiently with the model's pretraining distribution to support accurate zero-shot inference.
- Evidence anchors:
  - [abstract] "Three open-weight LLMs were tested with zero-shot prompting... Gemma-3 27B achieved the top macro-F1 (0.82), followed by Llama-3.1 8B (0.79)"
  - [section] "For each report, zero-shot prompts (Figure 2) were employed to generate a JavaScript Object Notation (JSON) dictionary, which contained a pseudo-ID number and a true/false decision for each of the 15 labels"
  - [corpus] Limited direct corpus support; neighboring paper "Agent-Based Uncertainty Awareness" suggests LLM labeling is viable but requires uncertainty quantification for trustworthiness
- Break condition: Performance degrades substantially on reports with highly ambiguous or contradictory findings that exceed the model's ability to resolve without explicit reasoning chains.

### Mechanism 2
- Claim: Agreement between models reflects shared interpretation of unambiguous findings, while disagreement clusters around clinically subjective cases.
- Mechan: Models converge on clear disease descriptors (e.g., "pleural effusion present") but diverge when reports contain qualifiers like "gravity-dependent atelectasis" or "too small to characterize" that lack standardized binary interpretations.
- Core assumption: Model disagreement signals label ambiguity rather than pure model error.
- Evidence anchors:
  - [abstract] "Performance differences were largely due to differing labeling practices, particularly for ambiguous findings like atelectasis"
  - [section] "When evaluated on the simplified manual set that assigned labels solely on whether a finding was present... F1 scores substantially increased for almost all models on kidney lesion and atelectasis"
  - [corpus] Weak corpus evidence; neighboring papers do not directly address subjectivity in labeling
- Break condition: Disagreement patterns become random rather than clustering around specific linguistic features, suggesting model failures rather than label ambiguity.

### Mechanism 3
- Claim: Majority-vote ensembles of heterogeneous models improve robustness by averaging out model-specific biases.
- Mechan: RBA, Llama-3.1 8B, and Gemma-3 27B have different failure modes (rule brittleness, hallucination, over-inference); voting reduces single-model errors when models make independent mistakes.
- Core assumption: Model errors are at least partially uncorrelated across architectural approaches.
- Evidence anchors:
  - [abstract] "Lightweight open-weight LLMs (Llama-3.1 8B, Gemma-3 27B) outperformed rule-based and RadBERT methods"
  - [section] "A majority vote ensemble, composed of the RBA, Llama-3.1 8B, and Gemma-3 27B, achieved a slightly higher macro F1 score than the best individual model... at 0.84"
  - [corpus] Ensemble methods are mentioned in neighboring literature but not directly validated for radiology labeling
- Break condition: Errors become highly correlated across models (e.g., all models misinterpret a specific phrase), eliminating ensemble benefit.

## Foundational Learning

- Concept: Zero-shot prompting with structured output
  - Why needed here: The entire evaluation framework depends on eliciting JSON-formatted multi-label outputs without fine-tuning or examples.
  - Quick check question: Can you write a system prompt that instructs a model to return only valid JSON with specific keys?

- Concept: Inter-rater agreement metrics (Cohen's Kappa)
  - Why needed here: Kappa is used to quantify model-to-model agreement and interpret performance relative to established benchmarks (0.61–0.80 = substantial, 0.81–1.00 = almost perfect).
  - Quick check question: Why is Kappa preferred over raw accuracy when comparing annotators with potential class imbalance?

- Concept: Pseudo-labeling and weak supervision
  - Why needed here: RadBERT was fine-tuned on RBA-generated pseudo-labels, which propagated RBA biases and limited its independence.
  - Quick check question: What are the risks of fine-tuning a model on labels generated by another model?

## Architecture Onboarding

- Component map:
  - RBA: Rule-based algorithm using regex patterns on findings sections
  - RadBERT: Domain-specific BERT fine-tuned on RBA pseudo-labels
  - Llama-3.1 8B / Gemma-3 27B: Instruction-tuned open-weight LLMs with zero-shot JSON extraction
  - Ensemble: Majority voting across RBA, Llama-3.1 8B, Gemma-3 27B

- Critical path:
  1. Extract "findings" section from CT reports (exclude impressions/history to avoid bias)
  2. Construct zero-shot prompt with 15 class labels and JSON schema
  3. Run inference on local GPU (A5000 for 8B models, 2×A6000 for 27B)
  4. Parse JSON outputs; flag malformed responses for review
  5. Compute Kappa for agreement and F1 against reference labels

- Design tradeoffs:
  - Binary labels vs. nuanced extraction: Binary enables comparability but discards clinical ambiguity (e.g., "too small to characterize")
  - Zero-shot vs. fine-tuning: Zero-shot preserves privacy and generalization; fine-tuning may improve accuracy but requires labeled data
  - RBA pseudo-labels vs. manual labels: Pseudo-labels scale but encode rule biases; manual labels are accurate but costly

- Failure signatures:
  - Llama-UM: Higher rate of misformatted JSON due to medical chatbot fine-tuning mismatch
  - All models: Low F1 on "normal" classes due to differing operational definitions
  - All models: Poor performance on kidney lesion and atelectasis due to subjective language

- First 3 experiments:
  1. Reproduce zero-shot JSON extraction on 100 reports; measure format compliance rate and parsing errors
  2. Compare Llama-3.1 8B vs. Gemma-3 27B on a held-out subset; compute per-label F1 to identify systematic differences
  3. Implement majority-vote ensemble; validate that ensemble F1 exceeds best single model on your manual subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can non-binary label schemas (e.g., graded severity or probability scores) better capture the clinical nuance of CT report findings than binary classification?
- Basis in paper: [explicit] "In future work, we aim to move beyond binary labeling and develop more flexible systems that can capture the nuances of linguistic ambiguity and clinical relevance."
- Why unresolved: Current study required binary labels for cross-model comparability, but this cannot fully represent findings like "too small to characterize" lesions or "gravity-dependent atelectasis."
- What evidence would resolve it: Experiments comparing multi-class/continuous schemas against binary labels, measuring alignment with clinical judgment and inter-rater reliability.

### Open Question 2
- Question: How much performance improvement can chain-of-thought reasoning or instruction-based fine-tuning provide over zero-shot prompting for CT report labeling?
- Basis in paper: [explicit] "The LLMs were evaluated using zero-shot prompting...which likely underestimates what could be achieved with chain-of-thought reasoning...instruction-based fine-tuning has been shown to improve performance."
- Why unresolved: Authors deliberately used zero-shot to assess baseline capabilities but did not test advanced prompting or fine-tuning strategies.
- What evidence would resolve it: Comparative experiments with chain-of-thought, few-shot prompting, and instruction-based fine-tuning on the same CT report datasets.

### Open Question 3
- Question: Can multi-institutional standardization of labeling criteria resolve performance discrepancies caused by differing clinical practices?
- Basis in paper: [explicit] "This inconsistency underscores the variability in labeling practices between datasets...exhaustively adjudicating such subjective cases across other disease labels and organ systems is not feasible at scale."
- Why unresolved: External validation showed performance gains largely attributable to different labeling conventions (e.g., CT-RATE marked dependent atelectasis positive while Duke labeled it negative).
- What evidence would resolve it: Multi-institutional study with pre-adjudicated labeling guidelines, comparing model performance before and after standardization.

### Open Question 4
- Question: Would multi-agent LLM frameworks with radiologist-defined specifications improve alignment with clinically actionable findings?
- Basis in paper: [explicit] "We also plan to explore multi-agent frameworks tailored to end-user specifications, in this case, radiologists, to better align model outputs with clinical needs."
- Why unresolved: Current single-model approach does not incorporate explicit clinical use-case specifications into the labeling process.
- What evidence would resolve it: Comparison of single-model vs. multi-agent framework outputs against radiologist judgments for specific clinical tasks.

## Limitations

- The exact zero-shot prompt structure (Figure 2) was not fully recoverable, requiring assumptions about JSON schema formatting
- RBA rule-based algorithm's regex definitions for 15 disease classes were not detailed, limiting baseline comparability
- External validation was limited to lung/pleura labels only, not the full 15-label set
- Binary labels inherently cannot capture clinical nuance of findings like "too small to characterize"

## Confidence

- **High Confidence**: LLM zero-shot prompting achieves substantial inter-model agreement (median κ=0.87) and outperforms rule-based methods on macro-F1 metrics
- **Medium Confidence**: Lightweight LLMs (8B and 27B) can effectively replace fine-tuned domain-specific models for CT report annotation
- **Low Confidence**: Lightweight LLMs generalize across organ systems without task-specific adaptation

## Next Checks

1. **Prompt Structure Validation**: Implement the zero-shot JSON extraction prompt on a held-out subset of 100 reports and measure format compliance rate and parsing error frequency
2. **Per-Label Performance Analysis**: Compute detailed per-label F1 scores for Llama-3.1 8B vs. Gemma-3 27B on a manually annotated subset to identify systematic differences in handling specific disease categories
3. **Ensemble Robustness Testing**: Validate that the majority-vote ensemble (RBA + Llama-3.1 8B + Gemma-3 27B) achieves higher macro-F1 than the best individual model on your manual annotation subset