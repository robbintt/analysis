---
ver: rpa2
title: 'Attention-Based Synthetic Data Generation for Calibration-Enhanced Survival
  Analysis: A Case Study for Chronic Kidney Disease Using Electronic Health Records'
arxiv_id: '2503.06096'
source_url: https://arxiv.org/abs/2503.06096
tags:
- data
- calibration
- synthetic
- mice
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Masked Clinical Modelling (MCM), an attention-based
  framework for generating synthetic healthcare data that enhances calibration in
  survival analysis. Unlike existing methods such as SMOTE and VAEs, MCM preserves
  hazard ratios and improves stratified calibration by balancing realism, utility,
  and practicality.
---

# Attention-Based Synthetic Data Generation for Calibration-Enhanced Survival Analysis: A Case Study for Chronic Kidney Disease Using Electronic Health Records

## Quick Facts
- arXiv ID: 2503.06096
- Source URL: https://arxiv.org/abs/2503.06096
- Reference count: 40
- Key outcome: 15% reduction in overall calibration loss and 9% reduction in mean calibration loss across 10 subgroups compared to 15 alternative methods

## Executive Summary
This study introduces Masked Clinical Modelling (MCM), an attention-based framework for generating synthetic healthcare data that enhances calibration in survival analysis. Unlike existing methods such as SMOTE and VAEs, MCM preserves hazard ratios and improves stratified calibration by balancing realism, utility, and practicality. Tested on a chronic kidney disease dataset, MCM reduced overall calibration loss by 15% and achieved a 9% reduction in mean calibration loss across 10 clinically stratified subgroups, outperforming 15 alternative methods. MCM supports both standalone dataset synthesis and conditional augmentation, making it adaptable for diverse healthcare research scenarios while ensuring equitable representation across patient populations.

## Method Summary
MCM uses a two-block attention-MLP architecture that learns clinical feature dependencies through masked reconstruction. The model is trained with dynamic random masking (10-95% of features per instance), where attention weights are computed via learnable linear transformation and masked positions are set to -∞ before softmax. During synthesis, 50% of features are masked and reconstructed to generate synthetic patients. The framework supports both standalone dataset synthesis and conditional augmentation for underrepresented subgroups without retraining, making it particularly valuable for improving calibration in stratified survival analysis.

## Key Results
- 15% reduction in overall calibration loss compared to baseline methods
- 9% reduction in mean calibration loss across 10 clinically stratified subgroups
- Hazard ratio preservation with synthetic data matching real data ratios (e.g., diabetes: HR 1.84 in real vs 1.85 in synthetic)

## Why This Works (Mechanism)

### Mechanism 1: Dependency Capture Through Masking
Randomly masking 10-95% of features and forcing reconstruction captures inter-variable dependencies critical for survival analysis. The attention layer computes feature-wise weights while masked positions are set to -∞ before softmax, ensuring reconstruction relies only on unmasked context. This preserves clinical covariate relationships necessary for downstream utility.

### Mechanism 2: Avoiding Collapse Modes
Attention-based dependency learning prevents mode/posterior collapse that degrades VAE/GAN alternatives. Unlike VAEs that compress diverse patients into shared latent space or GANs generating from random vectors, MCM's attention explicitly models pairwise feature relationships without compression, maintaining dataset diversity.

### Mechanism 3: Conditional Augmentation for Calibration
Conditional augmentation improves stratified calibration by oversampling underrepresented subgroups without retraining. The pre-trained model generates synthetic patients for specific conditions by filtering training data to subgroup matching, then applying synthesis. The attention weights learned on full population generalize to subgroups because learned feature dependencies transfer across strata.

## Foundational Learning

- **Concept: Cox Proportional Hazards Model**
  - Why needed here: The entire evaluation framework measures whether synthetic data preserves hazard ratios and improves calibration for downstream CoxPH models.
  - Quick check question: Can you explain what a calibration slope of 1.0 vs. 0.8 means for predicted vs. observed risks?

- **Concept: Masked Language Modeling (BERT-style)**
  - Why needed here: MCM adapts this paradigm from NLP to clinical tabular data; understanding the original helps decode the design.
  - Quick check question: How does predicting masked words from context differ from predicting masked clinical features from remaining covariates?

- **Concept: Calibration in Survival Analysis**
  - Why needed here: The paper's primary contribution is calibration improvement; without understanding calibration slope/error metrics, results are uninterpretable.
  - Quick check question: Why might a model have good discrimination (C-statistic) but poor calibration?

## Architecture Onboarding

- **Component map:** Preprocessed features -> Attention Layer (with masking) -> MLP (ReLU, LayerNorm, Linear) -> Residual connection + Attention (no masking) -> MLP (Sigmoid output) -> Loss (MSE on masked positions only)

- **Critical path:** 1) Preprocess: Box-Cox + normalize to [0,1] range 2) Train with dynamic masking (10-95% per batch) 3) Generate: Copy dataset -> mask 50% -> reconstruct -> replace masked values 4) Postprocess: Inverse transforms + threshold binaries at 0.5

- **Design tradeoffs:** Simple attention (single linear layer) vs. multi-head (authors tested multi-head, found no improvement); 50% masking at inference vs. higher (80% masking possible for novelty, but 50% used for stability); hidden dimension 64 (empirical choice for 491-patient dataset)

- **Failure signatures:** Synthetic binary variables not converging to 0/1 (check sigmoid output range and postprocessing threshold); hazard ratios in synthetic data diverging from real data (masking ratio too high destroying dependency structure); calibration worse than baseline (augmentation amplifying artifacts; try lower synthetic-to-real ratio)

- **First 3 experiments:** 1) Replicate realism check: Train MCM on CKD dataset, generate synthetic data, compare KDE plots and correlation matrices (Figures 2-4) 2) Ablate masking ratio: Test 30%, 50%, 70% masking at inference; measure calibration loss on held-out CoxPH 3) Cross-dataset validation: Train on one stratification (e.g., diabetes), evaluate calibration improvement on another (e.g., hypertension) to test conditional generalization

## Open Questions the Paper Calls Out

- **Open Question 1:** How does MCM performance scale to national-level datasets containing millions of records compared to the small dataset (n=491) used in this study?
- **Open Question 2:** To what extent do varying data mixture ratios in simulations influence MCM's effectiveness in correcting calibration errors across specific stratified subgroups?
- **Open Question 3:** Can privacy-preserving mechanisms be successfully integrated into the MCM framework without compromising the realism and calibration utility established in this study?

## Limitations
- Critical implementation details like exact "Attention Layer" architecture and batch size are unspecified, requiring reverse-engineering from equations
- The conditional augmentation assumption that subgroup feature dependencies transfer from full population training is reasonable but untested for severe distribution shifts
- The 9% calibration improvement is statistically significant in the 491-patient dataset, but generalizability to larger, more heterogeneous populations remains uncertain

## Confidence
- **High confidence:** The attention-based architecture can generate synthetic clinical data that preserves hazard ratios and improves calibration over baseline methods
- **Medium confidence:** The 9% reduction in mean calibration loss across 10 subgroups is real and replicable, but practical clinical significance needs validation in larger datasets
- **Medium confidence:** The claim that MCM avoids VAE/GAN collapse modes is plausible given explicit dependency modeling, but direct comparative analysis with these methods on same metrics is absent

## Next Checks
1. **Cross-dataset robustness:** Train MCM on one CKD stratification (e.g., diabetes patients) and evaluate calibration on a different stratification (e.g., hypertension patients) to test conditional generalization limits
2. **Masking sensitivity analysis:** Systematically vary inference masking ratios (30%, 50%, 70%) and measure the trade-off between synthetic data novelty and calibration degradation
3. **Calibration sensitivity to sample size:** Repeat experiments with 200, 491, and 1000 synthetic patients per subgroup to determine if calibration improvements scale with synthetic data volume