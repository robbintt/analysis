---
ver: rpa2
title: 'Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient LLM Agents'
arxiv_id: '2506.14852'
source_url: https://arxiv.org/abs/2506.14852
tags:
- caching
- plan
- cache
- arxiv
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high costs and latency of large language
  model (LLM) agents by introducing Agentic Plan Caching (APC), a novel approach that
  extracts, stores, adapts, and reuses structured plan templates from completed agent
  executions. Unlike traditional caching methods designed for chatbots, APC focuses
  on task-level caching by identifying keywords that capture the higher-level intent
  of queries, matching them against a cache of plan templates, and using lightweight
  models to adapt these templates for new tasks.
---

# Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient LLM Agents

## Quick Facts
- arXiv ID: 2506.14852
- Source URL: https://arxiv.org/abs/2506.14852
- Reference count: 40
- Reduces LLM agent costs by 50.31% and latency by 27.28% while maintaining 96.61% of optimal accuracy

## Executive Summary
This paper introduces Agentic Plan Caching (APC), a novel approach to reduce the cost and latency of LLM agents by caching and reusing structured plan templates. Unlike traditional caching methods designed for chatbots, APC focuses on task-level caching by extracting keywords that capture the higher-level intent of queries. The system uses lightweight models to adapt these templates for new tasks, achieving significant cost savings (50.31%) and latency reduction (27.28%) while maintaining 96.61% of optimal accuracy across diverse agent workloads.

## Method Summary
APC intercepts the planning stage of Plan-Act agents by caching successful execution templates. When a query arrives, a small LM extracts a keyword representing the task intent. If this keyword matches a cached entry, a small planner LM adapts the stored template to the current context. If no match exists (cache miss), a large planner LM generates a new plan, which is then executed and converted into a generalized template for future use. Template generation uses rule-based and LLM-based filters to remove context-specific details while preserving core logic.

## Key Results
- Reduces costs by 50.31% and latency by 27.28% on average across FinanceBench and TabMWP datasets
- Maintains 96.61% of optimal accuracy while using only 46% of the cost of accuracy-optimal baseline
- Exact keyword matching achieves O(1) lookup time, scaling efficiently to large cache sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Keyword-based matching outperforms query-level semantic similarity for cache retrieval in agent applications
- **Mechanism:** A lightweight LLM extracts abstract task intent (e.g., "working capital ratio") as a keyword, which serves as a cache key for exact matching against stored plan templates
- **Core assumption:** Task intent can be reliably decoupled from context-specific details, enabling reusable plan structures across different data contexts
- **Evidence anchors:** Section 3.2 shows keyword-based search achieving lower false positive and negative rates than query-based similarity across thresholds (0.7-0.9)
- **Break condition:** High false positives if keywords are overly broad, or high false negatives if keywords are too specific

### Mechanism 2
- **Claim:** A small planner LM can efficiently adapt cached templates into context-specific plans, preserving accuracy while reducing reliance on expensive large model invocations
- **Mechanism:** On cache hit, a small planner LM takes the task query, retrieved plan template, and past actor responses to generate a context-aware plan
- **Core assumption:** Small LMs are capable of plan instantiation given a high-quality template, and templates are sufficiently general to cover task variations
- **Evidence anchors:** Section 4.2 shows APC maintains 96.61% of optimal accuracy on average, with FinanceBench achieving 85.50% accuracy vs. 91.00% for accuracy-optimal baseline at 46% of the cost
- **Break condition:** Performance degradation if the small planner LM lacks reasoning capacity to adapt templates for novel or highly complex tasks

### Mechanism 3
- **Claim:** High-quality, reusable plan templates can be automatically generated from successful agent execution logs by combining rule-based and lightweight-LLM filtering
- **Mechanism:** After successful task completion, execution log is processed via rule-based filter (removes verbose reasoning) followed by LLM filter (removes specific entities and numbers) to produce a generalized JSON template
- **Core assumption:** Core logic of a plan can be cleanly separated from specific data values it operates on, and this separation can be automated
- **Evidence anchors:** Section 4.2 shows Full-History Caching underperforms APC in accuracy (72.00% vs. 85.50% on FinanceBench), demonstrating value of template extraction
- **Break condition:** Template quality degrades if filtering process over-generalizes (removing crucial logic) or under-generalizes (leaving in specific details)

## Foundational Learning

- **Concept: Plan-Act Agent Loop (ReAct)**
  - **Why needed here:** APC is specifically designed for agents that follow a two-stage Plan-Act pipeline. Understanding this loop is essential to see where caching intercepts the expensive planning stage.
  - **Quick check question:** Can you identify the two stages in the ReAct loop and explain why the planning stage is the primary target for cost reduction?

- **Concept: Semantic vs. Context Caching**
  - **Why needed here:** APC is positioned as an alternative to these traditional caching methods. Grasping their limitations clarifies APC's design rationale.
  - **Quick check question:** Why does semantic caching fail for data-dependent agent outputs, even when input queries are semantically similar?

- **Concept: Test-Time Memory**
  - **Why needed here:** APC is described as a "test-time memory" that learns from completed executions. This concept frames the system as dynamic and adaptive.
  - **Quick check question:** How does APC's test-time memory differ from an offline, pre-populated cache, particularly regarding its cold-start behavior?

## Architecture Onboarding

- **Component map:** Keyword Extractor (Small LM) -> Plan Cache -> Small Planner LM OR Large Planner LM -> Actor LM -> Template Generator (Filters) -> Plan Cache

- **Critical path:**
  1. Query received
  2. Keyword extraction
  3. Cache lookup (exact match on keyword)
  4. **Cache hit:** Small Planner LM adapts template -> Actor LM executes
  5. **Cache miss:** Large Planner LM generates plan -> Actor LM executes -> Template Generator creates new entry -> Cache updated

- **Design tradeoffs:**
  - **Exact vs. Fuzzy Matching:** Exact matching is fast and scalable (O(1)) but may miss similar intents. Fuzzy matching can improve hit rates but introduces significant latency and threshold-tuning challenges
  - **Template vs. Full-History Caching:** Templates are concise and effective for small LMs. Full histories are noisy and overwhelm small models, reducing accuracy
  - **Cold-Start Latency:** Initial queries incur higher cost and latency due to cache misses and template generation. Pre-warming can mitigate this

- **Failure signatures:**
  - **Accuracy drop on cache hit:** Indicates poor template quality or insufficient small planner LM capacity. Check template for over-generalization
  - **High false positive rate:** Keyword extraction is too broad. Refine the extraction prompt
  - **High latency:** Cache generation overhead is significant (avg. 3.99s). Consider dynamic disabling for low hit-rate workloads

- **First 3 experiments:**
  1. **Validate core functionality:** Run APC on a small dataset (e.g., 20 FinanceBench queries). Verify cache hits, template generation, and measure end-to-end cost and accuracy vs. an accuracy-optimal baseline
  2. **Test cache scalability:** Measure cache lookup latency with 10², 10⁴, and 10⁶ entries using exact matching. Confirm O(1) performance
  3. **Analyze cold start:** Run a time-series experiment on a dataset, plotting hit rate and marginal cost/latency as the cache warms from 0 to ~50 entries. Compare with a pre-warmed scenario

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can fuzzy keyword matching be implemented to improve cache hit rates without incurring the high latency penalties or accuracy degradation observed with semantic search?
- **Basis in paper:** Section 3.2 states the authors opted against fuzzy matching and leave it for future exploration; Section 4.4 demonstrates that while fuzzy matching increases hit rates, it scales poorly and degrades accuracy at lower thresholds
- **Why unresolved:** The authors determined that fuzzy matching reintroduces challenges of semantic caching and adds significant computational overhead
- **What evidence would resolve it:** An implementation maintaining sub-millisecond lookup times while preserving >95% accuracy-to-optimal ratio

### Open Question 2
- **Question:** How does Agentic Plan Caching impact performance and consistency in complex multi-agent systems compared to the single Plan-Act architecture evaluated?
- **Basis in paper:** Appendix E states that the current focus on a two-stage pipeline means "more complex multi-agent systems could present new challenges for maintaining cache consistency across multiple components"
- **Why unresolved:** The paper evaluates on a sequential Plan-Act loop but does not test scenarios where multiple agents might read from or write to the plan cache simultaneously
- **What evidence would resolve it:** Evaluation in a multi-agent environment measuring for race conditions, cache staleness, and success rate of plan reuse across different agent roles

### Open Question 3
- **Question:** Can parallel cache generation and speculative next-query inference effectively mitigate the 3.99-second average latency overhead observed during cache generation?
- **Basis in paper:** Section 4.3 notes that "most of the additional latency in our system comes from LLM-powered cache generation" and explicitly lists parallel generation and speculative inference as "part of future work"
- **Why unresolved:** While cost overhead is minimal, the wall-clock time for generating new cache entries is significant (3.99s), creating a latency spike during cache misses
- **What evidence would resolve it:** A modified system architecture where cache generation occurs asynchronously or speculatively, resulting in cache miss latency comparable to the accuracy-optimal baseline

### Open Question 4
- **Question:** What mechanisms are required to ensure privacy and data security when plan caches may inadvertently contain sensitive or proprietary information from execution logs?
- **Basis in paper:** Appendix E states that the system "raises questions about the long-term impact on data privacy, especially in cases where plan caches contain sensitive or proprietary information"
- **Why unresolved:** The paper focuses on cost and latency reduction via template extraction but does not analyze whether "generalized templates" successfully scrub all sensitive data
- **What evidence would resolve it:** A privacy analysis (e.g., membership inference attacks) on cached plan templates to verify that sensitive context cannot be reconstructed or leaked

## Limitations
- Keyword extraction brittleness: Exact matching on single keywords is fast but fragile, trading recall for efficiency without adaptive fallback mechanisms
- Cold-start penalty: Initial cache misses require full template generation (avg. 3.99s overhead), making APC unsuitable for low-hit-rate workloads without pre-warming
- Small LM adaptation limits: Template adaptation relies on Llama-3.1-8B reasoning capacity, with complex tasks potentially exceeding this threshold

## Confidence
- **High confidence:** Cost and latency reduction claims (50.31% cost, 27.28% latency) are well-supported by controlled experiments across two distinct datasets with clear statistical reporting
- **Medium confidence:** Accuracy preservation (96.61% of optimal) holds across benchmarks, though the gap widens for complex FinanceBench tasks (85.50% vs 91.00% optimal)
- **Medium confidence:** Keyword-based retrieval mechanism outperforms semantic similarity in internal comparisons, but lacks direct external validation

## Next Checks
1. **Cross-dataset generalizability:** Test APC on non-numerical reasoning tasks (e.g., creative writing, code generation) to validate keyword extraction and template adaptation beyond structured data contexts
2. **Threshold sensitivity analysis:** Systematically evaluate fuzzy matching performance across multiple thresholds (0.6-0.95) with varying dataset sizes to quantify the precision-latency tradeoff
3. **Long-tail performance:** Measure accuracy and cost impact for rare/edge-case queries that would generate unique templates, quantifying the overhead for building and maintaining a large template library