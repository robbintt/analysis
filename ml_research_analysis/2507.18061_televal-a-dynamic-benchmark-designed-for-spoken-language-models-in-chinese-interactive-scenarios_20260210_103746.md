---
ver: rpa2
title: 'TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese
  Interactive Scenarios'
arxiv_id: '2507.18061'
source_url: https://arxiv.org/abs/2507.18061
tags:
- response
- audio
- user
- dialect
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TELEVAL, a dynamic benchmark designed to
  evaluate spoken language models (SLMs) in realistic Chinese interactive scenarios.
  TELEVAL addresses the limitations of existing benchmarks that focus on task completion
  rather than natural conversational interaction.
---

# TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios

## Quick Facts
- arXiv ID: 2507.18061
- Source URL: https://arxiv.org/abs/2507.18061
- Reference count: 40
- Primary result: SLMs achieve only 50.52% accuracy on basic knowledge tasks; most models score below 40% on interactionally appropriate responses

## Executive Summary
TELEVAL addresses a critical gap in spoken language model evaluation by focusing on realistic Chinese interactive scenarios rather than task completion. The benchmark reveals that while current SLMs can handle explicit semantic content reasonably well, they struggle significantly with natural conversational interaction, particularly in incorporating paralinguistic cues and producing human-like responses. The best-performing model achieves only 50.52% accuracy on basic knowledge tasks, while most models score below 40% on interactionally appropriate responses, highlighting the need for more interaction-aware evaluation frameworks.

## Method Summary
TELEVAL is a dynamic benchmark evaluating SLMs across three dimensions: Explicit Semantics, Paralinguistic and Implicit Semantics, and System Abilities. It uses real-world dialogue formats with separate evaluation of text and audio outputs, featuring over 40,000 evaluation samples across 13 task categories. The benchmark employs hybrid audio sources (real recordings + synthesized speech), objective string matching for FAQA tasks, and LLM-as-judge with power scaling for OEAC tasks. Evaluation includes 11 acoustic robustness conditions and measures both Reliable Content Fulfillment and Interactional Appropriateness.

## Key Results
- Qwen3-Omni achieves highest basic knowledge accuracy at 50.52%, while most models score below 40% on interactionally appropriate responses
- Models show strong performance on explicit semantic tasks but struggle with paralinguistic cue integration and natural conversational interaction
- Safety & Morality scores (76-99%) far exceed Humanlike Chitchat scores (21-81%) for the same models, indicating format-driven behavior
- Baichuan-Omni-1.5 demonstrates strongest resilience to acoustic degradation despite lower absolute scores

## Why This Works (Mechanism)

### Mechanism 1: Paralinguistic-to-Response Coupling
Models that treat paralinguistic cues as classification targets fail to generate interactionally appropriate responses because they lack training for implicit cue incorporation. Paralinguistic information enters the model but is routed to explicit classification pathways rather than response generation modules, creating a "caption trap" where models describe cues instead of adapting behavior.

### Mechanism 2: Perceptual Robustness as Semantic Prerequisite
Acoustic distortions cause cascading failures from perception through semantic reasoning to social-pragmatic alignment. The "Competence Pyramid" establishes that Level 1 (perceptual robustness) enables Level 2 (semantic reasoning), which enables Level 3 (social-pragmatic alignment). Noise or distortion at Level 1 propagates upward.

### Mechanism 3: Evaluation Format Constrains Observable Behavior
Benchmark formats (MCQ, explicit instructions) create systematic blind spots by rewarding task completion over interactional appropriateness. When evaluation uses unnatural formats, models optimize for the format rather than the target capability, producing high scores that don't transfer to real-world interaction.

## Foundational Learning

- **Concept: Paralinguistic Cues vs. Explicit Semantic Content**
  - Why needed here: TELEVAL distinguishes between what users say and how they say it. Models must process both channels and integrate them for appropriate responses.
  - Quick check question: Can you explain why "You really thought of everything" spoken with angry tone requires different response logic than the same sentence spoken neutrally?

- **Concept: Acoustic Distortion Types and Their Effects**
  - Why needed here: The benchmark tests 11 conditions (reverberation, noise types, packet loss, distortion, low-pass filtering). Understanding how each affects speech signals is prerequisite for interpreting robustness results.
  - Quick check question: Why might babble noise (multiple speakers) be more disruptive to semantic comprehension than white noise at the same SNR?

- **Concept: End-to-End SLM Architecture vs. Cascaded Pipelines**
  - Why needed here: TELEVAL targets end-to-end SLMs that couple acoustic-linguistic processing, distinct from ASR→LLM→TTS cascades. This affects how paralinguistic information flows and how it can be evaluated.
  - Quick check question: In a cascaded system, where would paralinguistic information typically be lost? Where could it be recovered?

## Architecture Onboarding

- **Component map:** Audio input → perceptual robustness check → semantic extraction → paralinguistic cue integration → response generation → dual-modality evaluation (text + audio)
- **Critical path:** Audio input flows through perceptual robustness, semantic extraction, paralinguistic cue integration, response generation, then splits to text (reference matching/LLM scoring) or audio (quality metrics/emotion classification) evaluation
- **Design tradeoffs:** Real vs. synthetic audio (authenticity vs. scalability), objective vs. LLM evaluation (reproducibility vs. flexibility), single-turn vs. multi-turn evaluation (isolation vs. realism)
- **Failure signatures:** Caption Trap (describing cues vs. responding), dialect asymmetry (comprehension >> response), semantic-acoustic disconnect (text empathy ≠ audio prosody), robustness-performance inversion (minor perturbation > clean baseline)
- **First 3 experiments:**
  1. Run LlamaQA-zh through all 11 acoustic conditions; track degradation curves; compare slope against Baichuan-Omni-1.5 and SpeechGPT-2.0-preview
  2. Evaluate NSV-zh, Age-zh, and EmpatheticResponse_acoustic subsets; compute gap between acoustic-based and semantic-based empathy scores
  3. Run both ChineseQuiz variants (comprehension) and Chitchat dialect variants (response); compare comprehension vs. response adaptation scores

## Open Questions the Paper Calls Out

1. How can training objectives be restructured to eliminate the "Caption Trap," where models describe paralinguistic cues rather than adapting response behavior?
2. What interventions are required to bridge the gap between semantic empathy and acoustic realization in spoken dialogue models?
3. To what degree do LLM-based evaluation scores correlate with human judgments for the Open-Ended Audio Conversation tasks in TELEVAL?

## Limitations

- Benchmark relies on GPT4o as primary LLM evaluator, introducing potential English-centric reasoning bias despite Chinese-tuned evaluators
- 40,000+ sample size may not fully capture real-world conversational diversity, particularly for rare dialect variations
- Greedy decoding evaluation may not reflect optimal model performance achievable with sampling strategies
- Dataset not yet publicly downloadable, limiting immediate reproducibility

## Confidence

- High Confidence: Paralinguistic-to-Response Coupling mechanism (directly observable caption trap phenomenon)
- Medium Confidence: Perceptual Robustness Cascade (compelling framework but requires further validation)
- Medium Confidence: Evaluation Format Constraints (evident score disparities but causation vs. correlation needs controlled experiments)

## Next Checks

1. **Format Dependency Test:** Train identical model architectures on MCQ-style versus open-ended dialogue data, then evaluate both on TELEVAL to validate format-driven behavior
2. **Cross-Cultural Transferability:** Evaluate TELEVAL's English translation on Western SLMs to determine if paralinguistic integration challenges are language-specific
3. **Real-World Deployment Study:** Deploy top-performing TELEVAL models in actual conversational systems, measuring task completion rates and user satisfaction against benchmark predictions