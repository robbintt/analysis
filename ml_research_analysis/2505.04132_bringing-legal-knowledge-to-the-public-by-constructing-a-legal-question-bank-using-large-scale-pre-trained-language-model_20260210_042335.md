---
ver: rpa2
title: Bringing legal knowledge to the public by constructing a legal question bank
  using large-scale pre-trained language model
arxiv_id: '2505.04132'
source_url: https://arxiv.org/abs/2505.04132
tags:
- questions
- legal
- what
- question
- landlord
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the legal knowledge gap by improving navigability
  and comprehensibility of legal information for the public. The authors develop a
  three-step approach: translating legal concepts into plain language CLIC-pages,
  constructing a large-scale Legal Question Bank (LQB), and creating an AI-powered
  CLIC Recommender (CRec) to match user queries with relevant legal knowledge.'
---

# Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model

## Quick Facts
- arXiv ID: 2505.04132
- Source URL: https://arxiv.org/abs/2505.04132
- Reference count: 10
- Authors: Mingruo Yuan; Ben Kao; Tien-Hsuan Wu; Michael M. K. Cheung; Henry W. H. Chan; Anne S. Y. Cheung; Felix W. H. Chan; Yongxi Chen

## Executive Summary
This study addresses the legal knowledge gap by improving navigability and comprehensibility of legal information for the public. The authors develop a three-step approach: translating legal concepts into plain language CLIC-pages, constructing a large-scale Legal Question Bank (LQB), and creating an AI-powered CLIC Recommender (CRec) to match user queries with relevant legal knowledge. Machine-generated questions (MGQs) are created using GPT-3 with three different partitioning strategies (section-based, paragraph-based, and hybrid). The hybrid approach yields the best results: 93% coverage, 68% precision, and generates 3,362 MGQs compared to 2,686 human-composed questions. MGQs are more scalable, cost-effective, and diversified than human questions, while also identifying content gaps in CLIC-pages through "augmenting questions." The CRec prototype successfully recommends relevant legal questions based on user descriptions, demonstrating the approach's effectiveness in bridging the legal knowledge gap.

## Method Summary
The approach involves three steps: (1) converting complex legal documents into plain language CLIC-pages, (2) constructing a Legal Question Bank using GPT-3 with different partitioning strategies (section-based, paragraph-based, and hybrid), and (3) developing an AI-powered CLIC Recommender (CRec) that matches user queries to relevant legal questions using semantic similarity. The hybrid partitioning strategy, which provides section-level context while focusing on specific paragraphs, proved most effective for generating high-quality machine-generated questions. A human verifier manually labels questions as correct or augmenting, with augmenting questions serving as suggestions for improving the source material.

## Key Results
- Hybrid partitioning strategy achieves 68% precision and 93% coverage in generating machine-generated questions
- System generates 3,362 machine-generated questions compared to 2,686 human-composed questions
- Augmenting questions (32% of incorrect questions) provide valuable insights for content enhancement
- CRec prototype successfully matches user queries to relevant legal questions using semantic similarity

## Why This Works (Mechanism)

### Mechanism 1: Context-Attention Decoupling (Hybrid Partitioning)
- **Claim:** Generating high-quality questions requires exposing the model to broad context while forcing attention on specific information units.
- **Mechanism:** The "Hybrid" strategy injects a full section of text (context) into the prompt but explicitly instructs the model to generate questions for a specific paragraph label (attention). This reduces the hallucination rate seen in paragraph-only methods (which lack context) and prevents the vague generalizations seen in section-only methods.
- **Core assumption:** The pre-trained model (GPT-3) can attend to specific segments within a larger context window when explicitly instructed via prompt formatting.
- **Evidence anchors:**
  - [Section 4.1] "Hybrid partitioning gives a much higher precision compared with the other two methods... Hybrid includes a whole section... but repeats executing GPT-3, each time with a paragraph attention."
  - [Table 2] Shows Hybrid precision at 68% vs 41% (Paragraph) and 50% (Section).
  - [Corpus] While related papers like "QBR" support the use of question banks for retrieval, specific evidence for the *Hybrid partitioning* mechanism is isolated to this paper; corpus neighbors focus on the application (legal QA) rather than the generation prompt engineering.
- **Break condition:** If the input text structure lacks clear paragraph delineation or the semantic distance between the paragraph and the section is too wide, the model may still hallucinate.

### Mechanism 2: Semantic Gap-Filling via Generative Drift
- **Claim:** Generative "errors" (questions not answerable by the source text) function as a discovery mechanism for content gaps in the knowledge base.
- **Mechanism:** Unlike humans who stick to the text present, LLMs draw on external training data. When the model generates a relevant question that the text *cannot* answer (an "augmenting question"), it identifies a "legal knowledge gap" where the document is incomplete relative to user needs.
- **Core assumption:** The model's pre-training includes legal concepts sufficiently rich to propose relevant questions that the original authors missed.
- **Evidence anchors:**
  - [Section 4.2] "We call this kind of machine-generated questions... augmenting questions... The question provides hints on how the content of the CLIC-page could be enhanced."
  - [Section 4.2] Notes that 32% of questions were "incorrect" (not in the text), but manual inspection found many were "interesting questions that are relevant."
  - [Corpus] Implicitly supported by "From Superficial to Deep" (neighbor), which suggests integrating external knowledge improves question depth, though this paper frames it as error analysis.
- **Break condition:** If the model hallucinates legal procedures that do not exist or are factually incorrect (legal hallucinations), these "augmenting questions" become misinformation traps rather than constructive feedback.

### Mechanism 3: Scope-Based Dense Retrieval
- **Claim:** Mapping user situations to legal answers is more robust when retrieval compares the user's situation to the *answer scope* (embedded paragraph content) rather than just the question string.
- **Mechanism:** The CLIC Recommender (CRec) embeds the user's query and compares it against `va(q)` (the answer vector) rather than just `vs(q)` (the question string). This ensures the system retrieves the relevant *legal concept* even if the user's phrasing differs significantly from the model question's phrasing.
- **Core assumption:** The semantic embedding space aligns layperson descriptions of events with formal legal explanatory text.
- **Evidence anchors:**
  - [Section 5] "We compute the cosine similarity between v(tu) [user input] against all the questions' answer vectors va(q)'s."
  - [Figure 13] Demonstrates a user describing a "cover version of a song" being matched to copyright/parody questions effectively.
- **Break condition:** This fails if the user provides legally irrelevant details (noise) that dominate the embedding, or if the underlying embedding model (e.g., all-mpnet-base-v2) fails to capture domain-specific legal semantics.

## Foundational Learning

- **Concept: Prompt Engineering (Context vs. Attention)**
  - **Why needed here:** The paper's core success relies on the "Hybrid" strategy. You must understand that LLMs need both *background* (the section) and *focus* (the paragraph) to generate precise questions.
  - **Quick check question:** If you feed an LLM an entire 50-page legal document to generate questions, why might the results be worse than feeding it 5 pages at a time? (Answer: Dilution of attention / lack of specific focus).

- **Concept: Dense Retrieval (Bi-Encoders)**
  - **Why needed here:** The CRec system works by mapping text to vectors. You need to distinguish between matching *keywords* and matching *semantic intent* (embedding vectors).
  - **Quick check question:** Why does the system embed the *answer paragraphs* rather than just the *questions* to match against user queries? (Answer: To capture the semantic "solution space" of the law, not just the syntax of the question).

- **Concept: Evaluation Metrics for Generative AI**
  - **Why needed here:** The paper evaluates "Quantity, Precision, Coverage, Diversity." Standard accuracy is not enough.
  - **Quick check question:** If a generated question is "incorrect" (not in the text), is it useless? (Answer: No, it may be an "augmenting question" that highlights a content gap).

## Architecture Onboarding

- **Component map:** CLIC-pages -> GPT-3 with Hybrid Prompting -> Deduplication + Human Verifier -> Vector Store -> CRec -> User Query -> Top-K Questions
- **Critical path:** The prompt template. If the prompt does not successfully force the LLM to look at the specific paragraph while reading the section, the precision drops from 68% to ~40% (Paragraph baseline).
- **Design tradeoffs:**
  - **Precision vs. Cost:** The Hybrid method runs the LLM *N* times per section (where N = number of paragraphs), costing more API credits than Section-based but yielding higher quality.
  - **Recall vs. Noise:** Retaining "Augmenting Questions" (generated questions not in the text) improves the system's ability to suggest content updates but introduces noise into the retrieval index if not filtered.
- **Failure signatures:**
  - **Generic Questions:** Questions like "What is the law?" indicate the "Section-based" partitioning is failing to capture details.
  - **Hallucinated Details:** Questions asking about specific fines or dates not in the text indicate the model is ignoring the context or "Paragraph attention" is leaking.
  - **Retrieval Mismatch:** User asks "Can I sue?" but gets "How to file taxes." This suggests the embedding model is failing to distinguish between legal actions.
- **First 3 experiments:**
  1. **Ablation on Partitioning:** Run a small batch (10 pages) comparing Section-only vs. Hybrid prompts. Measure the "Correctness" (precision) manually to validate the 68% vs 50% claim.
  2. **Deduplication Threshold Testing:** Test cosine similarity thresholds (0.90 vs 0.95 vs 0.99) to see how many redundant questions slip through vs. how many unique variations are erroneously deleted.
  3. **Retrieval Baseline:** Create a test set of 20 user scenarios (e.g., "Landlord won't fix AC"). Embed them and search the LQB. Measure if the correct CLIC page appears in the top 3 results using Answer Vectors vs. Question Vectors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the quantitative retrieval performance (e.g., Recall@k, MRR) of the CLIC Recommender (CRec) in matching layperson queries to the Legal Question Bank?
- Basis in paper: [inferred] The paper details the quantitative evaluation of the LQB generation (Section 4) but limits the evaluation of the CRec system to illustrative examples and internal case scenarios (Section 5) without reporting standard retrieval metrics or user study results.
- Why unresolved: The authors state the approach is "promising" but focus their technical contribution on the generation and partitioning strategies, leaving the end-to-end retrieval efficacy unquantified.
- What evidence would resolve it: A benchmark evaluation using a test set of real-world user queries with ground-truth relevance judgments to measure CRec's retrieval accuracy.

### Open Question 2
- Question: Can an automated classifier effectively distinguish between "incorrect" and "augmenting" machine-generated questions (MGQs) to streamline the legal knowledge base update process?
- Basis in paper: [inferred] Section 4.2 notes that while "augmenting questions" are valuable for content enrichment, the current workflow relies on 400 person-hours of manual verification to separate them from irrelevant questions.
- Why unresolved: The paper identifies the category of augmenting questions but does not propose or test a method to detect them algorithmically, treating them as a byproduct of manual inspection.
- What evidence would resolve it: Development and validation of a classifier trained to label MGQs as "Answerable," "Augmenting," or "Irrelevant" based on the CLIC-page context.

### Open Question 3
- Question: To what extent does the "Hybrid" partitioning strategy generalize to legal domains with different document structures (e.g., strictly codified civil law vs. common law)?
- Basis in paper: [inferred] The method is evaluated exclusively on the CLIC dataset (Hong Kong law), which has a specific structure of sections and paragraphs tailored to the Hybrid strategy (Section 3.3).
- Why unresolved: The success of the Hybrid strategy is attributed to balancing section-level context with paragraph-level attention; it is unclear if this heuristic is robust for legal corpora with different formatting conventions.
- What evidence would resolve it: Cross-domain experiments applying the Hybrid strategy to other legal datasets (e.g., EU legislation or US case law) to compare performance against the Section-based and Paragraph-based baselines.

## Limitations
- The study lacks external validation of whether generated questions actually improve legal understanding among target users
- System performance depends heavily on the quality and comprehensiveness of source CLIC-pages
- The CRec system's effectiveness is demonstrated through a single prototype example rather than systematic user testing
- Augmenting questions assume GPT-3's legal knowledge is accurate, which could introduce misinformation if the model hallucinates non-existent legal procedures

## Confidence
- **High Confidence:** The comparative performance of different partitioning strategies (Section vs. Paragraph vs. Hybrid) and their precision metrics are directly measurable and reproducible
- **Medium Confidence:** The claim that augmenting questions identify content gaps is reasonable but requires manual verification of each generated question's relevance and accuracy
- **Low Confidence:** The practical utility of the system for actual legal information retrieval by laypersons is asserted but not empirically validated through user studies

## Next Checks
1. **User Comprehension Testing:** Conduct a randomized controlled trial where one group accesses legal information through traditional CLIC-pages while another uses the CRec system, measuring comprehension and information retrieval success rates
2. **Augmenting Question Verification:** Implement a systematic validation pipeline where legal experts review all "augmenting questions" to determine whether they represent genuine content gaps or model hallucinations
3. **Cross-Domain Generalization:** Test the hybrid partitioning strategy on non-legal knowledge bases (e.g., medical or technical documentation) to verify whether the mechanism generalizes beyond legal text structures