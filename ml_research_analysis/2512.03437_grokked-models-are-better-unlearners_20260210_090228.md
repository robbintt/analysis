---
ver: rpa2
title: Grokked Models are Better Unlearners
arxiv_id: '2512.03437'
source_url: https://arxiv.org/abs/2512.03437
tags:
- unlearning
- grokked
- data
- grokking
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors investigate whether the delayed generalization phenomenon
  of grokking (Power et al., 2022) can also benefit machine unlearning. Using CNNs,
  ResNets, and transformers, they compare unlearning performance from pre- and post-grokking
  checkpoints.
---

# Grokked Models are Better Unlearners

## Quick Facts
- arXiv ID: 2512.03437
- Source URL: https://arxiv.org/abs/2512.03437
- Reference count: 32
- Primary result: Post-grokking checkpoints achieve 6-8% better forgetting, 10-20% better retention, and more stable updates than pre-grokking checkpoints across multiple unlearning algorithms.

## Executive Summary
This paper investigates whether the delayed generalization phenomenon of grokking can benefit machine unlearning. The authors compare unlearning performance between pre- and post-grokking checkpoints using CNNs, ResNets, and transformers. Post-grokking models consistently achieve more effective forgetting (lower unlearning accuracy), better retain/test accuracy retention (higher accuracy), and more stable updates across seeds. The key insight is that grokking restructures internal representations into more modular forms, producing lower gradient alignment between forget/retain subsets and simpler loss landscapes, both of which facilitate selective forgetting. These findings suggest that the representational quality of grokked models provides an orthogonal lever to improve unlearning efficiency without modifying existing algorithms.

## Method Summary
The paper trains models (ResNet-18, CNN, transformers) for extended periods to observe grokking transitions, then applies standard unlearning algorithms (gradient ascent, ∇τ, SCRUB, Fisher) to both pre-grokking and post-grokking checkpoints. Unlearning performance is evaluated using three metrics: Unlearning Accuracy (UA↓), Retain Accuracy (RA↑), and Test Accuracy (TA↑). The key experimental design involves training models 5-6× longer than typical early stopping, monitoring for the grokking transition (prolonged overfitting followed by sudden validation improvement), and selecting checkpoints before and after this transition for unlearning comparison.

## Key Results
- Post-grokking models achieve 6-8% lower unlearning accuracy (better forgetting) while maintaining 10-20% higher retain/test accuracy
- Gradient correlation between forget/retain subsets drops from 0.990-0.999 (pre-grokking) to 0.426-0.521 (post-grokking)
- CKA between forget/retain representations decreases by 72% (0.459 to 0.129), indicating disentangled representations
- Local Complexity (LC) drops from 27.98 to 7.37 in grokked models, creating flatter loss landscapes that tolerate parameter perturbations

## Why This Works (Mechanism)

### Mechanism 1: Gradient Orthogonality Between Forget/Retain Sets
Grokking reduces gradient correlation between forget and retain data, enabling selective parameter updates. Post-grokking models develop modular representations where different data subsets activate distinct parameter subspaces. The expected gradient correlation E[corr(∇θℓ(x), ∇θℓ(x'))] = pρ, where p is module activation probability and ρ is within-module correlation. As modularity increases (p decreases), gradient spaces become more orthogonal. Evidence shows pre-grokking gradient correlation: 0.990-0.999; Post-grokking: 0.426-0.521.

### Mechanism 2: Representational Disentanglement via CKA Separation
Extended training beyond overfitting causes a phase transition from distributed "memorizing circuits" to sparse "generalizing circuits." CKA between forget/retain representations drops from 0.459 (entangled) to 0.129 (disentangled)—a 72% reduction. This separation indicates that grokked models process forget and retain data through largely independent pathways.

### Mechanism 3: Local Complexity Reduction (Loss Landscape Flattening)
Grokked models have simpler, flatter loss landscapes that tolerate parameter perturbations. Local Complexity (LC) measures linear region density around data points. Lower LC = smoother representations. Grokked ResNet: LC=7.37 vs pre-grokking LC=27.98. After unlearning, grokked models maintain lower LC (15.16 vs 35.41), indicating stability under modification.

## Foundational Learning

- **Concept: Grokking Phase Transition**
  - Why needed here: The entire paper hinges on identifying when models have transitioned from overfitting to delayed generalization.
  - Quick check question: Can you identify on a training curve where test loss/accuracy has plateaued or degraded, then suddenly improves after extended steps?

- **Concept: Machine Unlearning Objectives (UA/RA/TA Trade-off)**
  - Why needed here: Evaluation requires balancing three competing metrics. Optimizing only forgetting (low UA) often destroys retained knowledge (low RA/TA).
  - Quick check question: If a method achieves UA=0% but RA drops from 100% to 30%, is it successful unlearning?

- **Concept: Gradient Ascent vs. Distillation-Based Unlearning**
  - Why needed here: The paper tests multiple algorithm families (GA, ∇τ, SCRUB, Fisher). Understanding their failure modes helps interpret why grokking benefits all of them.
  - Quick check question: Why does gradient ascent on forget data often harm retain performance, and how does SCRUB address this?

## Architecture Onboarding

- **Component map:** Checkpoint selector → Forget set constructor → Unlearning engine → Evaluation suite
- **Critical path:**
  1. Train model 5-6× longer than typical early stopping (e.g., 500K steps vs 100K for CIFAR-10)
  2. Monitor for grokking transition (sharp test accuracy jump after plateau)
  3. Select θgrok checkpoint after sustained generalization
  4. Apply unlearning algorithm with identical hyperparameters to θpre and θgrok
  5. Compare UA/RA/TA to quantify benefits
- **Design tradeoffs:**
  - Training time: Grokking requires 5-6× longer training; authors position this as a research tool, not production recommendation
  - Checkpoint selection: Earlier θgrok = faster experiments but potentially incomplete modularization; later = more stable but diminishing returns
  - Forget set design: Class-level (easier, clearer boundaries) vs. example-level (harder, more realistic privacy scenarios)
- **Failure signatures:**
  - No grokking observed after extended training: May indicate dataset/architecture doesn't support grokking
  - High variance across seeds: Pre-grokking checkpoint selected too early; representations still entangled
  - UA tracks RA closely: Gradient correlation still high; grokking incomplete or forget/retain sets fundamentally inseparable
- **First 3 experiments:**
  1. Replicate CIFAR-10 grokking curve: Train ResNet-18 for 500K steps, plot train/test accuracy. Confirm delayed generalization jump. Identify θpre (step ~50-100K) and θgrok (step ~500K).
  2. Gradient correlation sanity check: Compute cosine similarity between batch gradients on Dforget vs Dretain for both checkpoints. Expect θpre > 0.95, θgrok < 0.55.
  3. Minimal unlearning comparison: Apply gradient ascent (single hyperparameter: learning rate) to both checkpoints. Measure UA/RA/TA after 20 steps. Grokked model should show lower UA with higher RA.

## Open Questions the Paper Calls Out

### Open Question 1
Can the beneficial representational properties of grokked models (modularity, gradient orthogonality, flat loss landscapes) be efficiently induced without the 5-6× training overhead of full grokking? The paper notes that SAM experiments showed partial benefits but did not match full grokking performance.

### Open Question 2
Do grokking's unlearning benefits scale to larger-scale vision models (e.g., ViT on ImageNet-21K) and larger language models (e.g., LLaMA-70B, GPT-scale)? Current experiments only cover CNNs/ResNets on CIFAR/SVHN/ImageNet-100 and Phi-1.5 (1.3B parameters) on TOFU.

### Open Question 3
What is the precise causal mechanism connecting grokking's representational restructuring to improved unlearning, beyond the correlational evidence provided? The paper establishes correlations but does not prove these cause the unlearning improvements.

### Open Question 4
Is the loss-based heuristic for identifying "locally grokked" examples in LLMs measuring the same representational quality as global grokking transitions? Local grokking was defined via loss thresholds without verification that these examples possess the modular representations seen in globally grokked models.

## Limitations
- The paper's core claim relies heavily on controlled synthetic setups (class-based forget sets, small datasets, specific architectures), with generalization to real-world scenarios untested
- The theoretical link between modularity and gradient orthogonality assumes independent module activation, but this assumption isn't experimentally verified
- The 5-6× training cost for grokking may be prohibitive in production contexts, though acknowledged as a research tool limitation

## Confidence

- **High Confidence**: Gradient correlation reduction (0.990→0.426) and CKA separation (0.459→0.129) are well-measured with clear statistical significance across multiple seeds and architectures
- **Medium Confidence**: The causal mechanism linking modular representations to stable unlearning is plausible but not definitively proven—correlation doesn't establish causation for all unlearning algorithms tested
- **Low Confidence**: Real-world applicability claims (uneven forget sets, larger models, production constraints) lack empirical validation beyond the controlled experimental framework

## Next Checks

1. **Module Independence Verification**: Test whether forget/retain data activate statistically independent parameter subspaces using targeted ablation (zero gradients from one subset and measure performance on the other)

2. **Scalability Assessment**: Apply the same methodology to ResNet-50 on full ImageNet (not ImageNet-100 subset) and a 1B-parameter transformer to verify grokking benefits persist at scale

3. **Realistic Forget Set Construction**: Replace class-level forgetting with example-level privacy requests (individual users' data) and measure whether grokking benefits survive non-uniform, noisy forget signals