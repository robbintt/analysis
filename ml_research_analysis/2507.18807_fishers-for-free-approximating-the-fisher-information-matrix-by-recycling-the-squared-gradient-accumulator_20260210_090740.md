---
ver: rpa2
title: Fishers for Free? Approximating the Fisher Information Matrix by Recycling
  the Squared Gradient Accumulator
arxiv_id: '2507.18807'
source_url: https://arxiv.org/abs/2507.18807
tags:
- fisher
- squisher
- gradient
- learning
- squared
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using the squared gradient accumulator (Squisher)
  from adaptive optimizers as a free approximation of the Fisher diagonal, which is
  commonly used to measure parameter sensitivity in various applications. The key
  insight is that the Squisher's sum-then-square structure aligns more closely with
  the joint Fisher diagonal than the traditional Fisher diagonal's square-then-sum
  structure.
---

# Fishers for Free? Approximating the Fisher Information Matrix by Recycling the Squared Gradient Accumulator

## Quick Facts
- arXiv ID: 2507.18807
- Source URL: https://arxiv.org/abs/2507.18807
- Authors: YuXin Li; Felix Dangel; Derek Tam; Colin Raffel
- Reference count: 28
- Key outcome: This paper proposes using the squared gradient accumulator (Squisher) from adaptive optimizers as a free approximation of the Fisher diagonal, which is commonly used to measure parameter sensitivity in various applications.

## Executive Summary
This paper proposes a computationally free method to approximate the Fisher diagonal by recycling the squared gradient accumulator (Squisher) maintained by adaptive optimizers like Adam during training. The key insight is that the Squisher's sum-then-square structure aligns more closely with the joint Fisher diagonal than the traditional Fisher diagonal's square-then-sum structure. Through experiments across five applications (model merging, pruning, sparse training, task similarity measurement, and continual learning), the Squisher consistently performed comparably to the Fisher diagonal while significantly outperforming Fisher-free baselines. The approach eliminates the computational costs of explicitly computing the Fisher while maintaining effectiveness.

## Method Summary
The method extracts the squared gradient accumulator (v_t) from the optimizer state dictionary at the end of training, which is already computed as part of adaptive optimizers like Adam and AdamW. For applications invariant to scaling (merging, pruning, FISH Mask, Task2Vec), the extracted v(t) is used directly as a drop-in replacement for the Fisher diagonal. For scale-dependent tasks like Elastic Weight Consolidation (EWC), the Squisher is rescaled by dataset size N and regularization strength λ is tuned accordingly. The approach leverages the fact that the EMA structure of the accumulator provides a stable estimate of parameter sensitivity over the entire training trajectory.

## Key Results
- The Squisher approximation performs comparably to the Fisher diagonal across all five tested applications
- The approach significantly outperforms Fisher-free baselines in model merging, pruning, and sparse training tasks
- For EWC continual learning, scaling the Squisher by dataset size N provides effective regularization comparable to the true Fisher
- The method eliminates the computational overhead of explicitly computing the Fisher while maintaining effectiveness

## Why This Works (Mechanism)

### Mechanism 1
The squared gradient accumulator from optimizers like Adam can approximate the Fisher diagonal because its "sum-then-square" structure aligns with the diagonal of the joint empirical Fisher. The standard diagonal empirical Fisher is computed as a sum of squared per-sample gradients, while the joint Fisher is the square of the sum of per-sample gradients. Adam's accumulator computes an EMA of squared batch gradients, which after scaling by N, is structurally identical to the joint empirical Fisher diagonal.

### Mechanism 2
The optimizer's EMA provides a stable estimate of parameter sensitivity by aggregating gradient information over the entire training trajectory. Instead of computing the Fisher on a separate data sample at a single point in time, the Squisher recycles the EMA that has been computed over thousands of training steps, smoothing out noise from stochastic gradients and capturing a holistic view of parameter importance.

### Mechanism 3
The Squisher works as a drop-in replacement for scale-invariant tasks like ranking parameters. For scale-dependent tasks like EWC, a simple heuristic rescaling (e.g., by dataset size N) aligns its magnitude with the expected Fisher. This compensates for structural differences between the Squisher and Fisher while preserving the relative importance rankings that many applications rely on.

## Foundational Learning

- **Fisher Information Matrix (FIM)**: Target of the approximation; its diagonal is a standard proxy for "parameter sensitivity" or "importance." Quick check: If a parameter has a high value on the diagonal of the Fisher, does changing it by a small amount likely cause a large or small change in the model's loss?

- **Adam Optimizer & EMA**: Source of the "free" approximation; you must understand how Adam's second moment estimator (v_t) works as an exponential moving average of squared gradients. Quick check: In Adam, what does the hyperparameter β₂ control, and what is its default value in most implementations?

- **Empirical vs. True Fisher**: The paper builds on the empirical Fisher, which uses ground-truth labels instead of sampling from the model's likelihood. Quick check: What is the primary difference in how the "true" Fisher and the "empirical" Fisher are computed, and why is the empirical version more common in practice?

## Architecture Onboarding

- **Component map**: Trained model with Adam/AdamW optimizer -> Optimizer state dictionary containing v_t accumulators -> Squisher extractor utility -> Application logic (direct use or rescaled use)

- **Critical path**: The primary architectural change is a post-training data extraction step. The critical action is to save and load the optimizer's state dictionary correctly so that the accumulator v_t is available for use.

- **Design tradeoffs**:
  - Cost vs. Precision: Gain massive computational efficiency (near-zero cost) while eliminating data access requirements. The tradeoff is an approximation that is not theoretically guaranteed to equal the Fisher diagonal.
  - Hyperparameter Coupling: The Squisher's quality is now coupled to the optimizer's hyperparameters (e.g., β₂). A poorly tuned optimizer could yield a poor Squisher estimate.
  - Temporal vs. Static Estimate: The Fisher is a static measure at a point in time; the Squisher is a temporal average over the training history.

- **Failure signatures**:
  - Degradation in undertrained models: If training was stopped prematurely, the EMA will not have a sufficient history to form a reliable estimate
  - Incorrect regularization strength: For EWC, forgetting to apply the heuristic rescaling will result in a regularizer that is either too weak or too strong
  - Non-standard optimizer settings: Using a non-default β₂ value may bias the Squisher too heavily toward recent gradients

- **First 3 experiments**:
  1. Validation in a Pruning Task: Compare pruning based on the Squisher versus a random baseline on your trained model
  2. Ablation on Training Duration: Train two versions of a model (to convergence vs. few epochs) and compare Squisher quality in a downstream task
  3. Test Rescaling Heuristic in EWC: Implement EWC using standard Fisher as baseline, then using Squisher with and without rescaling, comparing forgetting curves

## Open Questions the Paper Calls Out

- **Advanced optimizers**: Can optimizers that approximate the non-diagonal Fisher (like AdaFisher or IVON) provide more accurate parameter importance estimates when recycled compared to the diagonal-only Squisher?

- **Theoretical justification**: What is the principled theoretical justification for the N-scaling heuristic used in EWC regularization, and can a theoretically grounded rescaling scheme be derived?

- **Optimal EMA decay**: How does the optimal EMA decay coefficient (β₂) for Fisher approximation differ from optimization-focused defaults, and does task-specific tuning improve Squisher performance?

- **Extension to SGD**: Can the Squisher approach be extended to models trained without adaptive optimizers (e.g., vanilla SGD), and what computational overhead would post-hoc gradient accumulation introduce?

## Limitations

- Limited scope of evaluation covering only 5 applications where Fisher information is commonly used
- No formal theoretical guarantees that the Squisher is a valid proxy for the Fisher diagonal in all contexts
- Rescaling heuristics are empirical and lack theoretical grounding
- Performance depends on optimizer hyperparameters, creating coupling between training and inference

## Confidence

- **High confidence**: The Squisher works as a free approximation for scale-invariant tasks like model merging and pruning
- **Medium confidence**: The Squisher performs comparably to Fisher in complex applications like EWC and FISH Mask
- **Low confidence**: The theoretical connection between joint empirical Fisher and standard Fisher diagonal is well-motivated but not rigorously proven

## Next Checks

1. **Undertraining ablation study**: Compare Squisher quality from models trained to convergence versus those stopped after only a few epochs to quantify the impact of EMA warm-up

2. **Optimizer hyperparameter sensitivity**: Systematically vary β₂ (e.g., 0.9, 0.99, 0.999) and measure the resulting Squisher quality across different downstream tasks

3. **Cross-architecture validation**: Test the Squisher approximation on architectures not evaluated in the paper (e.g., convolutional networks for vision tasks) to assess generalizability beyond transformers and MLPs