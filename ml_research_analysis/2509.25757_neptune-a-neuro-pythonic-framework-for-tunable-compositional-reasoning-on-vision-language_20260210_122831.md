---
ver: rpa2
title: 'NePTune: A Neuro-Pythonic Framework for Tunable Compositional Reasoning on
  Vision-Language'
arxiv_id: '2509.25757'
source_url: https://arxiv.org/abs/2509.25757
tags:
- reasoning
- neptune
- object
- visual
- compositional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NePTune introduces a neuro-symbolic framework that combines neural
  vision models with symbolic reasoning through Python-based programs. It uses an
  LLM to parse natural language queries into executable code, grounding concepts via
  visual prompting and soft logic operations that handle uncertainty from VLMs.
---

# NePTune: A Neuro-Pythonic Framework for Tunable Compositional Reasoning on Vision-Language

## Quick Facts
- arXiv ID: 2509.25757
- Source URL: https://arxiv.org/abs/2509.25757
- Authors: Danial Kamali; Parisa Kordjamshidi
- Reference count: 34
- Primary result: Achieves 92.65% accuracy on CLEVR and 69.69% on Ref-GTA domain shift benchmark

## Executive Summary
NePTune introduces a neuro-symbolic framework that combines neural vision models with symbolic reasoning through Python-based programs. It uses an LLM to parse natural language queries into executable code, grounding concepts via visual prompting and soft logic operations that handle uncertainty from VLMs. Experiments show NePTune significantly outperforms strong baselines in compositional reasoning tasks, achieving 92.65% accuracy on CLEVR and 69.69% on the challenging Ref-GTA domain shift benchmark, while also enabling effective zero-shot generalization and fine-tuning.

## Method Summary
NePTune operates through three components: an LLM generates Python programs from natural language queries while extracting object names; GroundingDINO-B detects objects and VLMs score concepts via visual prompting with bounding boxes; a hybrid executor runs soft logic operations (fuzzy min/max over probability scores) combined with imperative Python control flow. The framework uses LoRA fine-tuning (r=16 LM, r=8 vision, α=32/16) with 1000 samples for domain adaptation. Atomic concept scoring treats VLM "Yes/No" logits as probability scores, while soft logic operators (min for AND, max for OR) compose these scores through control flow structures.

## Key Results
- Achieves 92.65% accuracy on CLEVR benchmark, significantly outperforming existing neuro-symbolic approaches
- Demonstrates strong domain adaptation with 69.69% accuracy on Ref-GTA benchmark (vs. 6.95% for InternVL2.5)
- Shows 94%+ F1 on CLEVR atomic concept scoring while maintaining compositional reasoning capability
- Improves from 34.92% to 69.90% on Ref-GTA through fine-tuning with only 1000 samples

## Why This Works (Mechanism)

### Mechanism 1: Soft Logic Operators Propagate Uncertainty Instead of Cascading Errors
The framework computes min(α_x, α_y) for conjunction and max(α_x, α_y) for disjunction over VLM confidence scores, preserving gradient information rather than thresholding early. This allows downstream operations to work with continuous uncertainty signals instead of binary decisions that compound errors in sequential reasoning pipelines.

### Mechanism 2: Hybrid Execution Decouples Control Flow from Concept Composition
An LLM generates Python code with standard control flow (loops, conditionals), but atomic predicates return Score objects that support overloaded operators (&, |). The interpreter handles branching while the Score objects handle fuzzy composition, enabling more complex reasoning patterns than either paradigm alone.

### Mechanism 3: Visual Prompting for Atomic Concepts Leverages VLM Strength
The score() function draws bounding boxes as visual prompts and queries atomic properties ("Is the object in the red bounding box blue?"). Table 8 shows 94%+ F1 on CLEVR atomic concepts vs. 27% on complex Ref tasks for the same VLM, suggesting VLMs are significantly more reliable at atomic concept scoring than complex multi-step reasoning.

## Foundational Learning

- **Concept: Fuzzy Logic T-Norms**
  - Why needed here: NePTune's soft operators implement t-norms (min for AND, max for OR) over probability distributions. Without understanding why these are differentiable and truth-functional, you cannot debug score composition behavior.
  - Quick check question: Given scores [0.8, 0.3] and [0.6, 0.9], what is the result of `score_a & score_b`?

- **Concept: Visual Prompting / Referring Expression Grounding**
  - Why needed here: The `score()` function relies on VLMs correctly attending to marked bounding boxes. Understanding how visual prompts affect VLM attention is critical for debugging grounding failures.
  - Quick check question: Why might a VLM fail when asked "Is the object in the red box blue?" even if it can answer "What color is the object?" correctly?

- **Concept: Program Synthesis from Natural Language**
  - Why needed here: The LLM must translate NL queries to executable Python. Understanding failure modes (incorrect arity, wrong variable references) helps diagnose executor crashes.
  - Quick check question: What is the difference between a syntax error and a semantic error in generated programs, and which does NePTune handle better?

## Architecture Onboarding

- **Component map:** [LLM Program Generator] → Python program + object names → [Perceptual Grounding] → Grounding DINO (object proposals) and VLM (score/query functions) → [Symbolic Executor] → Runs Python with overloaded Score objects → [Optional: Verification] → Pairwise arbiter or confidence gating

- **Critical path:** LLM program generation → Object detection quality → VLM atomic concept scoring → Soft logic composition. Errors at any stage propagate; the executor assumes correct program semantics.

- **Design tradeoffs:** Python vs. ProbLog: Python has 97% execution success vs. ~23% for NAVER's ProbLog, but Python programs can break the computation graph. Zero-shot vs. fine-tuned: Zero-shot relies entirely on VLM quality; fine-tuning on 1000 samples improved Ref-GTA from 34.92% to 69.90%. Imperative vs. declarative: Pure declarative (NeSyCoCo) gets 56% on CLEVR-Humans; adding imperative reasoning brings NePTune to 87.67%.

- **Failure signatures:** Dimensional mismatch (LLM generates score(query, 2) for single-object predicate → tensor shape error). Wrong bounding box color (LLM references "green box" instead of "red" → visual prompt doesn't match expected format). Analogical reasoning ("same color", "same size" concepts have lowest F1 0.60-0.84 and cause downstream errors). Domain shift (InternVL2.5 drops to 6.95% on Ref-GTA; NePTune recovers to 69.69% but still depends on VLM seeing relevant data).

- **First 3 experiments:** 1) Reproduce CLEVR atomic concept scoring (Table 12): Prompt VLM with visual bounding boxes on 200 CLEVR scenes; verify per-concept F1 scores match reported ranges (~0.94-0.99 for attributes, ~0.80-0.95 for spatial). 2) Ablate soft logic → crisp thresholds: Replace min() AND with thresholded Boolean AND at 0.5; measure accuracy drop on CLEVR Count and Exist categories (paper shows +12.5% and +6.09% gains from compositional structure). 3) Test domain shift robustness (Ref-GTA): Run NePTune zero-shot on Ref-GTA with InternVL2.5 backbone; expect ~70% accuracy. Then fine-tune with 100 samples to verify adaptation curve matches reported 69.90% ceiling.

## Open Questions the Paper Calls Out

None

## Limitations

- Performance heavily depends on LLM program generation quality and VLM confidence calibration, with no empirical validation of confidence-correctness correlation
- Visual prompting mechanism may not generalize to complex spatial reasoning or contextual understanding beyond atomic concepts
- Framework requires significant computational resources for multiple VLM queries per reasoning step, limiting practical deployment
- Zero-shot generalization claims difficult to verify without access to exact LLM prompt templates and program generation examples

## Confidence

**High confidence**: The compositional structure of NePTune (combining Python control flow with soft logic operations) is well-supported by experimental results showing consistent improvements over baselines across multiple benchmarks (CLEVR, Ref-GTA, etc.). The ablation studies demonstrating gains from soft logic over crisp thresholds are convincing.

**Medium confidence**: The claim that soft logic operators reduce error propagation relies on the assumption that VLM confidence scores are meaningful probability estimates. While Table 8 shows good atomic concept performance, the correlation between confidence and correctness for compositional queries isn't directly validated.

**Low confidence**: The zero-shot generalization claims are difficult to verify without access to the exact LLM prompt templates and program generation examples. The framework's robustness to systematic VLM failures (e.g., counting errors documented in related work) is assumed but not empirically tested.

## Next Checks

1. **Validate soft logic vs. crisp threshold ablation**: Implement both versions and measure accuracy on CLEVR Count (paper reports +12.5% gain) and Exist (paper reports +6.09% gain) categories. This directly tests whether continuous uncertainty propagation improves compositional reasoning.

2. **Stress-test VLM calibration correlation**: For 100 random CLEVR scenes, collect VLM confidence scores for all atomic concepts and compare against ground truth correctness. Compute calibration metrics (Expected Calibration Error) to verify the assumption that confidence scores meaningfully reflect perceptual accuracy.

3. **Test analogical reasoning failure modes**: Systematically evaluate NePTune's performance on "same color" and "same size" queries across different VLM backbones. Compare against a baseline that uses explicit similarity metrics rather than VLM-based analogical reasoning to isolate whether VLM limitations or the framework design cause these errors.