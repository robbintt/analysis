---
ver: rpa2
title: 'Sentiment Matters: An Analysis of 200 Human-SAV Interactions'
arxiv_id: '2510.08202'
source_url: https://arxiv.org/abs/2510.08202
tags:
- sentiment
- psychological
- ownership
- user
- polarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel dataset of 200 human-shared autonomous
  vehicle (SAV) interactions, featuring 2,136 conversational exchanges and empirical
  survey data. Using random forest modeling and LLM-based sentiment analysis, we identify
  key predictors of SAV acceptance and benchmark LLM performance against traditional
  sentiment tools.
---

# Sentiment Matters: An Analysis of 200 Human-SAV Interactions

## Quick Facts
- arXiv ID: 2510.08202
- Source URL: https://arxiv.org/abs/2510.08202
- Reference count: 32
- Key outcome: LLM-based sentiment analysis outperforms TextBlob for SAV interactions, with response sentiment polarity being the strongest predictor of perceived service quality.

## Executive Summary
This study introduces a novel dataset of 200 human-shared autonomous vehicle (SAV) interactions, featuring 2,136 conversational exchanges and empirical survey data. Using random forest modeling and LLM-based sentiment analysis, the researchers identify key predictors of SAV acceptance and benchmark LLM performance against traditional sentiment tools. Results show response sentiment polarity is the strongest predictor of perceived service quality, and LLM-based sentiment analysis aligns more closely with user-reported sentiment than TextBlob, despite modest correlation. The dataset and findings offer actionable insights for designing conversational SAV interfaces and advancing sentiment modeling in human-vehicle interactions.

## Method Summary
The study involved 50 participants interacting with four different SAV agent variants (Standard, PO, Anthropomorphism, Combined) across 200 total interactions. Participants engaged in conversational exchanges via a Python gradio interface, with responses generated by OpenAI's GPT-3.5-turbo. Post-interaction surveys measured six psychological factors and self-reported sentiment. Sentiment analysis was performed using both TextBlob lexicon-based methods and GPT-4o-mini zero-shot LLM classification. Random forest models identified item-level predictors of Behavioral Intention and Quality of Service, while correlation analysis compared computed sentiment metrics against survey-based ratings.

## Key Results
- Response sentiment polarity is the strongest predictor of perceived service quality (44.7-57.9% relative importance)
- LLM-based sentiment analysis achieves stronger correlation with user-reported sentiment (r=0.199) than TextBlob
- Minimum polarity (most negative exchange) correlates better with overall sentiment perception than mean or median measures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Response sentiment polarity is the strongest item-level predictor of perceived service quality in human-SAV interactions.
- Mechanism: Users form overall service quality judgments disproportionately from the emotional tone of SAV responses. When polarity is positive, users rate service quality higher, which in turn increases behavioral intention to use. This effect persists across all four prompting strategy variants.
- Core assumption: Users evaluate conversational agents similarly to how they evaluate human service interactions, where emotional tone carries substantial weight in quality assessments.
- Evidence anchors:
  - [abstract] "Results show response sentiment polarity is the strongest predictor of perceived service quality"
  - [section III-C] "In both groups, Polarity plays a key role in predicting Quality of Service (QoS), with relative importance of 44.7% and 57.9% respectively"
  - [corpus] Limited external validation; neighbor papers focus on psychological factors in SAV interactions but don't specifically test sentiment-service quality pathways.
- Break condition: If users prioritize functional task completion (e.g., accurate navigation) over conversational tone in high-stakes scenarios, this mechanism may weaken or reverse.

### Mechanism 2
- Claim: LLM-based sentiment analysis aligns more closely with user-reported sentiment than lexicon-based TextBlob, even with simple zero-shot prompts.
- Mechanism: LLMs capture contextual and implied sentiment that lexicon-based methods miss. TextBlob relies on predefined word associations, while LLMs interpret tone holistically, including negation, sarcasm, and conversational context.
- Core assumption: Zero-shot LLM prompts can extract sentiment judgments comparable to human perception without task-specific training.
- Evidence anchors:
  - [abstract] "LLM-based sentiment analysis aligns more closely with user-reported sentiment than TextBlob, despite modest correlation"
  - [section IV-C] "LLM-based minimum polarity measure (llm min) achieved the strongest correlation with users' self-reported polarity (r= 0.199, p= 0.005)"
  - [corpus] External validation is weak; no neighbor papers directly benchmark LLM vs. lexicon sentiment in conversational agent contexts.
- Break condition: If domain-specific vocabulary or multilingual inputs are introduced, zero-shot LLM performance may degrade without fine-tuning.

### Mechanism 3
- Claim: Minimum polarity (the most negative exchange) correlates better with overall sentiment perception than mean or median measures.
- Mechanism: Users weight negative experiences more heavily than positive ones when forming aggregate judgments (negativity bias). A single highly negative exchange disproportionately shapes their retrospective sentiment rating.
- Core assumption: Users' post-interaction survey responses reflect peak negative moments rather than average experience.
- Evidence anchors:
  - [section IV-C] "The prominence of the minimum polarity score indicates users are likely affected by the most negative ('worst-case') exchanges during the interactions"
  - [section IV-C] References negativity bias literature [30, 31] from social psychology
  - [corpus] No direct corpus validation for this specific mechanism in conversational agent contexts.
- Break condition: If interactions are brief (e.g., single-exchange transactions), minimum and mean converge, eliminating this differential weighting.

## Foundational Learning

- Concept: Random Forest feature importance for predictive modeling
  - Why needed here: The paper uses Random Forest to identify which survey items predict behavioral intention and service quality. Understanding Gini importance or permutation importance is required to interpret chord diagrams.
  - Quick check question: If item A has 30% relative importance and item B has 10%, can you conclude item A causes higher behavioral intention than item B? (Answer: No—importance indicates predictive power, not causal effect direction.)

- Concept: Zero-shot prompting for LLMs
  - Why needed here: Case Study 2 uses zero-shot prompts to extract sentiment scores without training examples. Understanding prompt design constraints is critical for replicating or improving results.
  - Quick check question: What is the difference between zero-shot and few-shot prompting, and why might zero-shot underperform on domain-specific sentiment? (Answer: Zero-shot provides no examples; few-shot includes labeled examples. Zero-shot may miss domain-specific sentiment markers.)

- Concept: Spearman correlation for non-parametric data
  - Why needed here: The paper uses Spearman's r to compare computed sentiment metrics against survey ratings. Understanding why Spearman (not Pearson) was chosen is necessary for proper interpretation.
  - Quick check question: Why would researchers choose Spearman over Pearson for correlating sentiment scores with survey ratings? (Answer: Survey data is ordinal and may not meet normality assumptions; Spearman ranks data and is robust to non-linear relationships.)

## Architecture Onboarding

- Component map: User Interface -> OpenAI API -> SAV Agent Backend -> Sentiment Analysis Pipeline -> Data Storage
- Critical path: Participant enters request → gradio UI captures input → Input + system prompt sent to OpenAI API → SAV response returned and displayed → Exchange logged to Data_human-SAVinteraction.xlsx → Post-interaction survey administered → Sentiment analysis run offline
- Design tradeoffs:
  - Using GPT-3.5-turbo for simulation: Provides realistic conversational variability but introduces model-specific biases; results may not generalize to other LLMs or production systems
  - Zero-shot sentiment prompt: Simple and reproducible but yields modest correlations (r≈0.2); fine-tuning or few-shot examples could improve alignment
  - Screen-based prototype: Controlled environment but lacks multimodal cues (speech prosody, facial expressions) that would exist in real vehicle cabins
- Failure signatures:
  - Sentiment correlation near zero: Indicates prompt may not be capturing user-perceived sentiment; check if SAV responses are too neutral or if prompt instructions are ambiguous
  - k-anonymity violation in demographics: The paper reports k=1 uniqueness; if extending this dataset, apply generalization or suppression before release
  - Ordering effects despite randomization: If participants report fatigue or pattern recognition across four interactions, within-subject design may introduce bias
- First 3 experiments:
  1. Replicate sentiment analysis with few-shot prompts: Add 3-5 labeled examples to the zero-shot prompt and measure if Spearman correlation with user-reported sentiment improves beyond r=0.199
  2. Test minimum vs. mean polarity weighting: Build a regression model predicting service quality using weighted sentiment (heavier weight on minimum polarity) and compare R² against unweighted mean
  3. Cross-validate on held-out prompting strategies: Train Random Forest on three SAV variants and test prediction accuracy on the fourth to assess generalization across conversational styles

## Open Questions the Paper Calls Out
None

## Limitations
- The zero-shot LLM sentiment analysis shows modest correlation with user-reported sentiment (r=0.199), suggesting room for improvement
- k-anonymity violation (k=1) in demographic data presents a privacy concern that limits dataset sharing potential
- Screen-based prototype may not fully capture multimodal interactions that would occur in real vehicle cabins, potentially limiting ecological validity

## Confidence
- High confidence in random forest modeling approach and sentiment analysis methodology
- Medium confidence in generalizability of LLM-based sentiment performance across different models and domains
- Low confidence in external validation of sentiment-service quality mechanism without additional empirical studies

## Next Checks
1. Conduct few-shot prompting experiments with labeled sentiment examples to determine if correlation with user-reported sentiment improves beyond the current r=0.199
2. Test the minimum polarity weighting hypothesis by building regression models that differentially weight negative vs. positive exchanges and compare predictive performance
3. Replicate the study with a multimodal interface (speech + visual) to assess whether findings hold when conversational context includes non-textual cues