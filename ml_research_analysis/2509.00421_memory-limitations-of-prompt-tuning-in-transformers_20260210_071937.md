---
ver: rpa2
title: Memory Limitations of Prompt Tuning in Transformers
arxiv_id: '2509.00421'
source_url: https://arxiv.org/abs/2509.00421
tags:
- prompt
- transformer
- tuning
- output
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We characterize the maximal memorization capability of a transformer\
  \ with respect to prompt tuning. That is we obtain an integer K\u2014depending on\
  \ the parameters of the transformer and on an upper bound R of the magnitude of\
  \ tokens\u2014such that for any inputs X1, ..., Xk \u2208 Rd\xD7m, the proportion\
  \ of outputs that are accessible through prompt tuning decreases exponentially with\
  \ k from the rank k \u2265 K."
---

# Memory Limitations of Prompt Tuning in Transformers

## Quick Facts
- arXiv ID: 2509.00421
- Source URL: https://arxiv.org/abs/2509.00421
- Authors: Maxime Meyer; Mario Michelessa; Caroline Chaux; Vincent Y. F. Tan
- Reference count: 40
- Primary result: Memorization capacity through prompt tuning scales at most linearly with prompt length

## Executive Summary
This paper establishes fundamental theoretical limits on how much information transformers can memorize through prompt tuning. The authors prove that the number of input/output pairs a transformer can reliably learn through prompt tuning is bounded by the prompt length, specifically k ∈ O(mp/m) where mp is prompt length and m is sequence length. Beyond a threshold K, the proportion of accessible outputs decreases exponentially with the number of pairs. The work provides rigorous mathematical bounds using covering/packing arguments and Lipschitz continuity, and extends to the mean-field limit for arbitrary sequence lengths.

## Method Summary
The authors analyze transformer memory capacity through theoretical proofs using covering and packing numbers under Lipschitz continuity assumptions. They establish bounds on the proportion of ε-accessible output sequences for transformers with fixed weights and learnable prompts. The framework extends to the mean-field limit, treating sequences as probability measures to handle arbitrary lengths. The analysis includes both standard and masked self-attention variants, and provides specific bounds for single-layer transformers showing geometric constraints on accessible outputs.

## Key Results
- The number of input/output pairs a transformer can memorize through prompt tuning scales at most linearly with prompt length: k ∈ O(mp/m)
- For k pairs beyond threshold K, the proportion of accessible outputs decreases exponentially with k
- Single-layer transformers can only access outputs lying in an approximate hyperplane for inputs sharing common tokens
- Approximate memorization also fails with minimum error proportional to (1 - ||W_1||·||W_2||)r/2

## Why This Works (Mechanism)

### Mechanism 1: Linear Scaling Bound on Prompt Information Capacity
The proof uses covering/packing number arguments. For prompt length mp, the pre-prompt space can be covered by approximately (3Lr/ε)^{d·mp} balls of radius ε/L. Meanwhile, the output space for k pairs contains approximately (r/3ε)^{d·m·k} distinct sequences. When k exceeds C·mp/m, the ratio of accessible to total outputs vanishes exponentially. Core assumption: inputs are bounded within radius r and r > 3ε.

### Mechanism 2: Exponential Degradation with Context Length Beyond Threshold
Using the mean-field framework, the output distribution space over probability measures has covering number that grows as exp(3d/ε^d), while the accessible distributions are bounded by the Lipschitz-constrained pre-prompt space. The ratio Cin/Cout yields exponential decay. Core assumption: Transformer weights are fixed and W_1 and W_2 have bounded spectral norms.

### Mechanism 3: Single-Layer Attention Expressivity Collapse
Single-head attention decomposes as Att(x_0, [P, x_i, x_0]) = a_P + a_i, where a_P depends only on the pre-prompt and a_i on the input token. This creates only h degrees of freedom (for h heads) to satisfy h+1 independent output constraints, making most output configurations geometrically inaccessible. Core assumption: ||W_1||_2 · ||W_2||_2 < 1.

## Foundational Learning

- Concept: **Lipschitz Continuity**
  - Why needed here: All bounds rely on the transformer being Lipschitz continuous with constant L, which constrains how many distinct outputs can be reached from a bounded pre-prompt space.
  - Quick check question: Given a function f with Lipschitz constant L, if two inputs differ by at most δ, what is the maximum difference in their outputs?

- Concept: **Covering and Packing Numbers**
  - Why needed here: The core proof technique counts how many ε-separated outputs exist (packing) versus how many can be reached via pre-prompts (covering). The ratio determines the proportion of accessible outputs.
  - Quick check question: In a unit cube in R^n, how does the ε-covering number scale as ε → 0?

- Concept: **Mean-Field Limit of Attention**
  - Why needed here: Extends discrete sequence analysis to continuous probability measures, enabling bounds for arbitrarily long sequences without the O(n) Lipschitz constant growth.
  - Quick check question: How does viewing a sequence as an empirical measure μ = (1/m)Σδ_{x_i} enable permutation-invariant analysis?

## Architecture Onboarding

- Component map: Input X concatenated with pre-prompt P → [P, X] → through attention and MLP layers with fixed weights → Output at positions mp: onward (excluding pre-prompt portion) → Accessibility requires output within ε of target Y

- Critical path: Input X concatenated with pre-prompt P → [P, X] → through attention and MLP layers with fixed weights → Output at positions mp: onward (excluding pre-prompt portion) → Accessibility requires output within ε of target Y

- Design tradeoffs:
  - Longer prompts vs. memorization: Increasing mp only linearly increases capacity; diminishing returns for in-context learning with many examples
  - Soft vs. hard prompts: Theoretical gain of soft prompts over discrete tokens is at most linear in embedding dimension
  - Masked vs. unmasked attention: Results hold for both, but masked attention requires position-aware distance metrics in mean-field regime

- Failure signatures:
  - Performance degrades sharply once number of I/O pairs exceeds ~mp/m threshold
  - Single-layer transformers cannot memorize outputs lying outside a specific subspace, regardless of pre-prompt
  - Approximate memorization also fails with minimum error proportional to (1 - ||W_1||·||W_2||)r/2

- First 3 experiments:
  1. **Threshold validation**: For a fixed transformer, vary k (number of I/O pairs) while holding mp constant. Plot memorization accuracy vs. k; expect sharp drop near k ≈ C·mp/m.
  2. **Lipschitz constant estimation**: Compute empirical Lipschitz constant L via gradient norm analysis; verify theoretical bound L ≤ ||W_v||(1 + 3||A||r²)exp(2||A||r²) holds on actual model weights.
  3. **Single-layer probe**: Construct synthetic input pairs sharing a common token; attempt prompt tuning to reach orthogonal target outputs. Measure the subspace constraint dimension empirically.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the memorization bounds for prompt tuning extend to generalization capacity, or are memorization and generalization fundamentally independent?
- Basis in paper: [inferred] The paper proves memorization limits but explicitly focuses on "the memorization capability of prompt tuning" without addressing generalization to unseen examples.
- Why unresolved: The proof techniques rely on counting distinct ε-separated output sequences/distributions, which captures memorization but not the ability to learn underlying patterns that transfer to new inputs.
- What evidence would resolve it: Theoretical analysis showing whether the Lipschitz-based arguments apply to generalization error, or empirical studies comparing memorization vs. generalization performance degradation as prompt length increases.

### Open Question 2
- Question: Can architectural modifications (recurrence, external memory, alternative attention mechanisms) overcome the O(m_p/m) scaling limitation, or do similar fundamental bounds apply universally?
- Basis in paper: [explicit] The conclusion states "transformers suffer from an intrinsic memory limitation, independent of context length" and questions whether architectural support helps.
- Why unresolved: The proofs rely on properties of the standard attention mechanism (Lipschitz continuity, output space geometry) and may not apply to architectures with fundamentally different computational structures.
- What evidence would resolve it: Extension of the theoretical framework to architectures with recurrence or explicit memory, showing whether similar or different bounds emerge.

### Open Question 3
- Question: How do the threshold constants (K, the Lipschitz constant L) scale with model depth, width, and parameter count?
- Basis in paper: [inferred] Theorem 4.7 provides bounds depending on L and r, and Section 4.3 introduces a threshold K, but the analysis is for a fixed arbitrary transformer without examining scaling behavior.
- Why unresolved: The results characterize limits for any given transformer but do not address whether larger models have proportionally larger capacities or whether the bounds become more or less restrictive with scale.
- What evidence would resolve it: Empirical measurement of how memorization capacity grows with model size, or theoretical analysis of how Lipschitz constants of transformers scale with architectural dimensions.

## Limitations

- The theoretical bounds rely on worst-case covering/packing number arguments that may overestimate practical limitations
- Single-layer analysis requires specific dimensional requirements (d - h² - 2h > 0) that many practical transformers may not satisfy
- The mean-field extension's practical implications are uncertain due to approximation steps between discrete and continuous analysis

## Confidence

**High Confidence (9/10)**: The linear scaling bound k ∈ O(mp/m) and exponential decay for k > K. These follow from standard covering/packing arguments under mild Lipschitz assumptions that are easily verified for practical transformers.

**Medium Confidence (6/10)**: The single-layer expressivity collapse. While the mathematical proofs are correct under stated assumptions, the practical relevance depends on whether real transformers satisfy the dimensionality requirements and whether the single-layer model captures meaningful behavior of deeper architectures.

**Low Confidence (4/10)**: The mean-field generalization's practical implications. The theoretical elegance of extending to arbitrary sequence lengths through probability measures is compelling, but the connection to finite, discrete transformers involves approximation steps that may not preserve the exponential decay behavior in practice.

## Next Checks

1. **Empirical Lipschitz Constant Verification**: Compute the actual Lipschitz constant L for a pretrained transformer by estimating ||W_v|| and ||A|| for each layer through gradient analysis. Compare the empirical L to the theoretical bound from Proposition 2.11 to verify the tightness of the covering argument.

2. **Single-Layer Subspace Constraint Test**: Implement a single-layer transformer with configurable parameters. For input pairs sharing a common token, attempt prompt tuning to reach orthogonal target outputs. Measure the dimension of the accessible output subspace and compare to the theoretical bound (d-h²-h)!/(d-h²-2h-1)! to validate Theorem 5.7.

3. **Threshold Behavior Validation**: Using a fixed transformer, systematically vary the number of I/O pairs k while holding prompt length mp constant. Plot the proportion of successfully memorized outputs against k and verify the predicted sharp drop near k ≈ C·mp/m, as well as the exponential decay for k beyond this threshold.