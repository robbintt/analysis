---
ver: rpa2
title: Reinforcement Learning-based Approach for Vehicle-to-Building Charging with
  Heterogeneous Agents and Long Term Rewards
arxiv_id: '2502.18526'
source_url: https://arxiv.org/abs/2502.18526
tags:
- power
- charging
- peak
- action
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the Vehicle-to-Building (V2B) energy management
  challenge by optimizing EV charging and discharging to reduce energy costs and peak
  demand in smart buildings. The authors formulate the problem as a Markov Decision
  Process with continuous action space and delayed, sparse rewards.
---

# Reinforcement Learning-based Approach for Vehicle-to-Building Charging with Heterogeneous Agents and Long Term Rewards

## Quick Facts
- arXiv ID: 2502.18526
- Source URL: https://arxiv.org/abs/2502.18526
- Reference count: 40
- Primary result: Novel DDPG framework with action masking and MILP guidance reduces building energy costs by up to 14.2% vs. Fast Charge baseline

## Executive Summary
This paper addresses the Vehicle-to-Building (V2B) energy management challenge by optimizing EV charging and discharging to reduce energy costs and peak demand in smart buildings. The authors formulate the problem as a Markov Decision Process with continuous action space and delayed, sparse rewards. They propose a novel Deep Reinforcement Learning framework combining DDPG with action masking and MILP-driven policy guidance to balance exploration and exploitation while ensuring user SoC requirements. Evaluated on nine months of real-world data from Nissan's research lab, the approach achieves the lowest total bills in eight out of nine months compared to baselines, reducing costs by up to 14.2% compared to the real-world Fast Charge baseline. The method demonstrates superior peak shaving performance, achieving positive peak shaving in six months, while ensuring all EVs meet their SoC requirements.

## Method Summary
The approach uses Deep Deterministic Policy Gradient (DDPG) with continuous action space to control 15 heterogeneous EV chargers (5 bidirectional, 10 unidirectional). The method employs action masking to enforce physical constraints (SoC bounds, peak power limits, departure requirements) and MILP-driven policy guidance to accelerate convergence. State features include time, building power, estimated peak power, SoC gaps, and remaining charging time. The policy is trained on daily episodes with estimated monthly peak power as a state feature to handle long-term demand charges. Training uses 60 clustered samples per month, with MILP solver stochastically injecting optimal actions into the replay buffer. The system guarantees constraint satisfaction while optimizing total cost (energy + demand charges) and achieving superior peak shaving performance.

## Key Results
- Achieves lowest total bills in 8 out of 9 months compared to baselines
- Reduces costs by up to 14.2% compared to real-world Fast Charge baseline
- Achieves positive peak shaving in 6 months while maintaining zero missing SoC
- MILP policy guidance improves convergence and final performance across all metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Action masking constrains exploration to feasible actions, improving sample efficiency and guaranteeing constraint satisfaction.
- **Mechanism:** Six differentiable masking operations (Masks 1-6) post-process raw actor network outputs to enforce physical and operational constraints: no charging when no EV connected, preventing overcharge on unidirectional chargers, forcing minimum charge/discharge before departure, respecting peak power bounds, and preventing discharge below building load.
- **Core assumption:** The paper assumes domain-specific constraints are known a priori and can be expressed as differentiable operations that don't interfere with policy gradient backpropagation.
- **Evidence anchors:**
  - [abstract] "combines DDPG with action masking... to balance the exploration of continuous action spaces to meet user charging demands"
  - [section 4.2.2] "Findings from [4, 6] confirm that differentiable action masking does not interfere with the policy gradient backpropagation process... preventing the policy from exploring invalid actions, thereby improving training efficiency"
  - [corpus] Limited direct corpus evidence on action masking in V2B specifically; related work on multi-agent RL (arXiv:2509.18088) discusses coordination mechanisms but not masking directly.

### Mechanism 2
- **Claim:** MILP-driven policy guidance accelerates convergence by injecting near-optimal expert demonstrations into the replay buffer.
- **Mechanism:** A stochastic process (biased coin flip with probability RPG) replaces RL actions with MILP-optimal actions computed with full future knowledge, storing these transitions in the replay buffer alongside exploratory actions. This guides the agent toward better regions of the action space without requiring full imitation learning.
- **Core assumption:** The MILP solver can efficiently solve the deterministic version of the problem (paper reports ~0.05 seconds for 15 EVs), and optimal actions under full information provide useful learning signal even when deployed without that information.
- **Evidence anchors:**
  - [abstract] "efficient MILP-driven policy guidance to balance exploration and exploitation"
  - [section 4.2.3] "By blending MILP-generated actions with those from the RL actor network, the agent explores a more effective action space, improving its ability to handle large continuous action spaces and long-term rewards"
  - [section 5.2 ablation] "RL\P approach, which removes policy guidance, results in decreased performance, highlighting its importance in optimizing actions during training"
  - [corpus] arXiv:2601.03476 (related V2B work) notes uncertainty handling challenges but doesn't use MILP guidance; arXiv:2601.01581 proposes negotiation frameworks instead of expert guidance.

### Mechanism 3
- **Claim:** Estimated monthly peak power as a state feature enables long-term demand charge optimization within daily training episodes.
- **Mechanism:** Rather than training on full monthly episodes (computationally expensive), the approach trains on daily episodes but includes an estimated peak power value (initialized from MILP solutions on training data, updated during inference) as a state feature. This allows the policy to reason about demand charge impact even when training horizons are shorter.
- **Core assumption:** A reasonable peak power estimate can be derived offline and provides sufficient signal for the policy to learn demand-aware behavior; the estimate can be adjusted during deployment.
- **Evidence anchors:**
  - [section 4.1 State] "the power gap between the current building power and the estimated peak power for the billing period... aids the RL model in estimating the optimal peak power for demand charge reduction"
  - [section 4.2.5] "we include a monthly peak power estimate for each month as an input feature derived from optimal action sequences generated by the MILP solver"
  - [section 5.2 ablation] "RL\E approach shows worse results, highlighting the importance of accurate long-term peak power estimation during training... When set to 0, the RL model fails to converge to a good global optimum"
  - [corpus] No direct corpus comparison for peak estimation as state feature; most prior work focuses on single-day horizons.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) with continuous action spaces**
  - Why needed here: The V2B problem requires sequential decisions under uncertainty where charging power is continuous (not discrete on/off). Standard Q-learning doesn't handle continuous actions.
  - Quick check question: Can you explain why discretizing charging power into 1kW intervals would be problematic for a 20kW charger with 15 EVs?

- **Concept: Deep Deterministic Policy Gradient (DDPG)**
  - Why needed here: DDPG extends DQN to continuous action spaces using an actor-critic architecture. The actor outputs continuous actions, the critic estimates Q-values for state-action pairs.
  - Quick check question: Why does DDPG need both an actor network and a critic network, unlike standard Q-learning?

- **Concept: Demand charges vs. energy charges**
  - Why needed here: Building electricity bills have two components—energy (total kWh consumed) and demand (peak kW drawn over billing period). Optimizing only energy cost can actually increase demand charges, requiring multi-objective optimization.
  - Quick check question: If you charge all EVs at maximum rate during off-peak hours to minimize energy cost, what happens to the demand charge?

## Architecture Onboarding

- **Component map:**
  ```
  Training Pipeline:
  Daily Sample → Environment Simulator → State Features (37 dims)
                                              ↓
              ┌───────────────────────────────┴────────────────────────────┐
              │                        Replay Buffer                        │
              │  [State, Action, Reward, Next_State] transitions            │
              │  Mixed sources: RL policy + MILP guidance (stochastic mix)  │
              └───────────────────────────────┬────────────────────────────┘
                                              ↓
              Actor Network (2 hidden layers, 96 units each)
                     ↓                                    ↓
              Raw Actions                         Action Masking
              (tanh → [-1,1] → scale)             (6 constraints)
                     ↓                                    ↓
              Masked Actions ←───────────────────────────┘
                     ↓
              Critic Network → Q-value → Loss → Policy Gradient

  Inference Pipeline:
  Real-time data → Peak Power Estimator → State Features → Trained Actor
                                                                     ↓
                                                              Action Masking
                                                                     ↓
                                                              Charger Commands
  ```

- **Critical path:**
  1. State feature engineering (reducing 100 to 37 features with domain knowledge)
  2. Peak power estimation initialization from MILP
  3. Action masking implementation (all 6 masks must be correct)
  4. Policy guidance probability tuning (RPG hyperparameter)
  5. Reward function weight balancing (λS, λE, λD)

- **Design tradeoffs:**
  - **Daily vs. monthly episodes:** Training efficiency vs. long-term reward optimization. Paper chose daily with peak power estimate.
  - **MILP guidance frequency (RPG):** More guidance = faster convergence but potentially less exploration. Paper uses 0.5-0.7.
  - **Action masking aggressiveness:** Stricter masks constrain exploration but guarantee feasibility. Paper applies all 6 masks.
  - **Heuristic vs. RL for off-peak/weekends:** Paper uses least-laxity heuristic during non-peak times to reduce complexity.

- **Failure signatures:**
  - **High missing SoC at departure:** Likely failure in Masks 2-4 (force charging). Check SoC constraints and time-to-departure calculations.
  - **Negative peak shaving:** Policy not learning demand charge awareness. Check peak power estimate quality and reward weight λD.
  - **Slow convergence or unstable training:** Check replay buffer mixing ratio, action noise scale, or gradient flow through masking operations.
  - **Constraint violations in deployment:** Action masking not applied at inference time, or real-world constraints differ from training assumptions.

- **First 3 experiments:**
  1. **Validate action masking correctness:** Run random actions through masking pipeline on test scenarios, verify 100% constraint satisfaction (no SoC violations, no exceeding peak bounds, no missing departures).
  2. **Ablate policy guidance:** Train with RPG={0, 0.3, 0.5, 0.7, 1.0} on one month of data, plot convergence speed and final total bill. Confirm paper's finding that moderate guidance (0.5-0.7) outperforms pure RL or pure imitation.
  3. **Test generalization across months:** Train on May-July, test on August-October without retraining. Measure performance degradation vs. month-specific models. This validates whether the learned policy generalizes or overfits to seasonal patterns.

## Open Questions the Paper Calls Out
- **Question:** Can the charger-to-EV assignment function be integrated into the RL action space to optimize assignment and power control jointly, rather than relying on the fixed FIFO heuristic?
- **Question:** Would a unified RL policy trained to handle both peak and off-peak periods perform better than the hybrid approach of switching between RL and heuristics?
- **Question:** What specific properties of the sampling distribution cause performance degradation when increasing the number of training samples beyond 60?

## Limitations
- Performance relies heavily on accurate MILP guidance and peak power estimation
- Assumes stationary charging patterns and building load distributions
- Action masking requires careful domain knowledge and may not generalize to different constraint structures
- MILP computational complexity could become prohibitive at scale

## Confidence

- **High confidence:** The action masking mechanism effectively enforces physical constraints and improves sample efficiency
- **Medium confidence:** MILP policy guidance accelerates convergence and improves final performance
- **Medium confidence:** Long-term peak power estimation enables demand charge optimization within daily training episodes

## Next Checks

1. **Constraint robustness testing:** Systematically vary building load patterns and EV arrival distributions to test action masking's effectiveness under distribution shift.
2. **MILP guidance ablation at scale:** Test the approach with 30+ EVs to verify MILP solve times remain acceptable and policy guidance remains effective.
3. **Cross-building generalization:** Train on Nissan data and test on a different building with different load patterns and EV usage to assess real-world transferability.