---
ver: rpa2
title: Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation
  Solution
arxiv_id: '2510.18019'
source_url: https://arxiv.org/abs/2510.18019
tags:
- languages
- language
- watermarking
- translation
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current multilingual watermarking methods fail to remain robust
  under translation attacks in medium- and low-resource languages, primarily due to
  tokenizer limitations that fragment words into subword units. This vulnerability
  allows adversaries to bypass watermark detection by translating text, undermining
  the security of LLM outputs across diverse languages.
---

# Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation Solution

## Quick Facts
- arXiv ID: 2510.18019
- Source URL: https://arxiv.org/abs/2510.18019
- Authors: Asim Mohamed; Martin Gubri
- Reference count: 39
- Primary result: Current multilingual watermarking methods fail under translation attacks in medium- and low-resource languages due to tokenizer limitations

## Executive Summary
Current multilingual watermarking methods show significant vulnerabilities when subjected to translation attacks, particularly for medium- and low-resource languages. The core issue stems from tokenizer fragmentation, where words are split into subword units, allowing adversaries to bypass watermark detection by translating text. This fundamental weakness undermines the security guarantees of LLM outputs across diverse languages. The study introduces STEAM, a back-translation-based detection method that restores watermark strength lost through translation, demonstrating substantial improvements over existing approaches.

## Method Summary
The research addresses the critical vulnerability of multilingual watermarking under translation attacks by introducing STEAM (Signal Through Translation-Enhanced Adversarial Mitigation), a back-translation-based detection method. STEAM works by detecting translated text and reversing the translation effects to restore watermark strength. The method is designed to be compatible with any existing watermarking approach, robust across different tokenizers and languages, non-invasive, and easily extendable. The approach involves applying back-translation to translated text and then running the standard watermarking detection on the recovered content, effectively bridging the gap between original and translated outputs.

## Key Results
- Average gains of +0.19 AUC and +40 percentage points TPR@1% over existing methods
- Improvements up to +0.33 AUC and +64.5 percentage points TPR@1% in optimal cases
- Validated across 17 languages with significant robustness improvements

## Why This Works (Mechanism)
The vulnerability in current multilingual watermarking stems from how tokenizers fragment words into subword units, particularly in medium- and low-resource languages where vocabulary coverage is limited. When text is translated, these fragmented tokens are reassembled differently, disrupting the watermark signal embedded in the original token sequence. STEAM works by applying back-translation to translated text, effectively reconstructing the original token patterns and restoring the watermark signal. This approach leverages the fact that while the surface form changes during translation, the underlying semantic content can be mapped back to recover the original watermark structure.

## Foundational Learning
- **Tokenization fragmentation**: Why needed - understanding how words split into subwords affects watermark embedding; Quick check - compare token counts before/after translation
- **Back-translation mechanics**: Why needed - knowing how to reverse translation effects; Quick check - measure semantic preservation after back-translation
- **Watermark detection metrics**: Why needed - evaluating TPR@1% and AUC for security assessment; Quick check - verify detection rates across language pairs
- **Adversarial attack vectors**: Why needed - identifying translation as bypass method; Quick check - test with multiple translation tools
- **Cross-lingual consistency**: Why needed - ensuring watermark works across language boundaries; Quick check - compare detection rates across all tested languages
- **Token-level vs sequence-level watermarking**: Why needed - understanding granularity of watermark embedding; Quick check - test with different tokenizer types

## Architecture Onboarding

**Component Map**: Text Input -> Tokenizer -> Watermark Embedding -> Translation Attack -> Back-translation (STEAM) -> Watermark Detection -> Security Assessment

**Critical Path**: The core pipeline involves receiving text, applying watermark embedding, detecting translation attacks, running back-translation recovery through STEAM, and performing watermark detection on the recovered text. The back-translation step is critical as it bridges the gap between attacked and original text.

**Design Tradeoffs**: STEAM trades computational overhead for security robustness. While back-translation adds latency, it provides universal compatibility with existing watermarking methods without requiring architectural changes. The method prioritizes non-invasiveness over optimization for specific tokenizer types.

**Failure Signatures**: Detection failures occur when back-translation cannot adequately reconstruct original token patterns, particularly with poor translation quality, idiomatic expressions, or when using highly specialized vocabulary. Performance degrades with aggressive paraphrasing attacks that change semantic structure beyond simple translation.

**First Experiments**:
1. Test STEAM against back-translation with varying intermediate languages to assess robustness
2. Compare detection rates across different tokenizer types (WordPiece, SentencePiece, BPE) on the same language
3. Evaluate STEAM performance on specialized domains (legal, medical) versus general text

## Open Questions the Paper Calls Out
None

## Limitations
- Focus primarily on back-translation as attack vector, potentially missing other translation-based attacks
- Computational overhead and latency impact on production systems not quantified
- Evaluation dataset may not fully represent real-world adversarial sophistication

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Watermarking vulnerability under translation attacks | High |
| Tokenizer fragmentation as primary cause | High |
| STEAM method effectiveness | Medium |
| Generalizability across tokenizers and languages | Medium |

## Next Checks

1. Test STEAM against alternative translation-based attacks including paraphrasing tools, round-trip translation with different intermediate languages, and commercial translation APIs to assess robustness beyond back-translation

2. Evaluate STEAM's performance on specialized domains (legal, medical, technical) where translation quality and word fragmentation patterns may differ significantly from general text

3. Measure the computational overhead and latency impact of STEAM in production scenarios, including memory usage and inference time increases compared to baseline watermarking methods