---
ver: rpa2
title: 'Challenging Multilingual LLMs: A New Taxonomy and Benchmark for Unraveling
  Hallucination in Translation'
arxiv_id: '2510.24073'
source_url: https://arxiv.org/abs/2510.24073
tags:
- hallucination
- translation
- source
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HalloMTBench, a multilingual benchmark for
  diagnosing translation hallucinations in Large Language Models (LLMs). The authors
  propose a taxonomy distinguishing Instruction Detachment (e.g., untranslated content,
  wrong target language) from Source Detachment (e.g., extraneous additions, repetition).
---

# Challenging Multilingual LLMs: A New Taxonomy and Benchmark for Unraveling Hallucination in Translation

## Quick Facts
- arXiv ID: 2510.24073
- Source URL: https://arxiv.org/abs/2510.24073
- Reference count: 5
- Primary result: Introduces HalloMTBench, a multilingual benchmark for diagnosing translation hallucinations in LLMs, achieving 93.68-100% agreement with human annotations

## Executive Summary
This paper addresses the critical challenge of translation hallucinations in multilingual Large Language Models by introducing HalloMTBench, a comprehensive benchmark that systematically categorizes and evaluates hallucination types. The authors develop a novel taxonomy distinguishing Instruction Detachment (e.g., untranslated content, wrong target language) from Source Detachment (e.g., extraneous additions, repetition). Through an ensemble of LLM judges and expert validation, they curated 5,435 high-quality hallucination instances across 11 English-to-X language pairs, providing researchers with a challenging testbed for evaluating and improving LLM translation quality.

## Method Summary
The authors constructed HalloMTBench through a multi-stage pipeline involving 4 frontier LLMs generating translation candidates, followed by evaluation from 3 LLM judges to identify potential hallucinations. Expert annotators then validated these instances, resulting in 5,435 high-quality hallucinations across 11 language pairs. The benchmark employs a dual taxonomy categorizing hallucinations as either Instruction Detachment (where models fail to follow translation instructions) or Source Detachment (where models add or modify content beyond the source). The evaluation methodology achieved high agreement with human annotations (93.68-100%), establishing reliability for the benchmark's hallucination instances.

## Key Results
- HalloMTBench contains 5,435 hallucination instances across 11 English-to-X language pairs with 93.68-100% agreement with human annotations
- 17 evaluated LLMs showed distinct hallucination patterns tied to model scale, source length, linguistic bias, and RL-induced language mixing
- The taxonomy effectively distinguishes Instruction Detachment from Source Detachment hallucinations, providing actionable diagnostic categories

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its systematic approach to hallucination detection and categorization. By employing multiple LLM judges followed by expert validation, the pipeline captures hallucinations that might be missed by single-evaluation methods. The dual taxonomy provides clear diagnostic categories that map directly to failure modes, enabling targeted improvements. The use of frontier models for both generation and evaluation ensures the benchmark remains challenging and relevant to current LLM capabilities, while the multilingual coverage reveals language-specific vulnerability patterns.

## Foundational Learning
- **Translation hallucination taxonomy**: Understanding the distinction between Instruction Detachment (model fails to follow translation instructions) and Source Detachment (model adds/modifies content beyond source) - needed to systematically diagnose and address translation failures; quick check: can identify which category a given hallucination belongs to.
- **Multilingual evaluation methodology**: Using ensemble of LLM judges plus expert validation to curate high-quality benchmark instances - needed to ensure benchmark reliability and representativeness; quick check: validation agreement rates exceed 90%.
- **Hallucination detection in translation**: Identifying patterns like untranslated content, wrong target language, extraneous additions, and repetition - needed to understand model failure modes; quick check: can distinguish hallucination types across different language pairs.
- **Model scale effects on translation quality**: Understanding how model size influences hallucination frequency and type - needed to guide model selection and training strategies; quick check: larger models show reduced hallucination rates but different failure patterns.
- **Reinforcement learning impacts**: Recognizing how RLHF training can induce language mixing and other hallucinations - needed to understand post-training optimization effects; quick check: RL-trained models show higher rates of certain hallucination types.

## Architecture Onboarding

**Component Map:**
Frontier LLMs (translation generation) -> LLM Judge Ensemble (candidate evaluation) -> Expert Validation (quality filtering) -> HalloMTBench Dataset

**Critical Path:**
Translation generation → hallucination detection → instance validation → benchmark construction → model evaluation

**Design Tradeoffs:**
- Multiple LLM judges vs. single human annotation: Balances scalability with accuracy
- Expert filtering vs. fully automated pipeline: Ensures quality but requires manual effort
- English-centric vs. multilingual approach: Easier to implement but may miss language-pair specific patterns
- Comprehensive taxonomy vs. simplified categories: Provides diagnostic detail but increases complexity

**Failure Signatures:**
- Low inter-annotator agreement indicating ambiguous hallucination definitions
- Benchmark overfitting if models learn to exploit specific evaluation patterns
- Limited language coverage missing important translation scenarios
- Selection bias toward hallucinations detectable by LLM judges

**First 3 Experiments:**
1. Test whether adding explicit instruction prompts reduces Instruction Detachment hallucinations
2. Evaluate if source text length thresholds correlate with hallucination frequency
3. Compare hallucination patterns between RL-trained and non-RL-trained model variants

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark construction may introduce selection bias toward hallucinations detectable by LLM judges
- Manual annotation agreement (93.68-100%) was computed on a subset (1,405 instances) of the full dataset
- Focus on English-to-X directions may not capture hallucination patterns in other translation scenarios

## Confidence
- **High Confidence**: The existence of distinct hallucination patterns (Instruction vs Source Detachment) and their correlation with model scale and linguistic factors; the benchmark's practical utility for evaluating multilingual translation quality.
- **Medium Confidence**: The generalizability of the taxonomy across different language families and translation scenarios; the completeness of the identified hallucination triggers.
- **Medium Confidence**: The specific thresholds for what constitutes "high-quality" hallucination instances, given the subjective nature of some judgments.

## Next Checks
1. **Cross-annotator validation**: Conduct a broader inter-annotator agreement study across the full dataset with multiple annotator pools to verify the 93.68-100% agreement rate is representative.

2. **Prompting strategy analysis**: Systematically test how different prompt formulations (few-shot examples, explicit instructions) affect hallucination rates across the benchmark to identify mitigation strategies.

3. **Bidirectional and multilingual-to-multilingual testing**: Extend evaluation to include non-English source languages and multilingual-to-multilingual translation scenarios to assess taxonomy generalizability.