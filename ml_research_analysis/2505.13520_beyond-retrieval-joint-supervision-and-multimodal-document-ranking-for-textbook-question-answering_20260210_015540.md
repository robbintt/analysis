---
ver: rpa2
title: 'Beyond Retrieval: Joint Supervision and Multimodal Document Ranking for Textbook
  Question Answering'
arxiv_id: '2505.13520'
source_url: https://arxiv.org/abs/2505.13520
tags:
- retrieval
- questions
- document
- question
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of improving retrieval quality\
  \ in multimodal textbook question answering, where relevant documents are often\
  \ long, complex, and visually rich. It introduces JETRTQA, a retriever\u2013generator\
  \ architecture that enhances semantic representations through joint training using\
  \ both pairwise ranking and weak supervision derived from answer logits."
---

# Beyond Retrieval: Joint Supervision and Multimodal Document Ranking for Textbook Question Answering

## Quick Facts
- arXiv ID: 2505.13520
- Source URL: https://arxiv.org/abs/2505.13520
- Reference count: 28
- Primary result: JETRTQA achieves 2.4% validation and 11.1% test accuracy improvement over prior methods on CK12-QA

## Executive Summary
This paper addresses the challenge of improving retrieval quality in multimodal textbook question answering, where relevant documents are often long, complex, and visually rich. It introduces JETRTQA, a retriever–generator architecture that enhances semantic representations through joint training using both pairwise ranking and weak supervision derived from answer logits. Unlike standard rerankers, JETRTQA is tailored to educational contexts and processes text and images jointly, enabling it to handle diagram-based questions effectively. Experimental results on the CK12-QA dataset show JETRTQA achieving a 2.4% improvement in validation accuracy and 11.1% on the test set over prior methods. Ablation studies confirm the importance of adaptive context selection, with diagram questions performing best with image-only retrieval, while text questions benefit from multiple relevant passages.

## Method Summary
JETRTQA combines a multimodal retriever with a generator in a joint training framework. The retriever uses ImageBind embeddings (1024-dim) refined by a 3-layer embedding enhancer network, trained with pairwise ranking loss and weak supervision from a frozen generator's logits. The generator is a Llama-3.2-Vision-11B model fine-tuned with LoRA for answer classification. During training, precomputed logits provide weak supervision signals, while the ranking loss compares document pairs for relative relevance. The system applies asymmetric context selection: diagram questions receive 0 text passages + 1 image, while non-diagram questions receive 3–6 text passages. The architecture is trained on the CK12-QA dataset using AdamW optimizer with learning rate 0.001 and batch size 16.

## Key Results
- JETRTQA achieves 80.20% validation accuracy and 77.07% test accuracy on CK12-QA
- 2.4% validation and 11.1% test accuracy improvement over prior methods
- Diagram questions show peak accuracy (78.35% val, 75.13% test) with 0P\1Image context vs. 74.47% and 73.24% with 3P\3Images
- Ablation confirms asymmetric context selection is critical for performance

## Why This Works (Mechanism)

### Mechanism 1: Pairwise Ranking Loss
Pairwise ranking loss improves document discrimination by learning relative relevance rather than absolute scoring. The model compares document pairs for each query, using a contrastive logit difference to prioritize documents that reduce generator loss. Relative relevance signals are more learnable than absolute relevance scores in educational contexts where documents share surface-level similarity. When relevant documents are lexically similar to irrelevant ones, pairwise comparison may not help.

### Mechanism 2: Weak Supervision from Generator Logits
Precomputed generator logits provide weak but effective supervision for retriever training without requiring full answer generation. The frozen Llama 3.2-Vision generator produces token-level logits for answer choices, which become weak supervision signals via cross-entropy loss. A fixed generator's confidence signals correlate sufficiently with document relevance for the downstream task. If the frozen generator is systematically miscalibrated on certain question types, its logits will propagate errors to the retriever.

### Mechanism 3: Modality-Aware Context Selection
Modality-aware context selection prevents cross-modal interference, particularly for diagram-based questions. Textual context harms visual reasoning (−9.35 to −10.44 accuracy points for DMC questions with full context). Visual reasoning requires undistracted attention to image features, and textual context introduces noise. When diagrams contain embedded text labels critical to answering, pure image-only context may miss textual annotations.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: JETRTQA is built on RAG architecture; understanding the retriever-generator pipeline is prerequisite to grasping joint training.
  - Quick check question: Can you explain why a retriever's relevance scoring might not align with what a generator needs for accurate answer production?

- Concept: **Contrastive and Pairwise Loss Functions**
  - Why needed here: The ranking loss Lrank uses contrastive logits between document pairs; understanding this is essential to implementing the training loop.
  - Quick check question: Given two documents with cosine similarities 0.65 and 0.60 to a query, how would a pairwise loss treat them differently than a pointwise loss?

- Concept: **Multimodal Embedding Spaces**
  - Why needed here: ImageBind projects text and images into a shared 1024-dimensional space; the embedding enhancer operates on this unified representation.
  - Quick check question: Why might a shared embedding space fail to capture task-specific relevance, necessitating the embedding enhancer module?

## Architecture Onboarding

- Component map:
  1. Multimodal RAG Retrieval (KDB.AI vector store) → Initial candidate retrieval via cosine similarity
  2. ImageBind Encoder → Produces 1024-dim embeddings for queries and documents
  3. Embedding Enhancer (3-layer feedforward: 1024→256→512→1024 with ReLU) → Refines embeddings for task-specific ranking
  4. Frozen Generator (Llama 3.2-Vision-11B + LoRA adapter) → Precomputed logits for weak supervision
  5. Joint Loss → Ltotal = Lrank + Lgen; only embedding enhancer parameters update

- Critical path: Query → ImageBind embedding → Embedding enhancer → Cosine similarity scoring → Top-k retrieval → Context selection (question-type-aware) → Generator inference
  - During training: Lrank uses pairwise document comparison; Lgen uses precomputed logits (not live generation)

- Design tradeoffs:
  - Precomputed logits reduce training cost but freeze generator knowledge (cannot adapt to retriever improvements)
  - ImageBind's 1024-dim embeddings capture rich semantics but require the enhancer network to project through bottleneck (256-dim)
  - Question-type-aware context selection improves accuracy but requires accurate classification

- Failure signatures:
  - High recall but low precision: Initial retrieval is too permissive; check cosine similarity threshold
  - DMC accuracy degradation with text context: Context selection policy not applied correctly
  - Generator logits uninformative: LoRA fine-tuning may be insufficient; verify generator achieves reasonable standalone accuracy first

- First 3 experiments:
  1. Reproduce baseline retriever: Implement ImageBind + cosine similarity retrieval on CK12-QA validation set; measure MRR, MAP, nDCG@10 against reported MRHF baseline (Table IV).
  2. Ablate weak supervision: Train embedding enhancer with Lrank only (no Lgen); compare against full Ltotal to quantify generator logit contribution.
  3. Context policy validation: Run inference with forced 3P\3Images vs. 0P\1Image for diagram questions; verify the −9 to −10 point DMC accuracy gap (Table III).

## Open Questions the Paper Calls Out

### Open Question 1
Does fully differentiable end-to-end joint training of the retriever and generator outperform the current frozen-generator approach?
The current JETRTQA framework freezes the generator and uses precomputed logits to reduce computational costs, preventing gradient flow from the generator back to the retriever. A comparative study on CK12-QA measuring accuracy differences between the current frozen-generator setup and a fully end-to-end trained architecture would resolve this.

### Open Question 2
Can incorporating structured external knowledge into the generation process reduce hallucinations and improve reasoning depth?
The current model uses a standard RAG framework which may struggle with factual consistency if the retrieved context is incomplete or noisy. Evaluating a knowledge-augmented variant of JETRTQA on factual consistency metrics and complex reasoning benchmarks within the CK12-QA dataset would provide evidence.

### Open Question 3
Does implementing a question-type classifier (factual, inferential, conceptual) to dynamically adjust retrieval strategies improve performance on specific fact-retrieval tasks?
The case study of NDQ 010371 revealed the model failed to retrieve specific facts (e.g., "Dolly the sheep") because semantic similarity matched general biology concepts rather than the specific answer. An ablation study testing a hybrid retrieval system that switches between dense semantic search and sparse keyword matching based on the predicted question type would resolve this.

## Limitations
- No code or checkpoint URLs provided; exact LoRA adapter configuration and training epochs unspecified
- Weighting λ for joint loss and negative sampling procedure in pairwise ranking not disclosed
- Frozen generator prevents adaptation to retriever improvements
- Question-type classification accuracy affects context selection effectiveness

## Confidence
- High confidence: Pairwise ranking improving document discrimination through relative relevance signals
- Medium confidence: Weak supervision approach using precomputed generator logits
- Medium confidence: Modality-aware context selection's effectiveness

## Next Checks
1. Reproduce baseline retriever: Implement ImageBind + cosine similarity retrieval on CK12-QA validation set; measure MRR, MAP, nDCG@10 against reported MRHF baseline (Table IV).
2. Ablate weak supervision: Train embedding enhancer with Lrank only (no Lgen); compare against full Ltotal to quantify generator logit contribution.
3. Context policy validation: Run inference with forced 3P\3Images vs. 0P\1Image for diagram questions; verify the −9 to −10 point DMC accuracy gap (Table III).