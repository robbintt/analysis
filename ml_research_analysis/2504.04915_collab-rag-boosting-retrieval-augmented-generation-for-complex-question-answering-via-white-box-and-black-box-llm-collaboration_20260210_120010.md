---
ver: rpa2
title: 'Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering
  via White-Box and Black-Box LLM Collaboration'
arxiv_id: '2504.04915'
source_url: https://arxiv.org/abs/2504.04915
tags:
- question
- collab-rag
- retrieval
- language
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Collab-RAG enhances black-box LLMs for complex question answering
  by introducing a collaborative framework between a white-box small language model
  (SLM) and a black-box LLM. The SLM decomposes complex queries into simpler sub-questions
  to improve retrieval accuracy, while the black-box LLM generates intermediate answers
  and synthesizes the final response.
---

# Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration

## Quick Facts
- arXiv ID: 2504.04915
- Source URL: https://arxiv.org/abs/2504.04915
- Reference count: 17
- Primary result: 3B SLM outperforms 32B frozen LLM in question decomposition for multi-hop QA

## Executive Summary
Collab-RAG addresses the challenge of complex question answering by introducing a collaborative framework between a white-box small language model (SLM) and a black-box large language model (LLM). The framework leverages the white-box SLM to decompose complex queries into simpler sub-questions, improving retrieval accuracy, while the black-box LLM generates intermediate answers and synthesizes the final response. Through iterative preference optimization using feedback from the black-box LLM, the SLM's decomposition capability is enhanced without requiring costly human annotations or distillation from frontier LLMs.

## Method Summary
Collab-RAG introduces a dual-model collaborative framework where a white-box SLM (Llama-3-8B) works in tandem with a black-box LLM. The SLM first decomposes complex questions into simpler sub-questions, which are then used for targeted retrieval. The black-box LLM generates intermediate answers from retrieved passages and synthesizes the final response. The framework employs iterative preference optimization, where the black-box LLM provides feedback on the SLM's decomposition attempts, allowing the SLM to improve its decomposition capability without human annotations. This approach aims to combine the transparency and controllability of white-box models with the superior reasoning capabilities of black-box LLMs.

## Key Results
- Collab-RAG outperforms black-box-only and SLM fine-tuning baselines by 1.8%-14.2% on average across five multi-hop QA datasets
- A fine-tuned 3B SLM surpasses a frozen 32B LLM in question decomposition tasks
- The framework demonstrates consistent improvements without requiring human annotations or distillation from frontier LLMs

## Why This Works (Mechanism)
The framework leverages the complementary strengths of white-box and black-box models: the white-box SLM provides transparent, controllable decomposition while the black-box LLM offers superior reasoning and synthesis capabilities. By iteratively refining the SLM's decomposition through preference optimization using the black-box LLM's feedback, the system achieves better retrieval accuracy for complex queries without the need for expensive human annotations.

## Foundational Learning
- **Question Decomposition**: Breaking complex queries into simpler sub-questions to improve retrieval accuracy - needed to handle multi-hop reasoning tasks, quick check: verify decomposition quality on simpler questions first
- **Retrieval-Augmented Generation**: Using retrieved documents to inform LLM responses - needed for grounding answers in factual information, quick check: compare retrieval quality with and without decomposition
- **Preference Optimization**: Using model-generated feedback to improve model performance - needed to avoid costly human annotations, quick check: test optimization stability across different feedback sources
- **Multi-hop QA**: Questions requiring reasoning over multiple pieces of information - needed to demonstrate effectiveness on complex reasoning tasks, quick check: verify single-hop baselines work as expected
- **White-box vs Black-box Models**: Trade-offs between model transparency and reasoning capability - needed to justify the collaborative approach, quick check: analyze where each model type performs better
- **Iterative Refinement**: Gradually improving model capabilities through repeated feedback loops - needed for efficient SLM improvement, quick check: monitor convergence behavior of the optimization process

## Architecture Onboarding

**Component Map**: SLM Decomposition -> Retrieval -> Black-Box LLM Intermediate Generation -> Answer Synthesis -> Preference Optimization Feedback -> SLM Update

**Critical Path**: Complex Question → SLM Decomposition → Targeted Retrieval → Black-Box LLM Intermediate Answers → Final Synthesis

**Design Tradeoffs**: The framework trades increased inference complexity (two models instead of one) for improved accuracy on complex questions, while avoiding the need for human annotations or distillation from larger models.

**Failure Signatures**: 
- Poor decomposition quality leads to irrelevant retrieval and cascade failures
- Black-box LLM feedback may be inconsistent or biased
- Optimization may not converge if feedback quality is low
- Computational overhead may outweigh benefits for simpler questions

**First 3 Experiments to Run**:
1. Ablation study comparing Collab-RAG with and without the SLM decomposition step on multi-hop QA datasets
2. Controlled test of preference optimization effectiveness by comparing SLM performance before and after optimization cycles
3. Single-hop QA evaluation to verify that the framework doesn't degrade performance on simpler questions

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on a single white-box SLM without ablation studies on SLM size or architecture
- Evaluation limited to multi-hop QA datasets, restricting generalizability claims
- Computational overhead of iterative preference optimization and dual-model inference not thoroughly analyzed
- Preference optimization may inherit biases from black-box LLM's intermediate answers

## Confidence
- High confidence in core empirical findings showing consistent performance improvements over baselines on tested multi-hop QA datasets
- Medium confidence in efficiency claims due to lack of detailed computational overhead analysis
- Low confidence in generalizability to non-QA reasoning tasks or different domain distributions

## Next Checks
1. Conduct ablation studies varying the white-box SLM size (e.g., 3B, 7B, 13B) to determine minimum effective model size and assess scalability

2. Test the framework on non-QA complex reasoning tasks (e.g., mathematical problem-solving, multi-step planning) to evaluate generalizability beyond current scope

3. Measure and compare end-to-end inference latency and computational costs between Collab-RAG and black-box-only approaches to provide complete efficiency analysis