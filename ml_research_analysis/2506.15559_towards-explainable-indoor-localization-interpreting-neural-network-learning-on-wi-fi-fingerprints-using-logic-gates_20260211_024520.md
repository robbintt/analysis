---
ver: rpa2
title: 'Towards Explainable Indoor Localization: Interpreting Neural Network Learning
  on Wi-Fi Fingerprints Using Logic Gates'
arxiv_id: '2506.15559'
source_url: https://arxiv.org/abs/2506.15559
tags:
- localization
- lognet
- indoor
- temporal
- logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of interpretability in deep learning
  (DL)-based indoor localization systems, which limits understanding of how models
  make predictions and respond to temporal variations in Wi-Fi signal data. The authors
  propose LogNet, a logic gate-based framework that interprets and enhances DL models
  by identifying the most influential access points (APs) for each reference point
  (RP) and revealing how environmental noise affects localization decisions.
---

# Towards Explainable Indoor Localization: Interpreting Neural Network Learning on Wi-Fi Fingerprints Using Logic Gates

## Quick Facts
- arXiv ID: 2506.15559
- Source URL: https://arxiv.org/abs/2506.15559
- Reference count: 22
- This paper proposes LogNet, a logic gate-based framework for interpretable indoor localization that achieves up to 1.1× to 2.8× lower localization error, 3.4× to 43.3× smaller model size, and 1.5× to 3.6× lower latency compared to prior deep learning models.

## Executive Summary
This paper addresses the critical lack of interpretability in deep learning-based indoor localization systems by proposing LogNet, a logic gate-based framework that interprets and enhances DL models. The framework identifies the most influential access points for each reference point and reveals how environmental noise affects localization decisions. By using binary logic gates to create transparent decision rules, LogNet enables traceability of critical APs, improves robustness to temporal variations, and supports explainable indoor localization in dynamic environments.

## Method Summary
LogNet operates by normalizing Wi-Fi RSS fingerprints to [0,1], binarizing them with a fixed threshold (φ=0.5), and applying pairwise logic gate operations across multiple layers to create compact latent representations. Each layer halves dimensionality while preserving interpretable relationships between access points. Only the final Softmax classifier is trained (150 epochs, Adam optimizer), while logic layers remain fixed transformations. The framework is evaluated across real-world building floorplans over two years, demonstrating significant improvements in localization accuracy, model size, and latency compared to standard deep learning approaches.

## Key Results
- LogNet achieves 1.1× to 2.8× lower mean localization error compared to DNN-DownSample baseline
- Model size reduction of 3.4× to 43.3× while maintaining or improving accuracy
- Latency improvements of 1.5× to 3.6× with smaller computational footprint
- Superior temporal robustness, showing flatter error curves across 10 consecutive CIs compared to DNN baselines

## Why This Works (Mechanism)

### Mechanism 1: Binary Threshold-Based Noise Filtering
RSS values are normalized to [0,1] then binarized using threshold φ=0.5, filtering non-Euclidean temporal noise by treating small variations as irrelevant. This discards minor fluctuations that characterize non-uniform temporal noise while preserving meaningful AP state changes.

### Mechanism 2: Logic Gates as Interpretable Pairwise AP Relationship Encoders
Each gate type (AND, OR, NAND, NOR, XOR, XNOR) captures specific spatial relationships between APs, compressing two binary inputs into one output while preserving interpretable semantics about co-occurrence, mutual exclusivity, or dead zones.

### Mechanism 3: Hierarchical Compression with Bit-Level Traceability
Multi-layer logic gate networks create compact latent representations where each bit can be traced back to original APs through deterministic gate paths, unlike DNN weight matrices. This enables AP-level influence analysis and failure diagnosis.

## Foundational Learning

- **Wi-Fi RSS Fingerprinting**: Understanding RSS = signal strength, APs = access points, RPs = reference locations is prerequisite. Quick check: Can you explain why the same location yields different RSS values at different times?

- **Euclidean vs Non-Euclidean Noise Distributions**: DNNs assume uniform noise while real noise is per-AP specific. Quick check: If AP:1 fluctuates by ±2dB and AP:2 by ±8dB over time, is this Euclidean or non-Euclidean noise?

- **Binary Logic Gate Truth Tables**: Understanding AND/OR/NAND/NOR/XOR/XNOR outputs for all input combinations is required. Quick check: What does NOR(1,0) output? What spatial pattern does this detect?

## Architecture Onboarding

- **Component map**: Raw RSS Vector (N APs) -> Normalization [0,1] -> Binarization (threshold φ=0.5) -> Layer 1: Pairwise Gate Operations -> Layer 2: Pairwise Gate Operations -> ... -> Latent Space (L_latent) -> Softmax Classifier -> RP Class

- **Critical path**: The threshold φ and gate type G are the only architectural hyperparameters affecting the logic layers (Softmax is standard). Training only updates Softmax weights; logic layers are fixed transformations.

- **Design tradeoffs**: NOR gate performed best (2.75m error) vs XOR worst (4.43m); more layers = smaller model but slightly higher error; fixed threshold at 0.5 without exploration of alternatives.

- **Failure signatures**: Error increases steadily from CI:2 onward indicates temporal variation exceeding model's noise assumptions; indistinguishable RPs suggest gate type not capturing discriminative patterns; over-compression causes error spikes with added layers.

- **First 3 experiments**: 1) Reproduce CI:0 baseline: Train LogNet-NOR-1H on Building 1 CI:0 training split, expect ~2.75m mean error. 2) Temporal robustness test: Train on CI:0, test sequentially on CI:0 through CI:9, plot error curve. 3) Latent space visualization: Extract L_latent for all RPs, visualize as binary heatmap, identify distinguishing bit positions.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a heterogeneous logic gate architecture with different gate types within the same layer outperform homogeneous structures? The methodology restricts to single gate type per variant.

- **Open Question 2**: How does arbitrary AP ordering in the input fingerprint vector influence logic rule formation when the architecture pairs adjacent APs? Spatial correlations between non-adjacent APs may be missed.

- **Open Question 3**: To what extent does the fixed binarization threshold (φ=0.5) limit discriminative power for APs with consistently weak or strong average signal strengths? Adaptive thresholding might preserve useful variance.

- **Open Question 4**: How can interpretable failure modes be utilized to trigger automated model recalibration without full retraining? The paper identifies traceability but doesn't demonstrate automated adaptation pipelines.

## Limitations
- Fixed binarization threshold (φ=0.5) without sensitivity analysis or adaptive alternatives
- Assumes one gate type suffices for all layers and adjacent fingerprint elements are spatially related
- Evaluation metric conversion from predicted RP class to physical meters is not specified

## Confidence
- **High confidence**: Localization error improvements, model size reduction, and latency gains are well-supported by direct measurements
- **Medium confidence**: Interpretability claims demonstrated through specific case studies but not systematically validated
- **Low confidence**: Noise filtering mechanism's effectiveness depends heavily on fixed threshold assumption

## Next Checks
1. **Threshold Sensitivity Analysis**: Reproduce CI:0 baseline while varying φ from 0.3 to 0.7 in 0.1 increments, plot localization error vs. threshold
2. **Temporal Robustness Under Stress**: Train LogNet on CI:0, test on CI:5 with artificial RSS perturbations (±3dB, ±6dB, ±9dB per AP), measure error degradation
3. **Gate Type Ablation**: Implement LogNet variants mixing gate types across layers (e.g., Layer 1: NOR, Layer 2: AND, Layer 3: XOR), compare localization error to single-gate-type variants