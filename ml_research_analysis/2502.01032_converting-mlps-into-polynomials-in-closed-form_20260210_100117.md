---
ver: rpa2
title: Converting MLPs into Polynomials in Closed Form
arxiv_id: '2502.01032'
source_url: https://arxiv.org/abs/2502.01032
tags:
- linear
- quadratic
- gaussian
- approximants
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to analytically derive polynomial approximations
  of neural networks under the assumption of Gaussian inputs. The core contribution
  is a closed-form least-squares solution for linear and quadratic approximations
  of MLPs and GLUs, enabling mechanistic interpretability through eigendecomposition
  of the coefficients.
---

# Converting MLPs into Polynomials in Closed Form
## Quick Facts
- arXiv ID: 2502.01032
- Source URL: https://arxiv.org/abs/2502.01032
- Reference count: 29
- Method to analytically derive polynomial approximations of neural networks under Gaussian input assumption

## Executive Summary
This paper presents a method to convert MLPs into polynomial approximations in closed form, specifically for linear and quadratic cases. The approach leverages properties of jointly Gaussian variables to compute integrals efficiently, enabling mechanistic interpretability through eigendecomposition of coefficients. The method is applied to analyze distributional simplicity bias in neural networks and to generate adversarial attacks by projecting inputs onto the nullspace of the approximation.

## Method Summary
The paper introduces a closed-form least-squares solution for polynomial approximations of neural networks under the assumption of Gaussian inputs. By exploiting the properties of jointly Gaussian variables, the method efficiently computes required integrals to derive linear and quadratic approximations. The approach involves projecting network outputs onto polynomial bases and using least-squares fitting to obtain coefficient matrices. These coefficients can then be analyzed through eigendecomposition to understand feature learning dynamics and network behavior.

## Key Results
- Demonstrated a phase transition during MNIST training where linear approximation quality degrades while quadratic quality remains stable
- Quadratic approximations explain over 95% of variance in network outputs by training completion
- Showed that polynomial approximations can generate effective adversarial attacks by projecting inputs onto the nullspace of the approximation

## Why This Works (Mechanism)
The method works by exploiting the mathematical properties of jointly Gaussian variables and polynomial bases. When inputs are Gaussian, integrals involving products of Gaussian variables and polynomials can be computed in closed form, enabling efficient least-squares fitting. The linear and quadratic approximations capture increasingly complex relationships in the data, with the quadratic terms modeling pairwise interactions between features that linear terms cannot capture.

## Foundational Learning
- Gaussian integration: Needed to compute expectations of polynomial terms under Gaussian distributions. Quick check: Verify that E[x] = 0 and E[xÂ²] = 1 for standard normal variables.
- Least-squares approximation: Required for fitting polynomial bases to network outputs. Quick check: Confirm that the normal equations provide the optimal solution.
- Eigendecomposition: Used to analyze polynomial coefficients and understand feature learning. Quick check: Verify that symmetric matrices have real eigenvalues.

## Architecture Onboarding
- Component map: Input distribution -> Polynomial basis projection -> Least-squares fitting -> Coefficient matrix -> Eigendecomposition analysis
- Critical path: Gaussian input assumption -> Closed-form integral computation -> Polynomial coefficient derivation -> Mechanistic interpretability
- Design tradeoffs: Closed-form solution requires Gaussian assumption vs. numerical methods for general distributions
- Failure signatures: Poor approximation quality when input distribution deviates significantly from Gaussian
- First experiments:
  1. Verify polynomial approximation accuracy on simple synthetic datasets
  2. Test eigendecomposition analysis on known network architectures
  3. Compare approximation quality across different activation functions

## Open Questions the Paper Calls Out
None

## Limitations
- Method assumes jointly Gaussian input distributions, limiting applicability to scenarios where this assumption holds
- Focuses only on linear and quadratic approximations, potentially missing higher-order interactions
- Requires networks to be trained with Gaussian inputs for the closed-form solutions to be valid

## Confidence
- Mathematical derivations: High
- Mechanistic interpretability claims: Medium
- Distributional simplicity bias findings: Medium
- Adversarial attack effectiveness: Medium

## Next Checks
1. Test the approximation method on non-Gaussian input distributions to assess robustness and potential extension strategies
2. Compare the polynomial approximation-based adversarial attacks against state-of-the-art attack methods to establish practical effectiveness
3. Apply the method to larger networks and more complex datasets to evaluate scalability and real-world applicability