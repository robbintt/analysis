---
ver: rpa2
title: Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise
  Tables
arxiv_id: '2508.07186'
source_url: https://arxiv.org/abs/2508.07186
tags:
- data
- summarization
- structured
- prompt
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a multi-agent framework for generating enterprise
  data summaries that combine structured reasoning with LLM-based narrative generation.
  It addresses the challenge of translating multi-dimensional, hierarchical business
  data into accurate, context-aware reports.
---

# Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables

## Quick Facts
- arXiv ID: 2508.07186
- Source URL: https://arxiv.org/abs/2508.07186
- Reference count: 13
- Primary result: Multi-agent pipeline achieves 83% faithfulness, 4.4/5 relevance, and 60% coverage on enterprise data summarization

## Executive Summary
This paper introduces a multi-agent framework for generating enterprise data summaries that combine structured reasoning with LLM-based narrative generation. The approach uses a modular pipeline with dedicated agents for slicing data, computing variances, enriching context, and generating summaries via Amazon Nova Micro. Evaluation on a Kaggle retail dataset shows the framework outperforms flat LLM prompting and template-based NLG across faithfulness, relevance, and coverage metrics, demonstrating the value of decomposing summarization into specialized reasoning and generation stages.

## Method Summary
The framework implements a LangGraph DAG with four sequential agents: SliceAgent filters data by dimensions and time periods, VarianceAgent computes percent change deltas between two time points, ContextAgent enriches variance output with external metadata signals, and SummaryAgent generates natural language summaries using structured JSON prompts fed to Amazon Nova Micro. The pipeline is inference-only, requiring no training, and processes multi-dimensional retail sales data to produce business-ready narratives with quantified metric changes and contextual explanations.

## Key Results
- Achieves 83% faithfulness compared to 43% for flat LLM prompting and 69% for template NLG
- Scores 4.4/5 relevance versus 3.1/5 for flat prompting and 3.7/5 for templates
- Covers 60% of significant changes versus 0% for flat prompting and 20% for templates
- VarianceAgent ablation drops faithfulness from 83% to 71% and coverage from 60% to 25%
- ContextAgent ablation reduces relevance from 4.4 to 4.1 while maintaining faithfulness at 83%

## Why This Works (Mechanism)

### Mechanism 1
Decomposing summarization into specialized agents improves faithfulness by isolating structured computation from generative synthesis. The SliceAgent and VarianceAgent perform deterministic filtering and delta calculation before any LLM invocation, grounding subsequent generation in explicitly quantified changes rather than requiring the LLM to infer them from raw tables. Core assumption: LLMs are more reliable at narrating pre-computed deltas than at performing arithmetic and comparative reasoning over raw tabular inputs.

### Mechanism 2
Context enrichment improves relevance by supplying explanatory signals the LLM cannot infer from the table alone. The ContextAgent injects static metadata (seasonality, promotions, anomalies) into the prompt structure, allowing the LLM to attribute observed deltas to business events rather than generating generic trend descriptions. Core assumption: The metadata is accurate, timely, and semantically aligned with the dimensions in the data slice.

### Mechanism 3
Structured prompt formatting with explicit metric deltas improves coverage of significant changes compared to flat prompting. The prompt template surfaces current vs. previous values and delta percentages for each metric in a JSON-like structure, focusing LLM attention on pre-identified changes rather than requiring the model to discover them. Core assumption: The delta computation correctly identifies significant changes and the threshold for inclusion is appropriately calibrated.

## Foundational Learning

- **Concept: Directed Acyclic Graph (DAG) execution for agent orchestration**
  - Why needed here: The framework uses a LangGraph-style DAG where each agent is a node with explicit data flow. Understanding conditional transitions and state passing between nodes is essential for debugging and extending the pipeline.
  - Quick check question: Can you trace how the output of VarianceAgent flows into the prompt structure used by SummaryAgent?

- **Concept: Delta computation for time-series comparison**
  - Why needed here: The VarianceAgent computes percentage change using δ = (M_t2 - M_t1) / (M_t1 + ε). This is the grounding mechanism for all downstream narrative claims.
  - Quick check question: Given current revenue of 2,899.9 and previous of 7,999.9, what delta percentage would VarianceAgent compute?

- **Concept: Structured prompting with metric slots**
  - Why needed here: The SummaryAgent receives a JSON-formatted prompt with explicit slots for metrics, context, and instructions. Understanding how to design and populate these slots is critical for controlling output focus and faithfulness.
  - Quick check question: What happens to coverage if you add a new metric to the prompt structure but VarianceAgent does not compute its delta?

## Architecture Onboarding

- **Component map:** SliceAgent -> VarianceAgent -> ContextAgent -> SummaryAgent
- **Critical path:** 1) SliceAgent filters data; if slice is empty, skip or flag 2) VarianceAgent computes deltas; if no significant changes, downstream relevance may degrade 3) ContextAgent enriches; if metadata missing, relevance drops but faithfulness unaffected 4) SummaryAgent generates; output quality depends on all prior stages
- **Design tradeoffs:** Batch-mode vs. streaming (current is batch-only), Static vs. dynamic metadata (ContextAgent uses pre-defined metadata), Template vs. free-form generation (structured prompts improve faithfulness but may limit stylistic flexibility)
- **Failure signatures:** High fluency, low faithfulness (likely VarianceAgent issue), High faithfulness, low relevance (likely ContextAgent issue), Low coverage (prompt may not include all significant metrics)
- **First 3 experiments:** 1) Disable VarianceAgent on sample slice; verify faithfulness drops to ~71% and coverage to ~25% 2) Inject incorrect context metadata (wrong promotion flags); measure relevance degradation (expect drop from 4.4 to ~4.1) 3) Add new metric to prompt without updating VarianceAgent; confirm coverage does not improve and identify where pipeline breaks

## Open Questions the Paper Calls Out

- **Open Question 1:** Does a hybrid workflow incorporating Text2SQL agents for intermediate reasoning stages yield higher coverage of significant changes than the current direct structured prompting approach? Basis: Future work will expand comparisons to include Text2SQL agents. Why unresolved: Current framework relies on pre-computed deltas; not yet compared against systems that generate SQL to query data dynamically. Evidence needed: Comparative study measuring coverage and faithfulness between current pipeline and Text2SQL-integrated variant.

- **Open Question 2:** Can predictive modeling agents be integrated into the pipeline to simulate forward-looking scenarios without degrading the 83% faithfulness score achieved for historical data? Basis: Future work lists integrating predictive modeling agents as primary direction. Why unresolved: Current architecture is designed for historical variance calculation; unknown if LLM can maintain factual grounding when generating text based on probabilistic forecasts. Evidence needed: Implementation of ForecastAgent and evaluation using modified faithfulness metric for probabilistic claims.

- **Open Question 3:** Does incorporating post-generation consistency checking or citation alignment significantly reduce hallucinations in the final narrative output? Basis: Authors note occasional overgeneralizations and suggest validation as remedy. Why unresolved: Current pipeline lacks verification step after SummaryAgent generates text. Evidence needed: Ablation study adding post-generation validation agent and measuring change in Faithfulness metric and reduction in vague attributions.

## Limitations
- ContextAgent's metadata quality and alignment mechanisms are not rigorously specified, creating potential brittleness in real-world deployment
- LLM-as-judge evaluation introduces subjectivity that may not generalize across domains or metric definitions
- No validation of temporal generalization beyond the 2023 retail dataset
- Missing error handling details for edge cases (empty slices, division by zero, conflicting metadata)

## Confidence
- **High confidence** in faithfulness improvement (83% vs 43%) given clear pre-computation mechanism
- **Medium confidence** in relevance gains (4.4/5) due to LLM-as-judge subjectivity
- **Medium confidence** in coverage claims (60%) given threshold sensitivity and no baseline threshold specification

## Next Checks
1. Test context enrichment sensitivity by injecting controlled metadata errors and measuring relevance degradation across multiple datasets
2. Conduct temporal generalization test by evaluating the framework on data from different years with varying seasonal patterns
3. Implement post-generation validation layer to automatically detect and flag hallucinated metrics in LLM outputs