---
ver: rpa2
title: 'Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?'
arxiv_id: '2502.20914'
source_url: https://arxiv.org/abs/2502.20914
tags:
- circuit
- neural
- network
- circuits
- neuron
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work systematically examines whether mechanistic interpretability
  (MI) criteria guarantee unique explanations of a fixed neural network behavior.
  Drawing on identifiability concepts from statistics, it investigates whether MI
  strategies ensure that only one computational abstraction satisfies validity criteria.
---

# Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?

## Quick Facts
- **arXiv ID**: 2502.20914
- **Source URL**: https://arxiv.org/abs/2502.20914
- **Reference count**: 40
- **Key outcome**: Systematic examination reveals mechanistic interpretability criteria do not guarantee unique explanations of neural network behavior, with experiments showing multiple valid circuits, interpretations, and algorithms can explain the same behavior.

## Executive Summary
This paper challenges the assumption that mechanistic interpretability (MI) produces unique explanations of neural network behavior. By framing interpretability as an inference problem analogous to statistical identifiability, the authors systematically test whether MI strategies guarantee unique computational abstractions. Using small MLPs trained on Boolean functions and exhaustive enumeration methods, they demonstrate that multiple circuits can replicate behavior, multiple interpretations exist for a given circuit, and different algorithms can be causally aligned with the same network. The findings suggest current MI criteria are insufficient for ensuring explanation uniqueness and prompt consideration of alternative criteria based on predictive and manipulability standards.

## Method Summary
The paper employs exhaustive enumeration on small MLPs (2, k, k, n) trained on Boolean logic tasks to test identifiability. Two main strategies are examined: "where-then-what" (enumerating all subgraphs meeting sparsity thresholds, then searching for consistent logic gate interpretations) and "what-then-where" (enumerating Boolean formulas up to depth 3, then searching for neuron mappings achieving perfect Intervention Interchange Accuracy). The method involves training networks until MSE loss < n×10⁻³, enumerating circuits with sparsity > 0.3, recursively searching for logic gate mappings, and computing IIA through counterfactual interventions. This brute-force approach is feasible only for small networks (k=2-5) where all explanations can be exhaustively counted.

## Key Results
- Multiple distinct circuits (85 unique circuits found) can achieve perfect accuracy on XOR in a small MLP
- Multiple algorithms can achieve perfect IIA=1 alignment with the same network
- Less than 2% of trained networks contain exactly one valid minimal mapping
- The number of valid explanations grows combinatorially with network width

## Why This Works (Mechanism)

### Mechanism 1: Circuit Redundancy (Where-then-what)
- **Claim:** In overparameterized networks, behavioral replication does not imply a unique subgraph implementation
- **Mechanism:** Parameter redundancy allows multiple sparse subgraphs to function independently while producing identical outputs
- **Core assumption:** Neural networks are overparameterized with more degrees of freedom than necessary
- **Evidence anchors:** Multiple circuits found with perfect accuracy; 85 unique circuits in XOR example
- **Break condition:** Minimal networks with extreme sparsity constraints

### Mechanism 2: Algorithmic Equifinality (What-then-where)
- **Claim:** Perfect causal alignment (IIA = 1) does not guarantee unique explanation
- **Mechanism:** Different high-level causal graphs can be intervention-equivalent for specific input sets
- **Core assumption:** Intervention sets do not fully span functional differences between algorithms
- **Evidence anchors:** 159 perfect mappings (IIA=1) found for 2 tested candidates; multiple algorithms causally aligned
- **Break condition:** Exhaustive intervention sets or strictly linear, disentangled features

### Mechanism 3: Non-Uniqueness of Mapping Localization
- **Claim:** Fixed algorithms can be implemented in multiple neural subspaces
- **Mechanism:** Distributed representations allow variables to be encoded across multiple neurons
- **Core assumption:** Networks use distributed rather than localist encoding
- **Evidence anchors:** Single algorithm can align with different network subspaces; <2% of networks have unique mappings
- **Break condition:** Enforced locality or absence of polysemanticity

## Foundational Learning

- **Concept: Identifiability (Statistical)**
  - **Why needed here:** Frames interpretability as an inference problem where unique parameter determination is the goal
  - **Quick check question:** Given input-output data alone, can I uniquely determine the internal causal structure?

- **Concept: Causal Alignment / IIA**
  - **Why needed here:** Quantitative metric validating explanations through consistency under intervention
  - **Quick check question:** If I intervene on the explanation's concept, does the network's output change as expected?

- **Concept: Computational Abstraction**
  - **Why needed here:** Formally defines explanations as tuples (Circuit, Mapping) to diagnose non-identifiability sources
  - **Quick check question:** Does this explanation specify both the algorithm and neural subgraph with translation layer?

## Architecture Onboarding

- **Component map:** Binary inputs -> MLP (2, k, k, n) -> Boolean outputs
- **Critical path:**
  1. Train overparameterized MLP on Boolean function
  2. **Strategy 1:** Enumerate subgraphs → Filter by error → Enumerate logic gate mappings
  3. **Strategy 2:** Enumerate Boolean formulas → Search neuron mappings → Calculate IIA
  4. Count valid explanations (>1 expected)

- **Design tradeoffs:**
  - Exhaustive enumeration vs. scale: Method relies on small networks; heuristics needed for LLMs
  - Sparsity threshold: Filters explosion but may miss dense valid circuits

- **Failure signatures:**
  - "Interpretability Illusion": Perfect IIA assumed to be the mechanism when conflicting algorithms also achieve perfect IIA
  - Exploding Explanations: Combinatorial growth in valid explanations with width

- **First 3 experiments:**
  1. XOR Stress-Test: Verify multiple subgraphs achieve 0% error in 2-layer MLP
  2. IIA Collision: Hand-craft two different logic circuits, test if both achieve IIA=1
  3. Scaling Check: Vary width (k=2-5), plot valid explanations count to observe growth

## Open Questions the Paper Calls Out

- **Open Question 1:** Does non-identifiability persist in large-scale models (e.g., Transformers) trained on complex data?
  - **Basis in paper:** Section 5.4 states "why the problems would disappear at larger scales must be demonstrated"
  - **Why unresolved:** Experiments restricted to small MLPs on Boolean functions
  - **What evidence would resolve it:** Multiple conflicting but valid circuits in LLMs for fixed behavior

- **Open Question 2:** Can stricter criteria based on full causal abstraction or faithfulness guarantee identifiability?
  - **Basis in paper:** Section 5.2 suggests stricter criteria from causal abstraction or faithfulness
  - **Why unresolved:** Current criteria allow incompatible explanations; stricter criteria untested
  - **What evidence would resolve it:** Demonstration that faithfulness constraints eliminate all but one valid explanation

- **Open Question 3:** Is explanatory uniqueness necessary for practical interpretability goals?
  - **Basis in paper:** Authors ask "Is uniqueness necessary?" and discuss pragmatic stance in Section 5.1
  - **Why unresolved:** Paper establishes non-identifiability but doesn't address utility for safety/control
  - **What evidence would resolve it:** Studies showing whether non-unique explanations fail in downstream tasks like model editing

## Limitations
- Exhaustive enumeration method is computationally tractable only for small MLPs
- Analysis focuses on Boolean logic tasks, not fully capturing real-world neural network complexity
- Some experimental details (activation functions, hyperparameters) are underspecified

## Confidence
- **High confidence:** Existence of non-identifiable explanations for small Boolean circuits (exhaustive enumeration)
- **Medium confidence:** General applicability to larger models (extrapolated from small-scale results)
- **Medium confidence:** Proposed alternative criteria as viable solutions (conceptual rather than empirically validated)

## Next Checks
1. Replicate XOR stress-test on architecture (2, k, k, 1) with k=2,3,4,5 to verify exponential growth in valid explanations
2. Implement IIA collision testing on simple AND gate network to confirm different Boolean formulas can achieve IIA=1
3. Run circuit enumeration on NOR gate network to check if sparsity threshold (>0.3) consistently filters while preserving valid circuits