---
ver: rpa2
title: Exploring Single Domain Generalization of LiDAR-based Semantic Segmentation
  under Imperfect Labels
arxiv_id: '2510.09035'
source_url: https://arxiv.org/abs/2510.09035
tags:
- noise
- segmentation
- domain
- lidar
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of domain generalization for LiDAR-based
  semantic segmentation under imperfect (noisy) labels, a critical challenge for robust
  autonomous driving. The authors introduce the DGLSS-NL benchmark, which incorporates
  controlled symmetric label noise into standard LiDAR datasets and evaluates model
  performance across in-domain and cross-domain settings.
---

# Exploring Single Domain Generalization of LiDAR-based Semantic Segmentation under Imperfect Labels

## Quick Facts
- **arXiv ID:** 2510.09035
- **Source URL:** https://arxiv.org/abs/2510.09035
- **Reference count:** 34
- **Primary result:** Achieves 56.86% mIoU on SemanticKITTI, 42.28% on nuScenes, and 52.58% on SemanticPOSS under 10% symmetric noise with DuNe framework

## Executive Summary
This paper addresses the critical challenge of domain generalization for LiDAR-based semantic segmentation under imperfect (noisy) labels, essential for robust autonomous driving. The authors introduce the DGLSS-NL benchmark with controlled symmetric label noise injected into standard LiDAR datasets and evaluate performance across in-domain and cross-domain settings. They adapt three noisy-label learning strategies from image classification to 3D point clouds, revealing significant limitations in direct transfer. The proposed DuNe dual-view framework achieves state-of-the-art performance by combining strong/weak branch consistency with confidence-aware partial and negative label supervision.

## Method Summary
The method introduces a dual-view framework with asymmetric augmentation (strong vs. weak) where the strong view receives PolarMix augmentation and the weak view preserves structural fidelity. Both views undergo sparsity augmentation via RowDrop. The framework employs feature-level consistency losses (SIFC for sparsity-invariant features, SCC for semantic correlations) and confidence-aware partial/negative label supervision through the NPN loss. During inference, only the strong branch is used. The approach adapts noisy-label learning strategies from image classification (TCL, DISC, NPN) to 3D point clouds and demonstrates significant improvements over baseline methods in both in-domain and cross-domain generalization under label noise.

## Key Results
- DuNe achieves 56.86% mIoU on SemanticKITTI, 42.28% on nuScenes, and 52.58% on SemanticPOSS under 10% symmetric noise
- Significant improvement over baselines: TCL (19.59%), DISC (41.42%), and DGLSS+NPN (52.05%) on K→K task
- Maintains robustness across noise levels: 56.86% (10%), 52.05% (20%), and 30.71% (50%) mIoU on K→K
- Arithmetic Mean (AM) of 49.57% and Harmonic Mean (HM) of 48.50% across target domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dual-view framework with asymmetric augmentation enables noise-robust learning by enforcing consistency between differently perturbed representations.
- **Mechanism:** Strong view receives heavy PolarMix augmentation producing denser point clouds with richer training signals, while weak view preserves structural fidelity. Feature-level consistency between views acts as regularizer.
- **Core assumption:** Noisy labels affect augmented views inconsistently; clean structural features remain stable across views.
- **Evidence anchors:** [abstract] "DuNe, a dual-view framework with strong and weak branches that enforce feature-level consistency"; [Section IV-B] Equations 5-12 define dual-view generation; [Section V-C] Ablation shows K→K improves to 55.75% vs. 32.99% baseline at 10% noise.
- **Break condition:** If augmentation corrupts geometric structure so severely that both views share same noise-induced errors, consistency provides no signal.

### Mechanism 2
- **Claim:** Confidence-aware partial and negative label supervision mitigates overfitting to corrupted annotations by selectively trusting predictions.
- **Mechanism:** NPN constructs candidate label set from high-confidence predictions and complementary label set (all other classes). Negative Learning loss penalizes predicting complementary labels, while Partial Label Learning encourages predicting one of candidates.
- **Core assumption:** Model predictions accumulate meaningful statistics over training; confident predictions tend toward correct classes even under noise.
- **Evidence anchors:** [Section IV-C] Equation 16 defines L_NL penalizing complementary labels; [Section V-C] DGLSS + NPN achieves 52.05% vs. 32.99% baseline on K→K at 10% noise.
- **Break condition:** If noise ratio exceeds ~50%, prediction statistics become unreliable and candidate label sets may consistently exclude true classes.

### Mechanism 3
- **Claim:** Sparsity-invariant and semantically correlated consistency losses improve cross-domain generalization by learning domain-agnostic feature representations.
- **Mechanism:** SIFC aligns features between sparse and dense versions of same scan (L1 distance). SCC enforces stable inter-class prototype relationships across scans. Together, they prevent learning sparsity-specific or domain-specific shortcuts.
- **Core assumption:** Semantic relationships between classes are stable across domains; only sensor sparsity and appearance vary.
- **Evidence anchors:** [Section IV-C] Equations 13-14 define L_SIFC and L_SCC; [Table I] Clean-label DGLSS baseline achieves 42.28% on nuScenes vs. 58.06% in-domain.
- **Break condition:** If target domain has fundamentally different class distributions or novel semantic relationships, prototype-based consistency may harm rather than help.

## Foundational Learning

- **Concept: Symmetric vs. Asymmetric Label Noise**
  - **Why needed here:** Paper injects symmetric noise (random label flips with equal probability) rather than asymmetric (semantically biased flips like truck→bus). Understanding distinction critical because symmetric noise is controlled stress test; asymmetric noise better reflects real annotation errors but requires class confusion statistics.
  - **Quick check question:** If "car" point is relabeled as "vegetation" with probability 0.1 and as "truck" with probability 0.1, is this symmetric or asymmetric noise?

- **Concept: Partial Label Learning vs. Negative Learning**
  - **Why needed here:** NPN combines both. PLL assumes true label is within candidate set (disjunctive). NL assumes true label is not in complementary set. Paper uses NL to penalize unlikely classes without committing to single candidate.
  - **Quick check question:** If candidate set = {car, truck} and complementary set = {bicycle, pedestrian, vegetation}, what does NL loss penalize?

- **Concept: Domain Generalization vs. Domain Adaptation**
  - **Why needed here:** Work targets DG (no target data during training) not DA (target statistics available). Model must generalize to unseen sensors/environments without fine-tuning.
  - **Quick check question:** Can you use nuScenes validation set statistics during training under DGLSS-NL protocol?

## Architecture Onboarding

- **Component map:** Input Scan → PolarMix Augmentation → [Strong View (dense) + Weak View (sparse)] → Sparsity Aug (RowDrop) → MinkowskiEngine Encoder → F_ss, F_sa ← Concat → ŷ_s; F_ws, F_wa ← Concat → (ŷ_ws, ŷ_wa) → NPN Loss (candidate/complementary) → Consistency Losses (SIFC + SCC + FC)

- **Critical path:** During inference, only the strong branch is used. Weak branch and all consistency losses are disabled. Deployment pipeline only needs encoder + decoder + strong-view classifier.

- **Design tradeoffs:**
  - Strong vs. weak branch for candidate labels: At 10-20% noise, strong branch provides richer cues. At 50% noise, strong branch's corrupted labels dominate, so switch to weak branch (Table IV: 30.71% vs. 54.13% at 50% vs. 10% noise).
  - PolarMix vs. NPN alone: PolarMix improves cross-domain (K→N: 33.48% vs. 21.76%) but NPN improves in-domain robustness (K→K: 52.05% vs. 32.99%). Combine both for best results.

- **Failure signatures:**
  - TCL baseline underperforms (19.59% at 10% noise): Clustering-based clean sample selection is too expensive for large point clouds; stochastic augmentation breaks prototype stability.
  - DISC baseline moderate improvement (41.42% at 10% noise): Threshold-based clean/hard label distinction unstable across varying point counts.
  - mIoU collapse at >50% noise: Even DuNe drops to 52.37% (K→K) at 50% noise vs. 56.86% at 10%. Model cannot distinguish signal from noise.

- **First 3 experiments:**
  1. Sanity check: Train DGLSS baseline with 0% noise on SemanticKITTI, evaluate on K/N/P. Confirm match paper's 58.06/42.28/49.09% mIoU.
  2. Noise injection test: Add 10% symmetric noise. Expect baseline collapse to ~33% in-domain. Verify noise injection follows Equation 3 exactly.
  3. Ablation run: Enable only NPN loss (no PolarMix, no dual-view consistency). At 10% noise, target ~52% K→K. This isolates contribution of partial/negative label supervision.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do noise-robust strategies perform under asymmetric label noise in LiDAR segmentation?
- **Basis in paper:** [explicit] Section III-A states authors focus only on symmetric noise as "principled starting point" because asymmetric noise requires prior knowledge of class-level confusion statistics.
- **Why unresolved:** Real-world annotation errors are often semantically biased (e.g., "truck" → "bus"), whereas current benchmark utilizes only uniform random label flips.
- **What evidence would resolve it:** Experiments evaluating DuNe framework and baselines using class-conditional noise transitions on DGLSS-NL benchmark.

### Open Question 2
- **Question:** Is effectiveness of dual-view framework consistent across different LiDAR segmentation backbones (e.g., point-based or projection-based)?
- **Basis in paper:** [inferred] Experiments rely exclusively on ResNet-based MinkowskiEngine backbone (Section III-B), leaving interaction between noise-robust strategies and other representations unexplored.
- **Why unresolved:** Voxel-based sparsity handling differs significantly from point-based aggregation or 2D projection methods, potentially affecting noise resilience.
- **What evidence would resolve it:** Benchmark results replicating study on non-voxel architectures like PointNet++ or SqueezeSeg.

### Open Question 3
- **Question:** Does proposed framework maintain robustness in multi-source domain generalization settings?
- **Basis in paper:** [inferred] Title and scope restrict study to "Single Domain Generalization" (K→N/P), implying multi-source scenarios remain untested.
- **Why unresolved:** Models trained on single source often suffer from domain bias; unclear if dual-view consistency scales effectively when trained on diverse, noisy datasets simultaneously.
- **What evidence would resolve it:** Experiments training on combination of source datasets (e.g., SemanticKITTI + nuScenes) with injected noise and evaluating on held-out target.

## Limitations
- Performance claims rely heavily on controlled synthetic noise injection and carefully tuned loss hyperparameters
- Switch from strong to weak branch candidate labels at 50% noise is critical but not fully justified with limited empirical evidence
- PolarMix augmentation hyperparameters are unspecified, making exact replication challenging
- Cross-domain gains may be sensitive to specific dataset statistics rather than purely noise-robust learning

## Confidence
- **High:** Dual-view consistency mechanism effectiveness, baseline performance comparisons, in-domain mIoU improvements
- **Medium:** Cross-domain generalization claims, candidate label branch switching at high noise, PolarMix contribution magnitude
- **Low:** Exact PolarMix hyperparameters, loss weight tuning sensitivity, 50% noise regime robustness

## Next Checks
1. **Hyperparameter sensitivity:** Systematically vary μ, ν, λ in NPN loss; measure impact on 10% noise K→K performance to identify stable operating ranges
2. **Cross-domain ablation:** Train DuNe on nuScenes (noisy) and test on SemanticKITTI to verify symmetric domain transfer capability
3. **Real noisy labels:** Apply DuNe to SemanticKITTI test set with manually annotated noisy labels and compare to synthetic noise performance