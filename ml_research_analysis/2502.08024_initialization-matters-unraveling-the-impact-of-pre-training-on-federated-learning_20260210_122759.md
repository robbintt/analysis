---
ver: rpa2
title: 'Initialization Matters: Unraveling the Impact of Pre-Training on Federated
  Learning'
arxiv_id: '2502.08024'
source_url: https://arxiv.org/abs/2502.08024
tags:
- lemma
- learning
- follows
- equation
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of pre-trained initialization
  on the generalization performance of federated learning (FL) with heterogeneous
  data. The authors introduce a two-layer convolutional neural network model with
  a signal-noise decomposition and analyze the effects of data heterogeneity on the
  learning of aligned and misaligned filters.
---

# Initialization Matters: Unraveling the Impact of Pre-Training on Federated Learning

## Quick Facts
- **arXiv ID:** 2502.08024
- **Source URL:** https://arxiv.org/abs/2502.08024
- **Reference count:** 40
- **One-line primary result:** Pre-trained initialization significantly improves generalization in federated learning by reducing misaligned filters that are particularly vulnerable to data heterogeneity.

## Executive Summary
This paper investigates why pre-trained initialization improves generalization performance in federated learning with heterogeneous data. The authors develop a theoretical framework using a two-layer CNN model that decomposes weights into signal and noise components, identifying that data heterogeneity specifically degrades the learning of "misaligned" filters while leaving "aligned" filters relatively unaffected. They prove that starting with a pre-trained model reduces the number of misaligned filters at initialization, thereby mitigating the adverse effects of heterogeneity on generalization. The theory is validated through experiments on synthetic data and real-world datasets like CIFAR-10 and TinyImageNet.

## Method Summary
The paper uses FedAvg with local SGD steps on ResNet18 (real data) and a two-layer CNN (synthetic data). Data is partitioned across clients using Dirichlet distribution to control heterogeneity. The key innovation is replacing BatchNorm with GroupNorm in ResNet18 to prevent divergence in heterogeneous FL. Two initialization strategies are compared: random initialization versus pre-trained initialization (ImageNet for ResNet18). The "misaligned filter" metric is computed by comparing filter signs at different training rounds to identify filters that fail to learn the signal due to heterogeneity.

## Key Results
- Pre-trained initialization significantly reduces test error in non-IID federated learning settings compared to random initialization
- Data heterogeneity specifically affects the learning of misaligned filters, causing them to stagnate while aligned filters learn efficiently
- The percentage of misaligned filters can be empirically measured and correlates with generalization performance degradation
- Large local steps (τ) benefit pre-trained models but harm random initialization models in non-IID settings

## Why This Works (Mechanism)

### Mechanism 1: The Misaligned Filter Stagnation Effect
Data heterogeneity specifically degrades the signal learning of filters that are initialized with a negative correlation to the target signal ("misaligned"), while leaving noise memorization unaffected. In the two-layer CNN model, data heterogeneity (measured by h) causes the signal learning coefficient (Γ) for misaligned filters to depend on the heterogeneity parameter h and local steps τ. As heterogeneity increases (h → 0) and local steps increase, misaligned filters fail to rotate toward the signal, whereas aligned filters learn the signal efficiently regardless of heterogeneity.

### Mechanism 2: Initialization as a Filter Alignment Procedure
Pre-training improves generalization primarily by reducing the number of misaligned filters at the start of federated training, effectively converting a heterogeneous learning problem into a homogeneous one for the remaining filters. Pre-training on a related task aligns filters with a general signal μpre. If μpre is sufficiently close to the downstream signal μ, the majority of filters begin federated training in the "aligned" set (Aj), bypassing the stagnation effect.

### Mechanism 3: Signal-to-Noise Ratio (SNR) Scaling
Generalization performance is bounded by the ratio of signal learning to noise memorization, which deteriorates for misaligned filters under heterogeneity. The test error is bounded by the ratio Γj,r/ΣPjr,ki. For aligned filters, this ratio remains high (constant). For misaligned filters, the denominator (noise) grows with local steps τ, while the numerator (signal) shrinks with heterogeneity, causing the ratio to collapse.

## Foundational Learning

**Federated Averaging (FedAvg) with Local Steps (τ):** The paper analyzes how the number of local steps (τ) interacts with heterogeneity to harm misaligned filters, showing that more local steps exacerbate the alignment problem. *Quick check: How does increasing local steps (τ) affect the signal learning rate of a misaligned filter in a highly heterogeneous (h ≈ 0) setting?* (Answer: It effectively zero-outs the signal learning growth relative to noise).

**Filter/Channel Alignment (Dot Product Similarity):** The central theoretical contribution relies on classifying filters as "aligned" or "misaligned" based on the sign of the dot product between the initial weight and the signal vector (⟨w(0), μ⟩). *Quick check: Define the condition for a filter to be considered "misaligned" at initialization.* (Answer: ⟨w(0)j,r, jμ⟩ < 0).

**Signal-Noise Decomposition:** The paper proves that weights can be expressed as a linear combination of initial weights, the signal vector, and noise vectors. This decomposition is the basis for tracking signal vs. noise learning. *Quick check: In the decomposition w(t) = w(0) + ..., what do the coefficients Γ and P represent?* (Answer: Signal learning and Noise memorization respectively).

## Architecture Onboarding

**Component map:** Data Generator -> Model -> Initialization -> Aggregator -> Evaluation

**Critical path:**
1. Initialize: Load pre-trained weights or random weights
2. Distribute: Partition data via Dirichlet distribution (Control heterogeneity h via concentration parameter α)
3. Train Local: Perform τ steps of Gradient Descent
4. Aggregate: Average weights
5. Evaluate: Measure Misalignment (Equation 8) alongside Accuracy

**Design tradeoffs:**
- **Local Steps (τ):** The paper suggests starting with large τ to reduce communication but warns that large τ hurts misaligned filters in non-IID settings. An adaptive strategy (decreasing τ) is proposed.
- **Pre-training Source:** If μpre is far from μ, pre-training offers less alignment benefit.

**Failure signatures:**
- High Training, Low Test: Indicates overfitting to noise (P terms growing) while signal (Γ) stagnates
- Plateauing Signal Coefficients: In synthetic experiments, seeing Γ flatline for certain filters indicates they are misaligned and stuck

**First 3 experiments:**
1. Verify the Ratio: Replicate the synthetic experiment to plot Γ/P for aligned vs. misaligned filters under IID vs. Non-IID data
2. Measure Misalignment: Train a ResNet18 on CIFAR-10 with random initialization and calculate the "Empirical Measure of Misalignment" to verify ~25% of filters are misaligned
3. Ablation on τ: Test the interaction of local steps. Show that increasing τ in Non-IID setting increases test error for random initialization but not for pre-trained initialization

## Open Questions the Paper Calls Out

**Open Question 1:** Can the theoretical framework of signal-noise decomposition and filter alignment be extended to deep neural networks (e.g., ResNets)? The authors list extending the analysis to deeper and more practical neural networks as a primary avenue for future work.

**Open Question 2:** Does pre-trained initialization maintain its ability to reduce misaligned filters in multi-class classification tasks? The conclusion identifies incorporating multi-class classification with more than two labels as a necessary extension.

**Open Question 3:** How does pre-trained initialization interact with FL algorithms designed specifically for heterogeneity, such as Scaffold or FedProx? The authors propose it is an interesting direction to see how pre-training affects other federated algorithms that explicitly incorporate heterogeneity reducing mechanisms.

## Limitations

- The theoretical framework relies heavily on a simplified two-layer CNN model, and extension to deeper architectures assumes similar filter alignment behavior
- The GroupNorm modification for BatchNorm is presented as critical but the paper doesn't systematically analyze what happens when BatchNorm is used
- Empirical validation focuses on CIFAR-10 and TinyImageNet with 20 clients, which may not represent real-world FL deployments

## Confidence

**High Confidence:** The existence of pre-training benefits in FL under non-IID settings (verified by extensive prior work and this paper's experiments)

**Medium Confidence:** The specific mechanism of "misaligned filter stagnation" - while theoretically sound in the simplified model, the empirical verification on ResNet18 is indirect

**Medium Confidence:** The recommendation to use large initial local steps with pre-trained models - theoretically justified but not extensively validated across different architectures

## Next Checks

1. **Mechanism Isolation Test:** Run experiments on the two-layer CNN model isolating the signal-noise decomposition by measuring Γ and P coefficients over training to directly verify the misaligned filter stagnation effect

2. **Architectural Generalizability Test:** Test the pre-training benefit and misalignment mechanism on architectures between the two-layer CNN and ResNet18 (e.g., three-layer CNN, VGG) to validate if the filter alignment hypothesis scales consistently with depth

3. **Norm Configuration Test:** Compare GroupNorm vs. BatchNorm implementations in ResNet18 under the same experimental conditions to determine if the pre-training benefit is amplified or diminished by the normalization choice