---
ver: rpa2
title: 'MemShare: Memory Efficient Inference for Large Reasoning Models through KV
  Cache Reuse'
arxiv_id: '2507.21433'
source_url: https://arxiv.org/abs/2507.21433
tags:
- cache
- memshare
- attention
- reasoning
- blocks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of high memory overhead in Large
  Reasoning Models (LRMs) due to their tendency to generate lengthy chain-of-thought
  sequences. The authors observe that LRMs frequently produce highly similar intermediate
  reasoning steps, leading to similar KV cache states across layers.
---

# MemShare: Memory Efficient Inference for Large Reasoning Models through KV Cache Reuse

## Quick Facts
- **arXiv ID:** 2507.21433
- **Source URL:** https://arxiv.org/abs/2507.21433
- **Reference count:** 7
- **Primary result:** Up to 84.79% throughput improvement while maintaining model accuracy through KV cache reuse in Large Reasoning Models

## Executive Summary
MemShare addresses the high memory overhead in Large Reasoning Models (LRMs) caused by lengthy chain-of-thought sequences through intelligent KV cache reuse. The key insight is that LRMs frequently generate highly similar intermediate reasoning steps, leading to similar KV cache states that can be shared. MemShare implements a two-stage collaborative filtering algorithm that first uses lightweight lexical similarity comparisons to identify candidate reusable blocks, then verifies similarity through precise Euclidean distance measurements. By integrating with vLLM's PagedAttention system, MemShare enables zero-copy cache reuse that significantly reduces memory overhead while improving throughput by up to 84.79% without sacrificing accuracy.

## Method Summary
MemShare implements a collaborative filtering algorithm that identifies and reuses similar KV cache blocks to reduce memory overhead during LRM inference. The method operates in two stages: Stage 1 uses tokenizer-based bag-of-words encoding with cosine similarity to efficiently filter candidate blocks (O(K) complexity), while Stage 2 computes normalized Euclidean distance across all layers and attention heads for blocks passing Stage 1. When reusable blocks are identified, the PagedAttention block table is updated with reference counting, enabling zero-copy sharing of physical KV cache blocks across sequences. The approach maintains accuracy by bounding attention perturbations through theoretical guarantees while significantly improving throughput.

## Key Results
- Achieves up to **84.79% improvement in throughput** on reasoning benchmarks
- Maintains model accuracy within **<3% degradation** compared to Dense Attention baseline
- Outperforms existing KV cache management methods under comparable affected KV cache ratios
- Demonstrates effectiveness across multiple models (QwQ-32B, DeepSeek-R1-Distill-Qwen-32B) and benchmarks (MATH-500, AIME 2024/2025, GPQA Diamond)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reusing KV cache blocks with similar content causes bounded, minor perturbations to attention outputs.
- **Mechanism:** Theorem 1 proves that replacing a key vector k_j with k'_j where ||k_j - k'_j|| < ε causes attention score change bounded by ||q_t|| · ε, and replacing a value vector v_j with v'_j where ||v_j - v'_j|| < δ causes output perturbation bounded by δ due to softmax Lipschitz continuity.
- **Core assumption:** Similar lexical content in reasoning steps correlates with small Euclidean distances in KV cache vectors.
- **Evidence anchors:** [section: Motivation, Theorem 1 proof] Formal bounds on attention changes; [section: Figure 3] Heatmaps showing lexically similar blocks have lower Euclidean distances; related work (D2O, KVMerger) examines token-level similarity but not LRM reasoning patterns.
- **Break condition:** If lexically similar reasoning steps do NOT produce KV cache vectors with small Euclidean distance, accuracy guarantee fails.

### Mechanism 2
- **Claim:** A two-stage hierarchical filtering process efficiently identifies reusable KV cache blocks while keeping computational overhead low.
- **Mechanism:** Stage 1 uses tokenizer-based bag-of-words encoding with cosine similarity (CPU-offloaded, O(K) complexity) to filter candidates. Stage 2 computes normalized Euclidean distance across all layers and attention heads for blocks passing Stage 1, reducing pairwise comparison cost from O(n²) to O(K).
- **Core assumption:** Lexical similarity is a sufficient proxy to prune the candidate space before expensive block-level distance computation.
- **Evidence anchors:** [section: Collaborative Filtering Algorithm, Algorithm 1] Explicit two-stage process; [section: Figure 7] Tokenizer encoding + Euclidean distance achieves highest throughput (203.87 tokens/s) while maintaining 91.2% accuracy vs alternatives.
- **Break condition:** If tokenizer-based lexical matching fails to capture meaningful similarity, Stage 1 over-prunes and reuse opportunities are missed.

### Mechanism 3
- **Claim:** Zero-copy KV cache reuse through PagedAttention block table updates reduces memory overhead without GPU memory bandwidth penalty.
- **Mechanism:** MemShare integrates with vLLM's block manager. When reusable blocks are identified, the CPU-managed block table is updated with reference counting; physical blocks are shared across sequences without data copying.
- **Core assumption:** Block-level sharing granularity aligns with reasoning step boundaries; tokens within blocks preserve sequential order.
- **Evidence anchors:** [section: Paged Attention Adapted KV Sharing Mechanism, Figure 4 & 5] Block table update workflow with reference counting; [abstract] "enables zero copy cache reuse to significantly reduce memory overhead."
- **Break condition:** If block boundaries do not align with reasoning step delimiters, partial reuse may introduce larger perturbations than Theorem 1 predicts.

## Foundational Learning

- **Concept: KV Cache in Autoregressive Inference (Prefill vs. Decode)**
  - **Why needed here:** MemShare targets the memory-bound decode stage where each new token requires access to the full historical KV cache. Understanding prefill (compute-bound, parallel) vs. decode (memory-bound, sequential) clarifies why KV cache size directly limits throughput and batch size.
  - **Quick check question:** In the decode stage, why does generating one token require accessing all previous KV cache entries?

- **Concept: PagedAttention and Block-Level KV Management**
  - **Why needed here:** MemShare builds on vLLM's PagedAttention, which partitions KV cache into fixed-size blocks managed via a block table. Understanding this indirection layer is essential to see how zero-copy reuse works through reference counting and pointer updates.
  - **Quick check question:** How does PagedAttention enable non-contiguous physical storage while preserving sequential token order for attention computation?

- **Concept: Similarity Metrics (Cosine vs. Euclidean Distance)**
  - **Why needed here:** Stage 1 uses cosine similarity for angular alignment; Stage 2 uses Euclidean distance for magnitude-aware comparison. The paper argues Euclidean is critical because attention computation depends on absolute values, not just direction.
  - **Quick check question:** Why might two vectors have high cosine similarity but large Euclidean distance, and which matters more for attention output?

## Architecture Onboarding

- **Component map:** Evaluator (CPU) -> Scheduler -> Block Manager (CPU) -> Model Executor (GPU) -> Cache Engine
- **Critical path:**
  1. New reasoning step generated → tokenizer encodes step content
  2. Stage 1 cosine similarity compares against prior steps; candidates above threshold proceed
  3. Stage 2 computes Euclidean distance between candidate KV blocks and current block
  4. If distance below threshold, block table updated (reference count incremented, physical block shared)
  5. Decoder continues generation using shared block without GPU memory copy

- **Design tradeoffs:**
  - **Threshold selection (0.8 vs 0.9):** Higher thresholds (0.9) yield fewer false positives (better accuracy) but fewer reuse opportunities (lower throughput). Table 1 shows 0.9/0.8 threshold maintains 97.5% accuracy for QwQ-32B on MATH-500.
  - **Tokenizer vs. embedding encoding:** Tokenizer is faster (CPU-offloaded) but less precise; embedding layer is more accurate but reduces throughput by ~60% (Figure 7).
  - **Block size alignment:** Assumption: step boundaries align with block boundaries. Misalignment could cause partial-block reuse with unpredictable perturbation.

- **Failure signatures:**
  - **Accuracy degradation:** Random sharing baseline drops accuracy significantly (e.g., MATH-500 from 0.927 to 0.689 for DeepSeek-R1-Distill-Qwen-32B). Indicates similarity detection is critical.
  - **Throughput regression:** If Stage 2 distance computation is triggered too frequently (low Stage 1 threshold), CPU overhead may negate memory savings.
  - **Memory fragmentation:** The paper claims block-level operations avoid fragmentation, but aggressive sharing with high reference counts could complicate eviction decisions.

- **First 3 experiments:**
  1. **Baseline comparison:** Run Dense (no sharing), Random Sharing, and MemShare on MATH-500 with QwQ-32B; measure throughput (tokens/s) and accuracy. Confirm MemShare achieves claimed ~70% throughput gain with <3% accuracy loss.
  2. **Threshold sweep:** Test step/block threshold pairs (0.7/0.7, 0.8/0.8, 0.9/0.8, 0.9/0.9) on AIME 2024; plot accuracy vs. affected ratio to identify Pareto frontier.
  3. **Ablation on similarity method:** Compare tokenizer+Euclidean vs. Sentence-BERT+cosine vs. embedding-layer+Euclidean (Figure 7 setup); verify tokenizer+Euclidean provides best throughput-accuracy tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MemShare perform on general-purpose LLMs or tasks that do not exhibit the "redundant thinking" phenomenon characteristic of Large Reasoning Models?
- Basis: The paper's motivation and evaluation are exclusively centered on LRMs (DeepSeek-R1, QwQ) and mathematical/logic tasks where redundant intermediate steps are frequent.
- Why unresolved: The paper does not evaluate the method on standard base models or non-reasoning tasks (e.g., summarization, translation) where reasoning redundancy is likely lower, potentially negating memory savings while adding computational overhead.
- What evidence would resolve it: Benchmarks on standard datasets (e.g., CNN/DailyMail) using base models to measure the trade-off between the overhead of similarity checking and the lack of KV cache reuse opportunities.

### Open Question 2
- Question: Can the lexical similarity filter be improved to capture semantic equivalence without incurring the high latency of embedding models?
- Basis: The paper explicitly rejects Sentence-BERT in favor of a tokenizer-based approach, noting that while semantic embeddings capture context, they introduce substantial computational overhead.
- Why unresolved: The current tokenizer-based approach relies on exact lexical overlaps; it may fail to identify reusable blocks where reasoning steps are semantically identical but phrased differently, limiting the theoretical maximum of KV cache reuse.
- What evidence would resolve it: A hybrid approach or optimized semantic hashing mechanism that identifies semantically similar blocks (missed by the current method) without degrading throughput.

### Open Question 3
- Question: How does the Block-Level Distance Measurement overhead impact tail latency in online serving scenarios compared to offline batch throughput?
- Basis: The evaluation focuses on "offline batch inference" and throughput (tokens/s), but the "Stage 2" Euclidean distance calculation requires iterating over layers and heads for every candidate block.
- Why unresolved: While throughput improves in batch settings, the paper does not report the specific latency cost of the distance calculation algorithm on the critical path of decoding, which is crucial for interactive applications.
- What evidence would resolve it: Latency breakdowns (e.g., TTFT and TPOT) showing the computation time of Algorithm 1 Stage 2 relative to the decoding step time in an online serving framework.

## Limitations
- **Generalization uncertainty:** The method's effectiveness depends heavily on the presence of redundant reasoning patterns characteristic of LRMs, which may not exist in general-purpose LLM tasks.
- **Threshold sensitivity:** Performance critically depends on carefully tuned step/block thresholds that may not generalize across different models, reasoning tasks, or domains.
- **Implementation complexity:** The two-stage collaborative filtering algorithm introduces additional computational overhead that must be carefully managed to avoid negating memory savings.

## Confidence

**High Confidence:** The core observation that LRMs produce highly similar intermediate reasoning steps is well-supported by empirical evidence across multiple benchmarks. The theoretical framework for bounding attention perturbations (Theorem 1) is mathematically sound given its assumptions. The experimental methodology comparing MemShare against baselines on standard benchmarks (AIME, MATH-500, GPQA) is rigorous and reproducible.

**Medium Confidence:** The effectiveness of the two-stage collaborative filtering algorithm relies on assumptions about lexical similarity serving as a good proxy for KV cache similarity. While Figure 3 provides supporting evidence through heatmap visualizations, this relationship may not hold universally across different reasoning patterns or model architectures. The claimed throughput improvements (84.79% max) are impressive but may depend heavily on specific implementation optimizations and threshold tuning not fully characterized.

**Low Confidence:** The long-term stability and robustness of the method across diverse LRM architectures and reasoning domains remains uncertain. The paper does not extensively explore edge cases where reasoning steps might be lexically similar but semantically different, or where block alignment assumptions break down. The impact of aggressive KV cache sharing on memory fragmentation and eviction policies in production environments is not thoroughly investigated.

## Next Checks

**Check 1: Threshold Sensitivity Analysis Across Models**
Conduct a comprehensive sweep of step/block thresholds (0.6/0.6 to 0.95/0.95) across at least three different LRM architectures (not just QwQ-32B and DeepSeek-R1 variants) on multiple reasoning benchmarks. Measure both throughput and accuracy to identify the Pareto frontier for each model and establish guidelines for threshold selection in new deployments.

**Check 2: Semantic vs. Lexical Similarity Correlation Study**
Systematically generate reasoning steps that are semantically equivalent but lexically different (e.g., through paraphrasing or alternative solution paths) and measure their KV cache similarity. Quantify the false negative rate of the tokenizer-based lexical matching stage and determine whether more sophisticated semantic similarity methods would significantly improve reuse opportunities without prohibitive computational overhead.

**Check 3: Long-Running Stability Test**
Deploy MemShare in a production-like environment with continuous reasoning task generation over extended periods (24+ hours). Monitor for memory fragmentation, accuracy drift, and scheduler bottlenecks. Measure the actual memory savings versus theoretical predictions and identify any long-term degradation patterns or corner cases that emerge only under sustained operation.