---
ver: rpa2
title: Multi-level Mixture of Experts for Multimodal Entity Linking
arxiv_id: '2507.07108'
source_url: https://arxiv.org/abs/2507.07108
tags:
- entity
- mention
- multimodal
- textual
- mmoe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MMoE, a multi-level mixture-of-experts model
  for multimodal entity linking (MEL), addressing two key issues: mention ambiguity
  caused by brief textual contexts, and dynamic selection of modal content importance.
  The model enhances mentions with relevant Wikidata descriptions using LLMs, employs
  a multimodal feature extraction module, and uses switch mixture-of-experts (SMoE)
  mechanisms at both intra-modal and inter-modal levels to dynamically select important
  features.'
---

# Multi-level Mixture of Experts for Multimodal Entity Linking

## Quick Facts
- **arXiv ID**: 2507.07108
- **Source URL**: https://arxiv.org/abs/2507.07108
- **Reference count**: 40
- **Primary result**: MMoE achieves up to 1.70% improvement in MRR and Hits@1 over state-of-the-art baselines on three MEL benchmarks

## Executive Summary
This paper introduces MMoE, a multi-level mixture-of-experts model designed to address key challenges in multimodal entity linking (MEL), specifically mention ambiguity from brief textual contexts and dynamic selection of modality importance. The model enhances mentions with relevant Wikidata descriptions using LLMs, employs a multimodal feature extraction module, and uses switch mixture-of-experts (SMoE) mechanisms at both intra-modal and inter-modal levels to dynamically select important features. Extensive experiments on three benchmarks (WikiMEL, RichpediaMEL, WikiDiverse) demonstrate consistent performance improvements over state-of-the-art baselines, with MMoE achieving up to 1.70% improvement in MRR and Hits@1 metrics.

## Method Summary
MMoE addresses multimodal entity linking through a sophisticated multi-level architecture that enhances mentions with LLM-generated Wikidata descriptions and employs mixture-of-experts mechanisms at both intra-modal and inter-modal levels. The model first enriches mentions using GPT-4 to extract relevant Wikidata descriptions, then processes multimodal inputs through specialized encoders for each modality. The key innovation lies in the SMoE modules that dynamically select important features within each modality (intra-modal) and across modalities (inter-modal), allowing the model to adaptively weigh different information sources based on their relevance to the linking task. This approach specifically targets the challenge of brief textual contexts that create mention ambiguity while leveraging the complementary information available across modalities.

## Key Results
- MMoE achieves up to 1.70% improvement in MRR over state-of-the-art baselines
- MMoE achieves up to 1.70% improvement in Hits@1 metric
- Ablation studies confirm effectiveness of each component, particularly the LLM-enhanced descriptions and SMoE mechanisms

## Why This Works (Mechanism)
The multi-level mixture-of-experts architecture works by addressing two fundamental challenges in MEL: the ambiguity created by brief textual contexts and the need for dynamic modality selection. The LLM-enhanced descriptions provide richer context for mentions that would otherwise be ambiguous, while the SMoE mechanisms at both levels allow the model to selectively focus on the most relevant features. Intra-modal SMoE handles modality-specific feature selection (e.g., identifying the most salient visual features), while inter-modal SMoE determines how to weight different modalities relative to each other based on their relevance to the entity linking task. This hierarchical approach enables more nuanced and context-aware decision making compared to single-level or static weighting approaches.

## Foundational Learning

**Mixture of Experts (MoE)**: A neural network architecture that employs multiple specialized models (experts) with a gating mechanism to select which experts are used for each input. Why needed: Enables specialization of different components for different types of inputs while maintaining efficiency. Quick check: Verify that the gating mechanism properly routes inputs to appropriate experts based on learned patterns.

**Switch Mixture of Experts (SMoE)**: A variant of MoE where each token or feature is routed to exactly one expert rather than being a weighted combination. Why needed: Reduces computational overhead while maintaining specialization benefits. Quick check: Confirm that the routing decisions are stable and meaningful across similar inputs.

**Multimodal Feature Extraction**: The process of extracting and representing features from multiple input modalities (text, image, etc.) in a unified manner. Why needed: Different modalities contain complementary information that can improve entity linking accuracy. Quick check: Ensure feature representations from different modalities are properly aligned and comparable.

**Entity Linking**: The task of matching entity mentions in text to their corresponding entries in a knowledge base. Why needed: Forms the core problem that MMoE aims to solve more effectively than previous approaches. Quick check: Verify that the linking decisions are consistent with both textual and multimodal evidence.

## Architecture Onboarding

**Component Map**: Text Encoder -> Image Encoder -> LLM Description Generator -> Intra-modal SMoE -> Inter-modal SMoE -> Entity Classifier

**Critical Path**: The most time-critical components are the modality encoders and the inter-modal SMoE layer, as they must process all input modalities before the final entity classification decision can be made. The intra-modal SMoE layers process in parallel within each modality, while the inter-modal SMoE requires all intra-modal outputs to be ready.

**Design Tradeoffs**: The choice of 10 experts per modality represents a balance between model capacity and computational efficiency. More experts could provide better specialization but would increase latency and memory requirements. The SMoE approach trades some accuracy (compared to full MoE) for significant efficiency gains, making the model more practical for deployment.

**Failure Signatures**: Performance degradation may occur when modality encoders fail to extract meaningful features, when the LLM description generator produces irrelevant or incorrect descriptions, or when the SMoE routing mechanisms make poor decisions about which features to emphasize. The model may also struggle with entities that have very limited multimodal information available.

**First Experiments**:
1. Test the individual modality encoders on isolated inputs to verify they extract meaningful features
2. Validate the LLM description generation quality by comparing to human-curated descriptions
3. Evaluate the SMoE routing decisions by examining which experts are selected for different input types

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on LLM-generated descriptions may introduce inaccuracies or miss relevant context
- High computational overhead due to multi-level SMoE architecture with 10 experts per modality
- Experiments focus on Wikipedia-based benchmarks, limiting generalizability to other domains

## Confidence

High: The architectural innovations are well-described and performance improvements are statistically significant
Medium: LLM-enhanced descriptions effectiveness shown but lacks sensitivity analysis for different prompts/models
Low: Real-world applicability claims lack evaluation on live data streams or user-facing applications

## Next Checks
1. Conduct controlled study comparing LLM-generated descriptions versus human-curated descriptions across benchmarks
2. Perform computational profiling to measure inference time and memory requirements versus baselines
3. Test model robustness on out-of-domain datasets or synthetic perturbations with noise in one or more modalities