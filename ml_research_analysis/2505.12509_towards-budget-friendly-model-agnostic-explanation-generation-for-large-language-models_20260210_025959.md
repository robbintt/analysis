---
ver: rpa2
title: Towards Budget-Friendly Model-Agnostic Explanation Generation for Large Language
  Models
arxiv_id: '2505.12509'
source_url: https://arxiv.org/abs/2505.12509
tags:
- qwen
- gpt-4o
- explanations
- llama
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of generating
  model-agnostic explanations for large language models (LLMs) by introducing a budget-friendly
  proxy framework. The method leverages smaller, efficient models to approximate the
  decision boundaries of expensive LLMs, using a statistical screen-and-apply mechanism
  to ensure fidelity before deployment.
---

# Towards Budget-Friendly Model-Agnostic Explanation Generation for Large Language Models

## Quick Facts
- **arXiv ID:** 2505.12509
- **Source URL:** https://arxiv.org/abs/2505.12509
- **Reference count:** 40
- **One-line primary result:** Achieves >90% explanation fidelity with 88.2% cost reduction using proxy models

## Executive Summary
This paper addresses the prohibitive computational cost of generating model-agnostic explanations (LIME, Kernel SHAP) for large language models by introducing a budget-friendly proxy framework. The approach uses smaller, efficient models to approximate expensive LLM decision boundaries while maintaining explanation fidelity through statistical screening mechanisms. Experiments across 12 state-of-the-art LLMs and seven diverse tasks demonstrate that proxy explanations achieve over 90% fidelity while reducing costs by 88.2%, making model-agnostic interpretability economically viable for real-world applications.

## Method Summary
The framework employs a two-stage screening mechanism to ensure proxy model fidelity before generating explanations. Task-level screening uses sequential one-sided paired t-tests to verify if proxy explanations are statistically sufficient for a dataset (requiring 99% confidence and 90% fidelity threshold). Instance-level screening checks if proxy and target models agree on predictions before generating explanations. When screening passes, explanations are generated using the proxy model; otherwise, the expensive target model is used as fallback. The approach is evaluated on SST-2 sentiment analysis, MMLU multiple-choice questions, and Google Natural Questions text generation tasks.

## Key Results
- Achieves >90% fidelity preservation across all evaluated tasks and model pairs
- Reduces computational costs by 88.2% compared to using target models directly
- Demonstrates effectiveness in downstream optimization tasks including prompt compression and poisoned example removal
- Works across diverse LLM pairs including GPT-4o, DeepSeek V3, and various Qwen and Llama models

## Why This Works (Mechanism)
The approach leverages the observation that smaller models can often approximate the decision boundaries of larger models when they agree on predictions. The statistical screening mechanism ensures explanations are only generated when the proxy model's behavior is sufficiently similar to the target model, preventing fidelity degradation. By focusing explanation generation on instances where models agree, the framework maintains high fidelity while avoiding expensive computations on difficult cases that require the full model.

## Foundational Learning
- **Statistical screening with sequential t-tests** - needed to determine when proxy models are sufficiently faithful; quick check: verify confidence intervals stay above zero threshold
- **Model-agnostic interpretability methods (LIME/SHAP)** - needed to generate local explanations that approximate decision boundaries; quick check: ensure perturbation sampling produces diverse enough inputs
- **Task-level vs instance-level screening** - needed to balance computational efficiency with explanation quality; quick check: monitor agreement rates between proxy and target models
- **Cost reduction metrics** - needed to quantify practical benefits; quick check: calculate actual API/compute savings versus theoretical estimates
- **Fidelity measurement** - needed to evaluate explanation quality; quick check: compare local surrogate model accuracy against oracle predictions
- **Fallback mechanisms** - needed to handle cases where proxy models are insufficient; quick check: track frequency of expensive target model invocations

## Architecture Onboarding

**Component Map:**
Task-Level Screening -> Instance-Level Screening -> Explanation Generation (Proxy) OR Fallback (Target)

**Critical Path:**
Input → Instance Screening (f(x) == f'(x)?) → Task Screening (statistical test) → Generate Proxy Explanation OR Generate Target Explanation

**Design Tradeoffs:**
- Larger proxy models increase agreement rates but reduce cost savings
- Stricter fidelity thresholds improve quality but increase fallback frequency
- More perturbation samples improve explanation quality but increase computational cost
- Sequential testing enables early stopping but requires careful threshold calibration

**Failure Signatures:**
- Low proxy-target agreement rates indicate proxy model is too weak for the task
- Failed task-level screening suggests systematic fidelity issues requiring model upgrade
- High fallback rates negate cost benefits, indicating need for better proxy selection
- Poor downstream optimization performance may indicate explanation quality issues

**Three First Experiments:**
1. Verify instance-level screening by measuring proxy-target agreement rates across different model pairs
2. Test task-level screening with varying fidelity thresholds to observe trade-offs between confidence and cost
3. Compare explanation quality (fidelity scores) between proxy-generated and oracle-generated explanations on a validation set

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Effectiveness depends heavily on proxy model's ability to agree with target model on instance predictions
- Perturbation generation strategy for text using "default values" lacks explicit documentation
- Statistical screening methodology may be sensitive to early stopping criteria in sequential t-tests
- Generalizability to entirely new task domains or model architectures beyond tested cases remains uncertain

## Confidence

**High Confidence:** The reported 88.2% cost reduction and >90% fidelity preservation across multiple LLM pairs and tasks are supported by comprehensive experiments across 12 target models and seven datasets.

**Medium Confidence:** The downstream optimization applications (prompt compression, poisoned example removal) show promise but were demonstrated on fewer examples and may require task-specific tuning.

**Low Confidence:** The generalizability of the approach to entirely new task domains or model architectures beyond those tested remains uncertain without additional validation.

## Next Checks
1. **Cross-Domain Transfer:** Apply the framework to a task type not in the original seven (e.g., code generation or long-form summarization) to test generalizability of the screening mechanism.
2. **Scale Sensitivity Analysis:** Systematically vary proxy model sizes below 0.5B and above 14B parameters to map the precise relationship between model capacity and fidelity/cost trade-offs.
3. **Adversarial Robustness:** Evaluate whether adversarially crafted inputs that fool the proxy model but not the target model could compromise the screening mechanism's effectiveness.