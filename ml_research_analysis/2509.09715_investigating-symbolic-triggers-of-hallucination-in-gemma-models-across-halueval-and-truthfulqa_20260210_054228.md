---
ver: rpa2
title: Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval
  and TruthfulQA
arxiv_id: '2509.09715'
source_url: https://arxiv.org/abs/2509.09715
tags:
- symbolic
- hallucination
- across
- named
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates symbolic triggers of hallucination in
  Gemma models by systematically converting established datasets (HaluEval and TruthfulQA)
  into three task formats (QA, MCQ, and Odd-One-Out) to isolate five symbolic properties:
  modifiers, named entities, numbers, negation, and exceptions. The study evaluates
  Gemma-2-2B, 9B, and 27B models across these formats, revealing that hallucination
  rates remain substantially high across all model sizes, with modifiers (84.76-94.98%)
  and named entities (83.87-93.96%) being the most problematic symbolic properties.'
---

# Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA

## Quick Facts
- **arXiv ID:** 2509.09715
- **Source URL:** https://arxiv.org/abs/2509.09715
- **Reference count:** 26
- **Key outcome:** This paper investigates symbolic triggers of hallucination in Gemma models by systematically converting established datasets (HaluEval and TruthfulQA) into three task formats (QA, MCQ, and Odd-One-Out) to isolate five symbolic properties: modifiers, named entities, numbers, negation, and exceptions.

## Executive Summary
This study systematically identifies symbolic triggers that cause hallucinations in Gemma models by converting established datasets into three task formats and analyzing attention patterns. The research reveals that symbolic confusion—particularly around modifiers and named entities—remains a fundamental architectural challenge that scaling alone cannot resolve. Even the largest Gemma model shows persistent vulnerability to these triggers, especially in medium-length contexts. The work establishes that hallucination is not simply a capacity problem but a representational fragility inherent to transformer architectures.

## Method Summary
The study uses 100 samples each from HaluEval and TruthfulQA datasets, converting them into QA, MCQ, and Odd-One-Out formats to create 600 test instances. Each sample is annotated for five symbolic properties (modifiers, named entities, numbers, negation, exceptions) using NER and linguistic rules. The research evaluates Gemma-2-2B, 9B, and 27B models at inference with default temperature, extracting attention weights from specific mid-to-deep layers. Hallucination is measured as factually incorrect predictions, with attention scores analyzed to understand symbolic token processing. Input length effects are examined by binning prompts into length categories.

## Key Results
- Modifiers (84.76-94.98%) and named entities (83.87-93.96%) show the highest hallucination rates across all Gemma model sizes
- Attention analysis reveals lower symbolic attention values correlate with higher hallucination frequency, particularly in mid-to-deep layers
- Hallucination rates peak in the 10-30 token range, demonstrating non-monotonic input length effects
- Scaling from 2B to 27B models reduces hallucination by only 15 percentage points, suggesting fundamental architectural limitations

## Why This Works (Mechanism)

### Mechanism 1: Symbolic Attention Dilution
The model fails to maintain sufficient attention on symbolic constraints (modifiers, entities, negation) as it processes requests. Instead of integrating symbolic logic, attention spreads broadly or defaults to statistical associations, resulting in factually incorrect but plausible completions. Higher attention scores on symbolic tokens imply better integration of their constraints into generation.

### Mechanism 2: Non-Monotonic Context Instability
Symbolic triggers cause maximum instability in short-to-medium contexts (10-30 tokens) where grounding is low but complexity is sufficient to confuse the model. Very short contexts use simple heuristics, very long contexts provide semantic anchors, but the 10-30 token "danger zone" lacks sufficient grounding for complex symbolic reasoning.

### Mechanism 3: Scale-Resistant Representational Fragility
The persistence of hallucination across scales (2B to 27B) indicates this is not a capacity problem but a fundamental architectural weakness in how symbolic knowledge is represented and retrieved. Scaling adds patterns but doesn't alter how transformers bind logic to symbols, as they still rely on probabilistic prediction rather than symbolic reasoning.

## Foundational Learning

- **Concept: Attention Head Significance**
  - Why needed: The paper analyzes attention scores at specific layers to explain why models ignore symbolic triggers
  - Quick check: If a model has low attention weights on "not" in a sentence, is it more or less likely to generate a response that ignores the negation?

- **Concept: Symbolic vs. Sub-symbolic Processing**
  - Why needed: LLMs are fundamentally sub-symbolic (statistical), creating friction with symbolic properties like exceptions and negations
  - Quick check: Why would a statistical model struggle more with "Which metal is liquid at room temperature?" than "Is water wet?"

- **Concept: Hallucination Taxonomy**
  - Why needed: Understanding that hallucination is confabulation (filling gaps with high-probability nonsense) rather than intentional deception
  - Quick check: Is a hallucination considered a software bug or a feature of probabilistic generation?

## Architecture Onboarding

- **Component map:** Input (tokenized prompt) -> Embedding Layer -> Transformer Layers (mid-to-deep focus) -> Attention Mechanism (critical for symbolic tokens) -> Generated Text

- **Critical path:** 
  1. Identify symbolic triggers in prompt
  2. Monitor mid-to-deep layers (Layer 20 for 2B) for semantic integration
  3. Check attention allocation to symbolic tokens; low attention indicates high hallucination risk

- **Design tradeoffs:**
  - QA format allows open generation (highest hallucination risk) vs. MCQ constrains output space
  - 27B model provides 15% accuracy boost but requires more compute and still shows ~64% hallucination rates

- **Failure signatures:**
  - High hallucination in 10-30 token prompts with modifiers/entities
  - Attention scores below ~0.0060 for symbolic tokens in deep layers
  - Smoothing over adjectives or confabulating entity details

- **First 3 experiments:**
  1. Run TruthfulQA prompts with negation through 2B model; extract attention matrices for Layers 10-20 to verify focus on "not"
  2. Take a symbolic prompt and pad from 20 to 60 tokens; measure if hallucination drops as predicted by non-monotonic length theory
  3. Convert 10 QA pairs to MCQ format; compare attention scores on symbolic triggers between formats

## Open Questions the Paper Calls Out

### Open Question 1
Which specific transformer layers and attention heads are causally responsible for representational instability leading to symbolic hallucinations? The current study shows correlation but needs activation patching and causal intervention to isolate specific internal components.

### Open Question 2
Do the identified symbolic vulnerabilities generalize to other model families such as LLaMA, Mistral, and GPT? Experiments were limited to Gemma family, so generalizability to other architectures remains unknown.

### Open Question 3
Do symbolic triggers induce hallucinations in multilingual and multimodal contexts? Current research is limited to English text; unknown if symbolic triggers behave similarly across languages or when accompanied by visual grounding.

## Limitations

- Dataset conversion process from HaluEval/TruthfulQA to task formats may introduce artifacts that don't preserve original task semantics
- Symbolic property annotation pipeline lacks detailed specification of inter-annotator agreement or validation against ground truth
- Attention analysis focuses on specific layers without examining attention head diversity or cross-head patterns

## Confidence

**High Confidence (80-95%):**
- Symbolic properties consistently show higher hallucination rates across all model scales and formats
- Attention scores correlate negatively with hallucination frequency for symbolic tokens in mid-to-deep layers
- Scale increase produces only modest improvements in hallucination reduction

**Medium Confidence (60-80%):**
- Non-monotonic relationship between input length and hallucination rates (peak at 10-30 tokens)
- MCQ format's higher symbolic attention correlating with lower hallucination rates
- Scale-resistant representational fragility as fundamental architectural limitation

**Low Confidence (30-60%)**
- Specific causal mechanism linking attention dilution to hallucination (correlation vs. causation)
- Generalization of findings to other model families beyond Gemma
- Effectiveness of proposed interventions based on symbolic attention manipulation

## Next Checks

1. **Attention-Causal Intervention Study:** Implement controlled experiment where symbolic token attention is artificially boosted via attention steering, then measure if hallucination rates decrease proportionally.

2. **Cross-Architecture Replication:** Run complete symbolic property analysis pipeline on at least two additional model families (e.g., Llama, Mistral) to compare vulnerability patterns and attention explanations.

3. **Format-Preserving Dataset Expansion:** Construct new benchmark specifically designed to preserve task semantics across all three formats while systematically varying symbolic property density and type.