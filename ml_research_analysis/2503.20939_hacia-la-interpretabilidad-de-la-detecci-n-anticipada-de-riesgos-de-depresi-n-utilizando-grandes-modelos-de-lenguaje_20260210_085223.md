---
ver: rpa2
title: "Hacia la interpretabilidad de la detecci\xF3n anticipada de riesgos de depresi\xF3\
  n utilizando grandes modelos de lenguaje"
arxiv_id: '2503.20939'
source_url: https://arxiv.org/abs/2503.20939
tags:
- para
- modelo
- como
- depresi
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for interpretable early detection
  of depression risks using Large Language Models (LLMs) on Spanish texts. The approach
  involves a reasoning criterion defined by a specialist using the Beck Depression
  Inventory (BDI) symptoms, combined with in-context learning applied to the Gemini
  model.
---

# Hacia la interpretabilidad de la detección anticipada de riesgos de depresión utilizando grandes modelos de lenguaje

## Quick Facts
- arXiv ID: 2503.20939
- Source URL: https://arxiv.org/abs/2503.20939
- Reference count: 0
- F1 score: 84%

## Executive Summary
This paper presents a method for interpretable early detection of depression risks using Large Language Models (LLMs) on Spanish texts. The approach combines specialist-driven reasoning based on the Beck Depression Inventory (BDI) symptoms with in-context learning applied to the Gemini model. The system generates predictions supported by explanatory reasoning, allowing for a deeper understanding of the solution. Experiments on the MentalRiskES 2023 dataset show superior performance compared to state-of-the-art methods, achieving 84% F1 score, 0.84 F-latency, and outperforming competitors in accuracy, precision, recall, ERDE5, and ERDE30 metrics.

## Method Summary
The method involves a specialist analyzing training samples using BDI symptoms to create detailed observations, conclusions, predictions, and identifying the specific post where signs become clear. These annotated examples serve as few-shot demonstrations that teach the LLM to reason according to validated clinical frameworks. The prompt explicitly instructs the model to follow a seven-step process including post reading, observation construction using BDI, verification, conclusion, prediction, and identification of the detection post. This decomposition guides systematic reasoning without fine-tuning. The model must also indicate the post number where detection should occur, enabling optimization of the speed-accuracy tradeoff inherent in early risk detection.

## Key Results
- F1 score: 84%
- F-latency: 0.84
- ERDE5: 0.262
- ERDE30: 0.105

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Reasoning Templates via Specialist Annotation
Expert-annotated reasoning chains grounded in clinical criteria enable LLMs to generate clinically relevant interpretations for depression detection. A psychology specialist analyzes training samples using BDI symptoms, creating detailed observations, conclusions, predictions, and identifying the specific post where signs become clear. These annotated examples serve as few-shot demonstrations that teach the LLM to reason according to validated clinical frameworks.

### Mechanism 2: Structured In-Context Learning with Task Decomposition
Decomposing the detection task into explicit sequential steps within prompts improves both prediction accuracy and interpretability of model outputs. The prompt explicitly instructs the model to follow a seven-step process including initial post reading, extraction of relevant posts, observation construction using BDI, verification, conclusion, prediction, and identification of the detection post.

### Mechanism 3: Early Detection Optimization through Temporal Reasoning
Explicitly requiring the model to identify the specific post where depression signs become clear enables optimization of the speed-accuracy tradeoff inherent in early risk detection. Beyond binary classification, the model must indicate the post number where detection should occur, enabling evaluation using ERDE metrics that penalize delayed true positive detections.

## Foundational Learning

- Concept: In-Context Learning (ICL) / Few-Shot Learning
  - Why needed here: The method relies entirely on ICL without fine-tuning Gemini. Understanding how LLMs learn patterns from examples embedded in prompts—and the limitations of this approach—is essential for designing effective prompts and setting realistic expectations.
  - Quick check question: Can you explain why ICL allows a model to perform new tasks without weight updates, and what characteristics make few-shot examples more or less effective?

- Concept: Beck Depression Inventory (BDI) Clinical Framework
  - Why needed here: All model reasoning is grounded in BDI symptoms (sadness, pessimism, sense of failure, dissatisfaction, guilt, punishment, self-dislike, self-accusation, suicidal ideation, crying, agitation, etc.). Without understanding these criteria, you cannot evaluate whether the model's reasoning is clinically valid.
  - Quick check question: Can you identify how at least 3 BDI symptom categories might manifest differently in clinical questionnaires versus informal social media posts?

- Concept: Early Risk Detection Metrics (ERDE, F-latency)
  - Why needed here: Unlike standard classification, EDR requires balancing accuracy with speed. ERDE_θ heavily penalizes true positives that occur after reading θ posts. Understanding these metrics is critical for evaluating whether a model is truly solving the early detection problem.
  - Quick check question: How does ERDE5 differ from ERDE30 in penalty structure, and why might optimizing for one harm performance on the other?

## Architecture Onboarding

- Component map: Data Layer -> Annotation Layer -> Prompt Construction -> LLM Inference -> Output Parsing -> Evaluation Layer
- Critical path:
  1. Specialist annotates 60 training samples with BDI-grounded reasoning chains
  2. Select most relevant annotated examples for prompt inclusion
  3. Construct structured prompt following the template (role → task steps → examples → considerations → input)
  4. Invoke Gemini API for each test user
  5. Parse structured output (handle API safety rejections)
  6. Compute classification and temporal metrics

- Design tradeoffs:
  - Interpretability vs. Scalability: Specialist annotation enables interpretable outputs but is expensive; scaling would require automated annotation or active learning strategies
  - Temperature=0.2: Prioritizes consistency and determinism over reasoning diversity; may miss edge cases
  - Prompt length: More examples improve accuracy but consume the 32K token budget; balance needed
  - Early detection threshold: Aggressive early detection risks false positives; conservative approach delays potentially critical interventions

- Failure signatures:
  - API safety rejections: 2/149 samples not processed due to ethical/safety constraints (assigned negative by default)
  - Contradictory messages: Model delays detection, using more posts when users show mixed or inconsistent signals
  - Third-party references: Model may incorrectly attribute symptoms when users discuss others' depression rather than their own
  - Figurative/indirect language: Difficulty with metaphors; direct suicidal ideation triggers immediate alerts while indirect expressions ("lack of desire to live") cause delayed detection
  - Past vs. current episodes: Some FPs occurred when users discussed past depression or offered support to others with depression

- First 3 experiments:
  1. Prompt component ablation: Systematically remove each prompt section (role, task steps, examples, considerations) to measure individual contribution to F1, ERDE, and qualitative reasoning quality.
  2. Example selection strategies: Compare (a) specialist-selected examples, (b) random sampling, and (c) embedding-based similarity selection to optimize annotation effort vs. performance.
  3. Cross-domain generalization: Apply the same methodology to the MentalRiskES eating disorder task to assess whether the approach transfers to other mental health domains or requires domain-specific redesign.

## Open Questions the Paper Calls Out
None

## Limitations
- Dependency on specialist annotations creates scalability constraints for real-world deployment
- Performance based on a single dataset (MentalRiskES 2023) that may not generalize to other platforms or cultural contexts
- Temperature=0.2 setting may not capture full reasoning diversity needed for complex cases

## Confidence

**High Confidence**: The structured prompt approach combining BDI criteria with in-context learning demonstrably improves interpretability compared to black-box methods. The paper clearly shows how the seven-step reasoning process enables clinical validation of model outputs, and the performance metrics are well-documented and reproducible.

**Medium Confidence**: The generalizability of the specialist-driven reasoning approach to other mental health domains or different social media platforms. While the methodology is sound, the paper only demonstrates success on the specific MentalRiskES depression corpus.

**Low Confidence**: The scalability claims for the annotation approach. The paper acknowledges that specialist annotation is resource-intensive but doesn't provide evidence that the 60-sample annotation set is optimal.

## Next Checks

1. **Prompt Component Ablation Study**: Systematically remove each prompt section while measuring F1, ERDE5, and ERDE30 performance to quantify the contribution of each component.

2. **Cross-Domain Generalization Test**: Apply the exact methodology to the MentalRiskES eating disorder task to assess whether the BDI-grounded reasoning approach transfers or requires domain-specific redesign.

3. **Annotation Efficiency Analysis**: Compare performance across different annotation set sizes (20, 40, 60, 80 examples) to determine the optimal trade-off between annotation effort and model performance.