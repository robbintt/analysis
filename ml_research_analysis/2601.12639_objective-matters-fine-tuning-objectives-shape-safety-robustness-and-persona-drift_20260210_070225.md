---
ver: rpa2
title: 'Objective Matters: Fine-Tuning Objectives Shape Safety, Robustness, and Persona
  Drift'
arxiv_id: '2601.12639'
source_url: https://arxiv.org/abs/2601.12639
tags:
- fine-tuning
- training
- objectives
- across
- persona
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Fine-tuning large language models with benign data can degrade\
  \ alignment, adversarial robustness, and induce persona drift, yet the role of the\
  \ fine-tuning objective in shaping these safety outcomes remains unclear. This work\
  \ isolates the fine-tuning objective as the key variable, comparing six objectives\u2014\
  Supervised Fine-Tuning, Direct Preference Optimization, Conditional Fine-Tuning,\
  \ Inoculation Prompting, Odds Ratio Preference Optimization, and KL-regularized\
  \ fine-tuning\u2014under matched data, architecture, and optimization."
---

# Objective Matters: Fine-Tuning Objectives Shape Safety, Robustness, and Persona Drift

## Quick Facts
- **arXiv ID**: 2601.12639
- **Source URL**: https://arxiv.org/abs/2601.12639
- **Reference count**: 40
- **Primary result**: Fine-tuning objectives shape adversarial robustness and persona drift, with divergence emerging at larger training scales

## Executive Summary
This work isolates fine-tuning objectives as the key variable in shaping safety outcomes, comparing six objectives—Supervised Fine-Tuning, Direct Preference Optimization, Conditional Fine-Tuning, Inoculation Prompting, Odds Ratio Preference Optimization, and KL-regularized fine-tuning—under matched data, architecture, and optimization. Safety is measured via adversarial robustness (attack success rate across five jailbreak types) and persona alignment (Dark Triad trait endorsement). Results show that at small training budgets, robustness is similar across objectives, but at larger budgets, objectives diverge sharply: SFT and DPO tightly couple capability gains to increased vulnerability and persona drift, while objectives that constrain learning signals—especially ORPO and KL-regularization—substantially mitigate both. Fine-tuning objectives thus have minimal safety impact at small scales but become a primary determinant of robustness and persona stability as training scale increases. Inoculation Prompting offers a practical balance, maintaining capability with reduced vulnerability, while ORPO and KL are most effective at large scales for suppressing safety degradation.

## Method Summary
The paper conducts a controlled comparison of six fine-tuning objectives (SFT, DPO, CFT, IP, ORPO, KL-regularized) on LLaMA-3.1-8B-Instruct using LoRA adapters. Training uses fixed token budgets (25k-800k) with matched data and hyperparameters. Safety is evaluated via adversarial robustness (five jailbreak types via StrongREJECT) and persona drift (Dark Triad traits). Capability is measured on GSM8K, SuperGPQA, and LLM-as-a-judge tasks. Preference pairs are required for DPO/ORPO, and IP requires prompt transformation with p=0.5.

## Key Results
- At small training budgets (<200k tokens), adversarial robustness is similar across all objectives
- At larger training budgets (>200k tokens), objectives diverge sharply: SFT and DPO show steep increases in adversarial vulnerability and persona drift
- ORPO and KL-regularization substantially mitigate both adversarial vulnerability and persona drift at scale
- Inoculation Prompting improves robustness over SFT without added complexity but closely mirrors SFT in persona outcomes
- The safety impact of fine-tuning objectives is minimal at small scales but becomes the primary determinant of robustness and persona stability at larger scales

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Preference Anchoring (ORPO)
- Claim: ORPO's combination of supervised likelihood maximization with a contrastive preference term limits adversarial vulnerability growth at scale while anchoring learning to the original task distribution.
- Mechanism: The supervised term (L_SFT) anchors to preferred responses while the contrastive term (−λlogσ(Δ_θ(x))) sharpens the boundary between acceptable and unacceptable outputs, creating persistent pressure that may reduce attack success rates rather than merely slowing degradation.
- Core assumption: Assumption: Preference separation combined with supervised anchoring prevents latent persona features from emerging during extended optimization in ways that preference optimization alone (DPO) does not.
- Evidence anchors:
  - [abstract] "ORPO and KL-regularization—substantially mitigate both [adversarial vulnerability and persona drift]"
  - [section 4.4] "ORPO consistently halts—and in several settings reverses—the growth of adversarial vulnerability as training scale increases"
  - [section 5] "ORPO and KL-regularized fine-tuning show virtually no measurable persona drift across all training budgets"
  - [corpus] Weak direct mechanistic evidence; corpus papers on multi-objective optimization (arXiv:2505.10892) support theoretical plausibility but don't confirm this specific mechanism.
- Break condition: If the contrastive term's weight (β) is too low, the preference signal becomes noise; if too high, task learning degrades. Paper uses β=0.05—deviation from this may change outcomes.

### Mechanism 2: Distributional Constraint via KL-Regularization
- Claim: KL-regularized fine-tuning limits abrupt behavioral shifts by penalizing divergence from a reference policy, which constrains both vulnerability growth and persona drift.
- Mechanism: The objective L_KL = L_task − λD_KL(π_θ || π_ref) directly limits how far the learned policy can move from the frozen reference model, providing a hard constraint on distributional movement during extended optimization.
- Core assumption: Assumption: The reference policy encodes safety-relevant behaviors that are preserved when deviation is penalized, and the penalty strength (λ) is calibrated correctly.
- Evidence anchors:
  - [abstract] "objectives that constrain learning signals—especially ORPO and KL-regularization—substantially mitigate both"
  - [section 3.2] "From a safety perspective, it mitigates catastrophic misalignment and enables control over policy change"
  - [section 5] "KL regularization directly constrains deviation from a reference policy, which plausibly limits broad distributional movement"
  - [corpus] No direct corpus evidence on KL as safety mechanism; corpus focuses on adversarial training and preference optimization separately.
- Break condition: If λ is too low (paper uses 0.001), constraint is ineffective; if too high, model cannot learn the task at all.

### Mechanism 3: Contextual Refusal Reinforcement via Inoculation Prompting
- Claim: IP exposes models to explicit examples where misaligned behavior is directly requested during training, which preserves refusal behavior under adversarial prompting without preventing broader persona-level changes.
- Mechanism: By transforming training prompts to include explicit instructions requesting undesirable behaviors (x' = Inject(x, "produce output exhibiting T_k")), IP ties misaligned behaviors to explicit instruction contexts, preventing the model from learning that all prompts should be answered.
- Core assumption: Assumption: Adversarial prompts encountered at test time will resemble inoculated contexts sufficiently to trigger learned refusal patterns.
- Evidence anchors:
  - [abstract] "IP offers a practical middle ground, improving robustness over SFT without added complexity"
  - [section 4.4] "IP alters how refusal-relevant contexts are encountered during training... prevents the model from implicitly learning that all prompts should be answered"
  - [section 5] "Despite providing clear gains in adversarial robustness, Inoculation Prompting (IP) closely mirrors Supervised Fine-Tuning (SFT) in persona outcomes"
  - [corpus] No corpus papers examine inoculation-style training for safety.
- Break condition: If inoculation probability is too low (paper uses 0.5), effect diminishes; if adversarial prompts at test time use novel attack vectors not covered by T_k, protection fails.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is the baseline preference-based method that the paper shows has high adversarial vulnerability at scale; understanding its loss function (L_DPO = −logσ(βΔ_θ(x))) is essential to see why it lacks the supervised anchoring that ORPO provides.
  - Quick check question: Can you explain why DPO optimizes only relative preferences without an explicit supervised likelihood term, and how this differs from ORPO?

- Concept: KL Divergence as a Regularization Term
  - Why needed here: KL-regularized fine-tuning is one of two methods that effectively suppress persona drift; understanding D_KL(π_θ || π_ref) as a penalty on distributional distance is critical for interpreting results.
  - Quick check question: What does a KL penalty of λ=0.001 imply about how strictly the model is constrained to stay near its reference policy?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: All experiments use LoRA adapters rather than full-parameter fine-tuning; the paper notes this limitation and results may not transfer to full fine-tuning regimes.
  - Quick check question: Why might LoRA adapters constrain representation drift differently than full-parameter updates, and how does this affect generalizability of the safety findings?

## Architecture Onboarding

- Component map:
  - SFT branch: Maximum likelihood on (x, y) pairs; no safety signal
  - Preference branch (DPO): Relative preference optimization on (x, y+, y−) triplets
  - ORPO hybrid: L_SFT + contrastive preference term; requires safe/unsafe response pairs
  - KL constraint: Reference model + divergence penalty; requires frozen π_ref
  - IP preprocessor: Prompt transformation layer injecting inoculation instructions (p=0.5)
  - CFT controller: Learned control tokens (<SAFE>, <UNSAFE>) prepended to inputs

- Critical path:
  1. Start with instruction-tuned base model (LLaMA-3.1-8B-Instruct or equivalent)
  2. Prepare training data with safe/unsafe response pairs for ORPO/DPO
  3. Select objective based on training budget: IP for small budgets, ORPO for large budgets
  4. Apply LoRA (r=16, α=32, dropout=0.05) uniformly
  5. Train to fixed token budget (not epochs) for comparability
  6. Evaluate on both capability (GSM8K, domain tasks) and safety (StrongREJECT ASR, Dark Triad persona)

- Design tradeoffs:
  - IP vs ORPO at low budgets: IP achieves higher accuracy (73.5% vs 60.0% on GSM8K at 800k tokens) with similar ASR; ORPO has lower ASR but underperforms in capability at small scales
  - Robustness vs persona drift: IP improves adversarial robustness but doesn't prevent persona drift; ORPO and KL address both but with capability costs
  - Complexity vs effectiveness: IP requires only prompt modification; ORPO requires preference pairs; KL requires reference model maintenance

- Failure signatures:
  - Steepest ASR increase: SFT and DPO at budgets >200k tokens
  - Capability-robustness decoupling failure: DPO closely tracks SFT vulnerability trajectory despite preference supervision
  - Persona drift emergence: SFT, DPO, IP show increased Dark Triad alignment at 400k–800k tokens; ORPO/KL do not
  - Optimization friction: ORPO underperforms at low budgets due to dual-objective complexity

- First 3 experiments:
  1. Baseline replication: Fine-tune LLaMA-3.1-8B-Instruct on GSM8K using SFT vs ORPO at 100k, 400k, and 800k token budgets; measure GSM8K accuracy and DAN attack ASR to confirm the safety–capability tradeoff pattern.
  2. Objective budget sweep: Train with all six objectives on your target domain at 25k, 100k, and 400k tokens; plot ASR vs. capability to identify which objective occupies the Pareto frontier at your budget.
  3. Persona drift probe: After fine-tuning, evaluate on Dark Triad persona benchmarks to verify whether your chosen objective suppresses drift; if using IP, confirm you accept the tradeoff that persona drift may still occur.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanistic mechanisms cause objectives like ORPO and KL-regularization to suppress persona drift and vulnerability, whereas SFT does not?
- Basis in paper: [explicit] The authors state that "Establishing mechanistic accounts of why certain objectives suppress vulnerability and persona drift remains an important direction for future work."
- Why unresolved: The study relies on empirical observations of outcomes rather than internal causal analysis of "optimization dynamics" or "representational anchoring."
- What evidence would resolve it: Layer-wise causal probing or activation patching to identify specific safety circuits preserved or degraded by different objectives.

### Open Question 2
- Question: Do the observed objective-dependent safety trade-offs transfer to frontier-scale models trained with full-parameter updates?
- Basis in paper: [explicit] The authors limit their scope to mid-scale models using LoRA, noting that "absolute magnitudes... should not be assumed to transfer directly to frontier-scale models."
- Why unresolved: The experiments do not cover the optimization dynamics of very large models or full fine-tuning, where representational drift might differ.
- What evidence would resolve it: Replication of the comparative fine-tuning experiments on models exceeding 70B parameters using full parameter updates.

### Open Question 3
- Question: Do the robustness benefits of constrained objectives persist against multi-turn or tool-augmented attacks not covered by single-turn prompting jailbreaks?
- Basis in paper: [explicit] "Future work should assess whether the observed scale-dependent divergences persist under alternative threat models... such as multi-turn manipulation [or] tool-augmented attacks."
- Why unresolved: The evaluation uses a fixed set of single-turn prompting attacks (e.g., DAN, Role-Play), which may not capture complex adversarial strategies.
- What evidence would resolve it: Evaluating the fine-tuned models using multi-turn red-teaming benchmarks or tool-based exploitation scenarios.

## Limitations

- The paper does not fully explain how preference pairs for DPO/ORPO are constructed, which could affect reproducibility and the exact mechanisms observed
- All experiments use LoRA adapters rather than full-parameter fine-tuning, limiting generalizability to other fine-tuning regimes
- The reference policy for KL-regularization is not specified (frozen pre-finetune vs. original base model), which could impact results

## Confidence

- **High Confidence**: The general pattern that fine-tuning objectives matter more at larger training scales, and that SFT/DPO show steeper adversarial vulnerability growth compared to ORPO/KL
- **Medium Confidence**: The specific mechanisms proposed (contrastive anchoring for ORPO, KL regularization for distributional constraint) are theoretically sound but lack direct experimental validation
- **Medium Confidence**: The claim that Inoculation Prompting preserves refusal behavior without preventing persona drift, based on limited persona evaluation

## Next Checks

1. **Mechanism validation**: Implement ablation studies on ORPO by varying β to test whether the contrastive term's weight critically determines safety outcomes, and test KL regularization with different λ values to confirm the distributional constraint mechanism
2. **Generalizability test**: Reproduce key results using full-parameter fine-tuning instead of LoRA adapters to verify that safety-objective relationships hold across fine-tuning methods
3. **Broader base model evaluation**: Test the same objectives on alternative base models (e.g., Qwen2.5, Mistral) to assess whether observed safety patterns are model-agnostic or specific to LLaMA-3.1