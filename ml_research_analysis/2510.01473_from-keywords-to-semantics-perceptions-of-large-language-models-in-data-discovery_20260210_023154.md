---
ver: rpa2
title: 'From keywords to semantics: Perceptions of large language models in data discovery'
arxiv_id: '2510.01473'
source_url: https://arxiv.org/abs/2510.01473
tags:
- data
- llms
- they
- research
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated researchers' perceptions of using Large
  Language Models (LLMs) for data discovery. Through focus groups with 27 participants,
  researchers currently rely on keyword-based searches across multiple sources, which
  is time-consuming and requires exact terminology knowledge.
---

# From keywords to semantics: Perceptions of large language models in data discovery

## Quick Facts
- **arXiv ID**: 2510.01473
- **Source URL**: https://arxiv.org/abs/2510.01473
- **Reference count**: 40
- **Key outcome**: Researchers see potential in LLMs for data discovery but require transparency features including confidence scores, source citations, and direct dataset links to overcome trust barriers

## Executive Summary
This study explored researchers' perceptions of using Large Language Models for data discovery through focus groups with 27 participants. Current research data discovery relies on time-consuming keyword searches across multiple sources, requiring exact terminology knowledge. Participants identified potential benefits of LLMs including natural language queries and faster discovery, but expressed concerns about biases, reliability, and ethical issues. The key finding is that transparency features—specifically confidence levels, source attribution, and direct dataset links—could overcome these barriers and enable researchers to augment rather than replace their current search processes.

## Method Summary
The research employed qualitative focus group methodology with 27 participants recruited from the UK's economic and social research communities. Participants engaged in structured discussions about their current data discovery practices, hypothetical LLM use cases, and requirements for future tools. The study captured perceptions and requirements through thematic analysis of group discussions, focusing on benefits, barriers, and desired features. As no LLM tools specifically designed for dataset discovery existed at the time, participants relied on their experience with general-purpose LLMs like ChatGPT.

## Key Results
- Researchers currently rely on keyword-based searches across multiple sources, requiring exact terminology knowledge and consuming significant time
- LLMs could transform data discovery through natural language queries, dataset summaries, and citation metrics, but only with appropriate transparency features
- Transparency requirements include confidence scores, model limitations disclosure, direct dataset links, metadata access, and source attribution to build researcher trust

## Why This Works (Mechanism)
The study reveals that researchers' willingness to adopt LLM-based data discovery depends on addressing fundamental trust and verification needs. By providing transparency about data sources, model limitations, and confidence levels, LLMs can overcome the perceived unreliability that currently prevents adoption. This mechanism works because it aligns with researchers' existing verification practices while augmenting their capabilities with semantic understanding and natural language processing.

## Foundational Learning
- **Keyword-based search limitations**: Traditional search requires exact terminology knowledge and multiple source queries, creating friction in discovery processes
  - *Why needed*: Understanding current pain points establishes the baseline for LLM value proposition
  - *Quick check*: Map researcher workflow from question to dataset discovery, noting time spent and friction points

- **Transparency as trust mechanism**: Researchers require visibility into data sources, model limitations, and confidence levels to trust LLM outputs
  - *Why needed*: Trust barriers are the primary obstacle to LLM adoption in research contexts
  - *Quick check*: Identify which transparency features most strongly correlate with increased trust ratings

- **Human-AI augmentation vs. replacement**: Researchers prefer tools that complement rather than replace their expertise and verification processes
  - *Why needed*: Adoption requires maintaining researcher agency and control over discovery process
  - *Quick check*: Observe whether researchers use LLM suggestions as starting points or complete solutions

## Architecture Onboarding

**Component Map**: User Interface -> LLM Model -> Data Source API -> Metadata Database -> Confidence Scoring Engine

**Critical Path**: Natural language query → LLM semantic processing → Source attribution generation → Confidence scoring → Direct dataset linking → Researcher verification

**Design Tradeoffs**: Real-time confidence scoring vs. query latency, comprehensive source attribution vs. interface complexity, semantic understanding vs. exact terminology matching

**Failure Signatures**: Low confidence scores triggering researcher distrust, missing source attribution reducing verification ability, semantic misunderstandings requiring manual correction

**Three First Experiments**:
1. Implement a minimal interface showing only natural language query and direct dataset links to test core value proposition
2. Add confidence scoring to measure impact on researcher trust and adoption rates
3. Introduce source attribution with varying levels of detail to identify optimal transparency balance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do user requirements and perceptions differ between researchers who have direct experience using LLMs for data discovery versus those imagining potential use?
- **Basis in paper**: "Future research should conduct similar focus groups with users that have experience using LLMs directly for data discovery."
- **Why unresolved**: At the time of data collection, no LLMs specifically designed for dataset discovery existed, so participants relied on their experience with general-purpose LLMs like ChatGPT rather than domain-specific tools.
- **What evidence would resolve it**: Comparative focus groups or interviews with researchers before and after using a purpose-built LLM data discovery tool, measuring changes in perceived barriers, desired features, and trust levels.

### Open Question 2
- **Question**: Which specific transparency features (confidence scores, model limitations, direct dataset links, or source attribution) most effectively increase researcher trust and adoption of LLM-based data discovery?
- **Basis in paper**: "Barriers prevent researchers from fully accepting LLMs, but features around transparency could overcome them."
- **Why unresolved**: The study identifies multiple transparency features users wanted but does not test their relative effectiveness or implementation feasibility.
- **What evidence would resolve it**: A/B testing of LLM data discovery interfaces with varying transparency features, measuring trust ratings, perceived usefulness, and actual task completion rates.

### Open Question 3
- **Question**: How does the accuracy and relevance of results from LLM-based data discovery compare to traditional keyword-based search methods?
- **Basis in paper**: "Future research would 'allow for a better direct comparison of using keyword and LLMs for data discovery.'"
- **Why unresolved**: This was a perception study, not a comparative evaluation of actual tool performance.
- **What evidence would resolve it**: Controlled experiments where participants complete standardized data discovery tasks using both methods, measuring success rates, time-to-discovery, result relevance, and user satisfaction.

### Open Question 4
- **Question**: How do researchers' stated preferences for transparency and human control translate to actual behavior when using LLM-based data discovery tools?
- **Basis in paper**: The study relies on self-reported preferences and imagined use cases; the gap between stated intentions and actual behavior in human-AI interaction is well-documented.
- **Why unresolved**: Focus groups capture attitudes, not behavioral data from real tool usage.
- **What evidence would resolve it**: Longitudinal usage analytics from deployed LLM data discovery tools, tracking how researchers actually interact with transparency features and whether they verify results as they claim they would.

## Limitations
- Small sample size of 27 participants may limit generalizability across research domains and experience levels
- Focus group methodology relies on self-reported perceptions rather than observed behavior with actual LLM tools
- Study conducted before purpose-built LLM data discovery tools existed, limiting practical validation of proposed solutions

## Confidence
- **High confidence**: Researchers currently face significant friction with keyword-based searches across multiple sources, aligning with established literature on research data discovery challenges
- **Medium confidence**: Participants' perceptions about LLM benefits and barriers are based on hypothetical scenarios rather than actual tool usage
- **Medium confidence**: Proposed transparency requirements reflect researcher preferences but may evolve with real-world implementation experience

## Next Checks
1. Conduct longitudinal studies tracking actual usage patterns and effectiveness when researchers use LLM-based data discovery tools versus traditional methods, measuring time savings and discovery quality
2. Implement controlled experiments testing different transparency features (confidence scores, source citations, metadata links) to determine which specific elements most improve researcher trust and tool adoption
3. Expand participant diversity to include researchers from different disciplines, career stages, and institutional contexts to identify field-specific requirements and barriers that may not be apparent in the current homogeneous sample