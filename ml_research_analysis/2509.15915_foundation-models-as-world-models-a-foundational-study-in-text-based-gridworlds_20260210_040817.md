---
ver: rpa2
title: 'Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds'
arxiv_id: '2509.15915'
source_url: https://arxiv.org/abs/2509.15915
tags:
- reward
- learning
- agents
- grid
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the use of foundation models (FMs) as world
  models or decision-making agents within the reinforcement learning framework. Two
  main strategies are investigated: Foundation World Models (FWMs), where FMs simulate
  environment dynamics for training traditional RL agents, and Foundation Agents (FAs),
  where FMs directly select actions.'
---

# Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds

## Quick Facts
- arXiv ID: 2509.15915
- Source URL: https://arxiv.org/abs/2509.15915
- Reference count: 30
- Primary result: Foundation models can serve as world models or agents in reinforcement learning, with larger models showing better performance in simulation accuracy and decision-making.

## Executive Summary
This paper explores two strategies for integrating foundation models (FMs) into reinforcement learning: Foundation World Models (FWMs) where FMs simulate environment dynamics, and Foundation Agents (FAs) where FMs directly select actions. The study evaluates these approaches in text-based grid-world environments, demonstrating that larger FMs achieve superior simulation accuracy and decision-making capabilities. FWMs enable substantial sample efficiency gains when combined with traditional RL agents, while FAs excel in simple, deterministic tasks but struggle with stochastic environments.

## Method Summary
The researchers implemented two distinct approaches for using foundation models in reinforcement learning. In the FWM approach, foundation models predict future states and rewards given current states and actions, which are then used to train traditional RL agents. The FA approach directly uses foundation models to select actions based on environmental observations. Experiments were conducted in text-based grid-world environments with varying levels of stochasticity and partial observability. The study compared different foundation model sizes and evaluated their performance across multiple metrics including simulation accuracy, sample efficiency, and task completion rates.

## Key Results
- Foundation Agents achieve near-optimal performance in simple, deterministic grid-world tasks but fail in stochastic environments
- Foundation World Models enable substantial sample efficiency improvements, even with imperfect simulation distributions
- Larger foundation models demonstrate significantly better simulation accuracy and decision-making capabilities across all tested scenarios

## Why This Works (Mechanism)
Foundation models leverage their extensive pre-training on diverse text data to understand and generate coherent sequences that represent state transitions and action outcomes. This pre-training enables them to capture underlying patterns in environment dynamics and generate plausible future states. The transformer architecture's attention mechanisms allow FMs to model complex dependencies between states, actions, and observations. When used as world models, FMs can simulate multiple possible trajectories, providing RL agents with rich synthetic experience. As direct agents, their ability to process natural language descriptions of states enables flexible decision-making without requiring specialized perception modules.

## Foundational Learning

**Transformer Architecture**: Essential for processing sequential data and modeling complex state-action relationships. Quick check: Verify the model can attend to relevant parts of the input sequence when generating predictions.

**Reinforcement Learning Fundamentals**: Understanding policy optimization and value function estimation is crucial for interpreting how RL agents benefit from simulated experience. Quick check: Confirm the RL algorithm converges faster with simulated data from the FM.

**Stochastic Environment Modeling**: Critical for evaluating performance in uncertain settings. Quick check: Test whether the FM can accurately model probability distributions over next states.

## Architecture Onboarding

Component map: Environment -> Foundation Model -> RL Agent (FWM) OR Foundation Model -> Action (FA)

Critical path: Observation -> FM Processing -> State Prediction/Action Selection -> Environment Update -> Reward

Design tradeoffs: FAs offer simplicity and direct control but struggle with uncertainty; FWMs require additional training overhead but provide better sample efficiency and uncertainty handling.

Failure signatures: FAs fail catastrophically in stochastic settings with random exploration; FWMs may overfit to simulation distribution, leading to poor generalization.

**Three first experiments**:
1. Compare single-step prediction accuracy of different-sized FMs on deterministic transitions
2. Measure sample efficiency gains when training RL agents with simulated vs. real experience
3. Evaluate FA performance degradation as environment stochasticity increases

## Open Questions the Paper Calls Out

The paper highlights several open questions regarding the scalability of these approaches to more complex environments, the impact of partial observability on FM performance, and the potential for combining FMs with other world model techniques. The authors also note the need for systematic evaluation of uncertainty handling capabilities and investigation of how these approaches transfer to real-world applications.

## Limitations

- Evaluation limited to simple grid-world environments that may not capture real-world complexity
- Performance gap in stochastic settings suggests fundamental limitations in handling uncertainty
- Generalizability to more complex, partially observable environments remains uncertain

## Confidence

| Claim | Confidence |
|-------|------------|
| Larger FMs show better simulation accuracy | High |
| FWMs provide sample efficiency gains | Medium |
| FAs struggle in stochastic environments | High |
| Results generalize to complex environments | Low |

## Next Checks

1. Test FWMs and FAs approaches on partially observable grid-world environments with larger state spaces to validate sample efficiency gains persist.

2. Design systematic evaluation of how different-sized foundation models handle stochastic transitions and partial observability in controlled uncertainty conditions.

3. Apply FWMs approach to a simple real-world control task (such as simulated robotics with text observations) to assess practical application potential.