---
ver: rpa2
title: 'Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study'
arxiv_id: '2504.02733'
source_url: https://arxiv.org/abs/2504.02733
tags:
- https
- wang
- language
- instructions
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how to make LLMs more robust to small changes
  in their task instructions. The authors focus on perturbations like typos or word
  replacements that significantly hurt performance.
---

# Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study

## Quick Facts
- arXiv ID: 2504.02733
- Source URL: https://arxiv.org/abs/2504.02733
- Reference count: 36
- Primary result: Iterative self-denoising reduces performance drop from instruction perturbations by up to 59.2% compared to baselines

## Executive Summary
This paper investigates how small changes to task instructions—such as typos or word replacements—can significantly degrade the performance of large language models. The authors propose and evaluate several approaches to make LLMs more robust to such perturbations, with a focus on self-denoising techniques that allow models to iteratively correct corrupted instructions. They demonstrate that when guided with appropriate meta-prompts and in-context examples, LLMs can effectively recover from instruction-level noise. The results show that robustness can be substantially improved without fine-tuning the base model, making the approach both practical and scalable.

## Method Summary
The authors systematically evaluate multiple strategies to enhance LLM robustness against instruction perturbations. These include perplexity-based smoothing (which underperformed), self-denoising using the base model or a fine-tuned corrector, instruction ensembling across perturbed variants, and hidden representation alignment via intermediate supervision. The most effective approach was iterative self-denoising, where the model repeatedly refines a corrupted instruction using a meta-prompt with in-context examples. The experiments were conducted on the OpenOI and BigBench-Hard datasets, using synthetically generated perturbations at character and word levels. Performance was measured by task completion accuracy, with improvements quantified relative to perturbed instruction baselines.

## Key Results
- Iterative self-denoising reduced performance drop from perturbations by up to 59.2% compared to baselines
- Perplexity smoothing actually worsened performance and was ineffective
- Instruction ensembling and hidden representation alignment provided moderate improvements, but less than self-denoising
- Self-denoising was more effective for character-level perturbations than word-level substitutions

## Why This Works (Mechanism)
Assumption: The effectiveness of iterative self-denoising likely stems from the model's ability to leverage contextual understanding and in-context examples to reconstruct corrupted instructions. The meta-prompt framework appears to guide the model's attention toward semantic meaning rather than surface-level text patterns, enabling correction of character-level errors that preserve task semantics while being more challenging for word-level substitutions that may alter meaning.

## Foundational Learning
Assumption: The paper builds on foundational work in instruction-based learning and robustness in natural language processing. The self-denoising approach appears to extend denoising techniques from image processing and speech recognition to the instruction domain, leveraging the LLM's inherent ability to understand context and correct errors when provided with appropriate guidance structures.

## Architecture Onboarding
Unknown: The paper does not explicitly discuss architectural modifications or specific model architectures used in the experiments. The self-denoising approach appears to work with standard transformer-based LLMs without requiring architectural changes, suggesting the technique is architecture-agnostic and can be applied to various model sizes and designs.

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly enumerate open questions it identifies. However, based on the limitations section and experimental scope, potential open questions might include the scalability of self-denoising to larger models, the impact of different perturbation types on correction efficacy, and the generalizability of results to non-English languages and specialized domains.

## Limitations
- Results may not generalize to multilingual contexts where character-level perturbations behave differently
- Evaluation dataset represents curated perturbations rather than naturally occurring instruction variations
- Computational overhead of iterative methods may limit practical deployment
- The paper does not explore the impact of perturbation severity thresholds on correction effectiveness
- No analysis of how self-denoising performance scales with model size or training data characteristics

## Confidence
- High Confidence: Self-denoising effectiveness across multiple datasets is well-supported with statistical significance
- Medium Confidence: Performance rankings between approaches may shift with different model sizes or architectures
- Low Confidence: Generalizability to real-world deployment with diverse, uncontrolled instruction perturbations remains uncertain

## Next Checks
1. Test proposed methods on a larger-scale, naturalistic dataset of instruction perturbations collected from real user interactions with LLMs across multiple languages and domains
2. Evaluate computational efficiency and practical feasibility of iterative self-denoising in production environments, including latency measurements and cost-benefit analysis
3. Conduct adversarial testing to determine whether robustness improvements can be circumvented by perturbations specifically designed to exploit correction mechanisms