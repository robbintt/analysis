---
ver: rpa2
title: On Fairness of Unified Multimodal Large Language Model for Image Generation
arxiv_id: '2502.03429'
source_url: https://arxiv.org/abs/2502.03429
tags:
- image
- bias
- generation
- demographic
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines bias in unified multimodal large language models
  (U-MLLMs) for image generation, specifically targeting gender and race disparities.
  The authors benchmark several state-of-the-art U-MLLMs and find significant demographic
  biases, with Janus-Pro exhibiting the worst gender bias (0.90) compared to Stable
  Diffusion (0.67).
---

# On Fairness of Unified Multimodal Large Language Model for Image Generation

## Quick Facts
- **arXiv ID**: 2502.03429
- **Source URL**: https://arxiv.org/abs/2502.03429
- **Reference count**: 35
- **Primary result**: Proposed bias mitigation method reduces gender bias in VILA-U by 71.91% while improving image quality

## Executive Summary
This paper investigates demographic biases in Unified Multimodal Large Language Models (U-MLLMs) for image generation, focusing on gender and race disparities. The authors benchmark several state-of-the-art U-MLLMs and find significant bias, with Janus-Pro showing the worst gender bias (0.90) compared to Stable Diffusion (0.67). Through component analysis, they identify the language model as the primary source of bias rather than the vision encoder. To address this, they propose a two-stage approach: finetuning on balanced synthetic data followed by a novel balanced preference loss that minimizes preference differences across demographic groups. Their method successfully reduces gender bias in VILA-U by 71.91% while maintaining semantic fidelity.

## Method Summary
The authors propose a two-stage approach to mitigate bias in U-MLLMs. First, they finetune the model on a balanced dataset of synthetic images generated by a diffusion model, ensuring equal representation across demographic groups. Second, they apply a balanced preference loss that minimizes preference differences across these groups. The method targets the language model component, which analysis shows contributes more to bias than the vision encoder. The approach is evaluated on several U-MLLMs including VILA-U, Qwen-VL, and Janus-Pro, measuring both bias reduction and image quality metrics.

## Key Results
- Janus-Pro exhibits the highest gender bias (0.90) among benchmarked models, compared to Stable Diffusion's 0.67
- The proposed method reduces gender bias in VILA-U by 71.91% (from 0.89 to 0.25)
- Intersectional bias is reduced to 0.15 in the treated model
- Image quality metrics improve alongside bias reduction

## Why This Works (Mechanism)
The bias mitigation works by addressing the root cause of bias in U-MLLMs through targeted intervention in the language model component. By finetuning on balanced synthetic data, the model learns to generate diverse representations across demographic groups. The balanced preference loss then reinforces this learning by explicitly minimizing preference differences, creating a feedback loop that continuously adjusts the model's outputs to be more equitable across groups.

## Foundational Learning
- **U-MLLMs**: Unified Multimodal Large Language Models combine language understanding with visual processing in a single architecture. *Why needed*: To understand the target models being evaluated and improved. *Quick check*: Review the transformer architecture and how it handles both text and image inputs.
- **Diffusion Models**: Generative models that create images through a stepwise denoising process. *Why needed*: Used to generate the balanced synthetic training data. *Quick check*: Understand the denoising process and how it can be controlled for demographic attributes.
- **Preference Loss**: A training objective that aligns model outputs with human preferences. *Why needed*: The balanced preference loss is central to the bias mitigation approach. *Quick check*: Review how preference learning differs from standard supervised learning.

## Architecture Onboarding

**Component Map**: Language Model <-> Vision Encoder <-> Image Generation Module

**Critical Path**: Prompt Input -> Language Model Processing -> Vision Encoder Interpretation -> Image Generation

**Design Tradeoffs**: The approach prioritizes bias reduction over maintaining raw performance, accepting potential changes to model behavior in exchange for fairness improvements. The use of synthetic data introduces potential artifacts but enables controlled bias mitigation.

**Failure Signatures**: If bias reduction is accompanied by significant quality degradation or if the balanced preference loss causes mode collapse (generating overly similar images across prompts).

**3 First Experiments**:
1. Test bias reduction on a held-out demographic group not used in training
2. Measure semantic drift by comparing outputs before and after treatment
3. Evaluate model stability under adversarial prompting designed to expose bias

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis focuses primarily on gender and race, potentially overlooking other critical dimensions like age, disability status, and body size
- The benchmarking relies on a subset of available U-MLLMs, with some models not fully open-sourced
- The synthetic data generation approach may introduce uncharacterized artifacts or limitations

## Confidence

**High Confidence**: U-MLLMs exhibit significant gender and race bias with measurable disparities across models; language models contribute more to bias than vision encoders.

**Medium Confidence**: The two-stage bias mitigation approach effectively reduces bias in controlled benchmarks, though real-world deployment validation is limited.

**Low Confidence**: Claims about preserved semantic fidelity during bias reduction, as extensive evaluation of non-targeted quality aspects is lacking.

## Next Checks
1. **Intersectional Bias Validation**: Test the bias mitigation approach on a broader range of intersectional identities beyond binary gender to assess generalizability.
2. **Long-term Stability Analysis**: Evaluate whether bias reduction remains stable over time and across different prompt distributions, including edge cases.
3. **Real-world Deployment Assessment**: Conduct user studies or application-specific evaluations to verify bias reduction translates to meaningful improvements in real-world use cases.