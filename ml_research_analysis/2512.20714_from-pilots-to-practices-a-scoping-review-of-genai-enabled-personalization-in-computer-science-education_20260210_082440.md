---
ver: rpa2
title: 'From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization
  in Computer Science Education'
arxiv_id: '2512.20714'
source_url: https://arxiv.org/abs/2512.20714
tags:
- learning
- feedback
- education
- https
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This scoping review of 32 studies (2023\u20132025) synthesizes\
  \ how generative AI enables personalized computer science education. It identifies\
  \ five application domains: intelligent tutoring, personalized materials, formative\
  \ feedback, AI-augmented assessment, and code review."
---

# From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education

## Quick Facts
- arXiv ID: 2512.20714
- Source URL: https://arxiv.org/abs/2512.20714
- Reference count: 0
- Synthesizes how generative AI enables personalized computer science education across five application domains

## Executive Summary
This scoping review examines 32 studies from 2023-2025 to understand how generative AI enables personalized computer science education. The review identifies five key application domains: intelligent tutoring systems, personalized learning materials, formative feedback generation, AI-augmented assessment, and code review. The findings reveal that structured, constraint-based AI interactions consistently produce more positive learning outcomes than unconstrained chat interfaces. The evidence suggests generative AI can provide precision scaffolding when embedded in audit-ready workflows that maintain productive struggle while scaling personalized support.

## Method Summary
The scoping review analyzed 32 studies published between 2023 and 2025, focusing on generative AI applications in computer science education. The review synthesized findings across multiple application domains, examining design patterns, implementation approaches, and learning outcomes. Studies were evaluated for their methodological rigor, implementation details, and evidence of educational impact. The review particularly focused on identifying which design approaches consistently showed positive results across different contexts.

## Key Results
- Five application domains identified: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review
- Structured AI interactions with explanation-first guidance and solution withholding show better learning outcomes than open-ended chat
- Graduated hint ladders and artifact grounding are key design patterns for effective personalized support
- Evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows

## Why This Works (Mechanism)
Generative AI enables personalized scaffolding by dynamically adapting to individual learner needs while maintaining appropriate challenge levels. The mechanism works through constraint-based interactions that guide learners through productive struggle rather than providing immediate solutions. This approach preserves the cognitive benefits of problem-solving while scaling personalized support that would be impossible for human instructors alone. The audit-ready workflows ensure reliability and pedagogical effectiveness while allowing for systematic improvement of AI-generated content.

## Foundational Learning
- Productive struggle concept: why needed - maintains cognitive benefits of problem-solving; quick check - observe whether students develop deeper understanding through guided difficulty
- Constraint-based learning design: why needed - prevents over-reliance on AI solutions; quick check - measure student dependency on AI versus independent problem-solving
- Graduated scaffolding: why needed - provides appropriate support at each learning stage; quick check - track student progress through hint levels
- Artifact grounding: why needed - ensures AI responses are based on verifiable sources; quick check - audit trails of AI-generated content sources
- Formative feedback loops: why needed - enables continuous adaptation to learner needs; quick check - measure improvement in learning outcomes over time
- Audit-ready workflows: why needed - ensures reliability and fairness in AI-assisted learning; quick check - systematic evaluation of AI-generated content quality

## Architecture Onboarding
Component map: Student -> Learning Interface -> AI Engine -> Constraint Manager -> Content Repository -> Assessment Engine
Critical path: Student interaction flows through interface, processed by AI engine with constraint management, retrieving content from repository, generating personalized responses, and feeding assessment data back for continuous improvement
Design tradeoffs: Open-ended chat provides flexibility but reduces learning gains; constraint-based systems improve outcomes but may feel restrictive; audit-ready workflows ensure quality but add implementation complexity
Failure signatures: Over-reliance on AI solutions, inconsistent feedback quality, lack of appropriate challenge levels, insufficient grounding in educational artifacts
First experiments: 1) Compare constraint-based tutoring vs. open chat interfaces on learning outcomes; 2) Test graduated hint ladder effectiveness across different CS topics; 3) Evaluate audit-ready workflow impact on content quality and reliability

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Limited to 32 studies from 2023-2025, representing a narrow temporal window
- Many studies conducted in controlled settings rather than authentic classrooms
- Focus exclusively on English-language publications, potentially missing non-English research
- Findings may not generalize to longer-term outcomes or different CS contexts

## Confidence
High confidence: Identification of five application domains and general finding that structured AI interactions outperform open-ended chat interfaces
Medium confidence: Specific design patterns (explanation-first, solution withholding, graduated hints, artifact grounding) show promise but may vary by learner characteristics and CS topics
Low confidence: Claims about precision scaffolding and scaling personalized support require more longitudinal validation across diverse learner populations

## Next Checks
1. Conduct replication studies in authentic classroom settings across diverse CS courses and institutions to validate findings outside controlled environments
2. Implement A/B testing comparing constraint-based AI tutoring systems against traditional instruction and unconstrained chat interfaces across multiple semesters to measure learning gains and student engagement
3. Develop and test frameworks for auditing AI-generated educational content and interactions to ensure reliability, fairness, and pedagogical effectiveness before scaling deployment