---
ver: rpa2
title: 'Ariel-ML: Computing Parallelization with Embedded Rust for Neural Networks
  on Heterogeneous Multi-core Microcontrollers'
arxiv_id: '2512.09800'
source_url: https://arxiv.org/abs/2512.09800
tags:
- iree
- ariel-ml
- embedded
- tinyml
- rust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Ariel-ML, a novel embedded Rust platform
  for executing TinyML models on heterogeneous multi-core microcontrollers. The key
  innovation is integrating IREE-based compilation with a greedy multicore scheduler,
  enabling parallelized inference on ARM Cortex-M, RISC-V, and ESP32 architectures.
---

# Ariel-ML: Computing Parallelization with Embedded Rust for Neural Networks on Heterogeneous Multi-core Microcontrollers

## Quick Facts
- arXiv ID: 2512.09800
- Source URL: https://arxiv.org/abs/2512.09800
- Reference count: 40
- Primary result: Up to 200% inference speedup on multi-core MCUs vs C/C++ baselines

## Executive Summary
Ariel-ML introduces a novel embedded Rust platform for executing TinyML models on heterogeneous multi-core microcontrollers. The system integrates IREE-based compilation with a greedy multicore scheduler, enabling parallelized inference across ARM Cortex-M, RISC-V, and ESP32 architectures. By packaging models into IREE modules with VM bytecode and parallel tile configurations, Ariel-ML achieves up to 200% performance improvements over existing C/C++-based solutions like RIOT-ML while maintaining comparable memory footprints.

## Method Summary
Ariel-ML builds a co-compilation pipeline where IREE compiles models to VM bytecode with tiled work items, metadata specifies core count and entry points, and the build system packages this with a Rust application and Ariel OS into a firmware image. At runtime, the greedy scheduler dispatches conflict-free work items to available cores. The platform supports models from TensorFlow, PyTorch, and ONNX through the model zoo, with quantized LeNet-5 serving as the primary benchmark.

## Key Results
- Single-core inference: 63.721ms (RP2040) vs 70.117ms (RIOT-ML) baseline
- Dual-core speedup: 1.5× improvement on RP2040 (31.543ms vs 46.757ms single-core)
- Memory footprint: 42.844kB RAM and 172.204kB Flash on RP2040, 1.1-2.5× larger than RIOT-ML

## Why This Works (Mechanism)

### Mechanism 1
IREE-based compilation accelerates inference compared to uTVM-based pipelines. The IREE compiler applies modern lowering pipelines and target-specific optimizations via its embedded LLVM backend, producing more efficient operator implementations than uTVM. Model weights, operator implementations, and parallel tile configurations are packaged into an IREE module with VM bytecode describing dataflow and invocation sequences.

### Mechanism 2
Greedy multicore scheduling extracts near-hardware-limit speedup on multi-core MCUs. During compilation, operators are partitioned into contention-free tiles ("work items"). At runtime, a greedy scheduler continuously pops work items from a workload queue and dispatches each to any available core until the queue empties. Because tiles are conflict-free, synchronization overhead is minimized.

### Mechanism 3
Rust-based runtime achieves comparable memory footprint to C/C++ while providing safety benefits. Ariel-ML core wraps the C-based IREE runtime in Rust bindings, leveraging Rust's ownership model for application-level safety. The IREE runtime itself (9% of RAM) requires a 16kB stack vs 2kB for RIOT-ML due to deep VM call chains—this is the primary memory overhead source, not Rust itself.

## Foundational Learning

- **MLIR/IREE Compilation Pipeline**: Why needed: Ariel-ML relies on IREE's multi-level intermediate representation to lower models from TensorFlow/PyTorch to MCU-specific machine code with parallel tile configurations. Quick check: Can you explain how IREE's HAL abstraction enables the same model artifact to run on Cortex-M and RISC-V without redesign?

- **Work-Item Tiling and Contention-Free Scheduling**: Why needed: The greedy scheduler assumes work items are conflict-free—understanding how operators are partitioned into parallel tiles is essential for debugging load imbalance. Quick check: Given a matrix-vector multiplication, how would you decompose it into independent work items across two cores?

- **Embedded Rust Memory Model**: Why needed: Ariel-ML's safety claims rest on Rust's ownership/borrowing; understanding stack vs. heap allocation in `no_std` contexts is critical for memory budgeting. Quick check: Why does the IREE runtime require a 16kB stack in Ariel-ML versus 2kB in RIOT-ML, and where is this configured?

## Architecture Onboarding

- **Component map**: Host-side build system (IREE compiler + metadata constructor + cross-compile toolchain) -> produces firmware image; Device-side runtime (Ariel-ML core (Rust bindings + profiler + greedy scheduler) -> IREE runtime (C library) -> Ariel OS (multicore threading, heap allocator)

- **Critical path**: 1. Model compilation: Target MCU info -> IREE compiler -> IREE module (weights + operators + VM bytecode); 2. Metadata construction: Core count + entry points encoded; 3. Co-compilation: IREE module + Rust application + Ariel OS -> firmware image -> flash to device; 4. Runtime: IREE VM interprets bytecode -> command buffer -> executor decodes work items -> greedy scheduler dispatches to cores

- **Design tradeoffs**: IREE vs uTVM: IREE offers faster inference but larger Flash/RAM footprint (1.1-2.5×); Rust vs C: Safety and modern tooling vs FFI complexity linking Rust to C-based IREE runtime; Greedy vs sophisticated scheduling: Low overhead and simplicity vs potential suboptimal load balancing on heterogeneous cores

- **Failure signatures**: Stack overflow on devices with <16kB available RAM (IREE VM deep call chain); Bus contention on RP2040 limiting speedup to ~1.5× regardless of scheduler improvements; Flash exhaustion on sub-256kB devices due to IREE runtime size (32% of Flash footprint)

- **First 3 experiments**: 1. Baseline single-core comparison: Deploy LeNet-5 on nRF52840 using both RIOT-ML and Ariel-ML; measure inference latency, RAM, and Flash to reproduce Table 1-3 results; 2. Multicore scaling test: On RP2040, toggle single-core vs dual-core mode in Ariel-ML; verify 1.5× speedup and identify any bus contention bottlenecks via built-in profiler; 3. Memory budget analysis: Enable Ariel-ML profiler to capture per-operator heap/stack usage; identify which operators dominate RAM and whether model quantization reduces footprint to fit sub-100kB targets

## Open Questions the Paper Calls Out

### Open Question 1
Can the IREE runtime memory footprint be sufficiently reduced through feature-tailored build profiles or Rust-native re-implementations to align with strict microcontroller constraints? Basis: Section 6 explicitly states "Future work should explore more feature-tailored build profiles, and Rust-native re-implementations of core VM components to substantially shrink the runtime." Unresolved because the current IREE runtime is identified as the primary source of RAM and Flash overhead. Evidence needed: A Rust-native runtime variant demonstrating significant reduction in heap/stack usage and binary size while maintaining inference latency.

### Open Question 2
Can Ariel-ML be extended to support on-device training or continuous learning via a lightweight backpropagation mechanism? Basis: Section 6 notes "One perspective is to enhance Ariel-ML with a lightweight backpropagation mechanism, adapted IREE compilation and scheduling workflow." Unresolved because the current platform targets inference execution only; the compilation and scheduling workflows do not support backward passes or gradient updates. Evidence needed: Implementation of backpropagation support and benchmarks showing on-device fine-tuning convergence without exceeding MCU resource limits.

### Open Question 3
How can Ariel-ML's modularity be leveraged to create a secure and efficient pipeline for Over-the-Air (OTA) model updates? Basis: The authors highlight that "Developing a secure OTA pipeline that leverages this modularity will be essential to maintain model quality... on fleets of device in the field." Unresolved because while IREE modules are self-contained, the system currently lacks specific software mechanisms to securely deliver and install differential updates (weights or operators). Evidence needed: A functional OTA update mechanism that securely patches deployed models with minimal downtime and bandwidth.

## Limitations
- Memory overhead: IREE runtime requires 16kB stack vs 2kB for RIOT-ML, creating constraints for sub-100kB devices
- Limited model support: Currently optimized for quantized LeNet-5; scalability to larger models untested
- Hardware specificity: Performance claims tied to specific MCU architectures; generalization to other heterogeneous cores unverified

## Confidence

- **High confidence**: IREE-based compilation provides measurable inference speedup (63.721ms → 46.757ms on RP2040); empirical data from Table 1 is concrete and reproducible
- **Medium confidence**: Rust-based runtime achieves comparable memory footprint to C/C++ alternatives; while RAM/Flash measurements are provided, the comparison methodology assumes equivalent optimization levels and doesn't account for potential Rust-specific overhead
- **Medium confidence**: Greedy multicore scheduling extracts near-hardware-limit speedup; the 1.5× improvement is demonstrated, but the claim of approaching hardware limits is based on theoretical arguments rather than systematic characterization across workloads

## Next Checks

1. **Cross-architecture scalability**: Deploy Ariel-ML on ESP32-S3 dual-core (300MHz) and measure whether the 200% performance claim holds relative to single-core baselines, or if bus contention limits differ from RP2040

2. **Memory-constrained deployment**: Quantize LeNet-5 to 4-bit weights/activations and verify whether Ariel-ML can operate within 64kB Flash constraint while maintaining inference accuracy above 95%

3. **Scheduling robustness**: Implement a round-robin scheduler as alternative to greedy approach; measure load imbalance and synchronization overhead on RP2040 when processing operator chains with heterogeneous computational complexity