---
ver: rpa2
title: Can the capability of Large Language Models be described by human ability?
  A Meta Study
arxiv_id: '2504.12332'
source_url: https://arxiv.org/abs/2504.12332
tags:
- evaluation
- llms
- arxiv
- benchmarks
- capabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores whether the capabilities of large language
  models (LLMs) can be described by human ability metrics. The authors collected performance
  data from over 80 models across 37 evaluation benchmarks, categorized into 6 primary
  abilities and 11 sub-abilities in human terms.
---

# Can the capability of Large Language Models be described by human ability? A Meta Study

## Quick Facts
- arXiv ID: 2504.12332
- Source URL: https://arxiv.org/abs/2504.12332
- Reference count: 10
- Primary result: LLM capabilities under 10B parameters can be partially described using human ability metrics, while some abilities like summarization remain unstable

## Executive Summary
This meta-study investigates whether LLM capabilities can be meaningfully described using human ability taxonomies. The authors analyze performance data from over 80 models across 37 benchmarks, clustering them based on ranking consistency using Spearman's correlation. They find that while some LLM abilities align with human-defined categories, others diverge significantly, and capability structures shift with model scale. The study reveals that abilities considered interrelated in humans often appear nearly uncorrelated in LLMs.

## Method Summary
The authors collected performance scores from 52 models under 10B parameters and 30 models between 10-20B parameters across 37 evaluation benchmarks. They converted raw scores to rankings within each benchmark, then computed pairwise Spearman's rank correlation coefficients to measure benchmark similarity. Using hierarchical agglomerative clustering with average-linkage on the distance-transformed correlation matrix, they grouped benchmarks into clusters representing coherent abilities. These clusters were compared against pre-defined human ability categories (6 primary abilities, 11 sub-abilities) to assess alignment.

## Key Results
- Some LLM capabilities under 10B parameters can be described using human ability metrics
- Summarization and common-sense reasoning show unstable performance across benchmarks
- Abilities considered interrelated in humans (understanding, analysis, summarization) appear nearly uncorrelated in LLMs
- Capability clustering patterns shift significantly with model scale

## Why This Works (Mechanism)

### Mechanism 1
Stable ranking patterns across benchmarks indicate coherent, measurable abilities. The authors apply Spearman's rank correlation to model performance across benchmarks, clustering those with consistent rankings. If Model A consistently outranks Model B across multiple benchmarks, those benchmarks are inferred to measure the same underlying ability. Core assumption: Consistent rankings imply a shared latent ability rather than benchmark-specific artifacts.

### Mechanism 2
Human-defined ability categories only partially align with LLM capability structures. The authors compare clustering results (data-driven) with pre-defined human ability categories (theory-driven). For example, "understanding" and "analysis" are interrelated in humans but form separate clusters in LLMs. Core assumption: The 6 primary abilities and 11 sub-abilities from human psychology literature are appropriate reference categories for LLM evaluation.

### Mechanism 3
Capability stability and clustering patterns shift with model scale. Separate clustering analyses for <10B and 10-20B parameter models reveal different structures. In 10-20B models, more benchmarks converge into larger clusters, and knowledge application benchmarks that were separate in <10B models cluster together at larger scales. Core assumption: Observed clustering changes reflect genuine capability emergence rather than sampling noise.

## Foundational Learning

- **Spearman's rank correlation coefficient**
  - Why needed here: Core metric for measuring benchmark relationships without relying on absolute scores that vary across evaluation systems
  - Quick check question: Given two model ranking lists across benchmarks, can you compute rs and interpret whether |rs| ≥ 0.7 indicates strong correlation?

- **Hierarchical clustering with average-linkage agglomeration**
  - Why needed here: Method for grouping benchmarks without pre-specifying cluster count; robust to non-convex data structures
  - Quick check question: Why would K-means be inappropriate when cluster count is unknown and data may be non-convex?

- **Scaling laws for language models**
  - Why needed here: Provides theoretical grounding for why capabilities change with parameter count; contextualizes emergence findings
  - Quick check question: What performance pattern would you expect when comparing 7B vs. 13B models on the same benchmark set?

## Architecture Onboarding

- **Component map:**
  Data layer (80+ models × 37 benchmarks score matrix) -> Ranking layer (convert scores to per-benchmark rankings) -> Correlation layer (compute pairwise Spearman rs) -> Distance transformation (ds = √(2(1−rs))) -> Clustering layer (hierarchical agglomerative clustering) -> Validation layer (compare to human ability categories)

- **Critical path:**
  1. Collect complete benchmark scores for ≥50 models in a parameter band
  2. Ensure each benchmark has ≥2 evaluations per hypothesized sub-ability
  3. Verify within-cluster correlation coefficients after clustering

- **Design tradeoffs:**
  - Ranking vs. raw scores: Ranking ignores score magnitude; appropriate when benchmarks have incomparable scales but may lose signal
  - <10B vs. 10-20B separation: Controls for scale effects but limits sample size; authors note >20B analysis is underpowered
  - 6 abilities / 11 sub-abilities taxonomy: Human-interpretable but may not reflect LLM internal structure

- **Failure signatures:**
  - Unstable abilities (e.g., summarization, common-sense reasoning) show low within-cluster rs (< 0.3)
  - Single benchmarks isolated in their own clusters indicate no correlated ability
  - Cluster membership changes drastically between scale bands

- **First 3 experiments:**
  1. Replicate clustering on a held-out set of 20 models <10B to verify cluster stability
  2. Add 3 new summarization benchmarks to test whether the "summarization is unstable" finding holds or was benchmark-specific
  3. Map within-cluster correlation strength to benchmark contamination scores to rule out memorization as confound

## Open Questions the Paper Calls Out

### Open Question 1
Can the capability clustering patterns observed in models under 20 billion parameters be generalized to ultra-large models exceeding 50 billion parameters? The authors state there are too few models with >50B parameters to support ranking-based clustering research, making it difficult to obtain stable results for ultra-large-scale models.

### Open Question 2
Do capabilities found to be "unstable" in smaller models (specifically summarization and common-sense reasoning) become statistically stable as model scale increases? The paper establishes instability at the <10B scale but does not verify if these specific previously-unstable capabilities solidified into reliable "abilities" in the 10-20B range.

### Open Question 3
Why do abilities that are cognitively interrelated in humans (such as text understanding and summarization) appear decoupled or uncorrelated in Large Language Models? This is a phenomenological finding; the paper identifies that the discrepancy exists but does not investigate the architectural or training data causes for this decoupling.

## Limitations
- Benchmark contamination risk may confound correlation-based clustering results
- Human ability taxonomy may not adequately capture LLM internal processing structures
- Limited model availability for >20B parameter range constrains scale-dependent analysis

## Confidence

- **High confidence**: <10B parameter models can be partially described by human ability metrics; summarization and common-sense reasoning show instability
- **Medium confidence**: Capability structures differ between <10B and 10-20B models; human-interrelated abilities appear uncorrelated in LLMs
- **Low confidence**: Claims about capability convergence at larger scales; exact mapping between human and LLM ability structures

## Next Checks

1. Apply contamination detection metrics to each benchmark and re-cluster to test whether unstable abilities correlate with high contamination scores
2. Cross-validate clustering results using alternative distance metrics (e.g., Kendall's tau) to ensure robustness to ranking method choice
3. Test taxonomy alignment by applying the same clustering methodology to human cognitive test batteries and comparing structure to LLM results