---
ver: rpa2
title: Learning Representational Disparities
arxiv_id: '2505.17533'
source_url: https://arxiv.org/abs/2505.17533
tags:
- loss
- logit
- biasr
- disparity
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a fair machine learning algorithm to model interpretable
  differences between observed and desired human decision-making, with the goal of
  reducing disparity in downstream outcomes. The method learns a shallow neural network
  where a portion of the hidden layer models representational disparities - the differences
  in input representations seen by observed and desired decision-makers.
---

# Learning Representational Disparities

## Quick Facts
- **arXiv ID:** 2505.17533
- **Source URL:** https://arxiv.org/abs/2505.17533
- **Authors:** Pavan Ravishankar; Rushabh Shah; Daniel B. Neill
- **Reference count:** 40
- **Primary result:** Proposed method achieves substantially reduced disparity compared to LFR while maintaining higher accuracy and greater consistency in recommendations.

## Executive Summary
This paper proposes a fair machine learning algorithm that learns interpretable differences between observed and desired human decision-making to reduce outcome disparities. The method uses a shallow neural network architecture that partitions hidden representation nodes between observed and desired decision-makers, creating a "difference engine" that models representational disparities. Under simplifying assumptions, the authors prove that the learned weights are interpretable and can fully mitigate outcome disparity. Experiments on German Credit, Adult, and Heritage Health datasets demonstrate that the proposed method achieves substantially reduced disparity compared to a competing fair representation learning approach (LFR), while also achieving higher accuracy and greater consistency in its recommendations.

## Method Summary
The method learns a shallow neural network where representation nodes are partitioned between observed and desired decision-makers. Nodes R₁ to Rₘ′ are used by the observed decision-maker, while nodes Rₘ′₊₁ to Rₘ capture representational disparities. The network is trained using a multi-objective loss function that heavily weights reconstruction terms (C, D) to anchor the model to observed reality while disparity minimization (A) and L1 regularization (B) compete to find minimal corrective interventions. The method operates in two phases: first training on reconstruction objectives only to freeze observed decision and outcome process weights, then training with full loss across 100 random initializations to select the best fit.

## Key Results
- LRD achieves substantially reduced outcome disparity compared to LFR across all three datasets
- LRD maintains higher accuracy than LFR while reducing disparity
- LRD demonstrates greater consistency in recommendations across train-test splits
- The learned weights are interpretable under simplifying assumptions, showing clear compensation for observed discrimination

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partitioning hidden representation nodes between observed and desired decision-makers isolates the causal pathway through which outcome disparities emerge.
- Mechanism: The architecture dedicates nodes R₁ to Rₘ′ to the observed decision-maker and nodes Rₘ′₊₁ to Rₘ exclusively to representational disparities. This creates a "difference engine" where disparity nodes receive input from sensitive attribute S and non-sensitive attributes X, then inject corrective signals into H via outgoing weights. The ReLU activation ensures only activating disparities contribute, making attribution traceable.
- Core assumption: Observed and desired decision-makers differ primarily in their internal representations of the same inputs, not in the decision function itself.
- Evidence anchors: [section 3] "We assume that the former uses only a subset of the representation nodes, R₁ to Rₘ′, to decide H, while the latter uses all of the representation nodes R₁ to Rₘ to decide H. Thus, nodes Rₘ′₊₁ to Rₘ capture the representational disparities."
- Break condition: If observed and desired decision-makers use fundamentally different decision functions, the partitioning assumption fails.

### Mechanism 2
- Claim: Multi-objective loss with heavily weighted reconstruction terms anchors the model to observed reality while disparity minimization and L1 regularization compete to find minimal corrective interventions.
- Mechanism: The loss function aA + bB + cC + dD with c = d >> a forces the network to first learn accurate models of the observed decision process Pr(H|S,X) and outcome process Pr(Y|S,X,H). With these anchored, the disparity nodes can only modify decisions through the representational disparity pathway. L1 regularization on disparity node weights encourages sparsity, ensuring only necessary corrections survive.
- Core assumption: The outcome process Pr(Y|S,X,H) is fixed and can be learned from historical data; only the human decision H can be modified to compensate.
- Evidence anchors: [section 3] "Since the desired decision-maker uses additional representation nodes along with the nodes used by the observed decision-maker, it is imperative to accurately learn the weights corresponding to the observed decision-maker to precisely interpret the representational disparity."
- Break condition: If outcome process changes when decisions change, the fixed Pr(Y|H,S,X) assumption fails.

### Mechanism 3
- Claim: Theoretical guarantees under simplifying assumptions ensure interpretable weights that provably eliminate outcome disparity, with experimental validation showing robust performance when assumptions are relaxed.
- Mechanism: Theorems 4.1-4.3 prove that under assumptions (A1-A6), gradient descent converges to global optima where representational disparity weights have clear interpretations. The proofs exploit convexity in restricted weight regions when a >> b. Multiple random initializations recover these optima in practice.
- Core assumption: Simplifying assumptions including independence of S and X, conditional independence of Y given H, and disparity loss substantially outweighing interpretability loss.
- Evidence anchors: [section 4] "Under reasonable simplifying assumptions, we prove that our neural network model of the representational disparity learns interpretable weights that fully mitigate the outcome disparity."
- Break condition: When assumptions are violated, convergence guarantees disappear.

## Foundational Learning

- **Demographic Parity / Outcome Disparity**
  - Why needed here: Objective A directly minimizes |Pr(Y=1|S=1) - Pr(Y=1|S=0)|, a demographic parity notion extended to downstream outcomes rather than decisions. Understanding this fairness metric is essential to interpret what the model optimizes.
  - Quick check question: If Pr(Y=1|S=1) = 0.4 and Pr(Y=1|S=0) = 0.6, what is the outcome disparity the model seeks to eliminate?

- **Multi-Objective Optimization with Loss Weighting**
  - Why needed here: The total loss aA + bB + cC + dD with c >> a, c = d, b = 1-a requires understanding how to balance competing objectives and why heavy weighting on reconstruction enables interpretability.
  - Quick check question: Why would setting c = a (equal weight on disparity and reconstruction) likely produce less interpretable results?

- **L1 Regularization and Sparsity**
  - Why needed here: Objective B uses L1 regularization on disparity node weights to encourage sparsity, directly enabling interpretability by zeroing unnecessary connections.
  - Quick check question: If all disparity node weights remain non-zero after training with high L1 penalty, what might this indicate about the problem structure?

## Architecture Onboarding

- **Component map**: Input layer (S, X) -> Hidden layer R (R₁...Rₘ′ for observed, Rₘ′₊₁...Rₘ for disparities) -> Decision layer H (sigmoid) -> Outcome layer Y (sigmoid)
- **Critical path**: 1) Train on C and D only to freeze observed and outcome weights; 2) Initialize disparity node weights in feasible regions; 3) Train with full loss using Adam optimizer; 4) Run 100 fits with different random seeds; 5) Extract interpretability from non-zero disparity node weights
- **Design tradeoffs**: Higher a (disparity weight) → more fairness but potentially less stability; more disparity nodes → theoretically single node suffices but multiple may help with complex X; choice of m′ (observed decision nodes) requires cross-validation
- **Failure signatures**: Disparity loss A remains high → likely initialization outside feasible regions; all disparity node weights near zero but disparity persists → assumptions violated; high variance in corrections → model selection unstable; reconstruction loss high → increase c and d weights
- **First 3 experiments**:
  1. **Synthetic validation with known δ**: Generate data satisfying Theorem 4.2 assumptions (S⊥X, Y⊥S|H). Train with a=0.999, b=0.001. Verify that w_S_R' · w ≈ δ and only one disparity node activates.
  2. **Sensitivity analysis on hyperparameter a**: Train synthetic data with a ∈ {0.1, 0.5, 0.9, 0.99, 0.999}. Plot losses A, B, C, D vs. a. Confirm that high a eliminates disparity while C, D remain near optimal.
  3. **Real dataset with semi-synthetic outcomes**: Take German Credit, use class as H, generate Y with known bias structure (Cases I-V from paper). Compare LRD vs. LFR on accuracy, disparity reduction, and consistency.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely heavily on simplifying assumptions (A1-A6) that rarely hold in practice
- Two-phase training assumes observed and outcome processes can be learned independently of disparity correction
- Proof strategy for Theorem 4.3 only shows at most two local minima exist for a = b, not that gradient descent finds them
- Limited empirical validation of consistency claims across diverse datasets and parameter settings

## Confidence

| Claim | Confidence |
|-------|------------|
| Partitioning representation nodes enables interpretable disparity correction | High |
| Multi-objective loss structure with heavy reconstruction weighting enables interpretability | High |
| Theoretical guarantees under simplifying assumptions are mathematically sound | Medium |
| Method consistently outperforms LFR in real-world settings | Low |

## Next Checks

1. **Assumption violation analysis:** Systematically test how violations of assumptions A1-A6 affect convergence and interpretability. Compare performance across German, Adult, and Health datasets to identify which assumptions are most fragile.

2. **Cross-dataset generalization:** Reproduce experiments on additional datasets with different sensitive attribute types and outcome structures to validate robustness beyond the three datasets presented.

3. **Consistency measurement validation:** Verify the consistency measure calculation and compare LRD vs. LFR consistency across multiple train-test splits and random seeds. The paper claims LRD shows "greater consistency" but provides limited distributional evidence for this assertion.