---
ver: rpa2
title: 'DrivingGen: A Comprehensive Benchmark for Generative Video World Models in
  Autonomous Driving'
arxiv_id: '2601.01528'
source_url: https://arxiv.org/abs/2601.01528
tags:
- driving
- video
- arxiv
- world
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DrivingGen introduces a comprehensive benchmark for generative
  world models in autonomous driving, addressing the need for rigorous evaluation
  of visual realism, trajectory plausibility, temporal coherence, and controllability.
  The benchmark includes a diverse dataset spanning varied weather, time of day, geographic
  regions, and complex driving maneuvers, alongside novel metrics that jointly assess
  video and trajectory quality.
---

# DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving

## Quick Facts
- arXiv ID: 2601.01528
- Source URL: https://arxiv.org/abs/2601.01528
- Reference count: 40
- Introduces comprehensive benchmark for generative world models in autonomous driving with 14 evaluated models and 11 metrics across 4 evaluation dimensions

## Executive Summary
DrivingGen establishes the first comprehensive benchmark for evaluating generative video world models in autonomous driving, addressing the critical need for rigorous assessment of both visual realism and physical plausibility. The benchmark combines a diverse dataset spanning varied weather, time of day, geographic regions, and complex driving maneuvers with a novel suite of metrics including distribution-level FVD/FTD, perceptual quality, driving-specific imaging quality, temporal consistency at scene and agent levels, and trajectory alignment (ADE/DTW). Evaluation of 14 state-of-the-art models reveals fundamental trade-offs: general video models excel visually but break physics, while driving-specific models capture motion realistically but lag in image fidelity.

## Method Summary
DrivingGen evaluates generative world models through a four-dimensional framework: distribution metrics (FVD for video, FTD for trajectories), quality metrics (CLIP-IQA+ for perceptual quality, MMP for driving-specific imaging), temporal consistency metrics (scene-level, agent-level, and trajectory-level), and trajectory alignment metrics (ADE, DTW). The benchmark uses a 400-sample dataset (200 open-domain, 200 ego-conditioned) covering diverse weather, time, regions, and maneuvers. A robust trajectory reconstruction pipeline with failure handling ensures 100% coverage. Models are evaluated across general, physics-based, and driving-specific categories.

## Key Results
- General models (Wan2.2, Kling, Gen-3) achieve superior visual quality but fail on trajectory fidelity, breaking physical plausibility
- Driving-specific models (Vista, GEM, dVAE) capture motion realistically but produce less visually appealing outputs
- No single model excels across all four evaluation dimensions simultaneously
- Models consistently fail to follow ego-trajectory conditioning despite showing improvements

## Why This Works (Mechanism)

### Mechanism 1: Multi-dimensional Evaluation Framework
- Claim: A comprehensive evaluation framework that jointly assesses visual realism, trajectory plausibility, temporal coherence, and controllability can reveal hidden failure modes in driving world models that single-metric approaches miss.
- Mechanism: The benchmark uses a 4-dimensional evaluation approach combining distribution metrics, quality metrics, temporal consistency metrics, and trajectory alignment metrics. This exposes trade-offs where models excel in one dimension but fail in others.
- Core assumption: Driving world models require both visual fidelity AND physical plausibility; optimizing for one dimension alone is insufficient for safe deployment.
- Evidence anchors: The abstract states the benchmark "jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability" and exposes "failure modes hidden from prior single metric."

### Mechanism 2: Diverse Dataset Construction with Controlled Distributions
- Claim: A balanced dataset covering diverse weather, time of day, geographic regions, and complex driving maneuvers enables more robust evaluation of generative models' generalization capabilities.
- Mechanism: Two-track dataset construction: Open-Domain Track with Internet-sourced data controlling for weather (<60% normal), time (50% night/sunset/sunrise), and geographic coverage (7 regions), plus Ego-Conditioned Track from 5 driving datasets covering diverse conditions.
- Core assumption: Real-world driving scenarios are highly variable, and evaluation datasets must reflect this diversity to assess deployment readiness.
- Evidence anchors: The abstract mentions "varied weather, time of day, regions, and driving maneuvers" and section 3.1 details geographic coverage across North America (20.7%), East Asia & Pacific (22.1%), Europe & Central Asia (26.6%), and Middle East & North Africa (12.1%).

### Mechanism 3: Robust Trajectory Reconstruction with Failure Handling
- Claim: A trajectory reconstruction pipeline that handles SLAM failures ensures comprehensive evaluation of all generated videos, including the most challenging cases where artifacts would cause standard pipelines to fail.
- Mechanism: Custom SLAM pipeline with PnP-based reconstruction using SIFT and RANSAC with UniDepthV2 for depth estimation, plus failure-recovery strategy: constant velocity extrapolation with random perturbations when SLAM fails, ensuring 100% reconstruction rate vs ~85% for standard pipelines.
- Core assumption: Poor-quality videos that cause SLAM failures are exactly the cases that should be evaluated, not discarded, as they represent critical failure modes.
- Evidence anchors: Section B.2 shows "successful rate increases to 20/20" with failure handling, while section 3.2 details the pipeline "guaranteeing that all videos reconstruct trajectories."

## Foundational Learning

- Concept: **Fréchet Video Distance (FVD) and Fréchet Trajectory Distance (FTD)**
  - Why needed here: These distribution-level metrics quantify how close the generated distribution is to the real data distribution, but they don't capture all aspects of quality. FVD measures video distribution similarity, while FTD uses the MTR encoder to measure trajectory distribution similarity.
  - Quick check question: Can you explain why a model might achieve good FVD scores (low distance) but poor trajectory fidelity (high ADE/DTW)?

- Concept: **Temporal Consistency in Video Generation**
  - Why needed here: Driving scenarios require coherent object persistence and smooth motion over time; inconsistency can indicate failure modes. The benchmark measures this at scene-level (DINOv3 features between adaptively downsampled frames), agent-level (tracking individual agents with SAM2), and trajectory-level (velocity/acceleration stability).
  - Quick check question: What's the difference between scene-level and agent-level temporal consistency, and why does agent-level consistency matter more for driving safety?

- Concept: **World Models as Generative Simulators**
  - Why needed here: Understanding that video generation models can serve as world models that predict future states is fundamental to this benchmark's purpose. These models imagine ego and agent futures, enabling scalable simulation and safe testing of corner cases.
  - Quick check question: How does a driving world model differ from a general video generation model in terms of evaluation priorities (hint: think about physical plausibility vs. visual appeal)?

## Architecture Onboarding

- Component map:
  Input Processing (Vision + Language + Optional Action) -> Video Generation (14 models across categories) -> Trajectory Extraction (SLAM pipeline with failure handling) -> Metric Computation (4 dimensions, 11 metrics) -> Reporting (full metric tables with average rank)

- Critical path:
  1. Load 400-sample dataset (200 open-domain, 200 ego-conditioned)
  2. Generate 100-frame videos for each sample (~20-30 min per video for large models like Wan2.2-14B)
  3. Extract trajectories using robust SLAM pipeline with failure handling
  4. Compute all 11 metrics across 4 dimensions (trajectory metrics are fast: minutes; image/video consistency takes hours)
  5. Aggregate results and compute average rank
  Expected runtime: 1-2 days on single modern GPU (video generation dominates; evaluation suite ~1-2 days for 400 videos)

- Design tradeoffs:
  - Dataset size (400 samples) vs evaluation comprehensiveness: Balanced for efficiency while maintaining meaningful diversity across weather/time/regions/maneuvers
  - Robust trajectory reconstruction (100% success) vs accuracy: Slightly higher ADE (16.84 vs 15.18 without failure handling) but ensures all videos are evaluated, including worst cases
  - Multiple transparent metrics vs single composite score: Full metric tables provided; average rank serves as quick summary but not definitive score

- Failure signatures:
  - High FVD but poor trajectory quality: Model generates visually appealing videos with unrealistic motion (common in general models)
  - Low agent disappearance score: Model creates unrealistic video with objects vanishing abruptly (non-physical disappearance)
  - High ADE/DTW with good visual quality: Model ignores ego-trajectory conditioning (trajectory alignment remains limited across all models)
  - Near-static video with artificially high temporal consistency: Model gaming the metric by minimizing motion (adaptive downsampling mitigates this)

- First 3 experiments:
  1. **Baseline comparison**: Evaluate a general video model (e.g., Wan2.2-I2V) vs a driving-specific model (e.g., Vista) across all metrics to understand the visual quality vs trajectory fidelity trade-off
  2. **Ablation on failure handling**: Compare trajectory reconstruction success rates and ADE values with/without failure handling on 20 challenging nuPlan videos to verify robustness vs accuracy trade-off
  3. **Diversity stress test**: Stratify evaluation results by weather conditions (normal vs snow/fog) and time of day (day vs night) to verify that dataset diversity reveals performance differences

## Open Questions the Paper Calls Out
None

## Limitations
- No single model excels across all four evaluation dimensions simultaneously, revealing fundamental trade-offs
- Trajectory reconstruction pipeline introduces approximation errors for challenging cases where SLAM fails
- Current 400-sample evaluation may not capture full diversity of real-world driving scenarios, particularly rare edge cases critical for safety validation

## Confidence
**High Confidence:** The multi-dimensional evaluation framework effectively reveals trade-offs between visual quality and physical plausibility. The dataset diversity metrics (coverage across weather, time, regions, maneuvers) are reliably measured and meaningfully challenge models. The trajectory reconstruction pipeline achieves consistent success rates with acceptable accuracy trade-offs.

**Medium Confidence:** The relative performance rankings between model categories (general vs. physics-based vs. driving-specific) are robust, but absolute metric values may be sensitive to implementation details. The temporal consistency metrics, while novel, may not fully capture all aspects of temporal coherence relevant to driving safety.

**Low Confidence:** The relationship between benchmark performance and real-world deployment safety remains uncertain. The 400-sample evaluation may not adequately stress-test models against the full spectrum of rare but critical driving scenarios. Cross-dataset generalization of results is not yet validated.

## Next Checks
1. **Safety-critical scenario stress test:** Expand the evaluation to specifically target rare but safety-critical scenarios (pedestrian jaywalking, sudden vehicle braking, construction zones) to verify whether current model rankings hold under safety-critical conditions.

2. **Cross-dataset generalization:** Evaluate the same models on an independent driving dataset not used in the original benchmark to validate that performance rankings generalize across different data distributions.

3. **Real-world deployment correlation:** Conduct a study correlating benchmark performance with actual autonomous driving system performance in simulation to validate whether the benchmark metrics predict real-world safety and reliability.