---
ver: rpa2
title: 'Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial
  Understanding'
arxiv_id: '2512.06769'
source_url: https://arxiv.org/abs/2512.06769
tags:
- spatial
- image
- left
- llav
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Stitch and Tell addresses spatial hallucinations in vision-language
  models by injecting structured spatial supervision into training data. The method
  stitches two images along a spatial axis and generates spatially-aware captions
  or question-answer pairs based on the layout.
---

# Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding

## Quick Facts
- arXiv ID: 2512.06769
- Source URL: https://arxiv.org/abs/2512.06769
- Authors: Hang Yin; Xiaomin He; PeiWen Yuan; Yiwei Li; Jiayi Shi; Wenxiao Fan; Shaoxiong Feng; Kan Li
- Reference count: 40
- Key outcome: Stitching images and generating spatially-aware captions improves VLM spatial understanding without human annotation

## Executive Summary
Stitch and Tell addresses spatial hallucinations in vision-language models by introducing structured spatial supervision through data augmentation. The method stitches two images along a spatial axis and generates spatially-aware captions or question-answer pairs based on the layout, creating synthetic training data that enhances spatial understanding capabilities. This annotation-free approach avoids costly model generation or human annotation while improving performance on spatial reasoning tasks.

The method was evaluated across three model architectures (LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B, HALVA-7B), two datasets, and thirteen benchmarks, demonstrating significant improvements in spatial understanding tasks (MME-Position +5.50%, Spatial-MM +4.19%) while maintaining or improving performance on general vision-language benchmarks (COCO-QA +1.02%, MMBench +4.76%).

## Method Summary
Stitch and Tell introduces a novel data augmentation technique that addresses spatial hallucinations in vision-language models by injecting structured spatial supervision during training. The method works by stitching two images along a spatial axis (horizontal or vertical) and generating spatially-aware captions or question-answer pairs that describe the layout and spatial relationships between objects in the stitched image. This creates synthetic training data that teaches models to better understand spatial positioning and relationships without requiring additional human annotation or model-based generation. The approach is annotation-free and can be applied to any vision-language model during training to improve spatial reasoning capabilities.

## Key Results
- Spatial understanding benchmarks improved significantly: MME-Position (+5.50%) and Spatial-MM (+4.19%)
- General vision-language performance maintained or improved: COCO-QA (+1.02%), MMBench (+4.76%)
- Successful across three model architectures: LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B, HALVA-7B
- Annotation-free approach eliminates need for costly human annotation or model generation

## Why This Works (Mechanism)
The method works by leveraging structured spatial supervision through synthetic data augmentation. By physically stitching images together, the model receives explicit spatial context about object positioning and layout relationships. The generated captions or QA pairs reinforce this spatial understanding by describing where objects are located relative to each other in the stitched scene. This direct spatial supervision helps models overcome the common hallucination problem where VLMs generate incorrect spatial relationships between objects, leading to more grounded and accurate spatial reasoning capabilities.

## Foundational Learning

**Image Stitching**: Combining two images along a spatial axis to create a composite scene. *Why needed*: Creates synthetic spatial contexts for training. *Quick check*: Verify stitching produces coherent visual scenes without obvious artifacts.

**Spatial Caption Generation**: Creating descriptions that explicitly reference spatial relationships in stitched images. *Why needed*: Provides structured supervision for spatial understanding. *Quick check*: Captions should accurately describe object positions relative to stitching boundary.

**Vision-Language Model Fine-tuning**: Adapting pre-trained VLMs to improved spatial understanding through augmented data. *Why needed*: Transfers spatial knowledge to the target model. *Quick check*: Model should show improved spatial reasoning on held-out benchmarks.

**Spatial Relation Classification**: Evaluating model understanding of object positioning and relationships. *Why needed*: Measures effectiveness of spatial augmentation. *Quick check*: Improvements should be specific to spatial tasks rather than general performance gains.

## Architecture Onboarding

**Component Map**: Raw Images -> Image Stitching -> Spatial Caption Generation -> Augmented Dataset -> VLM Fine-tuning -> Spatial Understanding Model

**Critical Path**: The core pipeline follows: (1) select two images, (2) stitch along chosen axis, (3) generate spatial-aware caption/QA pair, (4) add to training data, (5) fine-tune VLM. Each step must maintain spatial coherence for the method to be effective.

**Design Tradeoffs**: The single-axis stitching approach is computationally efficient but may not capture complex spatial relationships. Annotation-free generation saves costs but relies entirely on stitching quality and caption generation accuracy.

**Failure Signatures**: Poor stitching quality (visible seams, unnatural object placement) can introduce noise rather than supervision. Generated captions that don't accurately reflect spatial relationships may confuse rather than improve spatial understanding.

**First Experiments**:
1. Test stitching on simple two-object scenes to verify spatial relationship preservation
2. Evaluate caption generation quality on stitched images before full training
3. Conduct ablation study with different stitching axes to determine optimal configuration

## Open Questions the Paper Calls Out
None

## Limitations
- Spatial improvements may not generalize to complex three-dimensional spatial reasoning or real-world scenarios with multiple interacting objects
- Single-axis stitching may be insufficient for capturing more complex spatial relationships beyond basic positioning
- Improvements on general VLM benchmarks are modest, suggesting limited impact on broader vision-language capabilities beyond spatial tasks

## Confidence

**High Confidence**: The core technical contribution of spatial stitching and caption generation is well-documented and reproducible. Experimental results showing improvements on spatial benchmarks (MME-Position +5.50%, Spatial-MM +4.19%) are clearly presented.

**Medium Confidence**: The claim of being the "first annotation-free spatial augmentation method" requires careful literature examination. Generalizability of spatial improvements to broader VLM capabilities has medium confidence due to limited evaluation scope.

**Low Confidence**: The assertion that the approach completely eliminates the need for supervision is overstated - while avoiding human annotation, it still requires model-based caption generation.

## Next Checks
1. Evaluate trained models on spatial reasoning tasks from datasets not seen during training to verify generalization beyond specific benchmarks
2. Design experiments with multi-object scenes requiring understanding of relative positions, occlusion, and depth to test complex spatial relationship generalization
3. Systematically test different stitching strategies (multiple axes, random positions, overlapping regions) to identify optimal configurations and potential failure modes