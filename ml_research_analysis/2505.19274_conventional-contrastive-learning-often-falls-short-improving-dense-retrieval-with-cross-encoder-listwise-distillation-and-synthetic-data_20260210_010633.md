---
ver: rpa2
title: 'Conventional Contrastive Learning Often Falls Short: Improving Dense Retrieval
  with Cross-Encoder Listwise Distillation and Synthetic Data'
arxiv_id: '2505.19274'
source_url: https://arxiv.org/abs/2505.19274
tags:
- queries
- training
- retrieval
- query
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that fine-tuning dense retrieval models with standard
  contrastive learning often reduces effectiveness, contrary to common assumptions.
  To address this, the authors propose combining contrastive learning with cross-encoder
  listwise distillation, which leverages richer relevance signals from a teacher model
  rather than treating relevance as binary.
---

# Conventional Contrastive Learning Often Falls Short: Improving Dense Retrieval with Cross-Encoder Listwise Distillation and Synthetic Data

## Quick Facts
- **arXiv ID**: 2505.19274
- **Source URL**: https://arxiv.org/abs/2505.19274
- **Reference count**: 22
- **Primary result**: Fine-tuning dense retrieval models with standard contrastive learning often reduces effectiveness, contrary to common assumptions

## Executive Summary
This paper demonstrates that conventional contrastive learning for fine-tuning dense retrieval models can paradoxically reduce retrieval effectiveness, even in already-supervised models. To address this, the authors propose combining contrastive learning with cross-encoder listwise distillation, which leverages richer relevance signals from a teacher model rather than treating relevance as binary. They also show that training with a diverse mix of synthetic query types (questions, claims, titles, keywords, and user search queries) outperforms single-query-type approaches and matches human-written query performance. Using this combined approach, they fine-tune a BERT-base model achieving state-of-the-art performance among BERT-based retrievers while being trained solely on synthetic queries and general-purpose corpora.

## Method Summary
The authors propose a combined training approach that addresses the limitations of standard contrastive fine-tuning for dense retrievers. They use a hybrid loss combining 0.1× contrastive loss with listwise distillation loss (KL divergence) from a cross-encoder teacher. The synthetic query generation pipeline creates six query types (questions, claims, titles, keywords, zero-shot user queries, few-shot user queries) from sampled passages using Llama-3.1 (8B). Queries are filtered to ensure the original passage ranks first after retrieval and reranking. Hard negatives are filtered using a 60% threshold based on cross-encoder scores. The model is fine-tuned with large batch sizes (4096) using GradCache optimization, with checkpoints selected based on development loss.

## Key Results
- Standard contrastive fine-tuning with InfoNCE loss consistently reduced effectiveness across BGE, GTE, and Arctic models
- Combined listwise distillation and contrastive learning achieved state-of-the-art performance among BERT-based retrievers
- Synthetic queries with diverse types (questions, claims, titles, keywords, user queries) matched or exceeded single-query-type approaches
- The approach achieved strong results on BEIR datasets and TREC DL19/DL20 without using human-written queries

## Why This Works (Mechanism)
Standard contrastive learning treats relevance as binary and optimizes for distinguishing relevant from irrelevant passages, which can degrade already-strong retrievers by disrupting learned representations. The combined approach uses cross-encoder listwise distillation to leverage richer relevance signals from the teacher model, capturing graded relevance rather than binary distinctions. This preserves useful learned representations while adding domain-specific knowledge from synthetic queries. The diverse synthetic query types ensure the model learns to handle various query formulations, making it more robust than single-query-type approaches.

## Foundational Learning
- **Contrastive Learning with InfoNCE**: Uses temperature-scaled softmax over positive and negative pairs to learn embeddings that bring relevant items closer together
  - *Why needed*: Standard approach for fine-tuning dense retrievers by learning to distinguish relevant from irrelevant passages
  - *Quick check*: Verify that InfoNCE loss is implemented with temperature scaling and proper negative sampling

- **Cross-Encoder Architecture**: Processes query-passage pairs together through the full transformer, allowing interaction between all tokens
  - *Why needed*: Provides richer relevance signals than bi-encoders by capturing token-level interactions
  - *Quick check*: Confirm cross-encoder outputs continuous scores that can be normalized for distillation

- **Listwise Distillation**: Uses KL divergence to transfer the teacher's probability distribution over passages to the student model
  - *Why needed*: Preserves graded relevance information rather than binary labels
  - *Quick check*: Verify KL divergence implementation with appropriate temperature scaling for both student and teacher

- **Synthetic Query Generation**: Creates diverse query types from passages using large language models
  - *Why needed*: Provides scalable training data without human annotation
  - *Quick check*: Ensure generated queries are semantically related to source passages

- **Hard Negative Filtering**: Excludes negatives that score too high according to cross-encoder, preventing false negatives
  - *Why needed*: Improves contrastive learning quality by removing confusing negatives
  - *Quick check*: Verify filtering threshold is applied consistently across batches

## Architecture Onboarding

**Component Map**: Passage Sampler → Synthetic Query Generator → Query Filter → Cross-Encoder Reranker → Hard Negative Filter → Combined Loss Trainer

**Critical Path**: Passage sampling → Query generation → Query filtering → Training with combined loss

**Design Tradeoffs**:
- Synthetic vs. human queries: Synthetic provides scalability but requires careful generation and filtering
- Contrastive vs. listwise distillation: Contrastive is simpler but can degrade effectiveness; listwise preserves richer signals but requires cross-encoder computation
- Hard negative threshold: Higher thresholds include more negatives but risk false negatives; lower thresholds are safer but may miss useful negatives

**Failure Signatures**:
- Model performance degrades after fine-tuning → likely contrastive loss issues
- Training stalls or produces poor gradients → check temperature scaling and filtering thresholds
- Synthetic queries don't match passage content → verify generation prompts and filtering

**First Experiments**:
1. Test contrastive fine-tuning alone on a small dataset to confirm degradation effect
2. Verify cross-encoder teacher improves over retriever on held-out data before using for distillation
3. Compare single-query-type vs. mixed-query-type training to validate diversity benefit

## Open Questions the Paper Calls Out

### Open Question 1
Can the listwise distillation approach be adapted to work with highly effective listwise rerankers that output rankings rather than continuous scores? The current method requires continuous score distributions from cross-encoders, not discrete rankings. A modified distillation loss that operates on rank-order information would be needed.

### Open Question 2
Does the combined approach scale effectively to LLM-based embedding models and newer encoder architectures like ModernBERT? All experiments used BERT-base models (110M parameters), leaving larger architectures unexplored. Testing on larger backbones would determine if proportional improvements hold at scale.

### Open Question 3
Why does conventional contrastive fine-tuning reduce effectiveness in already-supervised models, even after false negative filtering? The mechanism remains unclear—whether from distribution shift, overfitting to synthetic queries, or loss of learned representations. Ablation studies analyzing representation drift could clarify this.

### Open Question 4
What causes cross-encoder teachers to underperform smaller retrievers on specific datasets like FEVER and ArguAna? The pattern is observed but not investigated. Analysis of cross-encoder confidence calibration and controlled experiments with different teacher training data could reveal the cause.

## Limitations
- The approach relies on high-quality synthetic query generation, which depends on prompt engineering not fully specified in the paper
- Cross-encoder teacher computation is expensive, limiting scalability for very large corpora
- The 60% hard-negative filtering threshold was tuned for specific datasets without showing sensitivity analysis across domains

## Confidence
- **High confidence**: Fine-tuning with standard contrastive learning can degrade effectiveness in already-strong dense retrievers
- **Medium confidence**: Synthetic query generation with diverse types can match human-written query performance
- **Low confidence**: The 60% threshold for hard-negative filtering generalizes across different domains

## Next Checks
1. Conduct ablation studies isolating the contribution of synthetic query diversity, listwise distillation, and cross-encoder selection to understand their individual and combined effects
2. Test the 60% hard-negative filtering threshold across a broader range of domains to assess generalizability and sensitivity to corpus characteristics
3. Implement and evaluate alternative cross-encoder teachers on the same datasets to determine if the current choice is optimal, particularly for domains where it underperforms