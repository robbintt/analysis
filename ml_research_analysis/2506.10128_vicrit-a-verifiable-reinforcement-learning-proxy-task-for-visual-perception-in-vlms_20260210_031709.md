---
ver: rpa2
title: 'ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception
  in VLMs'
arxiv_id: '2506.10128'
source_url: https://arxiv.org/abs/2506.10128
tags:
- visual
- arxiv
- image
- task
- vicrit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ViCrit, an RL-based proxy task for improving
  visual perception in vision-language models (VLMs). The core idea is to train VLMs
  to detect subtle, synthetically injected visual hallucinations in detailed image
  captions.
---

# ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs

## Quick Facts
- arXiv ID: 2506.10128
- Source URL: https://arxiv.org/abs/2506.10128
- Authors: Xiyao Wang; Zhengyuan Yang; Chao Feng; Yongyuan Liang; Yuhang Zhou; Xiaoyu Liu; Ziyi Zang; Ming Li; Chung-Ching Lin; Kevin Lin; Linjie Li; Furong Huang; Lijuan Wang
- Reference count: 35
- The paper introduces ViCrit, an RL-based proxy task for improving visual perception in vision-language models by training them to detect synthetically injected hallucinations in detailed image captions.

## Executive Summary
ViCrit addresses the challenge of improving visual perception in VLMs by introducing a verifiable proxy task. The core innovation is training VLMs to detect subtle, synthetically injected visual hallucinations in detailed image captions through exact string matching. Starting from human-written captions, ViCrit minimally alters a single object description and tasks the model with identifying the erroneous span. This formulation preserves perceptual difficulty while enabling easy verification, allowing scalable RL training. Experiments demonstrate that ViCrit-RL reduces hallucination rates and improves performance on diverse VL benchmarks, with strong correlation between ViCrit-Bench performance and overall VLM capability.

## Method Summary
ViCrit trains VLMs to localize synthetic visual hallucinations injected into paragraph-length image captions using GRPO reinforcement learning. Given an image and a corrupted caption with one injected hallucination, the model outputs the specific token span representing the hallucination. The training objective uses a binary exact string match reward (0.9 × answer + 0.1 × format). The method employs the PixMo-Cap dataset (875K training pairs after hallucination injection) and introduces ViCrit-Bench (607 manually curated samples across 4 domains and 8 hallucination types) for evaluation. The approach leverages the verifiability of exact string matching to enable efficient RL training while maintaining the perceptual difficulty of the underlying visual understanding task.

## Key Results
- ViCrit-RL significantly reduces hallucination rates, decreasing CHAIRs benchmark scores from 28.0 to 25.2 at 7B parameters
- Strong correlation between ViCrit-Bench performance and general VLM capability across different model scales
- The 72B model shows improvements on downstream reasoning tasks (+4.9% on MathVision, +4.5% on Blind) despite training data being only 7-10% abstract/mathematical images
- ViCrit-RL achieves top open-source performance on ViCrit-Bench, demonstrating effective transfer of perceptual improvements

## Why This Works (Mechanism)

### Mechanism 1: Verifiable Reward Compression
ViCrit converts the inherently open-ended visual perception task into a binary exact-match problem without sacrificing perceptual difficulty. Instead of generating 200+ word captions (hard to verify), the model receives a near-correct caption with one injected hallucination and must output only the corrupted 2-5 word span. The task difficulty remains equivalent to perfect captioning (the model must still perceive the entire scene), but the reward collapses to string equality. A model that can reliably detect any hallucination must have perceived and understood the full visual scene.

### Mechanism 2: Cross-Modal Verification Strategy Acquisition
RL training on hallucination detection induces a learned strategy of systematically verifying textual claims against visual evidence. The GRPO objective rewards trajectories that correctly identify mismatches. Through exploration, the model internalizes verification behaviors—scanning the image, comparing described attributes to visual regions, and flagging inconsistencies—that transfer to downstream tasks requiring precise perception. The verification strategy learned on natural-image hallucinations generalizes to abstract reasoning and visual math domains.

### Mechanism 3: Perception-First Transfer Learning
Improvements in fine-grained perception propagate to higher-level reasoning tasks even when those tasks involve different image domains. By training on the ViCrit task, the model develops better grounding—the ability to map textual concepts to visual regions accurately. This improved grounding benefits any downstream task that requires extracting information from images, including math diagrams and charts. Accurate perception is a prerequisite for correct reasoning; improving the former will improve the latter.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: ViCrit uses GRPO rather than PPO; understanding the relative advantage formulation (Ai = ri - r̄) is necessary to reproduce training.
  - Quick check question: Given a batch of 8 samples with rewards [1, 0, 1, 1, 0, 0, 1, 0], what is the advantage A for the first sample?

- **Concept: Exact-match vs. semantic-match reward design**
  - Why needed here: The paper deliberately avoids semantic similarity metrics; understanding why string matching is preferred helps assess when this approach generalizes.
  - Quick check question: If a model outputs "the blue car" when the ground truth is "blue car," should this receive full, partial, or zero reward under ViCrit's design?

- **Concept: Hallucination taxonomy in VLMs (object, attribute, count, spatial, text, condition, material, shape)**
  - Why needed here: ViCrit-Bench evaluates across eight types; knowing these categories is essential for diagnostic analysis and understanding failure modes.
  - Quick check question: If a caption describes "three people sitting in a circle" when the image shows four people in a line, which hallucination type(s) does this represent?

## Architecture Onboarding

- **Component map:** PixMo-Cap images → LLM-based hallucination injection (GPT-4 prompted with manual examples) → filtered caption pairs → Qwen2.5-VL backbone → GRPO optimizer → reward computation (0.9 × answer + 0.1 × format) → ViCrit-Bench evaluation

- **Critical path:** Hallucination injection quality → Reward signal integrity → Training data diversity

- **Design tradeoffs:** Single-span output enables verification but limits supervision density compared to dense captioning; synthetic hallucinations are controllable but may not reflect natural model failure modes; GRPO requires sufficient batch size for stable advantage estimation

- **Failure signatures:** Model learns to output common nouns without verification (high false positive rate on unchanged captions); performance plateaus on specific hallucination types (spatial and text showed drops in the paper); large gains on ViCrit-Bench without corresponding gains on general benchmarks suggests overfitting to task format

- **First 3 experiments:**
  1. Reproduce the hallucination injection pipeline on a small caption subset (100 samples), manually verify that replacements are visually plausible yet unambiguous
  2. Train a 7B model on ViCrit task for 1 epoch, evaluate on ViCrit-Bench before/after to confirm the paper's reported gains are reproducible
  3. Ablate the reward weighting: train with r_answer = 1.0 (no format reward) and compare convergence speed and final performance to the 0.9/0.1 split

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific properties of training data complexity are required for ViCrit-based RL to produce meaningful perceptual gains in large-scale VLMs (70B+ parameters)?
- **Basis in paper:** [explicit] The authors state: "RL training led to substantial performance gains for the 7B model on ViCrit-Bench, whereas the improvements for the 72B model are relatively marginal. We hypothesize that this is due to the constructed training set being insufficiently challenging for Qwen-2.5-VL-72B... To further enhance the performance of the 72B model, more complex and demanding data may be required."
- **Why unresolved:** The paper does not characterize what makes hallucination samples "sufficiently challenging," nor does it define the relationship between model scale and required task difficulty.
- **What evidence would resolve it:** Systematic experiments varying hallucination subtlety across different model scales, with analysis of learning dynamics and plateau points.

### Open Question 2
- **Question:** Why does ViCrit-RL training cause performance degradation on Spatial and Text hallucination detection, and can curriculum-based or balanced sampling mitigate this?
- **Basis in paper:** [explicit] "We observe a significant drop in accuracy for the Spatial and Text tasks after RL training. We attribute this to data imbalance in the construction of the training set."
- **Why unresolved:** The paper identifies the symptom but does not confirm the cause through controlled experiments, nor does it test potential solutions. The mechanism by which training on other hallucination types interferes with spatial/text perception remains unexplained.
- **What evidence would resolve it:** Ablation studies with stratified sampling across all eight hallucination types, plus analysis of learned attention patterns before/after training.

### Open Question 3
- **Question:** What mechanism enables ViCrit-RL's cross-domain generalization to abstract/mathematical images despite these domains comprising only 7% and 10% of training data respectively?
- **Basis in paper:** [inferred] The paper shows strong improvements on MathVision (+4.9%), VLMsAreBlind (+4.5%), and ChartXiv (+3.9%) for the 72B model, and explicitly notes that "math and abstract images only account for 7% of the PixMo-Cap training data, and 10% for chart and document images." The authors claim models learn "a transferable strategy for 'how to look' at an image."
- **Why unresolved:** The paper claims but does not mechanistically demonstrate what this transferable strategy is. Qualitative examples suggest improved attention to detail, but do not establish causality or characterize the learned representations.
- **What evidence would resolve it:** Probing experiments on intermediate representations, analysis of attention patterns on out-of-domain images, and comparison with explicit "verification" training objectives.

## Limitations
- The synthetic nature of hallucinations may create a training distribution that differs substantially from naturally occurring model failures
- The reliance on GPT-4 for hallucination injection introduces potential variability in training data quality and distribution
- Performance gains on general VL benchmarks, while present, lack ablation studies isolating perception-specific improvements from other factors

## Confidence
- **High confidence**: The core mechanism of converting perception verification into a verifiable binary task is sound and well-demonstrated through controlled experiments on ViCrit-Bench
- **Medium confidence**: Transfer of perceptual improvements to downstream reasoning tasks is supported by benchmark improvements but lacks ablation studies isolating perception-specific gains
- **Medium confidence**: The claim that the task preserves "full perceptual difficulty" is theoretically sound but difficult to empirically verify

## Next Checks
1. Conduct an ablation study comparing ViCrit-RL to a control RL task that detects hallucinations in randomly corrupted text (without visual grounding) to isolate perception-specific improvements
2. Evaluate the same models on the ViPER benchmark to determine whether gains transfer to naturally occurring hallucinations versus synthetic ones
3. Test the hypothesis that perception improvements drive reasoning gains by training a model on ViCrit for perception but freezing visual layers during math reasoning fine-tuning, then comparing to fully fine-tuned models