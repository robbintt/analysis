---
ver: rpa2
title: 'Hierarchical Koopman Diffusion: Fast Generation with Interpretable Diffusion
  Trajectory'
arxiv_id: '2510.12220'
source_url: https://arxiv.org/abs/2510.12220
tags:
- koopman
- diffusion
- image
- generation
- one-step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hierarchical Koopman Diffusion (HKD), a framework
  that achieves one-step image generation while maintaining interpretability and controllability
  of the diffusion trajectory. By leveraging Koopman operator theory, the method transforms
  nonlinear diffusion dynamics into a latent space governed by globally linear operators,
  enabling closed-form trajectory solutions.
---

# Hierarchical Koopman Diffusion: Fast Generation with Interpretable Diffusion Trajectory

## Quick Facts
- **arXiv ID**: 2510.12220
- **Source URL**: https://arxiv.org/abs/2510.12220
- **Authors**: Hanru Bai; Weiyang Ding; Difan Zou
- **Reference count**: 40
- **Primary result**: Achieves one-step image generation with competitive FID scores (CIFAR-10: 3.30, FFHQ: 5.70) while maintaining trajectory interpretability

## Executive Summary
This paper introduces Hierarchical Koopman Diffusion (HKD), a framework that achieves one-step image generation while maintaining interpretability and controllability of the diffusion trajectory. By leveraging Koopman operator theory, the method transforms nonlinear diffusion dynamics into a latent space governed by globally linear operators, enabling closed-form trajectory solutions. A hierarchical architecture further decomposes generative dynamics across spatial resolutions, capturing coarse-to-fine details. HKD achieves competitive FID scores on CIFAR-10 (3.30) and FFHQ (5.70) datasets, demonstrating strong one-step generation quality.

## Method Summary
HKD maps noisy images to a Koopman observable space where evolution follows linear dynamics. A hierarchical U-Net encoder extracts multi-resolution features, each governed by scale-specific Koopman operators. The solution uses matrix exponential for closed-form trajectory evolution. A decoder integrates evolved features via skip connections. Training combines trajectory consistency loss (supervising intermediate states) with endpoint reconstruction loss, using LPIPS+MSE distance. The framework enables controllable editing through spectral analysis of generative dynamics.

## Key Results
- Achieves one-step generation with FID scores of 3.30 on CIFAR-10 and 5.70 on FFHQ 64×64
- Provides principled spectral analysis linking Koopman eigenvalues to generative dynamics
- Enables controllable image editing through frequency-specific interventions
- Demonstrates superior expressiveness compared to direct noise-to-image mappings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lifting nonlinear diffusion dynamics into a Koopman latent space enables closed-form trajectory solutions, eliminating iterative sampling while preserving access to intermediate states.
- Mechanism: The encoder Eθ maps noisy images xt to Koopman observables z(l)t. Evolution in this space follows dz/dt = A·z, where A is a learned linear operator with block-diagonal structure (each 2×2 block corresponding to complex conjugate eigenvalue pairs). The solution zt = e^(A(t-s))·zs is computed analytically via matrix exponential.
- Core assumption: The true diffusion dynamics can be adequately approximated by linear evolution in the learned observable space (finite-dimensional Koopman approximation).
- Evidence anchors:
  - [abstract] "lifts the nonlinear diffusion dynamics into a latent space where evolution is governed by globally linear operators, enabling closed-form trajectory solutions"
  - [Section 3.1, Eq. 3-5] Defines hierarchical Koopman subspace with spatially-varying operators and explicit evolution formula
  - [corpus] Neighbor paper "Unfolding Generative Flows with Koopman Operators" validates Koopman-based acceleration for normalizing flows
- Break condition: If the diffusion dynamics exhibit strong nonlinear coupling across spatial locations that cannot be captured by the block-diagonal A structure, the linear approximation degrades.

### Mechanism 2
- Claim: Hierarchical decomposition across spatial resolutions captures coarse-to-fine generative dynamics, improving fidelity by modeling scale-specific evolution.
- Mechanism: U-Net encoder extracts multi-resolution features {z(l)} at L levels. Each level has its own Koopman operator A(l), allowing different dynamics at different scales (coarse global structure vs. fine texture). Decoder integrates evolved features via skip connections.
- Core assumption: Generative dynamics factorize meaningfully across spatial scales, with different temporal behaviors at each resolution.
- Evidence anchors:
  - [abstract] "hierarchical architecture that disentangles generative dynamics across spatial resolutions via scale-specific Koopman subspaces"
  - [Section 4.2/Fig. 4] Spectral analysis shows low-range modes capture global structure, mid-range capture shape, high-range capture fine details
  - [corpus] Limited direct corpus validation for hierarchical Koopman specifically
- Break condition: If dynamics at different scales are strongly coupled, separate operators cannot capture cross-scale interactions.

### Mechanism 3
- Claim: Trajectory consistency loss supervises intermediate states, enforcing coherent noise-to-image mappings beyond endpoint-only supervision.
- Mechanism: Sample random timesteps t, encode xt, evolve to time ε via Koopman dynamics, decode and compare to ground truth xε using LPIPS+MSE. This constrains the entire trajectory, not just T→0 mapping.
- Core assumption: The perceptual distance (LPIPS) in image space provides meaningful gradients for learning the Koopman embedding.
- Evidence anchors:
  - [Section 3.2, Def. 3.1] Formal definition of trajectory consistency loss
  - [Fig. 2] Visual evidence of consistent reconstruction from noisy inputs at all timesteps
  - [corpus] No direct corpus comparison; this is a novel contribution
- Break condition: If the loss landscape has poor gradients for the linear operator A (e.g., near-zero eigenvalues causing vanishing gradients), learning stalls.

## Foundational Learning

- Concept: Koopman Operator Theory
  - Why needed here: Core theoretical basis—transforms nonlinear dynamics into linear evolution in observable space. Without this, the closed-form trajectory solution doesn't exist.
  - Quick check question: Given a nonlinear system ẋ = f(x), what does the Koopman operator K act on, and what property makes it useful for analysis?

- Concept: Probability Flow ODE
  - Why needed here: HKD operates on deterministic ODE trajectories from diffusion models, not the stochastic SDE. Understanding Eq. (1) is essential.
  - Quick check question: How does the probability flow ODE relate to the score function ∇log p_t(x_t), and why does it preserve marginals while being deterministic?

- Concept: Spectral Analysis of Linear Dynamical Systems
  - Why needed here: Eigenvalues α_k ± iβ_k of A encode decay rates and oscillation frequencies. Controllable editing relies on modifying specific spectral components.
  - Quick check question: In a linear system ż = Az, what do the real and imaginary parts of eigenvalues determine about trajectory behavior?

## Architecture Onboarding

- Component map:
  - **Encoder Eθ**: U-Net downsampling path → multi-resolution features {z(l)}
  - **Koopman Operators {A(l)}**: Block-diagonal 2×2 matrices per spatial location (i,j) per level l
  - **Decoder Dϕ**: U-Net upsampling path integrating evolved latents
  - **Loss**: Lt-consist (trajectory) + Lrecon (endpoint), with LPIPS+MSE distance

- Critical path: xt → Eθ → {z(l)t} → e^(A·Δt) evolution → {z(l)ε} → Dϕ → x̂ε

- Design tradeoffs:
  - Block-diagonal A: Fewer parameters but limits expressiveness (Prop. C.1 justifies this under similarity transform)
  - Image-space vs. latent-space loss: Image-space (chosen) enables perceptual metrics; latent-space would be more direct but less perceptually meaningful
  - Hierarchical vs. single Koopman space: More expressive but adds complexity and parameters

- Failure signatures:
  - FID degrades but reconstruction looks sharp: Mode collapse; check diversity of generated samples
  - Training unstable with gradient explosion: Eigenvalues of A may be growing unbounded; add spectral normalization
  - Poor intermediate state quality: Trajectory consistency loss weight (λ1 annealing) may be too aggressive

- First 3 experiments:
  1. **Sanity check**: Train on single-scale Koopman (no hierarchy), verify it learns but underperforms vs. hierarchical (ablation row in Tab. 3 shows FID 4.78 → 3.30)
  2. **Spectral visualization**: Decode latent states with only low/mid/high spectral bands active (replicate Fig. 4) to verify semantic separation
  3. **Intervention test**: At t=T/2, inject reference image high-frequency components at varying ratios (replicate Fig. 5) to confirm controllability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the hierarchical Koopman framework maintain spectral stability and generation quality when scaling to high-resolution image synthesis (e.g., 512px or higher)?
- Basis in paper: [explicit] The Discussion section states, "While HKD performs well on standard-resolution datasets, its potential for high-resolution generation... remains underexplored."
- Why unresolved: While the hierarchical architecture handles multi-scale features, the accuracy of finite-dimensional Koopman approximations is known to degrade as system complexity (state space dimensionality) increases.
- What evidence would resolve it: Successful application of HKD to datasets like ImageNet or LAION at 256px or 512px with competitive FID scores.

### Open Question 2
- Question: Can the spectral decomposition in HKD be extended to enable text-guided or attribute-specific editing beyond the currently demonstrated frequency manipulations?
- Basis in paper: [explicit] The Discussion notes the framework "opens up possibilities for text-guided editing, attribute-specific control. While these directions are currently unexplored..."
- Why unresolved: The current spectral analysis links modes to frequency (structure vs. texture), but mapping these continuous spectral components to discrete semantic attributes or text embeddings is a non-trivial alignment problem.
- What evidence would resolve it: An interface where text prompts (e.g., "add glasses") specifically modulate relevant Koopman eigenvalues to alter image attributes without retraining.

### Open Question 3
- Question: Does the assumption of spatially independent linear operators $A^{(l)}(i, j)$ limit the modeling of long-range spatial dependencies compared to coupled dynamics?
- Basis in paper: [inferred] Equation 3 defines the linear operator $A^{(l)}(i, j)$ as specific to each spatial location $(i, j)$, implying pixels evolve independently in the Koopman space.
- Why unresolved: Realistic image structures often require modeling correlations between distant pixels; decoupling the operator by location simplifies computation but theoretically ignores global spatial interaction.
- What evidence would resolve it: A comparison between the current independent-operator HKD and a coupled-operator variant on tasks requiring strict global structural consistency.

## Limitations

- The finite-dimensional Koopman approximation introduces approximation errors that aren't fully characterized, potentially limiting expressiveness for complex image structures
- Performance claims are based on relatively small-scale datasets (CIFAR-10, FFHQ 64×64), with scalability to higher resolutions unverified
- The spatially independent linear operators may fail to capture long-range spatial dependencies present in realistic images

## Confidence

- **High confidence**: The hierarchical architecture design and Koopman theory application are methodologically sound. The spectral analysis framework for interpretability is well-grounded.
- **Medium confidence**: The quantitative results (FID scores) are promising but achieved on relatively small-scale datasets (CIFAR-10, FFHQ 64×64). Performance on higher-resolution tasks remains unverified.
- **Low confidence**: The claim about superior expressiveness compared to direct noise-to-image mappings lacks direct ablation studies comparing different trajectory supervision strategies.

## Next Checks

1. **Generalization test**: Evaluate HKD on higher-resolution datasets (e.g., FFHQ 256×256 or LSUN) to verify scalability of the one-step generation approach.
2. **Ablation study**: Compare trajectory consistency loss against simpler endpoint-only supervision with perceptual losses to isolate its contribution to performance.
3. **Spectral stability analysis**: Systematically vary the spectral radius constraints on Koopman operators and measure impact on both generation quality and trajectory interpretability.