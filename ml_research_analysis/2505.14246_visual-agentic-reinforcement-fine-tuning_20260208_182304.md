---
ver: rpa2
title: Visual Agentic Reinforcement Fine-Tuning
arxiv_id: '2505.14246'
source_url: https://arxiv.org/abs/2505.14246
tags:
- reasoning
- agentic
- answer
- search
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Visual-ARFT, a reinforcement fine-tuning
  method for training large vision-language models (LVLMs) to perform multimodal agentic
  reasoning using external tools like web search and image-processing code. It uses
  rule-based verifiable rewards and GRPO to guide learning without human feedback.
---

# Visual Agentic Reinforcement Fine-Tuning

## Quick Facts
- **arXiv ID**: 2505.14246
- **Source URL**: https://arxiv.org/abs/2505.14246
- **Reference count**: 40
- **Primary result**: Visual-ARFT achieves +18.6% F1 and +13.0% EM on MAT-Coding and +10.3% F1 and +8.7% EM on MAT-Search, surpassing GPT-4o

## Executive Summary
Visual-ARFT is a reinforcement fine-tuning method for training large vision-language models (LVLMs) to perform multimodal agentic reasoning using external tools like web search and image-processing code. The approach uses rule-based verifiable rewards and GRPO to guide learning without human feedback, enabling flexible and adaptive reasoning abilities. Visual-ARFT significantly outperforms baselines on both MAT tasks and generalizes to existing multi-hop QA benchmarks, demonstrating strong data efficiency with only 20 training examples for search tasks.

## Method Summary
Visual-ARFT trains LVLMs using GRPO with rule-based verifiable rewards for multimodal agentic tasks. The method uses structured output formats with tags for reasoning (჻...ᓴ), tool invocation (<search>, <code>), and answers (<answer>). Rewards combine format compliance, F1 scores for answers, semantic similarity for search queries, and binary rewards for executable code. The training uses base models Qwen2.5-VL-3B/7B, MAT-Search (20 train/150 test) and MAT-Coding (1,200 train/200 test) benchmarks, and achieves strong generalization to text-only multi-hop QA tasks.

## Key Results
- Visual-ARFT achieves +18.6% F1 and +13.0% EM on MAT-Coding benchmark
- Visual-ARFT achieves +10.3% F1 and +8.7% EM on MAT-Search benchmark
- Strong generalization to text-only multi-hop QA: +29.3 F1% / +25.9% EM gains on 2Wiki and HotpotQA

## Why This Works (Mechanism)

### Mechanism 1: Rule-Based Verifiable Rewards as Policy Guidance
Visual-ARFT replaces learned reward models with deterministic, rule-based correctness signals for effective policy optimization without human feedback. The total reward combines format compliance with accuracy metrics (F1, semantic similarity, executable code rewards) providing gradient-like signals to GRPO while maintaining KL-divergence proximity to reference model.

### Mechanism 2: Structured Output Formatting for Tool-Augmented Reasoning
Enforcing structured output formats with dedicated tags enables models to learn when and how to use external tools. Format rewards encourage step-by-step reasoning with problem analysis, appropriate tool calls, and final answer synthesis through iterative GRPO updates.

### Mechanism 3: Cross-Modal Transfer of Agentic Reasoning Patterns
Training on small multimodal agentic examples enables generalization to text-only multi-hop QA through learned reasoning and decomposition patterns. The model internalizes patterns like "identify missing information → formulate search query → integrate results" that apply across modalities.

## Foundational Learning

- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Replacing traditional RLHF with deterministic correctness checks. Quick check: Explain why R(q,o) = I[o = ground-truth(q)] differs from learned reward models and what tradeoffs each approach introduces.
- **Group Relative Policy Optimization (GRPO)**: Specific algorithm for policy updates with KL regularization. Quick check: What role does β·KL(π_θ||π_ref) play in preventing policy degradation, and what failure mode emerges if β is set too low?
- **Multimodal Tool Execution Environments**: Integration with external tools (search APIs, code execution sandboxes). Quick check: When generating Python code for image processing, what security and execution constraints must the environment enforce?

## Architecture Onboarding

- **Component map**: Input Layer (multimodal inputs → Vision encoder → Projector → LLM backbone) → Policy Model (structured outputs with tags) → Tool Execution Environment (Serper API, sandboxed Python) → Reward Calculator (format checker + F1 scorer + semantic similarity + code executability) → GRPO Optimizer (policy gradient updates with KL regularization)
- **Critical path**: Prompt engineering for structured output format → Tool execution environment setup → Reward function implementation → GRPO training loop (8 GPUs, 8 sampled generations) → Evaluation on MAT and downstream benchmarks
- **Design tradeoffs**: F1 vs. EM rewards (F1 provides smoother gradients), code content supervision (avoided for exploration flexibility), training data scale (20 vs 1,200 examples), code reward design (all executable code receives reward=1)
- **Failure signatures**: Reward hacking (valid formats but unhelpful content), tool invocation loops, format drift, generalization collapse, RAG-style degradation
- **First 3 experiments**: 1) Baseline sanity check on MAT-Coding without fine-tuning, 2) Reward ablation with EM-based rewards instead of F1, 3) Format-only training to isolate structured output learning

## Open Questions the Paper Calls Out
The paper identifies three open questions: 1) How Visual-ARFT performance scales with additional tool types beyond search and coding, 2) Whether incorporating content-level supervision for generated code would improve performance, and 3) How the method transfers to LVLM architectures beyond Qwen2.5-VL.

## Limitations
- Current implementation focuses only on search and coding tools, not covering other tool-augmented use cases
- Cross-modal generalization from multimodal to text-only tasks lacks direct validation
- Rule-based reward approach may fail for subjective or creative visual reasoning tasks requiring human judgment

## Confidence
- **High confidence**: Visual-ARFT achieves superior performance on MAT benchmark tasks
- **Medium confidence**: Visual-ARFT generalizes to text-only multi-hop QA benchmarks
- **Low confidence**: Rule-based verifiable rewards alone are sufficient for effective policy optimization in multimodal agentic reasoning

## Next Checks
1. Ablation on reward design: Compare Visual-ARFT with learned reward models versus rule-based verifiable rewards
2. Cross-modal transfer validation: Train models exclusively on text-only vs multimodal tasks and compare generalization
3. Generalization stress test: Evaluate on diverse multimodal agentic tasks requiring subjective judgment or tools not present in training