---
ver: rpa2
title: 'Matina: A Large-Scale 73B Token Persian Text Corpus'
arxiv_id: '2502.09188'
source_url: https://arxiv.org/abs/2502.09188
tags:
- data
- persian
- content
- text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Matina, a 72.9B-token Persian text corpus
  designed to address the lack of high-quality, diverse Persian datasets for training
  language models. It combines web-crawled data, crawled books and papers, and social
  media, applying rigorous preprocessing and deduplication.
---

# Matina: A Large-Scale 73B Token Persian Text Corpus

## Quick Facts
- arXiv ID: 2502.09188
- Source URL: https://arxiv.org/abs/2502.09188
- Reference count: 28
- 72.9B-token Persian corpus improves NLP performance, achieving 56.54 on Arman Emo and 74.92 on Pars-ABSA

## Executive Summary
Matina is a 72.9B-token Persian text corpus designed to address the lack of high-quality, diverse Persian datasets for training language models. The corpus combines web-crawled data, crawled books and papers, and social media, applying rigorous preprocessing and deduplication. The authors demonstrate improved performance on masked language model training and large language model pretraining, with both the dataset and preprocessing codes publicly available.

## Method Summary
The authors created Matina by aggregating six Persian datasets (books, papers, social media, web, CulturaX FA, MADLAD-400 FA), then applying a three-stage preprocessing pipeline (character-level, line/paragraph-level, document-level) with source-specific thresholds. They performed MinHash + LSH deduplication with language-specific normalization before tokenizing with Llama 3.1 tokenizer. The corpus was validated through continued pretraining of XLM-RoBERTa and LLaMA 3.1 models on domain subsets.

## Key Results
- 72.9B token Persian corpus with source-specific preprocessing
- Improved MLM performance on Arman Emo (56.54) and Pars-ABSA (74.92)
- Public availability of both dataset and preprocessing code

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Source-diverse corpora with tailored preprocessing per source type improve downstream NLP task performance compared to homogeneous datasets.
- Mechanism: Different sources provide complementary linguistic patterns—books offer long-form formal text (162,648.9 avg tokens), social media captures colloquial language (351.6 avg tokens), papers provide technical vocabulary. Tailored preprocessing preserves source-specific value while removing noise.
- Core assumption: Model performance gains stem from linguistic diversity rather than solely from corpus scale.
- Evidence anchors:
  - [abstract] "Existing Persian datasets are typically small and lack content diversity, consisting mainly of weblogs and news articles."
  - [Section 3] "These three categories—web-based crawled data, crawled books and papers, and social media—form the core of our dataset, each with distinct preprocessing requirements."
  - [corpus] Weak direct comparison—no ablation testing diversity vs. scale contribution independently.
- Break condition: If downstream tasks only test formal Persian (news, academic), informal social media data may add noise without benefit.

### Mechanism 2
- Claim: Multi-stage hierarchical preprocessing (character → line → document) with source-specific thresholds yields higher data quality than single-stage filtering.
- Mechanism: Coarse filters at document level (e.g., <30 words, >50% non-Persian) remove obvious noise efficiently; fine-grained character/line processing handles encoding issues, HTML artifacts, and structural noise. Source-specific thresholds (e.g., relaxed filtering for Virgool technical content) prevent over-filtering.
- Core assumption: Heuristic thresholds derived from manual sample inspection generalize across the full corpus.
- Evidence anchors:
  - [Section 3.1] "After inspecting samples from various domains, we defined heuristic functions to modify documents and remove those deemed irrelevant."
  - [Figure 3] Preprocessing reduced documents by ~24% on average; deduplication further reduced by 18.83%.
  - [corpus] No comparison to alternative preprocessing approaches (e.g., learned quality classifiers).
- Break condition: If heuristics over-filter minority dialects or technical domains not represented in inspected samples, linguistic diversity contracts.

### Mechanism 3
- Claim: MinHash + LSH deduplication with language-specific normalization reduces redundancy more effectively than generic multilingual deduplication.
- Mechanism: Normalizing dates/numbers before hashing catches near-duplicates that differ only in temporal/numerical details (common in price posts, news reposts). Restoring original content post-deduplication preserves informativeness.
- Core assumption: Near-duplicates differing only in numbers/dates provide negligible additional training signal.
- Evidence anchors:
  - [Section 3.3] "We observed that many messages from different sources differed only in date or pricing... We removed all numeric values and dates before deduplication."
  - [Section 3.4] "Although CulturaX FA and Madlad-400 FA claim to have already undergone processing and deduplication, our language-specific preprocessing steps... further reduced their size."
  - [corpus] Weak—no direct comparison of deduplication strategies on downstream performance.
- Break condition: If numerical patterns carry task-relevant signal (e.g., learning to extract prices), this normalization may harm specific capabilities.

## Foundational Learning

- Concept: **MinHash and Locality-Sensitive Hashing (LSH)**
  - Why needed here: Core deduplication mechanism for 72.9B token corpus; exact matching infeasible at scale.
  - Quick check question: Can you explain why MinHash approximates Jaccard similarity efficiently for large document sets?

- Concept: **Transformer tokenization and token counting**
  - Why needed here: Corpus statistics (72.9B tokens) are tokenizer-dependent; Llama 3.1 tokenizer used for all counts.
  - Quick check question: Why would token counts differ significantly between a Persian-optimized tokenizer vs. a multilingual tokenizer?

- Concept: **Continued pretraining vs. fine-tuning**
  - Why needed here: Evaluation methodology uses continued pretraining on domain subsets before instruction tuning.
  - Quick check question: What is the difference between continued pretraining and instruction tuning in terms of objective function and data format?

## Architecture Onboarding

- Component map: Crawlers (web, social media, PDF extractors) -> Parsers (BeautifulSoup, Selenium, Pytesseract) -> Preprocessing pipeline (character→line→document) -> MinHash LSH deduplication -> Output format

- Critical path:
  1. Language detection/confidence filtering (especially for public datasets like CulturaX/Madlad)
  2. Character-level normalization (Unicode, Arabic-Persian mapping)
  3. Document-level quality thresholds (length, Persian ratio, repetition)
  4. Source-aware deduplication (normalize numbers/dates for social media; preserve for temporal content)

- Design tradeoffs:
  - **Strict filtering vs. recall**: Social media retained only 1.6% of documents but ~10% of tokens (longer surviving documents)
  - **OCR accuracy vs. coverage**: Image-based papers had ~50% document loss; threshold tuned to remove garbled text
  - **Sub-document vs. document deduplication**: Authors note memory constraints prevented sentence-level deduplication (Limitation #1)

- Failure signatures:
  - High OOV word ratio (>2.5%) indicates corrupted text or code fragments
  - Short line proportion >50% suggests tables of contents or lists, not prose
  - Average word length <3 or >10 characters signals encoding issues or concatenation errors

- First 3 experiments:
  1. **Baseline quality audit**: Random sample 1000 documents per source; manually label noise types; validate heuristics catch >90%
  2. **Deduplication ablation**: Compare MinHash-only vs. exact matching on a 1B token subset; measure redundancy reduction and downstream perplexity
  3. **Tokenizer sensitivity**: Tokenize same subset with Llama 3.1 vs. a Persian-specific tokenizer; quantify token count variance and vocabulary coverage gaps

## Open Questions the Paper Calls Out
- How does the lack of intra-document deduplication affect training dynamics and memorization behavior?
- To what extent does retained offensive language in social media influence toxicity levels of fine-tuned models?
- Does significant data reduction during preprocessing of public datasets indicate multilingual corpora require language-specific tuning?

## Limitations
- Heavy reliance on existing public datasets rather than novel collection
- No ablation testing to isolate contributions of diversity vs. scale
- Tokenizer dependency on non-Persian-optimized Llama 3.1 tokenizer
- Resource constraints prevented sentence-level deduplication

## Confidence
- **High confidence**: Preprocessing pipeline design and implementation are technically sound
- **Medium confidence**: Downstream performance improvements are reasonable but lack ablation testing
- **Low confidence**: "Large-scale" characterization may be overstated given heavy use of existing datasets

## Next Checks
1. **Tokenization sensitivity analysis**: Process a 100M token sample with both Llama 3.1 tokenizer and a Persian-optimized tokenizer. Compare token counts, vocabulary coverage, and downstream perplexity to quantify tokenization impact on corpus utility.

2. **Diversity contribution ablation**: Train three models on subsets: (a) Matina web-only (largest source), (b) Matina web+books (formal diversity), (c) full Matina. Evaluate on both formal (news) and informal (social media) downstream tasks to isolate diversity vs. scale effects.

3. **Heuristic threshold validation**: Implement an active learning loop where manual quality annotations on 10K documents feed back into preprocessing thresholds. Measure whether automated thresholds achieve >95% precision on a held-out validation set.