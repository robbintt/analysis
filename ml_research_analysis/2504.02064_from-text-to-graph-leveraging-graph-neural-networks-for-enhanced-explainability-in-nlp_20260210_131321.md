---
ver: rpa2
title: 'From Text to Graph: Leveraging Graph Neural Networks for Enhanced Explainability
  in NLP'
arxiv_id: '2504.02064'
source_url: https://arxiv.org/abs/2504.02064
tags:
- graph
- language
- arxiv
- explainability
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of achieving explainability
  in natural language processing tasks, particularly those involving large language
  models (LLMs) that rely on tokenisation, which can fragment text into sequences
  lacking inherent semantic meaning. The proposed methodology automatically converts
  sentences into constituency trees and then into graphs, maintaining semantic meaning
  through nodes and relations.
---

# From Text to Graph: Leveraging Graph Neural Networks for Enhanced Explainability in NLP

## Quick Facts
- arXiv ID: 2504.02064
- Source URL: https://arxiv.org/abs/2504.02064
- Reference count: 35
- Key outcome: Proposed methodology converts text to constituency graphs and uses GNN surrogates with SubgraphX to explain LLM classifications, achieving F1 scores of 0.87 (AG News) and 0.78 (SST-2).

## Executive Summary
This paper addresses the challenge of explainability in NLP tasks involving large language models that rely on tokenisation. The proposed methodology automatically converts sentences into constituency trees and then into graphs, maintaining semantic meaning through nodes and relations. The graphs are used to train a graph neural network (GNN) to replicate the LLM's behavior for specific tasks like sentiment analysis or topic classification. Post-hoc explainability techniques, specifically SubgraphX, are applied to the GNN to determine the most critical components within the text structure for the given classification.

## Method Summary
The methodology converts text to constituency trees using a parser, then transforms these trees into graphs where nodes represent words or phrase types and edges represent syntactic hierarchy. A pre-trained language model (BERT) generates embeddings for the graph nodes, which are used to train a simpler GNN (GCN) to replicate the LLM's classification outputs. SubgraphX, employing Monte Carlo Tree Search and Shapley values, identifies connected critical subgraphs in the GNN that explain the classification decisions.

## Key Results
- GNN achieves F1 scores of 0.87 on AG News and 0.78 on SST-2 datasets
- SubgraphX successfully identifies critical subgraphs that explain classification decisions
- The approach provides insights into important entities and structural properties associated with correct and incorrect classifications

## Why This Works (Mechanism)

### Mechanism 1: Semantic Preservation via Graph Structuring
The methodology uses a parser to convert sentences into constituency trees, which are then treated as directed graphs. Nodes represent words or phrase types (e.g., Noun Phrase), and edges represent syntactic hierarchy. This forces the model to reason over explicit linguistic structures rather than implicit token patterns.

### Mechanism 2: Surrogate Modeling via Knowledge Distillation
The system uses a fine-tuned LLM (e.g., BERT) to generate node embeddings and classification labels, which are used to train a simpler GNN (e.g., GCN). By training the GNN to replicate the LLM's outputs using the LLM's own feature representations, the GNN learns a "distilled" version of the decision logic that is structurally simpler to interrogate.

### Mechanism 3: Connected Subgraph Identification (SubgraphX)
The system employs SubgraphX, which uses Monte Carlo Tree Search (MCTS) to explore different subgraphs. It employs Shapley values to score the importance of a connected subgraph by comparing the prediction probability when the subgraph is present vs. masked.

## Foundational Learning

- **Concept: Constituency vs. Dependency Parsing**
  - Why needed here: The paper relies specifically on constituency trees (hierarchical phrase structure) to build graphs. Understanding the difference between "Noun Phrase" (constituency) and "nsubj" (dependency) is required to interpret the nodes in the graph.
  - Quick check question: Can you identify if a "Verb Phrase" node in the graph represents a syntactic dependency or a hierarchical constituent in this architecture?

- **Concept: Graph Neural Networks (GCN)**
  - Why needed here: The surrogate model is a Graph Convolutional Network. You must understand message passing (how a node aggregates features from its syntactic neighbors) to diagnose why certain structural properties correlate with correct classifications.
  - Quick check question: In this architecture, does the GCN learn new embeddings from scratch, or does it consume frozen embeddings from the pre-trained LLM?

- **Concept: Post-hoc Explainability**
  - Why needed here: The system explains the model *after* training using SubgraphX. It assumes the model is a black box (or frozen) and probes it via perturbation.
  - Quick check question: Why is "Fidelity" defined as $1 - |S_m - S_u|$ in this context, and what does a low Fidelity score imply about the explanation?

## Architecture Onboarding

- **Component map:** Parser (RoBERTa fine-tuned on SPMRL) -> Graph Builder (Trees to NetworkX graphs) -> Featurizer (BERT generates node embeddings) -> Surrogate (2-layer GCN) -> Explainer (SubgraphX with AutoGOAL)

- **Critical path:** The **Graph Builder** logic is the integration point. If the conversion from CoNLL format to the graph's adjacency matrix breaks the hierarchy (e.g., misinterpreting the "root" node), the GNN will aggregate information from incorrect syntactic neighbors, and SubgraphX will identify wrong causal chains.

- **Design tradeoffs:**
  - Fidelity vs. Sparsity: The AutoGOAL optimizer tries to balance keeping the subgraph small (sparsity) while maintaining high prediction confidence (fidelity).
  - Tokenizer granularity: Using BERT embeddings for nodes means handling sub-word tokenization (e.g., "unfriendly" might be split). The paper uses a **mean operation** to aggregate these into a single node embedding, which may dilute specific semantic nuances.

- **Failure signatures:**
  - High "Fragment" Node Count: Indicates the parser failed to structure the sentence.
  - Inverted Fidelity: If $S_m < S_u$, the identified subgraph introduces noise rather than signal.
  - F1 Drop > 10%: If the GNN F1 is substantially lower than the LM F1, the surrogate is invalid.

- **First 3 experiments:**
  1. Unit Test Parser: Run the RoBERTa parser on a dataset sample and verify that "Sentence" nodes appear as roots and "Noun Phrases" correctly encapsulate nouns.
  2. Overfit Sanity Check: Train the GCN on a tiny batch (10 samples) of LM-generated embeddings. It should reach near 100% accuracy; if not, the GCN architecture or embedding loading is broken.
  3. Explainability Check: Run SubgraphX on a correctly classified "Sports" article. Verify that the "Most Important Subgraph" contains sports-related entities and not generic stopwords, and check the Fidelity score.

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of language model architecture (e.g., DeBERTa vs. BERT) impact the GNN's ability to replicate behavior and the quality of the resulting explanations? The current experiments rely exclusively on BERT for embedding generation; it is undetermined if models with different attention mechanisms (like disentangled attention in DeBERTa) would require different GNN configurations or yield different fidelity scores.

### Open Question 2
Can the semantic extraction methodology be adapted to handle heterogeneous graphs containing cross-relations, such as those combining constituency and dependency parsing? The current algorithm relies on strictly hierarchical parent-child relationships to extract important words, which breaks down if the graph structure includes non-hierarchical edges (cross-relations).

### Open Question 3
To what extent do alternative post-hoc explainability modules (beyond Shapley values) provide deeper insights or higher fidelity for GNN-based NLP tasks? The study relies entirely on SubgraphX; other factual or counterfactual explanation methods might yield different trade-offs between sparsity and fidelity or reveal different structural properties.

## Limitations
- The methodology is limited to hierarchical constituency trees and cannot handle cross-relations in graph structures
- The approach depends on the accuracy of the constituency parser, which may struggle with ambiguous or malformed sentences
- The GNN surrogate may not perfectly capture the LLM's decision boundary, potentially limiting explanation fidelity

## Confidence
- Semantic preservation via graph structuring: High
- Surrogate modeling via knowledge distillation: Medium
- Connected subgraph identification (SubgraphX): Medium

## Next Checks
1. Verify that the constituency parser correctly identifies root nodes and hierarchical structures in sample sentences
2. Confirm that the GCN achieves near 100% accuracy when trained on a small batch of LM-generated embeddings
3. Validate that SubgraphX identifies semantically relevant entities in correctly classified examples by checking Fidelity scores and subgraph content