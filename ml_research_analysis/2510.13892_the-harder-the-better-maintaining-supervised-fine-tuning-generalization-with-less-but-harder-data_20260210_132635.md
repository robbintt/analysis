---
ver: rpa2
title: 'The Harder The Better: Maintaining Supervised Fine-tuning Generalization with
  Less but Harder Data'
arxiv_id: '2510.13892'
source_url: https://arxiv.org/abs/2510.13892
tags:
- data
- thtb
- instruction
- selection
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data

## Quick Facts
- **arXiv ID**: 2510.13892
- **Source URL**: https://arxiv.org/abs/2510.13892
- **Reference count**: 0
- **Primary result**: Maintains or improves SFT performance with only 5% of original training data by selecting higher-cognitive-complexity samples.

## Executive Summary
This paper proposes a three-stage filtering pipeline that dramatically reduces supervised fine-tuning dataset size while maintaining or improving model performance. The key innovation is prioritizing "harder" data based on cognitive complexity rather than simply removing low-quality samples. By selecting data that requires higher-level reasoning (Analyze, Evaluate, Create) and interdisciplinary thinking, the method aims to improve generalization while reducing training time and cost. The approach achieves comparable results on MMLU-Pro and AlpacaEval using only 5% of the original Alpaca dataset.

## Method Summary
THTB employs a sequential three-stage filtering pipeline. Stage 1 uses a reward model to filter out the bottom 80% of samples based on quality. Stage 2 computes an "Intrinsic Hardness" score combining Bloom taxonomy classification (cognitive complexity) and interdisciplinary complexity (semantic distance between domains). Stage 3 calculates an "Extrinsic Hardness" score using instruction response evaluation index (IREI) and Silhouette Coefficient (sample isolation). The final subset contains approximately 5% of the original data, which is then used for supervised fine-tuning with LoRA.

## Key Results
- Maintains comparable performance to full Alpaca dataset on MMLU-Pro and AlpacaEval benchmarks
- Reduces training data from 52K to ~2.6K samples (95% reduction)
- Validated on both general instruction following and specialized TCM gynecology domain
- Shows generalization improvement by focusing on higher-cognitive-level tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selecting data based on cognitive complexity (Bloom's Taxonomy) forces the model to learn reasoning patterns rather than surface-level mimicking, improving generalization.
- **Mechanism:** The framework assigns higher scores to "Analyze," "Evaluate," and "Create" tasks. By filtering out lower-level "Remember" or "Understand" tasks, the training distribution shifts toward samples that require synthesizing information, which acts as a regularizer against overfitting to simple linguistic patterns.
- **Core assumption:** LLMs generalize better when fine-tuned on tasks that demand higher computational reasoning, mirroring human cognitive difficulty hierarchies.
- **Evidence anchors:**
  - [abstract]: "THTB prioritizes higher-level cognitive instructions... offering interpretable and quantifiable criteria."
  - [section 2.2.1]: Defines the Bloom Score based on the taxonomy levels (Remember -> Create).
  - [corpus]: Related work in "Self-Evolving Curriculum for LLM Reasoning" supports the general trend that curriculum difficulty improves reasoning, though THTB specifically operationalizes this via taxonomy.
- **Break condition:** If the base model lacks the foundational knowledge to perform "Analyze" or "Create" tasks, filtering for these levels may yield data that is "too hard," leading to gradient instability or hallucination.

### Mechanism 2
- **Claim:** Interdisciplinary Complexity (IC) serves as a proxy for data hardness by measuring the semantic distance between domains involved in a single instruction.
- **Mechanism:** The mechanism posits that solving a problem requiring knowledge from distant domains (e.g., "Biology" + "Law") is cognitively harder and offers richer training signals than single-domain tasks. It calculates this via the distance between discipline embeddings.
- **Core assumption:** Semantic distance in embedding space correlates with the objective difficulty of reasoning across those domains for a language model.
- **Evidence anchors:**
  - [section 2.2.2]: "The more disciplines an instruction involves, and the greater the distance between these disciplines, the more challenging it is to solve."
  - [corpus]: Weak direct evidence in corpus for "discipline distance" specifically; "Improving Task Diversity" focuses on task variety rather than cross-domain semantic distance.
- **Break condition:** Breaks if the embedding model fails to capture semantic nuance (e.g., mapping unrelated disciplines closely), causing the selection of incoherent or noisy multi-domain samples.

### Mechanism 3
- **Claim:** Filtering for "isolated" data points (via Silhouette Coefficient) improves generalization by focusing on under-represented regions of the data manifold.
- **Mechanism:** By clustering data (TF-IDF) and selecting samples with high Silhouette Coefficients (specifically those far from other clusters or on decision boundaries), THTB identifies "tail" examples that are likely unfamiliar to the model, reducing redundancy.
- **Core assumption:** Data samples that are isolated in the vector space represent unique learning opportunities rather than noise.
- **Evidence anchors:**
  - [section 2.3.2]: "Identify samples... that are both isolated and representative, as these are more likely to be unfamiliar to the model."
  - [section 3.2]: Notes that random sampling outperforms full data, supporting the redundancy reduction hypothesis.
  - [corpus]: "Table Detection with Active Learning" supports the broader principle that selecting informative/uncertain samples improves efficiency, though using Silhouette Coefficient for SFT data selection is a specific THTB choice.
- **Break condition:** If the dataset has distinct noise clusters, this method may actively select low-quality outliers, degrading performance.

## Foundational Learning

- **Concept: Bloom's Taxonomy**
  - **Why needed here:** This is the theoretical backbone of the "Intrinsic Hardness Score." Without understanding the hierarchy (Remember $\to$ Create), one cannot interpret why the system prioritizes specific samples.
  - **Quick check question:** Can you distinguish between an "Apply" task (use a formula) and an "Analyze" task (deconstruct a result) in your specific dataset?

- **Concept: Silhouette Coefficient**
  - **Why needed here:** Used in the "Extrinsic Hardness" stage to quantify how unique a sample is relative to its cluster. Understanding this helps diagnose why certain outliers are kept or discarded.
  - **Quick check question:** Does a high Silhouette score indicate a sample is close to the center of its own cluster, or far from the nearest neighboring cluster? (THTB seeks isolation/uniqueness).

- **Concept: Reward Models (RM)**
  - **Why needed here:** The first filter (Quality Filtering) relies on an RM to score (instruction, response) pairs. Understanding RM alignment is crucial to ensure you don't filter out "hard but unconventional" data.
  - **Quick check question:** If your Reward Model is biased towards short answers, will the THTB pipeline potentially filter out long, complex reasoning chains in the first stage?

## Architecture Onboarding

- **Component map:** Raw Instruction Dataset -> Reward Model (Stage 1, top 20%) -> LLM Classifier + Embedding Model (Stage 2, top 50%) -> Length calc + TF-IDF/Clustering (Stage 3, top 50%) -> Final Subset (~5%)

- **Critical path:** The **Bloom Score classification** (Stage 2). If the LLM classifier fails to accurately categorize the cognitive level of the instruction, the "Harder is Better" hypothesis collapses, and you are left with randomly sorted "high quality" data.

- **Design tradeoffs:**
  - **Cost:** THTB requires inference passes over the *full* dataset using an LLM (for Bloom/Discipline tagging) and a Reward Model *before* training starts. This upfront cost may exceed the training savings for smaller datasets.
  - **Aggression:** The pipeline aggressively discards 95% of data. In low-data regimes (e.g., <1k samples), this strict filtering might remove necessary diversity.

- **Failure signatures:**
  - **Performance Collapse on Simple Tasks:** If the model forgets basic facts, the "hardness" filter may have discarded all "Remember" type data, which is necessary for knowledge retention.
  - **Domain Drift:** If the "Interdisciplinary Complexity" score dominates, the model may become a generalist at the expense of the specific vertical domain you intended to tune for.

- **First 3 experiments:**
  1. **Ablation on Hardness:** Run SFT using only Intrinsic Hardness vs. only Extrinsic Hardness to determine which metric drives performance gains in your domain.
  2. **Scaling Law Check:** Plot performance curves at 2%, 5%, 10%, and 20% data selection to verify that 5% is indeed the optimal point for your specific model size (the paper uses 1B-8B; smaller models may need >5%).
  3. **Noise Stress Test:** Intentionally inject 10% noise into the dataset before running THTB to verify if the Reward Model (Stage 1) successfully catches it before the hardness scoring phases.

## Open Questions the Paper Calls Out

- **Open Question 1:** Is the sequential filtering pipeline (fixed 80%-50%-50% reduction rates) optimal, or do these thresholds require tuning for different base datasets?
  - **Basis in paper:** [inferred] Section 2.4 "Overall Pipeline" specifies fixed percentages without ablating these hyperparameters.
  - **Why unresolved:** The paper does not demonstrate if these specific ratios are robust across datasets of varying initial quality or size.
  - **What evidence would resolve it:** A sensitivity analysis showing model performance variance when altering the filtering ratios (e.g., 90%-60%-40%) on diverse datasets.

- **Open Question 2:** To what extent does the accuracy of the LLM-based classifier for Bloom's Taxonomy levels impact the final SFT performance?
  - **Basis in paper:** [inferred] Section 2.2.1 relies on a large language model to classify cognitive levels, but provides no evaluation of the classifier's accuracy.
  - **Why unresolved:** If the LLM misclassifies "Remember" tasks as "Create" tasks, the fundamental premise of selecting "harder" data fails.
  - **What evidence would resolve it:** A correlation analysis between the classifier's accuracy (verified by human experts) and the resulting model's benchmark scores.

- **Open Question 3:** Does the "Harder The Better" hypothesis hold for domains requiring strict syntactic correctness, such as code generation, where "hard" instructions may imply ambiguity?
  - **Basis in paper:** [inferred] Section 3.3 validates the method on Traditional Chinese Medicine (TCM), a knowledge-heavy domain, but does not test on logical/syntactic domains.
  - **Why unresolved:** The definition of "hardness" via Interdisciplinary Complexity or length may correlate with ambiguity in programming tasks, potentially harming performance.
  - **What evidence would resolve it:** Application of THTB to code generation benchmarks (e.g., HumanEval, MBPP) to verify if "harder" samples improve code correctness.

## Limitations

- The effectiveness of Bloom taxonomy and interdisciplinary complexity as proxies for objective model difficulty remains empirically under-validated across diverse domains.
- The aggressive 95% data reduction may eliminate critical training signals in domains requiring diverse example coverage for proper generalization.
- Reliance on LLM-based classification for Bloom scores introduces potential subjectivity, as different models may interpret cognitive complexity differently.

## Confidence

- **High Confidence**: The empirical results showing 2.6K samples achieving comparable performance to full Alpaca dataset (52K samples) on MMLU-Pro and AlpacaEval. The three-stage filtering methodology is clearly specified and reproducible.

- **Medium Confidence**: The theoretical mechanism linking cognitive complexity to generalization improvement. While the Bloom taxonomy provides a reasonable framework, the direct causal relationship between higher taxonomy levels and model generalization remains to be rigorously established across diverse domains.

- **Low Confidence**: The Interdisciplinary Complexity metric's validity, as the corpus provides limited direct evidence for semantic distance in embedding space correlating with reasoning difficulty. The assumption that domain distance equals cognitive difficulty needs more validation.

## Next Checks

1. **Ablation Study on Filtering Stages**: Systematically disable each of the three filtering stages (Quality, Intrinsic Hardness, Extrinsic Hardness) to quantify their individual contributions to performance gains. This would reveal whether the cognitive complexity hypothesis or other factors drive improvements.

2. **Domain Generalization Test**: Evaluate models trained with THTB-selected data on out-of-distribution tasks not present in the original dataset to verify that the "harder" data truly improves generalization rather than just domain-specific performance.

3. **Robustness to Noise**: Intentionally inject varying levels of noise (10%, 25%, 50%) into clean datasets and test whether THTB's three-stage pipeline successfully identifies and removes corrupted samples while preserving valuable data.