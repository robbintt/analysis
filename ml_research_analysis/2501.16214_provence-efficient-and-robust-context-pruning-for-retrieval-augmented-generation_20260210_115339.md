---
ver: rpa2
title: 'Provence: efficient and robust context pruning for retrieval-augmented generation'
arxiv_id: '2501.16214'
source_url: https://arxiv.org/abs/2501.16214
tags:
- context
- provence
- pruning
- sentences
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Provence introduces an efficient and robust context pruning method
  for retrieval-augmented generation (RAG). It addresses the problem of computational
  overhead and propagation of irrelevant information in RAG by formulating context
  pruning as sequence labeling and unifying it with reranking in a single lightweight
  DeBERTa model.
---

# Provence: efficient and robust context pruning for retrieval-augmented generation

## Quick Facts
- arXiv ID: 2501.16214
- Source URL: https://arxiv.org/abs/2501.16214
- Reference count: 40
- Primary result: Provence achieves negligible to no drop in RAG performance across QA domains while providing substantial context compression

## Executive Summary
Provence introduces a lightweight context pruning method for retrieval-augmented generation that dynamically detects needed pruning per context. The approach formulates pruning as binary sequence labeling and unifies it with reranking in a single DeBERTa model, achieving substantial efficiency gains without sacrificing performance. By generating silver labels through LLM citation prompts and training on MS MARCO data, Provence demonstrates robust performance across multiple QA benchmarks while operating at almost no cost in the RAG pipeline.

## Method Summary
Provence uses a DeBERTa-v3 backbone with two heads: a reranking head (scalar from BOS token) and a pruning head (per-token binary labels). The model is trained from a pretrained cross-encoder reranker using silver labels generated by prompting Llama-3-8B-Instruct to answer questions with citation format [i]. Training combines binary cross-entropy for pruning with MSE regularization between predicted and teacher reranking scores (λ=0.05). Inference applies threshold-based pruning with sentence rounding, operating at negligible overhead due to the unified architecture.

## Key Results
- Achieves quality-compression Pareto frontier on NQ, HotpotQA, PopQA, TyDi QA, BioASQ, SyllabusQA, and RGB benchmarks
- Preserves reranking performance (BEIR nDCG: 55.9 vs baseline 55.4) while adding pruning capabilities
- Outperforms existing methods like LLMLingua2 and RECOMP across multiple QA domains
- Provides substantial context compression (up to 85%) with negligible to no drop in LLM-as-judge performance

## Why This Works (Mechanism)

### Mechanism 1: Cross-Encoder Sequence Labeling for Query-Dependent Pruning
Formulating context pruning as binary sequence labeling with joint query-context encoding enables dynamic, query-aware sentence selection that outperforms fixed-ratio or independent encoding approaches. DeBERTa-v3 encodes concatenated query and context together, outputting per-token binary labels that cluster by sentence. Threshold T binarizes probabilities, with sentence rounding keeping sentences where >50% tokens are selected. Cross-encoding captures query-relevance semantics that independent sentence encoding cannot.

### Mechanism 2: Reranking-Pruning Unification with Score Distillation
Joint training of reranking and pruning heads eliminates pruning's computational overhead while preserving reranking performance via knowledge distillation. The unified model initializes from pretrained cross-encoder reranker, adds pruning head, and trains with loss combining BCE for pruning and MSE regularizer between predicted and teacher reranking scores (λ=0.05). Single forward pass provides both tasks at inference.

### Mechanism 3: Citation-Based Silver Label Generation
Training on silver labels generated by prompting Llama-3-8B-Instruct to answer with citations produces robust, adaptable pruners without manual annotation. For each query-context pair, the LLM answers using only context context and cites sentences with [i] markers. Parsed citations become positive labels, with cases where LLM answers but doesn't cite filtered out. This enables large-scale training without manual annotation.

## Foundational Learning

- **Cross-encoders vs. bi-encoders in IR** - Understanding this tradeoff explains why extractive RECOMP underperforms. Quick check: Why can't you pre-compute embeddings for cross-encoder inference?

- **Token classification / sequence labeling (NER-style)** - The pruning head is trained as per-token binary classification, identical to NER. Quick check: What happens if adjacent tokens get conflicting labels? How does Provence handle this?

- **Knowledge distillation for multi-task preservation** - The reranking regularizer is pointwise score distillation from teacher to student. Quick check: Why use MSE on scores rather than KL divergence on logits for distillation here?

## Architecture Onboarding

- **Component map:**
  Training: Question + Retrieved Passage → Llama-3-8B (citation) → Silver Labels
  Question + Passage → DeBERTa-v3 → [Pruning Head: per-token binary]
                                        → [Rerank Head: BOS scalar] ← MSE from teacher
  Inference: Query → Retriever (SPLADE-v3) → Top-k passages → Provence (unified)
           → [Rerank scores + Pruned passages] → Generator LLM → Answer

- **Critical path:**
  1. Split documents into 1-10 sentence chunks with titles. Retrieve top-5 per query using SPLADE-v3 → DeBERTa reranker.
  2. Generate silver labels with Llama-3-8B-Instruct using citation prompt. Filter incomplete citations. Result: ~370k labeled examples from MS MARCO.
  3. Initialize from naver/trecdl22-crossencoder-debertav3, add pruning head. Train 1 epoch, LR=3e-6, batch=48, λ=0.05.
  4. Apply threshold T ∈ {0.1, 0.5} with sentence rounding (>50% tokens kept).

- **Design tradeoffs:**
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | Token-level vs. sentence-level labels | Richer token representations | Requires sentence rounding post-hoc |
  | Unified vs. standalone pruner | Zero overhead (shared forward pass) | Slight reranking degradation risk |
  | T=0.1 vs. T=0.5 | Higher compression at T=0.5 | Potential recall loss for sparse relevant info |
  | DeBERTa-base vs. -large | Faster inference (base) | Slightly lower compression rate (base) |

- **Failure signatures:**
  - Over-pruning: Generated answers say "No answer" or are ungrounded. Check: compression rate >85%, threshold too aggressive, or domain shift.
  - Under-pruning: Minimal speedup, noise propagation persists. Check: T too low, or model not converging on pruning head.
  - Reranking collapse: Retrieval quality degrades. Check: λ too low, or learning rate too high destabilizing pretrained weights.
  - Position bias: Sentences at context boundaries incorrectly pruned. Check: training data position distribution.

- **First 3 experiments:**
  1. Threshold sweep on validation set: Run Provence with T ∈ {0.05, 0.1, 0.3, 0.5, 0.7}. Plot compression rate vs. QA performance. Identify Pareto-optimal T.
  2. Ablation: standalone vs. unified: Compare reranker → Provence (separate) vs. unified Provence. Measure total latency, reranking NDCG, QA performance.
  3. Domain transfer test: Train on MS MARCO, evaluate on target domain (biomedical, legal). Compare against no pruning, LLMLingua2, RECOMP. If performance gap >5%, consider fine-tuning on domain-specific silver labels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Provence be effectively adapted for multi-passage contexts where reasoning across documents is required?
- Basis in paper: The Conclusion states the model currently operates on "a single passage processed at a time" and suggests future work should consider "multi-passage contexts."
- Why unresolved: The current implementation focuses on pruning individual passages, whereas multi-hop QA requires retaining information that may only appear relevant when viewed with other passages.
- What evidence would resolve it: Experiments on multi-hop datasets measuring performance when Provence is applied independently to each passage versus a modified architecture that processes multiple passages jointly.

### Open Question 2
- Question: Does augmenting training data to include more examples of relevant sentences at the start and end of contexts eliminate the performance drop at extreme context positions?
- Basis in paper: Section 4.3 notes a performance drop at "leftmost and rightmost positions" in needle-in-a-haystack tests, attributing it to a lack of training examples in those positions.
- Why unresolved: While the cause (data imbalance) is hypothesized, the paper does not verify if specific data augmentation for boundary positions successfully corrects the model's bias toward the middle of the text.
- What evidence would resolve it: Ablation studies comparing the current model against one trained on a dataset enriched with relevant sentences specifically placed at index 0 and the final index of the input context.

### Open Question 3
- Question: Does combining Provence with token-level pruning methods yield additive efficiency gains without compromising downstream task performance?
- Basis in paper: The Introduction notes that sentence-level pruning and token-level pruning are "orthogonal and could potentially be combined."
- Why unresolved: The paper evaluates Provence against token-level baselines but does not test a pipeline where Provence prunes sentences first and a token-level pruner compresses the remaining text.
- What evidence would resolve it: Benchmarks measuring the compression rate and QA accuracy of a hybrid pipeline (Provence followed by LLMLingua2) compared to either method used in isolation.

## Limitations

- Dependence on silver labels generated by LLM introduces potential biases and quality concerns that remain untested externally
- Position bias observed in training data may propagate to the pruner, leading to systematic pruning errors
- Contexts exceeding 512-token limit require chunking/truncation, potentially disrupting semantic coherence
- Evaluation relies heavily on LLM-as-judge (SOLAR-10.7B), introducing uncertainty about true quality of pruned contexts

## Confidence

- **High Confidence:** Negligible overhead claim is well-supported by Table 4 showing preserved BEIR performance. Basic architecture and training procedure are clearly specified.
- **Medium Confidence:** Cross-encoding advantage is plausible but lacks direct comparative evidence. Quality-compression tradeoff results are convincing within evaluated domains.
- **Low Confidence:** Generalizability of silver labels across diverse domains and languages is uncertain. Assumption that citation patterns reliably indicate relevance has not been externally validated.

## Next Checks

1. **Cross-Encoder Advantage Validation:** Compare Provence against bi-encoder baseline with identical silver labels and training setup. Measure both compression efficiency and downstream QA performance to quantify cross-encoding advantage.

2. **Silver Label Quality Audit:** Manually annotate 100 random query-context pairs from silver training data. Calculate precision and recall against human judgments. Identify systematic biases and assess impact on pruner performance.

3. **Domain Transfer Robustness Test:** Train Provence on MS MARCO, evaluate on low-resource domain (biomedical or legal) with no fine-tuning. Compare against no pruning, LLMLingua2, RECOMP baselines. If performance degradation exceeds 10%, investigate domain-specific silver labeling.