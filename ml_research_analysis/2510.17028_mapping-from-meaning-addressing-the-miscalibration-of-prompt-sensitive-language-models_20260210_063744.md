---
ver: rpa2
title: 'Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language
  Models'
arxiv_id: '2510.17028'
source_url: https://arxiv.org/abs/2510.17028
tags:
- uncertainty
- calibration
- semantic
- language
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates prompt sensitivity in large language models
  (LLMs), where semantically equivalent prompts produce inconsistent answer distributions,
  leading to poor uncertainty calibration. The authors frame this as token-level overfitting
  and propose sampling across semantic concept space using paraphrasing perturbations
  to recover better-calibrated uncertainty estimates.
---

# Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models

## Quick Facts
- **arXiv ID**: 2510.17028
- **Source URL**: https://arxiv.org/abs/2510.17028
- **Reference count**: 3
- **Primary result**: Paraphrasing perturbations improve LLM uncertainty calibration (AUROC up to 88.1±0.5) without sacrificing accuracy, outperforming temperature scaling and other methods

## Executive Summary
This paper addresses prompt sensitivity in large language models (LLMs), where semantically equivalent prompts produce inconsistent answer distributions, leading to poor uncertainty calibration. The authors propose sampling across semantic concept space using paraphrasing perturbations to recover better-calibrated uncertainty estimates. They introduce an embedding-variance-based uncertainty metric that decomposes total uncertainty into epistemic (inter-sample) and aleatoric (intra-sample) components, improving upon entropy-based methods by modeling semantic continuities. Experiments on TriviaQA and Natural Questions show that paraphrasing perturbations significantly improve calibration without sacrificing accuracy, and reveal that RLHF fine-tuning increases prompt sensitivity.

## Method Summary
The method generates multiple paraphrases of each input question, queries the target LLM multiple times per paraphrase, and embeds all responses into a continuous vector space. Total uncertainty is computed as the trace of the covariance matrix of embeddings, then decomposed into aleatoric uncertainty (intra-paraphrase variance) and epistemic uncertainty (inter-paraphrase variance). The framework assumes that semantically equivalent phrasings should produce consistent outputs, and deviations indicate poor calibration. Paraphrasing serves as a mechanism to sample the semantic concept space rather than overfitting to specific token sequences.

## Key Results
- Paraphrasing perturbations significantly improve calibration (AUROC up to 88.1±0.5) without sacrificing accuracy
- Embedding-variance uncertainty metric outperforms entropy-based methods by modeling semantic continuities
- RLHF fine-tuned models (Llama 2-Chat) show higher prompt sensitivity than base models, measured by epistemic-to-total uncertainty ratio
- Optimal perturbation strategy allocates more budget to paraphrases (n_p) than samples per paraphrase (n_s)

## Why This Works (Mechanism)

### Mechanism 1
- Sampling across the "semantic concept space" via paraphrasing recovers uncertainty calibration in prompt-sensitive LLMs better than sampling from a single token representation
- Core assumption: Paraphrasing perturbations preserve semantic meaning perfectly, and the set of paraphrases adequately samples the semantic concept space
- Evidence anchors: Abstract shows paraphrasing improves calibration; Section 3 claims space average approximates concept uncertainty; corpus supports stochastic evaluation over meaning-preserving perturbations
- Break condition: If paraphrasing introduces semantic drift, the aggregated uncertainty measures noise rather than semantic consistency

### Mechanism 2
- An uncertainty metric based on embedding variance captures semantic continuities and decomposes uncertainty more effectively than entropy-based methods
- Core assumption: The embedding space accurately reflects semantic similarity, and trace of covariance matrix is sufficient for uncertainty
- Evidence anchors: Abstract claims embedding variance improves upon entropy; Section 4 introduces the metric using trace of covariance; corpus supports mapping linguistic uncertainty to internal representations
- Break condition: If embeddings fail to distinguish lexically similar but distinct concepts, variance metric underestimates true uncertainty

### Mechanism 3
- The ratio of Epistemic to Total Uncertainty (ρ_u) serves as a diagnostic for "prompt sensitivity" and post-training overfitting
- Core assumption: Well-calibrated models exhibit low epistemic uncertainty relative to total uncertainty
- Evidence anchors: Section 4 quantifies prompt sensitivity via ρ_u; Section 5 shows chat models have higher ρ_u than base models; corpus corroborates sensitivity as measurable phenomenon
- Break condition: If dataset contains inherently ambiguous questions where multiple interpretations are valid, high epistemic uncertainty might reflect valid semantic ambiguity

## Foundational Learning

- **Uncertainty Calibration**
  - Why needed: The paper's primary goal is fixing "miscalibration" where stated confidence doesn't match actual correctness likelihood
  - Quick check: If a model assigns 80% confidence to 100 answers, how many must be correct for perfect calibration?

- **Epistemic vs. Aleatoric Uncertainty**
  - Why needed: The core contribution decomposes total uncertainty into these components; aleatoric is intra-sample noise, epistemic is inter-sample lack of knowledge
  - Quick check: If you ask a model the same question 10 times and get 10 different answers, is this Aleatoric or Epistemic uncertainty?

- **Token Representation vs. Semantic Concept**
  - Why needed: The paper argues LLMs overfit to specific wording (token representation) rather than reasoning via underlying meaning (semantic concept)
  - Quick check: Why does relying on "token representation" cause a model to be "prompt-sensitive"?

## Architecture Onboarding

- **Component map**: Perturbation Engine -> Sampling Interface -> Embedder -> Variance Calculator -> Calibration Evaluator
- **Critical path**: The Perturbation Engine is critical; if paraphrases aren't semantically invariant, Epistemic uncertainty becomes invalid. Embedder quality is second critical path.
- **Design tradeoffs**: Allocating more budget to paraphrases (n_p) than samples per paraphrase (n_s) often outperforms reverse split but requires higher latency. Embedding variance is computationally cheaper than affinity-graph methods but may lose nuance.
- **Failure signatures**: High ρ_u in Base Models contradicts findings (check paraphrasing quality); Low Calibration AUROC (~0.5) indicates embedding space inappropriateness; Semantic drift indicates paraphrases alter query difficulty.
- **First 3 experiments**:
  1. Baseline Sensitivity: Compare (n_p=1, n_s=6) vs. (n_p=6, n_s=1) on GPT-3.5 or Llama 2 Chat to verify multi-paraphrase sampling yields better calibration
  2. Decomposition Validation: Calculate ρ_u for Llama 2 Base vs. Llama 2 Chat to confirm Chat model exhibits higher Epistemic uncertainty
  3. Metric Comparison: Compare "Variance (Total)" metric against "Entropy" on TriviaQA to validate semantic continuities improve calibration scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does prompt sensitivity in post-trained models stem from structural shift in knowledge encoding or superficial artifact of alignment constraints?
- Basis: Conclusion states future work should explore whether prompt-sensitivity indicates deeper structural shift or superficial artifact of alignment
- Why unresolved: Paper observes RLHF models are more sensitive but doesn't isolate internal mechanism causing miscalibration
- Evidence needed: Mechanistic interpretability studies comparing internal representations between base and aligned models

### Open Question 2
- Question: Does weighting paraphrases by natural language frequency improve theoretical validity of uncertainty estimation?
- Basis: Theoretical motivation assumes each token representation is equally likely to apply Birkhoff ergodic theorem, but high-frequency phrasings are statistically dominant
- Why unresolved: Uniform weighting simplifies math but may misrepresent true semantic concept space if model is more confident in high-frequency phrasings
- Evidence needed: Experiments comparing calibration when weighting by inverse perplexity or corpus frequency against current uniform sampling

### Open Question 3
- Question: Can semantic-invariant perturbation framework maintain accuracy in tasks requiring precise logical structures like mathematics or code?
- Basis: Experiments restricted to TriviaQA and Natural Questions; paraphrasing logical constraints or code syntax carries higher risk of altering ground truth
- Why unresolved: Unclear if semantic-invariance holds for non-natural language tasks where syntax is strictly coupled with meaning
- Evidence needed: Applying framework to reasoning benchmarks or code generation tasks measuring accuracy drop relative to calibration gain

## Limitations
- **Paraphrasing Quality Dependency**: Framework critically depends on perfect semantic preservation by paraphrasing models, which is not specified and could artificially inflate calibration improvements
- **Embedding Space Appropriateness**: Uncertainty decomposition relies on embeddings capturing semantic similarity, but the paper doesn't specify or validate embedding model appropriateness for the domains
- **Generalizability Beyond QA**: Results demonstrated only on TriviaQA and Natural Questions; effectiveness for other task types or domains remains unknown

## Confidence

**High Confidence Claims**:
- Prompt sensitivity is measurable phenomenon in LLMs (AUROC improvements from paraphrasing are well-demonstrated)
- RLHF fine-tuning increases prompt sensitivity (consistent pattern across Llama 2-Base vs. Llama 2-Chat)
- Embedding-variance uncertainty metric provides better calibration than entropy (clear AUROC improvements)

**Medium Confidence Claims**:
- Epistemic/aleatoric decomposition meaningfully separates model uncertainty sources (additive decomposition holds mathematically but semantic interpretation requires validation)
- Paraphrasing perturbations provide semantic coverage of "concept space" (relies on unvalidated assumption about paraphrase distribution)
- Framework generalizes to other black-box LLMs (tested only on GPT-3.5 and Llama 2 family)

## Next Checks
1. **Paraphrase Semantic Validation**: Conduct human evaluation of paraphrased questions to verify semantic preservation and measure semantic drift rates correlated with calibration performance degradation
2. **Cross-Dataset Generalization Test**: Apply framework to different task type (e.g., HellaSwag or Diplomacy) to verify calibration improvements transfer beyond QA datasets and compare to baseline entropy-based methods
3. **Embedding Ablation Study**: Systematically replace embedding model with alternatives (BERT, RoBERTa, domain-specific) and measure impact on uncertainty decomposition quality and calibration performance to validate embedding space quality correlation with metric effectiveness