---
ver: rpa2
title: 'From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training'
arxiv_id: '2509.20072'
source_url: https://arxiv.org/abs/2509.20072
tags:
- audio
- text
- sequence
- tokens
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work identifies a fundamental mismatch in existing audio-language
  models: they uniformly apply autoregressive generation to both text and audio tokens,
  overlooking that text depends on target-target dependencies (causal, sequential)
  while audio is driven by source-target dependencies (conditioning on source text).
  To address this, the authors propose Text-to-Talk (TtT), a unified framework that
  integrates autoregressive text generation with non-autoregressive audio diffusion
  within a single Transformer.'
---

# From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training

## Quick Facts
- **arXiv ID**: 2509.20072
- **Source URL**: https://arxiv.org/abs/2509.20072
- **Reference count**: 40
- **Primary result**: Proposes TtT framework integrating autoregressive text generation with non-autoregressive audio diffusion, achieving state-of-the-art performance among efficient models (3B parameters) on speech-to-speech benchmarks.

## Executive Summary
This work identifies a fundamental mismatch in existing audio-language models: they uniformly apply autoregressive generation to both text and audio tokens, overlooking that text depends on target-target dependencies (causal, sequential) while audio is driven by source-target dependencies (conditioning on source text). To address this, the authors propose Text-to-Talk (TtT), a unified framework that integrates autoregressive text generation with non-autoregressive audio diffusion within a single Transformer. By leveraging the any-order autoregressive property of absorbing discrete diffusion, TtT provides a unified training objective that respects the distinct dependency structures of each modality. The model employs modality-aware attention and three training strategies to reduce train-test discrepancies. Comprehensive experiments on Audio-QA, ASR, AAC, and speech-to-speech benchmarks show that TtT consistently outperforms strong autoregressive and non-autoregressive baselines. Notably, the 3B-parameter Pretrain+TtT model achieves state-of-the-art performance among efficient models, demonstrating that hybrid AR-NAR design enables compact models to match or exceed the capabilities of significantly larger systems.

## Method Summary
TtT uses Qwen2.5-Base backbone (1.5B/3B) with expanded vocabulary including GLM-4-Voice audio tokens and special control tokens. The architecture employs modality-aware attention: text spans use causal attention, while audio spans use bidirectional attention within their own span and causal attention to prior context. Training uses hybrid AR-NAR loss—standard cross-entropy for text positions and absorbing discrete diffusion for audio positions. Three training strategies reduce train-test discrepancy: BANOM (with probability 0.3, skips diffusion masking so text tokens observe clean audio), PPM (preserves prefix audio spans unmasked), and SST (randomly truncates final audio spans). Inference alternates between AR decoding for text and block-wise NAR diffusion for audio, with nucleus sampling (k=10, p=0.95), 200 diffusion steps, block length 32, span length 640, and CFG scale 0.1.

## Key Results
- TtT (1.5B) outperforms autoregressive and non-autoregressive baselines on Audio-QA, ASR, and AAC benchmarks
- Pretrain+TtT (3B) achieves state-of-the-art performance among efficient models on speech-to-speech tasks
- Training strategies (BANOM, PPM, SST) significantly improve performance, with SST particularly important for variable-length audio generation
- The hybrid AR-NAR architecture enables compact models to match or exceed capabilities of significantly larger systems

## Why This Works (Mechanism)

### Mechanism 1: Dependency-Structure-Aligned Training Objective
The framework aligns training objectives with modality-specific dependency structures, using AR loss for text tokens enforcing left-to-right causal prediction while using absorbing discrete diffusion for audio tokens enabling any-order prediction conditioned on source text. This respects text's target-target dependencies while leveraging audio's source-target dependencies. The unified objective L_Unified(x) = L_AR(x) + L_AO(x) provides an upper bound on negative log-likelihood of the joint distribution. If audio tokens exhibit strong sequential dependencies within spans, the any-order assumption weakens and NAR gains diminish.

### Mechanism 2: Modality-Aware Attention Enforcing Causal-Bidirectional Hybrid
The architecture uses explicit attention masking per modality: text tokens attend causally to all prior context while audio tokens use bidirectional attention within their span but causal attention to prior spans and text. This enables parallel training of audio spans in a single forward pass while preventing cross-contamination. Within an audio span, token predictions can leverage future context without degrading quality. If audio tokens require strict temporal causality (e.g., real-time streaming with no lookahead), bidirectional attention becomes invalid.

### Mechanism 3: Train-Test Discrepancy Reduction via Stochastic Strategies
Three complementary training strategies bridge the distribution gap between masked training inputs and clean inference contexts: BANOM (with probability 0.3, skips diffusion masking so text tokens observe clean audio), PPM (preserves prefix audio spans unmasked), and SST (randomly truncates final audio spans, forcing content-aware ⟨EOA⟩ prediction rather than positional bias). The dominant failure modes are exposure bias from masked training and positional overfitting for span termination. If masking probability or truncation probability are poorly tuned, strategies may introduce noise without closing the train-test gap.

## Foundational Learning

- **Autoregressive vs Non-Autoregressive Generation**
  - Why needed here: The paper's core thesis hinges on understanding why AR suits text but NAR suits audio
  - Quick check question: Given sequence [A, B, C], can an NAR model predict B without seeing A? Can an AR model?

- **Absorbing Discrete Diffusion**
  - Why needed here: The audio generation uses this specific diffusion variant; understanding the "any-order AR" equivalence is crucial
  - Quick check question: In absorbing diffusion, what happens to a token once it becomes [M]? What must the model learn to reverse this?

- **Partial Order Factorization**
  - Why needed here: Section 3.3 formalizes the hybrid objective using posets and linear extensions
  - Quick check question: In a partial order where audio tokens within a span form an "antichain," what does that imply about their generation order requirements?

## Architecture Onboarding

- **Component map**: Qwen2.5-Base Transformer -> Expanded vocabulary (V_text ∪ V_audio ∪ S) -> Modality-aware attention -> Hybrid AR-NAR loss -> GLM-4-Voice audio codec
- **Critical path**: Input -> Tokenizer -> Interleaved sequence with ⟨SOA⟩/⟨EOA⟩ markers -> Training: Apply masking per strategy -> Forward pass -> Compute L_AR on text positions, L_AO on masked audio positions -> Inference: AR decode until ⟨SOA⟩ -> Switch to NAR block-wise diffusion -> Upon ⟨EOA⟩, return to AR -> Repeat until ⟨EOS⟩
- **Design tradeoffs**: Block size B (larger increases parallelism but delays ⟨EOA⟩ detection), diffusion steps T (more improve quality but add latency), BANOM/PPM/SST probabilities (higher reduce train-test gap but may slow convergence)
- **Failure signatures**: Run-on audio (SST not applied, model predicts ⟨EOA⟩ only at fixed positions), audio-text misalignment (BANOM disabled, text tokens never see clean audio context), error propagation across spans (PPM disabled, early span errors corrupt later span conditioning)
- **First 3 experiments**: 1) Validate AR-only vs NAR-only baselines on held-out Audio-QA set; confirm hybrid outperforms both. 2) Disable each training strategy individually; measure WER on AISHELL-2 and accuracy on LLaMAQuestions to identify dominant contributor. 3) Test B ∈ {16, 32, 64, 128} while holding T=200; plot first-token latency vs. WER to find efficiency-quality Pareto point.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several critical uncertainties emerge from the methodology and results. The empirical validation focuses heavily on performance metrics while leaving theoretical assumptions about modality-specific dependencies and the practical efficiency of the diffusion-based approach largely unverified.

## Limitations

- The paper's empirical validation has critical gaps—training strategies (BANOM, PPM, SST) appear to contribute more to performance gains than the fundamental AR-NAR architectural innovation itself
- The proposed absorbing discrete diffusion mechanism lacks ablation evidence for diffusion-specific components; gains may come from parallel span processing rather than diffusion formulation
- The "source-target" dependency structure assumption needs more empirical validation, particularly for tasks requiring prosodic or temporal coherence within audio spans

## Confidence

**High confidence**: The empirical superiority of hybrid AR-NAR over pure AR or pure NAR baselines (Table 1 main results). The 3B-parameter Pretrain+TtT achieving state-of-the-art performance among efficient models is directly measurable and reproducible.

**Medium confidence**: The theoretical justification for modality-specific dependency structures (text: target-target, audio: source-target). While the formal arguments in Section 3.3 are sound, the claim that audio tokens depend "mainly" on source text rather than sequential dependencies needs more empirical validation.

**Low confidence**: The three training strategies (BANOM, PPM, SST) as the primary mechanism for performance gains. The ablation results show dramatic drops when strategies are removed, but these strategies appear to address exposure bias and positional artifacts rather than fundamental architectural advantages.

## Next Checks

1. **Ablation of diffusion-specific components**: Replace absorbing discrete diffusion with a simpler non-autoregressive audio generation method (e.g., masked language modeling) while keeping the block-wise parallel processing. If performance remains similar, the diffusion mechanism itself may not be the critical factor.

2. **Dependency structure validation**: Create synthetic audio tasks where temporal dependencies within audio spans are artificially emphasized (e.g., echo cancellation or pitch correction). Test whether the bidirectional attention within audio spans degrades performance compared to causal processing, which would challenge the "source-target" dependency assumption.

3. **Cross-dataset generalization**: Evaluate Pretrain+TtT on out-of-distribution speech-to-speech tasks not included in pretraining (e.g., emotional speech conversion or cross-lingual speech-to-speech translation). This would test whether the dependency-structure alignment generalizes beyond the specific pretraining distribution.