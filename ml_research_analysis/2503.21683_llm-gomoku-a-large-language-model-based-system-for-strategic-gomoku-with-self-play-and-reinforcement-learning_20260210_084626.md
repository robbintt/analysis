---
ver: rpa2
title: 'LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with
  Self-Play and Reinforcement Learning'
arxiv_id: '2503.21683'
source_url: https://arxiv.org/abs/2503.21683
tags:
- gomoku
- game
- language
- position
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LLM-Gomoku, a large language model-based system
  for strategic Gomoku gameplay using self-play and reinforcement learning. The system
  enables LLMs to read the board, understand rules, select strategies, and evaluate
  positions through a carefully designed prompt template and parallel position evaluation
  framework.
---

# LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning

## Quick Facts
- arXiv ID: 2503.21683
- Source URL: https://arxiv.org/abs/2503.21683
- Authors: Hui Wang
- Reference count: 9
- One-line primary result: LLM-based Gomoku system achieves 5x speed improvement and significantly improved gameplay through self-play and reinforcement learning

## Executive Summary
This paper presents LLM-Gomoku, a system that enables large language models to play Gomoku strategically by combining self-play, reinforcement learning, and carefully designed prompting. The system addresses key challenges in using LLMs for board games, including illegal move generation and slow inference times. Through local position evaluation and parallel processing, the system achieves a 5x speed improvement while eliminating illegal moves. After training on 1,046 self-play games, the model demonstrates significantly enhanced Gomoku-playing capabilities, validating the effectiveness of combining LLMs with reinforcement learning for complex strategic games.

## Method Summary
The system uses a large language model to play Gomoku by reading the board state, selecting strategies from a curated set of 52 options, and evaluating positions through a local scoring mechanism. For each move, the system selects one strategy and one analytical logic from a set of 9 options to guide the LLM's reasoning. Instead of directly generating moves, the LLM scores candidate positions and their neighbors, with the highest-scoring legal position selected. Self-play reinforcement learning trains a DQN to select optimal strategy-logic combinations, using per-turn win probability rewards from an evaluator agent. The system employs Ray for parallel position evaluation, reducing inference time from 150 to 28 seconds per move, and uses a database to persist state-action-reward pairs for fault tolerance.

## Key Results
- Eliminates illegal move generation through local position evaluation
- Achieves 5x speed improvement from 150 to 28 seconds per move
- Improves survival steps against AlphaZero from 5 to 12 after training on 1,046 self-play games

## Why This Works (Mechanism)

### Mechanism 1
Local position evaluation reduces illegal move generation by constraining the LLM's output space to pre-validated legal positions. Instead of asking the LLM to directly output a move coordinate (which frequently produces occupied positions), the system (1) identifies candidate move positions and their first-order neighbors, (2) filters for legality, (3) asks the LLM to score each legal position on a 0-10 scale, and (4) selects the highest-scoring legal position. This shifts the LLM's role from move generator to position evaluator. The core assumption is that the LLM can reliably evaluate relative position quality even if it cannot reliably generate legal coordinates directly. Experimental results show that while these two methods can effectively alleviate the issue of selecting illegal positions in the early stages of the game, they still fail to completely overcome this problem as the number of pieces on the board increases.

### Mechanism 2
Decomposing strategic reasoning into explicit strategy selection + analytical logic selection provides structured prompting that improves move quality over unstructured reasoning. The system maintains a curated set of 52 strategies (basic tactics, defensive, offensive, opening methods) and 9 analytical logics (causal, conditional, comparative relationships). For each move, the DQN selects one strategy and one logic to include in the prompt, forcing the LLM to reason through that specific lens rather than generating free-form analysis. The core assumption is that a single strategy-logic combination is sufficient for good decision-making; the selection policy can be learned via RL. During the thinking process, the large language model will select one strategy and one logic from the collected ones for in-depth thinking.

### Mechanism 3
Parallel position evaluation with intermediate rewards enables tractable RL training despite slow LLM inference. Uses Ray framework to assign independent LLM instances to each candidate position for simultaneous scoring (reducing 150s→28s per move). Introduces per-turn win-probability rewards from a dedicated evaluator agent, rather than waiting for game-outcome rewards only. A three-layer MLP DQN (input: 15×15 board, output: 529 strategy-logic combinations) learns action values. The average speed of the entire process has been dramatically reduced from '150 seconds per move' to '28 seconds per move'. This study aims to introduce per-turn rewards in the middle of the game... This Agent assesses the two players in the current game, provides the win rate for each player.

## Foundational Learning

- **Concept: Deep Q-Network (DQN)**
  - **Why needed here:** The system uses DQN to learn which strategy-logic combination to select for each board state. Understanding Q-values, experience replay, and the Bellman equation is essential for debugging training instability.
  - **Quick check question:** Can you explain why DQN uses a target network separate from the main Q-network?

- **Concept: Self-Play Reinforcement Learning**
  - **Why needed here:** The model improves by playing against itself, with game outcomes (win=+10, lose=-10) used as terminal rewards. This is the core training paradigm borrowed from AlphaZero.
  - **Quick check question:** Why might self-play alone lead to strategy collapse (e.g., always playing the same opening)?

- **Concept: Prompt Engineering for Structured Reasoning**
  - **Why needed here:** The system's performance depends on carefully designed prompts that encode board state, rules, selected strategy, and analytical logic. Poor prompt design leads to incoherent LLM outputs.
  - **Quick check question:** What information must be included in a prompt for an LLM to evaluate a Gomoku position?

## Architecture Onboarding

- **Component map:** [Board State] → [Prompt Template] + [Selected Strategy/Logic from DQN] → [LLM Position Scorer] (parallel via Ray) → [Local Position Filter] → [Best Legal Move] → [Evaluator Agent] → [Per-Turn Win Probabilities] → [Reward Signal] → [DQN Update] → [Strategy Selection Policy]

- **Critical path:** 1. Board state serialization and prompt construction 2. DQN forward pass to select strategy-logic pair 3. Parallel LLM scoring of local positions 4. Legal move selection and game update 5. Evaluator agent win-probability computation 6. DQN backward pass with reward signal

- **Design tradeoffs:** Single vs. multi-strategy selection: Current design uses one strategy for speed; multi-strategy would improve comprehensiveness but increase latency and output complexity. Per-turn vs. game-outcome rewards: Per-turn rewards accelerate learning but depend on evaluator accuracy; pure outcome rewards are unbiased but sparse. Local vs. global position evaluation: Local evaluation ensures legality but may miss globally optimal distant moves.

- **Failure signatures:** Infinite loop on illegal positions → Local position evaluation not finding any legal candidates (extend to second-order neighbors). API call timeouts → State-Action-Reward database not persisting; check database write confirmation. DQN divergence (Q-values exploding) → Per-turn reward scaling too large; reduce reward magnitude or add gradient clipping. Slow training (>150s/move persisting) → Ray parallel framework not distributing; verify worker initialization.

- **First 3 experiments:**
  1. Baseline legality test: Run zero-shot prompting on 100 board states; measure illegal move rate. Then run local position evaluation on same states; confirm rate drops to 0%.
  2. Ablation on strategy selection: Compare random strategy selection vs. DQN-learned selection after 500 episodes; measure average survival steps against AlphaZero.
  3. Scalability test: Profile end-to-end latency with 1, 4, 8, and 16 parallel Ray workers; confirm near-linear speedup and identify bottleneck (likely LLM API rate limits).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does utilizing a combination of multiple strategy and analytical logic sets, rather than a single pair, significantly improve the depth of game analysis and win rates?
- Basis in paper: The Discussion section states that limiting selection to one strategy and one logic "limits the comprehensiveness and depth of the model's game analysis," and proposes adopting "multiple sets" in future research.
- Why unresolved: The current architecture simplifies the reasoning process by enforcing a single selection; the efficacy of aggregating multiple conflicting strategies within the LLM's context window remains untested.
- What evidence would resolve it: Ablation studies comparing the "survival steps against AlphaZero" and human-assessed playing levels of the single-strategy model against a multi-strategy ensemble model.

### Open Question 2
- Question: Can integrating expert game data (e.g., from AlphaZero) to guide the LLM's reasoning process accelerate the convergence of reinforcement learning and reduce the reliance on time-consuming self-play?
- Basis in paper: The authors explicitly propose to "use the results of AlphaZero to guide the language model to think in the direction of the most correct move" to address the issue that "self-play process is too time-consuming."
- Why unresolved: The current system relies on random exploration and self-play reinforcement learning which is slow to grasp basic rules; expert distillation has not been implemented.
- What evidence would resolve it: Training curves showing the number of episodes required to reach a specific performance threshold (e.g., 12 survival steps) with and without AlphaZero-guided prompts.

### Open Question 3
- Question: Can Large Vision-Language Models (LVLMs) improve spatial reasoning and reduce illegal move generation compared to the text-based array representation used in the current study?
- Basis in paper: The Future Work section suggests exploring "state-of-the-art large vision-language models" to enhance performance.
- Why unresolved: The current model relies on parsing a text-based array (0, player_id, -player_id) which necessitates a complex "local position evaluation" module to fix illegal moves; visual processing might handle spatial constraints natively.
- What evidence would resolve it: A comparative analysis of illegal move rates and positional accuracy between the text-based LLM-Gomoku and an LVLM variant processing raw board images.

## Limitations
- Reliance on hand-crafted library of 52 strategies and 9 analytical logics limits scalability and comprehensiveness
- Current single strategy-logic selection may be insufficient for complex positions requiring multiple simultaneous considerations
- System remains significantly slower (28s per move) than traditional game engines despite 5x improvement

## Confidence

- **High Confidence:** Local position evaluation effectively eliminates illegal moves through constrained output spaces (supported by Table 1 comparison)
- **Medium Confidence:** Self-play with DQN improves move selection (supported by survival steps improvement from 5 to 12, but dependent on evaluator accuracy)
- **Low Confidence:** Strategic reasoning through single strategy-logic pairs produces optimal moves (acknowledged limitation in paper, future work planned)

## Next Checks
1. Conduct ablation study comparing random strategy selection vs. DQN-learned selection over 500 episodes to quantify learning effectiveness
2. Validate evaluator agent's per-turn win probability estimates by comparing against strong Gomoku engine evaluations
3. Test scalability by measuring latency with 1, 4, 8, and 16 parallel Ray workers to identify bottlenecks and verify claimed 5x speedup