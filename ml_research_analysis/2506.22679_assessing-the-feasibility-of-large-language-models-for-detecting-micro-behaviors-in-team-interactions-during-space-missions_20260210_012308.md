---
ver: rpa2
title: Assessing the feasibility of Large Language Models for detecting micro-behaviors
  in team interactions during space missions
arxiv_id: '2506.22679'
source_url: https://arxiv.org/abs/2506.22679
tags:
- classification
- fine-tuning
- language
- llms
- micro-behaviors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper explores using large language models to detect micro-behaviors\u2014\
  subtle uplifting or discouraging expressions\u2014in team conversations during simulated\
  \ space missions. It compares encoder-only models (RoBERTa, DistilBERT) with decoder-only\
  \ Llama-3.1, using zero-shot, fine-tuned, and few-shot prompting approaches on text-only\
  \ transcripts."
---

# Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions

## Quick Facts
- arXiv ID: 2506.22679
- Source URL: https://arxiv.org/abs/2506.22679
- Reference count: 0
- Explores using large language models to detect micro-behaviors in team conversations during simulated space missions

## Executive Summary
This paper investigates the use of large language models (LLMs) for detecting micro-behaviors—subtle uplifting or discouraging expressions—in team conversations during simulated space missions. The study compares encoder-only models (RoBERTa, DistilBERT) with decoder-only Llama-3.1 using zero-shot, fine-tuned, and few-shot prompting approaches on text-only transcripts. Encoder-only models struggled with underrepresented classes like discouraging speech, even with weighted fine-tuning and paraphrasing. Llama-3.1, using few-shot prompting, achieved the best results: 44% macro F1-score for 3-way classification and 68% for binary classification, with notably higher recall for both uplifting and discouraging behaviors.

## Method Summary
The study used text transcripts from four short-duration HERA mission simulations to train and evaluate LLMs for micro-behavior detection. Three prompting strategies were tested: zero-shot, fine-tuned, and few-shot prompting. Encoder-only models (RoBERTa, DistilBERT) were compared with decoder-only Llama-3.1. Performance was assessed using macro F1-scores and recall for binary (uplifting vs. discouraging) and three-way (uplifting, discouraging, neutral) classifications.

## Key Results
- Llama-3.1 achieved 44% macro F1-score for 3-way classification and 68% for binary classification using few-shot prompting
- Encoder-only models struggled with underrepresented classes like discouraging speech
- Decoder-only models showed notably higher recall for both uplifting and discouraging behaviors

## Why This Works (Mechanism)
Decoder-only models like Llama-3.1 are better suited for nuanced, context-dependent micro-behavior detection due to their generative capabilities and ability to handle complex prompt structures, particularly in few-shot settings.

## Foundational Learning
- **Micro-behaviors**: Subtle expressions that can be uplifting or discouraging in team interactions; crucial for understanding team dynamics in isolated environments
- **Zero-shot prompting**: Using models without task-specific training; useful for initial feasibility assessment
- **Fine-tuning**: Adapting pre-trained models to specific tasks; attempted to improve performance for underrepresented classes
- **Few-shot prompting**: Providing a small number of examples within the prompt; significantly improved Llama-3.1's performance
- **Encoder-only vs. decoder-only architectures**: Different LLM designs with distinct strengths in task performance
- **Macro F1-score**: Evaluation metric that considers both precision and recall across all classes, especially important for imbalanced datasets

## Architecture Onboarding
- **Component map**: Text transcripts -> Pre-processing -> LLM (RoBERTa/DistilBERT/Llama-3.1) -> Prompting strategy -> Classification output
- **Critical path**: Transcript pre-processing -> Prompt engineering -> LLM inference -> Performance evaluation
- **Design tradeoffs**: Text-only analysis vs. multimodal data; encoder-only efficiency vs. decoder-only generative capabilities
- **Failure signatures**: Poor performance on underrepresented classes; inability to capture context-dependent nuances
- **First experiments**: 1) Test additional decoder-only models on the same dataset; 2) Evaluate longer-duration mission transcripts; 3) Incorporate multimodal data for comparison

## Open Questions the Paper Calls Out
None

## Limitations
- Restricted sample size of four short-duration HERA missions
- Binary classification setup showed better performance than three-way classification
- Reliance on text-only transcripts excludes non-verbal and paralinguistic cues

## Confidence
- Confidence in Llama-3.1 outperforming encoder-only models: High
- Confidence in decoder-only models being more effective for micro-behavior detection: Medium
- Confidence in broader applicability to real space missions: Low

## Next Checks
1. Test additional decoder-only models (e.g., GPT-4, Claude) and encoder-only variants on the same dataset to assess robustness of the architecture effect
2. Expand evaluation to longer-duration or real space mission transcripts, if available, to assess model performance across varied interaction contexts
3. Incorporate multimodal data (e.g., audio, video) to evaluate the impact of non-textual cues on micro-behavior detection accuracy