---
ver: rpa2
title: Exploring Magnitude Preservation and Rotation Modulation in Diffusion Transformers
arxiv_id: '2505.19122'
source_url: https://arxiv.org/abs/2505.19122
tags:
- config
- magnitude
- dit-s
- training
- dit-xs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores magnitude preservation and weight control in
  Diffusion Transformers (DiTs) to improve training stability. The authors extend
  magnitude preservation techniques from U-Net to DiTs by ensuring activation magnitudes
  remain constant across all model components without normalization layers.
---

# Exploring Magnitude Preservation and Rotation Modulation in Diffusion Transformers

## Quick Facts
- **arXiv ID**: 2505.19122
- **Source URL**: https://arxiv.org/abs/2505.19122
- **Reference count**: 40
- **Primary result**: Magnitude preservation reduces FID scores by ~12.8% compared to baseline DiTs

## Executive Summary
This paper introduces magnitude preservation techniques for Diffusion Transformers (DiTs) to improve training stability without relying on normalization layers. The authors extend U-Net magnitude preservation concepts to DiTs by ensuring activation magnitudes remain constant across all model components. They also propose rotation modulation as a novel conditioning method that uses learned rotations instead of traditional scaling or shifting, preserving magnitude by design. Through ablation studies on small-scale DiT models trained on ImageNet at 128×128 resolution, the authors demonstrate significant performance improvements while reducing parameter count.

## Method Summary
The method extends magnitude preservation from U-Net to DiTs by constraining activation magnitudes through careful layer design. Key components include cosine attention (removing magnitude dependence), row-normalized linear projections, scaled residual connections using √α scaling, and magnitude-preserving SiLU activation scaled by 1/0.596. Rotation modulation is introduced as an alternative to traditional scale/shift conditioning, applying orthogonal rotations to 2D sub-vectors to preserve magnitude. Training uses high learning rates (1×10⁻² vs standard 1×10⁻⁴) and forced weight normalization to prevent unbounded weight growth.

## Key Results
- Magnitude-preserving strategies reduce FID scores by ~12.8% compared to baseline DiTs
- Rotation modulation combined with scaling performs competitively with Adaptive Layer Normalization (AdaLN) while requiring ~5.4% fewer parameters
- Config E (full magnitude preservation without normalization) achieves 70.86 FID on DiT-S/2

## Why This Works (Mechanism)

### Mechanism 1: Magnitude Preservation via Layer-wise Norm Control
Constraining activation magnitudes across all DiT components improves training stability and reduces FID by approximately 12.8%. Each layer is designed such that output magnitude M[f_MP(x)] ≤ M[x]. Linear projections use row-normalized weights; residual connections apply √α scaling; attention inherently cannot increase magnitude (softmax produces convex combinations). This prevents uncontrolled norm growth without requiring explicit normalization layers.

### Mechanism 2: Rotation Modulation as Magnitude-Preserving Conditioning
Conditioning via learned rotations (instead of scale/shift) preserves token magnitude by design while remaining competitive with AdaLN using fewer parameters. Partition each token into d/2 disjoint 2D sub-vectors and predict a rotation angle θ_i per pair. Apply rotation matrix [[cos θ, -sin θ], [sin θ, cos θ]] to each sub-vector. Since rotation matrices are orthogonal (R^⊤R = I), magnitude is preserved exactly: M[Rx] = M[x].

### Mechanism 3: Forced Weight Normalization for Gradient Control
Normalizing weight vectors to unit magnitude after each training step prevents unbounded weight growth and stabilizes training dynamics. After each gradient update, normalize each weight vector: w_i ← w_i / ||w_i||. This projects gradients onto the tangent plane of the unit hypersphere, preventing magnitude growth while preserving directional updates.

## Foundational Learning

- **Expected Magnitude M[x]**: Defined as √[(1/n) Σ E[x_i²]] for random vector x ∈ Rⁿ.
  - Why needed here: This is the core metric the entire paper optimizes around. All components are designed to preserve or bound this quantity.
  - Quick check question: Given a 64-dim vector with each component having variance σ², what is M[x]? (Answer: σ)

- **Cosine Attention vs Dot-Product Attention**: Cosine attention computes similarity as (q·k)/(||q||·||k||), removing magnitude dependence.
  - Why needed here: Standard dot-product attention allows attention scores to grow with vector magnitudes. Cosine attention prevents this, supporting the magnitude-preservation goal.
  - Quick check question: Why does cosine attention prevent "uncontrollable growth of activations"? (Answer: Similarity depends only on angle, not vector length)

- **Power-Function EMA**: Instead of fixed decay β, uses decay (1 - 1/t)^(γ+1) that adapts over training time.
  - Why needed here: The paper uses this for parameter smoothing. Understanding it is necessary to reproduce the training setup.
  - Quick check question: What advantage does power-function EMA have over standard EMA early in training? (Answer: Places less weight on random initialization)

## Architecture Onboarding

- **Component map**:
  Input (VAE latents + bias channel) → MP Embedding (row-normalized linear) → MP Position Encoding → [DiT Block × N]: → Rotation Modulation (or AdaLN) → Cosine Attention (normalized Q, K) → MP Residual (√α scaling) → MP SiLU (scaled by 1/0.596) → MP Residual → Output projection (row-normalized) Post-training: Forced weight norm on all linear layers

- **Critical path**:
  1. Ensure input latents are normalized (M[x] ≈ σ at entry)
  2. All linear layers use row-normalized weights (Lemma 3)
  3. Attention uses cosine similarity; softmax produces bounded output
  4. Residual connections scaled by √α and √(1-α)
  5. SiLU scaled by 1/0.596 (computed numerically for unit normal)
  6. Rotation modulation applies orthogonal transformation (preserves magnitude)
  7. After each training step, force weight normalization

- **Design tradeoffs**:
  - Runtime: Config E incurs 8.5% overhead vs baseline DiT
  - Modulation choice: Scale+Shift (69.28 FID) > Scale+Rotate (70.86 FID) > Scale alone (72.03 FID)
  - Normalization removal: Config E (no LayerNorm) ≈ Config D (with LayerNorm) in performance, but LayerNorm may mask magnitude drift
  - Learning rate: MP configs require 100× higher LR (1e-2 vs 1e-4)—critical for convergence

- **Failure signatures**:
  - Magnitude explosion across blocks: Config A shows M[x] growing from ~0.5 to ~4.0 after 400K steps (Figure 5)
  - LayerNorm creating artificial spikes: Config D shows MLP input magnitude spike due to normalization
  - Training instability with standard LR: MP configs fail to converge at 1e-4; require 1e-2
  - SiLU breaking magnitude preservation: Cannot extend to arbitrary magnitudes; consider Leaky ReLU alternative (Lemma 6)

- **First 3 experiments**:
  1. **Magnitude baseline**: Train DiT-XS/2 Config A (baseline) vs Config C (MP only) for 100K steps; plot activation magnitude per block at init and after training. Expect Config A to show growth, Config C to remain bounded.
  2. **Modulation ablation**: On DiT-S/2, compare Scale-only, Scale+Shift (AdaLN), and Scale+Rotate with identical training budget. Verify Scale+Rotate achieves within 2 FID points of AdaLN with ~5% fewer parameters.
  3. **Learning rate sensitivity**: Train Config E at LR ∈ {1e-4, 1e-3, 1e-2}. Confirm 1e-4 diverges or stalls; 1e-2 converges. This validates the LR scaling requirement for magnitude-preserving architectures.

## Open Questions the Paper Calls Out
None

## Limitations
- The magnitude preservation framework relies heavily on input feature independence assumptions that may not hold in practice
- The SiLU extension to arbitrary magnitudes is explicitly stated as impossible, limiting the technique's generality
- The rotation modulation's patchified approximation uses only d/2 parameters versus 2d for full scale+shift, potentially constraining expressiveness in high-dimensional spaces

## Confidence

- **High Confidence**: The 12.8% FID improvement from magnitude preservation techniques (supported by direct experimental results in Table 1)
- **Medium Confidence**: Rotation modulation achieving competitive performance with 5.4% fewer parameters (supported by Table 2, but limited to one dataset/scale)
- **Low Confidence**: Claims about forced weight normalization improving stability (Config D shows mixed results, slight FID regression)

## Next Checks
1. Test magnitude preservation on non-i.i.d. inputs by training on datasets with correlated features to validate the independence assumption
2. Compare full n-dimensional rotation modulation versus the patchified approximation on high-dimensional tasks
3. Evaluate the learning rate sensitivity across multiple model scales (beyond XS/S) to verify the 100× LR scaling requirement generalizes