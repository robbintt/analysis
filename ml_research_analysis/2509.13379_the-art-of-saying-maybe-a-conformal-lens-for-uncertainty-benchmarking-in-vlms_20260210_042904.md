---
ver: rpa2
title: 'The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in
  VLMs'
arxiv_id: '2509.13379'
source_url: https://arxiv.org/abs/2509.13379
tags:
- uncertainty
- size
- across
- vlms
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks uncertainty quantification in Vision-Language
  Models (VLMs) using conformal prediction across 18 state-of-the-art models on six
  multimodal datasets. The core method employs three scoring functions (LAC, APS,
  MS) to generate prediction sets with 90% coverage guarantees.
---

# The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs

## Quick Facts
- arXiv ID: 2509.13379
- Source URL: https://arxiv.org/abs/2509.13379
- Reference count: 35
- Primary result: Larger VLMs produce more calibrated uncertainty with smaller prediction sets across 18 state-of-the-art models on six multimodal datasets

## Executive Summary
This study benchmarks uncertainty quantification in Vision-Language Models (VLMs) using conformal prediction across 18 state-of-the-art models on six multimodal datasets. The core method employs three scoring functions (LAC, APS, MS) to generate prediction sets with 90% coverage guarantees. Results show that larger models consistently produce more calibrated uncertainty with smaller prediction sets, correlating with higher accuracy. LAC scoring yields the most compact sets, and uncertainty performance varies by task domain, with visually intensive reasoning tasks showing poorer calibration. The framework also extends to closed-source models via instruction-guided likelihood proxies.

## Method Summary
The method applies conformal prediction to VLMs by extracting token-level log-probabilities for each answer option, then computing three scoring functions (LAC, APS, MS) to determine prediction sets. Models are evaluated on six multimodal multiple-choice datasets split 50/50 into calibration and test sets, with coverage targets set at 90%. For closed-source models, a proxy approach uses instruction-guided prompts to elicit option-wise likelihoods in JSON format, which are then treated as pseudo-probabilities for conformal scoring. The framework measures prediction set size, accuracy, and coverage rate across different model scales and task types.

## Key Results
- Larger VLMs consistently produce smaller prediction sets while maintaining 90% coverage, showing better uncertainty calibration
- LAC scoring function generates the most compact prediction sets compared to APS and MS across all datasets
- Visually intensive reasoning tasks (MathVision) show significantly poorer calibration than text-supportive tasks (ScienceQA)
- The correlation between model accuracy and uncertainty calibration quality suggests scaling benefits both performance and reliability

## Why This Works (Mechanism)
Conformal prediction provides statistical coverage guarantees by constructing prediction sets around model outputs based on calibration data. The framework transforms VLM outputs into uncertainty-aware prediction sets where the model can express "maybe" by including multiple options when confidence is low. The three scoring functions (LAC, APS, MS) provide different ways to rank option plausibility, with LAC using the negative log-probability of the true label, APS using cumulative probability up to the true label, and MS using the margin between top two options. The 90% coverage target ensures that the true answer appears in the prediction set at least 90% of the time, making uncertainty quantification statistically rigorous rather than heuristic.

## Foundational Learning
- Conformal prediction theory: Provides finite-sample, distribution-free coverage guarantees without requiring distributional assumptions
  - Why needed: Traditional uncertainty quantification in VLMs lacks rigorous statistical guarantees
  - Quick check: Verify coverage rate ≥ 90% on calibration set

- Multimodal token probability extraction: Mapping image-question pairs to log-probabilities for each answer option
  - Why needed: VLMs produce token-level outputs that must be aggregated to option-level scores
  - Quick check: Confirm log-prob extraction matches model outputs for simple cases

- Score function design: Different ways to transform token probabilities into uncertainty-aware rankings
  - Why needed: The choice of scoring function affects prediction set size and practical utility
  - Quick check: Compare set sizes across LAC, APS, and MS on validation data

## Architecture Onboarding

**Component map:** Datasets -> Preprocessing -> VLM Inference -> Log-probability Extraction -> Scoring Functions -> Conformal Prediction -> Metrics

**Critical path:** Calibration set → Score computation → Quantile estimation → Test set prediction sets → Coverage verification

**Design tradeoffs:** 
- Coverage vs. set size: Higher coverage guarantees larger prediction sets
- Scoring function choice: LAC minimizes set size but may sacrifice interpretability
- Model scale: Larger models improve calibration but increase computational cost

**Failure signatures:** 
- Coverage < 90%: Indicates miscalibration or insufficient calibration set size
- Empty prediction sets: Score computation or quantile estimation errors
- Inconsistent set sizes: Non-deterministic inference or scoring implementation bugs

**First experiments:**
1. Verify coverage rate on calibration set meets 90% target before testing
2. Compare prediction set sizes across the three scoring functions on a single dataset
3. Test closed-source proxy method by validating JSON schema extraction

## Open Questions the Paper Calls Out
**Open Question 1:** How can conformal prediction frameworks be adapted for open-ended generative tasks in VLMs, moving beyond the multiple-choice constraints used in this study?
- Basis in paper: The Limitations section states, "Future efforts could adapt for generative tasks with unbounded outputs," noting the current study was restricted to multiple-choice datasets.
- Why unresolved: The conformal methods applied (LAC, APS, MS) rely on finite label spaces to calculate scores and thresholds, which do not translate directly to free-form text generation.
- Evidence: A framework that provides statistical coverage guarantees for open-ended text or object detection without relying on fixed option sets.

**Open Question 2:** Can conformal prediction be utilized for dynamic, real-time calibration in deployed VLMs, rather than the static calibration sets used in benchmarks?
- Basis in paper: The Conclusion suggests this work establishes a foundation for "future research on dynamic calibration and adaptive uncertainty in real-world applications."
- Why unresolved: The current methodology relies on a fixed 50/50 split for calibration and testing; it does not explore how prediction sets adapt to continuous data streams or shifting distributions online.
- Evidence: Implementation of an online conformal algorithm for VLMs that maintains coverage guarantees while updating thresholds dynamically during inference.

**Open Question 3:** Why do VLMs exhibit significantly poorer uncertainty calibration on visually-intensive reasoning tasks compared to tasks where images merely support text?
- Basis in paper: Results show high set sizes (poor calibration) for MathVision (visual reasoning) versus low set sizes for ScienceQA (visual support), implying a gap in the model's ability to quantify uncertainty when visual data is the primary reasoning driver.
- Why unresolved: The paper identifies the correlation between task type and calibration quality but does not isolate whether this stems from visual encoder noise, projection layers, or the reasoning architecture itself.
- Evidence: Ablation studies isolating visual extraction and reasoning modules to identify the specific component degrading calibration in visually-intensive domains.

## Limitations
- Evaluation restricted to zero-shot, multiple-choice settings with 90% coverage targets
- Limited model scale range (up to 8B parameters for open models) may not capture full scaling trends
- Closed-source proxy method introduces approximation error not fully characterized

## Confidence
- **High confidence:** Coverage rates meeting 90% target, set size differences across scoring functions, correlation between model size and calibration quality
- **Medium confidence:** LAC scoring advantage (context-dependent performance), calibration gap in visually-intensive tasks (small dataset basis)

## Next Checks
1. Evaluate the conformal framework on open-ended question answering tasks where prediction sets must cover sequences of words rather than discrete options
2. Systematically compare additional scoring functions (entropy-based, margin-based variants) across all datasets
3. Test scaling relationship using models at 30B+ parameters to determine whether observed trends continue or plateau