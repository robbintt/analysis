---
ver: rpa2
title: Watermarking Diffusion Language Models
arxiv_id: '2509.24368'
source_url: https://arxiv.org/abs/2509.24368
tags:
- watermark
- tokens
- token
- diffusion
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce the first watermark tailored for diffusion language
  models (DLMs), an emergent LLM paradigm able to generate tokens in arbitrary order,
  in contrast to standard autoregressive language models (ARLMs) which generate tokens
  sequentially. While there has been much work in ARLM watermarking, a key challenge
  when attempting to apply these schemes directly to the DLM setting is that they
  rely on previously generated tokens, which are not always available with DLM generation.
---

# Watermarking Diffusion Language Models

## Quick Facts
- arXiv ID: 2509.24368
- Source URL: https://arxiv.org/abs/2509.24368
- Reference count: 40
- Primary result: First watermark scheme for diffusion language models achieving >99% TPR@1%FPR with minimal quality impact

## Executive Summary
This work introduces the first watermarking technique specifically designed for diffusion language models (DLMs), which generate tokens in arbitrary order unlike traditional autoregressive models. The key innovation addresses the challenge that DLMs may not have access to previously generated tokens when applying watermarking, which existing autoregressive schemes rely upon. The method applies watermarking in expectation over context distributions and promotes tokens that increase watermark strength when used as context for other tokens, achieving reliable detection while maintaining text quality.

## Method Summary
The method extends Red-Green list watermarking to DLMs by computing expected hash distributions when context is uncertain, then applying exponential tilting to the token probabilities to boost green-list membership. At each diffusion step, the algorithm computes hash probabilities using SumHash or MinHash over the context set, calculates an energy function that measures green-list alignment, and iteratively adjusts token probabilities via KL-constrained optimization. The approach uses δ-parameterization for quality control, applies a single fixed-point iteration for efficiency, and employs top-k approximation for tractable energy computation. Detection uses a standard binomial test on green token counts after deduplication.

## Key Results
- Achieves >99% true positive rate at 1% false positive rate across multiple DLM architectures
- Minimal quality impact: average 1.8% perplexity increase (up to 6.4% at δ=5)
- Robust to various attacks including random edits, context-aware substitution, and hybrid attacks
- Both expectation boost and predictive bias components are necessary for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The watermark can be applied even when context tokens are unknown by computing the expected green-list membership over the distribution of possible context hashes.
- **Mechanism:** Rather than requiring a deterministic hash of previously generated tokens (as in ARLMs), the method computes `h_t(q)`—the probability distribution over hash values given the current token distribution `q`. The watermark boost is then applied as `exp(δG^T h_t(q))`, which weights each token's green-list status by its expected context hash probability.
- **Core assumption:** The hash distribution can be tractably computed from the factorized probability distribution; assumes the DLM provides meaningful probability distributions at each diffusion step.
- **Evidence anchors:**
  - [abstract] "(i) applying the watermark in expectation over the context even when some context tokens are yet to be determined"
  - [Section 3.1, Eq. 2] Shows the expectation formulation: `E_Ω~q[γ̂(Ω)] = (1/L) Σ_t h_t(q)^T · G · q_t`
  - [corpus] DMark (arXiv:2510.02902) independently proposes similar order-agnostic approaches, suggesting the expectation-over-context strategy is a recognized solution pattern for this problem class.
- **Break condition:** If the hash distribution `h_t(q)` becomes approximately uniform (high entropy contexts), the expectation boost provides minimal signal; quality degradation may outweigh detection gains.

### Mechanism 2
- **Claim:** Tokens that create favorable hash conditions for neighboring positions receive additional probability mass, creating a cooperative watermark signal across the sequence.
- **Mechanism:** The predictive bias term `exp(δG p_{t+1})` explicitly promotes tokens at position `t` whose hash values would make likely tokens at position `t+1` green. This creates bidirectional signal amplification unavailable in ARLMs. The gradient `α_t(p)` decomposes into expectation boost plus `Σ_{s≠t} P[H_s(Ω)=h | Ω_t=u](G p_s)_h`.
- **Core assumption:** Future token distributions `p_{t+1}` provide useful signal for current sampling decisions; the DLM's parallel probability estimates are sufficiently calibrated.
- **Evidence anchors:**
  - [abstract] "(ii) promoting tokens which increase the watermark strength when used as context for other tokens"
  - [Section 3.3, Eq. 10] Decomposition shows `exp(δG^T p_{t-1})` (expectation boost) and `exp(δG p_{t+1})` (predictive bias)
  - [Section 4.3, Fig. 4 top right] Ablation confirms both components together outperform either alone
  - [corpus] Corpus evidence is limited for this specific predictive bias mechanism; it appears to be a novel contribution of this work.
- **Break condition:** If the DLM's future-token predictions are poorly calibrated (low correlation with actual generation), the predictive bias may introduce noise rather than signal.

### Mechanism 3
- **Claim:** The optimal watermark distribution follows an exponential tilt of the original distribution, where the tilt magnitude is determined by a KL-divergence quality constraint.
- **Mechanism:** Theorem 3.1 proves `q*_t ∝ p_t exp(δ_t α_t(q*))` where `δ_t` uniquely satisfies `KL(q*_t, p_t) = ε`. This connects quality degradation directly to watermark strength, providing a principled trade-off mechanism. In practice, δ-parameterization (fixed δ) outperforms ε-parameterization because KL-divergence is an imperfect proxy for text quality.
- **Core assumption:** The KL-constraint accurately captures "acceptable" quality degradation; the fixed-point iteration converges to a useful solution.
- **Evidence anchors:**
  - [Section 3.1, Eq. 1] Optimization formulation with KL constraint
  - [Section 3.1, Theorem 3.1] Exact form of optimal solution
  - [Section 4.3, Fig. 4 bottom right] δ-parameterization achieves better detectability-quality trade-offs than ε-parameterization
  - [corpus] LR-DWM (arXiv:2601.12376) appears to address efficiency concerns for similar optimization-based approaches.
- **Break condition:** If KL-divergence constraints are too tight (small ε), watermark becomes undetectable; if too loose, text quality degrades unacceptably.

## Foundational Learning

- **Concept: Discrete Diffusion Models for Language**
  - **Why needed here:** Understanding how DLMs iteratively unmask tokens in arbitrary order is essential to grasping why ARLM watermarking fails and what constraints the solution must satisfy.
  - **Quick check question:** Can you explain why a token's context might be partially unknown during DLM generation, and how this differs from autoregressive generation?

- **Concept: Red-Green List Watermarking (Kirchenbauer et al.)**
  - **Why needed here:** This work extends the Red-Green watermark paradigm; understanding how context hashing drives green-list selection in ARLMs clarifies what must change for DLMs.
  - **Quick check question:** In standard Red-Green watermarking, what information is required to determine whether a token is "green," and why is this problematic for non-sequential generation?

- **Concept: Constrained Optimization with KL-Divergence**
  - **Why needed here:** The theoretical framework formulates watermarking as optimization under KL constraints; comfort with Lagrangian methods and KL properties is prerequisite to understanding Theorem 3.1.
  - **Quick check question:** Why might KL-divergence be an imperfect proxy for text quality, and what alternative parameterization does the paper recommend?

## Architecture Onboarding

- **Component map:** DLM forward pass -> Hash distribution module -> Energy function calculator -> Exponential tilting module -> Modified distribution output
- **Critical path:**
  1. DLM forward pass produces factorized distribution `p ∈ Δ(Σ)^L`
  2. Hash probabilities computed for all positions (parallelizable)
  3. Energy function gradients computed via matrix operations
  4. Exponential tilt applied (1-5 iterations)
  5. Sampling from modified distribution at positions selected for unmasking

- **Design tradeoffs:**
  - **δ vs ε parameterization:** δ is simpler and empirically superior; ε provides theoretical KL guarantees but underperforms.
  - **Context size |C|:** Larger contexts improve security against spoofing but don't significantly affect detectability (Fig. 12).
  - **Top-k approximation:** k=50 provides near-optimal results; lower k reduces computation with minimal strength loss.
  - **Iteration count:** Single iteration sufficient; 3-5 iterations provide marginal improvement with linear cost increase.

- **Failure signatures:**
  - Low TPR with high perplexity increase: δ may be too high or temperature too low (Fig. 8).
  - Strong watermark on entropy-remasking but weak on random remasking: Suggests optimization is overfitting to low-entropy distributions.
  - Detection fails on short sequences (<100 tokens): Inherent statistical limitation; requires longer generation contexts.

- **First 3 experiments:**
  1. **Baseline comparison:** Apply naive ARLM watermark (only watermark when context fully known) vs. proposed method on LLADA-8B with C={-1}, δ=4. Expected: >30% TPR improvement at equivalent perplexity.
  2. **Component ablation:** Run with only expectation boost, only predictive bias, and both components. Expected: both together achieve best quality-detectability trade-off (Fig. 4).
  3. **Robustness stress test:** Apply 20-30% word substitution/deletion to watermarked outputs, measure TPR degradation. Expected: context-aware substitution should be more robust than random due to expectation-over-context mechanism (Fig. 3).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the fixed-point iteration algorithm provably converge to the optimal watermarked distribution?
- **Basis in paper:** [explicit] Section 3.2 states the iterative approach is used "despite the lack of theoretical guarantees of convergence to $q^*$".
- **Why unresolved:** The paper relies on empirical evidence showing improved strength with more iterations but provides no formal mathematical proof of stability or convergence bounds.
- **What evidence would resolve it:** A theoretical proof establishing conditions under which the iterative tilting of probabilities converges to the solution of the constrained optimization problem.

### Open Question 2
- **Question:** Is Kullback-Leibler (KL) divergence a valid proxy for text quality in this watermarking context?
- **Basis in paper:** [explicit] Section 4.3 observes that $\epsilon$-parameterization (using KL) performs worse than $\delta$-parameterization, suggesting "the KL constraint... [is] an imperfect measure of text quality."
- **Why unresolved:** Theoretically, KL divergence is a standard measure of distribution shift, but empirically it appears to constrain the watermark more than necessary for preserving human-perceived quality.
- **What evidence would resolve it:** Identification of an alternative constraint function that correlates better with semantic quality metrics (like GPT-4 scores) while maintaining theoretical guarantees.

### Open Question 3
- **Question:** How can the framework be adapted for open-ended generation where the sequence length is not fixed a priori?
- **Basis in paper:** [inferred] The optimization formulation in Eq. (1) explicitly depends on a fixed sequence length $L$ to compute the hash distribution $h_t$, whereas many generation tasks require dynamic or unknown lengths.
- **Why unresolved:** The algorithm requires computing probabilities for all positions $t \in [1, \dots, L]$ simultaneously to compute the gradient/energy, which is infeasible if $L$ is unbounded.
- **What evidence would resolve it:** A modified algorithm capable of updating the hash distribution dynamically as the sequence length grows or changes during the diffusion process.

### Open Question 4
- **Question:** Does the dependency introduced by "predictive bias" create new vulnerabilities to adversarial watermark removal?
- **Basis in paper:** [inferred] The paper demonstrates robustness to random edits but notes that "predictive bias" creates correlations between tokens to boost greenness, which could theoretically be exploited by context-aware attacks.
- **Why unresolved:** While robustness to substitution is tested, the specific security implications of the inter-token dependencies introduced by the optimization have not been analyzed against adaptive adversaries.
- **What evidence would resolve it:** Evaluation of the watermark's resilience against targeted attacks designed to reverse-engineer or disrupt the predictive bias signal.

## Limitations

- **Context set sensitivity**: The empirical evidence for larger context sets improving robustness against context-aware substitution attacks is limited, with detectability remaining similar across different context sizes.
- **Quality degradation quantification**: The assessment relies heavily on automated metrics (perplexity, GPT-4o scores) without human evaluation studies to confirm semantic quality preservation.
- **Computational overhead**: The exact computational overhead compared to baseline DLM generation is not quantified, though additional steps include hash distribution computation and exponential tilting.

## Confidence

- **High Confidence**: The core mechanism of applying watermarking in expectation over context distributions is well-supported by rigorous mathematical formulation and experimental results demonstrating >99% TPR@1%FPR.
- **Medium Confidence**: The claims about robustness to context-aware substitution attacks and advantages of larger context sets are supported by some evidence but lack comprehensive security analysis against sophisticated attacks.
- **Low Confidence**: The quality degradation assessment relies primarily on automated metrics without human evaluation, and the assumption that KL-divergence constraints accurately capture acceptable quality degradation is theoretically sound but practically unproven.

## Next Checks

1. **Human evaluation study**: Conduct a blinded human evaluation comparing watermarked vs. non-watermarked outputs across different δ values and context sizes. Measure not just perceived quality but also detectability of watermark artifacts to validate automated metric assessments.

2. **Comprehensive security analysis**: Design and execute a battery of attack scenarios including context-aware substitution, rephrasing, and hybrid attacks that combine multiple strategies. Measure TPR degradation across these attacks for different context sizes to validate robustness claims.

3. **Computational overhead benchmarking**: Measure wall-clock time for DLM generation with and without watermarking across different model sizes. Include profiling of hash distribution computation, energy function evaluation, and exponential tilting steps to quantify practical efficiency and identify optimization opportunities.