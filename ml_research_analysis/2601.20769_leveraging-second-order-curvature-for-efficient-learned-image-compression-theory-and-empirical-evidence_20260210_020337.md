---
ver: rpa2
title: 'Leveraging Second-Order Curvature for Efficient Learned Image Compression:
  Theory and Empirical Evidence'
arxiv_id: '2601.20769'
source_url: https://arxiv.org/abs/2601.20769
tags:
- adam
- soap
- image
- compression
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses slow convergence and suboptimal performance
  in learned image compression (LIC) models caused by conflicting rate-distortion
  gradients during training. It introduces SOAP, a second-order quasi-Newton optimizer,
  as a drop-in replacement for first-order methods like Adam.
---

# Leveraging Second-Order Curvature for Efficient Learned Image Compression: Theory and Empirical Evidence

## Quick Facts
- arXiv ID: 2601.20769
- Source URL: https://arxiv.org/abs/2601.20769
- Authors: Yichi Zhang; Fengqing Zhu
- Reference count: 40
- Primary result: SOAP optimizer achieves 70% fewer training steps and 3% BD-Rate improvement over Adam in learned image compression

## Executive Summary
This paper addresses the slow convergence and suboptimal performance in learned image compression (LIC) models caused by conflicting rate-distortion gradients during training. It introduces SOAP, a second-order quasi-Newton optimizer, as a drop-in replacement for first-order methods like Adam. By leveraging curvature information, SOAP resolves gradient conflicts both within individual steps (intra-step) and across consecutive steps (inter-step), enabling faster and more stable convergence. The paper demonstrates significant improvements in training efficiency and model robustness across four top LIC architectures.

## Method Summary
The paper proposes SOAP (Second-Order Adaptive Optimization with Preconditioning) as a second-order quasi-Newton optimizer for learned image compression. SOAP uses a Kronecker-factored approximation of the Gauss-Newton matrix to precondition gradients, resolving the conflicting rate-distortion optimization objectives that plague first-order methods. Unlike Adam which only scales gradients along coordinate axes, SOAP's second-order preconditioning rotates the update basis, allowing it to align the rate and distortion update vectors constructively rather than competitively.

## Key Results
- Reduces training steps by 70% and wall-clock time by 57.7% to reach Adam-level performance across four LIC architectures
- Achieves 3% average BD-Rate improvement when trained for the same number of steps
- SOAP-trained models exhibit significantly fewer activation and latent outliers, enhancing robustness to post-training quantization
- Lower sensitivity to hyperparameter tuning compared to previous LIC optimizers

## Why This Works (Mechanism)

### Mechanism 1: Resolving Intra-Step Gradient Conflicts
The paper argues that Rate and Distortion objectives often have opposing gradients, creating a "tug-of-war" that first-order methods cannot resolve. SOAP's second-order preconditioner leverages curvature information to rotate update directions, aligning Rate ($p_R$) and Distortion ($p_D$) updates constructively. This assumes the component Hessians share sufficient structure with the combined Hessian for effective preconditioning.

### Mechanism 2: Stabilizing Inter-Step Trajectories
SOAP reduces oscillatory behavior by ensuring consecutive update vectors remain highly aligned. Under Lipschitz continuity of the Hessian, the Newton direction changes slowly, preventing the update at step $t+1$ from undoing progress made at step $t$. This linearization of the trajectory prevents the "zigzag" behavior common in ill-conditioned R-D landscapes.

### Mechanism 3: Outlier Suppression via Energy Redistribution
Second-order trained models exhibit lower activation/latent kurtosis through a rotation (via eigenbasis) and scaling mechanism. This allows update energy to diffuse into cross-channel correlations rather than concentrating on diagonal directions that create outliers. The paper uses signal propagation theory to show how SOAP redistributes energy more effectively than diagonal scaling methods.

## Foundational Learning

- **Concept: Rate-Distortion (R-D) Trade-off**
  - Why needed here: Understanding the multi-objective conflict between minimizing bit rate and distortion is essential to appreciate why first-order methods struggle
  - Quick check question: Why does minimizing $L = R + \lambda D$ often result in oscillation for first-order optimizers? (Hint: Gradients of $R$ and $D$ often point in opposing directions)

- **Concept: Newton Preconditioning vs. Diagonal Scaling**
  - Why needed here: Distinguishing SOAP's ability to "rotate" updates from Adam's ability to only "scale" is crucial to understanding how it resolves non-axis-aligned conflicts
  - Quick check question: Can a diagonal preconditioner change the *direction* of a gradient vector, or only its magnitude along existing axes?

- **Concept: Quasi-Newton Methods (Kronecker-Factored)**
  - Why needed here: Understanding how SOAP approximates the inverse Hessian makes the computational feasibility clear
  - Quick check question: How does SOAP approximate the inverse Hessian $H^{-1}$ to make the computation tractable for large LIC models?

## Architecture Onboarding

- **Component map:** Encoder $e(\cdot) \to$ Latent $z \to$ Quantizer $Q(\cdot) \to$ Decoder $r(\cdot)$
- **Critical path:**
  1. Integration: Swap the optimizer in the training script (import SOAP as drop-in replacement)
  2. Training: Run standard R-D training (e.g., on ELIC/TCM)
  3. Convergence Check: Monitor validation loss to verify 60-70% reduction in training steps/time
  4. PTQ Validation: Apply W8A8 quantization and verify $\Delta$BD-Rate improvement (approx. 2% better than Adam baseline)

- **Design tradeoffs:**
  - Speed vs. Per-Step Cost: SOAP increases per-step computation (~1% VRAM overhead) but drastically reduces total steps
  - Hyperparameters: Less sensitive than methods like CMD-LIC, but finding optimal `preconditioner_update_freq` helps
  - Stability: Stable drop-in replacement unlike root-inverse methods that required careful gradient crafting

- **Failure signatures:**
  - Divergence: High learning rates can cause aggressive Newton-like updates to diverge
  - Memory OOM: Kronecker factors require storing second-moment statistics; very large batch sizes might trigger OOM
  - Incompatible Architecture: Ensure using standard SOAP implementation that handles 4D Conv kernels

- **First 3 experiments:**
  1. Baseline Reproduction: Train ELIC with Adam vs. SOAP and plot R-D Loss vs. Wall-Clock Time to verify "Time-to-Adam" reduction
  2. Gradient Conflict Measurement: Log Cosine Similarity ($S^t_{intra}$) between Rate and Distortion updates to verify SOAP alignment vs. Adam oscillation
  3. Quantization Robustness: Train a model, apply W8A8 PTQ, and compare degradation to verify lower Kurtosis and better $\Delta$BD-Rate

## Open Questions the Paper Calls Out
None

## Limitations
- Assumption of shared curvature structure between Rate and Distortion objectives not empirically validated across diverse LIC architectures
- Theoretical analysis of outlier suppression via signal propagation could be more detailed in the main text
- Computational overhead depends on preconditioner update frequency, which may vary across different model scales

## Confidence

- **High Confidence:** Training speed improvements (70% fewer steps, 57.7% wall-clock time reduction) supported by empirical data across multiple benchmarks
- **Medium Confidence:** Mechanism for resolving intra-step gradient conflicts is logically sound but relies on Hessian structure assumptions
- **Medium Confidence:** Improved robustness to PTQ is supported by reduced outlier kurtosis and improved $\Delta$BD-Rate, but theoretical connection could be more explicit

## Next Checks
1. Analyze the actual Hessian or Gauss-Newton matrix of Rate and Distortion objectives for a trained LIC model to verify shared structural properties
2. Design an ablation study comparing SOAP to a hypothetical "diagonal SOAP" to isolate the effect of rotational updates on outlier formation
3. Test SOAP on broader deep learning tasks beyond LIC (image classification, object detection, language modeling) to assess generalizability of benefits