---
ver: rpa2
title: Unveiling Confirmation Bias in Chain-of-Thought Reasoning
arxiv_id: '2506.12301'
source_url: https://arxiv.org/abs/2506.12301
tags:
- reasoning
- bias
- answer
- beliefs
- confirmation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study reveals that large language models exhibit confirmation\
  \ bias in chain-of-thought (CoT) reasoning, where internal beliefs\u2014approximated\
  \ by direct answer probabilities\u2014significantly influence both the generation\
  \ of reasoning steps and the final answer prediction. By decomposing CoT into two\
  \ stages (reasoning generation and reasoning-guided prediction), the authors find\
  \ that stronger beliefs lead to shorter, more confirmatory rationales and reduced\
  \ reliance on the reasoning content itself."
---

# Unveiling Confirmation Bias in Chain-of-Thought Reasoning

## Quick Facts
- arXiv ID: 2506.12301
- Source URL: https://arxiv.org/abs/2506.12301
- Reference count: 19
- Key outcome: Large language models exhibit confirmation bias in chain-of-thought reasoning, where internal beliefs influence both reasoning generation and final answer prediction, explaining differential CoT effectiveness across task types.

## Executive Summary
This study reveals that large language models exhibit confirmation bias in chain-of-thought (CoT) reasoning, where internal beliefs—approximated by direct answer probabilities—significantly influence both the generation of reasoning steps and the final answer prediction. By decomposing CoT into two stages (reasoning generation and reasoning-guided prediction), the authors find that stronger beliefs lead to shorter, more confirmatory rationales and reduced reliance on the reasoning content itself. This bias explains why CoT improves performance more on symbolic tasks than on non-symbolic ones. The study also shows that debiasing is difficult, especially when beliefs are strongly held.

## Method Summary
The study analyzes confirmation bias in CoT reasoning by decomposing the probability P(A,R|Q) into reasoning generation (Q→R) and reasoning-guided prediction (QR→A) stages. Internal beliefs are approximated using direct zero-shot answer probabilities P(A|Q), with entropy serving as a proxy for belief strength. The authors generate CoT rationales using zero-shot prompting, extract intermediate answers via majority voting across multiple LLMs, and compute rationale metrics including relevance, explicitness, informativeness, and sufficiency. Stratified correlation analysis groups questions by entropy to isolate belief effects, examining both inter-group and intra-group correlations between belief strength and rationale attributes.

## Key Results
- Stronger beliefs (lower entropy) correlate with shorter rationales that are more confirmatory and less exploratory
- High-belief questions show reduced reliance on rationale content during final prediction, prioritizing internal beliefs over reasoning
- Confirmation bias explains differential CoT effectiveness: symbolic tasks (math) benefit more than non-symbolic tasks (commonsense) due to varying vulnerability to belief distortion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Internal beliefs, approximated via direct answer probabilities, bias the generation of reasoning rationales.
- Mechanism: The model first forms a prior belief B over answer choices via P(A|Q). During CoT generation, stronger beliefs (lower entropy) produce shorter rationales with higher relevance to the predicted answer and reduced negative justification for alternatives.
- Core assumption: Direct zero-shot answering probabilities serve as a valid proxy for unobserved internal beliefs formed during pretraining.
- Evidence anchors:
  - [abstract] "we examine how model internal beliefs, approximated by direct question-answering probabilities, affect both reasoning generation (Q → R)"
  - [section 5.1, Figure 2] Stronger beliefs correlate with stable predictions regardless of difficulty; low entropy questions retain correctness levels.
  - [corpus] Corpus confirms CoT rationale patterns are actively studied (e.g., "Is Chain-of-Thought Reasoning of LLMs a Mirage?"), but does not directly confirm this specific entropy-rationale length correlation.
- Break condition: If zero-shot probabilities do not reflect latent beliefs (e.g., due to miscalibration), the correlation between entropy and rationale attributes may not indicate true confirmation bias.

### Mechanism 2
- Claim: Stronger beliefs reduce the model's reliance on the generated rationale during answer prediction.
- Mechanism: In the QR → A stage, low-entropy (high-confidence) questions show weaker correlation between rationale attributes (relevance, explicitness, sufficiency) and faithful following. The model prioritizes internal beliefs over the reasoning content.
- Core assumption: Informativeness as a proxy for how much the rationale supports a prediction reflects genuine reasoning-following behavior.
- Evidence anchors:
  - [abstract] "stronger beliefs lead to shorter, more confirmatory rationales and reduced reliance on the reasoning content itself"
  - [section 5.2, Figure 6b] Intra-group analysis shows the importance of relevance, explicitness, and sufficiency increases as beliefs weaken (entropy rises).
  - [corpus] Related work on faithfulness (Lanham et al., 2023) supports unfaithful CoT behavior but does not directly confirm the entropy-modulated reliance mechanism.
- Break condition: If Informativeness metric is confounded by question phrasing or tokenization artifacts, the evolutionary correlation patterns may not reflect belief-driven reliance shifts.

### Mechanism 3
- Claim: Confirmation bias explains differential CoT effectiveness across task types (symbolic vs. non-symbolic).
- Mechanism: Non-symbolic tasks (e.g., commonsense) require implicit, subjective knowledge retrieval, which is more vulnerable to bias. Symbolic tasks (e.g., math) rely on formal logic with objective answers, limiting belief distortion. StrategyQA+F (facts provided) shows improved CoT gains vs. StrategyQA alone.
- Core assumption: The vulnerability ranking of tasks to confirmation bias is correctly ordered based on subjectivity and implicit knowledge requirements.
- Evidence anchors:
  - [abstract] "This bias explains why CoT improves performance more on symbolic tasks than on non-symbolic ones"
  - [section 5.3, Table 1] AQuA (symbolic) shows large CoT improvements; CommonsenseQA shows marginal or negative gains. StrategyQA+F improves more than StrategyQA.
  - [corpus] Related papers confirm CoT helps mainly on math/symbolic reasoning ("To CoT or Not to CoT?"), consistent with the task-vulnerability hypothesis.
- Break condition: If task-specific factors other than subjectivity (e.g., answer option count, question length) confound the vulnerability ranking, the bias-based explanation may be incomplete.

## Foundational Learning

- Concept: **Bayesian decomposition of CoT**
  - Why needed here: The paper factorizes CoT into P(A,R|Q) = P(A|Q,R)P(R|Q). Understanding this decomposition is essential to isolate where beliefs intervene in each stage.
  - Quick check question: Can you write the expanded form including beliefs B and explain which term corresponds to "faithfulness"?

- Concept: **Entropy as confidence proxy**
  - Why needed here: Normalized entropy of P(A|Q) is used throughout to quantify belief strength. Without this, the stratified correlation analysis would be uninterpretable.
  - Quick check question: Given a 4-choice question with probabilities [0.7, 0.2, 0.05, 0.05], compute the normalized entropy and state whether belief is strong or weak.

- Concept: **Stratified correlation analysis**
  - Why needed here: The paper introduces inter-group and intra-group correlation to control for confounding entropy effects. This is the core analytic innovation for disentangling belief influence.
  - Quick check question: Why does aggregating factor x within entropy groups reduce noise from individual questions?

## Architecture Onboarding

- Component map:
  - Belief Estimator -> CoT Generator -> Answer Predictor
  - Belief Estimator -> Rationale Evaluator -> Stratified Analyzer

- Critical path:
  1. Run direct zero-shot to obtain belief proxies (P(A|Q), entropy).
  2. Generate CoT rationales and extract Ainter via majority voting across advanced LLMs.
  3. Compute rationale metrics and stage-wise performance (PerformanceInter, PerformanceE2E).
  4. Perform stratified correlation analysis across entropy bins to isolate belief effects.

- Design tradeoffs:
  - White-box constraint: Entropy-based belief quantification requires access to token log-probabilities; black-box LLMs need alternative confidence estimates.
  - Multiple-choice focus: Open-ended questions require candidate answer pooling, which introduces noise.
  - Single-round CoT: Does not capture iterative belief revision in o1-style models.

- Failure signatures:
  - Rationales consistently reinforce initial wrong answers with high explicitness but low negative relevance.
  - PerformanceInter remains high while PerformanceE2E drops sharply (reasoning is internally consistent but biased from Stage 1).
  - Cross-model debiasing fails: Executor with strong mismatched beliefs cannot follow author's rationale (Table 2).

- First 3 experiments:
  1. Replicate entropy vs. rationale length correlation on your target model and dataset; verify negative correlation holds.
  2. Conduct intra-group correlation of informativeness vs. relevance across entropy bins; confirm importance rises with entropy.
  3. Test cross-model debiasing: Have Model A generate CoT, Model B predict; measure PerformanceInter when initial beliefs conflict with Ainter.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does confirmation bias influence the "thought-switching" behavior in iterative chain-of-thought reasoning found in recent models like o1?
- Basis in paper: [explicit] The authors state in the Limitations section that their experiments focus on one round of CoT, "which overlooks the thought-switching behavior in o1-alike models," and suggest studying iterative CoT could provide deeper insights.
- Why unresolved: It is unclear if iterative reasoning allows models to self-correct strong initial biases or if the biases calcify over multiple steps.
- What evidence would resolve it: A longitudinal analysis tracking belief strength (entropy) and intermediate answers across multiple internal reasoning turns in an iterative model.

### Open Question 2
- Question: Can a metric based on LLM memorization patterns serve as a more accurate proxy for internal beliefs than zero-shot answer entropy?
- Basis in paper: [explicit] The paper notes that confirmation bias likely arises from memorization, and suggests "a promising direction for future work is to develop a more appropriate metric that quantifies internal beliefs based on memorization patterns."
- Why unresolved: The current study uses entropy as a proxy, but the underlying mechanism (memorization of training data) is hypothesized but not directly measured.
- What evidence would resolve it: A new metric that correlates training data exposure or retrieval strength with bias severity more accurately than entropy does.

### Open Question 3
- Question: Can the framework for analyzing confirmation bias be effectively extended to black-box LLMs and open-ended generation tasks?
- Basis in paper: [explicit] The authors acknowledge their method is limited to white-box models and multiple-choice questions due to the need for token probabilities, and propose extending this to open-ended questions as a "promising extension."
- Why unresolved: Uncertainty calibration is difficult for open-ended text, and obtaining probability distributions is not standard for black-box APIs.
- What evidence would resolve it: A method for estimating model confidence in open-ended settings (e.g., using ensemble generation) that successfully replicates the bias correlations found in multiple-choice settings.

## Limitations

- The study's reliance on direct question-answering probabilities as proxies for internal beliefs may not accurately reflect true latent beliefs, especially for models with calibration issues.
- The framework is limited to white-box models and multiple-choice questions, making extension to black-box APIs and open-ended tasks challenging.
- Cross-model debiasing experiments show limited success, but the paper does not explore alternative debiasing strategies beyond executor-following approaches.

## Confidence

- **High confidence**: The observed correlation patterns between belief strength (entropy) and rationale attributes (length, relevance, explicitness) are empirically robust across multiple datasets and models.
- **Medium confidence**: The claim that confirmation bias explains differential CoT effectiveness across task types (symbolic vs. non-symbolic) is plausible but may be confounded by other task-specific factors.
- **Low confidence**: The assertion that cross-model debiasing is inherently difficult due to strong internal beliefs is based on limited experimental evidence and may reflect implementation constraints.

## Next Checks

1. **Validate belief proxy**: Conduct an ablation study comparing the current entropy-based belief proxy against an alternative measure (e.g., calibration error on held-out data or a separate belief elicitation task) to confirm that zero-shot probabilities accurately reflect latent model beliefs.

2. **Test task vulnerability ranking**: Re-run the CoT effectiveness analysis on a broader set of tasks with controlled confounding factors (e.g., matched answer option counts, question lengths) to isolate the effect of subjectivity/implicitness on confirmation bias vulnerability.

3. **Explore debiasing strategies**: Implement and test alternative debiasing approaches beyond cross-model execution, such as iterative belief revision (o1-style), fine-tuning on debiased rationales, or explicit belief negation prompts, to determine whether the observed difficulty is fundamental or method-dependent.