---
ver: rpa2
title: An Open-Source Web-Based Tool for Evaluating Open-Source Large Language Models
  Leveraging Information Retrieval from Custom Documents
arxiv_id: '2502.10916'
source_url: https://arxiv.org/abs/2502.10916
tags:
- speech
- acts
- latest
- experiment
- better
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces an open-source web-based tool for evaluating\
  \ open-source large language models (LLMs) in conversational AI settings. The key\
  \ innovation is incorporating user speech acts\u2014communicative intents\u2014\
  into the conversational agent's input to improve response relevance and alignment."
---

# An Open-Source Web-Based Tool for Evaluating Open-Source Large Language Models Leveraging Information Retrieval from Custom Documents

## Quick Facts
- arXiv ID: 2502.10916
- Source URL: https://arxiv.org/abs/2502.10916
- Reference count: 14
- Introduces an open-source web-based tool for evaluating open-source LLMs in conversational AI with speech act integration

## Executive Summary
This paper presents an open-source web-based evaluation tool for large language models that incorporates speech acts (communicative intents) into conversational AI systems. The tool integrates speech act identification, document-based knowledge retrieval, and comprehensive performance evaluation using 10 standard metrics. Experiments with five open-source models (Llama2:13B, TinyLlama, Llama3-ChatQA, Llama3, Mistral) across two knowledge documents demonstrate that larger models show improved alignment and conversational depth when speech acts are included, while smaller models exhibit increased perplexity and mixed performance. The findings highlight both the potential of speech acts to enhance conversational relevance and the need for model-specific optimizations.

## Method Summary
The tool employs a multi-component architecture where user queries are processed through speech act identification, followed by document retrieval from custom knowledge sources, and finally generation of responses using open-source LLMs. Performance is evaluated across 10 metrics including ROUGE variants, BERT-based precision/recall/F1 scores, QA reference/candidate comparisons, METEOR, and perplexity. The system processes 100 prompts per knowledge document (a technical paper and Wikipedia article) to generate responses with and without speech act integration. The evaluation framework allows for systematic comparison of model performance across different sizes and architectures while measuring both linguistic similarity and knowledge retrieval effectiveness.

## Key Results
- Larger models (Llama2, Llama3) showed improved alignment and higher QA scores when speech acts were included
- Smaller models (TinyLlama) demonstrated increased perplexity and mixed performance with speech act integration
- Computational overhead and response time increases were observed with larger models, though not systematically quantified

## Why This Works (Mechanism)
The tool works by explicitly incorporating communicative intent into the conversational flow through speech act identification. By recognizing and processing user speech acts (requests, questions, commands), the system can better align responses with user intent and retrieve more relevant information from custom documents. This approach bridges the gap between user intent and model response generation, enabling more contextually appropriate and knowledge-grounded conversations. The speech act framework provides structure to otherwise ambiguous user inputs, allowing models to generate responses that are both linguistically similar to reference answers and factually accurate based on retrieved document content.

## Foundational Learning

**Speech Act Theory**: Understanding communicative intent in language
- Why needed: Enables modeling of user intent beyond literal text
- Quick check: Verify speech act identification correctly classifies diverse query types

**Document Retrieval Systems**: Knowledge base access for LLMs
- Why needed: Provides factual grounding for responses
- Quick check: Confirm retrieval returns relevant passages for test queries

**Evaluation Metrics Suite**: Comprehensive performance measurement
- Why needed: Quantifies model effectiveness across multiple dimensions
- Quick check: Validate metric calculations against known reference outputs

**Open-Source LLM Deployment**: Running models without proprietary APIs
- Why needed: Enables research accessibility and customization
- Quick check: Verify model loading and inference work correctly

## Architecture Onboarding

Component map: User Input -> Speech Act Identifier -> Document Retriever -> LLM Generator -> Evaluator

Critical path: The sequence from user query through speech act identification to document retrieval and response generation represents the core workflow. Performance bottlenecks occur at the LLM generation stage, particularly with larger models processing speech act-enriched prompts.

Design tradeoffs: The system prioritizes comprehensive evaluation over real-time performance, accepting increased computational overhead for more thorough analysis. Speech act integration adds complexity but improves response relevance. The choice of 10 evaluation metrics provides breadth but may be redundant in some cases.

Failure signatures: Common failures include speech act misidentification leading to irrelevant responses, document retrieval returning unrelated passages, and LLM generation timeouts with larger models. Model-specific failures include TinyLlama's increased perplexity with complex prompts and general response quality degradation with speech act integration.

First experiments: 1) Test speech act identification accuracy on diverse query types, 2) Verify document retrieval returns relevant passages for known queries, 3) Benchmark baseline LLM performance without speech act integration.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to two knowledge documents (technical paper and Wikipedia article), not representing diverse real-world scenarios
- Computational overhead with larger models noted but not systematically quantified
- Speech act identification accuracy and effectiveness across diverse conversational domains unverified
- Sample size of 100 prompts may not capture edge cases in speech act processing

## Confidence

High confidence:
- Web-based tool functionality and 10 evaluation metrics implementation
- Comparative performance differences between model sizes (Llama2/Llama3 vs TinyLlama)

Medium confidence:
- Speech act integration improves conversational depth and relevance
- Larger models handle speech acts better than smaller models
- Computational overhead findings (lacks quantitative detail)

Low confidence:
- Generalizability to domains beyond technical and factual knowledge
- Long-term effectiveness across varied conversational contexts

## Next Checks

1. Conduct cross-domain evaluation with diverse document types (legal, medical, conversational, creative writing) to assess speech act effectiveness across different knowledge domains and response requirements.

2. Perform systematic benchmarking of computational overhead by measuring exact response time increases, memory usage, and processing costs when speech acts are integrated versus standard prompting across all tested models.

3. Implement ablation studies removing speech act identification to quantify its specific contribution to performance improvements, and test alternative speech act identification methods to validate the current approach's robustness.