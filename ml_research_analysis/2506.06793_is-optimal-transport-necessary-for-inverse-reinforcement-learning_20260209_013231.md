---
ver: rpa2
title: Is Optimal Transport Necessary for Inverse Reinforcement Learning?
arxiv_id: '2506.06793'
source_url: https://arxiv.org/abs/2506.06793
tags:
- reward
- expert
- learning
- rewards
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work challenges the necessity of Optimal Transport in IRL
  by introducing two simple reward functions: Minimum-Distance Reward and Segment-Matching
  Reward. These methods eliminate the need for solving optimization problems and rely
  on spatial proximity, with Segment-Matching adding minimal temporal structure.'
---

# Is Optimal Transport Necessary for Inverse Reinforcement Learning?

## Quick Facts
- arXiv ID: 2506.06793
- Source URL: https://arxiv.org/abs/2506.06793
- Reference count: 16
- Primary result: Simple proximity-based reward functions match or exceed OT-based IRL methods on 32 benchmarks

## Executive Summary
This paper challenges the necessity of Optimal Transport (OT) in Inverse Reinforcement Learning by introducing two simple baselines: Minimum-Distance Reward and Segment-Matching Reward. These methods assign rewards based on spatial proximity to expert demonstrations without requiring optimization. Across extensive experiments in both online and offline RL settings with four downstream algorithms, these simple approaches consistently match or exceed the performance of more complex OT-based methods, suggesting that OT's benefits may stem from basic proximity alignment rather than its optimal coupling formulation.

## Method Summary
The paper proposes two simple reward labeling functions for IRL: Minimum-Distance Reward (Min-Dist) assigns rewards based on negative distance to the nearest expert state, while Segment-Matching Reward (Seg-match) partitions the expert trajectory into segments and assigns rewards based on proximity to temporally-aligned segments. Both methods avoid OT optimization entirely. The approach is evaluated across 32 benchmarks using D4RL datasets (MuJoCo, Adroit, Antmaze) for offline RL and MetaWorld for online RL, with downstream algorithms including IQL, ReBRAC, and DrQ-v2. Reward post-processing includes exponential squashing and rescaling, with specific hyperparameters tuned per environment.

## Key Results
- Min-Dist and Seg-match match or exceed OT performance on locomotion tasks (MuJoCo)
- Simple methods outperform OT on maze navigation tasks (Antmaze)
- Seg-match achieves best overall performance (708.73 total score) matching TemporalOT on MuJoCo
- Min-Dist fails on challenging online manipulation tasks where temporal structure is critical

## Why This Works (Mechanism)

### Mechanism 1: Proximity-Based Reward Assignment
Simple spatial proximity between agent and expert states serves as an effective reward signal. For each non-expert state $s_t$, reward is computed as $r(s_t) = -\min_{s^e \in \tau^e} \text{Dist}(s_t, s^e)$. This treats states near expert demonstrations as "good" without optimization. Core assumption: expert trajectory provides adequate state-space coverage and proximity correlates with task progress. Works well on Antmaze navigation but fails when temporal ordering matters.

### Mechanism 2: Segment-Based Temporal Alignment
Lightweight temporal structure through fixed segment partitioning improves over pure proximity. Partition expert trajectory into $T$ contiguous segments matching agent trajectory length. Each agent state $s_t$ receives reward $r(s_t) = -\min_{s^e \in \Gamma_t} \text{Dist}(s_t, s^e)$ to temporally-corresponding segment. Core assumption: agent and expert progress at similar speeds. Works best on MuJoCo locomotion but degrades when trajectory lengths differ substantially.

### Mechanism 3: OT Benefits May Derive from Proximity
The empirical success of OT-based IRL methods may stem from implicit proximity alignment rather than optimal coupling properties. OT computes coupling minimizing transport cost, but if $\mu^*$ concentrates on nearby states, the effective signal approximates simple nearest-neighbor rewards. Core assumption: OT coupling favors spatially proximate state pairs. Evidence shows Seg-match and Min-Dist match or exceed OT across 23 offline benchmarks.

## Foundational Learning

- **Concept: Inverse Reinforcement Learning (IRL)**
  - Why needed here: IRL recovers reward functions from expert demonstrations without explicit reward labels, which is the paper's premise
  - Quick check question: Given expert trajectories without rewards, can you explain why recovering $R^*$ might enable better generalization than behavior cloning?

- **Concept: Optimal Transport (Wasserstein Distance)**
  - Why needed here: The paper critiques OT-based IRL methods; understanding OT helps contextualize what complexity is being eliminated
  - Quick check question: What does a coupling matrix $\mu$ represent in OT, and why does computing it require iterative optimization?

- **Concept: Offline vs. Online RL Settings**
  - Why needed here: The paper evaluates both settings; offline RL learns from fixed datasets while online RL requires environment interaction
  - Quick check question: In offline IRL, why is reward labeling necessary before applying standard RL algorithms?

## Architecture Onboarding

- **Component map:** Expert Trajectory(s) → Reward Labeling Function → Reward Post-processing → Downstream RL Algorithm → Policy

- **Critical path:**
  1. Load expert trajectory $\tau^e$ and non-expert dataset $D$
  2. For each non-expert state, compute reward using chosen function
  3. Apply exponential squashing: $r \leftarrow \alpha \cdot \exp(\beta \cdot r)$
  4. Rescale rewards by dataset statistics (offline) or first-episode normalization (online)
  5. Train downstream RL algorithm (IQL, ReBRAC, DrQ-v2)

- **Design tradeoffs:**
  - Min-Dist: Maximum simplicity, no temporal awareness, works surprisingly well on Antmaze navigation
  - Seg-match: Minimal temporal structure via partitioning, best overall on MuJoCo, assumes speed alignment
  - OT/TemporalOT: Principled alignment, hyperparameter sensitivity, higher compute cost

- **Failure signatures:**
  - Min-Dist fails on complex online manipulation tasks (MetaWorld) where temporal ordering matters
  - Seg-match degrades when $T \ll T_e$ (agent trajectory much shorter than expert)
  - All methods sensitive to choice of distance metric (cosine outperforms Euclidean)

- **First 3 experiments:**
  1. Reproduce Table 1 results on MuJoCo hopper-medium-replay with IQL: compare OT vs. Seg-match vs. Min-Dist normalized scores
  2. Ablate distance metric: run Seg-match on halfcheetah-medium with cosine vs. Euclidean distance
  3. Test generalization: train on single expert trajectory, evaluate on Antmaze-large-diverse to assess sparse expert coverage limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can proximity-based reward functions match OT-based methods when alignment must occur in learned representation spaces rather than raw state spaces?
- Basis in paper: [explicit] The conclusion states: "Moving forward, a promising direction is to extend the study to alignment in learned representation spaces, where proximity may reﬂect more structured behavior and task-relevant similarities."
- Why unresolved: All experiments use either raw states or fixed vision encoders; no learned representation spaces were tested. The paper's core claim may not generalize when proximity itself is learned.
- What evidence would resolve it: Evaluate Min-Dist and Segment-Matching against OT methods using representations from encoders trained via contrastive learning, VAEs, or reward learning.

### Open Question 2
- Question: What causes Min-Dist to succeed in offline RL but fail catastrophically in online RL settings?
- Basis in paper: [inferred] Table 3 omits Min-Dist because "it fails in these challenging online tasks," yet Tables 1–2 show strong offline performance. No explanation is provided for this discrepancy.
- Why unresolved: The paper attributes OT's benefits to proximity alignment, yet the purest proximity method (Min-Dist) is not viable online. This suggests additional factors may interact with reward design differently across settings.
- What evidence would resolve it: Ablation studies isolating whether failure stems from lack of temporal structure, online exploration requirements, or reward signal characteristics.

### Open Question 3
- Question: Can the gap between IRL-derived rewards and oracle ground-truth rewards be closed while preserving simplicity, or does it fundamentally require more sophisticated methods?
- Basis in paper: [explicit] Appendix A.3 notes: "a more sophisticated IRL method than both OT and Segment-matching may be required to close this gap in future work," referring to the substantial gap to oracle ReBRAC in Antmaze.
- Why unresolved: Even tuned methods achieve ~473–482 total vs. 656.5 oracle—a large gap. The paper demonstrates simplicity suffices to match OT, but not to reach ground-truth reward performance.
- What evidence would resolve it: Systematic analysis of what oracle rewards capture that proximity-based rewards miss; testing whether augmenting simple rewards with learned potential-based shaping closes the gap.

## Limitations
- Performance on tasks requiring complex temporal reasoning or multimodal expert demonstrations remains untested
- Focus on dense state-space environments; behavior in sparse-reward settings or high-dimensional observation spaces is unknown
- Assumption of similar movement speeds between agent and expert could fail dramatically in tasks with variable execution speeds

## Confidence
**High confidence**: The empirical results demonstrating that simple proximity-based rewards can match or exceed OT-based methods on standard benchmarks (MuJoCo, Antmaze, MetaWorld).

**Medium confidence**: The mechanism claim that OT benefits derive primarily from proximity rather than optimal coupling. Requires more ablation studies to definitively separate proximity effects.

**Low confidence**: The generalizability claim that simple baselines will consistently outperform complex OT methods across all IRL applications. Tests a limited set of domains and doesn't explore edge cases.

## Next Checks
1. **Temporal reasoning stress test**: Evaluate Seg-match and Min-Dist on MetaWorld tasks with strong temporal dependencies (e.g., Sawyer assembly tasks) to identify where simple proximity fails versus OT-based methods.

2. **Expert coverage ablation**: Systematically vary the density and diversity of expert demonstrations in Antmaze-large to quantify the exact threshold where Min-Dist's reliance on proximity becomes insufficient.

3. **Cross-domain transfer**: Test whether OT-based methods show advantages when transferring learned rewards between similar domains (e.g., from antmaze-umaze to antmaze-medium-play) compared to proximity-based methods.