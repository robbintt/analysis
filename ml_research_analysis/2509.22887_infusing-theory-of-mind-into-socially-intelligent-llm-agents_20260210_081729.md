---
ver: rpa2
title: Infusing Theory of Mind into Socially Intelligent LLM Agents
arxiv_id: '2509.22887'
source_url: https://arxiv.org/abs/2509.22887
tags:
- goal
- toma
- agent
- agents
- mental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ToMAgent (TOMA), a method for improving LLM
  social reasoning by integrating Theory of Mind (ToM). TOMA combines explicit ToM
  predictions with dialogue lookahead simulation to select mental states and utterances
  that best achieve social goals.
---

# Infusing Theory of Mind into Socially Intelligent LLM Agents

## Quick Facts
- arXiv ID: 2509.22887
- Source URL: https://arxiv.org/abs/2509.22887
- Reference count: 40
- ToMAgent (TOMA) improves LLM social reasoning by integrating Theory of Mind predictions with dialogue lookahead simulation

## Executive Summary
This paper introduces ToMAgent (TOMA), a method for improving LLM social reasoning by integrating Theory of Mind (ToM) predictions with dialogue lookahead simulation. TOMA generates multiple mental state hypotheses per dialogue turn, simulates future conversations using a partner model, and retains high-scoring state-utterance pairs for fine-tuning. Experiments on the Sotopia benchmark show TOMA outperforms baselines, achieving up to 18.9% and 6.9% score improvements over the best base model variants for Qwen2.5-3B and Qwen2.5-7B respectively, and performs competitively with GPT-5-nano.

## Method Summary
TOMA enhances LLM social reasoning through a multi-stage training framework. First, it generates multiple mental state hypotheses per dialogue turn using the model's internal reasoning capabilities. These hypotheses are paired with candidate utterances and evaluated through dialogue lookahead simulation using a partner model. The simulation runs up to four future turns to assess how well each state-utterance pair would achieve social goals. High-scoring pairs are retained for fine-tuning, creating a dataset of ToM-augmented dialogue examples. The final model conditions both mental state predictions and utterances on social goals, enabling strategic reasoning about partner beliefs and intentions.

## Key Results
- TOMA achieves 18.9% score improvement over best Qwen2.5-3B baseline and 6.9% over best Qwen2.5-7B baseline
- TOMA performs competitively with GPT-5-nano on the Sotopia benchmark
- TOMA generates more first-order beliefs and focuses on intentions over emotions, leading to improved social goal achievement

## Why This Works (Mechanism)
TOMA works by explicitly modeling partner mental states and using these predictions to guide strategic conversation planning. The dialogue lookahead simulation provides a forward-looking evaluation of how different mental state hypotheses and utterances will impact social goals. By conditioning both mental state generation and utterance selection on these goals, the model learns to prioritize information that is instrumentally useful for achieving objectives rather than just generating plausible ToM content. The training framework ensures that the model learns to generate mental states that are not only coherent but also strategically valuable.

## Foundational Learning
- Theory of Mind: Understanding that others have beliefs, desires, and intentions different from one's own; needed for modeling social interactions
  - Quick check: Can the agent predict what a partner believes about a situation?
- Dialogue simulation: Generating plausible future conversation turns to evaluate current choices; needed for planning and goal-directed behavior
  - Quick check: Can the agent predict how a partner will respond to different utterances?
- Social goal reasoning: Explicitly optimizing for relationship maintenance and objective achievement; needed for practical social intelligence
  - Quick check: Does the agent balance goal achievement with relationship preservation?

## Architecture Onboarding

### Component Map
TOMA -> Mental State Generator -> Dialogue Lookahead -> Partner Model -> Training Dataset -> Fine-tuned LLM

### Critical Path
The critical path flows from mental state generation through dialogue lookahead simulation to training data creation. Each dialogue turn generates multiple mental state hypotheses, which are paired with candidate utterances. The partner model simulates up to four future turns for each pair, and the resulting scores determine which pairs are retained for fine-tuning. This creates a feedback loop where better mental state predictions lead to better simulated outcomes, which in turn create better training examples.

### Design Tradeoffs
The primary tradeoff is between computational cost and performance. Dialogue lookahead simulation with multiple hypotheses and four-turn horizons is computationally expensive but provides rich training signals. The authors chose to limit simulation to four turns to balance computational feasibility with sufficient planning horizon. Another tradeoff is between generating diverse mental state hypotheses versus focusing on the most likely ones - the approach generates multiple hypotheses to explore the space but must balance diversity with coherence.

### Failure Signatures
The model may generate mental states that are instrumentally useful but factually incorrect about the partner's actual beliefs. Over-reliance on the partner model for simulation could lead to overfitting to simulated rather than real partner behaviors. The focus on intentions over emotions might cause the agent to miss important emotional cues in socially complex situations. Long-horizon planning errors could compound through the simulation process.

### First Experiments
1. Generate mental state hypotheses for a simple dialogue turn and validate their coherence
2. Run dialogue lookahead simulation for a two-turn conversation and verify score correlation with goal achievement
3. Fine-tune a small model on synthetic ToM-augmented data and test on a held-out Sotopia scenario

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains are limited to the Sotopia benchmark and may not generalize to other social reasoning tasks
- The computational overhead of dialogue lookahead simulation during inference is not addressed
- The approach may overfit to simulated partner behaviors rather than real human interactions

## Confidence
- Social goal achievement improvements: Medium (evaluated on controlled benchmark)
- Strategic behavior claims: Medium (qualitative analysis without quantitative validation)
- Computational efficiency: Low (not addressed in experiments)

## Next Checks
1. Test TOMA on diverse social reasoning benchmarks beyond Sotopia to establish generalizability
2. Measure and report inference latency with dialogue lookahead simulation to assess practical deployment feasibility
3. Conduct human evaluation studies to validate that the claimed strategic behaviors align with human perceptions of social intelligence