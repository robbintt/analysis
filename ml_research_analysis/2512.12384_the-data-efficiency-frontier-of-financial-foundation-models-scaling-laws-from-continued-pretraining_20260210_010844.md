---
ver: rpa2
title: 'The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws
  from Continued Pretraining'
arxiv_id: '2512.12384'
source_url: https://arxiv.org/abs/2512.12384
tags:
- loss
- domain
- pretraining
- financial
- validation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the data efficiency of domain-adaptive
  pretraining (DAPT) on financial language by training Llama-3.2 models on SEC filings.
  Models were trained for one epoch over 400M tokens, with validation checkpoints
  at 50M, 100M, 200M, and 400M tokens.
---

# The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining

## Quick Facts
- arXiv ID: 2512.12384
- Source URL: https://arxiv.org/abs/2512.12384
- Reference count: 9
- Models: Llama-3.2-1B and 3B trained on SEC filings

## Executive Summary
This study investigates the data efficiency of domain-adaptive pretraining (DAPT) on financial language by training Llama-3.2 models on SEC filings. Models were trained for one epoch over 400M tokens, with validation checkpoints at 50M, 100M, 200M, and 400M tokens. SEC domain validation loss decreased steadily for both 1B and 3B models, with the largest improvements occurring in the first 200M tokens and diminishing returns thereafter. Power-law analysis revealed shallow scaling exponents, indicating that financial text is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remained effectively unchanged, showing no signs of catastrophic forgetting. A data-efficiency frontier demonstrated that both models improved specialization with negligible degradation on mixed-domain text. These findings suggest that meaningful financial domain adaptation can be achieved with modest token budgets and provide empirical guidance for scaling financial foundation models, indicating that larger models (7B-70B) remain tractable under projected data requirements.

## Method Summary
The study trained Llama-3.2-1B and 3B models on 400M tokens from SEC filings (10-K, 10-Q, DEF 14A) using single-epoch continued pretraining with full parameter updates. Text extraction used EdgarTools with whitelist filtering for narrative sections, followed by MinHash LSH deduplication. Validation occurred every ~25M tokens using 4M-token SEC and Wikipedia holdout sets. The training used AdamW optimizer with fixed learning rate of 5e-6, batch size of 8, and 1024-token context. Power-law scaling analysis was performed in log-log space to characterize learning efficiency, and a data-efficiency frontier was constructed to visualize the specialization-forgetting tradeoff.

## Key Results
- SEC validation loss decreased steadily for both 1B and 3B models, with largest gains within first 200M tokens followed by diminishing returns
- Power-law fits revealed shallow exponents (|b|≪1), indicating highly regular and efficiently learnable financial language
- General-domain validation loss remained effectively constant across all token budgets, showing no catastrophic forgetting
- Data-efficiency frontier showed both models improved specialization with negligible degradation on mixed-domain text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Financial domain language exhibits shallow power-law scaling exponents during continued pretraining, enabling efficient adaptation with modest token budgets.
- Mechanism: SEC filings follow highly regular reporting conventions with consistent structure and recurring terminology. This low-entropy structure allows models to acquire essential financial patterns without requiring the data volumes needed for general-purpose training. The model already possesses broad linguistic structure from initial pretraining, and additional domain tokens primarily refine terminology and phrasing rather than learn fundamental patterns.
- Core assumption: The observed scaling behavior from 1B-3B models extrapolates predictably to larger model scales.
- Evidence anchors:
  - [abstract]: "Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining."
  - [Section 4.2]: "The fitted exponents are shallow (|b|≪1), characteristic of late stage continued pretraining rather than training from scratch."
  - [corpus]: Related work on scaling laws (Hoffmann/Chinchilla cited) establishes token-scaling methodology; neighbor paper "The interplay between domain specialization and model size" addresses similar DAPT scaling questions but corpus lacks direct validation of financial-domain exponent values.
- Break condition: If target financial corpus has high lexical diversity (e.g., earnings call transcripts with unstructured dialogue, analyst notes), expect steeper exponents and higher token requirements.

### Mechanism 2
- Claim: Continued pretraining on narrow domain distributions produces minimal catastrophic forgetting when the domain shift is structurally bounded.
- Mechanism: SEC language introduces specialized vocabulary and document structure but does not require the model to unlearn broad syntactic or semantic patterns. The general-domain representations remain largely intact because the domain-specific adaptation operates on a subset of the model's capacity without conflicting gradient signals.
- Core assumption: Validation loss on Wikipedia proxy adequately captures general-domain capability preservation.
- Evidence anchors:
  - [abstract]: "General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting."
  - [Section 4.3]: "Mixed-domain validation loss remains effectively constant across all token budgets for both the 1B and 3B models... overall variation is extremely small, on the order of 0.01."
  - [corpus]: Weak direct evidence—neighbor papers focus on scaling laws but do not provide comparative forgetting rates across domains.
- Break condition: If continued pretraining extends to highly dissimilar domains (e.g., code, multilingual text) or dramatically larger token budgets, expect measurable general-domain degradation.

### Mechanism 3
- Claim: Domain adaptation gains concentrate in early training tokens, with diminishing returns after the model saturates learnable domain signals.
- Mechanism: The model rapidly acquires domain-specific vocabulary, formatting patterns, and register conventions from initial exposure. As these low-hanging signals are captured, remaining loss reductions require learning increasingly fine-grained distinctions that provide marginal practical value.
- Core assumption: The 400M-token corpus is representative of the broader SEC filing distribution.
- Evidence anchors:
  - [abstract]: "Both models show steady reductions in SEC validation loss with the largest gains within the first 200M tokens, followed by diminishing returns."
  - [Section 4.1]: "Loss drops noticeably between 50M and 200M tokens, after which the rate of improvement slows. Beyond roughly 250M tokens, both curves begin to flatten."
  - [corpus]: Neighbor paper "Scaling Laws for Optimal Data Mixtures" suggests mixture composition affects convergence, but no direct validation of the 200M inflection point for financial domains.
- Break condition: If corpus contains heterogeneous subdomains with distinct vocabularies (e.g., mixing 10-K filings with regulatory commentaries), expect multiple gain phases rather than single early saturation.

## Foundational Learning

- Concept: **Power-law scaling in language models**
  - Why needed here: The paper's central analytical framework assumes familiarity with how validation loss scales as a power function of training tokens (L ∝ N^b) and how log-log plots reveal linear relationships obscured in linear space.
  - Quick check question: If a model's validation loss decreases from 2.5 to 2.3 when trained on 100M tokens, and the scaling exponent is -0.05, what approximate loss would you expect at 400M tokens?

- Concept: **Catastrophic forgetting**
  - Why needed here: The paper explicitly evaluates whether domain specialization degrades general capabilities, using general-domain validation loss as the stability metric.
  - Quick check question: In continued pretraining, what training modifications could reduce forgetting if mixed-domain validation loss began increasing?

- Concept: **Domain-adaptive pretraining (DAPT) vs. fine-tuning vs. training from scratch**
  - Why needed here: The paper positions DAPT as a middle ground between full retraining and task-specific fine-tuning, updating all parameters on domain data without instruction supervision.
  - Quick check question: For a 70B model requiring ~8-15B domain tokens per this paper's projections, what factors would determine whether DAPT is more practical than training a smaller domain-specialized model from scratch?

## Architecture Onboarding

- Component map:
  - Raw SEC filing download via EDGAR -> text extraction (EdgarTools) -> whitelist filtering for narrative content (MD&A, Risk Factors, etc.) -> ~9.6% exclusion
  - MinHash LSH deduplication -> ~1.9% token removal
  - Tokenization and packing -> 400M token corpus
  - Single-epoch continued pretraining with validation checkpoints at ~25M intervals
  - Log-log analysis of loss scaling; data-efficiency frontier plotting

- Critical path:
  1. Raw SEC filing download via EDGAR → text extraction (EdgarTools)
  2. Whitelist filtering for narrative content (MD&A, Risk Factors, etc.) → ~9.6% exclusion
  3. MinHash LSH deduplication → ~1.9% token removal
  4. Tokenization and packing → 400M token corpus
  5. Single-epoch continued pretraining with validation checkpoints at ~25M intervals
  6. Log-log analysis of loss scaling; data-efficiency frontier plotting

- Design tradeoffs:
  - **Full parameter update vs. parameter-efficient tuning**: Paper updates all parameters; adapter-based approaches would reduce compute but may yield different scaling dynamics.
  - **Single-epoch vs. multi-epoch**: Single epoch prevents overfitting but limits exposure to rare financial patterns.
  - **Narrow corpus vs. mixed-domain**: Pure SEC text preserves specialization but may limit transfer to adjacent financial tasks (earnings calls, news).

- Failure signatures:
  - Validation loss plateau before 100M tokens → corpus may lack diversity or contain excessive duplicates
  - General-domain loss increasing → domain shift too extreme; consider data mixing
  - Large gap between training and validation loss → overfitting to corpus artifacts

- First 3 experiments:
  1. Reproduce baseline: Train Llama-3.2-1B on 200M SEC tokens (early saturation point) with identical hyperparameters; verify ~0.02-0.03 SEC validation loss reduction.
  2. Ablate data composition: Train on 200M tokens with 80/20 SEC/Wikipedia mixture; measure whether mixing further reduces general-domain drift (already minimal).
  3. Scale test: Train 3B model on 800M tokens to validate whether scaling exponent holds beyond observed range or if new saturation point emerges.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do reductions in SEC-domain validation loss directly translate to improved performance on downstream financial NLP tasks?
- Basis in paper: [explicit] The authors state that future work must evaluate "how loss reductions translate into task level performance on SEC QA, risk classification, structured extraction, or anomaly detection."
- Why unresolved: The study measured success exclusively via validation loss (perplexity) rather than task-specific benchmarks or functional utility.
- What evidence would resolve it: Correlations between lower perplexity scores and higher F1/accuracy on standardized financial benchmarks (e.g., FinQA).

### Open Question 2
- Question: Do the observed scaling laws and data efficiency frontiers hold for significantly larger models (7B–70B parameters)?
- Basis in paper: [explicit] The paper notes that extrapolations suggest larger models remain tractable, but acknowledges experiments were limited to 1B and 3B models.
- Why unresolved: The power-law fits and "shallow exponent" claims are derived from small-scale models, introducing uncertainty when projecting to foundation-scale models.
- What evidence would resolve it: Replicating the DAPT protocol on 7B–70B parameter models to verify if token requirements scale predictably.

### Open Question 3
- Question: Does incorporating adjacent domains (e.g., earnings calls, news) overcome the diminishing returns observed with pure SEC filing data?
- Basis in paper: [explicit] The authors conclude that achieving larger gains "will likely require larger or more diverse financial corpora, or mixtures that incorporate adjacent domains."
- Why unresolved: The current study used a single-source corpus (SEC filings) which showed saturation/diminishing returns after 250M tokens.
- What evidence would resolve it: Comparative training runs using mixed-domain financial datasets to see if loss curves sustain improvements beyond the 400M token mark.

## Limitations

- The extrapolation from 1B-3B models to projected 7B-70B scaling behavior carries substantial uncertainty, as power-law exponents can vary significantly across model scales.
- The single-epoch training regime limits exposure to rare financial patterns, potentially underestimating token requirements for robust domain specialization.
- The use of Wikipedia as a proxy for general-domain capability preservation may not adequately capture performance drift on truly diverse text sources or downstream tasks.

## Confidence

**High Confidence**: The empirical observations of steady SEC validation loss reduction and stable general-domain performance within the 1B-3B model range. The data-efficiency frontier construction from observed checkpoints is methodologically sound and directly supported by the experimental results.

**Medium Confidence**: The characterization of financial text as having low entropy and shallow scaling exponents. While supported by the observed power-law fits, this depends on the assumption that the 400M-token corpus is representative and that the shallow exponents extrapolate predictably to larger models.

**Low Confidence**: The projections for 7B-70B models requiring 8-15B domain tokens. These estimates extrapolate beyond the experimental scope without empirical validation and may not account for scale-dependent changes in learning dynamics or corpus requirements.

## Next Checks

1. **Scale extrapolation validation**: Train Llama-3.2-7B on 1-2B tokens of SEC data to empirically verify whether the shallow scaling exponents observed in 1B-3B models persist at larger scales, or if steeper exponents emerge that would invalidate the 8-15B token projections.

2. **General-domain drift assessment**: Replace the Wikipedia validation proxy with diverse multi-domain test sets (including news, social media, and academic text) to measure actual general-domain capability preservation across the full specialization spectrum, particularly at the 200M+ token inflection point.

3. **Corpus diversity stress test**: Create variant training corpora with controlled diversity levels (e.g., 100% 10-K filings vs. mixed 10-K/10-Q/DEF 14A vs. inclusion of earnings call transcripts) to determine whether the observed early saturation at ~200M tokens is robust to domain heterogeneity or represents corpus-specific overfitting.