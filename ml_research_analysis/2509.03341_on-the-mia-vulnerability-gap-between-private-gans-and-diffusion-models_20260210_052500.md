---
ver: rpa2
title: On the MIA Vulnerability Gap Between Private GANs and Diffusion Models
arxiv_id: '2509.03341'
source_url: https://arxiv.org/abs/2509.03341
tags:
- training
- privacy
- loss
- stability
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the privacy vulnerability gap between differentially
  private GANs and diffusion models against membership inference attacks (MIAs). The
  authors provide a theoretical analysis showing that GANs exhibit fundamentally lower
  sensitivity to data perturbations than diffusion models due to their training dynamics
  - specifically, diffusion models' weighted multi-pass denoising objective amplifies
  the effect of parameter changes.
---

# On the MIA Vulnerability Gap Between Private GANs and Diffusion Models

## Quick Facts
- arXiv ID: 2509.03341
- Source URL: https://arxiv.org/abs/2509.03341
- Reference count: 7
- Key outcome: DP-GANs leak significantly less membership information than DP-diffusion models under identical privacy budgets

## Executive Summary
This paper investigates the privacy vulnerability gap between differentially private GANs and diffusion models against membership inference attacks (MIAs). The authors provide a theoretical analysis showing that GANs exhibit fundamentally lower sensitivity to data perturbations than diffusion models due to their training dynamics. Specifically, diffusion models' weighted multi-pass denoising objective amplifies the effect of parameter changes. Empirically, they conduct a comprehensive study on MNIST using standardized evaluation metrics. Results consistently show that DP-GANs leak significantly less membership information than DP-diffusion models under identical privacy budgets, with attack AUC scores dropping near random guessing for moderate GAN privacy budgets while diffusion models retain non-trivial leakage even at high privacy budgets.

## Method Summary
The authors combine theoretical analysis with empirical validation to compare privacy leakage between DP-GANs and DP-diffusion models. They derive formal bounds relating model stability to membership inference advantage, proving that lower stability leads to higher MIA risk. The theoretical framework shows that GANs have lower sensitivity to data perturbations due to their training dynamics compared to diffusion models' weighted multi-pass denoising objective. Empirically, they conduct experiments on MNIST using standardized MIA evaluation metrics, measuring attack AUC scores across different privacy budgets (epsilon values) for both model types under identical differential privacy guarantees.

## Key Results
- GANs exhibit fundamentally lower sensitivity to data perturbations than diffusion models due to training dynamics
- Lower stability leads to higher MIA risk, with formal bounds relating stability to membership advantage
- DP-GANs leak significantly less membership information than DP-diffusion models under identical privacy budgets
- Attack AUC scores drop near random guessing (0.5) for moderate GAN privacy budgets (ε ≤ 10)
- Diffusion models retain non-trivial leakage even at high privacy budgets (ε = 1)

## Why This Works (Mechanism)
The mechanism centers on the fundamental difference in training dynamics between GANs and diffusion models. GANs operate through adversarial training where the generator learns to produce samples that fool the discriminator, creating a relatively stable training process with lower sensitivity to individual data points. Diffusion models, conversely, rely on a weighted multi-pass denoising objective where each denoising step amplifies parameter changes, making them more sensitive to data perturbations. This higher sensitivity directly translates to greater membership information leakage during inference attacks.

## Foundational Learning
- Differential Privacy: Provides mathematical guarantees for privacy preservation by bounding the influence of individual data points
  - Why needed: Essential framework for comparing privacy guarantees between models
  - Quick check: Verify ε-differential privacy definitions and composition theorems are correctly applied

- Membership Inference Attacks: Methods to determine whether specific data points were used in training
  - Why needed: Primary threat model for evaluating privacy leakage
  - Quick check: Confirm attack success is measured appropriately (AUC scores, advantage metrics)

- Model Stability: Measures how sensitive model parameters are to training data perturbations
  - Why needed: Key theoretical link between training dynamics and privacy vulnerability
  - Quick check: Validate stability bounds are correctly derived and applied

- Weighted Multi-pass Denoising: Diffusion model training objective involving sequential noise removal steps
  - Why needed: Explains why diffusion models have higher sensitivity
  - Quick check: Verify the amplification effect through denoising steps is correctly characterized

- Sensitivity Analysis: Quantification of how much model outputs change with input variations
  - Why needed: Core concept connecting training dynamics to privacy leakage
  - Quick check: Confirm sensitivity bounds are correctly computed for both model types

## Architecture Onboarding

Component Map: Data → Training Process → Model Parameters → Inference → MIA Attack

Critical Path: Training Dynamics → Sensitivity → Privacy Leakage → Membership Inference Risk

Design Tradeoffs: The paper reveals a fundamental trade-off between sample quality and privacy robustness. While diffusion models typically generate higher-quality samples, their training dynamics make them more vulnerable to MIAs. GANs, though potentially producing lower-quality samples, offer stronger inherent privacy guarantees due to their more stable training process.

Failure Signatures: High MIA AUC scores despite strong privacy budgets indicate model architecture may be amplifying privacy leakage beyond what privacy parameters alone would suggest. Persistent non-random attack performance even at low epsilon values signals fundamental architectural vulnerability.

3 First Experiments:
1. Replicate the MNIST experiments comparing DP-GAN vs DP-diffusion model MIA vulnerability across epsilon values 0.1 to 10
2. Measure and compare sensitivity of both model types to data perturbations during training
3. Evaluate sample quality (FID scores) alongside MIA performance to quantify the quality-privacy trade-off

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation limited to MNIST, a relatively simple dataset, constraining generalizability
- Theoretical framework assumes specific conditions around parameter sensitivity that may not capture all practical variations
- Does not fully explore the interplay between privacy properties and generated sample quality

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical analysis linking stability to MIA vulnerability | High |
| Formal proof structure connecting training dynamics to privacy leakage | High |
| Empirical results showing privacy gap on MNIST | Medium |
| Claims about architecture's fundamental role in privacy leakage | Medium |
| Theoretical bounds on membership advantage | Medium |

## Next Checks
1. Replicate experiments on more complex datasets (CIFAR-10, CelebA) to assess scalability of findings
2. Test whether the stability-MIA relationship holds when varying network architectures within each model class
3. Investigate the sample quality-privacy trade-off by measuring FID scores alongside MIA performance across privacy budgets