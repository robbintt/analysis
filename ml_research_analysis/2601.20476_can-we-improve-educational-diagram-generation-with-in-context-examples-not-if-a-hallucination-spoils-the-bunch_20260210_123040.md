---
ver: rpa2
title: Can We Improve Educational Diagram Generation with In-Context Examples? Not
  if a Hallucination Spoils the Bunch
arxiv_id: '2601.20476'
source_url: https://arxiv.org/abs/2601.20476
tags:
- diagram
- text
- hallucination
- which
- diagrams
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates a novel RST-based in-context learning method
  for educational diagram generation. The approach uses RST analysis to select relevant
  examples for diagram generation, aiming to reduce hallucination and improve diagram
  quality.
---

# Can We Improve Educational Diagram Generation with In-Context Examples? Not if a Hallucination Spoils the Bunch

## Quick Facts
- arXiv ID: 2601.20476
- Source URL: https://arxiv.org/abs/2601.20476
- Authors: Evanfiya Logacheva; Arto Hellas; Tsvetomila Mihaylova; Juha Sorva; Ava Heinonen; Juho Leinonen
- Reference count: 40
- Primary result: RST1 with o3 produced highest quality diagrams free from hallucination

## Executive Summary
This study evaluates a novel RST-based in-context learning method for educational diagram generation. The approach uses RST analysis to select relevant examples for diagram generation, aiming to reduce hallucination and improve diagram quality. Computer science educators assessed 150 diagrams generated by GPT-4o and o3 models using a rubric covering logical organization, connectivity, and layout aesthetic, while also detecting factual and faithfulness hallucination. Results show that o3 outperformed GPT-4o, and RST1 (using source texts as examples) produced the highest quality diagrams free from hallucination. Automated evaluation with in-context examples and rubric-based instructions moderately aligned with human assessment.

## Method Summary
The method employs a 7-step pipeline for educational diagram generation using LLMs. Step 1 performs RST analysis on source text, Step 2 selects the most similar pre-made example from a dictionary, and Step 4 generates Graphviz dot code using in-context learning with the selected example. The pipeline includes iterative repair loops (Steps 5, 7) to fix syntax errors and a refinement step (Step 6) for improvement. Three methods are compared: RST1 (source text as input), RST2 (RST-analyzed text as input), and 0-shot baseline. Evaluation uses human expert assessment on a 3-criteria rubric and hallucination detection across multiple types.

## Key Results
- o3 model outperformed GPT-4o across all evaluation metrics
- RST1 method (using source text as examples) produced the highest quality diagrams free from hallucination
- Complex input contexts increased hallucination risk, with inherited hallucination from generation pipelines degrading quality
- Automated evaluation moderately aligned with human assessment (Krippendorff's α 0.3-0.66)

## Why This Works (Mechanism)

### Mechanism 1
RST-based example selection improves diagram faithfulness by structuring input context coherence relations. RST analysis decomposes source text into Elementary Discourse Units (EDUs) and their hierarchical relations. The model selects the most structurally similar pre-made example from a dictionary based on RST patterns, providing in-context clues about how to map coherence relations (elaboration, sequence, contrast, condition) to visual layout choices. Core assumption: LLMs can implicitly recognize and transfer discourse structure patterns from examples to new inputs, even without explicit RST terminology in the final prompt. Break condition: If source texts contain coherence relations not covered in the 4 pre-made examples, or if EDU parsing fails, the similarity search may select suboptimal examples.

### Mechanism 2
Multi-step pipelines with intermediate outputs enable iterative repair but risk inherited hallucination propagation. The pipeline separates concerns—Step 1 produces structured analysis, Step 2 selects examples, Step 4 generates code, Steps 5-7 repair syntax errors. Each step's output becomes the next step's input, allowing targeted fixes. However, faithfulness hallucinations from early steps cascade as Hinh, degrading final quality even if later steps execute correctly. Core assumption: Syntactic correctness is achievable through iterative repair, but semantic correctness cannot be easily repaired once corrupted upstream. Break condition: If Step 1 introduces faithfulness hallucinations—particularly for complex or advanced texts—the entire downstream pipeline amplifies rather than corrects these errors.

### Mechanism 3
Providing source text directly (RST1) rather than LLM-analyzed text (RST2) as input yields higher quality diagrams by avoiding intermediate hallucination introduction. RST1 provides the original source text to the generation step alongside an RST-structured example, letting the model extract relevant structure itself. RST2 provides LLM-generated RST analysis as input, which may contain parsing errors or mislabeled relations that the generation step treats as ground truth. Core assumption: The model's internal understanding of discourse structure is sufficient when given good examples; explicit RST annotation adds structure but also adds failure surface. Break condition: If the model lacks implicit understanding of certain discourse relations present in the source text, RST1 may produce less structured diagrams than RST2 for simple texts where Step 1 hallucination risk is low.

## Foundational Learning

- Concept: **Rhetorical Structure Theory (RST)**
  - Why needed here: The entire method depends on RST for example selection and structure mapping. Without understanding EDUs, nuclei, satellites, and relation types, you cannot interpret the pipeline's intermediate outputs or debug why certain examples were selected.
  - Quick check question: Given "If the router is connected to the recipient, it forwards the packet. Otherwise, it passes to another router," can you identify the relation type and which EDU is the nucleus?

- Concept: **Hallucination Taxonomy (Factuality vs Faithfulness)**
  - Why needed here: The paper evaluates both hallucination types with different remediation strategies. Factuality hallucination (contradicts world knowledge) differs from faithfulness hallucination (instruction/context/logical inconsistency), and conflating them leads to incorrect debugging.
  - Quick check question: A diagram shows a "third edge from a diamond conditional block"—is this a factuality or faithfulness hallucination? Which subtype?

- Concept: **In-Context Learning (ICL) Sensitivity**
  - Why needed here: ICL performance depends critically on example selection, ordering, and format. The paper's earlier experiments with 7 static examples failed due to positional bias. Understanding that ICL doesn't override pre-training preferences explains why complex RST analysis instructions fail even with good examples.
  - Quick check question: If you observe that RST2 performs worse than RST1 for GPT-4o but better for simple texts with o3, what hypothesis about ICL sensitivity would you form?

## Architecture Onboarding

- Component map:
  Source Text → RST Analysis → Similarity Search → Example Selection → ICL Generation → Repair Loops → Final Diagram

- Critical path: Steps 1→2→4 for RST1/RST2 (RST analysis quality directly impacts example selection). For all methods, Step 4's ICL generation is the primary quality determinant—repair loops only fix syntax, not semantic errors. Step 6 refinement introduces H6 hallucinations ~50% of the time but doesn't strongly correlate with final quality.

- Design tradeoffs:
  - RST1 vs RST2: RST1 avoids Step 1 hallucinations but may produce less structured outputs. RST2 provides explicit structure but inherits H1 errors. For o3, RST1 wins; for weaker models, tradeoff depends on text complexity.
  - Example count: 4 examples balance coverage vs. context window; more examples risk positional bias and increased hallucination from complex prompts.
  - Automated evaluation: E2 (ICL + instructions) and E3 (instructions only) with o3 moderately align with human judgment. Use for triage, not replacement.

- Failure signatures:
  - Low C1 + high context complexity: Likely Hinh from Steps 1-2 hallucinations
  - High Hc with 0-shot: Model prioritizing parametric knowledge over source text
  - Render failures after multiple repair attempts: Syntax-level hallucinations in dot generation
  - High H6 without quality degradation: Refinement introduces aesthetic issues but preserves semantics

- First 3 experiments:
  1. Baseline calibration: Run all three methods with both models on 5 texts of known difficulty levels. Compute C1, C2, C3 scores and hallucination rates. Expected: RST1+o3 highest Gr, RST2+GPT-4o lowest.
  2. Hallucination tracing: For RST2 pipeline, manually inspect Step 1 RST analyses for texts at each difficulty level. Calculate H1 rate and correlate with final diagram quality. Expected: Advanced texts show higher H1, which propagates as Hinh, reducing Ginh.
  3. Example coverage test: Add 2 new examples covering relation types absent from current 4. Re-run RST1 on texts requiring these relations. Compare C1 scores and Hc rates. Assumption: Better coverage reduces context inconsistency hallucinations.

## Open Questions the Paper Calls Out
None

## Limitations
- Small corpus of 25 source texts and 4 pre-made examples limits generalizability
- Reliance on subjective human expert assessment introduces potential bias despite IRR calculations
- Inherited hallucination mechanism is theoretically compelling but requires further empirical validation
- Automated evaluation alignment (Krippendorff's α 0.3-0.66) suggests moderate reliability but insufficient for replacement

## Confidence
- **High confidence**: o3 model superiority over GPT-4o; RST1 method producing hallucination-free diagrams; correlation between text complexity and hallucination rates
- **Medium confidence**: Inherited hallucination propagation explanation; effectiveness of source-text vs analyzed-text input modalities; automated evaluation moderate alignment with human judgment
- **Low confidence**: RST-based example selection superiority claims; generalizability of the 4-example dictionary approach; specific contribution of each hallucination type to overall quality degradation

## Next Checks
1. **Corpus expansion test**: Validate RST1/RST2 methods across 100+ source texts spanning multiple domains to confirm hallucination patterns hold beyond the current dataset
2. **Example dictionary stress test**: Systematically vary the number and diversity of pre-made examples (2, 4, 8, 16 examples) to determine optimal balance between coverage and context window constraints
3. **Cross-model hallucination audit**: Apply the 7-step pipeline to Claude 3.5 Sonnet and Gemini 1.5 Pro to assess whether inherited hallucination patterns are model-specific or represent fundamental pipeline limitations