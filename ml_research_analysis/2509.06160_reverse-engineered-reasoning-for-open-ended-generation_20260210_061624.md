---
ver: rpa2
title: Reverse-Engineered Reasoning for Open-Ended Generation
arxiv_id: '2509.06160'
source_url: https://arxiv.org/abs/2509.06160
tags:
- reasoning
- thinking
- deep
- process
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of instilling deep reasoning
  in large language models for open-ended, creative generation tasks where clear reward
  signals are absent. The proposed Reverse-Engineered Reasoning (REER) paradigm synthesizes
  high-quality reasoning trajectories by working backwards from known good solutions
  using gradient-free local search guided by perplexity minimization.
---

# Reverse-Engineered Reasoning for Open-Ended Generation

## Quick Facts
- arXiv ID: 2509.06160
- Source URL: https://arxiv.org/abs/2509.06160
- Reference count: 40
- The paper introduces Reverse-Engineered Reasoning (REER) to synthesize high-quality reasoning trajectories for open-ended generation tasks, achieving competitive performance with proprietary models.

## Executive Summary
This paper addresses the challenge of instilling deep reasoning in large language models for open-ended, creative generation tasks where clear reward signals are absent. The proposed Reverse-Engineered Reasoning (REER) paradigm synthesizes high-quality reasoning trajectories by working backwards from known good solutions using gradient-free local search guided by perplexity minimization. This approach avoids the sample inefficiency of reinforcement learning and the high cost of instruction distillation. Using REER, the authors created DeepWriting-20K, a dataset of 20,000 deep reasoning trajectories, and fine-tuned Qwen3-8B to produce DeepWriter-8B. DeepWriter-8B significantly outperforms strong open-source baselines and achieves performance competitive with or superior to leading proprietary models like GPT-4o and Claude 3.5 on benchmarks including LongBench, HelloBench, and WritingBench, demonstrating that sophisticated deep reasoning for open-ended generation can be cultivated without costly RL or distillation.

## Method Summary
The method employs a gradient-free local search algorithm to synthesize reasoning trajectories by iteratively refining segments to minimize perplexity of the reference solution. Starting with a query and solution pair, the generator produces an initial reasoning trajectory. Each segment is then refined through local search, generating candidates and selecting those that minimize perplexity conditioned on the current trajectory. The process iterates until an end-of-thinking condition is met or a perplexity threshold is achieved. The resulting trajectories are filtered for repetition and used to fine-tune a base model, blending with existing reasoning datasets to preserve general capabilities.

## Key Results
- DeepWriter-8B significantly outperforms strong open-source baselines across LongBench, HelloBench, and WritingBench benchmarks
- Achieves competitive performance with or superior to proprietary models GPT-4o and Claude 3.5 on writing tasks
- Demonstrates that sophisticated deep reasoning can be cultivated without reinforcement learning or instruction distillation

## Why This Works (Mechanism)
The method works by reverse-engineering reasoning trajectories from solutions, leveraging the fact that perplexity minimization effectively identifies coherent intermediate steps that lead to the desired output. By iteratively refining segments based on their contribution to reducing perplexity, the algorithm discovers reasoning paths that are both logically sound and textually coherent. This approach circumvents the need for expensive RL or distillation by directly synthesizing high-quality reasoning examples from problem-solution pairs.

## Foundational Learning
- **Perplexity Minimization:** Why needed - serves as the objective function for evaluating reasoning quality; Quick check - verify that refined segments consistently reduce perplexity compared to initial generation
- **Iterative Local Search:** Why needed - enables systematic refinement of reasoning segments without gradient-based methods; Quick check - monitor perplexity reduction per iteration to ensure convergence
- **End-of-Thinking Detection:** Why needed - prevents excessive reasoning and ensures natural termination; Quick check - validate that detected end points align with logical conclusion of thought process
- **Repetition Pruning:** Why needed - eliminates degenerate trajectories with redundant content; Quick check - measure n-gram repetition rates before and after filtering
- **Blending with Existing Reasoning Data:** Why needed - prevents catastrophic forgetting of general reasoning capabilities; Quick check - evaluate performance on non-writing reasoning tasks post-training

## Architecture Onboarding

**Component Map:** Query-Solution Pairs -> Initial Trajectory Generation -> Segment-wise Refinement -> Perplexity Evaluation -> Filtering -> Fine-tuning

**Critical Path:** The synthesis pipeline (generation → refinement → filtering) is the critical path, as it directly determines the quality of training data for the final model.

**Design Tradeoffs:** REER trades computational efficiency for data quality, using expensive synthesis to avoid costly RL. The approach balances between synthetic trajectory diversity and coherence through perplexity-based selection.

**Failure Signatures:** Degenerate reasoning (excessive repetition), premature termination (missing key reasoning steps), and incoherent intermediate steps (high perplexity despite refinement).

**3 First Experiments:**
1. Validate perplexity reduction by comparing initial vs. refined trajectories on a small subset
2. Test end-of-thinking detection by manually inspecting termination points across diverse examples
3. Evaluate repetition filtering effectiveness by measuring n-gram diversity metrics pre and post-filtering

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several are implied by the methodology and evaluation approach.

## Limitations
- Critical hyperparameters for the synthesis process (perplexity thresholds, candidate counts, iteration limits) are not specified, hindering reproducibility
- Evaluation relies on LLM-as-a-judge methodology, which may introduce alignment biases and subjectivity
- The synthetic nature of trajectories may not capture full diversity of human reasoning, particularly for culturally nuanced writing

## Confidence
- **High Confidence:** Effectiveness of REER for synthesizing reasoning trajectories and superiority of DeepWriter-8B over open-source baselines
- **Medium Confidence:** Competitive performance with proprietary models given potential benchmark-specific advantages and LLM evaluation methodology
- **Low Confidence:** Scalability to domains beyond writing, as cross-domain generalization is not demonstrated

## Next Checks
1. Reproduce a Minimal Synthesis Pipeline: Implement REER synthesis on a small scale (100-200 examples) with varying perplexity thresholds and candidate counts to identify sensitivity to hyperparameters.

2. Cross-Domain Generalization Test: Evaluate DeepWriter-8B on non-writing reasoning tasks (mathematical reasoning, code generation) to assess catastrophic forgetting and transfer of deep reasoning capability.

3. Human Evaluation Validation: Conduct human evaluation comparing DeepWriter-8B outputs against proprietary models on a subset of WritingBench tasks, focusing on qualitative aspects like coherence, creativity, and reasoning depth.