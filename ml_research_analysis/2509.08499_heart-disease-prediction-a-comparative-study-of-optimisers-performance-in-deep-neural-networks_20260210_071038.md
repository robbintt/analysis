---
ver: rpa2
title: 'Heart Disease Prediction: A Comparative Study of Optimisers Performance in
  Deep Neural Networks'
arxiv_id: '2509.08499'
source_url: https://arxiv.org/abs/2509.08499
tags:
- learning
- training
- convergence
- optimizer
- optimizers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the performance of 10 optimizers in training
  a deep neural network for heart disease prediction using a publicly available dataset.
  The key outcome is that RMSProp demonstrated the most balanced performance across
  key metrics: it achieved the highest precision (0.765), strong recall (0.827), and
  a solid AUC score (0.841), along with faster convergence (18 epochs).'
---

# Heart Disease Prediction: A Comparative Study of Optimisers Performance in Deep Neural Networks

## Quick Facts
- **arXiv ID:** 2509.08499
- **Source URL:** https://arxiv.org/abs/2509.08499
- **Reference count:** 40
- **Primary result:** RMSProp achieved highest precision (0.765), strong recall (0.827), and solid AUC (0.841) with fastest convergence (18 epochs) on heart disease prediction task.

## Executive Summary
This study systematically evaluates 10 optimizers for training deep neural networks on heart disease prediction using a publicly available clinical dataset. The research reveals that RMSProp delivers the most balanced performance across key metrics—precision, recall, and AUC—while converging faster than most alternatives. The work highlights fundamental trade-offs between convergence speed and stability, with adaptive methods like Adam showing rapid learning but higher overfitting risk. By incorporating dropout regularization, hyperparameter tuning, and early stopping, the final model achieved an improved ROC-AUC score of 92%, demonstrating the importance of optimizer selection and regularization in healthcare applications.

## Method Summary
The study used the Kaggle "Heart Statlog (Cleveland and Hungary)" dataset with 1190 samples, preprocessed by dropping low-variance features, removing duplicates, imputing zeros, and applying RobustScaler. A 6-layer hourglass neural network (16→32→64→32→16→8 neurons) with ReLU activations was trained using 10 optimizers (SGD variants, RMSProp, Adagrad, Adadelta, Adam variants) for 50 epochs each with identical weight initialization. The best optimizer (RMSProp) was then enhanced with dropout (p=0.2), early stopping (patience=15), and grid search learning rates (0.001, 0.01, 0.1) using 5-fold cross-validation, achieving 92% ROC-AUC.

## Key Results
- RMSProp achieved highest precision (0.765), strong recall (0.827), and solid AUC (0.841) with fastest convergence (18 epochs)
- Trade-off observed: Adam/AdamW converged fastest (9 epochs) but showed highest validation loss variance (0.214864), while Adadelta/Adagrad converged slowest (49 epochs) with highest stability (0.001403/0.005564)
- Enhanced model with dropout and early stopping improved ROC-AUC from 0.841 to 0.92
- Small validation set (20% of training fold) and hourglass architecture may amplify metric variance

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Learning Rate Normalization via RMSProp
RMSProp's moving average of squared gradients enables balanced convergence speed and predictive performance on tabular clinical data. RMSProp maintains an exponential moving average of squared gradients per parameter and divides the current gradient by the square root of this average. This effectively creates per-parameter adaptive learning rates, preventing steep directions from causing oscillations while allowing flat directions to move faster. Core assumption: The optimal optimizer for tabular healthcare data balances convergence speed against overfitting risk, not purely minimizing training loss.

### Mechanism 2: Convergence–Stability Trade-off in Adaptive Methods
Adaptive optimizers exhibit a fundamental trade-off where faster convergence correlates with higher validation loss variance. Optimizers like Adam and Nadam accumulate first and second moment estimates, enabling aggressive initial updates. This reduces epochs to convergence but amplifies oscillations in the loss trajectory. Adagrad and Adadelta accumulate gradients more conservatively, yielding slower but smoother optimization paths. Core assumption: Validation loss standard deviation meaningfully captures optimizer stability relevant to deployment reliability.

### Mechanism 3: Regularization Ensemble Mitigates Adaptive Optimizer Overfitting
Combining dropout regularization, early stopping, and hyperparameter tuning with RMSProp improves generalization from AUC 0.841 to 0.92. Dropout (p=0.2) randomly silences neurons during training, forcing the network to learn redundant representations and reducing co-adaptation. Early stopping (patience=15) halts training before overfitting accelerates. Grid search over learning rates selects the best-performing configuration. Core assumption: The observed AUC improvement is attributable to the combined regularization strategy, not optimizer change alone.

## Foundational Learning

- **Gradient Descent Optimization**
  - Why needed here: Understanding how different optimizers update weights is essential to interpreting convergence speed, stability, and final performance differences.
  - Quick check question: Why might RMSProp converge faster than vanilla SGD on sparse or ill-conditioned loss surfaces?

- **Binary Classification Metrics (Precision, Recall, ROC-AUC)**
  - Why needed here: The study evaluates optimizers on healthcare-specific metrics where false positives and false negatives have asymmetric costs.
  - Quick check question: If Adagrad achieves perfect recall (1.0) but low precision (0.449), what does this imply about its clinical utility?

- **Overfitting and Regularization**
  - Why needed here: The enhanced training phase relies on dropout and early stopping to bridge the generalization gap observed in fast-converging adaptive optimizers.
  - Quick check question: How does early stopping qualitatively differ from L2 regularization in preventing overfitting?

## Architecture Onboarding

- **Component map:** Input(10 clinical features) -> FC(16) -> FC(32) -> FC(64) -> FC(32) -> FC(16) -> FC(8) -> Output(Sigmoid)
- **Critical path:** Preprocess data → Initialize weights identically → Train 10 optimizers for 50 epochs → Select RMSProp → Add dropout and early stopping → Grid-search learning rate with 5-fold CV → Report final ROC-AUC
- **Design tradeoffs:** Hourglass topology compresses to 8 neurons before output—assumes mid-level bottleneck aids generalization on small tabular data (1190 samples). No batch normalization or residual connections—architecture simplicity prioritizes isolating optimizer effects. Stability measured as validation loss std dev rather than training loss—aligns with generalization focus but may underweight convergence acceleration.
- **Failure signatures:** Adam/AdamW: Very low training loss (0.114) but high validation loss (>1.3) indicates severe overfitting despite fast convergence. Adadelta: Very low AUC (0.385) despite perfect recall suggests near-random ranking—optimizer may have stalled or learning rate too conservative. High stability + low AUC combination signals underfitting rather than robust optimization.
- **First 3 experiments:** 1) Replicate RMSProp baseline with identical architecture, confirm reported AUC (~0.84) and convergence (~18 epochs) on same Kaggle dataset split. 2) Ablate dropout (set p=0) while keeping early stopping—measure change in validation loss and AUC to isolate dropout contribution. 3) Swap dataset for larger public heart disease dataset (>5000 samples) with same architecture—compare optimizer rankings to test generalizability claim.

## Open Questions the Paper Calls Out
- What specific theoretical or empirical mechanisms drive the observed trade-off between convergence speed and training stability in this context? The conclusion explicitly states that "future work should focus on investigating the reason behind the trade-offs between the convergence speed and stability of the optimizers used in this study."
- Do the performance rankings of optimizers, particularly the balanced superiority of RMSProp, hold when applied to significantly larger and more complex clinical datasets? The authors note that the dataset size and structure may have influenced outcomes and explicitly recommend "conducting experiments on larger datasets" to test generalizability.
- To what extent is the choice of neural network architecture responsible for the observed optimizer performance compared to simpler machine learning algorithms? The conclusion suggests investigating "different learning architectures, including simpler traditional algorithms which may perform well on datasets as small as the one used in this study."

## Limitations
- No explicit batch size or optimizer-specific learning rates disclosed, requiring assumptions for reproduction
- Small dataset (1190 samples) limits generalizability to larger or more complex clinical datasets
- Hourglass architecture and small validation split (20% of training fold) may amplify variance in metric estimates

## Confidence
- **High confidence**: RMSProp’s balanced performance across precision, recall, and AUC on this dataset; the trade-off between convergence speed and stability in adaptive optimizers
- **Medium confidence**: Generalization of these optimizer rankings to other heart disease datasets or larger sample sizes; attribution of AUC improvement solely to regularization rather than optimizer interaction
- **Low confidence**: Optimal dropout rate and early stopping patience for this specific architecture; stability metric interpretation without confidence intervals

## Next Checks
1. Replicate baseline RMSProp training with documented hyperparameters to verify convergence speed (~18 epochs) and AUC (~0.84)
2. Perform ablation study: train without dropout but with early stopping to isolate regularization impact on generalization
3. Transfer the same architecture and optimizer protocol to a larger public heart disease dataset (>5000 samples) to test robustness of optimizer rankings