---
ver: rpa2
title: Secret mixtures of experts inside your LLM
arxiv_id: '2512.18452'
source_url: https://arxiv.org/abs/2512.18452
tags:
- experts
- student
- layer
- variance
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the internal structure of Multilayer Perceptron
  (MLP) layers in transformer models, which are notoriously difficult to interpret
  due to their dense computation and lack of visualizability. The key hypothesis is
  that these MLP layers can be effectively approximated by sparsely-activating Mixture
  of Experts (MoE) models, meaning they compute sparse functions despite their dense
  appearance.
---

# Secret mixtures of experts inside your LLM

## Quick Facts
- arXiv ID: 2512.18452
- Source URL: https://arxiv.org/abs/2512.18452
- Authors: Enric Boix-Adsera
- Reference count: 8
- Key outcome: MLP layers in transformers can be approximated by sparsely-activating MoE models, with MoE students using up to 8x fewer active parameters while matching dense MLP student performance

## Executive Summary
This paper investigates the internal structure of Multilayer Perceptron (MLP) layers in transformer models, which are notoriously difficult to interpret due to their dense computation and lack of visualizability. The key hypothesis is that these MLP layers can be effectively approximated by sparsely-activating Mixture of Experts (MoE) models, meaning they compute sparse functions despite their dense appearance.

The theoretical foundation is built on connecting dictionary-sparse structure in neural activations (where activations are sparse in some learned dictionary) to MoE structure. The authors prove that under dictionary-sparse input distributions, MLPs can be well-approximated by MoEs, while this is not possible under Gaussian input distributions. Empirically, the authors validate this hypothesis by distilling MLP layers from pretrained models (Pythia-410M, Gemma-270M, Pythia-70M) to MoE student models, showing that MoE students can achieve comparable performance to dense MLP students while using significantly fewer active parameters - up to 8 times fewer in some cases.

## Method Summary
The authors develop a theoretical framework connecting dictionary-sparse activation distributions to MoE-approximability of MLPs. They prove that under dictionary-sparse input distributions, MLPs can be well-approximated by MoEs, while this is not possible under Gaussian input distributions. The empirical validation involves distilling MLP layers from pretrained models to MoE student models. The authors train MoE students with low-rank routers on synthetic dictionary-sparse data and real CIFAR-100 data, comparing their performance to dense MLP students. They also conduct control experiments distilling to Gaussian data to demonstrate that the MoE advantage disappears when the activation distribution structure is removed.

## Key Results
- MoE students can achieve comparable performance to dense MLP students while using up to 8x fewer active parameters
- The MoE advantage disappears when distilling to Gaussian data, confirming the importance of activation distribution structure
- Low-rank routers in MoE students are sufficient for capturing MLP layer functionality
- Theoretical proof establishes connection between dictionary-sparse distributions and MoE-approximability

## Why This Works (Mechanism)
The mechanism works because MLP layers, despite their dense appearance, actually compute sparse functions when the input follows a dictionary-sparse distribution. This sparsity structure allows the MLP to be approximated by a Mixture of Experts model where only a small subset of experts are active for any given input. The dictionary-sparse distribution means that inputs can be represented sparsely in some learned dictionary basis, which creates natural clustering of similar inputs that can be handled by the same expert. This explains why MoE architectures work well in transformers - they're capturing the inherent sparse computation that's already happening inside the dense MLP layers.

## Foundational Learning
- **Dictionary-sparse distributions**: Data distributions where inputs can be represented with few non-zero coefficients in some learned dictionary basis. Why needed: This structure is crucial for proving that MLPs can be approximated by MoEs. Quick check: Verify that activation distributions in trained models exhibit dictionary-sparse structure by measuring sparsity in learned dictionary bases.

- **Mixture of Experts (MoE)**: A neural network architecture where multiple expert networks exist and a gating network selects which experts to use for each input. Why needed: The paper shows that MLP layers can be approximated by MoE models, explaining MoE effectiveness. Quick check: Compare performance of MoE vs dense approximations on the same MLP layer.

- **Low-rank approximation**: Using matrices with reduced rank to approximate full-rank matrices, reducing computational cost. Why needed: The paper uses low-rank routers in MoE students to reduce parameter count while maintaining performance. Quick check: Measure performance degradation as router rank decreases.

- **Knowledge distillation**: Transferring knowledge from a large "teacher" model to a smaller "student" model. Why needed: The empirical validation uses distillation to test whether MLPs can be approximated by MoEs. Quick check: Compare student performance to teacher performance across different approximation methods.

- **Neural network approximation theory**: Mathematical framework for understanding when and how one network architecture can approximate another. Why needed: Provides theoretical foundation for proving MLP-to-MoE approximation. Quick check: Verify approximation bounds hold empirically on test cases.

- **Activation distribution analysis**: Studying the statistical properties of neural network activations. Why needed: Central to understanding the dictionary-sparse structure that enables MoE approximation. Quick check: Measure activation sparsity and clustering in trained models.

## Architecture Onboarding

Component Map:
MLP Layer -> Dense computation -> Output
MoE Layer -> Router (low-rank) -> Multiple Experts -> Sparse combination -> Output

Critical Path:
Input -> Router (selects experts) -> Active experts compute -> Sparse combination of expert outputs -> Final output

Design Tradeoffs:
- Dense MLPs use all parameters for every input but may compute sparse functions
- MoEs use gating to activate only relevant experts, saving computation but adding routing complexity
- Low-rank routers reduce parameters but may limit routing capacity
- Approximation quality vs. efficiency tradeoff in choosing number of experts

Failure Signatures:
- Poor performance when input distribution lacks dictionary-sparse structure
- Routing collapse when router cannot distinguish between different input types
- Expert imbalance where some experts are rarely or never used
- Overfitting when too many experts relative to available data

First Experiments:
1. Distill a small MLP layer to both MoE and dense student models on dictionary-sparse data
2. Repeat distillation on Gaussian data to verify MoE advantage disappears
3. Vary router rank in MoE students to find minimum rank needed for good performance

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on specific assumptions about data distribution that may not hold universally across all transformer applications
- Empirical validation is limited to relatively small models (Pythia-410M, Gemma-270M, Pythia-70M) and only tests on synthetic data and a single real dataset (CIFAR-100)
- Distillation methodology assumes access to full activation distributions, which may not be practical for larger, more complex models where such analysis becomes computationally prohibitive
- Work does not address how the found MoE structure might generalize across different layers or model architectures

## Confidence
- High: The core empirical finding that MLP layers can be well-approximated by MoEs on the tested models and datasets
- Medium: The theoretical connection between dictionary-sparse distributions and MoE approximability
- Low: The broader claim that this explains why MoEs work well in transformers in general

## Next Checks
1. Test the distillation approach on larger models (1B+ parameters) and diverse real-world tasks to verify scalability and generality
2. Analyze whether the MoE structure discovered in one layer type (e.g., attention MLPs) transfers to other layer types (e.g., FFN layers) within the same model
3. Validate the dictionary-sparse distribution hypothesis by explicitly measuring activation sparsity in learned dictionaries across multiple pretrained models and tasks