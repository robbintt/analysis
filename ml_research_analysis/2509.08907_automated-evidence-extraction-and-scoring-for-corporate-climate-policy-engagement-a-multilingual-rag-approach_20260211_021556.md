---
ver: rpa2
title: 'Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement:
  A Multilingual RAG Approach'
arxiv_id: '2509.08907'
source_url: https://arxiv.org/abs/2509.08907
tags:
- policy
- climate
- stance
- supporting
- organization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a multilingual RAG system for automated extraction
  and scoring of corporate climate policy engagement evidence. It combines layout-aware
  parsing, dense embeddings (Nomic, Qwen), semantic chunking, and LLM-based stance
  classification with reranking.
---

# Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach

## Quick Facts
- **arXiv ID**: 2509.08907
- **Source URL**: https://arxiv.org/abs/2509.08907
- **Reference count**: 28
- **Primary result**: Multilingual RAG system combining layout-aware parsing, semantic chunking, and LLM-based stance classification achieves high retrieval accuracy (nLCS > 0.7, recall > 0.76) with reranking improvements.

## Executive Summary
This paper introduces a multilingual RAG system for automated extraction and scoring of corporate climate policy engagement evidence from PDF documents. The system combines layout-aware parsing with semantic chunking, dense embeddings, and LLM-based stance classification enhanced by reranking and few-shot prompting. Evaluations demonstrate that Docling parsing with semantic chunking and Nomic embeddings achieves strong retrieval performance, while BGE reranking and strategic few-shot prompting further improve precision and alignment with human judgments. The approach accelerates evidence extraction but maintains human oversight due to nuanced analytical needs.

## Method Summary
The system processes corporate PDFs through a pipeline: layout-aware Docling parsing with EasyOCR for multilingual support, semantic chunking via CHONKIE (similarity threshold 0.75, 1536 tokens), Nomic-embed-text:v1.5 embedding, Weaviate vector storage, and semantic search with BGE reranking. Stance classification uses Qwen3 LLM with few-shot prompting strategies. The method handles 13 predefined climate policy queries across English and non-English documents, employing nLCS metrics for parsing quality and recall/MRR for retrieval evaluation.

## Key Results
- Docling parsing with semantic chunking and Nomic embeddings achieves nLCS > 0.7 and recall > 0.76
- BGE reranking improves precision, with First Retrieved (FR) strategy showing highest helpfulness
- Few-shot prompting with "Few Query Few Stance" strategy reaches hit rates up to 0.7 at 4B and 14B model scales
- Retrieved evidence can sometimes outperform human-annotated gold snippets in stance generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layout-aware parsing combined with semantic chunking improves retrieval quality over linear parsing methods.
- Mechanism: Docling preserves structural markers during parsing, enabling chunking algorithms to respect document semantics rather than arbitrary text boundaries. This produces chunks that maintain topical coherence, which dense embeddings can better represent in vector space.
- Core assumption: Corporate climate disclosures follow predictable layouts where semantic units align with topical units.
- Evidence anchors: [abstract] "a combination of layout-aware parsing, the Nomic embedding model, and few-shot prompting strategies yields the best performance"; [section 5.1] "Docling outperforms PyMuPDF in parsing fidelity for both English and the overall dataset."
- Break condition: Highly irregular document layouts may negate layout-aware advantages.

### Mechanism 2
- Claim: Reranking retrieved evidence improves stance generation accuracy by prioritizing helpfulness over mere textual overlap.
- Mechanism: Initial semantic retrieval casts a wide net (high recall). The BGE reranker then scores chunk-query pairs with a model trained specifically for relevance prediction, elevating chunks that LLMs find more useful for reasoning.
- Core assumption: Relevance for retrieval ≠ relevance for generation; rerankers capture the latter better than embedding similarity alone.
- Evidence anchors: [abstract] "BGE reranking further improving precision"; [section 5.5] "FR consistently outperforms BM in both metrics, highlighting the strength of BGE reranking in surfacing useful evidence early."
- Break condition: When non-English chunks are fewer and longer, reranking adds limited value.

### Mechanism 3
- Claim: Few-shot prompting with representative query-stance examples improves LLM alignment with human analyst judgments.
- Mechanism: Providing the model with exemplars that span the stance scale (-2 to +2) grounds its reasoning in the specific scoring schema. "Few Query Few Stance" strategy exposes the model to diverse queries even when demonstrations focus on single stances.
- Core assumption: Climate policy stance classification follows learnable patterns that transfer across companies and document types.
- Evidence anchors: [abstract] "few-shot prompting strategies yields the best performance in extracting and classifying evidence"; [section 5.4] "Among few-shot (FS) strategies, 'Few Query Few Stance' achieves the highest alignment at both the 4B and 14B scales."
- Break condition: Novel query types outside the exemplar distribution may require additional examples or prompt engineering.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The entire system architecture hinges on separating evidence retrieval from stance generation; understanding this two-stage pipeline is prerequisite to debugging either component.
  - Quick check question: Can you explain why the paper evaluates retrieval metrics (recall, MRR) separately from generation metrics (hit rate, exact match)?

- Concept: **Semantic vs. Layout-based Chunking**
  - Why needed here: Section 5.1 shows these strategies have opposite effectiveness depending on the parser; this tradeoff is critical for document processing decisions.
  - Quick check question: Why does layout chunking outperform semantic chunking with Docling, but underperform with PyMuPDF?

- Concept: **Oracle-based Diagnostics (Faithfulness, Helpfulness, Conciseness)**
  - Why needed here: Section 5.5 reveals that high faithfulness to gold evidence does not guarantee better stance predictions; helpfulness matters more for generation success.
  - Quick check question: If a retrieved chunk has high faithfulness (0.83) but low helpfulness (0.42), would you expect strong or weak stance generation performance?

## Architecture Onboarding

- Component map: PDF Input → Docling Parser → Chunker → Embedder → Weaviate Vector DB → Ollama Semantic Search → BGE Reranker → Qwen3 LLM → Stance Score + Reason

- Critical path: Parse quality (P_nLCS) → Chunk coherence (C_nLCS) → Embedding retrieval (Recall) → Reranking (MRR) → Stance generation (Hit Rate). Failures propagate downstream; parsing errors cannot be recovered later.

- Design tradeoffs:
  - **Nomic embeddings**: Best overall, English-optimized; **Qwen**: Superior for non-English recall but less precise chunks.
  - **Semantic chunking**: Higher recall, benefits more from reranking; **Layout chunking**: Better chunk precision (higher C_nLCS).
  - **First Retrieved (FR)**: Best helpfulness, fastest inference; **All Retrieved (AR)**: Highest faithfulness, risks information overload.
  - **Few-shot prompting**: Better alignment at larger model sizes; **Zero-shot**: Competitive only at 0.6B scale where context limits matter less.

- Failure signatures:
  - Low exact match despite high hit rate → evidence is directionally correct but imprecise; check chunk boundaries.
  - High recall but low MRR → reranker may be misconfigured or embedding space is poorly discriminative.
  - Stance predictions fail on ground truth but succeed on retrieved evidence → gold annotations may be incomplete; retrieved context provides missing reasoning cues.
  - Non-English documents show inflated recall → fewer chunks create easier retrieval; verify with chunk count normalization.

- First 3 experiments:
  1. **Parsing fidelity baseline**: Run Docling and PyMuPDF on 20 sample PDFs (mixed languages); compute P_nLCS against manually extracted gold snippets to confirm Docling advantage for English, PyMuPDF for non-English.
  2. **Chunking strategy ablation**: Using best parser, compare layout vs semantic chunking; measure C_nLCS and downstream recall to validate the tradeoff curve.
  3. **Reranker impact on generation**: Freeze retrieval (Nomic + semantic chunks), compare no-rerank vs BGE rerank; measure both MRR and final stance hit rate to confirm helpfulness gains translate to task success.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can ranking algorithms be optimized to better handle fine-grained evidence chunks within complex corporate documents?
  - Basis in paper: [explicit] The conclusion states that future work should focus on "improving ranking algorithms for fine-grained chunks."
  - Why unresolved: Current retrieval strategies show trade-offs between faithfulness and helpfulness, but precise ranking specifically for granular chunks remains a challenge.
  - What evidence would resolve it: A study comparing novel ranking methodologies specifically on fine-grained semantic chunks versus broader layout-based chunks to optimize the helpfulness metric.

- **Open Question 2**: What novel evaluation metrics can effectively capture the nuances of precise evidence extraction from complex corporate reports?
  - Basis in paper: [explicit] The conclusion calls for "developing better evaluation metrics that account for the inherent challenges of precise evidence extraction from complex corporate documents."
  - Why unresolved: Current metrics like nLCS and faithfulness scores are approximations that may not fully reflect nuanced LLM reasoning failures or the specific difficulties of corporate document layouts.
  - What evidence would resolve it: The proposal and validation of a new metric that correlates more strongly with human analyst judgment on evidence relevance and completeness than current nLCS-based metrics.

- **Open Question 3**: To what extent would explicit mechanisms for multi-document context integration and annotation error detection improve system robustness?
  - Basis in paper: [inferred] The limitations section notes the system "currently lacks explicit mechanisms to detect annotation errors or leverage multi-document context," suggesting these as areas for improving real-world robustness.
  - Why unresolved: The current system processes documents in isolation relative to an entity's broader context or potential labeling mistakes, relying heavily on single-document retrieval quality.
  - What evidence would resolve it: An ablation study introducing cross-document reasoning modules and automated consistency checks to measure performance deltas against the current single-document baseline.

## Limitations
- Reliance on proprietary InfluenceMap dataset that cannot be independently validated or benchmarked
- Performance on highly irregular document layouts or non-standard disclosure formats is unknown
- Multilingual validation limited to Spanish, German, and Japanese; broader language testing needed

## Confidence
- **High Confidence**: Docling parsing advantage over PyMuPDF for English documents, semantic chunking effectiveness with Docling, Nomic embeddings superiority for retrieval
- **Medium Confidence**: Few-shot prompting impact on stance generation, BGE reranking precision improvements, multilingual performance claims
- **Low Confidence**: Retrieved evidence outperforming gold annotations claim, Qwen embeddings superiority for non-English based on chunk count effects

## Next Checks
1. **Cross-dataset Validation**: Apply complete pipeline to independent climate disclosure dataset to verify performance metrics transfer beyond InfluenceMap corpus
2. **Layout Robustness Testing**: Systematically degrade document quality and measure parsing fidelity degradation to identify operational limits
3. **Reranker Ablation with Diverse Languages**: Compare BGE reranking vs. no reranking across languages with varying chunk counts, measuring both MRR and downstream stance generation hit rates