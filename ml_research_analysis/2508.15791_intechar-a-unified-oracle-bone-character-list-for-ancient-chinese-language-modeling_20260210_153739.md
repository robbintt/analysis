---
ver: rpa2
title: 'InteChar: A Unified Oracle Bone Character List for Ancient Chinese Language
  Modeling'
arxiv_id: '2508.15791'
source_url: https://arxiv.org/abs/2508.15791
tags:
- ancient
- chinese
- character
- language
- intechar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InteChar, a unified and extensible Unicode
  character list that integrates previously unencoded oracle bone characters with
  traditional and modern Chinese, enabling consistent digitization and representation
  of ancient texts. To evaluate its effectiveness, the authors construct OracleCS,
  a corpus of oracle bone transcriptions combining expert-annotated samples with LLM-assisted
  data augmentation.
---

# InteChar: A Unified Oracle Bone Character List for Ancient Chinese Language Modeling

## Quick Facts
- arXiv ID: 2508.15791
- Source URL: https://arxiv.org/abs/2508.15791
- Reference count: 12
- Models trained on OracleCS with InteChar significantly outperform baselines, achieving up to 842% improvement in NDCG@10 for oracle bone text tasks.

## Executive Summary
This paper introduces InteChar, a unified and extensible Unicode character list that integrates previously unencoded oracle bone characters with traditional and modern Chinese, enabling consistent digitization and representation of ancient texts. The authors construct OracleCS, a corpus of oracle bone transcriptions combining expert-annotated samples with LLM-assisted data augmentation. Experiments show that models trained on OracleCS with InteChar significantly outperform baselines across embedding-based and fine-tuning tasks, including cloze completion on oracle inscriptions and commentary-to-text retrieval, achieving up to 842% improvement in NDCG@10. These results establish InteChar and OracleCS as foundational resources for advancing ancient Chinese NLP and historical language modeling.

## Method Summary
The method involves creating InteChar through a four-stage pipeline: initializing from Unicode CJK characters, integrating existing encoded ancient characters from trusted font libraries with corpus-based filtering, constructing new characters via radical-based reconstruction for unencoded glyphs, and expert-guided proofreading with Siamese network deduplication. OracleCS is built by combining expert-annotated excavated texts with LLM-augmented instruction samples and pre-Qin classics. Models are trained using extended embedding layers on ancient corpora while freezing pretrained backbones, allowing transfer of semantic knowledge to low-resource scripts.

## Key Results
- Models trained on OracleCS with InteChar achieve up to 842% improvement in NDCG@10 compared to baselines.
- Consistent gains across multiple tasks: cloze completion, commentary-to-text retrieval, translation, polysemous matching, and parsing.
- The unified encoding approach enables robust modeling of ancient scripts that were previously difficult to represent digitally.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified character encoding for previously unencoded oracle bone glyphs enables language models to learn representations for low-resource ancient scripts.
- Mechanism: InteChar extends the vocabulary through a four-stage pipeline: (1) initialize from Unicode CJK characters; (2) integrate existing encoded ancient characters from trusted font libraries with corpus-based filtering; (3) construct new characters via radical-based reconstruction for unencoded glyphs; (4) expert-guided proofreading with Siamese network deduplication. This creates a consistent mapping from glyph images to machine-readable code points.
- Core assumption: Characters appearing in authentic historical contexts but missing from modern encoding standards carry semantic information that can be learned if consistently represented.
- Evidence anchors:
  - [abstract] "InteChar enables consistent digitization and representation of historical texts, providing a foundation for robust modeling of ancient scripts."
  - [section] "The resulting InteChar character set contains both standardized and newly encoded characters... offering robust support for historical language modeling."
  - [corpus] Limited direct corpus signal; neighbor papers focus on detection/OCR tasks rather than vocabulary extension mechanisms.
- Break condition: If oracle bone glyphs lack systematic compositional structure (radicals) that maps to known components, the radical-based reconstruction step would fail to produce valid encodings.

### Mechanism 2
- Claim: Radical-based compositional reconstruction scales new character creation beyond manual expert tracing.
- Mechanism: Instead of stroke-by-stroke manual digitization, the pipeline uses object detection models to identify oracle radicals within glyph images, maps predicted radicals to modern Chinese equivalents, and assembles intermediate compositional glyphs validated by paleographers. Final glyphs are vectorized and assigned Unicode-style internal code points.
- Core assumption: Oracle bone characters share structural regularities with traditional Chinese that allow decomposition into known radical components.
- Evidence anchors:
  - [abstract] "InteChar, a unified and extensible character list that integrates unencoded oracle bone characters with traditional and modern Chinese."
  - [section] "By detecting known radicals, we can reconstruct new characters radical by radical, rather than stroke by stroke, significantly accelerating the expansion of the character list."
  - [corpus] Neighbor paper "Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with BÃ©zier Curves" suggests related geometric reconstruction approaches, supporting plausibility.
- Break condition: If undeciphered characters contain novel radicals not present in the training data for the recognition model, the pipeline would produce incorrect or incomplete glyph compositions.

### Mechanism 3
- Claim: Training extended embedding layers on ancient corpora while freezing pretrained backbones transfers semantic knowledge to low-resource scripts.
- Mechanism: Replace the model's original character embedding layer with a newly initialized embedding matrix sized for InteChar vocabulary. Train only this layer on OracleCS while keeping transformer weights frozen. The pretrained backbone provides general language understanding; the new embeddings learn script-specific character representations.
- Core assumption: The semantic relationships learned by pretrained models on modern/Classical Chinese transfer sufficiently to oracle bone contexts through embedding adaptation alone.
- Evidence anchors:
  - [abstract] "models trained on OracleCS with InteChar significantly outperform baselines across embedding-based and fine-tuning tasks... achieving up to 842% improvement in NDCG@10."
  - [section] Table 1 shows Qwen-7B-Chat improving NDCG@10 from 0.302 to 0.842 with InteChar; Table 3 shows consistent gains across translation, polysemous matching, and parsing tasks.
  - [corpus] AncientBench and related evaluation benchmarks confirm interest in ancient Chinese comprehension but do not directly validate the frozen-backbone adaptation mechanism.
- Break condition: If oracle bone semantics diverge fundamentally from modern/Classical Chinese such that backbone representations are actively misleading, fine-tuning would underperform full model retraining.

## Foundational Learning

- Concept: **Unicode Character Encoding Standards**
  - Why needed here: InteChar extends Unicode to handle unencoded glyphs; understanding code point assignment, compatibility layers, and font rendering is essential for debugging digitization pipelines.
  - Quick check question: Can you explain why Unicode's Han Unification creates challenges for distinguishing variant oracle bone glyphs from their modern descendants?

- Concept: **Transfer Learning with Frozen Encoders**
  - Why needed here: The core approach freezes pretrained backbones and trains only embedding layers; understanding what knowledge transfers and what doesn't is critical for architectural decisions.
  - Quick check question: If backbone representations for modern Chinese actively harm oracle bone understanding, what symptoms would you observe in embedding-space clustering?

- Concept: **Information Retrieval Metrics (NDCG, MRR)**
  - Why needed here: Model evaluation uses ranking metrics for cloze completion and retrieval tasks; interpreting these results requires understanding graded relevance vs. binary relevance.
  - Quick check question: Why might NDCG@10 improve dramatically while MRR@10 shows more modest gains, and what would this suggest about model behavior?

## Architecture Onboarding

- Component map:
  - **InteChar Character List** (11,288 characters): Unicode-compatible vocabulary with internal code points; delivered as TrueType font + mapping table
  - **OracleCS Corpus** (173,459 samples): Expert-annotated excavated texts + LLM-augmented instruction samples + pre-Qin classics; includes radical decompositions and modern Chinese mappings
  - **Embedding Adapter Layer**: Newly initialized embedding matrix sized for InteChar; trainable parameters
  - **Frozen Backbone**: Pretrained transformer (BERT, Qwen, GLM, etc.) with weights locked
  - **Evaluation Benchmark**: Two embedding tasks (cloze, retrieval) + three fine-tuning tasks (translation, polysemous matching, parsing)

- Critical path:
  1. Load OracleCS samples encoded with InteChar code points
  2. Initialize extended embedding layer with random weights or heuristic initialization
  3. Train embeddings on corpus while keeping backbone frozen (10 epochs, lr=3e-5, AdamW)
  4. Evaluate on benchmark tasks using frozen backbone + trained embeddings
  5. (Optional) Apply LoRA fine-tuning for downstream tasks with lr=1e-5

- Design tradeoffs:
  - **Extensibility vs. Stability**: InteChar uses internal code points for new characters rather than official Unicode allocation; enables rapid iteration but creates interoperability risks with external tools.
  - **Frozen vs. Full Fine-tuning**: Freezing backbone reduces compute and preserves general capabilities, but may cap performance ceiling for highly divergent scripts.
  - **Expert Validation vs. Scalability**: Radical-based reconstruction accelerates encoding but introduces error propagation; expert proofreading catches errors but doesn't scale automatically.

- Failure signatures:
  - **OOM during embedding initialization**: InteChar vocabulary (11,288 chars) exceeds original model vocabulary; embedding matrix size must be recalculated.
  - **Zero gradients in embedding layer**: If corpus contains only characters already in original vocabulary, new embedding rows never receive updates; verify corpus coverage of new code points.
  - **Degraded modern Chinese performance**: If embedding extension disrupts alignment with pretrained representations, modern benchmarks will regress; monitor both ancient and modern tasks.
  - **Silent glyph mismatches**: If font rendering doesn't match expected glyphs for new code points, visual inspection will show incorrect characters; validate font file consistency.

- First 3 experiments:
  1. **Vocabulary Coverage Audit**: Count how many OracleCS tokens map to newly constructed InteChar characters vs. existing Unicode; if coverage < 15%, the unified encoding benefit is marginal and corpus filtering may be the real driver.
  2. **Embedding Initialization Ablation**: Compare random initialization vs. copying embeddings from cognate modern characters vs. radical-composition initialization; the paper doesn't specify initialization strategy, so this may explain variance.
  3. **Backbone Thaw Analysis**: Run controlled comparison of frozen-backbone vs. LoRA vs. full fine-tuning on the same task split; if full fine-tuning substantially outperforms frozen, the mechanism claim needs qualification.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation doesn't specify whether frozen-backbone training outperforms full fine-tuning, which is critical for validating the transfer-learning mechanism claim.
- Corpus composition is ambiguous - unclear what proportion consists of newly encoded oracle bone characters versus existing transcriptions.
- The character encoding pipeline's handling of oracle bone characters with novel radicals not present in modern Chinese is not addressed.

## Confidence

**High Confidence**:
- InteChar successfully creates a unified character list integrating oracle bone glyphs with modern Chinese encoding
- The four-stage pipeline produces consistent character mappings
- OracleCS corpus construction follows standard practices for ancient Chinese NLP datasets

**Medium Confidence**:
- Extended embedding layers trained on ancient corpora while freezing pretrained backbones improves oracle bone task performance
- The claimed "842% improvement" is real but may conflate multiple effects (encoding, data augmentation, instruction tuning)
- Radical-based compositional reconstruction accelerates character creation compared to manual methods

**Low Confidence**:
- Whether InteChar's encoding mechanism is essential versus alternative vocabulary extension approaches
- The exact proportion of OracleCS consisting of newly encoded oracle bone characters versus existing transcriptions
- How the system handles oracle bone characters with novel radicals absent from training data

## Next Checks
1. **Vocabulary Coverage Audit**: Analyze OracleCS token distribution to determine what percentage actually uses newly encoded InteChar characters versus existing Unicode code points. If <15% of tokens use new characters, the unified encoding benefit may be marginal and corpus filtering/data augmentation may be the primary drivers of performance gains.

2. **Initialization Strategy Ablation**: The paper doesn't specify how new embedding rows are initialized. Compare random initialization versus copying embeddings from semantically related modern characters versus radical-composition initialization. This could explain unexplained variance in downstream performance and validate whether semantic transfer occurs through embedding initialization.

3. **Backbone Thaw Analysis**: Conduct controlled experiments comparing frozen-backbone training (as proposed) versus LoRA fine-tuning versus full fine-tuning on the same task split. If full fine-tuning substantially outperforms frozen-backbone, the mechanism claim that frozen backbones effectively transfer semantic knowledge needs qualification, and the computational efficiency tradeoff becomes more critical.