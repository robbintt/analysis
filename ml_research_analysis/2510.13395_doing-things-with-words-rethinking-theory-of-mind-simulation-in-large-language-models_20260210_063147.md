---
ver: rpa2
title: 'Doing Things with Words: Rethinking Theory of Mind Simulation in Large Language
  Models'
arxiv_id: '2510.13395'
source_url: https://arxiv.org/abs/2510.13395
tags:
- actions
- agents
- agent
- action
- beliefs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines whether large language models (LLMs) can simulate
  Theory of Mind (ToM) by making genuine inferences about others' beliefs in social
  contexts. Using the Generative Agent-Based Model Concordia, the researchers embedded
  utterances in complex scenarios and assessed whether GPT-4 could select actions
  based on belief attribution rather than linguistic memorization.
---

# Doing Things with Words: Rethinking Theory of Mind Simulation in Large Language Models

## Quick Facts
- arXiv ID: 2510.13395
- Source URL: https://arxiv.org/abs/2510.13395
- Reference count: 40
- This study challenges claims of emergent Theory of Mind capabilities in LLMs, showing GPT-4 fails to select context-appropriate actions based on belief attribution rather than linguistic pattern matching.

## Executive Summary
This study challenges the prevailing assumption that large language models (LLMs) possess genuine Theory of Mind (ToM) capabilities by demonstrating that their performance on ToM tasks likely stems from shallow statistical associations rather than true mentalizing. Using the Concordia Generative Agent-Based Model framework, the researchers embedded utterances in complex scenarios where agents must select actions based on belief attribution. The results show GPT-4 consistently failed to choose context-appropriate actions, suggesting its ToM-like performance reflects linguistic pattern recognition rather than genuine reasoning about others' beliefs. The study also reveals that GPT-4 struggles to generate coherent causal effects from agent actions, with average coherence ratings of only 2.11/5.

## Method Summary
The researchers used the Concordia Generative Agent-Based Model framework to embed utterances in complex social scenarios where agents must select actions based on belief attribution. The study employed a two-phase evaluation: (1) a Multiple Choice Question Answering (MCQA) phase where GPT-4o-mini selected actions consistent with inferred belief states across 200 simulations covering 7 linguistic phenomena (requests, suggestions, declinations, threats, and three types of verbal irony), and (2) a Direct Effect Externality phase generating event statements and causal effects with coherence ratings. Agent memories included Big Five personality traits, first/second-order beliefs, goals, and locations. The framework used a Game Master to mediate between agents and the environment, with evaluation combining LLM-as-judge and human annotator verification.

## Key Results
- GPT-4o-mini consistently failed to select context-appropriate actions in belief attribution scenarios, performing below chance levels despite strong linguistic comprehension.
- The model achieved only 2.11/5 average coherence ratings for causal effects generated from agent actions, indicating poor reasoning about consequences.
- Performance did not improve with second-order belief tasks, suggesting limitations in genuine mentalizing rather than simple pattern matching.

## Why This Works (Mechanism)
The study demonstrates that LLMs can produce ToM-like outputs through linguistic pattern matching without genuine belief attribution reasoning. By embedding utterances in complex scenarios where correct action selection requires inferring mental states rather than recognizing surface patterns, the researchers reveal that GPT-4's apparent ToM capabilities reflect shallow statistical associations with linguistic forms rather than true mentalizing.

## Foundational Learning
- Theory of Mind (ToM): The ability to attribute mental states to others and understand that beliefs, desires, and intentions may differ from one's own. Needed to evaluate whether LLMs can genuinely reason about others' mental states rather than just pattern matching.
- False-belief paradigms: Experimental scenarios where an agent holds beliefs that differ from reality, requiring the model to track what others know versus what is objectively true. Quick check: Can the model select actions appropriate to the agent's beliefs rather than objective facts?
- Indirect speech acts: Utterances where the literal meaning differs from the intended communicative purpose (requests, suggestions, threats, etc.). Quick check: Does the model infer the intended meaning based on context and speaker beliefs?
- Verbal irony: Language where the intended meaning is opposite to the literal meaning, requiring understanding of speaker intention and context. Quick check: Can the model distinguish sarcastic from sincere statements based on belief attribution?
- Coherence ratings: Subjective assessment of how logically connected and plausible generated outputs are, scored 1-5. Quick check: Do generated causal effects follow logically from agent actions and belief states?
- Chain-of-Thought (CoT): A prompting strategy where the model explains its reasoning step-by-step before generating final outputs. Quick check: Does CoT preserve contextual information or lose it during summarization?

## Architecture Onboarding

Component map: Concordia Game Master -> Agent Memories -> GPT-4o-mini API -> Action Selection/Causal Effects -> Evaluation

Critical path: The evaluation framework requires (1) constructing stimulus sets with embedded utterances in false-belief scenarios, (2) running MCQA phase for action selection, (3) generating event statements and causal effects, and (4) collecting coherence ratings. Each phase depends on the previous one's outputs.

Design tradeoffs: The study uses GPT-4o-mini (a smaller, more efficient model) rather than GPT-4 to test whether ToM capabilities scale with model size or emerge at smaller scales. The Concordia framework trades off realism for controlled evaluation of belief attribution versus pattern matching.

Failure signatures: Consistent failure to select context-appropriate actions across all 7 linguistic phenomena, low coherence ratings (below 2.5/5) for causal effects, and no improvement on second-order belief tasks compared to first-order tasks.

Three first experiments:
1. Test action selection accuracy on simple direct speech acts versus complex indirect speech acts to isolate whether failure stems from pragmatic complexity or belief attribution.
2. Compare coherence ratings for causal effects generated with versus without Chain-of-Thought prompting to identify information loss points.
3. Evaluate whether providing explicit belief state annotations improves action selection accuracy, distinguishing between inference limitations and information access issues.

## Open Questions the Paper Calls Out
None

## Limitations
- The study only tests GPT-4o-mini, limiting generalizability to other model architectures and sizes.
- Exact prompt templates and hyperparameters are not fully specified, making precise replication challenging.
- Coherence ratings, while using LLM-as-judge plus human verification, remain somewhat subjective and may not capture all aspects of reasoning quality.

## Confidence
- Claims of LLM ToM limitations: Medium - Results are compelling but limited to one model architecture
- Action-based evaluation framework validity: Medium - Innovative approach but requires further validation across different models
- Distinction between pattern matching and genuine mentalizing: High - Well-supported by failure to improve on second-order belief tasks

## Next Checks
1. Replicate the MCQA accuracy results using the full prompt templates and exact API parameters, comparing performance across different model families (GPT-4, Claude, LLaMA) to assess whether the failure pattern is architecture-specific.
2. Conduct ablation studies on the Concordia framework to isolate whether low coherence ratings stem from the Chain-of-Thought process losing contextual information or from fundamental limitations in the model's causal reasoning capabilities.
3. Test whether fine-tuning on the specific belief attribution scenarios improves action selection accuracy, helping distinguish between shallow statistical associations and genuine inference limitations.