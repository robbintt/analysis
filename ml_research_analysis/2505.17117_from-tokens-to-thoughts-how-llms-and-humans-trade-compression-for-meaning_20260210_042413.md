---
ver: rpa2
title: 'From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning'
arxiv_id: '2505.17117'
source_url: https://arxiv.org/abs/2505.17117
tags:
- rosch
- human
- semantic
- llms
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how LLMs and humans balance compression
  and semantic meaning when organizing knowledge into concepts. Using an Information
  Bottleneck framework, the authors compare 40+ LLMs against classic cognitive benchmarks
  (Rosch, 1975; McCloskey & Glucksberg, 1978) to evaluate categorical alignment, internal
  semantic structure, and compression-meaning trade-offs.
---

# From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning

## Quick Facts
- **arXiv ID:** 2505.17117
- **Source URL:** https://arxiv.org/abs/2505.17117
- **Reference count:** 40
- **Primary result:** LLMs achieve mathematically optimal compression-meaning balance compared to humans, sacrificing semantic nuances essential for human-like understanding.

## Executive Summary
This study investigates how LLMs and humans balance compression and semantic meaning when organizing knowledge into concepts. Using an Information Bottleneck framework, the authors compare 40+ LLMs against classic cognitive benchmarks to evaluate categorical alignment, internal semantic structure, and compression-meaning trade-offs. The research demonstrates that while LLMs excel at statistical compression, they sacrifice the semantic nuances essential for human-like understanding, revealing fundamental differences between artificial and natural intelligence.

## Method Summary
The authors use an Information Bottleneck framework to analyze how 40+ LLMs and humans organize knowledge into concepts. They evaluate categorical alignment using Adjusted Mutual Information (AMI) against human benchmarks (Rosch, 1975; McCloskey & Glucksberg, 1978), measure internal semantic structure through typicality gradients (Spearman correlations), and quantify compression-meaning trade-offs using an L objective (Rate + β·Distortion). The analysis includes static embeddings, contextual layers, and training dynamics across different model architectures.

## Key Results
- LLMs broadly align with human categories (AMI ≈ 0.55) but fail to capture fine-grained semantic distinctions like item typicality (Spearman correlations ρ < 0.2)
- Most LLMs achieve mathematically "optimal" compression-meaning balance (lower L scores) compared to human categories, suggesting they prioritize statistical efficiency over semantic richness
- Encoder models surprisingly outperform much larger decoder models in human alignment despite smaller scales
- Training dynamics reveal rapid initial concept formation followed by architectural reorganization, with semantic processing migrating from deep to mid-network layers

## Why This Works (Mechanism)

### Mechanism 1: Divergent Optimization (Rate-Distortion Trade-off)
- **Claim:** LLMs and humans utilize fundamentally different strategies for concept organization; LLMs prioritize statistical compression while humans preserve semantic nuance.
- **Mechanism:** The standard training objective drives models to minimize the information-theoretic L objective, forcing representations toward tight, low-variance clusters that are mathematically optimal but lack high-entropy "inefficiencies" found in human cognitive data.
- **Core assumption:** The L objective faithfully captures the trade-off between semantic fidelity and storage efficiency.
- **Evidence anchors:** LLMs aggressively compress achieving more optimal information-theoretic compression at the cost of semantic richness; LLM-derived clusters consistently achieve more "optimal" compression-meaning balance; Human conceptualizations appear less statistically compact.
- **Break condition:** If models were trained with an explicit penalty for cluster purity or reward for high internal entropy, this divergence would likely narrow.

### Mechanism 2: Two-Phase Semantic Migration
- **Claim:** Concept formation follows a specific migration pattern from deep to mid-layers during training.
- **Mechanism:** Training dynamics reveal an initial rapid formation phase where categorical alignment rises sharply, followed by reorganization shifting semantic processing from deep layers to mid-network layers, potentially reflecting a shift from memorization to sparse, efficient encoding.
- **Core assumption:** The peak AMI layer accurately represents the primary locus of semantic processing at a given training step.
- **Evidence anchors:** Semantic processing migrates from layer 29 to layer 23, suggesting the model discovers more efficient internal representations.
- **Break condition:** If architectural constraints were altered significantly, the migration depth or speed might change, potentially stalling the efficiency phase.

### Mechanism 3: Encoder Advantage in Human Alignment
- **Claim:** Bidirectional architectures capture human-like categorical boundaries more efficiently than causal decoders, independent of parameter scale.
- **Mechanism:** Encoder models likely leverage bidirectional context to form global geometric clusters that align more closely with human prototypes, while decoders optimize for sequential generation which may conflict with static global structure required for high AMI.
- **Core assumption:** The static embedding space of encoders is functionally comparable to the hidden states of decoders for this specific clustering task.
- **Evidence anchors:** BERT-large-uncased (340M parameters) achieves AMI = 0.60, matching or exceeding models 100× larger.
- **Break condition:** If fine-tuning decoders specifically for embedding retrieval rather than generation, this performance gap would likely disappear or invert.

## Foundational Learning

- **Concept:** Information Bottleneck (IB) / Rate-Distortion Theory
  - **Why needed here:** The entire paper is framed around the L objective, which balances compression (Rate/I(X;C)) against accuracy (Distortion). Without this, the "optimality" of LLMs vs. "inefficiency" of humans is unintelligible.
  - **Quick check question:** If we increase β in the L objective, do we prioritize lower complexity (fewer clusters) or lower distortion (tighter clusters)?

- **Concept:** Prototype Theory & Typicality Gradients
  - **Why needed here:** The paper measures alignment using classic benchmarks where "robin" is a central prototype and "penguin" is an outlier. Understanding that humans grade category membership is key to seeing why LLMs fail RQ2 (Spearman ρ < 0.2).
  - **Quick check question:** Why would a model that achieves high categorical alignment (RQ1) simultaneously fail to capture typicality (RQ2)?

- **Concept:** Static vs. Contextual Embeddings
  - **Why needed here:** The authors distinguish between the input embedding matrix (Static/E-matrix) and hidden layer outputs (Contextual). The surprising result that static embeddings often rival contextual ones is central to their efficiency claims.
  - **Quick check question:** Does the "Encoder advantage" hold true for static embeddings, or does it require contextual layers to manifest?

## Architecture Onboarding

- **Component map:** Words -> Tokenizer -> Static Embeddings (E-matrix) -> Contextual Layers (Deep → Mid migration) -> Clustering (k-means) -> L Objective & Alignment Metrics
- **Critical path:** Extracting embeddings at the correct layer is vital. Do not default to the final layer; for OLMo, the peak semantic layer migrates from 28 → 23 during training.
- **Design tradeoffs:**
  - Decoder Models: superior generation/fluency but lower alignment-per-parameter; high compression (low L)
  - Encoder Models: superior alignment/structure; lower computational cost for categorization tasks
- **Failure signatures:** High Compression, Low Meaning: A model with very low L score but Spearman ρ < 0.1 indicates it has "solved" statistical clustering but lost internal geometry (typicality) of human concepts.
- **First 3 experiments:**
  1. Reproduce RQ1 (Categorical Alignment): Extract static embeddings from a small decoder (e.g., Pythia-160M) and a small encoder (e.g., BERT-base). Compute AMI against the Rosch (1975) dataset. Verify if the encoder punches above its weight class.
  2. Probe RQ3 (The L Frontier): Run k-means on the embeddings for K ∈ [5, 50]. Plot the L curve. Confirm if the model's curve sits below the human baseline (indicating "superior" but potentially "meaning-poor" compression).
  3. Spot Check Typicality (RQ2): Take the "Bird" category. Calculate cosine similarity between the centroid and items like "Robin" (typical) vs. "Penguin" (atypical). Check if the ranking matches human intuition (it likely won't for large decoders).

## Open Questions the Paper Calls Out
None

## Limitations

- **Generalization across benchmarks** (Medium confidence): Findings are based on narrow slice of human conceptual knowledge from Rosch and McCloskey & Glucksberg datasets; generalizability to other semantic domains remains unclear.
- **Static vs. contextual embedding equivalence** (Low confidence): The paper treats static and contextual embeddings as interchangeable for clustering, but this equivalence assumption is critical yet insufficiently validated.
- **Layer selection methodology** (Medium confidence): Identifying peak semantic layers through maximum AMI values may oversimplify complex relationship between depth and semantic processing.

## Confidence

- **RQ1: Categorical alignment** (High): Well-supported by consistent results across multiple benchmarks and model families.
- **RQ2: Failure to capture typicality** (High): Robust finding with clear theoretical significance.
- **RQ3: Superior compression-meaning balance** (Medium): Mathematically sound but interpretation depends on valuing compression optimality vs semantic richness.
- **Encoder advantage** (Medium): Well-documented but underlying mechanism remains speculative.
- **Two-phase training dynamics** (Low): Based on single-model analysis requiring replication across architectures.

## Next Checks

1. **Benchmark expansion validation**: Replicate the full analysis pipeline using additional semantic benchmark datasets from different cognitive domains to test generalizability of the compression-meaning trade-off findings.

2. **Static vs. contextual disentanglement**: Design an experiment that explicitly separates static embedding contributions from contextual processing by ablating static embeddings and comparing clustering performance.

3. **Layer-wise semantic migration verification**: Conduct a fine-grained analysis of semantic processing across training steps by measuring AMI, typicality correlation, and L scores at each layer individually throughout training.