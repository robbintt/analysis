---
ver: rpa2
title: 'ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language
  Models'
arxiv_id: '2601.15812'
source_url: https://arxiv.org/abs/2601.15812
tags:
- error
- errors
- failure
- language
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ErrorMap, a method to systematically chart
  the sources of failure in large language models (LLMs) by analyzing incorrect predictions
  to extract structured taxonomies of model errors. Unlike existing evaluations that
  measure success rates, ErrorMap focuses on understanding why models fail, transforming
  raw errors into interpretable categories.
---

# ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models

## Quick Facts
- arXiv ID: 2601.15812
- Source URL: https://arxiv.org/abs/2601.15812
- Reference count: 40
- Primary result: Introduces ErrorMap method to systematically chart LLM failure sources through structured error taxonomies

## Executive Summary
ErrorMap presents a novel approach to understanding why large language models fail by systematically analyzing incorrect predictions to create structured taxonomies of model errors. Unlike traditional evaluations that focus on success rates, this method transforms raw errors into interpretable categories through an LLM judge that extracts and labels the first major error in each incorrect output. The approach reveals that many prevalent but understudied error types, such as "Missing Required Element" and "Specification Misinterpretation," dominate model failures across diverse datasets and tasks.

The authors construct ErrorAtlas, a comprehensive taxonomy containing 17 high-level error categories that uncovers distinct failure patterns across 83 models and 35 datasets. This framework demonstrates practical utility in model debugging, benchmark analysis, and model selection while providing insights that standard benchmarks cannot capture. The method is validated for accuracy, coverage, and robustness, with code and taxonomy publicly released for ongoing updates and community contributions.

## Method Summary
ErrorMap systematically charts LLM failures by analyzing incorrect predictions to extract structured taxonomies of model errors. The method employs an LLM judge to analyze each incorrect output, identifying and labeling the first major error with a concise category. These error labels are then recursively grouped into a multi-level taxonomy through clustering algorithms. Applied across 35 datasets and 83 models, this approach transforms raw error data into interpretable categories, revealing underlying failure patterns that traditional success-rate metrics miss. The process creates ErrorAtlas, a comprehensive failure taxonomy that captures 17 high-level error categories prevalent across different model architectures and task types.

## Key Results
- ErrorAtlas reveals 17 high-level error categories, including understudied types like "Missing Required Element" and "Specification Misinterpretation"
- Method demonstrates distinct failure patterns across different models and tasks, providing insights beyond standard benchmarks
- ErrorMap validated for accuracy, coverage, and robustness, with practical utility demonstrated in model debugging and selection

## Why This Works (Mechanism)
The method works by shifting focus from measuring what models get right to understanding what they get wrong and why. By systematically categorizing errors rather than just counting them, ErrorMap reveals structural weaknesses in model behavior that aggregate performance metrics obscure. The LLM judge serves as an automated error analyst, extracting meaningful patterns from individual failures and aggregating them into a hierarchical taxonomy. This bottom-up approach to error analysis captures the nuanced ways models fail, enabling targeted improvements and more informed model selection.

## Foundational Learning
- **Error taxonomy construction**: Systematic categorization of model failures provides actionable insights for improvement
  - *Why needed*: Traditional metrics hide failure modes that could guide model development
  - *Quick check*: Taxonomy covers majority of observed errors across diverse datasets

- **LLM-based error analysis**: Using language models to analyze and categorize their own failures
  - *Why needed*: Manual error analysis is labor-intensive and inconsistent across annotators
  - *Quick check*: Judge accuracy validated against human annotations

- **Recursive clustering**: Hierarchical grouping of error categories from specific to general
  - *Why needed*: Creates interpretable structure from large number of individual error types
  - *Quick check*: Cluster stability across different datasets and error distributions

## Architecture Onboarding

**Component map**: Dataset → Model → Error → LLM Judge → Error Label → Clustering → Taxonomy

**Critical path**: The LLM judge analyzing incorrect outputs to extract the first major error is the core component. Without accurate error identification and labeling, the entire taxonomy construction fails. This step requires high precision to ensure meaningful categories emerge from the clustering process.

**Design tradeoffs**: The method trades computational overhead (running LLM judge on every incorrect prediction) for deeper insights into failure modes. This approach prioritizes interpretability and actionable feedback over raw speed, accepting that comprehensive error analysis requires significant processing time but yields more valuable results for model improvement.

**Failure signatures**: Taxonomy construction may fail if the LLM judge cannot accurately identify errors, if clustering produces inconsistent groupings, or if error distributions are too sparse across datasets. The method assumes errors can be meaningfully categorized and that patterns exist across different model architectures.

**3 first experiments**: 1) Run ErrorMap on a single dataset with one model to validate judge accuracy and error extraction. 2) Apply clustering to extracted error labels to test taxonomy generation on small scale. 3) Compare ErrorAtlas-derived insights against traditional benchmark analysis for the same models.

## Open Questions the Paper Calls Out
None

## Limitations
- Taxonomy comprehensiveness depends on LLM judge accuracy, potentially missing subtle or complex failure modes
- Recursive clustering may produce inconsistent groupings across different datasets or error distributions
- Method focuses only on incorrect predictions, missing errors where models produce plausible but factually wrong outputs

## Confidence
- ErrorAtlas taxonomy construction: High confidence (validated for accuracy, coverage, and robustness)
- ErrorAtlas revealing distinct failure patterns: Medium confidence (depends on dataset and model representativeness)
- Practical utility claims for debugging and selection: Low confidence (requires independent validation)

## Next Checks
1. Independent replication of ErrorMap on new datasets and models to verify taxonomy consistency and error classification accuracy
2. Human evaluation study comparing LLM judge error labels against expert annotators to assess labeling quality and identify missed error categories
3. Longitudinal analysis of ErrorAtlas updates over time to evaluate taxonomy stability and evolution as new models are added