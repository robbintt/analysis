---
ver: rpa2
title: 'Annif at the GermEval-2025 LLMs4Subjects Task: Traditional XMTC Augmented
  by Efficient LLMs'
arxiv_id: '2508.15877'
source_url: https://arxiv.org/abs/2508.15877
tags:
- synthetic
- records
- data
- system
- subject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper describes an automated subject indexing system for bibliographic
  records using efficient large language models (LLMs). The system combines traditional
  multi-label classification with LLM-based synthetic data generation and ranking.
---

# Annif at the GermEval-2025 LLMs4Subjects Task: Traditional XMTC Augmented by Efficient LLMs

## Quick Facts
- arXiv ID: 2508.15877
- Source URL: https://arxiv.org/abs/2508.15877
- Reference count: 16
- The system achieved first place in both quantitative (nDCG@20=0.5697) and qualitative evaluations in the GermEval-2025 LLMs4Subjects Task 2

## Executive Summary
This paper presents an automated subject indexing system for bibliographic records that combines traditional multi-label classification with efficient large language models (LLMs). The system uses synthetic data generation to augment limited training data, with metadata records translated into German and English using multiple small, efficient LLMs. By combining Omikuji Bonsai, MLLM, and XTransformer base models into weighted ensembles with LLM ranking, the approach achieved first place in both quantitative and qualitative evaluations of the GermEval-2025 LLMs4Subjects Task.

## Method Summary
The system integrates traditional multi-label classification with LLM-based synthetic data generation and ranking. It translates metadata records into German and English using small, efficient LLMs, then generates synthetic training data to augment the limited original dataset. The approach uses Omikuji Bonsai, MLLM, and XTransformer base models, combined into weighted ensembles with LLM ranking for final subject indexing predictions.

## Key Results
- Achieved first place in both quantitative (nDCG@20=0.5697) and qualitative evaluations in GermEval-2025 LLMs4Subjects Task 2
- Demonstrated that combining efficient LLMs with traditional NLP methods provides competitive performance
- Showed balanced computational efficiency and indexing quality through the hybrid approach

## Why This Works (Mechanism)
The system leverages the strengths of both traditional multi-label classification and modern LLM capabilities. By using efficient small LLMs for translation and synthetic data generation, it overcomes the limitation of limited training data while maintaining computational efficiency. The ensemble approach combines multiple base models (Omikuji Bonsai, MLLM, XTransformer) with LLM ranking, creating a robust system that benefits from diverse classification strategies while using LLMs for final quality assessment and ranking.

## Foundational Learning
- Multi-label text classification (XMTC): Needed for assigning multiple subject categories to bibliographic records; quick check involves verifying the system can handle multiple simultaneous labels per document
- Synthetic data generation: Required to augment limited training datasets; quick check involves validating that synthetic examples maintain realistic distribution patterns
- Model ensemble methods: Essential for combining diverse model strengths; quick check involves confirming weighted averaging or voting produces better results than individual models
- LLM-based ranking: Used for final quality assessment; quick check involves measuring improvement in ranking metrics when LLMs are applied versus traditional ranking

## Architecture Onboarding

**Component Map:** Translation LLMs -> Synthetic Data Generator -> Base Models (Omikuji, MLLM, XTransformer) -> LLM Ranking -> Final Ensemble

**Critical Path:** Raw bibliographic records → Translation (German/English) → Synthetic data generation → Base model classification → LLM-based ranking → Weighted ensemble output

**Design Tradeoffs:** The system prioritizes computational efficiency by using small, efficient LLMs for translation and synthetic data generation rather than large models, accepting some potential quality trade-offs for faster processing. The ensemble approach balances multiple classification strategies against increased complexity and resource requirements.

**Failure Signatures:** Poor translation quality would propagate through the pipeline, affecting synthetic data quality and downstream model performance. Over-reliance on synthetic data could introduce bias or unrealistic patterns. Model ensemble weighting that doesn't reflect actual model performance would reduce overall accuracy.

**First Experiments:** 1) Test translation quality on sample bibliographic records in multiple languages, 2) Validate synthetic data generation produces realistic and diverse examples, 3) Compare individual base model performance against ensemble performance

## Open Questions the Paper Calls Out
None

## Limitations
- Performance was evaluated on a specific dataset from a national technical library, limiting generalizability to other domains
- Reliance on synthetic data generation raises concerns about bias amplification and representativeness of real-world bibliographic records
- Weighted ensemble approach may not be optimal and could benefit from more systematic hyperparameter tuning

## Confidence
- High confidence in system effectiveness on evaluated dataset (achieved first place in both quantitative and qualitative evaluations)
- Medium confidence in general effectiveness of combining efficient LLMs with traditional NLP methods (results compelling but limited to single task)
- Low confidence in computational efficiency claims (paper lacks detailed runtime and resource utilization metrics)

## Next Checks
1. Evaluate the system on bibliographic datasets from different domains and languages to assess generalizability
2. Conduct ablation studies to quantify the contribution of synthetic data generation versus traditional models
3. Perform detailed computational efficiency analysis comparing resource usage and inference times against both traditional and larger LLM-based approaches