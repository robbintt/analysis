---
ver: rpa2
title: 'GeoBS: Information-Theoretic Quantification of Geographic Bias in AI Models'
arxiv_id: '2509.23482'
source_url: https://arxiv.org/abs/2509.23482
tags:
- geo-bias
- spatial
- scores
- performance
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GeoBS, an information-theoretic framework
  for quantifying geographic bias (geo-bias) in AI models. Existing geo-bias metrics
  are often model-specific or spatially implicit, limiting fair comparison across
  models.
---

# GeoBS: Information-Theoretic Quantification of Geographic Bias in AI Models

## Quick Facts
- **arXiv ID:** 2509.23482
- **Source URL:** https://arxiv.org/abs/2509.23482
- **Reference count:** 27
- **Key outcome:** GeoBS introduces a model-agnostic, spatially explicit framework for quantifying geographic bias in AI models using information theory and spatial point pattern analysis.

## Executive Summary
This paper addresses the critical problem of geographic bias (geo-bias) in AI models, where model performance systematically varies across different geographic locations. Existing geo-bias metrics are often model-specific or spatially implicit, limiting fair comparison across models. The proposed GeoBS framework bridges this gap by leveraging spatial point pattern analysis and information theory to create model-agnostic, spatially explicit geo-bias scores. The framework decomposes geo-bias metrics into three dimensions: the map used (location or performance), the reference pattern (e.g., Gaussian, Poisson), and the difference measure (e.g., statistical, information-theoretic). Three novel geo-bias scores—Scale-Grid SRE, Distance-Lag SRE, and Direction-Sector SRE—are proposed to capture multi-scalability, distance decay, and anisotropy. Extensive experiments on 3 tasks, 8 datasets, and 8 models demonstrate that both task-specific GeoAI models and general-purpose foundation models exhibit substantial geo-bias, highlighting the importance of spatially explicit evaluation.

## Method Summary
The GeoBS framework quantifies geographic bias by treating location maps and performance maps as spatial point patterns. It compares observed patterns against predefined reference patterns (e.g., Gaussian, Poisson) using statistical or information-theoretic measures. The framework introduces five geo-bias scores: Unmarked SSI and Marked SSI (which measure dataset and model performance bias respectively), and three Spatial Relative-Entropy (SRE) scores that capture specific spatial factors. The SRE scores use KL divergence to measure the information gap between ROI-level and patch-level performance distributions, with three partitioning strategies (Scale-Grid, Distance-Lag, Direction-Sector) to isolate distinct spatial factors of bias. The approach is model-agnostic and can be applied to any AI model that outputs performance metrics at geographic locations.

## Key Results
- Both task-specific GeoAI models and general-purpose foundation models exhibit substantial geo-bias across multiple datasets and tasks
- The three SRE scores (Scale-Grid, Distance-Lag, Direction-Sector) successfully capture different spatial factors of bias: multi-scalability, distance decay, and anisotropy
- The framework demonstrates good scalability and computational efficiency, enabling analysis across large geospatial datasets
- A Python package, GeoBS, is provided for efficient computation of these scores, making the framework accessible to researchers and practitioners

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spatial point patterns provide a principled basis for geo-bias quantification because model performance distributions across geographic space can be compared against reference "unbiased" patterns.
- Mechanism: Location maps (unmarked patterns) and performance maps (marked patterns) are treated as spatial point patterns. The framework quantifies geo-bias by measuring the difference between observed patterns and predefined reference patterns using statistical or information-theoretic measures. This decomposition enables model-agnostic comparison.
- Core assumption: A "fair" model should exhibit performance that is spatially homogeneous according to a chosen reference distribution; deviation from this homogeneity indicates bias.
- Evidence anchors:
  - [section 4.1] "From a theoretical perspective of spatial point pattern analysis, this intuition can be precisely described as comparing a location/performance map with a predefined, spatially homogeneous reference pattern, and the more different they are the more geo-biased the model is."
  - [abstract] "GeoBS bridges this gap by leveraging spatial point pattern analysis and information theory to create model-agnostic, spatially explicit geo-bias scores."

### Mechanism 2
- Claim: KL divergence provides an information-theoretic measure of geo-bias by quantifying the "information gap" between ROI-level and patch-level performance distributions.
- Mechanism: For each patch Pk within a partitioned ROI N, the local SRE score computes D_KL(h(Pk) || h(N)), where h represents histogram-based performance distributions. The weighted sum across patches yields the local score. Lower values indicate more homogeneous performance across spatial partitions; higher values indicate stronger localized deviations.
- Core assumption: Histogram-based empirical distributions adequately capture the performance characteristics relevant to bias; KL divergence is appropriate for discrete probability comparisons.
- Evidence anchors:
  - [section 4.4] "KL divergence has physical meanings in that it measures the information gap between two distributions (i.e., relative-entropy), which can be interpreted as the bits needed to transform a geo-biased map into an unbiased one."
  - [section 4.3.2] Definition 4.3 formally defines the Local SRE Score as the weighted sum of KL divergences.

### Mechanism 3
- Claim: Three spatially explicit partitioning strategies (Scale-Grid, Distance-Lag, Direction-Sector) isolate distinct spatial factors of geo-bias, enabling interpretable diagnosis.
- Mechanism: Scale-Grid partitions ROIs into equal-area squares to capture multi-scale heterogeneity. Distance-Lag uses concentric rings to capture distance-decay effects. Direction-Sector uses angular sectors to capture anisotropy. Each partitioning exposes different spatial dependencies.
- Core assumption: The chosen partitioning aligns with the spatial factors relevant to the model's bias; rectangular, radial, and sectoral geometries capture the dominant modes of spatial variation.
- Evidence anchors:
  - [abstract] "Three novel geo-bias scores—Scale-Grid SRE, Distance-Lag SRE, and Direction-Sector SRE—are proposed to capture multi-scalability, distance decay, and anisotropy."
  - [section 4.3.1] "We are interested in three specific partitionings: scale-grid, distance-lag, and direction-sector... They correspond to three important spatial factors – multi-scalability, distance decay, and anisotropy, respectively."

## Foundational Learning

- Concept: **Spatial Point Pattern Analysis (SPP)**
  - Why needed here: The entire GeoBS framework treats location maps and performance maps as spatial point patterns; understanding first-order vs. second-order statistics, marked vs. unmarked patterns, and summary statistics is essential to interpret geo-bias scores.
  - Quick check question: Given a set of model performance values at geographic locations, can you explain whether this is a marked or unmarked pattern and why that matters for bias quantification?

- Concept: **Information Theory Fundamentals (Entropy, KL Divergence)**
  - Why needed here: SRE scores use KL divergence as the difference measure; understanding relative entropy as "bits needed to transform one distribution into another" helps interpret score magnitudes and compare across models.
  - Quick check question: If a patch has performance distribution h(Pk) identical to the ROI distribution h(N), what is the KL divergence value and what does this imply about geo-bias?

- Concept: **Spatial Heterogeneity and MAUP**
  - Why needed here: The paper explicitly connects to the Modifiable Areal Unit Problem and Simpson's Paradox in spatial contexts; understanding how spatial partitioning affects conclusions is critical for selecting appropriate partition functions and interpreting results.
  - Quick check question: Why might the same model show different geo-bias scores under Scale-Grid vs. Distance-Lag partitioning, and which should you report?

## Architecture Onboarding

- Component map: **Performance Map -> Partition Function -> Histogram Generation -> KL Divergence Computation -> Local Score Aggregation -> Global Score**

- Critical path:
  1. Input performance map M_{D,π} := {(L_i, π_i)} from model evaluation.
  2. Define ROI centers (e.g., via sampling or grid) and radius r based on data density (ensure ≥100 points per ROI; ≥2 patches with ≥10 points).
  3. Select partition function (Scale-Grid, Distance-Lag, or Direction-Sector) based on spatial factor of interest.
  4. Compute patch-level histograms h(P_k) and ROI-level histogram h(N).
  5. Compute KL divergence D_KL(h(P_k) || h(N)) for each patch.
  6. Aggregate weighted divergences to local SRE score γ_SRE(N).
  7. Aggregate local scores to global score Γ_SRE via weighted sum.

- Design tradeoffs:
  - **KL divergence vs. Wasserstein distance**: KL is O(n) for discrete distributions and interpretable as information gap; Wasserstein is O(n³) but captures geometric distances between bins.
  - **Histogram bin selection**: Choice of bins affects distribution estimation; binary (correct/incorrect) used in classification; empirically thresholded for regression.
  - **Partition granularity vs. statistical stability**: Finer partitions capture more localized bias but require more data per patch; coarser partitions may miss fine-grained patterns.
  - **ROI radius and spatial scale**: Larger radii capture broader spatial patterns but may conflate distinct bias types; smaller radii localize analysis but reduce sample sizes.

- Failure signatures:
  - **Nan/inf in scores**: Occurs when patches have zero counts in histogram bins; addressed via background point generator with adaptive density.
  - **Identical scores across models (for U-SSI)**: Unmarked SSI depends only on location map, not model performance, so it measures dataset bias.
  - **High variance in repeated runs**: Original SSI implementations used random background points; GeoBS uses Fibonacci Lattice for reproducibility.
  - **SRE = 0 for all patches**: Indicates perfectly homogeneous performance across the partitioning; may signal too coarse partitioning or genuinely unbiased model.

- First 3 experiments:
  1. **Baseline geo-bias profiling on existing model**: Apply all five geo-bias scores to your trained model on a geospatial test set. Report which spatial factors show highest bias. Verify hyperparameters meet sample size thresholds.
  2. **Dataset vs. model bias decomposition**: Compare Unmarked SSI vs. Marked SSI to determine whether observed geo-bias stems from data distribution or model behavior.
  3. **Cross-partition consistency check**: Compute SRE scores under all three partitionings for the same model/dataset. If bias is concentrated in one partition type, this indicates anisotropic bias; if all three are elevated, bias is multi-faceted.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can second-order spatial statistics (point interactions) be integrated into GeoBS to capture geo-bias more effectively than the proposed first-order methods?
- Basis in paper: [explicit] Section 4.1 states, "Most existing geo-bias metrics... are first-order. We leave the investigation of second-order geo-bias metrics to the future."
- Why unresolved: The current framework relies on first-order summary statistics, which overlooks spatial dependencies and interactions between points that second-order statistics would reveal.
- What evidence would resolve it: A derivation of geo-bias scores using second-order measures like Ripley's K-function, demonstrating their ability to detect interaction-based biases invisible to current SRE scores.

### Open Question 2
- Question: Can the proposed GeoBS scores be effectively utilized as differentiable loss functions to mitigate geo-bias during model training?
- Basis in paper: [explicit] Section 6 notes, "We see great potential in introducing the geo-bias scores as debiasing loss functions and help train more fair models."
- Why unresolved: The paper validates GeoBS strictly as an evaluation metric; the mathematical feasibility and stability of using these information-theoretic scores as optimization objectives remain untested.
- What evidence would resolve it: Empirical results showing successful backpropagation using a GeoBS-based loss term that reduces evaluation geo-bias without causing significant degradation in primary task performance.

### Open Question 3
- Question: How can the GeoBS framework be extended to quantify geo-bias in non-Euclidean spatial structures, such as networks, or dynamic time-space contexts?
- Basis in paper: [explicit] Section 6 lists future work to "design more geo-bias scores... that deal with other intricate spatial factors, for example, network and time-space."
- Why unresolved: The current partition functions assume standard geometric containers in 2D/3D space and do not account for topological connectivity in networks or temporal evolution.
- What evidence would resolve it: Novel partition functions defined for graph-based spatial data or spatiotemporal cubes, integrated into the SRE framework to measure bias in network-constrained or time-series geospatial tasks.

## Limitations

- **Reference pattern sensitivity**: The choice of Gaussian, Poisson, or Permutation as reference patterns is not empirically validated and may not hold for all geospatial tasks or geographic contexts.
- **Model comparison validity**: While presented as model-agnostic, the framework's sensitivity to different performance metrics and spatial scales is not systematically explored across heterogeneous models.
- **Computational assumptions**: The framework assumes sufficient spatial point density but doesn't address how performance degrades with sparse data or how to select optimal hyperparameters for heterogeneous datasets.

## Confidence

- **High confidence**: The mathematical formulation of the framework and implementation details are well-specified and reproducible.
- **Medium confidence**: Experimental results showing substantial geo-bias are credible, but interpretation of which spatial factors are most problematic lacks statistical validation.
- **Low confidence**: The claim that GeoBS provides a "principled basis" for geo-bias quantification is not fully substantiated by corpus evidence.

## Next Checks

1. **Reference pattern sensitivity analysis**: Systematically vary reference patterns and quantify how geo-bias scores change across different models and datasets to validate robustness to pattern selection.

2. **Cross-task generalizability test**: Apply GeoBS to non-geospatial tasks with inherent spatial structure (e.g., medical imaging across anatomical regions) to assess whether the framework generalizes beyond tested geospatial domains.

3. **Benchmark against established fairness metrics**: Compare GeoBS scores with traditional fairness metrics on tasks where geographic and demographic attributes overlap to validate that GeoBS captures meaningful bias beyond standard approaches.