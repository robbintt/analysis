---
ver: rpa2
title: 'Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor
  Graph Color Refinement'
arxiv_id: '2509.09219'
source_url: https://arxiv.org/abs/2509.09219
tags:
- learning
- graph
- policy
- which
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Vejde, a framework that combines data abstraction,
  graph neural networks, and reinforcement learning to produce inductive policy functions
  for decision problems with richly structured states, such as object classes and
  relations. MDP states are represented as databases of facts about entities, and
  Vejde converts each state to a bipartite graph, which is mapped to latent states
  through neural message passing.
---

# Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement

## Quick Facts
- arXiv ID: 2509.09219
- Source URL: https://arxiv.org/abs/2509.09219
- Authors: Jakob Nyberg; Pontus Johnson
- Reference count: 40
- Key outcome: Vejde policies generalize to unseen problem instances with minimal performance loss compared to instance-specific MLP agents and online planning algorithms.

## Executive Summary
This paper presents Vejde, a framework that combines data abstraction, graph neural networks, and reinforcement learning to produce inductive policy functions for decision problems with richly structured states, such as object classes and relations. MDP states are represented as databases of facts about entities, and Vejde converts each state to a bipartite graph, which is mapped to latent states through neural message passing. The factored representation of both states and actions allows Vejde agents to handle problems of varying size and structure. The authors evaluated Vejde agents on eight problem domains defined in RDDL, with ten problem instances each, where policies were trained using both supervised and reinforcement learning.

## Method Summary
Vejde combines data abstraction, graph neural networks, and reinforcement learning to create inductive policy functions for decision problems with structured states. The framework represents MDP states as databases of facts about entities, converts each state to a bipartite graph, and maps it to latent states through neural message passing. The factored representation of states and actions enables Vejde agents to handle problems of varying size and structure. Policies were trained using both supervised and reinforcement learning across eight problem domains defined in RDDL.

## Key Results
- Vejde policies in average generalize to test instances without significant loss in score.
- Inductive agents received scores on unseen test instances that on average were close to instance-specific MLP agents.
- Test results on unseen instances for Vejde agents were compared to MLP agents trained on each problem instance, as well as the online planning algorithm Prost.

## Why This Works (Mechanism)
Vejde works by leveraging graph neural networks to process structured state representations, enabling inductive learning across problem instances with varying structures. The framework's ability to generalize stems from its factored representation of states and actions, which captures the relational structure of the decision problems. By converting MDP states to bipartite graphs and using neural message passing, Vejde can learn policies that transfer knowledge across similar but structurally different problem instances.

## Foundational Learning
- **Graph Neural Networks**: Why needed: To process structured state representations and capture relational information. Quick check: Verify that message passing effectively aggregates information across graph nodes.
- **Factor Graph Color Refinement**: Why needed: To identify and group similar nodes in the bipartite graph representation. Quick check: Ensure color refinement produces meaningful node clusters for policy learning.
- **Inductive Learning**: Why needed: To enable policy generalization across problem instances with varying structures. Quick check: Validate that policies trained on one instance perform well on structurally similar unseen instances.

## Architecture Onboarding
**Component Map**: MDP State -> Bipartite Graph -> Neural Message Passing -> Latent State -> Policy Network
**Critical Path**: The transformation of MDP states to bipartite graphs followed by neural message passing to latent states is critical for enabling inductive policy learning.
**Design Tradeoffs**: The framework trades computational efficiency for the ability to handle richly structured states and generalize across problem instances. The use of graph neural networks adds complexity but enables better representation of relational structures.
**Failure Signatures**: Poor performance on unseen instances may indicate insufficient training data or inadequate graph representation of the problem structure.
**First Experiments**: 1) Test policy generalization on structurally similar problem instances. 2) Compare performance with and without factor graph color refinement. 3) Evaluate the impact of different graph neural network architectures on policy performance.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation is limited to eight problem domains defined in RDDL with ten instances each.
- The study does not report statistical significance tests for the performance comparisons between Vejde, MLP agents, and Prost.
- The paper lacks analysis of how Vejde performs with noisy or incomplete state representations, and does not explore scalability beyond the tested domains.

## Confidence
- **High confidence**: The methodology for converting MDP states to bipartite graphs and using neural message passing is technically sound and well-described.
- **Medium confidence**: The generalization claims are supported by the experimental results, but the lack of statistical testing reduces confidence in the significance of performance differences.
- **Low confidence**: The framework's robustness to real-world noise and scalability to more complex domains remains untested.

## Next Checks
1. Conduct statistical significance tests (e.g., paired t-tests) on the performance comparisons between Vejde, MLP agents, and Prost across all domains.
2. Evaluate Vejde's performance on domains with noisy or incomplete state representations to assess robustness.
3. Test scalability by applying Vejde to larger, more complex problem domains beyond the eight RDDL domains used in this study.