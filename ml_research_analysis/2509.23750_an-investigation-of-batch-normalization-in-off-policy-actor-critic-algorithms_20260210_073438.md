---
ver: rpa2
title: An Investigation of Batch Normalization in Off-Policy Actor-Critic Algorithms
arxiv_id: '2509.23750'
source_url: https://arxiv.org/abs/2509.23750
tags:
- critic-i
- training
- learning
- mode
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Batch Normalization (BN) faces limited adoption in deep reinforcement
  learning (DRL) due to non-i.i.d. data and shifting distributions.
---

# An Investigation of Batch Normalization in Off-Policy Actor-Critic Algorithms

## Quick Facts
- arXiv ID: 2509.23750
- Source URL: https://arxiv.org/abs/2509.23750
- Reference count: 40
- Batch Normalization (BN) faces limited adoption in deep reinforcement learning (DRL) due to non-i.i.d. data and shifting distributions.

## Executive Summary
This work systematically investigates Batch Normalization mode selection in off-policy actor-critic algorithms, addressing why BN sees limited adoption in DRL due to non-i.i.d. data and shifting distributions. The study identifies that using training mode BN during policy evaluation can cause instability through distribution mismatch, while evaluation mode BN provides more stable Q-value estimates. Based on these empirical findings, the authors propose Mode-Aware Batch Normalization (MA-BN), which strategically employs evaluation mode BN during policy updates to enhance learning stability.

## Method Summary
The research systematically examines how different Batch Normalization modes affect off-policy actor-critic algorithms through controlled experiments. The authors implement TD3 and SAC algorithms with configurable BN modes during both critic training and policy updates. They conduct extensive empirical studies across DeepMind Control Suite and MuJoCo environments, comparing training mode BN, evaluation mode BN, and no normalization scenarios. The key innovation is MA-BN, which switches normalization modes based on whether the network is being used for policy evaluation or policy improvement, effectively addressing the distribution mismatch problem inherent in DRL.

## Key Results
- MA-BN accelerates training convergence compared to standard BN or no normalization
- The method broadens the effective learning rate range, making hyperparameter tuning more forgiving
- MA-BN enhances exploration capabilities and reduces optimization difficulty across tested environments

## Why This Works (Mechanism)
The mechanism relies on addressing the fundamental mismatch between training and inference data distributions in reinforcement learning. During policy evaluation (Q-value estimation), using training mode BN accumulates statistics from the current policy's behavior, which may differ significantly from the data used during policy improvement. Evaluation mode BN instead uses fixed statistics from the training data, providing more stable estimates that don't fluctuate with the current policy's distribution. This stability in Q-value estimates translates to more reliable policy gradient signals during learning.

## Foundational Learning
- **Batch Normalization mechanics**: Understanding how BN normalizes activations using batch statistics is crucial for recognizing why distribution shifts cause instability in DRL
  - Why needed: BN's reliance on batch statistics makes it sensitive to data distribution changes
  - Quick check: Verify understanding of training vs evaluation mode differences in BN

- **Off-policy actor-critic framework**: The interaction between policy evaluation (critic) and policy improvement (actor) creates unique challenges for normalization
  - Why needed: Different network components have different data distribution requirements
  - Quick check: Confirm understanding of how critic and actor networks interact during learning

- **Non-i.i.d. data in RL**: Unlike supervised learning, RL data is temporally correlated and generated by changing policies
  - Why needed: Explains why standard BN assumptions break down in DRL
  - Quick check: Recognize that RL data distributions shift as policies improve

## Architecture Onboarding
- **Component map**: Policy Network -> BN (evaluation mode) -> Action Selection; Critic Network -> BN (training mode during critic update, evaluation mode during policy update)
- **Critical path**: Critic evaluation → Q-value estimation → Policy gradient computation → Policy update
- **Design tradeoffs**: Evaluation mode BN provides stability but may be less adaptive to current data distribution; training mode BN is more adaptive but introduces variance
- **Failure signatures**: Training mode BN in policy evaluation causes Q-value estimation instability and learning oscillations
- **First experiments**: 1) Compare Q-value variance between training and evaluation mode BN during policy evaluation, 2) Measure learning stability with different BN modes across varying learning rates, 3) Test exploration behavior differences between normalization strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Findings are limited to specific algorithms (TD3, SAC) and environments (DMC, MuJoCo), with unclear generalizability to other DRL settings
- Theoretical justification for why evaluation mode BN stabilizes learning is primarily empirical without rigorous mathematical analysis
- The interaction between normalization modes and other hyperparameters beyond learning rate remains unexplored

## Confidence
- **High**: Empirical observations about training mode BN causing instability in policy evaluation
- **Medium**: Claim that MA-BN improves learning speed and stability across tested environments
- **Low**: Theoretical explanation for why evaluation mode BN yields more stable Q-value estimates

## Next Checks
1. Test MA-BN across a broader range of DRL algorithms including on-policy methods like PPO and value-based methods like DQN to assess generalizability
2. Conduct ablation studies comparing MA-BN against alternative normalization techniques (Weight Normalization, Group Normalization) to better understand its relative advantages
3. Perform extensive hyperparameter sensitivity analysis to identify interactions between learning rate, discount factor, and normalization mode choices across different task complexities