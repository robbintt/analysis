---
ver: rpa2
title: 'KnowRL: Teaching Language Models to Know What They Know'
arxiv_id: '2510.11407'
source_url: https://arxiv.org/abs/2510.11407
tags:
- arxiv
- https
- self-knowledge
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces KnowRL, a reinforcement learning framework\
  \ that enhances large language models\u2019 self-knowledge by improving their ability\
  \ to recognize the limits of their own capabilities. The method uses introspection,\
  \ where the model generates tasks it judges as feasible or infeasible, and consensus-based\
  \ rewarding, where internal agreement among multiple self-assessments reinforces\
  \ accurate self-knowledge."
---

# KnowRL: Teaching Language Models to Know What They Know

## Quick Facts
- **arXiv ID:** 2510.11407
- **Source URL:** https://arxiv.org/abs/2510.11407
- **Reference count:** 40
- **Primary result:** Improves LLM self-knowledge by up to 28% accuracy and 12% F1 using consensus-based RL on self-generated introspection data

## Executive Summary
KnowRL is a reinforcement learning framework that enhances large language models' self-knowledge by improving their ability to recognize the limits of their own capabilities. The method uses introspection, where the model generates tasks it judges as feasible or infeasible, and consensus-based rewarding, where internal agreement among multiple self-assessments reinforces accurate self-knowledge. By using only internally generated data and minimal supervision, KnowRL improves both intrinsic consistency and extrinsic benchmark performance. Experiments with LLaMA-3.1-8B and Qwen-2.5-7B show up to 28% accuracy and 12% F1 gains over baseline models within 30 iterations.

## Method Summary
KnowRL uses an iterative reinforcement learning loop where the model generates tasks it believes are feasible or infeasible, then assesses these tasks multiple times to compute consensus-based rewards. Starting from a small seed set of 100 verified examples, the model performs introspection to propose new tasks, which are filtered for reward hacking, then evaluated through k=8 independent self-analyses. The reward is the proportion of outputs agreeing with the majority label. This consensus score serves as the RL reward, reinforcing tasks where the model exhibits high internal consistency in its feasibility assessment. The process repeats for 30 iterations, progressively refining the model's understanding of its knowledge boundaries.

## Key Results
- Up to 28% accuracy gains and 12% F1 improvements over baseline models on extrinsic benchmarks
- Steady monotonic improvement across 30 iterations demonstrating self-improvement in knowledge awareness
- Effective with minimal supervision using only 100 seed examples and internally generated data
- Successful implementation on LLaMA-3.1-8B and Qwen-2.5-7B models using 8Ã—RTX 4090 GPUs

## Why This Works (Mechanism)

### Mechanism 1: Consensus-based Internal Reward Signal
- Claim: KnowRL improves self-knowledge by using