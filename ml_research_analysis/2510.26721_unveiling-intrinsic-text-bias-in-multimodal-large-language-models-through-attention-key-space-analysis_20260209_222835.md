---
ver: rpa2
title: Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention
  Key-Space Analysis
arxiv_id: '2510.26721'
source_url: https://arxiv.org/abs/2510.26721
tags:
- text
- image
- visual
- bias
- divergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether text bias in multimodal large language
  models arises from internal architectural misalignment rather than external data
  factors. The authors hypothesize that visual key vectors are out-of-distribution
  relative to text key spaces learned during language-only pretraining, causing systematic
  under-utilization of visual information during attention computation.
---

# Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis

## Quick Facts
- **arXiv ID**: 2510.26721
- **Source URL**: https://arxiv.org/abs/2510.26721
- **Reference count**: 9
- **Key outcome**: Visual and textual key vectors occupy distinct subspaces in attention space, with inter-modal divergence exceeding intra-modal variation by orders of magnitude.

## Executive Summary
This study investigates whether text bias in multimodal large language models arises from internal architectural misalignment rather than external data factors. The authors hypothesize that visual key vectors are out-of-distribution relative to text key spaces learned during language-only pretraining, causing systematic under-utilization of visual information during attention computation. To test this, they extract and analyze key vectors from LLaVA and Qwen2.5-VL models using t-SNE for qualitative visualization and Jensen-Shannon divergence for quantitative comparison. Results show that visual and textual keys occupy distinct subspaces in the attention space, with inter-modal divergence exceeding intra-modal variation by orders of magnitude. This provides direct evidence that text bias stems from intrinsic misalignment within the attention key space, not solely from data imbalance.

## Method Summary
The researchers extract key vectors from LLaVA-1.5-7B and Qwen2.5-VL-7B models during inference on MMMU and MMBench-CN benchmarks. They hook into specific decoder layers to capture K-projection outputs, labeling tokens as image or text. The extracted vectors undergo standardization, PCA reduction to 50 dimensions, then t-SNE visualization. Quantitative analysis computes MMD and Jensen-Shannon divergence between visual and text key distributions, comparing cross-modal gaps against intra-modal baselines. The study examines architectural differences between linear projectors (LLaVA) and Q-Former adapters (Qwen) to understand their impact on key-space alignment.

## Key Results
- Visual and textual keys form compact, distinct clusters in t-SNE visualizations, with visual tokens creating isolated "balls" separate from diffuse text "clouds"
- Mean MMD for cross-modality gap is 0.408, exceeding intra-modal variation by orders of magnitude
- LLaVA's linear projection adaptor exhibits larger and more robust modality bias compared to Qwen's Q-Former design, though both show high JS divergence

## Why This Works (Mechanism)

### Mechanism 1: Key-Space Distribution Mismatch (The OOD Hypothesis)
- Claim: If visual key vectors lie out-of-distribution relative to the text-trained key space, they will receive systematically lower attention scores.
- Mechanism: The LLM backbone is pre-trained on text, optimizing the query ($Q$) and key ($K$) projection spaces for textual statistics. When a vision projector maps image features into this space, the resulting visual keys occupy a distinct region. Because attention scores rely on the dot-product similarity between queries and keys, text-derived queries "favor" text-derived keys, effectively filtering out visual information.
- Core assumption: The model relies on standard scaled dot-product attention where similarity correlates with information retention.
- Evidence anchors:
  - [abstract] "Visual key vectors are out-of-distribution (OOD) relative to the text key space... receiving systematically lower similarity scores."
  - [page 1] "Visual keys (Visual K) lie out-of-distribution with respect to this text-centric space."
  - [corpus] While specific key-space OOD mechanisms are rare in the provided neighbors, related work "Unleashing the Intrinsic Visual Representation Capability..." notes that "visual information is often underutilized compared to textual representations in deeper layers," supporting the observation of modal imbalance.
- Break condition: If the vision projector is trained or designed to map visual features specifically into the high-density regions of the text key manifold, this mechanism would diminish.

### Mechanism 2: Geometric Separability Enforcing Modality Bias
- Claim: A significant geometric distance (divergence) between the distributions of visual and text keys enforces modality bias regardless of the semantic content of the image.
- Mechanism: By measuring the Jensen-Shannon (JS) divergence and Maximum Mean Discrepancy (MMD), the study shows that visual and text keys form distinct clusters. This geometric separation prevents cross-modal attention heads from easily integrating information, as the "attention landscape" requires bridging a large statistical gap.
- Core assumption: High statistical divergence in the key space directly translates to functional isolation during attention computation.
- Evidence anchors:
  - [page 4] "The mean MMD for the Cross-Modality Gap... is 0.408... exceeding intra-modal variation by orders of magnitude."
  - [page 3] t-SNE visualizations show visual tokens forming "compact clusters distinct from the textual manifold."
  - [corpus] The neighbor paper "RollingQ" addresses similar issues in multimodal transformers where "dynamic fusion strategies... fail," implying that geometric or dynamic misalignment in attention is a known failure mode in multimodal systems.
- Break condition: If divergence metrics (MMD/JS) were low (near the intra-modal baseline) while text bias persisted, this geometric hypothesis would be falsified.

### Mechanism 3: Connector Architecture Modulation
- Claim: The design of the vision-language connector (e.g., linear projector vs. Q-Former) determines the severity of the key-space misalignment.
- Mechanism: A simple linear projector (as in LLaVA) might preserve the raw structure of vision encoder features but fails to rotate them into the LLM's text-aligned key space. A more complex adapter (like Qwen's Q-Former) may partially reduce the mean distance (MMD), but distributional shape mismatch (JS divergence) often persists.
- Core assumption: The complexity of the adapter correlates with its ability to align distributional shapes, not just mean vectors.
- Evidence anchors:
  - [page 4] "LLaVA... simple linear projection adaptor exhibits a larger and more robust modality bias... Qwen's more complex Q-Former design managed to partially bring the mean features closer [but] high residual JS Divergence... indicated distinct shapes."
  - [page 2] Describes LLaVA as using "linear projection" and Qwen as using "Q-Former adaptor."
- Break condition: If a linear projector achieved low JS divergence, the complexity of the adapter would not be the determining factor for alignment.

## Foundational Learning

- Concept: **Attention Key-Value (KV) Space**
  - Why needed here: The paper analyzes the "Key" vector specifically, rather than the Value or Hidden states. You must understand that in $Attention(Q, K, V)$, the $K$ vector determines *where* to look (retrieval), while $V$ determines *what* to extract.
  - Quick check question: If visual keys are OOD but visual values are high-quality, would the model still struggle to use visual data? (Answer: Yes, because the attention mechanism wouldn't "find" the relevant values).

- Concept: **Jensen-Shannon (JS) Divergence & MMD**
  - Why needed here: The paper uses these specific metrics to prove "separation." You need to distinguish between MMD (distance between mean embeddings) and JS Divergence (similarity of probability distributions).
  - Quick check question: Why does the paper emphasize that JS divergence remains high for Qwen even when MMD drops? (Answer: MMD measures distance between centers; JS measures overlap of distributions. The keys might be closer on average, but their "shape" or spread remains distinct).

- Concept: **Modality Gap / Distributional Shift**
  - Why needed here: The core hypothesis is that visual tokens are effectively "out-of-distribution" inputs to the text-pretrained LLM layers.
  - Quick check question: Why does pretraining on text-only cause a problem for image tokens later? (Answer: The model's weights, specifically the query/key projections, learn to expect specific statistical patterns found in language; images violate these expectations).

## Architecture Onboarding

- Component map:
  Vision Encoder (CLIP/SigLIP) -> Projector/Adapter (Linear / Q-Former) -> LLM Backbone (Vicuna/Qwen)

- Critical path:
  The **Projector** $\rightarrow$ **LLM Key Projection ($W_K$)** path is the locus of the failure. The projector outputs a vector that, when passed through the LLM's pre-trained $W_K$, produces a "Visual Key" that is geometrically far from "Text Keys."

- Design tradeoffs:
  - **Linear Projector (LLaVA):** Cheaper, faster, preserves spatial info better. *Tradeoff:* Higher modality gap (Key-Space misalignment), severe text bias.
  - **Q-Former / Resampler (Qwen):** More parameters, learns to compress/extract info. *Tradeoff:* Reduces mean gap (MMD) but struggles to match the full distribution (JS), still shows bias.

- Failure signatures:
  - **High JS Divergence (>0.3):** Indicates visual and text keys are statistically distinct.
  - **t-SNE Cluster Separation:** Visual tokens form a tight "ball" isolated from the diffuse text "cloud."
  - **Text Bias Behavior:** Model answers correctly using text hints but ignores contradictory visual evidence.

- First 3 experiments:
  1. **Key Vector Extraction:** Hook into the LLM backbone (e.g., Layers 0, 16, 31) and extract the output of the Key projection ($K$) for both image and text tokens during inference.
  2. **Distributional Visualization:** Apply PCA (to 50d) followed by t-SNE (to 2d) on the extracted keys. Color code by modality (Image vs. Text). Look for distinct clusters.
  3. **Quantitative Divergence Check:** Compute MMD and JS Divergence between Image vs. Text keys. Compare this against a baseline of Image vs. Image keys (control). If Inter-modal >> Intra-modal, the mechanism is confirmed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specific regularization techniques or adapter designs effectively align visual key vectors into the text key subspace to mitigate text bias?
- Basis in paper: [inferred] The conclusion states the work "shifts the remediation paradigm from data balancing toward addressing this intrinsic key-space disparity," implying the need for architectural solutions.
- Why unresolved: The paper diagnoses the existence and source of the misalignment but does not propose or test a specific method to realign the subspaces.
- What evidence would resolve it: A follow-up study demonstrating that minimizing JS divergence in the key space via a new loss function or adapter improves performance on vision-centric benchmarks.

### Open Question 2
- Question: Does forcing visual keys into the text key distribution causally improve multimodal reasoning, or does it risk degrading unique visual semantics?
- Basis in paper: [inferred] The authors hypothesize that out-of-distribution keys cause "under-utilization," but they do not verify if closing the distribution gap actually enhances reasoning capabilities.
- Why unresolved: The study establishes a correlation between distinct subspaces and text bias but lacks interventional experiments to prove that unifying the subspaces resolves the bias.
- What evidence would resolve it: Experimental results where visual keys are artificially projected into the text manifold, showing a measurable increase in the model's reliance on visual evidence.

### Open Question 3
- Question: Is the observed key-space divergence a universal property of all MLLMs, or is it specific to architectures that project visual features into a frozen LLM?
- Basis in paper: [inferred] The study is limited to LLaVA and Qwen2.5-VL, which utilize specific adaptor designs (linear projection and Q-Former) on top of pre-trained LLMs.
- Why unresolved: It remains unclear if models trained with native multimodal integration (e.g., from scratch or with unfrozen attention layers) exhibit the same rigid text-centric key-space structure.
- What evidence would resolve it: Comparative analysis of key-space divergence in models trained with different fusion strategies, such as cross-attention or early fusion.

## Limitations
- The study focuses exclusively on key space analysis while leaving query and value spaces unexamined
- The causal mechanism linking key-space OOD to text bias remains correlative rather than experimentally validated
- The analysis is confined to two specific model architectures and two benchmarks, limiting generalizability

## Confidence

- **High Confidence**: The observation that visual and text keys occupy distinct subspaces (measured by t-SNE separation and divergence metrics) is directly measurable and reproducible.
- **Medium Confidence**: The claim that this distributional separation causes text bias is supported but not definitively proven, requiring additional experimental validation.
- **Medium Confidence**: The architectural comparison between linear projectors and Q-Former adapters shows clear differences in alignment metrics, but interpretations are based on limited samples.

## Next Checks

1. **Intervention Experiment**: Implement a key-space alignment technique (e.g., projector fine-tuning to minimize JS divergence) and measure whether text bias reduction correlates with distributional alignment.

2. **Complete Attention Space Analysis**: Extend the analysis to include query and value spaces to determine if similar distributional separations exist and contribute to text bias.

3. **Cross-Architecture Generalization**: Test the key-space misalignment hypothesis on additional MLLM architectures and tasks beyond MMMU/MMBench-CN to determine if the mechanism is universal or architecture-specific.