---
ver: rpa2
title: 'TIMING: Temporality-Aware Integrated Gradients for Time Series Explanation'
arxiv_id: '2506.05035'
source_url: https://arxiv.org/abs/2506.05035
tags:
- time
- timing
- series
- methods
- points
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of current time series explainable
  AI (XAI) methods, which primarily estimate attribution magnitudes while overlooking
  the directional impact on predictions. The authors propose novel evaluation metrics,
  Cumulative Prediction Difference (CPD) and Cumulative Prediction Preservation (CPP),
  to assess the ability of XAI methods to identify significant positive and negative
  points in time series data.
---

# TIMING: Temporality-Aware Integrated Gradients for Time Series Explanation

## Quick Facts
- arXiv ID: 2506.05035
- Source URL: https://arxiv.org/abs/2506.05035
- Authors: Hyeongwon Jang; Changhun Kim; Eunho Yang
- Reference count: 40
- Key outcome: Introduces TIMING, a segment-based masking extension of Integrated Gradients that achieves higher CPD/CPP scores on time series explanation tasks while maintaining computational efficiency

## Executive Summary
This paper addresses fundamental limitations in time series explainable AI (XAI) methods, which primarily estimate attribution magnitudes while overlooking directional impact on predictions. The authors demonstrate that conventional Integrated Gradients (IG) outperforms recent methods under newly proposed cumulative evaluation metrics (CPD/CPP) but has limitations when directly applied to time series due to ignoring temporal relationships and introducing out-of-distribution samples. To overcome these challenges, they introduce TIMING, a temporality-aware extension of IG that incorporates segment-based random masking to capture complex temporal dependencies while maintaining theoretical properties.

## Method Summary
TIMING extends Integrated Gradients by replacing the standard zero baseline with segment-based random masking that retains n randomly selected segments (of lengths between smin and smax) while scaling the rest by interpolation factor α. This creates interpolation paths that maintain some temporal structure while systematically disrupting others. The method accumulates gradients over these masked interpolation points and normalizes by the number of times each feature was unmasked. A key theoretical contribution shows that random mask sampling at each interpolation point suffices for accurate approximation, enabling computational efficiency comparable to standard IG.

## Key Results
- On MIMIC-III mortality prediction, TIMING achieves CPD of 0.366 (K=50) and 0.505 (K=100), outperforming conventional IG (0.342, 0.469)
- TIMING demonstrates strong robustness to hyperparameter choices across synthetic and real-world datasets
- Computational efficiency is competitive, with TIMING achieving 0.04 sec/sample compared to 0.034 sec/sample for IG
- Extensive experiments on 8 datasets show TIMING outperforms existing time series XAI baselines across multiple metrics

## Why This Works (Mechanism)

### Mechanism 1
Cumulative evaluation metrics (CPD/CPP) correctly assess directional attribution methods without sign-canceling artifacts. CPD sequentially removes top-K points by absolute attribution, measuring prediction difference between consecutive steps rather than a single before/after comparison. This prevents positive and negative contributions from canceling out during evaluation, revealing true feature importance. Core assumption: Important features should cause measurable prediction shifts when removed individually or in sequence, regardless of attribution sign.

### Mechanism 2
Segment-based masking preserves meaningful temporal structure while enabling gradient-based attribution. TIMING replaces IG's zero baseline with (1-M)⊙x where M is a binary mask selecting n segments of length [smin, smax]. This creates interpolation paths that maintain some temporal dependencies while systematically disrupting others. The expectation over random segment masks captures contributions under varied temporal disruption scenarios. Core assumption: Temporal dependencies can be meaningfully probed by selectively preserving vs. disrupting contiguous time segments during gradient integration.

### Mechanism 3
Random mask sampling at interpolation points achieves computational efficiency while preserving theoretical guarantees. Proposition 4.1 shows that instead of computing IG repeatedly over different masks, one random mask per interpolation point suffices. This creates a "fluctuating path" from baseline to input, approximating the expectation without multiple full IG computations. Core assumption: The bounded partial derivatives condition holds; random sampling provides unbiased expectation estimation.

## Foundational Learning

- Concept: **Integrated Gradients fundamentals**
  - Why needed here: TIMING extends IG; understanding the baseline, path integral, and completeness axiom is prerequisite
  - Quick check question: Can you explain why IG uses a baseline (typically zero) and integrates gradients along the interpolation path?

- Concept: **Attribution sign semantics (positive vs. negative contributions)**
  - Why needed here: The paper's core insight is that existing metrics unfairly penalize methods with mixed-sign attributions
  - Quick check question: For a binary classifier, what does a positive vs. negative attribution value mean for the predicted class probability?

- Concept: **Out-of-distribution (OOD) samples in explanation paths**
  - Why needed here: Standard IG creates OOD samples when uniformly scaling time series; TIMING's segment masking addresses this
  - Quick check question: Why might gradients computed at interpolation points αx for α∈(0,1) be unreliable for a model trained only on real data?

## Architecture Onboarding

- Component map: Input x → [Segment Mask Generator G(n,smin,smax)] → Masked interpolation points → [Gradient Computation ∂Fŷ/∂x] → [Accumulation over α steps] → [Normalization by unmask count] → Attribution A(F,x)

- Critical path:
  1. For each α ∈ {0, 1/nsamples, 2/nsamples, ..., 1}: generate n random segments, create masked input ˜x
  2. Compute gradient ∂Fŷ(˜x)/∂˜x, accumulate TotalGrad
  3. Track unmask_count per (t,d) position
  4. Final attribution: TotalGrad ⊙ (x - Baselines) / (nsamples - unmask_count)

- Design tradeoffs:
  - Completeness vs. temporal awareness: TIMING sacrifices IG's completeness axiom for better temporal dependency handling
  - Segment granularity: (smin=10, smax=48) balances local vs. extended temporal patterns; smaller smax degrades performance
  - Sample efficiency: nsamples trades off accuracy vs. speed; paper uses single-sample approximation per α

- Failure signatures:
  - Very low CPD scores: Check if model has weak gradient signal; consider increasing nsamples
  - High variance across runs: Segment masking may be too aggressive; reduce n or increase smax
  - Attributions not clinically interpretable: Verify model is actually using clinically relevant features

- First 3 experiments:
  1. Reproduce MIMIC-III CPD comparison with IG, GradSHAP, and TIMING to validate implementation
  2. Ablation: Test RandIG (point-wise random masking) vs. TIMING (segment-based) to isolate segment contribution
  3. Hyperparameter sweep on (n, smin, smax) following Table 6 grid to understand sensitivity for your specific dataset

## Open Questions the Paper Calls Out
- Can TIMING be modified to satisfy the completeness axiom while preserving its temporal awareness and OOD mitigation properties? The paper acknowledges TIMING sacrifices completeness for flexibility.
- How can segment hyperparameters (n, smin, smax) be optimally adapted to different time series domains without manual tuning? The paper demonstrates robustness but provides no principled selection method.
- Can CPD/CPP metrics be extended to provide point-wise directional feedback without the cumulative ordering constraint? The paper notes the constraint may obscure nuanced feature interactions.

## Limitations
- Novel cumulative metrics lack extensive ablation studies on synthetic data to validate sensitivity to different attribution patterns
- Segment masking approach lacks theoretical guarantees for specific temporal dependency structures
- Computational efficiency claim relies on single-sample approximation without variance analysis

## Confidence
- High confidence: TIMING improves over standard IG on the proposed CPD/CPP metrics for real-world datasets
- Medium confidence: The cumulative metrics meaningfully evaluate directional attribution capabilities; segment masking effectively addresses OOD issues
- Low confidence: The single-sample approximation maintains accuracy across all scenarios; theoretical completeness trade-off is well-characterized

## Next Checks
1. Generate synthetic datasets with known directional attribution patterns and verify CPD/CPP correctly rank methods by their ability to identify these patterns without sign cancellation
2. Systematically vary segment sizes (smin, smax) on synthetic temporal dependency benchmarks to quantify TIMING's ability to capture different temporal scales
3. Compare single-sample vs. multi-sample TIMING implementations on validation set to measure variance and confirm computational efficiency claims don't compromise attribution quality