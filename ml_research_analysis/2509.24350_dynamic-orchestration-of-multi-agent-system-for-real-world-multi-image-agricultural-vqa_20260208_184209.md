---
ver: rpa2
title: Dynamic Orchestration of Multi-Agent System for Real-World Multi-Image Agricultural
  VQA
arxiv_id: '2509.24350'
source_url: https://arxiv.org/abs/2509.24350
tags:
- agricultural
- arxiv
- question
- reasoning
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a self-reflective and self-improving multi-agent
  framework for agricultural visual question answering. The framework integrates four
  specialized roles: a Retriever for context enrichment, a Reflector for self-reflection
  on evidence adequacy, Answerers for parallel response drafting, and an Improver
  for iterative refinement.'
---

# Dynamic Orchestration of Multi-Agent System for Real-World Multi-Image Agricultural VQA

## Quick Facts
- arXiv ID: 2509.24350
- Source URL: https://arxiv.org/abs/2509.24350
- Reference count: 38
- Primary result: Achieves 90.78% overall accuracy on AgMMU benchmark, outperforming existing models particularly in multi-image settings

## Executive Summary
This paper introduces a self-reflective and self-improving multi-agent framework for agricultural visual question answering that handles real-world multi-image scenarios. The framework integrates four specialized roles—Retriever, Reflector, Answerers (×2), and Improver—that collaborate through iterative loops to enhance answer quality. Experiments on the AgMMU benchmark demonstrate competitive performance across five question categories with particular strength in multi-image inputs.

## Method Summary
The framework employs four specialized agents: a Retriever that formulates queries and gathers external agricultural information, a Reflector that assesses evidence adequacy and triggers retrieval refinement loops, two parallel Answerers that draft and cross-check candidate responses, and an Improver that evaluates and iteratively refines answers. The system handles multi-image inputs (1-10 images) through explicit cross-image grounding evaluation, addressing the common limitation of single-image fixation in existing multimodal models. Agents communicate through structured prompts and quality thresholds to ensure evidence meets domain-specific criteria before generating final responses.

## Key Results
- Achieves 90.78% overall accuracy on AgMMU benchmark, outperforming proprietary and open-source baselines
- Demonstrates stable performance across multi-image settings: 91.67% (1 image), 90.07% (2 images), 90.79% (3 images), 95.00% (≥4 images)
- Shows particular strength in challenging categories like disease/pest/species identification and management instruction

## Why This Works (Mechanism)

### Mechanism 1: Self-Reflective Retrieval Loop
Iterative query reformulation triggered by adequacy assessment improves evidence quality for agricultural VQA. The Reflector evaluates retrieved evidence against weighted criteria (topical relevance, factual consistency, timeliness, alignment with crop type/growth stage) and triggers sequential reformulation and renewed retrieval when quality falls below threshold. Core assumption: Agricultural queries require context-sensitive, domain-specific evidence that single-pass retrieval cannot reliably provide.

### Mechanism 2: Parallel Answer Drafting with Cross-Evaluation
Two Answerers operating independently and cross-checking outputs reduces individual bias and captures complementary reasoning paths. Both Answerers receive identical query and validated evidence, generate candidate answers independently, then cross-evaluate each other's outputs. Disagreements trigger reconciliation through a reconsideration function. Core assumption: Agricultural questions often admit multiple plausible interpretations, and independent reasoning paths help surface inconsistencies and gaps.

### Mechanism 3: Multi-Image Alignment Through Iterative Improvement
Explicit evaluation of cross-image grounding prevents over-reliance on single images and improves answer completeness. The Improver evaluates draft answers across dimensions including visual evidence grounding, completeness, and consistency, specifically checking whether responses integrate evidence from all images. Core assumption: Existing multimodal models tend to fixate on single images and neglect complementary information from others in multi-image settings.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Grounding LMM outputs in external agricultural context (weather, policy, literature) reduces hallucination risk. Quick check: Can you explain why grounding LMM outputs in retrieved documents reduces factual errors compared to parametric knowledge alone?

- **Multi-Agent Orchestration**: Coordinating four specialized agents with distinct responsibilities through role separation and message passing. Quick check: What is the difference between a pipeline architecture and an iterative orchestration architecture with feedback loops?

- **Multi-Image Visual Reasoning**: Integrating complementary evidence across spatial scales and growth stages requires cross-image alignment beyond single-image reasoning. Quick check: Why might a model that performs well on single-image VQA struggle with multi-image inputs that require cross-image alignment?

## Architecture Onboarding

- **Component map**: Query → Retriever → Reflector (with potential loop-back) → Answerers (parallel) → Reconciliation → Improver (with potential loop-back to Answerers) → Final output
- **Critical path**: Query flows through Retriever to gather evidence, Reflector evaluates adequacy with threshold gating, two Answerers generate parallel drafts with cross-evaluation, Improver performs final quality check with possible revision loops
- **Design tradeoffs**: Iterative loops improve quality but increase latency and API costs; parallel Answerers reduce bias but duplicate computation; threshold-based gating adds tunable parameters requiring domain-specific calibration
- **Failure signatures**: Retriever loop exhaustion (insufficient external sources for niche queries), Reflector over-rejection (threshold set too high), Answerer agreement on wrong answer (shared systematic bias), Improver loop (persistent quality failures)
- **First 3 experiments**: 1) Single-agent baseline: Run same LMM without multi-agent orchestration on AgMMU; 2) Ablation by role: Remove each agent role sequentially and measure accuracy degradation; 3) Threshold sensitivity: Vary Reflector and Improver quality thresholds to map accuracy-latency tradeoff

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the framework maintain its performance advantage when applied to open-ended generation tasks rather than multiple-choice selection? The evaluation methodology is explicitly restricted to multiple-choice accuracy, whereas the introduction emphasizes providing knowledge and recommendations.

- **Open Question 2**: What is the computational overhead and latency impact of the iterative reflection and improvement loops compared to single-pass baselines? The framework relies on sequential calls between agents and conditional loops, but no latency or cost analysis is provided.

- **Open Question 3**: How sensitive is the Reflector's effectiveness to the specific predefined quality thresholds and weighted criteria? The paper does not analyze if these static parameters generalize well across varying query complexities or noise levels in retrieval.

## Limitations
- Does not specify the base LMM powering the agents, making it unclear whether performance gains come from architecture or model capacity
- Lacks analysis of computational overhead and latency introduced by iterative reflection and improvement loops
- Quality thresholds and weighted criteria for Reflector and Improver are not disclosed, preventing exact reproduction

## Confidence
- **High confidence**: Framework architecture (four specialized roles with defined responsibilities) is clearly specified and implementable
- **Medium confidence**: Performance claims on AgMMU are verifiable given the benchmark dataset, though exact reproduction depends on unknown hyperparameters
- **Low confidence**: Claims about bias reduction through parallel Answerers and effectiveness of self-reflective retrieval loops lack direct empirical validation

## Next Checks
1. **Ablation by role**: Remove each agent role sequentially (Retriever → Reflector → Improver) and measure accuracy degradation to identify which components contribute most to performance gains
2. **Iteration analysis**: Profile the number of retrieval and improvement iterations per query to quantify computational cost of self-reflective loops and assess whether performance gains justify overhead
3. **Multi-image grounding validation**: For multi-image questions, verify that generated answers explicitly cite evidence from all input images rather than defaulting to the first image, using automated citation extraction or human annotation