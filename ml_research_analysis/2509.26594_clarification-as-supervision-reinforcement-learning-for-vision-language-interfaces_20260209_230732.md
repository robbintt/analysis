---
ver: rpa2
title: 'Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces'
arxiv_id: '2509.26594'
source_url: https://arxiv.org/abs/2509.26594
tags:
- clarification
- reasoner
- ac-rl
- reasoning
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adaptive-Clarification Reinforcement Learning (AC-RL) aligns vision-language
  models with text-only reasoners through interaction. By using tiered rewards and
  clarification requests as implicit feedback, AC-RL teaches captioners what visual
  details reasoners need without explicit supervision.
---

# Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces

## Quick Facts
- **arXiv ID:** 2509.26594
- **Source URL:** https://arxiv.org/abs/2509.26594
- **Reference count:** 40
- **Primary result:** AC-RL improves vision-language model accuracy by 4.4 points on mathematical reasoning tasks through clarification-aware training

## Executive Summary
This paper introduces Adaptive-Clarification Reinforcement Learning (AC-RL), a method that aligns vision-language models with text-only reasoners through interaction. By using tiered rewards and clarification requests as implicit feedback, AC-RL teaches captioners what visual details reasoners need without explicit supervision. The approach enables models to front-load reasoner-relevant information into initial captions, reducing reliance on follow-up questions. Experiments across seven mathematical reasoning benchmarks show AC-RL improves average accuracy by 4.4 points over pretrained baselines, with gains particularly strong in quantitatively-intensive domains like geometry and algebra.

## Method Summary
AC-RL employs a reinforcement learning framework where a trainable vision-language captioner interacts with a frozen text-only reasoner. The captioner generates descriptions of images paired with questions, and the reasoner evaluates whether these descriptions contain sufficient information to answer the question. If clarification is needed, a frozen reference policy provides additional details. The trainable captioner receives tiered rewards: 1.0 for correct answers without clarification, 0.7 for correct answers requiring clarification, and 0 for incorrect answers. This reward structure creates optimization pressure to front-load relevant visual details into initial captions. The method uses KL-regularized Beta-Normalization Policy Optimization to balance task performance against divergence from pretrained initialization.

## Key Results
- AC-RL improves average accuracy by 4.4 points over pretrained baselines across seven mathematical reasoning benchmarks
- Clarification request rates drop by 29-39% (from 40.69% to 28.95% on MathVision, 49.57% to 30.28% on MathVerse)
- Gains are particularly strong in quantitatively-intensive domains like geometry and algebra
- Method maintains consistent performance across difficulty levels
- Single-pass accuracy improves without sacrificing the ability to handle complex problems requiring clarification

## Why This Works (Mechanism)

### Mechanism 1: Tiered Reward Densification
Converting binary task success into three-tier rewards (1.0/0.7/0) densifies the learning signal for caption generation. Episodes that would receive zero reward under binary schemes (nearly-sufficient captions missing one detail) instead receive partial credit (0.7). The penalty (1-0.7)=0.3 creates optimization pressure toward self-sufficient initial captions while still providing gradient signal during early exploration.

### Mechanism 2: Frozen Reference Policy for Gradient Isolation
Using a frozen checkpoint for clarification responses forces all learning pressure onto the initial caption. During training, if the reasoner requests clarification, a frozen π_ref generates the response. No gradients flow through the clarification response or the clarification decision. The captioner can only improve by front-loading information into the initial caption, not by improving multi-turn coordination.

### Mechanism 3: Information Gap Revelation via Clarification Requests
Clarification requests during training serve as implicit supervision revealing what visual details the reasoner needs. The reasoner's clarification decision exposes systematic information gaps in the captioner's output. Over thousands of interactions, the captioner learns which visual features (quantitative values, spatial relationships, structural patterns) are frequently requested and preemptively includes them.

## Foundational Learning

- **Concept: Policy Gradient with KL Regularization**
  - Why needed here: AC-RL optimizes a KL-regularized objective balancing task performance against divergence from pretrained initialization. Understanding why KL penalties prevent catastrophic forgetting and mode collapse is essential.
  - Quick check question: What happens to generation diversity if β (KL weight) is set too high or too low?

- **Concept: Decoupled Vision-Language Architecture**
  - Why needed here: The paper assumes a modular setup where a trainable captioner produces text descriptions consumed by a frozen reasoner. The interface bottleneck is the core problem AC-RL addresses.
  - Quick check question: Why might a caption optimized for human readability fail to serve a text-only reasoner?

- **Concept: Reward Shaping and Credit Assignment**
  - Why needed here: The tiered reward (1/0.7/0) is a form of reward shaping that provides intermediate credit assignment. Understanding how this differs from sparse binary rewards—and why it maintains unbiased policy gradients—is critical.
  - Quick check question: Why does the proof in Appendix B require that post-action randomness p(ξ|τ) be θ-independent?

## Architecture Onboarding

- **Component map:** Trainable Captioner (V_θ) -> Frozen Reasoner (R) -> Frozen Reference Policy (π_ref) -> Reward Module -> BNPO Optimizer

- **Critical path:**
  1. Sample (image, question) from training dataset
  2. Captioner generates c₀ ~ π_θ(· | I, Q)
  3. Reasoner evaluates sufficiency → decides SOLVED or NEED MORE INFO
  4. If NEED MORE INFO: frozen π_ref provides c₁; reasoner produces answer from (c₀, q₁, c₁); reward = 0.7 if correct, 0 otherwise
  5. If SOLVED: reasoner produces answer from c₀; reward = 1 if correct, 0 otherwise
  6. Compute BNPO advantage; update π_θ via clipped surrogate with KL penalty
  7. At inference: single-pass only (c₀ → answer), no clarification allowed

- **Design tradeoffs:**
  - Captioner size vs. training efficiency: 2B-3B models chosen for tractable RL; larger captioners may improve accuracy but require more compute
  - α = 0.7 penalty: Higher α = less front-loading pressure but more exploration credit; lower α = stronger pressure but weaker gradient signal early
  - LoRA rank (128-256): Balances adaptation capacity vs. regularization; full fine-tuning may overfit
  - Group size M = 8: BNPO uses 8 samples per batch for advantage estimation; larger groups stabilize but increase compute

- **Failure signatures:**
  - High clarification rate at inference (>40%): Captioner failed to learn front-loading; check α value, training duration, reference policy quality
  - Low diversity (uniform-reward batches >50% in Appendix C): Mode collapse; increase KL penalty β or reduce learning rate
  - Accuracy degradation on specific subjects (e.g., WeMath -1.5 points): Overfitting to training distribution; consider curriculum or data augmentation
  - Negative clarification gap (Appendix C.4): Clarification responses introducing noise; review π_ref prompt design

- **First 3 experiments:**
  1. **Baseline sanity check**: Run Pretrained + Reasoner configuration on MathVista testmini. Expected: ~60% accuracy. If significantly lower, verify reasoner API and prompt formatting.
  2. **Reward ablation**: Compare Binary-Reward RL vs. AC-RL on 1K samples. Measure clarification rate reduction and accuracy delta. If <1 point difference, α may need tuning or training steps insufficient.
  3. **Clarification pattern analysis**: Run clarification-enabled evaluation on 500 held-out samples. Log which visual features trigger clarification requests. Expected: quantitative details (angles, lengths, coordinates) dominate. If requests are random or trivial, review reasoner decision prompt in Appendix D.2.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does extending the framework to multi-turn clarification with decaying rewards further reduce information gaps or improve accuracy?
  - Basis in paper: Conclusion states future work could explore "multi-turn clarification with decaying rewards."
  - Why unresolved: The current implementation restricts interaction to a single clarification request to enforce single-pass inference capability.
  - Evidence: Comparing AC-RL against a variant allowing multiple interaction turns, measuring the trade-off between final accuracy and inference token cost.

- **Open Question 2:** Can the framework be adapted for bidirectional training where both the captioner and reasoner co-evolve their interface strategies?
  - Basis in paper: Conclusion proposes investigating "bidirectional adaptation where both modules co-evolve."
  - Why unresolved: The current method freezes the reasoner to ensure the captioner unilaterally adapts to a fixed downstream tool.
  - Evidence: Training both models simultaneously via RL to observe if the reasoner learns to ask more targeted questions and the captioner learns to answer them more efficiently.

- **Open Question 3:** Is the fixed partial reward penalty (α=0.7) optimal, or does the ideal value depend on the specific reasoner's capability or task domain?
  - Basis in paper: Section 3.3 selects α=0.7 heuristically without providing an ablation study on how this hyperparameter affects the balance between exploration and single-pass pressure.
  - Why unresolved: It is unclear if a different penalty would better balance the densification of rewards against the pressure to avoid clarification requests.
  - Evidence: An ablation study varying α across different benchmarks to observe changes in clarification rates and final task accuracy.

## Limitations

- The method assumes a specific reasoner behavior pattern (requesting clarifications for missing visual details), which may not transfer to different reasoner architectures or reasoning styles
- The optimal α value of 0.7 is selected heuristically without systematic ablation studies across different mathematical domains
- The 2B-3B captioner size represents a tradeoff between accuracy and computational efficiency, potentially limiting performance gains from larger models

## Confidence

- **High Confidence:** Tiered rewards densify learning signal and reduce clarification requests; Frozen reference policy effectively isolates gradient updates to initial caption; AC-RL improves accuracy over pretrained baselines across seven benchmarks
- **Medium Confidence:** 4.4-point average accuracy improvement is consistent across all difficulty levels; Clarification request reduction (29-39%) translates directly to better single-pass performance; KL regularization prevents catastrophic forgetting without overly constraining adaptation
- **Low Confidence:** Optimal α value of 0.7 is universally appropriate across all mathematical domains; 2B-3B captioner size represents the ideal tradeoff between accuracy and efficiency; Method generalizes to non-mathematical visual reasoning tasks without modification

## Next Checks

1. **Reference Policy Sensitivity Test**: Train AC-RL with three different frozen reference qualities (untrained, supervised, and intermediate checkpoint) and measure impact on clarification reduction and accuracy. This validates whether the reference quality affects the learning signal integrity.

2. **α Value Sensitivity Analysis**: Systematically vary α from 0.5 to 0.9 in 0.1 increments and measure the tradeoff between clarification reduction and accuracy. This determines if the chosen α=0.7 represents the optimal balance across all tested domains.

3. **Cross-Reasoner Generalization**: Apply AC-RL to a different frozen reasoner (e.g., Qwen2.5-32B-Chat) and evaluate whether the learned front-loading strategies transfer or require retraining. This tests the method's dependence on reasoner-specific clarification patterns.