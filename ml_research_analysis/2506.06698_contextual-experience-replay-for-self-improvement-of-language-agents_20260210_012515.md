---
ver: rpa2
title: Contextual Experience Replay for Self-Improvement of Language Agents
arxiv_id: '2506.06698'
source_url: https://arxiv.org/abs/2506.06698
tags:
- page
- agent
- agents
- tasks
- experiences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Contextual Experience Replay (CER), a training-free
  framework that enables language agents to improve through self-learning during inference.
  CER accumulates past experiences into a dynamic memory buffer, distilling environment
  dynamics and decision-making patterns from previous trajectories, then retrieving
  and replaying this knowledge in new tasks to enhance adaptability.
---

# Contextual Experience Replay for Self-Improvement of Language Agents

## Quick Facts
- arXiv ID: 2506.06698
- Source URL: https://arxiv.org/abs/2506.06698
- Reference count: 40
- Key outcome: Training-free framework achieving 36.7% success rate on WebArena (51.0% relative improvement over GPT-4o baseline) and 31.9% on VisualWebArena through self-learning during inference.

## Executive Summary
This paper introduces Contextual Experience Replay (CER), a training-free framework that enables language agents to improve through self-learning during inference. CER accumulates past experiences into a dynamic memory buffer, distilling environment dynamics and decision-making patterns from previous trajectories, then retrieving and replaying this knowledge in new tasks to enhance adaptability. Evaluated on WebArena and VisualWebArena benchmarks, CER achieves competitive performance and demonstrates strong stability and plasticity as a self-improvement system.

## Method Summary
CER is a four-module framework that enables self-improvement without weight updates. It distills raw trajectories into structured experiences (environment dynamics with page summaries and URLs, plus skills with step-by-step guidelines and action examples), stores them in a dynamic buffer, retrieves top-k relevant experiences for new tasks, and injects them into the agent's context window to influence decision-making. The framework supports offline (human-annotated trajectories), online (self-generated), and hybrid learning modes, using GPT-4o with temperature 0.1 and BrowserGym environment.

## Key Results
- 36.7% success rate on WebArena (51.0% relative improvement over GPT-4o baseline)
- 31.9% success rate on VisualWebArena
- Demonstrates strong stability and plasticity across offline, online, and hybrid learning settings
- Compatible with other methods like trajectory sampling and reranking
- Comprehensive analysis validates efficiency and effectiveness across different learning modes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distilling trajectories into abstracted dynamics and skills reduces state-action space exploration.
- Mechanism: The distillation module summarizes raw trajectories into environment dynamics (page descriptions, URLs, usages) and skills (step-by-step guidelines with variables). Retrieval selects top-k relevant experiences to inject into context, highlighting promising states and actions.
- Core assumption: Agents can generalize from abstracted, variable-based skill representations to new task instances.
- Evidence anchors:
  - [abstract] CER accumulates and synthesizes past experiences into a dynamic memory buffer encompassing environment dynamics and decision-making patterns.
  - [section 3.1] Dynamics distillation produces page summaries with URLs; skill distillation outputs abstract skill names and step-by-step guidelines with both natural language summaries and concrete action examples.
  - [corpus] Neighboring work "Scaling Agent Learning via Experience Synthesis" similarly emphasizes experience synthesis for scalability, supporting the high-level approach but not CER-specific mechanisms.
- Break condition: When trajectories are non-goal-oriented or highly unstructured, distillation yields noisy, misleading experiences that degrade performance (see Limitations and Appendix A.5).

### Mechanism 2
- Claim: Experience replay in context leverages in-context learning without weight updates.
- Mechanism: Selected experiences are programmatically transformed into natural language descriptions and merged into the model's context window. This augmented context biases the decision-making policy toward actions informed by past patterns, enabling self-improvement at inference time.
- Core assumption: The base LLM has sufficient in-context learning capacity to follow retrieved experiences and the context window can accommodate them.
- Evidence anchors:
  - [abstract] A training-free framework to enable efficient self-improvement for language agents in their context window.
  - [section 3.3] Experiences are transformed into natural language and integrated into context C, yielding augmented context C' influencing the policy.
  - [corpus] Weak direct corpus evidence for the specific context-augmentation mechanism; related works focus more on memory storage than in-context replay.
- Break condition: When context window overflow occurs or retrieved experiences are irrelevant/noisy, the agent may ignore or be misled by them.

### Mechanism 3
- Claim: Separating dynamics and skills provides complementary heuristics for state navigation and action selection.
- Mechanism: Dynamics inform state-aware decisions and direct URL navigation; skills inspire action patterns. The ablation study shows both components contribute, with dynamics slightly more impactful in web environments where URL navigation is useful.
- Core assumption: Web environments have structured, URL-addressable states where dynamics summaries are actionable.
- Evidence anchors:
  - [section 5.7] Ablation shows CER outperforms CER-skills and CER-dynamics; both dynamics and skills synergize.
  - [section 3.1] Dynamics provide page summaries and URLs; skills provide step-by-step guidelines.
  - [corpus] No direct corpus ablation comparisons; mechanism is CER-specific.
- Break condition: In non-web environments without URL-addressable states, dynamics utility may diminish.

## Foundational Learning

- Concept: Experience replay (reinforcement learning)
  - Why needed here: CER is inspired by RL experience replay; understanding buffer storage and sampling helps grasp CER's memory and retrieval design.
  - Quick check question: How does prioritized experience replay differ from uniform sampling?

- Concept: In-context learning in LLMs
  - Why needed here: CER relies on the model's ability to follow retrieved experiences in context without fine-tuning.
  - Quick check question: What are the limitations of in-context learning regarding context length and task complexity?

- Concept: ReAct (reasoning + acting)
  - Why needed here: The base agent and distillation modules use ReAct-style think-action formats for decision-making and distillation.
  - Quick check question: How does interleaving reasoning with actions improve task decomposition?

## Architecture Onboarding

- Component map: Distillation Module (DynamicsDistiller, SkillsDistiller) -> Dynamic Experience Buffer (Îµ) -> Retrieval Module (DynamicsRetriever, SkillsRetriever) -> Base Agent (ReAct-style)

- Critical path:
  1. Trajectory collection (online from agent, offline from human/LLM explorer)
  2. Distillation per trajectory into dynamics and skills, merging into buffer while avoiding duplicates
  3. For each new task, retrieve top-k dynamics and skills
  4. Transform selected experiences into natural language and inject into agent context
  5. Agent executes with augmented context; resulting trajectory feeds back to step 2 in online/hybrid modes

- Design tradeoffs:
  - Offline vs. online: Offline provides immediate experience but requires annotation; online enables self-evolution but cold-starts with no experience. Hybrid balances both.
  - Retrieval top-k (kd, ks): Higher k increases context load and potential noise; lower k may miss relevant experiences. Default kd=5, ks=5.
  - Distillation abstraction level: Too specific risks overfitting to trajectories; too abstract loses actionability.

- Failure signatures:
  - Low-quality trajectories (random exploration) produce noisy distillations, degrading performance (Appendix A.5)
  - Irrelevant retrieval misleads agent, especially when buffer contains many unrelated experiences
  - Context window overflow when too many experiences are retrieved or trajectories are long

- First 3 experiments:
  1. Reproduce WebArena offline setting with human-annotated trajectories; measure success rate and token cost vs. baseline
  2. Ablate dynamics vs. skills retrieval on Forum split; compare cross-template success rates to assess generalization
  3. Run online setting starting from empty buffer; plot success rate over task sequence to evaluate stability and plasticity

## Open Questions the Paper Calls Out

- Question: How can low-quality, non-goal-oriented trajectories be effectively utilized or filtered for offline learning?
  - Basis in paper: [explicit] Limitations section states "The more fine-grained utilization of low-quality trajectories could be explored in the future." Table 9 shows self-guided exploration trajectories underperform human-annotated ones in hybrid settings.
  - Why unresolved: CER currently requires goal-oriented trajectories; random explorations produce noisy experiences that can mislead agents.
  - What evidence would resolve it: A trajectory quality scoring or filtering mechanism that enables random exploration data to match human-annotated performance.

- Question: How can environment dynamics representations be adapted for non-web agent tasks like embodied navigation?
  - Basis in paper: [explicit] "It would be interesting to investigate how we can utilize the environment dynamics in other agent tasks, such as real-world navigation (Shridhar et al., 2021) in future work."
  - Why unresolved: Current dynamics leverage URLs for direct navigation, which has no equivalent in physical environments.
  - What evidence would resolve it: CER adaptations achieving competitive performance on embodied AI benchmarks (e.g., ALFWorld).

- Question: What automated methods can generate high-quality training trajectories without human annotation?
  - Basis in paper: [explicit] "How to generate such trajectories in a more efficient way would be an interesting topic to dive into in the future."
  - Why unresolved: Hybrid setting depends on human-annotated trajectories for optimal performance, limiting scalability.
  - What evidence would resolve it: An automated trajectory generation approach matching human-annotated hybrid performance.

- Question: Can CER be modified to improve effectiveness on weaker open-source models?
  - Basis in paper: [inferred] Section 5.4 shows smaller relative improvement (26.53%) with Llama-3.1-70B versus GPT-4o (51.0%), attributed to formatting robustness and distillation quality issues.
  - Why unresolved: Only one weaker model was tested; no interventions were explored to address the capability gap.
  - What evidence would resolve it: Systematic evaluation across model scales with targeted improvements (e.g., constrained output formats) narrowing the performance gap.

## Limitations

- The framework's performance degrades with low-quality or random exploration trajectories, and there's no mechanism to filter or prioritize high-quality experiences
- The generalizability across diverse web environments beyond the tested benchmarks remains uncertain
- The context window limitations of current LLMs may constrain the scalability of experience replay as buffer size grows

## Confidence

- **High Confidence**: The mechanism of distilling trajectories into structured dynamics and skills, and the basic retrieval-augmented inference framework are well-supported by the ablation study and experimental results
- **Medium Confidence**: The claimed improvements (51.0% relative improvement on WebArena, 21.8% on VisualWebArena) are specific to the tested benchmarks and may not generalize to all web navigation tasks or other domains
- **Low Confidence**: The assertion that CER is "training-free" and the long-term stability of the framework in continuous online settings without catastrophic forgetting are not thoroughly validated

## Next Checks

1. **Cross-Benchmark Generalization**: Evaluate CER on a different web navigation benchmark (e.g., Mind2Web) to assess whether the observed improvements transfer to new environments and task distributions

2. **Experience Quality Impact**: Systematically vary the quality of trajectories in the buffer (e.g., using only expert trajectories vs. random exploration) and measure the corresponding impact on agent performance to quantify the importance of experience curation

3. **Long-Term Online Stability**: Run CER in continuous online mode for 100+ tasks, tracking both success rate and buffer diversity over time, to identify potential catastrophic forgetting or performance degradation from accumulated noise