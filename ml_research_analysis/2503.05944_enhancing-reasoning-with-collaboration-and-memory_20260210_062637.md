---
ver: rpa2
title: Enhancing Reasoning with Collaboration and Memory
arxiv_id: '2503.05944'
source_url: https://arxiv.org/abs/2503.05944
tags:
- memory
- reasoning
- ncot
- agents
- exemplars
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a multi-agent collaborative system enhanced
  by memory for reasoning tasks, combining chain-of-thought prompting, diverse context
  agents, and memory retrieval mechanisms. The system features varied-context agents
  that each use different exemplars from a memory bank, offering diverse perspectives
  beyond standard temperature sampling.
---

# Enhancing Reasoning with Collaboration and Memory

## Quick Facts
- arXiv ID: 2503.05944
- Source URL: https://arxiv.org/abs/2503.05944
- Reference count: 28
- Multi-agent system with memory improves reasoning task accuracy through exemplar diversity and collaboration.

## Executive Summary
This work introduces a multi-agent collaborative system enhanced by memory for reasoning tasks, combining chain-of-thought prompting, diverse context agents, and memory retrieval mechanisms. The system features varied-context agents that each use different exemplars from a memory bank, offering diverse perspectives beyond standard temperature sampling. Experiments on three reasoning tasks using Gemini 1.0 Pro and Ultra models show that varied-context agents perform comparably to self-consistency, with random exemplar retrieval often outperforming similarity-based retrieval. Analogical prompting proves more robust than chain-of-thought when using memory. Frozen memory performs similarly to learned memory while being more efficient.

## Method Summary
The system uses a memory bank (frozen or learned) to store exemplars from correct reasoning chains. A retrieval mechanism (fixed, random, or similarity-based) selects exemplars for reasoning agents. Agents can be single greedy, self-consistency (10 agents, shared exemplars), or varied-context (10 agents, independently sampled exemplars). A summarizer agent or majority voting aggregates responses. Experiments use Gemini 1.0 Pro/Ultra on FOLIO, RACO, and TSO tasks with accuracy as the metric.

## Key Results
- Varied-context agents match or exceed self-consistency performance on most model-task pairs
- Random exemplar retrieval often outperforms similarity-based retrieval due to diversity benefits
- Frozen memory provides comparable performance to learned memory while being more efficient to build
- Analogical prompting is more robust than chain-of-thought when using memory retrieval

## Why This Works (Mechanism)

### Mechanism 1: Random Retrieval for Memory Diversity
Random sampling increases exemplar diversity, avoiding repetitive or overly similar contexts that may mislead the model. Similarity-based retrieval can cluster exemplars that are too alike, reducing the breadth of reasoning patterns shown to the model.

### Mechanism 2: Varied-Context Agents for Perspective Diversity
Each agent receives a unique sample of exemplars, creating diverse "perspectives" or reasoning approaches. Aggregating answers via voting or a summarizer leverages this diversity, similar to ensemble methods.

### Mechanism 3: Frozen Memory as an Efficient Baseline
A one-time pass over the training set with a zero-shot CoT agent collects correct reasoning chains. This static bank can be reused, avoiding the computational cost of incrementally updating memory during training.

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning**: All agents use CoT variants to generate reasoning steps before answers. Quick check: What is the core difference between zero-shot CoT and few-shot CoT prompting?
- **Self-Consistency (SC)**: Involves sampling multiple outputs with temperature > 0 and selecting the most common answer by voting. Quick check: In self-consistency, how is the final answer determined from multiple agent outputs?
- **In-Context Learning (ICL)**: The memory bank's primary function is to supply exemplars for in-context learning. Quick check: How does providing exemplars in the context enable a model to adapt to a new task?

## Architecture Onboarding

- **Component map**: Problem -> Retrieval Mechanism -> Reasoning Agents -> Aggregation Module -> Final Answer
- **Critical path**: 1) Problem is received. 2) Retrieval mechanism queries the Memory Bank to obtain exemplars. 3) Exemplars are distributed to Reasoning Agents based on their type (shared or varied). 4) Each agent generates a chain-of-thought and answer. 5) Aggregation Module (voting or summarizer) combines agent outputs into the final answer.
- **Design tradeoffs**:
  - Memory: Frozen vs. Learned — Frozen is cheaper to build; Learned adapts online but is computationally expensive
  - Retrieval: Random vs. Similarity — Random promotes diversity; Similarity focuses relevance but risks redundancy
  - Agents: Identical (SC) vs. Varied-Context — Identical is simpler; Varied-Context may improve diversity but requires a larger memory pool
  - Aggregation: Voting vs. Summarizer — Voting is cheap and robust; Summarizer is more expensive and can help weaker models but adds complexity
- **Failure signatures**:
  - Accuracy collapse to near zero: Often indicates poor exemplar quality or a memory bank that failed to accumulate useful examples
  - Learned memory underperforming frozen memory: Suggests the online learning process is not adding valuable exemplars or is introducing noise
  - Summarizer agent degrading performance vs. voting: Likely occurs with already-strong reasoning agents where the summarizer introduces unnecessary complexity
- **First 3 experiments**:
  1. Establish a baseline by comparing Zero-Shot CoT vs. Few-Shot CoT with a small fixed set of exemplars on your target reasoning task
  2. Implement a Frozen Memory Bank with Random Retrieval and compare single-agent performance against the Zero-Shot baseline
  3. Compare Self-Consistency (10 agents, shared exemplars) against Varied-Context Agents (10 agents, randomly sampled exemplars) using the Frozen Memory Bank

## Open Questions the Paper Calls Out

- Does the finding that random retrieval outperforms similarity-based retrieval generalize to reasoning domains beyond the three grounded logic tasks tested?
- Why does analogical prompting outperform chain-of-thought on FOLIO and RACO but underperform on Tracking Shuffled Objects?
- Under what conditions do in-context exemplars distract rather than assist reasoning, and can this be predicted a priori?

## Limitations

- Claims rely on comparisons between multi-agent configurations that are not always fully isolated
- Specific hyperparameters like k for similarity-based retrieval and the size of fixed exemplar sets are not specified
- Evaluation is limited to accuracy on three reasoning tasks without ablation on memory size or agent count
- Learned memory's inferior performance on certain task-model pairs is noted but not deeply analyzed

## Confidence

- **High Confidence**: Varied-context agents match self-consistency performance; frozen memory is viable and efficient
- **Medium Confidence**: Random retrieval often outperforms similarity-based retrieval, though underlying cause is speculative
- **Low Confidence**: Summarizer agent most beneficial when reasoning agents are weak, based on limited evidence

## Next Checks

1. Systematically vary the k parameter for similarity-based retrieval and the number of exemplars in the fixed set to assess robustness
2. Quantify varied-context agent performance as memory bank size and agent count scale to identify diminishing returns
3. Test frozen and learned memory configurations on held-out problems from different distributions than training set to assess robustness