---
ver: rpa2
title: Neural expressiveness for beyond importance model compression
arxiv_id: '2512.06440'
source_url: https://arxiv.org/abs/2512.06440
tags:
- pruning
- expressiveness
- network
- nexp
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "Expressiveness" as a novel criterion for
  neural network pruning, which measures a neuron's ability to redistribute informational
  resources based on activation overlap, contrasting with traditional importance-based
  methods that rely on weight magnitude or gradients. The approach demonstrates that
  expressiveness can be effectively estimated using minimal or arbitrary data, enabling
  data-agnostic pruning strategies.
---

# Neural expressiveness for beyond importance model compression

## Quick Facts
- arXiv ID: 2512.06440
- Source URL: https://arxiv.org/abs/2512.06440
- Reference count: 40
- Key outcome: Introduces expressiveness pruning achieving up to 10x greater parameter compression than weight-based methods with only 1% average performance degradation

## Executive Summary
This paper introduces "Expressiveness" as a novel criterion for neural network pruning that measures a neuron's ability to redistribute informational resources based on activation overlap. Unlike traditional importance-based methods that rely on weight magnitude or gradients, expressiveness evaluates discriminative capacity through binarized activation patterns across diverse inputs. The approach demonstrates that expressiveness can be effectively estimated using minimal or arbitrary data, enabling data-agnostic pruning strategies. Experiments show expressiveness-based pruning achieves up to 10x greater parameter compression than weight-based methods with only 1% average performance degradation across CIFAR-10, ImageNet, and YOLOv8 on COCO.

## Method Summary
The method computes expressiveness (NEXP) scores by binarizing activation patterns from a mini-batch, computing pairwise Hamming distances between all sample pairs per filter, and averaging to form a normalized score. This score measures a filter's discriminative capacity through activation pattern overlap. The pruning process uses global ranking of filters by NEXP scores, iteratively removing bottom-ranked filters while fine-tuning. Notably, expressiveness can be approximated using minimal arbitrary data (60 random samples) with >99% similarity to full-dataset estimates. The method also shows promise for pruning at initialization, with NEXP maps at initialization showing 84-87% correlation with converged maps.

## Key Results
- Expressiveness-based pruning achieves up to 10x greater parameter compression than weight-based methods
- On YOLOv8 for object detection, expressiveness achieves 46.1% MACs reduction by removing 55.4% of parameters while improving mAP50-95 by 3%
- NEXP scores can be accurately approximated using minimal arbitrary data (60 samples) with >99% similarity to full-dataset estimates
- Expressiveness at initialization correlates with post-training expressiveness (84-87% cosine similarity), enabling effective pruning-at-initialization

## Why This Works (Mechanism)

### Mechanism 1
Neural expressiveness identifies prunable filters by measuring discriminative capacity via activation pattern overlap, enabling data-agnostic pruning. For each filter, binarize activations across a mini-batch, compute pairwise Hamming distances between all sample pairs, and average to form a normalized NEXP score. Filters producing similar activation patterns across diverse inputs are "less expressive" and ranked for removal first.

### Mechanism 2
Expressiveness can be accurately approximated using minimal arbitrary data (60 random samples), eliminating dependency on training data distribution. Random input samples generate activation patterns; their pairwise dissimilarities approximate the "true" NEXP computed over the full dataset. Cosine similarity between approximate and full-dataset NEXP exceeds 99%.

### Mechanism 3
Expressiveness at initialization correlates with post-training expressiveness, enabling pruning before training. NEXP maps computed at weight initialization show cosine similarity of 84-87% with converged maps for DenseNet-40 and VGG-19. Early layers show highest consistency, supporting "critical paths" formation.

## Foundational Learning

- **Structural vs. Non-Structural Pruning**: NEXP operates at filter/channel level (structural), removing entire filters rather than individual weights. This affects how compression translates to actual speedup and hardware efficiency. *Quick check: Can you explain why removing a filter (structural) vs. setting individual weights to zero (non-structural) has different implications for inference acceleration?*

- **Activation-Based vs. Weight-Based Importance**: The paper explicitly contrasts expressiveness (activation-phase) with importance (weight-phase). Understanding this distinction is essential for interpreting why NEXP can be data-agnostic and stateless. *Quick check: Why might weight magnitude fail to capture a neuron's actual contribution to downstream discrimination?*

- **Global vs. Local Pruning Scope**: The paper uses global pruning (ranking filters across all layers) rather than local (within-layer). This affects compression ratios and layer collapse risk. *Quick check: What is the risk of aggressive local pruning in early layers versus distributing removal globally?*

## Architecture Onboarding

- **Component map**: Forward pass hook -> Binarization module -> Pairwise distance calculator -> NEXP aggregator -> Pruning engine
- **Critical path**: 1) Select mini-batch (60 random samples or k-means representatives) 2) Forward pass to collect all activation maps 3) Binarize activations per filter 4) Compute pairwise Hamming distances per filter 5) Aggregate to NEXP scores; rank filters globally 6) Remove bottom-κ filters; update dependent structures 7) Iterate until target FLOPs reduction τ achieved
- **Design tradeoffs**: Batch size vs. accuracy (larger batches improve estimation but increase memory), one-shot vs. iterative (one-shot faster but iterative with fine-tuning yields better recovery), global vs. local pruning (global avoids layer imbalance but may over-prune critical early layers)
- **Failure signatures**: Flat NEXP scores (activation collapse, check for dead ReLUs), layer collapse (monitor per-layer filter counts), poor recovery after fine-tuning (may indicate over-pruning or insufficient fine-tuning epochs)
- **First 3 experiments**: 1) Baseline validation on CIFAR-10 ResNet-56: implement NEXP with 60 random samples, target 2× FLOPs reduction, compare accuracy and params reduction against L1-norm baseline 2) Data sensitivity ablation: run NEXP with batch sizes [16, 32, 64, 128] using random vs. k-means sampling; compute cosine similarity against "true" NEXP 3) PaI feasibility test: compute NEXP at initialization on untrained VGG-16/CIFAR-10, prune to 10× compression, train from scratch, compare accuracy vs. SNIP/SynFlow baselines

## Open Questions the Paper Calls Out

### Open Question 1
How can the "hybrid" solution space, combining expressiveness and importance criteria, be optimized efficiently rather than relying on linear grid searches? Current experiments use a linear combination of weights (alpha), which is computationally exhaustive and may not capture non-linear interactions between the two criteria. An algorithmic framework that automatically determines the optimal weighting strategy between NEXP and importance metrics would resolve this.

### Open Question 2
Is neural network training fundamentally about revealing pre-existing knowledge possessed at initialization rather than learning from scratch? While NEXP demonstrates effective pruning at initialization, the theoretical link between initial expressiveness and the final converged state remains a hypothesis requiring formal proof. A theoretical demonstration that critical paths identified by NEXP at initialization strictly dictate the information capacity of the converged model would resolve this.

### Open Question 3
Can the neural expressiveness criterion be effectively adapted for non-convolutional architectures, such as Transformers or Large Language Models? The method relies on spatial activation overlap (Hamming distance of feature maps); it is unclear how this metric translates to the attention mechanisms or token embeddings used in Transformers. Successful application to Vision Transformers (ViT) or NLP models would resolve this.

## Limitations
- Lacks formal theoretical proofs linking NEXP scores to information-theoretic measures of filter utility
- Data-agnostic claims may not generalize to domains with sparse activation patterns or highly structured inputs
- Limited theoretical justification for pruning-at-initialization mechanism, with relatively modest improvements over existing methods

## Confidence
- **High confidence** in experimental results and quantitative comparisons against baselines for standard pruning tasks
- **Medium confidence** in data-agnostic approximation claims, as they rely on specific network architectures and datasets
- **Low confidence** in pruning-at-initialization mechanism, given limited theoretical justification

## Next Checks
1. **Layer-wise sensitivity analysis**: Systematically vary pruning ratios across different network layers to identify structural vulnerabilities and optimal distribution strategies
2. **Cross-dataset generalization**: Apply NEXP-based pruning trained on one dataset (e.g., CIFAR-10) to architectures evaluated on different datasets (e.g., ImageNet) to test robustness to data distribution shifts
3. **Activation pattern analysis**: Visualize and analyze binarized activation distributions across filters to identify failure modes like saturation or collapse that could invalidate NEXP rankings