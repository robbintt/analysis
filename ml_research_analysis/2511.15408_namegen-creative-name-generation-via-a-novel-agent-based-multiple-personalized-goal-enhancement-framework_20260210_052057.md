---
ver: rpa2
title: 'NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized
  Goal Enhancement Framework'
arxiv_id: '2511.15408'
source_url: https://arxiv.org/abs/2511.15408
tags:
- generation
- language
- arxiv
- namegen
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-objective flexibility
  and interpretive complexity in creative natural language generation (CNLG), particularly
  in short-form text generation like Chinese baby naming. To tackle these issues,
  the authors propose NAMeGEn, a novel multi-agent optimization framework that iteratively
  alternates between objective extraction, name generation, and evaluation.
---

# NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework

## Quick Facts
- arXiv ID: 2511.15408
- Source URL: https://arxiv.org/abs/2511.15408
- Reference count: 40
- Primary result: A multi-agent optimization framework for short-form creative text generation that outperforms six baselines across six LLM backbones without requiring model-specific training

## Executive Summary
This paper introduces NAMeGEn, a novel multi-agent framework for creative natural language generation (CNLG) that addresses the challenge of balancing multiple personalized objectives in short-form text generation. The framework employs three specialized agents - Multi-Objective Manager (MOM), Multi-Objective Generator (MOG), and Multi-Objective Evaluator (MOE) - to iteratively extract objectives, generate creative names, and evaluate results against both user-specified and implicit interpretive objectives. The approach is particularly applied to Chinese baby naming but claims broader applicability to other short-form CNLG tasks.

## Method Summary
NAMeGEn operates through an iterative alternation process between three agents. The Multi-Objective Manager (MOM) extracts and refines both explicit user-specified objectives and implicit interpretive objectives from the task context. The Multi-Objective Generator (MOG) then generates candidate names based on these extracted objectives, while the Multi-Objective Evaluator (MOE) assesses the generated outputs against multiple criteria including creativity, interpretability, and compliance with objectives. This cycle continues until satisfactory results are achieved, with the framework designed to work across different LLM backbones without requiring task-specific training.

## Key Results
- Achieves superior performance with average EC (96.72), IC (92.70), and CC (94.71) scores across six different LLM backbones
- Outperforms six baseline methods in creative name generation tasks without requiring model-specific training
- Demonstrates strong multi-objective balancing capabilities while providing meaningful explanations for generated outputs

## Why This Works (Mechanism)
The framework's effectiveness stems from its specialized multi-agent architecture that decomposes the complex CNLG task into manageable subtasks. By separating objective extraction, generation, and evaluation into distinct agents, the system can focus computational resources on each specific aspect while maintaining coherence across the entire generation process. The iterative nature allows for refinement and improvement of outputs through multiple cycles, with each agent building upon the work of the others to progressively enhance both creativity and interpretability of the generated names.

## Foundational Learning

**Multi-Objective Optimization** - Why needed: Creative tasks require balancing multiple competing criteria (creativity, interpretability, user preferences). Quick check: Verify that the framework can handle conflicting objectives without degradation in overall performance.

**Agent-based Architecture** - Why needed: Decomposing complex tasks into specialized components improves focus and efficiency. Quick check: Confirm that each agent performs its designated function independently while contributing to the overall goal.

**Iterative Refinement** - Why needed: Single-pass generation often fails to achieve optimal results in creative tasks. Quick check: Measure improvement in output quality across successive iterations of the three-agent cycle.

## Architecture Onboarding

**Component Map:** MOM -> MOG -> MOE -> MOM (iterative cycle)

**Critical Path:** User input → MOM (objective extraction) → MOG (name generation) → MOE (evaluation) → MOM (objective refinement) → repeat until convergence

**Design Tradeoffs:** The framework trades computational efficiency for quality by using multiple agents and iterative refinement, but this allows it to handle complex multi-objective scenarios without task-specific training.

**Failure Signatures:** Poor objective extraction by MOM leads to irrelevant generations; inadequate evaluation by MOE results in non-creative outputs; failure to converge in the iterative cycle indicates conflicting or unachievable objectives.

**First 3 Experiments:** 1) Test individual agent performance in isolation, 2) Run ablation studies removing each agent sequentially, 3) Compare iterative vs. single-pass generation across different objective combinations

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation metrics (EC, IC, CC) lack clear definition and validation against established benchmarks
- Framework has only been tested on Chinese baby naming, limiting generalizability to other domains
- Computational costs and latency implications of the multi-agent system are not addressed

## Confidence

**Technical novelty and framework design:** Medium - The three-agent architecture is innovative but similar decomposition approaches exist in the literature.

**Performance superiority claims:** Medium - Strong empirical results but lack of comparison to more recent advanced baselines.

**Interpretability and explanation quality:** Low - Explanation quality depends on MOE without independent verification of interpretability.

**Cross-domain generalizability:** Low - Framework has only been demonstrated on a single task domain (Chinese baby naming).

## Next Checks

1. Conduct ablation studies removing each agent (MOM, MOG, MOE) to quantify their individual contributions to the overall performance

2. Validate EC, IC, CC metrics against human evaluations using established creativity and interpretability benchmarks

3. Test the framework on at least two additional short-form CNLG domains (e.g., product naming, character naming in fiction) to assess generalizability beyond Chinese baby naming