---
ver: rpa2
title: Leveraging Digitized Newspapers to Collect Summarization Data in Low-Resource
  Languages
arxiv_id: '2511.14598'
source_url: https://arxiv.org/abs/2511.14598
tags:
- teasers
- summarization
- data
- articles
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel method for creating summarization datasets
  using front-page teasers from digitized historical newspapers. The approach identifies
  teasers using newspaper-specific keywords and matches them to corresponding articles,
  supporting both single and multi-document summarization.
---

# Leveraging Digitized Newspapers to Collect Summarization Data in Low-Resource Languages

## Quick Facts
- **arXiv ID:** 2511.14598
- **Source URL:** https://arxiv.org/abs/2511.14598
- **Reference count:** 40
- **Primary result:** Creates summarization datasets from digitized newspapers by extracting front-page teasers and matching them to articles, achieving 86% teaser identification accuracy and 90% matching accuracy with zero-shot LLM approach.

## Executive Summary
This paper presents a novel method for creating summarization datasets using front-page teasers from digitized historical newspapers. The approach identifies teasers using newspaper-specific keywords and matches them to corresponding articles, supporting both single and multi-document summarization. Human evaluation by journalism experts found high quality summaries across different teaser lengths, with relevance improving as length increases. The method was applied to create HEBTEASESUM, a 7,774-sample Hebrew multi-document summarization dataset. When evaluating state-of-the-art LLMs across seven languages using this data, models showed similar performance gaps were larger in lower-resource languages, highlighting the need for quality datasets in these settings.

## Method Summary
The method employs a two-step pipeline: first, rule-based teaser identification using newspaper-specific phrases (e.g., "continued on p. 5") to detect potential summaries on front pages; second, semantic matching between identified teasers and articles on referenced pages using TF-IDF, Sentence Transformers, or zero-shot LLM classification. The approach leverages editorial standards, as front-page teasers are professionally written summaries designed to entice readers. The pipeline adapts to resource availability with different matching methods: low-resource TF-IDF, medium-resource Sentence Transformer, and high-resource zero-shot LLM. The dataset HEBTEASESUM was created using "Hadashot" newspaper issues from 1984-1993, containing 7,774 samples of multi-document summarization.

## Key Results
- Human evaluation by journalism experts scored teasers highly on Coherence (4.31-4.88), Consistency (4.32-4.75), and Fluency (4.36-4.98) on 5-point scale
- Automatic extraction achieved 86% accuracy for teaser identification and 90% accuracy for matching teasers to articles using zero-shot LLM
- State-of-the-art LLMs showed larger performance gaps in lower-resource languages when evaluated on this dataset
- The dataset supports both single and multi-document summarization tasks

## Why This Works (Mechanism)

### Mechanism 1: Editorial Teasers as Ground-Truth Summaries
- **Claim:** Front-page teasers written by professional editors function as high-quality, abstractive summaries for source articles.
- **Mechanism:** Newspapers use teasers to entice readers; this requires editors to condense article(s) into coherent, fluent, and relevant blurbs. This process mirrors abstractive summarization, creating a naturally annotated summary-article pair without manual annotation.
- **Core assumption:** Editorial standards of the selected newspaper titles yield summaries that are factually consistent and relevant to the source articles.
- **Evidence anchors:** Human evaluation by journalism experts found high quality summaries across different teaser lengths, with relevance improving as length increases; Expert annotators scored teasers highly on Coherence (4.31-4.88), Consistency (4.32-4.75), and Fluency (4.36-4.98) on a 5-point scale.
- **Break condition:** If OCR errors are substantial and uncorrected, they corrupt both teaser and article text, degrading summary quality and alignment.

### Mechanism 2: Keyword-Based Extraction Pipeline
- **Claim:** A scalable, two-step pipeline uses simple heuristics to identify teasers and match them to articles.
- **Mechanism:** The pipeline exploits structural regularity. Step 1 uses newspaper-specific phrases (e.g., "continued on p. 5") to detect teasers. Step 2 uses semantic similarity to link teasers to article(s) on referenced page(s).
- **Core assumption:** Newspaper layout is consistent enough that key phrases reliably signal a teaser, and referenced pages contain correct source articles.
- **Evidence anchors:** The automatic extraction achieved 86% accuracy for teaser identification and 90% accuracy for matching teasers to articles using a zero-shot LLM approach; Visually and textually describes the two-step process: 1) Identify teasers by key phrases and 2) Match teasers to corresponding articles.
- **Break condition:** Pipeline fails with highly irregular layouts or non-standard teaser phrasing. Performance degrades with poor OCR quality.

### Mechanism 3: Resource-Adaptive Matching
- **Claim:** The matching component adapts to available computational and linguistic resources.
- **Mechanism:** A tiered approach: low-resource TF-IDF, medium-resource Sentence Transformer, and high-resource zero-shot LLM. This allows application across resource settings.
- **Core assumption:** Even in low-resource settings, sufficient signal exists in shared vocabulary (TF-IDF) or embeddings (Sentence Transformer) to establish matches.
- **Evidence anchors:** Reports accuracy for TF-IDF (86%), Sentence Transformer (81%), and Zero-shot LLM (90%); Describes methods "suited to varying degrees of resource availability, from TF-IDF (low resource) to zero-shot classification (medium to high resource)."
- **Break condition:** TF-IDF and Sentence Transformer methods fail with highly abstractive teasers (little vocabulary overlap) or complex morphology not handled by tokenizers/embedding models.

## Foundational Learning

- **Concept: Abstractive vs. Extractive Summarization**
  - **Why needed here:** This paper's core insight is that front-page teasers are *abstractive* summaries, meaning editors write new text rather than selecting sentences. Understanding this distinction is crucial for appreciating why this data source is valuable and why models struggle with coverage.
  - **Quick check question:** Does the paper's proposed method rely on selecting sentences from the article (extractive) or generating new text (abstractive)?

- **Concept: OCR (Optical Character Recognition) and its Error Modes**
  - **Why needed here:** The entire pipeline depends on digitized newspapers, which are subject to OCR errors. The paper explicitly notes this as a limitation and performs OCR post-correction. Understanding this is key to anticipating data quality issues.
  - **Quick check question:** What are two types of errors in the teaser identification step that are directly caused by OCR issues?

- **Concept: Multi-Document Summarization (MDS)**
  - **Why needed here:** A key contribution of the dataset is that it supports MDS, where a single teaser summarizes multiple articles. This adds complexity to the matching task and the evaluation.
  - **Quick check question:** How does the MDS nature of the dataset affect the article matching step in the proposed pipeline?

## Architecture Onboarding

- **Component Map:** Digitized Newspaper Corpus -> OCR & Post-Correction Module -> Teaser Identifier -> Article Matcher -> Dataset Builder

- **Critical Path:** The most critical step is the **Article Matcher**. A failure here propagates misaligned training or evaluation data. The **OCR quality** is a foundational dependency; poor OCR here undermines all subsequent steps.

- **Design Tradeoffs:**
  - **Rule-based vs. ML-based Matching:** Rule-based teaser identification is transparent and requires no training data but is brittle to layout changes. ML-based matching is more robust but requires more compute/resources.
  - **Dataset Size vs. Quality:** Using a stricter threshold in the matcher increases precision (higher quality pairs) but decreases recall (smaller dataset). The paper notes that rule-based identification errors are mostly false negatives, prioritizing quality.

- **Failure Signatures:**
  - **High False Negative Rate:** Many valid teasers are missed by the key phrase search (due to OCR errors or non-standard phrasing). This reduces dataset size but not its quality.
  - **Low Coverage Scores:** LLMs evaluated on the generated dataset show low "coverage" scores, suggesting they miss key information present in the human-written teaser. This could indicate a limitation of the models or a property of the teaser style.

- **First 3 Experiments:**
  1. **Baseline Validation:** Manually inspect 50-100 samples from the *Teaser Identifier* output to calculate precision and recall, confirming the paper's reported 95% precision and 85% recall for your specific newspaper source.
  2. **Matcher Ablation:** Implement the TF-IDF and Zero-Shot LLM matchers. Run them on a small, manually annotated set of ~30 teasers and compare their F1 scores against the paper's reported values (83 for TF-IDF, 88 for Zero-Shot).
  3. **Data Quality Check:** Fine-tune a small summarization model (e.g., BART) on a portion of the created dataset and evaluate its ROUGE scores on a held-out set. Compare these scores to a baseline model trained on a different dataset to gauge the dataset's utility.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the high performance observed in Norwegian (the lowest-resource language studied) reflect genuinely better model capabilities, or are evaluation metrics less reliable for very low-resource languages?
- **Open Question 2:** Does fine-tuning on front-page teaser datasets improve LLM summarization performance in low-resource languages?
- **Open Question 3:** To what extent does OCR noise in historical newspapers degrade the quality of extracted summarization datasets and downstream model performance?
- **Open Question 4:** Is LLM-as-a-judge evaluation systematically biased in favor of model-generated text when applied to low-resource language summarization?

## Limitations

- **Major dependency on high-quality OCR and consistent newspaper layouts:** Performance may degrade significantly for historical newspapers with irregular layouts or substantial OCR degradation.
- **Computational resource requirements for zero-shot LLM approach:** Creates barriers for truly low-resource language applications despite the method's theoretical accessibility.
- **Assumption of consistent editorial practices:** The method assumes front-page teasers consistently provide adequate coverage of referenced articles, which may not hold for all editorial styles.

## Confidence

- **High Confidence:** The core mechanism of using editorial teasers as ground-truth summaries is well-supported by expert evaluation scores (Coherence 4.31-4.88, Consistency 4.32-4.75, Fluency 4.36-4.98 on 5-point scale) and the observed 95% precision in teaser identification.
- **Medium Confidence:** The resource-adaptive matching framework is theoretically sound and demonstrated with multiple methods, but the optimal thresholds and performance may vary significantly across different newspaper sources and languages.
- **Low Confidence:** The generalizability of the approach to truly low-resource languages with poor digitization infrastructure remains unproven, as the paper's experiments focus on Hebrew and other well-digitized languages.

## Next Checks

1. Test the pipeline on newspapers from different time periods and geographic regions to assess robustness to layout variations and OCR quality degradation over time.
2. Implement the TF-IDF matching approach and systematically vary the similarity threshold to empirically determine optimal values for different language pairs and newspaper styles.
3. Evaluate the approach on a truly low-resource language with limited digitization infrastructure to identify practical barriers beyond computational requirements.