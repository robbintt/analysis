---
ver: rpa2
title: Automated Planning for Optimal Data Pipeline Instantiation
arxiv_id: '2503.12626'
source_url: https://arxiv.org/abs/2503.12626
tags:
- data
- operators
- pipeline
- operator
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of optimal deployment of data pipelines
  on distributed computing clusters, specifically focusing on how to group operators
  with different runtime requirements to minimize total execution time. The authors
  model this optimization problem as planning with action costs and propose heuristic
  search methods using automated planning techniques.
---

# Automated Planning for Optimal Data Pipeline Instantiation

## Quick Facts
- **arXiv ID**: 2503.12626
- **Source URL**: https://arxiv.org/abs/2503.12626
- **Reference count**: 25
- **Primary result**: Planning-based heuristics optimize operator grouping in data pipelines, with connection heuristic minimizing execution time and random grouping exploiting parallelism in high-workload scenarios.

## Executive Summary
This work addresses optimal deployment of data pipelines on distributed computing clusters by modeling operator grouping as a planning problem with action costs. The authors propose heuristic search methods encoded in PDDL and solved using the ENHSP planner, implementing four approaches: connection-based, node-based, random, and single-group baselines. Experiments with synthetic Fibonacci sequence workloads in sequential and parallel pipeline topologies demonstrate that the connection heuristic consistently outperforms other methods in execution time, while random grouping with more groups can exploit parallelism better under high workloads. The results validate AI-based optimization for improving both setup and execution times of data pipelines through intelligent operator grouping.

## Method Summary
The optimization problem is modeled as planning with action costs, where pipeline operators and their tag constraints become planning objects in PDDL. The ENHSP planner searches for optimal grouping plans that minimize total cost, encoding communication overhead (intragroup=5, intergroup=20 for connection heuristic) and group instantiation penalties. Four approaches are implemented: two greedy heuristics (connection-based and node-based), a random baseline, and a default single-group baseline. The system converts pipeline graphs (JSON) into PDDL domain/problem files, solves with ENHSP, then converts the optimal plan back to optimized pipeline JSON for execution on Kubernetes clusters. Synthetic Fibonacci sequence workloads in sequential and parallel topologies are used to evaluate setup time, execution time, and total time metrics.

## Key Results
- Connection heuristic consistently achieved lowest total time across both sequential and parallel topologies
- Random baseline with more groups outperformed connection heuristic in parallel topology execution time under high workload conditions
- Setup time varied significantly between approaches, with connection heuristic minimizing grouping overhead
- Total time improvements demonstrated the practical benefit of AI-based optimization for pipeline deployment

## Why This Works (Mechanism)

### Mechanism 1
Encoding pipeline deployment as a planning problem with state-dependent action costs enables systematic optimization of operator groupings. The system converts pipeline graphs into PDDL domain/problem files, where operators and tag constraints become planning objects. The ENHSP planner searches for action sequences that minimize total cost, with costs encoding communication overhead (intragroup=5, intergroup=20) and group instantiation penalties. The core assumption is that the cost function correlates with actual execution performance. If actual cluster latency and setup overheads differ substantially from hardcoded cost ratios, the planner's solutions will be suboptimal.

### Mechanism 2
Connection heuristic outperforms alternatives by explicitly penalizing intergroup edges, which reduces serialization at cross-group boundaries. By assigning 4x higher cost to intergroup vs intragroup connections, the planner prefers groupings that keep tightly-coupled operators together, minimizing data serialization/deserialization overhead when operators communicate across container boundaries. The core assumption is that intergroup communication dominates execution cost more than group count or setup time for typical workloads. For highly parallel topologies with long-running operators, reduced parallelism from fewer groups can outweigh communication savings.

### Mechanism 3
Random grouping with more groups can outperform heuristics in parallel topologies under high workload by exploiting parallelism. More groups enable more independent containers, better utilizing parallel execution paths. When computation time dominates setup time, parallel speedup compensates for increased intergroup communication and longer setup. The core assumption is that the cluster has sufficient worker nodes to execute multiple groups in parallel without resource contention. On clusters with fewer nodes than groups, or with low-compute workloads, overhead from setup and communication will dominate, making random grouping worse than heuristics.

## Foundational Learning

- **Planning Domain Definition Language (PDDL)**: The entire optimization approach relies on encoding pipeline grouping as a PDDL planning task solvable by generic planners. Quick check: Given a simple 3-operator pipeline with one tag constraint, can you sketch the corresponding PDDL predicates and actions?

- **Flow-based programming and operator graphs**: Data pipelines are modeled as directed graphs where nodes (operators) transform data and edges represent dataflow; grouping decisions affect which operators share containers. Quick check: If operators A and B are in the same group but C is separate, what happens to data flowing from A → B vs A → C?

- **State-dependent action costs in heuristic search**: The ENHSP planner uses numeric planning with costs that depend on pipeline state (current groupings, remaining operators), not just action count. Quick check: Why would a planner with uniform action costs (all actions cost 1) fail to optimize for execution time in this setting?

## Architecture Onboarding

- **Component map**: VFlow Pipeline Engine -> JSON→PDDL Converter -> ENHSP Planner -> Plan→JSON Converter -> VFlow Profiler

- **Critical path**: 1. Define pipeline in VFlow (operators + edges + tags) 2. Export to JSON → convert to PDDL (choose heuristic) 3. Run ENHSP → obtain grouping plan 4. Convert plan → optimized JSON 5. Deploy on Kubernetes cluster → collect metrics

- **Design tradeoffs**:
  - Connection heuristic: Lower setup time, fewer groups, best for sequential/low-parallelism pipelines. Risk: underutilizes parallel execution
  - Node heuristic: Maximizes group size, minimizes group count. Risk: ignores communication topology
  - Random baseline: High variance, potentially more groups. Better for parallel topologies with high compute per operator, worse for low-workload clusters
  - Default single-group: No optimization overhead, but fails when tag constraints require multiple images

- **Failure signatures**:
  - Setup time spikes: Too many groups (random) or cold starts. Mitigate with warm-start benchmarks or cap group count
  - Execution time not improving: Heuristic cost weights mismatch actual cluster behavior. Re-tune 5/20/medium weights
  - Planner timeout: Large pipelines (>50 operators) may exceed solve time. Reduce problem size or use satisficing planner settings
  - Tag constraint violations: If PDDL encoding misses an operator's tag, resulting plan will fail at deployment

- **First 3 experiments**:
  1. Replicate line topology with fibo_step=1, special_ops=2: Run all four approaches on a 14-operator sequential pipeline
  2. Test parallel topology with increasing workload: Fix special_ops=2, vary fibo_step from 1 to 3
  3. Ablate cost weights: Modify connection heuristic costs (try 10/5 or 30/5 instead of 20/5) and measure sensitivity

## Open Questions the Paper Calls Out

- Can a hybrid strategy that optimizes independent branches of a parallel topology individually outperform global heuristics in complex pipeline structures? The authors state "A hybrid strategy could also be explored to optimize each line of the parallel topology individually."

- How does the optimal operator grouping change when the objective function includes infrastructure cost or resilience rather than just execution time? The authors note "other desirable evaluation metrics can be included, such as the infrastructure cost... or the resilience of the deployed graph."

- Can a deterministic heuristic be developed to consistently capture the performance gains seen in the random baseline for high-workload parallel executions? The random approach outperformed heuristics in parallel scenarios, suggesting current heuristics undervalue parallelism.

## Limitations

- Experimental scope limited to synthetic pipelines (14 operators) with fixed topologies and workloads, lacking validation on real-world production data pipelines
- Cost function (20/5/medium weights) is hardcoded without empirical justification or generalizability testing across different cluster environments
- Connection heuristic's superiority relies on assumptions about intergroup communication costs that may not hold across different hardware, network conditions, or operator types
- Use of proprietary VFlow engine and ENHSP planner without open-source alternatives limits reproducibility and independent validation

## Confidence

- **Connection heuristic's dominance**: High - well-supported by consistent experimental results across both topologies
- **Mechanism explaining random grouping in parallel execution**: Medium - supported by observed workload thresholds but lacks mechanistic depth
- **Generalizability of cost encoding to real deployments**: Low - due to hardcoded weights and absence of cross-environment validation

## Next Checks

1. Replicate experiments with varying cost weights (10/5, 30/5) to determine sensitivity and optimal settings for different workloads
2. Test on larger synthetic pipelines (50+ operators) to evaluate scalability and planner solve times
3. Implement the same optimization on a public Kubernetes-based pipeline system (e.g., Apache Airflow) to verify results outside the VFlow environment