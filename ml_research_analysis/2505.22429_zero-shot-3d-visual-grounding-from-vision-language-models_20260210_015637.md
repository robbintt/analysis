---
ver: rpa2
title: Zero-Shot 3D Visual Grounding from Vision-Language Models
arxiv_id: '2505.22429'
source_url: https://arxiv.org/abs/2505.22429
tags:
- visual
- spatial
- object
- arxiv
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SeeGround, a zero-shot 3D visual grounding
  framework that leverages 2D vision-language models (VLMs) without requiring 3D-specific
  training. The key innovation is reformulating 3D scenes into query-aligned rendered
  views combined with spatially enriched textual descriptions to bridge the modality
  gap.
---

# Zero-Shot 3D Visual Grounding from Vision-Language Models

## Quick Facts
- arXiv ID: 2505.22429
- Source URL: https://arxiv.org/abs/2505.22429
- Reference count: 40
- Primary result: Achieves 44.1% accuracy on ScanRefer, 7.7% above prior zero-shot methods

## Executive Summary
This paper introduces SeeGround, a zero-shot 3D visual grounding framework that leverages 2D vision-language models (VLMs) without requiring 3D-specific training. The key innovation is reformulating 3D scenes into query-aligned rendered views combined with spatially enriched textual descriptions to bridge the modality gap. The framework employs two main components: a Perspective Adaptation Module that dynamically selects optimal viewpoints based on the query, and a Fusion Alignment Module that integrates visual and spatial signals to reduce grounding ambiguity. Evaluated on ScanRefer and Nr3D benchmarks, SeeGround achieves state-of-the-art zero-shot results, improving over prior methods by 7.7% and 7.1% respectively, and narrowing the gap with fully supervised alternatives.

## Method Summary
SeeGround is a training-free framework that converts 3D scenes into a hybrid 2D-3D representation for zero-shot 3D visual grounding. The pipeline begins with an open-vocabulary 3D detector (Mask3D) generating an Object Lookup Table (OLT) containing bounding boxes and semantic labels. A query parser extracts target and anchor objects from natural language queries via few-shot prompting. The Perspective Adaptation Module then computes camera poses aligned with the anchor's spatial context, rendering query-specific views. The Fusion Alignment Module projects OLT objects onto these views, filters occluded instances using depth maps, and overlays visual prompts. Finally, a 2D VLM (Qwen2-VL-72b) processes the rendered image with spatial descriptions and predicts the target object's ID, which is mapped back to the 3D bounding box via the OLT.

## Key Results
- Achieves 44.1% accuracy on ScanRefer (7.7% above prior zero-shot methods)
- Improves Nr3D accuracy by 7.1% over previous zero-shot approaches
- Reduces gap to fully supervised methods from 18.6% to 13.1% on ScanRefer
- Dynamic viewpoint selection (query-aligned) outperforms static strategies by 5.7% on view-dependent queries

## Why This Works (Mechanism)

### Mechanism 1: Query-Aligned Dynamic Viewpoint Selection
The Perspective Adaptation Module uses few-shot prompting to extract an anchor object from the query, then positions a virtual camera facing the anchor with backward/upward offsets to capture both local object detail and global scene context. This dynamic selection based on query-implied spatial context improves grounding accuracy over static multi-view or bird's-eye strategies, achieving 46.1% overall vs 43.3% for bird's-eye view and 40.2–41.5% for fixed trajectories.

### Mechanism 2: Hybrid 2D-3D Representation via Rendering + Spatial Text
An open-vocabulary 3D detector produces an Object Lookup Table (OLT) with bounding boxes and semantic labels. This OLT is converted to natural language descriptions and combined with a query-aligned rendered image, allowing 2D VLMs to jointly reason over visual appearance (color, texture, shape) and precise spatial semantics. This reformulation enables pretrained 2D VLMs to perform 3D grounding without 3D-specific training.

### Mechanism 3: Depth-Aware Visual Prompting for Instance Disambiguation
The Fusion Alignment Module projects 3D object points onto the 2D image using the camera pose, filters occluded points via depth comparison against a rendered depth map, and places visual markers at visible projection centers. This explicitly associates each prompt with a unique object ID, grounding the VLM's attention and reducing ambiguity when multiple similar instances exist in a scene.

## Foundational Learning

- **Concept: 3D-to-2D projection and depth-based occlusion handling**
  - Why needed: The entire pipeline depends on rendering 3D scenes to 2D and filtering occluded objects via depth comparison.
  - Quick check: Given a 3D point (x, y, z), camera intrinsics K, and extrinsics (R, T), compute the 2D pixel (u, v). Given a depth map D, how do you determine if (u, v) is occluded?

- **Concept: Few-shot prompting for structured extraction**
  - Why needed: The framework uses in-context examples to parse queries into (target, anchor) pairs without fine-tuning.
  - Quick check: Design a 3-shot prompt that extracts target and anchor from: "the black keyboard on the desk next to the window."

- **Concept: Zero-shot transfer from 2D pretraining to 3D tasks**
  - Why needed: The method's core claim is training-free 3D grounding via 2D VLMs; understanding why this transfer works is critical for debugging and extension.
  - Quick check: Why would CLIP or Qwen2-VL, trained on web image-text pairs, recognize "floral-patterned chair" in a synthetic rendering of a 3D scan?

## Architecture Onboarding

- **Component map**: Open-vocabulary 3D detector (Mask3D) -> Object Lookup Table (OLT) -> Query Parser (2D-VLM) -> Perspective Adaptation Module -> Renderer -> Visual Prompter -> Fusion Alignment VLM -> OLT lookup

- **Critical path**: Query → VLM parsing → anchor → camera pose → render → depth-aware prompting → VLM prediction → OLT lookup → 3D bbox output

- **Design tradeoffs**: Detector choice (Mask3D yields 44.1% vs OVIR-3D at 30.7%; GT boxes reach 59.5%), VLM choice (Qwen2-VL-72b vs GPT-4V), rendering scope (full-scene vs object-centric)

- **Failure signatures**: Spatial relation errors (19% of failures), egocentric reference failures, detection cascades (missing OLT entries), similar-instance confusion

- **First 3 experiments**: 1) Render pipeline validation with 5 queries, 2) Perspective ablation comparing query-aligned vs bird's-eye view on 50 Nr3D queries, 3) Robustness test removing 1-2 anchor objects from OLT for 20 queries

## Open Questions the Paper Calls Out

1. Can incorporating high-fidelity rendering techniques (e.g., neural radiance fields or 3D Gaussian splatting) significantly improve grounding accuracy in cluttered scenes? Current rendering uses raw ScanNet point clouds, limiting visual detail.

2. Would a dedicated spatial reasoning module reduce the 19% spatial relation error rate in zero-shot 3D visual grounding? Current fusion approach relies on VLM's implicit spatial understanding, which struggles with complex spatial predicates.

3. How can viewpoint selection be improved to reliably handle complex egocentric references (e.g., "when the window is on the left," "upon entering from the door")? Current perspective adaptation targets object-centric anchors, but egocentric descriptions require reasoning about observer position.

4. What is the performance ceiling for zero-shot 3D visual grounding, and how much of the remaining ~13% gap to fully supervised methods is attributable to detector limitations versus cross-modal alignment? Table 5 shows 59.5% accuracy with ground-truth boxes versus 44.1% with Mask3D.

## Limitations
- Limited rendering quality from raw point clouds hampers object discrimination in cluttered scenes
- Spatial relation interpretation errors remain frequent (19% of failures)
- Viewpoint selection struggles with complex egocentric references (e.g., "when the window is on the left")
- Heavy reliance on open-vocabulary 3D detectors creates a hard ceiling on performance

## Confidence
- Perspective Adaptation Module: High confidence (consistent improvements across benchmarks)
- Hybrid 2D-3D Representation: High confidence (explicit bridging of modality gaps)
- Fusion Alignment Module: Medium confidence (3.8% improvement but lacks strong analogs)

## Next Checks
1. Test the framework on a subset of 50 Nr3D view-dependent queries with intentionally ambiguous anchors to quantify failure rates on egocentric references.

2. Systematically remove detected objects from the OLT for 20 ScanRefer queries and measure accuracy degradation to establish the performance ceiling imposed by detection quality.

3. Implement the Fusion Alignment Module with two prompt variations—one with spatial text only and one with spatial text plus visual prompts—to isolate the 3.8% improvement contribution.