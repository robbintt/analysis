---
ver: rpa2
title: Leveraging LLM-Based Agents for Intelligent Supply Chain Planning
arxiv_id: '2509.03811'
source_url: https://arxiv.org/abs/2509.03811
tags:
- planning
- supply
- chain
- task
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Generative AI-powered agentic framework for
  intelligent supply chain planning, deployed at JD.com to address inefficiencies
  in data processing, coordination, and decision-making across sales, inventory, and
  operations. The system automates task decomposition, data retrieval, and analysis,
  enabling natural language-driven plan generation and real-time adaptation.
---

# Leveraging LLM-Based Agents for Intelligent Supply Chain Planning

## Quick Facts
- arXiv ID: 2509.03811
- Source URL: https://arxiv.org/abs/2509.03811
- Reference count: 24
- Reduces planners' weekly manual workload from 120 minutes to 5 minutes

## Executive Summary
This paper presents a Generative AI-powered agentic framework for intelligent supply chain planning, deployed at JD.com to address inefficiencies in data processing, coordination, and decision-making across sales, inventory, and operations. The system automates task decomposition, data retrieval, and analysis, enabling natural language-driven plan generation and real-time adaptation. The framework demonstrates scalable, resilient, and adaptive supply chain planning through human-machine collaboration.

## Method Summary
The framework implements a cognitive loop where a foundation model interprets user intent, decomposes tasks, retrieves data, executes analysis, and iteratively refines plans. Key technical innovations include atomic code generation for data analysis, retrieval-augmented generation for SOP injection, and a dynamic subtask refinement mechanism. The system bridges unstructured business intent and structured execution through semantic interpretation, task orchestration, and iterative correction cycles.

## Key Results
- Reduced planners' weekly manual workload from 120 minutes to 5 minutes
- Improved planning accuracy by 22% through iterative correction mechanisms
- Increased stock fulfillment rates by 2-3% during peak events, translating to approximately RMB 2 million GMV uplift

## Why This Works (Mechanism)

### Mechanism 1
The system bridges unstructured business intent and structured execution via semantic interpretation and Standard Operating Procedure (SOP) injection. A Query Enhancement Module parses natural language into slots, while a Task Orchestration Agent retrieves relevant SOPs via RAG. The LLM then decomposes high-level intent into structured task lists grounded in organizational best practices.

### Mechanism 2
Decomposing analytical code generation into atomic primitives improves execution reliability. The Data Analysis Agent uses a grammar of four operations—Filter, Transform, Groupby, and Sort—generating code iteratively rather than monolithically. This reduces cognitive load on the model and minimizes syntax errors.

### Mechanism 3
Planning accuracy improves through a closed-loop Execute-Evaluate-Refine cycle. Dynamic Subtask Refinement feeds execution observations back to the Task Planning Agent, which then prunes, re-sequences, or adds tasks based on real-time data. A Plan Correction Agent monitors deviations and triggers remedial actions.

## Foundational Learning

**Retrieval-Augmented Generation (RAG)**
Why needed: RAG injects specific Standard Operating Procedures into prompt context, preventing generic plans unsuited to organizational constraints.
Quick check: Can you explain how a vector database connects a user's vague query to a specific company policy document before the LLM processes the plan?

**Chain-of-Thought (CoT) & ReAct**
Why needed: The Task Orchestration Agent functions via a ReAct-style loop—generating thoughts, creating plans, executing, observing, and repeating.
Quick check: How does the "Observation" field in the prompt influence the next step in task list generation?

**Text-to-SQL**
Why needed: The Data Acquisition Agent bridges natural language and the database, translating user queries into SQL dynamically.
Quick check: If a user asks for "sales yesterday," how does the system translate "yesterday" into the correct SQL WHERE clause?

## Architecture Onboarding

**Component map:**
Cognitive Core (Foundation Model + Memory + RAG) -> Input Layer (Query Enhancement) -> Management Layer (Intent Classifier -> Task Orchestrator) -> Execution Layer (Data Acquisition -> Data Analysis) -> Feedback Layer (Plan Correction)

**Critical path:**
1. User Query -> 2. Intent Classification -> 3. SOP Retrieval -> 4. Task List Generation -> 5. SQL Generation -> 6. Atomic Code Execution -> 7. Observation -> 8. Plan Update

**Design tradeoffs:**
- Atomic Code vs. Monolithic Code: Restricts code gen to four primitives to increase reliability, but limits flexibility for complex statistical modeling.
- Function Call vs. Code Gen: Uses function calls for deterministic math and code gen for data wrangling, creating high maintenance costs for maintaining two execution pathways.

**Failure signatures:**
- Context Drift Loop: Agent repeatedly adds new sub-tasks without declaring the plan "Final."
- SQL Hallucination: Text-to-SQL agent invents non-existent column names in the schema.
- SOP Misalignment: RAG retrieves outdated SOPs, causing invalid sub-task generation.

**First 3 experiments:**
1. Unit Test the Query Enhancer: Verify slot extraction accuracy with varied natural language inputs.
2. Validate Atomic Code Gen: Test chaining Filter -> Groupby -> Sort on sample dataframes without syntax errors.
3. Observe the Correction Loop: Simulate plan diagnosis scenarios where execution data deviates from the plan.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Real-world deployment evidence is proprietary without public validation datasets or independent replication.
- Effectiveness depends heavily on well-documented SOPs and stable database schemas, unproven in fragmented data environments.
- Four-primitive limitation may fail for complex analytics requiring advanced statistical modeling without pre-built function calls.

## Confidence

**High Confidence:** The architectural framework combining RAG, atomic code generation, and iterative refinement is technically sound and aligns with established LLM agent patterns.

**Medium Confidence:** The 22% accuracy improvement claim is plausible given the iterative correction mechanism, but lacks independent verification.

**Low Confidence:** The specific 2-3% stock fulfillment improvement during peak events is difficult to attribute solely to the LLM system given supply chain complexity.

## Next Checks

1. **Cross-company replication:** Deploy the framework in a non-JD supply chain environment with different data structures to test generalizability beyond the original implementation.

2. **A/B testing validation:** Conduct controlled experiments comparing LLM-generated plans against traditional methods using identical input data to isolate the system's specific contribution to performance gains.

3. **Edge case stress testing:** Evaluate system behavior when handling ambiguous queries, contradictory data signals, or database schema changes to identify failure modes in real-world conditions.