---
ver: rpa2
title: 'Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal
  Inference Possible?'
arxiv_id: '2601.15254'
source_url: https://arxiv.org/abs/2601.15254
tags:
- setting
- assumption
- sparse
- instruments
- high-dimensional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of estimating causal effects in
  settings where data are unpaired across environments (i.e., X and Y cannot be measured
  jointly), under hidden confounding. The authors cast the problem as a high-dimensional
  instrumental variable (IV) regression, with environments serving as instruments.
---

# Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?

## Quick Facts
- **arXiv ID:** 2601.15254
- **Source URL:** https://arxiv.org/abs/2601.15254
- **Authors:** Felix Schur; Niklas Pfister; Peng Ding; Sach Mukherjee; Jonas Peters
- **Reference count:** 40
- **Primary result:** Introduces SplitUP, a cross-fold sample splitting estimator for causal inference with high-dimensional instruments and unpaired data, which is consistent and asymptotically normal as the number of environments grows while the sample size per environment remains constant.

## Executive Summary
This paper addresses the problem of estimating causal effects in settings where data are unpaired across environments (i.e., X and Y cannot be measured jointly), under hidden confounding. The authors cast the problem as a high-dimensional instrumental variable (IV) regression, with environments serving as instruments. The core contribution is SplitUP, a cross-fold sample splitting strategy that addresses the key challenge that standard two-sample IV estimators fail when the number of instruments grows but the number of observations per instrument remains fixed. SplitUP uses a GMM-type estimator with cross-fold splitting to remove measurement-error bias, achieving consistency in this high-dimensional regime.

## Method Summary
The method leverages environments as high-dimensional instruments to estimate causal effects from unpaired data. It transforms the unpaired causal problem into a system of moment conditions using the exclusion restriction. For finite-dimensional instruments, it uses standard GMM estimation with optional ℓ1 regularization for sparse effects. The key innovation is SplitUP, which addresses the measurement-error bias that occurs when the number of instruments grows large relative to the sample size per instrument. SplitUP splits the X-sample into K folds and constructs the denominator matrix using covariance estimates from different folds, eliminating the bias term. For sparse causal effects, it uses ℓ1-regularized estimation with post-selection refitting. The method also provides an analytic approximation for the infinite-split limit.

## Key Results
- SplitUP is consistent and asymptotically normal as the number of environments grows but the sample size per environment remains constant.
- Standard two-sample IV estimators are asymptotically biased in the high-dimensional instrument regime due to measurement-error effects.
- SplitUP achieves support recovery and consistent estimation in sparse causal effect settings where the number of instruments is smaller than the number of covariates.
- Finite-sample variance reduction techniques (analytic vs. Monte Carlo splitting) are provided for practical implementation.

## Why This Works (Mechanism)

### Mechanism 1: Environment as a High-Dimensional Instrument
- **Claim:** If experimental environments influence the covariates X but do not directly affect the outcome Y (except through X), they serve as valid instrumental variables even when X and Y are never observed jointly.
- **Mechanism:** The method leverages the exclusion restriction (E[ε|I] = 0) to transform the unpaired causal problem into a system of moment conditions. It correlates the environment distribution from the X-sample with the outcome distribution from the Y-sample, circumventing the need for paired observations.
- **Core assumption:** Assumption 1 (ii) requires the noise ε to be independent of the environment I, and Assumption 1 (i) requires the covariance structure between environment and covariates to be consistent across the two unpaired samples.

### Mechanism 2: Cross-Fold Sample Splitting (Bias Removal)
- **Claim:** Standard two-sample IV estimators fail (are asymptotically biased) when the number of instruments m grows large relative to the sample size (n/m → r), due to a specific measurement-error effect in the denominator of the estimator.
- **Mechanism:** The method splits the (I, X)-sample into K folds. It constructs the denominator matrix C_XX by computing the outer product of covariance estimates from different folds (e.g., fold h vs fold k). Because the estimation errors in independent folds are uncorrelated (E[E_h^T E_k] = 0), the bias term E^T E vanishes, restoring consistency.
- **Core assumption:** The errors in covariance estimation between folds are statistically independent, and the limit Q = lim m · Cov(I, X)^T Cov(I, X) exists and is positive definite.

### Mechanism 3: Sparse Identification via Restricted Nullspace
- **Claim:** Causal effects can be identified even when the number of instruments m is smaller than the number of covariates d, provided the true causal effect vector β* is sparse (few non-zero entries).
- **Mechanism:** Instead of requiring a full-rank covariance matrix (which implies m ≥ d), the method requires only that the nullspace of the covariance operator does not contain any sparse vectors (the "restricted nullspace" condition). This allows ℓ1-regularized estimation to isolate the true causal signal.
- **Core assumption:** The "restricted eigenvalue" condition (Assumption 3 iii or 5 iv) holds, ensuring the optimization landscape separates sparse signals from noise.

## Foundational Learning

- **Concept: Instrumental Variables (IV) & Exclusion Restriction**
  - **Why needed here:** The entire approach rests on the validity of the "environment" as an instrument. If the environment affects Y directly (e.g., through a path not involving X), the exclusion restriction is violated, and the estimate reflects correlation, not causation.
  - **Quick check question:** Does the experimental condition change anything about Y other than the distribution of X?

- **Concept: Measurement Error in Moments**
  - **Why needed here:** In high-dimensional settings, the error in estimating Cov(I, X) does not vanish quickly. Standard methods treat this estimated covariance as "truth," leading to biased ratios. Understanding this bias is key to understanding why sample splitting is necessary.
  - **Quick check question:** If you calculate A/B where both A and B have noise, how does the noise in the denominator skew the result compared to calculating E[A]/E[B]?

- **Concept: L1 Regularization (Lasso) & Restricted Eigenvalues**
  - **Why needed here:** When dealing with many covariates (d > m), the problem is under-determined. Lasso allows selection of relevant variables, but it only works if the "noise" variables aren't too correlated with the "signal" variables (Restricted Eigenvalue condition).
  - **Quick check question:** Why does Lasso fail to select the correct variables if the instruments are weak or the covariates are perfectly multicollinear?

## Architecture Onboarding

- **Component map:** 
  Input data -> Cross-Covariance Engine -> SplitUP Core (cross-fold denominator) -> Solver (linear/L1) -> Post-Processor (refitting) -> Output β*

- **Critical path:** The Cross-Fold Denominator calculation is the novel step. If folds are not independent (e.g., data leakage) or the splitting logic is incorrect, the measurement error bias returns.

- **Design tradeoffs:**
  - **Analytic vs. Monte Carlo Splitting:** The "Analytic" version uses a closed-form correction for all possible splits, which is faster and more stable than repeatedly re-splitting the data (Monte Carlo), though theoretically equivalent as H → ∞.
  - **Variance vs. Bias:** While SplitUP removes bias, sample splitting discards some data efficiency. For small sample sizes (N), the variance may be higher than biased estimators, but it is the only consistent option as m grows.

- **Failure signatures:**
  - **Rank Deficiency:** If C_XX is singular (e.g., too few environments or collinear instruments), the solver will fail or produce unstable results.
  - **Heterogeneity:** If the data generating process varies fundamentally between the (I, Y) and (Ĩ, X̃) samples (violating Assumption 1.i), estimates will diverge.
  - **Peaking Phenomenon:** In the sparse setting with low-rank instruments, naive TS-IV exhibits a "peaking" error at specific instrument dimensions; Switch to SplitUP to mitigate.

- **First 3 experiments:**
  1. **Baseline Bias Check (Setting 2):** Generate data with high-dimensional instruments (n/m fixed, e.g., 8). Run both TS-IV and SplitUP. Verify that TS-IV error plateaus (biased) while SplitUP error decreases (consistent).
  2. **Sparse Recovery (Setting 3):** Setup d > m with a sparse β*. Compare standard TS-IV (which fails to identify) vs. L1-regularized SplitUP. Check if the support recovery (selected non-zeros) matches the ground truth.
  3. **Stress Test (Analytic vs. Split):** Compare the "Analytic" implementation against the "Monte Carlo" implementation (e.g., H=10 splits) on small data. Confirm they produce indistinguishable results, validating the analytic approximation for speed.

## Open Questions the Paper Calls Out
None

## Limitations
- The method relies heavily on the exclusion restriction holding exactly and the two samples being drawn from identical distributions for the environment-covariate relationship.
- Strong assumptions may not hold in real-world settings where experimental conditions could directly influence outcomes or where data collection differs across samples.
- The sparsity-based identification requires the restricted nullspace condition, which may be difficult to verify in practice.

## Confidence

- **High confidence:** The consistency and asymptotic normality of SplitUP in the high-dimensional instrument regime (Theorem 4.2 and 4.3). The theoretical derivation of the measurement-error bias in standard two-sample IV estimators is mathematically rigorous.
- **Medium confidence:** The practical performance gains shown in synthetic experiments, as these are limited to controlled settings with known ground truth. The finite-sample variance reduction techniques (analytic vs. Monte Carlo splitting) are demonstrated but not extensively benchmarked against alternatives.
- **Low confidence:** The real-world applicability of the method given the strong assumptions required, particularly the exact exclusion restriction and distributional equivalence across unpaired samples. The paper provides minimal empirical validation beyond synthetic data.

## Next Checks

1. **Assumption violation sensitivity:** Systematically test how violations of Assumption 1 (i) and (ii) - such as direct environment effects on Y or distributional shifts between samples - impact estimator bias and variance. Use semi-synthetic data where the true causal structure is known but assumptions are deliberately violated.

2. **Real-world application:** Apply SplitUP to a real experimental dataset where paired observations are partially available. Compare SplitUP estimates to those obtained from complete paired data to assess practical performance and validate the exclusion restriction empirically.

3. **Restricted nullspace verification:** Develop and validate practical diagnostics for checking whether the restricted nullspace condition holds in real data. Test whether the method remains robust when this condition is approximately but not exactly satisfied.