---
ver: rpa2
title: Bayesian Joint Additive Factor Models for Multiview Learning
arxiv_id: '2406.00778'
source_url: https://arxiv.org/abs/2406.00778
tags:
- factor
- factors
- data
- bayesian
- shared
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces two Bayesian factor regression models for
  integrating multiview data to improve prediction of outcomes. The baseline Joint
  Factor Regression (JFR) uses a single set of latent factors across all views, while
  Joint Additive Factor Regression (JAFAR) decomposes variation into shared and view-specific
  components.
---

# Bayesian Joint Additive Factor Models for Multiview Learning

## Quick Facts
- arXiv ID: 2406.00778
- Source URL: https://arxiv.org/abs/2406.00778
- Reference count: 40
- Introduces Bayesian methods for integrating multiview data to improve outcome prediction

## Executive Summary
This paper presents two Bayesian factor regression models for multiview data integration: Joint Factor Regression (JFR) and Joint Additive Factor Regression (JAFAR). JFR uses a single set of latent factors across all views, while JAFAR decomposes variation into shared and view-specific components. The key innovation is the development of dependent cumulative shrinkage process (D-CUSP) priors that adaptively learn the number of factors and prevent misallocation of view-specific factors to shared ones. Both methods demonstrate superior predictive performance on immunome, metabolome, and proteome data for predicting time-to-labor onset.

## Method Summary
The authors develop two Bayesian factor regression models for multiview data. JFR shares a single set of latent factors across all views, while JAFAR decomposes variation into shared factors affecting all views and view-specific factors unique to each view. To ensure identifiability of shared components in JAFAR, they introduce dependent cumulative shrinkage process (D-CUSP) priors that adaptively learn the number of factors and prevent misallocation of view-specific factors to shared ones. The methods use Gibbs sampling with tempering strategies for computational efficiency and can handle block-wise missing data.

## Key Results
- JAFAR achieves similar prediction accuracy to JFR but with 55% faster computation
- Both methods outperform state-of-the-art competitors in predicting time-to-labor onset
- JAFAR provides greater interpretability through explicit separation of shared and view-specific factors
- Methods handle non-normal data and outcomes flexibly

## Why This Works (Mechanism)
The D-CUSP priors solve the identifiability problem in JAFAR by adaptively shrinking loadings of view-specific factors toward zero in other views, while allowing shared factors to remain active across all views. The tempering strategy in the Gibbs sampler improves computational efficiency by reducing the impact of high-dimensional latent factor updates. The additive decomposition in JAFAR captures both common variation across views and view-specific patterns, improving prediction accuracy and interpretability compared to single-factor approaches.

## Foundational Learning
- Cumulative shrinkage priors: Needed to automatically determine the number of factors; quick check: verify loadings for later factors shrink toward zero
- Dependent cumulative shrinkage process: Needed to distinguish shared vs view-specific factors; quick check: ensure view-specific loadings are sparse across views
- Tempered Gibbs sampling: Needed for computational efficiency; quick check: monitor ESS vs computational time tradeoff

## Architecture Onboarding

**Component Map:**
Data views -> Factor loading matrices -> Shared + View-specific factors -> Outcome prediction

**Critical Path:**
1. Initialize factor loadings with CUSP priors
2. Sample shared and view-specific factors using D-CUSP constraints
3. Update parameters via tempered Gibbs sampling
4. Predict outcomes using factor scores

**Design Tradeoffs:**
- Single factor set (JFR) vs additive decomposition (JAFAR): simplicity vs interpretability
- Fully collapsed vs partially collapsed Gibbs: mixing vs computational cost
- Fixed vs adaptive tempering: stability vs efficiency

**Failure Signatures:**
- Poor mixing in factor loadings suggests D-CUSP constraints too restrictive
- Diverging predictions indicate tempering schedule inappropriate
- Overfitting in high dimensions suggests insufficient shrinkage

**First 3 Experiments:**
1. Simulate data with known shared/view-specific structure to validate D-CUSP performance
2. Compare prediction accuracy with varying numbers of factors
3. Test computational scaling with increasing view dimensionality

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What are the theoretical and practical implications of different choices for the tempering function in the tempered CUSP (T-CUSP) updates?
- Basis in paper: [explicit] The authors state in Appendix D that the choice of tempering function "warrants a more rigorous and systematic investigation, but defer it to future work."
- Why unresolved: The current implementation focuses on a specific rescaling strategy ($T_m = \min(n, p_m)/p_m$) designed for robustness in extreme large-p-small-n scenarios, leaving the optimality of this choice unexplored.
- What evidence would resolve it: Theoretical proofs regarding the convergence properties of the pseudo-Gibbs sampler under various tempering functions, alongside simulation studies comparing their predictive accuracy and rank estimation.

### Open Question 2
- Question: Can the JAFAR framework be extended to incorporate prior feature annotation data using structured increasing shrinkage priors?
- Basis in paper: [explicit] The Discussion section notes that "analogous constructions can be readily developed using the structured increasing shrinkage prior... allowing for the inclusion of prior annotation data on features."
- Why unresolved: The current methodology relies on I-CUSP and D-CUSP priors which do not explicitly utilize external biological or structural annotation data to guide factor loadings.
- What evidence would resolve it: An implementation of JAFAR utilizing structured priors that demonstrates improved interpretability or factor separation when feature annotations are provided.

### Open Question 3
- Question: How can the computational efficiency of the supervised JAFAR Gibbs sampler be optimized for large sample sizes ($n$)?
- Basis in paper: [inferred] Appendix B.2 notes that in the supervised case, the response interdependence makes the partially collapsed sampler computationally expensive ($O(n \cdot \tilde{K}^2)$), forcing a trade-off with sequential updates that may affect mixing.
- Why unresolved: Unlike the unsupervised setting where precision matrices can be cached, the supervised setting requires per-subject computations that scale with sample size, hindering scalability.
- What evidence would resolve it: A modified sampling algorithm that reduces the complexity dependence on $n$ for the supervised latent factor updates without degrading the effective sample size (ESS).

## Limitations
- Validation relies on a single real-world dataset focused on time-to-labor onset prediction
- Limited comparison to only a few benchmark methods
- Computational efficiency claims based on specific hardware and dataset size

## Confidence
- High: Theoretical soundness of Bayesian framework with D-CUSP priors
- Medium: Comparative performance claims due to limited benchmark methods and potential dataset-specific advantages
- Low: Generalizability across different multiview problem domains

## Next Checks
1. Test methods on diverse multiview datasets spanning different domains and data types to assess generalizability beyond biomedical applications
2. Evaluate scalability to high-dimensional settings with thousands of features per view, particularly for the computationally intensive JAFAR model
3. Conduct ablation studies to quantify the specific contribution of the D-CUSP priors to performance improvements, separating this effect from other modeling choices