---
ver: rpa2
title: Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion
  and Large Language Models
arxiv_id: '2512.04425'
source_url: https://arxiv.org/abs/2512.04425
tags:
- gait
- fusion
- multimodal
- feature
- mlge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal RGB-D fusion framework for explainable
  Parkinson's disease gait recognition, integrating dual YOLOv11-based encoders, a
  Multi-Scale Local-Global Extraction (MLGE) module, and a Cross-Spatial Neck Fusion
  mechanism. The system captures both fine-grained limb motion and overall gait dynamics,
  achieving superior robustness to environmental variations and occlusion.
---

# Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models

## Quick Facts
- arXiv ID: 2512.04425
- Source URL: https://arxiv.org/abs/2512.04425
- Authors: Manar Alnaasan; Md Selim Sarowar; Sungho Kim
- Reference count: 29
- One-line primary result: Achieves 89.0% Rank-1 accuracy and 91.8% AUC using RGB-D fusion with LLM-based clinical explanations

## Executive Summary
This paper introduces a multimodal RGB-D fusion framework for explainable Parkinson's disease gait recognition. The system integrates dual YOLOv11-based encoders, a Multi-Scale Local-Global Extraction (MLGE) module, and a Cross-Spatial Neck Fusion mechanism to capture both fine-grained limb motion and overall gait dynamics. Interpretability is enhanced by incorporating a frozen Large Language Model (LLM) to translate visual embeddings and structured metadata into clinically meaningful textual explanations. The approach demonstrates improved accuracy and robustness compared to single-modality baselines while providing actionable clinical insights.

## Method Summary
The framework processes synchronized RGB and depth frames from an Azure Kinect DK sensor through dual YOLOv11 backbones. Depth frames undergo disparity normalization before feature extraction. The MLGE module explicitly separates local limb-level features from global posture-level features using different receptive fields. Cross-Spatial Neck Fusion with C2PSA attention merges RGB and depth streams at multiple scales. Visual embeddings are projected and concatenated with structured metadata (symptom tags) before being fed to a frozen TinyLlama-1.1B-Chat-v1.0 model for clinical report generation. Training uses AdamW optimizer with classification, bbox regression, objectness, and fusion-regularization losses.

## Key Results
- Achieves 89.0% Rank-1 accuracy and 91.8% AUC on Parkinson's gait recognition
- RGB-D fusion shows improved robustness to lighting variations and occlusion compared to single-modality approaches
- LLM integration with metadata injection improves classification accuracy from 70.0% to 82.5%
- MLGE module provides +2.5% Rank-1 improvement over baseline YOLOv11

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Depth modality compensates for RGB fragility in variable lighting and occlusion
- **Mechanism:** RGB relies on texture and color, which degrade in low light or homogeneous clothing. Depth sensors capture geometric distance maps that remain stable regardless of illumination
- **Core assumption:** Depth sensors provide sufficient signal-to-noise ratio for fine-grained motion, and occlusion doesn't completely destroy the 3D point cloud structure
- **Evidence anchors:** [abstract] "capturing... even in challenging scenarios such as low lighting or occlusion caused by clothing"; [section 3.1] "Depth stream provides 3D structural cues... consistent across lighting or viewpoint changes"

### Mechanism 2
- **Claim:** Explicit separation of local (limb-level) and global (posture-level) features improves detection of subtle Parkinsonian signs
- **Mechanism:** MLGE module forces bifurcation: one path preserves local minima via small-receptive-field convolutions, while the other extracts global maxima via Global Average Pooling and dense attention
- **Core assumption:** PD symptoms manifest distinctly as either high-frequency local variations or low-frequency global trends
- **Evidence anchors:** [section 3.3] "MLGE addresses this by explicitly modeling both scales"; [table 1] "YOLOv11 + MLGE" shows +2.5% Rank-1 improvement

### Mechanism 3
- **Claim:** Structured metadata injection is the primary driver of clinical interpretability
- **Mechanism:** The frozen LLM relies on a projection of visual features concatenated with structured metadata (symptom tags) to generate clinically meaningful text
- **Core assumption:** YOLOv11 detection layer accurately classifies gait subtype so metadata provided to LLM is factually correct
- **Evidence anchors:** [table 3] "YOLOv11 + MLGE embeddings + metadata" (82.5% acc) significantly outperforms "embeddings only" (70.0% acc)

## Foundational Learning

- **Concept:** Disparity Normalization (Depth Preprocessing)
  - **Why needed here:** Raw depth values vary drastically with sensor distance. Converting to inverse depth (disparity) and normalizing maps data to bounded [0,1] range, stabilizing gradient descent
  - **Quick check question:** If a subject walks from 2m to 4m away, does the raw depth value double, and how does disparity normalization prevent the feature extractor from treating this as a change in body size?

- **Concept:** Feature Map Resolution vs. Receptive Field
  - **Why needed here:** MLGE module relies on F4 (40x40) for local cues and F5 (20x20) for global cues. Understanding this tradeoff is crucial for debugging why model might miss very fine tremors or whole-body posture
  - **Quick check question:** Which backbone layer (F4 or F5) would you monitor to detect a subtle "wrist tremor" versus a "forward lean"?

- **Concept:** Frozen vs. Fine-Tuned LLM Adapters
  - **Why needed here:** The paper uses a frozen LLM. This implies "medical reasoning" is actually done by the projection layer and quality of input tokens/metadata, not by updating LLM's internal weights
  - **Quick check question:** Why would fine-tuning the LLM on a small dataset be riskier than using a frozen model with well-designed projection layer?

## Architecture Onboarding

- **Component map:** Input: Azure Kinect (RGB + Depth) -> Preproc: Resize/Normalize/Disparity conversion -> Backbone: Dual YOLOv11 (RGB & Depth branches) -> Neck: MLGE (Local F4 + Global F5 processing) -> Cross-Spatial Neck Fusion (SPFF + C2PSA attention) -> Head: Detection/Classification Head -> Projection: GAP + Linear Layer -> Embedding e -> LLM: Embedding e + Metadata -> TinyLlama -> Text Report
- **Critical path:** The Cross-Spatial Neck Fusion (specifically the C2PSA attention block). If RGB and Depth features are not aligned here, the "Global" and "Local" streams will concatenate noise
- **Design tradeoffs:** Latency: Adding MLGE and LLM adds ~2ms inference overhead (18ms -> 20ms). Complexity: Using two YOLO encoders doubles backbone parameter count. Interpretability vs. Accuracy: LLM is frozen; improvements depend entirely on YOLO/MLGE training
- **Failure signatures:** Hallucinated Reports: If visual detector confidence is low but LLM fed generic metadata, it may generate plausible-sounding but false clinical advice. Depth Shadowing: If subject is too close to wall, depth shadows might be interpreted as "missing limbs"
- **First 3 experiments:** 1. Modality Ablation: Train/Evaluate RGB-only vs. Depth-only vs. RGB-D to confirm fusion benefit. 2. MLGE Validation: Replace MLGE with standard concatenation to reproduce ~2.5% accuracy drop. 3. LLM Input Muting: Run LLM with random embeddings but correct metadata to test if "reasoning" comes from visual features or structured text labels

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the proposed framework perform when trained and evaluated on broader, multi-center clinical populations compared to current institution-specific dataset?
- **Basis in paper:** [Explicit] The conclusion states that while current validation uses specific datasets, "future work will extend training to broader and more heterogeneous clinical populations"
- **Why unresolved:** Current study relies on constructed dataset collected at single institution with specific sensor setups, limiting demonstrated generalizability
- **What evidence would resolve it:** Successful replication of 89.0% Rank-1 accuracy and 91.8% AUC metrics on external, multi-centric datasets containing diverse ethnicities, comorbidities, and varying environmental conditions

### Open Question 2
- **Question:** To what extent can integrating temporal consistency modeling or part-whole gait decomposition further improve system's accuracy and robustness?
- **Basis in paper:** [Explicit] The conclusion suggests that "Additional performance gains may also be achieved by integrating advanced optimization strategies, such as temporal consistency modeling or part-whole gait decomposition"
- **Why unresolved:** Current architecture primarily relies on YOLOv11-based spatial feature extraction and MLGE modules, which may not fully exploit long-range temporal dependencies or granular part-based biomechanics
- **What evidence would resolve it:** Ablation studies quantifying performance improvements when temporal consistency loss functions or hierarchical part-whole attention modules are added

### Open Question 3
- **Question:** Can the RGB-Depth-LLM alignment paradigm be effectively generalized to other neurological or musculoskeletal movement disorders without architectural redesign?
- **Basis in paper:** [Explicit] The conclusion notes the paradigm "can be extended to other neurological or musculoskeletal movement disorders, paving the way for broader applications"
- **Why unresolved:** Model is specifically tuned for Parkinsonian features using specific metadata and prompts; its utility for detecting different motor signatures is hypothesized but unproven
- **What evidence would resolve it:** Evaluation of cross-spatial neck fusion and LLM explanation pipeline on distinct pathology datasets (e.g., stroke or ALS gait) to see if learned embeddings transfer effectively

## Limitations
- Dataset Dependency: Performance claims hinge on proprietary RGB-D gait dataset with synchronized metadata; no public benchmark or external validation dataset cited
- LLM Interpretability Gap: Quality and clinical reliability of generated outputs depend entirely on accuracy of visual feature extraction and metadata injection pipeline
- Generalization Risk: Framework tested on controlled indoor setup with Azure Kinect DK; real-world deployment in diverse environments may degrade performance

## Confidence

- RGB-D Fusion Robustness: **High** - Ablation evidence in Table 2 and literature precedent for multimodal fusion provide strong support
- MLGE Feature Separation: **Medium** - Table 1 shows measurable improvement, but specific architectural contribution vs. general capacity increase is unclear
- LLM Clinical Interpretability: **Low** - 82.5% vs 70.0% difference suggests metadata is critical, but no independent assessment of clinical correctness or usefulness of generated text

## Next Checks

1. **Modality Ablation Test:** Re-run experiments with RGB-only and Depth-only models to quantify true contribution of fusion mechanism and isolate whether depth is compensating for RGB or vice versa

2. **Metadata Ablation Test:** Evaluate LLM report quality with correct metadata but scrambled visual embeddings to confirm that LLM's reasoning relies on metadata labels, not visual features

3. **Cross-Environment Validation:** Test trained model on held-out dataset collected under different lighting conditions, backgrounds, or with different depth sensor to assess real-world robustness claims