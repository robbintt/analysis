---
ver: rpa2
title: 'LLMPerf: GPU Performance Modeling meets Large Language Models'
arxiv_id: '2503.11244'
source_url: https://arxiv.org/abs/2503.11244
tags:
- kernel
- execution
- performance
- input
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLMPerf, a pioneering approach that leverages
  large language models (LLMs) to predict GPU kernel execution times from static OpenCL
  source code without requiring runtime information. The authors develop a large-scale
  OpenCL performance dataset through automated techniques including memory analysis-based
  input generation and execution configuration sampling, resulting in over 400K diverse
  kernel launch configurations.
---

# LLMPerf: GPU Performance Modeling meets Large Language Models

## Quick Facts
- arXiv ID: 2503.11244
- Source URL: https://arxiv.org/abs/2503.11244
- Reference count: 25
- LLMPerf-2B achieves 24.25% MAPE on validation, 46.1% MAPE on public OpenCL benchmarks

## Executive Summary
This paper introduces LLMPerf, a pioneering approach that leverages large language models (LLMs) to predict GPU kernel execution times from static OpenCL source code without requiring runtime information. The authors develop a large-scale OpenCL performance dataset through automated techniques including memory analysis-based input generation and execution configuration sampling, resulting in over 400K diverse kernel launch configurations. They adapt the CodeGen LLM architecture by replacing the language prediction head with a regression head for direct execution time estimation.

## Method Summary
LLMPerf fine-tunes the CodeGen-Multi LLM architecture (350M/2B parameters) for regression by replacing the language modeling head with an MLP regression head. The model predicts execution time from structured text prompts containing OpenCL kernel source code, input argument metadata, and execution configuration parameters (global/local work sizes). Training uses MSE loss on log2-transformed execution times with AdamW optimizer (lr=1e-6, weight decay=0.01). The dataset generation pipeline includes memory analysis-based input size inference from access patterns and execution configuration sampling stratified across idle-SM, under-utilized, and fully-utilized GPU regimes.

## Key Results
- LLMPerf-2B achieves 24.25% MAPE on the validation set and 46.1% MAPE on public OpenCL benchmarks (SHOC, Rodinia)
- Memory analysis-based input generation improves validation MAPE from 34.2% to 23.61% compared to baseline approaches
- Dataset size matters more than model size: 400K samples with 350M parameters outperforms 200K samples with 2B parameters

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Encoded Static Performance Features
Encoding kernel source, argument metadata, and execution configuration as structured text enables the LLM to learn performance patterns without dynamic profiling. The pre-trained CodeGen model's code understanding transfers to recognizing computational complexity patterns that correlate with execution time. Core assumption: Performance-determining features are recoverable from static code structure and explicit configuration parameters. Break condition: Kernels whose performance depends on input data values (e.g., branch divergence from array contents).

### Mechanism 2: Memory-Analysis-Based Input Generation
Statically inferring array sizes from memory access patterns creates training data with more diverse performance characteristics. By instrumenting memory accesses and identifying affine relationships between array sizes and global work size, the method exposes the model to non-trivial input-size correlations. Core assumption: GPU kernels exhibit regular memory access patterns expressible as affine functions of work-item IDs. Break condition: Kernels with irregular memory access cannot be analyzed this way.

### Mechanism 3: Execution Configuration Sampling Stratified by Utilization Regimes
Sampling workgroup counts across idle-SM, under-utilized, and fully-utilized regimes with IQR-based balancing produces training data covering the performance response curve. Under-utilized configurations "contain the most performance information due to varying parallelism per SM." Core assumption: The relationship between N_WG and execution time follows a learnable pattern across regimes. Break condition: Kernels with non-monotonic or highly variable execution time vs. N_WG relationships.

## Foundational Learning

- **GPU Workgroup Scheduling and SM Utilization**: The sampling strategy and predictions depend on understanding how workgroups map to SMs and latency hiding. Quick check: Given N_SM = 80, lsize = 128, gsize = 4096, what regime is the GPU in? (Answer: Idle-SM regime, since N_WG = 32 < 80.)

- **Prompt Engineering for Structured Regression**: LLMPerf converts regression to text-to-number via prompts. Quick check: Why include argument qualifiers but not array element values? (Answer: Qualifiers affect memory hierarchy access patterns; element values are excluded to keep prompts tractable.)

- **Transfer Learning from Code-Pretrained LLMs**: LLMPerf fine-tunes CodeGen rather than training from scratch. Quick check: What modification does LLMPerf make to CodeGen's architecture? (Answer: Replaces the language modeling head with a regression head projecting concatenated final hidden states to a scalar.)

## Architecture Onboarding

- **Component map**: [OpenCL Kernel Source] → [Prompt Formatter] → [CodeGen-2B Backbone] → [Regression Head (MLP)] → [Predicted log(time)]

- **Critical path**: Dataset generation → Prompt construction → Fine-tuning → Inference. Dataset generation (memory analysis + IQR balancing) determines model quality more than architectural choices.

- **Design tradeoffs**:
  - Context length vs. information completeness: Prompts exceeding 2048 tokens are discarded; array element values are excluded to stay within limits
  - Dataset diversity vs. coherence: Memory analysis adds diversity but only applies to ~30K of 400K samples
  - Model size vs. overfitting: LLMPerf-350M-400K shows 1.86% train MAPE but 43.65% val MAPE—larger models regularize better

- **Failure signatures**:
  - High MAPE on kernels where input size ≠ global size (spmv_csr_scalar: 53.62%)
  - Predictions that capture relative magnitude but have systematic bias
  - Overfitting pattern: low train MAPE + high val MAPE on smaller models

- **First 3 experiments**:
  1. Reproduce the 400K dataset statistics by running memory analysis pipeline on sample kernels
  2. Ablate the prompt structure by training with only source code (no input args or gsize/lsize)
  3. Evaluate specifically on kernels with known input-size/global-size decoupling to measure generalization

## Open Questions the Paper Calls Out

- **How can LLMs be made more effective and accurate for GPU performance modeling tasks?** Authors ask how to make new AI models useful for performance modeling; current 46.1% MAPE results are described as "humble" and requiring further research.

- **How much can models infer about program performance without executing the program?** Authors question the limits of static-only inference; the trade-off between static analysis capability and prediction accuracy remains unexplored.

- **Can LLMPerf be extended to 2D and 3D kernel execution spaces while maintaining accuracy?** Paper explicitly constrains to 1D kernels (70% of corpus), leaving 30% of real-world kernels unaddressed.

- **Can increasing varied input size samples overcome prediction failures for kernels without input size-global size correlation?** Model fails on kernels like spmv_csr_scalar where input size and global size are uncorrelated; authors suggest memory analysis-based generation as potential solution.

## Limitations
- Memory analysis-based input generation covers only ~30K of 400K training samples, limiting generalization
- Systematic bias on kernels where input size ≠ global size (53.62% MAPE on spmv_csr_scalar)
- Claims about avoiding runtime profiling are technically true but practically limited by 46.1% MAPE on public benchmarks

## Confidence
- **High**: Basic premise that static code features + execution configuration can predict performance for regular kernels (24.25% MAPE validation)
- **Medium**: Effectiveness of memory-analysis-based input generation (improves MAPE but limited sample fraction)
- **Medium**: Three-regime sampling strategy's optimality (rationale sound but untested against alternatives)
- **Low**: Claims about avoiding runtime profiling (technically true but practically limited by accuracy)

## Next Checks
1. Ablate training data composition: Train separate models using only the 30K memory-analysis samples vs. only the 365K simple samples
2. Test on kernel categories: Systematically evaluate on kernels where input size ≠ global size, branching on array contents, and irregular memory access
3. Compare sampling strategies: Evaluate alternative work-group sampling approaches (uniform, importance-weighted, or based on different utilization thresholds)