---
ver: rpa2
title: Self-attention vector output similarities reveal how machines pay attention
arxiv_id: '2512.21956'
source_url: https://arxiv.org/abs/2512.21956
tags:
- attention
- context
- similarity
- token
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a new method for quantifying information
  processing in transformer architectures by analyzing the vector space emerging from
  self-attention layers. The authors developed a context similarity matrix that measures
  the scalar product between token vectors output by each attention head, providing
  a more fundamental view of how tokens are transformed as they propagate through
  the architecture.
---

# Self-attention vector output similarities reveal how machines pay attention

## Quick Facts
- arXiv ID: 2512.21956
- Source URL: https://arxiv.org/abs/2512.21956
- Authors: Tal Halevi; Yarden Tzach; Ronit D. Gross; Shalom Rosner; Ido Kanter
- Reference count: 0
- Introduces context similarity matrices as a novel method to quantify information processing in transformers

## Executive Summary
This study introduces a new method for quantifying information processing in transformer architectures by analyzing the vector space emerging from self-attention layers. The authors developed a context similarity matrix that measures the scalar product between token vectors output by each attention head, providing a more fundamental view of how tokens are transformed as they propagate through the architecture.

The analysis was conducted on BERT-12 using 20,000 text samples. Key findings include: (1) In final layers, attention maps focus on sentence separator tokens, suggesting a practical approach to text segmentation based on semantic features; (2) Different attention heads within the same layer specialize in different linguistic characteristics - some identify token repetitions while others recognize tokens of common appearance and their surrounding context; (3) As layers progress, context similarity shifts from long-range to short-range patterns, with final layers showing preference for strong similarities within the same sentence; (4) Each head tends to focus on a unique token from the text and builds similarity pairs centered around it.

## Method Summary
The method extracts individual attention head outputs from BERT-12 (128×64 dimensions per head), computes context similarity matrices by taking dot products between token vectors, and analyzes the resulting 128×128 matrices to reveal how information is processed across layers and heads. The approach zeros diagonal elements to focus on off-diagonal similarities and uses attention-map-derived masks to identify high-attention regions.

## Key Results
- Final layers show attention maps focused on sentence separator tokens, enabling semantic text segmentation
- Attention heads specialize in different linguistic characteristics - some detect repetitions while others recognize contextually common tokens
- Layer progression shows systematic shift from long-range to short-range similarities, with final layers preferring same-sentence correlations
- Each head typically focuses on 9-10 unique tokens across text samples, building similarity clusters around these focal points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Computing context similarity matrices from attention head outputs reveals information processing patterns that attention maps alone cannot capture.
- Mechanism: For each attention head, multiply the output matrix (SequenceLength × embedding_dim/heads) by its transpose to produce a 128×128 context similarity matrix: `ContextSimilarityMatrix = headOutput · headOutput^T`. Higher values indicate stronger alignment between token vector representations.
- Core assumption: The output vector space encodes learned relationships beyond what attention weights express.
- Evidence anchors:
  - [abstract] "measuring the scalar product between two token vectors was derived, revealing distinct similarities between different token vector pairs within each head and layer"
  - [section 2] "Unlike the attention weights in the attention map, this places greater emphasis on the input alterations of the token vector space"
  - [corpus] Weak corpus support—no directly comparable context similarity methodology found in neighbors
- Break condition: If diagonal elements dominate all off-diagonal similarity signals, the method provides no additional insight over attention maps.

### Mechanism 2
- Claim: Transformer layers exhibit a systematic progression from broad, long-range similarities to sentence-local correlations.
- Mechanism: Early layers distribute context similarity across the full sequence. Middle layers concentrate similarity along the diagonal (local token relationships). Final layers expand similarity within sentence boundaries while suppressing cross-sentence similarity.
- Core assumption: Layer depth correlates with syntactic abstraction level.
- Evidence anchors:
  - [abstract] "initial layers exhibit substantially long-range similarities; however, as the layers progress, a more short-range similarity develops, culminating in a preference for attention heads to create strong similarities within the same sentence"
  - [section 4, Fig. 12-13] Shows summed context similarity matrices at layers 1, 6, and 11 with corresponding distance distributions
  - [corpus] No direct corroboration; corpus contains tangential attention dynamics work but no layer-wise similarity progression studies
- Break condition: If middle layers fail to show diagonal concentration, the proposed progression pattern does not hold for the architecture.

### Mechanism 3
- Claim: Individual attention heads specialize on unique tokens per input, building similarity clusters around their focal token.
- Mechanism: Each head identifies a "most common token" appearing in its high-similarity elements. The head amplifies relationships between this token and others, with different heads selecting different focal tokens (averaging ~9-10 unique tokens across 12 heads).
- Core assumption: Head specialization is a learned property, not random assignment.
- Evidence anchors:
  - [abstract] "Each attention head tends to specialize in a unique token from the text and builds similarity pairs centered around it"
  - [section 3, Fig. 11] "Probability distribution function of the number of unique most common tokens...averaged over 1,000 text examples" shows specialization pattern
  - [corpus] Weak support—"Unified CNNs and transformers" (cited in paper) discusses multi-head modus vivendi but with different methodology
- Break condition: If all heads converge to the same focal token across samples, specialization is not occurring.

## Foundational Learning

- Concept: Self-attention Q/K/V computation
  - Why needed here: The context similarity matrix builds on attention head outputs, which derive from Query × Key attention weights applied to Value vectors.
  - Quick check question: Can you explain why attention weights alone might not explain learned representations?

- Concept: Dot product as similarity measure
  - Why needed here: The paper's core contribution uses unnormalized dot products to measure vector alignment and magnitude jointly.
  - Quick check question: Why does the diagonal need to be zeroed to interpret off-diagonal similarities?

- Concept: Multi-head attention specialization
  - Why needed here: Understanding that heads develop distinct roles (e.g., repetition detection vs. contextual recognition) is essential for interpreting results.
  - Quick check question: If two heads always produce identical similarity patterns, what would that suggest about the model?

## Architecture Onboarding

- Component map: Input: 128-token sequences → 768-dim embeddings → 12 transformer layers → 12 attention heads per layer → Context similarity matrix per head

- Critical path:
  1. Extract attention head outputs (pre-concatenation, pre-projection)
  2. Compute `headOutput @ headOutput.T` for each head
  3. Zero diagonal to highlight off-diagonal similarities
  4. Apply attention-map-derived masks (using threshold 0.3 on normalized column sums) to delineate high-attention regions
  5. Aggregate across samples for statistical patterns

- Design tradeoffs:
  - Using unnormalized vectors preserves magnitude information but makes similarity scale-dependent
  - Threshold 0.3 for high-attention selection is empirically derived (Fig. A1) and may not transfer to other architectures
  - Fixed 128-token length limits analysis of longer-range dependencies

- Failure signatures:
  - Context similarity matrices appearing uniform/noisy → check output extraction point
  - All heads showing identical patterns → verify heads are being analyzed separately
  - No sentence-boundary clustering in final layers → may indicate model not trained sufficiently or different tokenization

- First 3 experiments:
  1. Reproduce Fig. 12: Generate summed context similarity matrices for layers 1, 6, and 11 on 100+ samples to verify layer-wise progression.
  2. Reproduce Fig. 7: Compute distance histograms for high-similarity pairs per head to identify long-range vs. short-range specialization.
  3. Test generalization: Apply the same methodology to a different BERT variant (e.g., DistilBERT) to assess pattern transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can contextual comprehension of long-range correlations be effectively achieved in transformer architectures?
- Basis in paper: [explicit] The discussion explicitly asks, "how can contextual comprehension of long-range correlations be achieved?" while noting the model currently captures only medium-range correlations.
- Why unresolved: Current architectures act primarily on short-range interactions similar to physical spin-glass models, lacking mechanisms for direct interactions over long textual ranges.
- What evidence would resolve it: The development and validation of architectures capable of direct long-range information interaction, potentially mimicking biological learning systems.

### Open Question 2
- Question: What quantifiable language can accurately describe the Signal-to-Noise Ratio (SNR) within the self-attention vector space?
- Basis in paper: [explicit] The authors state, "the need for a quantifiable language for Signal-to-Noise ratio (SNR) persists," noting that current vector spaces contain noise elements not explained by linguistic comprehension.
- Why unresolved: While context similarity matrices offer a new framework, translating tokenized accuracy into a vectorized signal-to-noise metric requires further research.
- What evidence would resolve it: A defined mathematical metric or framework that successfully distinguishes linguistic signal propagation from noise in the vector space.

### Open Question 3
- Question: Is the behavior of the context similarity matrix universal across different transformer architectures?
- Basis in paper: [explicit] The conclusion states that "a more comprehensive analysis of the context similarity matrices of other architectures is required in order to assess its universality."
- Why unresolved: This study restricted its analysis to the BERT-12 architecture, leaving the behavior of encoder-decoder or other architectures unknown.
- What evidence would resolve it: Comparative studies applying the context similarity matrix method to diverse architectures (e.g., translation systems, generative models) to test for consistent features.

### Open Question 4
- Question: Are current attention-mechanism architectures optimal, or can more efficient alternatives be developed?
- Basis in paper: [explicit] The authors ask, "whether the presented attention-mechanism architecture is optimal" and call for "developing more efficient and smarter alternatives to advanced attention techniques."
- Why unresolved: High complexity, costs, and the presence of noise in current large embedding dimensions suggest potential inefficiencies in learning representations.
- What evidence would resolve it: The design of architectures that reduce latency and memory usage via efficient pruning or noise mitigation without performance loss.

## Limitations

- Context similarity methodology lacks direct methodological precedents in literature, limiting reproducibility
- Fixed 128-token limit constrains analysis of longer-range dependencies that might exist in larger contexts
- Empirical threshold values (0.3 for attention maps, unclear for similarity matrices) appear without systematic justification

## Confidence

- **High confidence**: The technical mechanism of computing dot-product similarity matrices from attention head outputs is mathematically sound and clearly specified. The observation that final layers focus on separator tokens is empirically demonstrated with clear visual evidence.
- **Medium confidence**: The systematic progression from long-range to short-range similarities across layers is well-supported by Fig. 12-13 but relies on aggregated statistics rather than individual sample analysis. The claim about heads specializing in unique tokens (averaging 9-10 unique tokens) shows statistical patterns but could reflect random variation.
- **Low confidence**: The assertion that this methodology provides a "quantifiable language for assessing learning performance" remains largely aspirational. The paper acknowledges the need for "further research to develop a comprehensive signal-to-noise ratio framework," suggesting the current method is exploratory rather than definitive.

## Next Checks

1. **Layer-wise similarity progression replication**: Generate context similarity matrices for layers 1, 6, and 11 on a held-out test set (minimum 100 samples) to verify the systematic shift from long-range to sentence-local correlations. Compare distance distributions of high-similarity pairs to confirm diagonal concentration in middle layers and sentence-boundary expansion in final layers.

2. **Head specialization stability test**: Track focal token selection across multiple random subsets of the dataset (e.g., 5,000-sample subsets). Measure the consistency of which heads select which tokens as their "most common" focal points. Test whether specialization patterns persist when analyzing different text domains or when applying the method to DistilBERT.

3. **Threshold sensitivity analysis**: Systematically vary the attention threshold (0.3) and similarity threshold values across a range (0.1-0.5) to assess robustness of key findings. Document how sentence-boundary detection, head specialization counts, and layer-wise progression patterns change with different threshold settings.