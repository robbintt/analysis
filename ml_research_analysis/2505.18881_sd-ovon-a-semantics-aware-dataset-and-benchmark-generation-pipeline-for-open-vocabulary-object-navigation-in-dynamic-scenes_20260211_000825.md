---
ver: rpa2
title: 'SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary
  Object Navigation in Dynamic Scenes'
arxiv_id: '2505.18881'
source_url: https://arxiv.org/abs/2505.18881
tags:
- object
- navigation
- semantic
- objects
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SD-OVON, a pipeline that generates photo-realistic
  dynamic scenes for training and evaluating open-vocabulary object navigation agents.
  The pipeline uses pretrained visual-language models to place manipulatable objects
  on appropriate receptacles in scanned real-world environments, adhering to daily
  commonsense and semantics.
---

# SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes

## Quick Facts
- arXiv ID: 2505.18881
- Source URL: https://arxiv.org/abs/2505.18881
- Authors: Dicong Qiu; Jiadi You; Zeying Gong; Ronghe Qiu; Hui Xiong; Junwei Liang
- Reference count: 40
- Primary result: Semantics-aware pipeline generates dynamic scenes improving navigation success rates up to 14.17% vs 0.0204% for vision-only baselines

## Executive Summary
This paper introduces SD-OVON, a pipeline that generates photo-realistic dynamic scenes for training and evaluating open-vocabulary object navigation agents. The pipeline uses pretrained visual-language models to place manipulatable objects on appropriate receptacles in scanned real-world environments, adhering to daily commonsense and semantics. It integrates with the Habitat simulator and provides two pre-generated datasets (3k and 10k episodes) derived from 2.5k scans of real-world environments and 0.9k manipulatable object models. The method enables generation of infinite unique scene variants, covering dynamic environments unlike prior static datasets.

## Method Summary
The SD-OVON pipeline extracts open-vocabulary instances from RGB-D observations using Grounded SAM, fuses multi-view instances via spatial-semantic proximity, identifies receptacle surfaces via EM plane detection, and scores object-receptacle-region relevance using LLM prompts. Objects are then sampled and placed with physics simulation to settle into stable configurations. Two navigation baselines (Random Receptacle Navigation A* and Semantic Navigation A*) leverage environmental semantics for improved navigation success rates. The pipeline generates two datasets (SD-OVON-3k with 3,000 episodes and SD-OVON-10k with 10,000 episodes) and is publicly available.

## Key Results
- SD-OVON-3k evaluation shows Semantic A* achieves 14.17% success rate vs 0.0204% for VLFM vision-only baseline
- Random A* baseline achieves 9.74% success rate, demonstrating benefit of semantic reasoning
- L3MVN achieves 1.59% success rate, confirming vision-only methods struggle with dynamic scenes
- Detection quality significantly impacts performance: GT detector boosts Semantic A* SR from 14.17% to 80.81%

## Why This Works (Mechanism)

### Mechanism 1: Semantics-Aware Object Placement
- Claim: VLM/LLM-guided object placement produces scenes that follow everyday commonsense, enabling more realistic training and evaluation of navigation agents
- Mechanism: The pipeline extracts open-vocabulary instances from RGB-D observations using Grounded SAM, fuses multi-view instances via spatial-semantic proximity, identifies receptacle surfaces via EM plane detection, and scores object-receptacle-region relevance using LLM prompts (e.g., "dumbbell" → desk: 9, bed: 2). Objects are then sampled and placed with physics simulation to settle into stable configurations
- Core assumption: Pretrained VLM/LLM knowledge of everyday object-location associations transfers meaningfully to simulated scene arrangement and improves downstream task realism
- Evidence anchors:
  - [abstract] "utilizes pretraining multimodal foundation models to generate infinite unique photo-realistic scene variants that adhere to real-world semantics and daily commonsense"
  - [section 3.4] Equation 5 defines joint object-region-receptacle relevance P(l_obj|l_rgn,l_rec) derived from LLM scores
  - [corpus] Weak direct evidence—neighbor papers (osmAG-LLM, DualMap) use semantic maps for navigation but do not validate procedural object placement realism
- Break condition: If target environments contain domain-specific object arrangements (e.g., hospital ICUs, industrial warehouses) underrepresented in LLM training, commonsense priors may misplace objects

### Mechanism 2: Dynamic Scene Variant Generation from Static Scans
- Claim: Combining static environment scans with manipulatable object models enables scalable generation of diverse, physically plausible dynamic scenes
- Mechanism: Receptacle planes are extracted from fused point clouds via Algorithm 1 (EM plane detection). Objects are uniformly sampled on convex polygonal surfaces (Algorithm 2), spawned at height 0.3m, rotated randomly, and settled via physics simulation. Collision constraints prevent overlap
- Core assumption: Photo-realistic scans of environments/objects and physics-constrained placement yield scene variants transferable to real-world robotics (real-to-sim and sim-to-real)
- Evidence anchors:
  - [abstract] "derived from the SD-OVON-Scenes dataset with 2.5k photo-realistic scans of real-world environments and the SD-OVON-Objects dataset with 0.9k manually inspected scanned and artist-created manipulatable object models"
  - [section 3.3-3.4] Physics simulation ensures "objects settle in physically feasible positions"
  - [corpus] Weak—neighbor papers do not address procedural scene generation; focus on semantic mapping and navigation
- Break condition: If articulated receptacles (drawers, cabinets) or multi-object interactions (stacking, containment) are required, the current plane-based placement fails

### Mechanism 3: Environmental Semantic Prior Knowledge Improves Navigation
- Claim: Agents with prior knowledge of environment semantics (receptacle locations + region semantics) significantly outperform those without in dynamic settings
- Mechanism: Semantic A* builds region-receptacle semantic maps during initial exploration, then prioritizes receptacles by P(l_obj|l_rgn,l_rec) relevance scores. Random A* randomly orders receptacles. SOTA baselines (VLFM, L3MVN) lack persistent semantic memory across episodes
- Core assumption: In dynamic environments where objects move but structure persists, storing and reusing semantic priors reduces search space
- Evidence anchors:
  - [abstract] "incorporating environmental semantics significantly improves navigation success rates (up to 14.17% vs 0.0204% for VLFM)"
  - [section 5.2, Table 3] Semantic A*: 14.17% SR; Random A*: 9.74%; VLFM: 2.04%; L3MVN: 1.59%
  - [corpus] Consistent—osmAG-LLM and DualMap also leverage semantic maps for navigation, though evaluated on different tasks
- Break condition: If object placements consistently violate semantic priors (e.g., cups on beds, pillows on tables), relevance-based prioritization degrades toward random search

## Foundational Learning

- Concept: **Open-Vocabulary 3D Instance Segmentation and Fusion**
  - Why needed here: Pipeline requires extracting and merging object instances from multi-view RGB-D observations to build scene representations
  - Quick check question: Can you explain how Grounded SAM generates instance masks from text prompts, and how spatial-semantic similarity (Eq. 3) fuses instances across views?

- Concept: **LLM-based Semantic Relevance Scoring**
  - Why needed here: Object placement and navigation prioritization depend on LLM-generated relevance scores between objects, receptacles, and regions
  - Quick check question: Given a target object "laptop," how would you use LLM prompts (Appendix C.3-C.4) to rank receptacle-region pairs?

- Concept: **A* Navigation with Semantic Priors**
  - Why needed here: Both Random A* and Semantic A* build on frontier-based exploration + A* path planning, with different receptacle ordering strategies
  - Quick check question: What modifications to standard A* are needed to incorporate semantic memory M(s) for prioritizing navigation points?

## Architecture Onboarding

- Component map: Observation Sampling -> Instance Extraction -> Instance Fusion -> Receptacle Plane Detection -> Semantic Map Generation -> Object Placement -> Physics Settlement -> Episode Generation -> Navigation

- Critical path:
  1. Environment scans → Observation sampling → Instance extraction/fusion → Receptacle plane detection
  2. Semantic map generation → LLM relevance scoring → Object placement → Physics settlement
  3. Episode generation → Goal viewpoint validation → Dataset output (SD-OVON-3k/10k)
  4. For navigation: Initial exploration → Semantic memory construction → Per-episode prioritized search

- Design tradeoffs:
  - **Scanned vs synthetic objects**: Scanned (YCB, GSO) provide realism but limited categories; synthetic (AI2-THOR, HSSD) expand coverage but may lack visual fidelity
  - **Coverage threshold vs sampling iterations**: Higher coverage thresholds increase observation quality but extend preprocessing time (Section 3.1)
  - **Semantic vs random receptacle ordering**: Semantic A* requires upfront exploration cost; Random A* has lower overhead but lower SR
  - **Detector quality**: Ablation (Table 4) shows GT detector boosts SR from 14.17% to 80.81% for Semantic A*, indicating detector bottleneck

- Failure signatures:
  - **Low instance fusion IoU threshold**: Over-merging dissimilar instances (Section 3.2 error correction)
  - **Plane detection failure on cluttered surfaces**: EM algorithm requires sufficient inlier ratio ρ_min (Algorithm 1)
  - **Invisible goal objects**: Filtered out during episode generation if no valid viewpoint exists (Section 3.5)
  - **Detector misses small objects**: VLFM/L3MVN favor large objects (boxes, cushions); Semantic A* handles small items better (Table 5)

- First 3 experiments:
  1. **Validate instance extraction on held-out scene**: Sample observations from SD-OVON-Scenes subset, run Grounded SAM + fusion, measure instance recall and label accuracy against manual annotations
  2. **Ablate semantic relevance weighting**: Run Semantic A* with shuffled vs LLM-ranked receptacle orders; confirm SR gap matches Table 3
  3. **Test cross-domain transfer**: Generate episodes using objects from GSO (unseen during SD-OVON-Objects curation); evaluate Semantic A* SR drop to quantify domain sensitivity

## Open Questions the Paper Calls Out
None

## Limitations
- Realism of LLM-guided commonsense priors unverified against real-world object distributions
- Physics-based placement assumes rigid-body interactions without modeling articulated receptacles or complex multi-object configurations
- Evaluation limited to small and medium objects, with large objects handled only by vision-only SOTA methods

## Confidence
- **High**: Dynamic scene generation from static scans + physics simulation - supported by explicit equations and ablation
- **Medium**: Semantics-aware object placement improves navigation - correlation observed but causality not isolated from detector improvements
- **Low**: LLM commonsense priors transfer to real-world object placement - no real-world validation provided

## Next Checks
1. Evaluate Semantic A* on a held-out subset of SD-OVON-Scenes with manually annotated ground-truth object locations to isolate detector vs semantic reasoning contributions
2. Generate episodes with out-of-distribution receptacle types (e.g., medical equipment, industrial shelves) and measure object placement realism via human preference study
3. Test cross-dataset transfer by evaluating trained Semantic A* agents on AI2-THOR or RoboTHOR to assess sim-to-real gap