---
ver: rpa2
title: 'Reasoning''s Razor: Reasoning Improves Accuracy but Can Hurt Recall at Critical
  Operating Points in Safety and Hallucination Detection'
arxiv_id: '2510.21049'
source_url: https://arxiv.org/abs/2510.21049
tags:
- think
- reasoning
- classification
- safety
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically evaluates reasoning's impact on classification\
  \ tasks under strict low false positive rate (FPR) regimes. The authors compare\
  \ two inference paradigms\u2014Think On (with reasoning) and Think Off (without\
  \ reasoning)\u2014across safety detection and hallucination detection tasks."
---

# Reasoning's Razor: Reasoning Improves Accuracy but Can Hurt Recall at Critical Operating Points in Safety and Hallucination Detection

## Quick Facts
- **arXiv ID**: 2510.21049
- **Source URL**: https://arxiv.org/abs/2510.21049
- **Reference count**: 40
- **Key outcome**: Reasoning improves overall accuracy but degrades recall at low false positive rate (FPR) thresholds, particularly at FPR ≤ 1%, due to polarization effects that make incorrect predictions appear overly confident.

## Executive Summary
This paper systematically evaluates the trade-off between reasoning and recall performance in classification tasks under strict low FPR constraints. The authors compare two inference paradigms—Think On (with reasoning) and Think Off (without reasoning)—across safety detection and hallucination detection tasks. Their key finding reveals that while reasoning improves overall accuracy, it significantly degrades recall at critical operating points (e.g., FPR ≤ 1%). The authors attribute this to reasoning's polarization effect, where incorrect predictions become increasingly overconfident during reasoning chains. Token-based scoring substantially outperforms self-verbalized confidence at low FPR thresholds. Ensembling both modes recovers the strengths of each approach, achieving both high accuracy and practical low-FPR recall.

## Method Summary
The study fine-tunes Llama-3-8B on the GuardReasoner dataset (87K examples) for 3 epochs with learning rate 2e-4 on 8×A100 GPUs. Two inference modes are evaluated: Think On (generate reasoning before classification) and Think Off (direct classification). Token-based scoring extracts logits for class tokens after appending partial JSON, while self-verbalized confidence prompts the model to output a confidence number. The primary metric is TPR@FPR=α (recall at constrained FPR thresholds of 1%, 3%, 5%), with secondary metrics including accuracy, AUROC, and greedy FPR. An ensemble approach averages Think On and Think Off scores with equal weighting.

## Key Results
- Reasoning improves average accuracy but reduces recall at critical operating points, with fine-tuned safety classifiers achieving 40.0% recall without reasoning but only 13.8% with reasoning at 1% FPR
- Token-based scoring substantially outperforms self-verbalized confidence at low FPR thresholds (verbalized confidence achieves zero recall on several datasets at 1% FPR)
- Ensembling Think On and Think Off scores recovers both high accuracy and practical low-FPR recall, matching or exceeding the best individual mode across datasets
- Reasoning polarizes model confidence toward extreme values, making incorrect high-confidence predictions indistinguishable from correct ones under strict FPR constraints

## Why This Works (Mechanism)

### Mechanism 1
Reasoning polarizes model confidence toward extreme values, degrading discriminability at low FPR thresholds. As reasoning chains progress, token-level class probabilities are repeatedly sampled and reinforced; the model amplifies its initial hunch, converging toward near-certainty even for incorrect predictions. This makes erroneous high-confidence predictions indistinguishable from correct ones when FPR is constrained.

### Mechanism 2
Polarization produces heavier distribution tails, increasing false-positive overlap with the high-confidence region required for low-FPR operation. Heavier tails mean more safe/faithful examples receive high positive-class scores; at strict thresholds (e.g., FPR=1%), these crowd out true positives, reducing TPR.

### Mechanism 3
Ensembling Think On and Think Off confidence scores recovers both high accuracy and practical low-FPR recall. Think On and Think Off modes make complementary errors; averaging their scores smooths confidence distributions and reduces tail heaviness while preserving accuracy gains where reasoning helps.

## Foundational Learning

- **Operating-point metrics (TPR@FPR=α)**: Why needed - Average accuracy or AUROC can mask catastrophic performance at deployment-critical low-FPR thresholds. Quick check - "At 1% FPR, what recall can we achieve?" vs. "What's the overall accuracy?"
- **Confidence calibration and polarization**: Why needed - The paper shows reasoning systematically shifts confidence distributions; understanding calibration is essential for threshold-based deployment. Quick check - "Are incorrect predictions made with high or low confidence? Is the confidence distribution polarized or moderate?"
- **Token-based vs. self-verbalized confidence scoring**: Why needed - Token-based scoring substantially outperforms self-verbalized confidence at low FPR; the two methods are not interchangeable. Quick check - "How are confidence scores extracted—via token logits or by asking the model to state a number?"

## Architecture Onboarding

- **Component map**: Think On (reasoning → classification) -> Token-based scoring OR Self-verbalized confidence -> Threshold-based classification vs. Think Off (direct classification) -> Token-based scoring OR Self-verbalized confidence -> Threshold-based classification vs. Ensemble (average Think On and Think Off scores) -> Threshold-based classification
- **Critical path**: 1) Determine acceptable FPR constraint (e.g., 1%, 3%, 5%) 2) If FPR ≤ 1%: prefer Think Off or ensemble; avoid Think On alone 3) Always use token-based scoring for precision-sensitive tasks 4) Evaluate TPR@FPR=α across multiple thresholds before deployment
- **Design tradeoffs**: Accuracy vs. low-FPR recall: Think On improves average accuracy; Think Off or ensemble improves TPR@low-FPR. Interpretability vs. precision: Self-verbalized confidence is interpretable but fails catastrophically at 1% FPR. Compute cost: Ensemble requires running both modes, doubling inference cost
- **Failure signatures**: Zero TPR at 1% FPR (especially with verbalized scoring), high greedy FPR (15–17%) despite acceptable accuracy, confidence scores clustering at extremes (0.0–0.1, 0.9–1.0) rather than spread across range
- **First 3 experiments**: 1) Compare token-based vs. self-verbalized confidence on your classification task at multiple FPR thresholds (1%, 3%, 5%) 2) Plot confidence distributions for Think On vs. Think Off to diagnose polarization and tail heaviness 3) Test ensemble: average Think On and Think Off scores; measure accuracy and TPR@FPR=1% across datasets to confirm recovery of both strengths

## Open Questions the Paper Calls Out

- **Question**: Does reasoning-augmented inference degrade low-FPR recall in classification tasks beyond safety and hallucination detection, such as medical diagnosis, fraud detection, or content moderation for other categories? Basis - "broader evaluation across domains and reasoning paradigms... is needed to assess generality"
- **Question**: How do prompt templates, decoding parameters, and context length interact with reasoning's effect on confidence polarization? Basis - "the experimental design isolates reasoning effects by fixing prompt templates, decoding parameters, and context length; potential interactions among these factors remain unexplored"
- **Question**: Can reasoning models be explicitly trained or fine-tuned to produce well-calibrated confidence distributions that avoid polarization at the extremes? Basis - The paper identifies polarization as the mechanism but only proposes ensemble as a post-hoc remedy, stating "alternative approaches warrant further investigation"

## Limitations

- The study focuses on Llama-3-8B and similar LRMs, with unknown performance for smaller or larger models
- The polarization mechanism is well-documented for reasoning chains but may not extend to all forms of deliberation or chain-of-thought
- The ensemble approach requires doubling inference cost, which may be prohibitive in resource-constrained settings

## Confidence

- **High Confidence**: The polarization mechanism causing extreme confidence scores, the superiority of token-based scoring over self-verbalized confidence at low FPR thresholds, and the fundamental trade-off between accuracy and low-FPR recall
- **Medium Confidence**: The ensemble approach's effectiveness across all scenarios and the claim that polarization is primarily driven by reasoning dynamics (not model scale)
- **Low Confidence**: The assertion that polarization is exclusively a reasoning phenomenon and may not be complete

## Next Checks

1. **Model Scale Ablation**: Test the same experimental protocol across different model sizes (e.g., 7B, 13B, 70B parameters) to verify whether polarization effects scale with model capacity or are inherent to the reasoning mechanism itself
2. **Reasoning Strategy Variation**: Compare polarization effects across different reasoning prompt structures (structured templates vs. free-form chain-of-thought) to determine if the mechanism is universal or prompt-dependent
3. **Cost-Benefit Analysis of Ensembling**: Measure the actual inference latency and computational cost of the ensemble approach across different hardware configurations, and compare the cost-per-unit-accuracy improvement against simpler alternatives like threshold adjustment or model selection