---
ver: rpa2
title: 'Leading the Follower: Learning Persuasive Agents in Social Deduction Games'
arxiv_id: '2510.09087'
source_url: https://arxiv.org/abs/2510.09087
tags:
- player
- game
- players
- response
- werewolf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of persuasive communication
  in social deduction games (SDGs) by formalizing turn-based dialogue as a Stackelberg
  competition where the current player acts as a leader who strategically influences
  the follower's response. The authors propose a reinforcement learning framework
  that trains agents to optimize utterances for persuasive impact by measuring how
  different utterances shift the probability distribution of subsequent players' responses.
---

# Leading the Follower: Learning Persuasive Agents in Social Deduction Games

## Quick Facts
- arXiv ID: 2510.09087
- Source URL: https://arxiv.org/abs/2510.09087
- Reference count: 33
- Primary result: Reinforcement learning framework that treats SDG dialogue as Stackelberg competition achieves higher win rates by optimizing persuasive utterances

## Executive Summary
This paper addresses the challenge of persuasive communication in social deduction games by formalizing turn-based dialogue as a Stackelberg competition where the current player acts as a leader who strategically influences the follower's response. The authors propose a reinforcement learning framework that trains agents to optimize utterances for persuasive impact by measuring how different utterances shift the probability distribution of subsequent players' responses. Through extensive experiments across Werewolf, Avalon, and ONUW, their approach significantly outperforms baselines, achieving higher win rates by effectively guiding conversations and influencing other players' behaviors. The improvements are particularly notable when integrated with stronger baseline methods, demonstrating that the framework complements rather than replaces existing strategies while adding crucial persuasive communication capabilities.

## Method Summary
The authors formalize SDG dialogue as a Stackelberg game where the leader (current player) makes strategic utterances to influence the follower's (next player's) response. They train persuasive agents using reinforcement learning that optimizes utterances based on their impact on shifting response distributions. The framework measures persuasion effectiveness by analyzing how different utterances affect the probability distribution of subsequent players' responses. Experiments were conducted across three social deduction games - Werewolf, Avalon, and ONUW - comparing against various baseline methods. The approach is designed to complement existing strategic methods while adding persuasive communication capabilities.

## Key Results
- Proposed Stackelberg-based reinforcement learning framework significantly outperforms baseline methods in social deduction games
- Achieves higher win rates by effectively guiding conversations and influencing other players' behaviors
- Performance improvements are particularly notable when integrated with stronger baseline methods, demonstrating complementary capabilities

## Why This Works (Mechanism)
The framework works by treating persuasive communication as a strategic leadership problem where the current player's utterances are optimized to maximize their influence on subsequent players' responses. By modeling this as a Stackelberg game, the approach captures the sequential nature of SDG dialogue where players must anticipate and influence how others will react to their statements. The reinforcement learning component allows agents to discover optimal persuasive strategies through interaction with the game environment, learning which types of utterances most effectively shift other players' behavior in desired directions.

## Foundational Learning

**Stackelberg Competition**: A sequential game theory model where one player (leader) moves first and the other (follower) responds optimally. Needed because SDGs involve strategic turn-based communication where current actions influence future responses. Quick check: Verify that the game's turn structure matches Stackelberg assumptions.

**Reinforcement Learning**: Learning through interaction with an environment to maximize cumulative reward. Needed to optimize persuasive utterances based on their impact on game outcomes. Quick check: Confirm that reward signals appropriately capture persuasive success.

**Social Deduction Game Dynamics**: Games where players must deduce hidden roles while concealing their own information. Needed because persuasion strategies must account for uncertainty about other players' roles and intentions. Quick check: Validate that persuasion strategies work across different role distributions.

## Architecture Onboarding

**Component Map**: Agent -> Stackelberg Model -> Reinforcement Learning Policy -> Utterance Generator -> Game Environment -> Response Observer -> Reward Calculator

**Critical Path**: The reinforcement learning policy learns to select utterances that maximize persuasion impact, which is measured by how much they shift the probability distribution of follower responses in favorable directions.

**Design Tradeoffs**: The framework prioritizes persuasive effectiveness over other communication goals, which may limit its ability to handle scenarios requiring cooperative rather than deceptive communication. The Stackelberg assumption may not fully capture the complexity of human social reasoning in SDGs.

**Failure Signatures**: Poor performance when opponents use highly unpredictable strategies, overfitting to specific game dynamics, or when the reinforcement learning fails to generalize beyond training scenarios.

**First Experiments**: 1) Test persuasion effectiveness on controlled opponent types, 2) Measure win rate improvements against baseline methods, 3) Evaluate framework performance across different SDG variants and player counts.

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Limited generalizability across different SDG variants beyond the three tested games
- No ablation studies on the relative contribution of persuasive versus strategic components
- Absence of human evaluation studies to validate that LLM opponents exhibit realistic social reasoning

## Confidence
The paper's claims are evaluated with Medium confidence due to several methodological limitations and gaps in validation. The experimental design relies heavily on simulated opponent environments rather than human-human or human-AI interaction data, raising questions about ecological validity of the persuasive effectiveness claims. The Stackelberg formulation assumes deterministic follower responses to leader utterances, but real social deduction games involve probabilistic reasoning under uncertainty that isn't fully captured. The evaluation metrics focus primarily on win rates and persuasion shift scores without qualitative analysis of conversation quality or interpretability of agent strategies. The training methodology depends on large language models with unspecified hyperparameters and fine-tuning details that could significantly impact reproducibility. Additionally, the paper doesn't address potential biases in training data or safety considerations around deceptive agent behaviors.

## Next Checks
1. Conduct human subject studies comparing persuasive agents against human baselines in controlled experiments
2. Perform ablation analysis to isolate the contribution of the Stackelberg leadership component versus other strategic elements
3. Test the framework's robustness across additional SDG variants and player count variations to assess generalizability