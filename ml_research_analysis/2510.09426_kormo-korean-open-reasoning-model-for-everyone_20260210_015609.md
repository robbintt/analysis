---
ver: rpa2
title: 'KORMo: Korean Open Reasoning Model for Everyone'
arxiv_id: '2510.09426'
source_url: https://arxiv.org/abs/2510.09426
tags:
- data
- korean
- english
- training
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KORMo introduces the first fully open bilingual Korean-English
  LLM, trained predominantly on synthetic data (68.74% of Korean portion). Through
  systematic experiments, we demonstrate that synthetic data, when carefully curated
  with balanced linguistic coverage and diverse instruction styles, does not cause
  instability or degradation during large-scale pretraining.
---

# KORMo: Korean Open Reasoning Model for Everyone

## Quick Facts
- arXiv ID: 2510.09426
- Source URL: https://arxiv.org/abs/2510.09426
- Reference count: 37
- Primary result: First fully open bilingual Korean-English LLM trained predominantly on synthetic data, achieving performance comparable to contemporary open-weight multilingual baselines

## Executive Summary
KORMo introduces the first fully open bilingual Korean-English LLM, trained predominantly on synthetic data (68.74% of Korean portion). Through systematic experiments, we demonstrate that synthetic data, when carefully curated with balanced linguistic coverage and diverse instruction styles, does not cause instability or degradation during large-scale pretraining. The resulting 10.8B-parameter model achieves performance comparable to contemporary open-weight multilingual baselines across a wide range of reasoning, knowledge, and instruction-following benchmarks. By fully releasing all components including data, code, training recipes, and logs, this work establishes a transparent framework for developing synthetic data-driven fully open models in low-resource settings and sets a reproducible precedent for future multilingual LLM research.

## Method Summary
KORMo uses a 10.8B-parameter bilingual Korean-English LLM architecture with Pre-LN + RMSNorm + SwiGLU + RoPE, trained on 3,462B total tokens across English and Korean data sources. The training pipeline consists of two-stage pretraining (Stage 1: 1T web-heavy tokens, Stage 2: 1.8T high-quality + synthetic tokens), followed by mid-training (10.3B long-context + 157.8B reasoning tokens) and SFT (8.02B tokens). The model employs intra-document attention masking, GQA with 8 KV heads, and a 125K vocab byte-level BPE tokenizer trained on 80% English synthetic, 5% Korean crawled, and 15% Korean synthetic data. Training uses 128×H200 GPUs with FSDP in bfloat16.

## Key Results
- Synthetic data can sustain long-horizon pretraining without model collapse when diversity constraints are applied
- Bilingual tokenizer design with 60-80% synthetic data proportion achieves superior compression for non-Latin scripts
- Staged curriculum with explicit difficulty-based filtering transfers reasoning capability cross-lingually
- Model achieves performance comparable to contemporary open-weight multilingual baselines across reasoning, knowledge, and instruction-following benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data can sustain long-horizon pretraining without model collapse when diversity constraints are applied.
- Mechanism: Multiple synthesizers (Qwen, GPT-OSS variants) combined with varied seed pools and prompt styles prevent the self-consuming loop that causes distributional narrowing. The paper explicitly shows that single-synthesizer training caused irreversible degradation in Stage 2, while multi-source synthetic data maintained stable convergence.
- Core assumption: Model collapse arises from repeated exposure to narrow generative distributions, not from synthetic data per se.
- Evidence anchors: [abstract] "synthetic data can reliably sustain long-horizon pretraining without model collapse"; [section 4.3] Stage 2 failure case: single-synthesizer Korean data led to performance dropping to early Stage 1 levels.
- Break condition: If synthetic data comes from a single model with uniform prompt templates, collapse occurs regardless of volume.

### Mechanism 2
- Claim: Bilingual tokenizer design with 60-80% synthetic data proportion achieves superior compression for non-Latin scripts.
- Mechanism: Korean synthetic data has different distributional properties from crawled web text. The paper finds Korean requires ~80% crawled data for optimal compression vs. ~60% for English, but synthetic-dominant tokenizers (EK, EPK) outperform commercial ones in downstream Korean tasks despite lower raw compression.
- Core assumption: Compression efficiency and downstream generalization have a non-monotonic relationship; token efficiency doesn't directly predict task performance.
- Evidence anchors: [section 3.2] Figure 2 shows Korean requires higher crawled proportion for compression parity; [section 3.4] EK-125K tokenizer achieves 44.93% English avg vs. LlamaTok's 43.17%, despite lower BPT.
- Break condition: If vocabulary is too large (196K), downstream performance degrades despite better compression.

### Mechanism 3
- Claim: Staged curriculum with explicit difficulty-based filtering transfers reasoning capability cross-lingually.
- Mechanism: The two-stage difficulty filter (incorrect on both Qwen-30B/4B, then consensus on "hard" label) selects samples requiring genuine reasoning traces. Translating these filtered English samples to Korean preserves the reasoning structure while adapting linguistic context.
- Core assumption: Reasoning difficulty is language-independent; a sample requiring multi-step inference in English will maintain that property after translation.
- Evidence anchors: [section 5.2, Algorithm 1] Two-stage filtering retains only 1% of STEM, 11% of math samples; [section 5.2, Table 18] Midtrain(Reason) achieves 54.12% Korean avg vs. Stage 1's 43.43%.
- Break condition: If translation quality degrades logical structure, or if difficulty labels are unreliable, filtering may remove high-value samples.

## Foundational Learning

- Concept: Pre-Layer Normalization (Pre-LN) vs. Post-LN
  - Why needed here: The paper compares Pre-LN against MixLN in 1B proxy experiments. Understanding gradient flow differences is essential for interpreting why Pre-LN achieved 43.38% avg vs. MixLN's 41.28%.
  - Quick check question: Given a 40-layer transformer, can you sketch where normalization would be applied in Pre-LN architecture, and why this stabilizes training for large models?

- Concept: Intra-Document Attention Masking
  - Why needed here: This is the chosen masking strategy for handling packed sequences. The paper reports it outperformed causal masking (44.48% vs. 43.38%) by eliminating cross-document attention noise.
  - Quick check question: If you pack three documents of lengths 500, 1000, and 800 tokens into a 4096-token sequence, which token pairs would intra-doc masking allow attention between?

- Concept: Grouped-Query Attention (GQA) with Multi-Query Attention (MQA)
  - Why needed here: The architecture uses 8 KV heads for 32 query heads. This balances inference efficiency (fewer KV cache reads) against representation quality.
  - Quick check question: For a 10B model with 32 query heads, how does changing from 8 KV heads to 4 KV heads affect memory bandwidth during generation?

## Architecture Onboarding

- Component map: Tokenizer -> Backbone -> Attention -> Normalization -> Training
- Critical path:
  1. Data prep: Heuristic filtering → BFF deduplication (old-both mode) → fastText quality filtering
  2. Stage 1 pretrain: 1T tokens (web-heavy), LR=7e-4, optimizer reinit at stage boundary
  3. Stage 2 pretrain: 1.8T tokens (synthetic + reasoning), multi-synthesizer requirement
  4. Mid-train long: 10.3B tokens, extend to 32K context
  5. Mid-train reasoning: 157.8B tokens, reasoning traces with excluded loss on `<think/>`
  6. SFT: 8B tokens, difficulty-balanced, hybrid reasoning mode
- Design tradeoffs:
  - Tokenizer vocab size: 125K chosen over 196K (better downstream despite worse compression)
  - Korean data ratio: ~5.6% of total (lower than Kanana's ~10%, trading Korean performance for English balance)
  - Deduplication strictness: old-both removes ~70% of Korean corpus but improves avg by ~2.5%
  - MTP vs NTP: NTP chosen; MTP showed -2% avg on 1B proxy despite gains on QA tasks
- Failure signatures:
  - Single-synthesizer Stage 2: Loss oscillates, Korean performance drops to Stage 1 levels
  - Long-context without short-text mix: Math/code performance degrades
  - Over-aggressive quality filtering: Removes too many Korean tokens, underfits language patterns
  - Empty reasoning block in SFT without mode toggle: Model fails to trigger reasoning when needed
- First 3 experiments:
  1. Reproduce the 1B proxy tokenizer comparison: Train 6 tokenizers (EK-125K, EK-196K, EPK-125K, etc.) on 20GB each, measure BPT on held-out web and synthetic splits, then train 1B models on 9B KR + 51B EN tokens and compare downstream on KMMLU/BoolQ.
  2. Ablate synthetic diversity: Train three Stage 2 runs with (a) single-synthesizer Synth-Nemo-HQ only, (b) two synthesizers 50/50, (c) five synthesizers as in paper. Monitor loss spikes and Korean benchmark oscillation.
  3. Validate difficulty filtering transfer: Apply Algorithm 1 to Nemotron-Post-V1, translate filtered samples to Korean, train a 1B model on the translated set alone, and compare reasoning benchmarks (Ko-Reasoning, translated GPQA) against an unfiltered baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do advanced stabilization techniques like MixLN or DeepNorm become necessary to maintain stability when pre-training models of tens to hundreds of billions of parameters on high proportions of synthetic data?
- Basis in paper: [explicit] The paper states that while Pre-LN sufficed for their 1B–10B scale, "a re-evaluation is necessary for models with tens to hundreds of billions of parameters," specifically regarding techniques like MixLN.
- Why unresolved: The study only empirically validated stability up to 10.8B parameters; behavior at 100B+ scales remains unverified.
- What evidence would resolve it: Training runs of 100B+ parameter models using synthetic data with MixLN/DeepNorm, showing loss curves and downstream benchmark performance compared to standard Pre-LN.

### Open Question 2
- Question: What data design or augmentation strategies are specifically required to achieve stable information retrieval in Korean for context lengths exceeding 21K tokens?
- Basis in paper: [explicit] The authors note that Korean Needle-In-A-Haystack performance "gradually declined beyond the 13K token range," concluding that "further improvements in data design and augmentation strategies specifically tailored for Korean long-context learning are required."
- Why unresolved: Current long-context training led to distinct performance degradation in Korean (dropping below 60% accuracy) that was not observed in English.
- What evidence would resolve it: Ablation studies testing different Korean long-context datasets (e.g., varying synthetic ratios or domain coverage) resulting in sustained high accuracy at 32K+ tokens.

### Open Question 3
- Question: Would Cross-Lingual Document Attention (XLDA) provide measurable performance gains over Intra-document causal masking in bilingual synthetic pre-training?
- Basis in paper: [explicit] The paper explicitly states regarding attention masking, "We did not explore methods like Cross-Lingual Document Attention (XLDA), which could be a promising direction for future work."
- Why unresolved: The authors focused on Intra-doc masking to reduce noise but did not compare it against mechanisms designed to facilitate explicit cross-lingual transfer during the pre-training phase.
- What evidence would resolve it: Comparative training runs utilizing XLDA showing improvements in bilingual benchmarks, particularly in tasks requiring cross-lingual knowledge transfer.

## Limitations

- Primary limitation centers on reproducibility of synthetic data generation due to unspecified synthesizer model versions and training configurations
- Tokenizer design uncertainties regarding the relationship between synthetic data properties and optimal tokenizer design may not generalize across different synthetic generation methods
- Cross-lingual reasoning transfer relies on theoretical assumptions about language-independent reasoning difficulty without systematic validation

## Confidence

- High Confidence: The model architecture and training pipeline specifications are clearly documented and technically sound
- Medium Confidence: The claim that synthetic data sustains long-horizon pretraining without collapse is supported by limited empirical validation (single controlled failure experiment)
- Medium Confidence: The bilingual tokenizer design improvements are demonstrated through controlled ablation studies, but generalizability is uncertain
- Low Confidence: The cross-lingual reasoning transfer mechanism lacks direct validation beyond theoretical assumptions

## Next Checks

1. **Synthetic Data Diversity Validation**: Reproduce the Stage 2 failure case by training three models: (a) single-synthesizer Korean data only, (b) two synthesizers 50/50, (c) five synthesizers as specified. Monitor for loss oscillation and performance degradation below Stage 1 levels to confirm the multi-synthesizer requirement.

2. **Tokenizer Design Generalizability**: Train tokenizers with varying synthetic-to-crawled ratios (60/40, 80/20, 90/10) for Korean and English separately, then train 1B proxy models on each. Measure both compression efficiency (BPT) and downstream task performance to validate whether the observed non-monotonic relationship between compression and generalization holds across different tokenizer configurations.

3. **Cross-Lingual Reasoning Transfer Validation**: Apply the two-stage difficulty filtering algorithm to an English reasoning dataset, translate the filtered samples to Korean, and train a 1B model exclusively on the translated set. Compare reasoning benchmark performance against an unfiltered baseline to test whether the filtering and translation process preserves reasoning complexity across languages.