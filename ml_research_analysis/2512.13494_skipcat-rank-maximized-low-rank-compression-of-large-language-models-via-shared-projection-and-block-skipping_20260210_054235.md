---
ver: rpa2
title: 'SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via
  Shared Projection and Block Skipping'
arxiv_id: '2512.13494'
source_url: https://arxiv.org/abs/2512.13494
tags:
- compression
- low-rank
- arxiv
- matrix
- matrices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying large language
  models (LLMs) on resource-constrained edge devices by proposing a novel low-rank
  compression framework called SkipCat. The core idea is to maximize the number of
  effective ranks retained during compression while maintaining the same compression
  rate, thereby preserving model performance.
---

# SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping

## Quick Facts
- arXiv ID: 2512.13494
- Source URL: https://arxiv.org/abs/2512.13494
- Reference count: 40
- This paper addresses the challenge of deploying large language models (LLMs) on resource-constrained edge devices by proposing a novel low-rank compression framework called SkipCat.

## Executive Summary
This paper addresses the challenge of deploying large language models (LLMs) on resource-constrained edge devices by proposing a novel low-rank compression framework called SkipCat. The core idea is to maximize the number of effective ranks retained during compression while maintaining the same compression rate, thereby preserving model performance. SkipCat introduces two key techniques: (1) Cat, which enables multiple matrices within a layer to share a common low-rank projection, and (2) Skip, which allows block skipping within low-rank decompositions to further reduce computation. Experimental results demonstrate that SkipCat outperforms existing low-rank compression methods by up to 7% in zero-shot task accuracy and significantly reduces perplexity without additional fine-tuning.

## Method Summary
SkipCat is a low-rank compression framework that maximizes retained ranks at fixed compression rates through two complementary techniques. First, it uses intra-layer shared low-rank projection (Cat) where multiple weight matrices sharing the same input (like Q/K/V or gate/up matrices) use a common projection matrix, allowing more ranks to be retained for the same parameter budget. Second, it employs block skipping (Skip) inspired by Schur complement, which reformulates the computation to eliminate storage of a square sub-block of the projection matrix, further reducing parameters. The method includes numerical stabilization via column permutation using Strong Rank-Revealing QR to prevent overflow in low-precision inference. A calibration step with 512 mixed samples from WikiText-2 and C4 training splits is used for preprocessing and rank selection.

## Key Results
- Outperforms existing low-rank compression methods by up to 7% in zero-shot task accuracy across ARC-e, ARC-c, HellaSwag, OBQA, WinoGrande, MathQA, and PIQA benchmarks
- Achieves lower perplexity on WikiText-2 and C4 without additional fine-tuning compared to baseline compression methods
- Provides effective compression across various model sizes (LLaMA2-7B/13B, Qwen3-8B/14B) and compression rates
- Maintains inference efficiency with reported speedups (e.g., 1.15x at 20% compression) on A100 GPUs

## Why This Works (Mechanism)

### Mechanism 1: Intra-Layer Shared Projection (Cat)
- **Claim:** Sharing a single low-rank projection matrix across multiple weight matrices that share the same input allows the model to retain higher effective ranks for the same parameter budget.
- **Mechanism:** Instead of factorizing weights $W_Q, W_K, W_V$ independently, the method concatenates them along the output dimension to form $W_{QKV}$. It then performs a single low-rank decomposition: $W_{QKV} \approx B_{QKV} A_{shared}$. The shared projection $A_{shared}$ projects the input once, and $B_{QKV}$ is sliced to reconstruct individual outputs.
- **Core assumption:** The weight matrices receiving the same input (e.g., Q, K, V projections) have sufficiently correlated subspaces such that a shared projection does not destroy task-critical unique features.
- **Evidence anchors:**
  - [abstract] "Intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection."
  - [section 3.2] "We refer to the above method as Cat... [it] enables the retention of more ranks under the same memory and computational budget."
  - [corpus] *DeltaLLM* supports the concept of shared weights/low-rank deltas for compression, suggesting weight sharing is a viable architectural direction, though specific intra-layer concatenation is distinct here.

### Mechanism 2: Block Skipping via Schur Complement (Skip)
- **Claim:** It is possible to omit the storage and explicit computation of a square sub-block of the projection matrix by folding it into the reconstruction matrix via a Schur complement identity.
- **Mechanism:** The projection matrix $A$ is partitioned into $[A_1, A_2]$ where $A_1$ is square ($r \times r$). The operation $B A x$ is reformulated as $B A_1 (x_1 + A_1^{-1} A_2 x_2)$. The $A_1$ block is absorbed into the reconstruction matrix ($B' = B A_1$) and the inverse into a transformed residual projection ($A' = A_1^{-1} A_2$), reducing total parameters by $r^2$.
- **Core assumption:** The sub-block $A_1$ is invertible and well-conditioned enough that its inversion does not introduce catastrophic numerical errors.
- **Evidence anchors:**
  - [section 3.3] "Block Skipping... inspired by the Schur complement... reformulation eliminates the need to explicitly compute products involving $A_1$."
  - [figure 2] Illustrates "Block Skipping" in the architecture diagram.
  - [corpus] Weak/None. While *FlashSVD* accelerates SVD, the specific algebraic reformulation via Schur complement to skip parameter storage appears unique to this paper in the provided context.

### Mechanism 3: Numerical Stabilization via Column Permutation
- **Claim:** A column permutation strategy is required to prevent overflow in FP16 inference caused by the inversion of the projection sub-block ($A_1^{-1}$).
- **Mechanism:** Before applying block skipping, the method reorders the input columns using Strong Rank-Revealing QR (RRQR). This ensures the selected columns form a well-conditioned matrix, keeping values in $A_1^{-1}$ bounded and preventing overflow when multiplied by activations.
- **Core assumption:** The activation outliers in the input $x$ can be managed or are less problematic than the numerical instability induced by inverting an ill-conditioned sub-matrix.
- **Evidence anchors:**
  - [section 3.3] "The permutation enhances numerical stability by avoiding inversion of poorly conditioned matrices."
  - [figure 3] Visual proof of overflow reduction: "Column permutation stabilizes the distribution, preventing overflow in low-precision inference."
  - [corpus] Weak/None. Related works like *Activation-Informed Pareto-Guided* address outliers generally, but this specific preconditioning for Schur-based skipping is specific to SkipCat.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD) & Truncation**
  - **Why needed here:** The paper builds upon SVD as the base compression primitive. You must understand how truncating singular values approximates a matrix with fewer parameters to grasp what "retained rank" means.
  - **Quick check question:** If you truncate 50% of the singular values in a diagonal matrix $\Sigma$, does the matrix dimension change, or just the precision of the reconstruction?

- **Concept: Matrix Condition Number**
  - **Why needed here:** The "Skip" mechanism relies on inverting a sub-block ($A_1$). A high condition number makes this inversion unstable, leading to the overflow issues the paper explicitly solves.
  - **Quick check question:** Why is inverting a matrix with a condition number of $10^5$ dangerous in FP16 precision?

- **Concept: Schur Complement**
  - **Why needed here:** This is the mathematical tool used to algebraically "skip" the block. It allows rewriting a matrix inverse involving sub-blocks, which the paper leverages to reshape the computation graph.
  - **Quick check question:** How does the Schur complement allow you to express the inverse of a 2x2 block matrix in terms of smaller inverses?

## Architecture Onboarding

- **Component map:**
  - Input: Vector $x$
  - Permutation Layer: Applies $P^\top$ (optimized via RRQR) to reorder features
  - Split: Divides permuted input into $\tilde{x}_1$ (first $r$ dims) and $\tilde{x}_2$ (remainder)
  - Residual Projection: Computes $\tilde{A}' \tilde{x}_2$ (where $\tilde{A}' = \tilde{A}_1^{-1} \tilde{A}_2$)
  - Add: Sums $\tilde{x}_1$ and projected residual
  - Shared Reconstruction: Applies $B'$ (concatenated weights for Q/K/V or Gate/Up)
  - Output: Sliced outputs for respective heads/layers

- **Critical path:** The compression script must run **Strong Rank-Revealing QR** (RRQR) to find the permutation $P$ before calculating the decomposition. If this is skipped or implemented with a naive QR, the resulting weights will likely overflow during inference.

- **Design tradeoffs:**
  - **Shared vs. Independent Projection:** Sharing (Cat) increases capacity (higher rank) but creates a coupling constraint between Q/K/V (or Gate/Up). If these matrices require strictly orthogonal subspaces, performance may drop.
  - **Skip vs. Stability:** Block Skipping (Skip) saves $r^2$ parameters but introduces an inverse operation that is sensitive to noise.

- **Failure signatures:**
  - **NaN / Inf during inference:** Occurs immediately in FP16 if the column permutation (RRQR) is skipped or fails to find a well-conditioned submatrix.
  - **High Perplexity Drop (>10%):** Indicates the shared projection (Cat) is too aggressive or the rank $r$ is too low relative to the aggressive "Skipping" savings.

- **First 3 experiments:**
  1.  **Sanity Check (FP32 vs FP16):** Run the "Skip" mechanism on a small model (e.g., LLaMA-7B layer) without column permutation. Verify that FP32 runs while FP16 produces NaNs (replicating Figure 3/left).
  2.  **Ablation on "Cat":** Compare perplexity of compressing Q/K/V independently vs. concatenating them before SVD at a fixed compression rate (e.g., 30%).
  3.  **Throughput Benchmark:** Measure the Time to First Token (TTFT) on an A100. Verify that the reported speedup (e.g., 1.15x at 20% compression from Table 9) holds, ensuring the algebraic reformulation translates to real kernel efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the intra-layer shared projection method be effectively combined with cross-layer parameter sharing techniques to maximize memory savings?
- **Basis in paper:** [inferred] The "Related Works" section contrasts SkipCat's intra-layer sharing with "Basis Sharing" (Wang et al. 2024), which shares bases across different layers but does not reduce computational cost. The paper suggests SkipCat addresses computation/memory transfer, implying the two methods target different redundancies.
- **Why unresolved:** The paper does not experiment with combining the two approaches. It is unclear if the error introduced by block skipping would compound with the error from cross-layer basis sharing, rendering the combined model unusable.
- **What evidence would resolve it:** An experimental evaluation applying both SkipCat and a cross-layer basis sharing method to a model like LLaMA2-7B and reporting the perplexity and accuracy trade-offs.

### Open Question 2
- **Question:** Does the column permutation strategy sufficiently stabilize the block skipping mechanism for sub-8-bit quantization (e.g., 4-bit) without requiring extensive fine-tuning?
- **Basis in paper:** [inferred] The paper notes that while column permutation stabilizes activations for FP16, it was "insufficient for stable low-precision quantization" when moving to 8-bit, requiring additional Hadamard transforms and scaling. The trend suggests further instability at 4-bit.
- **Why unresolved:** The paper only validates the quantization stability down to 8-bit precision. Because the block skipping mechanism involves matrix inversion ($A^{-1}$), the risk of outliers and overflow increases as precision decreases, potentially making the method unstable for 4-bit edge deployment.
- **What evidence would resolve it:** Evaluation of the SkipCat-compressed model under 4-bit quantization settings (e.g., AWQ or GPTQ) to observe if the permutation and transforms prevent overflow and accuracy collapse.

### Open Question 3
- **Question:** To what extent does the specific algebraic structure of block skipping require custom hardware kernels to realize the theoretical throughput gains on resource-constrained edge devices?
- **Basis in paper:** [inferred] The abstract and introduction highlight deployment on "edge devices," but the throughput results (Figure 7) are presented for an A100 GPU. The block skipping reformulation ($B'(x_1 + A'x_2)$) changes the memory access pattern compared to standard matrix multiplication.
- **Why unresolved:** Standard deep learning libraries are heavily optimized for dense matrix multiplication. The reformulation used in SkipCat may not map efficiently to standard tensor cores or mobile NPU instruction sets without specialized kernel development.
- **What evidence would resolve it:** Profiling of inference latency and memory bandwidth utilization on actual edge hardware (e.g., mobile SoC or edge GPU) comparing a standard GEMM implementation versus the block-skipping implementation.

## Limitations
- The paper does not provide complete pseudocode for the Strong Rank-Revealing QR implementation, which is critical for numerical stability.
- The exact mapping between rank selection and target compression rates is not explicitly specified, requiring manual tuning during reproduction.
- The assumption that Q/K/V matrices have sufficiently correlated subspaces may not hold for all transformer architectures or tasks.
- Experimental validation is limited to LLaMA2-7B/13B and Qwen3-8B/14B models without systematic ablation across the full compression rate spectrum.

## Confidence
- **High confidence:** The theoretical framework of combining shared projections (Cat) with block skipping (Skip) is mathematically sound and the experimental results show consistent perplexity and accuracy improvements over baseline low-rank compression methods.
- **Medium confidence:** The numerical stabilization claims via column permutation are supported by Figure 3's visual comparison, but the actual impact on downstream task performance without such stabilization is not empirically demonstrated.
- **Low confidence:** The assertion that the method works "across various model sizes and compression rates" is based on limited experimental coverage (only 7B/13B LLaMA2 and 8B/14B Qwen3 models) without systematic ablation studies across the compression rate spectrum.

## Next Checks
1. **Numerical stability validation:** Implement the Cat-Skip pipeline on a small LLaMA-7B layer without column permutation and verify FP32 runs while FP16 produces NaNs, replicating the overflow scenario in Figure 3.
2. **Ablation study:** Compare perplexity between independent SVD compression of Q/K/V matrices versus the concatenated approach (Cat) at identical compression rates to quantify the shared projection benefit.
3. **Throughput verification:** Measure actual wall-clock Time to First Token (TTFT) on A100 GPU for compressed models at different compression rates to confirm the claimed speedups (e.g., 1.15x at 20% compression) reflect real kernel efficiency gains.