---
ver: rpa2
title: Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food
  Learning
arxiv_id: '2511.13351'
source_url: https://arxiv.org/abs/2511.13351
tags:
- learning
- tasks
- food
- replay
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in multimodal continual
  food learning by proposing a Dual-LoRA architecture with Quality-Enhanced Pseudo
  Replay. The framework introduces specialized and cooperative LoRA adapters per task,
  with orthogonal constraints to prevent interference between tasks.
---

# Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning

## Quick Facts
- arXiv ID: 2511.13351
- Source URL: https://arxiv.org/abs/2511.13351
- Reference count: 34
- Primary result: Dual-LoRA with Quality-Enhanced Pseudo Replay achieves significant improvements over baseline methods in multimodal continual food learning, maintaining strong performance across ingredient recognition, recipe generation, and nutrition estimation tasks without requiring task labels during inference.

## Executive Summary
This paper addresses catastrophic forgetting in multimodal continual learning for food-related tasks by proposing a Dual-LoRA architecture combined with Quality-Enhanced Pseudo Replay. The framework introduces specialized and cooperative LoRA adapters per task, with orthogonal constraints to prevent interference between tasks. Pseudo replay leverages the model's own generative capability, enhanced by self-consistency and semantic similarity filtering to reduce hallucinations. Experiments on the Uni-Food dataset show significant improvements over baseline methods, with the model maintaining strong performance across multiple tasks while baselines experience severe forgetting.

## Method Summary
The Dual-LoRA framework introduces specialized and cooperative LoRA adapters for each task in a continual learning setting. The specialized LoRA learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, preventing gradient interference. The cooperative LoRA leverages cross-task knowledge sharing and is trained using pseudo replay samples generated by the model itself. Quality enhancement filters these pseudo samples through majority voting for ingredients and semantic similarity for recipes, reducing hallucination-induced noise. The approach achieves task-agnostic inference without requiring task labels during testing.

## Key Results
- Achieves IoU of 38.54 for ingredient recognition after Task 3
- Maintains BLEU score of 36.99 and Rouge_L of 28.91 for recipe generation
- Prevents catastrophic forgetting observed in baselines (LLaVA-LoRA drops from IoU 36.95 to 12.73 after Task 2)
- Outperforms LLaVA-LoRA and LLaVA-OLoRA across all metrics
- Achieves continual learning for complex food tasks without requiring task labels during inference

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Orthogonal subspace constraints between LoRA adapters reduce catastrophic forgetting by preventing gradient interference across sequentially learned tasks.
- **Mechanism:** The specialized LoRA for task $T_t$ is trained in a subspace orthogonal to the cooperative LoRA of task $T_{t-1}$. Orthogonality loss $L_o(A^t) = \sum_{i,j}\|O^t[i,j]\|^2$ where $O^t = A^t_{specialized} \cdot A^{t-1}_{cooperative}$ penalizes overlap with historically important parameter directions.
- **Core assumption:** Low-rank LoRA subspaces adequately approximate the gradient subspaces that matter for preserving past-task knowledge.
- **Evidence anchors:**
  - [abstract] "specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces"
  - [Section 3.3] "We let our specialized LoRA of task $T_t$ incrementally learn the new task in directions orthogonal to the cooperative LoRA subspace of task $T_{t-1}$"
  - [corpus] O-LoRA (Wang et al., 2023) applies similar orthogonal subspace learning to LLMs; corpus shows orthogonality-based methods are active research but not universally settled.
- **Break condition:** If tasks require significant positive transfer or share mutually supportive gradient directions, strict orthogonality may impede beneficial knowledge sharing.

### Mechanism 2
- **Claim:** Quality-enhanced pseudo replay reduces hallucination-induced noise in synthetic training samples, improving retention without storing real data.
- **Mechanism:** The model generates $n=5$ pseudo samples per input. For ingredients, majority voting with threshold $t=4$ retains only high-confidence items. For recipes, Sentence-BERT embeddings compute pairwise cosine similarity; the recipe with highest average consistency is selected.
- **Core assumption:** Self-consistency across multiple generations correlates with factual correctness; hallucinations are less consistent than accurate outputs.
- **Evidence anchors:**
  - [abstract] "pseudo replay leverages the model's own generative capability, enhanced by self-consistency and semantic similarity filtering"
  - [Section 3.4] "Using self-consistency principles, we apply majority voting to select reliable replay samples"
  - [corpus] "Replay Can Provably Increase Forgetting" suggests replay mechanisms can backfire under certain conditions—quality filtering matters.
- **Break condition:** If hallucinations become highly self-consistent (e.g., systematic biases), quality enhancement may amplify rather than reduce error.

### Mechanism 3
- **Claim:** Decoupling task-specific (specialized) and shared (cooperative) knowledge into separate adapters enables task-agnostic inference without requiring task labels.
- **Mechanism:** At inference, both specialized and cooperative LoRAs contribute. The specialized LoRA handles task-specific patterns while the cooperative LoRA provides domain-general culinary knowledge. No task ID is required.
- **Core assumption:** Food-related tasks share sufficient underlying structure (ingredients, cooking concepts) that a single cooperative adapter can capture cross-task knowledge.
- **Evidence anchors:**
  - [abstract] "without requiring task labels during inference"
  - [Section 3.3] "A cooperative LoRA that leverages cross-task knowledge sharing for performance enhancement"
  - [corpus] Weak direct evidence; most corpus work focuses on task-labeled settings.
- **Break condition:** If tasks diverge significantly (e.g., non-food tasks added), a single cooperative adapter may become a bottleneck or source of interference.

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The entire architecture builds on LoRA's low-rank decomposition ($W = W_{init} + BA$) to enable parameter-efficient continual learning. Understanding rank, A/B matrices, and which layers are adapted is essential.
  - **Quick check question:** Can you explain why LoRA adds only $(d \times r) + (r \times k)$ parameters instead of $d \times k$, and what the rank $r$ controls?

- **Concept: Catastrophic Forgetting in Sequential Learning**
  - **Why needed here:** The paper's core motivation is preventing performance collapse on prior tasks when learning new ones. Table 1 shows LLaVA-LoRA dropping from IoU 36.95 to 12.73 after Task 2.
  - **Quick check question:** If a model achieves 90% on Task A then 85% on Task B, but drops to 20% on Task A afterward, what specific phenomenon is this?

- **Concept: Pseudo/Generative Replay**
  - **Why needed here:** The cooperative LoRA is trained on model-generated samples from prior tasks. Understanding the trade-offs versus experience replay (memory cost) versus generative replay (hallucination risk) is critical.
  - **Quick check question:** Why might pseudo replay fail if the model's own generations are systematically biased or low-quality?

## Architecture Onboarding

- **Component map:** Base model (LLaVA-v1.5-7B) -> Specialized LoRA (task-specific, orthogonally constrained) -> Cooperative LoRA (shared knowledge, replay-trained) -> Quality enhancement module (majority voting/Sentence-BERT filtering)

- **Critical path:**
  1. Train specialized LoRA on current task with orthogonal loss vs. previous cooperative LoRA
  2. Generate pseudo samples for all prior tasks using current model
  3. Apply quality enhancement (voting/similarity filtering)
  4. Train cooperative LoRA on current real + quality-filtered pseudo samples
  5. Inference: both adapters active, no task label required

- **Design tradeoffs:**
  - **Replay proportion:** 1%→5% yields +0.56 IoU; 5%→10% yields only +0.02 (diminishing returns). Paper uses 5%.
  - **Orthogonal coefficient $\lambda_o$:** 0.5 optimal; values 0.1–5.0 all "decent" (robust but not insensitive).
  - **Generation count $n$:** Higher $n$ improves quality filtering but increases compute; paper uses $n=5$.

- **Failure signatures:**
  - Model outputs nutrition answers when asked about ingredients (task confusion—seen in LLaVA-LoRA after Task 3)
  - BLEU/Rouge_L dropping to near-zero on prior tasks (complete forgetting)
  - Hallucinated ingredients appearing consistently (quality enhancement failing to filter)
  - Training instability if $\lambda_o$ too high (over-constrained, plasticity loss)

- **First 3 experiments:**
  1. **Baseline reproduction:** Train LLaVA-LoRA sequentially on all 3 tasks; verify catastrophic forgetting (expect IoU drop similar to Table 1).
  2. **Ablation on replay quality:** Compare raw pseudo samples vs. quality-enhanced samples; expect degraded metrics without filtering.
  3. **Hyperparameter sweep:** Test $\lambda_o \in \{0.1, 0.5, 1.0, 2.0\}$ and replay proportion $\in \{1\%, 5\%, 10\%\}$; verify robustness claims and identify sweet spot for your compute budget.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but several areas remain unresolved including generalization to non-food domains, robustness to task ordering, and performance on longer task sequences.

## Limitations
- Does not specify LoRA rank or training epochs per task, making precise reproduction difficult
- Unclear implementation details for Sentence-BERT model and exact thresholds for semantic filtering
- Nutrition estimation task formulation and metrics are vaguely described
- No exploration of task ordering effects on performance
- Limited validation of task-agnostic inference with cross-task probe tests

## Confidence

- **High Confidence:** The core framework of Dual-LoRA with specialized and cooperative adapters, the orthogonal constraint mechanism, and the sequential continual learning setup on Uni-Food are clearly specified and reproducible. The observed mitigation of catastrophic forgetting (e.g., IoU drop from 36.95 to 12.73 in baseline vs. stable performance in Dual-LoRA) is a direct, measurable outcome.

- **Medium Confidence:** The pseudo replay with quality enhancement (majority voting and semantic similarity filtering) is well-defined in principle, but the exact implementation details (Sentence-BERT model, thresholds) are unspecified, potentially affecting results. The inference process without task labels is claimed but not deeply validated with cross-task probe tests.

- **Low Confidence:** The nutrition estimation task's specifics (loss/metric, generation vs. regression) are unclear, and the interaction between orthogonal constraints and replay (whether they are complementary or competing) is not experimentally explored.

## Next Checks

1. **Orthogonality Ablation:** Run Dual-LoRA with λ_o=0 (no orthogonality) and λ_o=5 (over-constrained). Confirm that without orthogonality, IoU on Task 1 drops >50% after Task 2, and with high λ_o, learning new tasks is severely impaired.

2. **Pseudo Replay Quality:** Compare Dual-LoRA with and without quality enhancement (raw vs. filtered pseudo samples). Expect IoU/recipe metrics to degrade significantly without filtering, validating the necessity of self-consistency and semantic similarity.

3. **Inference Task Confusion:** Test Dual-LoRA on mixed prompts (e.g., ingredient query after nutrition training) to verify it doesn't output nutrition answers for ingredient questions, confirming true task-agnostic inference.