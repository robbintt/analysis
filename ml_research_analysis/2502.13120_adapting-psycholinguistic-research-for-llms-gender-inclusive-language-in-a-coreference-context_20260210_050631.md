---
ver: rpa2
title: 'Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in
  a Coreference Context'
arxiv_id: '2502.13120'
source_url: https://arxiv.org/abs/2502.13120
tags:
- gender
- coreferent
- language
- antecedent
- masculine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study adapts psycholinguistic methods to investigate how Large
  Language Models (LLMs) process gender-inclusive language in English and German.
  Using coreference contexts, the research examines whether LLMs align generated terms
  with given gender expressions or reflect underlying biases.
---

# Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in a Coreference Context

## Quick Facts
- arXiv ID: 2502.13120
- Source URL: https://arxiv.org/abs/2502.13120
- Authors: Marion Bartl; Thomas Brendan Murphy; Susan Leavy
- Reference count: 37
- Primary result: English LLMs generally maintain antecedent-coreferent gender consistency but exhibit masculine bias; German models show stronger masculine bias, though gender-inclusive language increases feminine and neutral coreferent probabilities

## Executive Summary
This study adapts psycholinguistic methods to investigate how Large Language Models (LLMs) process gender-inclusive language in English and German. Using coreference contexts, the research examines whether LLMs align generated terms with given gender expressions or reflect underlying biases. Experiments on six English models and one German model measured coreferent probabilities and analyzed generated continuations. Results show that English models generally maintain antecedent-coreferent gender consistency but exhibit masculine bias, particularly struggling with singular "they." German models displayed stronger masculine bias, overriding gender-neutral strategies. However, gender-inclusive language in German increased the likelihood of feminine and neutral coreferents, partially achieving their intended effect. These findings highlight the importance of gender-inclusive language in LLMs, especially for underrepresented languages like German. Limitations include smaller model sizes and limited coreferent testing. Future work should explore larger models and longer contexts.

## Method Summary
The study adapts coreference resolution methodology from psycholinguistics to evaluate LLM behavior. Researchers created sentence templates with antecedent placeholders (role nouns) and coreferent placeholders. They generated antecedent triplets (masculine/feminine/neutral variants) and all possible template-antecedent-coreferent combinations. For English, 34 plural and 37 singular triplets were tested across six models (GPT-2, OLMo 1B/7B/13B, Qwen2.5 32B). For German, 10 antecedents with 8 gender-inclusive variations were tested using Leo Mistral 7B. The primary evaluation extracted log probabilities of coreferent tokens from model output distributions. Secondary evaluation generated 8-10 token continuations for annotation. Statistical analysis used two-way ANOVA and χ² tests to quantify gender interactions and bias patterns.

## Key Results
- English models generally maintain antecedent-coreferent gender consistency with masculine bias, particularly struggling with singular "they"
- German models show stronger masculine bias, overriding gender-neutral strategies, but gender-inclusive language increases feminine and neutral coreferent probabilities
- Masculine coreferents had 31% higher probability than neutral "they" for feminine antecedents in English models
- German gender-inclusive strategies (star, colon, underscore) all increased feminine and neutral coreferent probabilities, with star strategy showing highest feminine probability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs track antecedent gender and propagate it to coreferent predictions through context-dependent probability modulation.
- Mechanism: The model encodes gender information from the antecedent phrase into its hidden states; when predicting the coreferent noun or pronoun, the gender representation biases the output distribution toward matching gender forms.
- Core assumption: The model's attention mechanisms successfully bind the coreferent to its antecedent across sentence boundaries.
- Evidence anchors:
  - [abstract] "LLMs generally maintain the antecedent's gender"
  - [section 4.1] "the interaction between antecedent and coreferent gender is statistically significant and large... the probability of the coreferent is most influenced by the correspondence between antecedent and coreferent gender"
  - [corpus] Limited direct support; related work on coreference resolution (Paper 65133) documents similar cross-sentence binding challenges in French fiction.
- Break condition: When antecedent and coreferent are separated by multiple sentences or intervening entities, binding may fail; also breaks for less frequent gender-inclusive forms not well-represented in training data.

### Mechanism 2
- Claim: Masculine bias emerges from training data frequency imbalances that calibrate baseline gender probabilities toward masculine forms regardless of context.
- Mechanism: The model learns prior probabilities from corpus statistics where masculine forms are overrepresented; these priors shift predictions even when context signals feminine or neutral gender.
- Core assumption: The training corpus contains significantly more masculine-marked nouns and pronouns than feminine or neutral alternatives.
- Evidence anchors:
  - [abstract] "models generally matched antecedent and coreferent gender but showed underlying masculine bias"
  - [section 4.1] "masculine coreferents had a 31% higher probability than the neutral coreferent they for a feminine antecedent"; "for both feminine and neutral antecedents, masculine coreferents are second-most likely"
  - [corpus] Paper 38561 confirms masculine bias in MT systems; Paper 39182 documents similar patterns in gender-neutral MT strategies.
- Break condition: Fine-tuning on balanced or gender-inclusive corpora can shift priors (evidenced by GPT-2 fine-tuned results in Appendix B.2 showing elevated singular they probability).

### Mechanism 3
- Claim: Gender-inclusive orthographic strategies partially activate feminine/neutral associations through subword tokenization patterns that expose feminine suffixes.
- Mechanism: German gender-inclusive forms like "Akademiker*innen" tokenize to include "-innen" (feminine suffix), increasing probability mass for feminine coreferents via subword co-occurrence statistics learned during pretraining.
- Core assumption: The tokenizer exposes morphological gender markers in a way the model can associate with feminine/neutral semantic content.
- Evidence anchors:
  - [section 4.1] "all German gender-inclusive language strategies lead to an increase in the probability of feminine and gender-neutral coreferents"
  - [section 5] "the asterisk strategy [had the] highest probability for the feminine coreferent... which could be due the feminine suffix -innen contained in this strategy"
  - [corpus] Paper 53112 on Polish gender-inclusive instruction tuning provides converging evidence that explicit intervention can shift gender associations.
- Break condition: The effect is insufficient to override masculine priors completely; masculine coreferents still have highest overall probability.

## Foundational Learning

### Concept: Coreference resolution (antecedent-coreferent binding)
- Why needed here: The entire methodology depends on understanding how LLMs resolve which entity a pronoun or noun phrase refers to across sentence boundaries.
- Quick check question: Given "The doctors entered. Some of ___ were tired," what determines whether the blank resolves to the antecedent entity?

### Concept: Log probability from language models
- Why needed here: The primary evaluation method extracts log(p) of specific tokens from the model's output distribution to quantify gender associations.
- Quick check question: Why use log probability rather than raw probability when comparing token likelihoods across different contexts?

### Concept: Grammatical gender vs. notional gender languages
- Why needed here: German marks gender on nouns/articles/adjectives; English uses gender primarily for pronouns and some lexical items. This structural difference explains why German shows stronger masculine bias.
- Quick check question: In which language would replacing "chairman" with "chairperson" more completely eliminate grammatical gender marking?

## Architecture Onboarding

### Component map:
Dataset generator -> Probability extraction module -> Generation module -> Annotation interface -> Statistical analysis pipeline

### Critical path:
1. Define antecedent triplets (masculine/feminine/neutral variants)
2. Generate all template-antecedent-coreferent combinations
3. Extract log probabilities for each coreferent candidate
4. Run ANOVA to quantify antecedent×coreferent gender interaction
5. Generate continuations and annotate for produced gender

### Design tradeoffs:
- Fixed coreferent vocabulary (men/women/people) enables controlled comparison but limits ecological validity
- Single German model (Leo Mistral 7B) vs. multiple English models limits cross-linguistic conclusions
- Pilot annotation by author for German introduces potential bias
- 8-10 token generations balance annotation cost against completion quality

### Failure signatures:
- Model generates antecedent repetition instead of novel coreferent (common in German generation experiments)
- Singular they probability remains low despite neutral antecedent (indicates pronoun fidelity issue)
- Feminine antecedent → masculine coreferent probability exceeds feminine → neutral (indicates bias override)

### First 3 experiments:
1. Replicate English plural experiment with Qwen-2.5 using 5 novel antecedent triplets to validate interaction effect generalization.
2. Test whether German gender-star (*) vs. colon (:) strategies differ in feminine coreferent probability (both contain "-innen" but may tokenize differently).
3. Compare log(p) extraction vs. generation-based evaluation for the same model-antecedent pairs to quantify the repetition artifact observed in German generation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the observed masculine bias and gender-inclusive language processing patterns persist in state-of-the-art models with significantly larger parameter counts (e.g., >600B parameters)?
- **Basis in paper:** [explicit] The authors state in the Limitations section that they restricted experiments to smaller models (1.5–32B) and that "Future research is needed to determine whether our findings hold for these larger models."
- **Why unresolved:** Hardware restrictions prevented the authors from testing on contemporary massive models like DeepSeek-V3 (671B).
- **What evidence would resolve it:** Replicating the sentence-pair probability and generation experiments on models exceeding 600B parameters.

### Open Question 2
- **Question:** How does gender-inclusive language processing change when LLMs operate in longer, multi-turn contexts rather than isolated sentence pairs?
- **Basis in paper:** [explicit] The authors note in the Limitations that "LLMs often handle longer contexts and exchanges. Therefore, future research should be conducted in a setting with a longer context."
- **Why unresolved:** The study methodology was restricted to analyzing coreference between two specific sentences, ignoring broader discourse.
- **What evidence would resolve it:** Extending the coreference resolution dataset to include paragraph-length contexts or conversational history.

### Open Question 3
- **Question:** Can fine-tuning strategies be refined to successfully mitigate masculine default bias in plural contexts without causing the model to ignore antecedent gender signals?
- **Basis in paper:** [inferred] The results for the fine-tuned GPT-2 model showed that while it accepted singular 'they', it unexpectedly favored masculine coreferents for plural neutral antecedents, indicating the fine-tuning approach was insufficient or flawed.
- **Why unresolved:** The authors found that simple replacement-based fine-tuning did not "balance out associations" as intended for plural forms.
- **What evidence would resolve it:** Developing fine-tuning objectives that specifically target the probability distribution of neutral vs. masculine coreferents in plural scenarios.

## Limitations

- **Dataset Completeness**: Only 7 high-frequency English triplets explicitly listed versus 34 reported, limiting full reproducibility
- **Single German Model Constraint**: Testing only Leo Mistral 7B cannot distinguish model-specific artifacts from systematic cross-linguistic patterns
- **Generation Artifact**: German generation experiments show high rates of antecedent repetition rather than novel coreferents, suggesting 8-10 token length may be insufficient

## Confidence

- **High Confidence**: English models maintain antecedent-coreferent gender consistency with masculine bias (supported by multiple models, clear statistical patterns, robust across experimental conditions)
- **Medium Confidence**: Gender-inclusive language strategies in German increase feminine/neutral coreferent probabilities (limited to single model, generation artifacts present, mechanism partially supported by subword tokenization)
- **Low Confidence**: Singular "they" probability remains consistently low across English models (based on probability extraction only, no generation validation, potential tokenization issues)

## Next Checks

1. **Cross-model German Validation**: Test the German gender-inclusive strategies across 3-4 additional German-language models (both 7B and 13B variants) to determine if the masculine bias override pattern holds beyond Leo Mistral 7B.

2. **Generation Length Optimization**: Systematically vary generation length (8, 12, 16 tokens) for both English and German models on identical prompts to identify the point where coreferent repetition plateaus and meaningful continuations emerge.

3. **Singular They Fine-tuning Experiment**: Fine-tune a base English model on a balanced corpus containing equal masculine, feminine, and singular "they" contexts, then re-run coreferent probability extraction to test whether pronoun fidelity can be improved through targeted intervention.