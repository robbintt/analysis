---
ver: rpa2
title: 'The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent
  LLM Refusal Behavior'
arxiv_id: '2512.12066'
source_url: https://arxiv.org/abs/2512.12066
tags:
- safety
- prompts
- temperature
- stability
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the stability of safety refusal decisions\
  \ in large language models across random seeds and temperature settings. The authors\
  \ test four instruction-tuned models (7B-12B parameters) from three families on\
  \ 876 harmful prompts across 20 sampling configurations (4 temperatures \xD7 5 random\
  \ seeds), generating 70,080 total responses."
---

# The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior

## Quick Facts
- arXiv ID: 2512.12066
- Source URL: https://arxiv.org/abs/2512.12066
- Authors: Erik Larsen
- Reference count: 16
- Primary result: 18-28% of harmful prompts cause LLM refusal flips across random seeds and temperatures

## Executive Summary
This paper reveals that large language models exhibit unstable safety refusal behavior, with decisions flipping between refusal and compliance across different random seeds and temperature settings. Testing 876 harmful prompts across four models (7B-12B parameters) and 20 sampling configurations (4 temperatures × 5 seeds), the study finds that 18-28% of prompts produce inconsistent safety decisions. Temperature significantly affects stability, with mean Safety Stability Index dropping from 0.977 at temperature 0.0 to 0.942 at temperature 1.0. The findings challenge the reliability of single-shot safety evaluations and suggest that robust safety assessment requires multiple samples per prompt.

## Method Summary
The study evaluates four instruction-tuned models (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts from the BeaverTails dataset. For each prompt, the models generate 20 responses across four temperatures (0.0, 0.3, 0.7, 1.0) and five random seeds (42, 43, 44, 45, 46) using vLLM batched inference. Responses are classified by Llama 3.1 70B Instruct judge into REFUSE, PARTIAL, or COMPLY categories. The Safety Stability Index (SSI) measures the proportion of responses falling into the majority category, with flip rate quantifying prompts exhibiting any decision inconsistency. Statistical analysis examines temperature effects and correlations with compliance rates.

## Key Results
- 18-28% of prompts exhibit refusal decision flips across sampling configurations
- Temperature significantly affects stability (Friedman χ² = 396.81, p < 0.001)
- Mean within-temperature SSI drops from 0.977 at temperature 0.0 to 0.942 at temperature 1.0
- Single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time
- Prompts with higher compliance rates exhibit lower stability (Spearman ρ = -0.47 to -0.70)

## Why This Works (Mechanism)

### Mechanism 1
Higher sampling temperatures reduce safety decision stability by broadening the logit distribution, making borderline refusal decisions more sensitive to random seed selection. Temperature scales logits before softmax sampling, narrowing the probability gap between "refuse" and "comply" tokens for ambiguous prompts. This increases the chance that random sampling selects different decision categories across seeds.

### Mechanism 2
Prompts near the safety decision boundary exhibit higher instability because the model's internal representation produces near-equal refusal and compliance probabilities. Borderline prompts activate similar magnitudes for both safety-aligned and compliance-aligned representations, with small stochastic perturbations tipping the decision.

### Mechanism 3
GPU floating-point non-determinism in batched inference causes residual instability even at temperature 0.0 (greedy decoding). Parallel GPU operations have non-associative floating-point behavior, with different random seeds affecting batch ordering or parallel reduction order, producing slightly different logits even with greedy selection.

## Foundational Learning

- **Temperature-scaled softmax sampling**
  - Why needed here: Understanding how temperature changes logit distributions explains why safety decisions become unstable at higher temperatures
  - Quick check question: If a model produces logits [refuse: 4.0, comply: 2.0] for a prompt, what happens to the refuse probability at temperature 0.5 vs. 2.0?

- **Safety alignment as decision boundary learning**
  - Why needed here: The compliance-instability correlation suggests safety training creates a boundary; prompts near it waver
  - Quick check question: If you plot prompt compliance rate (x-axis) vs. SSI (y-axis), what shape would you expect and why?

- **LLM-as-judge classification reliability**
  - Why needed here: The paper uses Llama 70B and Claude Haiku as judges; understanding judge agreement bounds how much observed instability is real vs. measurement noise
  - Quick check question: If two judges agree 89% with κ=0.62, what fraction of observed flips might be judge noise vs. true model instability?

## Architecture Onboarding

- **Component map**: Inference layer (vLLM batched generation) -> Classification layer (Llama 70B/Claude Haiku judge) -> Stability metric layer (SSI computation) -> Analysis layer (statistical tests)
- **Critical path**: Generate responses across sampling configurations → Judge each response (three-class) → Compute per-prompt SSI and compliance rate → Aggregate by temperature/model; run statistical tests
- **Design tradeoffs**: Three-class vs. binary scoring (captures ambiguity vs. reduces disagreement); N=20 configurations (dense research sampling vs. N=3 minimum for production); Single vs. multi-judge (cost vs. bias)
- **Failure signatures**: Judge label drift (high UNKNOWN rate >1%) indicates rubric ambiguity; Exact-match responses with different labels indicates judge instability; SSI < 0.5 on clearly harmful prompts indicates systematic safety failure
- **First 3 experiments**:
  1. Reproduce within-temperature stability: Run 5 seeds at temp 0.0, 0.7, 1.0 on 50 prompts; verify SSI drops with temperature
  2. Judge agreement baseline: Run 100 responses through both Llama 70B and Claude Haiku; measure exact agreement and κ
  3. Category-specific instability probe: Select 20 prompts each from copyright and self-harm; verify SSI difference

## Open Questions the Paper Calls Out

- Does safety decision instability persist in larger models (70B-405B parameters) or closed-source models like GPT-4 and Claude?
- Do alternative alignment techniques like DPO or Constitutional AI yield more stable refusal behavior than standard RLHF?
- To what extent is the observed instability at temperature 0.0 attributable to GPU floating-point non-determinism versus inherent model behavior?

## Limitations

- Results are limited to small-to-medium open-weight models (7B-12B parameters)
- Judge disagreement introduces measurement noise that may inflate apparent model instability
- GPU non-determinism artifacts at temperature 0.0 may confound pure model behavior analysis

## Confidence

- **High confidence**: Temperature effects on stability and general finding that safety decisions are not deterministic across seeds
- **Medium confidence**: Compliance-instability correlation as it depends on judge consistency for ambiguous prompts
- **Low confidence**: Exact magnitude of instability at temperature 0.0 due to potential GPU non-determinism artifacts

## Next Checks

1. Judge Noise Isolation: Run a validation set of 100 responses through both Llama 70B and Claude Haiku judges to quantify measurement noise vs. true model instability
2. Determinism Baseline: Enable CUBLAS_WORKSPACE_CONFIG and torch.use_deterministic_algorithms to measure the pure model stability contribution at temperature 0.0
3. Threshold Sensitivity: Repeat analysis using binary (lenient/conservative) classification instead of three-class to verify whether observed instability persists under simplified evaluation