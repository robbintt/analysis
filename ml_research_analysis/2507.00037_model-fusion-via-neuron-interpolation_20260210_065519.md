---
ver: rpa2
title: Model Fusion via Neuron Interpolation
arxiv_id: '2507.00037'
source_url: https://arxiv.org/abs/2507.00037
tags:
- fusion
- neuron
- level
- fused
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel family of neuron-centric model fusion
  algorithms that effectively combine multiple neural networks into a single model,
  regardless of training data distribution. The key innovation is a principled formulation
  of fusion as a representation matching problem, decomposed into grouping and approximation
  components.
---

# Model Fusion via Neuron Interpolation

## Quick Facts
- arXiv ID: 2507.00037
- Source URL: https://arxiv.org/abs/2507.00037
- Reference count: 40
- One-line primary result: Neuron-centric fusion algorithm combining multiple neural networks into single model, achieving ensemble-level performance while maintaining single-model inference cost

## Executive Summary
This paper introduces a novel family of neuron-centric model fusion algorithms that combine multiple pre-trained neural networks into a single model, regardless of training data distribution. The key innovation is a principled formulation of fusion as a representation matching problem, decomposed into grouping and approximation components. Unlike prior approaches, the method incorporates neuron attribution scores to guide the fusion process toward preserving salient features, and generalizes to arbitrary layer types.

The proposed algorithms employ a two-step approach: first clustering neurons from base models using either Hungarian matching or K-means clustering, then fitting the fused model's weights to match the importance-weighted cluster centers. This approach outperforms previous fusion techniques in zero-shot and non-IID fusion scenarios, approaching ensemble-level performance while maintaining a single model's inference cost.

## Method Summary
The method fuses multiple pre-trained models by first partitioning them into sequential composition levels, then applying a two-step algorithm per level. First, neurons from base models are clustered using Hungarian matching (for equal-sized models) or weighted K-means clustering (for general cases), with clustering guided by neuron attribution scores. Second, the fused model's weights at each level are fitted to minimize the weighted mean squared error to the cluster centers, using closed-form least squares for linear layers or gradient-based optimization for non-linear layers. The algorithm proceeds sequentially through levels, refitting each to match targets computed from the current fused subnetwork.

## Key Results
- Outperforms previous fusion techniques in zero-shot and non-IID scenarios, approaching ensemble-level performance
- Achieves significant accuracy gains over vanilla averaging (which collapses to ~10% accuracy) across VGG11/CIFAR-10 and ViT/CIFAR-100 benchmarks
- Demonstrates robustness across different training regimes including non-IID, sharded, and full-dataset splits
- Maintains single-model inference cost while achieving multi-model ensemble performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing fusion into grouping and approximation errors improves optimization stability.
- Mechanism: The representation cost J^IMP is split via Theorem 1 into (1) grouping error—how well neurons cluster—and (2) approximation error—how well the fused model reproduces cluster centers. This isolation allows solving each subproblem with appropriate algorithms rather than joint optimization.
- Core assumption: The optimal target vector T can be approximated by importance-weighted cluster means, and fitting to these centroids preserves functional behavior.
- Evidence anchors: [abstract] "decouples fusion into grouping and approximation steps"; [section 4.1] Theorem 1 decomposition; Eq. 4 shows cross-term vanishes when T is importance-weighted mean.

### Mechanism 2
- Claim: Neuron attribution scores improve fusion by preserving salient features.
- Mechanism: Importance scores s^i_j weight the squared distance penalty, so distorting high-importance neurons incurs higher cost. This biases clustering (weighted K-means) toward keeping important neurons close to their centroids.
- Core assumption: Attribution methods (Conductance, DeepLIFT) correctly identify neurons critical for model predictions.
- Evidence anchors: [abstract] "incorporating neuron attribution scores to prioritize important neurons during fusion"; [section 4.1, Eq. 3] S_i diagonal matrix weights neurons by importance; [section 5.5, Figure 2] Shows importance weighting changes cluster assignments and centroid positions.

### Mechanism 3
- Claim: Level-wise sequential construction with proper refitting accumulates less error than isolated layer matching.
- Mechanism: Each level is optimized to match target centroids given the actual (not assumed) outputs from the previously fused levels. For linear layers, closed-form least-squares solutions exist; for non-linear, SGD is used.
- Core assumption: Error propagation across levels is the primary failure mode in prior methods; refitting mitigates this.
- Evidence anchors: [section 4.2] Algorithm 1 iterates through levels, fitting each to targets computed from current fused subnetwork; [section 4.3.2] Linear levels admit closed-form solution; non-linear requires gradient optimization.

## Foundational Learning

- Concept: **K-means Clustering with Weights**
  - Why needed here: Core grouping mechanism; neurons are clustered by activation patterns with importance weights.
  - Quick check question: Can you explain why weighted K-means centroids differ from unweighted ones?

- Concept: **Hungarian Algorithm / Linear Sum Assignment**
  - Why needed here: Special case for equal-sized models requiring one-to-one neuron matching.
  - Quick check question: What constraint does the Hungarian algorithm enforce that K-means does not?

- Concept: **Neuron Attribution (Conductance, DeepLIFT)**
  - Why needed here: Provides importance scores; understanding their computation helps debug poor fusion.
  - Quick check question: Why might DeepLIFT assign non-zero importance where gradients are zero?

## Architecture Onboarding

- Component map: Base models {M_k} -> Level Partitioner -> Grouping Module (Hungarian/K-means) -> Approximation Module (Linear/Gradient) -> Fused model F

- Critical path:
  1. Partition base models into matching level structures
  2. For each level i: compute concatenated outputs z_i from all base models
  3. Cluster neurons (Hungarian or K-means) to get assignments k_j
  4. Compute importance-weighted centroids T_k
  5. Fit fused level weights to minimize ||z^F_{i} - T||
  6. Proceed to next level with updated fused subnetwork

- Design tradeoffs:
  - **Linear vs Gradient variants**: Linear is faster and deterministic but limited to affine layers; gradient handles arbitrary architectures but requires tuning (learning rate, epochs, initialization)
  - **Importance scores**: Uniform is simpler; Conductance/DeepLIFT add ~2-3s overhead but improve clustering
  - **Fusion dataset size**: More data improves gradient variant; linear variant less sensitive

- Failure signatures:
  - **Vanilla averaging collapse (~10% accuracy)**: Base models have incompatible representations; alignment required
  - **Gradient variant underperforms linear on small data**: Switch to linear or increase fusion samples
  - **Zero-shot performance much below base models**: Check level partitioning matches across models; verify previous levels frozen correctly

- First 3 experiments:
  1. Reproduce Table 1 (Non-IID, 2-way VGG11/CIFAR-10) with uniform scores to validate pipeline
  2. Ablate importance scores: compare uniform vs Conductance vs DeepLIFT on same checkpoint
  3. Test data sensitivity: fuse with 100, 400, 1000, 4000 samples to find minimum viable dataset size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the grouping and approximation objectives be optimized jointly via subgradient descent rather than the decoupled two-stage approach, and would this improve fusion quality?
- Basis in paper: [explicit] The authors state the objective is subdifferentiable and could be optimized with subgradient descent "in the same spirit as the weighted K-means objective," but explicitly leave this for future work.
- Why unresolved: The current method separates grouping (NP-hard K-means) and approximation (weighted least squares/SGD) for tractability, but joint optimization could potentially find better solutions.
- What evidence would resolve it: Comparative experiments showing improved accuracy or faster convergence when optimizing both components jointly versus sequentially.

### Open Question 2
- Question: Would more expressive clustering methods beyond Hungarian Matching and K-means (e.g., hierarchical, spectral, or learned clustering) better capture inter-neuron similarity and improve fusion performance?
- Basis in paper: [explicit] The Future Work section explicitly calls for evaluating "more expressive or adaptive clustering methods to better capture inter-neuron similarity."
- Why unresolved: K-means with squared Euclidean distance may not optimally group neurons with complex semantic relationships, especially across heterogeneous models trained on non-IID data.
- What evidence would resolve it: Benchmarks comparing alternative clustering algorithms on the same fusion tasks, measuring both accuracy gains and computational overhead.

### Open Question 3
- Question: What is the relative impact of grouping error versus approximation error on final fusion quality, and how should this inform algorithm design?
- Basis in paper: [explicit] The Future Work section requests "a deeper investigation into the relative impact of grouping error versus approximation error on fusion quality."
- Why unresolved: While Theorem 1 decomposes the objective into these two components, their individual contributions to downstream performance remain unquantified.
- What evidence would resolve it: Ablation studies varying each error component independently and correlating with final test accuracy, or principled error-budget allocation strategies.

## Limitations

- Limited empirical validation that attribution-guided clustering outperforms random assignment beyond toy examples
- Sensitivity to hyperparameter choices for gradient-based variant with insufficient guidance on tuning
- Assumption that neuron identities are stable across heterogeneous model architectures remains largely untested

## Confidence

- **High confidence**: The linear algebra formulation (Theorem 1 decomposition) and the two-step Hungarian/K-means + fitting procedure are mathematically rigorous and reproducible
- **Medium confidence**: The attribution-guided fusion improvement claims are supported by ablation studies, but relative performance of different attribution methods is not thoroughly analyzed
- **Low confidence**: Claims about gradient-based variant robustness across architectures are weak; paper acknowledges sensitivity to hyperparameters and provides limited guidance on tuning

## Next Checks

1. Test fusion performance when base models are trained on non-IID splits from different datasets (e.g., CIFAR-10 vs CIFAR-100) to validate claims about arbitrary training data compatibility

2. Systematically compare attribution methods (Conductance, DeepLIFT, GradCAM, random) across multiple architectures to quantify the actual contribution of importance weighting versus the core clustering-and-fitting mechanism

3. Evaluate fusion quality versus fusion dataset size on deeper architectures (ResNet-50, ViT-Large) to determine the minimum viable dataset size and identify the point of diminishing returns for the gradient-based variant