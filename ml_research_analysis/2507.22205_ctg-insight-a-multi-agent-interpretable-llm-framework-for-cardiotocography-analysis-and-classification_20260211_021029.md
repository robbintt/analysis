---
ver: rpa2
title: 'CTG-Insight: A Multi-Agent Interpretable LLM Framework for Cardiotocography
  Analysis and Classification'
arxiv_id: '2507.22205'
source_url: https://arxiv.org/abs/2507.22205
tags:
- ctg-insight
- decelerations
- classification
- fetal
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CTG-Insight addresses the interpretability challenge in remote\
  \ fetal monitoring by introducing a multi-agent LLM framework for cardiotocography\
  \ (CTG) analysis. The system decomposes each CTG trace into five medically defined\
  \ features\u2014baseline, variability, accelerations, decelerations, and sinusoidal\
  \ pattern\u2014each analyzed by a dedicated agent."
---

# CTG-Insight: A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification

## Quick Facts
- arXiv ID: 2507.22205
- Source URL: https://arxiv.org/abs/2507.22205
- Reference count: 40
- Primary result: 96.4% accuracy, 97.8% F1-score on fetal heart rate classification

## Executive Summary
CTG-Insight introduces a multi-agent LLM framework that decomposes cardiotocography (CTG) trace analysis into five specialized feature agents (Baseline, Variability, Accelerations, Decelerations, Sinusoidal pattern) and an aggregator agent. Each agent analyzes one medically-defined feature from visualized CTG traces using vision-language models, with the aggregator synthesizing results into final fetal health classifications with natural language explanations. Evaluated on the NeuroFetalNet Dataset, the system achieves state-of-the-art performance while providing interpretable, clinically-aligned outputs that outperform both traditional deep learning and LLM-based baselines.

## Method Summary
The framework renders raw CTG time-series data into standardized visual traces using Matplotlib, then processes each trace through five parallel feature agents using GPT-4.1 vision models. Each agent applies specific FIGO clinical guidelines to classify its assigned feature as Normal/Suspicious/Pathological. A final aggregator agent combines these five classifications into an overall fetal health assessment. The system explicitly incorporates clinical rules into prompts to ensure medically consistent reasoning, trading computational efficiency for interpretability and transparent decision-making.

## Key Results
- Achieves 96.4% accuracy and 97.8% F1-score on the NeuroFetalNet Dataset
- Outperforms Direct Prompt baseline (79.80% accuracy) by 16.6 percentage points
- Surpasses existing deep learning methods (NeuroFetalNet 95.00%, FHR-Net 93.30%) while providing natural language explanations

## Why This Works (Mechanism)

### Mechanism 1: Task Decomposition Reduces Instruction Forgetfulness
The system separates complex classification into specialized sub-tasks to prevent LLMs from dropping specific clinical instructions. By isolating feature extraction into five parallel agents rather than a single monolithic prompt, the architecture mitigates "lost in the middle" errors common in long context windows. This decomposition ensures each agent focuses on its specific rules without interference from other instructions.

### Mechanism 2: Visual Rendering as a Translation Layer
Converting numerical time-series into standardized CTG trace images allows general-purpose VLMs to perform specialized signal analysis tasks. This leverages the LLM's pre-trained visual pattern recognition capabilities, bypassing the need to learn raw numerical syntax. The visual approach aligns with clinical interpretation methods while utilizing the model's inherent visual understanding.

### Mechanism 3: Guideline-Grounded Prompt Engineering
Hard-coding FIGO clinical guidelines into structured prompts (Definition/Rule/Role sections) enforces medically consistent reasoning and output formatting. This grounds probabilistic LLM generation in deterministic clinical rules, reducing hallucination and ensuring outputs match established medical consensus. The approach creates transparent decision logic that can be clinically verified.

## Foundational Learning

- **Cardiotocography (CTG) Feature Taxonomy**
  - Why needed: The multi-agent architecture relies on the medical validity of five features
  - Quick check: Does "Suspicious" require two pathological features or just one suspicious feature? (Answer: Suspicious = 1 suspicious + others normal; Pathological = 1 pathological OR 2 suspicious)

- **Chain-of-Thought (CoT) via Multi-Agent Systems**
  - Why needed: CTG-Insight externalizes intermediate reasoning steps into separate agent interactions
  - Quick check: How does the Aggregator differ from a standard LLM router? (Answer: It synthesizes pre-analyzed logic, not routes intent)

- **Context Window vs. Attention Span**
  - Why needed: The paper argues against Direct Prompting due to instruction forgetting
  - Quick check: Why is splitting across 5 agents more effective than one long prompt? (Answer: Isolates attention to specific rules per agent)

## Architecture Onboarding

- **Component map:** Python script (Matplotlib) rendering CSV/Time-series → PNG Image → 5 Feature Agents → Aggregator Agent → Final Classification
- **Critical path:** Visualization quality must preserve clinical details; prompt thresholds must match FIGO guidelines; agents must provide accurate feature classifications
- **Design tradeoffs:** 6 LLM calls introduce latency vs. single ResNet forward pass; hard-coded rules create rigidity vs. data-driven flexibility
- **Failure signatures:** Borderline drift (edge case misclassification); agent conflict (contradictory feature classifications); visual artifacts (artefacts interpreted as clinical features)
- **First 3 experiments:**
  1. Visual Perturbation Test: Add noise/change line properties to test rendering robustness
  2. Agent Ablation: Remove one Feature Agent to test Aggregator's handling of missing information
  3. Rule Violation Test: Feed synthetic trace violating specific rules to verify agent detection

## Open Questions the Paper Calls Out

### Open Question 1
How do expectant parents and clinicians perceive the natural language explanations generated by CTG-Insight in terms of interpretability, trust, and emotional reassurance?
- Basis: Authors explicitly state they haven't conducted user studies to assess stakeholder engagement with explanations
- Why unresolved: Current evaluation focuses solely on computational accuracy metrics
- Resolution evidence: Semi-structured interviews and Likert-scale surveys measuring user trust and comprehension

### Open Question 2
Can the multi-agent architecture be effectively re-architected to support continuous, real-time streaming data for dynamic home or clinical use?
- Basis: System currently operates offline and plans to extend for real-time streaming
- Why unresolved: Implementation relies on pre-recorded 20-minute traces and asynchronous processing
- Resolution evidence: Successful deployment on mobile/wearable platforms with continuous state tracking

### Open Question 3
Does incorporating clinical metadata—such as gestational age, maternal history, and labor stage—improve the system's classification accuracy and clinical relevance?
- Basis: Authors acknowledge current design doesn't account for contextual factors
- Why unresolved: Agents analyze signal morphology in isolation from broader clinical context
- Resolution evidence: Comparative performance analysis between signal-only and signal-plus-metadata inputs

### Open Question 4
Does the reported high performance (96.4% accuracy) generalize to the full dataset, given the reliance on a small evaluation subset?
- Basis: Authors note the 50-sample subset may limit generalizability and statistical significance
- Why unresolved: Small sample sizes introduce high variance and potential selection bias
- Resolution evidence: Evaluation metrics over complete NeuroFetalNet test set rather than random sample

## Limitations

- **Clinical Generalization Risk**: Performance on real-world CTG traces with artifacts and signal loss remains untested
- **Guideline Adherence Gap**: Hard-coded FIGO 2015 guidelines lack mechanisms for updates without architectural changes
- **Parallel Agent Independence**: Five agents operate without cross-validation, potentially failing when features are interdependent

## Confidence

- **High Confidence**: Task decomposition mechanism effectively reduces instruction forgetfulness (16.6% accuracy improvement vs. Direct Prompt baseline)
- **Medium Confidence**: Visual rendering successfully translates numerical signals into LLM-readable formats (strong performance but no rendering quality ablation)
- **Medium Confidence**: Guideline-grounded prompt engineering ensures medically consistent outputs (supported by interpretability but not clinical expert review)

## Next Checks

1. **Artifact Robustness Test**: Evaluate CTG-Insight on CleanCTG dataset containing multi-artefact corrupted traces to measure performance degradation and identify artifact interpretation failure modes
2. **Clinical Expert Validation**: Conduct blinded review by obstetricians comparing CTG-Insight classifications and explanations against gold-standard clinical assessments on real-world patient data
3. **Temporal Consistency Test**: Run the same CTG trace through the system at different times to measure inter-session variability, addressing the probabilistic nature of LLM-based classification