---
ver: rpa2
title: 'RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with
  an Application to City Scale Injury Severity Prediction'
arxiv_id: '2512.07848'
source_url: https://arxiv.org/abs/2512.07848
tags:
- injury
- data
- class
- severity
- shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RaX-Crash is a resource-efficient, explainable small model pipeline
  for city-scale injury severity prediction using NYC collision data. It integrates
  crash, person, and vehicle tables into a unified schema and trains compact tree-based
  ensembles (Random Forest and XGBoost) for classification, achieving accuracy of
  0.783 and 0.779, respectively.
---

# RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with an Application to City Scale Injury Severity Prediction

## Quick Facts
- arXiv ID: 2512.07848
- Source URL: https://arxiv.org/abs/2512.07848
- Reference count: 21
- XGBoost achieves 0.7828 accuracy, Random Forest 0.7794 on 3-class injury severity prediction.

## Executive Summary
RaX-Crash is a resource-efficient, explainable small model pipeline for city-scale injury severity prediction using NYC collision data. It integrates crash, person, and vehicle tables into a unified schema and trains compact tree-based ensembles (Random Forest and XGBoost) for classification, achieving accuracy of 0.783 and 0.779, respectively. Small language models (SLMs) prompted with textual summaries perform worse (0.594 and 0.496). Class imbalance mitigation improves fatal recall with modest accuracy trade-offs. SHAP attribution identifies human vulnerability, timing, and location as dominant factors. The hybrid design combines efficient tabular models with SLM-generated narratives for interpretability, supporting scalable, explainable city-scale analytics.

## Method Summary
RaX-Crash integrates three NYC collision tables (crash, person, vehicle) into a unified schema, aggregates features to event level, and trains compact tree-based ensembles (XGBoost and Random Forest) for 3-class injury severity prediction. Temporal split: last 5,000 events (October 2025) for test, prior 20,000 for training. Models use inverse frequency class weights and achieve accuracy of 0.7828 (XGBoost) and 0.7794 (Random Forest). SHAP TreeExplainer provides feature importance. SLMs generate narratives for interpretability, but perform worse than tree models (0.594 and 0.496 accuracy).

## Key Results
- XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496).
- Class weighting and focal loss improve fatal recall with modest accuracy trade-offs.
- SHAP attribution identifies human vulnerability, timing, and location as dominant factors.
- Hybrid design combines efficient tabular models with SLM-generated narratives for interpretability.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tree-based ensembles substantially outperform small language models (SLMs) on structured, data-rich tabular classification tasks like injury severity prediction.
- Mechanism: Tree ensembles (Random Forest, XGBoost) natively partition the feature space based on learned non-linear interactions between numerical and categorical features. SLMs, when applied via text serialization and prompting without fine-tuning, struggle to robustly process aggregated numeric cues and instead rely on high-level keyword patterns in the prompt.
- Core assumption: The unified feature schema captures predictive signal; SLMs are not fine-tuned for this tabular task, and their pre-training does not prioritize precise numerical reasoning.
- Evidence anchors:
  - [abstract] "XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496)."
  - [section IV.D] "SLMs often underweight aggregated numeric cues and rely more on high level keywords in the prompt..."
  - [corpus] Corpus evidence for direct SLM vs. tree model comparison on crash severity is weak; the related "Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection" (arXiv:2508.11504) confirms strong XGBoost/RF performance but lacks an SLM baseline.
- Break condition: If data were extremely scarce (few-shot regime), SLMs could become competitive. If SLMs were heavily fine-tuned for tabular reasoning, the performance gap would narrow.

### Mechanism 2
- Claim: Class imbalance mitigation (class weighting, focal loss) improves recall for the rare fatal class at the cost of a modest reduction in overall accuracy.
- Mechanism: Standard loss functions are dominated by the majority class. Inverse frequency weighting and focal loss increase the penalty for misclassifying rare, hard examples (fatal crashes), forcing the model's decision boundary to become more sensitive to them.
- Core assumption: The operational cost of missing a fatal crash (false negative) is significantly higher than the cost of a false positive (e.g., misclassifying a non-fatal crash as fatal).
- Evidence anchors:
  - [abstract] "class imbalance analysis shows that simple class weighting improves fatal recall with modest accuracy trade offs"
  - [section IV.C] "Focal loss improved fatal recall the most (+0.05 over the baseline), while maintaining stable accuracy."
  - [corpus] "Tabular Data with Class Imbalance: Predicting Electric Vehicle Crash Severity..." (arXiv:2509.11449) highlights class imbalance as a common challenge in crash severity prediction, reinforcing the need for such techniques.
- Break condition: If the cost of false positives becomes prohibitive (e.g., overwhelming emergency services), the accuracy/recall trade-off may be unacceptable.

### Mechanism 3
- Claim: SHAP attribution combined with SLM-generated narratives creates a trustworthy and interpretable explanation layer for model predictions.
- Mechanism: SHAP values provide a quantitative, game-theoretic decomposition of a prediction into feature contributions. The SLM, prompted with this structured information (event features + model probabilities), produces a coherent, human-readable narrative. The alignment between the two (SHAP features mentioned in the narrative) validates the SLM as a communication tool.
- Core assumption: The SLM, when conditioned on structured inputs and model outputs, will faithfully verbalize the provided data and not contradict it with hallucinations.
- Evidence anchors:
  - [abstract] "hybrid design combines efficient tabular models with SLM-generated narratives for interpretability"
  - [section V.D] "LLaMA 3.2 achieves an Alignment Score of 0.61... indicating that both SLMs tend to emphasize the same dominant factors highlighted by SHAP."
  - [corpus] Direct corpus evidence for this hybrid SHAP-SLM alignment mechanism is not present in the provided neighbors.
- Break condition: If the SLM generates narratives that contradict the SHAP values or input features (hallucination), trust in the system degrades.

## Foundational Learning
- Concept: SHAP (SHapley Additive exPlanations) Values
  - Why needed here: To decode the "black box" tree ensemble model, identify that human vulnerability and timing are dominant factors, and provide quantitative evidence for the SLM's qualitative narratives.
  - Quick check question: If a model predicts "fatal" and the SHAP value for "event hour" is high and positive, what does that mean?
- Concept: Class Imbalance & Cost-Sensitive Learning
  - Why needed here: Fatal crashes are <0.5% of the data. Without explicit handling (weighting, focal loss), the model will ignore this class to maximize overall accuracy.
  - Quick check question: Why can a model with 99% accuracy be completely useless for predicting fatal crashes?
- Concept: SLMs as Communication Modules vs. Primary Classifiers
  - Why needed here: This is a core architectural decision. Understanding that SLMs are too slow and inaccurate for bulk scoring but valuable for narrative generation is key to the hybrid design.
  - Quick check question: In this pipeline, which component handles the primary injury severity classification for 100,000 events: the XGBoost model or the SLM?

## Architecture Onboarding
- Component map:
  Data Integration (ETL) -> Model Training/Inference -> SHAP Computation -> Selective SLM Narrative Generation
- Critical path: Data Integration (ETL) -> Model Training/Inference -> SHAP Computation (for explanation) -> Selective SLM Narrative Generation
- Design tradeoffs:
  - **Prediction Accuracy vs. Interpretability:** XGBoost is accurate but opaque. Hybrid design adds latency/complexity (SLM, SHAP) to gain explainability.
  - **Throughput vs. Rich Explanation:** SLM inference is slow. Trade-off is to invoke it selectively (e.g., only for high-risk events), not for all events.
  - **Fatal Recall vs. Overall Accuracy:** Using class weighting/focal loss improves critical fatal recall but slightly depresses overall accuracy.
- Failure signatures:
  - **SLM Hallucination:** Narrative contradicts the XGBoost probability or the input features (e.g., SLM says "dry road" when data indicates "wet").
  - **Catastrophic Minority Class Forgetting:** After a model update, `Recall_fatal` drops to 0.0, indicating the imbalance mitigation has failed or the new data distribution shifted.
  - **SHAP-SLM Misalignment:** The SLM consistently emphasizes factors (e.g., vehicle type) that SHAP analysis shows have low importance, breaking user trust.
- First 3 experiments:
  1.  **Reproduce Baseline:** Verify XGBoost accuracy (~0.78) vs SLM accuracy (~0.49-0.59) on the provided temporal split.
  2.  **Imbalance Ablation:** Compare `Recall_fatal` for (a) no weighting, (b) class weighting, and (c) focal loss. Confirm the modest accuracy trade-off.
  3.  **SLM Alignment Check:** For a sample of 50 test events, compare the top-3 SHAP features with the factors mentioned in the SLM narrative. Compute the Alignment Score.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can advanced reweighting schemes or focal losses significantly improve fatal injury recall without compromising overall accuracy compared to the simple class weighting used in RaX-Crash?
  - Basis in paper: [explicit] The Conclusion states future work includes "addressing class imbalance with more advanced reweighting schemes or focal losses to further improve fatal injury recall."
  - Why unresolved: The current study primarily utilized simple inverse frequency class weighting and basic ablations (SMOTE, oversampling), leaving advanced loss functions unexplored in the final pipeline.
  - What evidence would resolve it: Benchmarking XGBoost models trained with focal loss against the current weighted baseline, specifically tracking macro-F1 and fatal class recall trade-offs.
- **Open Question 2:** To what extent does the unified feature schema generalize to other jurisdictions with differing collision reporting standards?
  - Basis in paper: [explicit] The authors list as future work the plan to "study how the unified feature schema and trained models transfer to other jurisdictions with different reporting practices."
  - Why unresolved: RaX-Crash was constructed and validated exclusively on NYC Open Data; the portability of the schema (designed on semantic types) to other cities remains theoretical.
  - What evidence would resolve it: Applying the pipeline without structural modification to a distinct city-scale dataset (e.g., Los Angeles or London) and evaluating the performance retention of the pre-trained or retrained models.
- **Open Question 3:** Does domain-specific fine-tuning of Small Language Models (SLMs) improve their ability to interpret aggregated numeric cues and uncertainty compared to the prompting-only approach tested?
  - Basis in paper: [explicit] The paper notes that SLMs currently "underweight aggregated numeric cues" and suggests "fine tuning SLMs on domain specific narratives" as a necessary future step.
  - Why unresolved: The current implementation relies on zero-shot or few-shot prompting (LLaMA 3.2, DeepSeek-R1), which resulted in poor accuracy (0.594 and 0.496) compared to tree-based models.
  - What evidence would resolve it: A comparative study measuring the prediction accuracy and SHAP-alignment of fine-tuned SLMs versus the current prompted baselines on the same test set.

## Limitations
- The precise feature engineering details for aggregating person and vehicle tables to the event level, particularly for PCTEJECTED and PCTWITHSAFETYEQUIPMENT, are unspecified.
- The SHAP-SLM alignment mechanism lacks strong independent corpus validation; while an Alignment Score of 0.61 is reported, external verification is needed.
- The SLM component is invoked selectively for explanation, not primary classification, making its role auxiliary and the system's interpretability dependent on the quality of SLM narrative generation.

## Confidence
- **High Confidence:** The core finding that tree-based ensembles (XGBoost, Random Forest) significantly outperform SLMs on structured tabular data for injury severity prediction. This is directly supported by quantitative results and the stated mechanism (SLMs struggle with aggregated numeric cues without fine-tuning).
- **Medium Confidence:** The effectiveness of class imbalance mitigation techniques (class weighting, focal loss) in improving fatal recall. While results show improvement, the operational cost-benefit of the accuracy trade-off is context-dependent.
- **Medium Confidence:** The utility of SHAP attribution for identifying key factors (human vulnerability, timing, location). This is a standard technique, but the novel aspect of combining it with SLM narratives for a trust layer lacks strong independent corpus validation.

## Next Checks
1. **Feature Engineering Verification:** Independently derive the formulas for PCTEJECTED and PCTWITHSAFETYEQUIPMENT from the raw person table fields and verify they match the paper's reported values on a sample of events.
2. **SLM Alignment Validation:** Conduct a blinded study where human raters assess the factual accuracy of SLM-generated narratives against the ground truth features and XGBoost predictions for 100 test events. Compare the observed alignment rate to the reported 0.61 score.
3. **Cost-Benefit Analysis of Imbalance Mitigation:** Simulate the operational impact of the fatal recall vs. accuracy trade-off. Calculate the expected number of additional fatal crashes correctly identified (true positives) versus the number of non-fatal crashes incorrectly flagged as fatal (false positives) when using class weighting or focal loss compared to no weighting.