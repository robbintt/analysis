---
ver: rpa2
title: Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for
  Depression Detection Using Large Language Models
arxiv_id: '2505.17119'
source_url: https://arxiv.org/abs/2505.17119
tags:
- depression
- reasoning
- detection
- responses
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a systematic evaluation of LLM-based depression
  detection using machine-generated reasoning and PHQ-9 symptom labeling. The authors
  decompose the task into subtasks (self-reference, PHQ-9 symptoms, overall diagnosis)
  and evaluate LLMs' performance through few-shot learning and human annotation.
---

# Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models

## Quick Facts
- **arXiv ID:** 2505.17119
- **Source URL:** https://arxiv.org/abs/2505.17119
- **Reference count:** 11
- **Primary result:** DPO optimization achieved 23.7% joint correct ratio for complete 11-subtask depression detection, outperforming SFT and few-shot baselines.

## Executive Summary
This paper presents a systematic evaluation of large language models for depression detection using machine-generated reasoning and PHQ-9 symptom labeling. The authors decompose the task into 11 subtasks (self-reference, 9 PHQ-9 symptoms, final diagnosis) and evaluate LLMs through few-shot learning and human annotation. Key findings include superior performance on explicit depression language versus implicit expressions, keyword-triggered detection biases, and significant improvement in joint task performance using Direct Preference Optimization (DPO) over Supervised Fine-Tuning (SFT). The DPO approach with sophisticated reasoning achieved the highest correct ratio (23.7%) for complete analysis across all subtasks.

## Method Summary
The study uses the DepTweet dataset with 3,132 balanced tweets (1,566 depressed, 1,566 non-depressed) filtered for annotator confidence > 0.95. Human annotation by 6 professionals provides ground truth for 11 subtasks. The method employs few-shot prompting with 2 contrastive examples (positive/negative) across 5 LLMs. Outputs are partitioned into TC (all correct), TP (partially correct), and TW (all wrong). SFT uses correct responses from TP, while DPO uses paired correct/wrong responses from TP to train preference optimization. Evaluation focuses on micro F1 scores for individual subtasks and joint correct ratio C = Nc/N for all 11 subtasks correct.

## Key Results
- LLMs achieved 83.6% F1 for depression detection with Llama-3.1-8B in few-shot mode, but only 39.5% joint correct ratio for all 11 subtasks.
- DPO optimization improved joint correct ratio from 0% baseline to 23.7% for sophisticated reasoning versus 12.0% for SFT.
- Models showed keyword-triggered bias: FP rates spiked when depression keywords were present, FN rates spiked when absent.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Task decomposition into subtasks improves diagnostic reliability by creating verifiable checkpoints in the reasoning chain.
- **Mechanism:** Breaking depression detection into 11 binary subtasks forces explicit commitment to intermediate conclusions that can be individually validated against human annotations.
- **Core assumption:** Accurate intermediate predictions correlate with correct final diagnosis—though joint correctness remains challenging.
- **Evidence anchors:** Task decomposition described in section 2; joint correctness challenge noted in section 4.3.2.
- **Break condition:** If subtask predictions are individually accurate but jointly inconsistent, decomposition adds overhead without reliability gains.

### Mechanism 2
- **Claim:** Contrastive few-shot prompting with chain-of-thought reduces keyword-triggered false positives.
- **Mechanism:** Providing one positive example (exhibiting "feeling down" symptom) and one negative example (containing "depression" keyword but describing someone else) teaches keyword distinction.
- **Core assumption:** Models can transfer "keyword ≠ depression" patterns from examples to novel inputs.
- **Evidence anchors:** Contrastive examples described in section 3.2.2; persistent bias noted in section 4.3.2.
- **Break condition:** If negative example is too specific, models may overfit without generalizing to other non-depressed keyword contexts.

### Mechanism 3
- **Claim:** DPO outperforms SFT for multi-subtask joint decision-making by learning from comparative preferences.
- **Mechanism:** DPO trains on paired (correct, incorrect) responses, teaching preference for reasoning paths leading to fully correct joint predictions.
- **Core assumption:** Incorrect responses contain learnable signal about reasoning failures that correct responses alone don't capture.
- **Evidence anchors:** DPO vs SFT comparison in section 4.4.2; joint correct ratio improvement from 0% to 22.0% on hard samples.
- **Break condition:** If preference pairs contain spurious correlations, DPO may optimize for surface features rather than reasoning quality.

## Foundational Learning

- **Concept: PHQ-9 Framework**
  - **Why needed here:** Provides clinical taxonomy for symptom detection subtasks, making outputs clinically interpretable.
  - **Quick check question:** Can you name at least three PHQ-9 symptoms and explain why "feeling down" might be more prevalent in social media than "suicidal ideation"?

- **Concept: Chain-of-Thought Prompting**
  - **Why needed here:** Required explicit step-by-step reasoning for each subtask, not just final labels.
  - **Quick check question:** Given "My depression is back but I'm fighting it," what intermediate reasoning steps should a model show before concluding self-reference?

- **Concept: Direct Preference Optimization**
  - **Why needed here:** Mechanism that achieved significant performance gains; essential for reproducing results.
  - **Quick check question:** Why would training on (preferred, dispreferred) response pairs help better joint decision-making than preferred responses alone?

## Architecture Onboarding

- **Component map:** Expert Annotation → Human-derived ground truth → Prompt Engineering → Instruction set + 2 contrastive examples → LLM Few-shot → 5 open-source models → Quality Analysis → Format adherence, readability, similarity checks → Instruction Tuning → SFT/DPO with reasoning criteria → Evaluation on TW (hard samples)

- **Critical path:** Annotation → Prompt design → Few-shot generation → Correctness filtering (TC/TP/TW split) → DPO training pairs from TP → Evaluation on TW

- **Design tradeoffs:** SFT vs DPO (simpler vs better multi-task alignment), Intuitive vs Sophisticated reasoning (first prediction vs revisions), Format strictness (automation reliability vs data yield)

- **Failure signatures:** Short generations (incomplete analysis), Label confusion (correct reasoning but inverted labels), Keyword bias (FP spike with keywords, FN spike without), Joint correctness collapse (high individual F1 but low joint ratio)

- **First 3 experiments:**
  1. Reproduce few-shot baseline on DepTweet subset (100 samples) with Llama-3.1-8B, measuring per-subtask F1 and joint correct ratio.
  2. Construct TP/TW split and train DPO with 50 preference pairs; compare joint correct ratio improvement against SFT baseline on same TW hard set.
  3. Ablate keyword bias: Evaluate on balanced MD/NMD subsets to quantify FP/FN asymmetry before and after DPO optimization.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the substantial performance gap between individual subtask accuracy (>80%) and joint decision-making correctness (39.5%) be systematically closed?
- **Open Question 2:** Can sophisticated reasoning (allowing prediction revisions) consistently improve DPO optimization beyond marginal gains observed with only two differing samples?
- **Open Question 3:** What prompt-level or fine-tuning interventions can reduce the keyword-triggered bias without requiring additional human annotation?

## Limitations
- Joint correct ratio remains low (39.5% baseline, 23.7% with DPO) despite high individual subtask accuracy.
- Keyword-triggered detection biases persist even after DPO optimization, with increased false negatives in non-depression mention groups.
- Exact prompt templates and DPO hyperparameters are unspecified, limiting reproducibility.

## Confidence
- **High confidence:** Decomposition methodology and human annotation quality (inter-annotator agreement > 0.95)
- **Medium confidence:** DPO performance gains demonstrated but lacking ablation studies on hyperparameter sensitivity
- **Low confidence:** Generalizability to real-world deployment scenarios given controlled balanced dataset

## Next Checks
1. **Prompt template validation:** Reconstruct and test exact few-shot prompt with contrastive examples on held-out validation set to verify 83.6% F1 score.
2. **Keyword bias quantification:** Systematically evaluate FP/FN rates across MD/NMD groups using larger, temporally diverse tweet corpus.
3. **DPO hyperparameter sensitivity:** Conduct ablation studies varying learning rates, batch sizes, and preference pair selection criteria.