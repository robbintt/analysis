---
ver: rpa2
title: 'Fake News Classification in Urdu: A Domain Adaptation Approach for a Low-Resource
  Language'
arxiv_id: '2512.22778'
source_url: https://arxiv.org/abs/2512.22778
tags:
- news
- fake
- mbert
- domain
- frozen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates domain adaptation for fake news detection
  in Urdu, a low-resource language, by applying domain-adaptive pretraining to multilingual
  language models before fine-tuning them on downstream fake news classification tasks.
  Two widely used models, XLM-RoBERTa and mBERT, were adapted using a publicly available
  Urdu news corpus of one million articles.
---

# Fake News Classification in Urdu: A Domain Adaptation Approach for a Low-Resource Language

## Quick Facts
- arXiv ID: 2512.22778
- Source URL: https://arxiv.org/abs/2512.22778
- Reference count: 40
- Domain-adaptive pretraining of multilingual models on Urdu news corpus improves fake news detection

## Executive Summary
This study investigates domain adaptation for fake news detection in Urdu, a low-resource language, by applying domain-adaptive pretraining to multilingual language models before fine-tuning them on downstream fake news classification tasks. Two widely used models, XLM-RoBERTa and mBERT, were adapted using a publicly available Urdu news corpus of one million articles. The domain-adapted XLM-RoBERTa consistently outperformed its vanilla counterpart across all four Urdu fake news datasets, showing improved accuracy and F1-scores. In contrast, domain-adapted mBERT exhibited mixed results, with minimal gains in some cases and slight degradation in others. These findings highlight the potential of domain adaptation in enhancing model performance for low-resource language tasks, while also emphasizing the importance of model choice in determining the effectiveness of such approaches.

## Method Summary
The researchers implemented domain adaptation through additional masked language modeling pretraining on a one-million-article Urdu news corpus. They used two multilingual transformer models (XLM-RoBERTa and mBERT) as base models. After adaptation, they fine-tuned the models using a two-stage approach: first training only newly added classification layers while freezing the base model for 20 epochs, then unfreezing the entire model for another 20 epochs with a lower learning rate. The classification head consisted of dense layers (256→128 neurons) with batch normalization and dropout.

## Key Results
- Domain-adapted XLM-RoBERTa consistently outperformed vanilla XLM-RoBERTa across all four Urdu fake news datasets
- XLM-RoBERTa showed improved accuracy and F1-scores (e.g., 0.91→0.93 on Ax-to-Grind; 0.97→0.98 on UFN2023)
- Domain-adapted mBERT showed inconsistent results with minimal gains in some cases and slight degradation in others
- Both models achieved lower perplexity after adaptation, but only XLM-RoBERTa translated this to classification gains

## Why This Works (Mechanism)

### Mechanism 1: Domain-Adaptive Pretraining via Masked Language Modeling
- Claim: Intermediate pretraining on unlabeled in-domain data may improve downstream task performance for low-resource languages by reducing domain shift
- Mechanism: A pretrained multilingual model undergoes additional masked language modeling (MLM) on 1M Urdu news articles before task-specific fine-tuning
- Core assumption: Domain shift (distribution mismatch between pretraining and target data) is a significant bottleneck; reducing it via in-domain exposure transfers to downstream classification
- Evidence anchors: [abstract] domain-adaptive pretraining before fine-tuning; [Section 3.2.1] 15% token masking with 0.2 whole-word masking probability
- Break condition: If base model has insufficient capacity or weak target-language representations, perplexity may drop without downstream gains—as observed with mBERT

### Mechanism 2: Staged Fine-Tuning with Gradual Unfreezing
- Claim: A two-stage fine-tuning protocol appears to stabilize learning and may reduce catastrophic forgetting
- Mechanism: Stage 1 freezes the pretrained backbone while training newly added classification layers for 20 epochs. Stage 2 unfreezes the full model and continues training with lower learning rate for another 20 epochs
- Core assumption: Initial isolation of task-specific layers allows stable feature extraction before joint refinement
- Evidence anchors: [Section 3.2.2] two-stage training protocol; [Section 4.1.2] smoother convergence with staged approach
- Break condition: If frozen-stage learning rate is too aggressive or added layers lack capacity, model may fail to establish useful task representations before unfreezing

### Mechanism 3: Model Architecture Moderates Adaptation Effectiveness
- Claim: Downstream benefit of domain adaptation is contingent on base model architecture and pretraining composition
- Mechanism: XLM-R (larger, better Urdu vocabulary coverage, dynamic masking during pretraining) shows consistent F1 improvements. mBERT (smaller, less Urdu exposure) shows perplexity reduction but inconsistent downstream gains
- Core assumption: Lower perplexity on domain data does not guarantee better task representations; architectural capacity and original multilingual coverage mediate transfer
- Evidence anchors: [Section 6, Table 2] both models achieve lower perplexity but only XLM-R translates to classification gains
- Break condition: If base model's representations are too sparse or misaligned for target language, additional pretraining may overfit to domain statistics without improving discriminative features

## Foundational Learning

- Concept: **Domain Shift / Dataset Shift**
  - Why needed here: Paper explicitly frames problem as mismatch between general multilingual pretraining and Urdu news domain
  - Quick check question: Can you explain why a model trained on general web text might fail on Urdu news articles, even if Urdu is in its pretraining vocabulary?

- Concept: **Masked Language Modeling (MLM)**
  - Why needed here: Domain adaptation is implemented via additional MLM pretraining on Urdu news; understanding token masking is essential to interpret perplexity results
  - Quick check question: If you mask 15% of tokens in a Urdu sentence, what must the model learn to predict them correctly?

- Concept: **Perplexity vs Downstream Task Performance**
  - Why needed here: Paper shows reduced perplexity does not always improve classification; this distinction is critical for evaluating adaptation success
  - Quick check question: Why might a model become "better" at predicting masked tokens in Urdu news (lower perplexity) without improving at distinguishing fake from real news?

## Architecture Onboarding

- Component map:
  Input: Urdu text → Tokenizer → MLM head (domain adaptation) → PLM_UN → [CLS] token → Dense(256) → Dense(128) → Dropout → Sigmoid output

- Critical path:
  1. Verify Urdu tokenization behavior (check vocabulary coverage, subword fragmentation rate)
  2. Run MLM pretraining on Urdu 1M; monitor training/validation loss curves
  3. Evaluate perplexity on held-out Urdu news before/after adaptation
  4. Fine-tune adapted vs vanilla models on each fake news dataset using staged protocol
  5. Compare accuracy, precision, recall, F1; check if perplexity reduction correlates with F1 gains

- Design tradeoffs:
  - Chunk size (128 tokens) preserves information but may truncate long articles
  - Whole-word masking (0.2 probability) improves semantic learning but increases masking complexity
  - Two-stage fine-tuning adds stability but doubles training time
  - XLM-R offers better Urdu coverage but is larger/slower than mBERT

- Failure signatures:
  - Validation accuracy oscillates wildly during frozen stage → suggests insufficient domain alignment
  - Perplexity drops but F1 stays flat or degrades → model lacks capacity or representations for discriminative task
  - Training accuracy reaches 99%+ while validation plateaus or declines → overfitting

- First 3 experiments:
  1. Replicate domain adaptation on XLM-R with Urdu 1M corpus; measure perplexity before/after
  2. Fine-tune adapted vs vanilla XLM-R on Ax-to-Grind dataset using staged protocol; compare F1 scores
  3. Run same comparison on UFN2023 and UrduFake2021; observe whether length modulates adaptation benefit

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow evaluation scope: adaptation mechanism's generalizability to other low-resource languages remains untested
- Urdu news corpus representation: one million articles but no detailed breakdown of source domains or temporal coverage
- Staged fine-tuning protocol lacks comparative ablation against alternative fine-tuning strategies
- Confidence in perplexity-downstream performance link is low due to mBERT's disconnect between metrics

## Confidence
**High Confidence:** Domain-adapted XLM-R outperforms vanilla XLM-R across all tested datasets (accuracy and F1 improvements of 1-2 percentage points)
**Medium Confidence:** Domain adaptation reduces domain shift for low-resource languages, demonstrated for XLM-R but not consistently for mBERT
**Low Confidence:** Staged fine-tuning uniquely stabilizes learning compared to alternative approaches, lacking comparative experiments

## Next Checks
1. **Cross-linguistic validation**: Apply identical domain adaptation protocol to another low-resource language (e.g., Swahili, Nepali) using its news corpus and fake news datasets
2. **Ablation on fine-tuning strategy**: Replicate XLM-R experiments using single-stage fine-tuning with various learning rate schedules and gradual unfreezing
3. **Corpus representation analysis**: Segment Urdu news corpus by source type and time period; evaluate whether adaptation performance correlates with corpus diversity or overfitting to specific subdomains