---
ver: rpa2
title: A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation
arxiv_id: '2508.12282'
source_url: https://arxiv.org/abs/2508.12282
tags:
- temporal
- dataset
- chronoqa
- question
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChronoQA is a large-scale Chinese benchmark dataset designed to
  evaluate temporal reasoning in Retrieval-Augmented Generation (RAG) systems. Constructed
  from over 300,000 news articles published between 2019 and 2024, it contains 5,176
  high-quality question-answer pairs that cover absolute, aggregate, and relative
  temporal types with both explicit and implicit time expressions.
---

# A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2508.12282
- Source URL: https://arxiv.org/abs/2508.12282
- Authors: Ziyang Chen; Erxue Min; Xiang Zhao; Yunxin Li; Xin Jia; Jinzhi Liao; Jichao Li; Shuaiqiang Wang; Baotian Hu; Dawei Yin
- Reference count: 29
- Primary result: ChronoQA is a large-scale Chinese benchmark dataset designed to evaluate temporal reasoning in Retrieval-Augmented Generation (RAG) systems.

## Executive Summary
ChronoQA is a large-scale Chinese benchmark dataset constructed from over 300,000 news articles published between 2019 and 2024. It contains 5,176 high-quality question-answer pairs that evaluate temporal reasoning in RAG systems across absolute, aggregate, and relative temporal types. The dataset features both explicit and implicit time expressions, supports single- and multi-document scenarios, and includes structured annotations for detailed performance analysis. Created through a multi-stage pipeline involving LLM-based extraction, question synthesis, and rigorous validation, ChronoQA reveals significant performance gaps between single- and multi-document settings, establishing itself as a challenging benchmark for advancing time-sensitive RAG systems.

## Method Summary
The ChronoQA dataset was constructed through a three-stage pipeline. First, raw news articles (2019-2024) were processed into "intensive temporal paragraphs" using gpt-4o-mini to extract factual assertions and temporal markers. Second, gpt-4o generated single-temporal QA pairs from these processed paragraphs, covering absolute, aggregate, and relative temporal types with explicit and implicit expressions. Third, the system composed these pairs into complex multi-document questions using "circuit" logic (Parallel and Series compositions) and validated them through rule-based checks, LLM evaluation, and human review. The resulting dataset contains 5,176 high-quality QA pairs with 100% temporal relevance, validated by >95% human correctness and Cohen's Kappa=0.85.

## Key Results
- ChronoQA reveals significant performance gaps between single- and multi-document settings in temporal RAG systems
- Query Decomposition baseline achieves best overall performance, especially in challenging multiple-document scenarios
- Series circuits (37% of dataset) present particular difficulty due to sequential dependency requirements
- The dataset effectively stresses temporal reasoning capabilities across absolute, aggregate, and relative temporal types

## Why This Works (Mechanism)

### Mechanism 1: Temporal Density via LLM Extraction
Filtering raw documents into "intensive temporal paragraphs" via LLM extraction improves signal-to-noise ratio for temporal retrievers by reducing semantic clutter. Instead of indexing raw news articles, the system uses gpt-4o-mini to distill text into concise units containing only verifiable facts and explicit temporal anchors. This approach assumes the extraction LLM accurately preserves temporal logic without hallucinating dates. However, if the source corpus lacks explicit dates or the extraction prompt discards temporal modifiers, the resulting paragraphs will lose necessary ambiguity context.

### Mechanism 2: Circuit-Style Composition for Multi-Hop Stress Testing
Structuring multi-document questions as "Series" or "Parallel" circuits forces models to perform explicit dependency resolution rather than simple keyword matching. Series circuits require intermediate answers (e.g., specific dates) to resolve final queries, while Parallel circuits require aggregation of independent temporal facts. This architecture tests a system's ability to maintain state and logic across retrieval steps. The core assumption is that retrieval systems can decompose complex queries or maintain context across multiple retrieval steps; single-step retrieval will fail these circuit types.

### Mechanism 3: Reference-Date Grounding for Implicit Temporal Logic
Resolving implicit time expressions requires anchoring queries to specific reference timelines (`questiondate`), a feature standard RAG pipelines often lack. The dataset provides a specific `questiondate` field for every sample, allowing systems to resolve relative terms like "last year" or "recently" relative to a fixed point in time (2019-2024). This simulates real-world temporal drift. The assumption is that the model has access to the current date or specific reference date during inference to resolve relative temporal expressions.

## Foundational Learning

- Concept: **Temporal Alignment (Absolute vs. Relative)**
  - Why needed here: The dataset explicitly distinguishes between absolute times (e.g., "2020-10-13") and relative times (e.g., "last year"). Understanding this distinction is critical for designing query rewriting logic.
  - Quick check question: Does your retrieval system normalize "last year" to a specific date range before sending the query to the vector database, or does it rely on the LLM to guess the date?

- Concept: **Query Decomposition**
  - Why needed here: The paper identifies Query Decomposition as the best-performing baseline, specifically because it handles the "circuit" logic of multi-document questions better than Native RAG.
  - Quick check question: Can your system break the query "Who was president during the event mentioned in the previous article?" into two distinct retrieval steps?

- Concept: **NDCG and MAP Metrics**
  - Why needed here: The paper uses NDCG (Normalized Discounted Cumulative Gain) and MAP (Mean Average Precision) to rank retrieval quality. These metrics penalize systems that retrieve the right document but rank it low.
  - Quick check question: If your retriever finds the correct evidence but ranks it 5th, does your evaluation metric penalize the system compared to ranking it 1st?

## Architecture Onboarding

- Component map: Corpus Processor -> QA Synthesizer -> Validator -> Evaluation Harness
- Critical path: The Series Circuit Generation is the most fragile step, requiring finding "bridges" between two independent documents and synthesizing questions that link them.
- Design tradeoffs:
  - Synthetic vs. Natural: Uses LLM-generated questions rather than real user logs, ensuring 100% temporal coverage but potentially lacking real-world linguistic messiness
  - Static vs. Dynamic: Covers 2019-2024; to remain relevant, the "Source Article Preparation" pipeline must be re-run periodically
- Failure signatures:
  - Semantic Drift: Retrieving documents about the correct entity but from the wrong year
  - Implicit Resolution Failure: Answering "last year" based on training data date rather than provided `questiondate`
- First 3 experiments:
  1. Baseline Retrieval: Run standard dense retriever on full corpus without temporal processing; expect low performance on "Implicit" and "Relative" categories
  2. Query Decomposition: Implement query decomposition module to handle "Multiple Reference Document" queries (37% of data); compare NDCG scores against Native RAG baseline
  3. Temporal Filtering: Apply post-processing filter restricting retrieval to documents within `temporal_scope` derived from `questiondate`; measure lift in precision for "Short-term" vs "Long-term" queries

## Open Questions the Paper Calls Out

### Open Question 1
Can retrieval strategies be optimized to handle "series circuit" reasoning dependencies where the search query for a subsequent step is contingent on the answer of a previous one? The paper introduces "series circuits" where reasoning is sequential, and notes that current models struggle significantly with multi-document questions, suggesting current retrieval methods fail at this dependency. The baseline experiments show that while Query Decomposition improves performance, overall scores for multiple-document scenarios remain significantly lower than single-document ones. What evidence would resolve it: The development of an iterative retrieval agent that dynamically updates search queries based on intermediate answers, achieving parity between series-circuit and parallel-circuit performance.

### Open Question 2
To what extent does the use of GPT-4o for both data synthesis and validation introduce a stylistic bias that favors GPT-family models during evaluation? The Methods section describes a pipeline where gpt-4o-mini extracts "intensive temporal paragraphs" and gpt-4o generates and validates questions, creating a dataset entirely shaped by a specific model family's internal reasoning patterns. Synthetic data generation often encodes the generating model's biases or hallucinations as ground truth, potentially making the benchmark "easier" for models with similar parametric knowledge. What evidence would resolve it: A cross-model validation study analyzing whether non-GPT models systematically underperform on questions identified as having high "linguistic perplexity" relative to GPT-4o.

### Open Question 3
How does the restriction of the source corpus to news articles affect the generalizability of temporal reasoning capabilities to domains with less explicit time markers? The authors explicitly select news articles for their "inherent temporal grounding" and explicit time references, implying the benchmark does not assess reasoning over noisier or less structured temporal data found in other fields. Real-world applications often involve temporal reasoning over sources like scientific literature or legal documents where time expressions may be relative, ambiguous, or deeply embedded in technical jargon. What evidence would resolve it: Expansion of the ChronoQA framework to include scientific or legal corpora, demonstrating that current retrieval baselines maintain their performance rankings outside the news domain.

## Limitations

- Synthetic question validity remains uncertain as the dataset relies entirely on LLM-generated questions rather than real user queries, potentially not capturing real-world linguistic complexity
- Temporal drift handling is limited to 2019-2024 with fixed reference dates, not addressing how temporal expressions evolve over longer periods or future queries
- Chinese-language specificity may limit generalizability to other languages with different temporal expression patterns and cultural contexts

## Confidence

- **High Confidence**: The core dataset construction methodology (LLM extraction → QA generation → circuit composition) is clearly described and reproducible; baseline evaluation results showing performance gaps are well-documented
- **Medium Confidence**: The claim that ChronoQA effectively stresses temporal reasoning capabilities is supported by circuit architecture, but real-world relevance of synthetic questions remains partially untested
- **Low Confidence**: The assertion that Query Decomposition is universally "best" across all temporal query types is based on limited baselines and may not hold against more sophisticated temporal-aware retrieval strategies

## Next Checks

1. **Temporal Drift Test**: Evaluate the same RAG system on ChronoQA using different `questiondate` reference points to measure performance degradation when resolving "last year" or "recently" across different time periods.

2. **Real vs. Synthetic Comparison**: Generate a small subset of questions from actual user query logs and compare model performance between synthetic and real temporal queries of the same type.

3. **Cross-Lingual Transfer**: Test whether a model trained on ChronoQA can effectively handle temporal queries in English or other languages, measuring performance drop-off as an indicator of language-specific vs. universal temporal reasoning capabilities.