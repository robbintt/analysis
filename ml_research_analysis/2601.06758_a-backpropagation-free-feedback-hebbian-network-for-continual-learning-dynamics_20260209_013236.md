---
ver: rpa2
title: A Backpropagation-Free Feedback-Hebbian Network for Continual Learning Dynamics
arxiv_id: '2601.06758'
source_url: https://arxiv.org/abs/2601.06758
tags:
- feedback
- output
- forward
- pair
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that a compact backpropagation-free feedback-Hebbian
  network with local plasticity can express continual-learning-relevant behaviors
  in a controlled setting. The architecture uses dedicated feedforward and feedback
  pathways, both trained by a unified local rule combining centered Hebbian covariance,
  Oja-style stabilization, and supervised drive.
---

# A Backpropagation-Free Feedback-Hebbian Network for Continual Learning Dynamics

## Quick Facts
- arXiv ID: 2601.06758
- Source URL: https://arxiv.org/abs/2601.06758
- Reference count: 16
- Primary result: A compact backpropagation-free feedback-Hebbian network with local plasticity can express continual-learning-relevant behaviors (unlearning, retention, co-maintenance) in a simple two-pair association task.

## Executive Summary
This study introduces a compact backpropagation-free feedback-Hebbian network that can express continual-learning-relevant behaviors in a controlled setting. The architecture uses dedicated feedforward and feedback pathways, both trained by a unified local rule combining centered Hebbian covariance, Oja-style stabilization, and supervised drive. On a simple two-pair association task, the model shows three key behaviors: LTD-like unlearning at the forward output stage during sequential A→B training, retention of A-related connectivity in the feedback pathway during B acquisition, and concurrent maintenance of both associations under alternating training. Architectural controls confirm that dedicated feedback is essential for regeneration and co-maintenance, while rule-term ablations show that the supervised component is necessary for selective unlearning and conditioning.

## Method Summary
The architecture comprises two feedforward and two feedback layers, each with 10×10 weight matrices. All four weight matrices are trained by the same local plasticity rule combining centered Hebbian covariance, Oja-style stabilization, and supervised drive. The forward pathway predicts output targets, while the feedback pathway reconstructs earlier-layer activity and injects it as context. The rule is applied per sample, with exponential running means ⟨x⟩, ⟨y⟩ across phases. Two training regimes are used: sequential A→B (10 epochs each) and interleaved A,B,A,B... (10 epochs, 100 samples/epoch). Weights are initialized with variance-scaled uniform, and activity is bounded by a scaled tanh activation.

## Key Results
- The network exhibits LTD-like unlearning of A-targets in the forward output stage when training switches from A to B.
- Feedback connectivity preserves an A-related trace during B acquisition, enabling later regeneration.
- Under alternating training, both A and B associations are concurrently maintained at elevated connectivity levels.

## Why This Works (Mechanism)

### Mechanism 1
Dedicated feedback pathways can retain earlier-association traces while forward pathways adapt to new associations under sequential training. The feedback pathway is trained to reconstruct earlier-layer activity (not current output targets), so its weights encode historical input-output correlations. When the forward pathway shifts to a new association (A→B), the feedback weights for A are not forced to update toward B-related targets, preserving an "A-trace" that can later support regeneration. Core assumption: Retention requires that feedback reconstruction targets decouple from immediate supervised output targets. Evidence anchors: [abstract] "feedback connectivity preserves an A-related trace during acquisition of B"; [Section III-B] "feedback layer 2 preserved an A-related trace: input connectivity at the pair A-related sites remained near-stable during Phase B". Break condition: If feedback were trained to reconstruct current outputs rather than earlier-layer inputs, retention would collapse.

### Mechanism 2
The local supervised term in the plasticity rule enables LTD-like unlearning of previously learned associations at the forward output stage. The supervised term `(t_i - y_i) x_i` provides a local error signal that drives weights toward current targets. When training switches from A to B, this term actively depresses A-related output connectivity because the A-targets are no longer present (output activity doesn't match the now-dominant B-pattern), producing net weakening—functionally analogous to long-term depression. Core assumption: Unlearning requires an explicit error-driven component; pure Hebbian covariance + Oja normalization cannot selectively suppress non-target outputs. Evidence anchors: [abstract] "supervised component is necessary for selective unlearning and conditioning"; [Section III-E] "removing the local supervised drive disrupted output selectivity, impairing both LTD-like unlearning under sequential training". Break condition: Remove the supervised term; A-related connectivity will persist inappropriately at the output stage during B-training.

### Mechanism 3
Temporal alternation of associations enables concurrent co-maintenance in both pathways. When A and B samples alternate within epochs, both sets of input-output correlations are reinforced before either decays significantly. The Oja term prevents runaway growth, while centered covariance prevents one pattern from dominating. Both associations stabilize at elevated connectivity levels rather than one suppressing the other. Core assumption: Co-maintenance depends on interleaving frequency exceeding the effective unlearning timescale. Evidence anchors: [abstract] "concurrent maintenance of both associations under alternating training"; [Section III-C] "connectivity to the pair A target outputs and to the pair B target outputs increases during training and remains elevated". Break condition: If alternation interval exceeds retention window (controlled by α, learning rate, Oja decay), one association will still dominate.

## Foundational Learning

- **Concept: Hebbian covariance learning**
  - Why needed here: The core plasticity rule uses centered `(x - ⟨x⟩)(y - ⟨y⟩)` covariance rather than raw product; understanding why centering prevents high-activity units from dominating is essential.
  - Quick check question: Why does centering activity help mitigate interference across training phases?

- **Concept: Oja-style stabilization**
  - Why needed here: The rule includes `-β(y - ⟨y⟩)² w` to bound weight growth; without this, Hebbian learning diverges under recurrent feedback.
  - Quick check question: What happens to synaptic scale if the Oja term is removed during feedback-driven training?

- **Concept: Forward vs. feedback pathway roles**
  - Why needed here: The architecture separates prediction (forward) from reconstruction/context injection (feedback); their targets differ, enabling the retention mechanism.
  - Quick check question: If feedback were trained to reconstruct the current output instead of the earlier layer, would retention still work?

## Architecture Onboarding

- **Component map:**
  - Input (10-dim) → Forward Layer 1 (10×10, unsupervised) → Forward Layer 2 (10×10, supervised toward task targets) → Output (10-dim)
  - Forward Layer 2 activity → Feedback Layer 2 (10×10, reconstructs FL1 input) → Feedback Layer 1 (10×10, reconstructs external input) → Additive context injection to forward layers at t+1
  - All four weight matrices trained by the same local rule (Eq. 1).

- **Critical path:**
  1. Initialize weights (fan-in scaled uniform).
  2. For each sample, compute forward pass; at t≥1, add feedback reconstruction from previous step.
  3. Apply Eq. 1 to each matrix using activities and available targets.
  4. Update exponential running means ⟨x⟩, ⟨y⟩ across phases.
  5. Log connectivity trajectories (row/column means) and retention indices.

- **Design tradeoffs:**
  - Depth: 2FF+2FB sufficient for two-pair task; deeper (3FF+3FB) adds no observable benefit here.
  - Feedback presence: Removing feedback abolishes regeneration and destabilizes conditioning under alternation (Section III-D).
  - Supervised term: Required for selective unlearning; removing it impairs both sequential suppression and concurrent maintenance.

- **Failure signatures:**
  - Weights grow unbounded → check Oja term (β too small or missing).
  - No retention in feedback during sequential training → verify feedback is trained to reconstruct earlier-layer input, not output.
  - Conditioning fails (one association dominates) → check alternation schedule or supervised term presence.
  - Non-target outputs remain active → supervised term may be disabled or learning rate too low.

- **First 3 experiments:**
  1. **Single-association baseline:** Train on pair A only; verify output concentrates on targets 8,9 and feedback regenerates input site 3. Confirm weight trajectories show selective strengthening.
  2. **Sequential A→B with retention measurement:** Run Phase 1 (A only), Phase 2 (B only). Plot forward layer 2 output connectivity and feedback layer 2 input connectivity. Compute retention index R for A-related sites in feedback—should be near 0 (minimal Phase 2 change).
  3. **Ablation sanity check:** Remove supervised term from Eq. 1; re-run sequential and interleaved protocols. Verify LTD-like unlearning fails (A-targets not suppressed in sequential) and conditioning fails (one association dominates under alternation).

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the local feedback-Hebbian learning dynamics demonstrated on a two-pair association task scale to higher-dimensional inputs, longer task sequences, structured task overlap, and noisier settings? Basis: Outlook section states the task is minimal and scaling behavior was not tested. What evidence would resolve it: Replicate sequential and interleaved protocols on multi-class datasets while tracking layer-wise retention metrics.

- **Open Question 2:** What scheduling strategies and task regimes would make hybrid training (local feedback-Hebbian pretraining followed by gradient-based optimization) beneficial compared to either approach alone? Basis: Outlook section proposes hybridization but does not implement or evaluate any combined training scheme. What evidence would resolve it: Systematic experiments varying duration of local pretraining, timing of gradient-based fine-tuning, and nature of auxiliary reconstruction objective.

- **Open Question 3:** Can coupling the feedback reconstruction pathway to a persistent storage mechanism support episodic-memory-like retrieval of previously encountered internal states across extended time horizons? Basis: Outlook section mentions coupling reconstruction pathway to persistent storage for episodic-memory-like operation. What evidence would resolve it: Augment the model with a storage buffer and test on delayed recall or replay-consolidation protocols.

- **Open Question 4:** How robust are the observed continual-learning primitives to alternative reconstruction targets, partial or gated feedback, and sparsity constraints? Basis: Outlook section mentions testing additional biologically motivated motifs like alternative reconstruction targets. What evidence would resolve it: Ablation studies varying reconstruction objectives and measuring retention indices under sequential and interleaved protocols.

## Limitations
- The task is highly constrained (10×10 arrays, binary inputs) and may not scale to naturalistic domains.
- Retention metrics rely on artificial indices; functional significance beyond pattern reconstruction is not demonstrated.
- No comparison to standard continual learning baselines to contextualize performance.

## Confidence
- High confidence in the core mechanism: dedicated feedback pathways preserving historical traces while forward pathways adapt is well-supported by connectivity and retention data.
- Medium confidence in the unlearning claim: LTD-like suppression is observed, but the analogy to biological LTD is qualitative rather than mechanistic.
- Medium confidence in co-maintenance: Alternating training stabilizes both associations, but the critical alternation frequency relative to unlearning timescale is not quantified.

## Next Checks
1. Vary the alternation interval (e.g., 10, 50, 100 samples between pairs) to empirically map the retention window and determine the threshold for catastrophic interference.
2. Replace the 10×10 synthetic task with a small real-world continual learning benchmark to test scalability and robustness.
3. Implement and compare against a local replay mechanism to isolate the benefit of pure local plasticity versus memory-based stabilization.