---
ver: rpa2
title: Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding
arxiv_id: '2602.00781'
source_url: https://arxiv.org/abs/2602.00781
tags:
- lookahead
- reward
- k-step
- state
- lg1t
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles online reinforcement learning in non-episodic
  finite-horizon Markov Decision Processes, where the agent must learn to maximize
  cumulative rewards within a fixed, non-resetting trajectory. The challenge is that
  standard RL methods struggle due to the need to estimate returns over the full horizon
  with only one trajectory.
---

# Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding

## Quick Facts
- **arXiv ID**: 2602.00781
- **Source URL**: https://arxiv.org/abs/2602.00781
- **Reference count**: 40
- **Key outcome**: Achieves minimax optimal constant regret for K=1 and O(max((K-1),C_{K-1})√(SAT log(T))) regret for K≥2 in non-episodic finite-horizon RL

## Executive Summary
This paper addresses the challenge of online reinforcement learning in non-episodic finite-horizon Markov Decision Processes, where standard methods struggle due to the need to estimate returns over the full horizon from a single trajectory. The authors propose learning a K-step lookahead Q-function combined with a thresholding mechanism that selects actions only when their estimated value exceeds a time-varying threshold. They introduce the LGKT algorithm, which achieves optimal constant regret for K=1 and sublinear regret for K≥2, significantly outperforming existing tabular RL methods across synthetic and benchmark environments.

## Method Summary
The core approach learns a K-step lookahead Q-function that truncates planning to the next K steps, dramatically reducing sample complexity compared to full-horizon estimation. For K=1, the method uses a lower confidence bound (LCB) thresholding mechanism with g(t) = 3log(t) to rapidly eliminate sub-threshold actions, achieving constant regret. For K≥2, a structured exploration subroutine estimates K-1 step rewards without contaminating the main policy's decisions. The algorithm balances exploration and exploitation through an epsilon-greedy framework with probability inversely proportional to visit counts, maintaining theoretical guarantees while achieving practical performance.

## Key Results
- Achieves minimax optimal constant regret for K=1 with threshold-based action selection
- Proves O(max((K-1),C_{K-1})√(SAT log(T))) regret bound for any K≥2
- Demonstrates superior cumulative rewards compared to state-of-the-art tabular RL methods
- Validated across synthetic MDPs and environments including JumpRiverswim, FrozenLake, and AnyTrading

## Why This Works (Mechanism)

### Mechanism 1: Truncated Planning Horizon Reduces Estimation Variance
Learning a K-step lookahead Q-function instead of the full-horizon Q-function dramatically reduces sample complexity in non-episodic settings. Standard finite-horizon RL must estimate Q-values spanning the entire horizon T from a single trajectory. By truncating to K steps, the learning target becomes shorter-horizon, reducing variance. When K=1, the problem reduces to a contextual bandit (states as context), which has provably lower sample complexity.

### Mechanism 2: Lower Confidence Bound Thresholding Eliminates Bad Actions Fast
Using LCB (not empirical mean) against a threshold achieves minimax optimal constant regret for K=1. The algorithm constructs a candidate set of actions passing the threshold using LCB with g(t) = 3log(t). Sub-threshold actions are eliminated rapidly—their LCB drops below threshold quickly. This differs from prior thresholding bandit work that uses empirical means.

### Mechanism 3: Structured Exploration via Subroutine Decouples Estimation from Main Policy
For K≥2, the algorithm achieves sublinear regret by using a separate sampling subroutine to estimate K-1 step rewards without contaminating the main policy's decisions. K-step reward decomposes into 1-step reward + expected (K-1)-step continuation. The algorithm uses epsilon-greedy exploration to trigger an independent algorithm for K-1 consecutive steps, collecting unbiased samples while bounding regret from exploration.

## Foundational Learning

- **Concept: Q-learning and Bellman Optimality Equations**
  - Why needed here: The paper's K-step lookahead reward is defined via Bellman equations. Understanding how Q-values decompose across time is essential.
  - Quick check question: Can you derive why Q*_h(s,a) = R_{s,a} + E_{s'}[V*_{h+1}(s')] implies that K-step lookahead requires only K recursive evaluations?

- **Concept: Lower/Upper Confidence Bounds in Bandits**
  - Why needed here: The core innovation uses LCB (not UCB) for thresholding. The tighter g(t) = 3log(t) bound enables constant-time elimination of bad actions.
  - Quick check question: Why does using LCB instead of empirical mean for threshold testing lead to faster elimination of sub-threshold actions?

- **Concept: Regret Definitions for Non-Standard Objectives**
  - Why needed here: The paper defines regret against π_{K,γ} (K-step thresholding policy), not the optimal policy π^*. This is intentional—competing against π^* has linear lower bound.
  - Quick check question: Why is the regret defined as E_{π_{K,γ}}[∑c(s_t,a_t)] - E_π[∑c(s_t,a_t)] with cost c(s_t,a_t) = (γ_t - r_{s_t,a_t})1{r_{s_t,a_t} < γ_t} instead of the standard reward-based definition?

## Architecture Onboarding

- **Component map:**
  ```
  LGKT (Algorithm 2)
  ├── State-action visit counters: N^{(t)}_{s,a}, N^{(t)}_{s,a,K-1}
  ├── Empirical reward trackers: \hat{\phi}^1_{s,a}, \hat{\phi}^{K-1}_{s,a}
  ├── LCB calculator: LCB^{(t)}_{s,a} = \hat{r}^1 + \hat{r}^{K-1} - \sqrt{g(N_{s,a}+2)/(N_{s,a}+2)} - \sqrt{g(N_{s,a,K-1}+2)/(N_{s,a,K-1}+2)}
  ├── Threshold tester: \tilde{G}_t = {a: LCB^{(t)}_{s_t,a} >= \gamma_t}
  ├── Exploration controller: \epsilon_t = min(1, 1/(N^{(t)}_{s_{t-1},a_{t-1}}+1)^{p \cdot min(\eta, 1/2)})
  └── Subroutine: ALG_{K-1}(·|s_{t-1}, a_{t-1}) — runs for K-1 steps to estimate r^{K-1}
  ```

- **Critical path:**
  1. Observe state s_t
  2. Compute LCB for all actions at current state
  3. Build candidate set \tilde{G}_t of actions passing threshold
  4. With probability \epsilon_t: trigger exploration subroutine (K-1 steps)
  5. With probability 1-\epsilon_t: select action from \tilde{G}_t by highest LCB, or uniformly if empty
  6. Observe reward and next state, update counters and LCB

- **Design tradeoffs:**
  - K vs. convergence speed: Smaller K = faster convergence (constant regret for K=1) but potentially suboptimal oracle
  - Threshold γ level: Lower γ = faster convergence but selects from more actions, potentially lower cumulative reward
  - Subroutine choice: UCB for K=2 (C_{K-1} = A), UCBVI-BF for K>2 (C_{K-1} = K^2 SA)

- **Failure signatures:**
  - No actions pass threshold: If \tilde{G}_t is always empty, algorithm falls back to uniform random selection. Reduce γ
  - Linear regret trajectory: If C_{K-1}√(SA) >> √(T), regret becomes dominated by exploration term
  - Slow convergence despite K=1: Check that gap Δ^+_1 isn't extremely small
  - Continuous state spaces: Tabular method requires state recurrence; not a failure per se but requires state discretization

- **First 3 experiments:**
  1. Validate K=1 on synthetic MDP: Generate random MDPs (S=10, A=5), run LG1T with γ=0.3, compare running average reward against Q-learning and UCRL2
  2. Test threshold sensitivity: Run LG1T on same MDP with γ ∈ {0.1, 0.3, 0.5, 0.7, 0.9}. Plot cumulative reward vs. time
  3. Verify exploration subroutine for K=2: On JumpRiverSwim (S=5), run LG2T with UCB as subroutine. Track both main policy regret and subroutine regret separately

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the K-step lookahead thresholding approach be extended to deep RL settings with function approximation while preserving fast convergence guarantees?
  - Basis: Conclusion states "Extending this work to deep learning methods is an important direction for future research"
  - Why unresolved: Current theoretical analysis relies on tabular assumptions with count-based confidence bounds
  - What evidence would resolve it: Theoretical analysis extending regret guarantees to function approximation or empirical demonstration on deep RL benchmarks

- **Open Question 2**: What is the principled method for selecting the optimal switching time in adaptive K algorithms (e.g., LG1-2T)?
  - Basis: The adaptive variant LG1-2T uses ad-hoc switch times (t_c = 100, 10000, 30 for different environments) without theoretical justification
  - Why unresolved: Paper provides no formal criterion for when to increase K; choices appear tuned per environment
  - What evidence would resolve it: Theoretical characterization of optimal switching times based on horizon length, state space size, and estimation variance

- **Open Question 3**: Can the stochastic dominance assumption (Assumption 3.2) for optimal K-step greedy policies be relaxed while maintaining sublinear regret?
  - Basis: Theorem 3.3 proves optimality under stochastic dominance for binary states, but Theorem 3.4 shows linear optimality gap exists without this assumption
  - Why unresolved: Gap between the two theoretical results leaves unclear whether intermediate structural assumptions could guarantee partial optimality
  - What evidence would resolve it: Identification of weaker sufficient conditions that guarantee bounded suboptimality

## Limitations
- The approach relies critically on Assumptions 4.1 and 4.3 which may not hold in environments with sparse rewards or rapidly changing dynamics
- Tabular method requires state recurrence, problematic in continuous or large state spaces
- Choice of K involves fundamental tradeoff between convergence speed and oracle performance, with worst-case optimality gap becoming linear when K << T

## Confidence
- **High**: K=1 regret bound (Theorem 4.2) and constant regret claim - supported by explicit proof and matching lower bound
- **Medium**: K≥2 regret bound (Theorem 4.4) - depends on unspecified instance-dependent parameter C_{K-1} and subroutine efficiency
- **Medium**: Empirical performance claims - based on 100 repetitions but benchmark hyperparameters and random seeds unspecified

## Next Checks
1. **Threshold Sensitivity Analysis**: Run LG1T on synthetic MDPs with varying γ values to empirically verify the tradeoff between convergence speed and cumulative reward
2. **Subroutine Regret Decomposition**: For K=2, instrument LG2T to separately track main policy regret and exploration subroutine regret, confirming the O(A√(STlog(T))) decomposition
3. **Assumption Robustness Test**: Evaluate LGKT performance on MDPs with (a) sparse rewards, (b) non-stationary dynamics, and (c) extremely small action gaps to assess sensitivity to key assumptions