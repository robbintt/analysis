---
ver: rpa2
title: 'ms-Mamba: Multi-scale Mamba for Time-Series Forecasting'
arxiv_id: '2504.07654'
source_url: https://arxiv.org/abs/2504.07654
tags:
- mamba
- ms-mamba
- arxiv
- forecasting
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-scale Mamba (ms-Mamba), a novel architecture
  for time-series forecasting that processes input data at multiple temporal scales
  using several Mamba blocks with different sampling rates. The authors address the
  limitation of existing architectures that process time-series data at a single temporal
  scale by incorporating multiple SSMs with different sampling rates to capture multi-scale
  information.
---

# ms-Mamba: Multi-scale Mamba for Time-Series Forecasting

## Quick Facts
- arXiv ID: 2504.07654
- Source URL: https://arxiv.org/abs/2504.07654
- Authors: Yusuf Meric Karadag; Sinan Kalkan; Ipek Gursel Dino
- Reference count: 40
- Primary result: Multi-scale Mamba outperforms S-Mamba on 13 time-series forecasting benchmarks while using fewer parameters and less memory

## Executive Summary
This paper introduces Multi-scale Mamba (ms-Mamba), a novel architecture for time-series forecasting that processes input data at multiple temporal scales using several Mamba blocks with different sampling rates. The authors address the limitation of existing architectures that process time-series data at a single temporal scale by incorporating multiple State Space Models (SSMs) with different sampling rates to capture multi-scale information. Experiments on thirteen benchmark datasets show that ms-Mamba outperforms state-of-the-art methods including S-Mamba, achieving significant improvements particularly on traffic datasets, while maintaining efficiency advantages in terms of parameters and computational overhead.

## Method Summary
ms-Mamba processes time-series data through multiple parallel Mamba blocks operating at different temporal sampling rates (Δ₁, Δ₂, ..., Δₙ). The architecture employs an embedding layer that maps input sequences to a fixed dimension, followed by multi-scale Mamba modules that apply Mamba blocks with distinct sampling rates in parallel, averaging their outputs. The model uses bidirectional processing to capture both causal and anti-causal dependencies. Three strategies for obtaining different sampling rates are explored: fixed multipliers of a learned base rate (α = 1, 2, 4, 8), learnable rates, and dynamic rates estimated from input embeddings. The method maintains the selective state space mechanism of Mamba while extending it to capture multi-scale temporal patterns.

## Key Results
- ms-Mamba achieves 0.229 MSE on Solar-Energy dataset compared to 0.240 MSE for S-Mamba
- Outperforms S-Mamba on 13 benchmark datasets with significant gains on traffic datasets
- Uses fewer parameters, less memory, and reduced computational overhead than baseline
- Fixed multipliers (1, 2, 4, 8) provide optimal multi-scale configuration based on ablation study
- Learnable temporal scales offer slightly better performance with fewer hyperparameters to tune

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Processing time-series at multiple temporal scales captures complementary patterns that single-scale approaches miss.
- Mechanism: Multiple parallel Mamba blocks operate with different sampling rates (Δ₁, Δ₂, ..., Δₙ), where slower rates capture long-term trends and faster rates capture fine-grained dynamics. Their outputs are averaged to form a unified representation.
- Core assumption: The time-series contains information distributed across multiple temporal scales that cannot be adequately represented by a single sampling rate.
- Evidence anchors: [abstract] "ms-Mamba incorporates multiple temporal scales by using multiple Mamba blocks with different sampling rates (Δs)" and [section 4.2] "El_m = Avg(Mamba(El; Δ₁), ..., Mamba(El; Δₙ))"

### Mechanism 2
- Claim: Fixed multipliers of a learned base rate (α = 1, 2, 4, 8) provide the most robust multi-scale configuration.
- Mechanism: A base sampling rate Δ₁ is learned via backpropagation; other rates are derived as Δᵢ = αᵢ × Δ₁. This constrains the scale space while allowing the model to adapt the base rate to the data.
- Core assumption: Optimal scales follow a geometric progression from some learned base, rather than arbitrary independent values.
- Evidence anchors: [section 5.3, Table 4] "using fixed temporal scales with coefficients (1, 2, 4, 8) provides the best results" with 6 best and 2 second-best results out of 12 configurations

### Mechanism 3
- Claim: Bidirectional processing with reversed input captures both causal and anti-causal dependencies for forecasting.
- Mechanism: The Multi-scale Mamba module is applied in both forward and reverse directions, allowing the model to condition on both past-to-present and present-to-past transitions.
- Core assumption: Forecasting benefits from modeling temporal dependencies in both directions, not just autoregressive forward processing.
- Evidence anchors: [section 4.2] "we employ our Multi-scale Mamba module in both directions as illustrated in Figure 2, following prior work"

## Foundational Learning

- Concept: **State Space Models (SSMs) and Discretization**
  - Why needed here: ms-Mamba builds on continuous-to-discrete SSM transformation via sampling rate Δ. Understanding Equations 2-5 is prerequisite to grasping how varying Δ changes temporal resolution.
  - Quick check question: Given discrete transition Ā = exp(ΔA), what happens to temporal dynamics when Δ increases?

- Concept: **Multi-scale Signal Processing**
  - Why needed here: The core hypothesis is that time-series contain multi-scale information. Familiarity with scale-space theory, wavelets, or multi-resolution analysis helps understand why parallel processing at different rates is sensible.
  - Quick check question: Why might a traffic dataset benefit from multiple scales more than an exchange-rate dataset?

- Concept: **Mamba Architecture and Selective SSMs**
  - Why needed here: ms-Mamba modifies Mamba's sampling rate strategy but inherits its selective state space mechanism. Understanding content-dependent reasoning in Mamba clarifies what is preserved vs. changed.
  - Quick check question: How does Mamba's content-based selection differ from Transformer attention, and does ms-Mamba alter this mechanism?

## Architecture Onboarding

- Component map: Input (L×D) → Embedding Layer (L → Dₑ) → Multi-scale Mamba Layer [parallel blocks at Δ₁, Δ₂, Δ₃, Δ₄] → Avg → LayerNorm → MLP → Next layer or Linear Projection → Output (F×D)

- Critical path:
  1. Embedding reduces variable-length input to fixed token length Dₑ
  2. Four parallel Mamba blocks with distinct Δ values process the same embedding
  3. Averaging combines multi-scale features; bidirectional pass repeats this in reverse
  4. LayerNorm + MLP provide nonlinearity between encoder blocks
  5. Final linear projection maps to forecast horizon F

- Design tradeoffs:
  - Fixed vs. learnable Δ: Fixed multipliers (1, 2, 4, 8) offer slightly better peak performance but require hyperparameter tuning; learnable Δ is simpler with marginally lower performance
  - Number of scales: Four scales used in experiments; more scales increase parameters and may overfit
  - Bidirectional vs. unidirectional: Bidirectional improves accuracy but doubles computation and memory

- Failure signatures:
  - If MSE does not improve over single-scale Mamba, check whether dataset has dominant single-scale structure
  - If learnable Δ collapses to similar values across blocks, multi-scale benefit is lost; consider fixed multipliers
  - If memory spikes on high-variate datasets (e.g., Traffic with 862 variables), note that ms-Mamba may use more parameters than S-Mamba in this regime

- First 3 experiments:
  1. Replicate the ablation on ETTh2 with fixed multipliers (1, 2, 4, 8) vs. (0.5, 1, 1.5, 2) to validate scale-sensitivity on your data
  2. Compare learnable vs. fixed Δ on a small dataset to determine which strategy suits your tuning budget
  3. Profile memory and MACs on your target variate count; if D > 500, verify parameter efficiency claims hold

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ms-Mamba architecture effectively generalize to modalities beyond time-series, such as text and images?
- Basis in paper: [explicit] The authors explicitly state in "Limitations and Future Work" that "It is a promising research direction to apply ms-Mamba for other types of modalities, e.g., text and images."
- Why unresolved: The current study restricts validation to thirteen time-series forecasting benchmarks.
- What evidence would resolve it: Successful application and competitive benchmarking of ms-Mamba on standard natural language processing and computer vision datasets.

### Open Question 2
- Question: Can ms-Mamba be effectively combined with other deep learning modules like scaled-dot-product attention or linear attention?
- Basis in paper: [explicit] The conclusion suggests that "ms-Mamba can complement other types of deep modules, e.g., scaled-dot-product attention, linear attention."
- Why unresolved: The paper evaluates ms-Mamba as a standalone architecture without exploring hybrid models.
- What evidence would resolve it: Implementation and evaluation of hybrid models integrating ms-Mamba blocks with attention mechanisms to assess synergistic performance.

### Open Question 3
- Question: How can the architecture be optimized to maintain parameter efficiency when handling high-dimensional datasets with a large number of variates?
- Basis in paper: [inferred] In Experiment 3, the authors note that on the Traffic dataset (862 variates), ms-Mamba achieves better accuracy but fails to maintain the efficiency gains (parameters, memory, MACs) seen in lower-dimensional datasets like ETTh2.
- Why unresolved: The current parameter scaling appears unfavorable for high-dimensional inputs compared to the S-Mamba baseline.
- What evidence would resolve it: Architectural modifications that reduce parameter counts for high-variate data while preserving the multi-scale performance benefits.

## Limitations
- The paper leaves critical design choices underspecified, particularly embedding dimensions, hidden sizes, and exact Mamba block parameters
- The ablation on fixed multipliers (1, 2, 4, 8) lacks comparison against alternative multi-scale strategies like wavelet decomposition
- Memory efficiency claim is dataset-dependent - ms-Mamba can use more parameters on high-variate cases like Traffic with 862 variables

## Confidence

- **High confidence**: Multi-scale processing with fixed multipliers (1, 2, 4, 8) provides superior performance - supported by systematic ablation showing 6 best and 2 second-best results out of 12 configurations
- **Medium confidence**: Learnable temporal scales offer a simpler alternative with only slight performance degradation - based on single dataset comparison
- **Medium confidence**: ms-Mamba achieves efficiency gains (fewer parameters, less memory, reduced computation) - claim holds for most datasets but not universally

## Next Checks

1. Replicate the ablation study on ETTh2 with fixed multipliers (1, 2, 4, 8) vs. alternative scale configurations to verify scale-sensitivity on your specific data
2. Profile memory and MACs on your target variate count - if D > 500, verify parameter efficiency claims hold as they may break down on high-variate datasets
3. Compare ms-Mamba's fixed multiplier strategy against wavelet or Fourier-based multi-scale approaches to establish whether the Mamba-specific implementation provides unique advantages