---
ver: rpa2
title: Compressing Vision Transformers in Geospatial Transfer Learning with Manifold-Constrained
  Optimization
arxiv_id: '2601.08882'
source_url: https://arxiv.org/abs/2601.08882
tags:
- low-rank
- training
- dlrt
- learning
- geospatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of compressing large vision
  transformer-based geospatial foundation models for deployment on resource-constrained
  edge devices while maintaining high downstream task performance. The authors propose
  using the dynamical low-rank training (DLRT) framework to compress these models
  during transfer learning by enforcing structured low-dimensional parameterizations
  aligned with downstream objectives.
---

# Compressing Vision Transformers in Geospatial Transfer Learning with Manifold-Constrained Optimization

## Quick Facts
- **arXiv ID:** 2601.08882
- **Source URL:** https://arxiv.org/abs/2601.08882
- **Reference count:** 28
- **Primary result:** DLRT compresses ViT-based geospatial models by 64-82% while maintaining <1% accuracy drop during transfer learning.

## Executive Summary
This paper addresses the challenge of compressing large vision transformer-based geospatial foundation models for deployment on resource-constrained edge devices while maintaining high downstream task performance. The authors propose using the dynamical low-rank training (DLRT) framework to compress these models during transfer learning by enforcing structured low-dimensional parameterizations aligned with downstream objectives. They compare DLRT against standard low-rank adaptation methods like LoRA across multiple remote sensing benchmarks (UCM, AID, NWPU) using both ImageNet-21k and OReole-MR pretrained models. The results show that DLRT consistently outperforms LoRA, achieving comparable or better validation accuracy while removing 64-82% of parameters.

## Method Summary
The authors employ Dynamical Low-Rank Training (DLRT) to compress vision transformers during transfer learning for geospatial classification. DLRT parameterizes weight matrices using low-rank factorizations (W = USV^T) and performs optimization on the manifold of low-rank matrices. During training, latent coefficients are updated while basis matrices U and V are periodically re-orthonormalized and truncated based on singular value thresholds. The method is compared against LoRA across three remote sensing benchmarks using ViT-B16, ViT-H14, and OReole-MR pretrained models. Hyperparameter sweeps optimize rank, truncation threshold, learning rate, and training epochs to maximize compression while preserving accuracy.

## Key Results
- DLRT achieves 64-82% parameter reduction while maintaining <1% accuracy drop compared to baseline models
- On UCM dataset, DLRT preserves accuracy within 0.5-1% of baseline while removing 68-81% of parameters
- DLRT consistently outperforms LoRA across all tested datasets and model configurations
- ImageNet-21k pretrained models compress more effectively than OReole-MR models, though OReole-MR achieves higher baseline accuracy

## Why This Works (Mechanism)
DLRT works by constraining the optimization to a low-dimensional manifold that aligns with downstream task objectives, preventing overfitting to noise while maintaining task-relevant representations. The dynamical truncation and orthonormalization steps maintain numerical stability during training while the low-rank factorization inherently reduces model capacity in a structured way. This contrasts with LoRA's static low-rank adaptation which may not capture the full optimization space needed for complex geospatial tasks.

## Foundational Learning
- **Dynamical Low-Rank Training (DLRT):** Optimization on low-rank matrix manifolds during training; needed for understanding the core compression mechanism; quick check: verify that W = USV^T factorization is maintained throughout training
- **Manifold-constrained optimization:** Optimization restricted to specific geometric structures; needed to understand why DLRT outperforms unconstrained methods; quick check: confirm that U and V bases are orthonormalized periodically
- **Low-rank adaptation (LoRA):** Static low-rank parameterization added to pretrained models; needed as baseline comparison; quick check: verify compression ratios match between DLRT and LoRA implementations
- **SVD truncation:** Singular value decomposition with rank reduction; needed for the dynamical truncation step in DLRT; quick check: confirm truncation threshold τ is applied correctly
- **Transfer learning in remote sensing:** Adapting pretrained models to specialized geospatial datasets; needed to contextualize the problem domain; quick check: verify dataset splits and preprocessing match specifications
- **Vision transformer architecture:** Self-attention based models for image classification; needed to understand model structure being compressed; quick check: confirm model configurations (patch size, embedding dimension) match reported values

## Architecture Onboarding
**Component map:** Dataset loaders -> Model initialization (ViT variants) -> DLRT layer implementation -> Training loop (10 epochs) -> Evaluation
**Critical path:** Data preprocessing → Model + DLRT setup → Training with basis updates → Final evaluation
**Design tradeoffs:** DLRT offers better accuracy-preservation vs. LoRA at cost of implementation complexity and hyperparameter sensitivity; OReole-MR provides better baseline accuracy but is harder to compress
**Failure signatures:** Accuracy drop >5% suggests improper DLRT implementation or missing warm-up epoch for self-supervised models; training instability indicates learning rate too high
**First experiments:** 1) Implement DLRT on ViT-B16 with UCM dataset at rank-100 initialization; 2) Compare accuracy vs. baseline at compression ratios 70-80%; 3) Run LoRA baseline with matched compression ratios for comparison

## Open Questions the Paper Calls Out
None

## Limitations
- OReole-MR pretrained checkpoints are not publicly available, limiting reproducibility of self-supervised model results
- Exact DLRT implementation details are not provided, requiring reverse-engineering from cited literature
- Comparison to LoRA lacks specification of exact ranks, alpha values, and target modules, making fair benchmarking difficult

## Confidence
- **High confidence:** ImageNet-21k DLRT results on UCM, AID, NWPU datasets (reproducible with public checkpoints and specified hyperparameters)
- **Medium confidence:** Generalization across different ViT architectures and pretrained models (depends on OReole-MR availability and exact implementation)
- **Low confidence:** Absolute compression ratios and efficiency claims (require precise implementation matching the authors' DLRT variant)

## Next Checks
1. Implement DLRT with s*=181, τ=0.0176, λ=1.33e-4, γ=0.025 on UCM dataset with ViT-B16 and verify <1% accuracy drop at 68-81% compression
2. Reproduce LoRA baseline at matching compression ratios (rank ~32-64) to validate relative performance claims
3. Test OReole-MR model if checkpoints become available, verifying the need for warm-up epoch and comparing ImageNet-21k vs self-supervised performance at similar compression levels