---
ver: rpa2
title: 'PRIOT: Pruning-Based Integer-Only Transfer Learning for Embedded Systems'
arxiv_id: '2503.16860'
source_url: https://arxiv.org/abs/2503.16860
tags:
- training
- priot
- accuracy
- priot-s
- integer-only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of on-device transfer learning
  on microcontrollers without floating-point units, which require integer-only training.
  The key difficulty is that existing integer-only training methods with static quantization
  scales often fail due to training collapse.
---

# PRIOT: Pruning-Based Integer-Only Transfer Learning for Embedded Systems

## Quick Facts
- arXiv ID: 2503.16860
- Source URL: https://arxiv.org/abs/2503.16860
- Reference count: 12
- Primary result: On-device transfer learning on Raspberry Pi Pico achieves 8.08-33.75 percentage points higher accuracy than existing static-scale integer-only methods

## Executive Summary
This paper addresses the challenge of on-device transfer learning on microcontrollers without floating-point units, which require integer-only training. The key difficulty is that existing integer-only training methods with static quantization scales often fail due to training collapse. To overcome this, the authors propose PRIOT, a novel pruning-based integer-only training method that freezes pre-trained weights and optimizes the network by pruning edges using the edge-popup algorithm. This approach prevents the instability caused by updating weights and ensures stable training with static scales. Experiments on rotated MNIST and CIFAR-10 datasets demonstrate that PRIOT achieves significantly higher accuracy compared to existing methods, while PRIOT-S reduces memory usage with minimal accuracy loss.

## Method Summary
PRIOT is a pruning-based integer-only training method that freezes pre-trained weights and optimizes networks by pruning selected edges rather than updating weights. The method uses the edge-popup algorithm to assign and update scores for each edge, with a mask based on top-k% scores determining which edges contribute during forward pass. During backward pass, the non-differentiable mask operation is bypassed, allowing gradient flow to scores proportional to weight-gradient interactions. PRIOT-S, a memory-efficient variant, assigns scores only to a subset of edges to reduce memory footprint. The method is evaluated on a Raspberry Pi Pico with tiny CNN models on rotated MNIST and VGG11 on rotated CIFAR-10 datasets.

## Key Results
- PRIOT achieves 8.08 to 33.75 percentage points higher accuracy compared to existing static-scale integer-only training methods
- PRIOT-S reduces memory usage by ~28% with only 3-6 percentage points accuracy loss
- On rotated MNIST 45°, PRIOT achieves 91.89% accuracy while static-scale methods collapse to 11-15%
- Weight-based edge selection in PRIOT-S provides better accuracy than random selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing pre-trained weights and training only pruning patterns prevents training collapse under static-scale integer-only quantization.
- Mechanism: Weight updates in static-scale integer-only training cause activation distribution shifts that overflow the fixed representable range, leading to incorrect gradient signals and weight updates in wrong directions. By freezing weights and only pruning edges, activation magnitudes remain bounded within the original quantization range, avoiding overflow cascades.
- Core assumption: Pre-trained weights already encode useful representations; adaptation requires only structural refinement, not weight magnitude changes.
- Evidence anchors:
  - [abstract] "optimizes the network by pruning selected edges rather than updating weights, allowing effective training with static scale factors"
  - [section II.B] "We believe that the training collapsed because the model outputs became inaccurate due to overflow, resulting in inaccurate feedback to the model and leading to model weight updates in the wrong direction"
  - [corpus] Related work (NITRO-D, InTreeger) addresses integer-only training but relies on dynamic rescaling; corpus lacks direct evidence for pruning-as-training approach.

### Mechanism 2
- Claim: The edge-popup algorithm enables gradient-based optimization of discrete pruning decisions through continuous score parameters.
- Mechanism: A score parameter is assigned to each edge. During forward pass, a mask based on top-k% scores determines which edges contribute. During backward pass, the non-differentiable mask operation is bypassed, allowing gradient flow to scores proportional to weight-gradient interactions. This enables learning which edges to retain without modifying weight values.
- Core assumption: The gradient approximation δS = W ⊙ ((δy)x⊤) sufficiently captures edge importance without mask differentiation.
- Evidence anchors:
  - [section III.A] "The pruning pattern is determined by the edge-popup algorithm, which trains a parameter named score assigned to each edge instead of the original parameters"
  - [section III.A] Equations 1-4 show forward pass with mask and gradient approximation for score updates
  - [corpus] Corpus does not contain edge-popup algorithm variants; mechanism is unique to this work within retrieved neighbors.

### Mechanism 3
- Claim: Restricting scores to a subset of edges (PRIOT-S) reduces memory with bounded accuracy loss.
- Mechanism: By pre-selecting a subset of edges to receive scores (based on random selection or weight magnitude), memory for score storage is reduced proportionally. Edges without scores are never pruned, preserving base model capacity while still allowing learned adaptation through scored edges.
- Core assumption: Important adaptations can be captured by modifying only a fraction of edges; the remaining edges provide sufficient baseline representational capacity.
- Evidence anchors:
  - [abstract] "PRIOT-S, which only assigns scores to a small fraction of edges"
  - [section IV.B, Table I] PRIOT-S (p=80%, weight-based) achieves 83.12% vs PRIOT's 88.94% on rotated MNIST 30°, with ~28% memory reduction vs ~72% increase for full PRIOT
  - [corpus] Sparse update methods in related work (TinyML training under 256KB) share motivation but use different sparsity mechanisms.

## Foundational Learning

- Concept: **Integer quantization and scale factors**
  - Why needed here: Understanding how floating-point values map to fixed 8-bit integers via scale factors is essential to grasp why static vs. dynamic scaling matters for training stability.
  - Quick check question: Given a 32-bit integer accumulator value of 15,360 and a scale factor requiring 7-bit right shift, what 8-bit value results? (Answer: 120, within [-128, 127] range)

- Concept: **Transfer learning vs. training from scratch**
  - Why needed here: PRIOT explicitly assumes pre-trained weights are available and only performs pruning-based adaptation; understanding this distinction clarifies when PRIOT is applicable.
  - Quick check question: If you have no pre-trained model for your target domain, can PRIOT be applied? (Answer: No—the method freezes pre-trained weights and requires meaningful initialization)

- Concept: **Gradient approximation for non-differentiable operations**
  - Why needed here: The edge-popup algorithm uses straight-through estimation to bypass the non-differentiable masking operation; understanding this explains how discrete pruning decisions are learned.
  - Quick check question: Why doesn't the backward pass use the pruned mask Ẇ when computing input gradients? (Answer: The mask operation is non-differentiable; the approximation uses full weights W instead)

## Architecture Onboarding

- Component map:
  - Pre-trained weight store -> Score tensor per layer -> Static scale factor table -> Mask generator -> Integer-only backprop

- Critical path:
  1. Host-side: Pre-train model → quantize weights → calibrate scale factors → compile to device binary
  2. Device-side init: Load weights, scores, scale factors to SRAM
  3. Per-sample training: Forward pass with masking → compute loss → backward pass updating scores only
  4. Inference: Apply final pruning mask based on score thresholds

- Design tradeoffs:
  - **PRIOT vs. PRIOT-S**: Full PRIOT stores scores for all edges (+72% memory) for maximal accuracy; PRIOT-S stores sparse scores (-28% memory) with 3-6 p.p. accuracy loss
  - **Random vs. weight-based edge selection**: Weight-based selection improves PRIOT-S accuracy but requires additional initialization computation
  - **Score threshold**: Lower threshold (-64 for PRIOT, 0 for PRIOT-S) retains more edges; trade-off between adaptability and inference sparsity

- Failure signatures:
  - **Training collapse (static-scale NITI)**: Accuracy drops suddenly mid-training (e.g., 79% → 11%); output values overflow to ±127
  - **Score divergence**: Accuracy oscillates without convergence; check if learning rate is too high relative to score initialization variance
  - **Insufficient adaptation**: Final accuracy barely improves from pre-trained baseline; may indicate scored edge subset is too small or poorly selected

- First 3 experiments:
  1. **Reproduce the MNIST rotation baseline**: Implement static-scale NITI on rotated MNIST 45° to observe training collapse; verify PRIOT prevents collapse with same scale factors. Confirms core mechanism.
  2. **Ablate score initialization variance**: Test score initializations with σ ∈ {8, 16, 32, 64} to verify paper's claim that initialization has minimal impact. Validates robustness claim.
  3. **Profile PRIOT-S memory/accuracy frontier**: Vary scored edge ratio from 5% to 50% on both random and weight-based selection to identify application-specific optimal operating point. Informs deployment decisions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively would PRIOT generalize to more diverse neural network architectures and tasks beyond the limited image classification scenarios tested?
- Basis in paper: [explicit] "While evaluated in limited situations in this study, we expect that our proposals will also be effective in other tasks and models, and valuable for a broad range of applications requiring efficient integer-only training on resource-constrained devices."
- Why unresolved: The paper only evaluates PRIOT on a tiny CNN model with rotated MNIST and a VGG11 model with rotated CIFAR-10, leaving its applicability to other architectures and tasks unexplored.
- What evidence would resolve it: Evaluation of PRIOT on a wider variety of model architectures (e.g., RNNs, Transformers) and tasks (e.g., NLP, anomaly detection, health monitoring) in resource-constrained environments.

### Open Question 2
- Question: What is the optimal strategy for selecting which edges to score in PRIOT-S to minimize accuracy loss while maximizing memory reduction?
- Basis in paper: [inferred] The paper only tests two simple methods for selecting edges in PRIOT-S (random and weight-based) without extensive optimization.
- Why unresolved: The paper shows that different selection strategies impact accuracy but doesn't thoroughly explore or optimize the selection process.
- What evidence would resolve it: Systematic comparison of various edge selection strategies for PRIOT-S across different models and tasks, with analysis of the resulting accuracy-memory tradeoffs.

### Open Question 3
- Question: How sensitive is PRIOT to different score initialization distributions and threshold values across different transfer learning scenarios?
- Basis in paper: [inferred] The paper mentions that "the impact of the initialization method on accuracy is minimal" without providing detailed analysis across different transfer learning scenarios.
- Why unresolved: The paper uses a specific initialization (N(0,32)) and threshold values (-64 for PRIOT, 0 for PRIOT-S) without thoroughly analyzing how changes to these hyperparameters might affect performance in different scenarios.
- What evidence would resolve it: Comprehensive hyperparameter sensitivity analysis of score initialization distributions and threshold values across different transfer learning scenarios and model architectures.

### Open Question 4
- Question: Could PRIOT be combined with other compression techniques to further reduce memory footprint without significant accuracy loss?
- Basis in paper: [inferred] The paper focuses solely on pruning as a means to enable static-scale integer-only training but doesn't explore combining it with other compression methods.
- Why unresolved: There may be complementary techniques like structured pruning, weight sharing, or lower bit-width quantization that could further reduce memory footprint while maintaining accuracy.
- What evidence would resolve it: Experiments combining PRIOT with other compression techniques, measuring the resulting memory footprint and accuracy tradeoffs in resource-constrained environments.

## Limitations

- The method requires pre-trained weights to be well-aligned with target domain; large domain shifts may prevent effective adaptation through pruning alone
- Performance evaluation is limited to image classification tasks (MNIST and CIFAR-10) with specific model architectures
- The edge-popup algorithm implementation details (exact integer arithmetic, gradient quantization) are not fully specified

## Confidence

- High: PRIOT prevents training collapse relative to static-scale integer-only training baseline (experimentally demonstrated)
- Medium: Score initialization variance has minimal impact on final accuracy (supported by ablation but not exhaustively)
- Medium: PRIOT-S provides favorable memory-accuracy tradeoff (supported by experiments but dependent on application-specific requirements)

## Next Checks

1. Implement fine-grained overflow monitoring to quantify the relationship between activation saturation and training collapse in static-scale integer-only training
2. Systematically evaluate PRIOT performance under large domain shifts where pre-trained weights may be poorly aligned with target distribution
3. Compare PRIOT accuracy to full-precision transfer learning to establish the performance gap attributable to integer-only constraints