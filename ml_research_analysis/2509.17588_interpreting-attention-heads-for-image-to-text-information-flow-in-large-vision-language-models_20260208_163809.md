---
ver: rpa2
title: Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language
  Models
arxiv_id: '2509.17588'
source_url: https://arxiv.org/abs/2509.17588
tags:
- attention
- heads
- information
- head
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how large vision-language models (LVLMs) transfer
  image information to text for visual question answering. The key challenge is identifying
  which attention heads are responsible for this image-to-text information flow, as
  the process involves many heads operating simultaneously.
---

# Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2509.17588
- Source URL: https://arxiv.org/abs/2509.17588
- Authors: Jinyeong Kim; Seil Kang; Jiwoo Park; Junhyeok Kim; Seong Jae Hwang
- Reference count: 40
- Primary result: Proposes "head attribution" method to identify attention heads transferring image information to text, finding this flow follows a structured process through mid-to-late layer heads and role tokens

## Executive Summary
This paper analyzes how large vision-language models (LVLMs) transfer image information to text for visual question answering. The key challenge is identifying which attention heads are responsible for this image-to-text information flow, as the process involves many heads operating simultaneously. The authors propose "head attribution," an adapted component attribution method, to measure the contribution of each attention head by systematically ablating groups of heads and using linear regression to estimate each head's impact on final predictions. They validate this approach by showing it requires fewer heads than baselines to achieve the same performance (faithfulness >0.8 and completeness <0.2).

## Method Summary
The head attribution method ablates p=75% of attention heads randomly for M=10,000 iterations per sample, replacing image key/value pairs with mean baseline matrices. This generates binary head configurations that are mapped to normalized logits using ElasticNet regression (alpha=0.0005, l1_ratio=0.5). The method measures faithfulness by progressively activating heads ranked by attribution coefficients and computes explained variance (R² > 0.77) and Pearson correlation (ρ > 0.88) between predicted and actual logits. The approach is validated on LLaVA-1.5-7B using COCO dataset with 200 samples containing single main objects.

## Key Results
- Head attribution requires fewer heads than baselines to achieve faithfulness >0.8 and completeness <0.2
- A distinct subset of attention heads in mid-to-late layers facilitate image-to-text flow
- Selection of heads is governed by semantic content rather than visual appearance (photo vs. sketch)
- Text information first propagates to role-related tokens and the final token before receiving image information
- Image information is embedded in both object-related and background tokens

## Why This Works (Mechanism)

### Mechanism 1: Distributed Retrieval via Attention Head Superposition
Image-to-text information flow appears to be distributed across multiple attention heads rather than localized to a single "expert" head. The model utilizes "attention head superposition" where multiple heads collaborate to process visual information. When a single head is ablated, other heads compensate via self-repair mechanisms, masking their individual importance. Head attribution overcomes this by ablating groups of heads simultaneously and using linear regression to isolate the contribution of specific heads amidst this distributed processing.

### Mechanism 2: Semantic-Governed Head Selection
The selection of which attention heads transfer image information is determined by the semantic category of the object, not its visual appearance (e.g., style or texture). LVLMs route visual processing through specific subsets of attention heads that are invariant to domain shifts. Whether the input is a photo, sketch, or clipart of a "dog," the model activates a similar cluster of attention heads associated with the "dog" concept.

### Mechanism 3: Role-Token Mediated Injection
Image information is not injected directly into the question tokens but is instead routed through role tokens (e.g., "ASSISTANT") and the final token (e.g., ":") which act as semantic accumulators. Textual context (the question) first propagates to the role and final tokens. Subsequently, specific mid-to-late layer attention heads query the image tokens to inject visual information specifically into these "hub" tokens to generate the answer.

## Foundational Learning

- **Concept: Causal Ablation vs. Correlation (Attention Weights)**
  - Why needed here: A central finding is that high attention weights do not imply high importance. Learners must distinguish between *observing* attention (correlation) and *perturbing* the model (causation).
  - Quick check question: If an attention head has 90% attention weight on an image token but the model still answers correctly when that head is ablated, is that head "important"?

- **Concept: Superposition & Redundancy**
  - Why needed here: The paper relies on the idea that functions are spread over many heads (superposition) and that large models have redundant heads.
  - Quick check question: Why does removing one brick from a wall not make the wall fall down, and how does that analogy apply to "single head ablation" in LVLMs?

- **Concept: Linear Regression for Attribution**
  - Why needed here: This is the core technical contribution (Head Attribution). Understanding how to map binary "on/off" states of heads to continuous logit changes is key to the paper's methodology.
  - Quick check question: If you have 10 switches (heads) that can be on or off, how would you use a linear equation to figure out which switch contributes the most to the light bulb's brightness?

## Architecture Onboarding

- **Component map:** Vision Encoder (CLIP) -> Image Tokens -> Decoder-Only Transformer (LLM Backbone) -> Logits (Unembedding Matrix)
- **Critical path:** Vision Injection (mid-to-late layer MHA heads query Image Tokens) -> Hub Aggregation (heads write visual info to Role Tokens and Final Token) -> Prediction (Final Token's embedding projected to logits)
- **Design tradeoffs:** Attention weights are computationally cheap but unreliable indicators of importance. Head Attribution (Ablation + Regression) is expensive (requires 10,000 forward passes) but faithful. Larger models have more "redundant" heads, making them harder to interpret fully but potentially more robust.
- **Failure signatures:** High attention on background tokens that are not important for the answer; removing these degrades performance if they act as "sink" tokens, but retaining them is inefficient. Low completeness: interventions that work on small models may fail on larger ones due to increased redundancy.
- **First 3 experiments:**
  1. Sanity Check (Single Ablation): Ablate random individual heads in LLaVA-1.5-7B on visual task. Verify logit changes are minimal (<5%), confirming need for group analysis.
  2. Head Attribution Run: Implement regression pipeline. Randomly ablate 75% of heads over 1,000 iterations on single image. Train linear model to predict normalized logit. Check R² (should be > 0.77).
  3. Token Intervention: Identify "Role Token" and "Final Token" in prompt. Ablate image-key access only for these tokens and measure logit drop compared to ablating image access for question tokens. Expect massive drop for Role/Final tokens.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified mechanisms of image-to-text information flow—specifically the reliance on mid-to-late layer heads and role/final tokens—generalize to complex reasoning or localization tasks?
- Basis in paper: The authors state in Section 6 that the study is limited to visual object identification and that "the generalization of our findings to other tasks, such as reasoning and localization, is uncertain."
- Why unresolved: The paper intentionally restricted its scope to a minimal, single-word prediction task to isolate the information flow mechanism, leaving multi-step or spatially complex tasks unexplored.

### Open Question 2
- Question: Why do specific background tokens (outside the main object region) contribute significantly to the final prediction in LVLMs?
- Basis in paper: Section 5.2 notes that background tokens contribute to the information flow, potentially due to the vision encoder capturing global information or acting as "anchor tokens," but states "A more detailed analysis of this phenomenon is left for future work."
- Why unresolved: While the paper quantifies the contribution via attribution coefficients, it does not mechanistically isolate whether this utility stems from the vision encoder's global attention or the language model's storage strategy.

### Open Question 3
- Question: To what extent are the functional roles of attention heads predetermined by the backbone Large Language Model (LLM) architecture versus learned during vision-language alignment?
- Basis in paper: In Section D (Additional Results), the authors observe that models sharing the same backbone (LLaVA-1.5 and LLaVA-NeXT) exhibit similar head distributions, suggesting "the role of attention heads is predetermined by the backbone large language model."
- Why unresolved: The observation suggests a correlation between backbone choice and head specialization, but the paper does not perform the causal analysis required to confirm if these roles are rigid or malleable.

### Open Question 4
- Question: Can the contribution of attention heads be estimated accurately without the high computational cost of systematic ablation and linear regression?
- Basis in paper: Section 6 lists the high computational cost as a limitation, noting that "head attribution requires multiple forward passes" and identifying "scalable methods" as an important direction for future work.
- Why unresolved: The current method requires thousands of ablations per sample (10,000 passes mentioned in Appendix A.3), which is computationally prohibitive for real-time interpretation or large-scale analysis.

## Limitations
- High computational cost requiring thousands of forward passes per sample makes the method impractical for real-time analysis
- Linear regression assumption may oversimplify complex, nonlinear interactions between attention heads
- Findings may be specific to the chat-style prompt format used and not generalize to different architectures
- Limited to simple visual object identification task, with uncertain generalization to complex reasoning or localization

## Confidence

**High Confidence:** Attribution Method Validity - The head attribution method itself is well-validated through multiple metrics: high explained variance (R² > 0.77), strong correlation with actual logits (ρ > 0.88), and superior faithfulness/completeness compared to single-head ablation baselines.

**Medium Confidence:** Mid-to-Late Layer Head Importance - The identification of mid-to-late layer attention heads as responsible for image-to-text flow is supported by ablation results, but the exact layer boundaries and their consistency across different model architectures remain unclear.

**Low Confidence:** Universal Token Flow Pattern - The specific sequence where "text information first propagates to role-related tokens and the final token before receiving image information" may be prompt-format dependent rather than a universal architectural constraint.

## Next Checks

1. **Ablation Robustness Test:** Validate that the head attribution results are stable across different random seeds for the ablation configurations. Run the same 10,000 iterations with three different seeds and compare the top-10 ranked heads for consistency.

2. **Architecture Generalization:** Apply the same attribution methodology to a different LVLM architecture (e.g., LLaVA-NeXT or Idefics) to test whether the mid-to-late layer head importance and role token injection pattern holds across architectures, or if these findings are specific to LLaVA-1.5.

3. **Cross-Task Validation:** Test whether the identified image-to-text attention heads and their semantic selection patterns generalize beyond object identification to other visual question answering tasks (e.g., counting, reasoning about relationships). This would validate whether the findings represent fundamental information flow mechanisms or task-specific shortcuts.