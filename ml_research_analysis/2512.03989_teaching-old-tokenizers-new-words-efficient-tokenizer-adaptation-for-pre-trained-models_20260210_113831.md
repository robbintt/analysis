---
ver: rpa2
title: 'Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained
  Models'
arxiv_id: '2512.03989'
source_url: https://arxiv.org/abs/2512.03989
tags:
- tokens
- latn
- tokenizer
- pruning
- vocabulary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of common vocabulary extension
  methods for pre-trained tokenizers. The authors propose continued BPE training,
  which adapts an existing tokenizer by continuing the BPE merge process on new domain-specific
  data, avoiding the addition of unreachable tokens common in naive extension approaches.
---

# Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained Models

## Quick Facts
- arXiv ID: 2512.03989
- Source URL: https://arxiv.org/abs/2512.03989
- Authors: Taido Purason; Pavel Chizhov; Ivan P. Yamshchikov; Mark Fishel
- Reference count: 40
- Primary result: Continued BPE training achieves up to 9.6% higher tokenization efficiency than naive extension while producing zero unreachable tokens

## Executive Summary
This paper addresses the inefficiency of common vocabulary extension methods for pre-trained tokenizers. The authors propose continued BPE training, which adapts an existing tokenizer by continuing the BPE merge process on new domain-specific data, avoiding the addition of unreachable tokens common in naive extension approaches. They also introduce leaf-based vocabulary pruning, which removes redundant tokens while preserving tokenizer structure. Experiments across 70 languages show that continued BPE training achieves up to 9.6% higher tokenization efficiency and better utilization of extended vocabulary compared to naive extension.

## Method Summary
The paper introduces two key techniques for tokenizer adaptation. Continued BPE training extends an existing tokenizer by continuing the BPE merge process on new data, using the existing tokenizer to segment the corpus and count token pair frequencies within those segments. This ensures all new tokens are reachable during tokenization. Leaf-based pruning removes redundant tokens while preserving the BPE merge graph structure by iteratively pruning lowest-frequency leaf tokens (those not participating in downstream merges). The methods are implemented in an open-source toolkit that supports Byte-Pair Encoding (BBPE) and SentencePiece tokenizers.

## Key Results
- Continued BPE training achieves up to 9.6% higher tokenization efficiency than naive extension
- The method produces zero unreachable tokens compared to 3.6-10.5% for naive approaches
- Up to 62.5% of tokens can be pruned without performance loss using leaf-based pruning
- Extension with 8-16k tokens typically provides optimal compression gains for most languages

## Why This Works (Mechanism)

### Mechanism 1: Continued BPE Training Preserves Merge Compatibility
- Claim: Continuing BPE training from an existing tokenizer produces higher tokenization efficiency than appending tokens from an independently trained tokenizer.
- Mechanism: BPE tokenization depends on a deterministic merge sequence. The naive approach trains a new tokenizer on domain data and appends non-overlapping tokens, but these tokens often cannot be produced during tokenization because they conflict with the existing merge order. Continued training instead uses the existing tokenizer to segment the new data, counts token pair frequencies within those segments, and extends the merge sequence. All resulting tokens are reachable because they respect the original merge constraints.
- Core assumption: The original tokenizer's merge sequence correctly segments the base language, and extension should preserve this structure rather than override it.
- Evidence anchors:
  - [abstract]: "We propose continued BPE training, which adapts a pre-trained tokenizer by continuing the BPE merge learning process on new data."
  - [Section 2.1]: "This happens because BPE strictly follows the merge sequence, e.g., when a word information is originally tokenized as inform + ation, adding new tokens infor and mation is useless, even though they are new to the vocabulary."
  - [Section 3.1]: "We define continued BPE training as reapplying the BPE algorithm to in-domain data, using token pair frequencies of the text tokenized with the existing tokenizer to create the new merges."
  - [corpus]: Related work (AdaptBPE) addresses tokenizer specialization through longest-string matching, but this alters the tokenization function rather than extending the merge sequence.

### Mechanism 2: Unreachable Token Elimination via Self-Tokenization Test
- Claim: Naive extension produces unreachable tokens that never appear in tokenized output, wasting vocabulary capacity.
- Mechanism: The paper formalizes the Self-Tokenization Test (STT): a token is unreachable if tokenizing its own string representation does not reproduce that token. This occurs when naive extension adds tokens whose constituent characters would merge differently under the existing merge rules. Continued training produces zero unreachable tokens because each new token is created by a valid merge from existing tokens.
- Core assumption: BPE tokenizers produce tokens exclusively through merge operations (excluding merge-skipping mechanisms).
- Evidence anchors:
  - [Section 3.3]: "We define the Self-Tokenization Test (STT) as the number of tokens unreachable through merges: STT = Σ_{t∈V} 1[tokenize(t) ≠ [t]]"
  - [Table 1]: "Unreach. added" shows 0.0% for continued training vs. 3.6-10.5% for naive extension across model families.
  - [Table 3]: Unused added tokens on held-out data: 0.0-7.1% (continued) vs. 4.0-20.9% (naive).
  - [corpus]: No direct corpus evidence for STT formalization; this appears to be a novel diagnostic contribution.

### Mechanism 3: Leaf-Based Pruning Preserves Merge Graph Integrity
- Claim: Pruning tokens that are leaves in the BPE merge graph (tokens not participating in any downstream merge) avoids creating unreachable tokens.
- Mechanism: Naive frequency-based pruning can remove intermediate tokens needed for merges, breaking paths to downstream tokens. Leaf-based pruning iteratively identifies tokens with zero downstream merges (leaves), prunes the lowest-frequency leaves first, and updates the graph—newly created leaves then become eligible for pruning. This maintains a valid merge graph throughout.
- Core assumption: The BPE merge graph is a DAG where leaf tokens contribute only their direct frequency, while internal tokens contribute to multiple downstream tokens.
- Evidence anchors:
  - [Section 3.2]: "Leaf tokens are used to populate the priority queue, while atomic tokens cannot be removed and should never be added to the queue."
  - [Table 4]: Leaf-based frequency pruning maintains higher compression than naive frequency pruning at equivalent pruning levels.
  - [Table 5]: Naive frequency pruning creates thousands of unreachable tokens; leaf-based pruning creates zero.
  - [corpus]: No direct corpus evidence; related pruning methods (BPE-Knockout, PickyBPE) modify the tokenization function itself rather than vocabulary-only pruning.

## Foundational Learning

- Concept: Byte-Pair Encoding (BPE) Iterative Merging
  - Why needed here: Understanding that BPE vocabulary is built through sequential pair merging—each new token results from merging two existing tokens—is essential to grasp why merge order matters and why "adding tokens" naïvely fails.
  - Quick check question: Given vocabulary {a, b, c, d} and merge rules [(a,b)→ab, (ab,c)→abc], can the token "bc" ever be produced?

- Concept: Tokenization Efficiency Metrics
  - Why needed here: The paper optimizes for bytes-per-token (compression) and monitors Rényi efficiency. These metrics trade off: higher compression reduces sequence length but can lower Rényi efficiency, which correlates with downstream performance.
  - Quick check question: If tokenizer A produces 100 tokens for a 400-byte document and tokenizer B produces 80 tokens, which has higher bytes-per-token? Which would you expect to process faster during inference?

- Concept: BPE Merge Graph Topology
  - Why needed here: Pruning requires understanding which tokens are "leaves" (terminal, not merged further) versus "internal nodes" (participate in downstream merges). Removing internal nodes breaks paths to their descendants.
  - Quick check question: In a merge graph where (x,y)→xy and (xy,z)→xyz, is token "xy" a leaf? Can you safely remove it?

## Architecture Onboarding

- Component map:
  Existing Tokenizer (Vocab + Merges) -> Target Corpus -> Pre-tokenization (BBPE) or Script Segmentation (SentencePiece) -> Tokenize with Existing Tokenizer -> Count Token Pair Frequencies -> Continue BPE Merging -> New Merges + New Tokens -> [Optional] Prune Old Tokens -> Modified Tokenizer -> Update Model Embeddings (FVT)

- Critical path:
  1. Load existing tokenizer and extract vocabulary + merge list
  2. Pre-process target corpus according to tokenizer type (BBPE vs. SentencePiece constraints)
  3. Tokenize corpus with existing tokenizer, accumulate pair frequencies
  4. Run BPE merge loop until target vocabulary size reached
  5. If pruning: build merge graph, identify leaves, prune lowest-frequency leaves iteratively
  6. Export modified tokenizer in HuggingFace format
  7. Initialize new embeddings via FVT (copy overlapping tokens, average constituents for new tokens)

- Design tradeoffs:
  - Extension size vs. training cost: Adding 16k tokens to Llama-3 for Estonian achieved 4.46 bytes/token but required continued pre-training. Paper shows diminishing returns beyond 8-16k tokens for most languages (Table 1).
  - Pruning aggressiveness vs. task sensitivity: 62.5% pruning preserved discriminative task performance but machine translation (generative) showed earlier degradation (Figure 4).
  - Data budget: 100M characters sufficient for extension; 1B+ showed minimal additional gain (Appendix A, Figure 5).
  - Merge-skipping dependency: Tokenizers with merge-skipping (Llama-3, Mistral) partially mask naive extension failures; test with merge-skipping disabled to expose issues.

- Failure signatures:
  - High unreachable token count (>2%): Indicates merge paths are broken—verify pruning is leaf-based, not naive frequency.
  - Compression degradation in base language: Pruning removed tokens critical for original language; ensure pruning data includes base language samples.
  - Low added-token utilization (<95%): Extension produced tokens not useful for domain; verify training data is representative.
  - Embedding collapse after training: New tokens initialized poorly or undertrained—check FVT initialization and training token exposure (Appendix C, Figure 8).

- First 3 experiments:
  1. **Baseline compression comparison**: Take Llama-3 tokenizer, extend for a target language (e.g., Estonian) with 8k tokens using both naive and continued training. Measure bytes-per-token, unreachable tokens (STT), and unused tokens on held-out data. Expected: ~5% compression improvement, 0% unreachable for continued vs. ~7% unreachable for naive.
  2. **Pruning threshold calibration**: Apply leaf-based frequency pruning at 25%, 50%, 62.5% on Llama-3.1-8B for a bilingual (EN + target) scenario. Evaluate on both languages' downstream tasks. Identify the threshold where target-language performance is maintained but base-language performance degrades.
  3. **End-to-end adaptation**: Prune Llama-3 tokenizer to 64k tokens (preserving EN + target), extend back to 128k with target language via continued training, run 5B tokens of continued pre-training, evaluate on target-language benchmarks. Compare training time reduction vs. baseline (paper reports 26% reduction).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions (e.g., data scarcity, language similarity) is it more effective to modify a tokenizer versus replacing it entirely?
- Basis in paper: [explicit] The authors state, "We leave the question of what is the best strategy—modifying or replacing the tokenizer—to future research."
- Why unresolved: The paper focuses on modification techniques (pruning/extension) but does not compare them against the alternative strategy of full tokenizer replacement.
- What evidence would resolve it: A comparative study measuring downstream performance and training costs for both modification and full replacement across various low-resource scenarios.

### Open Question 2
- Question: How does continued BPE training perform in complex multilingual adaptation scenarios involving multiple new languages simultaneously?
- Basis in paper: [explicit] The limitations section notes that "more complex multilingual scenarios remain unexplored and warrant future investigation" as experiments were restricted to bilingual settings.
- Why unresolved: It is unclear if the efficiency gains observed when adding a single language hold when the vocabulary must accommodate several new languages at once without interference.
- What evidence would resolve it: Experiments extending a base tokenizer to 3+ distinct languages simultaneously, analyzing compression rates and merge conflicts.

### Open Question 3
- Question: Can the proposed leaf-based pruning and continued training methods be effectively adapted for non-BPE tokenization algorithms?
- Basis in paper: [explicit] The authors acknowledge the "current approach is also restricted to BPE-based tokenizers" and suggest extending the method to other segmentation algorithms.
- Why unresolved: The structural logic of leaf-based pruning relies on BPE merge graphs, which may not translate directly to probabilistic models like Unigram or WordPiece.
- What evidence would resolve it: Adapting the pruning logic to Unigram tokenizers and evaluating if structure-preserving vocabulary reduction is feasible.

### Open Question 4
- Question: What is the optimal vocabulary extension size required to maximize downstream performance without unnecessary embedding overhead?
- Basis in paper: [explicit] The paper states the authors "do not address... how much tokenizer extension is optimal from the perspective of downstream performance."
- Why unresolved: The study focuses on intrinsic metrics (compression) and fixed extension sizes, leaving the correlation between vocabulary size and task accuracy undefined.
- What evidence would resolve it: A sweep of extension sizes (e.g., 1k to 32k tokens) correlated with benchmark performance on tasks like translation or classification.

## Limitations

- The paper evaluates tokenizer adaptation primarily on compression metrics and downstream task performance, but does not assess the quality of the extended merge sequences themselves or the impact on training dynamics.
- Leaf-based pruning assumes the BPE merge graph is acyclic and that removing leaves preserves tokenization integrity, which may not hold for all tokenizer variants or extreme pruning scenarios.
- The paper reports minimal improvement for Chinese and Japanese scripts when extending Llama-3, suggesting fundamental limitations when the base tokenizer's merge sequence is incompatible with target language morphology.

## Confidence

- **High Confidence**: The core mechanism of continued BPE training producing zero unreachable tokens versus 4-21% for naive extension is well-demonstrated with clear empirical evidence across multiple tokenizer families.
- **Medium Confidence**: The claimed 9.6% tokenization efficiency improvement and 62.5% pruning capacity are supported by experiments, but the optimal extension size (8-16k tokens) and pruning thresholds may vary significantly based on domain characteristics not fully explored in the paper.
- **Low Confidence**: The paper's claims about merge-skipping behavior differences across tokenizers are noted but not systematically investigated. The impact of merge-skipping on downstream task performance remains unclear.

## Next Checks

1. **Merge Sequence Quality Assessment**: Extract and analyze the merge sequences produced by continued BPE training versus naive extension for a target language. Compare the linguistic quality of merges (e.g., whether they capture meaningful morphological units) using qualitative analysis or automated metrics that assess merge coherence.

2. **Extreme Pruning Robustness Test**: Apply leaf-based pruning beyond the reported 62.5% threshold (e.g., 75%, 85%, 90%) on multiple tokenizer families and evaluate downstream task performance across all task types. Identify the precise breaking point where performance degradation becomes unacceptable.

3. **Merge-Skipping Impact Investigation**: Systematically compare continued BPE training results with and without merge-skipping enabled across different tokenizer families. Measure both tokenization efficiency metrics and downstream task performance to quantify the masking effect of merge-skipping on tokenizer adaptation quality.