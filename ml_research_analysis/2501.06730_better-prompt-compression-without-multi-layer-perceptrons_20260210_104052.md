---
ver: rpa2
title: Better Prompt Compression Without Multi-Layer Perceptrons
arxiv_id: '2501.06730'
source_url: https://arxiv.org/abs/2501.06730
tags:
- prompt
- compression
- encoder
- xcompressor
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces the Attention-Only Compressor (AOC), a prompt\
  \ compression method that removes MLP layers from transformer blocks, reducing encoder\
  \ parameters by ~67% compared to baseline approaches. AOC achieves comparable or\
  \ better prompt regeneration performance than 500xCompressor across compression\
  \ ratios up to 480\xD7, with BLEU scores ranging from 0.588-0.984 and Exact-Match\
  \ scores from 0.082-0.889 depending on compression ratio and memory tokens used."
---

# Better Prompt Compression Without Multi-Layer Perceptrons

## Quick Facts
- arXiv ID: 2501.06730
- Source URL: https://arxiv.org/abs/2501.06730
- Reference count: 4
- Primary result: Removes MLP layers from compression encoders, reducing parameters by ~67% while maintaining regeneration quality

## Executive Summary
This paper introduces the Attention-Only Compressor (AOC), a prompt compression method that eliminates MLP layers from transformer blocks while maintaining competitive performance on prompt regeneration tasks. By training an encoder to compress prompts into memory tokens and regenerate them with a frozen decoder, AOC achieves comparable or better results than 500xCompressor across compression ratios up to 480×. The approach demonstrates that prompt compression encoders don't require identical architecture to their decoder language models, potentially enabling more efficient compression architectures. The authors also explore interpolation between compressed prompts to study latent space properties, finding smooth transitions between regenerated text.

## Method Summary
The method modifies a standard transformer by removing all MLP layers from the encoder blocks, replacing them with identity functions that only apply layer normalization. The encoder takes concatenated prompt tokens and learnable memory tokens, outputting per-layer KV pairs for the memory tokens. These compressed representations are then fed to a frozen decoder language model (Llama 3.2 1B Instruct) which regenerates the original prompt. The model is trained using cross-entropy loss with AdamW optimizer on arXiv abstracts (300K training, 3K validation, 3K test), with prompt lengths ranging from 96 to 480 tokens and memory tokens from 1 to 16. The ablation study compares full training against LoRA adaptation, finding that LoRA performs worse on the AOC architecture.

## Key Results
- AOC achieves BLEU scores of 0.588-0.984 and Exact-Match scores of 0.082-0.889 across compression ratios up to 480×
- The architecture reduces encoder parameters by approximately 67% compared to baseline approaches
- LoRA adaptation performs worse than full training on AOC, suggesting MLP layers contribute to LoRA effectiveness
- Interpolation between compressed prompts produces smooth transitions between regenerated text
- m=1 memory token models showed under-training, indicating need for more data or epochs at extreme compression ratios

## Why This Works (Mechanism)
The Attention-Only Compressor works by leveraging the expressive power of multi-head attention to capture prompt semantics without relying on position-wise MLPs. By removing MLPs and training only the attention mechanisms with memory tokens, the model learns to compress information into a compact latent representation. The frozen decoder then reconstructs the original prompt from these compressed representations, demonstrating that the attention-only encoder can capture sufficient information for accurate regeneration. The approach exploits the fact that prompt compression encoders don't need to perform the full range of language modeling tasks that decoders do, allowing for architectural simplification.

## Foundational Learning

**Multi-Head Attention**
- Why needed: Core mechanism for capturing relationships between prompt tokens and memory tokens
- Quick check: Verify attention weights properly distribute information across heads

**Layer Normalization**
- Why needed: Stabilizes training and maintains residual connections without MLPs
- Quick check: Ensure LN layers are properly positioned before residual additions

**Cross-Entropy Loss**
- Why needed: Measures reconstruction quality between original and regenerated prompts
- Quick check: Monitor training loss convergence across epochs

**Tokenization**
- Why needed: Converts text to model-compatible input format
- Quick check: Verify tokenization preserves prompt structure and length

## Architecture Onboarding

**Component Map**
Prompt Tokens + Memory Tokens -> Attention-Only Encoder -> KV Pairs -> Frozen Decoder -> Regenerated Prompt

**Critical Path**
1. Tokenization and memory token initialization
2. Encoder forward pass (attention-only blocks)
3. KV pair extraction for memory tokens
4. Decoder generation with frozen weights

**Design Tradeoffs**
- Parameter efficiency (67% reduction) vs. potential information loss from removing MLPs
- Full training vs. LoRA adaptation (LoRA performs worse on AOC)
- Extreme compression (m=1) vs. reconstruction quality

**Failure Signatures**
- Poor regeneration with m=1 memory tokens (under-training)
- LoRA-AOC underperforming baseline (requires full training)
- High variance in Exact-Match metric (early-token mismatches)

**First Experiments**
1. Train AOC with m=16 memory tokens and n=96 prompt length
2. Compare regeneration quality (BLEU/ROUGE-L) against baseline 500xCompressor
3. Test interpolation between two compressed prompts and analyze generated outputs

## Open Questions the Paper Calls Out
The paper identifies three key areas for future research: extending compressed prompts beyond interpolation to downstream natural language tasks, better understanding the global geometrical and semantic properties of the learned latent space, and scaling the approach to larger language models and more diverse datasets. The authors note that current experiments are limited to prompt regeneration metrics and arXiv abstracts with Llama 3.2 1B Instruct, suggesting broader evaluation is needed to assess practical utility.

## Limitations
- LoRA adaptation performs worse than full training, limiting practical deployment scenarios
- Evaluation restricted to arXiv abstracts may not generalize to other domains
- Memory token initialization without positional information could limit sequence order capture
- Computational efficiency during inference and KV cache size impacts not addressed

## Confidence

**High Confidence**
- Core claim that MLP layers can be removed while maintaining reconstruction quality
- Quantitative metrics showing consistent performance relative to baseline

**Medium Confidence**
- Qualitative evidence of meaningful latent space structure through interpolation
- Mathematical soundness of 67% parameter reduction claim

## Next Checks

1. Evaluate LoRA-AOC performance with varying ranks (8, 16, 32) to determine optimal configuration for practical deployment

2. Test compressed prompt representations on downstream tasks (classification, question answering) to assess task-relevant information capture

3. Measure inference-time KV cache sizes and generation latency for AOC-compressed prompts versus uncompressed prompts to quantify practical speedup benefits