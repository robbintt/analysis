---
ver: rpa2
title: Predicate Renaming via Large Language Models
arxiv_id: '2510.25517'
source_url: https://arxiv.org/abs/2510.25517
tags:
- names
- predicate
- predicates
- llms
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using Large Language Models (LLMs) to automatically
  rename unnamed predicates in logic rules, a common issue in Inductive Logic Programming
  (ILP) that hampers interpretability. The authors propose a pipeline where multiple
  LLMs generate name suggestions for each predicate, which are then judged by other
  LLMs to select the most suitable one.
---

# Predicate Renaming via Large Language Models

## Quick Facts
- **arXiv ID:** 2510.25517
- **Source URL:** https://arxiv.org/abs/2510.25517
- **Reference count:** 40
- **Primary result:** LLM pipeline achieves 15/16 correct predicate name suggestions on math case studies

## Executive Summary
This paper addresses the interpretability challenge in Inductive Logic Programming where systems generate logic rules with meaningless placeholder predicates (e.g., `inv`, `h0`). The authors propose using Large Language Models to automatically rename these predicates with semantically meaningful names. The approach uses a pipeline where multiple LLMs generate name suggestions, which are then judged by other LLMs to select the most appropriate renaming. Experiments demonstrate that larger models like GPT-4o and Gemini perform significantly better than smaller models, achieving high accuracy on hand-crafted case studies but struggling with real-world datasets.

## Method Summary
The proposed method implements a three-stage pipeline for predicate renaming. First, multiple LLMs generate name suggestions for each unnamed predicate through zero-shot prompting with structured prompts containing context and output format constraints. Second, each model selects its preferred suggestion from its generated options. Third, a panel of LLM judges scores all proposals using a standardized rubric (1 for correct, 0.5 for generic but correct, 0 for wrong). The system processes logic rules in Prolog syntax, applies name standardization (camelCase conversion, removing special characters), and aggregates scores to select final names. The approach was tested across seven different LLM models with varying sizes and capabilities.

## Key Results
- GPT models (ChatGPT-4o, ChatGPT-o3mini) and Gemini correctly renamed 15 out of 16 math predicates
- Smaller models (Llama 3.2 3B, FalconMamba 7B) performed poorly on the same task
- Human judges confirmed LLM performance, selecting correct names in most cases
- Real-world dataset results were significantly worse, suggesting domain sensitivity

## Why This Works (Mechanism)
The approach leverages LLMs' natural language understanding capabilities to map abstract predicate structures to semantically meaningful names. By using multiple models for generation and judging, the pipeline reduces individual model bias and improves robustness. The structured prompting constrains the output format while providing sufficient context for the models to infer appropriate naming conventions. The scoring mechanism allows for nuanced evaluation beyond simple binary correctness, accommodating cases where names are semantically related but not identical to ground truth.

## Foundational Learning

**Inductive Logic Programming (ILP)** - Machine learning approach using logic rules to represent knowledge and make predictions. Needed because ILP systems create placeholder predicates that lack semantic meaning, making results hard to interpret.

**Zero-shot prompting** - Prompting LLMs without providing examples in the prompt. Needed to test the model's ability to generalize naming conventions from the given context alone.

**Predicate logic** - Formal notation using predicates to represent relationships between entities. Needed as the target domain where renaming improves interpretability of logical rules.

## Architecture Onboarding

**Component map:** Logic rules -> LLM name generators (n models Ã— k suggestions) -> LLM judges (j models) -> Aggregated scores -> Selected names

**Critical path:** The pipeline's success depends on the quality of name generation, as poor initial suggestions limit the judges' ability to select good names. Model selection is crucial, with larger models showing substantially better performance.

**Design tradeoffs:** Using multiple models increases computational cost but improves robustness. The scoring system (1/0.5/0) adds nuance but requires careful calibration. The approach trades deterministic rule-based renaming for probabilistic LLM-based suggestions.

**Failure signatures:** Smaller models frequently produce invalid predicate names (containing spaces or special characters) or fail to follow output format constraints. Some models rewrite rule bodies instead of just suggesting names, requiring strict prompt constraints.

**First experiments to run:**
1. Test the pipeline on synthetic case studies using GPT-4o to establish baseline performance
2. Compare zero-shot vs few-shot prompting on smaller models to assess performance gap
3. Validate LLM judge selections against human expert evaluation on the same dataset

## Open Questions the Paper Calls Out

None specified in the paper.

## Limitations

- Performance significantly degrades on real-world datasets compared to synthetic case studies
- Results are highly dependent on specific LLM model capabilities and may change with API updates
- The approach requires multiple LLM calls per predicate, making it computationally expensive

## Confidence

**High confidence:** Experimental setup and methodology for synthetic case studies are well-documented and reproducible.

**Medium confidence:** LLM performance differences between models are consistent, but absolute accuracy may vary with different API versions or parameter settings.

**Low confidence:** Real-world dataset results are limited to one example, making generalization claims premature.

## Next Checks

1. Test the pipeline on additional real-world ILP datasets to assess domain generalization.
2. Implement systematic parameter tuning (temperature, top_p) to establish optimal settings across different LLM families.
3. Conduct ablation studies comparing zero-shot vs few-shot prompting effects on smaller models' performance.