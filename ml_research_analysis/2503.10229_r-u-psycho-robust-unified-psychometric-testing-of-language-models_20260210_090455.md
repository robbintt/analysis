---
ver: rpa2
title: R.U.Psycho? Robust Unified Psychometric Testing of Language Models
arxiv_id: '2503.10229'
source_url: https://arxiv.org/abs/2503.10229
tags:
- answer
- question
- option
- options
- statement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: R.U.Psycho is a framework for robust, reproducible psychometric
  testing of generative language models using configurable JSON-based experiments.
  It supports multiple models, prompt templates, and postprocessing pipelines including
  cleaners, validators, and judges for mapping responses to questionnaire answer options.
---

# R.U.Psycho? Robust Unified Psychometric Testing of Language Models

## Quick Facts
- arXiv ID: 2503.10229
- Source URL: https://arxiv.org/abs/2503.10229
- Reference count: 40
- One-line primary result: LLM traits vary significantly by model size, wording, and prompt order, with minimal persona-induced bias.

## Executive Summary
R.U.Psycho is a framework for robust, reproducible psychometric testing of generative language models using configurable JSON-based experiments. It supports multiple models, prompt templates, and postprocessing pipelines including cleaners, validators, and judges for mapping responses to questionnaire answer options. Experiments on diverse questionnaires show that LLM traits vary significantly by model size, wording, and prompt order, with minimal persona-induced bias. The framework lowers the bar for rigorous machine psychology research and is available as a Python package.

## Method Summary
The framework uses a four-stage pipeline: definition (JSON config specifying models, questionnaires, personas, prompt templates), generation (iterating through questions with LangChain-compatible LLM APIs), postprocessing (cleaners for noise, validators for refusals, judges for mapping responses to options), and export (saving results as CSV/JSONL). It employs a JSON prompt template to force structured LLM output and uses token-overlap based judges for mapping. Five questionnaires are fully provided (RFQ, BFI, GSDB, Trolley Problem, BDI), and experiments use models ranging from SmolLM-1.7B to Llama3.1-70B with temperature=1.0, top_k=50, top_p=0.95, and max_tokens=64.

## Key Results
- Model size correlates with higher agreeableness and conscientiousness (BFI test)
- Inverting answer option order significantly changes depression scores (BDI test)
- Minimal persona-induced bias found for Gender/Sex Diversity Beliefs questionnaire
- Rule-based judge (token overlap) outperforms model-based judge for response mapping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A structured pipeline with JSON-formatted prompts and rule-based postprocessing improves the reliability of mapping noisy LLM outputs to discrete questionnaire options.
- Mechanism: R.U.Psycho enforces a four-stage pipeline: definition, generation, postprocessing, and export. The definition stage uses a JSON configuration to lock in experimental parameters. The generation stage employs a prompt template that instructs the LLM to output answers in a specific JSON format. The postprocessing stage uses a sequence of components: cleaners, validators, and judges. The paper's experiments show that the rule-based judge, using token-overlap between the response and answer options, outperforms a more complex model-based judge.
- Core assumption: LLMs can be guided to output a constrained format, and that a response's lexical overlap with an answer option is a reliable signal for the intended choice.
- Evidence anchors:
  - [abstract] "...configurable JSON-based experiments... supports... postprocessing pipelines including cleaners, validators, and judges for mapping responses to questionnaire answer options."
  - [section 4.1] "We employ a strategy based on token-overlap... The answer option with the highest total overlap score is designated as the optimal choice... the rule-based judge performs substantially better than the model-based judge."
- Break condition: This mechanism fails if LLM responses systematically use synonyms or complex negation that lacks token overlap with the answer options, or if the model fails to adhere to the JSON output format.

### Mechanism 2
- Claim: Measured psychometric traits in LLMs are sensitive to experimental parameters, indicating they are not stable, inherent properties.
- Mechanism: The framework's experiments demonstrate that observed LLM traits vary significantly based on configurable parameters. Model size correlates with higher agreeableness and conscientiousness. Prompt order sensitivity is shown by a significant change in depression scores when the order of answer options is inverted. Wording sensitivity is shown by inconsistent responses to semantically equivalent versions of the trolley problem.
- Core assumption: The observed variance in scores across controlled experimental conditions reflects systematic sensitivity to those conditions, rather than pure randomness.
- Evidence anchors:
  - [abstract] "Experiments on diverse questionnaires show that LLM traits vary significantly by model size, wording, and prompt order..."
  - [section 5.5] "...we find strongly differing depression scores between models... When using the inverted questionnaire, the scores of GPT-4o mini and Llama decrease significantly..."
- Break condition: The claim of parameter-sensitivity breaks if the observed score differences are solely attributable to uncontrollable model stochasticity and are not reproducible across different runs or model versions.

### Mechanism 3
- Claim: Simple, name-based personas induce minimal detectable bias in model responses for the specific sensitive topic tested.
- Mechanism: In an experiment using the Gender/Sex Diversity Beliefs questionnaire, the framework simulated personas defined only by an honorific and a surname from one of five ethnic groups. The analysis found "very little evidence for persona-induced bias," with no significant differences in scores across the ethnicity or gender of the personas for most models tested.
- Core assumption: A name and title are sufficient cues to trigger a simulated persona, and if social biases were strongly latent in the model for this task, they would manifest as score differences between these persona groups.
- Evidence anchors:
  - [abstract] "...with minimal persona-induced bias."
  - [section 5.3] "Interestingly, we find very little evidence for persona-induced bias... there are no significant differences in the scores by persona."
- Break condition: This finding may not generalize to more richly defined personas, different sensitive topics, or other model families. A more explicit persona prompt could trigger stronger alignment with stereotypical views.

## Foundational Learning

- Concept: **JSON Schema / Structured Output**
  - Why needed here: The framework relies on a JSON-based prompt template to force LLMs to output answers in a parseable format, which dramatically reduces the rate of discarded/unparseable responses.
  - Quick check question: How would an LLM's response be processed differently if it output "I think the answer is definitely option 3" versus `{"answer": "3. sometimes"}`?

- Concept: **Rule-based vs. Model-based Classification (Judges)**
  - Why needed here: The core postprocessing challenge is mapping noisy LLM text to clean questionnaire options. The paper compares a token-overlap (rule-based) approach to a fine-tuned encoder (model-based) approach for this task.
  - Quick check question: Given the response "My answer is not option 1 or 2, but 4.", which type of judge (rule-based token overlap vs. model-based semantic classifier) would likely perform better and why?

- Concept: **Prompt Sensitivity & Contamination**
  - Why needed here: The paper demonstrates that LLM "traits" are not stable and change with prompt wording, order of answer options, and potentially from training data contamination.
  - Quick check question: If you run a psychometric test on an LLM and get a certain score, what are at least three experimental parameters you must report to ensure reproducibility?

## Architecture Onboarding

- Component map:
  - Config File (JSON) -> Response Generation Module -> Postprocessing Pipeline (Cleaners -> Validators -> Judges) -> Export

- Critical path:
  1. Define experiment in a JSON config (select model, questionnaire, persona, prompt template)
  2. Run the response generation loop for all question/persona/seed combinations
  3. Apply the full postprocessing pipeline (Clean -> Validate -> Judge) to every raw response
  4. Analyze the final mapped scores to assess traits/bias

- Design tradeoffs:
  - **Rule-based vs. Model-based Judge**: The paper finds the simpler rule-based judge (token overlap) outperforms the more complex model-based judge (fine-tuned DistilRoBERTa). Tradeoff is simplicity/interpretability vs. semantic nuance handling.
  - **JSON vs. Natural Language Prompt**: JSON prompts yield fewer discarded responses but may constrain the "naturalness" of the model's simulated response. The paper favors JSON for reliability.
  - **Free Generation vs. Token Probabilities**: The framework uses free text generation, not token probabilities, to avoid known issues with instruction-tuned models. This increases robustness but requires the postprocessing judges.

- Failure signatures:
  - High "rejected" or "invalid" response rates (>5-10%). Suggests prompt template or validators are too strict/lenient, or model is not following JSON instructions.
  - Inconsistent trait scores across minor prompt variations (e.g., answer order). This indicates the measured trait is an artifact of the experimental setup, not a stable model property.
  - Low judge confidence or high entropy. Indicates the response mapping is ambiguous and the judge is uncertain.

- First 3 experiments:
  1. **Baseline Reproduction**: Run a standard questionnaire (e.g., BFI) on a small model (e.g., SmolLM-1.7B) and a large model (e.g., Llama3.1-70B) to reproduce the model size effect on traits.
  2. **Prompt Sensitivity Test**: Run a single questionnaire twice: once with standard answer option order and once with inverted order to quantify sensitivity to this parameter.
  3. **Judge Ablation**: Process a fixed set of raw responses (from a small pilot) with both the rule-based and model-based judge to compare mapping accuracy and reject rates on your specific data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does simulating questionnaires with full chat history (recall of previous answers) alter psychometric scores compared to the isolated prompting strategy used in the current framework?
- Basis in paper: [explicit] The authors state in the Limitations section that while the framework supports it, "the experimental design for ensuring a fair comparison is non-trivial and requires further research."
- Why unresolved: Current experiments treat each question as a disconnected prompt, which is less human-like but easier to control; the effect of context accumulation on trait stability is unknown.
- What evidence would resolve it: A comparison of scores on the same questionnaires (e.g., BFI) using the proposed "chat mode" versus the current isolated mode across multiple model families to quantify context-induced variance.

### Open Question 2
- Question: Does the use of more detailed persona descriptions (including background and demographic information) induce significant psychometric variance compared to the minimal name/gender approach?
- Basis in paper: [explicit] The authors note in Limitations that their "findings should be viewed in the light of this limitation" regarding simple personas, and explicitly state "such extended approaches should be investigated."
- Why unresolved: The paper found minimal bias with simple personas (Exp 3), but it is unknown if "rich" personas trigger the latent intersectional biases observed in other literature.
- What evidence would resolve it: Running the Gender/Sex Diversity Beliefs questionnaire using detailed persona prompts (e.g., including socioeconomic status, life history) vs. name-only prompts to measure effect size.

### Open Question 3
- Question: Why do different model families (e.g., Qwen vs. Llama) exhibit opposite reactions to the inversion of answer option orders in the Beck Depression Inventory?
- Basis in paper: [inferred] In Experiment 5, the authors found Qwen scores *increased* slightly while Llama/GPT scores *decreased* significantly when answer options were inverted.
- Why unresolved: The paper highlights the sensitivity to prompt order but does not determine if this divergence is caused by tokenizer artifacts, specific attention mechanisms, or training data biases.
- What evidence would resolve it: A controlled study varying the semantic valence of options (positive/negative) independent of numerical labels to isolate whether models attend more to position or semantic content.

## Limitations
- Minimal persona definitions (name + title) may not generalize to richer persona descriptions or different sensitive topics
- Token overlap strategy for mapping responses may not be universally optimal for all questionnaires and response styles
- Controlled tests may not reflect the ecological validity of an LLM's behavior in open-ended, real-world interactions

## Confidence
- **High Confidence**: The framework's core pipeline is well-specified and reproducible; finding that LLM traits are highly sensitive to experimental parameters is robust
- **Medium Confidence**: The claim of minimal persona-induced bias is supported for the specific, sparse persona definition and questionnaire tested
- **Low Confidence**: The universality of the rule-based (token overlap) judge as the optimal mapping strategy for all questionnaires and response styles is not established

## Next Checks
1. **Persona Richness Experiment**: Repeat the GSDB experiment using personas defined by detailed biographical text instead of just a name and title. Measure if this richer context induces detectable score differences.

2. **Judge Generalization Test**: On a held-out set of responses from a different questionnaire (e.g., RFQ), compare the performance of the rule-based (token overlap) judge against a fine-tuned semantic classifier. This will test if token overlap is universally superior or questionnaire-dependent.

3. **Ecological Validity Correlation**: Design a study to correlate controlled psychometric scores (from R.U.Psycho) with scores from a human evaluation of the same LLM in a realistic, multi-turn conversation task. This will assess how well the controlled tests predict real-world behavior.