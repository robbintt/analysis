---
ver: rpa2
title: Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous
  Driving
arxiv_id: '2509.04712'
source_url: https://arxiv.org/abs/2509.04712
tags:
- driving
- learning
- vehicle
- policy
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a deep reinforcement learning framework for
  autonomous driving that integrates a suboptimal rule-based controller to address
  exploration challenges in complex traffic scenarios. The proposed method combines
  soft constraints via KL divergence and reward-augmented demonstrations from a heuristic
  overtaking controller with the Soft Actor-Critic algorithm.
---

# Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving

## Quick Facts
- arXiv ID: 2509.04712
- Source URL: https://arxiv.org/abs/2509.04712
- Reference count: 40
- One-line result: Deep RL framework using suboptimal policies achieves 100% success rate in escaping traffic traps, outperforming vanilla SAC, CQL, and GAIL.

## Executive Summary
This paper presents a deep reinforcement learning framework for autonomous driving that addresses exploration challenges in complex traffic scenarios by integrating a suboptimal rule-based controller. The proposed method combines soft constraints via KL divergence and reward-augmented demonstrations from a heuristic overtaking controller with the Soft Actor-Critic (SAC) algorithm. Experiments on a highway overtaking scenario show that the approach achieves 100% success rate in escaping traffic traps, significantly outperforming baseline methods including vanilla SAC (0%), CQL (0%), and GAIL (0%). The method also demonstrates superior performance in terms of accumulated reward, average speed, and travel distance while maintaining collision-free operation.

## Method Summary
The method extends discrete SAC by incorporating a suboptimal rule-based overtaking controller through two mechanisms: (1) a soft KL-divergence penalty that nudges the learned policy toward the demonstrator during early training, and (2) reward-augmented demonstration data stored in the replay buffer with an annealing ratio. The KL penalty is tied to SAC's entropy coefficient, naturally decaying as training progresses, while the reward augmentation provides implicit guidance without enforcing hard constraints on the Q-function. The suboptimal controller implements a backward gap overtaking strategy that is difficult for vanilla RL to discover due to negative intermediate rewards in the "traffic trap" scenario.

## Key Results
- 100% success rate in escaping traffic traps, compared to 0% for vanilla SAC, CQL, and GAIL
- Higher accumulated reward (44.92 vs 24.70 for suboptimal controller)
- Improved average speed (12.55 m/s vs 11.51 m/s)
- Greater travel distance (1853.88 m vs 1727.01 m)
- Collision-free operation maintained

## Why This Works (Mechanism)

### Mechanism 1
A KL-divergence penalty nudges the RL agent's policy toward a suboptimal rule-based policy during early training, improving exploration without hard constraints. The paper modifies SAC's objective by adding a soft KL term that penalizes deviation from the demonstration policy. A stochastic version of the deterministic rule-based policy is constructed, allowing for KL computation. The KL weight is tied to SAC's entropy coefficient, naturally decaying as training progresses. This mechanism fails if the suboptimal policy is too poor, providing bad guidance that the agent cannot recover from, or if the KL penalty is too strong/weak, causing either over-constraint or insufficient guidance.

### Mechanism 2
Populating the replay buffer with demonstration data via reward augmentation accelerates early learning and stabilizes training more effectively than Q-value margin losses. Demonstration transitions are generated and stored, then sampled alongside online experience at an annealing ratio. A constant bonus reward is added to the demonstrated actions' rewards, which implicitly raises their Q-values and guides the agent without enforcing a hard constraint on the Q-function. This mechanism may fail if the bonus reward is too large, causing the agent to over-value the suboptimal policy and failing to improve beyond it.

### Mechanism 3
Using a suboptimal, heuristic rule-based controller as a guide is sufficient to overcome the "traffic trap" exploration problem, which vanilla RL fails to solve. The problem is framed as a "traffic trap" where following slow vehicles is a local optimum, while the true optimal strategy (overtaking via a backward gap) requires intermediate negative rewards. The rule-based controller implements this difficult strategy. By providing guidance for this specific behavior, the agent is bootstrapped out of the conservative local optimum. This mechanism is specific to scenarios with clear strategic traps solvable by human-designed heuristics and would fail in scenarios where no simple rule-based strategy exists or where the optimal policy is highly counter-intuitive.

## Foundational Learning
- **Concept: Soft Actor-Critic (SAC) with Entropy Regularization**
  - Why needed here: The entire framework builds upon the SAC algorithm. Understanding how SAC maximizes both reward and entropy is crucial because the proposed KL penalty is tied to SAC's entropy coefficient.
  - Quick check question: How does maximizing policy entropy aid exploration?

- **Concept: Kullback-Leibler (KL) Divergence**
  - Why needed here: The core soft constraint mechanism uses KL divergence to measure the distance between the learned policy and the demonstration policy.
  - Quick check question: Why is the KL divergence term added as a penalty to the objective function?

- **Concept: Off-policy Reinforcement Learning and Replay Buffers**
  - Why needed here: The method is fundamentally off-policy, leveraging both online experience and offline demonstration data stored in a single replay buffer.
  - Quick check question: How does annealing the ratio of offline demonstration data to online data affect the learning process?

## Architecture Onboarding
- **Component map**: Environment -> Rule-Based Controller -> SAC Agent -> Guidance Module (KL Calculator, Demonstration Buffer)
- **Critical path**: Generate demonstration data using the Rule-Based Controller → Stochasticize the rule-based controller's output to create a compatible policy distribution for KL computation → Pre-fill a portion of the SAC Replay Buffer with demonstration data → Training Loop: For each step, sample a mini-batch from the replay buffer according to the current online/offline ratio → Compute the SAC loss, augmented with the KL penalty term and/or the reward bonus on demonstration actions → Update networks and decay the online/offline ratio and KL penalty strength
- **Design tradeoffs**:
  1. Demonstration Quality vs. Availability: A suboptimal policy is easier to obtain than an expert one, but its performance sets a floor the agent must surpass.
  2. Constraint Softness: A soft KL constraint allows the agent to surpass the demonstrator, whereas a hard constraint would cap performance.
  3. Reward Shaping vs. Q-Loss: The ablation study shows reward augmentation (implicit guidance) is more stable and effective than large-margin Q-loss (explicit guidance).
- **Failure signatures**:
  1. Conservative Collapse: The agent remains stuck in the "car-following" local optimum, never attempting to overtake.
  2. Over-imitation: The agent perfectly mimics the suboptimal controller but shows no improvement, suggesting the guidance is too strong.
  3. Training Instability: High variance in rewards and collision rates, potentially caused by over-reliance on imperfect demonstration data.
- **First 3 experiments**:
  1. Baseline Reproduction: Implement vanilla discrete SAC in the trap scenario. Confirm it fails to escape (0% success rate), validating the core problem.
  2. Demonstration Integration: Implement the rule-based overtaking controller. Generate trajectories. Convert its deterministic action output into a stochastic policy. Verify the KL divergence can be computed.
  3. Ablation Study: Implement the full agent with the KL penalty and reward-augmented buffer. Compare against variants with only one of these components to confirm the combined benefit.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unaddressed based on the experimental setup and results.

## Limitations
- Generalization Beyond the Trap Scenario: The experimental validation is limited to a specific highway overtaking scenario with traffic traps, leaving the method's effectiveness in more complex, diverse driving scenarios unverified.
- Demonstration Data Quality and Quantity: The quality and quantity of demonstration data (200 episodes) may not be representative of all possible situations, potentially limiting the agent's ability to generalize.
- Safety Considerations: While collision-free operation is reported in experiments, the safety of the learned policy in real-world conditions or under sensor noise/disturbances is not addressed.

## Confidence
- High Confidence: The paper's core contribution of integrating a suboptimal policy via KL divergence and reward augmentation with SAC is clearly described and supported by the ablation study comparing different integration strategies.
- Medium Confidence: The experimental results showing superior performance (100% success rate, improved reward and speed) are compelling but are limited to a single, albeit challenging, scenario. The method's broader applicability is inferred rather than empirically demonstrated.
- Low Confidence: The paper's claim that the method is more efficient than pure RL (e.g., "fewer samples" or "faster convergence") is not explicitly quantified or compared in terms of sample efficiency metrics.

## Next Checks
1. **Generalization Test**: Evaluate the trained policy in a variety of traffic scenarios (e.g., with different traffic densities, vehicle types, or road geometries) to assess its robustness and generalization.
2. **Real-World Transfer**: Conduct a safety and performance assessment of the policy when transferred to a real-world or high-fidelity simulator with realistic sensor noise and disturbances.
3. **Sample Efficiency Analysis**: Conduct a detailed comparison of the sample efficiency (learning curves) of the proposed method against vanilla SAC and other baselines to quantify the claimed efficiency gains.