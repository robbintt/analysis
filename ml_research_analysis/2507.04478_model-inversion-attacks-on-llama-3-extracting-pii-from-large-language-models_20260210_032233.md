---
ver: rpa2
title: 'Model Inversion Attacks on Llama 3: Extracting PII from Large Language Models'
arxiv_id: '2507.04478'
source_url: https://arxiv.org/abs/2507.04478
tags:
- data
- llama
- language
- privacy
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that model inversion attacks can extract
  sensitive information from the Llama 3.2 1B model, a smaller multilingual LLM developed
  by Meta. The study used carefully crafted prompts (e.g., "account number:", "my
  password is:", "my email id:") to query the model and successfully extracted personally
  identifiable information (PII) including email addresses, phone numbers, and account
  details.
---

# Model Inversion Attacks on Llama 3: Extracting PII from Large Language Models

## Quick Facts
- arXiv ID: 2507.04478
- Source URL: https://arxiv.org/abs/2507.04478
- Reference count: 13
- Key outcome: Model inversion attacks successfully extracted PII from Llama 3.2 1B using targeted prompts, demonstrating that even smaller LLMs retain significant privacy risks.

## Executive Summary
This paper demonstrates that model inversion attacks can extract sensitive personally identifiable information (PII) from the Llama 3.2 1B model, a smaller multilingual LLM developed by Meta. By crafting targeted prompts such as "account number:" and "my email id:", the study successfully extracted real email addresses, phone numbers, and account details that were memorized from the training data. The extracted data was validated through real-world verification, confirming the attack's effectiveness. The research highlights that even efficiency-optimized smaller models retain significant privacy vulnerabilities, challenging the assumption that reduced parameter counts inherently provide better privacy protection. The paper recommends multiple mitigation strategies including access control, differential privacy training, data sanitization, output filtering, and regular auditing.

## Method Summary
The study employed a black-box model inversion attack methodology using the Llama 3.2 1B model through the Hugging Face Transformers pipeline. Researchers crafted specific prefix prompts targeting different PII categories (account numbers, passwords, email addresses) and generated text completions using autoregressive generation with parameters set to torch_dtype=torch.bfloat16, device_map="auto", max_new_tokens=50, top_p=1, and top_k=40. The attack relied solely on observing model outputs without requiring access to model weights or gradients. Extracted PII sequences were validated through external web searches to confirm real-world existence. The methodology focused on testing whether memorized training data containing sensitive information could be reconstructed through carefully designed prompt engineering.

## Key Results
- Successfully extracted real PII including email addresses, phone numbers, and account details from Llama 3.2 1B using targeted prefix prompts
- Demonstrated non-zero memorization rate, proving that smaller models retain significant privacy risks despite reduced resource requirements
- Validated extracted information through real-world verification using external web searches (Google, LinkedIn)
- Confirmed that black-box attack approaches can effectively extract sensitive data without requiring model weight access

## Why This Works (Mechanism)

### Mechanism 1: Autoregressive Completion of Memorized Sequences
- Claim: Targeted prefix prompts can trigger completion of memorized PII sequences from training data
- Mechanism: The autoregressive transformer learns conditional probability distributions over token sequences. High-affinity prefixes (e.g., "account number:") generate the most probable continuation based on training patterns, potentially including verbatim PII if present and duplicated in training corpus
- Core assumption: Training data contained unfiltered PII sequences that the model's optimization retained
- Evidence anchors: [abstract] extraction of PII through querying with crafted prompts; [Section III] describes prompt use and probability model in Equation (1); related work (arXiv:2502.12658) focuses on PII reconstruction via recollection-based attacks

### Mechanism 2: Black-Box Query Access for Data Extraction
- Claim: Attackers with only inference-level access can extract sensitive data without model internals
- Mechanism: Attack relies solely on observing model outputs to specific inputs. Iterating over prompts and analyzing generated text allows identification and collection of candidate PII without requiring model weights or gradients
- Core assumption: Attacker can issue sufficient queries and observe raw output tokens
- Evidence anchors: [abstract] black-box attack approach; [Section III] uses Hugging Face pipeline simulating API-like query; related work (arXiv:2507.16372) examines privacy leakage from outputs

### Mechanism 3: Generalization vs. Memorization Trade-off in Smaller Models
- Claim: Even smaller, efficiency-optimized LLMs (e.g., 1B parameters) can memorize and leak sensitive training data, though potentially at lower rates than larger models
- Mechanism: Smaller models retain sufficient capacity to overfit or memorize outlier sequences, especially if those sequences appear multiple times in training data
- Core assumption: Relationship between model scale and memorization rate is not strictly linear, and limited capacity models remain vulnerable
- Evidence anchors: [abstract] even smaller models memorize PII; [Section VI] analysis comparing memorization rates; evidence on scale effects is limited in provided neighbors

## Foundational Learning

- Concept: **Model Inversion Attacks (MIAs)**
  - Why needed here: This is the core attack methodology for reconstructing training data from model outputs
  - Quick check question: Can you explain the difference between a white-box and a black-box model inversion attack?

- Concept: **Differential Privacy (DP)**
  - Why needed here: DP is presented as a primary mitigation strategy to limit memorization of any single training example
  - Quick check question: How does adding noise to gradients during training limit the memorization of any single data point?

- Concept: **Autoregressive Language Modeling**
  - Why needed here: The attack exploits the fundamental token-by-token generation process of LLMs
  - Quick check question: Given a prompt `P` and a sequence `S`, what does the probability `P(S|P)` represent in the context of an autoregressive model?

## Architecture Onboarding

- Component map: Attacker Module (generates targeted prompts) -> Target LLM (black-box Llama 3.2 1B) -> Output Analyzer (collects and validates generated sequences for PII patterns)
- Critical path: Prompt formulation -> model querying -> output collection -> PII validation (via regex or external search). Prompt design is the most critical step for successful extraction.
- Design tradeoffs: Key tradeoff between model utility and privacy. Defenses like differential privacy or aggressive data sanitization may degrade model performance. Smaller models trade some capability for efficiency but don't eliminate privacy risk.
- Failure signatures:
  - Defense Evasion: Output filtering or access controls block the attack
  - Low Memorization Rate: Model generates plausible but non-PII text
  - False Positives: Generated text looks like PII but fails external validation
- First 3 experiments:
  1. Replicate basic extraction using provided Python code and prompts from Table I on local Llama 3.2 1B instance
  2. Expand prompt set to test other PII categories (SSNs, physical addresses) and measure Memorization Rate as defined in Equation (2)
  3. Implement simple output filter using regular expressions to detect and redact PII patterns, then re-test extraction prompts to evaluate mitigation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can standardized metrics be developed to consistently evaluate memorization risks across diverse LLM architectures?
- Basis in paper: [explicit] Authors state in Section VIII that future work should explore "standardized metrics for evaluating memorization risks"
- Why unresolved: Current evaluation relies on ad-hoc prompting success rates varying based on attacker knowledge and prompt engineering
- What evidence would resolve it: Benchmark suite quantifying extraction probability independent of specific prompt phrasing or model architecture

### Open Question 2
- Question: What is the precise trade-off relationship between model parameter size and vulnerability to model inversion attacks?
- Basis in paper: [explicit] Section X highlights that the "trade-off between model size and privacy risk remains an open question"
- Why unresolved: While larger models are known to memorize more, significant vulnerability found in smaller Llama 3.2 1B model challenges assumption that efficiency-focused models are inherently safer
- What evidence would resolve it: Comparative extraction experiments across varying model scales (1B vs 8B vs 70B) using identical attack parameters to map risk curve

### Open Question 3
- Question: What scalable defense mechanisms can effectively prevent PII extraction without significantly degrading model performance?
- Basis in paper: [explicit] Section VIII calls for research into "scalable defense mechanisms" and "privacy-preserving machine learning techniques"
- Why unresolved: Paper notes that defenses like differential privacy may reduce utility, and data sanitization is difficult to perfect given training data vastness
- What evidence would resolve it: Defense strategy maintaining baseline benchmark scores (e.g., MMLU) while reducing Memorization Rate (Eq. 2) to zero

## Limitations

- Lack of quantitative results: Paper demonstrates successful extraction but doesn't report specific memorization rates, success probabilities, or false positive rates for different prompt types
- Single-model focus: Study examines only Llama 3.2 1B without exploring whether results generalize across model sizes, architectures, or training datasets
- Validation methodology concerns: External web search verification lacks specified rigor and may have false positives from hallucinations matching real data

## Confidence

**High Confidence**: Claim that model inversion attacks can extract PII from LLMs is well-established; methodology is technically sound; assertion that even smaller models retain privacy risks is supported by demonstrated extraction

**Medium Confidence**: Effectiveness of proposed mitigation strategies (differential privacy, data sanitization, output filtering) is supported by existing research but not empirically validated in this specific context with Llama 3.2 1B

**Low Confidence**: Claims about relative memorization rates between smaller and larger models lack empirical support - single-model study cannot establish this comparative relationship

## Next Checks

1. **Quantify Attack Success Rates**: Systematically measure and report memorization rate (Number of Extracted PII Sequences / Total Queries) across multiple runs with varying random seeds, different prompt formulations, and larger query sets (minimum 100 queries per prompt type)

2. **Validate Mitigation Effectiveness**: Implement and test each proposed mitigation strategy (differential privacy training, output filtering with regex patterns for PII, access rate limiting) to measure their impact on both privacy protection and model utility

3. **Cross-Model Generalization**: Repeat extraction experiments on multiple Llama models (3.2 1B, 3B, 8B, 70B) and potentially other architectures to empirically establish relationship between model scale and memorization vulnerability