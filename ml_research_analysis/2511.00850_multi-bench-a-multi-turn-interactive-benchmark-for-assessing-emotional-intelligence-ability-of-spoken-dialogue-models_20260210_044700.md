---
ver: rpa2
title: 'MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence
  ability of Spoken Dialogue Models'
arxiv_id: '2511.00850'
source_url: https://arxiv.org/abs/2511.00850
tags:
- dialogue
- arxiv
- multi-turn
- emotion
- interactive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-Bench introduces the first benchmark designed to evaluate
  emotional intelligence in spoken dialogue models (SDMs) through genuine multi-turn
  interactive dialogues. Unlike existing benchmarks focused on single-turn exchanges,
  Multi-Bench employs a hierarchical structure assessing both basic emotion understanding
  and advanced emotion application.
---

# MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models

## Quick Facts
- arXiv ID: 2511.00850
- Source URL: https://arxiv.org/abs/2511.00850
- Reference count: 0
- Introduces the first benchmark for evaluating emotional intelligence in spoken dialogue models through genuine multi-turn interactive dialogues

## Executive Summary
MULTI-Bench introduces a pioneering benchmark designed to evaluate emotional intelligence capabilities in spoken dialogue models through authentic multi-turn interactive dialogues. Unlike existing single-turn benchmarks, it employs a hierarchical structure that assesses both basic emotion understanding and advanced emotion application across five distinct tasks. The benchmark comprises approximately 3.2K samples covering emotion recognition, paralinguistic recognition, style inference, emotion inference, and interactive dialogue tasks. Experiments with six representative SDMs reveal that while models perform well on basic understanding tasks, they struggle significantly with advanced multi-turn interactive dialogue and reasoning tasks, particularly in emotion awareness and application. GPT-4o demonstrates the best overall performance, with Step Audio 2 showing strong results in emotion-related tasks.

## Method Summary
MULTI-Bench employs a hierarchical evaluation framework consisting of five tasks designed to progressively assess emotional intelligence in spoken dialogue models. The benchmark includes emotion recognition, paralinguistic recognition, style inference, emotion inference, and interactive dialogue tasks, totaling approximately 3.2K samples. Evaluation is conducted using both acoustic and textual assessment methods, validated against human judgment. The framework specifically targets multi-turn interactive dialogues rather than single-turn exchanges, addressing a critical gap in existing benchmarks. Six representative SDMs were evaluated, revealing performance variations across different emotional intelligence dimensions, with GPT-4o achieving the highest overall scores and Step Audio 2 performing particularly well on emotion-related tasks.

## Key Results
- GPT-4o demonstrates the best overall performance across all tasks in the MULTI-Bench evaluation
- Models show strong performance on basic emotion understanding tasks but struggle significantly with advanced multi-turn interactive dialogue and reasoning tasks
- Step Audio 2 exhibits particularly strong results in emotion-related tasks, highlighting architectural differences in emotional intelligence capabilities

## Why This Works (Mechanism)
The hierarchical structure of MULTI-Bench effectively captures the progressive nature of emotional intelligence development in dialogue systems. By separating basic emotion understanding from advanced emotion application tasks, the benchmark reveals where models succeed and where they fail in emotional reasoning. The multi-turn interactive format more accurately reflects real-world conversational scenarios compared to single-turn benchmarks, exposing limitations in models' ability to maintain emotional context and respond appropriately over extended dialogues.

## Foundational Learning
- Hierarchical task design: Enables progressive assessment of emotional intelligence from basic recognition to complex application
  - Why needed: Emotional intelligence develops in stages, requiring different cognitive capabilities
  - Quick check: Verify that task progression follows logical emotional development sequence
- Multi-turn dialogue evaluation: Captures emotional context maintenance across conversation turns
  - Why needed: Real conversations require sustained emotional awareness and adaptation
  - Quick check: Ensure dialogues contain sufficient turns to test emotional continuity
- Acoustic and textual assessment: Combines multiple modalities for comprehensive evaluation
  - Why needed: Emotional expression manifests through both verbal and non-verbal channels
  - Quick check: Validate that both assessment methods align with human judgment

## Architecture Onboarding

Component Map:
MULTI-Bench Architecture: Task Definition -> Sample Generation -> Model Evaluation -> Human Validation -> Performance Analysis

Critical Path:
The critical path flows from task definition through sample generation, where multi-turn dialogues are created with specific emotional content, followed by model evaluation using both acoustic and textual metrics, human validation to ensure assessment reliability, and final performance analysis to identify capability gaps.

Design Tradeoffs:
The benchmark balances comprehensiveness with practicality by limiting sample size to approximately 3.2K while covering five distinct emotional intelligence dimensions. The choice of six representative SDMs provides diversity but may not capture the full spectrum of architectural approaches. The multi-modal assessment approach increases evaluation reliability but requires more complex implementation.

Failure Signatures:
Models consistently fail on tasks requiring sustained emotional awareness across multiple dialogue turns, particularly when emotional context shifts or when responses require emotional reasoning rather than simple recognition. Performance drops significantly on emotion inference and interactive dialogue tasks compared to basic recognition tasks.

First Experiments:
1. Evaluate model performance on basic emotion recognition tasks to establish baseline understanding
2. Test multi-turn dialogue consistency by measuring emotional context maintenance across conversation turns
3. Assess emotion inference capabilities by analyzing responses to implicit emotional cues

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The evaluation is limited to six representative SDMs, which may not capture the full diversity of available models
- The relatively small sample size of approximately 3.2K samples may not provide comprehensive coverage of all emotional scenarios
- Validation against human judgment is not fully detailed, raising questions about the reliability of automated evaluation metrics

## Confidence
- High confidence: GPT-4o's superior performance is well-supported by experimental results and the hierarchical task structure provides a clear framework for emotional intelligence assessment
- Medium confidence: The claim about models struggling with advanced multi-turn interactive dialogue tasks is supported but could benefit from more extensive testing across diverse model architectures
- Medium confidence: Step Audio 2's strong performance in emotion-related tasks is supported but may be influenced by specific task configurations and evaluation metrics

## Next Checks
1. Expand evaluation to include a broader range of SDMs with different architectural approaches to verify generalizability of performance rankings
2. Conduct a larger-scale human evaluation study to validate automated assessment metrics and establish stronger correlations with human judgment
3. Implement cross-cultural validation by testing the benchmark with dialogues involving diverse cultural contexts and languages to assess robustness across different social norms and expression patterns