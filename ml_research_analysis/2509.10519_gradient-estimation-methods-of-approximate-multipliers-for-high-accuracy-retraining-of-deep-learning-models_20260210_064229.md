---
ver: rpa2
title: Gradient Estimation Methods of Approximate Multipliers for High-Accuracy Retraining
  of Deep Learning Models
arxiv_id: '2509.10519'
source_url: https://arxiv.org/abs/2509.10519
tags:
- retraining
- accuracy
- appmults
- gradient
- appmult
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of gradient estimation in approximate
  multipliers (AppMults) used in deep learning accelerators, where inaccuracies in
  AppMults degrade model accuracy and necessitate retraining. The authors propose
  two methods to compute more precise gradients of AppMults compared to the conventional
  straight-through estimator (STE) approach, which approximates AppMult gradients
  using those of accurate multipliers.
---

# Gradient Estimation Methods of Approximate Multipliers for High-Accuracy Retraining of Deep Learning Models

## Quick Facts
- **arXiv ID:** 2509.10519
- **Source URL:** https://arxiv.org/abs/2509.10519
- **Reference count:** 40
- **Primary result:** LUT-2D and LUT-1D methods improve retraining accuracy by 3.83% and 3.72% on CIFAR-10, and 23.69% on ImageNet compared to STE.

## Executive Summary
This paper addresses the accuracy degradation problem in deep learning models when approximate multipliers (AppMults) replace accurate multipliers in hardware accelerators. The authors propose two gradient estimation methods—LUT-2D and LUT-1D—that compute more precise gradients for AppMults compared to the conventional straight-through estimator (STE) approach. By characterizing the specific error profiles of AppMults through lookup tables, these methods enable more effective retraining, recovering accuracy lost due to approximation errors. Experimental results show significant improvements across various architectures including CNNs and Vision Transformers.

## Method Summary
The paper proposes two gradient estimation methods for approximate multipliers in deep learning models. LUT-2D uses 2-dimensional lookup tables to store fine-grained gradients of the AppMult function across all input pairs, while LUT-1D uses 1-dimensional lookup tables storing averaged gradients dependent only on weights. Both methods involve forward simulation of AppMults through LUTs and backward gradient computation using pre-computed gradient LUTs. The approach smooths discontinuous AppMult outputs to create differentiable functions, then computes precise gradients rather than relying on STE's approximation. The methods are implemented as custom CUDA kernels within a PyTorch retraining framework, supporting various AppMult designs from EvoApproxLib and custom implementations.

## Key Results
- LUT-2D and LUT-1D methods improve CIFAR-10 retraining accuracy by 3.83% and 3.72% on average compared to STE.
- On ImageNet with Vision Transformer models, LUT-1D improves retraining accuracy by 23.69% on average over state-of-the-art frameworks.
- LUT-1D achieves comparable accuracy to LUT-2D with significantly lower runtime overhead (1.2x vs 1.9x) and reduced memory usage ($2^B$ vs $2^{2B}$ entries).

## Why This Works (Mechanism)

### Mechanism 1: Smoothing Discontinuous Approximations
Approximate multiplication functions often have stair-step patterns that create zero gradients in flat regions and spikes at discontinuities. The authors apply a local averaging filter to create a differentiable curve, preventing gradient issues during training. The core assumption is that local average trends are more relevant than exact discrete jumps for learning. Evidence shows without smoothing, gradients are zero for most inputs and exhibit large values at stair edges. Break condition: if half-window size is too large relative to bit-width, smoothing may over-generalize and wash out critical gradient details.

### Mechanism 2: Fine-Grained 2D Gradient Characterization (LUT-2D)
Instead of assuming constant slopes like STE, LUT-2D pre-computes smoothed slopes for every input pair and stores them in 2D lookup tables. During backpropagation, the framework retrieves precise gradient values corresponding to current weight and activation values. The core assumption is that AppMult error derivatives vary meaningfully across input space. Evidence shows this captures error-induced deviations better than constant slope assumptions. Break condition: if memory bandwidth is constrained or AppMult error is very small, storage overhead may not justify marginal gains.

### Mechanism 3: Marginal Gradient Averaging (LUT-1D)
LUT-1D reduces gradient storage to a single dimension dependent only on weights, retaining sufficient accuracy information while significantly lowering memory and compute overhead. It computes average changing rates of AppMult output across all possible inputs for fixed weights. The core assumption is that gradient variance with respect to input is less critical than magnitude with respect to weight. Evidence shows this achieves comparable accuracy with 1.2x runtime overhead versus 1.9x for LUT-2D. Break condition: if AppMult has massive localized errors, averaging might dilute corrective signals needed for specific inputs.

## Foundational Learning

- **Concept:** **Straight-Through Estimator (STE)**
  - **Why needed here:** The paper positions its methods as direct alternatives to STE, which ignores AppMult's specific error profile.
  - **Quick check question:** If an AppMult consistently outputs Y=0 for inputs X < 10, what gradient would STE estimate versus LUT-2D?

- **Concept:** **Quantization-Aware Training (QAT)**
  - **Why needed here:** The retraining framework operates on integer-simulated models with forward pass simulating integer AppMults but requiring floating-point gradients in backward pass.
  - **Quick check question:** Why is the "dequantization" step necessary before calculating loss?

- **Concept:** **Lookup Table (LUT) based Simulation**
  - **Why needed here:** The proposed methods rely on pre-computing function outputs and gradients into LUTs rather than deriving them analytically in real-time.
  - **Quick check question:** For an 8-bit multiplier, how many entries are required for the function LUT versus the LUT-2D gradient LUT?

## Architecture Onboarding

- **Component map:** Forward Pass: Standard Conv/Linear layers → Quantizer (Int8) → AppMult LUT → Dequantizer (FP32). Backward Pass: Loss → Gradient LUT-1D/2D → Weight Update.
- **Critical path:** The Backward Propagation Kernel, where custom CUDA kernel retrieves values from Grad2DX or Grad1DX. If lookup is not optimized, training time balloons (LUT-2D takes 1.9x longer than STE).
- **Design tradeoffs:** STE: Fastest, lowest memory, lowest accuracy. LUT-1D: Balanced (1.2x runtime, moderate memory, high accuracy). LUT-2D: Heaviest (1.9x runtime, high memory, highest accuracy).
- **Failure signatures:** Accuracy Collapse (0.1%) observed before retraining indicates error too large for model to handle without gradient correction. Stalled Retraining if HWS is set improperly (vanishing or exploding gradients).
- **First 3 experiments:**
  1. Visual Validation: Plot raw AppMult output Y vs X and overlay smoothed curve and derived gradients to verify HWS=32.
  2. Sanity Check: Train VGG-like model on CIFAR-10 using high-error AppMult, compare STE vs LUT-1D convergence curves to reproduce ~15% accuracy gap.
  3. Stress Test: Apply method to Vision Transformer on ImageNet with aggressive multiplier to verify 68%+ improvement over STE.

## Open Questions the Paper Calls Out

- **Question:** Does integrating LUT-based gradient estimation into the search loop of layer-wise assignment frameworks yield better accuracy-hardware trade-offs than using them as post-processing step?
- **Basis in paper:** Section II-C states layer-wise assignment methods are complementary and can be applied once assignment is chosen. Section VI-B confirms experiments replaced all multipliers with same type, leaving coupled optimization unexplored.
- **Why unresolved:** Current study treats retraining as distinct step following multiplier selection rather than feedback mechanism to guide selection of heterogeneous approximate multipliers across layers.
- **What evidence would resolve it:** Experimental results combining LUT-1D/2D gradients within multi-objective search algorithm (e.g., NSGA-II) to jointly optimize layer-wise AppMult selection and weight parameters.

- **Question:** Can the half window size (HWS) hyperparameter be adapted automatically based on error profile of specific AppMult rather than set empirically?
- **Basis in paper:** Section VI-B1 notes HWS is set to 32 for 8-bit and 16 for 7-bit AppMults as empirical choice that works well.
- **Why unresolved:** Paper uses fixed HWS values for specific bit-widths but does not analyze sensitivity or propose mechanism to tune parameter for multipliers with vastly different error characteristics.
- **What evidence would resolve it:** Ablation study showing retraining accuracy sensitivity to HWS values, or heuristic formula linking optimal HWS to AppMult error metrics.

- **Question:** How can LUT-based gradient estimation methods be adapted for AppMults with bit-widths greater than 8 where memory footprint becomes prohibitive?
- **Basis in paper:** Section VI states paper does not study AppMults beyond 8 bits as 8-bit and sub-8-bit arithmetic are commonly used. Section IV-A justifies LUT approach by noting tables remain compact only for bit-widths ≤ 8.
- **Why unresolved:** LUT-2D method scales exponentially with bit-width (2^2B entries), making it infeasible for 16-bit or floating-point approximate multipliers without modification.
- **What evidence would resolve it:** Modified algorithm using sparse LUTs, function approximation, or on-the-fly gradient computation maintaining accuracy benefits of LUT-2D for higher bit-widths.

## Limitations
- HWS sensitivity may not generalize across all AppMult designs; too large risks over-smoothing critical gradient features.
- LUT-2D's 2^2B storage may become prohibitive for 8-bit models with many layers despite manageable runtime overhead.
- Generalizability to other architectures with different activation distributions (e.g., recurrent networks) remains unclear.

## Confidence
- **High Confidence:** Core mechanism of gradient smoothing and LUT-based gradient storage is technically sound and well-supported by ablation results.
- **Medium Confidence:** Claim that LUT-1D achieves comparable accuracy to LUT-2D with significantly lower overhead is supported but edge cases may challenge this balance.
- **Low Confidence:** Paper does not explore impact of HWS tuning or memory constraints in detail, leaving practical deployment considerations partially unaddressed.

## Next Checks
1. **HWS Sensitivity Analysis:** Systematically vary HWS (16, 32, 64) for LUT-2D on CIFAR-10 and plot accuracy vs smoothing to identify optimal ranges.
2. **Memory Profiling:** Measure VRAM usage of LUT-2D vs LUT-1D across multiple layers to confirm scalability claims.
3. **Cross-Architecture Testing:** Apply method to transformer-based architecture with ReLU vs GELU activations to assess gradient estimation robustness.