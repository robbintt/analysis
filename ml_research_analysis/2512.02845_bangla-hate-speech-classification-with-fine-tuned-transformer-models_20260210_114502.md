---
ver: rpa2
title: Bangla Hate Speech Classification with Fine-tuned Transformer Models
arxiv_id: '2512.02845'
source_url: https://arxiv.org/abs/2512.02845
tags:
- hate
- speech
- bangla
- detection
- transformer-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of hate speech detection in
  the low-resource Bangla language by developing and evaluating transformer-based
  models on the BLP 2025 dataset. The authors fine-tuned several models including
  DistilBERT, BanglaBERT, m-BERT, and XLM-RoBERTa, alongside reproducing and creating
  baseline methods such as Majority, Random, SVM, Logistic Regression, Random Forest,
  and Decision Tree.
---

# Bangla Hate Speech Classification with Fine-tuned Transformer Models

## Quick Facts
- arXiv ID: 2512.02845
- Source URL: https://arxiv.org/abs/2512.02845
- Authors: Yalda Keivan Jafari; Krishno Dey
- Reference count: 9
- Primary result: BanglaBERT achieves micro F1-scores of 0.71 (Subtask 1A) and 0.68 (Subtask 1B) on BLP 2025 dataset

## Executive Summary
This study addresses the challenge of hate speech detection in the low-resource Bangla language by developing and evaluating transformer-based models on the BLP 2025 dataset. The authors fine-tuned several models including DistilBERT, BanglaBERT, m-BERT, and XLM-RoBERTa, alongside reproducing and creating baseline methods such as Majority, Random, SVM, Logistic Regression, Random Forest, and Decision Tree. The primary result shows that all transformer-based models outperformed the baseline methods for both subtasks, with BanglaBERT achieving the best performance.

## Method Summary
The authors evaluated transformer-based models (DistilBERT, BanglaBERT, m-BERT, XLM-RoBERTa) alongside traditional machine learning baselines on the BLP 2025 dataset for Bangla hate speech classification. Models were fine-tuned for two subtasks: hate type classification and target identification. The evaluation compared transformer approaches against baseline methods including Majority, Random, SVM, Logistic Regression, Random Forest, and Decision Tree classifiers.

## Key Results
- Transformer-based models outperformed all baseline methods for both subtasks
- BanglaBERT achieved the highest performance with micro F1-score of 0.71 and accuracy of 0.71 for Subtask 1A
- BanglaBERT achieved micro F1-score of 0.68 and accuracy of 0.71 for Subtask 1B
- Language-specific pre-training showed clear benefits for Bangla hate speech detection

## Why This Works (Mechanism)
Language-specific pre-training enables better understanding of Bangla linguistic patterns and hate speech indicators. Transformer architectures with self-attention mechanisms capture contextual relationships in text more effectively than traditional baselines. The fine-tuning process adapts pre-trained representations to the specific task of hate speech detection while maintaining language-specific knowledge gained during pre-training.

## Foundational Learning
1. **Transformer architecture** - why needed: enables parallel processing and captures long-range dependencies; quick check: verify self-attention mechanism implementation
2. **Fine-tuning process** - why needed: adapts pre-trained models to specific downstream tasks; quick check: confirm learning rate and epochs used
3. **Evaluation metrics (micro F1-score, accuracy)** - why needed: measures classification performance accounting for class imbalance; quick check: verify metric calculations match paper
4. **Cross-lingual models (m-BERT, XLM-R)** - why needed: enable zero-shot or few-shot learning across languages; quick check: confirm multilingual tokenization
5. **Language-specific models (BanglaBERT)** - why needed: optimized for Bangla linguistic patterns; quick check: verify training corpus size and domain
6. **Baseline methods (SVM, Logistic Regression)** - why needed: provide comparison with traditional approaches; quick check: confirm feature engineering techniques

## Architecture Onboarding

**Component map:** Input text -> Tokenizer -> Transformer encoder -> Classification head -> Output predictions

**Critical path:** Text input flows through language-specific tokenizer, transformer layers with self-attention, pooling layer, and classification head to produce subtask predictions

**Design tradeoffs:** Language-specific models (BanglaBERT) provide better performance but require Bangla-specific pre-training data; multilingual models offer flexibility but sacrifice some language-specific optimization

**Failure signatures:** Poor performance on out-of-domain text, sensitivity to tokenization errors in Bangla script, potential overfitting on small datasets

**First experiments:**
1. Test transformer model on held-out validation set before full evaluation
2. Compare tokenization outputs between BanglaBERT and m-BERT on sample Bangla text
3. Evaluate baseline methods on balanced vs imbalanced versions of the dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on single dataset (BLP 2025) without cross-validation across multiple hate speech corpora
- No standard deviation or confidence intervals reported for performance metrics
- Traditional baseline comparisons use relatively simple feature engineering approaches

## Confidence
**High confidence:** Transformer models outperform baseline methods
**Medium confidence:** BanglaBERT specifically benefits from language-specific pre-training
**Medium confidence:** Absolute performance numbers reported

## Next Checks
1. Conduct k-fold cross-validation on the BLP 2025 dataset to establish confidence intervals for all reported metrics
2. Evaluate the same models on at least two additional Bangla hate speech datasets to verify generalizability
3. Compare with more sophisticated traditional baselines incorporating advanced feature engineering (e.g., word embeddings, character n-grams with TF-IDF)