---
ver: rpa2
title: Refining Transcripts With TV Subtitles by Prompt-Based Weakly Supervised Training
  of ASR
arxiv_id: '2509.04491'
source_url: https://arxiv.org/abs/2509.04491
tags:
- training
- speech
- subtitles
- transcripts
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of using TV subtitles for automatic
  speech recognition (ASR) when subtitles are not perfectly aligned with audio. The
  authors propose a novel approach that treats subtitles as context-rich prompts for
  fine-tuning Whisper, rather than using them as direct training targets.
---

# Refining Transcripts With TV Subtitles by Prompt-Based Weakly Supervised Training of ASR

## Quick Facts
- arXiv ID: 2509.04491
- Source URL: https://arxiv.org/abs/2509.04491
- Reference count: 23
- Final WER achieved: 10.34% on manually annotated test set

## Executive Summary
This paper addresses the challenge of using TV subtitles for automatic speech recognition (ASR) when subtitles are not perfectly aligned with audio. The authors propose a novel approach that treats subtitles as context-rich prompts for fine-tuning Whisper, rather than using them as direct training targets. Their method combines iterative refinement with pseudo transcripts generated by Whisper, using subtitles to guide improvements. Additionally, they introduce a weighted attention mechanism during inference that emphasizes relevant subtitle tokens based on Gini coefficient calculations from cross-attention weights. Experiments on Flemish broadcast data show significant improvements in word error rate (WER), with the final model achieving 10.34% WER on a manually annotated test set.

## Method Summary
The authors propose a weakly supervised training approach for ASR using TV subtitles as prompts rather than direct targets. The method involves generating initial pseudo transcripts with pre-trained Whisper, then iteratively fine-tuning the model using these pseudo transcripts as targets while subtitles provide contextual guidance. The subtitle tokens are prepended to the decoder input with special markers (`<|sop|>` and `<|sot|>`) and used to condition the self-attention mechanism. The training is performed over three iterations, each time generating new pseudo transcripts with the updated model. During inference, a weighted attention mechanism emphasizes subtitle tokens that show concentrated cross-attention patterns (measured by Gini coefficient) as more likely to be speech-relevant.

## Key Results
- Final model achieves 10.34% WER on manually annotated test set (vs 13.07% baseline Whisper-large-v3)
- Subtitle prompting (SP) alone reduces rare word error rate (rWER) from 31.04% to 24.63% and OOV word error rate (oWER) from 74.94% to 70.22%
- Weighted attention (WA) applied to all decoder layers yields best WER of 11.02%
- Iterative refinement shows diminishing returns after iteration 3, with WER improving from 11.54% (iter1) → 10.82% (iter2) → 10.52% (iter3)

## Why This Works (Mechanism)

### Mechanism 1: Subtitle-as-Prompt Contextual Guidance
- **Claim:** Subtitles provide lexical cues for rare/OOV words without requiring exact alignment with audio.
- **Mechanism:** Subtitles are prepended to the decoder input as `<|sop|>subtitles<|sot|>...`, enabling self-attention to condition on subtitle tokens while the loss is computed only on pseudo transcripts. This provides named entity spelling and vocabulary hints without forcing verbatim matching.
- **Core assumption:** Subtitles contain enough overlapping keywords with speech to provide useful lexical context, even when temporally misaligned or semantically divergent.
- **Evidence anchors:**
  - [abstract] "generated pseudo transcripts become the primary targets, with subtitles acting as guiding cues"
  - [Section V-A, Table II] rWER drops from 31.04% to 24.63% with SP; oWER from 74.94% to 70.22%
  - [corpus] Weak supervision with noisy transcripts shows gains in related work (arXiv:2505.17088), supporting the premise that imperfect text can guide ASR.
- **Break condition:** If subtitle content has <10% lexical overlap with speech, prompt provides negligible signal and may increase hallucination risk.

### Mechanism 2: Iterative Pseudo-Label Refinement
- **Claim:** Multi-pass training progressively extracts information from subtitles into improved pseudo transcripts.
- **Mechanism:** Initial pseudo transcripts Y_pt^0 from pre-trained Whisper are refined each iteration: model learns to incorporate subtitle context, generates Y_pt^1, retrains on updated targets. Over t iterations, pseudo labels converge toward subtitle-informed accuracy.
- **Core assumption:** Subtitle prompts inject net-positive information that outweighs error propagation risks from self-training loops.
- **Evidence anchors:**
  - [Section III-A] "This process is repeated over t iterations, gradually evolving the pseudo labels from Y_pt^0 to Y_pt^t"
  - [Section V-C, Table IV] WER improves from 11.54% (iter1) → 10.82% (iter2) → 10.52% (iter3)
  - [corpus] Iterative correction frameworks (arXiv:2509.15095) support multi-pass refinement efficacy, though in LLM-based post-processing.
- **Break condition:** Diminishing returns set in by iteration 3; continued training risks overfitting to residual errors.

### Mechanism 3: Gini-Based Weighted Attention for Token Relevance
- **Claim:** Cross-attention distribution sparsity (measured by Gini coefficient) identifies subtitle tokens most relevant to speech.
- **Mechanism:** For each subtitle token i, compute Gini coefficient g_i over its cross-attention weights across N speech frames. Tokens with concentrated attention (g_i → 1) are speech-relevant; uniform attention (g_i → 0) indicates distraction. Weights are applied to K_p, V_p in self-attention: K' = G⊗K_p ⊕ K_t.
- **Core assumption:** Speech-aligned tokens exhibit sharper cross-attention peaks than irrelevant tokens.
- **Evidence anchors:**
  - [Section III-B, Eq. 1] Gini formula defined; applied to first cross-attention layer for direct audio-text mapping
  - [Section V-B, Fig. 2] WA applied to all layers yields best WER (11.02%) vs. no weighting (11.54%)
  - [corpus] No direct corpus validation of Gini-based attention weighting found; this appears novel.
- **Break condition:** Max-weighting causes hallucinations; entropy-based weighting loses sensitivity—Gini's intermediate sparsity measure is critical.

## Foundational Learning

- **Concept: Weak Supervision in ASR**
  - **Why needed here:** Understanding that noisy/imprecise labels can still provide training signal when properly conditioned.
  - **Quick check question:** Can you explain why training directly on misaligned subtitles (34.3% WER vs. verbatim) would degrade model performance, but using them as prompts does not?

- **Concept: Cross-Attention in Encoder-Decoder Architectures**
  - **Why needed here:** The WA mechanism relies on interpreting cross-attention patterns as audio-text alignment indicators.
  - **Quick check question:** In Whisper's architecture, which layer's cross-attention weights best represent raw audio-text correspondence before higher-level text representations form?

- **Concept: Prompt Engineering for Autoregressive Decoders**
  - **Why needed here:** Subtitles must be structured as prompts (not targets) to avoid output truncation from content overlap.
  - **Quick check question:** Why does prefixing subtitles to decoder input without careful token design cause Whisper to produce truncated or empty outputs?

## Architecture Onboarding

- **Component map:**
  - Whisper encoder: Processes 15-30s audio segments into frame-level features (N frames)
  - Whisper decoder: Autoregressive generation with cross-attention to encoder, self-attention over prompt + generated tokens
  - SP training module: Prepends subtitle tokens via `<|sop|>...<|sot|>` prefix; computes loss only on transcript portion
  - WA inference module: Extracts first-layer cross-attention CA[i,k], computes per-token Gini coefficients G, applies G⊗K_p, G⊗V_p weighting to all decoder layers

- **Critical path:**
  1. Generate Y_pt^0 with pre-trained Whisper-large-v3
  2. Filter hallucinations (repetition detection, length thresholding)
  3. Fine-tune with SP for 1 epoch → generate Y_pt^1
  4. Repeat steps 2-3 for t=3 iterations
  5. At inference: compute Gini weights, apply WA to all decoder layers

- **Design tradeoffs:**
  - **Medium vs. Large model:** Large model overfits to pseudo-label errors without SP; medium model more stable but lower ceiling. SP enables large model recovery.
  - **WA layer selection:** Applying to all layers yields best results but increases inference compute; single-layer application shows variable results across folds.
  - **Iteration count:** Diminishing returns after iteration 3; computational cost scales linearly with iterations.

- **Failure signatures:**
  - **Self-training without SP:** WER degrades from 13.07% to 21.49% (large model) due to deletion error propagation (>80% of errors)
  - **Direct subtitle prompting pre-trained Whisper:** Produces truncated/empty outputs from content overlap
  - **Max-weighting WA:** Induces hallucinations from over-reliance on single-frame attention peaks
  - **Entropy-weighting WA:** Compresses most tokens to near-zero weights, losing discriminative signal

- **First 3 experiments:**
  1. **Baseline validation:** Run pre-trained Whisper-large-v3 on subs-annot test set; confirm ~13% WER baseline. Then fine-tune WITHOUT prompts on Y_pt^0 to observe error propagation (expected: WER increase).
  2. **SP ablation:** Fine-tune Whisper-medium with SP for 1 iteration; compare WER, rWER, oWER against no-prompt baseline. Verify ~6% absolute rWER reduction.
  3. **WA layer sweep:** Apply Gini-weighted attention to decoder layers 1, 4, 8, all; test on 5-fold split of subs-annot to identify optimal layer configuration and validate across-fold consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the subtitle prompting approach generalize to languages with different subtitle conventions or non-alphabetic writing systems (e.g., Chinese, Arabic)?
- Basis in paper: [explicit] The authors state "We experiment on Flemish, a Dutch dialect" and position the work for "underrepresented and low-resource domains," but all experiments are limited to this single language family.
- Why unresolved: Subtitle timing conventions, compression ratios, and text-audio alignment patterns may differ significantly across languages and broadcasting cultures, potentially affecting the Gini-based attention weighting mechanism.
- What evidence would resolve it: Experiments on typologically diverse languages with different subtitle conventions, comparing WER improvements across language families.

### Open Question 2
- Question: What is the minimum subtitle quality threshold (in terms of alignment accuracy or content overlap with speech) required for the method to provide benefits over self-training without prompts?
- Basis in paper: [explicit] The paper reports subtitles have 34.3% WER on the test set, motivating the prompt-based approach, but does not investigate the lower bound of subtitle quality.
- Why unresolved: If subtitle quality degrades beyond a certain point, the distraction tokens may outweigh relevant tokens, potentially harming rather than helping refinement.
- What evidence would resolve it: Controlled experiments with artificially degraded subtitles at varying WER levels (e.g., 40%, 50%, 60%) to identify the quality floor.

### Open Question 3
- Question: How should the optimal number of iterative training cycles be determined for a given dataset, rather than arbitrarily stopping at three iterations?
- Basis in paper: [inferred] The paper notes "the rate of improvement diminishes with each subsequent iteration" and "training is halted after three updates," but provides no principled stopping criterion.
- Why unresolved: Convergence behavior likely depends on dataset size, subtitle quality, and initial pseudo-transcript quality; a fixed iteration count may be suboptimal for different scenarios.
- What evidence would resolve it: Analysis of convergence curves across multiple datasets with different characteristics, potentially developing an automatic stopping criterion based on validation metrics.

### Open Question 4
- Question: Can the Gini-based attention weighting mechanism be adapted for transformer architectures where cross-attention patterns differ from Whisper's monotonic alignment?
- Basis in paper: [inferred] The method relies on the observation that Whisper's first cross-attention layer "exhibits the most straightforward monotonic alignment," but this property may not hold for other ASR architectures.
- Why unresolved: The Gini coefficient approach assumes concentrated attention indicates relevance; this assumption depends on the specific attention patterns learned by the model.
- What evidence would resolve it: Experiments applying the method to other encoder-decoder ASR models (e.g., conformer-based architectures) and analyzing whether Gini weighting remains effective.

## Limitations

- **Subtitle alignment assumptions:** The paper assumes subtitles provide sufficient lexical overlap to guide refinement, but no analysis quantifies actual overlap rates between subtitle text and spoken content.
- **Gini coefficient novelty:** The WA mechanism using Gini-based attention weighting appears novel but lacks validation beyond WER improvements and doesn't verify whether high-Gini tokens actually correspond to speech-aligned content.
- **Hyperparameter sensitivity:** Critical parameters like hallucination filtering thresholds, iteration count, and WA weighting strength are set empirically without thorough ablation studies.

## Confidence

- **High confidence:** The core insight that misaligned subtitles can provide contextual guidance when used as prompts rather than targets is well-supported by iterative improvement patterns and ablation studies showing SP's necessity.
- **Medium confidence:** The 10.34% WER improvement claim is reproducible given the described methodology, but the exact contribution of each component (SP vs WA vs iterations) remains partially unclear.
- **Low confidence:** The interpretation of Gini coefficients as measuring subtitle token relevance lacks external validation and doesn't verify whether high-Gini tokens actually correspond to speech-aligned content versus coincidental attention concentration.

## Next Checks

1. **Lexical overlap analysis:** Compute and report the percentage of subtitle tokens that appear in corresponding speech transcripts across the training corpus. This would quantify the fundamental signal available to the subtitle-as-prompt mechanism and help predict generalization to other domains.

2. **Attention interpretability study:** For a subset of utterances, manually annotate which subtitle tokens align with speech content, then correlate these annotations with Gini coefficient rankings. This would validate whether the Gini-based weighting actually identifies speech-relevant tokens versus capturing spurious patterns.

3. **Domain transfer experiment:** Apply the complete methodology (SP + WA + 3 iterations) to a different ASR domain with misaligned text supervision (e.g., YouTube captions, meeting transcripts) and compare performance relative to baseline Whisper. This would test whether the approach generalizes beyond Flemish broadcast television.