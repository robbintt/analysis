---
ver: rpa2
title: 'PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions'
arxiv_id: '2510.19060'
source_url: https://arxiv.org/abs/2510.19060
tags:
- image
- docent
- judgments
- scene
- posh
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PoSh, a metric for evaluating detailed image
  descriptions that uses scene graphs as structured rubrics to guide LLM-as-a-Judge.
  PoSh identifies granular errors (mistakes and omissions) in generated descriptions
  by comparing them against reference descriptions using scene graph extraction and
  question answering, then aggregates these into interpretable coarse scores.
---

# PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions

## Quick Facts
- **arXiv ID:** 2510.19060
- **Source URL:** https://arxiv.org/abs/2510.19060
- **Authors:** Amith Ananthram; Elias Stengel-Eskin; Lorena A. Bradford; Julia Demarest; Adam Purvis; Keith Krut; Robert Stein; Rina Elster Pantalony; Mohit Bansal; Kathleen McKeown
- **Reference count:** 33
- **Primary result:** Introduces PoSh, a metric achieving stronger correlations with human judgments than existing metrics for detailed image description evaluation

## Executive Summary
PoSh introduces a novel metric for evaluating detailed image descriptions by using scene graphs as structured rubrics to guide LLM-as-a-Judge evaluation. The system converts reference and generated descriptions into scene graphs, then uses an LLM to verify the presence of each component through question answering, ultimately producing interpretable granular and coarse scores for mistakes and omissions. PoSh outperforms existing metrics on the DOCENT benchmark and demonstrates robustness across different image types while running efficiently on a single GPU.

## Method Summary
PoSh operates through a three-step pipeline: first, it extracts scene graphs from reference and generated texts using dependency parsing and coreference resolution to identify objects, attributes, and relations; second, it uses an LLM (Qwen-2.5-14B) to verify each graph component's presence in the other text through templated question answering; finally, it aggregates these granular scores into interpretable coarse scores for mistakes (precision errors) and omissions (recall errors). The system uses unique identifiers for graph components to handle coreference and ensure accurate verification.

## Key Results
- PoSh achieves Spearman ρ correlation of 0.580 for mistake detection and 0.680 for omission detection
- Outperforms existing metrics including GPT-4o on DOCENT benchmark (+0.05 Spearman ρ improvement)
- When used as a reward function, PoSh-tuned models produce descriptions with fewer missing details despite having more mistakes
- Runs efficiently in 2 seconds per description on a single H100 GPU

## Why This Works (Mechanism)

### Mechanism 1: Structural Normalization via Scene Graphs
Converting unstructured text into scene graphs isolates compositional facts, reducing surface-form variation noise. The system parses descriptions into subject-relation-object triples and object-attribute pairs, forcing evaluation to judge specific factual claims rather than general semantic similarity. This relies on dependency parsing and coreference resolution accurately extracting relational structure from complex descriptions.

### Mechanism 2: LLM-Guided Soft Verification
Using an LLM to verify graph components via question answering handles synonyms and paraphrasing better than hard matching or embedding similarity. Instead of checking if "sitting" equals "seated," the system prompts the LLM with templated questions, assigning soft scores (1-5) that bridge rigid logic and fuzzy semantics. This assumes the judge LLM can perform grounded reasoning without hallucinating implied attributes.

### Mechanism 3: Granularity-to-Coarse Aggregation
Aggregating fine-grained binary judgments into coarse scores provides a more robust proxy for human preference than end-to-end regression. The system averages verification scores of all scene graph components to produce "Mistake" (Precision) and "Omission" (Recall) scores, mimicking human rubrics of counting specific errors rather than holistic scores. This assumes all components contribute equally to quality.

## Foundational Learning

- **Scene Graphs & Compositional Representation**: Understanding how "A red car on the left" becomes Nodes (Car) and Edges (Color=Red, Position=Left) is essential for debugging PoSh's graph extraction accuracy.
- **Precision vs. Recall in Generation**: PoSh explicitly separates "Mistakes" (hallucinations/precision errors) from "Omissions" (lack of coverage/recall errors), crucial for interpreting metric outputs.
- **LLM-as-a-Judge / Calibration**: The mechanism relies on an LLM acting as a classifier, requiring understanding of prompt engineering and LLM bias toward "present" labels.

## Architecture Onboarding

- **Component map:** Input -> Graph Extractor -> Templater -> Verifier -> Aggregator
- **Critical path:** The Graph Extractor is the bottleneck. If coreference resolution fails (e.g., identifying "the man" vs "a man" in a crowd), the Verifier receives ambiguous prompts, leading to noisy evaluations.
- **Design tradeoffs:** Uses 14B open-weight model (Qwen) for replicability over potential performance gains from larger models like GPT-4; slower than embedding-based metrics due to LLM inference for every graph component.
- **Failure signatures:** Low agreement on complex images (>10 people) due to entity linking failures; hallucination propagation if reference text contains errors.
- **First 3 experiments:** 1) Unit test graph extraction by verifying >90% of subject-attribute pairs in DOCENT references; 2) Ablate the verifier by replacing LLM with embedding similarity to quantify LLM value; 3) Calibration check by running PoSh on 50 pairs where Text A is clearly superior and verifying correlation with ground truth.

## Open Questions the Paper Calls Out

None

## Limitations

- **Graph extraction robustness**: The system's heavy dependence on dependency parsing and coreference resolution accuracy isn't validated for complex descriptions with multiple subjects or ambiguous pronouns.
- **LLM-as-a-Judge reliability**: The paper doesn't establish whether Qwen-2.5-14B has particular advantages over alternatives or address potential biases in how LLMs handle "present/absent" judgments.
- **Generalizability to non-art contexts**: While shown robust on CapArena, the evaluation doesn't systematically test performance on everyday scenes, medical images, or technical diagrams where visual element salience differs.

## Confidence

**High confidence**: Correlation improvements over existing metrics are well-supported by DOCENT benchmark data, and scene graph extraction methodology is clearly specified and reproducible.

**Medium confidence**: The claim about producing fewer omissions while maintaining more mistakes than supervised fine-tuning is supported but doesn't explore whether this tradeoff is optimal for all use cases.

**Low confidence**: The paper doesn't provide sufficient evidence that 14B LLM verifier is optimal - smaller models might achieve similar results with better efficiency, and specific heuristics for unique identifier selection aren't fully validated for edge cases.

## Next Checks

1. **Error analysis on graph extraction failures**: Run PoSh on 100 DOCENT pairs where reference contains >3 subjects and manually verify coreference resolution accuracy, documenting percentage of attributes incorrectly attached to entities.

2. **Ablation study on LLM size**: Replace Qwen-2.5-14B with Qwen-1.8-7B and Qwen-2.5-72B on DOCENT benchmark, measuring correlation drops and processing time differences to establish whether 14B is the sweet spot.

3. **Cross-domain robustness test**: Apply PoSh to 100 image descriptions from COCO and 100 from medical imaging datasets, comparing correlation with human judgments to DOCENT results to quantify domain generalization.