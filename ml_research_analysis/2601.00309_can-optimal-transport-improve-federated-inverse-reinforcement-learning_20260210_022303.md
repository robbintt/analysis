---
ver: rpa2
title: Can Optimal Transport Improve Federated Inverse Reinforcement Learning?
arxiv_id: '2601.00309'
source_url: https://arxiv.org/abs/2601.00309
tags:
- reward
- learning
- barycenter
- wasserstein
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a federated inverse reinforcement learning
  framework that aggregates local MaxEnt IRL rewards using a Wasserstein barycenter
  rather than simple parameter averaging. By treating each reward function as a probability
  measure over a shared state-action support and computing an entropically regularized
  Wasserstein barycenter, the method preserves geometric structure, suppresses artifacts
  from undertrained clients, and yields a more faithful global reward.
---

# Can Optimal Transport Improve Federated Inverse Reinforcement Learning?

## Quick Facts
- **arXiv ID:** 2601.00309
- **Source URL:** https://arxiv.org/abs/2601.00309
- **Reference count:** 10
- **Primary result:** Federated IRL with Wasserstein barycenter aggregation outperforms parameter averaging, especially in heterogeneous settings

## Executive Summary
This paper addresses the challenge of aggregating local reward functions in federated inverse reinforcement learning (IRL) by introducing a Wasserstein barycenter-based approach. Instead of naive parameter averaging, each client's MaxEnt IRL reward is treated as a probability measure over a shared state-action space, and an entropically regularized Wasserstein barycenter is computed to fuse these rewards. The method preserves geometric structure and mitigates artifacts from undertrained clients. Theoretical analysis provides stability and parameter-error bounds, while experiments demonstrate consistent improvements over parameter averaging in both discrete and continuous-control benchmarks.

## Method Summary
The framework treats each local reward function as a probability measure over a shared state-action support and computes an entropically regularized Wasserstein barycenter to aggregate them. This approach preserves the geometric structure of reward functions, unlike parameter averaging which can produce artifacts when local rewards are undertrained or heterogeneous. The method is built on the Maximum Entropy IRL framework and incorporates federated learning principles where clients train locally and only the aggregated reward is communicated to a central server.

## Key Results
- Barycentric fusion consistently outperforms parameter averaging in discrete and continuous-control benchmarks
- Largest performance gains observed in heterogeneous client settings
- Theoretical bounds show fused reward contracts toward true reward under bounded local estimation error

## Why This Works (Mechanism)
By treating reward functions as probability measures and computing a Wasserstein barycenter, the method preserves the geometric relationships between different reward structures. This geometric preservation is crucial because simple averaging can create artifacts when local rewards are undertrained or when clients have heterogeneous data distributions. The entropic regularization makes the optimization tractable while maintaining approximation quality.

## Foundational Learning
- **Maximum Entropy IRL:** Why needed - provides probabilistic framework for reward learning; Quick check - verify log-likelihood computation matches standard MaxEnt formulations
- **Wasserstein Barycenters:** Why needed - enables geometric aggregation of probability measures; Quick check - confirm barycentric computation reduces to averaging when measures are Dirac deltas
- **Entropic Regularization:** Why needed - makes Wasserstein distance computation tractable; Quick check - verify Sinkhorn iterations converge within acceptable iterations
- **Federated Learning:** Why needed - enables privacy-preserving collaborative learning; Quick check - confirm communication only involves aggregated reward, not raw data
- **Probability Measures over State-Action Spaces:** Why needed - provides mathematical framework for representing rewards; Quick check - ensure measure representations are normalized
- **Error Propagation in Distributed Learning:** Why needed - critical for understanding stability guarantees; Quick check - verify theoretical bounds hold under simulated local estimation errors

## Architecture Onboarding
- **Component Map:** Clients (local MaxEnt IRL) -> Wasserstein Barycenter Computation -> Global Reward
- **Critical Path:** Local training → Reward measure construction → Barycentric aggregation → Evaluation
- **Design Tradeoffs:** Computational cost of OT vs. fidelity of aggregation; communication efficiency vs. aggregation quality
- **Failure Signatures:** Aggregation artifacts when heterogeneity is extreme; numerical instability in high dimensions
- **First 3 Experiments:** 1) Simple gridworld with 2-3 clients; 2) Continuous control with synthetic heterogeneity; 3) Ablation study comparing barycentric vs. averaging

## Open Questions the Paper Calls Out
None

## Limitations
- Computational scalability challenges in high-dimensional state-action spaces
- Performance uncertainty in extremely heterogeneous client settings
- Theoretical assumptions may not hold under complex real-world noise patterns

## Confidence
- Empirical claims: **High** for tested discrete and moderate-scale continuous benchmarks; **Medium** for scalability and extreme heterogeneity
- Theoretical bounds: **High** within stated assumptions
- Scalability claims: **Medium** due to limited high-dimensional testing

## Next Checks
1. Benchmark the method on high-dimensional continuous-control tasks (e.g., 100+ state dimensions) to assess computational feasibility and performance degradation.
2. Systematically vary the degree of heterogeneity across clients to identify breaking points where barycentric aggregation fails to outperform simple averaging.
3. Evaluate robustness to non-i.i.d. data distributions by introducing structured biases in local client datasets and measuring impact on reward fidelity.