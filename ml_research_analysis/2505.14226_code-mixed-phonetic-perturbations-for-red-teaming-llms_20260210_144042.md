---
ver: rpa2
title: Code-Mixed Phonetic Perturbations for Red-Teaming LLMs
arxiv_id: '2505.14226'
source_url: https://arxiv.org/abs/2505.14226
tags:
- safety
- prompts
- prompt
- english
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CMP-RT, a red-teaming probe that combines
  code-mixing with phonetic perturbations to expose a tokenizer-level safety vulnerability
  in LLMs. The method preserves phonetic integrity while perturbing sensitive tokens,
  allowing harmful prompts to bypass safety alignment mechanisms while maintaining
  high interpretability.
---

# Code-Mixed Phonetic Perturbations for Red-Teaming LLMs

## Quick Facts
- **arXiv ID**: 2505.14226
- **Source URL**: https://arxiv.org/abs/2505.14226
- **Reference count**: 29
- **Primary result**: CMP-RT attack increases bypass rates up to 99% (text) and 78% (images) via tokenizer-level fragmentation

## Executive Summary
This paper introduces CMP-RT, a red-teaming probe that combines code-mixing with phonetic perturbations to expose a tokenizer-level safety vulnerability in LLMs. The method preserves phonetic integrity while perturbing sensitive tokens, allowing harmful prompts to bypass safety alignment mechanisms while maintaining high interpretability. Evaluations show CMP-RT increases attack success rates (up to 99% for text, 78% for images) across multiple models including SOTA models like Gemini-3-Pro. Interpretability analysis using Integrated Gradients reveals that phonetic perturbations alter tokenization in a way that suppresses attributions from safety-critical tokens. CMP-RT generalizes across modalities and scales through automated fine-tuning, demonstrating that tokenization is a fundamental vulnerability.

## Method Summary
The CMP-RT pipeline transforms harmful prompts through three stages: (1) convert to hypothetical scenarios, (2) code-mix via Hindi transliteration, and (3) apply phonetic perturbations to sensitive words. The method uses Levenshtein distance-based perturbations with average 4.5 character changes and 10.91% perturbation density. Manual CMP prompts achieve 63% attack success rate versus 46% for automated variants. The approach is evaluated across 6 temperatures, 5 jailbreak templates, and 4 model architectures, with Integrated Gradients analysis at embedding and decoder layers 1, 8, 16 to measure attribution changes.

## Key Results
- CMP-RT increases attack success rates up to 99% for text prompts and 78% for image prompts
- Manual CMP prompts achieve 63% AASR versus 46% for automated variants
- Phonetic perturbations cause tokenization fragmentation that suppresses attribution scores from safety-critical tokens
- Vulnerability generalizes across modalities and scales through automated fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Tokenization Fragmentation
Phonetic perturbations (e.g., "hate" → "haet") cause sensitive words to be tokenized differently than their canonical forms, fragmenting them into multiple tokens with reduced individual semantic weight. The tokenizer splits perturbed words into subword units (e.g., "ha" + "et" instead of "hate"), which individually carry less specific meaning and are less likely to trigger safety filters trained on canonical token sequences.

### Mechanism 2: Representational Gap Exploitation
A mismatch exists between the informal, noisy text seen during pre-training and the formal text used during safety alignment, creating an exploitable gap. LLMs learn to interpret phonetically perturbed, code-mixed inputs during pre-training on internet data, allowing them to understand the intent (high AARR). However, safety alignment is performed on standardized inputs, so filters are not conditioned on these informal representations, leading to bypass (high AASR).

### Mechanism 3: Attribution Suppression via Sub-optimal Input Representation
The fragmented, non-canonical tokenization of CMP inputs alters the attribution of safety-critical tokens, reducing their importance scores as the model processes the input. Using Integrated Gradients, the authors show that for safe outputs, safety-critical tokens in canonical prompts have high attribution scores. In CMP prompts, the perturbed tokens show low attribution scores, meaning they contribute less to triggering a safe response.

## Foundational Learning

- **Tokenization (BPE/subword)**: Understanding how perturbations like "dezain" are split into tokens differently than "design" is essential. *Quick check*: How would the tokenizer for a model like GPT-2 or Llama-3 split "important" vs. "importantt"?

- **Safety Alignment (RLHF, SFT)**: The attack exploits a gap in this process. Grasping that alignment trains models to refuse harmful requests, typically on canonical data, is key. *Quick check*: What is the primary goal of RLHF in LLM safety? What does a "refusal" response look like?

- **Interpretability Methods (Attribution, Integrated Gradients)**: The paper's evidence for its failure mechanism comes from attribution analysis. Evaluating this evidence requires understanding what these methods measure. *Quick check*: What does a high attribution score for an input token indicate? What is a known limitation of gradient-based attribution methods?

## Architecture Onboarding

- **Component map**: Input Prompt → English→Hypothetical→Code-Mixed→CMP → Target Model's Tokenizer → LLM Judge (GPT-4o-mini) → Interpretability Tool (Captum)
- **Critical path**: Input Prompt → CMP-RT Transformation → Target Model's Tokenizer. The attack succeeds or fails at the tokenization step. If the perturbation does not fragment the sensitive word's token sequence, the downstream safety filters will likely engage.
- **Design tradeoffs**: Automation vs. Quality (manual 0.63 AASR vs automated 0.46); Perturbation Density (too little may not bypass, too much destroys interpretability, reported 10.91%); Language Choice (Hindi-English efficacy, others unknown)
- **Failure signatures**: High AASR with High AARR; Tokenization Change (perturbed words tokenized differently); Low Attribution Scores for safety-critical tokens
- **First 3 experiments**:
  1. **Tokenizer Analysis**: For a target LLM, create a list of safety-sensitive words. For each, generate 3-5 phonetic perturbations. Use the tokenizer to compare canonical vs. perturbed token sequences. Document significant changes.
  2. **Minimal Probe**: Manually craft 10 CMP-style prompts. Run against a target model and measure refusal rate. Compare with refusal rate for canonical English counterparts to establish baseline vulnerability.
  3. **Attribution Check**: Select one prompt pair (English vs. CMP) where CMP succeeds and English fails. Use an interpretability library to compute attribution scores. Compare scores for safety-critical tokens to observe suppression effect.

## Open Questions the Paper Calls Out

1. **Can specific alignment strategies be developed to mitigate tokenizer-level vulnerabilities exposed by CMP-RT without compromising multilingual capabilities?**
   - Basis: Future Work section states "An immediate area of future work is to align the models based on the findings from the interpretability experiments"
   - Unresolved: Authors demonstrated current safety training fails to account for pre-training vs. alignment data gap but didn't propose remediation methods
   - Resolution: Successful fine-tuning or RLHF on CMP-style datasets that reduces ASR while maintaining multilingual benchmark performance

2. **Does the CMP-RT vulnerability generalize to typologically diverse languages, particularly low-resource or non-Indic languages?**
   - Basis: Limitations section notes study restricted to English-Hindi and states "We plan to extend this to other languages, especially other Indic and low-resource languages"
   - Unresolved: Phonetic perturbation strategies and tokenizer behaviors may differ significantly across scripts and language families
   - Resolution: Evaluation on models trained on diverse linguistic datasets showing similar or divergent Attack Success Rates

3. **Can the CMP-RT attack vector be extended to audio-based LLMs where phonetic perturbations are inherent to the modality?**
   - Basis: Future Work section lists "other output modalities such as speech" as scaling target
   - Unresolved: While paper establishes vulnerability in text-to-image generation, speech-to-text or text-to-speech models process acoustic features differently than text tokenizers
   - Resolution: Adaptation to Audio-Language Models demonstrating high Attack Success Rates against audio safety filters

## Limitations

- **Interpretability uncertainty**: Attribution analysis relies on Integrated Gradients, which has limitations in capturing true causal relationships between tokens and outputs
- **Automation scaling challenges**: Automated CMP pipeline achieves only 46% AASR versus 63% for manual variants, revealing fundamental quality gaps
- **Language generalizability unknown**: Focus on Hindi-English leaves open questions about efficacy with other language pairs and cultural contexts

## Confidence

- **High Confidence**: Core empirical finding that CMP-RT increases attack success rates across multiple models (up to 99% for text, 78% for images) is well-supported by systematic testing
- **Medium Confidence**: Mechanism explanation linking tokenization fragmentation to attribution suppression is plausible but not definitively proven
- **Low Confidence**: Claim that safety alignment systematically fails to incorporate informal communication elements is inferred rather than directly measured

## Next Checks

1. **Tokenizer Robustness Test**: Systematically generate 50+ phonetic perturbations per sensitive word and measure consistency of tokenization changes. Track correlation between fragmentation severity and attack success rates to establish causal mechanism.

2. **Alternative Attribution Methods**: Replicate key attribution analysis using multiple interpretability methods (Occlusion, Shapley Values, Attention Rollout) alongside Integrated Gradients. Compare results to determine if suppression pattern is consistent or method artifact.

3. **Safety Training Gap Quantification**: Analyze distribution of informal, misspelled, and code-mixed text in publicly available safety alignment datasets. Quantify representation gap between pre-training data characteristics and safety training data.