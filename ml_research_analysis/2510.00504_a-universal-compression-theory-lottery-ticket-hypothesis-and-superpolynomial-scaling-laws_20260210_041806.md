---
ver: rpa2
title: 'A universal compression theory: Lottery ticket hypothesis and superpolynomial
  scaling laws'
arxiv_id: '2510.00504'
source_url: https://arxiv.org/abs/2510.00504
tags:
- function
- error
- compression
- symmetric
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that any permutation-symmetric function can be
  compressed from $d$ objects to $\operatorname{polylog}(d)$ objects with vanishing
  error by matching tensor moments. The key insight is that permutation symmetry allows
  decomposition into a composition of low-dimensional objects, and when $d$ is large,
  redundancy in dense regions enables compression.
---

# A universal compression theory: Lottery ticket hypothesis and superpolynomial scaling laws

## Quick Facts
- arXiv ID: 2510.00504
- Source URL: https://arxiv.org/abs/2510.00504
- Authors: Hong-Yi Wang; Di Luo; Tomaso Poggio; Isaac L. Chuang; Liu Ziyin
- Reference count: 36
- Primary result: Proves permutation-symmetric functions can be compressed from d objects to polylog(d) objects with vanishing error via tensor moment matching

## Executive Summary
This paper establishes a universal compression theory for permutation-symmetric functions, proving that any such function from d objects to m objects can be compressed to polylog(d) objects while maintaining vanishing error. The compression relies on moment matching of tensors and exploits the inherent redundancy in dense regions when d is large. This theoretical framework unifies two major implications: a dynamical lottery ticket hypothesis showing that neural network layers can be compressed to polylogarithmic width while preserving exact training dynamics, and a fundamental improvement in neural scaling laws from power-law to stretched-exponential behavior.

The key insight is that permutation symmetry enables decomposition into compositions of low-dimensional objects, making compression possible through redundancy exploitation. Numerical experiments validate both the effectiveness of the compression technique in reducing test loss while preserving dynamics, and the scaling law improvements that emerge when applying this compression to neural networks.

## Method Summary
The core method involves matching tensor moments to compress permutation-symmetric functions from d dimensions to polylog(d) dimensions. The approach decomposes any permutation-symmetric function into a composition of low-dimensional objects, leveraging the redundancy that emerges in dense regions when d is large. For neural networks, this translates to finding subnetworks that preserve both the functional output and the training dynamics of the original network. The compression algorithm identifies these lottery ticket subnetworks through moment matching, where the error vanishes as d grows. The technique applies to any neural network layer and can be extended to entire architectures through layer-wise compression.

## Key Results
- Proves any permutation-symmetric function can be compressed from d to polylog(d) objects with vanishing error via tensor moment matching
- Establishes dynamical lottery ticket hypothesis: neural network layers can be compressed to polylogarithmic width while preserving training dynamics exactly
- Demonstrates improvement in neural scaling laws from power-law L ~ d^(-α) to stretched-exponential L ~ exp(-α' √[m]{d})
- Validates compression effectiveness through numerical experiments showing reduced test loss and preserved dynamics
- Shows doubling of scaling exponent when compressing to ⌈16√d⌉ objects in experimental settings

## Why This Works (Mechanism)
The compression works by exploiting permutation symmetry to decompose high-dimensional functions into compositions of lower-dimensional objects. When d is large, dense regions of the function space contain redundant information that can be captured by polylog(d) objects. Moment matching ensures that the compressed representation preserves all statistical properties of the original function, leading to vanishing approximation error. For neural networks, this translates to preserving both the functional mapping and the training dynamics because the moments capture the essential statistical structure needed for gradient-based optimization.

## Foundational Learning
- **Tensor moment matching**: Why needed - to ensure the compressed function preserves all statistical properties of the original; Quick check - verify that all moments up to a certain order are preserved within tolerance
- **Permutation symmetry decomposition**: Why needed - enables reduction from d to polylog(d) dimensions; Quick check - confirm that the decomposition maintains the symmetric property
- **Dynamical system preservation**: Why needed - ensures training trajectories remain unchanged; Quick check - compare loss curves and gradient statistics between original and compressed networks
- **Scaling law analysis**: Why needed - to understand how compression affects model performance as size increases; Quick check - plot scaling curves with and without compression
- **Lottery ticket hypothesis extension**: Why needed - generalizes sparse subnetworks to preserve dynamics; Quick check - verify that compressed networks train identically to originals
- **High-dimensional redundancy**: Why needed - explains why compression is possible when d is large; Quick check - measure information content in different dimensional projections

## Architecture Onboarding

**Component map**: Original network -> Moment matching compression -> Compressed subnetwork -> Preserved dynamics

**Critical path**: Input data -> Forward pass through original network -> Moment extraction -> Subnetwork identification -> Forward pass through compressed network -> Output with preserved dynamics

**Design tradeoffs**: 
- Compression ratio vs. approximation error: Higher compression yields smaller models but may increase moment matching error
- Moment order vs. computational cost: Higher-order moments capture more information but require more computation
- Exact dynamics preservation vs. generalization: Preserving dynamics exactly may not always optimize test performance
- Layer-wise vs. end-to-end compression: Layer-wise is more modular but may miss global redundancies

**Failure signatures**: 
- Training dynamics diverge from original network (loss curves differ)
- Moment matching error does not decrease with increasing d
- Generalization performance degrades despite preserved dynamics
- Compression ratio insufficient to achieve polylog(d) scaling

**3 first experiments**:
1. Apply moment matching compression to a simple 2-layer MLP on synthetic permutation-symmetric data and verify polylog(d) scaling
2. Compress a convolutional layer while preserving training dynamics on CIFAR-10 and compare loss trajectories
3. Test scaling law improvements by training networks with different compression ratios on ImageNet and measuring the scaling exponent

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Practical implementation details for high-dimensional moment matching remain unspecified
- Empirical validation across diverse function classes beyond explicitly tested cases is needed
- Relationship between moment matching error and generalization performance lacks rigorous establishment
- Claim of exact dynamics preservation requires clearer scope definition for different architectures
- Experimental evidence for scaling law improvements is limited to specific compression ratios

## Confidence

**High confidence**: Theoretical framework of moment matching for permutation-symmetric functions is sound and mathematically rigorous

**Medium confidence**: Practical applicability of compression technique across diverse neural architectures shows promise but requires broader validation

**Low confidence**: Universal applicability of stretched-exponential scaling law improvement needs more extensive empirical verification

## Next Checks

1. Implement the moment matching compression algorithm for a range of non-trivial permutation-symmetric functions and measure the trade-off between compression ratio and approximation error across multiple dimensions

2. Conduct systematic experiments comparing training dynamics (loss trajectories, gradient norms, learning rate schedules) between original and compressed networks across different architecture families

3. Test the scaling law improvements on multiple benchmark tasks (image classification, language modeling, reinforcement learning) with varying compression ratios to establish the robustness of the exp(-α' √[m]{d}) scaling relationship