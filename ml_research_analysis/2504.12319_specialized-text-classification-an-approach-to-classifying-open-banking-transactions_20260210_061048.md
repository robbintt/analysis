---
ver: rpa2
title: 'Specialized text classification: an approach to classifying Open Banking transactions'
arxiv_id: '2504.12319'
source_url: https://arxiv.org/abs/2504.12319
tags:
- transaction
- data
- classification
- banking
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study developed a specialized text classification system for
  categorizing Open Banking transactions in the French context. The approach involved
  data collection, preprocessing (cleaning, human name anonymization, similarity filtering),
  rule-based labeling, and machine learning modeling using word n-grams or Word2Vec
  features with classifiers like Random Forest and Linear SVM.
---

# Specialized text classification: an approach to classifying Open Banking transactions

## Quick Facts
- arXiv ID: 2504.12319
- Source URL: https://arxiv.org/abs/2504.12319
- Reference count: 6
- Primary result: 95% weighted precision, recall, and F1 on French banking transaction classification across 84 categories

## Executive Summary
This paper presents a specialized text classification system for categorizing Open Banking transactions in the French context. The approach combines domain-specific preprocessing (cleaning, human name anonymization, similarity filtering) with machine learning modeling using word n-grams or Word2Vec features. The system achieves 95% weighted precision, recall, and F1 scores on a dataset of 203,865 French banking transactions across 84 categories. The results demonstrate the effectiveness of tailored preprocessing and language-specific modeling for high-accuracy transaction classification in Open Banking environments.

## Method Summary
The system processes French banking transaction descriptions through a multi-stage pipeline: text cleaning removes metadata (dates, account numbers, currencies, locations), human name anonymization using a French name dictionary, and TF-IDF-based similarity filtering to reduce near-duplicates. Transactions are labeled using rule-based keyword matching with inclusive/exclusive terms validated manually. For modeling, the system uses either word n-grams with TF-IDF vectorization and Linear SVM, or Word2Vec embeddings with PCA dimensionality reduction and Random Forest classifier. The multi-class classification approach handles 84 categories with weighted metrics to address severe class imbalance.

## Key Results
- Weighted precision, recall, and F1 scores reach 95% using Word2Vec + Random Forest
- Linear SVM with word n-grams achieves 94% F1 using 80% of training data
- Human name anonymization affects over 10% of transactions and improves classification accuracy

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific preprocessing removes noise while preserving merchant/category signals. The pipeline strips dates, account numbers, currencies, locations, and human names (10%+ of transactions contain names), reducing vocabulary sparsity. TF-IDF-based similarity filtering then identifies near-duplicates efficiently via matrix multiplication instead of pairwise distance calculations. Core assumption: Transaction semantics are primarily carried by merchant names and category keywords, not by metadata like dates or card numbers. Break condition: If merchant names are consistently anonymized or obfuscated in new datasets, preprocessing may over-prune signal.

### Mechanism 2
Word2Vec embeddings capture semantic relationships that improve classification over pure n-gram features. Word2Vec vectors (100-300 dimensions) trained on the transaction corpus, followed by PCA to 300 components retaining 98% variance, enable Random Forest to achieve 95% F1 versus 94% for Linear SVM with n-grams. Core assumption: The training corpus is sufficiently large to learn meaningful embeddings for the domain vocabulary. Break condition: Rare categories with few training examples may have poorly learned embeddings, degrading performance.

### Mechanism 3
Multi-class classification scales better than pairwise SVM ensembles for large category sets. A single classifier handles 84 categories simultaneously, avoiding the combinatorial explosion of pairwise approaches (which would require 3,486 binary classifiers). Core assumption: Categories are sufficiently separable in feature space without pairwise refinement. Break condition: For highly overlapping categories (e.g., "FAST FOODS" vs. "RESTAURANT"), pairwise approaches may still outperform.

## Foundational Learning

- Concept: **TF-IDF Vectorization**
  - Why needed here: Converts text to numerical features for ML models; also used in similarity filtering for near-duplicate detection.
  - Quick check question: Can you explain why TF-IDF downweights common words and upweights rare but informative terms?

- Concept: **Word2Vec Embeddings**
  - Why needed here: Alternative to n-grams; captures semantic similarity between words, potentially generalizing better to unseen merchant names.
  - Quick check question: What is the relationship between vector dimension, corpus size, and embedding quality?

- Concept: **Multi-class Classification with Class Imbalance**
  - Why needed here: 84 categories with heavy imbalance ("GROCERIES" has 11,726 samples; "ADVANCE SALARY" has 11); weighted metrics account for this.
  - Quick check question: Why would accuracy be misleading for imbalanced datasets, and how do weighted precision/recall/F1 address this?

## Architecture Onboarding

- Component map: Data Ingestion -> Preprocessing Pipeline -> Feature Extraction -> Classifier -> Evaluation
- Critical path: Preprocessing quality directly determines feature vocabulary size. Human name detection requires a French name dictionaryâ€”this is a language-specific dependency. Similarity filtering reduces training data from 5M to ~200K, enabling tractable model training.
- Design tradeoffs:
  - n-grams + Linear SVM: Faster training, interpretable features, 94% F1
  - Word2Vec + Random Forest: Better semantic generalization, 95% F1, but requires corpus training and PCA
  - Rule-based labeling: Scalable ground truth generation, but may propagate systematic errors
- Failure signatures:
  - Low recall on rare categories (e.g., "ADVANCE SALARY" at 77% precision): insufficient training samples
  - "GROCERIES" at 87% precision: reliance on merchant names that may be ambiguous
  - Categories with <100 samples show degraded performance
- First 3 experiments:
  1. **Baseline n-gram pipeline**: Implement TF-IDF n-grams (n=3) with Linear SVM on 80/20 train/test split; target ~94% F1
  2. **Ablate preprocessing**: Train without human name anonymization; expect degradation on transfer transactions
  3. **Word2Vec comparison**: Train Word2Vec on corpus, apply PCA to 300 components, test Random Forest; target 95% F1, compare per-category performance on rare classes

## Open Questions the Paper Calls Out

- **Open Question 1**: Can integrating external merchant databases significantly improve classification accuracy for merchant-dependent categories like "GROCERIES"? Basis: The authors state external data is a future solution for categories where classification relies heavily on merchant names. Unresolved because current system relies solely on internal transaction descriptions. Resolution would require experiment results comparing current model against hybrid model enriched with external merchant data.

- **Open Question 2**: What techniques can effectively mitigate the performance degradation caused by heavily imbalanced datasets in banking classification? Basis: The conclusion identifies "imbalanced data" as a specific challenge requiring further research. Unresolved because the paper notes worst performance occurs in categories with fewer entries, implying current handling is insufficient. Resolution would require comparative study of resampling methods or cost-sensitive learning on minority class F1-scores.

- **Open Question 3**: How does the system perform when applied to non-French Open Banking contexts or multilingual transaction descriptions? Basis: The paper explicitly states the system is "specifically tailored" to the French market and language. Unresolved because preprocessing and labeling are language-dependent, and it's unclear if pipeline is robust to other languages. Resolution would require evaluation of classification accuracy on translated datasets or raw non-French transaction logs.

- **Open Question 4**: How can the model adapt to "evolving transaction types" without requiring frequent, complete retraining? Basis: Section V lists "evolving transaction types" as a challenge for which further research is necessary. Unresolved because the paper doesn't address concept drift or how static rule-based labeling handles new merchant categories or changing description formats over time. Resolution would require analysis of model drift over longitudinal data or implementation of online learning techniques.

## Limitations

- Rule-based labeling introduces systematic bias that cannot be quantified without ground truth validation
- Human name dictionary and similarity filtering thresholds are language- and corpus-specific, making direct replication challenging
- Absence of the actual dataset prevents independent verification of preprocessing quality and its impact on downstream performance

## Confidence

- **High Confidence**: The multi-class classification architecture and weighted metric selection for imbalanced data (84 classes with severe imbalance)
- **Medium Confidence**: The preprocessing pipeline's effectiveness (mechanisms supported by stated percentages but not independently verified)
- **Low Confidence**: The superiority of Word2Vec over n-grams (based on single-dataset comparison without ablation studies)

## Next Checks

1. **Ablation study on preprocessing components**: Train models with individual preprocessing steps disabled (e.g., without human name anonymization, without similarity filtering) to quantify each component's contribution to the 95% F1 score.

2. **Cross-lingual transfer validation**: Apply the trained French model to a held-out set of transactions from another language or banking system to test the generalizability of the Word2Vec embeddings and rule-based labeling approach.

3. **Rare category performance analysis**: Conduct detailed error analysis on the lowest-support categories (<100 samples) to identify whether performance degradation stems from insufficient training data or fundamental feature separability issues.