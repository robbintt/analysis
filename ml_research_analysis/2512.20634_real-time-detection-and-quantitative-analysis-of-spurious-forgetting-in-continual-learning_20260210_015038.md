---
ver: rpa2
title: Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual
  Learning
arxiv_id: '2512.20634'
source_url: https://arxiv.org/abs/2512.20634
tags:
- alignment
- forgetting
- deep
- shallow
- spurious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in continual learning
  by distinguishing spurious forgetting (alignment disruption) from true forgetting
  (knowledge loss). The authors introduce the shallow versus deep alignment framework,
  which provides the first quantitative metrics to measure alignment depth across
  token positions, revealing that current approaches maintain only shallow alignment
  (3-5 tokens deep), making them vulnerable to forgetting.
---

# Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning

## Quick Facts
- arXiv ID: 2512.20634
- Source URL: https://arxiv.org/abs/2512.20634
- Reference count: 40
- One-line primary result: 86.2-90.6% identification accuracy for distinguishing spurious forgetting from true forgetting in continual learning

## Executive Summary
This paper addresses catastrophic forgetting in continual learning by distinguishing spurious forgetting (alignment disruption) from true forgetting (knowledge loss). The authors introduce the shallow versus deep alignment framework, which provides the first quantitative metrics to measure alignment depth across token positions, revealing that current approaches maintain only shallow alignment (3-5 tokens deep), making them vulnerable to forgetting. They develop a comprehensive framework with real-time detection during training, specialized analysis tools for alignment visualization, and adaptive mitigation strategies that automatically distinguish forgetting types. Experiments on Qwen2.5 models (3B to 32B parameters) demonstrate 86.2-90.6% identification accuracy and show that promoting deep alignment improves robustness against forgetting by 3.3-7.1% over baselines.

## Method Summary
The framework uses a shallow versus deep alignment distinction to detect and mitigate forgetting. Alignment depth D(θ,T) measures how many consecutive output tokens maintain high alignment scores (A_t ≥ τ_align=0.7). Reversibility score R(θ,T) combines alignment, representation similarity (CKA), and gradient norms to distinguish spurious forgetting (high R, low D) from true forgetting (low R). The detection system monitors D every 100 training steps and triggers selective repair (output layer only, 50-100 samples) for spurious cases or experience replay for true cases. Deep alignment training uses three strategies: token-position weighted loss, multi-position alignment regularization, and sequential alignment training. The framework achieves 86.2-90.6% identification accuracy on Qwen2.5 models with 8-12% training overhead.

## Key Results
- 86.2-90.6% identification accuracy for distinguishing spurious from true forgetting across Qwen2.5-3B to Qwen2.5-32B models
- Shallow alignment vulnerability (D≤5 tokens) is the primary cause of forgetting, with standard training maintaining only 3-5 tokens of alignment
- Promoting deep alignment (D>10) reduces forgetting rate from ~11-12% to ~2-3%
- Adaptive mitigation strategies achieve 3.3-7.1% improvement over fixed freezing baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard training produces shallow alignment (3-5 tokens deep) due to gradient flow bias toward early tokens in transformer architectures.
- Mechanism: Gradient magnitudes decay exponentially with token position (∥∇θLt∥ ∝ α^t, α≈0.6-0.7), causing optimization to prioritize early token alignment. When initial tokens become misaligned during new task training, the error cascades through autoregressive generation (Error(yt) ∝ Σ Error(yi)·α^(t-i)), producing apparent forgetting despite preserved representations.
- Core assumption: The observed gradient decay pattern generalizes across transformer architectures beyond Qwen2.5 models tested.
- Evidence anchors:
  - [abstract] "current task alignment approaches suffer from shallow alignment - maintained only over the first few output tokens (approximately 3-5)"
  - [section 3.2 & A.1] Theoretical derivation of gradient bias and formal error propagation model
  - [corpus] Limited direct validation; related work (arXiv:2512.07400) independently notes "gap between deep feature-space and shallow classifier-level forgetting" but does not validate the gradient mechanism
- Break condition: If later token positions receive comparable gradient magnitudes during training (e.g., through position-weighted losses), the mechanism predicts alignment depth should increase—confirmed in Group 5 experiments (D >12 achieved).

### Mechanism 2
- Claim: Alignment depth D(θ,T) distinguishes spurious from true forgetting and predicts recovery effort.
- Mechanism: Spurious forgetting is characterized by shallow alignment (D≤5), high representation similarity (CKA >0.85), and high reversibility scores (R>0.6), indicating intact knowledge with disrupted output alignment. True forgetting shows low representation similarity (<0.67) and low reversibility (R≤0.6), requiring extensive retraining.
- Core assumption: The thresholds (τ_align=0.7, τ_R=0.6, τ_deep=0.7) transfer across model scales and architectures.
- Evidence anchors:
  - [abstract] "86.2-90.6% identification accuracy" across Qwen2.5-3B to Qwen2.5-32B
  - [section 3.4, Table 3-5] Spurious cases recover in 9-13s with 50-100 samples; true cases require 115-168s
  - [corpus] Foundational work (arXiv:2501.13453) establishes reversibility concept; this paper extends it with quantitative depth metrics
- Break condition: If identification accuracy drops significantly on non-Qwen architectures, threshold recalibration is needed.

### Mechanism 3
- Claim: Promoting deep alignment during training (D>10) reduces forgetting rate from ~11-12% to ~2-3%.
- Mechanism: Three strategies counteract gradient bias: (1) Token-position weighted loss (w_t = 1 + α·t/T), (2) Multi-position alignment regularization (penalizing adjacent position score differences), (3) Sequential alignment training with curriculum sampling. These ensure later tokens receive sufficient gradient signal.
- Core assumption: The training overhead (+8-12%) is acceptable and the strategies work synergistically.
- Evidence anchors:
  - [abstract] "promoting deep alignment improves robustness against forgetting by 3.3-7.1% over baselines"
  - [Table 6] Alignment depth increases from D≤3 (standard) to D>12 (deep training); forgetting rate drops from 11-12.5% to 2.2-3.1%
  - [corpus] No independent validation of these specific training strategies found
- Break condition: Ablation study (Table 7) shows removing alignment metric causes -3.2% accuracy drop, confirming its foundational role.

## Foundational Learning

- Concept: Autoregressive token generation dependencies
  - Why needed here: The entire shallow alignment vulnerability stems from how early token misalignment cascades through sequential generation. Without this, the error propagation formula (Section A.2) is opaque.
  - Quick check question: Can you explain why misalignment at token position 3 affects generation at position 10 in an autoregressive model?

- Concept: Representation similarity metrics (CKA, cosine similarity)
  - Why needed here: The framework uses CKA to measure whether internal representations remain intact (distinguishing spurious from true forgetting). Section 3.4 and A.4 assume familiarity.
  - Quick check question: Given two sets of hidden states H1 and H2 from the same model before/after training, how would you compute whether representations changed?

- Concept: Catastrophic forgetting vs. task interference
  - Why needed here: The paper distinguishes performance degradation from actual knowledge loss. Understanding that forgetting can be "reversible" is essential for why alignment matters.
  - Quick check question: Why might a model score 60% on a task after learning a new task, even if its internal representations for that task haven't changed?

## Architecture Onboarding

- Component map:
  - Alignment Depth Monitor -> Reversibility Analyzer -> Integrated Detector -> Mitigation Module
  - Analysis Tools (alignment heatmaps, dynamic tracker) provide visualization and monitoring

- Critical path: Start with baseline training → monitor D(θ,T) every 100 steps → if S>0.6 and R>0.6 and D≤5, trigger selective repair; else if S>0.6 and R≤0.6, trigger experience replay. Total overhead: ~12% (Table 13).

- Design tradeoffs:
  - Monitoring frequency (50-200 steps): 100 steps optimal (Table 14); 50 steps adds ~1% accuracy for +2% overhead
  - Threshold sensitivity: τ_align=0.7 optimal (0.6→74.2% ACC, 0.7→76.4% ACC, 0.9→74.5% ACC)
  - Fixed vs. adaptive freezing: Adaptive achieves D>10; fixed maintains D≤3 (Table 11)

- Failure signatures:
  - False positives (3.2%): Model flagged for spurious forgetting but actually true forgetting → wasted selective repair attempt (fails to recover)
  - False negatives (4.1%): Spurious forgetting missed → unnecessary experience replay overhead
  - Threshold mismatch on new architectures: If identification accuracy drops below 80%, recalibrate τ_align, τ_R

- First 3 experiments:
  1. **Baseline control** (Group 1): Run standard sequential training on CLINC-150 with Qwen2.5-3B; measure natural forgetting rate (~11-12%). Establishes D≤3 reference.
  2. **Spurious forgetting validation** (Group 2): Freeze bottom 30% layers, train on Task 1 then Task 2; verify high R (>0.6), low D (≤3), fast recovery (<15s). Confirms detection pipeline.
  3. **Deep alignment training** (Group 5): Apply token-position weighted loss + regularization on same setup; verify D increases to >12, forgetting rate drops to <4%. Validates core intervention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alignment depth be accurately estimated using synthetic data or model introspection to eliminate the need for accessing original task data?
- Basis in paper: [explicit] Appendix A.5.6 lists the data requirement for alignment depth measurement as a limitation, noting privacy or storage constraints may prevent access.
- Why unresolved: The current methodology relies on computing alignment scores $A(\theta, T)$ using datasets $D_T$, which are unavailable in data-free scenarios.
- What evidence would resolve it: A proposed data-free estimation method achieving identification accuracy comparable to the current 86.2-90.6% baseline.

### Open Question 2
- Question: Does the shallow versus deep alignment framework effectively characterize forgetting in long-sequence generation tasks (e.g., >100 tokens)?
- Basis in paper: [explicit] Appendix A.5.6 states the analysis focuses on task-specific outputs (10-20 tokens), but longer sequences may exhibit different alignment dynamics.
- Why unresolved: It is unclear if the error propagation formula (Eq. 6) and gradient decay factors ($\alpha \approx 0.6-0.7$) derived for short sequences hold for extended generation.
- What evidence would resolve it: Experimental results on long-form generation datasets showing consistent alignment depth metrics and recovery behaviors.

### Open Question 3
- Question: Can adaptive threshold selection methods be developed to automatically adjust $\tau_S$, $\tau_R$, and $\tau_{align}$ for different model architectures?
- Basis in paper: [explicit] Appendix A.5.6 notes that optimal thresholds may vary by architecture and suggests future work should develop methods to automate this selection.
- Why unresolved: Current thresholds are fixed via cross-validation on specific Qwen models; their transferability to other architectures without manual tuning is unproven.
- What evidence would resolve it: A dynamic threshold algorithm that maintains high identification accuracy across diverse, unseen model architectures without manual intervention.

## Limitations
- Model Architecture Generalization: Framework validated exclusively on Qwen2.5 models; gradient bias mechanism and thresholds may not transfer to other architectures.
- Dataset Bias: Experiments limited to intent and topic classification; performance on generation, reasoning, or multimodal tasks untested.
- Computational Overhead: 8-12% training overhead may be prohibitive for large-scale or resource-constrained deployments.
- Threshold Sensitivity: Identification accuracy (86.2-90.6%) sensitive to threshold calibration; false positives/negatives could accumulate.

## Confidence

- **High Confidence**: The shallow versus deep alignment framework provides a valid quantitative distinction between spurious and true forgetting. Experimental results showing 86.2-90.6% identification accuracy and correlation between alignment depth D and forgetting recovery effort are well-supported.
- **Medium Confidence**: Three training strategies for promoting deep alignment are effective within tested domain, but generalizability and optimal parameterization across architectures require further validation.
- **Low Confidence**: Specific gradient decay mechanism (∥∇θLt∥ ∝ α^t, α≈0.6-0.7) as root cause of shallow alignment is inferred from limited data and theoretical derivation; independent experimental validation needed.

## Next Checks

1. **Cross-Architecture Validation**: Apply framework to non-Qwen models (e.g., LLaMA, Mistral, BERT variants) on CLINC-150 and 20 Newsgroups. Measure if identification accuracy remains above 80% and whether threshold values require recalibration.

2. **Generation Task Evaluation**: Test framework on text generation tasks (e.g., summarization, story continuation) where alignment at later token positions is critical. Measure if promoting deep alignment (D>10) improves generation quality metrics in addition to classification accuracy.

3. **Ablation of Gradient Bias Mechanism**: Conduct controlled experiments where gradient magnitudes are artificially equalized across token positions (e.g., via gradient clipping or position-weighted optimizers). If shallow alignment vulnerability disappears under equalized gradients, this provides strong causal evidence for the proposed mechanism.