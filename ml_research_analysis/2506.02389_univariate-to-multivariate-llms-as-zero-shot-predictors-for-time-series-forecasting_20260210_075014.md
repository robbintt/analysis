---
ver: rpa2
title: 'Univariate to Multivariate: LLMs as Zero-Shot Predictors for Time-Series Forecasting'
arxiv_id: '2506.02389'
source_url: https://arxiv.org/abs/2506.02389
tags:
- prediction
- llmpred
- values
- multivariate
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LLMPred is a framework for time-series forecasting using large
  language models (LLMs) by converting time-series data into text prompts. It addresses
  the challenges of handling complex, noisy, and multivariate time-series data by
  introducing two key techniques: frequency decomposition of univariate sequences
  into low- and high-frequency components, and a lightweight prompt-processing strategy
  to extend univariate prediction to multivariate scenarios.'
---

# Univariate to Multivariate: LLMs as Zero-Shot Predictors for Time-Series Forecasting

## Quick Facts
- **arXiv ID:** 2506.02389
- **Source URL:** https://arxiv.org/abs/2506.02389
- **Authors:** Chamara Madarasingha; Nasrin Sohrabi; Zahir Tari
- **Reference count:** 40
- **Primary Result:** LLMPred framework achieves competitive or superior performance compared to SOTA baselines for zero-shot time-series forecasting using smaller LLMs

## Executive Summary
This paper introduces LLMPred, a novel framework for time-series forecasting that leverages large language models (LLMs) in a zero-shot manner by converting time-series data into text prompts. The framework addresses the challenge of handling complex, noisy, and multivariate time-series data through two key techniques: frequency decomposition of univariate sequences into low- and high-frequency components, and a lightweight prompt-processing strategy to extend univariate prediction to multivariate scenarios. Experiments demonstrate that LLMPred achieves competitive or superior performance compared to state-of-the-art baselines, with significant improvements in MSE and MAE metrics, particularly when moving from univariate to multivariate prediction.

## Method Summary
LLMPred converts time-series forecasting into a text generation problem by transforming numerical sequences into text prompts for LLMs. The method employs Butterworth filter-based frequency decomposition to split data into high- and low-frequency components, which are then max-normalized and shifted to positive ranges before being converted to text. The LLM generates predictions based on historical sequences, which are then refined through post-processing involving a 5-layer MLP trained to correct low-frequency predictions and Gaussian transformation to align high-frequency predictions with observed distributions. This approach enables zero-shot forecasting without model fine-tuning while handling both univariate and multivariate scenarios.

## Key Results
- 26.8% reduction in Mean Squared Error (MSE) for univariate forecasting compared to baselines
- 17.4% improvement when extending from univariate to multivariate prediction
- Competitive or superior performance compared to state-of-the-art forecasting methods
- Ablation studies confirm the importance of frequency decomposition and post-processing components

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to leverage LLMs' strong pattern recognition capabilities in a zero-shot setting by converting numerical time-series data into textual format. The frequency decomposition technique allows the model to separately handle different temporal patterns, with low-frequency components capturing long-term trends and high-frequency components capturing short-term fluctuations. The post-processing steps refine the LLM's predictions by correcting systematic biases and aligning the output distributions with observed data patterns, effectively compensating for LLMs' limitations in precise numerical prediction.

## Foundational Learning
- **Frequency Decomposition**: Splitting time-series into low- and high-frequency components using Butterworth filters is needed to separately capture different temporal patterns; quick check: verify decomposition preserves signal energy.
- **Zero-Shot Learning**: Using LLMs without fine-tuning by leveraging their pre-trained knowledge through prompt engineering; quick check: test with different prompt templates to optimize performance.
- **Text-to-Time Conversion**: Converting numerical sequences to text format requires careful normalization and formatting to maintain numerical relationships; quick check: validate that round-trip conversion preserves relative magnitudes.

## Architecture Onboarding

**Component Map:** Data → Frequency Decomposition → Text Conversion → LLM Inference → Post-processing (MLP + Gaussian Transform) → Forecast

**Critical Path:** The most critical sequence is frequency decomposition followed by proper text formatting, as errors in either step cascade through the entire pipeline and prevent accurate numerical prediction.

**Design Tradeoffs:** The framework trades the precision of traditional numerical models for the generalization capabilities of LLMs, accepting that post-processing correction is necessary to achieve competitive accuracy.

**Failure Signatures:** Common failures include tokenization errors where numerical values are split incorrectly, context length overflow in multivariate scenarios, and improper frequency decomposition due to sampling rate mismatches.

**3 First Experiments:**
1. Test text conversion with simple numerical sequences to verify tokenization and round-trip conversion accuracy
2. Implement frequency decomposition on sample data to validate the Butterworth filter parameters and sampling rate interpretation
3. Run LLM inference with temperature=1.0, top_p=0.9, do_sample=True on simple patterns to establish baseline prediction capabilities

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- **Sampling Rate Ambiguity:** The 100Hz sampling rate specified for Butterworth decomposition is incompatible with hourly datasets without clarification on resampling requirements
- **MLP Training Protocol:** Unclear whether the 5-layer MLP is trained per-time-series, per-dataset, or per-window, which significantly impacts reproducibility
- **Tokenization Sensitivity:** Numerical sequence formatting is critical, and exact tokenization rules are not fully specified, making performance sensitive to implementation details

## Confidence

**High Confidence:** The overall framework architecture and its validation through ablation studies are clearly described and well-supported.

**Medium Confidence:** Implementation details for text conversion and prompt construction are specific, but the critical sampling rate issue introduces uncertainty.

**Low Confidence:** MLP training protocol and tokenization strategy lack sufficient detail for exact reproduction.

## Next Checks
1. Verify the sampling rate interpretation by testing both raw hourly data and resampled 100Hz versions on a small dataset to observe impact on decomposition quality
2. Implement the exact MLP architecture (Figure 7) and determine training protocol (per-series vs. per-dataset) through controlled experiments
3. Test tokenization sensitivity by varying whitespace patterns and decimal precision to establish minimum requirements for numerical prediction accuracy