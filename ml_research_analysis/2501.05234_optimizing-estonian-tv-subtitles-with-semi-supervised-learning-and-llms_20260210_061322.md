---
ver: rpa2
title: Optimizing Estonian TV Subtitles with Semi-supervised Learning and LLMs
arxiv_id: '2501.05234'
source_url: https://arxiv.org/abs/2501.05234
tags:
- subtitles
- subtitle
- data
- dataset
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating high-quality,
  same-language subtitles for Estonian TV content. The authors fine-tune the Whisper
  model on human-generated Estonian subtitles and enhance it with iterative pseudo-labeling
  and large language model (LLM) based post-editing.
---

# Optimizing Estonian TV Subtitles with Semi-supervised Learning and LLMs

## Quick Facts
- arXiv ID: 2501.05234
- Source URL: https://arxiv.org/abs/2501.05234
- Reference count: 10
- Primary result: Fine-tuning Whisper on Estonian TV subtitles, enhanced with iterative pseudo-labeling and GPT-4o post-editing, achieves subtitle quality close to human standard.

## Executive Summary
This paper presents an approach to generating high-quality, same-language subtitles for Estonian TV content by fine-tuning the Whisper model on human-generated Estonian subtitles. The authors enhance the model with iterative pseudo-labeling using unlabeled audio data and LLM-based post-editing at test time. Their experiments demonstrate notable subtitle quality improvement through pseudo-labeling with an unlabeled dataset. They find that applying LLM-based editing at test time enhances subtitle accuracy, while its use during training does not yield further gains. The approach holds promise for creating subtitle quality close to human standard and could be extended to real-time applications.

## Method Summary
The authors fine-tune Whisper large-v3 on 778 hours of human-generated Estonian TV subtitles, then apply iterative pseudo-labeling with 3,923 hours of unlabeled audio data. The process involves generating pseudo-labels for unlabeled samples, combining them with original supervised data using a weighted loss (λ=0.35), and retraining the model. This is repeated for two iterations. At inference, GPT-4o is used to post-edit generated subtitles in 40-block chunks. The approach leverages semi-supervised learning to improve subtitle quality and LLM-based editing to correct residual errors.

## Key Results
- Iterative pseudo-labeling with unlabeled audio data improves subtitle generation quality across all test metrics.
- LLM-based post-editing at test time enhances subtitle accuracy (SubER 34.2) compared to no post-editing (35.1), but using LLMs during training does not yield further gains.
- Fine-tuning on subtitle data (not verbatim transcripts) yields notably lower SubER values, demonstrating the importance of learning subtitling conventions.

## Why This Works (Mechanism)

### Mechanism 1
Iterative pseudo-labeling with unlabeled audio data improves subtitle generation quality. A model trained on supervised data generates pseudo-labels for unlabeled samples; these pseudo-labels are combined with original supervised data (with speed perturbation) to retrain the model. The process repeats, with each iteration refining the model's ability to produce subtitle-style output. Core assumption: Pseudo-labels, despite containing errors, provide sufficient signal about subtitle style when combined with clean supervised data at an appropriate weighting (λ = 0.35). Evidence anchors: [abstract] "Our experiments demonstrate notable subtitle quality improvement through pseudo-labeling with an unlabeled dataset." [section 4.6] "findings suggest that iterative semi-supervised learning enhances subtitle quality, as evidenced by improvements across all test metrics."

### Mechanism 2
LLM-based post-editing at test time improves subtitle accuracy, but LLM correction of pseudo-labels during training does not. GPT-4o receives generated subtitle chunks (40 blocks) with instructions to fix errors while preserving timestamps. At test time, this corrects residual ASR/subtitling errors. During training, LLM-corrected pseudo-labels do not improve—and may introduce—noise that misaligns with the acoustic signal. Core assumption: Test-time LLM corrections address systematic errors without introducing structural changes. Training-time corrections may create "textual hallucinations" that decouple from audio features. Evidence anchors: [abstract] "applying LLM-based editing at test time enhances subtitle accuracy, while its use during training does not yield further gains." [section 4.6] "contrary to findings in (Xi et al., 2024), applying LLM-based post-editing to pseudo-labeled subtitles in the unsupervised dataset does not yield further improvements."

### Mechanism 3
Fine-tuning on human-generated subtitles (not verbatim transcripts) teaches the model subtitling conventions—condensation, rephrasing, readability. Subtitles differ from verbatim speech; they omit fillers, restructure sentences, and prioritize readability. Training on subtitle-audio pairs directly learns this mapping rather than learning verbatim transcription followed by a separate compression step. Evidence anchors: [section 1] "Subtitles represent typically a condensed version of the speech... may omit filler words, repetitions, and non-verbal sounds." [section 4.6] "fine-tuning on subtitle data yields notably lower SubER values compared to fine-tuning on verbatim transcripts."

## Foundational Learning

- **Whisper architecture (encoder-decoder Transformer for speech)**: Why needed here: The entire approach builds on fine-tuning Whisper large-v3; understanding its tokenization, language handling, and pre-training scope is prerequisite. Quick check question: Can you explain why Whisper's multilingual pre-training helps low-resource languages like Estonian, but task-specific fine-tuning is still required?

- **Semi-supervised learning / Pseudo-labeling**: Why needed here: Core technique for leveraging 3,923 hours of unlabeled audio; understanding weighted loss (L_total) and iteration strategy is essential. Quick check question: Why might pseudo-labels improve a model even though they contain errors from the generating model?

- **Subtitle evaluation metrics (SubER, BLEURT)**: Why needed here: WER is inadequate for subtitles due to intentional rephrasing; SubER handles timing/segmentation, BLEURT captures semantic equivalence. Quick check question: Why would BLEURT scores remain high for both verbatim transcripts and condensed subtitles, and what does this imply about metric limitations?

## Architecture Onboarding

- **Component map**: Audio Input → VAD (Silero) → Whisper large-v3 (fine-tuned) → Raw Subtitles → [Test-time only] → LLM (GPT-4o) → Corrected Subtitles

- **Critical path**:
  1. Supervised fine-tuning on 778 hours of human-subtitled Estonian TV audio
  2. Generate pseudo-labels for 3,923 hours of unlabeled audio
  3. Re-train combining supervised + pseudo-labeled data with weighted loss
  4. Repeat iteration 2-3 one more time
  5. At inference: generate subtitles, then apply GPT-4o post-editing in 40-block chunks

- **Design tradeoffs**:
  - LLM selection: Only GPT-4o improved SubER; open models (Llama 3.1 405B, Qwen 2.5 72B) did not (Table 1).
  - Chunk size: 40 subtitle blocks balances parallelization against context window limits.
  - Training-time LLM: Explicitly rejected—no benefit, adds cost and complexity.

- **Failure signatures**:
  - LLM timestamp corruption: LLMs "struggle to output the exact timestamps and block numbers correctly" → requires verification and retry logic.
  - Domain mismatch: News content in unsupervised data was not in supervised data; monitor for genre-specific degradation.
  - Over-correction: LLM may "fix" correct subtitles; threshold revert logic mitigates this.

- **First 3 experiments**:
  1. Baseline verification: Fine-tune Whisper on supervised subtitle data only; measure SubER, t-BLEURT, AS-BLEURT against native Whisper and verbatim-fine-tuned Whisper.
  2. Single pseudo-labeling iteration: Add pseudo-labeled data with λ=0.35 weighting; compare against baseline to validate IPL signal.
  3. Test-time LLM ablation: Apply GPT-4o post-editing to baseline output vs. IPL output; isolate LLM contribution from IPL contribution.

## Open Questions the Paper Calls Out

### Open Question 1
Why does LLM-based post-editing of pseudo-labels during training fail to improve subtitle quality, despite success in other ASR domains? Basis in paper: [explicit] The authors note that "contrary to findings in (Xi et al., 2024), applying LLM-based post-editing to pseudo-labeled subtitles in the unsupervised dataset does not yield further improvements." Why unresolved: The paper documents the negative result but does not investigate whether the cause lies in the language (Estonian vs. Mandarin-English), the subtitle vs. verbatim transcription task, or the compression/condensation inherent to subtitling. What evidence would resolve it: Ablation studies applying the same training-time LLM correction to verbatim ASR tasks in Estonian, or analyzing the types of errors LLMs introduce when editing condensed pseudo-labels versus verbatim transcripts.

### Open Question 2
Can the proposed approach be adapted for real-time subtitle generation without sacrificing quality? Basis in paper: [explicit] The conclusion states: "Future work will focus on adapting our approach to real-time scenarios." Why unresolved: The current method involves iterative pseudo-labeling (multiple training passes), LLM post-editing at test time, and chunk-based processing—all of which introduce latency incompatible with live broadcasting. What evidence would resolve it: Latency measurements and quality comparisons between streaming vs. batch processing variants, potentially using streaming Whisper variants and smaller, faster LLMs for post-editing.

### Open Question 3
Would human evaluation confirm the improvements indicated by automatic metrics (SubER, BLEURT)? Basis in paper: [explicit] The authors acknowledge: "Although a formal human evaluation of the generated subtitles was not conducted, the authors' subjective assessment suggests that minimal manual post-editing would be required." Why unresolved: Automatic metrics may not fully capture readability, timing appropriateness, or viewer comprehension. BLEURT's surprisingly high scores for verbatim transcripts suggest semantic similarity may not align with subtitle-specific quality judgments. What evidence would resolve it: A structured human evaluation study measuring comprehension, reading effort, and preference ratings comparing model outputs to human-authored subtitles.

## Limitations

- **Domain Specificity**: The approach is narrowly validated on Estonian TV content, limiting generalizability to other languages or domains.
- **Metric Limitations**: While SubER and BLEURT are designed for subtitle evaluation, they may not fully capture subjective aspects like readability or viewer comprehension.
- **Cost and Scalability**: The reliance on GPT-4o for post-editing introduces significant computational and financial costs, which may limit real-time or large-scale deployment.

## Confidence

- **High Confidence**: The core finding that iterative pseudo-labeling improves subtitle quality is well-supported by experimental results (Section 4.6).
- **Medium Confidence**: The claim that LLM post-editing at test time is more effective than during training is supported by empirical evidence but lacks theoretical justification.
- **Low Confidence**: The generalizability of the approach to other languages, domains, or real-world scenarios (e.g., live subtitling) is not established.

## Next Checks

1. **Cross-Lingual Validation**: Apply the approach to another low-resource language (e.g., Latvian or Lithuanian) to test generalizability. Compare the effectiveness of pseudo-labeling and LLM post-editing across languages.

2. **Real-Time Feasibility**: Implement a lightweight version of the pipeline (e.g., using smaller LLMs or reduced pseudo-labeling iterations) to evaluate its performance for real-time subtitling. Measure latency and subtitle quality trade-offs.

3. **User Study**: Conduct a user study to assess whether the improved SubER and BLEURT scores translate to better viewer comprehension and satisfaction. Compare the model's output with human-generated subtitles in a blind test.