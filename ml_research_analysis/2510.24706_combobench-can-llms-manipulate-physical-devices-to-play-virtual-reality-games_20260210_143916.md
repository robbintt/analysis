---
ver: rpa2
title: 'ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?'
arxiv_id: '2510.24706'
source_url: https://arxiv.org/abs/2510.24706
tags:
- performance
- llms
- interaction
- shot
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ComboBench, a benchmark for evaluating LLMs\u2019\
  \ ability to translate high-level semantic actions into precise VR device manipulation\
  \ sequences. Tested across 262 scenarios from four VR games, seven LLMs showed strong\
  \ task decomposition but struggled with procedural reasoning and motor action mapping."
---

# ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?

## Quick Facts
- **arXiv ID:** 2510.24706
- **Source URL:** https://arxiv.org/abs/2510.24706
- **Authors:** Shuqing Li; Jiayi Yan; Chenyu Niu; Jen-tse Huang; Yun Peng; Wenxuan Wang; Yepang Liu; Michael R. Lyu
- **Reference count:** 40
- **Primary result:** Current LLMs can decompose high-level VR tasks but fail at procedural reasoning and motor action mapping, with Gemini-1.5-Pro performing best among seven tested models.

## Executive Summary
This paper introduces ComboBench, a benchmark for evaluating LLMs' ability to translate high-level semantic actions into precise VR device manipulation sequences. Tested across 262 scenarios from four VR games, seven LLMs showed strong task decomposition but struggled with procedural reasoning and motor action mapping. Gemini-1.5-Pro performed best overall, while SOP scores remained low, indicating sequencing challenges. Few-shot examples improved performance significantly, especially for procedural reasoning. Results reveal that current LLMs lack embodied understanding required for reliable VR interactions, highlighting the need for multimodal training and architectural innovations to bridge the gap between linguistic and physical reasoning in virtual environments.

## Method Summary
ComboBench evaluates whether LLMs can translate high-level semantic actions (e.g., "tame the horse") into fine-grained VR device manipulation sequences across four VR games. The benchmark uses 262 scenarios with ground truth annotations for device manipulations. Seven LLMs were tested in zero-shot to five-shot settings, with outputs evaluated using multiple metrics: Strict Step-by-Step Matching (SSM), Normalized Step Alignment Score (NSAS), Sequential Order Preservation (SOP), and Semantic Step Coverage (SSC). Step matching requires cosine similarity > 0.8387 using text-embedding-3-large. The evaluation focuses on four key capabilities: task decomposition, procedural reasoning, spatial reasoning, and motor action mapping.

## Key Results
- Seven LLMs achieved high task decomposition scores (7.8-8.5) but struggled with procedural reasoning and motor action mapping (0.5-4.5)
- Gemini-1.5-Pro achieved the highest overall performance with NSAS of 0.397, SOP of 0.297, and SSC of 0.786
- Few-shot examples significantly improved performance, with 3-shot prompting yielding 10-20x improvement in SOP scores
- Performance varied by game type, with discrete interaction paradigms (Vivecraft) showing higher scores than physics-based interactions (Half-Life: Alyx)

## Why This Works (Mechanism)

### Mechanism 1: Few-shot Examples for Temporal Sequencing
Providing few-shot examples significantly improves the temporal sequencing of device manipulations, even if it does not guarantee perfect step matching. In-context examples appear to activate latent procedural reasoning capabilities by demonstrating the expected level of action granularity and temporal dependencies (e.g., "till soil before planting"). The paper observes a 10-20x increase in Sequential Order Preservation (SOP) scores when moving from zero-shot to few-shot settings, suggesting that examples help models resolve ambiguities in *how* steps chain together. The improvement from zero-shot to 3-shot significantly exceeds that from 3-shot to 5-shot, implying this mechanism cannot bridge fundamental gaps in physical or spatial reasoning through prompting alone.

### Mechanism 2: Semantic Understanding Without Physical Grounding
LLMs succeed at high-level task decomposition but fail at motor action mapping due to a lack of grounded physical understanding. Current LLMs process language hierarchically, making them adept at breaking "tame the horse" into "approach," "equip saddle," etc. However, translating "equip saddle" into "press Y button while looking at inventory" requires a mapping between semantic concepts and specific hardware states (controller buttons, HMD orientation) that text-only training does not provide. The disparity between high decomposition scores (7.8-8.5) and low motor mapping scores (0.5-4.5) is causal: the model lacks the "embodied" link between the abstract action and the physical affordance.

### Mechanism 3: Interaction Paradigm Consistency
Performance is contingent on the consistency of the game's interaction paradigm; discrete, consistent actions are easier to model than physics-based nuances. Models perform better in environments like *Vivecraft* (Minecraft VR), where interactions are discrete and block-based, creating consistent linguistic-to-physical mappings. In contrast, games like *Half-Life: Alyx* require continuous, physics-based manipulation, leading to a degradation in performance as the model cannot predict the precise motor adjustments needed for nuanced interactions. The variance in SOP scores across games is primarily driven by the complexity of the interaction physics rather than just the vocabulary difficulty.

## Foundational Learning

### Embodied Reasoning / Grounding
**Why needed here:** To understand why LLMs fail to map "semantic actions" (text) to "device manipulations" (physical reality). The core challenge is that the model has no sensory experience of the VR controller or space.
**Quick check question:** Can the model distinguish between the linguistic concept of "pressing a button" and the physical prerequisite of "moving the controller to the correct spatial location first"?

### Sequential Order Preservation (SOP)
**Why needed here:** SOP is a critical metric in this paper. It measures not just *what* actions are taken, but *when*. Understanding this metric is key to diagnosing the "procedural reasoning" gap identified by the authors.
**Quick check question:** If a model outputs steps [A, B, C] but the ground truth is [A, C, B], how does the SOP metric penalize this compared to the NSAS metric?

### VR Interaction Paradigms (Discrete vs. Continuous)
**Why needed here:** The paper shows performance varies by game type. Recognizing the difference between a "discrete" action (placing a block) and a "continuous" one (swinging a pickaxe) explains why certain benchmarks (like *Vivecraft*) show inflated performance scores.
**Quick check question:** Why would a text-trained model perform significantly better on a "point-and-click" style VR interface than on a physics-based throwing mechanic?

## Architecture Onboarding

### Component Map
Input: High-level Semantic Action + Few-shot Context
  -> LLM Processor (e.g., Gemini-1.5-Pro, GPT-4o)
  -> Output: Fine-grained Device Manipulation Sequence
  -> Evaluator: ComboBench Metrics (SSM, SOP, NSAS)

### Critical Path
1. Select VR scenario and extract semantic action description
2. Inject action into LLM prompt (Zero-shot or Few-shot)
3. Parse LLM output into a structured manipulation sequence
4. Evaluate sequence against human-annotated ground truth using SOP and NSAS

### Design Tradeoffs
- **Metric Strictness:** Using Strict Step-by-Step Matching (SSM) is often too brittle (scores near 0%) compared to Normalized Step Alignment Score (NSAS) or SOP, which allow for partial credit. Engineering teams should use SOP for assessing utility and SSM for assessing safety/reliability.
- **Few-shot Cost:** 3-shot examples provide the bulk of the performance gain; 5-shot yields diminishing returns.

### Failure Signatures
- **High NSAS, Low SOP:** The model identifies *what* needs to happen (correct steps) but hallucinates the order (wrong sequence). This indicates a failure in temporal/causal reasoning.
- **"Motor Action Mapping" Hallucination:** The model invents non-existent buttons or incorrect controller hands (e.g., pressing a non-existent "Z" button). This indicates a lack of grounding in the specific hardware schema.

### First 3 Experiments
1. **Baseline capability test:** Run *Vivecraft* scenarios (discrete interactions) on a baseline LLM (e.g., GPT-3.5) to establish the "upper bound" of performance in a simple interaction paradigm.
2. **Sensitivity analysis:** Measure the delta in SOP scores when moving from Zero-shot to 3-shot prompting specifically for *Half-Life: Alyx* to quantify the limits of in-context learning for complex physics.
3. **Error taxonomy check:** Analyze failures in the "Motor Action Mapping" dimension to see if errors are due to unknown buttons (knowledge gap) or incorrect spatial directions (spatial reasoning gap).

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can architectural innovations that explicitly model temporal and causal dependencies improve Sequential Order Preservation (SOP) scores beyond the limits of few-shot prompting?
**Basis in paper:** The discussion notes that low SOP scores "suggest that improved performance may require architectural innovations that better capture temporal and causal reasoning."
**Why unresolved:** Few-shot prompting improves SOP but suffers from diminishing returns, failing to achieve human-level ordering (SOP < 0.3).
**What evidence would resolve it:** Testing models with specialized temporal reasoning modules on ComboBench to observe if SOP scores exceed the current 0.30 ceiling.

### Open Question 2
**Question:** Does incorporating multimodal inputs (spatial, visual, haptic) significantly improve performance on the "Motor Action Mapping" capability?
**Basis in paper:** The Conclusion states current LLMs lack embodied understanding and points to the need for "multimodal training approaches that incorporate spatial, visual, and haptic information."
**Why unresolved:** Text-only models struggle to translate abstract concepts to device manipulations, resulting in very low scores (0.5-4.5) in this specific capability.
**What evidence would resolve it:** A comparison of text-only versus multimodal models on ComboBench, specifically analyzing the Motor Action Mapping metric.

### Open Question 3
**Question:** Can training on synthetic VR interaction data effectively bridge the performance gap between LLMs and humans in spatial reasoning tasks?
**Basis in paper:** Section C.9 suggests "training on synthetic VR interaction data or through simulated embodiment might provide models with the experiential knowledge currently lacking."
**Why unresolved:** Humans currently outperform LLMs in spatial reasoning (8.3 vs. 7.5) and termination judgment due to grounded physical intuition.
**What evidence would resolve it:** Evaluating LLMs fine-tuned on synthetic VR datasets to see if their spatial reasoning scores converge with human baselines.

## Limitations
- **Dataset representativeness:** The ComboBench dataset draws from only four VR games, which may not capture the full diversity of interaction paradigms across VR.
- **Metric sensitivity:** The cosine similarity threshold of 0.8387 for step matching was derived from dataset-specific statistics and may not be universally applicable.
- **In-context learning limits:** The significant improvement from zero-shot to 3-shot prompting suggests strong in-context learning capability, but the marginal gains beyond 3 examples indicate fundamental architectural limitations.

## Confidence

- **High confidence:** Task decomposition capability assessment - consistent high scores (7.8-8.5) across models and games demonstrate robust semantic understanding of high-level goals.
- **Medium confidence:** Few-shot prompting effectiveness - while the 10-20x SOP improvement is well-documented, the mechanism (pattern recognition vs. knowledge acquisition) remains unclear.
- **Low confidence:** Cross-game generalization - performance variance across the four games is substantial, suggesting results may not transfer to other VR interaction paradigms.

## Next Checks

1. **Architecture ablation:** Test whether performance gaps in motor action mapping persist when using multimodal LLMs (vision-language models) that can process controller layouts and spatial relationships, compared to text-only models.

2. **Interaction paradigm stress test:** Create synthetic VR scenarios that progressively increase in interaction complexity (discrete → continuous → physics-based) to identify the precise failure threshold for current LLMs.

3. **Grounding verification:** Design experiments where semantic actions require implicit physical knowledge (e.g., "carefully pick up the fragile vase" vs. "grab the vase") to determine whether failures stem from missing embodied reasoning or simply insufficient training data.