---
ver: rpa2
title: Industry-Aligned Granular Topic Modeling
arxiv_id: '2601.11762'
source_url: https://arxiv.org/abs/2601.11762
tags:
- topic
- topics
- tide
- modeling
- granular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TIDE introduces a novel LLM-based framework for granular topic
  modeling that outperforms existing methods in generating detailed, business-relevant
  topics. The core approach uses clustering to break down documents into smaller semantic
  groups before generating specific topics, enabling finer granularity than holistic
  document analysis.
---

# Industry-Aligned Granular Topic Modeling

## Quick Facts
- **arXiv ID**: 2601.11762
- **Source URL**: https://arxiv.org/abs/2601.11762
- **Reference count**: 36
- **One-line primary result**: Novel LLM-based framework for granular topic modeling that outperforms existing methods in generating detailed, business-relevant topics.

## Executive Summary
TIDE introduces a novel LLM-based framework for granular topic modeling that generates detailed, business-relevant topics by clustering documents before LLM processing. The approach breaks documents into smaller semantic groups, enabling finer granularity than holistic document analysis. Extensive experiments on three public datasets and two real-world business datasets show TIDE consistently achieves superior performance across multiple evaluation metrics including P1, ARI, and NMI, while generating significantly more topics than baseline approaches.

## Method Summary
TIDE processes documents through an optional summarization step for long texts, applies K-means clustering on all-mpnet-base-v2 embeddings, then uses a single LLM call per cluster to both generate topic labels and assign documents. The framework supports refinement, topic hierarchy detection, and distillation via SetFit for cost-effective classification. Evaluation uses established clustering metrics (P1, ARI, NMI) for labeled data and LLM-as-judge metrics for unlabeled data.

## Key Results
- TIDE outperforms baselines on P1, ARI, and NMI metrics across Banking77, Bills, and Wiki datasets
- Generates significantly more topics than baseline approaches while maintaining high quality
- Summarization improves performance on long documents (Wiki) but not on short ones (Banking77)
- Distillation module achieves cost-effective inference with SetFit and fine-tuned mpnet

## Why This Works (Mechanism)

### Mechanism 1: Cluster-then-Generate for Granular Topic Discovery
Clustering documents before LLM topic generation produces finer-grained topics than holistic corpus analysis. K-means clustering groups semantically similar documents; the LLM then generates topics from each smaller cluster, surfacing fine-grained patterns that would be overshadowed at corpus scale.

### Mechanism 2: Business-Definition-Guided Summarization
Incorporating business definitions during summarization aligns extracted content with downstream topic relevance for long documents. The summarization module receives domain descriptions and topic definitions to guide LLM summaries, preserving both explicit content and implicit inferences relevant to target topics.

### Mechanism 3: Single-Pass Topic Assignment via Cluster Batching
Assigning topics within each cluster via one LLM call (O(N) complexity) is more resource-efficient than scoring all document-topic pairs (O(N×k)). Each cluster triggers one LLM invocation that both generates topic labels and assigns all cluster documents, eliminating pairwise scoring overhead.

## Foundational Learning

### Concept 1: K-means Clustering with Embedding Representations
**Why needed**: TIDE's granularity is controlled by cluster count (K); understanding how embedding quality affects cluster coherence is essential for tuning.
**Quick check**: Given a corpus with 10,000 documents and expected ~100 topics, what range of K would you test, and what metric would signal K is set too high?

### Concept 2: Harmonic Mean of Purity (P1), ARI, and NMI Metrics
**Why needed**: Paper evaluates topic alignment using these clustering metrics; interpreting them correctly is necessary to assess performance claims.
**Quick check**: Why does the paper use Harmonic Mean of Purity rather than raw Purity, and what does a high ARI but low NMI suggest about topic alignment?

### Concept 3: SetFit for Few-Shot Classification Distillation
**Why needed**: TIDE's distillation module uses SetFit for cost-effective inference when topic labels are scarce; understanding its contrastive learning approach explains when distillation will succeed.
**Quick check**: How does SetFit's sentence-transformer fine-tuning differ from standard classification head training, and why does this help with limited labeled data?

## Architecture Onboarding

### Component map:
Documents → (Summarization if long) → Embeddings → K-means → Per-cluster topic generation → Topic assignment → (Refinement) → Output

### Critical path:
Documents → (Summarization if long) → Embeddings → K-means → Per-cluster topic generation → Topic assignment → (Refinement) → Output

### Design tradeoffs:
- **Cluster count (K)**: Higher K = finer granularity but more LLM calls and fragmentation risk
- **Summarization threshold**: Recommended for long texts (e.g., Wiki at ~3K words); raw input preferred for short texts to preserve detail
- **LLM vs. distilled inference**: LLM for initial discovery; distillation trades slight accuracy for major cost savings on future classification

### Failure signatures:
- **Too few topics / over-merging**: K too low or refinement overly aggressive
- **Overly fragmented topics**: K too high or poor cluster coherence
- **Misaligned topics**: Missing or incorrect business definitions
- **Long document drop-off**: Summarization losing critical details or context window exceeded
- **Assignment inconsistencies**: Embeddings not capturing domain semantics; consider domain-adapted embeddings

### First 3 experiments:
1. **Cluster count sensitivity**: On a validation set, sweep K from 0.5× to 1.5× expected topic count; plot P1/ARI/NMI and topic count to find granularity-quality tradeoff inflection point
2. **Summarization ablation**: Compare TIDE with vs. without summarization across short (<100 words), medium (100–500), and long (>500) documents; establish length thresholds where summarization helps vs. harms
3. **Distillation quality gap**: Generate topics with full LLM pipeline, fine-tune SetFit on assignments, then evaluate classification accuracy delta between LLM and distilled model on held-out test set

## Open Questions the Paper Calls Out

### Open Question 1
How can the distillation module be improved to maintain performance when topic modeling outputs contain ambiguous data points or severely imbalanced class distributions? The current dependency on potentially noisy or skewed pseudo-labels creates a bottleneck for the downstream classifier.

### Open Question 2
What specific mechanisms can mitigate information loss when processing documents that significantly exceed LLM context windows, beyond standard summarization? While summarization helps fit text into context windows, the paper admits it may discard details required for granular topics.

### Open Question 3
Is TIDE's cluster-then-generate architecture inherently suboptimal for datasets consisting primarily of short texts? On the Banking77 dataset (avg. 11 words/text), BERTopic significantly outperformed TIDE on P1 and NMI metrics.

## Limitations
- Clustering quality and optimal K selection remain empirically unvalidated despite controlling granularity
- Business-definition-guided summarization benefits lack rigorous ablation studies isolating summarization impact
- Efficiency claims (O(N) vs O(N×k)) lack direct empirical comparison with baseline methods

## Confidence

**High Confidence**: Claims about superior performance on established clustering metrics (P1, ARI, NMI) across multiple public datasets.

**Medium Confidence**: Claims about granularity improvements through clustering are plausible but under-specified, lacking validation of cluster quality thresholds.

**Low Confidence**: Efficiency claims lack direct empirical comparison with baseline methods; distillation module effectiveness claims would benefit from cross-model comparisons.

## Next Checks

1. **Cluster Quality Sensitivity Analysis**: Systematically vary K from 0.5× to 2× expected topic count on Banking77 and Bills datasets. Measure cluster silhouette scores and topic coherence metrics to establish the relationship between cluster quality, K selection, and final topic quality.

2. **Granularity vs. Coherence Tradeoff**: On Wiki dataset, compare TIDE with standard BERTopic at multiple granularity levels. Measure topic distinctiveness (P1) and cross-topic similarity to determine whether increased topic counts from clustering actually improve topic separation or create semantic overlap.

3. **Efficiency Benchmarking**: Implement LLooM baseline exactly as described and measure end-to-end runtime and API costs on CCC dataset. Compare with TIDE's cluster-based approach under identical hardware and API conditions to validate the claimed computational efficiency improvements.