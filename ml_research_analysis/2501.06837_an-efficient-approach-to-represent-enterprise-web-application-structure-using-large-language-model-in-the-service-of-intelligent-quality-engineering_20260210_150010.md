---
ver: rpa2
title: An efficient approach to represent enterprise web application structure using
  Large Language Model in the service of Intelligent Quality Engineering
arxiv_id: '2501.06837'
source_url: https://arxiv.org/abs/2501.06837
tags:
- test
- application
- cases
- testing
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The research introduces a hierarchical representation methodology
  for enterprise web applications using Large Language Models (LLMs) to enhance intelligent
  quality engineering. The approach addresses challenges in automated software testing
  by developing a structured format enabling LLMs to understand web application architecture
  through in-context learning.
---

# An efficient approach to represent enterprise web application structure using Large Language Model in the service of Intelligent Quality Engineering

## Quick Facts
- arXiv ID: 2501.06837
- Source URL: https://arxiv.org/abs/2501.06837
- Reference count: 35
- Primary result: 90% test execution success rate on Swag Labs, 70% on MediBox using hierarchical LLM-driven test generation

## Executive Summary
This research introduces a hierarchical representation methodology for enterprise web applications using Large Language Models (LLMs) to enhance intelligent quality engineering. The approach addresses automated software testing challenges by developing a structured format that enables LLMs to understand web application architecture through in-context learning. The methodology comprises five phases: comprehensive DOM analysis, multi-page synthesis, test suite generation, execution, and result analysis. Experimental evaluation using two distinct web applications demonstrated significant improvements in automated testing efficiency while maintaining contextual relevance across diverse application types.

## Method Summary
The methodology extracts interactive elements from web applications while preserving parent-child relationships and organizes them into hierarchical page-level summaries, section-level contexts, and element-level details. This structured format enables LLMs to reason about web application architecture despite context window constraints. The system identifies page types from a predefined taxonomy, establishes navigation flows, and generates test cases iteratively with cumulative context. Test cases are converted to Selenium commands and executed with synthetic data generation, with results summarized using LLM-enhanced reporting.

## Key Results
- Achieved 90% test execution success rate on Swag Labs e-commerce application
- Demonstrated 70% success rate on MediBox healthcare application with more complex DOM structure
- High relevance scores across multiple evaluation criteria for generated test cases
- Successfully reduced time and effort required for enterprise web application testing through automated generation

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical compression of DOM structures enables LLMs to reason about web application architecture despite context window constraints. The methodology extracts interactive elements, preserves parent-child relationships, and organizes them into page-level summaries → section-level contexts → element-level details. This hierarchical encoding maintains structural relationships while reducing token count compared to raw DOM trees. The LLM receives navigational pathways and element relationships in a format optimized for its attention mechanism.

### Mechanism 2
Page type classification and navigation flow mapping enable contextually relevant test case generation across multi-page applications. The system identifies page types from a closed set using pattern recognition and LLM analysis. Cross-page relational mapping establishes navigation flows and page hierarchies. This context allows the LLM to generate test cases that respect application flow rather than treating pages in isolation.

### Mechanism 3
Iterative test generation with cumulative context produces progressively refined test suites that avoid redundancy. The LLM receives the hierarchical site representation along with already-generated test cases, extracted URL patterns, and instructions in an iterative loop. Required test types combine predefined types with instruction-extracted types. The LLM validates for structural integrity, uniqueness, and contextual relevance before incorporating each new test case.

## Foundational Learning

- **Document Object Model (DOM) hierarchy and element relationships**
  - Why needed here: The entire methodology depends on extracting, preserving, and reasoning about parent-child element relationships. Without understanding DOM structure, you cannot interpret how the hierarchical representation compresses information.
  - Quick check question: Given an HTML form with nested divs containing input fields, can you identify which elements share a parent and how that affects their interaction patterns?

- **In-context learning (few-shot learning) in LLMs**
  - Why needed here: The approach explicitly avoids fine-tuning in favor of in-context learning. Understanding why context format matters for LLM reasoning is essential to grasp why hierarchical representation works better than raw DOM dumps.
  - Quick check question: If an LLM has a 4K token context window and a typical e-commerce page DOM is 50K tokens, what compression ratio must the hierarchical representation achieve?

- **Test automation frameworks (Selenium) and locator strategies**
  - Why needed here: Phase 4 requires converting LLM-generated test cases into executable Selenium scripts. Understanding element locators (id, class, xpath) explains why the scraping phase captures specific attributes.
  - Quick check question: Why would sibling elements with similar locators cause test execution failures, and how does unique element identification solve this?

## Architecture Onboarding

- **Component map**: Scraper Agent → Site Analysis Module → LLM Test Generator → Test Data Mapper → Execution Interpreter → Report Analyzer
- **Critical path**: Scraper → Site Analysis (DOM must be correctly extracted with relationships intact) → Site Analysis → Test Generator (hierarchical representation must preserve navigation context) → Test Generator → Execution (test cases must have valid locators and data mappings) → Execution → Report (failures must be traceable to specific elements)
- **Design tradeoffs**:
  - Compression vs. fidelity: Aggressive compression may lose element details needed for edge cases; minimal compression hits context limits
  - Predefined vs. extracted test types: Predefined types (E) ensure coverage; extracted types (A) address application-specific requirements but may miss scenarios
  - Synthetic vs. real test data: Synthetic enables autonomous execution; real data better validates production scenarios
- **Failure signatures**:
  - 90% Swag Labs vs. 70% MediBox success rate suggests complexity or DOM structure differences affect reliability
  - Timeout failures on "performance glitch user" test indicates the system struggles with latency scenarios
  - NoneType errors on "Successful User Registration" point to race conditions in response validation
  - Assertion failures on page title checks reveal weak locator strategies for dynamic content
- **First 3 experiments**:
  1. Run the scraper on a target application and manually verify that parent-child relationships are preserved in the hierarchical output. Check that navigation flow matches actual click paths.
  2. Provide the hierarchical representation to an LLM with minimal instructions and evaluate whether generated test cases reference correct element identifiers and logical page flows.
  3. Execute generated tests on a simple application (e.g., Swag Labs) and categorize failures into: locator issues, data mapping issues, timing issues, or logic issues. Use this taxonomy to identify which phase needs refinement.

## Open Questions the Paper Calls Out

### Open Question 1
Will domain-specific fine-tuning of LLMs for test automation significantly improve performance on large context windows compared to general-purpose models using in-context learning? The current approach relies on few-shot in-context learning with general-purpose LLMs; domain fine-tuning has not been experimentally validated.

### Open Question 2
Can a knowledge graph architecture effectively reduce input size requirements for page-level test generation while preserving the hierarchical relationships needed for accurate test case synthesis? The proposed knowledge graph approach is conceptual only; no implementation or evaluation has been conducted.

### Open Question 3
What modifications to the hierarchical representation methodology are required to maintain structural integrity for applications with exceptionally large DOM structures? The paper acknowledges this limitation but does not propose or test solutions for large-DOM applications.

## Limitations
- Applications with exceptionally large DOM structures challenge the algorithm's ability to maintain hierarchical relationships
- The 20 percentage point gap in execution success rates between Swag Labs (90%) and MediBox (70%) raises questions about generalizability across different application domains
- The methodology does not address dynamic web applications where DOM structures change during runtime or testing

## Confidence

- **High Confidence**: The core mechanism of hierarchical DOM compression for context window management is well-supported by the need to reduce 50K+ token DOMs to fit within LLM constraints
- **Medium Confidence**: The iterative test generation approach with cumulative context is logically sound, but the claimed 87% correctness rate lacks methodological detail
- **Low Confidence**: The effectiveness of the predefined page type taxonomy across diverse enterprise applications is asserted but not empirically validated beyond the two studied cases

## Next Checks

1. Implement the hierarchical representation using the described compression approach and measure the actual token reduction ratio achieved compared to raw DOM representation, verifying it enables in-context learning within typical LLM context windows.

2. Apply the methodology to three additional enterprise web applications with different architectures (e-commerce, content management, and financial services) and measure whether the 90% success rate baseline holds or degrades significantly.

3. Systematically evaluate the predefined page type classification against applications with unconventional structures to quantify false positives/negatives and determine whether the fixed taxonomy limits applicability to standard enterprise patterns.