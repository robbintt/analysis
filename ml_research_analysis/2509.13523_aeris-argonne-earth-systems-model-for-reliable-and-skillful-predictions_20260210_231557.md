---
ver: rpa2
title: 'AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions'
arxiv_id: '2509.13523'
source_url: https://arxiv.org/abs/2509.13523
tags:
- parallelism
- weather
- arxiv
- data
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces AERIS, a high-resolution pixel-level diffusion
  transformer for weather and climate forecasting, and SWiPe, a novel window-based
  parallelism strategy. AERIS achieves 10.21 ExaFLOPS sustained performance and 11.21
  ExaFLOPS peak performance on Aurora, scaling to 10,080 nodes and 120,960 GPUs, while
  remaining stable on seasonal scales to 90 days.
---

# AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions

## Quick Facts
- **arXiv ID:** 2509.13523
- **Source URL:** https://arxiv.org/abs/2509.13523
- **Reference count:** 40
- **Primary result:** 10.21 ExaFLOPS sustained performance and 11.21 ExaFLOPS peak performance on Aurora, scaling to 10,080 nodes and 120,960 GPUs, while remaining stable on seasonal scales to 90 days.

## Executive Summary
AERIS is a high-resolution pixel-level diffusion transformer for weather and climate forecasting that achieves breakthrough performance on Aurora, scaling to 10,080 nodes (120,960 GPUs) with 10.21 ExaFLOPS sustained performance. The model uses a novel SWiPe (Sequence-Window Parallelism) strategy to shard billion-parameter diffusion transformers across distributed hardware without communication overhead. AERIS demonstrates competitive medium-range forecast skill compared to state-of-the-art models and is the first billion-parameter diffusion model for high-resolution global weather and climate prediction, maintaining stability on seasonal scales up to 90 days.

## Method Summary
AERIS employs a non-hierarchical Swin Transformer operating at pixel-level (1x1 patches) without downsampling, using 30×30 or 60×60 shifted windows for localized attention. The model is trained with a TrigFlow diffusion objective that predicts residual velocity between noise and data, enabling stable autoregressive rollouts. SWiPe parallelism composes window parallelism (spatial shard), sequence parallelism (within-node), and pipeline parallelism (across nodes) to manage the computational burden. The system uses ERA5 reanalysis data at 0.25° resolution with 6-hourly intervals, training on 1979-2018 data and evaluating on 2019-2020 test periods.

## Key Results
- Achieves 10.21 ExaFLOPS sustained performance and 11.21 ExaFLOPS peak performance on Aurora
- Scales to 10,080 nodes (120,960 GPUs) with 95.5% weak scaling efficiency
- Maintains stability on seasonal scales to 90 days with competitive medium-range forecast skill
- Accurately predicts hurricane intensification with 5-day lead time at 0.25° resolution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SWiPe enables training of billion-parameter diffusion transformers on high-resolution global data by sharding attention windows across devices without incurring prohibitive communication overhead.
- **Mechanism:** The strategy exploits the locality of Swin Transformers by treating non-overlapping attention windows as independent processing units distributed across a grid of nodes. It layers window parallelism on top of sequence and pipeline parallelism, aligning data partitioning with the window shifting mechanism to avoid redundant data exchanges or halo swaps.
- **Core assumption:** The localized attention mechanism of Swin Transformers provides sufficient global context when shifted across layers, such that windows can be processed largely independently across distributed hardware without breaking spatial dependencies.
- **Evidence anchors:** [Abstract] "SWiPe... composes window parallelism with sequence and pipeline parallelism to shard window-based transformers without added communication cost or increased global batch size." [Section V-A] "SWiPe avoids introducing additional communication or synchronization points."
- **Break condition:** If window size must increase significantly to capture global teleconnections that exceed memory capacity of a single device, the current sharding logic would require restructuring.

### Mechanism 2
- **Claim:** A diffusion-based objective combined with autoregressive rollouts provides stable, skillful forecasts on seasonal (90-day) timescales where deterministic models typically diverge or blur.
- **Mechanism:** Instead of regressing directly to a single future state, AERIS learns the conditional probability distribution and uses "TrigFlow" parameterization to estimate residual velocity between noise and data. By injecting noise during inference and sampling from the learned distribution, the model maintains ensemble spread and avoids spectral collapse common in MSE-trained models.
- **Core assumption:** Atmospheric dynamics can be modeled as a stochastic process where the conditional probability for the next step is sufficiently captured by the diffusion model, and errors do not accumulate within 90 days to push the state out of the training distribution.
- **Evidence anchors:** [Abstract] "Recent diffusion-based methods address spectral biases... remains stable on seasonal scales to 90 days." [Section VI-B] "Our formulation produces... ensembles competitive on medium-range with impressive seasonal stability."
- **Break condition:** If under-dispersive nature worsens over longer time horizons, the ensemble may collapse to a deterministic mean, failing to capture "butterfly effect" or tail risks.

### Mechanism 3
- **Claim:** A non-hierarchical, pixel-level Swin Transformer architecture preserves high-frequency spatial details necessary for extreme weather prediction better than hierarchical or patch-based alternatives.
- **Mechanism:** AERIS operates at "pixel-level" (1x1 patch size) without downsampling, preserving native 0.25° resolution through network depth. It utilizes modern LLM-inspired blocks (SwiGLU, RoPE) to stabilize training of this high-dimensional sequence.
- **Core assumption:** Computational burden of processing full-resolution tokens can be managed via SWiPe, and localized window attention is sufficient to resolve fine-scale dynamics without full global attention.
- **Evidence anchors:** [Section V-B] "AERIS... maintains a non-hierarchical structure... map full resolution images in pixel-space." [Section VII-B] "AERIS accurately predicts this [hurricane] intensification period with a lead time of 5 days."
- **Break condition:** If small-scale dynamics require cross-window global context strictly larger than the local window size, the model may miss crucial interactions, leading to physically unrealistic artifacts.

## Foundational Learning

- **Concept: Swin Transformer & Shifted Windows**
  - **Why needed here:** AERIS relies on "Shifted Window" attention to process global weather maps efficiently. Without understanding this, the parallelism strategy looks like standard data sharding.
  - **Quick check question:** How does "shifting" the window partition in layer $N+1$ allow information to travel between windows that were isolated in layer $N$?

- **Concept: Diffusion Models (v-prediction / TrigFlow)**
  - **Why needed here:** The paper frames weather prediction not as mapping Input $\to$ Output, but as a denoising process (Diffusion). You must understand the "velocity" prediction to interpret the loss function and inference sampler.
  - **Quick check question:** In diffusion, does the model predict the final clean image directly, or the "noise" (or velocity vector) added to the image?

- **Concept: Weak vs. Strong Scaling in HPC**
  - **Why needed here:** The paper boasts "0.54 ExaFLOPS" and "95.5% weak scaling." Understanding the difference is vital to diagnosing whether the system is bottlenecked by compute or communication.
  - **Quick check question:** If you double the number of GPUs and keep the total batch size the same, are you testing strong scaling or weak scaling?

## Architecture Onboarding

- **Component map:** Data Loading (Sharded HDF5) -> Pipeline Stage 1 (Patch Embed) -> [Distributed Window Attention (SWiPe)] -> Pipeline Stage $N$ (Decode) -> Loss Calculation -> Optimizer Step
- **Critical path:** Data Loading (Sharded HDF5) $\to$ Pipeline Stage 1 (Patch Embed) $\to$ [Distributed Window Attention (SWiPe)] $\to$ Pipeline Stage $N$ (Decode) $\to$ Loss Calculation $\to$ Optimizer Step
- **Design tradeoffs:**
  - Pixel-level (1x1) vs. Patches: Maximizes detail (good for hurricanes) but explodes sequence length (requires SWiPe to manage)
  - Non-hierarchical vs. Hierarchical: Maintains consistent resolution (good for stability) but forfeits multi-scale feature extraction of U-Nets/Pyramids
  - Window Parallelism: Reduces activation memory and allows small batch sizes, but adds complexity to scheduling of "shifted" window communications
- **Failure signatures:**
  - Training Divergence: Look for "NaNs" in loss curve; likely due to BF16 overflow in attention or gradient reduction
  - Under-dispersion: If ensemble spread is too low (Spread/Skill Ratio < 1), model is overfitting or "churn" noise is insufficient
  - Pipeline Bubbles: Low GPU utilization at start/end of steps indicates pipeline schedule (1F1B) is not effectively overlapping communication/computation
- **First 3 experiments:**
  1. Baseline Scaling Verification: Run 1.3B model on 48 nodes vs. 1920 nodes. Verify throughput scales linearly (weak scaling) and identify communication overhead threshold
  2. Ablation on Window Parallelism (WP): Train two identical small models, one with WP enabled and one using standard Sequence/Tensor parallelism. Compare memory footprint and samples/sec to quantify "activation memory" reduction
  3. Diffusion Stability Test: Run 90-day rollout (autoregressive). Check if physical fields retain realistic power spectra or if they dissolve into noise/blurriness (spectral bias)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can tuning the stochastic churning schedule or adding initial condition perturbations resolve AERIS's under-dispersive ensemble spread?
- **Basis in paper:** [explicit] Section VII-C notes the model is under-dispersive (SSR<1) and explicitly suggests these interventions to improve spread.
- **Why unresolved:** Current training configuration prioritized stability and deterministic skill over probabilistic calibration.
- **Evidence:** Experiments showing increased Spread/Skill Ratio (SSR) approaching 1.0 without regression in RMSE or CRPS.

### Open Question 2
- **Question:** Can consistency distillation compress AERIS inference to a single step without significant loss of forecast skill?
- **Basis in paper:** [explicit] Section VII-C states that diffusion parameterization allows for consistency distillation to lower computational costs.
- **Why unresolved:** While theoretically possible, specific trade-off between sample quality and speed for high-resolution weather diffusion models remains unverified in results.
- **Evidence:** Comparative skill metrics (e.g., RMSE, CRPS) between single-step distilled model and standard 10-step solver.

### Open Question 3
- **Question:** To what extent does adopting zero-bubble pipeline parallelism improve SWiPe efficiency over the current 1F1B schedule?
- **Basis in paper:** [explicit] Section VII-C identifies pipeline bubble size as current limitation and proposes zero-bubble pipeline parallelism as solution.
- **Why unresolved:** Current 1F1B scheduling used in SWiPe inherently introduces idle time (bubbles), limiting maximum throughput.
- **Evidence:** Profiling data showing reduced bubble overhead and higher sustained FLOPS or Model FLOPS Utilization (MFU).

## Limitations

- Scalability claims mask potential strong scaling limitations - only 1.38x speedup when going from 48 to 1,920 nodes with same batch size
- 90-day stability claim lacks rigorous statistical validation and doesn't quantify how under-dispersion evolves over the horizon
- Extreme event prediction claims based on single case study insufficient to establish general capability for rare, high-impact phenomena

## Confidence

**High Confidence:** Architectural innovations (non-hierarchical Swin Transformer, SWiPe parallelism) and their implementation are well-documented with specific hyperparameters, layer configurations, and scaling benchmarks. 0.54 ExaFLOPS sustained performance and 11.21 ExaFLOPS peak performance on Aurora are verifiable through standard HPC profiling.

**Medium Confidence:** Diffusion model's skill in medium-range forecasting (1-2 weeks) is supported by competitive RMSE and CRPS scores against established models. However, claimed advantages over deterministic models for seasonal forecasting require longer-term validation beyond 90-day horizon tested.

**Low Confidence:** Claims about preserving high-frequency spatial details for extreme weather prediction lack systematic evaluation. Single hurricane case study insufficient to establish general capability, and paper doesn't address potential artifacts introduced by pixel-level approach or window-based attention limitations.

## Next Checks

1. **Long-Horizon Stability Analysis:** Run 180-day autoregressive forecasts and analyze evolution of key physical fields. Compute spectral power distributions at multiple time horizons to quantify spectral bias accumulation and compare against climatology to detect drift.

2. **Extreme Event Robustness Test:** Systematically evaluate model's performance on diverse set of extreme weather events across multiple years. Use event-based metrics beyond RMSE/CRPS, such as false alarm rates, hit rates, and intensity error distributions, to assess skill on rare but high-impact phenomena.

3. **Architectural Ablation Study:** Compare non-hierarchical pixel-level approach against hierarchical variants with progressive downsampling. Measure both computational efficiency (FLOPS, memory) and forecast skill across different spatial scales to quantify trade-off between resolution preservation and multi-scale feature extraction.