---
ver: rpa2
title: 'CRAwDAD: Causal Reasoning Augmentation with Dual-Agent Debate'
arxiv_id: '2511.22854'
source_url: https://arxiv.org/abs/2511.22854
tags:
- causal
- arxiv
- debate
- answer
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a dual-agent debate framework for improving\
  \ causal reasoning in language models. The approach pairs two reasoning language\
  \ models\u2014Qwen3 and DeepSeek-R1\u2014to debate answers to causal inference questions,\
  \ with one agent critiquing the other's reasoning and both refining their responses\
  \ through structured discussion."
---

# CRAwDAD: Causal Reasoning Augmentation with Dual-Agent Debate

## Quick Facts
- arXiv ID: 2511.22854
- Source URL: https://arxiv.org/abs/2511.22854
- Reference count: 40
- Primary result: Dual-agent debate framework improves causal reasoning accuracy from 78.03% to 87.45% for DeepSeek-R1 and from 84.16% to 89.41% for Qwen3 on CLadder dataset

## Executive Summary
This paper introduces CRAwDAD, a dual-agent debate framework for improving causal reasoning in language models. The approach pairs two reasoning language models—Qwen3 and DeepSeek-R1—to debate answers to causal inference questions, with one agent critiquing the other's reasoning and both refining their responses through structured discussion. Evaluated on the CLadder dataset, the debate framework significantly improved both models' accuracy, particularly on counterfactual questions, demonstrating that collaborative reasoning through debate enhances causal inference performance even for strong models.

## Method Summary
CRAwDAD uses a structured debate framework where two reasoning language models (RLMs) take turns providing and critiquing causal inference responses. The first agent provides a structured 7-step causal inference using CausalCoT-style reasoning, while the second agent examines this reasoning for logical flaws. If they disagree, the debate continues for up to 4 rounds. Granite3.3-2B extracts yes/no answers and confidence scores from verbose RLM outputs. The framework uses early stopping on consensus and employs open-source RLMs (Qwen3-32B, DeepSeek-R1-Distill-Qwen-32B) to ensure reproducibility.

## Key Results
- Debate improved DeepSeek-R1's overall accuracy from 78.03% to 87.45% and Qwen3's from 84.16% to 89.41%
- The most significant gains occurred on counterfactual questions (Rung 3 of Pearl's Ladder)
- In 577 cases, confident incorrect answers were corrected through subsequent debate rounds
- DeepSeek-R1 produced very short responses (22.6% under 100 characters) in debate rounds despite substantive internal reasoning

## Why This Works (Mechanism)

### Mechanism 1: Structured Adversarial Critique
When Agent B scrutinizes Agent A's reasoning, they explicitly identify errors in graph extraction, query type classification, or do-calculus application. This forces correction of specific mistakes rather than generic re-generation.

### Mechanism 2: Model Heterogeneity
Using Qwen3 and DeepSeek-R1 (different architectures, training data) ensures agents bring distinct priors to causal problems. When they disagree, the debate explicitly tests which reasoning path is valid.

### Mechanism 3: Confidence-Weighted Persuasion
Models report confidence (0.0-1.0). Higher-confidence arguments are more persuasive. When a confident wrong answer meets a confident correct answer, debate reveals the flaw.

## Foundational Learning

- Concept: Pearl's Ladder of Causation (Rung 1: Association, Rung 2: Intervention, Rung 3: Counterfactuals)
  - Why needed here: The paper evaluates performance across all three rungs; results show Rung 3 (counterfactuals) benefits most from debate but remains hardest.
  - Quick check question: Given "Bob scored 85 after drinking coffee. Would he have scored higher without it?"—which rung is this, and what formal notation expresses it?

- Concept: Do-calculus and Backdoor Adjustment
  - Why needed here: Agents must formalize queries using do(·) notation and identify when conditioning on confounders is required.
  - Quick check question: If Demand → Supply and Demand → Price, why does marginal correlation between Supply and Price fail to identify the causal effect?

- Concept: Multi-Agent Debate Termination Conditions
  - Why needed here: The framework uses early stopping (consensus before round 4) and disagreement-triggered debate; understanding when to stop is critical for efficiency.
  - Quick check question: What are two conditions under which CRAwDAD terminates debate early, and why does round 4 always end debate regardless of consensus?

## Architecture Onboarding

- Component map: Agent A (Qwen3-32B) -> Granite3.3-2B -> Agent B (DeepSeek-R1-Distill-Qwen-32B) -> Granite3.3-2B -> Debate Orchestrator

- Critical path:
  1. Question → Agent A (CausalCoT prompt with confidence request)
  2. Agent A response → Granite extraction → Agent B (debate prompt with A's rationale)
  3. If disagree: rounds 3-4 continue; if agree: terminate
  4. Final answer logged after consensus or round 4

- Design tradeoffs:
  - Two agents vs. three (no judge): More direct adversarial engagement, but no neutral arbitrator if models deadlock
  - Max 4 rounds: Prevents degradation (observed in preliminary tests), but may truncate beneficial longer debates
  - Open-source RLMs only: Reproducible, but limits direct comparison to GPT-4/o1 baselines

- Failure signatures:
  - DeepSeek-R1 produces very short responses (22.6% under 100 chars) in debate rounds, weakening persuasion
  - Models occasionally loop (3 responses took 40-46 minutes due to repetition)
  - Systematic overconfidence: no confidence values below 60% observed

- First 3 experiments:
  1. Replicate single-agent baseline: Run both models with CausalCoT only, verify accuracy matches reported (Qwen3: 84.16%, DeepSeek-R1: 78.03%)
  2. Test homogeneous debate: Run Qwen3 vs. Qwen3 to isolate heterogeneity contribution
  3. Ablate confidence: Remove confidence estimation from prompts to measure its effect on persuasion dynamics

## Open Questions the Paper Calls Out

### Open Question 1
Does a scaling law exist between model size and formal causal inference capabilities in reasoning language models? Hardware limitations restricted experiments to 32B parameter models; larger variants were not evaluated.

### Open Question 2
Would alternative debate frameworks (3 agents, or 2 agents plus a judge) yield further performance gains over the dual-agent setup? Time constraints precluded ablation studies on framework variants.

### Open Question 3
Are the performance gains observed partially attributable to data contamination from CLadder exposure during model training? The authors chose not to generate new questions to maintain comparability with prior benchmark results.

### Open Question 4
Why does DeepSeek-R1 frequently produce extremely short debate responses despite engaging in substantive internal reasoning? The paper documents this asymmetry but does not identify its mechanistic cause.

## Limitations

- Unknown architecture details: Inference hyperparameters (temperature, top-p, max tokens) and exact Ollama versions are unspecified, making perfect reproduction difficult.
- Model capability ceiling: Counterfactual questions (Rung 3) show the largest gains but remain the weakest category even after debate, suggesting fundamental limitations in handling counterfactual reasoning.
- Confidence calibration: Both models show systematic overconfidence (no confidence values below 60%), which may reduce the effectiveness of confidence-weighted persuasion.

## Confidence

- High confidence: The dual-agent debate framework demonstrably improves causal reasoning accuracy on the CLadder dataset (78.03% → 87.45% for DeepSeek-R1).
- Medium confidence: The heterogeneity mechanism (different model architectures bringing diverse reasoning perspectives) contributes meaningfully to debate effectiveness.
- Low confidence: The systematic overconfidence of both models and its impact on debate quality, given confidence values never drop below 60%.

## Next Checks

1. Ablate confidence: Remove confidence estimation from debate prompts to measure whether persuasion dynamics depend on confidence scores.
2. Test homogeneous debate: Run Qwen3 vs. Qwen3 and DeepSeek-R1 vs. DeepSeek-R1 to isolate the contribution of model heterogeneity.
3. Examine looping cases: Analyze the 3 cases that required 40-46 minutes due to looping to understand failure modes and determine optimal debate round limits.