---
ver: rpa2
title: 'Explanova: Automatically Discover Data Insights in N \times M Table via XAI
  Combined LLM Workflow'
arxiv_id: '2601.12317'
source_url: https://arxiv.org/abs/2601.12317
tags:
- data
- feature
- shap
- features
- explanova
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Explanova, an automated data analysis workflow
  that combines XAI techniques with LLM-based natural language generation. The system
  addresses the challenge of expensive LLM consumption in automated data science by
  implementing a three-stage preset workflow: feature preparation, feature-to-feature
  statistics, and feature modeling.'
---

# Explanova: Automatically Discover Data Insights in N \times M Table via XAI Combined LLM Workflow

## Quick Facts
- arXiv ID: 2601.12317
- Source URL: https://arxiv.org/abs/2601.12317
- Reference count: 23
- Primary result: Combines XAI techniques with LLM-based natural language generation to efficiently explore all possible relationships in N×M tables through statistical analysis and SHAP-based explanations

## Executive Summary
Explanova addresses the challenge of expensive LLM consumption in automated data science by implementing a three-stage preset workflow: feature preparation, feature-to-feature statistics, and feature modeling. The system uses parallel processing and local small LLMs to efficiently explore all possible relationships in N×M tables. When tested on a marketing campaign dataset with 28 features and 2,240 samples, Explanova identified 176 significant feature relationships out of 720 analyzed, with 9 features achieving "HIGH" credibility scores. The workflow successfully generates detailed, human-readable reports with credibility scores that help users understand data insights while maintaining computational efficiency suitable for consumer-level GPUs.

## Method Summary
Explanova is a three-stage automated data analysis workflow that combines XAI techniques with LLM-based natural language generation. Stage 1 (Feature Preparation) detects feature types using LLM, handles missing values and outliers, and clusters features using Gower distance and HDBSCAN. Stage 2 (Feature-to-Feature Statistics) analyzes all feature pairs using 12 statistical tests across four variable-type combinations, with LLM interpreting significance. Stage 3 (Feature Modeling) trains four model families (Logistic/Linear, Decision Tree, MLP, Ensemble) with 5-fold CV, computes unified NLL, applies KernelSHAP with 3-fold perturbation stability, and calculates global SHAP entropy. The system generates credibility scores using the formula Score(R) = H / (|NLL| × |SHAP-Error|) and produces structured reports with ranked features.

## Key Results
- Identified 176 significant feature relationships out of 720 analyzed pairs in marketing campaign dataset
- 9 features achieved "HIGH" credibility scores (range: 57.6–6798.7)
- Successfully processed 28 features with 2,240 samples in approximately 5 minutes
- Generated detailed, human-readable reports with credibility scores helping users understand data insights

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preset workflow traversal with parallel processing reduces LLM consumption while maintaining comprehensive data exploration.
- Mechanism: Statistical analysis and ML modeling are computationally cheaper than LLM calls. By computing all relationships deterministically (Xn statistics, Xn1-Xn2 relationships, Xn to all others) in parallel, LLM is only needed for natural language description generation—not for reasoning about what to analyze.
- Core assumption: Valuable insights emerge from exhaustive statistical traversal rather than selective, reasoning-driven exploration.
- Evidence anchors:
  - [abstract] "Cheaper due to a Local Small LLM"
  - [section 1] "statistical analysis and machine learning modeling are far cheaper than LLM calling, and remaining LLM-needed procedures can be easily handled by parallel calling"
  - [corpus] Weak direct evidence—neighbor papers focus on agentic approaches rather than preset workflow comparisons.

### Mechanism 2
- Claim: Composite credibility scoring enables reliable identification of trustworthy feature relationships in modeling.
- Mechanism: Score(R) = H / (|NLL| × |SHAP-Error|) combines three factors: (1) SHAP entropy H measures how concentrated feature importance is—higher entropy = more interpretable; (2) NLL measures model fit quality; (3) SHAP-Error measures explanation stability across k-fold perturbations.
- Core assumption: High SHAP entropy indicates trustworthy interpretability; stable explanations across data perturbations indicate faithful representations of true feature relationships.
- Evidence anchors:
  - [section 3.4] "Lower NLL, lower SHAP-Error and higher entropy means more reliable model thus more accountable final findings"
  - [section 4.2] Shows 9 HIGH credibility features with scores ranging 57.6–6798.7
  - [corpus] "From XAI to Stories" paper examines LLM-generated explanation quality from XAI outputs but doesn't validate composite scoring schemes.

### Mechanism 3
- Claim: Unified NLL metric enables consistent comparison across classification and regression tasks for feature modeling.
- Mechanism: Both classification (probability outputs) and regression (Gaussian mean/variance outputs) are treated as probability models, allowing NLL computation for each. This unified metric permits model selection and comparison across all features regardless of target type.
- Core assumption: NLL adequately captures model quality for both discrete and continuous targets in equal measure.
- Evidence anchors:
  - [section 3.4] "This provides a unified interpretation that the smaller NLL the better model, allowing consistent comparison across classification and regression tasks"
  - [abstract] System analyzed 28 features with mixed types successfully
  - [corpus] No direct corpus validation of NLL unification approach.

## Foundational Learning

- Concept: SHAP (Shapley Additive Explanations)
  - Why needed here: Core to the feature modeling stage—Explanova uses KernelSHAP to explain how each predictor contributes to target predictions and computes global importance, entropy, and stability metrics.
  - Quick check question: Can you explain why SHAP values sum to the difference between model prediction and expected value?

- Concept: Negative Log-Likelihood (NLL)
  - Why needed here: Serves as the unified loss metric across classification and regression; understanding probability distributions is required to interpret NLL formulas in section 3.4.
  - Quick check question: For a Gaussian with variance σ², why does NLL include the term ½log(2πσ²)?

- Concept: Statistical Test Selection by Variable Type
  - Why needed here: Feature-to-feature stage uses different tests (Pearson/Spearman/Mutual Information vs. ANOVA/η² vs. χ²/Cramer's V) based on whether variables are continuous or discrete.
  - Quick check question: Why would you choose Kruskal-Wallis H over ANOVA F-statistic for a continuous-discrete relationship?

## Architecture Onboarding

- Component map: Feature Preparation -> Feature-to-Feature Statistics -> Feature Modeling -> Report Generator
- Critical path: Feature preparation (must complete for all columns) -> Feature-to-feature statistics (parallel by pair) -> Feature modeling (parallel by target feature, includes model training + SHAP + k-fold stability) -> Report generation (sequential synthesis)
- Design tradeoffs:
  - Exhaustiveness vs. efficiency: Traversing all N×(N-1)/2 pairs provides comprehensive coverage but scales quadratically
  - Local small LLM vs. quality: Using Qwen3-8B reduces cost but may limit nuance in natural language descriptions
  - Preset workflow vs. adaptability: Fixed stages ensure coverage but cannot dynamically redirect based on intermediate findings
- Failure signatures:
  - Low credibility scores across all features: Indicates poor model fits or unstable SHAP explanations—check data quality and feature rescaling
  - Parallel processing errors in SHAP: Paper notes "parallel bugs in Shapley-value-based analysis by featuring different intervals" as incomplete
  - Missing cluster feature: If SSE exceeds threshold, clustering is discarded—may indicate incompatible mixed-type data
- First 3 experiments:
  1. Run Explanova on a clean dataset with known feature relationships (e.g., synthetic data with injected correlations) to validate that significance thresholds correctly identify ground-truth relationships.
  2. Vary the LLM size (e.g., compare Qwen3-8B vs. larger model) on description quality to quantify the tradeoff between cost and output richness.
  3. Stress test scalability: measure runtime and memory on progressively larger N×M tables to identify parallelization bottlenecks, particularly in SHAP computation.

## Open Questions the Paper Calls Out

- Question: How does Explanova's performance compare to established agentic frameworks on standard benchmarks?
  - Basis in paper: [explicit] The conclusion explicitly states the need to "benchmark our Explanova... on InfiAgent-DABench / Comparison on Discovery-Bench."
  - Why unresolved: The paper currently provides only a single case study on a marketing dataset rather than standardized comparative metrics.
  - What evidence would resolve it: Quantitative results (accuracy, F1, cost) from running Explanova against agent baselines like Datawise or DeepAnalyze on DiscoveryBench.

- Question: Does the proposed "Credibility Score" correlate with the actual correctness or usefulness of an insight?
  - Basis in paper: [inferred] The paper introduces a novel mathematical formula for credibility ($H / |NLL| \times |SHAP-Error|$) but validates it only by ranking features, not by comparing against ground truth.
  - Why unresolved: It is unclear if a "HIGH" score genuinely indicates a reliable finding or merely a statistically stable model.
  - What evidence would resolve it: A user study or correlation analysis comparing the automated scores against expert human evaluation of the generated insights.

- Question: Can the "traverse-all" workflow remain computationally efficient for datasets with significantly higher dimensionality?
  - Basis in paper: [inferred] The workflow computes statistics for all feature pairs; the case study was limited to only 28 features.
  - Why unresolved: The computational complexity for feature-to-feature statistics is roughly $O(M^2)$, which may become prohibitive for wide tables.
  - What evidence would resolve it: Scalability tests measuring runtime and memory usage on datasets with hundreds or thousands of features.

## Limitations

- The preset workflow traversal may miss domain-specific or non-tabular insights that require adaptive reasoning rather than exhaustive statistical traversal
- Composite credibility scoring assumes SHAP entropy and stability metrics reliably indicate trustworthy relationships, but these may be misleading if models overfit or explanations are systematically biased
- NLL unification across classification and regression assumes comparable scales, which may not hold for all datasets

## Confidence

- **High**: The workflow architecture and parallel processing approach are clearly specified and implementable
- **Medium**: The statistical test selection and SHAP-based credibility scoring mechanisms are well-defined, but their effectiveness depends on dataset characteristics
- **Low**: The specific LLM prompts and threshold values for preprocessing steps are not fully specified in the paper

## Next Checks

1. Test Explanova on synthetic datasets with known ground-truth relationships to validate significance threshold accuracy
2. Compare output quality using different LLM sizes (Qwen3-8B vs. larger models) to quantify the cost-quality tradeoff
3. Benchmark scalability by measuring runtime and memory usage on progressively larger N×M tables, focusing on SHAP computation parallelization bottlenecks