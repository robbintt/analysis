---
ver: rpa2
title: 'Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical and
  Cultural Artifacts'
arxiv_id: '2502.14865'
source_url: https://arxiv.org/abs/2502.14865
tags:
- dynasty
- cultural
- period
- greek
- historical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TimeTravel introduces a benchmark for evaluating AI models on historical
  and cultural artifacts, addressing the gap in AI's understanding of non-modern cultural
  contexts. The dataset includes 10,250 expert-verified samples from 266 cultures
  across 10 historical regions, accompanied by detailed descriptions generated using
  GPT-4o.
---

# Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical and Cultural Artifacts

## Quick Facts
- arXiv ID: 2502.14865
- Source URL: https://arxiv.org/abs/2502.14865
- Reference count: 40
- Primary result: TimeTravel benchmark introduces 10,250 expert-verified samples spanning 266 cultures across 10 historical regions, revealing significant performance gaps between closed-source (GPT-4o-0806) and open-source LMMs on cultural artifact description tasks.

## Executive Summary
TimeTravel addresses a critical gap in AI evaluation by providing a benchmark for assessing Large Multimodal Models on historical and cultural artifacts. The dataset spans 266 distinct cultures across 10 major historical regions, with descriptions generated by GPT-4o and verified by domain experts. Evaluation reveals that closed-source models like GPT-4o-0806 significantly outperform open-source alternatives, though Qwen-2.5-VL demonstrates competitive performance. The benchmark enables systematic assessment of model biases toward well-represented civilizations and provides a foundation for improving AI's understanding of diverse cultural contexts.

## Method Summary
TimeTravel constructs a benchmark by extracting sparse metadata from museum archives, generating detailed descriptions using GPT-4o, and subjecting outputs to expert verification. The evaluation employs multiple metrics including BLEU, METEOR, ROUGE-L for n-gram overlap, SPICE for semantic scene graph fidelity, BERTScore for contextual embedding similarity, and LLM-Judge for coherence and factual accuracy assessment. The benchmark includes 10,250 image-text pairs with regional/cultural taxonomy labels, enabling analysis of performance across different historical civilizations and identification of systematic biases in model training data.

## Key Results
- GPT-4o-0806 achieved highest scores across most metrics, significantly outperforming open-source models
- Qwen-2.5-VL demonstrated strong performance among open-source alternatives, indicating potential for refinement
- Performance varied substantially across regions, with Roman Empire scoring 0.4463 vs. British Isles at 0.1899 on LLM-Judge
- Open-source models showed particular weakness on underrepresented cultures, suggesting training data limitations

## Why This Works (Mechanism)

### Mechanism 1
GPT-4o-generated descriptions can serve as pseudo-ground-truth when original museum metadata is incomplete, conditional on expert verification to ensure historical accuracy. The pipeline extracts sparse metadata from museum records, prompts GPT-4o to synthesize contextually-rich descriptions, and subjects outputs to domain-expert validation. This augmentation compensates for missing fields while maintaining scholarly credibility. Core assumption: GPT-4o's pre-trained historical knowledge is sufficiently reliable to generate plausible descriptions that experts can efficiently verify.

### Mechanism 2
Multi-metric evaluation combining n-gram overlap (BLEU, ROUGE-L), semantic similarity (BERTScore, SPICE), and LLM-based judgment captures complementary dimensions of description quality for historical artifacts. Traditional metrics assess surface-level linguistic alignment; SPICE evaluates scene-graph semantic fidelity; BERTScore captures contextual embedding similarity; LLM-Judge evaluates coherence and factual plausibility. Together they compensate for individual metric blind spots. Core assumption: LLM-Judge evaluations correlate with human expert judgments on historical accuracy and cultural sensitivity.

### Mechanism 3
Geographic and cultural diversity across 10 regions with 266 distinct cultures enables detection of model biases toward well-represented civilizations versus underrepresented ones. Stratified sampling ensures coverage of major civilizations; per-region LLM-Judge scoring reveals performance variance—GPT-4o-0806 scores 0.4463 on Roman Empire versus 0.1899 on British Isles, suggesting uneven pre-training exposure. Core assumption: Performance disparities across regions reflect training data distribution rather than inherent task difficulty per culture.

## Foundational Learning

**Concept: Multimodal evaluation metrics (BLEU, METEOR, ROUGE-L, SPICE, BERTScore)**
- Why needed here: Interpreting Table 2 requires understanding that BLEU/ROUGE-L measure surface similarity, METEOR captures synonyms, SPICE evaluates semantic scene graphs, and BERTScore uses contextual embeddings
- Quick check question: Why might a description score high on BERTScore but low on SPICE?

**Concept: Vision-Language Model (VLM/LMM) fine-tuning paradigms**
- Why needed here: The performance gap between closed-source (GPT-4o-0806) and open-source (Qwen-2.5-VL) models reflects both architecture and training data scale; understanding fine-tuning informs improvement strategies
- Quick check question: What additional training data would most likely close the gap for open-source models on underrepresented cultures?

**Concept: Cultural bias in AI systems**
- Why needed here: Section 5 explicitly acknowledges bias risks; interpreting regional performance differences requires distinguishing model limitations from data representation issues
- Quick check question: How would you distinguish between a model's lack of knowledge versus its training data's cultural bias?

## Architecture Onboarding

**Component map:**
Museum archives -> structured metadata extraction -> GPT-4o description generation -> domain expert verification -> multi-metric evaluation (BLEU/METEOR/ROUGE-L/SPICE/BERTScore/LLM-Judge) -> benchmark interface

**Critical path:** Data cleaning -> expert verification -> description generation -> evaluation. Expert verification is the bottleneck; budget ~15-20 minutes per artifact for thorough review.

**Design tradeoffs:**
- GPT-4o generation speed vs. expert verification throughput
- Metric coverage vs. computational cost (LLM-Judge adds latency)
- Cultural breadth (266 cultures) vs. depth per culture (average ~38 samples/culture)

**Failure signatures:**
- Generated descriptions with confident but historically inaccurate claims (hallucination)
- High metric scores on linguistically fluent but factually wrong outputs
- Consistent underperformance on specific regions indicating systematic bias

**First 3 experiments:**
1. Baseline reproduction: Run evaluation on all 7 models from Table 2 using provided code; verify score reproducibility within ±0.01
2. Error analysis on British Isles subset: Manually review 50 lowest-scoring GPT-4o-0806 outputs vs. ground truth to categorize failure modes (temporal confusion, material misidentification, cultural conflation)
3. Ablation on Qwen-2.5-VL with targeted augmentation: Fine-tune on 500 additional British Isles + Central America samples to test whether regional gaps are data-addressable

## Open Questions the Paper Calls Out

**Open Question 1**
Can fine-tuning open-source LMMs on TimeTravel significantly narrow the performance gap with closed-source models like GPT-4o-0806?
- Basis in paper: The authors note that "open-source models such as Qwen-2.5-VL showed strong performance, highlighting potential for further refinement" and that "ongoing improvements in open-source models highlight opportunities for fine-tuning and dataset expansion."
- Why unresolved: No fine-tuning experiments were conducted; the paper only reports zero-shot evaluation results
- What evidence would resolve it: Fine-tune Qwen-2.5-VL or Llama-3.2-Vision on a subset of TimeTravel and evaluate on held-out artifacts

**Open Question 2**
How does the use of GPT-4o-generated descriptions as ground truth affect benchmark validity when evaluating GPT-4o itself?
- Basis in paper: The methodology states that GPT-4o was used to generate detailed textual descriptions, which were then "refined by experts." However, GPT-4o is also the top-performing model being evaluated against these same descriptions
- Why unresolved: Potential circularity in using a model to generate ground truth that is then used to evaluate itself, even with expert refinement
- What evidence would resolve it: Compare expert-only ground truth descriptions against GPT-4o-generated ones and assess evaluation score differences across models

**Open Question 3**
What specific failure modes occur when models analyze artifacts from low-representation regions like Central America (5% of samples)?
- Basis in paper: Table 3 shows consistently lower LLM-Judge scores for Central America across all models, yet no error analysis or failure mode categorization is provided
- Why unresolved: The paper reports aggregate scores but does not investigate why certain regions underperform or what types of errors predominate
- What evidence would resolve it: Qualitative error analysis on Central American artifacts comparing model outputs against expert descriptions to identify systematic failure patterns

## Limitations
- Geographic distribution bias with significant imbalance (Greece 18% vs. Central America 5%) may artificially inflate performance on well-represented regions
- LLM-Judge metric lacks validation against human expert judgments, raising questions about its reliability as an evaluation tool
- Expert verification process details are insufficient (criteria, inter-rater reliability, multiple reviewers per description)

## Confidence

**High Confidence:** The benchmark construction methodology (data collection, GPT-4o augmentation, expert verification) is well-documented and reproducible. The multi-metric evaluation framework combining traditional and semantic metrics is methodologically sound.

**Medium Confidence:** The core claim that TimeTravel enables evaluation of LMMs on historical artifacts is supported, but the specific performance rankings depend on unverified LLM-Judge reliability and the quality of GPT-4o-generated ground truth.

**Low Confidence:** The assertion that observed regional performance differences directly reflect training data distribution rather than task difficulty or cultural proximity requires additional validation. The British Isles performance gap could stem from multiple factors beyond data scarcity.

## Next Checks

1. **LLM-Judge Validation:** Select 200 samples spanning high and low-scoring regions, have 3 independent domain experts rate descriptions on factual accuracy and cultural sensitivity, then compute correlation with LLM-Judge scores. Target minimum Pearson correlation of 0.7.

2. **Ground Truth Quality Audit:** Sample 100 expert-verified descriptions and trace them back to their original GPT-4o outputs. Categorize rejection reasons (historical inaccuracy, cultural insensitivity, irrelevance) to quantify augmentation success rate and identify systematic generation failures.

3. **Bias Amplification Analysis:** For each of the 10 regions, compute the ratio of ground truth samples to cultural representation in model pretraining corpora (estimated from training announcements). Test whether performance correlates with this ratio to distinguish data exposure effects from model capabilities.