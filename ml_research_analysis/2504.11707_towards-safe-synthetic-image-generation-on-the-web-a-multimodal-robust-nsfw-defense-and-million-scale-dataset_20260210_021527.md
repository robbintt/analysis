---
ver: rpa2
title: 'Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW
  Defense and Million Scale Dataset'
arxiv_id: '2504.11707'
source_url: https://arxiv.org/abs/2504.11707
tags:
- nsfw
- image
- multimodal
- text
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of preventing the generation of
  Not-Safe-For-Work (NSFW) content in AI image generation models, which are vulnerable
  to multimodal adversarial attacks that bypass existing text and image filters. To
  tackle this, the authors create a large-scale dataset (NSFWCorpus) containing over
  1 million text-image pairs, including both real and adversarial NSFW examples.
---

# Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset

## Quick Facts
- arXiv ID: 2504.11707
- Source URL: https://arxiv.org/abs/2504.11707
- Reference count: 19
- Primary result: A multimodal defense model that integrates text and image embeddings reduces attack success rates for multimodal adversarial NSFW prompts from ~80% to <4%

## Executive Summary
This work addresses the vulnerability of AI image generation models to multimodal adversarial attacks that bypass single-modality NSFW filters. The authors develop a large-scale NSFWCorpus dataset with over 1 million text-image pairs, including adversarial examples, and propose a multimodal defense that uses cross-attention between text and image embeddings to jointly classify content. Their approach significantly outperforms existing defenses, reducing attack success rates by 20-fold while improving accuracy and recall on human-generated prompts. The key insight is that integrating both modalities with context-aware filtering provides robust protection against sophisticated adversarial attacks.

## Method Summary
The authors created NSFWCorpus, a million-scale dataset containing real and adversarial NSFW text-image pairs, to train and evaluate robust defenses. They proposed a multimodal defense model that uses contrastive learning to align text and image embeddings, then applies cross-attention mechanisms to jointly classify content as safe or NSFW. The defense operates in two stages: first, individual text and image classifiers filter obvious cases; second, the multimodal component analyzes cases where the two modalities conflict or where content is ambiguous. This approach addresses the limitation of existing single-modality defenses that can be bypassed by adversarial multimodal attacks.

## Key Results
- Attack Success Rate (ASR) for multimodal attacks reduced from ~80% to <4% with the proposed defense
- F1-score improved from 72% (existing methods) to 94% on human-generated prompts
- Precision increased from 71% to 92% while recall improved from 72% to 95%
- The multimodal defense consistently outperformed both text-only and image-only approaches across all attack types

## Why This Works (Mechanism)
The multimodal defense works by leveraging cross-modal information that adversarial attacks cannot easily manipulate simultaneously. While attackers can craft text prompts that bypass text filters or use adversarial images that fool image classifiers, coordinating both modalities in a way that fools the joint cross-attention mechanism is significantly more difficult. The model learns to detect semantic inconsistencies between text descriptions and visual content, identifying cases where attackers attempt to hide NSFW content through conflicting modalities. This contextual understanding goes beyond simple pattern matching in individual modalities.

## Foundational Learning
- **Multimodal embeddings**: Text and image representations in a shared semantic space; needed to compare and contrast content across modalities; quick check: measure alignment scores between matched text-image pairs
- **Cross-attention mechanisms**: Allows the model to focus on relevant parts of both text and image when making decisions; needed to identify semantic conflicts between modalities; quick check: visualize attention weights for conflicting vs. consistent pairs
- **Contrastive learning**: Trains embeddings to bring positive pairs closer while pushing negative pairs apart; needed to create robust representations resistant to adversarial manipulation; quick check: test embedding distances for adversarial vs. clean examples
- **Adversarial attack taxonomy**: Understanding different attack vectors (text-only, image-only, multimodal); needed to evaluate defense comprehensively; quick check: verify attack success rates across different attack types
- **Zero-shot classification**: Ability to classify unseen content without fine-tuning; needed for practical deployment on diverse content; quick check: test performance on out-of-distribution examples
- **Cross-modal alignment**: Ensuring text and image embeddings correspond semantically; needed for accurate joint classification; quick check: measure retrieval accuracy for matched pairs

## Architecture Onboarding

Component map: Text encoder -> Image encoder -> Embedding alignment -> Cross-attention fusion -> NSFW classifier

Critical path: Input text and image → separate encoders → aligned embeddings → cross-attention layer → joint classification → safe/NSFW output

Design tradeoffs: The model trades computational efficiency for robustness, as multimodal processing requires more resources than single-modality approaches. However, this is justified by the significant improvement in defense capability against sophisticated attacks.

Failure signatures: The defense may struggle with cases where both modalities contain subtle NSFW content that is not immediately apparent, or when adversarial attacks successfully manipulate both modalities in coordinated ways that maintain semantic consistency.

First experiments:
1. Test baseline performance on clean, non-adversarial data to establish accuracy metrics
2. Evaluate defense against simple text-only adversarial attacks to verify unimodal protection
3. Test against multimodal attacks to measure the 20-fold reduction in attack success rate

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the research raises several important considerations for future work, including the need to test against more sophisticated attack vectors and evaluate performance across diverse cultural contexts.

## Limitations
- Dataset construction relies heavily on web-scraped NSFW content, potentially introducing biases
- Evaluation focuses primarily on synthetic adversarial examples rather than real-world attack scenarios
- Defense adds computational overhead that could impact user experience in production systems
- Limited discussion of false positive rates in real-world deployment scenarios

## Confidence
- 20-fold reduction in ASR: High
- Improved F1-score to 94%: High
- Multimodal approach outperforms unimodal: High
- Real-world deployment considerations: Medium
- Cultural bias evaluation: Low

## Next Checks
1. Conduct user studies with human evaluators to assess false positive rates and usability impacts in real-world deployment scenarios
2. Test the defense against more sophisticated attack vectors including image-only attacks and semantic-level manipulations
3. Evaluate performance across diverse cultural contexts and content types beyond the current dataset scope