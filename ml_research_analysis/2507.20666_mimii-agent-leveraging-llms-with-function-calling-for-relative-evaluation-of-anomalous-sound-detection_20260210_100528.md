---
ver: rpa2
title: 'MIMII-Agent: Leveraging LLMs with Function Calling for Relative Evaluation
  of Anomalous Sound Detection'
arxiv_id: '2507.20666'
source_url: https://arxiv.org/abs/2507.20666
tags:
- machine
- anomalous
- detection
- anomaly
- sound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MIMII-Agent, a method for generating machine-type-specific
  anomalies to evaluate the relative performance of unsupervised anomalous sound detection
  (UASD) systems across different machine types. The core idea leverages large language
  models (LLMs) with function calling to interpret textual descriptions of faults
  and automatically select audio transformation functions, converting normal machine
  sounds into diverse and plausible anomalous sounds.
---

# MIMII-Agent: Leveraging LLMs with Function Calling for Relative Evaluation of Anomalous Sound Detection

## Quick Facts
- arXiv ID: 2507.20666
- Source URL: https://arxiv.org/abs/2507.20666
- Reference count: 34
- Primary result: LLMs with function calling generate realistic synthetic anomalies that preserve relative detection difficulty rankings across machine types

## Executive Summary
This paper introduces MIMII-Agent, a method that uses large language models with function calling to generate machine-type-specific synthetic anomalies for evaluating unsupervised anomalous sound detection (UASD) systems. The approach interprets textual fault descriptions to automatically select audio transformation functions, converting normal machine sounds into plausible anomalous sounds without requiring anomalous training data. Experiments demonstrate that synthetic anomalies achieve consistent relative detection difficulty rankings across five machine types that align with real anomaly rankings, enabling relative evaluation of UASD systems using only normal sound data.

## Method Summary
MIMII-Agent generates synthetic anomalies by combining metadata-derived captions, LLM-based function selection, and DSP transformations. First, Flan-T5 converts metadata to captions. GPT-4o then selects appropriate audio transformation functions (e.g., squeaking, rattling) based on the caption and a predefined function list. The selected DSP functions modify normal sounds generated by MIMII-Gen latent diffusion model. An autoencoder trained on normal sounds from all machine types evaluates both synthetic and real anomalies using MSE and Mahalanobis distance, with AUC scores compared across machine types to establish relative difficulty rankings.

## Key Results
- Synthetic anomalies achieve AUC scores of 0.78-0.92, closely matching real anomaly rankings
- Relative detection difficulty rankings (Fan > Gearbox > Bearing > Slide rail > Valve) are preserved across both synthetic and real anomalies
- LLM-based function selection significantly outperforms random selection in producing correlated rankings
- Manual keyword-mapping achieves similar rankings to GPT-4o, suggesting scalability rather than superior selection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-based function calling enables contextually appropriate anomaly sound synthesis by mapping textual fault descriptions to machine-specific audio transformations
- **Mechanism:** The LLM receives metadata-derived captions and selects from a predefined library of DSP functions based on implicit world knowledge about which fault signatures are plausible for each machine type
- **Core assumption:** LLMs encode sufficient "common sense" about mechanical faults to select transformations that produce acoustically realistic anomalies
- **Evidence anchors:** Ablation study shows GPT-4o produces AUC rankings aligned with real anomalies while random selection shows no correlation; ~0.12-0.19 AUC gap between GPT-4o and random selection for some machine types
- **Break condition:** If LLM function selection degrades to near-random performance with ranking mismatch

### Mechanism 2
- **Claim:** Relative evaluation rankings persist across synthetic and real anomalies because transformation selection preserves machine-type-specific difficulty patterns
- **Mechanism:** Both MSE-based and Mahalanobis-based AUC scores produce identical rankings across five machine types for synthetic and real anomalies
- **Core assumption:** The relative difficulty of detecting anomalies in different machine types is determined by intrinsic acoustic characteristics
- **Evidence anchors:** Identical rankings (1-5) across both scoring methods and both data sources; no corpus evidence directly validates this ranking preservation
- **Break condition:** If synthetic anomaly AUC scores converge across machine types or rankings invert relative to real anomalies

### Mechanism 3
- **Claim:** DSP-based transformation functions inject fault signatures that autoencoder-based UASD systems detect through reconstruction error patterns
- **Mechanism:** Transformations create spectrogram patterns the autoencoder cannot reconstruct, producing elevated anomaly scores proportional to transformation intensity
- **Core assumption:** DSP transformations approximate the spectral characteristics of real mechanical faults sufficiently to produce correlated detection scores
- **Evidence anchors:** Autoencoder architecture processes 128 mel-bin spectrograms; 10 transformation types with fault mappings; "An Enhanced Audio Feature Tailored for Anomalous Sound Detection" suggests feature representation is critical
- **Break condition:** If transformations produce sounds that are either uniformly trivial to detect or indistinguishable from normal

## Foundational Learning

- **Concept: LLM Function Calling**
  - Why needed here: Core mechanism for autonomous function selection based on natural language context
  - Quick check question: Can you explain how an LLM determines which function to call given a user prompt and a function schema?

- **Concept: Unsupervised Anomaly Detection (Autoencoder-based)**
  - Why needed here: The evaluation target system; understanding reconstruction error as anomaly signal is essential
  - Quick check question: Why does an autoencoder trained only on normal sounds produce higher reconstruction errors for anomalies?

- **Concept: AUC (Area Under ROC Curve) for Detection Evaluation**
  - Why needed here: Primary metric for both absolute and relative evaluation; interpreting ranking preservation
  - Quick check question: If AUC scores are 0.92 for Machine A and 0.78 for Machine B, which machine's anomalies are harder to detect?

## Architecture Onboarding

- **Component map:** Metadata -> Flan-T5 caption generator -> GPT-4o with function schema -> DSP transformation functions -> anomalous audio -> Autoencoder UASD -> MSE/Mahalanobis scoring -> AUC calculation

- **Critical path:** 1. Caption generation from metadata, 2. LLM function selection via API call, 3. DSP transformation application, 4. AUC ranking comparison across machine types

- **Design tradeoffs:** Synthetic anomalies yield higher AUCs than real anomalies, acceptable for relative but not absolute benchmarking; manual mapping achieves similar rankings to GPT-4o suggesting scalability value; limited sample size (50 synthetic samples) may affect statistical robustness

- **Failure signatures:** Random function selection produces inverted rankings; synthetic AUCs exceeding 0.95 universally indicate overly severe transformations; inconsistent rankings between MSE and Mahalanobis scoring suggest metric-specific artifacts

- **First 3 experiments:** 1. Replicate Table 3 ranking comparison on held-out machine type, 2. Test alternative LLM backends to measure sensitivity to world knowledge quality, 3. Vary transformation intensity parameters to map relationship between synthetic severity and AUC scores

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the method be extended to predict absolute performance metrics rather than just relative rankings?
  - Basis: Synthetic anomalies consistently yield higher AUCs (0.78â€“0.92) compared to real anomalies
  - Why unresolved: Current approach preserves order but generates anomalies systematically easier to detect
  - Evidence: Developing calibration mechanism for DSP intensity that aligns synthetic AUC distributions with real-world baselines

- **Open Question 2:** Do the relative evaluation results generalize to other UASD architectures beyond autoencoders?
  - Basis: Experiments on single autoencoder architecture despite listing alternatives (GMM, embedding-similarity)
  - Why unresolved: Correlation may depend on specific sensitivity of autoencoder reconstruction errors
  - Evidence: Replicating ranking correlation experiment using distinct detector architectures (flow-based or metric learning models)

- **Open Question 3:** To what extent can improved prompt design and domain-specific constraints enhance the realism of generated anomalies?
  - Basis: Identified as future step in Section 5 (Conclusion)
  - Why unresolved: Authors identify this as future step but do not quantify improvement
  - Evidence: Ablation studies comparing generic LLM prompts against prompts augmented with physics-based acoustic constraints

## Limitations
- Synthetic anomalies systematically easier to detect than real anomalies (higher AUCs), limiting absolute benchmarking utility
- Method validated only on 5 specific machine types from MIMII-DG dataset without cross-machine-type generalization testing
- Limited sample size (50 synthetic anomalies per machine type) may not capture full variability of anomaly difficulty

## Confidence
- **High confidence:** Relative ranking preservation between synthetic and real anomalies is demonstrated with consistent results across two scoring methods
- **Medium confidence:** LLM-based contextual understanding provides meaningful advantage over random selection, though manual mapping achieves similar results
- **Low confidence:** Method's generalizability to unseen machine types and operating conditions without retraining captions or transformation functions

## Next Checks
1. Test ranking preservation on a held-out machine type not in the original five to validate generalizability
2. Perform prompt ablation study varying system prompts to measure sensitivity to LLM world knowledge quality
3. Scale synthetic sample size to 500+ per machine type to assess statistical robustness of ranking correlations