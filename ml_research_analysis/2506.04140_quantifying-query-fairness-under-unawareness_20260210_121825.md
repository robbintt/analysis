---
ver: rpa2
title: Quantifying Query Fairness Under Unawareness
arxiv_id: '2506.04140'
source_url: https://arxiv.org/abs/2506.04140
tags:
- fairness
- query
- quantification
- methods
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of estimating fairness in search
  rankings when sensitive attribute labels are unavailable, a scenario termed "fairness
  under unawareness." The authors propose using quantification techniques, which estimate
  the relative frequencies of classes in unlabeled data, as an alternative to standard
  classifiers that can produce unreliable fairness assessments due to dataset shift.
  Their method involves training a classifier offline and then using a smaller, query-specific
  correction pool to adjust prevalence estimates at query time, making the approach
  robust to sample selection bias.
---

# Quantifying Query Fairness Under Unawareness

## Quick Facts
- arXiv ID: 2506.04140
- Source URL: https://arxiv.org/abs/2506.04140
- Reference count: 11
- Primary result: Quantification-based methods (KDEy) outperform classifiers for fairness estimation under unawareness, especially with non-binary sensitive attributes

## Executive Summary
This paper addresses the problem of estimating fairness in search rankings when sensitive attribute labels are unavailable. The authors propose using quantification techniques, which estimate the relative frequencies of classes in unlabeled data, as an alternative to standard classifiers that can produce unreliable fairness assessments due to dataset shift. Their method involves training a classifier offline and then using a smaller, query-specific correction pool to adjust prevalence estimates at query time, making the approach robust to sample selection bias. Extensive experiments on the TREC 2022 Fair Ranking Track dataset with multiple sensitive attributes show that their quantification-based method outperforms existing baselines in predicting query fairness metrics.

## Method Summary
The authors propose a two-phase approach to fairness estimation under unawareness. First, they train a classifier offline on labeled data to predict sensitive attribute values. At query time, they apply this classifier to the retrieved documents and use quantification techniques to estimate the distribution of sensitive attributes in the ranking. The key innovation is the use of a correction pool - a small set of documents with known sensitive attribute labels that is specific to each query. This correction pool is used to adjust the prevalence estimates, making them robust to the selection bias introduced by the retrieval model. The method handles non-binary sensitive attributes and provides a protocol for measuring fairness across multiple queries and groups.

## Key Results
- KDEy quantification method achieves lowest average RAE (0.146) compared to classifiers and other quantification methods
- KDEy outperforms all methods in predicting rKL fairness metric with RAE of 0.048
- Method successfully handles non-binary sensitive attributes (geographic location, age groups) unlike previous approaches

## Why This Works (Mechanism)
The approach works by addressing the fundamental problem of dataset shift in fairness estimation. When sensitive attribute labels are unavailable at query time, classifiers trained offline suffer from selection bias because the distribution of documents retrieved by a search engine differs from the training data distribution. Quantification techniques, particularly KDEy, provide better prevalence estimates by modeling the underlying data distribution more effectively. The correction pool serves as a calibration mechanism, allowing the system to adjust for query-specific biases and improve the accuracy of fairness estimates. This combination of offline training, quantification, and online correction enables robust fairness assessment even when sensitive attributes are not directly observable.

## Foundational Learning

**Quantification** - The task of estimating the prevalence of classes in a set of unlabeled data instances. Needed because standard classification doesn't directly provide class distribution estimates, which are essential for fairness metrics. Quick check: Can you explain the difference between classification and quantification in one sentence?

**Dataset Shift** - The phenomenon where the distribution of data changes between training and test conditions. Critical because search rankings retrieved by a model differ from the training corpus, causing classifiers to produce biased estimates. Quick check: Why would a classifier trained on general web data perform poorly on search rankings?

**Selection Bias** - The bias introduced when the sample of data is not representative of the population due to the sampling mechanism. Important because search engines retrieve documents based on relevance, not representativeness, skewing sensitive attribute distributions. Quick check: How does selection bias affect fairness estimation differently than accuracy estimation?

**KDEy (Kernel Density Estimation with y-values)** - A quantification method that estimates class distributions by modeling the joint density of features and class labels. Needed because it provides more accurate prevalence estimates than standard classifiers under distribution shift. Quick check: What advantage does KDEy have over simple classifier-based prevalence estimation?

**rKL (relative Kullback-Leibler divergence)** - A metric for measuring disparity in representation between sensitive groups in search rankings. Essential for quantifying fairness in information retrieval contexts. Quick check: How does rKL differ from absolute representation metrics?

## Architecture Onboarding

Component map: Training data -> Offline classifier -> Query documents -> Sensitive attribute prediction -> Quantification algorithm -> Correction pool -> Adjusted prevalence estimates -> Fairness metrics

Critical path: The key steps are training a robust classifier on labeled data, applying it to query results, using quantification to estimate class distributions, and applying correction pool adjustments. The correction pool is the most critical component as it enables robustness to selection bias.

Design tradeoffs: The main tradeoff is between the size of the correction pool (affecting accuracy) and the cost of obtaining labeled data for each query. Larger pools improve accuracy but increase costs. The choice of quantification algorithm (KDEy vs others) involves balancing accuracy against computational complexity.

Failure signatures: Poor performance occurs when the correction pool is too small to provide reliable adjustments, when the offline classifier is poorly calibrated, or when the quantification algorithm cannot handle the specific data distribution. Failure manifests as high RAE values and unstable fairness estimates across different rank cutoffs.

First experiments:
1. Run KDEy quantification on a small subset of TREC data with known labels to verify basic functionality
2. Compare classifier-based vs quantification-based prevalence estimates on a held-out validation set
3. Test the impact of correction pool size on RAE by systematically varying pool size and measuring performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a normalized-discounted variant of Relative Absolute Error (RAE) provide a more reliable measure for fairness evaluation than existing metrics like rKL?
- Basis in paper: [explicit] The authors note a misalignment between RAE and rKL metrics, suggesting the analysis of a normalized-discounted RAE variant as future work (Sections 5.3, 6).
- Why unresolved: The paper currently leaves the formal definition and validation of this specific metric variant for future investigations.
- What evidence would resolve it: Empirical results showing the new metric better correlates with fairness goals in multiclass settings compared to standard metrics.

### Open Question 2
- Question: How can quantification-based Query Fairness Estimation methods be enhanced to provide confidence intervals for their predictions?
- Basis in paper: [explicit] The authors list "conveying uncertainty of fairness estimates" as an important problem they did not address (Section 6).
- Why unresolved: The proposed system currently outputs point estimates for fairness scores without indicating the statistical certainty of those estimates.
- What evidence would resolve it: A method modification that outputs valid confidence intervals and rigorous testing of their coverage probability.

### Open Question 3
- Question: How robust is the proposed approach when applied to collections where group prevalence naturally varies between training and test conditions?
- Basis in paper: [explicit] The authors state an interest in exploring "different collections where the group prevalence may have naturally varied" (Section 6).
- Why unresolved: The current experiments rely on the TREC 2022 collection where distribution shifts are artificially controlled or induced by the retrieval model.
- What evidence would resolve it: Evaluation on datasets exhibiting organic dataset shift (e.g., temporal drift) demonstrating that the correction pool method remains stable.

## Limitations

- Dependency on correction pool availability at query time may limit real-world applicability where data collection is costly or privacy-constrained
- Assumption that correction pool, though smaller, is representative enough to correct for selection bias may not hold across diverse query distributions
- Results demonstrated primarily on TREC 2022 dataset; generalizability to other domains or less structured data is not established

## Confidence

- Core claim (quantification outperforms classifiers under unawareness): High
- Robustness to varying correction pool sizes and rank cutoffs: Medium
- Novelty and significance of handling non-binary sensitive attributes: High

## Next Checks

1. Test the method on datasets from different domains (e.g., e-commerce, academic search) to assess generalizability
2. Evaluate the impact of correction pool size below the smallest tested threshold to identify the minimum viable pool size
3. Investigate the sensitivity of fairness estimates to the choice of quantification algorithm beyond KDEy, including deep learning-based methods