---
ver: rpa2
title: 'LMK > CLS: Landmark Pooling for Dense Embeddings'
arxiv_id: '2601.21525'
source_url: https://arxiv.org/abs/2601.21525
tags:
- pooling
- ndcg
- mean
- sequence
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Landmark (LMK) pooling, a novel approach
  for dense embedding that improves long-context generalization while maintaining
  strong short-context performance. Unlike traditional [CLS] or mean pooling methods,
  LMK pooling inserts special tokens at regular intervals across the input sequence
  and aggregates their embeddings via mean pooling.
---

# LMK > CLS: Landmark Pooling for Dense Embeddings

## Quick Facts
- arXiv ID: 2601.21525
- Source URL: https://arxiv.org/abs/2601.21525
- Reference count: 40
- Landmark pooling improves long-context generalization while maintaining short-context performance

## Executive Summary
This paper introduces Landmark (LMK) pooling, a novel approach for dense embedding that addresses the limitations of traditional [CLS] and mean pooling methods in long-context scenarios. By inserting special tokens at regular intervals across the input sequence and aggregating their embeddings via mean pooling, LMK pooling mitigates the positional bias of [CLS] pooling and the feature dilution of mean pooling. The method is evaluated across multiple languages, encoder architectures, and benchmarks, showing consistent improvements in long-context retrieval tasks while maintaining strong performance on short-context benchmarks.

## Method Summary
LMK pooling inserts special `[LMK]` tokens (or reuses `[SEP]`) at regular intervals (granularity) throughout the input sequence. The encoder processes this modified sequence, and the final embedding is computed by taking the mean of the contextualized embeddings corresponding to these landmark token positions. The method is evaluated using contrastive InfoNCE loss with hard negatives and distillation scores from teacher models, fine-tuned on MS MARCO passage and document collections with batch sizes up to 2048 tokens.

## Key Results
- LMK pooling achieves superior long-context generalization on MLDR benchmark (up to 32k tokens)
- Maintains or slightly improves short-context retrieval performance on BEIR and MTEB-v2
- Enables robust extrapolation beyond training sequence lengths with minimal computational overhead
- Shows consistent improvements across multiple encoder architectures and multilingual settings

## Why This Works (Mechanism)
LMK pooling works by creating multiple anchor points throughout the document rather than relying on a single [CLS] token at the beginning. This distributed representation captures information from different segments of the document, reducing the positional bias inherent in [CLS] pooling and avoiding the feature dilution that occurs when averaging all token embeddings. The landmark tokens act as summary points that are strategically placed to capture local context while maintaining global coherence through the transformer's attention mechanism.

## Foundational Learning
- **Contrastive Learning**: The training objective (InfoNCE loss) requires understanding positive/negative pairs and temperature scaling - needed for the fine-tuning procedure described
- **Dense Retrieval**: Understanding how fixed-size embeddings are used for semantic search and ranking - essential for interpreting the benchmark results
- **Sequence Length Extrapolation**: The concept of models trained on short sequences performing well on longer sequences - central to LMK's value proposition
- **Hard Negative Mining**: Using difficult negative examples during training to improve model robustness - mentioned in the training setup
- **Knowledge Distillation**: Leveraging teacher model outputs (scores) to guide student training - used for generating distillation scores

Quick check: Can you explain why mean pooling dilutes features while LMK pooling preserves them?

## Architecture Onboarding

### Component Map:
Tokenizer (modified with LMK tokens) -> Transformer Encoder -> LMK Pooling Head

### Critical Path:
**Tokenizer -> Encoder -> LMK Pooling Head.** The placement of landmark tokens in the tokenizer and their subsequent extraction and averaging in the pooling head are the critical modifications. The encoder itself remains architecturally unchanged.

### Design Tradeoffs:
- **Granularity vs. Overhead**: Smaller intervals (e.g., 32 tokens) create denser summaries but add more special tokens, increasing sequence length and compute. Larger intervals (e.g., 256 tokens) are more efficient but may miss fine-grained details.
- **Fixed vs. Variable Chunking**: Fixed intervals are simple but can introduce bias. Variable chunking (randomly sampling during training) makes the model more robust but adds a hyperparameter to tune.

### Failure Signatures:
- **CLS-like Performance on Long Documents**: If LMK underperforms like [CLS] on long-context tasks, verify landmark tokens are being inserted and used for pooling
- **Degraded Short-Context Performance**: If short-context performance drops significantly, the granularity may be too coarse or the model hasn't learned to rely on landmark tokens effectively

### First 3 Experiments:
1. **Baseline Reproduction**: Implement LMK pooling with ModernBERT-base and evaluate on BEIR to verify it doesn't degrade short-context performance compared to reported baselines
2. **Long-Context Extrapolation Stress Test**: Fine-tune on MS MARCO passages (512 MSL), then evaluate on MLDR at progressively longer lengths (1k, 2k, 4k, 8k, 16k tokens). Plot NDCG@10 curve to confirm LMK's superior extrapolation
3. **Ablation on Granularity**: Train separate models with fixed intervals (32, 64, 128, 256 tokens) and compare performance across short and long-context tasks to find optimal trade-off point

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What causes the directional bias observed in LMK embeddings, where they show higher affinity for left neighboring chunks than right neighbors?
- **Basis in paper**: Section 6.4 states LMK embeddings show higher affinity for left neighboring chunks than for right neighbors, indicating a directional bias
- **Why unresolved**: The paper identifies and quantifies this bias (â‰¥58% Hit@10 for left chunks) but offers no mechanistic explanation for its origin
- **What evidence would resolve it**: Ablation studies analyzing attention patterns to LMK tokens across layers; experiments with bidirectional or right-to-left positional encodings

### Open Question 2
- **Question**: Why does long-context training improve CLS pooling on in-domain benchmarks but fail to close the performance gap on out-of-domain long-context tasks?
- **Basis in paper**: Table 4 and Section 6.2 show increasing training sequence length improves CLS on MLDR but a substantial performance gap persists on Multi-EURLEX and LongEmbed
- **Why unresolved**: The paper demonstrates this phenomenon empirically but does not investigate whether the issue stems from distribution shift or architectural limitations
- **What evidence would resolve it**: Controlled experiments varying training data composition while holding sequence length constant; analysis of attention distribution changes with domain

### Open Question 3
- **Question**: Under what conditions does LMK pooling benefit from retrieval-oriented pretraining (RetroMAE), and why is this benefit architecture-dependent?
- **Basis in paper**: Section 6.3 notes LMK RetroMAE pretraining yields consistent performance gains on gte-en-mlm-base, but for ModernBERT-base both pooling strategies yield comparable results
- **Why unresolved**: The architectural differences between these models that cause divergent behavior are not analyzed
- **What evidence would resolve it**: Systematic comparison across architectures with controlled attention pattern variations; analysis of decoder reconstruction quality with LMK vs. CLS during pretraining

## Limitations
- Evaluation scope remains primarily focused on English and multilingual retrieval tasks, with limited demonstration on non-retrieval applications
- Analysis of landmark token placement strategies is limited to uniform intervals and random sampling from fixed sets
- Computational overhead analysis lacks detailed empirical measurements of training/inference time and memory usage across hardware configurations

## Confidence

**High Confidence**: Core claim that LMK pooling provides superior long-context generalization while maintaining short-context performance - supported by consistent improvements across multiple benchmarks

**Medium Confidence**: Claim of minimal computational overhead - lacks detailed runtime measurements to quantify this precisely

**Medium Confidence**: Multilingual generalization claim - improvements shown on BGE-m3 but dataset composition and specific languages evaluated are not fully detailed

## Next Checks

1. **Runtime and Memory Profiling**: Conduct detailed empirical analysis of training and inference time, as well as peak memory usage, for LMK pooling compared to CLS and mean pooling across different hardware configurations (A100, H100 GPUs) and batch sizes

2. **Non-Retrieval Task Evaluation**: Evaluate LMK pooling on diverse non-retrieval tasks such as text classification (sentiment analysis, topic classification), clustering, and semantic textual similarity to assess generalizability beyond retrieval

3. **Advanced Landmark Placement Strategies**: Investigate alternative placement strategies beyond uniform intervals and random sampling, including content-aware placement (inserting after salient phrases or topic shifts) or dynamic positioning based on model's internal representations