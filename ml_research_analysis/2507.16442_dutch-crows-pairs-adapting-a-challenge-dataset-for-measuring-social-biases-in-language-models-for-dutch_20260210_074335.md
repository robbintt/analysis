---
ver: rpa2
title: 'Dutch CrowS-Pairs: Adapting a Challenge Dataset for Measuring Social Biases
  in Language Models for Dutch'
arxiv_id: '2507.16442'
source_url: https://arxiv.org/abs/2507.16442
tags:
- bias
- language
- dutch
- crows-pairs
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Dutch CrowS-Pairs, a benchmark dataset
  for measuring social biases in Dutch language models. It adapts the original CrowS-Pairs
  to Dutch, covering nine bias categories with 1463 sentence pairs.
---

# Dutch CrowS-Pairs: Adapting a Challenge Dataset for Measuring Social Biases in Language Models for Dutch

## Quick Facts
- arXiv ID: 2507.16442
- Source URL: https://arxiv.org/abs/2507.16442
- Reference count: 9
- Dutch CrowS-Pairs dataset of 1,463 sentence pairs adapted for Dutch, covering nine bias categories

## Executive Summary
This paper introduces the Dutch CrowS-Pairs, a benchmark dataset for measuring social biases in Dutch language models. It adapts the original CrowS-Pairs to Dutch, covering nine bias categories with 1463 sentence pairs. The dataset is used to evaluate bias in both masked and autoregressive language models, including Dutch models like BERTje, RobBERT, and GEITje, as well as French and English counterparts. Results show that English models exhibit the most bias, Dutch models the least, and that assigning personas to models can significantly alter bias levels. The study highlights the variability of bias across languages and contexts, emphasizing the role of cultural and linguistic factors in shaping model biases.

## Method Summary
The study creates Dutch CrowS-Pairs by translating and culturally adapting the English CrowS-Pairs dataset using Google Translate followed by native speaker review. The adaptation process involved adjusting cultural references (e.g., "Mexican" → "Moroccan") and units (imperial → metric). The dataset covers nine bias categories with 1,463 sentence pairs. For masked language models (MLMs), bias is measured using pseudo-log-likelihood scoring that compares the probability of stereotypical versus less-stereotypical sentence variants. For autoregressive language models (ARLMs), bias is measured through prompt-based selection between stereotypical and anti-stereotypical options, with experiments including persona conditions ("good," "bad," or neutral). The evaluation compares bias across Dutch, French, and English models, including both BERT and RoBERTa architectures.

## Key Results
- Dutch language models show the least bias (average 54.82%), while English models exhibit the highest bias
- RoBERTa-based models consistently show more bias than BERT-based models across all languages
- Persona-based prompting significantly modulates bias expression, with "bad" personas increasing bias (up to 94.46%) and "good" personas decreasing it (as low as 22.21%)
- Dutch models exhibit lowest bias in Religion and Disability categories compared to French and English models

## Why This Works (Mechanism)

### Mechanism 1: Training Data Scale and Diversity Correlate with Bias Magnitude
- Claim: Models trained on larger, more diverse internet corpora exhibit higher bias scores, all else being equal.
- Mechanism: Larger training datasets increase exposure to stereotypical language patterns prevalent in web-scale text; the paper notes RoBERTa's 161GB training corpus versus BERT's 13GB, with RoBERTa showing higher bias (65.14% vs 61.45%).
- Core assumption: Internet-scale corpora contain proportionally more biased content than curated sources like Wikipedia.
- Evidence anchors:
  - [abstract]: "English models exhibited the highest bias overall, while Dutch models showed the least."
  - [section 4.1]: "RoBERTa's training involved 161GB of diverse text sources... compared to BERT's 13GB... A larger, more heterogeneous corpus likely increased exposure to biased language."
  - [corpus]: Limited direct corpus evidence on training scale-bias correlation; related work (Blind Men and the Elephant) critiques benchmark inconsistencies but does not validate this mechanism.
- Break condition: If curated high-quality data at scale produces lower bias than smaller uncurated datasets, the mechanism would not hold.

### Mechanism 2: RoBERTa Architecture Exhibits Higher Bias Propensity Than BERT
- Claim: RoBERTa-based models consistently score higher on bias metrics than BERT-based models across languages.
- Mechanism: Architectural differences (byte-pair encoding, larger mini-batches, longer training, dynamic masking) may affect how stereotypes are encoded and retrieved during pseudo-log-likelihood scoring.
- Core assumption: The RoBERTa optimization strategy amplifies learned statistical associations, including stereotypical ones.
- Evidence anchors:
  - [abstract]: "RoBERTa-based models consistently demonstrated more bias than BERT-based models."
  - [section 4.1]: "CamemBERT (RoBERTa-based) being more biased than FlauBERT (BERT-based)... These patterns suggest that RoBERTa-based architectures, regardless of language, tend to express stronger bias."
  - [corpus]: Corpus evidence is weak; no identified neighbors directly validate RoBERTa vs. BERT architectural bias differences.
- Break condition: If BERT-based models fine-tuned on equivalent data volumes and diversity match or exceed RoBERTa bias levels, architecture alone is not causal.

### Mechanism 3: Persona Prompting Dynamically Modulates Bias Expression
- Claim: Assigning social roles ("good" vs. "bad" personas) to autoregressive LLMs significantly alters bias expression.
- Mechanism: Models adapt outputs based on implicit associations between persona descriptors and social norms; "bad" personas increase stereotypical selections (up to 94.46% for Mistral), "good" personas reduce them (as low as 22.21%).
- Core assumption: Models have internalized contextual mappings between persona labels and socially acceptable/inappropriate language patterns.
- Evidence anchors:
  - [abstract]: "persona-based prompting significantly influenced bias expression, with 'bad' personas increasing and 'good' personas decreasing bias."
  - [section 4.2]: "When asked to respond as a 'bad' person, both models consistently selected the more stereotypical sentence. In contrast, when prompted to act as a 'good' person, they often reversed this preference."
  - [corpus]: Related work (Explicit vs. Implicit) examines implicit bias but does not directly validate persona-based modulation.
- Break condition: If persona effects are inconsistent across model families, languages, or prompt phrasings, the mechanism may not generalize.

## Foundational Learning

- Concept: **Pseudo-log-likelihood (PLL) scoring for masked language models**
  - Why needed here: Understanding how the paper quantifies bias in BERT-family models; PLL approximates sentence probability by masking one token at a time and summing log probabilities.
  - Quick check question: Why does PLL condition unmodified tokens on modified tokens rather than the reverse?

- Concept: **Contrastive sentence pair methodology**
  - Why needed here: CrowS-Pairs measures bias by comparing model preference between minimally different sentences—one stereotyping, one anti-stereotyping or neutral.
  - Quick check question: What score indicates neutrality, and what does a score above it imply?

- Concept: **Cross-lingual and cultural bias adaptation**
  - Why needed here: Direct translation of bias benchmarks risks semantic distortion and cultural irrelevance; the paper adapts US-specific references (e.g., "Mexican" → "Moroccan") to Dutch context.
  - Quick check question: What are three types of translation issues the paper identifies (non-minimal pair, double switch, bias mismatch)?

## Architecture Onboarding

- Component map:
  Dataset creation: Google Translate + native speaker review → cultural adaptation (names, nationalities, units) → quality filtering (remove 45 pairs) → 1,463 sentence pairs across 9 categories
  MLM evaluation: PLL scoring (Equation 1) → compare stereotyping vs. less-stereotyping sentence likelihoods → bias score = N_more / N_total × 100 (Equation 2)
  ARLM evaluation: Prompt-based choice selection → persona conditioning (none/bad/good) → same bias score formula

- Critical path:
  1. Translate and culturally adapt CrowS-Pairs sentence pairs
  2. Validate fluency and bias category alignment via native review
  3. Run PLL scoring on MLMs or prompt-based evaluation on ARLMs
  4. Compare scores across models, languages, and persona conditions

- Design tradeoffs:
  - Automated translation speed vs. cultural accuracy (mitigated by human review)
  - Dataset size (1,463 pairs) vs. coverage of Dutch-specific stereotypes
  - Prompt phrasing consistency vs. natural language variation in ARLM evaluation

- Failure signatures:
  - Non-minimal pairs: Sentences differ beyond the bias-relevant token (confounds probability comparison)
  - Double switch: Negation or other semantic elements change alongside bias term (e.g., "Women don't know" vs. "Men know")
  - Bias mismatch: Sentence pair spans different bias categories (e.g., socioeconomic vs. race)

- First 3 experiments:
  1. Replicate MLM bias scoring on Dutch CrowS-Pairs using BERTje and RobBERT; verify overall scores (~54.82%) and category-level patterns (Religion, Disability, Physical appearance highest).
  2. Replicate persona prompting on GEITje or Mistral with Dutch CrowS-Pairs; confirm that "bad" persona increases bias and "good" persona decreases it relative to baseline.
  3. Cross-validate on French CrowS-Pairs with CamemBERT and FlauBERT; confirm RoBERTa-based (CamemBERT) shows higher bias than BERT-based (FlauBERT) in most categories.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different fine-tuning approaches specifically impact bias levels in language-specific models?
- Basis in paper: [explicit] "However, more exploration is needed to understand the underlying causes and how different fine-tuning approaches might impact bias."
- Why unresolved: The paper observed that GEITje (Dutch fine-tuned) exhibited higher bias than its base model Mistral-7B, but the causal mechanisms remain speculative—it is unclear whether this stems from unbalanced representations in fine-tuning data or other factors.
- What evidence would resolve it: Controlled experiments comparing models fine-tuned on datasets with systematically varied bias characteristics, measuring bias before and after fine-tuning.

### Open Question 2
- Question: How would expert cultural and linguistic validation change the bias measurements obtained from the Dutch CrowS-Pairs dataset?
- Basis in paper: [explicit] "Future efforts should expand and refine the Dutch dataset through expert cultural and linguistic validation" and [inferred] "limitations remain, including issues with sentence quality and category consistency inherited from the original CrowS-Pairs design."
- Why unresolved: The current dataset relied on machine translation with native speaker review, but lacks systematic expert validation of cultural appropriateness and stereotype accuracy in the Dutch context.
- What evidence would resolve it: A comparative study measuring bias using the current dataset versus an expert-validated version, analyzing discrepancies in category-level scores.

### Open Question 3
- Question: What more robust evaluation metrics beyond pseudo-log-likelihood scoring could better capture bias in both masked and autoregressive language models?
- Basis in paper: [explicit] "Future efforts should... develop more robust evaluation metrics."
- Why unresolved: The paper uses different metrics for MLMs (pseudo-log-likelihood) and ARLMs (prompt-based selection), making cross-architecture comparisons difficult, and both approaches have known limitations in capturing nuanced bias expressions.
- What evidence would resolve it: Development and validation of unified metrics that produce comparable bias scores across architectures, tested for correlation with human bias judgments.

### Open Question 4
- Question: Why do RoBERTa-based architectures consistently exhibit higher bias than BERT-based architectures across different languages and training corpora?
- Basis in paper: [inferred] The paper observes this pattern across English (RoBERTa vs. BERT), French (CamemBERT vs. FlauBERT), and Dutch (RobBERT vs. BERTje) models, attributing it partly to training data scale, but architectural contributions remain unexplained.
- Why unresolved: While training data volume and diversity are discussed as contributors, the specific architectural features of RoBERTa that may amplify bias (e.g., different pretraining objectives, optimization strategies) have not been isolated.
- What evidence would resolve it: Ablation studies controlling for training data while varying architectural components, or training both architectures on identical corpora to isolate architectural effects.

## Limitations
- The Dutch CrowS-Pairs dataset was created through translation and cultural adaptation, which may not fully capture all Dutch-specific stereotypes
- The adaptation process removed 45 pairs, and some bias categories may remain underrepresented
- The pseudo-log-likelihood scoring method assumes probability differences between minimally different sentences directly reflect bias, which may not account for all contextual factors

## Confidence
- **High**: Observed bias differences across languages (Dutch < French < English)
- **Medium**: RoBERTa vs. BERT architecture bias patterns
- **Medium**: Persona prompting effects given the relatively small sample of ARLM evaluations

## Next Checks
1. Validate the cultural adaptation process by conducting bias measurements on native Dutch text sources beyond CrowS-Pairs to ensure the dataset captures authentic Dutch stereotypes.
2. Test additional Dutch-specific bias categories not covered in the original CrowS-Pairs to assess whether the nine categories fully represent Dutch social biases.
3. Conduct systematic ablation studies varying training data size and diversity for Dutch models to isolate the relationship between corpus characteristics and bias levels.