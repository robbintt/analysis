---
ver: rpa2
title: Pretrained LLMs Learn Multiple Types of Uncertainty
arxiv_id: '2505.21218'
source_url: https://arxiv.org/abs/2505.21218
tags:
- uncertainty
- should
- arxiv
- language
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to identify uncertainty representations
  within pretrained large language models (LLMs) by searching for linear directions
  in their latent space that predict generation correctness. The authors propose training
  linear classifiers at each transformer layer to find these uncertainty vectors,
  which they then use to assess how well the model captures uncertainty without further
  training.
---

# Pretrained LLMs Learn Multiple Types of Uncertainty

## Quick Facts
- **arXiv ID:** 2505.21218
- **Source URL:** https://arxiv.org/abs/2505.21218
- **Reference count:** 40
- **Primary result:** LLMs encode multiple dataset-specific, nearly orthogonal uncertainty vectors in their latent space, identifiable via linear probes that predict generation correctness.

## Executive Summary
This paper introduces a method to identify uncertainty representations within pretrained large language models by searching for linear directions in their latent space that predict generation correctness. The authors propose training linear classifiers at each transformer layer to find these uncertainty vectors, which they then use to assess how well the model captures uncertainty without further training. They demonstrate that LLMs indeed encode multiple distinct types of uncertainty—often dataset-specific and nearly orthogonal—suggesting that different uncertainty signals are learned for different tasks or knowledge domains. Their results show that these uncertainty vectors can significantly outperform random chance in predicting correctness across multiple datasets and models, and that the best performance typically occurs in intermediate layers. They also find that model size does not substantially improve uncertainty representation, whereas instruction-tuning and [IDK]-tuning greatly enhance the ability to capture uncertainty and generalize across tasks.

## Method Summary
The authors train logistic regression probes on hidden states from each transformer layer to predict whether the model's next-token generation will be correct. For each (model, layer, dataset) combination, they fit a linear classifier on the hidden states extracted from the last token position at that layer. The learned weight vector serves as an "uncertainty direction" for that dataset. They evaluate these vectors by measuring their accuracy in predicting correctness on held-out data, comparing performance across layers, model sizes, and training methods. They also examine cosine similarity between vectors from different datasets to assess whether uncertainty representations are distinct or unified.

## Key Results
- LLMs encode multiple dataset-specific uncertainty vectors that are nearly orthogonal to each other
- The best uncertainty predictions occur in intermediate layers (between L/2 and 3L/4)
- Model size does not substantially improve uncertainty representation
- Instruction-tuning and [IDK]-tuning greatly enhance the ability to capture uncertainty and generalize across tasks

## Why This Works (Mechanism)

### Mechanism 1: Linear Probing Extracts Latent Uncertainty Signals
Uncertainty correlates with linearly separable directions in transformer hidden states. The authors fit logistic regression probes on hidden states h_i(x) from layer i to predict whether the model's next-token generation will be correct. The learned weight vector u_i(D) serves as a "uncertainty direction" for dataset D. Classification: CORRECT if u_i(D)^T h_i(x) + b_i ≤ 0, else INCORRECT. The core assumption is that uncertainty manifests as a linear concept in the residual stream, with correctness serving as a proxy for uncertainty.

### Mechanism 2: Multiple Dataset-Specific Uncertainty Representations
LLMs encode multiple near-orthogonal uncertainty vectors, each tied to specific knowledge types or datasets. Uncertainty vectors u_i(D1) and u_i(D2) from different datasets show low cosine similarity (near-zero), indicating linear independence. Each vector predicts correctness well on its source dataset but poorly on others, except within thematic clusters (e.g., math datasets GSM8K, ASDiv, SVAMP transfer across each other). This suggests fragmented knowledge encoding rather than a unified uncertainty signal.

### Mechanism 3: Intermediate Layers Concentrate Uncertainty Information
Layers between L/2 and 3L/4 yield the most accurate correctness predictions. Early layers encode raw features; intermediate layers integrate task-relevant information before it's transformed into output logits at later layers. Precision drops in final layers, suggesting overconfidence. The assumption is that uncertainty-relevant features peak in intermediate representations before being "washed out" by output-focused processing.

## Foundational Learning

- **Concept: Logistic Regression Probing**
  - Why needed here: The entire framework relies on fitting linear classifiers to hidden states to extract "uncertainty directions."
  - Quick check question: Can you explain why a linear probe finding above-chance accuracy implies linear separability of the concept?

- **Concept: Transformer Hidden States (Residual Stream)**
  - Why needed here: The method extracts h_i(x) from the end of each layer; understanding how information accumulates across layers is essential.
  - Quick check question: What is the dimensionality of hidden states in Llama-3.1-8B, and why does this matter for probe training?

- **Concept: Cosine Similarity and Orthogonality**
  - Why needed here: Used to quantify whether different uncertainty vectors are distinct (near-zero cosine similarity ≈ linear independence).
  - Quick check question: If two vectors have cosine similarity of 0.1, what does this imply about their relationship?

## Architecture Onboarding

- **Component map:** Data Preparation -> Forward Pass Extraction -> Probe Training -> Evaluation
- **Critical path:** Correctness labeling (requires ground-truth answers and model inference) -> Hidden state extraction (memory-intensive; use hooks or modified forward pass) -> Probe fitting (scikit-learn LogisticRegression suffices; no regularization details specified) -> Cross-dataset evaluation (key for testing generalization claims)
- **Design tradeoffs:** Per-layer vs. single-layer analysis (per-layer is computationally heavier but reveals where uncertainty concentrates); Dataset-specific vs. unified probe (specific probes capture nuanced uncertainty types; unified probe tests generalization); Accuracy vs. precision (precision matters more for hallucination mitigation)
- **Failure signatures:** Probe accuracy at ~0.5 across all layers (uncertainty not linearly encoded or labeling noise); High accuracy but low cross-dataset transfer (confirms multi-representation hypothesis); Later layers outperforming intermediate (contradicts paper's findings, may indicate model-specific differences)
- **First 3 experiments:** Replicate single-dataset probing on Llama-3.1-8B with 3 datasets (e.g., GSM8K, NaturalQuestions, TruthfulQA); verify accuracy >0.5 at best layer; Compute cosine similarity matrix between u_i(D) vectors at a middle layer (e.g., layer 22); confirm near-orthogonality; Test cross-dataset transfer: train probe on GSM8K, evaluate on ASDiv and SVAMP; expect above-random transfer within math domain

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do non-linear probes uncover distinct uncertainty structures that linear vectors fail to capture?
- **Basis in paper:** Appendix A states the analysis is "limited to linear probes and does not explore more complex, nonlinear structures."
- **Why unresolved:** The study only tested linear separability within the latent space, potentially missing complex geometric relationships.
- **What evidence would resolve it:** Training non-linear classifiers (e.g., MLPs) on the same hidden states and comparing correctness prediction accuracy against the linear baseline.

### Open Question 2
- **Question:** Do these uncertainty vectors apply to open-ended generation tasks where binary correctness labels are ill-defined?
- **Basis in paper:** Appendix A notes correctness may not align "with how uncertainty manifests in open-ended or ambiguous generation scenarios."
- **Why unresolved:** Experiments were restricted to benchmarks with clear ground-truth answers (QA, Math, Code), excluding creative or subjective tasks.
- **What evidence would resolve it:** Validating vector activation strength against human evaluation scores for coherence and hallucination in free-form text generation.

### Open Question 3
- **Question:** Does the fragmentation of uncertainty representations directly cause hallucinations, and can manual unification reduce them?
- **Basis in paper:** The conclusion suggests this multiplicity "suggests an underlying explanation for... hallucinations," but validates only prediction capability.
- **Why unresolved:** The paper establishes correlation (vectors predict errors) but lacks causal intervention experiments to prove these distinct vectors drive the errors.
- **What evidence would resolve it:** Modifying activation vectors (e.g., steering) to unify uncertainty directions and observing the resulting impact on factual accuracy.

## Limitations

- The study relies on the strong assumption that correctness labels perfectly proxy for uncertainty, which may not capture correct-but-uncertain answers
- The methodology focuses exclusively on classification-based probing without exploring alternative uncertainty measures like entropy or calibrated confidence scores
- The finding that instruction-tuning enhances uncertainty representation raises questions about whether these representations are genuinely learned during pretraining or primarily emerge from supervised fine-tuning

## Confidence

- **High confidence:** The empirical demonstration that dataset-specific uncertainty vectors exist and are nearly orthogonal within models. The methodology is clearly specified and reproducible, with consistent patterns across multiple models and datasets.
- **Medium confidence:** The claim that intermediate layers concentrate uncertainty information. While supported by layer-wise accuracy plots, this could vary with model architecture (pre-norm vs post-norm) and the specific layer indexing convention used.
- **Medium confidence:** The assertion that model size doesn't substantially improve uncertainty representation. This is based on comparisons between Llama-3.1-8B and smaller models, but doesn't test much larger frontier models where different scaling behaviors might emerge.

## Next Checks

1. **Cross-dataset probe generalization test:** Train uncertainty probes on individual datasets (GSM8K, NaturalQuestions, TruthfulQA) and evaluate on all other datasets. Measure whether math datasets show cross-transfer while unrelated domains don't, confirming the multi-representation hypothesis.

2. **Layer-wise ablation study:** Systematically remove intermediate layers from pretrained models and re-evaluate uncertainty probe performance. If intermediate layers are truly critical, their removal should disproportionately degrade correctness prediction accuracy.

3. **Calibration vs. correctness probe comparison:** Train probes to predict both correctness and model entropy/calibrated confidence scores. Compare whether linear separability holds for these alternative uncertainty measures, testing the robustness of the linear encoding assumption.