---
ver: rpa2
title: Are LLMs reliable? An exploration of the reliability of large language models
  in clinical note generation
arxiv_id: '2505.17095'
source_url: https://arxiv.org/abs/2505.17095
tags:
- clinical
- note
- llms
- consistency
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the reliability of 12 open-weight and proprietary
  large language models (LLMs) in clinical note generation by measuring consistency
  and correctness across multiple iterations using the same prompt. Meta's Llama 70B
  and Mistral's Small model achieved perfect semantic consistency and outperformed
  proprietary models, making them the most reliable.
---

# Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation

## Quick Facts
- arXiv ID: 2505.17095
- Source URL: https://arxiv.org/abs/2505.17095
- Reference count: 10
- Open-weight models Meta's Llama 70B and Mistral Small achieved perfect semantic consistency for clinical note generation, outperforming proprietary models

## Executive Summary
This study evaluates the reliability of 12 large language models for clinical note generation by measuring consistency and correctness across multiple iterations using the same prompt. The researchers found that Meta's Llama 3.1-70B and Mistral Small achieved perfect semantic consistency and outperformed proprietary models like GPT-4o and Claude Sonnet. Most models demonstrated stable semantic consistency despite varying outputs, with Meta models generally performing better than OpenAI's ChatGPT models. The study recommends deploying these open-weight models locally to ensure data privacy compliance while improving healthcare provider efficiency in clinical documentation.

## Method Summary
The study used the aci-bench dataset containing 112 English doctor-patient conversation transcripts with expert-annotated ground truth notes. For each of 12 open-weight and proprietary LLMs, the researchers generated clinical notes from each transcript using deterministic parameters (temperature=0, top_p=0, top_k=1) across 10 iterations. They measured consistency rate (string-equivalent pairs), semantic consistency (BERTScore between generated note pairs), and semantic similarity (BERTScore between generated and ground truth notes). AWS Bedrock API and OpenAI API were used for inference, with Llama models requiring special formatting tokens.

## Key Results
- Meta's Llama 3.1-70B and Mistral Small achieved perfect semantic consistency (100%)
- Proprietary models like GPT-4o and Claude Sonnet showed 0% consistency rate despite deterministic parameters
- Most models maintained >96% semantic consistency regardless of consistency rate variations
- Open-weight models outperformed proprietary models on combined reliability metrics

## Why This Works (Mechanism)

### Mechanism 1
Setting sampling parameters to near-zero values increases output consistency across repeated generations from the same prompt. Parameters like temperature, top_p, and top_k control the stochasticity of token selection. Setting temperature=0, top_p=0, and top_k=1 forces the model to select the highest-probability token at each step, reducing variance across runs.

### Mechanism 2
Semantic consistency can be high even when string equivalence is low, making meaning preservation a more practical reliability metric for clinical documentation. BERTScore computes cosine similarity between contextual embeddings of token pairs, capturing semantic equivalence despite syntactic variation.

### Mechanism 3
Smaller open-weight models can match or exceed proprietary model reliability for clinical note generation while enabling local deployment for privacy compliance. Open-weight models like Llama 3.1-70B and Mistral Small can be deployed on-premises, eliminating data transmission to third-party APIs.

## Foundational Learning

- **Temperature and sampling parameters in autoregressive language models**: Understanding how temperature=0 forces greedy decoding is essential for interpreting why consistency rates differ from semantic consistency. Quick check: If temperature=1.0 were used instead of 0, would you expect semantic consistency to increase, decrease, or stay the same?

- **Embedding-based semantic similarity metrics (BERTScore)**: The study relies on BERTScore for both semantic consistency and correctness evaluation; understanding its limitations is critical for interpreting results. Quick check: Would two clinical notes with identical meaning but different medications listed receive a high or low BERTScore?

- **Intra-prompt vs. inter-prompt stability**: This study evaluates intra-prompt stability (same prompt, multiple iterations); understanding this distinction helps contextualize limitations for real-world prompt variation. Quick check: If a clinician reformulates their documentation request slightly between sessions, would intra-prompt stability results generalize?

## Architecture Onboarding

- **Component map**: Clinical conversation transcript -> formatted prompt template -> LLM inference with deterministic parameters -> BERTScore computation -> local inference infrastructure vs API integration
- **Critical path**: 1) Format transcript into model-specific prompt template, 2) Execute inference with deterministic parameters for k iterations, 3) Compute pairwise BERTScore across all note pairs for semantic consistency, 4) Compute BERTScore between each generated note and ground truth for semantic similarity, 5) Aggregate median scores across transcripts
- **Design tradeoffs**: Consistency rate vs. semantic consistency (string equivalence is stricter but less relevant), model size vs. deployment feasibility (70B models require significant GPU memory), open-weight vs. proprietary (privacy compliance vs. easier access)
- **Failure signatures**: High semantic consistency (>96%) with low consistency rate (<10%), high semantic similarity but low semantic consistency, perfect consistency rate with low semantic similarity
- **First 3 experiments**: 1) Baseline reproducibility test with Llama 3.1-70B, 2) Parameter sensitivity analysis across temperature values, 3) Cross-dataset validation on separate clinical note dataset

## Open Questions the Paper Calls Out
1. How does intra-prompt stability and correctness vary across different prompt designs for clinical note generation? (Section 6 suggests comparing measures across various prompts)
2. Can the reliability demonstrated on simulated role-play data be replicated in real-world clinical settings? (Section 6 notes dataset includes simulations and recommends clinical validation)
3. How well do automatic metrics like BERTScore correlate with human expert evaluation for clinical note correctness? (Section 6 proposes evaluation by human experts alongside automatic metrics)

## Limitations
- Some models (Claude Sonnet, GPT-4o) showed 0% consistency rate despite deterministic parameter settings, indicating additional nondeterminism sources
- Clinical relevance of high semantic similarity scores remains unvalidated against expert human judgment
- Simulated conversations may not capture real-world clinical documentation complexity and linguistic variability

## Confidence
- **High Confidence**: Meta's Llama 70B and Mistral Small achieving perfect semantic consistency with deterministic parameters is reproducible
- **Medium Confidence**: Local deployment recommendation balances privacy compliance with performance, but depends on infrastructure assumptions
- **Low Confidence**: Generalizability of intra-prompt stability to real-world clinical practice remains uncertain

## Next Checks
1. Have practicing clinicians review generated notes to validate whether BERTScore thresholds correspond to clinically acceptable documentation
2. Systematically vary temperature and sampling parameters to quantify tradeoff between output diversity and consistency
3. Evaluate top-performing models on real-world clinical note datasets from multiple institutions to assess generalization beyond simulated conversations