---
ver: rpa2
title: 'Start Making Sense(s): A Developmental Probe of Attention Specialization Using
  Lexical Ambiguity'
arxiv_id: '2511.21974'
source_url: https://arxiv.org/abs/2511.21974
tags:
- heads
- attention
- layer
- head
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops a systematic pipeline to probe attention mechanisms
  in Transformer language models using lexical ambiguity as a stimulus. The method
  identifies developmental inflection points in disambiguation performance and isolates
  attention heads whose behavior covaries with task performance across pretraining.
---

# Start Making Sense(s): A Developmental Probe of Attention Specialization Using Lexical Ambiguity

## Quick Facts
- arXiv ID: 2511.21974
- Source URL: https://arxiv.org/abs/2511.21974
- Reference count: 40
- Primary result: This study develops a systematic pipeline to probe attention mechanisms in Transformer language models using lexical ambiguity as a stimulus.

## Executive Summary
This study develops a systematic pipeline to probe attention mechanisms in Transformer language models using lexical ambiguity as a stimulus. The method identifies developmental inflection points in disambiguation performance and isolates attention heads whose behavior covaries with task performance across pretraining. By stress-testing candidate heads against stimulus perturbations and performing targeted ablations, the authors distinguish between robust and fragile disambiguation mechanisms. In Pythia-14M, key heads were highly sensitive to position and part-of-speech; in Pythia-410M, multiple heads showed generalizable behavior. Ablations impaired disambiguation performance, especially in 14M, indicating causal roles for identified heads. Across random seeds, performance trajectories were consistent, though head locations varied. The results suggest larger models may encode more abstract, robust disambiguation functions, and highlight the value of developmental analysis in interpretability research.

## Method Summary
The authors use lexical ambiguity as a probe to identify attention heads contributing to word sense disambiguation in Transformer language models. They analyze Pythia models (14M and 410M parameters) using the RAW-C dataset with 672 minimal sentence pairs containing ambiguous nouns. The pipeline tracks model performance (R² between human relatedness judgments and cosine distances of contextualized embeddings) across pretraining checkpoints, identifies heads whose attention patterns covary with performance, stress-tests these heads with perturbed stimuli, and performs targeted ablations (zero-ablation and step1-copy-ablation) to assess causal roles. Key heads identified include (3,2) in 14M and multiple heads in 410M, with ablation effects measured via ΔR² and fraction of intact performance.

## Key Results
- Disambiguation performance shows developmental inflection points, particularly around training steps 1000-2000
- In Pythia-14M, attention heads are fragile and sensitive to position and part-of-speech perturbations
- In Pythia-410M, multiple heads show generalizable behavior across perturbations
- Ablations impair disambiguation performance, especially in 14M, indicating causal roles for identified heads
- Performance trajectories are consistent across random seeds, though head locations vary

## Why This Works (Mechanism)

### Mechanism 1: Causal Role of Specific QK Matrices
Specific attention heads in early-to-middle layers are functionally necessary for word sense disambiguation. The Query ($W_Q$) and Key ($W_K$) matrices in specific heads learn to direct attention from an ambiguous target token to a disambiguating cue. When these weights are zero-ablated, the model's representation of the target word fails to incorporate context, degrading performance. The intervention isolates the specific head's contribution without triggering catastrophic interference from other layers. Evidence includes ablation impairments in 14M and the general premise that individual head capacity is critical for representational power. Break condition: if ablation effects are indistinguishable from baseline heads, the mechanism is likely redundant or distributed.

### Mechanism 2: Scale-Dependent Functional Abstraction
As model scale increases, attention heads transition from implementing surface-level heuristics to abstract disambiguation functions. In smaller models (14M), heads attend strongly to disambiguating cues because they are in the "1-back" position. In larger models (410M), heads emerge that maintain high attention to the semantic cue even when its position or part-of-speech is perturbed, suggesting encoding of a generalizable "disambiguation circuit." Generalization across perturbations implies a more abstract internal algorithm rather than overfitting to specific token patterns. Evidence includes multiple heads showing generalizable behavior in 410M and supporting context that specialization correlates with training/scale. Break condition: if identified "abstract" heads fail to attend to the semantic cue in novel syntactic configurations.

### Mechanism 3: Developmental Phase Transitions
Disambiguation capability emerges abruptly at specific training checkpoints ("inflection points") via coordinated changes in head attention patterns, rather than gradually. The weights in candidate heads reorganize significantly around steps 1000-2000, causing a spike in the correlation between attention to the cue and the model's ability to distinguish senses. This suggests the learning of specific linguistic relationships is a discrete event. Evidence includes clear discontinuities in R² with marked shifts in performance at specific steps and analysis of training topology to find structural shifts. Break condition: if inflection points disappear when using different random seeds or learning rate schedules.

## Foundational Learning

- **Concept: Lexical Ambiguity as a Probe**
  - Why needed: This paper uses ambiguity (words with multiple meanings) as the "stressor" to force the model to use context. Understanding the difference between static embeddings and contextualized embeddings is required to interpret the R² metric.
  - Quick check: If a model perfectly resolves lexical ambiguity, should the cosine distance between "river bank" and "bank teller" representations be high or low?

- **Concept: Self-Attention (Q, K, V)**
  - Why needed: The study specifically ablates WQ and WK matrices to disrupt attention. You must understand that Q represents what the token is looking for and K represents what the token contains to follow the intervention logic.
  - Quick check: Which matrix would you modify to stop a head from paying attention to adjectives?

- **Concept: Ablation Types (Zero vs. Copy)**
  - Why needed: The paper distinguishes between "Zero-Ablation" (removing the head entirely) and "Step1-Copy-Ablation" (reverting the head to an untrained state). This distinction is crucial for understanding whether a head is currently functional vs. whether it merely participates in the residual stream.
  - Quick check: Why might a "Zero-Ablation" cause more performance degradation than a "Step1-Copy-Ablation" in a late-layer head?

## Architecture Onboarding

- **Component map:** Pythia models (14M, 410M) -> RAW-C dataset (noun pairs) -> R² metric (cosine distance vs. human relatedness) -> Attention head identification -> Stress testing -> Ablation verification

- **Critical path:**
  1. Ingest: Load model checkpoints (step 0 to 143,000)
  2. Behavioral Profiling: Calculate R² for ambiguous pairs at every checkpoint to find inflection point
  3. Head Identification: Run linear regression between attention scores and R² to find candidate heads
  4. Stress Test: Feed perturbed inputs to see if candidates break
  5. Causal Verification: Ablate candidates and measure ΔR²

- **Design tradeoffs:**
  - 14M vs. 410M: 14M is tractable and interpretable (single head has large effect) but relies on "fragile" heuristics. 410M is performant and "robust" but suffers from high redundancy.
  - Perturbation Complexity: Increasing perturbations degrades 14M performance immediately, risking false negatives in identifying "robust" heads in small models.

- **Failure signatures:**
  - "1-Back" False Positive: A head appears to be a "disambiguation head" but is actually just a "previous-token" head.
  - Bimodality across Seeds: Replicating results on one seed works, but the specific head moves to a different layer in another seed.

- **First 3 experiments:**
  1. Replicate the Inflection Point: Run 14M on RAW-C sentences and plot R² by checkpoint to verify performance jump at step 2000.
  2. Visualize the "Baton Pass": Extract attention patterns for Layer 3 Heads 1 & 2 at steps 1000, 2000, and 5000 to observe the shift in attention from target to cue.
  3. Run the "Sort of" Test: Input positional perturbation ("friendly *sort of* lamb") and verify that 14M's Head (3,2) incorrectly attends to "of" instead of "friendly."

## Open Questions the Paper Calls Out

- Are the identified attention heads specialized for lexical ambiguity resolution, or do they perform general contextualization for all target words? The current study restricted its analysis to the RAW-C dataset, which exclusively contains ambiguous target words.

- What causal role do value matrices and the residual stream play in the identified disambiguation circuit? The analyses focused exclusively on the query-key (QK) matrices to direct and ablate attention patterns.

- Do these specialized disambiguation mechanisms generalize across different model families and languages? This study focused only on the Pythia suite using English stimuli.

- What mechanistic interactions drive the "passing the baton" phenomenon observed during training? The study documented the existence of these shifting functional roles but did not determine the underlying cause or trigger.

## Limitations

- Causal claims rely on targeted ablations in a small, fixed set of heads identified from one training run
- The distinction between "robust" and "fragile" mechanisms is based on a relatively narrow perturbation set
- The study focuses on noun disambiguation in a constrained dataset, limiting claims about broader linguistic capabilities
- Developmental analysis cannot distinguish discrete learning events from artifacts of checkpoint spacing

## Confidence

- **High confidence**: The existence of developmental inflection points in disambiguation performance and the general finding that ablation impairs task performance
- **Medium confidence**: The specific identification of causal heads and their functional characterization as either "robust" or "fragile" disambiguation mechanisms
- **Low confidence**: Claims about scale-dependent functional abstraction requiring broader validation across model families and linguistic phenomena

## Next Checks

1. Run the full pipeline on 3-5 additional random seeds for Pythia-14M and Pythia-410M to assess whether identified heads consistently emerge in the same layers

2. Design and test a more comprehensive set of linguistic perturbations including syntactic transformations, semantic substitutions, and pragmatic contexts

3. Apply the identical methodology to a non-ambiguous control task to establish whether identified patterns are specific to disambiguation or reflect general attention optimization processes