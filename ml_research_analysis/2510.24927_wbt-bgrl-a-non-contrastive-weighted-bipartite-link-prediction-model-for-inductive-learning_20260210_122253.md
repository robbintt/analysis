---
ver: rpa2
title: 'WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive
  Learning'
arxiv_id: '2510.24927'
source_url: https://arxiv.org/abs/2510.24927
tags:
- wbt-bgrl
- bipartite
- link
- prediction
- weighted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses link prediction in bipartite graphs using
  non-contrastive self-supervised learning. The proposed WBT-BGRL model extends T-BGRL
  with weighted edge handling and dual predictor architecture for bipartite asymmetries.
---

# WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning

## Quick Facts
- **arXiv ID**: 2510.24927
- **Source URL**: https://arxiv.org/abs/2510.24927
- **Reference count**: 12
- **Primary result**: WBT-BGRL achieves competitive global metrics and superior top-K performance on E-commerce data, but shows dataset-dependent performance with baselines excelling on Industry dataset.

## Executive Summary
This paper addresses link prediction in bipartite graphs using non-contrastive self-supervised learning. The proposed WBT-BGRL model extends T-BGRL with weighted edge handling and dual predictor architecture for bipartite asymmetries. It evaluates four variants (weighted/unweighted pretraining and link prediction) against adapted state-of-the-art models on Industry and E-commerce datasets. Results show dataset-dependent performance: on Industry, baselines achieve near-perfect scores while WBT-BGRL variants perform well, particularly NWP_NWB (ROC-AUC: 0.9944±0.0025). On E-commerce, WBT-BGRL achieves competitive global metrics with dramatically better top-K performance (Hits@50: 97% vs baselines' 3%). Weighted pretraining harms performance in skewed edge distributions but shows minimal impact in balanced cases. The study demonstrates that non-contrastive learning with explicit edge weighting can effectively handle inductive link prediction in bipartite graphs, with architectural trade-offs between global discrimination and top-K concentration.

## Method Summary
WBT-BGRL is a two-phase non-contrastive self-supervised learning framework for inductive link prediction in weighted bipartite graphs. The model uses dual GCN encoders with separate node type embeddings, dual projectors, and dual predictors. During pretraining (Phase 1), 200 epochs of self-supervised contrastive learning optimize a weighted loss function that accounts for edge weights. The EMA target network updates with τ=0.99 maintain consistency. Augmentations include feature dropping (p=0.1), weight-aware edge dropping, and corrupted views via feature shuffling. In Phase 2, the frozen encoder trains a 3-layer MLP decoder for supervised link prediction over 100 epochs with early stopping on Hits@50. The framework evaluates four variants: WP_WB (weighted pretraining and link prediction), WP_NWB, NWP_WB, and NWP_NWB, testing the impact of weight-aware pretraining and prediction.

## Key Results
- On Industry dataset: Baselines achieve near-perfect ROC-AUC (>0.9989), while WBT-BGRL variants perform well (best: NWP_NWB with ROC-AUC 0.9944±0.0025)
- On E-commerce dataset: WBT-BGRL achieves competitive global metrics (ROC-AUC 0.8867±0.0047) with superior top-K performance (Hits@50: 97% vs baselines' ~3%)
- Weighted pretraining (WP) degrades performance on skewed edge distributions (7.8% ROC-AUC drop on Industry) but has minimal impact on balanced distributions (0.3% drop on E-commerce)
- WBT-BGRL variants consistently outperform baseline models on top-K metrics, particularly Hits@50

## Why This Works (Mechanism)
The dual predictor architecture captures bipartite asymmetries by maintaining separate prediction heads for each node type, addressing the structural imbalance inherent in bipartite graphs. Non-contrastive self-supervised pretraining learns transferable node representations that generalize to unseen nodes, while weighted loss functions incorporate edge importance information. The EMA target network provides stable training targets, and the two-phase approach separates representation learning from task-specific adaptation.

## Foundational Learning
- **Bipartite Graph Structure**: Graphs with two disjoint node sets where edges only connect nodes from different sets - needed to understand asymmetric relationships and why standard graph models fail
- **Non-contrastive Self-Supervised Learning**: Learning representations without negative samples by maximizing similarity between augmented views - needed to understand the training objective without explicit contrastive pairs
- **Graph Convolutional Networks (GCNs)**: Neural networks that aggregate neighbor information through message passing - needed to understand how node representations are learned from graph structure
- **EMA Target Networks**: Exponential moving average updates of model parameters for stable training targets - needed to understand the consistency regularization mechanism
- **Inductive Learning**: Models that generalize to unseen nodes during inference - needed to understand the practical applicability to evolving graphs
- **Weighted Edge Handling**: Incorporating edge weights into learning objectives - needed to understand how transaction frequencies or importance scores are leveraged

Quick checks: Verify bipartite structure (edges only between node types), confirm no negative samples in loss, check message passing aggregation, validate EMA update rule, test on unseen nodes, examine weight normalization.

## Architecture Onboarding

**Component Map**: Input bipartite graph -> Dual GCN encoders -> Dual projectors -> Dual predictors -> EMA target network -> Weighted loss -> Frozen encoder -> MLP decoder -> Link prediction output

**Critical Path**: During pretraining: graph augmentation → dual GCN encoding → dual projection → dual prediction → weighted contrastive loss → EMA updates. During link prediction: frozen encoder → MLP decoder → sigmoid output.

**Design Tradeoffs**: Dual predictors add parameter overhead but capture bipartite asymmetries; weighted pretraining improves performance on balanced data but degrades on skewed distributions; EMA target network increases stability but adds memory overhead.

**Failure Signatures**: Weighted pretraining fails dramatically on skewed edge distributions (ROC-AUC drops 7.8%); dual predictor architecture may overfit on small datasets; EMA updates may slow convergence if τ is too close to 1.

**First Experiments**: 1) Train NWP_NWB variant on E-commerce-like data to verify superior top-K performance, 2) Compare WP vs NWP variants on datasets with varying weight skewness, 3) Test inductive generalization by evaluating on datasets with 30% new nodes.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can adaptive edge weighting schemes mitigate the performance degradation caused by highly skewed edge distributions during pretraining?
- **Open Question 2**: Can a hybrid model architecture be developed to combine WBT-BGRL's superior top-K performance with the global discrimination strengths of baselines like CCA-SSG?
- **Open Question 3**: Is the heuristic recommendation to avoid weighted pretraining unless "frequency std < 5" robust across diverse bipartite graph domains?

## Limitations
- Private Industry dataset prevents exact reproduction and independent validation
- Performance highly dependent on edge weight distribution, with weighted pretraining failing on skewed data
- Trade-off between global discrimination (ROC-AUC/AP) and top-K performance suggests no single best model across all metrics

## Confidence
- **High Confidence**: Fundamental architecture (dual GCN encoders + dual predictors + EMA updates) and two-phase training procedure are clearly specified
- **Medium Confidence**: Weighted edge handling mechanisms and four-variant ablation design are described but implementation details are ambiguous
- **Low Confidence**: Generalization to new datasets beyond the two studied is uncertain due to dataset-specific behaviors

## Next Checks
1. **Ablation on Edge Weight Distribution**: Test NWP vs WP variants on datasets with varying edge weight skewness to confirm weighted pretraining degrades performance in skewed distributions
2. **Top-K vs Global Metric Correlation**: On a new bipartite dataset, measure the correlation between ROC-AUC and Hits@50 across all models to validate the observation that these metrics capture different aspects of link prediction quality
3. **Inductive Generalization Test**: Evaluate all models on a dataset with significant new nodes in the test set to confirm non-contrastive pretraining improves inductive performance