---
ver: rpa2
title: 'LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking
  Tokens'
arxiv_id: '2510.11919'
source_url: https://arxiv.org/abs/2510.11919
tags:
- translation
- cotft
- ioft
- wang
- bleu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether generating intermediate reasoning
  tokens improves machine translation performance. Experiments across 10 language
  pairs and multiple benchmarks show that allowing large reasoning models to "think"
  before translating does not improve translation quality compared to direct translation.
---

# LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens

## Quick Facts
- arXiv ID: 2510.11919
- Source URL: https://arxiv.org/abs/2510.11919
- Reference count: 40
- Primary result: Intermediate reasoning tokens don't improve MT quality; translation attempts in traces matter more than reasoning explanations

## Executive Summary
This paper investigates whether generating intermediate reasoning tokens improves machine translation performance. Experiments across 10 language pairs and multiple benchmarks show that allowing large reasoning models to "think" before translating does not improve translation quality compared to direct translation. Fine-tuning models to "think" using distilled chain-of-thought explanations from teachers also fails to outperform standard input-output fine-tuning. However, using traces from modular translation-specific prompting strategies as intermediate information during fine-tuning yields improvements of up to 3.5 BLEU points. Analysis reveals that the presence of translation attempts within these traces is crucial for success.

## Method Summary
The study fine-tunes student models (gemma-3-4b-pt, gemma-3-1b-pt) using teacher models (Llama-4-Scout-17B-16E-Instruct, gemma-3-27b-it) on source→[trace]→target sequences. Three main approaches are compared: input-output fine-tuning (IOFT) with direct source-target pairs, chain-of-thought fine-tuning (CoTFT) with intermediate reasoning tokens, and data augmentation (IOFT-EXT) with expanded parallel corpora. Training uses 5k steps, lr=1e-5, and specific sequence length and gradient accumulation settings for each approach. Evaluation uses FLORES-200 devtest across 10 language directions with both BLEU and MetricX-24 metrics.

## Key Results
- Suppressing thinking tokens (non-thinking mode) matches or exceeds explicit reasoning across all 10 language pairs tested
- CoTFT with human-inspired reasoning templates (T1-T6) consistently underperforms standard IOFT
- Modular MT prompting strategies (MAPS, SBYS, TEaR) as intermediate tokens improve translation by up to 3.5 BLEU when they contain translation attempts
- Data augmentation using teacher-generated paraphrases achieves +4 BLEU over baseline, exceeding all CoTFT variants

## Why This Works (Mechanism)

### Mechanism 1: Translation Attempts, Not Explanations, Drive CoT Fine-Tuning Gains
- Claim: Intermediate tokens improve machine translation fine-tuning only when they contain actual translation attempts (drafts), not when they contain reasoning explanations alone.
- Mechanism: The paper finds that CoT distillation with human-inspired reasoning traces (T1-T6 templates) consistently fails to outperform standard input-output fine-tuning. However, when using traces from modular MT prompting strategies (MAPS, SBYS, TEaR), improvements of up to +3.5 BLEU and -2.0 MetricX occur. Analysis reveals these gains correlate with translation attempts embedded in the traces, not the reasoning structure.
- Core assumption: The model learns more from exposure to varied translation hypotheses than from explicit reasoning explanations about translation.
- Evidence anchors:
  - [abstract]: "Our findings underscore that the contribution of intermediate tokens during fine-tuning highly depends on the presence of translation attempts within them."
  - [Section 5.3, Figure 5]: IOFT-MAX (using best translation between ground truth and draft) outperforms CoTFT across MAPS, SBYS, TEaR, and Self-Refine.
  - [corpus]: Related work on CoT distillation (Li et al., 2025; Huang et al., 2024) shows success on reasoning tasks but limited evidence for MT-specific transfer.
- Break condition: When traces contain no translation attempts (pure CoT explanations), gains disappear; when CompTra provides only partial sentence translations, gains persist but are smaller.

### Mechanism 2: Suppressed Thinking Matches or Exceeds Explicit Reasoning for MT
- Claim: Large reasoning models perform equally well or better when explicitly prevented from generating thinking tokens before translation.
- Mechanism: The authors suppress thinking by appending " thinking\n\n" to prompts (Qwen3's non-thinking mode). Across 10 language pairs and 6 model sizes (0.6B-32B), non-thinking mode performs within 0.5 MetricX points of thinking mode, often slightly better. This suggests MT does not require the step-by-step decomposition that benefits mathematical reasoning.
- Core assumption: Translation relies more on pattern matching from pretraining data than on symbolic manipulation requiring explicit steps.
- Evidence anchors:
  - [Section 3, Table 1]: Non-thinking matches or exceeds thinking across all 10 directions for Qwen3-32B.
  - [Section 3, Figure 2]: Temperature analysis shows consistent non-thinking advantage across 0.2-1.0 sampling range.
  - [corpus]: Ma et al. (2025) similarly find "not thinking" can outperform explicit reasoning on some tasks.
- Break condition: Very small models (<4B) may benefit from thinking tokens as a crutch to remember target language, but this disappears at scale (≥8B).

### Mechanism 3: Data Augmentation Outperforms CoT Complexity
- Claim: Using a teacher to expand parallel corpora or refine targets provides greater gains than distilling its reasoning process.
- Mechanism: The paper compares IOFT-EXT (augmenting training data with teacher-generated paraphrases/translations) against CoTFT. IOFT-EXT with paraphrases achieves +4 BLEU over baseline IOFT, exceeding all CoTFT variants. Sentence decomposition strategies that generate new parallel pairs (paraphrases, syntactic paraphrases) outperform those generating only short phrases (hard expressions, CompTra).
- Core assumption: Translation quality scales with quantity and quality of parallel signal, not with reasoning complexity.
- Evidence anchors:
  - [Section 6.1, Figure 7]: IOFT-EXT(P) and IOFT-EXT(SP) achieve ~18 BLEU vs. CoTFT's ~15 BLEU.
  - [Section 5.3, Figure 6]: IOFT-BoA (best target across all strategies) reaches 18 BLEU vs. 14 BLEU baseline.
  - [corpus]: Weak direct evidence; related work on data augmentation for MT exists but not specifically comparing to CoT approaches.
- Break condition: Short phrase pairs (CompTra, hard expressions) provide less augmentation benefit but can still serve as useful intermediate tokens for CoTFT.

## Foundational Learning

- Concept: **Input-Output Fine-Tuning (IOFT) vs. Chain-of-Thought Fine-Tuning (CoTFT)**
  - Why needed here: The paper's core comparison; IOFT trains on source→target pairs directly, while CoTFT inserts intermediate reasoning tokens before the target.
  - Quick check question: Given a source sentence "Hello world" and target "Bonjour monde," what would IOFT train on versus CoTFT with a drafting strategy?

- Concept: **Modular MT Prompting Strategies (MAPS, SBYS, TEaR, Self-Refine, CompTra)**
  - Why needed here: These strategies generate multi-step translation traces that can be repurposed as intermediate tokens for CoTFT.
  - Quick check question: How does MAPS differ from vanilla zero-shot translation in terms of the information it produces?

- Concept: **BLEU and MetricX-24 Evaluation**
  - Why needed here: The paper uses both lexical (BLEU) and neural (MetricX-24) metrics; BLEU measures n-gram overlap while MetricX-24 is a learned quality estimator.
  - Quick check question: If a translation has BLEU=30 and MetricX=5.0, is that good or bad? (Higher BLEU is better; lower MetricX is better.)

## Architecture Onboarding

- Component map:
  - Teacher model (Llama-4-Scout-17B-16E-Instruct, gemma-3-27b-it) -> Generates CoT traces or translation attempts
  - Student model (gemma-3-4b-pt, gemma-3-1b-pt) -> Fine-tuned on source→[trace]→target sequences
  - Prompting strategies (MAPS, SBYS, TEaR, Self-Refine, CompTra) -> Produce multi-step traces
  - CoT templates (T1-T6) -> Human-translator-inspired reasoning templates

- Critical path:
  1. Select language pair and fine-tuning dataset (parallel corpus)
  2. Choose teacher and generate intermediate tokens (CoT traces OR prompting strategy outputs)
  3. Format training examples with target: " thinking\n{tokens}\n \n\nFinal Translation\n{target}"
  4. Fine-tune student for 5k steps with constant LR=1e-5, batch size 4, gradient accumulation 16, max seq length 2048
  5. Evaluate zero-shot on FLORES-200 devtest using greedy decoding

- Design tradeoffs:
  - **CoTFT vs. IOFT**: CoTFT requires 4x longer sequences (2048 vs. 512 tokens), increasing compute and memory; gains are inconsistent and depend on translation attempts in traces
  - **Teacher quality matters less than trace content**: A worse teacher (gemma-3-27b-it) can produce useful traces if they contain translation drafts
  - **GRPO after CoTFT provides marginal +1 BLEU**: Not worth the complexity vs. simply continuing IOFT

- Failure signatures:
  - CoTFT underperforms IOFT by 0.5-5 BLEU when using pure reasoning templates (T1-T6)
  - Small models (<4B) generate wrong-language outputs when struggling; thinking mode may mask this
  - Sampling with temperature >0 degrades performance for thinking models on MT

- First 3 experiments:
  1. **Baseline comparison**: Run IOFT vs. CoTFT with T3 (back-translation template) on English→Xhosa using gemma-3-4b-pt student; expect IOFT to win by ~0.5-2 BLEU.
  2. **Prompting strategy traces**: Generate MAPS traces for training data, format as intermediate tokens, run CoTFT; expect +2-3 BLEU over IOFT baseline.
  3. **Data augmentation alternative**: Instead of CoTFT, use IOFT-EXT with teacher-generated paraphrases as additional training pairs; expect this to match or exceed best CoTFT results at lower training cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would incorporating reasoning-like translation data (e.g., annotated translation decision processes) into pretraining corpora enable thinking tokens to improve MT performance, similar to how mathematical reasoning data supports CoT in math tasks?
- Basis in paper: [explicit] The authors state "unlike mathematics where step-by-step explanations are widely present in pre-training corpora (proofs), it is not the case for translation data. This scarcity of reasoning-like data may explain CoT's limited effectiveness in MT."
- Why unresolved: The paper hypothesizes this explanation but does not test whether adding such data during pretraining would change outcomes.
- What evidence would resolve it: Experiments pretraining models on corpora enriched with human translator reasoning annotations, then evaluating CoT effectiveness.

### Open Question 2
- Question: Can reinforcement learning with process-level rewards (rewarding intermediate reasoning steps rather than only final translations) enable thinking tokens to benefit MT, unlike current outcome-only reward formulations?
- Basis in paper: [explicit] The authors cite Zheng et al. (2025) and confirm that "CoT signals fail to induce meaningful reasoning when the reward is applied only to the final translation."
- Why unresolved: The paper only tests outcome-based rewards (BLEU, COMET, MetricX); process-level reward formulations remain unexplored.
- What evidence would resolve it: GRPO experiments with step-wise reward functions that evaluate intermediate reasoning quality, not just final translation quality.

### Open Question 3
- Question: What specific properties make partial translation pairs (as in CompTra) effective intermediate tokens while full reasoning explanations are not?
- Basis in paper: [explicit] The authors find "COTFT with CompTra outperforms IOFT-MAX(COMPTRA)" and note "sentence-translation pairs (related to the sentence considered but smaller and different) can serve as valuable intermediate information," but do not fully explain this asymmetry.
- Why unresolved: The mechanism distinguishing effective intermediate tokens (partial translations) from ineffective ones (CoT explanations) is not identified.
- What evidence would resolve it: Ablation studies systematically varying intermediate token properties (syntactic relatedness, lexical overlap, translation attempt presence) to isolate contributing factors.

## Limitations

- English-centric experiments: All but one direction involved English as source or target, limiting generalizability to truly distant language pairs
- Small model focus: Experiments use only student models ≤4B parameters, leaving open questions about larger model behavior
- Synthetic data reliance: Several language pairs use synthetic parallel data (TOPXGen), which may not capture real-world translation complexity

## Confidence

- **High Confidence**: The finding that non-thinking models match or exceed thinking models on MT tasks is well-supported by systematic experiments across 10 language pairs and multiple model sizes
- **Medium Confidence**: The conclusion that translation attempts within intermediate tokens drive CoTFT gains is supported by comparative analysis, though the paper doesn't isolate whether it's specifically the translation quality or quantity that matters
- **Medium Confidence**: The superiority of data augmentation over CoTFT complexity is demonstrated but relies on synthetic data generation, which may not generalize to all low-resource scenarios

## Next Checks

1. **Cross-lingual Generalization**: Test CoTFT and data augmentation approaches on language pairs with no English involvement (e.g., Chinese→Korean, Arabic→Russian) to verify if findings hold for truly distant languages

2. **Scale Sensitivity**: Evaluate the same experiments with larger student models (8B-70B parameters) to determine if scale changes the effectiveness of thinking tokens versus direct translation

3. **Real-world Data Impact**: Compare CoTFT and data augmentation on naturally occurring low-resource parallel corpora rather than synthetic data to assess practical deployment viability