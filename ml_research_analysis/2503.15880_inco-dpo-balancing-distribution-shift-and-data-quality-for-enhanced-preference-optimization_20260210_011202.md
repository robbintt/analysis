---
ver: rpa2
title: 'InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced Preference
  Optimization'
arxiv_id: '2503.15880'
source_url: https://arxiv.org/abs/2503.15880
tags:
- data
- on-policy
- reward
- off-policy
- inco-dpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies two key factors in preference data for direct
  preference optimization: distribution consistency and data quality. While on-policy
  data ensures consistency, its quality is limited by the current model''s capabilities.'
---

# InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced Preference Optimization

## Quick Facts
- **arXiv ID:** 2503.15880
- **Source URL:** https://arxiv.org/abs/2503.15880
- **Reference count:** 13
- **Key outcome:** Synthesizes preference data by continuing strong model prefixes with the policy model at lower temperatures, achieving state-of-the-art 60.8 win rate on Arena-Hard with Gemma-2 using vanilla DPO.

## Executive Summary
This paper identifies the fundamental trade-off in preference optimization between distribution consistency and data quality. On-policy data ensures consistency but is limited by the current model's capabilities, while off-policy data from stronger models offers higher quality but suffers from distribution shifts. InCo-DPO addresses this by synthesizing preference data through a novel prefix continuation method: strong models generate short prefixes, which the policy model then continues at lower temperatures. This approach dynamically balances reward improvement and consistency maintenance, consistently outperforming both on-policy and off-policy data across multiple benchmarks and model scales.

## Method Summary
InCo-DPO synthesizes preference pairs by having a strong teacher model generate short prefixes (2-8 tokens) for each prompt, which are then continued by the policy model at temperature 0.6. These full responses are scored by a reward model (ArmoRM), and preference pairs are created by selecting the highest and lowest-scoring completions. The method is trained using standard DPO with AdamW optimizer (LR 1e-6 for Gemma-2 or 5e-7 for Llama-3), batch size 128, and 1 epoch. The temperature reduction during continuation mitigates distribution shift while maintaining the quality benefits of off-policy data, and prefix length serves as a control parameter for the consistency-reward trade-off.

## Key Results
- Achieves state-of-the-art 60.8 win rate on Arena-Hard with Gemma-2 using vanilla DPO
- Consistently outperforms both pure on-policy and pure off-policy baselines across multiple datasets
- Demonstrates superior generalization, achieving strong results even on tiny models
- Shows optimal performance at prefix length of 2-10 tokens and temperature of 0.6

## Why This Works (Mechanism)

### Mechanism 1: Steering via High-Quality Prefixes
High-quality prefixes statistically lead to higher-quality full completions, effectively raising the policy model's performance ceiling by "priming" it with superior starting sequences.

### Mechanism 2: Temperature-Regulated Consistency Recovery
Reducing sampling temperature during continuation mitigates distribution shift by forcing the policy model to select higher probability tokens, keeping generation closer to its own policy distribution while benefiting from the prefix.

### Mechanism 3: Dynamic Trade-off Optimization
Prefix length serves as a control parameter, with shorter prefixes maintaining high consistency but lower quality gain, while longer prefixes offer high quality but high shift. The method identifies a "sweet spot" at 2-10 tokens.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed:** The underlying training objective that InCo-DPO optimizes, eliminating the need for a separate reward model during training
  - **Quick check:** How does DPO eliminate the need for a separate reward model during training?

- **Concept: On-Policy vs. Off-Policy Data**
  - **Why needed:** The central tension InCo-DPO addresses between consistent but limited on-policy data and high-quality but distribution-shifted off-policy data
  - **Quick check:** Why does "distribution shift" in off-policy data hinder convergence in RL/DPO?

- **Concept: Reward Modeling (ArmoRM)**
  - **Why needed:** The external scalar used to determine "quality" and select preference pairs in InCo-DPO
  - **Quick check:** In InCo-DPO, is the reward model used to train the final model, or to select the training data?

## Architecture Onboarding

- **Component map:** Instruction Source -> Teacher Model (Qwen2.5-72B) -> Prefix -> Policy Model (Gemma-2) -> Suffix (Temp=0.6) -> Reward Model (ArmoRM) -> Selector -> Trainer

- **Critical path:** Sample prefixes from Teacher → Continue with Policy at T=0.6 → Score full sequences → Select Max/Min reward pairs → Run DPO training

- **Design tradeoffs:** Prefix Length vs. Stability (increasing tokens boosts quality but degrades stability), Temperature vs. Diversity (lower T secures consistency; higher T creates diverse pairs but risks distribution mismatch)

- **Failure signatures:** Low Reward Margin (T too low or samples too similar), Catastrophic Forgetting (observed with WPO/off-policy), Quality Collapse (policy model too weak to continue coherently)

- **First 3 experiments:** 1) Baseline Sanity Check: Train DPO using pure on-policy vs. pure off-policy data, 2) Prefix Ablation: Implement InCo-DPO with prefix lengths [0, 2, 8, 50] and plot Consistency Weight vs. Average Reward, 3) Temperature Sweep: Fix prefix length at 4 and sweep temperature [0.5, 0.6, 0.7, 0.8] to verify the 0.6 optimum

## Open Questions the Paper Calls Out
- The primary dataset (UltraFeedback) lacks safety alignment and the evaluation benchmarks do not examine preferences for multi-turn instructions
- Whether the correlation between partial and final response rewards holds for complex reasoning tasks where early tokens do not determine correctness
- Whether the optimal prefix length and sampling temperature can be determined adaptively rather than requiring manual grid search

## Limitations
- Heavy dependence on access to a strong "teacher" model for prefix generation
- All experiments use UltraFeedback dataset, limiting understanding of performance on different instruction distributions
- Limited experimental scope with tiny models (only one specific case tested)

## Confidence
- **High Confidence:** Core empirical results showing InCo-DPO outperforming baselines across multiple benchmarks
- **Medium Confidence:** Theoretical mechanism of "prefix continuation" improving quality while maintaining consistency
- **Medium Confidence:** Claim of "superior generalization" particularly on tiny models

## Next Checks
1. Test InCo-DPO on at least two additional instruction datasets (e.g., OASST1, OpenHermes) to verify robustness beyond UltraFeedback
2. Systematically vary the relative strength between teacher and policy models to quantify the minimum quality gap required for InCo-DPO to outperform baselines
3. Conduct a human evaluation study comparing ArmoRM scores against human preference judgments for InCo-DPO generated pairs versus traditional on/off-policy pairs