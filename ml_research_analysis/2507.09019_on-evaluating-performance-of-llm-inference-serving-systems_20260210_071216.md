---
ver: rpa2
title: On Evaluating Performance of LLM Inference Serving Systems
arxiv_id: '2507.09019'
source_url: https://arxiv.org/abs/2507.09019
tags:
- latency
- evaluation
- performance
- inference
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current LLM inference evaluation suffers from fundamental anti-patterns
  that obscure true performance characteristics and impede scientific progress. These
  anti-patterns manifest across baseline fairness, evaluation setup, and metric design,
  uniquely problematic for LLM inference due to its dual-phase nature, heterogeneous
  workloads, and strict temporal requirements.
---

# On Evaluating Performance of LLM Inference Serving Systems

## Quick Facts
- arXiv ID: 2507.09019
- Source URL: https://arxiv.org/abs/2507.09019
- Reference count: 14
- Current LLM inference evaluation suffers from fundamental anti-patterns that obscure true performance characteristics and impede scientific progress.

## Executive Summary
LLM inference evaluation suffers from systematic anti-patterns that obscure true performance characteristics and impede scientific progress. These anti-patterns manifest across baseline fairness, evaluation setup, and metric design, uniquely problematic for LLM inference due to its dual-phase nature, heterogeneous workloads, and strict temporal requirements. The evaluation process frequently conflates implementation differences with algorithmic novelty, neglects parameter tuning, uses outdated or irrelevant models, employs non-representative workloads, ignores practical latency bounds, and relies on misleading or opaque metrics that obscure performance distributions and generation stalls. A comprehensive checklist framework is provided to identify and avoid these anti-patterns, with speculative decoding serving as a practical demonstration of how conventional metrics mislead.

## Method Summary
The paper presents a systematic analysis of evaluation anti-patterns in LLM inference serving systems, using speculative decoding as a case study. The methodology involves comparing speculative decoding against autoregressive baseline using Llama-3 70B model on 4x H100 GPUs with ShareGPT4 dataset at 0.25 QPS request rate. Experiments collect per-token timestamps to compute TBT CDFs, TPOT percentiles, TTFT by prompt size, and Fluidity Index. The assessment methodology relies on manual expert judgment across 12 recent systems, examining published materials, configurations, and artifact analysis to identify evaluation anti-patterns.

## Key Results
- Current LLM inference evaluation suffers from fundamental anti-patterns that obscure true performance characteristics
- These anti-patterns manifest across baseline fairness, evaluation setup, and metric design
- A comprehensive checklist framework is provided to identify and avoid these anti-patterns
- Speculative decoding evaluation exemplifies how conventional metrics mislead, with Time Between Tokens becoming bimodal and misleading

## Why This Works (Mechanism)

### Mechanism 1: Dual-Phase Resource Contention Creates Metric Blind Spots
- Claim: LLM inference's dual-phase nature (prefill + decode) creates fundamentally different resource contention patterns that single aggregated metrics fail to capture, leading to misleading system comparisons.
- Mechanism: Prefill is compute-bound with quadratic complexity in prompt length, while decode is memory-bandwidth-bound with sequential KV-cache access. When these phases run concurrently (batched serving), prefill can starve decode requests, causing generation stalls invisible to averaged metrics like TPOT.
- Core assumption: Interactive applications require consistent token delivery, not just high aggregate throughput.
- Evidence anchors:
  - [abstract]: "These anti-patterns are uniquely problematic for LLM inference due to its dual-phase nature combining distinct prefill and decode operations"
  - [section 2.3]: "The prefill phase is compute-bound... In contrast, the decode phase is memory-bound, generating one token at a time with intensive memory access patterns due to KV-cache requirements"
  - [corpus]: Bench360 notes existing benchmarks are fragmented and focus on isolated goals, offering little guidance for practical deployments
- Break condition: When systems disaggregate prefill and decode onto separate hardware (e.g., Splitwise), the contention dynamics fundamentally change, requiring different evaluation approaches.

### Mechanism 2: Normalization Dilutes Fixed Overheads Proportionally to Output Length
- Claim: Normalized metrics like TPOT and Normalized Latency systematically mask fixed overheads (scheduling delays, prefill time) by distributing them across output tokens, creating misleading equivalence between vastly different user experiences.
- Mechanism: A 10-second scheduling delay appears as 1.02s/token for a 10-token output but 0.03s/token for 1000 tokens. Both users wait identically, yet normalized metrics suggest dramatically different performance quality.
- Core assumption: Users experience absolute temporal delays, not per-token efficiency ratios.
- Evidence anchors:
  - [section 3.3]: "Consider an example: two requests generating 10 and 1000 tokens respectively, both experience a 10s scheduling delay... Normalized latency is dramatically different (1.02 s/token vs. 0.03 s/token), falsely suggesting a vast difference in user experience"
  - [section 3.3, Figure 2]: "vLLM having >25s scheduling delays for 60% of requests, yet this critical difference is nearly invisible in the normalized latency plot"
  - [corpus]: Weak direct support; comparative analysis papers focus on engine comparison rather than metric validity
- Break condition: When workload has narrow output length variance, normalization becomes more reliable for relative comparisons.

### Mechanism 3: Speculative Decoding Creates Bimodal Latency Distributions Misrepresented by Summary Statistics
- Claim: Speculative decoding produces fundamentally bimodal TBT distributions—near-zero latency on successful batch acceptance, high latency on verification failures—that summary statistics (median, mean, P99 alone) systematically misrepresent.
- Mechanism: Draft model proposes K tokens; verification accepts 0-K tokens in batch. Success yields near-instant tokens; failure incurs draft + verification cost for single token. Median captures only the successful mode.
- Core assumption: The bimodal distribution is inherent to speculative decoding's verify-then-accept pattern, not a tuning artifact.
- Evidence anchors:
  - [section 4]: "a large fraction (e.g., 75%) of inter-token intervals register near-zero latency due to batch acceptance. Reporting only a median TBT would capture only this best-case scenario, ignoring the significant delays during verification failures"
  - [section 4, Figure 3]: "While speculative decoding might appear favorable based on median TPOT (e.g., 1.3× lower), this advantage can reverse at the tail (e.g., 1.16× higher P99 TPOT)"
  - [corpus]: No direct corpus support for speculative decoding evaluation patterns
- Break condition: When draft model accuracy exceeds ~95%, the failure mode becomes rare enough that bimodality is less pronounced.

## Foundational Learning

- **Concept: Prefill vs. Decode Resource Profiles**
  - Why needed: Understanding that prefill is compute-bound (parallel, O(n²) attention) while decode is memory-bound (sequential, KV-cache scans) explains why these phases compete differently for GPU resources and require separate metric evaluation.
  - Quick check: Why does a 32K token prompt require 4× more prefill computation than 16K, and which latency metric would reveal this impact?

- **Concept: Latency Metric Taxonomy (TTFT, TBT, TPOT, Normalized Latency)**
  - Why needed: Each metric measures a different user-facing experience dimension; choosing incorrectly can hide critical pathologies like generation stalls or scheduling bottlenecks.
  - Quick check: For an interactive voice assistant, which metric(s) would detect disruptive pauses during speech generation?

- **Concept: Workload Heterogeneity Across Applications**
  - Why needed: Production traces show 5× differences in median output lengths and 2× in prompt lengths between code and conversation workloads; systems optimized for one may fail on another.
  - Quick check: Why might a system showing excellent performance on ShareGPT (short chat) struggle on Arxiv Summarization (long context and outputs)?

## Architecture Onboarding

- **Component map**: Request Scheduler (Ts) -> Prefill Engine (Tp) -> Decode Engine (Td) -> Memory Manager
- **Critical path**: Interactive use: Ts (scheduling delay) → Tp (TTFT) → Td sequence (TBT distribution determines fluidity); Batch throughput: Total tokens / (Ts + Tp + Td) across concurrent requests
- **Design tradeoffs**:
  - Chunked prefill vs. decode interleaving: Improves TTFT fairness, reduces throughput
  - Speculative decoding: ~1.3× lower median TPOT, higher TTFT, increased TBT variance
  - Disaggregated prefill/decode: Per-phase hardware optimization, coordination overhead
- **Failure signatures**:
  - High TTFT, good TPOT → Long-prefill starvation of decode requests
  - Bimodal TBT with speculative decoding → Draft model accuracy <80%
  - Good normalized latency, poor absolute TTFT → Request queue saturation
  - P99 >> P50 latency → Scheduling instability under load
- **First 3 experiments**:
  1. **Baseline CDF characterization**: Run vLLM on ShareGPT, plot full CDF of TTFT and TBT (not just P50/P99) to identify generation stall patterns and scheduling delay distribution.
  2. **Workload cross-validation**: Compare performance on ShareGPT (short chat) vs. Arxiv Summarization (long context) using identical hardware/config to expose workload-dependent behaviors per Table 1 trace characteristics.
  3. **Speculative decoding trade-off analysis**: Enable speculative decoding, measure TTFT overhead (draft model prefill), TBT bimodality (success vs. failure modes), and Fluidity Index alongside conventional TPOT to quantify the consistency vs. speed trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated tools be developed to systematically detect evaluation anti-patterns in LLM inference papers without manual expert analysis?
- Basis in paper: [inferred] The assessment methodology (Appendix A) relies entirely on manual expert judgment across 12 recent systems, requiring detailed examination of published materials, configurations, and artifact analysis—a labor-intensive process unlikely to scale with the field's rapid publication rate.
- Why unresolved: The paper provides a qualitative checklist but no algorithmic or automated detection mechanism; anti-pattern identification requires contextual judgment about implementation fairness, workload representativeness, and metric appropriateness.
- What evidence would resolve it: Development and validation of automated analysis tools that can parse paper methodologies and flag anti-patterns with high accuracy compared to expert assessment.

### Open Question 2
- Question: What is the quantitative impact of individual anti-patterns on the validity of reported performance claims across the LLM inference literature?
- Basis in paper: [inferred] While the paper qualitatively demonstrates that anti-patterns "can lead to misleading conclusions" and "inflate the perceived benefits of new techniques," it does not systematically quantify how much each anti-pattern distorts reported results across the surveyed systems.
- Why unresolved: The case study only examines speculative decoding in isolation; no systematic re-evaluation of multiple systems with and without anti-pattern correction to measure effect sizes.
- What evidence would resolve it: Controlled re-evaluation studies applying anti-pattern corrections to published systems and measuring the delta between originally reported and corrected performance claims.

### Open Question 3
- Question: How should evaluation methodologies adapt to emerging model architectures (e.g., Mixture-of-Experts, linear attention variants) that may invalidate current anti-pattern assumptions?
- Basis in paper: [explicit] "New architectures like Mixture of Experts (MoE) and Grouped Query Attention (GQA) exhibit different performance characteristics... Such advances can render evaluation assumptions based on older architectures misleading or irrelevant."
- Why unresolved: The framework derives from current-generation transformer systems; the paper acknowledges continuous architectural evolution but provides no forward-looking methodology for anticipating or adapting to architectural paradigm shifts.
- What evidence would resolve it: Longitudinal analysis tracking how anti-patterns evolve with architectural changes, or principled frameworks for extending evaluation checklists to novel architectures.

## Limitations
- The speculative decoding case study provides concrete evidence but validation across additional techniques (quantization, KV cache optimization, disaggregated serving) remains untested.
- The Fluidity Index metric introduced for consistency measurement represents a novel contribution whose generalizability to other performance anti-patterns needs broader validation.
- The corpus analysis revealed limited direct literature on evaluation methodology, suggesting either the paper identifies genuinely novel problems or these issues have not been previously documented in systematic reviews.

## Confidence

- **High confidence** in the dual-phase resource contention mechanism and its impact on metric interpretation - well-supported by established LLM architecture literature and the demonstrated scheduling delay effects.
- **Medium confidence** in the normalization-dilution mechanism - the mathematical argument is sound, but the practical prevalence of fixed overheads masking performance issues requires more systematic measurement across diverse workloads.
- **Medium confidence** in the speculative decoding bimodal distribution claim - the case study provides strong evidence, but the 75% success rate assumption may not generalize across different draft model qualities or workloads.

## Next Checks

1. **Cross-technique anti-pattern validation**: Apply the evaluation checklist to quantization (e.g., 8-bit vs 4-bit) and KV cache optimization techniques, measuring whether normalized metrics mask critical performance regressions in specific workload regimes.
2. **Consistency metric generalizability**: Test Fluidity Index against additional generation consistency problems like bursty decoding in PagedAttention systems or scheduling-induced stalls in multi-tenant deployments to validate it captures anti-patterns beyond speculative decoding.
3. **Workload breadth validation**: Replicate the speculative decoding case study using the Arxiv Summarization and code generation traces from Table 1 to determine whether the bimodal TBT distribution and consistency trade-offs persist across longer prompts and outputs.