---
ver: rpa2
title: Arithmetic-Intensity-Aware Quantization
arxiv_id: '2512.14090'
source_url: https://arxiv.org/abs/2512.14090
tags:
- quantization
- accuracy
- layers
- arxiv
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Arithmetic-Intensity-Aware Quantization (AIQ) is a mixed-precision
  quantization framework that optimizes per-layer bit-widths to maximize arithmetic
  intensity (AI) while minimizing accuracy loss. It treats quantization as a resource-allocation
  problem in the Roofline model space, using search algorithms to find configurations
  that improve AI and throughput on memory-bound models.
---

# Arithmetic-Intensity-Aware Quantization

## Quick Facts
- arXiv ID: 2512.14090
- Source URL: https://arxiv.org/abs/2512.14090
- Reference count: 18
- Primary result: Mixed-precision quantization framework optimizing per-layer bit-widths to maximize arithmetic intensity while minimizing accuracy loss.

## Executive Summary
Arithmetic-Intensity-Aware Quantization (AIQ) is a post-training mixed-precision quantization framework that optimizes per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. It treats quantization as a resource-allocation problem in the Roofline model space, using search algorithms to find configurations that improve AI and throughput on memory-bound models. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over FP32 while keeping accuracy within 1 percentage point, outperforming uniform INT8/INT4 baselines. For memory-bound MobileNetV2 on CPU, AIQ achieves 1.66× higher throughput than FP32 with minimal accuracy loss.

## Method Summary
AIQ operates by searching over per-layer bit-width configurations (FP32, INT8, INT4) to optimize an objective function combining arithmetic intensity and accuracy loss. The framework profiles model layers to compute FLOPs and memory traffic, then uses greedy or coordinate descent search algorithms to find near-optimal mixed-precision schemes. AI is computed analytically from the quantized configuration, while accuracy loss is measured empirically on small validation subsets. The method exploits the observation that different layers contribute unequally to memory traffic and have heterogeneous sensitivity to quantization, allowing aggressive quantization of high-traffic, low-sensitivity layers while preserving precision elsewhere.

## Key Results
- ResNet-20 on CIFAR-10: AIQ increases AI by ~50% over FP32 while maintaining accuracy within 1 percentage point
- MobileNetV2 on CPU: 1.66× higher throughput than FP32 with minimal accuracy loss
- AIQ naturally quantizes wider and later layers more aggressively, as they contribute more to memory traffic

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing arithmetic intensity (AI) improves throughput specifically for memory-bound workloads.
- **Mechanism:** Quantization reduces memory traffic (bytes transferred) while preserving FLOPs, thus increasing the FLOPs/byte ratio. Per the Roofline model, for kernels below the ridge point, throughput scales linearly with AI until hitting the compute ceiling.
- **Core assumption:** The target deployment is genuinely memory-bound; AI gains on compute-bound kernels do not yield throughput improvements.
- **Evidence anchors:** [abstract] "As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute." [Section I.A] "quantization reduces memory traffic and thus moves layers from memory-bound toward compute-bound regions"
- **Break condition:** Deployment on high-bandwidth hardware where layers are already compute-bound; AI gains will not translate to speedup.

### Mechanism 2
- **Claim:** Per-layer bit-width allocation outperforms uniform quantization by exploiting heterogeneous layer sensitivity and memory contribution.
- **Mechanism:** Different layers contribute unequally to total memory traffic (wider/later layers dominate) and have unequal accuracy sensitivity to quantization. A mixed-precision scheme can aggressively quantize high-traffic, low-sensitivity layers while preserving precision elsewhere.
- **Core assumption:** Accuracy sensitivity and memory contribution are not perfectly correlated; search can discover favorable allocations.
- **Evidence anchors:** [abstract] "AIQ naturally quantizes larger layers more aggressively." [Section III.A, Figure 2] "AI is impacted differently by layer-quantization... quantizing a single layer has a clear theoretical effect on AI but an unpredictable empirical effect on accuracy"
- **Break condition:** Architectures where all layers have similar width and sensitivity; mixed-precision gains diminish.

### Mechanism 3
- **Claim:** Joint optimization of AI and accuracy via search algorithms finds near-Pareto-optimal configurations without requiring gradient-based training.
- **Mechanism:** The AIQ loss L(q;λ) = -λAI(q) + (1-λ)AccLoss(q) trades off AI and accuracy. Since AI(q) is analytically computable from FLOPs and memory traffic, and AccLoss(q) is measured on a small validation set, greedy or coordinate descent search can efficiently explore the combinatorial space.
- **Core assumption:** Small validation sets provide reliable accuracy loss estimates; the loss surface is smooth enough for local search.
- **Evidence anchors:** [Section I.B] "both of the above algorithms are cheap because AI can be recomputed analytically, and evaluation runs use small validation subsets" [Section III.B, Figure 3(b)] "AI-Aware lies on the Pareto frontier, achieving the highest AI while maintaining accuracy above 91 percent"
- **Break condition:** Models with highly non-monotonic accuracy response to quantization; greedy/coordinated descent may trap in local minima.

## Foundational Learning

- **Concept: Roofline Model and Arithmetic Intensity**
  - **Why needed here:** AIQ's core objective is grounded in Roofline theory; understanding memory-bound vs. compute-bound regimes is prerequisite to interpreting results.
  - **Quick check question:** Given a kernel with 100 FLOPs and 50 bytes transferred, what is its AI? If peak bandwidth is 100 GB/s and peak compute is 500 GFLOP/s, is it memory-bound?

- **Concept: Post-Training Quantization (PTQ)**
  - **Why needed here:** AIQ operates on pretrained FP32 models without retraining; PTQ basics clarify what precision reduction does to weights and activations.
  - **Quick check question:** How does uniform INT8 PTQ differ from quantization-aware training (QAT)? What accuracy recovery mechanisms does PTQ typically require?

- **Concept: Mixed-Precision Quantization Search Space**
  - **Why needed here:** AIQ's search operates combinatorially over per-layer bit-widths; understanding search complexity motivates greedy/coordinate descent choices.
  - **Quick check question:** For a 20-layer network with 3 bit-width options (FP32, INT8, INT4), how many configurations exist? Why is exhaustive search impractical?

## Architecture Onboarding

- **Component map:** Profiling module -> Validation evaluator -> Search engine -> Loss aggregator -> Deployment wrapper
- **Critical path:** Profile model layers → Initialize to FP32 → Iterate search (evaluate AI analytically, accuracy empirically) → Select configuration minimizing L(q;λ) → Deploy quantized model
- **Design tradeoffs:**
  - λ selection: Higher λ prioritizes AI gains; lower λ protects accuracy. Paper uses λ=0.9, favoring efficiency.
  - Search algorithm: Greedy is faster per iteration; coordinate descent may find better optima but requires more passes.
  - Bit-width set: Restricting to {INT8, INT4} vs. including FP16 affects granularity vs. search cost.
  - Validation subset size: Smaller sets accelerate search but increase variance in accuracy estimates.
- **Failure signatures:**
  - Accuracy collapse (>5% drop): λ too high; validation set unrepresentative; sensitive layer over-quantized.
  - No throughput gain: Model not memory-bound on target hardware; bandwidth ceiling not reached.
  - Search does not converge: Loss surface highly non-smooth; consider expanding validation set or switching search algorithm.
- **First 3 experiments:**
  1. **Single-layer sensitivity sweep:** Quantize each layer individually to INT4; plot AI gain vs. accuracy drop per layer to validate heterogeneity assumption (replicate Figure 2).
  2. **Greedy vs. coordinate descent comparison:** Run both algorithms on ResNet-20/CIFAR-10 with λ=0.9; compare final AI, accuracy, and wall-clock search time.
  3. **Memory-bound vs. compute-bound deployment:** Deploy AIQ-quantized MobileNetV2 on both a bandwidth-limited CPU (2 vCPU Xeon per paper) and a high-bandwidth GPU; verify throughput gains only materialize on memory-bound configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Arithmetic-Intensity-Aware Quantization (AIQ) maintain its accuracy-efficiency trade-off when scaled to Large Language Models (LLMs) and larger vision architectures?
- **Basis in paper:** [explicit] The conclusion lists "extending AIQ to larger architectures" as a primary next step, noting that current experiments are limited to ResNet-20 and MobileNetV2 on CIFAR-10.
- **Why unresolved:** The paper only validates the method on relatively small image classification models; it is unknown if the greedy or coordinate descent search algorithms remain efficient or effective in the high-dimensional parameter space of LLMs.
- **What evidence would resolve it:** Empirical results showing AIQ performance (accuracy vs. throughput) on standard LLM benchmarks (e.g., WikiText) or large vision datasets (e.g., ImageNet).

### Open Question 2
- **Question:** Does integrating the AIQ objective into Quantization-Aware Training (QAT) yield superior efficiency compared to the current Post-Training Quantization (PTQ) implementation?
- **Basis in paper:** [explicit] The conclusion suggests "integrating AIQ into quantization-aware training to allow for the model to achieve better accuracy-efficiency tradeoffs."
- **Why unresolved:** The current implementation is a post-training method that searches over a fixed pre-trained model; allowing the model to train while optimizing for arithmetic intensity might find better local optima but was not implemented.
- **What evidence would resolve it:** A comparison of the Pareto frontier (accuracy vs. arithmetic intensity) generated by AIQ-PTQ versus a hypothetical AIQ-QAT framework on the same model architecture.

### Open Question 3
- **Question:** Does a per-channel quantization strategy provide significant gains in arithmetic intensity over the per-layer approach currently utilized?
- **Basis in paper:** [explicit] The conclusion identifies "studying per-channel effects of quantization" as a specific next step for future research.
- **Why unresolved:** The current method assigns a single bit-width to all channels within a layer. Channels may have varying sensitivities and memory footprints, suggesting a finer granularity could optimize the AI-accuracy objective further.
- **What evidence would resolve it:** An ablation study comparing the throughput and accuracy of per-layer AIQ schemes against per-channel AIQ schemes on memory-bound models like MobileNetV2.

## Limitations

- Validation limited to two CNN architectures (ResNet-20, MobileNetV2) on CIFAR-10 dataset
- Quantization scheme details (symmetric vs asymmetric, per-tensor vs per-channel) underspecified
- No systematic characterization of boundary between memory-bound and compute-bound regimes where AIQ would or would not help

## Confidence

**High Confidence:** The Roofline-based formulation of quantization as an AI optimization problem is mathematically rigorous. The observation that wider/later layers dominate memory traffic and can be quantized more aggressively is well-supported by the layer sensitivity analysis.

**Medium Confidence:** The empirical efficiency gains (1.66× throughput on MobileNetV2, ~50% AI increase on ResNet-20) are demonstrated, but the generality across architectures, datasets, and deployment scenarios remains unproven. The search algorithms are shown to work on the tested models, but their robustness to different loss landscapes is not established.

**Low Confidence:** The claim that AIQ is particularly beneficial for memory-bound models is supported by deployment on a bandwidth-limited CPU, but the paper does not systematically characterize the boundary between memory-bound and compute-bound regimes where AIQ would or would not help.

## Next Validation Checks

1. **Cross-architecture validation:** Apply AIQ to a transformer-based architecture (e.g., ViT or BERT) and measure whether the AI gains translate to throughput improvements on both memory-bound (CPU) and compute-bound (GPU) deployments.

2. **Sensitivity-accuracy correlation analysis:** Systematically measure per-layer accuracy sensitivity versus memory contribution across multiple architectures to quantify how often the favorable uncorrelated allocation assumption holds.

3. **Search algorithm robustness test:** Apply both greedy and coordinate descent algorithms to a model with known highly non-monotonic accuracy response to quantization (e.g., a very deep network with narrow layers) and compare final AI/accuracy to an exhaustive search baseline on a reduced configuration space.