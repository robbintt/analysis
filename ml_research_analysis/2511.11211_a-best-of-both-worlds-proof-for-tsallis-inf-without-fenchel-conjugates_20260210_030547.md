---
ver: rpa2
title: A Best-of-Both-Worlds Proof for Tsallis-INF without Fenchel Conjugates
arxiv_id: '2511.11211'
source_url: https://arxiv.org/abs/2511.11211
tags:
- algorithm
- have
- regret
- proof
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a simplified proof of the best-of-both-worlds
  regret guarantee for the Tsallis-INF algorithm in multi-armed bandit problems. The
  algorithm combines Follow-The-Regularized-Leader (FTRL) with Tsallis entropy regularization
  to achieve sublinear regret in both adversarial and stochastic settings without
  prior knowledge of the environment.
---

# A Best-of-Both-Worlds Proof for Tsallis-INF without Fenchel Conjugates
## Quick Facts
- arXiv ID: 2511.11211
- Source URL: https://arxiv.org/abs/2511.11211
- Reference count: 3
- The paper provides a simplified proof of the best-of-both-worlds regret guarantee for the Tsallis-INF algorithm in multi-armed bandit problems

## Executive Summary
This paper presents a novel proof technique for the Tsallis-INF algorithm that achieves sublinear regret in both adversarial and stochastic multi-armed bandit settings without using Fenchel conjugates. The algorithm employs Follow-The-Regularized-Leader (FTRL) with Tsallis entropy regularization and an importance-weighted estimator for bandit feedback. The key innovation is avoiding the traditional Fenchel conjugate analysis while maintaining the same regret guarantees.

The authors demonstrate that Tsallis-INF achieves O(√T) regret in the adversarial setting and O(lnT) regret in the stochastic setting, providing what is known as "best-of-both-worlds" performance. The proof relies on modern online convex optimization techniques, particularly FTRL with local norm analysis, making it more accessible than previous approaches that required complex conjugate function calculations.

## Method Summary
The method uses FTRL with Tsallis entropy regularization, where the algorithm maintains a probability distribution over arms using a time-varying learning rate. The Tsallis entropy is defined as H_α(q) = (1/(α-1))(Σq_i^α - 1) for α = 2, which gives the Tsallis entropy of order 2. The algorithm employs an importance-weighted estimator for bandit feedback, where the estimated loss for played arm i at time t is ŷ_t,i = y_t,i/π_t,i if i = I_t (the arm played), and 0 otherwise. The algorithm then updates the distribution using FTRL with the Tsallis entropy regularizer and the estimated cumulative loss.

## Key Results
- Achieves O(√T) expected regret in adversarial settings: bounded by 32G√(d-1)T
- Achieves O(lnT) pseudo-regret in stochastic settings: bounded by 256G²Σᵢ:µᵢ≠µ⋆(1+lnT)/∆ᵢ
- Avoids Fenchel conjugates while maintaining best-of-both-worlds guarantees
- Uses importance-weighted estimators for bandit feedback with Tsallis entropy regularization

## Why This Works (Mechanism)
The mechanism works by leveraging the specific properties of Tsallis entropy of order 2, which provides a unique regularization structure that enables both adversarial and stochastic regret bounds. The Tsallis entropy creates a distribution that is sufficiently concentrated to achieve logarithmic regret in stochastic settings while maintaining enough exploration to handle adversarial environments. The FTRL framework with local norm analysis allows the algorithm to adapt to the geometry of the problem space without requiring the complex analysis of Fenchel conjugates.

## Foundational Learning
1. **Follow-The-Regularized-Leader (FTRL)** - Needed to understand the core algorithmic framework; quick check: verify the update equation maintains a valid probability distribution
2. **Tsallis Entropy** - Special case with α=2 that provides the regularization; quick check: confirm the entropy term satisfies the required convexity properties
3. **Importance-weighted Estimators** - Required for bandit feedback setting; quick check: verify unbiasedness of the loss estimator
4. **Local Norm Analysis** - Modern OCO technique replacing Fenchel conjugates; quick check: confirm the local norm bounds are correctly applied
5. **Best-of-Both-Worlds Guarantees** - Framework for simultaneous adversarial and stochastic performance; quick check: verify the regret bounds hold under both settings
6. **Multi-armed Bandit Feedback** - Partial information setting where only played arm's loss is observed; quick check: confirm the algorithm handles bandit feedback correctly

## Architecture Onboarding
**Component Map:** FTRL -> Tsallis Entropy Regularization -> Importance-Weighted Estimator -> Time-varying Learning Rate
**Critical Path:** Loss observation → Importance-weighted estimation → FTRL update → Distribution update
**Design Tradeoffs:** Tsallis entropy provides aggressive exploitation in stochastic settings while maintaining sufficient exploration for adversarial robustness; simpler than Fenchel conjugate analysis but requires careful handling of the specific entropy properties
**Failure Signatures:** Poor performance in either setting likely indicates incorrect regularization strength or learning rate schedule; distribution collapse or excessive exploration suggests parameter misconfiguration
**First Experiments:**
1. Verify O(√T) regret bound in synthetic adversarial environment with known loss sequences
2. Confirm O(lnT) pseudo-regret in stochastic setting with Gaussian rewards and clear suboptimality gaps
3. Test algorithm behavior with varying numbers of arms (d) to validate √(d-1) scaling

## Open Questions the Paper Calls Out
None

## Limitations
- The proof technique may not generalize to other regularization functions beyond Tsallis entropy
- The specific constants (32G√(d-1)T and 256G²Σᵢ:µᵢ≠µ⋆(1+lnT)/∆ᵢ) may not be tight and require further optimization
- The approach relies heavily on the specific properties of Tsallis entropy of order 2, limiting flexibility in algorithm design

## Confidence
- Main claim validity: High
- Technical details verification: Medium
- Specific regret constants: Medium
- Generalization potential: Low

## Next Checks
1. Verify the local norm analysis steps and confirm they correctly handle the Tsallis entropy regularization without requiring Fenchel conjugate arguments
2. Check the bound on the variance of the importance-weighted estimator under bandit feedback conditions
3. Validate the time-varying learning rate schedule and confirm it properly balances the adversarial and stochastic regret terms