---
ver: rpa2
title: 'NILMFormer: Non-Intrusive Load Monitoring that Accounts for Non-Stationarity'
arxiv_id: '2506.05880'
source_url: https://arxiv.org/abs/2506.05880
tags:
- consumption
- nilmformer
- power
- data
- appliance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NILMFormer, a transformer-based architecture
  for non-intrusive load monitoring (NILM) that addresses the non-stationarity challenge
  in smart meter data. NILMFormer incorporates a stationarization/destationarization
  scheme and a novel timestamp-based positional encoding (TimeRPE) to handle distribution
  drift in subsequences.
---

# NILMFormer: Non-Intrusive Load Monitoring that Accounts for Non-Stationarity

## Quick Facts
- arXiv ID: 2506.05880
- Source URL: https://arxiv.org/abs/2506.05880
- Reference count: 40
- Primary result: Transformer-based NILM architecture achieving up to 51% and 151% improvements in daily and monthly consumption prediction accuracy respectively

## Executive Summary
NILMFormer introduces a transformer-based architecture for non-intrusive load monitoring that specifically addresses non-stationarity in smart meter data. The approach incorporates a stationarization/destationarization scheme to mitigate distribution drift and a novel timestamp-based positional encoding (TimeRPE) to handle temporal patterns. Experiments on four real-world datasets demonstrate significant performance improvements over state-of-the-art NILM approaches. The solution has been deployed at scale in EDF's Mon Suivi Conso service, providing detailed appliance consumption insights to millions of customers.

## Method Summary
NILMFormer is a sequence-to-sequence transformer architecture that addresses non-stationarity in NILM through z-normalization of input subsequences and learned projections to restore statistics for output denormalization. The architecture uses a 4-layer dilated convolutional embedding block, TimeRPE for timestamp-based positional encoding, and a 3-layer transformer with Multi-Head DMSA attention. TokenStats concatenates projected input statistics as an additional token, which is removed before the prediction head. The model is trained with MSE loss and evaluated on public (UK-DALE, REFIT) and private (EDF1, EDF2) datasets.

## Key Results
- Achieved up to 51% improvement in daily consumption prediction accuracy
- Achieved up to 151% improvement in monthly consumption prediction accuracy
- Outperformed state-of-the-art NILM approaches on four real-world datasets

## Why This Works (Mechanism)

### Mechanism 1: Subsequence Stationarization/Destationarization
Normalizing input subsequences and learning to project their removed statistics for output denormalization mitigates distribution drift. The model z-normalizes inputs, projects (μ, σ) to TokenStats, and separately learns projections Proj(μ), Proj(σ) to rescale outputs. This works because appliances share pattern shapes that can be learned on stationary inputs while power magnitude differences are restored via learned rescaling.

### Mechanism 2: TimeRPE Timestamp-Based Positional Encoding
Using timestamp-derived encodings instead of fixed or learnable position embeddings improves generalization by encoding periodic usage patterns. The model decomposes timestamps into minute/hour/day/month, embeds via sinusoidal projections followed by 1D convolution, and concatenates to feature maps. This captures temporal regularity in appliance usage patterns.

### Mechanism 3: TokenStats Injection and Transformer Mixing
Propagating removed statistics as a dedicated token enables the Transformer to condition on global distributional context. The model projects (μ, σ) to a d-dimensional TokenStats, concatenates along the time dimension as an additional token, and removes it post-Transformer for the prediction head. This provides global context for the network to modulate its representations.

## Foundational Learning

**Concept: Distribution Shift/Non-Stationarity**
- Why needed here: The core challenge is that smart meter subsequences exhibit varying means/stds
- Quick check question: Can you explain why a model trained on one distribution might fail on another?

**Concept: Transformers & Positional Encoding**
- Why needed here: NILMFormer relies on a Transformer backbone and introduces a custom timestamp-based PE
- Quick check question: Why do Transformers require explicit positional information?

**Concept: Normalization for Time Series**
- Why needed here: Understanding z-normalization and why restoring it via learned projections is novel
- Quick check question: What is z-normalization and why might simply restoring original statistics be suboptimal?

## Architecture Onboarding

**Component map:** Input subsequence -> z-normalization (extract μ, σ) -> Embedding Block + TimeRPE -> concatenate TokenStats -> Transformer Block -> remove TokenStats -> Head -> denormalize via ProjStats

**Critical path:** The stationarization/destationarization scheme enables the transformer to learn appliance patterns on normalized inputs while the TimeRPE provides temporal context through concatenated timestamp features.

**Design tradeoffs:** Allocating d/4 channels to TimeRPE was optimal; d/2 provided no gain. 3 Transformer layers with d=96 balanced accuracy and parameter count for deployment. Concatenating PE (vs. adding) yielded better performance.

**Failure signatures:** Underperformance on always-on appliances (e.g., Fridge) due to stationarization interfering with baseline isolation. RevIN-style restoration led to worse performance.

**First 3 experiments:** 1) Ablate TokenStats/ProjStats to confirm necessity. 2) Compare TimeRPE (concat) vs. other PEs. 3) Vary positional encoding ratio (d/8, d/4, d/2) to find optimal budget.

## Open Questions the Paper Calls Out

**Open Question 1:** How can NILMFormer's stationarization scheme be modified to better handle always-on appliances with constant baseline consumption, such as refrigerators? The z-normalization approach removes baseline information that is discriminative for always-on appliances, creating fundamental tension with the stationarization objective.

**Open Question 2:** Can NILMFormer be extended to jointly model multiple appliances in a single forward pass rather than training separate models per appliance? Joint multi-appliance modeling could capture inter-appliance dependencies but requires architectural changes to the current sequence-to-sequence design.

**Open Question 3:** Does TimeRPE's timestamp-based encoding inadvertently encode dataset-specific temporal patterns that limit transferability across different time periods or deployment contexts? The paper evaluates contemporaneous data but does not test temporal transferability or analyze whether TimeRPE causes overfitting to specific temporal patterns.

## Limitations
- The stationarization mechanism struggles with always-on appliances like refrigerators where baseline consumption is the key discriminative feature
- The architecture trains separate models per appliance rather than jointly modeling all appliances, limiting computational efficiency
- The TimeRPE positional encoding may encode region-specific temporal patterns that limit generalization to different geographic contexts

## Confidence
- **High:** The stationarization/destationarization mechanism's conceptual validity and its role in handling distribution drift
- **Medium:** The TimeRPE positional encoding design and its concatenation methodology
- **Medium:** The overall architectural framework and hyperparameter choices

## Next Checks
1. Implement DMSA attention as standard causal attention first, then refine once mechanism details are clarified in source code
2. Verify embedding block dilation rates by examining the provided source code repository
3. Test the stationarization scheme on a simpler baseline model before full NILMFormer implementation to isolate its impact