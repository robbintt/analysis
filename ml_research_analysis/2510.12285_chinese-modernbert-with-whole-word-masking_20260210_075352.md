---
ver: rpa2
title: Chinese ModernBERT with Whole-Word Masking
arxiv_id: '2510.12285'
source_url: https://arxiv.org/abs/2510.12285
tags:
- chinese
- arxiv
- modernbert
- masking
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chinese ModernBERT, a from-scratch Chinese
  encoder designed to leverage recent advances in encoder-only Transformers. It addresses
  the gap between English and Chinese encoders by incorporating a hardware-aware 32k
  BPE vocabulary, whole-word masking with a dynamic curriculum, a two-stage long-context
  pre-training pipeline, and a damped-cosine learning rate schedule.
---

# Chinese ModernBERT with Whole-Word Masking
## Quick Facts
- arXiv ID: 2510.12285
- Source URL: https://arxiv.org/abs/2510.12285
- Authors: Zeyu Zhao; Ningtao Wang; Xing Fu; Yu Cheng
- Reference count: 35
- One-line primary result: Chinese ModernBERT achieves competitive CLUE benchmark results and new long-sequence inference throughput under bf16.

## Executive Summary
Chinese ModernBERT is a from-scratch Chinese encoder designed to bridge the gap between English and Chinese transformer models. It incorporates recent advances in encoder-only Transformers, including a hardware-aware 32k BPE vocabulary, whole-word masking with a dynamic curriculum, a two-stage long-context pre-training pipeline, and a damped-cosine learning rate schedule. The model is pre-trained on approximately 1.2 trillion Chinese tokens from curated datasets. Chinese ModernBERT sets new benchmarks for long-sequence inference throughput under bf16 and achieves competitive results on the CLUE benchmark.

## Method Summary
Chinese ModernBERT introduces several innovations to address the limitations of existing Chinese encoders. It uses a hardware-aware 32k BPE vocabulary for efficient tokenization, whole-word masking with a dynamic curriculum to improve pre-training, and a two-stage long-context pre-training pipeline for better handling of extended sequences. The damped-cosine learning rate schedule is employed to stabilize training. The model is pre-trained on a large corpus of approximately 1.2 trillion Chinese tokens, curated from diverse sources. These design choices enable Chinese ModernBERT to achieve competitive performance on downstream tasks, particularly in semantic textual similarity (STS), where it surpasses Qwen-0.6B-embedding on the SimCLUE test set when fine-tuned with open contrastive data.

## Key Results
- Chinese ModernBERT achieves competitive results on the CLUE benchmark.
- Sets new benchmarks for long-sequence inference throughput under bf16.
- Surpasses Qwen-0.6B-embedding on the SimCLUE test set for semantic textual similarity when fine-tuned with open contrastive data.

## Why This Works (Mechanism)
Chinese ModernBERT leverages recent advances in encoder-only Transformers to address the gap between English and Chinese encoders. The hardware-aware 32k BPE vocabulary ensures efficient tokenization and reduced vocabulary size, which is critical for hardware optimization. Whole-word masking with a dynamic curriculum improves the model's ability to learn meaningful representations by focusing on complete words during pre-training. The two-stage long-context pre-training pipeline allows the model to handle extended sequences effectively, while the damped-cosine learning rate schedule stabilizes training and improves convergence. These mechanisms collectively enhance the model's performance on downstream tasks, particularly in semantic textual similarity and long-sequence inference.

## Foundational Learning
- **BPE Vocabulary**: Why needed: Reduces vocabulary size and improves hardware efficiency. Quick check: Compare tokenization speed and memory usage with other vocabularies.
- **Whole-Word Masking**: Why needed: Encourages the model to learn complete word representations. Quick check: Evaluate downstream performance with and without whole-word masking.
- **Dynamic Curriculum**: Why needed: Adapts the difficulty of masking tasks during pre-training. Quick check: Monitor training loss curves to ensure smooth learning progression.
- **Long-Context Pre-training**: Why needed: Enables handling of extended sequences. Quick check: Test model performance on tasks requiring long-context understanding.
- **Damped-Cosine Learning Rate Schedule**: Why needed: Stabilizes training and improves convergence. Quick check: Compare training stability and final performance with standard cosine schedules.

## Architecture Onboarding
- **Component Map**: Tokenizer (BPE) -> Pre-training (whole-word masking, dynamic curriculum) -> Long-context fine-tuning -> Downstream tasks
- **Critical Path**: Pre-training with whole-word masking and dynamic curriculum -> Long-context fine-tuning -> Downstream task fine-tuning
- **Design Tradeoffs**: Hardware-aware BPE reduces vocabulary size but may lose some linguistic nuances; whole-word masking improves representation learning but increases pre-training complexity.
- **Failure Signatures**: Poor tokenization due to BPE vocabulary; unstable training without proper learning rate scheduling; suboptimal performance on long-sequence tasks without long-context pre-training.
- **First Experiments**: 1) Evaluate tokenization efficiency and quality. 2) Test pre-training stability with different learning rate schedules. 3) Benchmark long-sequence inference throughput.

## Open Questions the Paper Calls Out
None.

## Limitations
- Reproducibility of claimed throughput gains is uncertain due to unspecified hardware configurations and benchmarking protocols.
- The impact of whole-word masking with dynamic curriculum and the damped-cosine learning rate schedule on downstream performance is not thoroughly dissected.
- The curation process for the 1.2 trillion token corpus is not detailed, raising questions about potential dataset biases or quality inconsistencies.

## Confidence
- Claims about CLUE benchmark results: Medium
- Claims about long-sequence inference throughput under bf16: Medium
- Claims about STS performance on SimCLUE: Medium

## Next Checks
1. Release the full training and evaluation codebase, including hardware-specific configurations, to enable independent reproduction of throughput benchmarks.
2. Conduct ablation studies to isolate the effects of whole-word masking with dynamic curriculum and the damped-cosine learning rate schedule on downstream performance.
3. Perform a detailed error analysis on the SimCLUE test set to assess whether gains in STS are consistent across diverse linguistic phenomena and domains.