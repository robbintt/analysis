---
ver: rpa2
title: 'CADDI: An in-Class Activity Detection Dataset using IMU data from low-cost
  sensors'
arxiv_id: '2503.02853'
source_url: https://arxiv.org/abs/2503.02853
tags:
- data
- dataset
- activities
- sensor
- activity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CADDI, a novel dataset for in-class activity
  detection using low-cost IMU sensors from smartwatches. The dataset includes 19
  activities (9 continuous, 10 instantaneous) performed by 12 participants in classroom-like
  settings, with synchronized accelerometer, gyroscope, rotation vector data, and
  stereo camera images.
---

# CADDI: An in-Class Activity Detection Dataset using IMU data from low-cost sensors

## Quick Facts
- arXiv ID: 2503.02853
- Source URL: https://arxiv.org/abs/2503.02853
- Reference count: 21
- Primary result: Novel dataset for in-class activity detection using IMU data, validated with 65% overall accuracy (77% instantaneous, 45% continuous)

## Executive Summary
This paper introduces CADDI, a novel dataset for in-class activity detection using low-cost IMU sensors from smartwatches. The dataset includes 19 activities (9 continuous, 10 instantaneous) performed by 12 participants in classroom-like settings, with synchronized accelerometer, gyroscope, rotation vector data, and stereo camera images. The authors validate the dataset using a deep learning model (1D CNN + LSTM), achieving 65% overall accuracy, with 77% for instantaneous actions and 45% for continuous ones. The work addresses the lack of labeled educational activity datasets and provides a comprehensive resource for developing multimodal activity recognition systems in educational environments.

## Method Summary
The dataset was collected using Samsung Galaxy Watch 5 sensors (accelerometer, gyroscope, rotation vector) at 100 Hz and ZED stereo cameras at 25-30 fps, synchronized via network time protocol. Activities were performed by 12 participants across 19 classes (9 continuous, 10 instantaneous) in classroom settings. Data was preprocessed into 2-second windows with 1-second overlap, then classified using a 1D CNN + LSTM architecture with leave-subject-out cross-validation (9 subjects for training, 2 for validation).

## Key Results
- 65% overall classification accuracy on 19 in-class activities
- 77% accuracy for instantaneous actions vs 45% for continuous activities
- Dataset includes 472 minutes of synchronized IMU and stereo visual data from 12 participants

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining IMU and visual data through temporal synchronization enables more robust activity recognition than single-modality approaches.
- **Mechanism:** IMU sensors capture motion dynamics at 100Hz while stereo cameras provide spatial context at 25-30fps. Timestamp alignment allows cross-modal validation and potential fusion architectures.
- **Core assumption:** Network time synchronization via router adequately aligns sensor streams; the buffering strategy preserves temporal fidelity.
- **Evidence anchors:** Synchronized stereo images offer comprehensive resource for developing multimodal algorithms; images and IMU data can be easily matched together; related work demonstrates cross-modal learning improves HAR performance.
- **Break condition:** WiFi latency spikes causing >50ms desynchronization; packet loss disrupting the 1-second batching rhythm.

### Mechanism 2
- **Claim:** Sliding window segmentation with CNN-LSTM architecture captures both local motion features and temporal dependencies for activity classification.
- **Mechanism:** 2-second windows with 1-second overlap provide input to 1D convolutions for local feature extraction, followed by LSTM layers for sequential pattern learning across the window.
- **Core assumption:** Classroom activities exhibit consistent temporal patterns within 2-second observation windows; overlap provides continuity across transitions.
- **Evidence anchors:** Deep learning model achieving 65% overall accuracy; data segmented into 2-second windows with 1-second overlap; related work suggests decomposition strategies may help continuous activity challenges.
- **Break condition:** Activities requiring temporal context >2 seconds; rapid transitions between activities occurring within single windows; class imbalance causing temporal feature collapse.

### Mechanism 3
- **Claim:** Instantaneous activities achieve higher classification accuracy than continuous activities due to their distinct, bounded temporal signatures.
- **Mechanism:** Instantaneous actions produce short, high-amplitude signal bursts with clear start/end boundaries. Continuous activities exhibit sustained but variable patterns that overlap across classes.
- **Core assumption:** The 20-repetition protocol for instantaneous activities captures sufficient intra-class variability; continuous activity 200-second samples represent stable behavioral patterns.
- **Evidence anchors:** 77% for instantaneous actions and 45% for continuous ones; model performs better on instantaneous actions; related work addresses resource-efficient hierarchical recognition.
- **Break condition:** Instantaneous activities with variable execution speeds blurring temporal boundaries; continuous activities with intermittent pauses or style changes; user-specific variations exceeding training distribution.

## Foundational Learning

- **Concept: IMU Sensor Fundamentals (Accelerometer, Gyroscope, Rotation Vector)**
  - Why needed here: Dataset provides triaxial accelerometer, gyroscope, and rotation vector—understanding what each measures and their coordinate frames is essential for feature engineering.
  - Quick check question: Given the smartwatch axes, would raising the right hand (watch on right wrist) produce primarily Y-axis or Z-axis acceleration changes?

- **Concept: Sliding Window Segmentation with Overlap**
  - Why needed here: The benchmark model uses 2-second windows with 50% overlap; understanding trade-offs between window size and overlap is critical for model design.
  - Quick check question: If you reduced window size to 1 second while keeping 0.5-second overlap, how would this affect detection of continuous activities like "typing on keyboard"?

- **Concept: Leave-Subject-Out Cross-Validation**
  - Why needed here: Training on 9 participants and validating on 2 held-out participants tests generalization to new users—this is stricter than random split.
  - Quick check question: Why might this validation strategy underestimate real-world performance if the smartwatch is calibrated per-user before deployment?

## Architecture Onboarding

- **Component map:**
  ```
  Data Collection Layer:
    Samsung Galaxy Watch 5 → [Accel(100Hz), Gyro(100Hz), Rotation(100Hz), HR(1Hz), Light(5Hz)]
    ZED Stereo Camera → [Left/Right 1080p images @ 25-30fps]
    WiFi Router → Data packets from watch to computer
  
  Preprocessing Layer:
    JSON parser → 100-sample batches per second
    Timestamp alignment → IMU-to-image synchronization
    Window generator → 2-second segments, 1-second overlap
  
  Model Layer (Benchmark):
    1D Conv → Local feature extraction, dimensionality reduction
    2x LSTM → Sequential temporal modeling
    Dense → 19-class classification output
  ```

- **Critical path:** Watch sensor sampling (100Hz continuous) → WiFi batching (1 packet/second) → Computer timestamp labeling → Window segmentation → CNN-LSTM inference. Latency at the WiFi transmission step determines real-time feasibility.

- **Design tradeoffs:**
  - Window size: 2 seconds balances temporal context vs. responsiveness; shorter windows hurt continuous activity detection (already at 45%), longer windows increase latency
  - Overlap: 50% provides redundancy at 2x computation cost; reduces missed detections at activity boundaries
  - Sensor selection: IMU-only deployment is feasible (privacy-preserving, low-cost); visual modality adds context but raises privacy concerns and hardware requirements
  - Subject split: Leave-subject-out is rigorous but pessimistic; per-user calibration could improve continuous activity performance

- **Failure signatures:**
  - Low continuous activity accuracy (45%) suggests class confusion between similar sustained motions
  - t-SNE visualization shows clustering between similar activities (e.g., retrieving from backpack/pocket, drawing/writing)
  - Variable-duration instantaneous activities may span multiple windows or fall within single windows unpredictably

- **First 3 experiments:**
  1. **Baseline reproduction:** Train the 1D CNN + LSTM on provided data splits; verify 65% overall accuracy and per-class breakdown; analyze confusion matrix to identify which continuous activity pairs are most confused.
  2. **Window size ablation:** Test 1-second, 2-second (baseline), and 4-second windows on instantaneous vs. continuous activity subsets; hypothesis: longer windows improve continuous activity accuracy but increase latency.
  3. **Modality comparison:** Train IMU-only model vs. multimodal IMU + visual model (using pre-trained image features); hypothesis: visual modality provides context that disambiguates similar continuous activities.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can multimodal architectures leveraging the synchronized stereo images significantly outperform the IMU-only baseline provided?
- **Basis in paper:** The authors include synchronized stereo images specifically to offer a "comprehensive resource for developing multimodal algorithms," yet the technical validation relies solely on a 1D CNN+LSTM using IMU data.
- **Why unresolved:** The potential improvement in classification accuracy or robustness gained by fusing visual data with sensor streams remains unquantified in the current work.
- **What evidence would resolve it:** Benchmark results from fusion models (e.g., RGB + IMU) compared against the current 65% baseline accuracy.

### Open Question 2
- **Question:** How can the recognition accuracy for continuous activities be improved, given the substantial performance gap compared to instantaneous actions?
- **Basis in paper:** The technical validation highlights a significant disparity, with the model achieving 77% accuracy on instantaneous actions but only 45% on continuous ones.
- **Why unresolved:** The paper establishes this low baseline for continuous actions but does not explore specific architectural or feature-engineering solutions to distinguish these similar, prolonged states.
- **What evidence would resolve it:** New models demonstrating improved separation of continuous classes in the confusion matrix, likely through long-term temporal dependencies or attention mechanisms.

### Open Question 3
- **Question:** What is the effect of the one-second IMU data buffering on the temporal precision of detecting instantaneous activities?
- **Basis in paper:** The Methods section notes that records are buffered in blocks of 100 readings (one second) to manage network load, rather than streaming in real-time.
- **Why unresolved:** The paper asserts the setup is for "in-class activity detection," but the impact of this deliberate buffering latency on the precise alignment of fast, instantaneous events with camera frames is not analyzed.
- **What evidence would resolve it:** An error analysis quantifying the temporal misalignment between the buffered IMU timestamps and the ground-truth visual timestamps for instantaneous gestures.

## Limitations
- Single deep learning architecture without ablation studies on window size, overlap, or sensor modality selection
- 45% accuracy for continuous activities may reflect model design limitations rather than fundamental data constraints
- Leave-subject-out validation strategy may underestimate real-world performance with user-specific calibration

## Confidence
- **High confidence**: Dataset creation methodology is sound, synchronization approach is technically feasible, activity taxonomy is comprehensive
- **Medium confidence**: Benchmark model architecture is described sufficiently for reproduction, but critical hyperparameters and preprocessing details are unspecified
- **Low confidence**: 45% accuracy for continuous activities may not reflect fundamental data limitations rather than model design choices

## Next Checks
1. **Architectural ablation study**: Systematically vary window size (1s, 2s, 4s) and overlap (0%, 50%, 75%) to determine optimal temporal segmentation for continuous vs. instantaneous activities.
2. **Modality importance analysis**: Train IMU-only, visual-only, and multimodal models to quantify the contribution of each sensor stream and identify which continuous activity confusions could be resolved with visual context.
3. **Cross-environment robustness**: Test model performance across different classroom configurations (lighting, desk types, seating arrangements) to establish generalization beyond the controlled collection environment.