---
ver: rpa2
title: Active Learning with Task-Driven Representations for Messy Pools
arxiv_id: '2510.25926'
source_url: https://arxiv.org/abs/2510.25926
tags:
- learning
- active
- representations
- acquisition
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of active learning in "messy"
  pools, where the unlabeled data contains widely varying relevance to the target
  task, such as class imbalance, redundant classes, and redundant information. The
  authors show that using unsupervised representations, which are commonly used in
  state-of-the-art methods, can undermine effectiveness in dealing with messy pools
  because they are task-agnostic and may fail to capture all task-relevant information.
---

# Active Learning with Task-Driven Representations for Messy Pools

## Quick Facts
- arXiv ID: 2510.25926
- Source URL: https://arxiv.org/abs/2510.25926
- Reference count: 40
- Key outcome: Task-driven representations significantly improve active learning performance on messy pools with class imbalance and redundancy, achieving up to 99.56% accuracy on F+MNIST.

## Executive Summary
This paper addresses the challenge of active learning in "messy" pools where unlabeled data contains widely varying relevance to the target task, such as class imbalance, redundant classes, and redundant information. The authors show that standard unsupervised representations undermine effectiveness in these scenarios because they are task-agnostic and may fail to capture all task-relevant information. They propose using task-driven representations that are periodically updated during the active learning process using previously collected labels, demonstrating significant improvements over state-of-the-art methods across three benchmark datasets.

## Method Summary
The authors propose updating unsupervised representations periodically during active learning using acquired labels. They introduce two strategies: (1) TD-SPLIT, which partitions the latent space into class-relevant and class-irrelevant components using semi-supervised learning, and (2) TD-FT, which fine-tunes an initial unsupervised representation with supervision from collected labels. The method uses EPIG (Expected Predictive Information Gain) as the acquisition function and decouples the representation learning from the prediction head by using a simple guidance classifier for encoder updates and a Random Forest for acquisition scoring. The encoder is retrained every 5 acquisition rounds on the accumulated labeled data.

## Key Results
- TD-FT achieves 99.56%±0.10 accuracy on F+MNIST, outperforming state-of-the-art methods by over 1%.
- Both TD-SPLIT and TD-FT significantly outperform using fixed unsupervised representations across all three datasets (F+MNIST, CIFAR-10+100, CheXpert).
- TD-FT is more computationally efficient than TD-SPLIT while achieving better performance.
- The split representation approach shows promise but suffers from instability in the semi-supervised learning component.

## Why This Works (Mechanism)

### Mechanism 1: Task-Driven Alignment of Latent Similarity
Periodic fine-tuning of representations prevents dilution of task-relevant features in messy pools. Unsupervised representations capture generic variations (e.g., scan angle, background) that dominate latent space in messy pools. By fine-tuning the encoder using acquired labels, the representation is forced to prioritize features predictive of the target class, effectively "sharpening" the notion of similarity relevant to the downstream task.

### Mechanism 2: Decoupling Representation Learning from Acquisition
Separating the "guidance classifier" (for encoder updates) from the "prediction head" (for acquisition) stabilizes the active learning loop. The guidance classifier is used solely to generate gradients for updating the encoder, while the prediction head provides stable, calibrated uncertainty estimates without suffering from gradient instability of deep networks in low-data regimes.

### Mechanism 3: Targeted Uncertainty Reduction (EPIG)
EPIG constrains acquisition to points that reduce uncertainty specifically regarding a target distribution p*(x*). When combined with task-driven representations, this ensures that "relevance" is doubly enforced—once by the representation (which ignores irrelevant features) and once by the acquisition function (which ignores irrelevant regions of the pool).

## Foundational Learning

- **Concept**: **Variational Autoencoders (VAE) & Contrastive Learning (SimCLR)**
  - **Why needed here**: The paper builds baselines and TD-SPLIT on VAEs, while TD-FT builds on SimCLRv2. Understanding how these architectures learn representations without labels explains why they fail in messy pools.
  - **Quick check question**: Can you explain why a standard SimCLR loss might prioritize learning about background scenery over a small foreground object if the scenery varies more across the dataset?

- **Concept**: **Bayesian Active Learning (BALD & EPIG)**
  - **Why needed here**: The paper uses EPIG as the primary acquisition function. Understanding the difference between BALD (reducing model uncertainty generally) and EPIG (reducing predictive uncertainty on a target) is crucial for interpreting results.
  - **Quick check question**: How does EPIG specifically address the "redundant class" problem better than standard entropy sampling?

- **Concept**: **Semi-Supervised Learning (CCVAE)**
  - **Why needed here**: The TD-SPLIT method uses a CCVAE objective that modifies a VAE to force a subset of latents (zc) to correlate with labels.
  - **Quick check question**: Why would you want to partition the latent space into zc (class-relevant) and z\(backslash c\) (class-irrelevant) rather than just forcing the whole vector z to represent the class?

## Architecture Onboarding

- **Component map**: Unlabeled Pool -> Encoder (ResNet18/50) -> Latent Embeddings -> Guidance Classifier (1-layer NN) + Random Forest (Prediction Head) -> EPIG Scores -> Acquisition Engine

- **Critical path**:
  1. Pretrain Encoder on entire unlabeled pool (unsupervised)
  2. Initialize with small random labeled set
  3. Loop: Fine-tune Encoder using current labels, train Random Forest on latents, calculate EPIG scores, acquire top-k points, add to labeled set

- **Design tradeoffs**:
  - TD-FT vs. TD-SPLIT: TD-FT is computationally cheaper and performed best; TD-SPLIT is theoretically cleaner for disentanglement but slower
  - Update frequency (k): Authors update every 5 rounds; too frequent creates disconnect, too infrequent misses information

- **Failure signatures**:
  - Performance < Random: Failing to handle messy classes; ensure redundant classes are labeled as "other"
  - Stagnant Accuracy: Guidance classifier may be overfitting; check classifier capacity
  - High Compute Cost: Using TD-SPLIT with full retraining; switch to TD-FT

- **First 3 experiments**:
  1. Baseline Check: Run EPIG with fixed SimCLRv2 encoder on F+MNIST; verify accuracy < 98.53%
  2. Mechanism Validation: Implement TD-FT loop; check if accuracy improves to ~99.5%
  3. Ablation: Replace Random Forest prediction head with Neural Network to verify RF provides better uncertainty estimates

## Open Questions the Paper Calls Out

- **Open Question 1**: Does aggregating all non-target classes into a single "redundant" category negatively impact learned representations compared to retaining distinct labels for irrelevant data?
- **Open Question 2**: Can the optimal frequency for updating the representation encoder (k) be determined dynamically during active learning?
- **Open Question 3**: To what extent does the mismatch between the prediction head used for acquisition and the updated representation contribute to acquisition bias in high-budget scenarios?

## Limitations
- Representation quality dependence: Effectiveness relies heavily on quality of initial unsupervised representation
- Computational overhead: Requires retraining encoder every 5 acquisition rounds, which may be prohibitive for large-scale applications
- Target distribution sensitivity: EPIG's effectiveness depends on appropriateness of the target distribution p*(x*)

## Confidence
- TD-FT significantly outperforms unsupervised representations: High confidence
- Task-driven representations are necessary for messy pools: Medium confidence
- EPIG acquisition is superior to other methods: High confidence
- Decoupling guidance classifier from prediction head improves stability: Medium confidence

## Next Checks
1. **Distribution Shift Robustness**: Test TD-FT on a dataset where the initial unlabeled pool contains a different distribution than the target classes to validate robustness to distribution shift.

2. **Minimal Supervision Analysis**: Systematically vary the initial labeled set size to determine the minimum supervision needed for effective fine-tuning without overfitting.

3. **Alternative Acquisition Functions**: Replace EPIG with standard BALD or entropy-based acquisition while keeping TD-FT representation updates to isolate whether representation update or acquisition function drives performance gains.