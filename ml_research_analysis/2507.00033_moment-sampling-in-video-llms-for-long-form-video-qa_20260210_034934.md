---
ver: rpa2
title: Moment Sampling in Video LLMs for Long-Form Video QA
arxiv_id: '2507.00033'
source_url: https://arxiv.org/abs/2507.00033
tags:
- video
- moment
- sampling
- frames
- frame
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "moment sampling," a novel, model-agnostic
  approach to improve long-form video question answering (VideoQA) by intelligently
  selecting frames based on their relevance to the question. Unlike traditional uniform
  frame sampling, which often misses crucial frames or includes redundant ones, moment
  sampling leverages a pre-trained moment retrieval model to guide frame selection.
---

# Moment Sampling in Video LLMs for Long-Form Video QA

## Quick Facts
- arXiv ID: 2507.00033
- Source URL: https://arxiv.org/abs/2507.00033
- Reference count: 40
- This paper introduces moment sampling, a novel, model-agnostic approach to improve long-form video question answering by intelligently selecting frames based on their relevance to the question.

## Executive Summary
This paper addresses the challenge of long-form VideoQA by introducing moment sampling, a method that uses a pre-trained moment retrieval model to guide frame selection. Unlike traditional uniform sampling, moment sampling prioritizes frames most relevant to the given question by combining relevance scores from moment retrieval with quality and uniformity scores. Extensive experiments on four long-form VideoQA datasets using four state-of-the-art Video LLMs demonstrate consistent performance improvements, highlighting its effectiveness in enhancing both accuracy and efficiency while providing interpretable temporal grounding of predictions.

## Method Summary
Moment sampling leverages a pre-trained text-to-video moment retrieval model (QD-DETR) to predict temporal segments relevant to a given question. These segment-level scores are converted to frame-level relevance via Gaussian smoothing. Three complementary scores are computed per frame: relevance from smoothed moment predictions, quality via Laplacian variance (blur penalty), and uniformity via squared temporal distance from already-selected frames. Final selection uses greedy optimization over the weighted combination. To ensure visual diversity, frame-level CLIP visual features are clustered into K groups via K-means, and at most one frame is sampled from each cluster. The selected frames are then fed to Video LLMs for answering multiple-choice questions.

## Key Results
- Moment sampling consistently outperforms uniform sampling across four VideoQA datasets and four VideoLLMs
- The method provides interpretable temporal grounding by highlighting which frames contributed to the answer
- Performance gains are most pronounced for datasets requiring visual reasoning (EgoSchema, CinePile)
- Frame clustering ensures visual diversity, preventing redundant information from being selected

## Why This Works (Mechanism)

### Mechanism 1: Query-Conditioned Moment Localization for Frame Prioritization
Moment sampling improves VideoQA by selecting frames that are semantically aligned with the question, rather than distributing frames uniformly across time. A pre-trained text-to-video moment retrieval model (QD-DETR) generates temporal segments ("moments") with relevance scores for the given question. These segment-level scores are converted to frame-level relevance via Gaussian smoothing and cumulative summation for overlapping moments. Core assumption: The moment retrieval model transfers effectively from its training domain (QVHighlights) to long-form VideoQA without domain-specific fine-tuning.

### Mechanism 2: Multi-Objective Score Fusion for Frame Quality
Combining relevance with quality and uniformity scores produces more informative and diverse frame sets than relevance alone. Three complementary scores are computed per frame: (1) relevance from smoothed moment predictions, (2) quality via Laplacian variance (blur penalty), and (3) uniformity via squared temporal distance from already-selected frames. Final selection uses greedy optimization over the weighted combination. Core assumption: Blur detection via Laplacian variance correlates with human-perceived frame utility for answering questions.

### Mechanism 3: Visual Redundancy Elimination via Feature Clustering
Enforcing cluster-level diversity reduces redundant visual content across selected frames. Frame-level CLIP visual features are clustered into K groups via K-means. During greedy selection, at most one frame is sampled from each cluster, ensuring visual diversity even when moments are temporally clustered. Core assumption: CLIP visual features capture semantic similarity relevant to VideoQA reasoning.

## Foundational Learning

- Concept: Moment Retrieval
  - Why needed here: Understanding how text-to-video moment retrieval models (e.g., QD-DETR) localize relevant temporal segments is essential for interpreting relevance scores and their limitations.
  - Quick check question: Can you explain how QD-DETR frames moment retrieval as a temporal detection problem and what inputs it requires?

- Concept: Video LLM Context Constraints
  - Why needed here: Video LLMs have finite context windows; frame sampling directly determines what visual information reaches the model, making sampling strategy critical for long-form videos.
  - Quick check question: Why does uniform frame sampling become increasingly suboptimal as video length grows relative to fixed context capacity?

- Concept: Greedy Selection with Non-Stationary Scores
  - Why needed here: The uniformity score changes after each selection, requiring understanding of how greedy algorithms handle dynamic objective functions.
  - Quick check question: How does updating uniformity scores after each frame selection differ from pre-computing all selection decisions independently?

## Architecture Onboarding

- Component map:
Video + Question → Moment Retrieval (QD-DETR) → Segment Predictions + Relevance Scores
                                    ↓
                    Gaussian Smoothing → Frame-level Relevance Scores
                                    ↓
                    Quality Scores (Laplacian variance) + Uniformity Scores
                                    ↓
                    CLIP Feature Extraction → K-means Clustering (K=30)
                                    ↓
                    Weighted Score Fusion → Greedy Frame Selection
                                    ↓
                    Selected Frames → Video LLM (InternVL2, GPT-4o, etc.) → Answer

- Critical path: Moment retrieval inference → score computation → greedy selection. The moment retrieval model is the bottleneck; its predictions determine downstream relevance estimates.

- Design tradeoffs:
  - Number of clusters (K=30): Higher K increases diversity but may dilute moment coverage; lower K risks missing distinct visual content.
  - Score weights (quality=0.5, uniformity=2): Tuned empirically; different datasets may require re-calibration.
  - Moment retrieval model choice: QD-DETR trained on QVHighlights; domain mismatch with egocentric/movies is assumed tolerable but unverified.

- Failure signatures:
  - All selected frames cluster in one temporal region → uniformity weight too low or moments too sparse.
  - Correct answer frames excluded → moment retrieval failed to localize relevant segments.
  - Blurry frames selected → quality weight too low or Laplacian threshold misconfigured.

- First 3 experiments:
  1. **Baseline comparison**: Run uniform sampling vs. moment sampling on a held-out validation split of one dataset (e.g., NextQA) with fixed frame budget (e.g., 10 frames), measuring accuracy gap.
  2. **Ablation study**: Disable each score component (relevance-only, relevance+quality, relevance+uniformity, full) to quantify individual contributions.
  3. **Domain transfer test**: Evaluate moment retrieval relevance scores on a small sample from a different domain (e.g., apply QD-DETR trained on QVHighlights to EgoSchema videos) to qualitatively inspect whether predicted moments align with question-relevant segments.

## Open Questions the Paper Calls Out
1. **End-to-end joint training**: Does end-to-end joint training of the moment retrieval and VideoQA components improve alignment between relevance estimation and final predictions? The authors note this as a key avenue for future work.
2. **Multimodal enhancements**: Can incorporating audio signals or structured video descriptions into the sampling framework further enrich context and accuracy? The authors plan to explore this direction.
3. **Temporal query decomposition**: Does decomposing complex queries into sub-questions localized to specific time segments enable more precise reasoning? The authors propose this as an exciting direction.

## Limitations
- **Domain Generalization**: The method assumes QD-DETR's moment retrieval predictions transfer from its training domain (QVHighlights) to long-form VideoQA domains without fine-tuning, which remains unverified.
- **Score Weight Sensitivity**: The relative weights (quality=0.5, uniformity=2) are tuned on development data but not validated across different datasets or video types.
- **Frame Number Specification**: The default number of frames sampled in main experiments is not explicitly stated, making precise replication challenging.

## Confidence
- **High Confidence**: Performance improvements are demonstrated across four datasets and four VideoLLMs, with clear accuracy gains over uniform sampling.
- **Medium Confidence**: The three-score fusion mechanism is plausible but lacks ablation studies isolating individual score contributions.
- **Low Confidence**: Domain transfer effectiveness of QD-DETR predictions remains unverified, representing a critical untested assumption.

## Next Checks
1. **Score Ablation Study**: Systematically disable each score component (relevance-only, relevance+quality, relevance+uniformity, full) on a held-out validation set to quantify individual contributions to performance gains.
2. **Domain Transfer Assessment**: Apply QD-DETR to a small sample from a different domain (e.g., EgoSchema videos if trained on QVHighlights) and qualitatively inspect whether predicted moments align with question-relevant segments.
3. **Frame Number Sensitivity**: Systematically vary the number of sampled frames (2-10 as shown in Fig. 3) on one dataset and plot accuracy curves to identify optimal frame budgets for different video types.