---
ver: rpa2
title: 'Instance-level Randomization: Toward More Stable LLM Evaluations'
arxiv_id: '2509.12678'
source_url: https://arxiv.org/abs/2509.12678
tags:
- random
- factors
- different
- evaluation
- variance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses instability in large language model (LLM) evaluations,
  where small changes in random factors like few-shot examples can drastically affect
  scores and model rankings. The authors propose Instance-level Randomization (ILR),
  a method that randomizes all random factors for each instance in a benchmark, runs
  multiple experiments, and averages the results.
---

# Instance-level Randomization: Toward More Stable LLM Evaluations

## Quick Facts
- arXiv ID: 2509.12678
- Source URL: https://arxiv.org/abs/2509.12678
- Reference count: 30
- The paper introduces Instance-level Randomization (ILR) to reduce variance and unfair comparisons in LLM evaluations by randomizing all random factors per instance across multiple experiments.

## Executive Summary
This paper addresses the critical issue of instability in large language model evaluations, where minor changes in random factors like few-shot examples, task descriptions, prompt formats, and option labels can significantly alter model scores and rankings. The authors propose Instance-level Randomization (ILR), a method that randomizes all these factors for each instance independently across multiple experimental runs and averages the results. Theoretical and empirical analyses demonstrate that ILR substantially reduces variance and improves ranking stability while requiring less than half the computational cost of previous approaches. The paper introduces Observed Reversal Probability (ORP) as a novel metric to quantify ranking stability, showing ILR significantly reduces ORP and leads to more reliable LLM evaluations.

## Method Summary
The authors propose Instance-level Randomization (ILR) to address instability in LLM evaluations caused by random factors such as few-shot examples, task descriptions, prompt formats, and option labels. ILR works by independently randomizing all four factors for each instance in a benchmark, running multiple experiments (typically 15-20), and averaging the results. This approach ensures that each instance is evaluated under diverse conditions, reducing the impact of any single random factor on the final score. The method is theoretically justified and empirically validated across multiple benchmarks (Winogrande, Hellaswag, BigBench-Hard, MMLU-Pro) and model sizes, demonstrating significant variance reduction and improved ranking stability at less than half the computational cost of previous methods.

## Key Results
- ILR reduces instance-level correlations substantially (e.g., Hellaswag from 0.457 to 0.119) and accelerates variance reduction by approximately 50% compared to fixed-factor evaluations
- The method achieves similar robustness to previous approaches while requiring less than half the computational cost
- ILR significantly reduces Observed Reversal Probability (ORP) AUC, demonstrating improved ranking stability across different experimental conditions

## Why This Works (Mechanism)
The instability in LLM evaluations stems from the fact that small changes in random factors (few-shot examples, task descriptions, prompt formats, option labels) can cause significant score fluctuations and ranking changes. Traditional approaches use fixed random factors across all instances, which means some models may be unfairly advantaged or disadvantaged based on their compatibility with those specific factors. ILR addresses this by randomizing all factors independently for each instance, ensuring that each model experiences the full distribution of possible evaluation conditions. This averaging over diverse conditions reduces the variance in scores and eliminates unfair advantages, leading to more stable and reliable rankings. The method is computationally efficient because it only requires averaging across multiple runs rather than developing more complex evaluation protocols.

## Foundational Learning

**Random Factor Impact**: Understanding how few-shot examples, task descriptions, prompt formats, and option labels affect LLM performance. *Why needed*: These factors are the primary source of instability in evaluations. *Quick check*: Verify that changing these factors independently affects model scores.

**Variance Reduction Theory**: Statistical principles behind reducing variance through averaging multiple independent measurements. *Why needed*: ILR's effectiveness relies on variance reduction through repeated randomization. *Quick check*: Confirm that averaging n independent runs reduces variance by factor 1/n.

**Ranking Stability Metrics**: Methods for quantifying how stable model rankings are across different experimental conditions. *Why needed*: Traditional metrics don't capture ranking instability caused by random factors. *Quick check*: ORP AUC should decrease when rankings become more stable.

**Computational Efficiency Analysis**: Techniques for comparing the computational cost of different evaluation methods. *Why needed*: ILR claims efficiency advantages over previous methods. *Why needed*: Need to verify that fewer runs are required for similar robustness.

## Architecture Onboarding

**Component Map**: Data preparation (factor pools) -> ILR execution (n parallel runs) -> Result aggregation (averaging) -> ORP calculation (σ_A, σ_B, ρ_AB) -> Stability analysis

**Critical Path**: The essential sequence is factor pool preparation → instance-level randomization → multiple experimental runs → result averaging → stability metric computation. Any failure in randomization or insufficient factor diversity will compromise the entire method.

**Design Tradeoffs**: ILR trades multiple experimental runs for improved stability and reduced unfair comparisons. The number of runs (n) must balance computational cost against variance reduction needs. More diverse factor pools improve stability but increase setup complexity.

**Failure Signatures**: Insufficient correlation reduction indicates inadequate factor diversity or too few experimental runs. ORP not decreasing suggests factors aren't being randomized independently per instance or the factor pools lack sufficient variation.

**Three First Experiments**:
1. Implement ILR with n=8 runs on Winogrande subset and verify instance-level correlation drops from baseline
2. Compare ORP AUC between fixed-factor and ILR approaches on Hellaswag
3. Test variance reduction speed by running increasing numbers of ILR experiments and plotting convergence

## Open Questions the Paper Calls Out
None

## Limitations
- The exact specifications of random factor pools (number of few-shot examples and text templates) are not fully detailed, which may affect reproducibility
- The evaluation focuses on specific benchmarks and model sizes, leaving generalizability to other domains or larger models untested
- While computationally efficient relative to prior methods, ILR still requires multiple experimental runs, which may not be feasible for extremely large-scale evaluations

## Confidence

**High Confidence**: The core claim that ILR reduces variance and stabilizes LLM evaluations is well-supported by both theoretical analysis and empirical results. The observed reduction in instance-level correlations and ORP AUC is consistent across experiments.

**Medium Confidence**: The claim about computational efficiency relative to prior methods is supported but depends on specific experimental conditions (e.g., number of runs, model sizes). The generalizability to other benchmarks or model families is plausible but not exhaustively tested.

**Low Confidence**: The exact impact of ILR on rankings in real-world deployment scenarios is not explored. The paper focuses on controlled experiments, and practical deployment may introduce additional sources of instability not captured here.

## Next Checks

1. **Replicate variance reduction on an additional held-out benchmark** (e.g., a subset of MMLU or a different reasoning task) to confirm generalizability beyond the reported datasets.

2. **Measure instance-level correlations with fewer ILR runs (e.g., n=8)** to assess whether the reported stability gains hold with reduced computational cost, as suggested for initial validation.

3. **Test ILR on a much larger model (>70B parameters)** to verify that the method scales and maintains efficiency benefits when compute requirements increase significantly.