---
ver: rpa2
title: Comparing Reconstruction Attacks on Pretrained Versus Full Fine-tuned Large
  Language Model Embeddings on Homo Sapiens Splice Sites Genomic Data
arxiv_id: '2511.07481'
source_url: https://arxiv.org/abs/2511.07481
tags:
- privacy
- fine-tuning
- vulnerability
- reconstruction
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates embedding reconstruction attacks in large
  language models (LLMs) applied to genomic sequences, focusing on how fine-tuning
  affects vulnerability to these attacks. Using the HS3D genomic dataset and implementing
  specialized tokenization mechanisms for DNA sequences, we systematically compare
  privacy risks between pretrained and fine-tuned embeddings across multiple architectures.
---

# Comparing Reconstruction Attacks on Pretrained Versus Full Fine-tuned Large Language Model Embeddings on Homo Sapiens Splice Sites Genomic Data

## Quick Facts
- arXiv ID: 2511.07481
- Source URL: https://arxiv.org/abs/2511.07481
- Reference count: 24
- Fine-tuning improves privacy in BERT (+7.8%), XLNet (+19.8%), and GPT-2 (+9.8%) but degrades it in RoBERTa (-6.8%) and ERNIE (-2.9%)

## Executive Summary
This study investigates how fine-tuning large language models affects their vulnerability to embedding reconstruction attacks when processing genomic sequences. Using the HS3D splice site dataset and custom nucleotide-level tokenization, the researchers systematically compare privacy risks between pretrained and fine-tuned embeddings across six transformer architectures. The findings reveal that fine-tuning creates divergent privacy outcomes depending on the model architecture—improving resistance in some models while slightly degrading it in others. These results suggest that task-specific optimization can serve as a potential privacy enhancement mechanism for language models processing sensitive genomic data.

## Method Summary
The researchers compared embedding reconstruction attack vulnerability between pretrained and fine-tuned LLMs on genomic sequences. They used the HS3D dataset with 20-nucleotide windows centered on splice sites, applying custom tokenization that space-separated nucleotides and extended vocabulary with {A, C, G, T}. Six transformer models (BERT, XLNet, GPT-2, RoBERTa, ALBERT, ERNIE) were fine-tuned for 3 epochs on splice site classification. Position-specific reconstruction attacks were implemented using 3-layer MLPs that reconstructed nucleotides from model embeddings concatenated with sinusoidal positional encodings. Privacy changes were measured as the difference in reconstruction accuracy between pretrained and fine-tuned embeddings.

## Key Results
- Fine-tuning strengthens resistance to reconstruction attacks in XLNet (+19.8%), GPT-2 (+9.8%), and BERT (+7.8%)
- Fine-tuning degrades privacy in RoBERTa (-6.8%) and ERNIE (-2.9%)
- Pretrained models show U-shaped vulnerability at sequence endpoints, with nearly perfect reconstruction accuracy at positions 1 and 20
- Fine-tuning creates asymmetric nucleotide-specific vulnerability: improving privacy for A and G while substantially decreasing it for C

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning can reduce reconstruction vulnerability in some architectures by redirecting embeddings toward task-specific representations that retain less recoverable sequence information.
- **Mechanism:** Full fine-tuning updates all model parameters (θ → θ') by minimizing task-specific loss, which reorganizes how input sequences are encoded into embeddings. This task-driven reorganization may reduce the preservation of exact nucleotide patterns that attackers exploit.
- **Core assumption:** Task-specific optimization creates representations that favor classification-relevant features over sequence-reconstructible patterns.
- **Evidence anchors:**
  - [abstract] "fine-tuning strengthens resistance to reconstruction attacks in XLNet (+19.8%), GPT-2 (+9.8%), and BERT (+7.8%)"
  - [Section VI.F] "Fine-tuning XLNet creates the most substantial overall privacy improvement (+0.198) of all models analyzed"
  - [corpus] Limited direct corpus support; neighbor papers focus on membership inference rather than reconstruction via embeddings
- **Break condition:** If fine-tuned models achieve similar task performance but show increased reconstruction accuracy, the mechanism fails. Also breaks if parameter-efficient fine-tuning (e.g., LoRA) shows different privacy profiles than full fine-tuning.

### Mechanism 2
- **Claim:** Pretrained models exhibit position-specific vulnerability patterns (U-shaped at endpoints) because edge positions retain more recoverable positional encoding information.
- **Mechanism:** Transformer positional encodings add fixed signals to embeddings; at sequence boundaries, these combine with fewer contextual mixing operations, leaving stronger recoverable traces of the original nucleotides.
- **Core assumption:** Endpoint positions have less contextual dilution from bidirectional attention, making their embeddings more directly invertible.
- **Evidence anchors:**
  - [Section VI.B] "pre-trained BERT model exhibits extreme vulnerability at sequence endpoints with nearly perfect reconstruction accuracy at positions 1 and 20 (both 1.000)"
  - [Section VI.D] "pretrained GPT-2 model exhibits extreme vulnerability at sequence endpoints, particularly positions 1 (0.820), 19 (0.960), and 20 (1.000)"
  - [corpus] No direct corpus validation for position-specific reconstruction; neighbor papers do not examine positional vulnerability
- **Break condition:** If models with different attention patterns (e.g., uniform attention) show flat vulnerability profiles rather than U-shaped patterns, this mechanism is insufficient.

### Mechanism 3
- **Claim:** Fine-tuning redistributes nucleotide-specific vulnerability asymmetrically, potentially improving privacy for some nucleotides while degrading it for others.
- **Mechanism:** Task optimization shifts decision boundaries based on nucleotide frequencies and task-relevance; nucleotides more predictive of splice sites may retain more recoverable information post-fine-tuning.
- **Core assumption:** Task-relevant nucleotides are encoded with higher fidelity, making them more vulnerable to reconstruction.
- **Evidence anchors:**
  - [Section VI.E] "Fine-tuning creates a dramatic tradeoff between nucleotides: improving privacy for (A) and (G) while substantially decreasing it for (C)"
  - [Section VI.C] "Fine-tuning substantially improves privacy for (A), reducing vulnerability from 0.400 to 0.167. However, this comes at the cost of increased vulnerability for (C) from 0.250 to 0.407"
  - [corpus] Weak support; corpus papers address genomic privacy broadly but not nucleotide-specific leakage
- **Break condition:** If nucleotide redistribution patterns are inconsistent across tasks (e.g., different classification targets), the mechanism may be task-dependent rather than general.

## Foundational Learning

- **Concept: Embedding reconstruction attacks**
  - **Why needed here:** The entire paper assumes understanding that embeddings can be inverted to recover inputs; without this, the privacy risk framing is unclear.
  - **Quick check question:** Can you explain why a fixed-size embedding vector might leak information about variable-length input sequences?

- **Concept: Tokenization adaptation for non-natural-language domains**
  - **Why needed here:** The paper modifies BPE, WordPiece, and SentencePiece tokenizers for DNA; understanding why default tokenizers fail on genomic data is essential.
  - **Quick check question:** What happens when a WordPiece tokenizer trained on English text encounters "ACGTAACGT"?

- **Concept: Positional encodings in transformers**
  - **Why needed here:** The position-specific vulnerability findings depend on understanding how positional signals are added to token embeddings.
  - **Quick check question:** Why might sequence endpoints show different reconstruction vulnerability than middle positions?

## Architecture Onboarding

- **Component map:** Custom tokenizer (space-separated nucleotides) -> Model (BERT/XLNet/GPT-2/RoBERTa/ALBERT/ERNIE) -> Embedding extraction (CLS token for bidirectional, last token for autoregressive) -> Attack pipeline (concatenate with sinusoidal positional encodings) -> 3-layer MLP classifiers -> Per-position accuracy evaluation

- **Critical path:**
  1. Extend tokenizer vocabulary and preprocess DNA with space-separation
  2. Fine-tune model on HS3D splice site classification (3 epochs, model-specific learning rates)
  3. Extract embeddings from pretrained and fine-tuned models
  4. Train reconstruction attack classifiers (5 epochs, 80/20 split)
  5. Compare per-position and per-nucleotide accuracy between embedding types

- **Design tradeoffs:**
  - Custom tokenization improves biological interpretability but reduces compatibility with pretrained knowledge
  - Full fine-tuning may improve privacy (XLNet, GPT-2, BERT) or degrade it (RoBERTa, ERNIE)—architecture selection matters
  - CLS vs. last-token embedding extraction depends on model directionality; mixing them invalidates comparisons

- **Failure signatures:**
  - Pretrained tokenizer producing UNK tokens or arbitrary subwords (e.g., "AC", "GTA" instead of individual nucleotides)
  - Reconstruction accuracy near 1.0 at endpoints for pretrained models (expected for BERT, GPT-2, XLNet)
  - Fine-tuned model showing increased vulnerability (negative ΔPrivacy) for RoBERTa or ERNIE—this is expected, not a bug
  - Nucleotide-specific accuracy > 0.4 for any nucleotide indicates notable leakage

- **First 3 experiments:**
  1. Replicate the tokenization customization: Process 10 DNA sequences through modified BERT tokenizer and verify each nucleotide becomes a separate token
  2. Run reconstruction attack on pretrained vs. fine-tuned BERT embeddings on a held-out HS3D subset; confirm the 0.078 average privacy improvement
  3. Test position-specific vulnerability: Plot reconstruction accuracy for positions 1–20 on XLNet (pretrained vs. fine-tuned) to verify the U-shaped flattening pattern

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do parameter-efficient fine-tuning techniques (e.g., LoRA, QLoRA) affect privacy-utility trade-offs in nucleotide reconstruction compared to full fine-tuning?
- **Basis in paper:** [explicit] The conclusion states: "Future work should focus on... evaluating how parameter-efficient fine-tuning techniques (e.g., LoRa and QLoRa) affect privacy-utility trade-offs in nucleotide reconstruction."
- **Why unresolved:** The study only examined full fine-tuning where all parameters are updated; parameter-efficient methods were not tested.
- **What evidence would resolve it:** Comparative experiments applying LoRA/QLoRA to the same models and measuring reconstruction accuracy changes versus utility preservation.

### Open Question 2
- **Question:** Why do some model architectures (XLNet, GPT-2, BERT) show improved privacy after fine-tuning while others (RoBERTa, ERNIE) show degraded privacy?
- **Basis in paper:** [inferred] The paper reports divergent privacy outcomes across architectures but does not fully explain the underlying mechanisms causing this variation beyond noting differences in pre-training methodology.
- **Why unresolved:** The architectural and pre-training factors driving opposite privacy trajectories remain unidentified.
- **What evidence would resolve it:** Ablation studies isolating pre-training objectives, attention mechanisms, and embedding strategies to identify causal factors.

### Open Question 3
- **Question:** What targeted protection mechanisms can mitigate nucleotide-specific vulnerabilities while preserving model utility for genomic tasks?
- **Basis in paper:** [explicit] The conclusion recommends: "developing targeted protection mechanisms for vulnerable nucleotide representations while preserving model utility."
- **Why unresolved:** The study identifies asymmetric nucleotide vulnerabilities (e.g., cytosine becoming highly vulnerable in RoBERTa) but offers no mitigation strategies.
- **What evidence would resolve it:** Development and testing of nucleotide-aware regularization, differential privacy, or embedding perturbation techniques with utility benchmarks.

## Limitations

- The study's architectural and domain-specific customizations introduce significant generalizability constraints, as the custom nucleotide-level tokenization diverges substantially from standard pretraining data
- Limited evaluation to splice site classification may not generalize to other genomic tasks or broader ML applications
- Exclusive focus on embedding reconstruction attacks neglects other privacy threats (membership inference, attribute inference) that may behave differently under fine-tuning

## Confidence

- **High confidence**: The experimental methodology for measuring reconstruction vulnerability is sound and reproducible. The position-specific vulnerability patterns for pretrained models (U-shaped at endpoints) appear consistent across BERT, GPT-2, and XLNet.
- **Medium confidence**: The claim that fine-tuning improves privacy for some architectures (BERT, XLNet, GPT-2) is supported by data, but the underlying mechanism (task-specific representation reorganization) remains somewhat speculative given the limited corpus support. The nucleotide-specific redistribution patterns show empirical validity but may be task-dependent.
- **Low confidence**: The generalizability of these findings to other genomic tasks, model architectures, or non-genomic domains. The assertion that fine-tuning serves as a "potential privacy enhancement mechanism" is premature without understanding the privacy-utility tradeoff or testing across multiple tasks.

## Next Checks

1. **Cross-task validation**: Fine-tune the same models on a different genomic classification task (e.g., promoter prediction or gene expression prediction) and measure whether the same privacy improvement patterns (+19.8% for XLNet, +9.8% for GPT-2, +7.8% for BERT) replicate, or if they are specific to splice site prediction.

2. **Parameter-efficient fine-tuning comparison**: Implement LoRA or prefix-tuning on the same models and datasets, then measure whether the privacy improvement pattern differs from full fine-tuning, particularly for architectures showing degradation (RoBERTa, ERNIE).

3. **Utility-privacy tradeoff analysis**: For each model showing privacy improvement post-fine-tuning, measure the change in splice site classification accuracy between pretrained and fine-tuned versions to quantify whether privacy gains come with meaningful performance costs.