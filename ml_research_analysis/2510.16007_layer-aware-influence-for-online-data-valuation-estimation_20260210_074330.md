---
ver: rpa2
title: Layer-Aware Influence for Online Data Valuation Estimation
arxiv_id: '2510.16007'
source_url: https://arxiv.org/abs/2510.16007
tags:
- influence
- training
- learning
- data
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a layer-aware online data valuation estimator
  that approximates sample influence during training using only loss-to-output gradients,
  avoiding full-network gradients. The method improves upon prior Hessian-free influence
  approaches by replacing per-layer feedback with a single output-layer signal, reducing
  noise and computational overhead while maintaining high fidelity to a Shapley-value
  baseline.
---

# Layer-Aware Influence for Online Data Valuation Estimation

## Quick Facts
- **arXiv ID:** 2510.16007
- **Source URL:** https://arxiv.org/abs/2510.16007
- **Authors:** Ziao Yang; Longbo Huang; Hongfu Liu
- **Reference count:** 40
- **Primary result:** Online data valuation estimator using only loss-to-output gradients improves training efficiency and model performance while reducing computational overhead.

## Executive Summary
This paper introduces Layer-Aware Influence (LAI), an online data valuation estimator that approximates sample influence during training using only loss-to-output gradients, avoiding full-network gradients. The method improves upon prior Hessian-free influence approaches by replacing per-layer feedback with a single output-layer signal, reducing noise and computational overhead while maintaining high fidelity to a Shapley-value baseline. Extensive experiments on LLM pre-training/fine-tuning and image/text classification show the estimator improves accuracy and reduces training costs, making dynamic data curation scalable and efficient. The approach enables single-run training with minimal overhead and is compatible with SGD-style updates.

## Method Summary
LAI computes influence scores for each training sample against a validation subset within each batch using the formula: $I_{LAI}(z_j)=-\sum_{z\in V}\left(\sum_{l=1}^L (a_z^{(l-1)})^\top a_j^{(l-1)}\right)\left(g_z^{(L)}\right)^\top g_j^{(L)}$, where $g^{(L)}=\partial\ell^{(L)}/\partial s^{(L)}$ is the output-layer gradient. The method caches multi-layer embeddings and last-layer gradients for validation samples, then scores each training sample per batch and excludes samples with negative influence scores. This online filtering is applied during training without requiring retraining, using Adam for LLMs and SGD for vision tasks with standard hyperparameters.

## Key Results
- Achieves Pearson correlation >0.95 with Shapley baseline on GPT-Neo pre-training while reducing variance compared to ghost influence
- Uses 50% fewer training samples per epoch on GLUE tasks while achieving 0.6-1.2% accuracy improvements and 20-50% validation loss reductions
- Maintains computational efficiency with minimal overhead, enabling single-run training without retraining requirements

## Why This Works (Mechanism)

### Mechanism 1
Replacing per-layer gradient feedback with a single output-layer channel reduces variance in influence estimation while preserving signal. Ghost influence aggregates β(l) across all L layers, accumulating noise from mini-batch statistics, nonlinear activations, and residual mixing at each layer. LAI replaces all {β(l)} with β(L), the output-layer similarity, which is mathematically equal to the shared signal β* under the backpropagation operator decomposition. This eliminates the additive noise term ∑α(l)ε(l) while retaining the multi-layer embedding similarities ∑α(l) for representational richness. The core assumption is that the output-layer gradient similarity β(L) captures the essential alignment between validation and training samples.

### Mechanism 2
Multi-layer embedding aggregation preserves representational fidelity even without per-layer gradient feedback. LAI computes embedding similarities α(l)_z,j = ⟨a(l-1)_z, a(l-1)_j⟩ across all layers, then aggregates them as a single weighting factor for β(L). This preserves the geometric relationship between validation and training samples in the learned feature space without requiring gradient backpropagation through intermediate layers. The core assumption is that embedding similarity correlates with functional similarity for influence purposes.

### Mechanism 3
Online sample exclusion based on negative influence improves convergence without requiring retraining. LAI scores each training sample against a validation subset within each batch. Samples with negative estimated influence (those whose gradient direction opposes validation loss reduction) are excluded from the optimizer step. This implements a form of gradient filtering that aligns SGD updates with validation objectives dynamically. The core assumption is that influence scores computed at step t generalize to near-future optimization.

## Foundational Learning

- **Concept: Influence Functions and Hessian-Free Approximations**
  - Why needed here: LAI builds on the inner-product (IP) influence formulation that replaces the Hessian inverse with an identity matrix. Understanding this simplification is essential to grasp why LAI can operate online without expensive matrix operations.
  - Quick check question: Can you explain why the IP influence I(z_j) = -∑_{z∈V} ∇ℓ(z)^T · ∇ℓ(z_j) approximates the full influence function, and what signal it captures?

- **Concept: Backpropagation Noise Accumulation**
  - Why needed here: Appendix A formalizes how stochasticity from mini-batch statistics, dropout, and normalization propagates through backpropagation layers. Understanding this explains why LAI's single-channel design reduces variance.
  - Quick check question: If backpropagation through layer l introduces perturbation ∆J(l), how does this affect the variance of β(l) compared to β(L)?

- **Concept: Online Data Curation vs. Static Two-Round Training**
  - Why needed here: The paper distinguishes between methods that identify detrimental samples post-hoc (requiring retraining) versus methods that filter during training. This distinction is central to LAI's practical value proposition.
  - Quick check question: Why might a sample identified as detrimental at a converged checkpoint not be detrimental when removed early in training?

## Architecture Onboarding

- **Component map:** Forward pass -> Embedding cache -> Output-layer gradient computation -> LAI scorer -> Filter -> Optimizer
- **Critical path:** 1) Embedding extraction during forward pass (must hook into all layer outputs) 2) Output-layer gradient computation for current batch (one backward to logits, not to parameters) 3) Dot-product scoring against validation cache (O(|V| × batch_size × d_L) per batch) 4) Threshold filtering and masked optimizer step
- **Design tradeoffs:** Validation cache size (larger V improves score stability but increases per-batch compute), threshold choice (zero threshold is default), self-influence vs. external validation (pre-training uses self-influence), optimizer compatibility (raw gradients for SGD, diagonally preconditioned for Adam)
- **Failure signatures:** Instability during early training (warm-start with standard training for 1-3 epochs), over-exclusion (>50% of samples consistently excluded), OOM on large models (consider checkpointing or last-layer-only variant)
- **First 3 experiments:** 1) Fidelity validation: Compute LAI scores and Shapley reference for 100 checkpoints on small model 2) Noisy label robustness: Train on CIFAR-10N-worst with LAI filtering 3) Ablation on validation cache size: Vary |V| from 100 to 5000 samples on text classification task

## Open Questions the Paper Calls Out

### Open Question 1
How can online data valuation methods achieve exact compatibility with adaptive optimizers (e.g., Adam) without incurring the cost of per-sample moment tracking? The paper notes that when using adaptive optimizers such as Adam there may be mild direction mismatches, and achieving exact Adam-consistent scoring requires per-sample moment statistics that are typically too costly in complexity and memory.

### Open Question 2
Under what conditions does online data valuation amplify validation-set biases, and can formal mitigation strategies be developed? The paper identifies amplification of validation-set bias as a potential risk and recommends careful design of the validation set as mitigation, but provides no specific analysis or methodology.

### Open Question 3
Does LAI systematically exclude underrepresented or long-tail subgroups during training, and what mechanisms could prevent such exclusion? The paper flags possible exclusion of long-tail groups as a risk requiring subgroup-aware evaluation but does not investigate whether samples from minority subgroups receive systematically different influence scores.

### Open Question 4
How does LAI's ranking fidelity and computational efficiency scale to models with billions of parameters and terabyte-scale datasets? Experiments use GPT-Neo 125M and relatively small datasets; modern foundation models are orders of magnitude larger. The method's reliance on caching embeddings and output-layer gradients may face memory bottlenecks at extreme scale.

## Limitations
- Method's dependence on validation cache representativeness introduces risk of systematic bias if the held-out set does not capture full data distribution
- Claim that output-layer gradients alone suffice assumes smooth loss landscape and may not hold for architectures with early-layer feature learning dominance
- Layer-wise embedding aggregation lacks formal justification for its weighting scheme, relying instead on empirical correlation with Shapley values

## Confidence
- **High confidence:** Fidelity correlation claims (>0.95 with Shapley baseline), computational efficiency improvements (50% fewer samples, 20-50% loss reduction), online filtering mechanism
- **Medium confidence:** Claims about layer-wise noise reduction and single-output-channel design's superiority over ghost influence
- **Low confidence:** Claims about LAI's universal applicability across pre-training and fine-tuning regimes without architecture-specific tuning

## Next Checks
1. Test LAI on architectures with known early-layer feature importance (e.g., ResNets with bottleneck layers) to verify the single-output-channel assumption holds
2. Systematically vary validation cache size and composition to quantify the relationship between cache representativeness and influence estimation bias
3. Implement the diagonally preconditioned variant (Appendix E) for Adam optimizers and compare performance to the default raw gradient version across multiple tasks