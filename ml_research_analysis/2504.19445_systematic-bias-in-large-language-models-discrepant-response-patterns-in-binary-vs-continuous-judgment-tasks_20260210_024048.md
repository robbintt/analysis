---
ver: rpa2
title: 'Systematic Bias in Large Language Models: Discrepant Response Patterns in
  Binary vs. Continuous Judgment Tasks'
arxiv_id: '2504.19445'
source_url: https://arxiv.org/abs/2504.19445
tags:
- llms
- binary
- responses
- response
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reveals that large language models (LLMs) exhibit systematic
  negative biases when responding to binary judgment tasks, such as being more likely
  to oppose value statements or classify sentiments as negative compared to continuous
  response formats. Across experiments with value statements and sentiment analysis
  tasks, LLMs consistently showed a shift toward more negative judgments in binary
  formats, a bias confirmed through hierarchical Bayesian modeling.
---

# Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks

## Quick Facts
- arXiv ID: 2504.19445
- Source URL: https://arxiv.org/abs/2504.19445
- Reference count: 2
- Primary result: LLMs exhibit systematic negative bias in binary response formats, classifying ambiguous cases as negative more frequently than in continuous formats.

## Executive Summary
This study reveals that large language models (LLMs) exhibit systematic negative biases when responding to binary judgment tasks, such as being more likely to oppose value statements or classify sentiments as negative compared to continuous response formats. Across experiments with value statements and sentiment analysis tasks, LLMs consistently showed a shift toward more negative judgments in binary formats, a bias confirmed through hierarchical Bayesian modeling. Control experiments ruled out superficial factors like label mapping, suggesting the bias stems from deeper structural issues in how LLMs process binary versus continuous inputs. These findings underscore the importance of response format in LLM-based decision-making and highlight the need for careful task design and potential calibration to mitigate unintended biases in applications like psychological text analysis.

## Method Summary
The study compared LLM judgments under binary and continuous response formats across two tasks: value judgment on 210 statements and sentiment analysis on 213 news headlines. Models (Llama-3.3-70b, Qwen-2.5-72b, DeepSeek-v3, GPT-4o-mini, GPT-4o) were prompted with human demographic profiles to respond either on a 0-10 scale (value) or 1-6 Likert scale (sentiment), or via binary Yes/No responses. Responses were analyzed using hierarchical Bayesian regression to quantify bias parameters, with 95% highest density intervals (HDI) indicating statistical significance. The primary metric was ΔP, the difference between binary response proportions and binarized continuous responses.

## Key Results
- LLMs showed a significant shift toward more negative judgments in binary formats (group-level θ_bc, M = −1.015, 95% HDI: [−1.736, −0.359])
- Strong preference for "No" responses emerged even when "No" mapped to positive sentiment (group-level θ_Yes, M = −1.320, 95% HDI: [−2.160, −0.465])
- Continuous responses showed better alignment with human judgments (r > 0.62) compared to binary formats
- Binary "Support" proportion dropped 10-15 percentage points versus dichotomized continuous responses

## Why This Works (Mechanism)

### Mechanism 1: Format-Dependent Decision Threshold Shift
Binary response formats induce a negative shift in LLM decision thresholds compared to continuous scales. When forced into binary classification, LLMs apply a stricter internal criterion for "positive/support" judgments, classifying ambiguous cases as negative more frequently than when allowed to express uncertainty on a continuous scale.

### Mechanism 2: Token Probability Miscalibration in Forced Choice
Binary formats exacerbate autoregressive token probability biases that do not reflect true semantic confidence. Next-token prediction training causes LLMs to conflate token frequency/position with semantic meaning. Binary choices force selection between tokens with unequal prior probabilities, introducing systematic skew.

### Mechanism 3: Absence of Coherent Internal World Model
LLMs lack stable internal representations that persist across format changes, causing inconsistent behavioral outputs. Without a coherent world model, LLM responses are highly context-dependent. Format changes alter the prompt context sufficiently to shift behavior, even when the underlying query is identical.

## Foundational Learning

- **Concept: Hierarchical Bayesian Regression**
  - Why needed here: The paper uses this method to separate item-level variance from model-level bias parameters, essential for understanding whether observed effects generalize across stimuli.
  - Quick check question: Can you explain why hierarchical modeling is preferred over simple averaging when estimating bias across multiple LLMs and multiple items?

- **Concept: Response Bias vs. Acquiescence Bias**
  - Why needed here: Humans show acquiescence bias (favoring "Yes"), while LLMs show the opposite pattern. Understanding this distinction prevents misapplying human psychology frameworks to model behavior.
  - Quick check question: What would it imply if LLMs showed acquiescence bias matching humans—would this indicate human-like reasoning or training data artifacts?

- **Concept: Probability Calibration in Language Models**
  - Why needed here: The observed bias relates to how token probabilities translate to decisions. Uncalibrated probabilities mean model "confidence" does not reflect true likelihood.
  - Quick check question: If an LLM assigns 0.7 probability to "positive" but is correct only 50% of the time on such cases, what type of miscalibration is this?

## Architecture Onboarding

- **Component map:** Profile selection -> Format-specific prompt -> Model inference (temperature=0) -> Response extraction -> Bias calculation (binary proportion minus dichotomized continuous proportion)
- **Critical path:** 1. Profile selection → 2. Format-specific prompt → 3. Model inference (temperature=0) → 4. Response extraction → 5. Bias calculation (binary proportion minus dichotomized continuous proportion)
- **Design tradeoffs:** Binary formats offer simpler interpretation but introduce systematic negative bias (ΔP ≈ −0.14 for value judgments). Continuous formats better align with human judgments but require threshold selection for classification.
- **Failure signatures:** Binary "Support" proportion drops 10-15 percentage points vs. dichotomized continuous; judgment curves shift right in binary conditions; strong "No" preference emerges even when "No" maps to "positive."
- **First 3 experiments:**
  1. Replication check: Run the same value statements with binary and continuous formats on a new model not in the original study; compute ΔP(Support) to verify bias direction and magnitude.
  2. Label abstraction test: Use entirely abstract labels (e.g., "Option A/B" with randomized mapping) to determine if semantic meaning of "Yes/No" drives the bias or if forced binary choice itself is sufficient.
  3. Threshold calibration: If binary output is required, fit a regression-based correction using continuous responses on a held-out set, then apply adjustment to binary outputs; measure residual bias post-correction.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the observed negative bias stem from the base pre-training data or from the post-training alignment processes (such as RLHF)? The study tested only instruction-tuned models, making it impossible to isolate the developmental stage responsible for the bias.

- **Open Question 2:** Is the negative bias robust across different prompting contexts, specifically when LLMs are not explicitly instructed to simulate human profiles? The findings "relied on prompts that explicitly asked LLMs to simulate human responses" and acknowledge "results may differ with other prompts."

- **Open Question 3:** To what extent do superficial token probabilities versus deeper semantic understanding drive this response discrepancy? While control experiments ruled out simple label mapping, they did not determine if the bias arises from the specific tokenization or probability distribution of negative classifiers in the model's vocabulary.

## Limitations
- Study relies on datasets not provided in the paper (value statements from Moore et al., 2024; GSS Agents Bank profiles), requiring access to external sources for full reproduction.
- Bias was tested on five specific models at temperature=0, limiting generalizability to other model families or temperature settings.
- Findings may be specific to the model versions tested and may not hold for future releases due to temporal instability of LLM behavior.

## Confidence
- **High confidence:** The statistical methodology (hierarchical Bayesian regression) is sound and properly executed. The systematic negative bias in binary formats is clearly demonstrated across multiple control experiments.
- **Medium confidence:** The proposed mechanisms (threshold shift, token probability miscalibration, absence of world model) are plausible but not definitively proven.
- **Low confidence:** The practical significance and magnitude of the bias in real-world applications remains uncertain.

## Next Checks
1. Test the binary vs. continuous bias across additional model families (e.g., Claude, Gemini) and at different temperature settings to assess robustness and generalizability.
2. Replicate the experiments with current versions of the same models to determine if the bias persists or has been mitigated through updates.
3. Apply the findings to an actual application (e.g., automated sentiment analysis of customer reviews) and measure the practical impact of response format on decision outcomes.