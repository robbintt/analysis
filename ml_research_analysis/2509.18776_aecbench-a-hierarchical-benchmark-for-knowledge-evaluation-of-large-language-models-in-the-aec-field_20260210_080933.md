---
ver: rpa2
title: 'AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language
  Models in the AEC Field'
arxiv_id: '2509.18776'
source_url: https://arxiv.org/abs/2509.18776
tags:
- design
- knowledge
- evaluation
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A hierarchical evaluation framework was proposed to assess LLM
  capabilities across five cognitive levels (Knowledge Memorization, Understanding,
  Reasoning, Calculation, Application) for AEC tasks. 4,800 questions across 23 tasks
  were curated by domain engineers and validated by experts.
---

# AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field

## Quick Facts
- arXiv ID: 2509.18776
- Source URL: https://arxiv.org/abs/2509.18776
- Reference count: 40
- Primary result: Proposed a hierarchical evaluation framework revealing LLM performance declines from knowledge recall to application in AEC tasks

## Executive Summary
AECBench is a hierarchical benchmark designed to assess Large Language Models' capabilities across five cognitive levels for Architecture, Engineering, and Construction (AEC) tasks. The framework organizes 4,800 questions across 23 tasks into levels from Knowledge Memorization to Application, revealing significant performance gaps in higher-order reasoning and calculation tasks. The benchmark employs an automated LLM-as-a-judge pipeline with expert-defined rubrics to evaluate open-ended responses, demonstrating that even the most advanced models struggle with complex AEC problem-solving despite strong performance on basic recall tasks.

## Method Summary
The evaluation framework uses a five-level cognitive hierarchy based on Bloom's Taxonomy, ranging from Memorization to Application. It includes 4,800 questions across 23 tasks, primarily multiple-choice with some open-ended generation tasks. Models are evaluated using the OpenCompass platform with "One-shot" prompting. For subjective tasks, DeepSeek-R1 serves as the LLM-as-a-judge after being validated for highest correlation with human experts. Calibration via isotonic regression corrects systematic scoring bias in the judge's compressed score distributions.

## Key Results
- Performance declines clearly across cognitive levels, with GPT-4o dropping from 4.6% to 24.2% accuracy
- Most models achieve near-perfect scores on Memorization tasks but fail significantly on Reasoning, Calculation, and Application
- The LLM-as-a-judge exhibits systematic score compression bias, overestimating poor responses and underestimating excellent ones
- Table-based knowledge representation remains challenging, with automated HTML conversion introducing significant layout errors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical cognitive frameworks reveal performance degradation in LLMs that single-score benchmarks mask, specifically highlighting the "Knowledge-Application Gap."
- **Mechanism:** By structuring tasks from Memorization to Creation, the benchmark forces models to transition from pattern matching to multi-step synthesis. Failure rates increase as tasks require integrating disparate facts rather than retrieving them, exposing brittle internal representations.
- **Core assumption:** AEC task complexity scales linearly with cognitive levels, and failure at lower levels propagates to higher levels.
- **Evidence anchors:**
  - "evaluation of nine LLMs... revealed a clear performance decline across five cognitive levels."
  - "This proves the model's failure is not a knowledge deficit but an application deficit."
- **Break condition:** Specific Chain-of-Thought optimizations may flatten performance decline at Reasoning/Calculation levels.

### Mechanism 2
- **Claim:** "LLM-as-a-Judge" systems exhibit systematic bias (score compression) which can be corrected via calibration methods like Isotonic Regression.
- **Mechanism:** LLMs tend to regress toward the mean when scoring, over-rating poor responses and under-rating excellent ones because they optimize for probabilistic plausibility rather than strict rubric adherence.
- **Core assumption:** The relationship between the LLM's raw score and human expert score is monotonic but non-linear.
- **Evidence anchors:**
  - "models tend to assign an average score of 4.25 to documents rated in the 0–2 range... average of only 7.96 to those rated between 8 and 10."
  - "reduction in the mean absolute error (MAE) from an initial 2.947 to 1.926."
- **Break condition:** If evaluation rubric is subjective or ambiguous, calibration cannot fix noise as ground truth itself is unstable.

### Mechanism 3
- **Claim:** LLMs fail to interpret tabular data from building codes due to insufficient context formatting, not merely lack of knowledge.
- **Mechanism:** Tabular data in AEC codes often relies on implicit spatial relationships. When converted to raw text or HTML, these relationships become linear sequences, disrupting the semantic logic required for Reasoning tasks.
- **Core assumption:** Poor performance on "Tabular Data" tasks stems from input degradation rather than inability to process logic once context is clear.
- **Evidence anchors:**
  - "The superior performance of Approach 1 [Manual Description] over Approach 2 [Automated HTML]... attributed to Approach 2's inability to correctly reproduce the complex layout."
  - Describes the difficulty of "interpreting knowledge from tables in building codes."
- **Break condition:** Highly irregular spanning cells or nested structures may cause even manual text conversion to introduce interpretation errors.

## Foundational Learning

- **Concept:** **Bloom's Taxonomy (Revised)**
  - **Why needed here:** AECBench structures its entire evaluation framework around this hierarchy (Remember, Understand, Apply, Analyze, Evaluate, Create). You cannot interpret results without understanding that "Reasoning" is cognitively distinct from "Memorization."
  - **Quick check question:** In the context of structural design, does "selecting the correct rebar spacing from a table" belong to the *Remember* or *Apply* level?

- **Concept:** **Context Formatting (Text vs. Tabular)**
  - **Why needed here:** The paper demonstrates that how you feed engineering data to the model (linear text vs. HTML table) drastically changes accuracy. Understanding this is crucial for designing RAG systems for AEC.
  - **Quick check question:** Why might an LLM fail to answer a question about "maximum beam height" if the data is provided as a raw Markdown table without headers?

- **Concept:** **Calibration of Classifier Probabilities**
  - **Why needed here:** The "LLM-as-a-Judge" relies on calibrating raw scores. You need to understand that raw model confidence is rarely a reliable proxy for absolute quality.
  - **Quick check question:** If an LLM judge assigns a score of 6.5 to a terrible document and 7.5 to a perfect one, what specific statistical property of the model's output has failed?

## Architecture Onboarding

- **Component map:** Benchmark Dataset -> Target LLMs -> Evaluation Pipeline -> Calibration Layer
- **Critical path:**
  1. **Rubric Definition:** Expert engineers define scoring criteria for open-ended tasks (Architecture/Structural Design)
  2. **Judge Selection:** Run Evaluation tasks (5-2-1) to correlate LLM judges with human scores; select highest correlation model (DeepSeek-R1)
  3. **Calibration:** Fit regression model on judge's scores vs. human scores to fix compression bias
  4. **Full Evaluation:** Apply calibrated judge to Creation tasks (5-3-1 to 5-3-3)
- **Design tradeoffs:**
  - **One-shot vs. Zero-shot:** Chose **One-shot** to clarify output formats without "befuddling" the model with complex examples. *Assumption: This balances guidance with interference*
  - **Manual vs. Automated Table Parsing:** Manual text descriptions yield higher accuracy (98%+) but are not scalable. Automated HTML parsing is scalable but introduces layout errors. *Assumption: For high-stakes compliance checking, manual conversion is necessary*
- **Failure signatures:**
  - **Knowledge-Application Gap:** Model correctly answers "What is the seismic grade?" in isolation but outputs the *wrong* grade when generating full design proposal
  - **Score Compression:** LLM-Judge clusters all scores between 4.0 and 8.0, failing to distinguish between "passable" and "excellent" engineering documents
- **First 3 experiments:**
  1. **Baseline Run:** Evaluate target model on Level 1 (Memorization) vs. Level 4 (Calculation) to quantify cognitive performance gap
  2. **Table Format A/B Test:** Feed same "Tabular Data" tasks using Approach 1 (Text Description) vs. Approach 2 (HTML) to measure information loss
  3. **Judge Calibration Check:** Have LLM-Judge score 10 sample documents, then apply Isotonic Regression to see if score distribution expands to match human ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific data transformation methods can enable the lossless and scalable encoding of complex tabular building codes for LLM reasoning?
- **Basis in paper:** The Discussion states that while text and HTML conversion help, "the challenge of efficiently transforming tabular data into fully utilizable and correct content for LLMs remains a critical area for future work."
- **Why unresolved:** Current methods face a trade-off: manual text descriptions are high-fidelity but unscalable, while automated HTML conversions often fail to correctly reproduce complex table layouts (e.g., spanning cells).
- **What evidence would resolve it:** A methodology that matches the near-100% accuracy of manual curation on tabular tasks while operating at the speed of automated parsing.

### Open Question 2
- **Question:** How does the inclusion of multimodal tasks, such as architectural drawing recognition, alter the capability profile of LLMs in the AEC domain?
- **Basis in paper:** The Conclusion explicitly notes, "Future work would incorporate tasks related to multimodal data, such as architectural drawing recognition, to better reflect the full spectrum of information modalities."
- **Why unresolved:** The current benchmark is restricted to text, failing to evaluate the visual processing skills necessary for interpreting engineering drawings and BIM models.
- **What evidence would resolve it:** An expansion of AECBench including image-based inputs, showing whether current text-based performance trends persist in visual domains.

### Open Question 3
- **Question:** Can calibration methods robustly eliminate the systematic bias of "LLM-as-a-Judge" in evaluating safety-critical engineering documents?
- **Basis in paper:** The Discussion identifies systematic bias where models compress scoring intervals. While calibration reduces MAE, the paper implies the necessity of "uncertainty assessments" for reliability in high-stakes contexts.
- **Why unresolved:** Calibration fits the model to a specific distribution of errors; it is unclear if this correction generalizes to novel document types or different failure modes not present in the validation set.
- **What evidence would resolve it:** Demonstrating that calibrated judge scores maintain high correlation with human experts across a distinct, out-of-distribution set of engineering reports.

## Limitations
- The "Knowledge-Application Gap" mechanism assumes linear task complexity scaling, but real AEC problems often involve non-linear interdependencies
- The calibration method assumes stable human ground truth, but AEC expert rubrics may contain subjective elements with moderate Inter-Annotator Agreement
- The table formatting experiment compares only two approaches, leaving open questions about other potential representations (JSON, structured markdown)

## Confidence

- **High Confidence:** The existence of performance degradation across cognitive levels (4.6% → 24.2% for GPT-4o) is directly supported by reported results
- **Medium Confidence:** The mechanism linking this degradation to "Knowledge-Application Gaps" is plausible but relies on untested assumptions about task complexity scaling
- **Low Confidence:** The calibration method's effectiveness cannot be fully validated without access to specific human expert scores used for fitting

## Next Checks

1. **Task Dependency Analysis:** Design an experiment where models complete higher-level tasks without first completing lower-level ones to test whether performance decline is truly hierarchical or merely reflects task-specific knowledge requirements

2. **Cross-Format Table Evaluation:** Compare model performance on tabular data using manual text conversion, HTML, JSON, and structured markdown formats to identify the optimal representation for different types of engineering tables

3. **Calibration Robustness Test:** Apply the isotonic regression calibration to LLM-judged scores across multiple domains (not just AEC) to determine if the compression bias and correction are domain-specific or represent a general LLM scoring phenomenon