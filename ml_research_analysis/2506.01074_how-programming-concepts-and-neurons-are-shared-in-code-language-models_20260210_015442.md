---
ver: rpa2
title: How Programming Concepts and Neurons Are Shared in Code Language Models
arxiv_id: '2506.01074'
source_url: https://arxiv.org/abs/2506.01074
tags:
- languages
- english
- language
- neurons
- other
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) represent
  multiple programming languages (PLs) and English within their concept space. The
  authors use logit lens to analyze intermediate layer embeddings during few-shot
  translation tasks across 21 PL pairs, revealing that the concept space is closer
  to English and PL keywords, with these tokens appearing in the second half of intermediate
  layers.
---

# How Programming Concepts and Neurons Are Shared in Code Language Models

## Quick Facts
- **arXiv ID**: 2506.01074
- **Source URL**: https://arxiv.org/abs/2506.01074
- **Reference count**: 40
- **Primary result**: Concept space is closer to English and PL keywords, with language-specific neurons concentrated in bottom layers and language-exclusive neurons in top layers.

## Executive Summary
This paper investigates how large language models represent multiple programming languages and English within their concept space through three interpretability analyses. The authors analyze intermediate layer embeddings during few-shot translation tasks across 21 programming language pairs, revealing that the concept space is closer to English and PL keywords, with these tokens appearing in the second half of intermediate layers. They also apply language activation probability entropy to analyze neuron activations across 11 programming languages and English, finding that language-specific neurons are concentrated in bottom layers while those exclusive to each PL appear in top layers. These findings reveal structural patterns in how LLMs internally represent PLs, with implications for more efficient multilingual code models.

## Method Summary
The authors conduct three interpretability analyses on Llama-based models: logit lens decoding during few-shot code translation across 21 PL pairs, cross-lingual alignment (MEXA) measurement between PLs, and language-specific neuron identification (LAPE) for 11 PLs plus English. They use a super-parallel dataset of 581 code snippets across 7 PLs from GeeksforGeeks, keyword sets for 22 PLs plus English from PanLex, and raw code from GitHub Code dataset (50k files per PL for 11 PLs) plus English Wikipedia. The analysis is performed on CodeLlama 7B and Llama 3.1 8B (pretrained, not instruction-tuned) using token probability/rank by layer, MEXA alignment scores via cosine similarity, and perplexity change after neuron ablation. Key hyperparameters include α=10 for top decoded tokens, τ=0.95 for activation quantile, and ν≈400 neurons per language for LAPE.

## Key Results
- Concept space is closer to English and PL keywords, with these tokens appearing in the second half of intermediate layers
- Language-specific neurons are concentrated in bottom layers, while those exclusive to each PL appear in top layers
- For PLs like C# and Java, which align closely with multiple other PLs, identifying language-specific neurons is challenging

## Why This Works (Mechanism)
The model's internal representation of programming languages is structured hierarchically, with shared linguistic patterns (English keywords, common PL syntax) processed in middle-to-upper layers, while language-specific patterns are handled in specialized bottom and top layers. This organization allows the model to efficiently share knowledge across related PLs while maintaining distinct representations for language-specific constructs. The logit lens analysis reveals how token probabilities evolve through layers, showing convergence toward language-appropriate outputs. The LAPE method identifies neurons that activate specifically for certain languages by measuring activation probability entropy across languages.

## Foundational Learning
- **Logit lens decoding**: A technique for analyzing intermediate layer representations by applying unembedding to hidden states; needed to understand how token probabilities evolve through layers; quick check: verify LayerNorm is applied before unembedding
- **MEXA alignment**: Measures cross-lingual similarity using cosine similarity of parallel vs non-parallel pairs; needed to quantify how closely PLs align in representation space; quick check: ensure position-weighted averaging formula is correctly implemented
- **LAPE method**: Identifies language-specific neurons using activation probability entropy; needed to discover which neurons specialize for particular languages; quick check: confirm entropy normalization is across languages, not across neurons
- **Neuron ablation analysis**: Measures performance impact when zeroing specific neurons; needed to validate functional importance of discovered neurons; quick check: verify sufficient samples per language (50k files) for reliable probability estimates
- **Super-parallel dataset construction**: Requires parallel code snippets across multiple PLs; needed to enable cross-lingual analysis; quick check: confirm snippet parallelism across all 7 PLs in dataset
- **Keyword filtering criteria**: Single-token extraction with number removal; needed to create clean PL keyword sets; quick check: verify exact filtering rules are applied consistently

## Architecture Onboarding
- **Component map**: Data preprocessing -> Logit lens analysis -> MEXA alignment computation -> LAPE neuron identification -> Neuron ablation validation
- **Critical path**: Raw code files → Tokenization → Layerwise hidden state extraction → Probability computation → Neuron selection → Ablation → PPL measurement
- **Design tradeoffs**: Balance between model size (7B vs 8B) and computational efficiency vs representation capacity; trade-off between dataset breadth (7 PLs) and depth (50k samples per PL)
- **Failure signatures**: Near-zero probabilities in early layers indicates LayerNorm unembedding error; similar ablation effects across all languages suggests insufficient per-language samples or incorrect entropy normalization
- **First experiments**: 1) Verify logit lens produces expected probability distributions across layers; 2) Confirm LAPE identifies ~400 neurons per language with τ=0.95; 3) Test MEXA alignment scores for known related PL pairs (C#/Java) vs unrelated pairs

## Open Questions the Paper Calls Out
- **Open Question 1**: Does pre-training specifically on code-heavy datasets result in higher neuron sharing across programming languages compared to general-purpose pre-training? The authors hypothesize CodeLlama's "ineffectiveness of neuron identification" stems from its training recipe, suggesting a need for investigation across other models.
- **Open Question 2**: Do structural programming paradigms (e.g., strict object-orientation) causally influence the model's internal representation alignment and generation bias? The paper observes high alignment in C-family languages but doesn't isolate language paradigm strictness as a distinct variable.
- **Open Question 3**: Can the observed alignment and neuron distribution patterns be validated across a wider set of programming languages using generated synthetic data? Current findings are bound by the constraints of the GeeksforGeeks parallel corpus.

## Limitations
- Missing implementation details for few-shot prompt construction and MEXA embedding aggregation formula create uncertainty in exact reproduction
- Study focuses on a single model family (CodeLlama 7B and Llama 3.1 8B), limiting generalizability to other architectures
- Neuron ablation experiments show perplexity changes but don't establish causal necessity for language processing

## Confidence
- **High confidence**: Overall structural findings about concept space proximity to English and PL keywords, and general pattern of language-specific neurons appearing in bottom vs top layers
- **Medium confidence**: Exact layer positions where specific token types peak, and precise neuron counts per language
- **Low confidence**: Claims about functional necessity of discovered neurons, as ablation results don't prove these neurons are actually used during normal inference

## Next Checks
1. Verify that applying LayerNorm before unembedding in the logit lens implementation produces the expected probability distributions across layers, particularly checking early layers where near-zero probabilities would indicate implementation errors
2. Test the LAPE method's sensitivity to the τ threshold parameter by varying it and confirming that the method consistently identifies ~400 neurons per language while maintaining specificity
3. Reproduce the MEXA alignment scores using parallel vs non-parallel PL pairs to confirm that the position-weighted averaging formula produces the reported similarity patterns between closely related languages (C#/Java) versus distantly related ones