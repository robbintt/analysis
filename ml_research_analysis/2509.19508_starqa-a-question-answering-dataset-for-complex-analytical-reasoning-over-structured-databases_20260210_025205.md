---
ver: rpa2
title: 'STARQA: A Question Answering Dataset for Complex Analytical Reasoning over
  Structured Databases'
arxiv_id: '2509.19508'
source_url: https://arxiv.org/abs/2509.19508
tags:
- player
- integer
- none
- text
- home
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STARQA is a new dataset of complex analytical reasoning questions
  and answers over structured databases, focusing on tasks that require calculations,
  time-series analysis, and scenario understanding. To tackle such questions, the
  authors propose TEXT2SQLCODE, a method that decomposes the task into SQL for data
  fetching and Python for complex reasoning.
---

# STARQA: A Question Answering Dataset for Complex Analytical Reasoning over Structured Databases

## Quick Facts
- **arXiv ID:** 2509.19508
- **Source URL:** https://arxiv.org/abs/2509.19508
- **Reference count:** 40
- **Primary result:** New dataset of complex analytical reasoning questions requiring calculations, time-series analysis, and scenario understanding over structured databases

## Executive Summary
STARQA is a dataset of 362 complex analytical reasoning questions over structured databases that require calculations, time-series analysis, and scenario understanding. The dataset includes IMDb, EuroSoccer, and Olist domains with reference SQL and Python implementations. The authors propose TEXT2SQLCODE, which decomposes tasks into SQL for data fetching and Python for complex reasoning, achieving 4-15% accuracy improvements over SQL-only approaches. The best models achieve only ~48% accuracy, highlighting the challenging nature of this task. The work demonstrates the unique value of structured data over parametric knowledge or web search for analytical reasoning tasks.

## Method Summary
The method evaluates three approaches: Text2SQL (single SQL query), TEXT2SQLCODE (SQL+Python decomposition with Single and Multi variants), and a Hybrid system that uses self-consistency to route questions between approaches. TEXT2SQLCODE decomposes questions into SQL for data fetching and Python for reasoning, leveraging each language's strengths. The Hybrid approach runs Text2SQL three times and invokes TEXT2SQLCODE only when self-consistency is low. Models are evaluated on exact match accuracy against gold answers, which are lists of tuples.

## Key Results
- STARQA is challenging for state-of-the-art LLMs, with best models achieving only about 48% accuracy
- TEXT2SQLCODE improves performance over SQL alone by 4-15%, especially when selectively invoked for difficult questions
- Parametric knowledge and web search perform poorly (<10% accuracy) on analytical reasoning tasks requiring precise calculations over specific data points
- The hybrid selective invocation mechanism reduces unnecessary Python usage from >80% to significantly lower rates

## Why This Works (Mechanism)

### Mechanism 1: Decomposition of Data Fetching vs. Reasoning
Separating natural language questions into SQL for data retrieval and Python for complex logic improves accuracy by leveraging each language's strengths. SQL handles set-based operations efficiently while Python manages procedural logic like string manipulation and iterative math. This decomposition works because LLMs can identify which steps belong in each language and generate correct code without losing context between steps.

### Mechanism 2: Hybrid Selective Invocation
A hybrid system that defaults to Text2SQL but invokes SQL+Python pipeline only when SQL outputs show low self-consistency improves robustness. Self-consistency (majority voting across 3 runs) acts as a proxy for question difficulty, preventing LLMs from overusing complex tools for simple questions where errors can compound. This routing mechanism addresses the tendency of LLMs to overuse Python even when SQL would suffice.

### Mechanism 3: Structured Data Grounding for Analytical QA
Complex analytical questions requiring multi-step calculations over specific data points cannot be solved reliably by parametric knowledge or web search alone. Questions often need precise aggregations or time-series analysis over potentially private or granular data that parametric models hallucinate or web search cannot find. Grounding via SQL/Python execution ensures answers derive from actual data rather than memorized statistics.

## Foundational Learning

- **Concept: Self-Consistency as a Confidence Proxy**
  - *Why needed:* The Hybrid architecture depends on using variance in model outputs to decide when to escalate to Python pipeline
  - *Quick check:* If an LLM generates three different SQL queries for the same question, should the system trust any of them directly, or route to a different solver?

- **Concept: Procedural vs. Declarative Logic**
  - *Why needed:* To effectively decompose questions, one must distinguish between declarative operations (SQL: GROUP BY, JOIN) and procedural steps (Python: loops, regex, complex conditionals)
  - *Quick check:* Is "find the longest sequence of months" easier to express as a recursive CTE in SQL or as a loop in Python?

- **Concept: Error Propagation in Multi-Step Pipelines**
  - *Why needed:* The Text2SQLCode architecture is a pipeline; an error in SQL fetching step will cause runtime error in Python step even if Python logic is correct
  - *Quick check:* If SQL step retrieves player_id but Python step expects player_name, where did the failure occur?

## Architecture Onboarding

- **Component map:** User Question -> Self-Consistency Check (3 SQL runs) -> (If Consistent) Return SQL Answer -> (If Inconsistent) Decomposer -> SQL Execution -> Python Execution -> Final Answer

- **Critical path:** User Question is processed through self-consistency routing, with consistent SQL answers returned directly and inconsistent ones routed through decomposition, SQL execution, and Python processing

- **Design tradeoffs:** Single variant generates code in one pass (lower latency, less orchestration) vs. Multi variant allows distinct prompts but increases token usage and latency. SQL is more secure and faster while Python is more expressive for logic

- **Failure signatures:** Over-decomposition (unnecessary Python wrapping), context loss (SQL omitting required columns), and syntactic drift (unsupported libraries or dialects)

- **First 3 experiments:**
  1. Ablation on Routing: Compare Hybrid system vs. Text2SQL-only vs. Text2SQLCode-only to quantify pipeline robustness vs. expressive power
  2. Sensitivity Analysis: Vary self-consistency voting K (3 vs. 5 runs) to optimize routing accuracy vs. latency costs
  3. Error Taxonomy: Classify 50 failures of Multi variant to distinguish Decomposition Errors vs. Execution Errors

## Open Questions the Paper Calls Out

- Can agentic workflows that plan, examine, and correct their own mistakes significantly improve performance compared to current TEXT2SQLCODE approach? (The authors suggest future work could explore iterative agents with verifier components)

- How can models be calibrated to accurately assess question difficulty and route tasks between SQL and Python, preventing oblivious overuse of Python generation? (Current routing is "almost oblivious" of actual accuracy)

- Can models trained on STARQA generalize to perform complex analytical reasoning over unseen databases in new specialized domains? (The dataset could be extended to enable cross-domain training studies)

## Limitations

- Dataset size (362 questions) is relatively small, limiting generalizability to more diverse analytical scenarios
- Self-consistency as a proxy for difficulty may not hold uniformly across different domains or question types
- Hybrid approach assumes Python execution environments will always be available and properly sandboxed

## Confidence

- **High Confidence**: Core finding that combining SQL and Python improves accuracy over SQL-only approaches is well-supported by experimental results showing consistent 4-15% gains
- **Medium Confidence**: Effectiveness of hybrid selective invocation mechanism is supported, but correlation between self-consistency and difficulty could vary with different models
- **Medium Confidence**: Claim that parametric knowledge and web search are insufficient is demonstrated convincingly for STARQA domains but may not generalize to all analytical QA scenarios

## Next Checks

1. **Generalization Test**: Evaluate hybrid approach on a larger, more diverse analytical QA dataset to confirm self-consistency routing effectiveness across different domains and question types

2. **Failure Analysis**: Conduct systematic error classification of the 52% of questions that all models fail on STARQA to identify whether failures are due to decomposition errors, execution issues, or fundamental complexity limitations

3. **Latency-Accuracy Trade-off**: Measure end-to-end latency of hybrid approach versus baselines across different self-consistency thresholds to optimize balance between performance gains and computational overhead