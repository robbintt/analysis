---
ver: rpa2
title: Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range
  Dependency Pitfalls
arxiv_id: '2510.00184'
source_url: https://arxiv.org/abs/2510.00184
tags:
- layer
- head
- icot
- attention
- long-range
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates why Transformers fail to learn multi-digit
  multiplication, a task that seems simple yet remains challenging for large language
  models. The authors compare a standard fine-tuned model, which fails, with a model
  trained using implicit chain-of-thought (ICoT), which succeeds.
---

# Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls

## Quick Facts
- arXiv ID: 2510.00184
- Source URL: https://arxiv.org/abs/2510.00184
- Reference count: 21
- Standard fine-tuned Transformers fail to learn multi-digit multiplication, while models using implicit chain-of-thought succeed.

## Executive Summary
This paper investigates why Transformers struggle to learn multi-digit multiplication, a task that seems simple yet remains challenging for large language models. The authors compare a standard fine-tuned model, which fails, with a model trained using implicit chain-of-thought (ICoT), which succeeds. By reverse-engineering the ICoT model, they identify three key insights: (1) The ICoT model encodes long-range dependencies necessary for multiplication, unlike the standard model, as shown by logit attributions and linear regression probes; (2) Mechanistically, it uses attention to build a sparse, binary-tree-like graph that caches and retrieves partial products; (3) Geometrically, it represents digits using a Fourier basis and computes partial products as Minkowski sums, forming a pentagonal prism structure. These mechanisms are absent in the standard model. The authors then revisit standard fine-tuning and find that the model converges to a local optimum lacking long-range dependencies, failing to learn middle digits. To address this, they introduce an auxiliary loss that predicts a "running sum," providing an inductive bias that enables the model to achieve near-perfect accuracy on multiplication without ICoT. This work highlights a pitfall in Transformers for learning long-range dependencies and suggests that the right inductive biases can overcome this limitation.

## Method Summary
The authors conduct a controlled comparison between standard fine-tuned Transformers and models trained with implicit chain-of-thought (ICoT) on the task of multi-digit multiplication. They reverse-engineer the successful ICoT model using logit attribution, linear regression probes, and attention pattern analysis to uncover its internal mechanisms. To validate their findings, they introduce an auxiliary loss that predicts a "running sum" during training, providing an inductive bias that enables standard Transformers to learn multiplication without ICoT.

## Key Results
- Standard fine-tuned Transformers fail to learn multi-digit multiplication, while ICoT-trained models succeed.
- The ICoT model encodes long-range dependencies necessary for multiplication, unlike the standard model.
- An auxiliary loss predicting a "running sum" enables standard Transformers to achieve near-perfect accuracy on multiplication.

## Why This Works (Mechanism)
The ICoT model succeeds by encoding long-range dependencies necessary for multiplication, building a sparse, binary-tree-like graph using attention to cache and retrieve partial products, and representing digits using a Fourier basis with partial products computed as Minkowski sums forming a pentagonal prism structure. These mechanisms are absent in the standard model, which converges to a local optimum lacking the necessary long-range dependency encoding.

## Foundational Learning
- **Long-range dependencies**: Understanding how information flows across distant positions in sequences. Why needed: Multiplication requires carrying information across digits. Quick check: Can the model maintain accuracy when middle digits are masked or corrupted?
- **Attention mechanisms**: How Transformers use attention to build sparse graphs for information routing. Why needed: The ICoT model uses attention to create a binary-tree-like structure for partial products. Quick check: Do attention patterns form the expected sparse graph structure?
- **Fourier basis encoding**: Representing discrete values (digits) using sinusoidal functions. Why needed: The ICoT model encodes digits in Fourier space to enable geometric operations. Quick check: Do probing experiments confirm Fourier-like activation patterns?
- **Minkowski sums**: Geometric operation for combining sets. Why needed: Partial products are computed as Minkowski sums of digit representations. Quick check: Does the geometric interpretation predict activation patterns?
- **Local optima in neural networks**: Understanding why models converge to suboptimal solutions. Why needed: Standard fine-tuning converges to a solution that fails to learn multiplication. Quick check: Does the auxiliary loss successfully escape the identified local optimum?
- **Inductive biases**: How architectural or training choices guide learning. Why needed: The running sum auxiliary loss provides the right bias for learning multiplication. Quick check: Does the model maintain accuracy on out-of-distribution examples?

## Architecture Onboarding

### Component Map
Input digits -> Embedding layer -> Transformer blocks (with attention and MLP layers) -> Output logits -> Loss function (with or without auxiliary running sum loss)

### Critical Path
The critical path for multiplication involves: (1) Encoding digit representations, (2) Building a sparse graph via attention to route partial products, (3) Computing partial products using geometric operations, and (4) Aggregating results with proper carry handling. The standard model fails at step 2, while the ICoT model succeeds.

### Design Tradeoffs
- **Standard vs. ICoT training**: ICoT provides explicit guidance for building the computation graph but may be less generalizable; standard training is more flexible but can converge to local optima.
- **Fourier vs. direct encoding**: Fourier encoding enables geometric interpretations but may be less interpretable than direct digit embeddings.
- **Auxiliary loss vs. pure supervision**: Auxiliary losses provide helpful inductive biases but add complexity to training.

### Failure Signatures
- **Middle digit failure**: The standard model shows significant accuracy degradation on middle digits of multiplication results, indicating a lack of long-range dependency encoding.
- **Probe failure**: Linear regression probes fail to decode relevant information from the standard model's representations, confirming the absence of necessary computation structures.
- **Attention pattern analysis**: The standard model's attention patterns lack the sparse, tree-like structure seen in the ICoT model.

### 3 First Experiments
1. Evaluate accuracy degradation across digit positions to confirm the middle-digit failure pattern in the standard model.
2. Apply linear regression probes to test for the presence of long-range dependency encoding in both models.
3. Visualize attention patterns to compare the sparse graph structure between standard and ICoT models.

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis of the ICoT model's internal mechanisms relies on post-hoc interpretability methods that cannot definitively prove the exact computational strategy.
- The claim that standard fine-tuning converges to a distinct local optimum requires further investigation into the nature of this basin in the loss landscape.
- The scalability of the auxiliary loss approach to more complex arithmetic operations or larger numbers remains untested.

## Confidence

**High Confidence:**
- The empirical observation that ICoT succeeds where standard fine-tuning fails on multiplication.
- The auxiliary loss approach achieving high accuracy on multiplication.

**Medium Confidence:**
- The characterization of the failure mode (lack of long-range dependency encoding in standard fine-tuning).
- The proposed mechanism (sparse binary-tree graph, Fourier encoding).
- The geometric interpretation of digit representations as Fourier components forming Minkowski sums.

## Next Checks
1. Perform causal interventions on the ICoT model (e.g., attention masking, activation ablation) to confirm that disrupting the proposed binary-tree graph structure degrades multiplication performance.
2. Evaluate the auxiliary loss method on multiplication with numbers exceeding the training distribution (e.g., 5-digit x 5-digit) to assess robustness and true generalization versus memorization.
3. Compare Transformer performance with models specifically designed for long-range dependencies (e.g., state space models, convolutional architectures) on the same multiplication task to isolate whether the limitation is fundamental to the self-attention mechanism or specific to standard training.