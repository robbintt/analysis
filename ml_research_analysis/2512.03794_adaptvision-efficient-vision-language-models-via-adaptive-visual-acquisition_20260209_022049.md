---
ver: rpa2
title: 'AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition'
arxiv_id: '2512.03794'
source_url: https://arxiv.org/abs/2512.03794
tags:
- visual
- tool
- answer
- tokens
- adaptvision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces AdaptVision, a Vision-Language Model framework
  that achieves efficient visual reasoning by dynamically determining the minimum
  number of visual tokens needed for each sample. The key innovation is a coarse-to-fine
  approach: the model first processes compressed low-resolution images and selectively
  acquires additional visual information by invoking a bounding box tool to crop relevant
  regions when necessary.'
---

# AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition

## Quick Facts
- **arXiv ID**: 2512.03794
- **Source URL**: https://arxiv.org/abs/2512.03794
- **Reference count**: 40
- **Primary result**: Achieves 5.8% average performance improvement while using only 7% more visual tokens than 25% baseline

## Executive Summary
AdaptVision introduces an efficient Vision-Language Model framework that dynamically determines the minimum number of visual tokens needed for each sample. The key innovation is a coarse-to-fine approach where the model first processes compressed low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop relevant regions when necessary. To effectively train this dual-objective policy, the authors propose Decoupled Turn Policy Optimization (DTPO), which separately optimizes tool usage and answer generation while computing distinct advantages for each objective. Extensive experiments on multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance with significantly fewer visual tokens compared to state-of-the-art efficient VLM methods.

## Method Summary
AdaptVision implements a coarse-to-fine adaptive token acquisition system built on Qwen2.5-VL-7B-Instruct. The model processes 1/4 resolution images initially (≈25% tokens) and uses a bounding box tool to selectively fetch high-resolution regions when needed. Training employs Decoupled Turn Policy Optimization (DTPO) that separates tool-token and answer-token optimization with distinct advantage estimates, preventing under-optimization of tool-learning signals. The reward shaping combines accuracy rewards, format rewards, balance penalties, and tool-specific area penalties to guide the policy toward minimal sufficient token usage. The framework uses veRL with DTPO loss computation, batch size 512, mini-batch 32, and LR 1e-6.

## Key Results
- Achieves 5.8% average performance improvement across VQA benchmarks
- Uses only 7% more visual tokens than the 25% baseline compression
- Outperforms state-of-the-art efficient VLM methods including FastV, SparseVLM, and VisionZip
- Maintains stable training convergence with DTPO versus unstable GRPO behavior
- Demonstrates 1.67× real-time inference speedup over VisionThink†

## Why This Works (Mechanism)

### Mechanism 1: Coarse-to-Fine Adaptive Token Acquisition
Starting with low-resolution images and selectively fetching high-resolution regions reduces average visual token consumption while preserving task-specific detail access. The model first processes a 1/4 resolution image (≈25% tokens). If the policy decides the information is insufficient, it generates a bounding box `<tool call>[x1, y1, x2, y2]</tool call>` to crop a region from the original high-resolution image, appending only those additional tokens to the context. Total tokens: `n_img = n_low + 1_tool × n_crop`.

### Mechanism 2: Decoupled Turn Policy Optimization (DTPO)
Separating tool-token and answer-token optimization with distinct advantage estimates prevents the under-optimization of tool-learning signals that occurs in vanilla GRPO. DTPO decomposes the policy loss by turn type, normalizing tool tokens (`Σ Li,t / Ti`) and answer tokens (`Σ Li,t / (Ni - Ti)`) independently. It computes two advantages: `A_tool` from tool reward and `A_oc` from outcome reward, combining them with weighting factor λ=0.3.

### Mechanism 3: Balanced Reward Shaping for Exploration Control
Combining accuracy rewards, format rewards, balance penalties, and tool-specific area penalties guides the policy toward minimal sufficient token usage without collapsing to trivial strategies. `R = R_oc + R_tool`. Outcome reward (`R_acc + R_form + R_bal`) penalizes over-reliance on tools (−0.1 for correct tool calls) and lucky guesses (−0.1 for low-confidence direct answers). Tool reward (`R_crop − α × R_area`) encourages correct crops while penalizing oversized bounding boxes via relative area clipping.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: DTPO builds directly on GRPO's objective formulation. Understanding baseline GRPO is prerequisite to appreciating why decoupling is necessary.
  - Quick check question: Can you explain why GRPO uses group-level advantage normalization (`Ai,t = Ri − mean(R) / std(R)`) instead of per-sequence baselines?

- **Concept: Visual Token Compression in VLMs**
  - Why needed here: AdaptVision's contribution is framed against existing compression methods (FastV, SparseVLM, VisionZip). Understanding what "passive" compression means clarifies the novelty of "active" acquisition.
  - Quick check question: What is the difference between pruning tokens at a fixed ratio (FastV) and adaptively deciding whether to fetch more tokens (AdaptVision)?

- **Concept: Tool-Use in Vision-Language Models**
  - Why needed here: The bounding box tool is not a separate module but a generated action within the LLM's output space. Understanding tool-calling conventions in VLMs is essential.
  - Quick check question: How does a VLM generate a bounding box—as pixel coordinates, normalized values, or through a separate detection head?

## Architecture Onboarding

- **Component map**: Input Image → 1/4 Resolution Processing → VLM → Policy Decision → (Tool Call → External Crop → Append Tokens) → Final Answer

- **Critical path**:
  1. Input: 1/4 resolution image + question → VLM
  2. Policy decision: Generate reasoning in `<think/>` tags, then either `<answer>` or `<tool call>` with bbox
  3. If tool called: External system crops high-res image, appends crop tokens, VLM generates final answer
  4. During training: Collect G=16 samples per prompt, compute R_oc and R_tool, apply DTPO with decoupled advantages

- **Design tradeoffs**:
  - Single tool (bbox only) vs. multi-tool (zoom levels, multi-crop): Paper notes this as a limitation; current design limits to one crop per sample
  - Two-turn maximum vs. deeper reasoning: Current constraint; complex tasks may need iterative refinement
  - Fixed 1/4 initial resolution vs. dynamic: Paper acknowledges this limits adaptability further

- **Failure signatures**:
  - Tool call ratio → 100%: Balance reward missing or insufficient; model over-uses tool
  - Tool call ratio → 0%: Tool reward missing; model never explores tool usage
  - Training instability (oscillating rewards): GRPO used instead of DTPO; credit assignment ambiguous
  - Large bbox selections (high area penalty): α too low or R_crop not discriminating enough

- **First 3 experiments**:
  1. Reproduce the GRPO vs. DTPO comparison (Figure 5b) on a subset of training data to validate that vanilla GRPO exhibits unstable tool call ratio while DTPO converges.
  2. Ablate balance reward (R_bal) and observe tool call ratio collapse patterns—confirm that without balance, model either over-tools or under-tools depending on initial conditions.
  3. Evaluate inference latency on RealWorldQA benchmark comparing AdaptVision (33% avg tokens) vs. VisionThink† (99% tokens) vs. vanilla (100% tokens)—verify claimed 1.67× speedup and identify any overhead from tool-call token generation.

## Open Questions the Paper Calls Out

- Can extending AdaptVision beyond a single bounding-box tool to a multi-tool ecosystem (e.g., multi-scale zoom, segmentation masks, or iterative refinement) further improve the accuracy-efficiency trade-off?
- How does DTPO generalize to architectures beyond Qwen2.5-VL-7B-Instruct, particularly to VLMs with different tokenization schemes or visual encoders?
- Does removing the reliance on GPT-4o for reward computation (crop correctness and answer evaluation) maintain comparable training stability and final performance?

## Limitations

- Training data representativeness may not reflect real-world VQA task distributions, potentially overstating efficiency gains
- Tool-call overhead assumptions lack empirical measurement of latency and token costs during inference
- Single-tool constraint limits effectiveness on complex multi-step reasoning tasks requiring iterative region refinement

## Confidence

- **High Confidence**: DTPO improves training stability over vanilla GRPO (Figure 5b) - direct, reproducible observation
- **Medium Confidence**: 5.8% average performance improvement with 7% more visual tokens - aggregated results without individual benchmark breakdowns or statistical significance testing
- **Low Confidence**: Real-time inference speedup claims - no timing measurements or latency analysis provided

## Next Checks

1. **GRPO vs. DTPO Stability Reproduction**: Replicate Figure 5b on a subset of training data (e.g., first 1000 samples) to verify that vanilla GRPO exhibits unstable tool call ratio (collapse to 0 then surge to 100%) while DTPO maintains stable convergence. Measure training curves for tool call ratio, accuracy, and reward stability.

2. **Tool-Call Overhead Measurement**: Instrument the inference pipeline to measure actual latency and token costs of tool invocation. Compare: (a) baseline 1/4 resolution processing, (b) AdaptVision with tool calls, (c) full-resolution processing. Report: total tokens per sample, average tool-call frequency, and wall-clock time for each approach on RealWorldQA benchmark.

3. **Single-Tool Constraint Ablation**: Modify the framework to allow multiple tool calls per sample and evaluate performance on multi-step reasoning tasks (e.g., DocVQA with complex document layouts). Compare AdaptVision (1 tool limit) vs. unlimited tool calls on the same test set, measuring accuracy gains vs. token cost increases.