---
ver: rpa2
title: Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered
  LLMs
arxiv_id: '2504.11765'
source_url: https://arxiv.org/abs/2504.11765
tags:
- cache
- query
- rag-dcache
- documents
- caches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the growing inference latency in large language
  models (LLMs) caused by longer input contexts in retrieval-augmented generation
  (RAG). To reduce time-to-first-token (TTFT) and improve throughput, the authors
  propose a disk-based key-value (KV) cache management system called Shared RAG-DCache.
---

# Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs

## Quick Facts
- **arXiv ID:** 2504.11765
- **Source URL:** https://arxiv.org/abs/2504.11765
- **Reference count:** 27
- **Primary result:** Throughput improvements of 15-71% and latency reductions of 12-65% in multi-instance RAG-powered LLM inference using disk-based shared KV cache management

## Executive Summary
This paper addresses the growing inference latency in large language models (LLMs) caused by longer input contexts in retrieval-augmented generation (RAG). To reduce time-to-first-token (TTFT) and improve throughput, the authors propose a disk-based key-value (KV) cache management system called Shared RAG-DCache. The system precomputes and stores KV caches for frequently retrieved documents on disk, enabling reuse across multiple LLM instances. A shared KV cache manager and a proactive cache generator prefetch caches during query wait times. Experiments on a dual-GPU, single-CPU server showed throughput improvements of 15-71% and latency reductions of 12-65% depending on resource configuration. The approach effectively leverages query locality and idle compute time to reduce redundant prefill computations in multi-instance RAG-powered LLM services.

## Method Summary
The method precomputes KV caches for document chunks offline and stores them in a disk-resident vector database. During inference, when documents are retrieved for a query, their cached KV tensors are injected directly into the LLM context, allowing the model to skip the quadratic-complexity attention computation for document tokens. The prefill phase then only processes the user query tokens. For multi-instance setups, a centralized Shared KV Cache Manager coordinates cache storage/retrieval across all LLM instances, while a background KV Cache Generator proactively precomputes caches for queued queries during wait times. The approach exploits document query locality, where a small subset of documents serves a large fraction of queries, making caching cost-effective.

## Key Results
- **Performance gains:** Throughput improvements of 15-71% and latency reductions of 12-65% across different resource configurations
- **Resource efficiency:** Configuration (B) using CPU for KV generation outperformed dedicated GPU generation, as both GPUs remained available for inference parallelism
- **Locality exploitation:** Only 3.1-31.4% of most frequently retrieved documents account for 50% of all queries, validating the cache efficiency approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Precomputing and reusing KV caches for frequently retrieved documents reduces prefill-stage computation without degrading accuracy when top-k=1.
- **Mechanism:** The system generates KV tensors for document chunks offline, stores them in a disk-resident vector database, and injects them directly into the model's context during inference. The LLM treats these cached key-value pairs as if it had already processed the documents, skipping the quadratic-complexity attention computation for the document tokens. The prefill phase then only processes the user query tokens.
- **Core assumption:** Documents in the vector database are relatively static; cached KV states remain valid across queries until document content changes.
- **Evidence anchors:**
  - [abstract] "leverages a disk-based key-value (KV) cache to lessen the computational burden during the prefill stage"
  - [Section III.A] "By caching each document's transformer key-value pairs in advance, the LLM can skip directly to using this cached representation when that document is retrieved for a query"
  - [corpus] HyperRAG (arXiv:2504.02921) achieves similar KV-cache reuse for rerankers; TableCache (arXiv:2601.08743) applies precomputation for schema caching in Text-to-SQL
- **Break condition:** Accuracy degrades when top-k > 1 because cross-attention between retrieved documents is not computed (Figure 8 shows this tradeoff).

### Mechanism 2
- **Claim:** Query-document locality enables a small document subset to serve a large fraction of queries, making caching cost-effective.
- **Mechanism:** Analysis of HotpotQA, SQuAD, and TriviaQA shows that only 3.1–31.4% of the most frequently retrieved documents account for 50% of all queries. By caching this subset, the system achieves high cache hit rates with bounded storage.
- **Core assumption:** Real RAG workloads exhibit skewed document access distributions; the paper's dataset analysis generalizes to production query patterns.
- **Evidence anchors:**
  - [abstract] "exploits the locality of documents related to user queries in RAG"
  - [Section III.C] "only 22.9%, 3.1%, and 31.4% of the most frequently retrieved documents account for 50% of all queries"
  - [corpus] Related work (RAGCache, TurboRAG) relies on similar locality assumptions but lacks multi-instance sharing
- **Break condition:** If query patterns are uniformly distributed over a massive document corpus, cache hit rates drop and overhead may exceed benefits.

### Mechanism 3
- **Claim:** Proactively generating KV caches during queue wait times improves throughput without additional latency penalty.
- **Mechanism:** When queries exceed the configured queue-wait threshold, a background KV Cache Generator retrieves relevant documents and precomputes their KV caches using idle CPU or GPU resources. By the time a query reaches an LLM instance, its document caches are ready for direct injection, shifting prefill work from the critical path.
- **Core assumption:** Queue wait times increase under load (Figure 2 shows exponential growth beyond ~2 QPS per instance); idle resources exist during these periods.
- **Evidence anchors:**
  - [abstract] "proactively generates and stores disk KV caches for query-related documents and shares them across multiple LLM instances"
  - [Section III.B] "For a query that is stuck in the queue...the KV Cache Generator takes action: it immediately computes an embedding for the query, uses it to search the vector database for top-k relevant documents, and then computes the KV caches"
  - [corpus] TeleRAG (arXiv:2502.20969) uses lookahead retrieval overlap; corpus does not directly validate proactive cache generation during queue waits
- **Break condition:** Under light load with minimal queueing, the prefetcher rarely triggers and provides marginal benefit.

## Foundational Learning

- **Concept: KV Cache in Autoregressive Transformers**
  - **Why needed here:** Understanding that prefill computes key-value matrices for all input tokens once, while decode reuses them token-by-token, explains why caching the prefill output avoids redundant quadratic computation.
  - **Quick check question:** During standard LLM inference, why is the KV cache reused rather than recomputed at each decoding step?

- **Concept: RAG Prompt Composition**
  - **Why needed here:** RAG concatenates retrieved document text with the user query, inflating token count. The system decomposes this into (document KV cache) + (query tokens), which requires understanding how prompts are structured.
  - **Quick check question:** In a typical RAG pipeline, what components are concatenated to form the final LLM prompt?

- **Concept: Multi-Instance Inference Coordination**
  - **Why needed here:** Shared RAG-DCache requires a centralized cache manager and synchronized access across instances. Engineers must understand shared-state coordination patterns and where race conditions could emerge.
  - **Quick check question:** If two GPU instances simultaneously request a KV cache not yet in memory, what coordination mechanism prevents redundant disk reads?

## Architecture Onboarding

- **Component map:**
  - Query arrives → embedding generated → vector search retrieves document IDs
  - Prompt Generator requests KV caches from Shared KV Cache Manager
  - Manager checks CPU RAM cache → if miss, loads from NVMe SSD
  - Prompt Generator injects KV cache + query tokens into LLM context
  - LLM performs abbreviated prefill (query only) then decode

- **Critical path:**
  1. Query arrives → embedded → vector search retrieves document IDs
  2. Prompt Generator requests KV caches from Shared KV Cache Manager
  3. Manager checks CPU RAM cache → if miss, loads from NVMe SSD
  4. Prompt Generator injects KV cache + query tokens into LLM context
  5. LLM performs abbreviated prefill (query only) then decode

- **Design tradeoffs:**
  - **GPU vs. CPU for KV generation:** Configuration (B) using CPU for cache generation outperformed (A) with dedicated GPU, because both GPUs remained available for inference parallelism (Table V).
  - **Storage size vs. hit rate:** KV cache size scales with model parameters (Table II: 5.9GB for OPT-1.3B, 16GB for OPT-6.7B); engineers must provision disk for corpus size × model dimension.
  - **Accuracy vs. throughput at top-k > 1:** Skipping cross-document attention improves speed but reduces accuracy (Figure 8); the paper's alternative approach computes combined KV for top-k document sets.

- **Failure signatures:**
  - Stale cache: Document updated but KV cache not regenerated → outputs reflect old content
  - Cache stampede: Multiple instances request same uncached document simultaneously → redundant precomputation
  - Disk bandwidth saturation: Under high query rates, NVMe I/O becomes bottleneck if RAM cache miss rate is high
  - Queue threshold misconfiguration: Threshold too low → prefetcher runs unnecessarily; too high → queries processed before prefetch completes

- **First 3 experiments:**
  1. **Baseline comparison:** Run single-instance RAG vs. RAG-DCache with OPT-1.3B on 500 SQuAD queries; measure TTFT breakdown (prefill time vs. cache load time) to confirm overhead is smaller than prefill savings.
  2. **Locality validation:** On your production query log, compute the CDF of document access frequency; verify that <30% of documents cover >50% of queries before investing in cache infrastructure.
  3. **Resource allocation test:** In a dual-GPU setup, compare Configuration (A) vs. (B) at 40 QPS load with top-k=2; confirm CPU-based KV generation yields higher throughput and lower latency per Table V.

## Open Questions the Paper Calls Out

- **Question:** How does network latency impact throughput when scaling Shared RAG-DCache to a distributed multi-host cluster?
  - **Basis in paper:** [Explicit] The introduction claims explicit support for "multi-host" environments to differentiate from prior work, but the evaluation (Section IV) is constrained to a single host setup.
  - **Why unresolved:** The system relies on a shared disk and RPCs; without distributed testing, the performance impact of network transfer speeds for large KV caches is unknown.
  - **What evidence would resolve it:** Benchmark results from a multi-node cluster showing throughput/latency with network-enabled cache sharing.

- **Question:** Does the Shared RAG-DCache generation method strictly maintain model accuracy compared to baseline RAG, particularly for varying top-k values?
  - **Basis in paper:** [Inferred] The authors note accuracy drops in the single-instance approach when top-k > 1 (due to missing cross-attention) but explicitly exclude accuracy measurements for the Shared RAG-DCache experiment, assuming equivalence.
  - **Why unresolved:** The assumption that proactive generation preserves accuracy without empirical verification leaves a potential quality gap in the proposed solution.
  - **What evidence would resolve it:** Accuracy metrics (e.g., F1, Exact Match) comparing Shared RAG-DCache against baseline RAG for the multi-instance workload.

- **Question:** Does the CPU-based KV generation strategy remain optimal when deploying significantly larger LLMs (e.g., 70B+ parameters)?
  - **Basis in paper:** [Inferred] The study identifies CPU-based generation as the optimal configuration (Config B) for models up to 6.7B parameters, but this balance may shift if prefill computation becomes a severe CPU bottleneck for larger models.
  - **Why unresolved:** The compute-to-memory bandwidth ratio changes with massive models, potentially making the GPU-dedicated generation strategy (Config A) necessary for larger scale.
  - **What evidence would resolve it:** Comparative benchmarks of Config A vs. Config B using 70B parameter models.

## Limitations

- **Top-k > 1 accuracy degradation:** The system's KV cache reuse assumes a single retrieved document, and accuracy drops when multiple documents are used due to missing cross-attention computation. This limits applicability to RAG workflows requiring diverse document sources or when retrieval uncertainty is high.
- **Dataset representativeness:** The HotpotQA, SQuAD, and TriviaQA query-document locality patterns (3.1-31.4% of documents covering 50% of queries) may not generalize to production RAG workloads with different document distributions or longer-tailed query patterns.
- **Resource configuration dependency:** Performance gains are highly sensitive to hardware setup, particularly the choice between GPU vs. CPU for KV cache generation and the queue wait threshold configuration. Optimal settings may vary significantly across deployment environments.

## Confidence

- **High confidence:** The core mechanism of KV cache reuse for single-document retrieval is well-supported by transformer theory and the paper's experimental results (15-71% throughput improvement with top-k=1).
- **Medium confidence:** The locality assumption and its practical implications are plausible based on the cited datasets but require validation in production environments with real query distributions.
- **Low confidence:** The multi-instance coordination implementation details (cache stampede prevention, race conditions, memory cache eviction policies) are not fully specified in the paper.

## Next Checks

1. **Accuracy validation with top-k=1:** Run the system on your production query log with top-k=1 to verify that answer quality matches baseline RAG before deploying KV cache reuse.
2. **Locality analysis:** Compute the document access frequency distribution on your production query logs to determine what percentage of documents would need caching to achieve 50% coverage.
3. **Resource configuration benchmarking:** Test both Configuration A (dedicated GPU for KV generation) and Configuration B (CPU for KV generation) at your expected query load to determine optimal setup for your hardware constraints.