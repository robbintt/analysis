---
ver: rpa2
title: 'Documents Are People and Words Are Items: A Psychometric Approach to Textual
  Data with Contextual Embeddings'
arxiv_id: '2509.08920'
source_url: https://arxiv.org/abs/2509.08920
tags:
- factors
- contextual
- data
- factor
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel psychometric approach for analyzing
  textual data using contextual embeddings from large language models. By treating
  documents as individuals and words as items, the method generates contextual scores
  through dot products of word-level and document-level embeddings, enabling psychometric
  analysis.
---

# Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings
## Quick Facts
- arXiv ID: 2509.08920
- Source URL: https://arxiv.org/abs/2509.08920
- Authors: Jinsong Chen
- Reference count: 39
- Primary result: 13 interpretable general factors extracted from Wiki STEM corpus using contextual embeddings and bifactor analysis

## Executive Summary
This paper introduces a novel psychometric framework that treats documents as individuals and words as items, enabling factor analysis of textual data using contextual embeddings from large language models. The method generates contextual scores through dot products between word-level and document-level embeddings, creating a response matrix suitable for psychometric analysis. Applied to the Wiki STEM corpus, the approach identifies 13 interpretable general factors representing knowledge dimensions, with strong evidence of normality, stability, and convergent/discriminant validity.

## Method Summary
The approach transforms textual data into psychometric response data by treating documents as individuals and words as items. BERT-large generates document embeddings (mean-pooled to avoid multicollinearity) and contextual word embeddings through targeted prompts. Contextual scores are computed as dot products between document embeddings and word-level embeddings for each word-document pair. After filtering to 1,423 common keywords across 20,000 documents (50-500 tokens each), parallel analysis estimates first-order factors (~60), followed by second-order factor analysis and exploratory bifactor analysis with Schmid-Leiman transformation. Classical item analysis validates the factor structure through convergent and discriminant validity checks.

## Key Results
- 13 interpretable general factors extracted via bifactor analysis from Wiki STEM corpus
- Contextual scores exhibit normality and stability across three time points
- Classical item analysis confirms strong convergent and discriminant validity
- Mean-pooling document embeddings avoids multicollinearity issues seen with CLS embeddings (r ≈ 0.63)

## Why This Works (Mechanism)
The approach leverages the representational power of contextual embeddings to capture nuanced word-document relationships. By computing dot products between document and word embeddings, it creates continuous response variables that reflect how well each word "fits" within each document's context. The bifactor structure separates general knowledge dimensions from specific factors, enabling interpretable latent variable analysis of textual data at scale.

## Foundational Learning
- **Contextual embeddings**: Word representations that capture meaning based on surrounding text; needed to encode semantic relationships between words and documents; quick check: verify embeddings change meaningfully with context
- **Bifactor analysis**: Factor model with one general factor and multiple group factors; needed to identify both broad and specific knowledge dimensions; quick check: examine factor loadings for general vs specific contributions
- **Schmid-Leiman transformation**: Orthogonal rotation method for bifactor models; needed to partition variance between general and specific factors; quick check: verify orthogonality of factor solutions
- **Parallel analysis**: Simulation-based method for determining number of factors; needed to avoid overfactoring sparse correlation matrices; quick check: compare scree plot to simulated eigenvalues
- **Classical item analysis**: Validation technique using item-total correlations; needed to assess psychometric quality of factors; quick check: verify convergent correlations exceed discriminant correlations
- **Mean-pooling vs CLS embeddings**: Different document representation strategies; needed to avoid multicollinearity in embeddings; quick check: compare word-pair correlations across methods

## Architecture Onboarding
- **Component map**: Corpus preprocessing -> TF-IDF keyword extraction -> BERT embedding generation -> Contextual score computation -> Factor analysis pipeline (parallel analysis -> second-order FA -> bifactor transformation) -> Validation
- **Critical path**: Document embedding generation and contextual score computation are bottlenecks; embedding quality directly determines factor interpretability
- **Design tradeoffs**: Mean-pooling sacrifices sentence-level information for computational stability; TF-IDF keyword selection balances coverage vs interpretability
- **Failure signatures**: Multicollinearity (high word-pair correlations), overfactoring (too many first-order factors), or poor interpretability (weak general factor loadings)
- **First experiments**: 1) Verify correlation between CLS and mean-pooled embeddings, 2) Test parallel analysis with simulated data to confirm ~60 factor estimate, 3) Run classical item analysis on sample contextual scores

## Open Questions the Paper Calls Out
None

## Limitations
- Unspecified FA rotation methods and parallel analysis implementation details limit exact replication
- Results depend heavily on BERT-large embedding quality and dot product assumptions
- Keyword filtering choices significantly affect factor interpretation and generalizability
- Stability analysis lacks detail on temporal variation in document composition

## Confidence
- **High confidence**: Core methodological framework is clearly specified and reproducible
- **Medium confidence**: Factor structure and validity results are sound but rotation methods unspecified
- **Low confidence**: Stability claims lack methodological detail for temporal consistency assessment

## Next Checks
1. Replicate first-order factor extraction using parallel analysis with same corpus preprocessing to verify ~60 factors emerge
2. Test alternative document embedding methods (CLS vs mean-pooling) to confirm multicollinearity issue (r ≈ 0.63)
3. Apply classical item analysis (within-item and between-item total correlations) on reproduced contextual scores to verify validity patterns