---
ver: rpa2
title: 'APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training'
arxiv_id: '2510.24397'
source_url: https://arxiv.org/abs/2510.24397
tags:
- uni00000048
- uni00000010
- uni00000016
- uni00000051
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces APTBench, a novel benchmark designed to evaluate
  the agentic potential of base language models during pre-training. Current benchmarks
  for base models focus on isolated skills like knowledge, math, and coding, but fail
  to capture the multi-turn planning and execution capabilities needed for real-world
  autonomous agents.
---

# APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training

## Quick Facts
- **arXiv ID**: 2510.24397
- **Source URL**: https://arxiv.org/abs/2510.24397
- **Reference count**: 40
- **Primary result**: Introduces APTBench, a novel benchmark that evaluates agentic potential of base LLMs by converting real-world agent tasks and successful trajectories into multiple-choice or text completion questions, better correlating with downstream agent performance than general benchmarks.

## Executive Summary
This paper introduces APTBench, a novel benchmark designed to evaluate the agentic potential of base language models during pre-training. Current benchmarks for base models focus on isolated skills like knowledge, math, and coding, but fail to capture the multi-turn planning and execution capabilities needed for real-world autonomous agents. APTBench addresses this gap by converting real-world agent tasks and successful trajectories into multiple-choice or text completion questions tailored for base models, focusing on core agentic abilities such as planning, action, and domain-specific atomic skills. The benchmark covers two key scenarios—software engineering and deep research—and includes tasks like environment setup, bug fixing, and web-based research with citation generation. Experiments across small, medium, and large models show that APTBench better correlates with downstream agent performance than general benchmarks. It also reveals an emergence threshold in model size for agent capabilities and highlights the critical role of agent-relevant pre-training data. APTBench offers an economical, scalable way to evaluate and guide agentic pre-training, providing actionable insights for developing more capable agent base models.

## Method Summary
APTBench converts real-world agent tasks and successful trajectories into multiple-choice or text completion questions tailored for base models. The process involves collecting tasks and trajectories from software engineering and deep research scenarios, extracting core agentic abilities (planning, action, atomic skills), generating correct answers from successful trajectories, and using LLMs to create degraded negative choices for MCQs. The benchmark is evaluated using 3-shot prompting with vLLM and FlashAttention2, measuring accuracy for MCQs and exact match/ROUGE for text completion tasks.

## Key Results
- APTBench scores correlate strongly with downstream agent performance (SWE-bench Verified) while general benchmarks show weak or negative correlation.
- A clear emergence threshold exists: models below ~4B parameters show significantly degraded agentic performance regardless of architecture.
- Training data composition is the most critical factor for agentic performance, with specialized agent-relevant pre-training data yielding substantial gains.
- Long-context tasks can confound evaluation, with correlation improving when these are removed.

## Why This Works (Mechanism)

### Mechanism 1: Trajectory-to-Question Decomposition
- Claim: Converting multi-turn agent trajectories into single-step decision points captures agentic potential in base models.
- Mechanism: APTBench extracts decision points from successful trajectories and reformulates them as either multiple-choice questions (for ambiguous/planning steps) or text completion (for concrete actions), enabling evaluation without requiring instruction-following or multi-turn execution capabilities.
- Core assumption: Single-step decision quality correlates with multi-turn agentic performance downstream.
- Evidence anchors:
  - [abstract]: "converting real-world agent tasks and successful trajectories into multiple-choice or text completion questions tailored for base models"
  - [section 2.1]: "We identify several core abilities that agents must demonstrate during their interactions, including Planning, Action, and Atomic Abilities"
  - [corpus]: Hephaestus paper similarly demonstrates that agent-oriented pre-training data improves fundamental agent capabilities, supporting the trajectory-decomposition approach.
- Break condition: If single-step decision scores do not correlate with end-to-end agent task performance (Pearson r near 0), the decomposition assumption fails.

### Mechanism 2: General Benchmark Decoupling
- Claim: Standard pre-training benchmarks fail to predict agent capabilities because they measure static, isolated skills rather than dynamic decision-making under uncertainty.
- Mechanism: General benchmarks (MMLU, GSM8K, EvalPlus) test knowledge retrieval and single-step reasoning, while agent tasks require planning-action-feedback loops with external environment interaction and error recovery.
- Core assumption: Agentic capabilities form a distinct capability axis from knowledge/math/coding skills.
- Evidence anchors:
  - [abstract]: "current pre-training benchmarks primarily focus on isolated and static skills, e.g., common knowledge or mathematical/code reasoning, and fail to reflect model's agentic capabilities"
  - [Figure 1]: "six models with similar MMLU scores (86-88) show a 30-point difference on SWE-Bench"
  - [corpus]: Scaling Agents paper notes "post-training approaches building upon general-purpose foundation models consistently underperform in agentic tasks"
- Break condition: If improvements in general benchmarks reliably transfer to agent performance improvements, the decoupling claim weakens.

### Mechanism 3: Emergence Threshold Effect
- Claim: Agentic capabilities exhibit a critical model-size threshold below which performance is significantly degraded regardless of architecture.
- Mechanism: Below approximately 4B parameters, models cannot effectively perform the reasoning required for multi-step planning and action selection, creating a floor effect on agent benchmarks.
- Core assumption: The threshold represents a fundamental capability limit, not merely training data exposure differences.
- Evidence anchors:
  - [section 3.1]: Qwen3-1.7B scores 24.27 on APTBench-SWE vs 38.75+ for 4B/8B/30B models—clear performance gap
  - [section 3.1]: "the emergence of agent capabilities requires the model to exceed a fundamental parameter size threshold"
  - [corpus]: Weak corpus evidence on size thresholds specifically; related work focuses on data quality over size.
- Break condition: If smaller models with specialized agent training data can match or exceed larger models' agent performance, the threshold is data-driven not capacity-driven.

## Foundational Learning

- Concept: **Base Model vs Instruct Model Evaluation Constraints**
  - Why needed here: Base models cannot follow complex instructions or execute multi-turn tasks, requiring MCQ/text-completion formats rather than open-ended execution.
  - Quick check question: Can the evaluation be completed with only next-token prediction and no instruction-following?

- Concept: **Planning-Action-Feedback Loop**
  - Why needed here: Understanding what differentiates agent tasks from static benchmarks—dynamic decision-making with external environment responses.
  - Quick check question: Does the task require adapting subsequent decisions based on feedback from previous actions?

- Concept: **Negative Choice Generation via Degradation**
  - Why needed here: APTBench relies on LLM-generated distractor options that are plausible but provably suboptimal to test genuine discrimination.
  - Quick check question: Are distractor options realistic enough that a capable model could plausibly select them?

## Architecture Onboarding

- Component map:
  Task/Trajectory Collection → Question Formulation → Answer Generation → Evaluation

- Critical path:
  1. Collect successful trajectories from real agent task executions (human or agent-generated)
  2. Decompose trajectories into decision points, categorized by ability type (planning/action/atomic)
  3. Format each decision point as MCQ (ambiguous, multiple valid approaches) or TC (deterministic, single correct action)
  4. Generate 5 incorrect choices per MCQ via LLM degradation with controlled error types
  5. Evaluate base models using 3-shot few-shot prompting; measure Accuracy (MCQ) or EM/ROUGE (TC)

- Design tradeoffs:
  - MCQ vs TC: MCQ handles planning ambiguity better but risks multiple valid answers; TC is precise but harder for base models
  - Long-context tasks: >16K token inputs test both agent reasoning and context-handling jointly—can confound results
  - Coverage vs correlation strength: Removing longest tasks improves downstream correlation, suggesting context-length confounds

- Failure signatures:
  - Low Pearson correlation with downstream agent benchmarks (e.g., SWE-bench Verified) indicates proxy failure
  - Significant score gap between similar-sized models suggests training data quality as dominant factor
  - Uniformly poor performance on specific ability types (e.g., planning but not action) indicates targeted capability gaps

- First 3 experiments:
  1. **Correlation validation**: Plot APTBench-SWE and APTBench-DR scores against SWE-bench Verified performance for same model families; target Pearson r > 0.7
  2. **Ability ablation**: Evaluate models on planning-only vs action-only vs atomic-only subsets to identify which ability drives overall scores
  3. **Size threshold test**: Evaluate models across 1B–30B+ parameter range on same APTBench subset to confirm emergence threshold location

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the static multiple-choice and text-completion evaluation framework effectively generalize to high-stakes, real-time agent domains (e.g., OS control) where the feedback loop is significantly more dynamic than in Software Engineering or Deep Research?
- Basis in paper: [explicit] The authors state in Section 2.5 that their "construction method is general and can be easily extended to other agent scenarios," yet the current evaluation is limited to SWE and Deep Research (Page 8).
- Why unresolved: The current benchmark relies on "successful trajectories" (Page 3) from tasks that are largely offline or semi-interactive. It is unclear if the "Planning-Action-Feedback" loop can be distilled into static MCQs for tasks requiring millisecond-level real-time decisions.
- What evidence would resolve it: Successful application of the APTBench construction pipeline to datasets like OSWorld or AndroidWorld, showing strong correlation with end-to-end agent performance in those environments.

### Open Question 2
- Question: To what extent is the performance on APTBench measuring distinct "agentic potential" versus measuring long-context processing capabilities, given that correlation improved when long-context tasks were removed?
- Basis in paper: [explicit] The discussion in Section 3.2 notes that after removing tasks with very long context, "APTBench exhibits a stronger correlation," suggesting that some models' lower scores were due to "less robust capacity to handle long-context" rather than a lack of agentic ability (Page 10).
- Why unresolved: The benchmark heavily features long-context inputs (Figure 3), potentially confounding the evaluation of core agentic skills (planning/action) with the model's ability to simply process large input windows.
- What evidence would resolve it: A controlled ablation study evaluating models with similar reasoning capabilities but varying context windows on a short-context version of APTBench.

### Open Question 3
- Question: What is the optimal data composition ratio for agentic pre-training to replicate the strong performance of specialized models like Seed-OSS-36B without sacrificing general reasoning capabilities?
- Basis in paper: [explicit] The authors observe in Section 3.1 that "Training data is the most essential part of agentic pre-training" and highlight significant performance gaps between models with similar sizes but different data mixes (Page 9).
- Why unresolved: While the paper establishes that data quality is the primary driver, it does not define the specific mixture (e.g., ratio of trajectory data to code/math data) required to achieve the "sweet spot" observed in the 30B parameter range.
- What evidence would resolve it: A systematic study of pre-training runs with varying percentages of agent-specific trajectory data, measuring both APTBench scores and general benchmarks (MMLU/MATH) to identify the trade-off point.

## Limitations

- Data Dependency and Quality: APTBench's effectiveness depends heavily on the quality and representativeness of the underlying agent trajectories used to generate questions.
- Context-Length Confounding: The correlation between APTBench scores and downstream agent performance weakens when long-context tasks are included, suggesting context-handling ability may be confounding agentic reasoning.
- Single-Step Proxy Validity: The fundamental assumption that single-step decision quality in base models correlates with multi-turn agent performance remains unproven at scale.

## Confidence

- **High Confidence**: The benchmark construction methodology (trajectory decomposition into MCQ/TC format) is well-specified and reproducible. The emergence threshold effect across model sizes is clearly observable in the presented data.
- **Medium Confidence**: The claim that APTBench better predicts downstream agent performance than general benchmarks is supported by SWE-bench correlations but needs broader validation across multiple agent benchmarks.
- **Low Confidence**: The assertion that agentic capabilities require a fundamental parameter threshold independent of training data quality is weakly supported, with limited evidence that smaller models cannot achieve comparable performance through specialized training.

## Next Checks

1. **Cross-Benchmark Validation**: Evaluate APTBench performance correlation with at least two additional agent benchmarks (beyond SWE-bench) such as DeepPlanning or AgentDo benchmarks to confirm the proxy validity across diverse agent domains.

2. **Data Quality Ablation**: Reconstruct APTBench using agent trajectories from different source systems (e.g., Claude's research agent vs. DeepResearch) and measure score variance to quantify sensitivity to trajectory source quality and diversity.

3. **Context-Length Isolation**: Recompute APTBench correlations after systematically filtering tasks by context length (e.g., <4K tokens vs >16K tokens) to definitively separate context-handling effects from pure agentic reasoning capability.