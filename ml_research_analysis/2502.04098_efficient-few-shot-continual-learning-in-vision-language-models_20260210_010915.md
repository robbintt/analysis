---
ver: rpa2
title: Efficient Few-Shot Continual Learning in Vision-Language Models
arxiv_id: '2502.04098'
source_url: https://arxiv.org/abs/2502.04098
tags:
- lorsu
- learning
- continual
- dataset
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LoRSU, a parameter-efficient fine-tuning
  method designed for few-shot continual learning in vision-language models. The approach
  selectively updates specific parameters in the image encoder, particularly attention
  heads and MLP layers, based on gradient importance scores.
---

# Efficient Few-Shot Continual Learning in Vision-Language Models

## Quick Facts
- arXiv ID: 2502.04098
- Source URL: https://arxiv.org/abs/2502.04098
- Reference count: 40
- Primary result: LoRSU achieves up to 25× computational savings compared to full model updates in few-shot continual learning for VLMs

## Executive Summary
This paper introduces LoRSU, a parameter-efficient fine-tuning method designed for few-shot continual learning in vision-language models. The approach selectively updates specific parameters in the image encoder, particularly attention heads and MLP layers, based on gradient importance scores. This structured adaptation improves task-specific performance while preserving general knowledge. Experimental results show LoRSU achieves up to 25× computational savings compared to full model updates and demonstrates consistent performance gains across ten VQA datasets in three few-shot continual learning settings, with minimal forgetting of previously learned knowledge.

## Method Summary
LoRSU adapts image encoders in VLMs for few-shot continual learning by selectively updating attention heads and MLP parameters based on gradient importance scores. The method computes importance scores for each attention head using gradient magnitudes of query, key, and value matrices, then selects the top-k heads for LoRA adaptation. Additionally, it applies binary masking to the first MLP layer (fc1) gradients, updating only the top fraction of parameters by magnitude. This structured approach enables efficient domain adaptation while minimizing interference with general knowledge, achieving 25× computational savings compared to full fine-tuning while maintaining performance on non-target tasks.

## Key Results
- LoRSU achieves up to 25× computational savings compared to full model updates
- Outperforms LoRA and SPU baselines across ten VQA datasets in CL-5, CL-20, and CL-50 settings
- Demonstrates consistent Target Improvement (TI) gains while maintaining positive Control Change (CC) scores
- Gradient-based head selection significantly outperforms random and full-head approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-based importance scoring identifies attention heads that, if updated, yield the largest reduction in task loss while minimizing interference with unrelated knowledge.
- Mechanism: For each attention head i, LoRSU computes importance score sᵢ = Σ(‖∇W_q L‖² + ‖∇W_k L‖² + ‖∇W_v L‖²). The top-k heads by score are selected for LoRA adaptation. This is theoretically justified as the solution to a constrained optimization maximizing preserved gradient norm under sparsity constraints (Eq. 6, Lemma A.2).
- Core assumption: Gradient magnitude at initialization correlates with parameter importance for the current task, and parameters with low gradient magnitude are safe to freeze without degrading task performance.
- Evidence anchors:
  - [abstract] "leverages theoretical insights to identify and update only the most critical parameters"
  - [Section 3] Eq. (3) defines importance scores; Eq. (6) provides theoretical justification via binary optimization
  - [corpus] Neighbor papers (e.g., "Decomposing and Composing") show LoRA-based selection is an active research direction, but comparative evidence for gradient-based head selection vs. alternatives remains limited
- Break condition: If gradient information is noisy (extremely few-shot, k<3) or the pre-trained encoder is already near-optimal for the target domain, importance scores may be unreliable, leading to sub-optimal head selection.

### Mechanism 2
- Claim: Applying LoRA only to selected attention heads reduces trainable parameters while preserving expressivity for domain-specific visual features.
- Mechanism: For selected heads i ∈ I, weight matrices are reparameterized as W′ = W + AB (rank-r decomposition). Only A, B are trained. Combined with freezing the output projection W_o, this localizes changes. Experiments show rank r=64 and top-k=2 heads perform best (Tables 23, 24).
- Core assumption: Task-relevant visual adaptation can be captured in a low-rank subspace of attention weights; higher-rank or full-matrix updates are unnecessary for few-shot domain shifts.
- Evidence anchors:
  - [Section 3] Eq. (4) defines the LoRA reparameterization for q, k, v matrices
  - [Section 4.5, Table 4] LoRSU outperforms LoRSU-AAH (all heads) and LoRSU-Rand (random heads)
  - [corpus] "Dual-Phase Continual Learning" and "Decomposing and Composing" papers corroborate LoRA's effectiveness in VLM continual learning, though head selection criteria vary
- Break condition: When visual domains require global representational changes (e.g., fundamentally different image statistics), low-rank updates to few heads may be insufficient, requiring higher rank or more heads.

### Mechanism 3
- Claim: Sparse gradient masking on the first MLP layer (fc1) provides complementary adaptation capacity without interfering with attention-based feature selection.
- Mechanism: A binary mask M_fc1 selects the top fraction of parameters (by squared gradient magnitude) in the first linear layer of each transformer's MLP block. Only these parameters receive gradient updates: ∇̂L = M ⊙ ∇L (Eq. 5). Sparsity=10% was used (Appendix B).
- Core assumption: fc1 acts as a key-value store for localized visual knowledge (inspired by "knowledge neuron" theory), and sparse updates suffice for correction without disrupting stored generic knowledge.
- Evidence anchors:
  - [Section 3] Eq. (5) defines the masked gradient update for fc1
  - [Section 2] SPU baseline (Zhang et al., 2024) motivates fc1 updates; LoRSU generalizes by adding attention head selection
  - [corpus] Evidence for fc1 as knowledge storage is indirect; corpus papers do not strongly validate this assumption
- Break condition: If task-specific adaptation requires modifying deeper MLP layers or LayerNorm parameters (e.g., for distribution shift normalization), fc1-only sparsity may underperform.

## Foundational Learning

- **Multi-Head Self-Attention (MSA) in Vision Transformers**:
  - Why needed here: LoRSU selectively updates attention heads based on importance scores derived from gradients of W_q, W_k, W_v. Understanding how these matrices shape token interactions is essential to interpret why certain heads are critical.
  - Quick check question: Given a 12-layer ViT with 16 heads per layer, how many attention weight matrices are there, and what does freezing all but 2 heads per layer imply for trainable capacity?

- **Low-Rank Adaptation (LoRA)**:
  - Why needed here: LoRSU relies on LoRA to parameterize weight updates as W + AB, enabling efficient fine-tuning. Understanding rank, initialization, and scaling is critical for implementation.
  - Quick check question: If a weight matrix is 1024×1024 and LoRA rank r=64, how many trainable parameters does LoRA introduce per matrix, and how does this compare to full fine-tuning?

- **Catastrophic Forgetting in Continual Learning**:
  - Why needed here: LoRSU is designed for few-shot continual learning where new tasks arrive sequentially. The method's sparse, localized updates aim to mitigate forgetting of generic knowledge.
  - Quick check question: Why does updating all parameters on a new task typically degrade performance on earlier tasks, and how does gradient masking help?

## Architecture Onboarding

- **Component map**:
  - Input: Image encoder (CLIP ViT-L/14) → frozen except LoRA adapters on selected attention heads and sparse fc1 updates
  - Image encoder outputs → MLP projector (frozen) → LLM (Vicuna-7B, frozen)
  - LoRSU modules per transformer block: (1) LoRA adapters on top-k attention heads (k=2 default), (2) Binary mask on fc1 gradients (10% sparsity)
  - Loss: CLIP contrastive loss (primary) or perplexity loss (optional, LoRSU-Ppl variant)

- **Critical path**:
  1. Forward pass through frozen CLIP encoder with LoRA adapters (if enabled)
  2. Compute task loss (CLIP or perplexity) on current mini-batch
  3. Backward pass to obtain gradients for all encoder parameters
  4. Compute importance scores s_i per attention head (Eq. 3); select top-k heads
  5. Apply binary mask to fc1 gradients (Eq. 5)
  6. Update only LoRA adapter parameters (A, B) and masked fc1 parameters
  7. Evaluate on target and control datasets to measure TI and CC accuracy

- **Design tradeoffs**:
  - **k (number of attention heads)**: Larger k increases capacity but risks more forgetting. Paper finds k=2 optimal (Table 24); k=16 (all heads) slightly improves TI but degrades CC on some datasets.
  - **LoRA rank r**: Higher rank (r=64 vs. r=8) improves TI but plateaus; r=128+ shows marginal degradation (Table 23).
  - **Sparsity for fc1**: 10% sparsity balances adaptation and stability; lower sparsity risks overfitting, higher may under-adapt.
  - **Loss function**: CLIP loss is 25× more compute-efficient (no LLM forward/backward). Perplexity loss (LoRSU-Ppl) yields higher TI on some VQA datasets but at greater computational cost.

- **Failure signatures**:
  - TI improves but CC drops sharply (e.g., < -5%): Overfitting to target domain; reduce k, sparsity, or training epochs.
  - TI near zero with stable CC: Gradient-based selection may be uninformative (insufficient data or already near-optimal encoder); consider alternative selection or increase shots.
  - Training instability (loss spikes): LoRA learning rate may be too high; reduce from 1e-5, check initialization.
  - ESAT-specific collapse (Table 1, F-FT): Full fine-tuning destroys satellite image knowledge; LoRSU's sparse updates prevent this—verify mask application if adapting to extreme domain shifts.

- **First 3 experiments**:
  1. **Reproduce Table 1 baseline**: Implement LoRSU with CLIP loss on GTS dataset (CL-5 setting). Report TI and CC; compare to LoRA and SPU baselines. Verify k=2, r=64, sparsity=10%.
  2. **Ablate attention head selection**: Run LoRSU-Rand (random k=2 heads) and LoRSU-AAH (all heads) on GTS. Compare TI/CC to validate importance scoring (Table 4).
  3. **Cross-domain stress test**: Fine-tune on ESAT (satellite images) and evaluate on DALLE (generated images) and GTS (traffic signs). Assess whether LoRSU maintains positive CC while improving TI, per Table 1 results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does LoRSU generalize effectively to other Vision-Language Model architectures and image encoders beyond the specific LLaVA and CLIP configurations tested?
- Basis in paper: [explicit] The authors state they focused on LLaVA and CLIP due to computational constraints but note that "our method is generic to any transformer model, and we plan to extend it to other VLMs and image encoders."
- Why unresolved: The empirical validation was restricted to a single VLM family (LLaVA-v1.5) and one image encoder (CLIP-L-14), leaving the method's utility in other architectures (e.g., BLIP-2, Qwen-VL) unproven.
- What evidence would resolve it: Benchmark results showing LoRSU's performance and efficiency when applied to VLMs with different connector modules (e.g., Q-Former) or alternative vision backbones (e.g., ViT-G, SigLIP).

### Open Question 2
- Question: Can the binary mask-based structured update mechanism scale efficiently to the parameter spaces of Large Language Models (LLMs)?
- Basis in paper: [explicit] The discussion notes that "LoRSU’s binary mask-based structured updates... scaling to larger architectures like LLMs poses challenges."
- Why unresolved: While effective for image encoders, binary masking creates memory and processing overheads that may become prohibitive when applied to the significantly larger parameter volumes of LLMs.
- What evidence would resolve it: Successful application of LoRSU to the LLM component of VLMs, demonstrating that the mask generation and application remain computationally feasible without sacrificing the parameter-efficient benefits.

### Open Question 3
- Question: Does utilizing a smaller LLM proxy model for perplexity-based updates (LoRSU-Ppl) provide a better trade-off between performance and computational cost?
- Basis in paper: [explicit] The authors identify "using a smaller LLM proxy model in perplexity-based methods" as a "promising direction" to improve scalability and resource usage.
- Why unresolved: The current LoRSU-Ppl method requires forward and backward passes through the full LLM to compute the perplexity loss, which creates a significant computational bottleneck compared to the CLIP-loss variant.
- What evidence would resolve it: Ablation studies comparing the Target Improvement accuracy and TFLOPs of LoRSU-Ppl when using a distilled or smaller proxy model versus the full LLM.

### Open Question 4
- Question: Is LoRSU effective at mitigating catastrophic forgetting in generative vision-language tasks, such as image captioning, as opposed to the discriminative VQA tasks tested?
- Basis in paper: [inferred] The experimental scope is limited entirely to Visual Question Answering (VQA) datasets (Section 4.1), with no evaluation on generative tasks where the output space is open-ended.
- Why unresolved: The method optimizes based on gradient magnitudes and CLIP/Perplexity losses; it is unclear if these signals are sufficient to preserve the syntactic and semantic coherence required for generative tasks without explicit replay.
- What evidence would resolve it: Evaluation on standard continual learning image captioning benchmarks (e.g., COCO Captions) to measure forgetting in terms of linguistic quality and semantic accuracy.

## Limitations

- The method's effectiveness relies heavily on the assumption that gradient magnitude correlates with parameter importance, which may break down in extremely few-shot settings (CL-5) or when the pre-trained encoder is already near-optimal for the target domain.
- The binary optimization formulation (Eq. 6) assumes the solution is achievable within the sparsity constraint, but no empirical validation of this assumption is provided.
- The knowledge neuron theory underpinning fc1 updates is cited but not directly validated for vision-language models, making the theoretical justification for this component weaker than for attention head selection.

## Confidence

- High confidence: Computational efficiency claims (25× savings vs. full updates) - directly measurable and well-specified
- Medium confidence: TI improvement claims - based on comprehensive experiments across ten datasets but dependent on the specific VQA adaptation protocol
- Medium confidence: Forgetting mitigation - supported by CC metrics but could be influenced by dataset ordering and class splits
- Low confidence: Theoretical justification via binary optimization - Lemma A.2 is stated but not empirically validated

## Next Checks

1. **Ablation study on gradient quality**: Run LoRSU with random importance scores (breaking the gradient-magnitude-to-importance assumption) and compare TI/CC to verify the selection mechanism matters
2. **Stress test on domain shift**: Apply LoRSU to an extreme cross-domain adaptation (e.g., natural images to medical imaging) and measure whether sparse updates maintain positive CC while improving TI
3. **Head selection consistency**: Verify that top-k head selection produces stable results across multiple training runs with different random seeds on the same dataset/session