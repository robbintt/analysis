---
ver: rpa2
title: Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations
arxiv_id: '2510.07064'
source_url: https://arxiv.org/abs/2510.07064
tags:
- agents
- ours
- human
- agent
- reppopmapped
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of constructing diverse sets
  of LLM agents to represent heterogeneous human populations. Instead of using a single
  agent, the authors propose a framework to select a set of agents whose behaviors
  collectively match a target human population.
---

# Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations

## Quick Facts
- arXiv ID: 2510.07064
- Source URL: https://arxiv.org/abs/2510.07064
- Reference count: 40
- Primary result: Constructs diverse LLM agent sets that better represent human populations than single agents or random baselines

## Executive Summary
This paper addresses the challenge of constructing diverse sets of LLM agents to represent heterogeneous human populations. Instead of using a single agent, the authors propose a framework to select a set of agents whose behaviors collectively match a target human population. Each agent is steered by a small set of human demonstrations via in-context learning, and the selection is cast as a submodular optimization problem. Experiments in educational and crowdsourcing domains show that the proposed methods construct more representative agent sets compared to baselines, and behavioral analyses confirm that the resulting agents reproduce human-like behaviors on new tasks.

## Method Summary
The method constructs a set of M LLM agents to represent a diverse human population by optimizing the representation gap. Each agent is an LLM conditioned on K human demonstrations (task-response pairs) through in-context learning. The framework offers three algorithms: REPPOP demo (greedy selection from all possible K-demonstration combinations), REPPOP mapped-1 (selection from proxy agents each using one human's demonstrations), and REPPOP mapped-2 (selection from proxy agents with K demonstrations selected greedily per human). The selection problem is formulated as maximizing a submodular objective, enabling greedy approximation guarantees. The approach is model-agnostic and validated on educational (EEDI) and crowdsourcing (OpinionQA) datasets.

## Key Results
- Multi-agent sets constructed by the proposed methods achieve lower representation error than single agents or random baselines across all tested domains
- Agents selected by the framework exhibit behaviors similar to the humans they represent on held-out test tasks, confirming behavioral capture
- REPPOP mapped-1 and mapped-2 methods provide tractable alternatives to REPPOP demo with approximation guarantees, reducing computational complexity from exponential to quadratic in population size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning agents on small sets of human demonstrations via in-context learning captures behavioral diversity that single LLMs cannot.
- Mechanism: Each agent receives K task-response pairs from humans in its prompt. The LLM's in-context learning steers its behavior toward the patterns in those demonstrations, creating specialized agents with distinct behavioral signatures.
- Core assumption: LLMs can reliably imitate specific behavioral patterns from limited demonstrations, and this imitation generalizes to unseen tasks.
- Evidence anchors:
  - [abstract]: "Each agent is an LLM whose behavior is steered by conditioning on a small set of human demonstrations (task-response pairs) through in-context learning."
  - [section 5.4]: Agents constructed by the method exhibit behaviors similar to the humans they represent on new test tasks (e.g., agents representing students with specific math misconceptions show those same misconceptions on held-out questions).
  - [corpus]: Neighbor papers confirm LLM agents can simulate human behavior via prompts, but this paper uniquely addresses diversity through multi-agent selection.
- Break condition: If in-context learning fails to generalize (e.g., agents produce homogeneous outputs despite varied demonstrations), the mechanism fails.

### Mechanism 2
- Claim: The representation gap objective is submodular, enabling greedy selection to provably approximate the optimal agent set.
- Mechanism: The objective f(L) measures average distance reduction between humans and their nearest agent. This exhibits diminishing returns: adding an agent helps less when many agents already exist. Greedy selection achieves (1-1/e) approximation of optimal.
- Core assumption: Distance between embedding vectors meaningfully captures behavioral similarity, and the embedding space is well-structured.
- Evidence anchors:
  - [section 3.2]: f(L) = g(∅) - g(L) is proven submodular (Proposition 1).
  - [table 1]: Greedy methods achieve performance guarantees with tractable time complexity.
  - [corpus]: Weak direct corpus evidence on submodular optimization for agent selection; this appears novel.
- Break condition: If embeddings fail to capture meaningful behavioral differences, or if the human population cannot be well-represented by any agent combination, optimization guarantees become meaningless.

### Mechanism 3
- Claim: Restricting the agent search space to human-mapped proxies preserves diversity while drastically reducing computational cost.
- Mechanism: Instead of searching all possible K-demonstration combinations (exponentially large), create one proxy agent per human using their own demonstrations. Select from this linear-sized pool via greedy optimization.
- Core assumption: Humans are sufficiently diverse that a subset of human-mapped agents can approximate the optimal agent set from the full space.
- Evidence anchors:
  - [section 4.2]: REPPOP mapped-1/2 methods reduce complexity to O(M·|H|²) while maintaining approximation guarantees (Theorem 2).
  - [figure 4]: Human-mapped agents cover different regions of behavioral embedding space.
  - [corpus]: No direct corpus precedent for this specific reduction technique.
- Break condition: If the optimal agent requires demonstrations from multiple different humans (not achievable via single-human mapping), this method may miss superior solutions.

## Foundational Learning

- Concept: **Submodular function maximization**
  - Why needed here: The paper formulates agent selection as maximizing a submodular function; understanding diminishing returns is essential to grasp why greedy selection works.
  - Quick check question: If adding agent A to set {B, C} improves coverage less than adding it to set {B}, what property does this demonstrate?

- Concept: **In-context learning with demonstrations**
  - Why needed here: The core mechanism relies on LLMs adapting behavior from prompt demonstrations without parameter updates.
  - Quick check question: What happens to agent behavior if you provide demonstrations from a high-performing student vs. a struggling student?

- Concept: **Facility location problems and covering**
  - Why needed here: The optimization problem is proven NP-hard via reduction to facility location; intuition transfers directly.
  - Quick check question: If selecting M facilities to serve N customers, where each customer goes to their nearest facility, how does this relate to selecting M agents to represent N humans?

## Architecture Onboarding

- Component map: Human demonstrations -> Behavioral embeddings -> Candidate agent pool -> Greedy selection -> Selected agent set
- Critical path:
  1. Extract behavioral embeddings from human demonstrations
  2. Generate candidate agent pool (full space, sampled, or human-mapped)
  3. Run greedy selection: at each iteration, add agent maximizing marginal gain f(L ∪ {l}) - f(L)
  4. Evaluate selected agent set on test tasks
- Design tradeoffs:
  - REPPOP demo: Higher cost O(M·K·|T|·|H|²), no formal guarantee, competitive performance
  - REPPOP mapped-1: Fast O(K·|H| + M·|H|²), guaranteed (1-1/e)(γ·f(L*) - ρ), lower-quality proxies
  - REPPOP mapped-2: Moderate cost O(K·|T|·|H| + M·|H|²), same guarantee, better proxies via greedy demonstration selection
  - SampleGreedy: Moderate cost, no coverage guarantee, requires sampling hyperparameter
- Failure signatures:
  - Representation error plateaus early: candidate pool lacks diversity, or embedding space is poorly structured
  - Agents exhibit homogeneous behavior despite varied demonstrations: LLM temperature too low, or demonstrations too similar
  - Runtime explodes with dataset size: using REPPOP demo on large populations instead of mapped variants
- First 3 experiments:
  1. **Sanity check**: Single agent (SINGLE) vs. random M agents (RANDOM) to confirm multi-agent benefit exists
  2. **Method comparison**: Run REPPOP mapped-1, mapped-2, SampleGreedy on EEDI dataset with M=5, K=3; compare test representation error
  3. **Behavioral validation**: For selected agents, visualize which humans they represent (UMAP embeddings) and verify agent behaviors match human group characteristics on held-out tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would fine-tuning-based agent construction compare to in-context learning approaches for representing diverse human populations?
- Basis in paper: [explicit] "we rely on prompting-based approaches rather than fine-tuning; future work could explore fine-tuning techniques for constructing an agent"
- Why unresolved: The current framework only uses in-context learning with demonstrations; fine-tuning may enable better behavioral capture but requires additional investigation into methods and trade-offs.
- What evidence would resolve it: Experiments comparing fine-tuned agents against prompting-based agents on the same representation gap metrics across multiple domains.

### Open Question 2
- Question: How does the ordering of demonstrations in prompts influence agent behavior and the quality of population representation?
- Basis in paper: [explicit] "we do not consider the ordering of demonstrations in prompts, and understanding how ordering influences model behavior would be a valuable extension"
- Why unresolved: The methods treat demonstrations as unordered sets, but LLMs are known to be sensitive to prompt structure and ordering.
- What evidence would resolve it: Ablation studies varying demonstration orderings within fixed demonstration sets, measuring resulting representation gaps and behavioral consistency.

### Open Question 3
- Question: How effective are the constructed representative agents for downstream applications such as teacher training, intervention assessment, or policy simulation?
- Basis in paper: [explicit] "this work primarily provides groundwork for more comprehensive evaluations of downstream applications, such as using the agents for training teachers, assessing the effectiveness of interventions, or simulating responses to government policy"
- Why unresolved: Current evaluation focuses on representation gap metrics and behavioral similarity on held-out tasks, not on practical utility in applied settings.
- What evidence would resolve it: End-to-end studies measuring downstream task performance (e.g., teacher training outcomes) when using representative agents versus real human subjects.

## Limitations

- The framework assumes behavioral diversity can be captured through demonstration-based in-context learning, which may fail if demonstrations are too sparse or the LLM's in-context learning is insufficient
- The embedding-based representation gap assumes behavioral differences are linear and can be captured by vector distances, which may not hold for complex human behaviors
- The claim that the method can represent "any" heterogeneous human population is overstated; some populations may have behavioral patterns too complex for demonstration-based steering

## Confidence

**High Confidence Claims:**
- The optimization problem is correctly formulated as submodular maximization, and greedy selection provides (1-1/e) approximation guarantees
- The computational complexity reductions through proxy agent pools are valid and mathematically sound
- The framework successfully constructs agent sets that outperform single-agent and random baselines in reducing representation error

**Medium Confidence Claims:**
- The behavioral similarity between agents and humans on held-out tasks indicates successful behavioral capture, though this relies on the quality of embeddings and demonstration selection
- The trade-offs between different selection methods (REPPOP demo, mapped-1, mapped-2) are accurately characterized in terms of performance and computational cost
- The model-agnostic nature allows the framework to work across different LLM families, though this requires empirical validation for each new model

**Low Confidence Claims:**
- The claim that the method can represent "any" heterogeneous human population is overstated; some populations may have behavioral patterns too complex for demonstration-based steering
- The generalization to real-world applications beyond educational and crowdsourcing domains remains untested
- The assertion that this is the first framework to construct diverse agent sets from demonstrations may not account for all prior work in multi-agent simulation

## Next Checks

1. **Behavioral generalization test**: Take agents selected from EEDI dataset and evaluate them on a completely new math domain (e.g., geometry problems if trained on algebra) to verify behavioral patterns transfer beyond the training task distribution.

2. **Ablation on demonstration quality**: Systematically vary the quality of demonstrations (e.g., using expert vs. novice responses) to test whether in-context learning reliably steers agent behavior toward demonstration patterns, and identify failure modes when demonstrations are noisy or contradictory.

3. **Scalability boundary**: Run the framework on progressively larger populations (100 → 1000 → 10000 humans) to empirically validate the claimed computational complexity and identify the point where proxy-based methods become necessary for tractability.