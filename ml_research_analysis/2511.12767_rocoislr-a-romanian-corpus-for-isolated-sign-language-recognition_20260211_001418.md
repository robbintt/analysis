---
ver: rpa2
title: 'RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition'
arxiv_id: '2511.12767'
source_url: https://arxiv.org/abs/2511.12767
tags:
- sign
- language
- recognition
- dataset
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RoCoISLR, the first large-scale, standardized
  dataset for Romanian Isolated Sign Language Recognition (RoISLR), comprising over
  9,000 video samples spanning nearly 6,000 standardized glosses. The dataset was
  created by merging and standardizing materials from three sources, with glosses
  unified through systematic cleaning to maintain semantic consistency.
---

# RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition

## Quick Facts
- arXiv ID: 2511.12767
- Source URL: https://arxiv.org/abs/2511.12767
- Reference count: 0
- Primary result: First large-scale Romanian ISLR dataset with 9,141 videos spanning 5,892 glosses; Swin Transformer achieves 34.1% top-1 accuracy

## Executive Summary
This paper introduces RoCoISLR, the first large-scale, standardized dataset for Romanian Isolated Sign Language Recognition (RoISLR), comprising over 9,000 video samples spanning nearly 6,000 standardized glosses. The dataset was created by merging and standardizing materials from three sources, with glosses unified through systematic cleaning to maintain semantic consistency. The authors evaluated seven state-of-the-art video recognition models—including I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D—under consistent experimental setups. The results show that transformer-based architectures, particularly Swin Transformer, achieved the highest top-1 accuracy of 34.1% on RoCoISLR, outperforming convolutional baselines. The study highlights the challenges posed by long-tail class distributions in low-resource sign languages and establishes a foundational benchmark for future research on RoISLR.

## Method Summary
The RoCoISLR corpus was constructed by merging three Romanian sign language datasets and standardizing 6,129 glosses down to 5,892 canonical glosses through variant merging, complexity filtering, and near-duplicate matching. Videos were preprocessed to 224×224 resolution, 25fps, and 64 frames via uniform sampling. Seven video recognition models (I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer V2, VideoMAE V2, PoseConv3D) were fine-tuned using MMAction2 framework with pre-trained weights from ImageNet/Kinetics-400/710. The classification task used 1,926 classes with ≥2 video instances, split n-1 train/1 test per class. Training ran up to 125 epochs with evaluation metrics of top-1 and top-5 accuracy.

## Key Results
- Swin Transformer achieved highest top-1 accuracy of 34.1% on RoCoISLR
- Transformer-based architectures outperformed convolutional baselines
- Severe class imbalance: 67% of glosses have only one video instance
- PoseConv3D (pose-based) underperformed RGB models with 25.7% top-1 accuracy
- Models showed overfitting to specific signers and recording setups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained video action recognition models transfer effectively to isolated sign language recognition through fine-tuning.
- Mechanism: Spatial-temporal features learned from large-scale action datasets (Kinetics-400, Kinetics-710, ImageNet) provide initialization points that reduce the data requirements for low-resource sign languages. The models inherit priors about motion patterns, object interactions, and temporal dynamics.
- Core assumption: The visual features in general human actions share sufficient overlap with sign language articulations (hand shapes, body movements, facial expressions) to enable positive transfer.
- Evidence anchors: All RGB models utilize weights from previous training on large-scale datasets; related ISLR papers confirm transfer approaches are standard.

### Mechanism 2
- Claim: Gloss standardization through systematic cleaning improves dataset consistency and reduces label noise.
- Mechanism: Three-stage pipeline—(i) variant merging unifies superficially different glosses with identical meaning; (ii) complexity filtering removes sentence-like constructions; (iii) near-duplicate matching (0.95 similarity threshold) reconciles annotation inconsistencies.
- Core assumption: Merged variants represent truly equivalent signs rather than dialectal or contextual variations that carry linguistic significance.
- Evidence anchors: Gloss count reduced from 6,129 to 5,892 through systematic cleaning; process claimed to ensure consistency without erasing lexical diversity.

### Mechanism 3
- Claim: Transformer-based architectures outperform convolutional baselines for isolated sign language recognition on low-resource datasets.
- Mechanism: Self-attention mechanisms enable global temporal modeling across frames, capturing long-range dependencies in sign articulations that local 3D convolutions may miss.
- Core assumption: Attention mechanism can learn meaningful sign-level patterns even with severe class imbalance and limited samples per class.
- Evidence anchors: Swin Transformer achieved highest score of 34.1%; comparative analysis papers show attention models outperform recurrent baselines on ISLR tasks.

## Foundational Learning

- Concept: Video Action Recognition Architectures (I3D, SlowFast, Swin Transformer, TimeSformer)
  - Why needed here: The paper benchmarks seven architectures; understanding their inductive biases (3D convolutions vs. attention, temporal sampling strategies) is essential for interpreting results and selecting models.
  - Quick check question: Can you explain why SlowFast uses two pathways with different temporal sampling rates, and what advantage this provides for sign recognition?

- Concept: Long-tail Distribution in Classification
  - Why needed here: RoCoISLR has severe imbalance—67% of glosses have only one video. This directly causes low accuracy and requires specific mitigation strategies.
  - Quick check question: What techniques (e.g., class-balanced sampling, focal loss, data augmentation) could address the sample-to-gloss ratio of 2.7?

- Concept: Transfer Learning and Domain Adaptation for Video
  - Why needed here: All models use pre-trained weights; understanding what transfers and what doesn't is critical for low-resource sign languages.
  - Quick check question: Why might features from Kinetics-400 (general actions) transfer less effectively to sign language than features from a sign-specific pre-training corpus?

## Architecture Onboarding

- Component map: Raw videos (3 sources) → Gloss standardization pipeline (variant merging, filtering, deduplication) → Video preprocessing (224×224, 25 fps, 64 frames, uniform sampling) → Pose extraction [MMPose, 133 keypoints] (optional, for PoseConv3D) → Model training [MMAction2 framework] → Evaluation (top-1, top-5 accuracy)

- Critical path: Gloss standardization is the gatekeeper. If this pipeline produces inconsistent labels, all downstream training is compromised. The 0.95 similarity threshold for near-duplicate matching is a key hyperparameter.

- Design tradeoffs:
  - Frame count (64): Preserves sign dynamics but increases memory/compute. Shorter clips lose temporal context; longer clips may include padding artifacts.
  - Classification head reset: Modifying output to 1,926 classes enables fine-tuning but discards pre-trained classifier knowledge.
  - Training epochs (125): Extended training may overfit on small classes; early stopping based on validation could help but was not reported.
  - Pose vs. RGB: PoseConv3D (25.7% top-1) underperformed Swin Transformer (34.1%), suggesting RGB appearance cues matter for Romanian signs.

- Failure signatures:
  - High training accuracy, low test accuracy: Indicates overfitting to signer appearance or recording setup.
  - Confident misclassifications with ~50% scores: Model is uncertain among overlapping classes; suggests feature space has poor class separation.
  - Random off-diagonal predictions in confusion matrix: No meaningful structure learned for many classes—expected when most classes have 1–2 samples.

- First 3 experiments:
  1. Reproduce the Swin Transformer baseline using the provided MMAction2 recipe on the 1,926-class split. Verify top-1 accuracy ≈34% and analyze per-class accuracy vs. samples-per-class correlation.
  2. Ablate the gloss standardization pipeline by training on raw (unmerged) glosses vs. canonical glosses. Quantify the impact of label noise on accuracy.
  3. Implement class-balanced sampling or focal loss to address the long-tail distribution. Compare against the baseline to determine if accuracy gains are concentrated in frequent classes or distributed across the tail.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can domain adaptation techniques effectively mitigate the severe train-test performance gap caused by models overfitting to specific signers and recording setups in RoCoISLR?
- **Basis in paper:** The authors state future work should focus on "developing stronger methods for domain adaptation," noting that degradation suggests overfitting to "visual appearances of specific signers or cameras/recorders."
- **Why unresolved:** The current benchmarks show a significant discrepancy between training performance and test accuracy (max 34.1%), indicating current state-of-the-art models fail to generalize well across the dataset's diverse sources.
- **What evidence would resolve it:** Demonstration of a model architecture or training regime that maintains high training accuracy while significantly increasing Top-1 accuracy on the held-out test set containing unseen signer variations.

### Open Question 2
- **Question:** To what extent does integrating explicit facial expression and body posture features improve recognition performance over standard RGB inputs in this low-resource context?
- **Basis in paper:** The authors suggest "investigating multimodal techniques that utilize body posture and facial expressions."
- **Why unresolved:** The pose-based model evaluated (PoseConv3D) significantly underperformed (25.7% Top-1) compared to the RGB-based Swin Transformer (34.1%), leaving the potential value of multimodal fusion untapped.
- **What evidence would resolve it:** A multimodal model combining RGB with high-fidelity face/pose keypoints outperforming the single-modality Swin Transformer baseline on the RoCoISLR test set.

### Open Question 3
- **Question:** How can Large Language Models (LLMs) be effectively applied to bridge the gap between isolated gloss recognition and coherent text translation for Romanian Sign Language?
- **Basis in paper:** The authors identify the "application of LLMs to complement gloss-to-text translation" as a specific direction for future interest.
- **Why unresolved:** The current study is limited to the classification of isolated signs (glosses) and does not address the semantic translation of these signs into natural Romanian sentences.
- **What evidence would resolve it:** A successful downstream task where recognized glosses from RoCoISLR are processed by an LLM to produce grammatically correct and contextually accurate Romanian text.

## Limitations
- Gloss standardization pipeline lacks validation for semantic equivalence versus dialectal variation preservation
- Severe class imbalance (67% of glosses have only one video) creates fundamental generalization limitations
- Overfitting to specific signers and recording setups remains unaddressed beyond acknowledging the problem

## Confidence
- High confidence: Dataset construction methodology (merging three sources, preprocessing pipeline) is clearly specified and reproducible
- Medium confidence: Claim that transformer-based architectures outperform convolutional baselines, based on single benchmark comparison
- Low confidence: Assertion that gloss standardization "ensures consistency without erasing natural lexical diversity" - requires linguistic validation

## Next Checks
1. Validate gloss standardization quality: Manually review 50 randomly selected merged glosses to determine if standardization preserved semantic distinctions versus collapsing distinct signs
2. Evaluate transfer learning impact: Train I3D and Swin Transformer from scratch on RoCoISLR and compare to pre-trained versions
3. Test long-tail mitigation strategies: Implement class-balanced sampling or focal loss and measure accuracy improvements across different frequency bins (high-frequency vs. tail classes)