---
ver: rpa2
title: 'Autotelic Reinforcement Learning: Exploring Intrinsic Motivations for Skill
  Acquisition in Open-Ended Environments'
arxiv_id: '2502.04418'
source_url: https://arxiv.org/abs/2502.04418
tags:
- agents
- learning
- where
- builder
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Autotelic Reinforcement Learning (ARL), a
  framework for autonomous skill acquisition through intrinsic motivation in open-ended
  environments. The core method involves agents autonomously generating and pursuing
  self-defined goals using a reward-free Markov Decision Process (MDP), guided by
  intrinsic motivations.
---

# Autotelic Reinforcement Learning: Exploring Intrinsic Motivations for Skill Acquisition in Open-Ended Environments

## Quick Facts
- arXiv ID: 2502.04418
- Source URL: https://arxiv.org/abs/2502.04418
- Authors: Prakhar Srivastava; Jasmeet Singh
- Reference count: 23
- Primary result: Framework enabling agents to autonomously generate and pursue self-defined goals using intrinsic motivation in reward-free environments, achieving up to 99% success rates.

## Executive Summary
Autotelic Reinforcement Learning (ARL) introduces a framework for autonomous skill acquisition through intrinsic motivation in open-ended environments. The approach enables agents to self-generate goals and learn goal-conditioned policies without external rewards, using knowledge-based and competence-based intrinsic motivations. The framework demonstrates robust performance across various tasks, with agents showing high success rates and adaptability to dynamic environments. ARL advances autonomous learning by mimicking human-like creativity and adaptability, allowing agents to self-organize goals and skills in complex, reward-free settings.

## Method Summary
The paper presents two main contributions: (1) a Graphical Referential Game (GREG) for studying emergent communication via sensory-motor graphical signals, and (2) an Architect-Builder Problem (ABP) where an architect with goal knowledge guides a reward-free builder through signals without predefined meanings. The approach uses contrastive learning with cosine similarity energy to align referent and expression representations across agents. ABIG alternates between modeling frames (architect learns builder behavior via behavioral cloning) and guiding frames (architect uses MCTS to generate optimal messages; builder self-imitates guided trajectories). The framework is evaluated in BuildWorld environments with various tasks and generalization tests.

## Key Results
- Agents achieved near-perfect training success rates (99%) across all game configurations in the Graphical Referential Game
- Robust performance under perturbation tests, maintaining effectiveness with 10-30% message corruption
- Successful generalization to compositional referents and unseen tasks, demonstrating the framework's ability to handle complex, open-ended scenarios

## Why This Works (Mechanism)

### Mechanism 1
Intrinsic motivations drive autonomous goal generation and skill acquisition in reward-free environments. Agents operate in a reward-free MDP where they self-generate goals guided by knowledge-based (curiosity/exploration) and competence-based (skill mastery) intrinsic signals. Goals are represented as constraints over states, and agents learn goal-conditioned policies π(a|s,g) without external reward functions.

### Mechanism 2
Contrastive learning enables emergent communication protocols between agents without predefined signal meanings. In the Graphical Referential Game (GREG), two agents learn a shared latent space where referents and expressions map via encoders f(r) and g(u). Energy is computed as cosine similarity E(r,u) = cos(f(r), g(u)). Agents maximize energy for positive pairs and minimize for negatives, converging on coherent communication.

### Mechanism 3
Structured interaction frames enable asymmetric agent collaboration where only one agent observes rewards. In the Architect-Builder Problem, the architect knows the goal but cannot act; the builder can act but receives no reward. ABIG alternates between modeling frames (architect learns builder behavior via BC) and guiding frames (architect uses MCTS with learned model to select messages; builder self-imitates guided trajectories).

## Foundational Learning

- Concept: **Markov Decision Processes (MDPs)**
  - Why needed here: ARL formalizes learning in reward-free MDPs; understanding state/action spaces, transitions, and the Bellman equations is essential.
  - Quick check question: Can you write the Bellman expectation equation for V^π(s)?

- Concept: **Goal-Conditioned Policies**
  - Why needed here: ARL agents learn π(a|s,g) that varies behavior based on self-generated goals; this differs from standard single-task RL.
  - Quick check question: How does a goal-conditioned policy differ from a standard policy?

- Concept: **Contrastive Learning**
  - Why needed here: CURVES uses energy-based contrastive learning to align referent and expression representations across agents.
  - Quick check question: What does maximizing cosine similarity between positive pairs achieve in representation learning?

## Architecture Onboarding

- Component map: Goal Generator -> Goal-Conditioned Policy Network -> Value Function -> Intrinsic Reward Module -> Communication Module (if multi-agent) -> Self-Imitation Buffer

- Critical path:
  1. Implement reward-free MDP environment (e.g., BuildWorld)
  2. Build goal representation module (one-hot, state features, or language-based)
  3. Train goal-conditioned policy with intrinsic rewards
  4. Add communication protocol (if multi-agent) via contrastive learning
  5. Evaluate using exploration diversity, generalization to unseen goals, and robustness to perturbations

- Design tradeoffs:
  - **Goal space granularity**: Fine-grained goals improve specificity but increase exploration burden
  - **Message vocabulary size**: Larger |V| (e.g., 72 vs. 2) improves expressivity but slows convergence
  - **Frame-based vs. continuous learning**: Frames provide stationarity but reduce data efficiency

- Failure signatures:
  - Training success high but test/generalization low → overfitting to training goals; need goal diversity
  - Communication success drops with perspective shifts → insufficient invariance in encoders
  - Builder never improves → architect not providing informative signals; check MCTS guidance quality

- First 3 experiments:
  1. **Single-agent goal generation**: Train agent in simple environment (e.g., object manipulation) with only intrinsic rewards; measure goal diversity and achievement rate over time.
  2. **Referential game baseline**: Implement CURVES in one-hot setting; verify 99% training success and compositional generalization.
  3. **Architect-Builder ablation**: Run ABIG with random messages (ABIG-no-intent) vs. full guidance; quantify gap in task success to validate self-imitation contribution.

## Open Questions the Paper Calls Out

### Open Question 1
Can a Vygotskian approach enable the builder agent to internalize the architect's guidance for autonomous execution? The current ABIG algorithm requires continuous interaction; the builder cannot operate independently once the architect is removed.

### Open Question 2
Does training with dynamic, non-stationary data buffers improve the data efficiency of the ABIG algorithm? The current implementation relies on structured interaction frames where one agent's policy is fixed while the other learns, which requires many episodes.

### Open Question 3
How do sensory-motor constraints influence the topology and compositionality of emergent graphical languages? While the CURVES algorithm generates coherent signals, experiments showed a lack of clear compositional structures in the generated articulations despite high success rates.

## Limitations
- The builder remains dependent on the architect's guidance, unable to operate autonomously without continuous interaction
- The fixed interaction framework is data-inefficient, requiring many episodes due to stationarity assumptions
- Emergent graphical signals show high success rates but lack clear compositional structures in some experimental settings

## Confidence

**High confidence**: The fundamental concept of autotelic learning in reward-free MDPs, supported by clear theoretical grounding and the established MAGELLAN framework.

**Medium confidence**: The Architect-Builder problem formulation and its ability to enable asymmetric collaboration, though specific performance metrics and generalization capabilities require further validation.

**Medium confidence**: The CURVES contrastive learning approach for emergent communication, with strong results in controlled settings but potential limitations in more complex or dynamic environments.

## Next Checks

1. Implement a systematic ablation study testing different message vocabulary sizes (|V| = 2 to 72) to quantify the trade-off between expressivity and convergence speed in the communication protocol.

2. Evaluate the framework's robustness to communication noise by introducing message corruption rates (e.g., 10-30% random substitutions) and measuring degradation in task success rates.

3. Test generalization capabilities by training agents on a subset of possible goals (e.g., 50%) and measuring performance on held-out goals to assess whether intrinsic motivation truly enables broad skill transfer rather than overfitting to specific goal distributions.