---
ver: rpa2
title: Trajectory Encoding Temporal Graph Networks
arxiv_id: '2504.11386'
source_url: https://arxiv.org/abs/2504.11386
tags:
- temporal
- node
- trajectory
- graph
- tetgn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of temporal graph representation
  learning in dynamic networks, particularly the trade-off between transductive and
  inductive settings in Temporal Graph Networks (TGNs). The proposed Trajectory Encoding
  TGN (TETGN) introduces automatically expandable learnable node IDs as temporal positional
  features combined with a trajectory encoding module that captures historical node
  relationships through message passing.
---

# Trajectory Encoding Temporal Graph Networks

## Quick Facts
- arXiv ID: 2504.11386
- Source URL: https://arxiv.org/abs/2504.11386
- Reference count: 39
- Achieves up to 2.4% improvement on non-attributed LastFM dataset for temporal graph representation learning

## Executive Summary
This paper introduces Trajectory Encoding TGN (TETGN), a Temporal Graph Network architecture that addresses the fundamental trade-off between transductive and inductive settings in temporal graph representation learning. The proposed method introduces automatically expandable learnable node IDs as temporal positional features and a trajectory encoding module that captures historical node relationships through message passing. TETGN extends the MP-TGN architecture by integrating a parallel trajectory message-passing stream alongside the standard TGN encoder, with outputs fused through multi-head attention.

## Method Summary
The core innovation of TETGN lies in its dual-stream architecture that combines standard TGN message passing with trajectory-aware message passing. The model introduces automatically expandable learnable node IDs that serve as temporal positional features, allowing the network to encode temporal information without relying on explicit time features. A trajectory encoding module captures historical relationships between nodes through a parallel message-passing stream, which is then fused with the standard TGN encoder output using multi-head attention. This design enables the model to effectively handle both symmetric and cyclic structures in temporal networks while maintaining strong performance in both transductive and inductive settings.

## Key Results
- Achieves up to 2.4% improvement on non-attributed LastFM dataset compared to baseline TGNs
- Consistently outperforms strong baselines on three real-world datasets (Wikipedia, Reddit, LastFM)
- Demonstrates robustness across both link prediction and node classification tasks
- Shows effective handling of symmetric and cyclic structures in temporal networks

## Why This Works (Mechanism)
The effectiveness of TETGN stems from its ability to capture long-range temporal dependencies through trajectory encoding while maintaining computational efficiency through the dual-stream architecture. The automatically expandable learnable node IDs provide a flexible way to encode temporal positional information without requiring fixed-dimensional time embeddings. The trajectory message-passing stream allows the model to aggregate information from historical node interactions, which is particularly important for capturing cyclic patterns and long-term dependencies that standard TGNs might miss.

## Foundational Learning
- Temporal Graph Networks (TGNs): Why needed - to model dynamic graphs with evolving structures over time; Quick check - understand message passing and memory modules
- Positional Encoding: Why needed - to inject temporal information into node representations; Quick check - compare different encoding schemes (temporal, relative, learnable)
- Multi-head Attention: Why needed - to fuse information from parallel streams effectively; Quick check - understand attention mechanisms and their role in feature fusion
- Transductive vs Inductive Learning: Why needed - to understand model generalization capabilities; Quick check - identify when each setting is appropriate
- Message Passing in Graphs: Why needed - fundamental operation for information propagation; Quick check - understand how information flows through graph structures

## Architecture Onboarding
- Component Map: Input Data -> Node/Edge Features -> Standard TGN Encoder -> Trajectory Encoder -> Multi-head Attention Fusion -> Output Predictions
- Critical Path: The trajectory encoding stream is the key innovation, running parallel to the standard TGN encoder and being fused through attention mechanisms
- Design Tradeoffs: The automatically expandable node IDs provide flexibility but increase parameter count; the dual-stream architecture improves performance but adds computational overhead
- Failure Signatures: Poor performance on datasets with limited temporal dynamics, failure to capture long-range dependencies, scalability issues with very large node populations
- First Experiments: 1) Test on a simple synthetic temporal graph with known cyclic patterns, 2) Compare performance with and without trajectory encoding on a small real dataset, 3) Validate automatic ID expansion on a dataset with rapidly growing node population

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Scalability concerns with automatically expandable learnable node IDs as parameter count grows with unique node count
- Additional computational overhead from trajectory encoding module not fully characterized
- Limited exploration of alternative positional encoding schemes in ablation studies
- Based on limited dataset diversity, primarily focusing on social network data

## Confidence
- Experimental performance claims (High): The 2.4% improvement on LastFM is well-documented through standard metrics, though results on other datasets show more modest gains
- Theoretical contributions (Medium): The framework for automatic ID expansion is novel but lacks rigorous complexity analysis
- Generalization claims (Medium): Results across transductive and inductive settings are promising but based on limited dataset diversity

## Next Checks
1. Conduct scalability analysis measuring memory and runtime as node count increases, particularly for the automatically expandable ID mechanism
2. Test the model on additional temporal graph datasets with different characteristics (e.g., social networks, communication networks, biological networks) to validate robustness claims
3. Perform ablation studies comparing different positional encoding methods (temporal encoding, relative positional encoding) to isolate the contribution of the trajectory encoding component