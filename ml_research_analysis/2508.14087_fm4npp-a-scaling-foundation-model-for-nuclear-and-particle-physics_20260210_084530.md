---
ver: rpa2
title: 'FM4NPP: A Scaling Foundation Model for Nuclear and Particle Physics'
arxiv_id: '2508.14087'
source_url: https://arxiv.org/abs/2508.14087
tags:
- track
- particle
- data
- sphenix
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops FM4NPP, a scalable foundation model for experimental
  nuclear and particle physics, addressing the challenge of applying self-supervised
  learning to sparse, high-dimensional detector data. A novel self-supervised pretraining
  objective is proposed, predicting k-nearest neighbors in the next radial neighborhood
  to avoid learning serialization artifacts, and a physics-informed hierarchical raster
  scan serialization method is introduced to convert unordered 3D spacepoints into
  sequential input.
---

# FM4NPP: A Scaling Foundation Model for Nuclear and Particle Physics

## Quick Facts
- arXiv ID: 2508.14087
- Source URL: https://arxiv.org/abs/2508.14087
- Reference count: 31
- Key outcome: FM4NPP demonstrates strong scaling behavior with up to 188M parameters, outperforming task-specific baselines across three downstream tasks using lightweight adapters

## Executive Summary
FM4NPP addresses the challenge of applying self-supervised learning to sparse, high-dimensional detector data in nuclear and particle physics. The model introduces a novel self-supervised pretraining objective that predicts k-nearest neighbors in the next radial neighborhood, avoiding serialization artifacts. A physics-informed hierarchical raster scan serialization method converts unordered 3D spacepoints into sequential input. When paired with lightweight adapters, FM4NPP consistently outperforms task-specific baselines across track finding, particle identification, and noise tagging tasks, demonstrating that task-agnostic representations can be specialized via a single linear transformation.

## Method Summary
The method employs a Mamba2 backbone with hierarchical raster scan serialization to convert unordered 3D detector hits into a 1D sequence. The self-supervised pretraining objective predicts the next k neighbors in radius (k=10) using MSE loss, with event-difficulty rescaling to stabilize training. The model is trained on simulated p+p collision data (11M+ events) and adapted to downstream tasks using lightweight adapters: a Mask2Former-style decoder for tracking and linear projections for classification tasks. The approach leverages physics-informed coordinate transformations and positional encoding to capture detector geometry.

## Key Results
- FM4NPP achieves ARI 0.9448, efficiency 96.08%, purity 93.08% on track finding
- Particle identification accuracy reaches 0.9039 with recall 0.7652 and precision 0.8782
- Noise tagging accuracy achieves 0.9713 with recall 0.9367 and precision 0.9190
- Performance plateaus at the largest model size (m6), suggesting a possible saturation point
- Representations are task-agnostic but can be specialized via a single linear mapping

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Raster Scan Serialization
The system transforms coordinates to cylindrical (r, φ, η) and partitions space into boxes ordered globally (r-major), while sorting points locally within boxes. This creates a sequence that follows particle trajectories outward from the collision point, reducing context gaps for sequence models compared to space-filling curves. The core assumption is that particle trajectories primarily propagate outward in radius, and local continuity along the r-axis correlates with track identity.

### Mechanism 2: Physics-Aligned Self-Supervision (k-Next-Nearest-Neighbor)
Instead of masking tokens, the model predicts coordinates of the k nearest neighbors with larger radii (r_j > r_i), enforcing causality consistent with particle physics (outward flight). An event-difficulty rescaling stabilizes training by re-weighting dense events. The geometric relationship between a point and its outward neighbors serves as a universal prior useful for track finding, PID, and noise tagging.

### Mechanism 3: Linear Decoupling of Representations
The pre-training task encodes local geometry, and PCA visualizations show raw embeddings are indistinguishable by track ID, but a learned linear projection yields distinct clusters. This suggests the FM captures the "shape" of data, requiring only a lightweight adaptor to map it to specific outputs. The assumption is that downstream tasks are linearly separable given a sufficiently rich geometric encoding of the spacepoints.

## Foundational Learning

**State Space Models (SSMs) / Mamba**
- Why needed: Particle collisions generate thousands of spacepoints (long sequences). Transformers suffer from quadratic complexity O(N²). Mamba offers linear O(N) scaling, essential for high-granularity TPC data.
- Quick check: Can you explain why Mamba's "selection mechanism" helps filter out detector noise compared to a standard RNN?

**Self-Supervised Learning (SSL) on Scientific Data**
- Why needed: Labeled physics data is expensive; simulated unlabeled data is plentiful. SSL bridges the gap by learning physics priors (geometry) from raw data.
- Quick check: How does the "k-Next-Nearest-Neighbor" objective differ from "Masked Autoencoding" used in vision?

**Coordinate Transformations (Cartesian to Cylindrical)**
- Why needed: Colliders are cylindrical. Sorting by Cartesian z is less physically meaningful than pseudorapidity η and radius r for particle propagation.
- Quick check: Why does sorting by radius r approximate the time-ordered flight path of a particle?

## Architecture Onboarding

**Component map:** (Energy, x, y, z) → Preprocess: Cylindrical (E, r, φ, η) + NeRF-style positional encoding → Serialized Sequence → Stacked Mamba Blocks (frozen after pretraining) → Adaptor: Linear Projection → Task Head (Transformer Decoder for tracking, MLP for PID)

**Critical path:** The Hierarchical Raster Scan is the non-differentiable component that determines model performance. If the serialization order is wrong, the sequence model sees random noise.

**Design tradeoffs:** Mamba vs. Transformer: Mamba chosen for training speed/memory on long sequences (>10³ points). Trade-off is potential loss of global attention mechanisms, mitigated by the serialization strategy. Frozen vs. Fine-tuned: Paper freezes FM weights to prove representation quality. Production deployments might fine-tune for marginal gains.

**Failure signatures:** Loss Spikes: Caused by high-variance event density. Mitigation: Check implementation of "event-difficulty rescaling" (Appendix). Fragmented Tracks: If serialization parameters (grid size 6×8×8) do not match detector geometry, tracks may break at box boundaries.

**First 3 experiments:**
1. Baseline Reproduction: Train the smallest model (m1) on 1% data to validate the serialization pipeline and loss function.
2. Scaling Verification: Plot validation MSE vs. FLOPs for models m1–m3 to confirm power-law scaling before committing to m6 (188M params).
3. Ablation on Serialization: Compare "Hierarchical Raster" vs. "Hilbert Curve" serialization on the Track Finding task to quantify the inductive bias value.

## Open Questions the Paper Calls Out
- Can the FM paradigm, trained on proton-proton collisions, generalize effectively to the higher multiplicity environment of heavy-ion collisions or unify across different collider experiments?
- Does the observed performance plateau at the largest model size (m6) indicate a limitation of the dataset size or the model architecture?
- Can the foundation model framework be extended to heterogeneous detector subsystems, such as calorimeters or silicon trackers?
- Does supervised fine-tuning jointly across multiple downstream tasks improve performance compared to the frozen-adapter approach?

## Limitations
- The hierarchical raster scan serialization relies on the assumption that particles propagate primarily outward in radius, which may not hold for low-momentum particles that curl or loop.
- The model's effectiveness on different collision systems (e.g., heavy-ion collisions) or different energy scales remains untested.
- The claim that FM embeddings are universally task-agnostic requires additional validation across a broader range of applications.

## Confidence
- High Confidence: The scaling behavior of FM4NPP with respect to both model size and dataset size is well-established through systematic experiments.
- Medium Confidence: The effectiveness of the k-Next-Nearest-Neighbor pretraining objective is supported by strong downstream performance across three diverse tasks.
- Medium Confidence: The physics-informed hierarchical raster scan serialization method shows clear advantages over naive space-filling curves.
- Low Confidence: The claim that FM embeddings are universally task-agnostic requires additional validation beyond the current experiments.

## Next Checks
1. Systematically vary the hierarchical raster scan parameters (grid sizes in r, η, φ) and measure downstream task performance to identify optimal configurations and assess sensitivity to serialization choices.
2. Evaluate FM4NPP performance on data from different collision systems (e.g., heavy-ion collisions) and different energy scales to assess the generalizability of the physics priors learned during pretraining.
3. Apply the pretrained model to real sPHENIX detector data and compare performance with simulation-based results to identify systematic differences between simulation and reality.