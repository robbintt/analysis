---
ver: rpa2
title: 'MultiLexNorm++: A Unified Benchmark and a Generative Model for Lexical Normalization
  for Asian Languages'
arxiv_id: '2601.16623'
source_url: https://arxiv.org/abs/2601.16623
tags:
- normalization
- language
- computational
- association
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends a multilingual lexical normalization benchmark
  (MultiLexNorm) to five Asian languages (Indonesian, Japanese, Korean, Thai, Vietnamese)
  covering different scripts and language families. The authors find that the current
  state-of-the-art normalization model (UFAL) performs poorly on these languages,
  particularly those with non-Latin scripts.
---

# MultiLexNorm++: A Unified Benchmark and a Generative Model for Lexical Normalization for Asian Languages

## Quick Facts
- **arXiv ID**: 2601.16623
- **Source URL**: https://arxiv.org/abs/2601.16623
- **Reference count**: 40
- **Primary result**: LLM-based pipeline with detection, lookup, and GPT-4o generation achieves 53.53% average ERR on 5 Asian languages, significantly outperforming byte-level models like UFAL on non-Latin scripts.

## Executive Summary
This paper extends the MultiLexNorm benchmark to five Asian languages (Indonesian, Japanese, Korean, Thai, Vietnamese) and proposes a new pipeline approach for lexical normalization. The authors find that existing byte-level normalization models (UFAL/ByT5) perform poorly on non-Latin scripts due to inefficient UTF-8 byte representations. Their solution combines a detection model, a lookup table, and in-context learning with large language models, achieving significantly better performance than previous approaches. The study reveals that normalization detection, spelling errors, and slang remain key challenges, with open-source LLMs still underperforming compared to closed-source models like GPT-4o.

## Method Summary
The authors propose a three-stage pipeline for lexical normalization: (1) Detection using XLM-R-base sequence labeling to identify tokens needing normalization, (2) Lookup using a statistical table constructed from training data with Miller-Madow entropy filtering for high-frequency slang, and (3) Generative LLM (GPT-4o or Llama) with 8-shot in-context learning prompts to perform the actual normalization. The pipeline is trained and evaluated on the MultiLexNorm++ benchmark covering five Asian languages with varying scripts and language families. The method is designed to overcome the segmentation bottleneck of byte-level models on non-Latin scripts by leveraging semantic representations from standard encoders and LLMs.

## Key Results
- GPT-4o achieves the highest average Error Reduction Rate (ERR) of 53.53% across all five Asian languages
- The detection model reduces token count by 95.25% and sets an upper bound of 85.59% ERR for the pipeline
- Adding the lookup component improves ERR by 4-14 points compared to LLM-only approaches
- UFAL (ByT5) baseline performs poorly on non-Latin scripts due to low characters-per-subword ratios (e.g., 0.34 for Japanese/Korean)
- "Wrong normalization" is the largest error category, accounting for most model failures across languages

## Why This Works (Mechanism)

### Mechanism 1
The pipeline architecture overcomes the representation bottleneck of byte-level models (like the previous SOTA UFAL) when processing non-Latin scripts. The authors identify that UFAL (based on ByT5) suffers in Asian languages because UTF-8 byte representations for scripts like Korean and Thai result in inefficient subword segmentation (fewer characters per subword). By switching to a pipeline that uses a standard encoder for detection and an LLM with a robust tokenizer for generation, the system leverages semantic representations rather than getting lost in inefficient byte-sequence mapping.

### Mechanism 2
Decoupling the "detection" of informal words from their "normalization" improves robustness and reduces over-normalization errors. Standard LLMs often struggle to distinguish between words that are intentionally informal (slang) and words that should remain unchanged (proper nouns, standard vocabulary). By training a dedicated XLM-R encoder to first identify which tokens need normalization, the system provides a constrained input to the generative LLM, preventing it from "correcting" standard text.

### Mechanism 3
The inclusion of a statistical lookup table provides a "fast path" for high-frequency slang that stabilizes performance before the LLM is invoked. Social media text often contains repetitive, short-form slang (e.g., "smh"). The lookup table handles these deterministic transformations efficiently, reserving the expensive and stochastic LLM for context-dependent or novel errors. The use of Miller-Madow entropy helps filter ambiguous candidates.

## Foundational Learning

- **Lexical Normalization vs. Spell Checking**: The paper defines normalization as transforming informal/social text to a standard variant, which differs from correcting typos. It includes 1-to-many or many-to-1 replacements (e.g., "u" -> "you", "gonna" -> "going to"). Understanding this distinction is crucial for interpreting the Error Reduction Rate (ERR) metric.
  - Quick check: If a model changes "cool" to "cold", is this a failure of normalization or a spelling error? (Answer: It is a semantic normalization error/wrong normalization)

- **UTF-8 and Byte-Level Representations**: The poor performance of the ByT5 baseline (UFAL) is explicitly attributed to how non-Latin scripts (Korean, Thai, etc.) map to bytes. Unlike Latin characters (often 1 byte), Asian characters often require 3 bytes. This "dilutes" the information density in a byte-level model, leading to the segmentation issues highlighted in Section 6.3.
  - Quick check: Why does a byte-level model like ByT5 struggle more with Thai than with English? (Answer: Thai characters take more bytes, leading to longer sequences and fewer characters per subword)

- **Error Reduction Rate (ERR)**: The authors use ERR as the primary metric rather than Accuracy or F1. ERR measures the percentage of the "task" solved relative to the baseline noise. It can be negative (making things worse) or approach 100% (perfect normalization). This provides a more nuanced view of utility than raw accuracy, which can be inflated by the large number of tokens that *don't* need changing.
  - Quick check: If a model fixes 5 errors but introduces 5 new errors in a sentence, what is the likely impact on the ERR? (Answer: The ERR would likely be close to 0 or negative, as the net improvement is minimal)

## Architecture Onboarding

- **Component map**: Input -> Detection (XLM-R encoder) -> Resolver (Lookup table + Generative LLM with 8-shot ICL) -> Output
- **Critical path**: The **Detection module** is the gatekeeper. If the XLM-R model fails to tag a word as needing normalization, the pipeline skips it entirely (Under-normalization). Section 6.4 notes the detection model sets an upper bound of 85.59 ERR.
- **Design tradeoffs**:
  - **Closed vs. Open Source**: The paper shows GPT-4o significantly outperforms Llama/Qwen on non-Latin scripts (Section 5). Choosing an open-source model saves cost/API dependency but sacrifices robustness on slang and spelling nuance (Section 6.1).
  - **Prompt Length**: 8-shot prompts were chosen as the tradeoff between context and performance (Section D), but this increases token usage/cost compared to zero-shot.
- **Failure signatures**:
  - **Script Inefficiency**: If using UFAL (ByT5), expect failures in Korean/Thai due to low chars/subword ratio (Section 6.3).
  - **Under-normalization**: With open-source LLMs (Llama/Qwen), expect high rates of the model refusing to generate or missing the instruction when informal text is confusing (Section 6.1).
  - **Wrong Normalization**: The largest error category (Figure 3). Models select a valid standard word that is contextually incorrect.
- **First 3 experiments**:
  1. **Detection Validation**: Run the XLM-R detector on a held-out sample to verify Precision/Recall against the "Gold" normalization tags. If detection F1 is low (<80%), the pipeline will fail before reaching the LLM.
  2. **Lookup Ablation**: Run the pipeline with Lookup ON vs. Lookup OFF (LLM only). Compare ERR to quantify how much "easy slang" the lookup captures vs. the LLM.
  3. **Script Segmentation Audit**: For the target language (e.g., Thai), inspect the tokenizer output of the chosen LLM vs. ByT5 to confirm the subword/char ratios align with the paper's findings (Table 3) before full training.

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating language-specific sub-character representations (such as stroke sequences) into LLMs significantly improve normalization performance for non-Latin scripts? The authors explicitly state in Section 7 that "future work should consider incorporating language-specific techniques, such as stroke-level or sub-character-based models," noting that current segmentation methods create bottlenecks for languages like Korean and Thai.

### Open Question 2
Do sentence-level normalization approaches yield better downstream task performance than the current word-level pipeline for Asian languages? The authors note in Section 7 that "normalization can also operate at multiple levels... future work could explore sentence-level approaches, as well as evaluation metrics that link normalization quality directly to downstream performance."

### Open Question 3
How can the disambiguation of multiple valid normalization candidates be improved to reduce the high rate of "wrong normalization" errors? Figure 3 and Section 6.1 show that "normalizing to the wrong normalization candidate is the largest error category" for most models. The authors also identify context-dependent normalizations (such as politeness markers) as a specific manual error source.

## Limitations
- **Data Coverage**: The dataset sizes vary considerably across languages (Japanese: 3,786 pairs, Korean: 2,400 pairs), potentially limiting generalizability to truly out-of-domain social media text.
- **LLM Dependency**: The significant performance gap between closed-source (GPT-4o) and open-source models raises concerns about practical applicability for researchers without API access.
- **Prompt Sensitivity**: The lack of systematic prompt ablation studies means reported performance might be sensitive to undocumented prompt engineering choices.

## Confidence

**High Confidence**: The finding that UFAL (ByT5) performs poorly on non-Latin scripts due to byte-level representation inefficiencies is well-supported by quantitative evidence (Table 3 showing chars/subword ratios). The detection model's contribution (reducing token count by 95.25%) is clearly demonstrated and reproducible.

**Medium Confidence**: The claim that the pipeline architecture is superior to UFAL is supported, but the magnitude of improvement should be interpreted cautiously due to different evaluation protocols and potential dataset-specific factors. The specific contribution of each pipeline component is reasonably well-established through ablation studies.

**Low Confidence**: The superiority of GPT-4o over open-source models for non-Latin scripts, while demonstrated, may be influenced by undocumented prompt engineering and the specific choice of 8-shot examples. The paper doesn't fully explore whether this gap could be closed with better prompt optimization for open-source models.

## Next Checks

1. **Cross-Dataset Generalization Test**: Evaluate the trained pipeline on a held-out test set from a different time period or social media platform than the training data to assess robustness to evolving slang patterns. This would validate whether the model generalizes beyond the specific datasets used.

2. **Open-Source Prompt Optimization Study**: Systematically test different prompt formats, shot counts, and instruction phrasings for Llama-3.3 and Qwen-2.5 to determine if the performance gap with GPT-4o can be reduced through prompt engineering alone.

3. **Error Analysis on Novel Slang**: Manually analyze the model's performance on slang terms that appear in the test set but not in the training set to quantify how often the lookup table fails and the LLM must handle truly novel normalization cases. This would validate the assumption that most slang is recurrent across datasets.