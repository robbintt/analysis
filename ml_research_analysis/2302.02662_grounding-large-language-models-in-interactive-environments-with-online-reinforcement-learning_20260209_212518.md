---
ver: rpa2
title: Grounding Large Language Models in Interactive Environments with Online Reinforcement
  Learning
arxiv_id: '2302.02662'
source_url: https://arxiv.org/abs/2302.02662
tags:
- agent
- action
- steps
- forward
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces GLAM, a method for functionally grounding
  Large Language Models (LLMs) in interactive environments using online reinforcement
  learning. It addresses the challenge of aligning LLM knowledge with environment
  dynamics, which is crucial for functional competence.
---

# Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning

## Quick Facts
- arXiv ID: 2302.02662
- Source URL: https://arxiv.org/abs/2302.02662
- Authors: Thomas Carta; Clément Romac; Thomas Wolf; Sylvain Lamprier; Olivier Sigaud; Pierre-Yves Oudeyer
- Reference count: 40
- Primary result: GLAM significantly improves sample efficiency and generalization in text-based BabyAI environments compared to zero-shot LLM use and supervised finetuning.

## Executive Summary
This paper introduces GLAM, a method for functionally grounding Large Language Models (LLMs) in interactive environments using online reinforcement learning. The core innovation addresses the challenge of aligning LLM knowledge with environment dynamics by treating the LLM as a policy that is incrementally updated through interactions. By leveraging pretrained LLMs as effective inductive biases, GLAM demonstrates significantly improved sample efficiency and generalization abilities compared to training agents from scratch or using zero-shot LLM approaches.

## Method Summary
The method uses an LLM (specifically Flan-T5) as a policy that selects actions by computing the log-probability of each possible action token sequence given the current prompt. Rather than generating text and parsing it to an action, the model scores all valid actions by summing token log-probabilities. A value head (MLP) is attached to the decoder to enable PPO optimization. The prompt structure includes a header with admissible actions, the goal, and a sliding window of recent observations and actions. The system uses the Lamorel library for parallelizing LLM calls across actions.

## Key Results
- GFlan-T5 achieves 0.8 success rate after 250,000 steps, outperforming non-pretrained baselines like NPAE-Flan-T5 (under 0.2).
- GLAM shows robustness to variations in action space size and number of distractors.
- The method maintains strong performance in zero-shot generalization tests with new objects and tasks.
- Pretrained LLMs significantly improve sample efficiency compared to training agents from scratch.

## Why This Works (Mechanism)

### Mechanism 1
Pretrained LLMs provide effective inductive bias for policy learning, significantly improving sample efficiency compared to training agents from scratch. The model leverages semantic priors (e.g., spatial relationships, action affordances) encoded during pretraining, starting with a "world model" that roughly aligns with the environment's dynamics. Core assumption: The LLM's pretraining corpus contains sufficient descriptions of spatial concepts and physical interactions that transfer to the target environment's logic.

### Mechanism 2
Treating action selection as a sequence likelihood scoring task allows the LLM to act as a policy without requiring ad-hoc output parsing or constrained text generation. Rather than generating a text string and mapping it to an action, the model computes the log-probability of every valid action token sequence given the current prompt. Core assumption: The discrete action space is small enough that computing forward passes for every possible action is computationally tractable, or can be parallelized.

### Mechanism 3
Attaching a value head to the LLM architecture enables standard RL algorithms (like PPO) to optimize the policy using the LLM's internal state representations. A Multi-Layer Perceptron (MLP) is mounted on the decoder block to estimate value, allowing PPO to compute advantages and backpropagate gradients through the LLM. Core assumption: The LLM's latent representations of textual observations and goals contain sufficient information to predict value (future rewards), not just text likelihood.

## Foundational Learning

- **Proximal Policy Optimization (PPO)**: PPO balances exploration and exploitation via a clipped surrogate objective, crucial for tuning the stability of LLM finetuning. Quick check: How does the clipping parameter ε prevent the policy from changing too drastically during a single update?

- **Encoder-Decoder Architectures (e.g., T5)**: Understanding where to attach the Value Head (on the Decoder side) requires distinguishing these components. Quick check: Why might the authors place the value head on the decoder block rather than the encoder output in an Encoder-Decoder model?

- **Functional Grounding vs. Direct Grounding**: The paper distinguishes its goal (aligning abstract symbols to environment dynamics) from standard "direct grounding" (mapping pixels to words). Quick check: Does functional grounding require visual input, or can it be achieved purely through textual interaction and feedback?

## Architecture Onboarding

- **Component map**: Input (Prompt) -> Flan-T5 (Encoder-Decoder) -> Policy Head (Scoring) -> Value Head (MLP on Decoder) -> Infrastructure (Lamorel)
- **Critical path**: The system bottlenecks at Inference Step 3. Calculating the probability of every action requires a forward pass for each action. Parallelization is required to maintain reasonable throughput.
- **Design tradeoffs**: Likelihood Scoring vs. Generation (scoring is robust but slow, generation is fast but brittle); Pretrained vs. Scratch (pretrained offers fast convergence but risks "hallucinated" priors, training from scratch is robust but sample-inefficient).
- **Failure signatures**: Stochastic paralysis (repeating valid actions that don't progress the goal), Catastrophic Forgetting (high PPO learning rates degrade linguistic capabilities), OOM (parallelizing forward passes for large action spaces consumes massive VRAM).
- **First 3 experiments**: 1) Baseline Check - run zero-shot evaluation of Flan-T5 to verify probabilistic scoring yields better than random performance; 2) Head Ablation - compare attaching Value Head to encoder output vs. decoder block; 3) Parallelization Stress Test - measure FPS as action space grows to quantify scoring method overhead.

## Open Questions the Paper Calls Out

1. **Generalization Impact**: Does functional grounding in one environment degrade an LLM's zero-shot abilities or plasticity in different environments? The study only evaluated within the single textual environment it was trained on.

2. **Multimodal Extension**: Can GLAM be effectively applied to multimodal or robotic environments using foundation models? Current experiments are limited to a textual environment, suggesting leveraging recent multi-modal Foundation models for broader environments.

3. **Computational Efficiency**: How can the computational inefficiency of computing action probabilities be mitigated for large LLMs? The paper identifies this as a limitation that constrained experiments to smaller models, requiring a forward pass for every possible action.

## Limitations

- **Architecture Specificity**: The value head placement lacks detailed justification or ablation studies comparing alternative placements.
- **Generalization Beyond Text**: Limited evidence about how well this approach transfers to more complex environments or those with visual inputs.
- **Computational Scalability**: The action-scoring method creates an O(N) inference bottleneck without comprehensive benchmarks on scaling.

## Confidence

- **High Confidence (4/5)**: Core claim that pretrained LLMs provide effective inductive bias for policy learning is well-supported by experimental results comparing GFlan-T5 to non-pretrained baselines.
- **Medium Confidence (3/5)**: Claim about treating action selection as sequence likelihood scoring being robust to any action space is supported but lacks extensive empirical validation across diverse action spaces.
- **Medium Confidence (3/5)**: Value head mechanism enabling standard RL algorithms is technically sound but specific architectural choice and its impact are not thoroughly investigated through ablation studies.

## Next Checks

1. **Prompt Template Verification**: Reconstruct the exact prompt template by examining the provided GitHub implementation or Appendix C. Test whether minor variations in prompt formatting significantly impact performance.

2. **Value Head Placement Ablation**: Implement and compare alternative value head placements (encoder output, second decoder block, attention layer outputs) to determine if the chosen placement is optimal or interchangeable.

3. **Action Space Scalability Test**: Systematically measure inference latency and sample efficiency as the action space grows from 3 to 20+ actions. Compare the parallelized scoring approach against a generation-based approach with post-hoc parsing to quantify the computational tradeoff.