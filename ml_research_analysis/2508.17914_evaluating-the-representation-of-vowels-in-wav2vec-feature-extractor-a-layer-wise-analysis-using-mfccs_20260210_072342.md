---
ver: rpa2
title: 'Evaluating the Representation of Vowels in Wav2Vec Feature Extractor: A Layer-Wise
  Analysis Using MFCCs'
arxiv_id: '2508.17914'
source_url: https://arxiv.org/abs/2508.17914
tags:
- layers
- mfccs
- feature
- speech
- phonetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined how convolutional layers in Wav2Vec encode
  phonetic information for front-back vowel classification. Using TIMIT data, SVM
  classifiers were trained on MFCCs, MFCCs+formants, and CNN activations from seven
  Wav2Vec layers.
---

# Evaluating the Representation of Vowels in Wav2Vec Feature Extractor: A Layer-Wise Analysis Using MFCCs

## Quick Facts
- arXiv ID: 2508.17914
- Source URL: https://arxiv.org/abs/2508.17914
- Reference count: 4
- Primary result: CNN activations in Wav2Vec layers outperform MFCCs for front-back vowel classification, achieving 95.7% accuracy at layer 5.

## Executive Summary
This study examines how convolutional layers in Wav2Vec encode phonetic information for front-back vowel classification. Using TIMIT data, SVM classifiers were trained on MFCCs, MFCCs+formants, and CNN activations from seven Wav2Vec layers. CNN activations outperformed spectral features, achieving 95.7% accuracy at layer 5. Mutual information analysis showed early CNN layers share spectral similarities with MFCCs, but deeper layers refine vowel representations beyond traditional features. MFCCs alone reached 93.1% accuracy, comparable to mid-level CNN layers. Results confirm convolutional layers progressively encode phonetic information, with deeper layers achieving higher accuracy, though requiring extensive pretraining to match MFCC effectiveness.

## Method Summary
The study extracted activations from seven CNN layers of a fine-tuned Wav2Vec 2.0 XLSR-53 model and compared them with MFCCs and formants for front-back vowel classification. Vowel segments from TIMIT were filtered to 1500-2000 samples, zero-padded to 2000, and classified using SVM with grid search. Classification accuracy and mutual information between features were computed to assess layer-wise phonetic encoding.

## Key Results
- CNN activations achieved peak accuracy of 95.7% at layer 5, outperforming MFCCs (93.1%) and MFCCs+formants (93.3%)
- Early CNN layers (0-2) showed high mutual information with MFCCs (0.0819-0.1040), indicating spectral encoding
- Deeper layers progressively transformed representations, with MI declining to 0.0078 at layer 5 while accuracy increased
- Front vowels were more frequently misclassified as back vowels than vice versa, suggesting model bias

## Why This Works (Mechanism)

### Mechanism 1
Early CNN layers in Wav2Vec encode spectral features similar to MFCCs, while deeper layers progressively transform these into more abstract phonetic representations. Seven-layer CNN feature extractor hierarchically processes raw waveforms. Early layers (0-2) show high mutual information with MFCCs (MI: 0.0819-0.1040), indicating spectral encoding. Deeper layers (3-6) show declining MI but improving classification accuracy (peak 95.7% at layer 5), suggesting transformation into task-optimized representations.

### Mechanism 2
Self-supervised pre-training on large multilingual data (53 languages, ~60k hours) enables CNN feature extractors to capture contextual phonetic patterns beyond local acoustic features. During pre-training, the CNN jointly optimizes with transformer layers through contrastive learning objectives. This exposes convolutional filters to phonetic co-occurrence patterns across contexts, encoding representations that outperform purely local spectral features (95.7% vs 93.1% accuracy).

### Mechanism 3
Front-back vowel classification relies primarily on spectral information (formant patterns), which both MFCCs and CNN layers capture effectively. Front-back vowel distinction correlates with F2 formant frequency. MFCCs implicitly encode spectral envelope including formant structure. CNN layers learn similar filters through data-driven optimization, achieving comparable mid-layer performance (layer 3: 93.3% vs MFCC+F1+F2: 93.3%).

## Foundational Learning

- Concept: Mel-Frequency Cepstral Coefficients (MFCCs)
  - Why needed here: Serves as the baseline for comparison; early CNN layers appear to learn similar spectral representations.
  - Quick check question: Can you explain why the mel scale approximates human auditory perception and how this relates to formant encoding?

- Concept: Mutual Information (MI) for feature comparison
  - Why needed here: Core analytical tool for quantifying similarity between CNN activations and spectral features across layers.
  - Quick check question: If MI between CNN activations and MFCCs decreases from layer 1 to layer 5, what does this imply about how representations change?

- Concept: Self-supervised contrastive learning in speech
  - Why needed here: Explains why Wav2Vec's CNN encodes more than local acousticsâ€”pre-training shapes filter behavior.
  - Quick check question: How does masking future frames and predicting them force the model to learn phonetic structure rather than just acoustic patterns?

## Architecture Onboarding

- Component map:
Raw Waveform (16kHz) -> CNN Feature Extractor (7 conv layers) -> Feature Vectors (768-dim) -> Transformer Encoder (contextual modeling) -> Task Outputs

- Critical path: Layer 5 appears optimal for phonetic information; this is where SVMs achieve peak accuracy. For probing tasks, extract from this layer first.

- Design tradeoffs:
  - Using fine-tuned model (xlsr-53-espeak-cv-ft) vs. base pre-trained: Fine-tuned may bias toward phonetic structure but obscures what was learned during pre-training vs. fine-tuning.
  - No normalization on CNN activations: May inflate SVM performance through scale artifacts; MFCCs were MinMax-scaled.
  - Vowel segment filtering (1500-2000 samples): Reduces duration variability but excludes 43% of segments, potentially biasing toward longer, more spectrally stable vowels.

- Failure signatures:
  - Layer 6 shows slight accuracy drop (94.8% vs 95.7%): May indicate over-abstraction or information bottleneck before transformer.
  - Confusion matrices: All models show front-vowel bias; back vowels more frequently misclassified as front. Class imbalance (1,736 front vs 946 back) likely contributes.
  - Formants not speaker-normalized: "may introduce variability" per authors; F1/F2 added minimal improvement (+0.2%).

- First 3 experiments:
  1. Replicate layer-wise SVM probing with normalized CNN activations (MinMax or z-score) to confirm performance gains are not scale artifacts.
  2. Test on held-out vowels including diphthongs (excluded here) to assess generalization beyond monophthongs.
  3. Compare fine-tuned vs. base pre-trained Wav2Vec to isolate which layer characteristics emerge from pre-training vs. fine-tuning.

## Open Questions the Paper Calls Out
- How do the phonetic representations in Wav2Vec's feature extractor generalize across different ASR architectures and varying phonetic contexts?
- Does treating vowel representations as isolated snapshots rather than continuous speech sequences significantly impact the detection of dynamic phonetic variations?
- To what extent does the lack of speaker normalization for formants and raw scaling for CNN activations influence the comparative classification performance?

## Limitations
- Performance advantage of CNN activations over MFCCs may partially reflect uncontrolled variance since CNN activations were used raw while MFCCs were MinMax-scaled.
- Vowel classification task represents a narrow phonetic category (front-back distinction) that may not generalize to broader speech features.
- Filtering to 1500-2000 sample segments excluded 43% of vowel data, potentially biasing toward spectrally stable segments.

## Confidence
- High confidence: Deeper CNN layers achieve higher vowel classification accuracy than earlier layers.
- Medium confidence: Accuracy improvement reflects genuine phonetic abstraction rather than representational capacity or scale artifacts.
- Low confidence: Layer-wise patterns generalize to other phonetic tasks or more dynamic speech contexts.

## Next Checks
1. Replicate the SVM probing experiments with normalized CNN activations (MinMax or z-score) to verify that performance gains are not artifacts of raw scale differences.
2. Test the layer-wise probing methodology on held-out vowel categories including diphthongs to assess generalization beyond monophthongs.
3. Compare layer characteristics between the fine-tuned model and base pre-trained Wav2Vec to isolate which representational properties emerge from pre-training versus fine-tuning.