---
ver: rpa2
title: 'REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real
  Websites'
arxiv_id: '2504.11543'
source_url: https://arxiv.org/abs/2504.11543
tags:
- agents
- real
- agent
- tasks
- websites
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REAL addresses the need for robust, reproducible benchmarks for
  autonomous web agents by providing 11 deterministic, high-fidelity replicas of real
  websites across key domains like e-commerce, travel, and communication. These environments
  enable realistic, multi-turn task evaluation without safety or state-change risks.
---

# REAL: Benchmarking Autonomous Agents on Deterministic Simulations of Real Websites

## Quick Facts
- arXiv ID: 2504.11543
- Source URL: https://arxiv.org/abs/2504.11543
- Reference count: 14
- Primary result: Leading models achieve at most 41% success rate on 112 practical web tasks across 11 deterministic website replicas

## Executive Summary
REAL introduces a comprehensive benchmark for evaluating autonomous web agents through deterministic replicas of real websites. The framework addresses critical gaps in reproducibility and safety by providing controlled environments where agents can practice multi-turn web interactions without affecting real systems. With 11 website replicas spanning e-commerce, travel, communication, and more, REAL offers 112 practical tasks that reflect everyday web navigation challenges. The evaluation methodology combines programmatic state verification with LLM-based rubric judgment to assess both action-based and information retrieval tasks. Empirical results demonstrate that even state-of-the-art models like Claude 3.7 Sonnet Thinking struggle to achieve reliable performance, highlighting significant room for improvement in autonomous web navigation capabilities.

## Method Summary
The REAL framework provides deterministic, high-fidelity replicas of real websites hosted on Vercel for evaluating autonomous web agents. Agents interact through a flexible harness supporting Playwright and Chrome DevTools Protocol integration, receiving observations via DOM, screenshots, or accessibility trees. Evaluation combines programmatic localStorage state verification for action-based tasks with LLM-guided rubric assessment for information retrieval. The framework includes 11 website replicas across key domains, pre-authenticated sessions to eliminate login complexity, and supports both local evaluation and public leaderboard submission. Agents signal task completion by navigating to /submit, triggering automated evaluation against predefined assertions or rubric-guided judgment.

## Key Results
- Claude 3.7 Sonnet Thinking achieves maximum 41% success rate across all tasks
- Action-based tasks show better performance than information retrieval tasks
- Navigation dead ends and inadequate state verification represent primary failure modes
- Deterministic environments enable reproducible evaluation but may not reflect real-world variability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deterministic website replicas enable reproducible, safe evaluation of multi-turn web agent behaviors.
- Mechanism: By fixing all data, timestamps, and UX elements in simulated websites while persisting state via browser localStorage, the framework eliminates variability across task executions. This allows identical conditions to be recreated for systematic comparison.
- Core assumption: Agents trained or evaluated on deterministic simulations will transfer capabilities to real websites with similar interaction patterns.
- Evidence anchors:
  - [abstract] "All interactions occur within this fully controlled setting, eliminating safety risks and enabling robust, reproducible evaluation of agent capability and reliability."
  - [Section 3.3] "To ensure reproducible evaluations... the websites were designed to be fully deterministic through several key features: Static Data... Predefined Temporal Settings... Replayability"
  - [corpus] Neighbor paper WAREX notes that current benchmarks measure performance in controlled environments but real-world reliability differs—suggesting the determinism-realism tradeoff is an open question.
- Break condition: If agent strategies overfit to fixed data patterns (e.g., memorizing element positions), performance gains may not generalize to dynamic production websites.

### Mechanism 2
- Claim: Combining programmatic state verification with LLM-based rubric judgment provides complementary evaluation coverage for action and retrieval tasks.
- Mechanism: Action-based tasks are evaluated by comparing pre-task and post-task localStorage state differences against expected key-value assertions (deterministic). Information retrieval tasks use an LLM judge to assess responses against task-specific rubrics. Combined tasks require both to pass.
- Core assumption: localStorage state diffs accurately capture task-relevant state changes, and LLM judges can reliably assess information correctness.
- Evidence anchors:
  - [abstract] "Evaluation combines programmatic state verification for action-based tasks with rubric-guided LLM judgments for information retrieval."
  - [Section 4.3] "Action-based Tasks (rA): Rewards are determined by programmatic verification function f_eval, which compares the difference between the initial (s0) and final (sT) 'localStorage' states against a set of predefined key-value assertions"
  - [corpus] No direct corpus evidence on LLM-judge reliability for web task evaluation—this remains an assumption requiring validation.
- Break condition: If agents find ways to modify localStorage without completing intended actions, or if LLM judges exhibit rubric interpretation inconsistencies, evaluation validity degrades.

### Mechanism 3
- Claim: Flexible integration via Playwright and CDP accommodates diverse agent architectures without forcing standardized observation/action spaces.
- Mechanism: The framework exposes three integration paths—high-level Playwright API for standard web interactions, low-level CDP for fine-grained browser control, and URL-based endpoints for black-box systems. Agents signal completion by navigating to /submit, triggering state capture.
- Core assumption: Rewarding outcome rather than process allows diverse agent approaches to be fairly compared.
- Evidence anchors:
  - [abstract] "The framework supports both open-source and proprietary agents via flexible integration options, including Playwright and Chrome DevTools Protocol."
  - [Section 6.1] "The harness offers three integration settings... direct Playwright integration... WebSocket endpoint for CDP... black-box systems... URL endpoints that expose the browser instance"
  - [corpus] CowPilot paper emphasizes human-agent collaboration for complex tasks—suggesting purely autonomous evaluation may miss important interaction patterns.
- Break condition: If outcome-only rewards miss critical process failures (e.g., agents taking dangerous intermediate actions that happen not to affect final state), safety-critical behaviors go unevaluated.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: The paper explicitly models agent-environment interaction as a POMDP where the underlying browser state is partially observable through various observation functions (screenshots, DOM, accessibility tree).
  - Quick check question: Can you explain why web navigation is modeled as partially observable rather than fully observable?

- Concept: Chrome DevTools Protocol (CDP)
  - Why needed here: REAL provides CDP access for low-level browser control, enabling agents to execute arbitrary JavaScript, intercept network requests, and manipulate the DOM directly—understanding this is essential for advanced agent integration.
  - Quick check question: What capabilities does CDP provide that Playwright's high-level API does not?

- Concept: Browser localStorage for state persistence
  - Why needed here: All website state changes are persisted in localStorage, forming the basis for programmatic evaluation. Understanding how React/Next.js applications manage client-side state is prerequisite to debugging evaluation failures.
  - Quick check question: How would you inspect localStorage state changes mid-task without completing the evaluation?

## Architecture Onboarding

- Component map: 11 website replicas (React/Next.js) -> Agent harness (Playwright/CDP) -> Configuration system -> Evaluation framework -> Leaderboard API

- Critical path: Task definition → /config endpoint initialization → Agent receives observation → Agent executes actions (via Playwright/CDP) → Agent navigates to /submit → localStorage + response captured → Evaluation functions compute reward → Result logged

- Design tradeoffs:
  - Determinism vs. realism: Fixed data enables reproducibility but may not reflect production website variability
  - Binary outcome rewards vs. dense step-wise rewards: Current design uses binary rewards; framework supports dense rewards for RL but requires additional implementation
  - Pre-authenticated sessions vs. login flows: Removes authentication complexity but skips evaluating login-related agent capabilities

- Failure signatures:
  - Inadequate state verification: Agent believes task complete but localStorage shows partial state changes (e.g., added one item to cart instead of two)
  - Navigation dead-ends: Agent enters sub-menu and cannot identify backtracking UI elements, enters click loops
  - Impossible task misrecognition: Agent hallucinates success on deliberately impossible tasks instead of reporting inability

- First 3 experiments:
  1. Run baseline agent on a single environment (e.g., Omnizon) with /finish endpoint inspection after each action to understand state change patterns and debug reward computation.
  2. Configure error injection (e.g., error_finding_driver=true on UDriver) to evaluate agent error recovery—compare trajectories with and without errors enabled.
  3. Implement a simple custom agent using Playwright interface with screenshot-only observations, evaluate on 10 easy tasks, and analyze failure modes against the qualitative observations in Section 8.1.

## Open