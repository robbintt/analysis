---
ver: rpa2
title: Building Models of Neurological Language
arxiv_id: '2506.06208'
source_url: https://arxiv.org/abs/2506.06208
tags:
- language
- clinical
- data
- medical
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report details the development of domain-specific language
  models for neurology, shifting focus from bespoke fine-tuning to leveraging retrieval-augmented
  generation (RAG) and representational models for secure, local deployment. Key contributions
  include the creation of neurology-specific datasets (case reports, QA sets, textbook-derived
  data), tools for multi-word expression extraction, and graph-based analyses of medical
  terminology.
---

# Building Models of Neurological Language

## Quick Facts
- **arXiv ID:** 2506.06208
- **Source URL:** https://arxiv.org/abs/2506.06208
- **Reference count:** 0
- **Primary result:** Local RAG-augmented Gemma-7b models achieved 96%+ accuracy on neurology textbook QA, approaching GPT-4 performance

## Executive Summary
This report details the development of domain-specific language models for neurology, shifting focus from bespoke fine-tuning to leveraging retrieval-augmented generation (RAG) and representational models for secure, local deployment. Key contributions include the creation of neurology-specific datasets (case reports, QA sets, textbook-derived data), tools for multi-word expression extraction, and graph-based analyses of medical terminology. The project produced scripts and Docker containers for local hosting. Fine-tuned Gemma-7b models with RAG achieved strong performance on neurology-specific question-answering tasks, approaching GPT-4 accuracy on both MRCPUK and textbook-derived datasets, with RAG consistently improving QA accuracy. For summarisation tasks, RAG provided little benefit, and fine-tuned models generally underperformed compared to GPT-4. Overall, RAG is most effective for question answering, while summarisation remains challenging for smaller models.

## Method Summary
The methodology involved processing 336k radiology reports and textbook data, extracting terminology via PMI-based multi-word expression analysis, building co-occurrence graphs for semantic clustering, and fine-tuning Gemma-7b using QLoRA on RTX 4090. The core innovation was integrating RAG with locally-hosted models, using BGE embeddings to retrieve relevant passages from curated knowledge bases (textbooks, NICE guidelines) before generating responses. Synthetic question-answering datasets were generated using GPT-4 prompts based on textbook passages. The system was containerized for deployment with Docker, supporting both QA and summarization tasks.

## Key Results
- Gemma-7b + RAG achieved 96%+ accuracy on textbook QA, approaching GPT-4 performance
- RAG consistently improved QA accuracy across all model variants (base Gemma-7b: 0.855 → Gemma-7b RAG: 0.958 on TextbookQA)
- RAG provided minimal benefit for summarization tasks, where fine-tuned models underperformed GPT-4
- PMI-based terminology extraction identified clinically relevant multi-word expressions like "periventricular white matter changes"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG improves question-answering accuracy for domain-specific neurology tasks more than fine-tuning alone.
- Mechanism: RAG prepends retrieved passages from curated knowledge bases (textbooks, NICE guidelines) to LLM prompts, grounding outputs in verified domain content rather than relying solely on parametric knowledge.
- Core assumption: The embedding model (BGE) captures neurology-specific semantics sufficiently to retrieve relevant passages.
- Evidence anchors:
  - [abstract] "NeuroBase, based on Gemma-7b with QLoRA fine-tuning and RAG, approached GPT-4 accuracy on medical question-answering (over 96% on textbook QA)"
  - [Section 4.2, Table 1] RAG consistently improves QA accuracy (e.g., Gemma-7b base: 0.855 → Gemma-7b RAG: 0.958 on TextbookQA)
  - [corpus] "Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial LLMs in Medical Tasks" (arXiv:2506.20009) supports RAG effectiveness in medical domains
- Break condition: If retrieval corpus lacks coverage for rare conditions, RAG provides no benefit; if embedding model fails on neurology terminology, retrieved passages will be irrelevant.

### Mechanism 2
- Claim: PMI-based multi-word expression extraction identifies domain-specific phrases that improve vocabulary coverage for downstream tasks.
- Mechanism: PMI quantifies how much more often two words co-occur than expected by chance; high PMI bigrams (e.g., "periventricular white matter changes") are candidate domain terms missed by unigram models.
- Core assumption: Statistical co-occurrence in the corpus reflects meaningful semantic units rather than artifact.
- Evidence anchors:
  - [Section 3.1] "PPMI(x, y) = max(0, log₂ p(x,y)/(p(x)p(y)))... A higher PPMI value indicates a stronger association between the two words"
  - [Section 3.2] Extracted MWE "periventricular white matter changes" showed strong MCC with dementia cluster
  - [corpus] Limited direct corpus evidence on PMI for MWE in medical domains; mechanism is standard NLP practice but domain-specific validation is thin
- Break condition: If corpus is too small, PMI estimates are noisy; if stop-word removal is poorly tuned, meaningful phrases may be fragmented.

### Mechanism 3
- Claim: Graph-based community detection on terminology co-occurrence networks reveals clinically meaningful semantic clusters (e.g., condition-descriptor-anatomy relationships).
- Mechanism: PMI-weighted co-occurrence graphs are clustered using soft Louvain or nested stochastic block models; high-centrality nodes become "head" terms for data-driven ontologies.
- Core assumption: Term co-occurrence patterns in radiology reports reflect real clinical relationships rather than documentation habits.
- Evidence anchors:
  - [Section 7.2, Figures 3-6] Sub-graphs for meningioma, encephalitis, aneurysm show coherent clusters of pathological, descriptive, and anatomical terms
  - [Section 6.1] "Each cluster or community represents a group of terminology that is closely related both in terms of semantics and presentation"
  - [corpus] No directly comparable corpus papers use PMI graphs for neurology ontology; approach is novel to this domain
- Break condition: If reports use inconsistent terminology, graph structure fragments; if thresholding is too aggressive, rare but valid associations are lost.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core architecture choice; understanding how retrieval and generation interact explains performance differences.
  - Quick check question: Can you explain why RAG improved QA accuracy but not summarization scores in this paper?

- Concept: **Pointwise Mutual Information (PMI)**
  - Why needed here: Underpins both MWE extraction and terminology graph construction.
  - Quick check question: Given word probabilities p("small")=0.01, p("vessel")=0.005, and joint p("small vessel")=0.0002, is this a strong bigram by PMI?

- Concept: **Parameter-Efficient Fine-Tuning (QLoRA)**
  - Why needed here: Enables fine-tuning 7B models on consumer GPUs (RTX 4090) by updating only adapter weights.
  - Quick check question: What fraction of model parameters are typically updated in QLoRA vs. full fine-tuning?

## Architecture Onboarding

- Component map:
  - Embedding service (Dockerfile_rag, BGE model) → vectorizes documents and queries
  - LLM service (Dockerfile_tgi, Gemma-7b) → generates responses
  - RAG API (Dockerfile_rag) → orchestrates retrieval + generation
  - Database (Dockerfile_db) → stores intermediate data, logs
  - Shell scripts → automate training, evaluation, hosting workflows

- Critical path:
  1. Build embedding index from textbook/NICE guideline corpora
  2. Host embedding service on port 8082
  3. Load fine-tuned Gemma-7b adapter weights
  4. Route queries through RAG API (retrieve → prepend → generate)

- Design tradeoffs:
  - RAG vs. fine-tuning only: RAG adds latency but improves factual accuracy; fine-tuning alone is faster but hallucinates more
  - Small local model vs. cloud API: Local preserves privacy but caps model capacity; cloud offers GPT-4 performance but fails data-security requirements
  - Hard vs. soft clustering: Soft Louvain allows terms in multiple communities (appropriate for non-specific terms like "inflammation")

- Failure signatures:
  - Retrieval returns irrelevant passages: Check embedding model compatibility with neurology terminology; verify corpus preprocessing
  - QA accuracy drops after fine-tuning: Overfitting to training distribution; validate on held-out MRCPUK set
  - Summarization quality poor: RAG does not help summarization (observed in Table 2); consider larger model or different prompt strategy

- First 3 experiments:
  1. Reproduce Table 1 results: Run `evaluations.sh` on base Gemma-7b vs. Gemma-7b + RAG on TextbookQA; verify ~10 point accuracy gain
  2. Validate MWE extraction: Run `pyMWE` on radiology reports; manually inspect top-50 bigrams for clinical relevance
  3. Test deployment isolation: Spin up full Docker stack locally; send curl request via `example_curl_request.sh` and confirm no external network calls

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does Retrieval-Augmented Generation (RAG) fail to improve summarisation performance in small neurology models, and can architectural modifications bridge this gap with GPT-4?
- Basis in paper: [explicit] Section 4.2 states that while RAG consistently improves QA accuracy, it "provides little benefit" for summarisation tasks where smaller models underperform compared to GPT-4.
- Why unresolved: The authors observed the phenomenon but did not investigate the specific mechanisms causing RAG's ineffectiveness for abstractive summarisation in their fine-tuned models.
- What evidence would resolve it: A comparative ablation study of attention mechanisms in RAG-augmented summarisation versus QA tasks within the Gemma-7b architecture.

### Open Question 2
- Question: To what extent does the integration of radiological imaging data with the text-based NeuroBase framework improve the fidelity of deep phenotyping?
- Basis in paper: [explicit] Section 9 identifies "Multimodal Integration" as a key future direction, proposing the adaptation of architectures like phi-4-multimodal to process images alongside text.
- Why unresolved: The current NeuroBase model and its phenotyping tools (Section 6) are limited to unstructured text and do not yet process visual clinical data.
- What evidence would resolve it: Evaluation metrics comparing the diagnostic accuracy or phenotypic granularity of a multimodal model against the text-only baseline on the same patient cohorts.

### Open Question 3
- Question: Can the deep latent representations discovered by NeuroBase effectively model the temporal trajectories of neurological diseases?
- Basis in paper: [explicit] Section 9 lists "Longitudinal Phenotyping" as a future objective, aiming to link text-derived phenotypes to outcomes over time.
- Why unresolved: The current analysis relies on static snapshots (case reports and radiology reports) and hierarchical clustering, which does not inherently capture disease progression or temporal dynamics.
- What evidence would resolve it: Demonstration of a predictive model using the latent representations to forecast disease stages or symptom progression from sequential clinical notes.

### Open Question 4
- Question: Do the latent term structures and diagnostic patterns discovered via graph-based analysis represent clinically valid disease ontologies or merely documentation artifacts?
- Basis in paper: [inferred] Section 7.3 notes the method "surfaces subtler term relationships that merit further clinical investigation," and Section 1.1 mentions the risk of models learning idiosyncratic styles rather than medical facts.
- Why unresolved: The ontology is derived unsupervised from co-occurrence statistics (PMI) and clustering, which may reflect biases in reporting styles at the NHNN rather than universal biological relationships.
- What evidence would resolve it: An expert-validated comparison of the data-driven ontology against gold-standard, curated neurological taxonomies (like SNOMED-CT) to assess clinical semantic alignment.

## Limitations
- The primary radiology report corpus (336k reports) is private, requiring reproduction to rely solely on open textbook/case report data
- Hyperparameter transparency issues with unspecified QLoRA parameters and GPT-4 synthetic data generation prompts
- RAG provided minimal benefit for summarization tasks, with fine-tuned models generally underperforming GPT-4
- Graph analysis validation lacks quantitative clinician annotation study to confirm clinical meaningfulness

## Confidence

- **High confidence:** RAG improves QA accuracy over fine-tuning alone (supported by multiple experiments across datasets showing consistent 10+ point gains)
- **Medium confidence:** PMI-based MWE extraction identifies clinically relevant terms (supported by MCC validation but limited direct corpus evidence in medical domains)
- **Medium confidence:** Graph-based terminology clustering reveals meaningful semantic relationships (supported by visual inspection of sub-graphs but lacking quantitative validation)

## Next Checks

1. Reproduce Table 1 QA accuracy gain: Run the provided evaluation scripts comparing Gemma-7b base vs. Gemma-7b + RAG on TextbookQA to verify the ~10 point accuracy improvement.

2. Validate MWE extraction pipeline: Process a sample of radiology reports through the pyMWE tool and manually review the top 50 bigrams for clinical relevance.

3. Test RAG effectiveness on summarization: Run the summarization evaluation pipeline (Table 2) to confirm that RAG provides minimal benefit for this task, validating the paper's observation about RAG's task-specific effectiveness.