---
ver: rpa2
title: You Need an Encoder for Native Position-Independent Caching
arxiv_id: '2602.01519'
source_url: https://arxiv.org/abs/2602.01519
tags:
- caching
- encoder
- zhang
- wang
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COMB addresses the inefficiency of prefix-based caching in Large
  Language Models (LLMs) by reintroducing an encoder into decoder-only architectures
  to enable native Position-Independent Caching (PIC). The method involves training
  the encoder to process static context chunks independently, generating reusable
  KV caches that can be concatenated in arbitrary order during inference.
---

# You Need an Encoder for Native Position-Independent Caching
## Quick Facts
- arXiv ID: 2602.01519
- Source URL: https://arxiv.org/abs/2602.01519
- Reference count: 40
- Primary result: 75% reduction in KV cache memory while maintaining accuracy

## Executive Summary
COMB addresses the inefficiency of prefix-based caching in Large Language Models (LLMs) by reintroducing an encoder into decoder-only architectures to enable native Position-Independent Caching (PIC). The method involves training the encoder to process static context chunks independently, generating reusable KV caches that can be concatenated in arbitrary order during inference. This design allows seamless integration with existing serving systems while maintaining non-intrusive operation.

Experiments show COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by up to 3× compared to baseline approaches, with accuracy matching or exceeding standard prefix-based attention. Memory usage for KV caches is reduced by 75%, and the system achieves these gains without permanently modifying the underlying decoder-only model.

## Method Summary
COMB introduces an encoder-decoder architecture where the encoder processes static context chunks independently to generate position-independent KV caches. These caches are stored and can be concatenated in any order during inference, eliminating the sequential dependency of traditional prefix-based caching. The encoder is trained alongside the decoder using a masking strategy that ensures position-independence, and the system maintains compatibility with existing serving frameworks by keeping the decoder-only model intact while adding the encoder as an auxiliary component.

## Key Results
- 51-94% reduction in Time-to-First-Token (TTFT) compared to baseline approaches
- 75% reduction in KV cache memory usage while maintaining accuracy
- Up to 3× increase in throughput during inference

## Why This Works (Mechanism)
COMB works by decoupling context processing from sequential token generation. The encoder processes static chunks independently, creating position-independent KV caches that can be reused and concatenated arbitrarily. This eliminates the need to cache all previous tokens sequentially, dramatically reducing memory requirements. During inference, the decoder simply combines these pre-computed caches, maintaining the same attention mechanisms as standard models but with far greater flexibility and efficiency.

## Foundational Learning
1. **Position-Independent Caching**: Why needed - eliminates sequential dependency in KV cache generation; Quick check - verify caches can be concatenated in any order without accuracy loss
2. **Encoder-Decoder Integration**: Why needed - enables static context processing separate from dynamic generation; Quick check - confirm decoder maintains original functionality
3. **Masking Strategy**: Why needed - ensures encoder learns position-independent representations; Quick check - validate masking doesn't degrade encoder performance
4. **Non-Intrusive Architecture**: Why needed - maintains compatibility with existing serving systems; Quick check - test integration with vLLM or FastChat

## Architecture Onboarding
**Component Map**: Static Context → Encoder → Position-Independent KV Cache → Cache Concatenation → Decoder
**Critical Path**: Encoder processing → KV cache generation → Cache storage → Cache retrieval during inference
**Design Tradeoffs**: Memory vs. throughput optimization, encoder training overhead vs. runtime gains, compatibility vs. architectural complexity
**Failure Signatures**: Memory bloat from improper cache management, accuracy degradation from incorrect cache concatenation, increased TTFT from encoder bottlenecks
**First Experiments**: 1) Test encoder-only performance on static context chunks, 2) Validate cache concatenation maintains accuracy, 3) Measure memory usage with varying context lengths

## Open Questions the Paper Calls Out
None

## Limitations
- Memory reduction claims lack comprehensive ablation studies across varying context lengths
- Operational overhead of maintaining dual encoder-decoder infrastructure not quantified
- Limited benchmarking against most recent state-of-the-art caching optimizations

## Confidence
- **High Confidence**: Architectural feasibility and non-intrusive integration demonstrated
- **Medium Confidence**: Quantitative performance improvements supported but lack comprehensive validation
- **Low Confidence**: Production system efficiency and operational overhead claims remain unverified

## Next Checks
1. **Scalability Testing**: Evaluate COMB's memory and throughput benefits across diverse context lengths (4K, 16K, 32K tokens) and model architectures (LLaMA, Mistral)
2. **Dynamic Context Evaluation**: Test COMB's efficacy in real-time applications where context evolves during inference
3. **Production Overhead Analysis**: Measure computational and memory overhead of maintaining encoder during inference, including cold-start costs