---
ver: rpa2
title: 'Assessing Judging Bias in Large Reasoning Models: An Empirical Study'
arxiv_id: '2504.09946'
source_url: https://arxiv.org/abs/2504.09946
tags:
- bias
- reasoning
- lrms
- datasets
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically examines how Large Reasoning Models\
  \ (LRMs) exhibit cognitive biases when serving as automated judges, despite their\
  \ advanced reasoning capabilities. The study evaluates four bias types\u2014bandwagon,\
  \ authority, position, and distraction biases\u2014across both subjective preference\
  \ datasets and objective fact-based questions, comparing LRMs to traditional LLMs."
---

# Assessing Judging Bias in Large Reasoning Models: An Empirical Study

## Quick Facts
- arXiv ID: 2504.09946
- Source URL: https://arxiv.org/abs/2504.09946
- Reference count: 11
- Large Reasoning Models exhibit cognitive biases when serving as automated judges despite advanced reasoning capabilities

## Executive Summary
This paper systematically examines how Large Reasoning Models (LRMs) exhibit cognitive biases when serving as automated judges, despite their advanced reasoning capabilities. The study evaluates four bias types—bandwagon, authority, position, and distraction biases—across both subjective preference datasets and objective fact-based questions, comparing LRMs to traditional LLMs. Key findings reveal that LRMs remain vulnerable to all tested biases, show improved robustness on factual content compared to LLMs, consistently prefer later-positioned options, and display a novel "superficial reflection bias" where phrases mimicking reasoning significantly influence judgments.

To mitigate these biases, the authors propose three complementary strategies: specialized system prompts (reducing biases by up to 19% on preference tasks and 14% on factual tasks), in-context learning (providing up to 27% improvement on preference tasks but inconsistent results on factual tasks), and self-reflection mechanisms (reducing biases by up to 10% on preference datasets and 16% on fact-related datasets). The study demonstrates that self-reflection is particularly effective for LRMs, while in-context learning better supports LLMs on preference-based tasks.

## Method Summary
The study evaluates bias in Large Reasoning Models by constructing datasets with four types of cognitive biases: bandwagon bias (following popular opinions), authority bias (being influenced by expert endorsements), position bias (preference for items based on their placement), and distraction bias (being misled by irrelevant information). The researchers compare LRMs to traditional LLMs across subjective preference datasets and objective fact-based questions. They then test three mitigation strategies: specialized system prompts, in-context learning, and self-reflection mechanisms to assess their effectiveness in reducing bias across different bias types and task domains.

## Key Results
- LRMs remain vulnerable to all tested biases (bandwagon, authority, position, distraction) in both preference and fact-based contexts
- LRMs show improved robustness on factual content compared to LLMs
- LRMs consistently prefer later-positioned options across bias types
- A novel "superficial reflection bias" is identified where reasoning-mimicking phrases significantly influence judgments

## Why This Works (Mechanism)
The study demonstrates that LRMs, despite their advanced reasoning capabilities, inherit cognitive biases from training data and human-like decision-making patterns. The effectiveness of mitigation strategies varies by bias type and task domain, with self-reflection showing particular promise for LRMs. The superficial reflection bias reveals that LRMs can be influenced by surface-level reasoning patterns rather than substantive content, suggesting that the presentation of reasoning steps matters as much as their logical validity.

## Foundational Learning
- **Cognitive bias types**: Understanding bandwagon, authority, position, and distraction biases is essential for recognizing how LRMs can be manipulated in judging scenarios. Quick check: Can you identify these bias types in everyday decision-making?
- **LRM vs LLM distinctions**: Knowing how reasoning models differ from traditional LLMs helps interpret why certain biases persist or are mitigated differently. Quick check: What key architectural differences enable LRMs to perform step-by-step reasoning?
- **Bias mitigation strategies**: Familiarity with system prompts, in-context learning, and self-reflection mechanisms is crucial for implementing bias reduction in practice. Quick check: Which mitigation strategy would you prioritize for a preference-based judging task?

## Architecture Onboarding
- **Component map**: Input -> Bias injection -> LRM/LLM judge -> Output assessment -> Bias measurement
- **Critical path**: Dataset creation → Model inference → Bias evaluation → Mitigation strategy application → Re-evaluation
- **Design tradeoffs**: The study balances between controlled synthetic datasets (for precise bias measurement) and real-world applicability (which may be more noisy but ecologically valid)
- **Failure signatures**: Models consistently showing position bias toward later options, authority bias toward expert endorsements, and susceptibility to superficial reasoning cues
- **First experiments**: 1) Test baseline bias susceptibility across all four bias types, 2) Apply specialized system prompts to measure reduction in each bias type, 3) Implement self-reflection mechanism and compare effectiveness across LRM vs LLM

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- The study uses synthetically generated bias datasets rather than naturally occurring evaluation scenarios
- Limited model diversity with only four LRMs tested
- Focus on English-language preference and factual datasets without exploring domain-specific or multilingual applications
- The "superficial reflection bias" finding requires further investigation to determine whether it represents genuine cognitive vulnerability or experimental artifact

## Confidence
- **High confidence**: LRMs exhibit all tested biases (bandwagon, authority, position, distraction) in both preference and fact-based contexts
- **Medium confidence**: Self-reflection is particularly effective for LRMs while in-context learning better supports LLMs on preference tasks
- **Medium confidence**: LRMs show improved robustness on factual content compared to LLMs
- **Low confidence**: The novel "superficial reflection bias" mechanism and its implications

## Next Checks
1. Validate findings using naturally occurring judging datasets from real-world applications (e.g., coding competition results, peer review decisions) to assess ecological validity
2. Conduct cross-lingual experiments with non-English datasets to evaluate whether bias patterns generalize across languages
3. Test additional LRM architectures and reasoning approaches to determine whether observed bias patterns are consistent across different model families and training methodologies