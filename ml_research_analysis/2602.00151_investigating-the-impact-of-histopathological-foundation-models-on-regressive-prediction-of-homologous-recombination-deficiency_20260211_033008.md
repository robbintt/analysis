---
ver: rpa2
title: Investigating the Impact of Histopathological Foundation Models on Regressive
  Prediction of Homologous Recombination Deficiency
arxiv_id: '2602.00151'
source_url: https://arxiv.org/abs/2602.00151
tags:
- foundation
- sampling
- performance
- pathology
- cancer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates histopathological foundation
  models for regressive biomarker prediction, focusing on homologous recombination
  deficiency (HRD) score estimation from whole slide images (WSIs) using multiple
  instance learning (MIL) frameworks. The researchers compared five state-of-the-art
  foundation models (UNI-2, Virchow-2, UNI, GPFM, CONCH) against a contrastive learning
  baseline (RetCCL) across breast, endometrial, and lung cancer cohorts from TCGA
  and CPTAC datasets.
---

# Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency

## Quick Facts
- arXiv ID: 2602.00151
- Source URL: https://arxiv.org/abs/2602.00151
- Reference count: 12
- Primary result: Foundation models outperform contrastive learning baseline for regressive HRD prediction from WSIs

## Executive Summary
This study systematically evaluates histopathological foundation models for regressive biomarker prediction, focusing on homologous recombination deficiency (HRD) score estimation from whole slide images (WSIs) using multiple instance learning (MIL) frameworks. The researchers compared five state-of-the-art foundation models (UNI-2, Virchow-2, UNI, GPFM, CONCH) against a contrastive learning baseline (RetCCL) across breast, endometrial, and lung cancer cohorts from TCGA and CPTAC datasets. Models were trained to predict continuous HRD scores, then converted to binary classification for comparison with prior work. Key findings include: foundation models consistently outperformed the baseline across all datasets; UNI-2 achieved the best overall performance (AUROC up to 0.91 on TCGA-BRCA); models showed superior generalization to external cohorts; a novel distribution-based upsampling algorithm significantly improved recall and balanced accuracy for underrepresented HRD+ patients; and ablation studies revealed that clustering-based sampling strategies outperformed random sampling. The results demonstrate that large-scale histopathological pretraining substantially enhances regressive biomarker prediction accuracy and transferability, advancing AI-driven precision oncology applications.

## Method Summary
The study performs regressive prediction of HRD scores from WSIs using MIL frameworks. Researchers extracted patches from TCGA and CPTAC cohorts, applied color normalization and background rejection, then used foundation models (UNI-2, Virchow-2, UNI, GPFM, CONCH) or contrastive learning baseline (RetCCL) to extract features. K-Means clustering (k=50) on patch features enabled cluster-size-weighted sampling for bag creation (50-4000 patches). Two MIL architectures (attMIL and SuRe Transformer) aggregated patch features for regression. Models were evaluated using 5-fold cross-validation on TCGA datasets, with external validation on CPTAC. A distribution-based upsampling algorithm addressed class imbalance by selectively augmenting rare HRD values.

## Key Results
- Foundation models consistently outperformed RetCCL baseline across all datasets (AUROC improvements up to 0.35)
- UNI-2 achieved highest internal validation performance (AUROC 0.8367 on TCGA-BRCA)
- Virchow-2 demonstrated best external generalization (AUROC 0.9885 on CPTAC-UCEC)
- Distribution-based upsampling significantly improved recall and balanced accuracy for underrepresented HRD+ patients
- Clustering-based sampling outperformed random sampling, particularly at constrained bag sizes

## Why This Works (Mechanism)

### Mechanism 1
Large-scale histopathological foundation models produce more informative and transferable patch-level features than contrastive learning baselines for regression tasks. Foundation models pre-trained on millions of WSIs (UNI-2: 350K+ slides; Virchow-2: 3M+ slides) learn general-purpose tissue representations through self-distillation and masked image modeling. These representations capture morphological patterns that correlate with molecular biomarkers like HRD, enabling the downstream MIL aggregator to attend to diagnostically relevant regions.

### Mechanism 2
Distribution-based upsampling improves model sensitivity to underrepresented high-HRD patients by selectively augmenting rare target values. The algorithm bins continuous HRD values, calculates a sampling budget for each bin proportional to its distance from the largest bin, and generates new training instances by resampling patches from rare-case patients. A budget cap (α=0.65) and scaling factor (β=0.25) prevent overfitting to specific patient slides while smoothing the target distribution.

### Mechanism 3
K-means clustering-based sampling produces more informative bag representations than random sampling, particularly at constrained bag sizes. Clustering (k=50) enforces feature-space coverage by guaranteeing at least one sample per cluster. This prevents random sampling from over-representing common tissue patterns and missing rare but potentially HRD-relevant regions (e.g., specific stromal or nuclear morphologies).

## Foundational Learning

- **Concept: Multiple Instance Learning (MIL)**
  - Why needed here: Gigapixel WSIs cannot be processed end-to-end. MIL treats each slide as a "bag" of patch instances with only patient-level labels available, enabling weakly supervised biomarker prediction.
  - Quick check question: Why does attention-based MIL aggregate patch features differently than the SuRe Transformer's radial decay attention?

- **Concept: Homologous Recombination Deficiency (HRD)**
  - Why needed here: HRD score (sum of LOH, TAI, LST genomic markers) is the regression target. Understanding that HRD reflects DNA repair impairment explains why certain morphological patterns (nuclear atypia, chromosomal instability signatures) might be visible in H&E slides.
  - Quick check question: Why might predicting continuous HRD scores be clinically more useful than binary classification at threshold 42?

- **Concept: Foundation Model Pretraining Paradigms**
  - Why needed here: The paper compares self-distillation with masked image modeling (UNI-2, Virchow-2, DINOv2-based) against contrastive learning (RetCCL). Understanding that the former learns dense representations via patch reconstruction while the latter learns via instance discrimination is critical for interpreting feature quality differences.
  - Quick check question: How does CONCH's image-text alignment differ from UNI-2's pure vision pretraining, and when might each be preferred?

## Architecture Onboarding

- **Component map:** WSI Preprocessing -> Feature Extraction -> Patient Bag Creation -> Instance Sampling -> Aggregation -> Regression Head
- **Critical path:** Feature extraction model → sampling strategy → bag size → aggregation architecture
  - Internal validation best: UNI-2 + attMIL + cluster-size-weighted sampling (bag 600–1200)
  - External generalization best: Virchow-2 + attMIL/SuRe (trains on TCGA, validates on CPTAC)
- **Design tradeoffs:**
  - UNI-2: Highest internal AUROC (0.8367 on BRCA) but variable cross-cancer transfer; ViT-H architecture
  - Virchow-2: Best external validation (0.9885 on CPTAC-UCEC); trained on 3M WSIs but potentially slower inference
  - attMIL: Handles unlimited patches; lower computational cost; no spatial modeling
  - SuRe Transformer: Captures local spatial context via radial decay attention; requires fixed bag size; modest bag-size dependency (correlation 0.30)
- **Failure signatures:**
  - High AUROC (>0.80) but low recall (<0.50) → class imbalance; apply distribution-based upsampling
  - Large internal-external performance gap (>0.15 AUROC) → poor generalization; switch to Virchow-2
  - Random sampling matching clustering at bag sizes <200 → insufficient patch diversity or cluster quality issues
- **First 3 experiments:**
  1. Replicate internal TCGA-BRCA baseline: Train attMIL with UNI-2 features, cluster-size-weighted sampling, bag size 1000, 5-fold CV. Verify AUROC ~0.83 at threshold 42.
  2. Test cross-cancer generalization: Train on TCGA-LUAD, evaluate on CPTAC-UCEC using UNI-2 vs. RetCCL. Expect ~0.35+ AUROC gap.
  3. Validate upsampling impact: Apply distribution-based upsampling (7 bins, α=0.65, β=0.25) to TCGA-LUAD training. Measure balanced accuracy and recall improvement on CPTAC-LUAD HRD+ cases.

## Open Questions the Paper Calls Out

### Open Question 1
Why does UNI exhibit poor transferability specifically in the LUAD→UCEC cross-cancer-type setting, and is this limitation consistent across additional external datasets? The authors note "an exceptional weak performance of UNI in the LUAD→UCEC setting" and explicitly state "this phenomenon should be further investigated with additional datasets."

### Open Question 2
Can domain-specific clustering mechanisms that explicitly target HRD-relevant morphological features outperform general K-Means clustering for patch selection? The conclusion states: "Incorporating HRD-tailored clustering mechanisms that leverage domain knowledge to emphasize the most relevant image regions, represents a promising direction for future work."

### Open Question 3
How robust are the distribution-based upsampling algorithm's hyperparameters (n_bins, α, β) across different biomarkers, cancer types, and imbalance severities? The upsampling algorithm uses fixed hyperparameters (7 bins, α=0.65, β=0.25) chosen for TCGA-LUAD, but no ablation study validates these choices or tests transferability to other datasets.

### Open Question 4
To what extent do these findings generalize to other continuous biomarkers beyond HRD in computational pathology? The introduction positions HRD as "a primary use case for this regressive approach," but the study only evaluates HRD, leaving unclear whether foundation model benefits transfer to other regressive biomarker tasks.

## Limitations

- The study compares foundation models against only one contrastive learning baseline (RetCCL) without benchmarking against other recent CPathFMs or domain-specific regression models.
- The distribution-based upsampling algorithm lacks direct comparison with standard oversampling techniques like SMOTE or class-weighted loss functions.
- The study focuses on regression performance without validating whether foundation model features capture clinically interpretable morphological patterns predictive of HRD.

## Confidence

- **High Confidence:** Foundation models outperform RetCCL baseline across all tested datasets; clustering-based sampling improves performance over random sampling at constrained bag sizes; upsampling improves recall for underrepresented HRD+ cases.
- **Medium Confidence:** Foundation models show superior cross-cohort generalization compared to contrastive learning; specific foundation models (UNI-2 for internal validation, Virchow-2 for external validation) consistently perform best.
- **Low Confidence:** The absolute performance advantage of foundation models over all possible alternatives; clinical utility of continuous HRD predictions versus binary classification.

## Next Checks

1. **Benchmark against multiple baselines:** Compare foundation models against other CPathFMs (e.g., EXAONE Path 2.0, DINOv2) and domain-specific regression models using the same TCGA/CPTAC datasets to establish relative performance.
2. **Validate feature interpretability:** Perform attention visualization and patch-level feature analysis to determine whether foundation model predictions align with known HRD-associated morphological patterns (nuclear atypia, chromosomal instability signatures).
3. **Test upsampling alternatives:** Implement and compare distribution-based upsampling with standard techniques (SMOTE, class-weighted loss, focal loss) to isolate the specific benefit of the proposed algorithm.