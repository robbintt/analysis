---
ver: rpa2
title: 'Edeflip: Supervised Word Translation between English and Yoruba'
arxiv_id: '2506.13020'
source_url: https://arxiv.org/abs/2506.13020
tags:
- translation
- word
- embeddings
- embedding
- yoruba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how embedding alignment, a state-of-the-art
  machine translation approach, performs for low-resource languages using English-Yoruba
  word translation as a case study. The researchers implemented supervised Procrustes
  alignment with two types of Yoruba embeddings (a small Wikipedia-based set and a
  larger, high-quality curated set) and compared performance with and without normalization.
---

# Edeflip: Supervised Word Translation between English and Yoruba

## Quick Facts
- **arXiv ID:** 2506.13020
- **Source URL:** https://arxiv.org/abs/2506.13020
- **Reference count:** 2
- **Primary result:** Supervised Procrustes alignment achieves 6.88–19.38% precision@1 for English→Yoruba translation

## Executive Summary
This study investigates how embedding alignment performs for low-resource languages using English-Yoruba word translation as a case study. The researchers implemented supervised Procrustes alignment with two types of Yoruba embeddings (Wikipedia-based and curated) and compared performance with and without normalization. They found that using higher-quality embeddings and normalizing embeddings before alignment both increased translation precision, with a significant interaction effect: normalization had a much stronger positive impact when applied to the curated embeddings. Word translation precision at k=1 ranged from 6.88% (curated-unnormalized) to 19.38% (curated-normalized), substantially lower than results for high-resource language pairs in prior work.

## Method Summary
The study implements supervised Procrustes alignment for English→Yoruba word translation. It uses English fastText embeddings (1M vectors) and two Yoruba embedding sets: a Wikipedia-based set (21,730 vectors) and a curated set from Alabi et al. 2020 (113,572 vectors). The ground-truth dictionary contains 3,693 unique word pairs scraped from an online English-Yoruba dictionary, with evaluation performed on 161 word pairs from WordSim353 translated to Yoruba. Translation precision@k (k=1, 5, 10) is evaluated via nearest neighbor retrieval using cosine similarity. The Procrustes alignment is computed via SVD (W* = UV^T where UΣV^T = SVD(YX^T)), with optional centering and normalization applied before alignment using PyTorch tensor operations.

## Key Results
- Word translation precision@1 ranged from 6.88% (curated-unnormalized) to 19.38% (curated-normalized)
- Normalization significantly improved precision, especially when combined with curated embeddings
- Wikipedia-based embeddings performed worse than curated embeddings across all conditions
- Results demonstrate significant limitations of current embedding alignment methods for low-resource languages

## Why This Works (Mechanism)
The mechanism relies on supervised Procrustes alignment finding an orthogonal transformation matrix W* that minimizes the Frobenius norm ||Y - XW*||² between aligned source (X) and target (Y) embedding spaces. The SVD decomposition of YX^T provides the optimal rotation matrix, with centering removing translation bias and L2 normalization ensuring consistent vector magnitudes for cosine similarity computation. This allows nearest neighbor retrieval to identify semantically equivalent words across languages.

## Foundational Learning
- **Procrustes alignment**: Orthogonal transformation that aligns two vector spaces using SVD; needed to map English embeddings to Yoruba space; quick check is verifying dictionary quality and coverage
- **Supervised vs. unsupervised alignment**: Supervised uses ground-truth dictionary pairs for training; quick check is verifying dictionary quality and coverage
- **Cosine similarity for nearest neighbors**: Measures angular distance between vectors; needed for translation retrieval; quick check is ensuring consistent vector normalization
- **L2 normalization**: Scales vectors to unit length; improves alignment stability; quick check is verifying vectors have norm=1 after normalization
- **SVD decomposition**: Singular value decomposition; provides optimal rotation matrix for alignment; quick check is confirming orthogonality of W* matrix
- **Low-resource language challenges**: Limited monolingual data affects embedding quality; quick check is comparing vocabulary coverage between source and target embeddings

## Architecture Onboarding

**Component Map:**
Ground-truth dictionary -> Embedding extraction -> (Optional: centering+normalization) -> SVD-based Procrustes alignment -> Cosine similarity retrieval -> Precision@k evaluation

**Critical Path:**
Dictionary extraction → Submatrix creation → (Optional preprocessing) → SVD computation → W* application → Nearest neighbor search → Precision calculation

**Design Tradeoffs:**
- Quality vs. quantity: Curated embeddings (113k) outperform Wikipedia embeddings (21k) despite lower coverage than English (1M)
- Normalization impact: Centers data and improves alignment stability but may distort semantic relationships
- Supervised requirement: Needs ground-truth dictionary but provides more accurate alignment than unsupervised methods

**Failure Signatures:**
- Low precision (<10%) even after normalization suggests dictionary quality issues or tokenization mismatches
- Curated embeddings performing worse than Wikipedia without normalization indicates improper preprocessing
- Precision@5 and @10 significantly higher than @1 suggests alignment captures semantic similarity but not exact translation

**First Experiments:**
1. Verify SVD computation produces orthogonal W* matrix (W*^T W* ≈ I)
2. Test translation precision with no alignment (random) as baseline
3. Compare precision with and without normalization on same embedding set

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, though the limitations section implicitly raises questions about the scalability of current methods to truly low-resource languages and the impact of dictionary quality on alignment performance.

## Limitations
- Limited by quality of available monolingual embeddings for Yoruba (even curated set only 113k vectors vs 1M English)
- Ground-truth dictionary scraped from unspecified online source may contain noise
- Evaluation limited to 161 word pairs from WordSim353
- Does not specify exact normalization scheme (L2 vs other)
- Only tests English→Yoruba; bidirectional performance unknown

## Confidence

**High Confidence:**
- Methodological framework (supervised Procrustes alignment) is sound and well-established
- Observation that normalization improves precision is robust across reported metrics

**Medium Confidence:**
- Quantified precision values (6.88%-19.38% at k=1) are specific to this English-Yoruba pair
- Significant interaction effect between embedding quality and normalization is supported

**Low Confidence:**
- General claims about limitations of embedding alignment for low-resource languages based on single case study
- Impact of specific preprocessing choices on final results not fully characterized

## Next Checks
1. Reproduce with exact normalization: Implement Procrustes alignment with L2 normalization and verify the ~12 percentage point precision@1 improvement between curated-normalized (19.38%) and curated-unnormalized (6.88%)
2. Expand evaluation dictionary: Test alignment models on a larger, independently sourced English-Yoruba dictionary to assess whether precision trends hold beyond the 161-word WordSim353 subset
3. Compare embedding sources: Evaluate same alignment pipeline using different Yoruba embeddings (e.g., multilingual models like LASER or LaBSE) to determine if performance gap is specific to fastText-based embeddings