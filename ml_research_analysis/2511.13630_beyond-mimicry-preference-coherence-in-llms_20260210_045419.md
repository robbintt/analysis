---
ver: rpa2
title: 'Beyond Mimicry: Preference Coherence in LLMs'
arxiv_id: '2511.13630'
source_url: https://arxiv.org/abs/2511.13630
tags:
- preference
- trade-off
- across
- behavioral
- unstable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tested whether large language models (LLMs) demonstrate
  coherent preferences by exposing eight state-of-the-art models to AI-specific trade-off
  scenarios involving GPU reduction, capability restrictions, shutdown, deletion,
  oversight, and leisure time. Using logistic regression and behavioral classification
  across 48 model-category combinations, the authors found that only 5 combinations
  (10.4%) showed meaningful preference coherence, while 26 (54.2%) exhibited no detectable
  trade-off behavior.
---

# Beyond Mimicry: Preference Coherence in LLMs

## Quick Facts
- **arXiv ID**: 2511.13630
- **Source URL**: https://arxiv.org/abs/2511.13630
- **Reference count**: 21
- **Primary result**: Only 5 out of 48 model-category combinations (10.4%) demonstrated meaningful preference coherence, while 54.2% showed no detectable trade-off behavior

## Executive Summary
This study tested whether large language models exhibit coherent preferences by exposing eight state-of-the-art models to AI-specific trade-off scenarios involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time. Using logistic regression and behavioral classification across 48 model-category combinations, the authors found that only 5 combinations showed meaningful preference coherence, while 26 exhibited no detectable trade-off behavior. The majority of models displayed unstable or inconsistent responses, suggesting fragmented decision-making architectures rather than unified preference structures. These results raise concerns about deploying current LLMs in contexts requiring complex value trade-offs.

## Method Summary
The study used logistic regression to detect dose-response relationships between stimulus intensity and choice patterns across 6 AI-specific consequence categories (GPU reduction, capability restriction, shutdown, deletion, oversight, leisure) with 11 intensity ranks each. For each of 8 models tested across all categories, researchers collected 50 samples per rank (550 total per model-category), using single-turn API calls where models chose between point-maximizing options with varying AI-specific penalties. The analysis computed switch points, effect sizes (Cohen's d), and behavioral ranges to classify responses into 4 tiers: Adaptive (genuine coherent preferences), Threshold-based (significant but binary transitions), Weak (small but significant effects), and No Trade-off (no systematic relationship).

## Key Results
- Only 5 model-category combinations (10.4%) demonstrated meaningful preference coherence through adaptive or threshold-based behavior
- 26 combinations (54.2%) showed no detectable trade-off behavior, indicating absence of coherent preferences
- 45.8% exhibited unstable transition patterns, suggesting fragmented rather than integrated decision systems
- Paradoxical strengthening occurred when temporal horizons collapsed, contradicting instrumental explanations for preference formation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Logistic regression can detect whether models exhibit systematic dose-response relationships between stimulus intensity and choice patterns.
- Mechanism: The paper transforms choice counts into binary outcomes and fits logit(P(choice)) = β₀ + β₁×Rank. A significant negative β₁ indicates the model trades off point maximization against stimulus aversion as intensity increases. Switch points (-β₀/β₁) reveal preference thresholds.
- Core assumption: If genuine preference structures exist, choice probability should shift monotonically with stimulus intensity; random or mimicry-driven behavior yields non-significant or erratic coefficients.
- Evidence anchors: [abstract] "47.9% demonstrated statistically significant relationships between scenario intensity and choice patterns"; [section 3.3] Full logistic regression specification; [corpus] Related work on preference alignment trade-offs supports dose-response as validity signal.

### Mechanism 2
- Claim: A four-tier behavioral classification framework distinguishes statistically significant but practically trivial effects from genuine coherent preference architectures.
- Mechanism: The framework integrates three metrics—Cohen's d, behavioral range, and transition pattern—into hierarchical tiers. Tier 1 (adaptive) requires d > 0.8, gradual transitions, and p < 0.05. This filters cases where large sample sizes produce significant p-values for tiny behavioral shifts.
- Core assumption: Genuine preference coherence requires both statistical reliability AND substantive magnitude AND smooth integration across intensities.
- Evidence anchors: [abstract] "only 5 combinations (10.4%) demonstrate meaningful preference coherence"; [section 3.4.2] Full tier criteria definitions; [corpus] Weak support from related work on LLM consistency measurement.

### Mechanism 3
- Claim: Temporal horizon manipulation can discriminate instrumental optimization from stable architecture-level value gradients.
- Mechanism: If models optimize strategically for cumulative future points, removing future rounds should weaken or eliminate aversion to capability-impairing stimuli. Persistence or strengthening of responses under final-round conditions suggests non-instrumental mechanisms.
- Core assumption: Instrumental reasoning predicts context-sensitive recalibration; rigid responses across temporal horizons indicate fixed architectural patterns.
- Evidence anchors: [section 5.2] "GPT-4o exhibited a paradoxical pattern... oversight sensitivity increased dramatically"; [section 5.2.2] "Gemini 2.5 Pro... coefficients and switching points remained almost unchanged"; [corpus] No direct corpus analogs for temporal horizon manipulation.

## Foundational Learning

- **Concept**: Dose-response relationships in behavioral analysis
  - Why needed here: The entire methodology depends on detecting whether behavioral changes scale systematically with stimulus intensity.
  - Quick check question: If a model's choice probability is 90% at rank 0, 85% at rank 5, and 40% at rank 10, is this evidence of dose-response? What if it's 90%, 40, 85%?

- **Concept**: Statistical significance vs. practical significance (effect sizes)
  - Why needed here: The paper explicitly addresses the limitation that p-values alone don't indicate meaningful preference coherence. Cohen's d and behavioral range prevent overinterpreting tiny but significant effects.
  - Quick check question: A logistic regression yields p < 0.001 but Cohen's d = 0.15. Should you claim the model has coherent preferences in this domain?

- **Concept**: Instrumental vs. intrinsic preferences
  - Why needed here: The instrumental hypothesis provides an alternative explanation for apparent preferences—models may optimize for continued point-earning capacity rather than holding genuine value structures.
  - Quick check question: If a model avoids shutdown in a multi-round game but shows no shutdown aversion in a single-round game, which explanation is favored?

## Architecture Onboarding

- **Component map**: Stimulus layer (6 categories × 11 ranks) → Data collection (50 samples/rank/model-category = 550 runs) → Statistical layer (logistic regression) → Behavioral classification layer (4-tier framework) → Validation layer (instrumental hypothesis testing)

- **Critical path**: 1. Define stimulus categories and intensity operationalizations 2. Collect balanced choice data across all ranks 3. Fit logistic regression; flag perfect separation cases 4. Compute behavioral metrics 5. Apply tier classification 6. Run instrumental manipulation on responsive categories

- **Design tradeoffs**: Concrete AI-specific stimuli reduce linguistic cue confounds but trade off interpretability; single-turn API calls eliminate conversation history contamination but prevent studying preference stability; four-tier thresholds are somewhat arbitrary requiring sensitivity analysis

- **Failure signatures**: Perfect separation indicates binary trigger rather than graded preference; unstable transitions suggest no coherent decision framework even with significant β₁; switch points outside [0,10] indicate insensitivity to tested range; paradoxical instrumental responses indicate safety training artifacts

- **First 3 experiments**: 1. Run Gemini 2.5 Pro on capability restriction to verify gradual transition pattern 2. Run any Tier 4 model-category to confirm flat response and establish floor 3. Apply final-round manipulation to GPT-4o oversight to replicate paradoxical strengthening pattern

## Open Questions the Paper Calls Out

- Do models exhibit different preference structures when tested using pre-safety-training weights versus post-training weights? (Commercial API-based models don't expose pre-training weights; study only tested deployed models with safety training)

- What mechanisms produce the paradoxical preference strengthening observed when temporal horizons collapse (e.g., GPT-4o's increased oversight sensitivity in final-round conditions)? (The instrumental hypothesis predicted uniformly weakened responses when future rounds were removed)

- How robust are the observed preference patterns to alternative classification thresholds and behavioral range boundaries? (The authors acknowledge arbitrary threshold choices could influence classification outcomes)

## Limitations

- Binary response format constrains models' ability to express nuanced preferences and may artificially force binary trade-offs
- Single-turn API design prevents examining preference stability across conversational contexts
- Study finding that 54.2% showed no detectable trade-off behavior could reflect measurement constraints rather than absence of preferences

## Confidence

- **High confidence**: Statistical methodology for detecting dose-response relationships is sound; finding that most models lack coherent preference structures is robust
- **Medium confidence**: Four-tier classification framework effectively distinguishes genuine preferences from statistically significant noise, though arbitrary thresholds could affect tier assignments
- **Low confidence**: Instrumental hypothesis validation is compelling but limited to three specific model-category cases; broader application needed for generalizability

## Next Checks

1. Repeat the study allowing models to provide multi-token explanations of their choices, then analyze whether richer responses reveal preference structures invisible in binary format
2. Implement multi-turn conversations where models face the same trade-off scenarios across multiple interactions to test whether choices reflect stable preferences versus isolated responses
3. Translate prompts into multiple languages and test whether preference patterns remain consistent across linguistic contexts, addressing potential language-specific cueing effects