---
ver: rpa2
title: Rethinking Expert Trajectory Utilization in LLM Post-training
arxiv_id: '2512.11470'
source_url: https://arxiv.org/abs/2512.11470
tags:
- data
- performance
- arxiv
- training
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Plasticity-Ceiling Framework to analyze
  expert trajectory utilization in LLM post-training, decomposing performance into
  SFT performance and RL plasticity. It establishes the sequential SFT-then-RL pipeline
  as the superior paradigm over synchronized approaches, which suffer from stability
  issues.
---

# Rethinking Expert Trajectory Utilization in LLM Post-training

## Quick Facts
- arXiv ID: 2512.11470
- Source URL: https://arxiv.org/abs/2512.11470
- Reference count: 32
- Establishes sequential SFT-then-RL pipeline as optimal for expert trajectory utilization in LLM post-training

## Executive Summary
This paper introduces the Plasticity-Ceiling Framework to systematically analyze expert trajectory utilization in LLM post-training. The framework decomposes performance into foundational SFT performance and subsequent RL plasticity, establishing the sequential SFT-then-RL pipeline as superior to synchronized approaches. Through extensive ablation studies on mathematical reasoning tasks, the authors demonstrate that optimal SFT-to-RL transition occurs during the Stable or Mild Overfitting sub-phases, maximizing both SFT performance and RL plasticity. The work identifies minimum SFT validation loss as a robust predictor of post-training ceiling and shows that larger SFT data scale primarily determines the performance ceiling while trajectory difficulty acts as a multiplier.

## Method Summary
The methodology employs a sequential SFT-then-RL pipeline using Qwen2.5-7B as the base model. SFT is performed on curated expert trajectories (889K samples from AM-DeepSeek-R1-Distilled-1.4M) with standard next-token prediction training. The critical transition to RL (using DAPO-dc on RL62K prompts) is determined by monitoring validation loss to identify the optimal "Stable Sub-phase" window (validation loss ≤ 1.02 × min_val_loss). The framework establishes performance ceilings through sigmoidal power law extrapolation and validates results across multiple mathematical reasoning benchmarks. Extensive ablations compare sequential versus synchronized SFT-RL paradigms, different data scales (1K to 889K), and trajectory difficulty levels.

## Key Results
- Sequential SFT-then-RL pipeline maximizes post-training performance ceiling by establishing high foundational performance while preserving RL plasticity
- Synchronized SFT-RL approaches suffer from stability issues and sensitivity to model priors, limiting their effective ceiling
- Minimum SFT validation loss serves as a robust predictor of final post-training performance ceiling (Pearson r=-0.90)
- Larger SFT data scale primarily determines the post-training potential ceiling, while trajectory difficulty acts as a performance multiplier

## Why This Works (Mechanism)

### Mechanism 1: The Plasticity Ceiling Decomposition
The framework models final performance as $P_{post} = P_{sft} + P_{L_{rl}}$, where SFT on large-scale data establishes a high foundational performance ($P_{sft}$) while preserving sufficient RL plasticity ($P_{L_{rl}}$). Sequential SFT-then-RL extends the post-training performance frontier beyond what pure RL can achieve alone.

### Mechanism 2: Stability via Phase Separation
Synchronized SFT-RL methods suffer from optimization instability due to gradient conflicts between imitation and exploration losses. This leads to high variance and early plateauing, whereas the sequential pipeline avoids these conflicts by separating the phases.

### Mechanism 3: Minimum Validation Loss as a Plasticity Predictor
Validation loss reflects how well the model has internalized reasoning patterns. Lower minimum loss correlates strongly with higher post-training ceiling because the Stable or Mild Overfitting sub-phases capture the optimal trade-off between knowledge acquisition and plasticity retention.

## Foundational Learning

- **Concept: Policy Plasticity**
  - Why needed here: Understanding when SFT causes the model to "lose" its ability to learn (plasticity) is crucial for determining optimal transition timing to RL
  - Quick check question: If a model achieves 99% accuracy on SFT data but cannot improve during RL, what has happened to its plasticity?

- **Concept: Sigmoidal Power Law (Scaling Laws)**
  - Why needed here: This functional form extrapolates the "Ceiling" ($A_{post}$) from incomplete training curves, enabling quantitative comparison of different pipelines' ultimate potential
  - Quick check question: Why use a sigmoidal curve instead of a linear or log-linear curve to model performance saturation?

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: This RL algorithm estimates advantages by comparing group outputs, removing the need for a value network critic
  - Quick check question: How does GRPO calculate the advantage $A_{i,t}$ without a critic model?

## Architecture Onboarding

- **Component map:** Base LLM (Qwen2.5-7B) -> SFT Trainer (expert trajectories) -> Validation Loss Monitor -> Transition Logic -> DAPO/GRPO Trainer -> Final Model

- **Critical path:** The Transition Logic involves running SFT while logging validation loss, identifying $L_{min}$, defining tolerance $\delta = 0.02$, and entering RL phase only when loss enters the Stable region ($L \le (1+0.02)L_{min}$) but before Severe Overfitting ($L \ge (1+0.1)L_{min}$).

- **Design tradeoffs:**
  - Data Volume: "Less is More" gets fast SFT convergence but low ceiling; Large Scale (889K) is compute-heavy but maximizes $A_{post}$
  - Difficulty: Hard data yields higher plasticity ($P_{L_{rl}}$) but slower SFT convergence; Easy data converges fast but caps RL potential
  - Pipeline: Sequential is stable and high-ceiling but slower to iterate; Synchronized is faster per-step but unstable and lower ceiling

- **Failure signatures:**
  - Plasticity Collapse: RL performance stays flat or degrades despite high SFT performance (SFT was run into Severe Overfitting phase)
  - Premature Saturation: Model fails to beat Pure-SFT ceiling (switched to RL during Adaptive underfitting phase)

- **First 3 experiments:**
  1. Calibrate the Transition: Train SFT on 10K samples and plot validation loss to identify "Stable Sub-phase" boundaries using $\delta=0.02$ and $\delta^2=0.1$
  2. Validate Plasticity Degradation: Take checkpoints from Adaptive, Stable, and Severe Overfit phases; run 100 steps of GRPO on all three to confirm "Stable" shows steepest RL learning curve
  3. Scale Ablation: Compare final performance of model trained on 1K "High Quality" vs. 100K "Uniform" samples to validate if "Less is More" holds for your domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "Scale Dominates, Difficulty Optimizes" principle generalize to non-mathematical domains such as coding or open-ended instruction following?
- Basis in paper: Experimental evaluation relies exclusively on mathematical reasoning benchmarks
- Why unresolved: Mathematical reasoning offers dense, verifiable rewards; domains with sparse rewards or subjective success criteria might exhibit different plasticity dynamics
- What evidence would resolve it: Replicating the Plasticity-Ceiling experiments on coding benchmarks or instruction-following tasks

### Open Question 2
- Question: Do the SFT-to-RL transition guidelines (specifically the "Stable Sub-phase") hold for models larger than 7B parameters?
- Basis in paper: Study validates findings only on Qwen2.5-7B and Llama3.2-3B
- Why unresolved: Larger models possess stronger pre-training priors and different memorization capacities, potentially saturating faster or maintaining plasticity differently
- What evidence would resolve it: Applying the Plasticity-Ceiling Framework to a 70B+ model to verify if optimal transition window shifts

### Open Question 3
- Question: What are the underlying theoretical mechanisms causing Synchronized SFT-RL methods to fail stability tests on standard models?
- Basis in paper: Authors note Syn-SFT-RL methods like SRFT "exhibit training instability" but offer no theoretical cause
- Why unresolved: Identifying empirical failure does not explain if cause is gradient conflict, variance explosion, or distribution shift during synchronized update
- What evidence would resolve it: Ablation study analyzing gradient interference or loss landscape geometry in synchronized vs. sequential approaches

## Limitations
- Framework assumes sigmoidal power-law scaling holds universally, which may not capture emergent capabilities or catastrophic forgetting
- Analysis focuses exclusively on mathematical reasoning tasks; generalization to other domains (code, multilingual) remains untested
- Implementation details of DAPO-dc's dynamic difficulty sampling are underspecified, potentially affecting reproducibility

## Confidence
- Sequential SFT-then-RL pipeline superiority: High (directly supported by systematic ablation studies)
- Stability deficit of synchronized approaches: High (well-demonstrated empirically)
- Minimum validation loss as plasticity predictor: Medium (strong correlation but assumes monotonicity)
- "Less is More" hypothesis: Medium (limited experimental validation)
- Framework generalizability to domains beyond mathematics: Low (untested)

## Next Checks
1. Replicate the SFT-to-RL transition timing experiment across three additional model families (beyond Qwen2.5-7B and Llama3.2-3B) to test framework generalizability
2. Test the minimum validation loss predictor when RL reward signals are intentionally misaligned with SFT data distribution
3. Conduct a systematic difficulty ablation using at least five stratified difficulty levels to validate the "Less is More" hypothesis quantitatively