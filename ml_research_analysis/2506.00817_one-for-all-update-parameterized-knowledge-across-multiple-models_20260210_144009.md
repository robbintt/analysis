---
ver: rpa2
title: 'One for All: Update Parameterized Knowledge Across Multiple Models'
arxiv_id: '2506.00817'
source_url: https://arxiv.org/abs/2506.00817
tags:
- editing
- knowledge
- edit
- arxiv
- once
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ONCE EDIT, a novel ensemble-based approach
  for updating knowledge across multiple large language models (LLMs) simultaneously.
  The core idea is to use a lightweight plug-in model as an editing module, which
  is updated with new knowledge and then integrated with target LLMs using an improved
  heterogeneous model ensemble.
---

# One for All: Update Parameterized Knowledge Across Multiple Models

## Quick Facts
- arXiv ID: 2506.00817
- Source URL: https://arxiv.org/abs/2506.00817
- Reference count: 9
- Primary result: Simultaneous multi-LLM knowledge editing via plug-in ensemble outperforms per-model editing on ZsRE and Counterfact datasets

## Executive Summary
ONCE EDIT introduces a novel ensemble-based approach for updating knowledge across multiple large language models (LLMs) simultaneously. The core innovation is using a lightweight plug-in model as an editing module that is updated with new knowledge and integrated with target LLMs using heterogeneous model ensemble. The framework introduces a dynamic weight mechanism via a [WEIGHT] token to adaptively adjust model contributions during ensemble, and an ensemble enhancement mechanism that includes search-space zero initialization and target augmentation to mitigate over-reliance on the central LLM.

## Method Summary
ONCE EDIT operates in two stages: an editing stage where a plug-in model is fine-tuned with new knowledge (including a special [WEIGHT] token for distinguishing edit-related inputs), and an ensemble stage where the plug-in and target LLMs are integrated via probability-level fusion in a shared relative representation space. The relative transfer matrices are computed using anchor words common to both models, enabling heterogeneous vocabulary integration. During inference, a dynamic weight α (predicted by the [WEIGHT] token) controls the contribution of each model, and gradient-based search decodes from the fused distribution using zero initialization to avoid LLM prior domination.

## Key Results
- Consistently outperforms existing knowledge editing methods on Llama2-7B, Mistral-7B-v0.1, and GPT-J-6B using ZsRE and Counterfact datasets
- Dynamic weight mechanism improves locality from 0.02→0.99 on Llama2-7B (ZsRE) while maintaining reliability
- Ensemble enhancement (zero init + target augmentation) improves ZsRE average from 0.71→0.91 and Counterfact from 0.43→0.68
- Requires fewer editing interventions and lower editing overhead than per-model editing approaches
- Successfully extends to larger models (Llama3-70B, Qwen2.5-7B) with diverse vocabularies

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Weight Mechanism via [WEIGHT] Token
The [WEIGHT] token enables instance-level weight adjustment between the plug-in model and the target LLM based on whether the input requires edited knowledge. During training, it learns to predict a binary label (edit-related vs. non-edit-related) through BCE loss alongside generation loss. At inference, sigmoid(logit_w(x)) produces ensemble weight α, controlling plug-in contribution. Core assumption: the model can learn to semantically distinguish inputs requiring edited knowledge from those using original knowledge. Evidence: Dynamic Weight improves locality from 0.02→0.99 on Llama2-7B (ZsRE) while maintaining reliability.

### Mechanism 2: Ensemble Enhancement via Search-space Zero Initialization
Initializing decoding search from zero rather than the LLM's output distribution prevents the central model's prior from dominating ensemble outputs. ONCE EDIT sets p_init = zeros_like(p_l), forcing gradient descent to construct decoding distribution purely from aggregated distribution P without LLM bias. Core assumption: gradient descent from zero can converge to meaningful distributions without being trapped. Evidence: Adding Ensemble Enhancement improves ZsRE average from 0.71→0.91 and Counterfact from 0.43→0.68 (three-model averages).

### Mechanism 3: Heterogeneous Transfer via Relative Representation
Anchor-word-based relative transfer matrices enable knowledge fusion across models with different vocabularies and embedding spaces. Shared tokens become anchor words, and for each model, compute R[i] = (cos(e_i, e_a1), ..., cos(e_i, e_a|A|)) capturing each word's relationship to anchors. Core assumption: relative semantic relationships are sufficiently consistent across heterogeneous models for meaningful transfer. Evidence: Migration cost is modest—equivalent to 600-1000 tokens of forward propagation per model pair.

## Foundational Learning

- **Concept: Probability-Level Ensemble**
  - Why needed here: ONCE EDIT fuses output distributions in relative space rather than logits or outputs. Understanding why probability calibration and distribution alignment matter is essential for debugging fusion failures.
  - Quick check question: Two models predict "Paris" for "France's capital"—one assigns p=0.8, another p=0.6. If their vocabularies differ (token IDs 284 vs. 1501), how do you combine these without a shared token space?

- **Concept: Knowledge Editing Constraints (Reliability, Generality, Locality)**
  - Why needed here: The framework explicitly optimizes tradeoffs among these three properties. Table 2 shows Dynamic Weight boosts locality (0.02→0.99) while Ensemble Enhancement trades ~5% locality for +20% reliability on Counterfact.
  - Quick check question: An edit changes "Biden is president" to "Trump" but also flips unrelated election facts. Which constraint failed, and which component (weight prediction or search initialization) is the likely culprit?

- **Concept: Gradient-Based Distribution Search**
  - Why needed here: The ensemble stage must translate relative-space distribution P back to LLM's absolute vocabulary via optimization. Understanding why direct sampling fails clarifies the need for this search.
  - Quick check question: Why can't we simply take argmax of P when P is in relative space (dimensions = anchor words, not vocabulary tokens)?

## Architecture Onboarding

- **Component map**:
  [Edit Data] → [Plug-in Model Fine-tuning] → [Anchor Words] → [Relative Transfer Matrix Computation] ← [Target LLM] → [Input] → [Plug-in Model] → p_s, logit_w → [Target LLM] → p_l → [Weight Predictor] → α = σ(logit_w) → [Distribution Fusion] → P = α(p_s×R_s) + (1-α)(p_l×R_l) → [Gradient Search Decoder] → p_d → [LLM Decode] → Output

- **Critical path**:
  1. Setup: Compute R_s and R_l for each plug-in–LLM pair (~10 TFLOPS, ~1000 tokens equivalent)
  2. Editing: Fine-tune plug-in model on edits with L_gen + λ·L_weight (λ=0.8; LR=1e-4)
  3. Inference: Encode input → predict α → run both models → fuse in relative space → gradient search → decode from LLM

- **Design tradeoffs**:
  | Decision | Option A | Option B | Paper's Choice |
  |----------|----------|----------|----------------|
  | Plug-in size | Larger (better capacity) | Smaller (faster editing) | Tiny-Llama (1.1B); Qwen2.5-1.5B works too |
  | Search initialization | Zero (unbiased) | LLM prior (faster) | Zero; +20% average score on Counterfact |
  | Weight mechanism | Fixed α | Learned [WEIGHT] | Learned; critical for locality (0.02→0.99) |
  | Anchor selection | Subset | All shared tokens | All shared (17K-32K anchors) |

- **Failure signatures**:
  - Locality crash (Loc. < 0.3): [WEIGHT] over-predicting α for unrelated inputs. Check weight distribution on SL samples; may need more unrelated training data or higher λ.
  - Generality gap (Gen. << Rel.): Plug-in overfit to edit phrasing. More pronounced on Counterfact than ZsRE; consider augmentation during fine-tuning.
  - Search non-convergence: Gradient descent fails to find valid distribution. Check learning rate, verify R_l and R_s are properly normalized.
  - Cross-model variance: One LLM underperforms others. Check vocabulary overlap (anchor count); GPT-J-6B has fewer anchors (17,830) vs. Llama2-7B (31,999).

- **First 3 experiments**:
  1. [WEIGHT] calibration test: Train on 50 edits + 50 unrelated samples. Plot α distribution for both sets. Target: clear separation (edit-related α > 0.7, unrelated α < 0.3).
  2. Single-LLM baseline: Run ONCE EDIT with Tiny-Llama + Llama2-7B only. Measure Rel./Gen./Loc. on 100 ZsRE samples.
  3. Vocabulary overlap ablation: Test with Llama2-7B (31,999 anchors) vs. GPT-J-6B (17,830 anchors). If GPT-J underperforms by >10%, investigate whether reduced anchor coverage is the cause.

## Open Questions the Paper Calls Out

- Can the ONCE EDIT framework be effectively adapted to handle sequential and multi-hop knowledge editing tasks? The authors state experiments "focused exclusively on the batch editing setting and did not explore more complex scenarios, such as sequential editing or multi-hop editing tasks."

- Does integrating advanced locate-then-edit techniques into the plug-in model improve the framework's performance compared to direct fine-tuning? The paper notes that the study "primarily adopted a direct fine-tuning approach" and suggests future work could "enhance the plug-in model by integrating more advanced editing techniques."

- How can the inference overhead associated with the plug-in model be minimized to facilitate low-latency deployment? The authors acknowledge that "ONCE EDIT inevitably incurs additional overhead due to the inclusion of the plug-in model" despite its small size.

## Limitations

- Hyperparameter sensitivity: Critical hyperparameters for plug-in model fine-tuning (batch size, epoch count, optimizer settings) and gradient-based decoding search (loss function, step count) are not disclosed, preventing faithful reproduction.

- Dataset composition gaps: The unrelated-knowledge set used during [WEIGHT] training is described only qualitatively, making it unclear whether the weight predictor's performance generalizes beyond the specific dataset construction used in experiments.

- Computational burden of heterogeneous transfer: While migration costs are modest per pair, the one-time computation of relative transfer matrices scales quadratically with the number of models, potentially becoming prohibitive for production deployments with many models.

## Confidence

- **High Confidence**: The core contribution of simultaneous multi-LLM knowledge editing via a shared plug-in model is technically sound and experimental results demonstrate consistent improvements over baselines.
- **Medium Confidence**: The effectiveness of the [WEIGHT] token mechanism for distinguishing edit-related from non-edit-related instances is supported by locality improvements, but relies on binary classification generalizing to unseen edit types without explicit similarity matching.
- **Low Confidence**: The ensemble enhancement mechanism's claim that search-space zero initialization prevents LLM prior domination is theoretically plausible but lacks direct empirical comparison to alternative initialization strategies within the paper's experiments.

## Next Checks

1. **Ablation Study on Unrelated Sample Balance**: Systematically vary the ratio of unrelated to edit samples in the [WEIGHT] training set (0%, 50%, 100% unrelated) and measure the resulting Locality scores to quantify sensitivity to dataset composition.

2. **Cross-Edit Generalization Test**: Evaluate ONCE EDIT on a held-out subset of ZsRE/Counterfact containing edit types not present in training (e.g., rare entities, complex reasoning edits) and measure whether the [WEIGHT] token maintains >0.7 Locality on these unseen edit patterns.

3. **Search Initialization Sensitivity Analysis**: Compare ONCE EDIT's zero initialization against LLM-prior initialization across multiple learning rates (1e-3, 1e-4, 1e-5) on a fixed validation set to directly test whether the claimed "preventing LLM domination" benefit is reproducible.