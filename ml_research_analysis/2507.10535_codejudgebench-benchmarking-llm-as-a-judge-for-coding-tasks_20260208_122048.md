---
ver: rpa2
title: 'CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks'
arxiv_id: '2507.10535'
source_url: https://arxiv.org/abs/2507.10535
tags:
- freq
- code
- llm-as-a-judge
- coding
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CodeJudgeBench introduces a novel benchmark to evaluate the performance
  of LLM-as-a-Judge models on coding tasks, addressing the gap in existing benchmarks
  that are limited in scope and complexity. The benchmark includes three critical
  coding tasks: code generation, code repair, and unit test generation, with a total
  of 5,352 curated pairs.'
---

# CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks

## Quick Facts
- arXiv ID: 2507.10535
- Source URL: https://arxiv.org/abs/2507.10535
- Reference count: 40
- Primary result: CodeJudgeBench reveals that thinking models significantly outperform non-thinking models on coding tasks, with even smaller models surpassing larger specialized judge models, while highlighting substantial randomness in judgments.

## Executive Summary
CodeJudgeBench introduces a comprehensive benchmark for evaluating LLM-as-a-Judge models on three critical coding tasks: code generation, code repair, and unit test generation. The benchmark addresses limitations in existing evaluations by providing 5,352 curated pairwise comparisons across these tasks. Through extensive testing of 26 models, the study reveals that recent thinking models substantially outperform non-thinking models, with even smaller models like Qwen3-8B surpassing larger specialized judge models. However, the research also identifies significant limitations in current LLM-as-a-Judge systems, including position bias and inconsistent judgments when evaluating responses from different LLMs.

## Method Summary
CodeJudgeBench evaluates LLM-as-a-Judge performance using pairwise comparisons on three coding tasks: code generation (selecting correct code from two candidates), code repair (identifying correct fixes for erroneous code), and unit test generation (choosing correct test cases). The benchmark includes 5,352 curated pairs from LiveCodeBench-v6, stratified by difficulty based on judge consensus. Each pair is evaluated twice with swapped response positions, using a JudgeBench-style prompt where models generate reference answers before comparing. The study measures accuracy, position bias effects, and cross-model generalization, finding that raw full responses outperform code-only inputs and that thinking models consistently surpass non-thinking models.

## Key Results
- Recent thinking models significantly outperform non-thinking models on all coding tasks
- Even smaller models like Qwen3-8B surpass larger specialized judge models up to 70B in size
- All models exhibit significant randomness in judgments, with up to 14% accuracy variation based on response ordering
- Pairwise comparison outperforms point-wise evaluation, with raw full responses showing better performance than code-only inputs

## Why This Works (Mechanism)
CodeJudgeBench works by systematically evaluating LLM-as-a-Judge models through pairwise comparisons across three fundamental coding tasks. The benchmark's effectiveness stems from its use of curated real-world coding problems from LiveCodeBench, standardized evaluation protocols with position-swapped comparisons, and comprehensive coverage of different difficulty levels. By measuring both accuracy and robustness metrics like position bias, the benchmark provides a nuanced understanding of judge model performance beyond simple accuracy scores.

## Foundational Learning

**Pairwise Comparison**: The methodology where models compare two responses to select the better one, needed for understanding relative performance evaluation; quick check: verify both position A/B evaluations are performed for each sample.

**Position Bias**: The phenomenon where response ordering affects judgment outcomes, needed for assessing judge robustness; quick check: measure accuracy differences between Correct@A and Correct@B conditions.

**Thinking vs Non-Thinking Models**: The distinction between models that use deliberate reasoning versus those that generate responses directly, needed for understanding performance differences; quick check: compare Qwen3-8B (thinking) against similar-sized non-thinking models.

**Code-Only vs Raw Response Evaluation**: The comparison of judging performance when using only extracted code versus full model outputs including comments and reasoning, needed for understanding input representation effects; quick check: measure performance difference between raw vs code-only inputs.

## Architecture Onboarding

Component map: Data Source (LiveCodeBench) -> Pairwise Evaluation Pipeline -> Position Bias Analysis -> Difficulty Stratification -> Cross-Model Comparison

Critical path: Dataset preparation → Pairwise evaluation with JudgeBench prompt → Position swap validation → Accuracy and bias measurement

Design tradeoffs: Raw responses vs code-only inputs (better performance vs cleaner evaluation), pairwise vs point-wise scoring (robustness vs granularity), single vs multiple position swaps (efficiency vs bias detection)

Failure signatures: Large accuracy swings (>10%) between position A and B, poor cross-model generalization, low performance on hard difficulty tasks

Three first experiments:
1. Run baseline pairwise evaluation on a small subset to verify JudgeBench prompt implementation
2. Test position bias by comparing accuracy with swapped response orderings
3. Compare raw response evaluation versus code-only input performance

## Open Questions the Paper Calls Out

**Open Question 1**: How can LLM-as-a-Judge models be trained or prompted to eliminate position bias and ensure consistent evaluation regardless of response ordering? The paper identifies significant position bias (up to 14% accuracy variation) but doesn't propose solutions, suggesting this requires dedicated research into bias mitigation techniques.

**Open Question 2**: Can fine-tuning thinking models specifically for judgment tasks using high-quality code data enable them to surpass general-purpose thinking models? The paper observes specialized judge models underperforming compared to general thinking models, hypothesizing insufficient code-related training data, but hasn't tested specialized fine-tuning approaches.

**Open Question 3**: Why does providing the full raw model response lead to better judging performance compared to code-only inputs? The paper shows raw responses improve performance but doesn't determine if this stems from reasoning traces or stylistic markers, requiring ablation studies to isolate the mechanism.

**Open Question 4**: How does the performance of LLM-as-a-Judge generalize to software engineering tasks beyond code generation, repair, and unit testing? The current benchmark's limitation to three tasks suggests the need for expansion to areas like code summarization and refactoring to assess broader applicability.

## Limitations
- Exact JudgeBench prompt format not fully specified, potentially affecting reproducibility
- No detailed sampling parameters (temperature, max tokens) provided for different model types
- Position bias findings may not generalize to all coding scenarios or prompt engineering approaches
- Performance rankings based on specific tasks may not extend to all coding problem types

## Confidence

High confidence: Benchmark construction methodology, dataset size (5,352 pairs), and basic evaluation framework are well-specified and reproducible

Medium confidence: Relative performance rankings between different model families are supported by data, though exact numerical results may vary with implementation details

Medium confidence: Position bias findings are well-demonstrated within this benchmark but may vary with different prompt engineering approaches

## Next Checks

1. Implement the JudgeBench-style pairwise prompt exactly as specified and compare results with the benchmark's reported accuracy rates across all three tasks

2. Test the position bias effect systematically by running evaluations with multiple response order swaps to quantify the full extent of ordering effects on judgment consistency

3. Compare raw response evaluation versus code-only evaluation on a subset of the benchmark to verify the claimed 2-6% performance improvement when retaining full model outputs including comments