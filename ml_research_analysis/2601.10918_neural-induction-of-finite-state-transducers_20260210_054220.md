---
ver: rpa2
title: Neural Induction of Finite-State Transducers
arxiv_id: '2601.10918'
source_url: https://arxiv.org/abs/2601.10918
tags:
- state
- input
- transducers
- sink
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for inducing finite-state transducers
  (FSTs) from data using recurrent neural networks (RNNs) as intermediate models.
  The approach addresses the challenge of constructing accurate FSTs automatically
  by leveraging the theoretical correspondence between RNN hidden state dynamics and
  finite-state automata.
---

# Neural Induction of Finite-State Transducers

## Quick Facts
- arXiv ID: 2601.10918
- Source URL: https://arxiv.org/abs/2601.10918
- Reference count: 40
- Primary result: Induced FSTs outperform classical algorithms like OSTIA by up to 87% accuracy, achieving competitive performance to expert-crafted transducers on morphological inflection, G2P, and historical normalization tasks.

## Executive Summary
This paper presents a method for automatically inducing finite-state transducers (FSTs) from data using recurrent neural networks (RNNs) as intermediate models. The approach addresses the challenge of constructing accurate FSTs by leveraging the theoretical correspondence between RNN hidden state dynamics and finite-state automata. The method involves training RNNs with a transduction objective and spectral norm penalty, collecting hidden states from both training and synthetic examples, clustering these states, and applying a state-splitting algorithm to resolve non-determinism. The induced FSTs are evaluated on morphological inflection, grapheme-to-phoneme conversion, and historical normalization tasks, showing substantial improvements over classical algorithms and achieving competitive performance to expert-crafted transducers in many cases.

## Method Summary
The method extracts FSTs through a multi-stage pipeline. First, input-output string pairs are aligned using CRPAlign (Chinese Restaurant Process Alignment). A single-layer Elman RNN is then trained with a transduction objective—predicting output symbols from concatenated hidden states and next input embeddings—while a spectral norm penalty encourages finite-state-like dynamics. Hidden states are collected from both training and synthetic strings (generated via tag-swapping or n-gram sampling). K-means clustering groups these states into FST states, and a state-splitting algorithm with SVM or logistic regression resolves non-determinism. The extracted FSTs are minimized using Hopcroft's algorithm. The entire process requires extensive hyperparameter tuning, with 100+ FST extraction runs per dataset optimizing validation F1.

## Key Results
- Achieved up to 87% accuracy improvement over OSTIA on morphological inflection tasks
- Outperformed minimum-edit-distance alignment on G2P tasks by 16% accuracy
- FSTs extracted via this method are often larger than expert-crafted versions (e.g., 280 vs 30 states in one case)

## Why This Works (Mechanism)

### Mechanism 1
The transduction objective and spectral norm penalty force RNN hidden states to cluster into discrete FST states. The transduction objective (predicting output symbol from concatenated hidden state + next input embedding) organizes the continuous state space into separable regions. The spectral norm penalty constrains the Lipschitz constant, encouraging fixed-point attractor dynamics that resemble finite-state machines. This works best for tasks with left-to-right dependencies.

### Mechanism 2
Synthetic data generation expands transition coverage for robust FST extraction. Training data alone provides incomplete coverage of possible input strings. By generating synthetic strings and collecting their hidden states, the method observes transitions that would otherwise be missing, enabling proper generalization. This is particularly important for tasks with theoretically infinite domains.

### Mechanism 3
State-splitting with classifiers resolves non-determinism from clustering errors. K-means clustering may merge activations that should belong to different FST states, creating non-deterministic transitions. The splitting algorithm identifies conflicting transitions exceeding threshold λ_trans, fits SVM or logistic regression to separate the cluster, and requeues affected upstream states.

## Foundational Learning

- **Finite-State Transducers (FSTs)**: Essential for understanding the output format and determinism requirements. Quick check: Given FST states Q={0,1,2}, if state 0 has transitions (a→x, 1) and (a→y, 2), what principle is violated?
- **Alignment for Sequence Pairs**: Required for knowing which input symbol maps to which output symbol. Quick check: For input "run" → output "runs", what alignment issue arises and how does epsilon insertion resolve it?
- **Spectral Norm and Lipschitz Continuity**: The spectral norm penalty constrains how much the hidden state can change per timestep, encouraging discrete-like state transitions. Quick check: If a function has Lipschitz constant K=0.1 vs K=10.0, which is more likely to exhibit finite-state-like behavior with distinct attractor regions?

## Architecture Onboarding

- **Component map**: CRPAlign -> Elman RNN -> State Collector -> K-means Clusterer -> Splitting Algorithm -> FST Minimizer
- **Critical path**: Alignment quality → RNN hidden state separability → Clustering coherence → Splitting completeness → FST accuracy. Errors propagate between stages and cannot be optimized end-to-end.
- **Design tradeoffs**: Elman RNN vs LSTM (simpler dynamics ease extraction but lack bidirectional modeling), more clusters (higher precision but risk overfitting), higher λ_trans (tolerates noise but may leave conflicts unresolved)
- **Failure signatures**: Low accuracy on G2P/normalization with right-context dependencies (unidirectional RNN limitation), extracted FST has 10× more states than expert version (clustering too fine or splitting cascade), OSTIA timeout (*) in tables (dataset too large for classical algorithm)
- **First 3 experiments**: 1) Replicate on single inflection dataset with default hyperparameters, 2) Ablate synthetic data generation and measure accuracy drop, 3) Replace CRPAlign with minimum-edit-distance alignment on G2P dataset

## Open Questions the Paper Calls Out

### Open Question 1
Can the induction framework be extended to learn bimachines from bidirectional models to effectively handle tasks with right-side dependencies? The authors identify this as a solution for tasks like G2P that require future context, noting that replacing the unidirectional RNN with a bidirectional RNN would allow extracting transducers via bimachine learning.

### Open Question 2
Can the method be adapted to extract Weighted Finite-State Transducers (WFSTs) by utilizing the probability distributions from the RNN? The conclusion states the approach could be adapted to produce weighted transducers by creating probability distributions for transitions, but the current implementation only produces unweighted FSTs.

### Open Question 3
How can the state extraction process be refined to produce minimal, compact FSTs comparable in size to those hand-crafted by human experts? The authors note extracted FSTs tend to be far larger than expert-crafted transducers (280 vs 30 states), suggesting opportunities for further optimization.

## Limitations
- The method cannot handle tasks requiring bidirectional context due to unidirectional RNN architecture, causing performance drops on G2P tasks with right-dependent phonological rules
- Extracted FSTs are often significantly larger than expert-crafted versions (sometimes by orders of magnitude), indicating potential inefficiencies in the clustering and splitting approach
- Extensive hyperparameter tuning is required (100+ FST extraction runs per dataset), making the method computationally expensive and difficult to apply to new domains

## Confidence
- **High Confidence**: Empirical improvements over OSTIA (up to 87% accuracy gains), competitive performance with expert-crafted FSTs on inflection tasks, general framework of using RNNs as intermediate models
- **Medium Confidence**: Mechanism by which spectral norm regularization encourages finite-state-like dynamics, necessity of synthetic data for robust extraction, state-splitting algorithm's effectiveness
- **Low Confidence**: Claims about achieving "optimal" alignment quality with CRPAlign versus alternatives, theoretical justification for specific synthetic data generation strategies, whether extracted FSTs are truly minimal

## Next Checks
1. **Architecture Limitation Test**: Design a controlled experiment using bidirectional RNNs (LSTM or GRU) for tasks with known right-context dependencies (like English G2P with final devoicing). Compare performance against the unidirectional Elman RNN to quantify the bidirectional modeling limitation.

2. **Synthetic Data Ablation with Controlled Generation**: For inflection tasks, systematically vary synthetic string generation parameters (number of synthetic examples, tag combination strategies) and measure the trade-off between extraction coverage and FST complexity. Include a variant using only real training data to quantify the synthetic data contribution precisely.

3. **Alignment Quality Impact Study**: Replace CRPAlign with minimum-edit-distance alignment on a subset of tasks and measure the degradation in FST accuracy. Additionally, implement a simpler Bayesian alignment approach to assess whether the complex CRPAlign is necessary or if simpler methods suffice for transducer extraction.