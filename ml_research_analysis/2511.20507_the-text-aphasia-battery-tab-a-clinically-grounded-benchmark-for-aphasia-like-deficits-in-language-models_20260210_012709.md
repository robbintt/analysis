---
ver: rpa2
title: 'The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like
  Deficits in Language Models'
arxiv_id: '2511.20507'
source_url: https://arxiv.org/abs/2511.20507
tags:
- aphasia
- language
- features
- clinical
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The Text Aphasia Battery (TAB) addresses the challenge of assessing
  language model impairments in a clinically-grounded manner by adapting established
  aphasia assessment frameworks for text-only evaluation. The benchmark comprises
  four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition,
  which evaluate complementary aspects of linguistic function using binary scoring
  criteria derived from clinical aphasia research.'
---

# The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models

## Quick Facts
- arXiv ID: 2511.20507
- Source URL: https://arxiv.org/abs/2511.20507
- Reference count: 39
- Primary result: Automated evaluation of aphasic features achieves prevalence-weighted Cohen's Kappa of 0.255 for model-consensus agreement, comparable to human-human agreement of 0.286

## Executive Summary
The Text Aphasia Battery (TAB) addresses the challenge of assessing language model impairments in a clinically-grounded manner by adapting established aphasia assessment frameworks for text-only evaluation. The benchmark comprises four subtests—Connected Text, Word Comprehension, Sentence Comprehension, and Repetition—which evaluate complementary aspects of linguistic function using binary scoring criteria derived from clinical aphasia research. To enable large-scale use, an automated evaluation protocol using Gemini 2.5 Flash was validated against expert human annotators, achieving comparable reliability while maintaining clinical relevance through its grounding in the Quick Aphasia Battery and APROCSA frameworks.

## Method Summary
TAB adapts clinical aphasia assessment frameworks (Quick Aphasia Battery and APROCSA) into a text-only benchmark with four subtests: Connected Text (19 binary features), Word Comprehension (6-way forced choice), Sentence Comprehension (Yes/No items), and Repetition (exact match). The automated evaluation uses few-shot prompting of Gemini 2.5 Flash with feature definitions and two annotated examples to detect aphasic features in Connected Text responses. Validation on 561 samples (306 from AphasiaBank, 255 from lesioned models) achieved prevalence-weighted Cohen's Kappa of 0.255 for model-consensus agreement versus 0.286 for human-human agreement, demonstrating comparable reliability. All subtests use standardized scoring with exact string matching for objective components.

## Key Results
- Automated evaluation of aphasic features achieves prevalence-weighted Cohen's Kappa of 0.255 for model-consensus agreement
- Human-human agreement on the same task achieves weighted Cohen's Kappa of 0.286
- The benchmark successfully translates clinical aphasia assessment frameworks into text-only evaluation while maintaining clinical relevance
- Binary scoring criteria derived from APROCSA's 27 features are successfully reduced to 19 binary features for automated detection

## Why This Works (Mechanism)

### Mechanism 1: Feature-Based Linguistic Profiling via APROCSA Adaptation
Binary classification of 19 clinically-grounded features can detect aphasic-like language breakdown in LLM outputs. The TAB adapts APROCSA from 27 features on 5-point scales to 19 binary features targeting lexical retrieval, morphosyntax, fluency, and coherence. Core assumption: text-identifiable signatures of aphasia (anomia, paragrammatism, perseverations) survive transcription. Evidence: binary scoring criteria derived from clinical aphasia research; LLMs exhibit clinically-aligned patterns under targeted lesions. Break condition: if deficits manifest primarily through prosody, gesture, or real-time self-correction, the benchmark may miss them.

### Mechanism 2: In-Context Learning Enables Scalable Annotation
Few-shot prompting of Gemini 2.5 Flash achieves human-comparable reliability for detecting aphasic features. The evaluating LLM receives feature definitions with examples, two annotated transcripts as ground-truth demonstrations, and structured JSON output instructions. Core assumption: judge model's linguistic knowledge transfers to clinical feature detection via in-context examples. Evidence: prevalence-weighted Cohen's Kappa of 0.255 for model-consensus agreement compared to 0.286 for human-human agreement; few-shot prompting enables consistent feature identification across large datasets. Break condition: if judge model systematically over- or under-detects specific features, accuracy degrades.

### Mechanism 3: Prevalence Weighting Corrects Skewed Feature Distribution
Unweighted agreement metrics are misleading when features have highly variable base rates; prevalence weighting restores meaningful comparison. Some features never occurred in validation data (κ undefined for humans, artificially perfect for models), while others appeared in 34% of samples. Prevalence weighting ensures clinically important features contribute proportionally to aggregate scores. Evidence: unweighted average κ=0.061 vs model κ=0.794 (misleading); weighted κ=0.286 vs model κ=0.255 (comparable). Core assumption: features that occur in real aphasic language are more important than absent features. Break condition: if prevalence differs dramatically between validation data and deployment population, weighted κ may not generalize.

## Foundational Learning

- **Concept: APROCSA (Auditory-Perceptual Rating of Connected Speech in Aphasia)**
  - Why needed: Understanding the 19 features and their clinical definitions is essential for interpreting TAB scores and debugging false positives.
  - Quick check: Can you distinguish between semantic paraphasia (substituting a related word) and neologism (invented word)?

- **Concept: Cohen's Kappa and Prevalence Weighting**
  - Why needed: The validation methodology relies on understanding why raw agreement fails and how weighting corrects for it.
  - Quick check: Why does κ approach 0 when a feature never occurs, even if raters agree on its absence?

- **Concept: Modality Constraint in Clinical-to-Computational Adaptation**
  - Why needed: TAB's design principle of eliminating auditory, visual, and motor dependencies is what makes it LLM-appropriate but also limits its clinical equivalence.
  - Quick check: Which aspects of aphasia cannot be assessed through text alone, and how does TAB handle this?

## Architecture Onboarding

- **Component map:**
  TAB Benchmark -> 4 Subtests (Connected Text, Word Comprehension, Sentence Comprehension, Repetition) -> Automated evaluation via Gemini 2.5 Flash with few-shot prompting

- **Critical path:**
  1. Administer all 4 subtests to target model with instruction-following prompts
  2. For Subtests 2-4: Apply deterministic scoring (string matching)
  3. For Subtest 1: Route responses through Gemini 2.5 Flash with provided prompt template
  4. Aggregate feature scores; compare against baseline or lesioned variants

- **Design tradeoffs:**
  - Binary vs. graded scoring: Binary enables automation but loses severity granularity
  - 5 items per subtest: Prioritizes speed over comprehensive coverage; may produce ceiling effects
  - Single judge model: Scalable but introduces model-specific bias; paper acknowledges need for multi-judge validation

- **Failure signatures:**
  - High false positive rate on "phonemic paraphasias" in text (orthographic errors may not reflect sound-level processing)
  - Models may refuse/persona-break on Connected Text prompts (mitigated by conversational framing)
  - Repetition failures may reflect instruction-following gaps rather than linguistic deficits

- **First 3 experiments:**
  1. **Baseline profiling**: Run TAB on an unmodified instruction-tuned model (e.g., Llama-3-8B-Instruct) to establish feature prevalence norms.
  2. **Targeted lesioning**: Ablate attention heads in specific layers and re-run TAB to test whether different components produce distinct aphasic profiles (corroborating corpus paper on component-level lesioning).
  3. **Judge robustness check**: Compare Gemini 2.5 Flash vs. Claude/GPT-4o as evaluators on a held-out subset to bound judge-specific variance.

## Open Questions the Paper Calls Out
None

## Limitations
- Text-only modality cannot assess prosody, timing, and self-corrections that are clinically significant aspects of aphasia
- Automated evaluation relies on a single judge model (Gemini 2.5 Flash), introducing model-specific biases not fully characterized
- Five-item subtests may produce ceiling effects, particularly for well-trained models, limiting sensitivity to subtle impairments
- Binary scoring loses severity granularity present in clinical 5-point APROCSA scales

## Confidence
- **High confidence**: Benchmark architecture and scoring methodology are well-specified; prevalence-weighted Kappa validation is sound
- **Medium confidence**: Clinical feature definitions are accurate but binary translation may lose clinically relevant severity information
- **Low confidence**: Generalizability of judge model performance across different LLM architectures and impact of judge model choice remain uncertain

## Next Checks
1. **Judge model robustness test**: Run the same TAB corpus through multiple judge models (Claude, GPT-4o, open-source alternatives) to quantify evaluator-dependent variance and establish confidence intervals.
2. **Severity gradient validation**: Test whether TAB scores correlate with known degrees of language model degradation (e.g., progressive attention head ablation, scaling laws) to confirm the benchmark captures meaningful severity gradients.
3. **Clinical validity extension**: Design a small-scale study comparing TAB scores with expert human ratings on a subset of LLM outputs to assess whether the automated protocol captures clinically meaningful patterns beyond the initial validation set.