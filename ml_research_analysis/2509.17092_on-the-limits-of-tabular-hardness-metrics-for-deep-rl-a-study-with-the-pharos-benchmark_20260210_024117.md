---
ver: rpa2
title: 'On the Limits of Tabular Hardness Metrics for Deep RL: A Study with the Pharos
  Benchmark'
arxiv_id: '2509.17092'
source_url: https://arxiv.org/abs/2509.17092
tags:
- state
- tabular
- hardness
- learning
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether tabular hardness metrics can be
  adapted to guide non-tabular benchmarking in deep RL. While tabular settings benefit
  from well-understood hardness measures like MDP diameter and suboptimality gaps,
  deep RL lacks a practically applicable theory of hardness.
---

# On the Limits of Tabular Hardness Metrics for Deep RL: A Study with the Pharos Benchmark

## Quick Facts
- arXiv ID: 2509.17092
- Source URL: https://arxiv.org/abs/2509.17092
- Reference count: 40
- Primary result: Tabular hardness metrics poorly predict deep RL agent performance; representation hardness dominates non-tabular environment difficulty

## Executive Summary
This paper investigates whether classical tabular hardness metrics (diameter, suboptimality gaps) can guide benchmarking in deep RL. Through extensive experiments using the new pharos library, the authors demonstrate that these metrics fail to predict deep RL performance, with representation hardness (the challenge of learning from different observation types) being the dominant factor. The same underlying MDP can be vastly easier or harder depending on whether agents receive state vectors or pixel-based observations. The study concludes that new, representation-aware hardness measures are urgently needed for deep RL benchmarking.

## Method Summary
The authors introduce pharos, a library for principled RL benchmarking that allows systematic control over both environment structure and agent representations. They systematically construct tabular MDPs and compute hardness metrics (diameter, suboptimality gaps) using state space enumeration. Deep RL agents (DQN and PPO) are trained on the same underlying MDPs with different observation functions (vector vs. image). Performance is measured via cumulative regret against optimal values. Linear regression models correlate tabular metrics with agent regret across different environment classes and representation types.

## Key Results
- Tabular hardness metrics predict DQN regret poorly (R²=0.09) across heterogeneous environments and representations
- Representation hardness dominates non-tabular environment difficulty, with the same MDP being vastly easier or harder depending on observation type
- Environment-specific models improve prediction (R²=0.65) but show inconsistent significance of predictors, confirming no universal hardness recipe exists
- DQN regret varies significantly with representation: vector observations better for simple_grid, image observations better for breakout

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representation hardness dominates non-tabular environment difficulty, often more than structural MDP properties.
- Mechanism: The same underlying MDP produces vastly different learning curves depending on whether the agent receives compact state vectors or pixel-based observations. Neural network inductive biases interact differently with each representation type—CNNs produce smoother function approximations suited to images, while MLPs on tabular-like vectors can struggle.
- Core assumption: The difficulty gap stems from representation learning challenges rather than fundamental changes to the transition dynamics.
- Evidence anchors:
  - [abstract] "The same underlying MDP can pose vastly different challenges depending on whether the agent receives state vectors or pixel-based observations."
  - [Section 4.2] Figure 3 shows DQN incurring higher regret with vector observations in breakout/freeway (points above diagonal) but higher regret with image observations in simple_grid (points below diagonal).
  - [corpus] Weak direct support; related work on tabular representations (SE-VAE) focuses on disentanglement, not RL hardness.
- Break condition: If agent performance becomes equivalent across representation types when given sufficient compute or architectural improvements, representation hardness may be a tractable engineering problem rather than a fundamental limitation.

### Mechanism 2
- Claim: Tabular hardness metrics (diameter, suboptimality gaps) poorly predict deep RL performance across heterogeneous environments.
- Mechanism: Linear models using tabular metrics achieve R²=0.09 when predicting DQN regret across all environment classes and representations. The metrics capture visitation and estimation complexity in tabular settings but ignore representation learning, which dominates non-tabular difficulty.
- Core assumption: The linear modeling approach is sufficient to detect meaningful relationships if they exist.
- Evidence anchors:
  - [Section 4.3.1] "The R² score of this model is 0.09... tabular hardness measures are not able to capture the hardness of the non-tabular task in a way that generalizes."
  - [Section 4.3.2] Representation-specific models show R²=0.6 for vector observations vs. R²=0.3 for image observations.
  - [corpus] No directly comparable studies on hardness metric generalization to deep RL.
- Break condition: If non-linear models or different metric combinations substantially improve prediction (e.g., R²>0.7), tabular metrics may contain more signal than linear analysis reveals.

### Mechanism 3
- Claim: Environment-specific models capture local regularities but no universal hardness recipe exists.
- Mechanism: Fitting separate linear models per environment class improves aggregate R² to 0.65, but significant predictors vary: frozen_lake relies on tabular metrics (representation not significant), freeway relies almost entirely on representation (tabular metrics not significant), breakout shows joint influence.
- Core assumption: Environment classes are the right granularity for analysis rather than finer-grained instance features.
- Evidence anchors:
  - [Section 4.3.3] "For freeway (R²=0.67), only the representation coefficient was significant, suggesting that for this hard-exploration task, the challenge is almost entirely representational."
  - [Section 4.3.3] "Overall, these environment-specific models (aggregate R²=0.65) provided a much better fit... but the inconsistent significance of predictors confirms that there is no universal recipe."
  - [corpus] MultiTab benchmark (arxiv:2505.14312) advocates multi-dimensional evaluation in tabular domains, supporting granularity arguments.
- Break condition: If a unified non-linear model or representation-aware metric achieves consistent high R² across classes, the environment-specific finding may reflect linear model limitations.

## Foundational Learning

- Concept: **Tabular vs. non-tabular RL distinction**
  - Why needed here: The paper's central argument hinges on why metrics designed for tabular settings (where each state-action pair has independent value estimates) fail in non-tabular settings (where function approximation creates relationships between unrelated states).
  - Quick check question: Can you explain why knowing Q(s,a) provides no information about Q(s',a') in a tree-structured episodic MDP, but might in a low-rank MDP with shared features?

- Concept: **Inductive bias in neural architectures**
  - Why needed here: The representation hardness finding depends on understanding that CNNs vs. MLPs have different implicit assumptions about input structure (spatial locality/smoothness vs. arbitrary coordinate encoding).
  - Quick check question: Why might a CNN trained on Freeway images outperform an MLP on the same game's RAM state vector, despite the RAM containing strictly more information?

- Concept: **Visitation complexity vs. estimation complexity**
  - Why needed here: The paper organizes tabular hardness into these two categories (diameter for visitation, suboptimality gaps for estimation), and argues non-tabular settings add a third: representation complexity.
  - Quick check question: Would increasing an MDP's diameter always increase the regret of an optimal tabular agent? Would it always increase a DQN agent's regret?

## Architecture Onboarding

- Component map:
  - **State Space Builder** -> **State Storage Matrix** -> **Transition Matrix** -> **Hardness Metrics**
  - **Representation Function** (on-demand) -> **Agent Training**
  - **Reward Function** (computed post-hoc) -> **Optimal Value Calculation**

- Critical path:
  1. Define environment class with `transition_function(s, action)`, `is_terminal(s)`, and `starting_state`.
  2. Run state space builder (hours for millions of states; memory bottleneck is hash map).
  3. Compute hardness metrics on the tabular representation.
  4. Train deep RL agents with alternative observation functions (image vs. vector) on same underlying MDP.
  5. Correlate tabular metrics with agent regret using linear or custom models.

- Design tradeoffs:
  - **On-disk vs. in-memory state storage**: Pharos trades IO speed for scalability; 100M states require 30GB+ in-memory hash maps, necessitating RocksDB.
  - **On-demand vs. pre-computed representations**: Images computed during rollouts to save 3TB+ storage, but slows training slightly.
  - **Deterministic-only transitions**: Current implementation assumes deterministic dynamics; stochastic extensions (sticky actions) increase value function computation from |S|-vector to |S|×|A|-matrix.

- Failure signatures:
  - **Hash map memory overflow**: State tuples as Python objects consume ~300 bytes each; exceeding available RAM crashes the builder. Mitigation: ensure RocksDB backend is configured.
  - **Metric-policy mismatch for sticky actions**: Standard value iteration assumes Markov property; sticky actions break this by making next-state depend on previous action, requiring augmented state space.
  - **Representation-regret inversion**: In simple_grid, images cause higher regret than vectors; in breakout, vectors cause higher regret. If you see consistent patterns opposite to expectations, check whether your architecture's inductive bias matches the observation type.

- First 3 experiments:
  1. **Sanity check**: Run tabular Q-learning and DQN on simple_grid (4×4) with both observation types. Tabular should converge; DQN should match Figure 2 patterns (vector better than image). If not, check reward scaling and exploration hyperparameters.
  2. **Representation ablation**: Train DQN on freeway with systematically varied image complexity (1-channel vs. 3-channel, different resolutions). Plot regret vs. representation dimensionality to verify representation hardness scales with observation complexity rather than MDP diameter.
  3. **Metric correlation replication**: Fit the linear model from Eq. 5 on your collected data. If R²>0.3 across all conditions, your environment distribution may differ from the paper's (check class balance and parameter ranges).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we develop tractable, representation-aware hardness measures that accurately predict deep RL agent performance?
- Basis in paper: [explicit] The Conclusion highlights the "urgent need for new, representation-aware hardness measures" because tabular metrics are poor predictors when representation learning is a factor.
- Why unresolved: Representation hardness depends on the interplay between specific neural network architectures and observation spaces (e.g., images vs. vectors), which current theory does not quantify.
- What evidence would resolve it: A new metric that correlates significantly better with Deep RL performance on image-based tasks than tabular metrics like diameter or suboptimality gaps.

### Open Question 2
- Question: Is it possible to create computationally tractable approximations of theoretical measures like the Eluder dimension for complex environments?
- Basis in paper: [explicit] Section 2.4 lists developing "practical approximations" as a key open problem, noting that exact computation is often intractable for high-dimensional problems.
- Why unresolved: These measures currently require assumptions that are restrictive or scale poorly with state space size.
- What evidence would resolve it: An algorithm that efficiently estimates these dimensions for standard deep RL benchmarks without relying on tabular state enumeration.

### Open Question 3
- Question: Do the limitations of tabular metrics generalize across a broader spectrum of agents and environment classes?
- Basis in paper: [inferred] The experiments section notes the analysis focuses on DQN and PPO with a limited set of environment classes, acknowledging that findings may not be universal.
- Why unresolved: Different agents utilize different function approximators (e.g., transformers vs. CNNs) which possess distinct inductive biases that may interact differently with environment hardness.
- What evidence would resolve it: A comprehensive study using `pharos` or similar tools to evaluate diverse agent architectures, showing that the dominance of representation hardness is consistent.

## Limitations

- The analysis assumes linear modeling is sufficient to detect relationships between tabular metrics and deep RL performance
- Results are based on a limited set of environment classes and agent architectures (DQN, PPO), limiting generalizability
- The pharos library is newly introduced and its computational requirements (particularly memory for state space enumeration) may limit practical adoption

## Confidence

- **High Confidence**: Tabular hardness metrics (diameter, suboptimality gaps) poorly predict deep RL performance when using linear models (R²=0.09)
- **Medium Confidence**: Representation hardness is the dominant factor in non-tabular environment difficulty, though this assumes current CNN/MLP architectures are optimal
- **Low Confidence**: The environment-specific finding that no universal hardness recipe exists may reflect linear model limitations rather than fundamental reality

## Next Checks

1. **Model Complexity Test**: Apply non-linear regression (e.g., random forests, neural nets) to the same dataset to verify whether tabular metrics contain hidden representation-aware signal
2. **Architecture Ablation**: Systematically vary CNN/MLP architectures (depth, width, regularization) on each observation type to determine if representation hardness is architecture-bound
3. **Stochastic Extension**: Implement sticky actions in `pharos` and verify that the metric-agent mismatch warning manifests as predicted, particularly in value function computation scaling