---
ver: rpa2
title: 'Pass@k Metric for RLVR: A Diagnostic Tool of Exploration, But Not an Objective'
arxiv_id: '2511.16231'
source_url: https://arxiv.org/abs/2511.16231
tags:
- pass
- objective
- probability
- arxiv
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the pass@k metric used in reinforcement learning
  for verifiable reasoning tasks. The authors derive the gradient of the pass@k objective
  and show it is simply a reweighted version of the pass@1 gradient, scaled by a factor
  that vanishes when the policy concentrates probability mass.
---

# Pass@k Metric for RLVR: A Diagnostic Tool of Exploration, But Not an Objective

## Quick Facts
- arXiv ID: 2511.16231
- Source URL: https://arxiv.org/abs/2511.16231
- Authors: Yang Yu
- Reference count: 16
- Primary result: Pass@k gradient is a reweighted pass@1 gradient that provides vanishing learning signal when exploration is most needed, making it unsuitable as an optimization objective.

## Executive Summary
This paper provides a theoretical analysis of the pass@k metric used in reinforcement learning for verifiable reasoning tasks (RLVR). The authors derive the gradient of the pass@k objective and show it is fundamentally a reweighted version of the simpler pass@1 gradient. They demonstrate that this objective fails to provide meaningful learning signals exactly when exploration is most critical—in low-performance regimes—and naturally induces "exploration collapse" as training progresses. The paper concludes that pass@k should be treated as a diagnostic tool for reasoning diversity rather than an optimization objective, suggesting that explicit exploration mechanisms are needed instead.

## Method Summary
The paper presents a theoretical analysis of the pass@k metric in RLVR settings. The authors derive the gradient of pass@k using chain rule, showing it equals a scalar multiple (αk ∈ [0,k]) of the pass@1 gradient. They analyze two failure modes: (1) vanishing learning signal when the policy's per-example success rate J1 is near zero, and (2) exploration collapse where the policy concentrates probability mass on discovered solutions, causing pass@k to converge toward pass@1. The analysis assumes standard REINFORCE-style policy gradient estimation with deterministic binary verification.

## Key Results
- The pass@k gradient is mathematically a scalar multiple of the pass@1 gradient, with scaling factor αk = k(1-J1)^(k-1) ∈ [0,k]
- Pass@k provides weak or zero learning signal in low-J1 regimes despite theoretical high scaling, due to sampling variance
- Iterative RL naturally induces exploration collapse, where the pass@k-pass@1 gap diminishes as the policy concentrates on discovered solutions
- Pass@k should be used as a diagnostic for solution diversity, not as an optimization objective

## Why This Works (Mechanism)

### Mechanism 1: Gradient Collinearity Between Pass@k and Pass@1
The pass@k gradient does not introduce a new optimization direction—it is a scalar multiple of the pass@1 gradient. Applying the chain rule to Jk(θ) = 1 - (1 - J1(θ))^k yields ∇θJk = k(1-J1)^(k-1) · ∇θJ1. The scaling factor αk ∈ [0, k] depends only on current per-example success rate. Gradients are estimated via sampling (e.g., REINFORCE); if no correct samples are drawn, the empirical gradient is zero regardless of αk.

### Mechanism 2: Vanishing Learning Signal at Critical Regimes
Pass@k provides weak or zero learning signal exactly when exploration is most needed (low J1) or when the model has converged (high J1). When J1 ≈ 0, αk ≈ k (maximum scaling), but sampling likely yields zero correct trajectories → zero empirical gradient. When J1 → 1, αk → 0, so gradient vanishes even though the objective appears satisfied.

### Mechanism 3: Exploration Collapse Under Iterative RL
As RL training progresses, the policy concentrates probability mass on discovered correct modes, causing pass@k to converge toward pass@1 and reducing the marginal value of k samples. If an undiscovered mode M2 has probability < ϵ, the chance of sampling it in k draws is bounded by ≈ kϵ. Gradients reinforce only observed (M1) samples, monotonically increasing π(M1) and shrinking the pass@k-pass@1 gap.

## Foundational Learning

- **Policy Gradient Estimation (REINFORCE)**: Understanding why empirical gradients vanish when no correct samples are drawn is essential to grasping Mechanism 2. Quick check: If you sample 10 trajectories and none receive positive reward, what is the REINFORCE gradient estimate for that batch?

- **Pass@k Metric Definition**: The entire analysis hinges on the functional form Jk = 1 - (1 - J1)^k and its derivative. Quick check: For a model with pass@1 = 0.3, what is pass@5? (Answer: 1 - 0.7^5 ≈ 0.832)

- **Exploration vs. Exploitation Trade-off**: "Exploration collapse" is the central failure mode identified; distinguishing when a policy is exploiting vs. exploring is diagnostic. Quick check: If pass@1 improves but pass@10 degrades during training, what does this suggest about the policy's exploration behavior?

## Architecture Onboarding

- **Component map**: Verifier V(x,y) → Sampler → Gradient Estimator → Policy πθ → Diagnostics
- **Critical path**: 1) Sample k trajectories per prompt, 2) Verify each trajectory → binary rewards, 3) Compute gradient estimate (REINFORCE or variant), 4) Update policy parameters, 5) Monitor exploration collapse via pass@k-pass@1 gap
- **Design tradeoffs**: Larger k provides better pass@k estimate but higher compute cost; diminishing returns as policy concentrates. Temperature adjustment: Higher T preserves diversity but may reduce pass@1; lower T sharpens policy but accelerates collapse. Explicit exploration (entropy bonus, noise): May slow convergence but prevent collapse
- **Failure signatures**: pass@1 rising while pass@k stagnates or falls → exploration collapse; gradient norms decaying to zero on hard examples (low J1) → vanishing signal problem; large k providing no marginal gain over k=1 → redundant sampling regime
- **First 3 experiments**: 1) Track pass@1, pass@10, and gap Δ across training epochs on GSM8K to confirm if Δ shrinks as training progresses, 2) Train with fixed sampling temperature T ∈ {0.5, 0.8, 1.0, 1.2} and compare final pass@10 and gap Δ, 3) Add entropy regularization term to objective and measure whether pass@k is preserved at higher levels without sacrificing pass@1 convergence

## Open Questions the Paper Calls Out

### Open Question 1
What specific exploration mechanisms can effectively prevent the "exploration collapse" described in the paper without relying on pass@k as a direct objective? The authors conclude that "mechanisms explicitly encouraging efficient exploration could offer a more effective path forward," suggesting the need for alternatives to standard RLVR. An algorithm that maintains a stable gap between pass@k and pass@1 during training while maximizing pass@1, compared to standard RLVR baselines, would resolve this.

### Open Question 2
Can off-policy estimation techniques recover the learning signal in the low-performance regime (J1 ≈ 0) where the on-policy pass@k gradient vanishes? The analysis notes that while the theoretical scaling factor is high when J1 ≈ 0, the empirical gradient is zero because the model fails to sample correct solutions. A derivation showing that importance sampling or replay buffers can provide a non-zero gradient estimate when the current policy success rate is near zero would resolve this.

### Open Question 3
How does modifying the assumption of independent sample evaluation affect the gradient collinearity with pass@1? The conclusion states that to optimize for pass@k, "algorithms must transcend the standard assumption of independent sample evaluation." A gradient derivation for a non-independent sampling strategy that shows a search direction distinct from pass@1 would resolve this.

## Limitations
- The analysis is purely theoretical without empirical validation on real RLVR benchmarks
- Assumes deterministic binary verifier and does not address stochastic or approximate verification scenarios
- Does not consider the effects of gradient estimation variance and reward sparsity on the identified failure modes

## Confidence
- **High Confidence**: The gradient derivation showing pass@k is a reweighted pass@1 objective is mathematically rigorous and correct (Theorem 3.1)
- **Medium Confidence**: The exploration collapse mechanism is theoretically sound but would benefit from empirical validation across diverse reasoning tasks and model scales
- **Medium Confidence**: The claim that pass@k should be used as a diagnostic rather than an objective is reasonable but depends on whether alternative exploration mechanisms can effectively address the identified limitations

## Next Checks
1. **Empirical Gradient Analysis**: Implement pass@k and pass@1 training on GSM8K. Track empirical gradient norms across training epochs, particularly in low-J1 regimes, to confirm the vanishing signal problem occurs in practice.

2. **Exploration Diversity Measurement**: Train models with different k values and measure solution diversity using pairwise trajectory similarity or coverage of distinct reasoning paths. Confirm whether higher k maintains solution diversity compared to k=1.

3. **Alternative Objective Comparison**: Implement and compare pass@k training against explicit exploration mechanisms (entropy bonuses, diverse sampling) on the same tasks. Measure whether these alternatives maintain higher pass@k-pass@1 gaps while achieving comparable or better final pass@1 performance.