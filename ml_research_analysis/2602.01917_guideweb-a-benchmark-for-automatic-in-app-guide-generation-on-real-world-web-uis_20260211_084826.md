---
ver: rpa2
title: 'GuideWeb: A Benchmark for Automatic In-App Guide Generation on Real-World
  Web UIs'
arxiv_id: '2602.01917'
source_url: https://arxiv.org/abs/2602.01917
tags:
- guide
- page
- wang
- text
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GuideWeb, a benchmark for automatic in-app
  guide generation on real-world web user interfaces. The task involves selecting
  which interactive UI elements require guidance and generating concise guide text
  aligned with user intent.
---

# GuideWeb: A Benchmark for Automatic In-App Guide Generation on Real-World Web UIs
## Quick Facts
- arXiv ID: 2602.01917
- Source URL: https://arxiv.org/abs/2602.01917
- Reference count: 13
- Authors: Chengguang Gan; Yoshihiro Tsujii; Yunhao Liang; Tatsunori Mori; Shiwen Ni; Hiroki Itoh
- Primary result: GuideWeb Agent achieves 30.79% guide target prediction accuracy, BLEU scores of 44.94 for intent generation, and 21.34 for guide-text generation

## Executive Summary
GuideWeb introduces a novel benchmark for automatic in-app guide generation on real-world web user interfaces, addressing the challenge of selecting which interactive UI elements require guidance and generating concise, intent-aligned guide text. The benchmark provides a comprehensive evaluation suite that jointly measures guide target selection accuracy and guide text quality, filling a gap in existing research that has primarily focused on either UI element detection or text generation in isolation. The authors propose a context-shortening mechanism in their GuideWeb Agent, which demonstrates substantial improvements over existing baselines, achieving 30.79% accuracy in guide target prediction and competitive BLEU scores for both intent and guide-text generation.

## Method Summary
The GuideWeb benchmark leverages real-world web UI screenshots paired with user intent descriptions and ground truth guides to create a comprehensive dataset for training and evaluating automatic guide generation systems. The proposed GuideWeb Agent employs a context-shortening mechanism to focus on relevant UI elements and generate concise guide text. The evaluation suite measures both the accuracy of guide target selection and the quality of generated guide text using standard metrics like BLEU scores. The benchmark's design emphasizes practical applicability by using real-world web interfaces and user intents, rather than synthetic or simplified scenarios.

## Key Results
- GuideWeb Agent achieves 30.79% accuracy in guide target prediction, outperforming existing baselines
- BLEU scores of 44.94 for intent generation and 21.34 for guide-text generation demonstrate strong text quality
- General-purpose models tend to over-generate guide targets, while task-specific training enables more precise guidance
- The comprehensive evaluation suite provides a more holistic assessment of guide generation systems compared to existing benchmarks

## Why This Works (Mechanism)
The GuideWeb approach works by combining task-specific training with a context-shortening mechanism that focuses the model on relevant UI elements and user intents. By training on real-world web UI screenshots paired with user intent descriptions and ground truth guides, the model learns to identify which interactive elements require guidance and generate concise, intent-aligned text. The context-shortening mechanism helps prevent over-generation of guide targets, a common issue with general-purpose models, by constraining the model's attention to the most relevant UI elements based on the user's stated intent.

## Foundational Learning
- **Context-shortening mechanism**: Why needed: Prevents over-generation of guide targets by focusing on relevant UI elements. Quick check: Compare guide target prediction accuracy with and without context-shortening.
- **Joint evaluation of guide target selection and text quality**: Why needed: Provides a more comprehensive assessment of guide generation systems than evaluating components in isolation. Quick check: Analyze correlation between guide target accuracy and guide text quality metrics.
- **Real-world web UI dataset**: Why needed: Ensures practical applicability by training on actual web interfaces rather than synthetic scenarios. Quick check: Evaluate performance on out-of-distribution web UI styles.
- **BLEU score for guide text evaluation**: Why needed: Provides a standardized metric for comparing text generation quality across different models. Quick check: Conduct human evaluation to validate BLEU score correlations with perceived text quality.
- **Task-specific training**: Why needed: Enables more precise guide generation by focusing on the specific requirements of in-app guidance. Quick check: Compare performance with general-purpose UI understanding models.

## Architecture Onboarding
- **Component map**: User Intent -> Context-Shortening Mechanism -> Guide Target Prediction -> Guide Text Generation
- **Critical path**: User Intent and UI context are processed through the context-shortening mechanism to identify relevant UI elements, which are then used to generate both the guide target selection and the accompanying guide text.
- **Design tradeoffs**: The context-shortening mechanism trades off some potential coverage of UI elements for increased precision in guide target selection, resulting in more focused and useful guidance but potentially missing some relevant elements.
- **Failure signatures**: Over-reliance on context-shortening may lead to missed guide opportunities for less prominent but still important UI elements. Under-training on diverse UI styles may result in poor generalization to novel web interfaces.
- **First experiments**:
  1. Ablation study comparing guide target prediction accuracy with and without the context-shortening mechanism
  2. Analysis of BLEU score improvements when using task-specific training versus general-purpose UI understanding models
  3. Evaluation of guide text quality and user satisfaction through human studies on a subset of generated guides

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark primarily covers static English web pages, limiting generalizability to non-English interfaces and dynamic content
- Performance metrics indicate significant room for improvement in real-world applicability
- Focus on static UI elements may not fully capture the complexity of modern web applications with frequent content updates and interactive features
- The effectiveness of the evaluation suite in measuring user satisfaction and practical utility of generated guides remains to be thoroughly validated

## Confidence
- High: The benchmark's design and its comprehensive evaluation suite are well-defined and methodologically sound
- Medium: The performance improvements of the GuideWeb Agent over existing baselines are supported by quantitative metrics
- Low: The real-world applicability and user satisfaction of the generated guides are not fully addressed

## Next Checks
1. Conduct user studies to assess the practical utility and user satisfaction of the generated guides across diverse web applications
2. Expand the benchmark to include non-English web interfaces and dynamic content to evaluate the model's generalizability
3. Implement a longitudinal study to measure the effectiveness of the generated guides over time as web interfaces evolve