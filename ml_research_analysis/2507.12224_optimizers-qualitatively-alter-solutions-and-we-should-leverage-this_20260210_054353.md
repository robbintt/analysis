---
ver: rpa2
title: Optimizers Qualitatively Alter Solutions And We Should Leverage This
arxiv_id: '2507.12224'
source_url: https://arxiv.org/abs/2507.12224
tags:
- learning
- neural
- optimizer
- https
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper argues that optimizers do more than just speed up convergence\u2014\
  they fundamentally shape the type of solutions learned by deep neural networks.\
  \ Due to the non-convex nature of neural network loss landscapes, different optimizers\
  \ can converge to qualitatively different minima, encoding inductive biases and\
  \ altering the effective expressivity of the model class."
---

# Optimizers Qualitatively Alter Solutions And We Should Leverage This

## Quick Facts
- arXiv ID: 2507.12224
- Source URL: https://arxiv.org/abs/2507.12224
- Reference count: 40
- Key outcome: Optimizers fundamentally shape the type of solutions learned by deep neural networks, affecting properties like sparsity, catastrophic forgetting, and forward transfer

## Executive Summary
This paper challenges the conventional view that optimizers merely accelerate convergence to a fixed solution. Instead, it demonstrates that different optimizers can converge to qualitatively different minima in non-convex neural network loss landscapes, encoding distinct inductive biases. Through experiments with second-order methods like Shampoo and sparsity-inducing preconditioners, the authors show that optimizer choice impacts learned representations in ways that complement or surpass architectural design. The paper argues for treating optimizers as a critical lever for encoding desiderata, particularly in continual learning and large-scale pretraining scenarios.

## Method Summary
The paper demonstrates optimizer effects through two main experimental setups: continual learning on permuted MNIST and class-incremental MNIST with sequential task learning. It compares SGD, AdamW, and Shampoo optimizers on a single hidden layer MLP (100 units) with matched initialization across optimizers and 10 random seeds. For continual learning, tasks are learned sequentially with early stopping per task. The paper also explores sparsity-inducing preconditioners and their mathematical equivalence to architectural reparameterization. Key metrics include average test accuracy across tasks, effective rank of representation covariance spectrum, and cross-class cosine similarity of features.

## Key Results
- Shampoo-trained MLPs show lower effective rank in representation covariance spectrum compared to SGD/AdamW on continual learning tasks
- Non-diagonal preconditioners reduce catastrophic forgetting by learning more localized, lower-dimensional representations
- Sparsity-inducing preconditioners can replace architectural reparameterization while achieving equivalent learning dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-diagonal preconditioners reduce catastrophic forgetting by learning more localized, lower-dimensional representations.
- Mechanism: Second-order methods with full preconditioners capture parameter correlations via off-diagonal elements, constraining representations to lower-dimensional subspaces and leaving more capacity for future tasks.
- Core assumption: Localized, less redundant representations reduce interference between sequentially learned tasks.
- Evidence anchors: Figure 3 shows Shampoo has lower effective rank than SGD/AdamW; Figure 4 shows reduced cross-class cosine similarity with Shampoo.
- Break condition: If tasks require highly distributed representations for generalization, localization could harm performance.

### Mechanism 2
- Claim: Sparsity-inducing preconditioners can replace architectural reparameterization with equivalent dynamics.
- Mechanism: A preconditioner matrix P = diag(|θ|^β) (β > 0) creates effective saddle points at zero, concentrating information in large weights and enabling threshold-based sparsification.
- Core assumption: The regularization effect cannot be equivalently achieved via additive regularization terms.
- Evidence anchors: Mathematical equivalence shown between preconditioner and Power-propagation; Power-propagation achieves state-of-art sparsification.
- Break condition: If standard L1 regularization achieves similar sparsity without convergence penalty, the mechanism's unique value diminishes.

### Mechanism 3
- Claim: Optimizer choice constrains the "reachable function class" beyond what architecture expressivity alone predicts.
- Mechanism: Non-convex loss landscapes contain multiple basins of attraction; different optimizers trace different trajectories during basin-hopping phase, converging to qualitatively different minima.
- Core assumption: The transient phase (early basin selection) is optimizer-dependent and consequential.
- Evidence anchors: Learning is split into basin-hopping and local convexity stages; RNN Turing-completeness argument about unreachable functions.
- Break condition: If basin selection is dominated primarily by initialization rather than optimizer dynamics.

## Foundational Learning

- **Non-convex loss landscapes**: Why needed here: The argument hinges on neural networks having multiple basins of attraction with qualitatively different properties. Quick check: Can you explain why convex optimization guarantees a unique global minimum but non-convex optimization does not?

- **Preconditioning in optimization**: Why needed here: Understanding how preconditioners reshape gradient directions is essential for grasping why non-diagonal preconditioners affect solution quality. Quick check: What does a preconditioner matrix do to a gradient, and why does capturing off-diagonal correlations matter?

- **Inductive bias**: Why needed here: The paper positions optimizers as vehicles for encoding inductive biases, requiring clarity on what inductive bias means beyond "regularization." Quick check: How does an optimizer's implicit bias differ from an explicit regularization term added to the loss?

## Architecture Onboarding

- **Component map**: Optimizer core (gradient computation + preconditioner matrix P + update rule) -> Preconditioner types (diagonal vs non-diagonal/full) -> Sparsity-inducing variant (custom P = diag(|θ|^β)) -> Evaluation axes (convergence speed, generalization, sparsity, forgetting, forward transfer)

- **Critical path**: 1) Identify target solution property (e.g., sparsity, continual learning stability) 2) Map property to optimizer mechanism (preconditioner structure, update noise, step size adaptation) 3) Implement or select optimizer variant 4) Tune hyperparameters aware of speed/quality tradeoff 5) Evaluate on property-specific metrics

- **Design tradeoffs**: Non-diagonal preconditioners offer better solution quality but higher compute/memory cost; sparsity-inducing preconditioners provide better sparsification but slower/noisier convergence; diagonal methods (Adam) are fast and scalable but may learn more redundant representations

- **Failure signatures**: Continual learning with Adam shows sharp accuracy drop on earlier tasks; high cross-class cosine similarity indicates capacity waste/interference; convergence stalling with sparsity preconditioners if poorly tuned

- **First 3 experiments**:
  1. Replicate Figure 3/4: Train small MLP on permuted MNIST with SGD, AdamW, and Shampoo; compare average final accuracy and representation covariance spectrum rank
  2. Test sparsity preconditioner: Implement P = diag(|θ|^β) on classification task; measure weight sparsity vs. baseline accuracy tradeoff
  3. Ablate basin-selection phase: Train from same initialization with different optimizers for first N steps, then switch to common optimizer; test whether early choice locks in solution properties

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we systematically design optimizers to induce specific qualitative properties (e.g., sparsity, compositional structure) in the learned solution, even at the expense of convergence speed?
- Basis in paper: The authors argue the community should "aim to build new learning algorithms with the explicit intent of inducing certain properties of the solution, rather than solely judging them based on their convergence rates."
- Why unresolved: The field currently prioritizes optimization speed, treating the optimizer as a tool to reach a minimum rather than a mechanism to shape the solution's structure.
- What evidence would resolve it: Development of optimizers that successfully trade off training efficiency for structural properties, such as guaranteed sparsity without explicit regularization or improved forward transfer in continual learning.

### Open Question 2
- Question: To what extent do non-diagonal preconditioners causally reduce catastrophic forgetting by enforcing lower-rank, localized representations?
- Basis in paper: Section 3.1 hypothesizes that second-order methods reduce "wasteful movement," leading to representations that occupy a lower-dimensional subspace, but notes the need to verify this mechanism across different architectures.
- Why unresolved: The paper provides empirical correlations in small MLPs but lacks formal theoretical guarantee or large-scale validation that lower effective rank is the primary driver of reduced forgetting.
- What evidence would resolve it: Ablation studies on large-scale continual learning benchmarks that independently manipulate effective rank and preconditioning strategy to isolate their effects on interference.

### Open Question 3
- Question: Is there a strict duality or performance trade-off between introducing inductive biases via optimizer preconditioning versus architectural reparametrization?
- Basis in paper: Section 4 asks, "Why putting the inductive bias into the optimizer rather than directly into the model via reparametrization?" and suggests the perspective needs further exploration.
- Why unresolved: While the paper sketches a mathematical equivalence for Power-propagation, computational efficiency and generalization differences between approaches in complex settings remain unclear.
- What evidence would resolve it: Rigorous comparison of Power-propagation (architecture change) against equivalent preconditioners (optimizer change) in terms of wall-clock time, memory usage, and final task performance.

## Limitations

- The observed qualitative differences between optimizers may not persist beyond specific continual learning tasks like permuted MNIST
- Computational overhead of non-diagonal preconditioners like Shampoo may limit practical applicability in large-scale settings
- The equivalence between sparsity-inducing preconditioners and architectural reparameterization lacks direct experimental validation

## Confidence

- **High confidence**: The core claim that optimizers shape learned solutions qualitatively is well-supported by empirical evidence
- **Medium confidence**: The mechanism linking non-diagonal preconditioners to localized representations and reduced forgetting is plausible but requires more ablation studies
- **Low confidence**: The equivalence between sparsity-inducing preconditioners and architectural reparameterization lacks direct experimental validation

## Next Validation Checks

1. Test whether Shampoo's advantage in reducing forgetting extends to language modeling tasks like continual pretraining of transformers on different text domains, measuring both task performance and representation overlap

2. Implement a variant of Shampoo that randomly perturbs the basin-selection trajectory early in training, then switches to standard Adam, to test whether the initial optimizer choice is truly responsible for final solution properties

3. Benchmark non-diagonal preconditioners on large transformer architectures (e.g., BERT-base) to quantify practical limits of optimizer-based solution shaping, measuring both compute overhead and solution quality benefits