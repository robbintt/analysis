---
ver: rpa2
title: Churn-Aware Recommendation Planning under Aggregated Preference Feedback
arxiv_id: '2507.04513'
source_url: https://arxiv.org/abs/2507.04513
tags:
- user
- policy
- belief
- optimal
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses sequential recommendation planning under privacy
  constraints where only aggregated user preference data is available. The Rec-APC
  model assumes a recommender system knows a prior distribution over user types and
  their aggregated preferences, but not individual user identities, while facing the
  risk of user churn from unsatisfactory recommendations.
---

# Churn-Aware Recommendation Planning under Aggregated Preference Feedback

## Quick Facts
- arXiv ID: 2507.04513
- Source URL: https://arxiv.org/abs/2507.04513
- Reference count: 40
- This paper addresses sequential recommendation planning under privacy constraints where only aggregated user preference data is available.

## Executive Summary
This paper addresses sequential recommendation planning under privacy constraints where only aggregated user preference data is available. The Rec-APC model assumes a recommender system knows a prior distribution over user types and their aggregated preferences, but not individual user identities, while facing the risk of user churn from unsatisfactory recommendations. The core method uses a branch-and-bound algorithm to compute approximately optimal recommendation policies. The algorithm exploits the theoretical insight that optimal policies converge to pure exploitation after finite time, enabling efficient search by pruning suboptimal branches.

## Method Summary
The Rec-APC problem is modeled as a POMDP where the system maintains a belief distribution over hidden user types and selects content categories to maximize expected social welfare. The method employs a branch-and-bound algorithm (Algorithm 1) that exploits the theoretical insight that optimal policies converge to pure exploitation after finite time. The algorithm computes upper bounds assuming perfect type knowledge and lower bounds using best fixed-action policies, pruning branches where the gap exceeds ε. Bayesian updates refine beliefs on positive feedback, while negative feedback terminates sessions. The approach is validated against the SARSOP solver on both synthetic and real-world MovieLens data.

## Key Results
- Optimal policies converge rapidly in practice, with uncertainty about user type decreasing geometrically after policy convergence.
- Algorithm 1 outperforms the POMDP solver SARSOP on rectangular instances (small number of categories relative to user types) and on real-world MovieLens data with noise.
- SARSOP performs better when the number of categories greatly exceeds the number of user types.
- Runtime comparisons on synthetic data demonstrate Algorithm 1's efficiency advantage for square or near-square problem instances.

## Why This Works (Mechanism)

### Mechanism 1: Bayesian Belief Refinement Under Binary Feedback
Positive feedback allows the system to reduce uncertainty about the anonymous user's latent type through Bayesian posterior updates. Each positive recommendation outcome updates the belief b over user types via τ(b,k)(m) ∝ b(m)·P(k,m), concentrating probability mass on types more likely to generate that outcome. This enables personalization without identity tracking. The mechanism breaks when P rows are nearly identical across types, providing negligible information gain.

### Mechanism 2: Finite-Time Convergence to Pure Exploitation
For well-separated instances (c(I) > 0), optimal policies transition to repeatedly recommending a single category after bounded exploration rounds. Belief walks visit only finitely many δ-unconcentrated beliefs. Once belief becomes sufficiently concentrated on type m, myopic selection of argmax_k P(k,m) is provably optimal, and the policy locks into that action. Convergence is not guaranteed when c(I) = 0 (e.g., identical preference matrix rows).

### Mechanism 3: Branch-and-Bound Search with Tight Value Bounds
Algorithm 1 efficiently finds ε-optimal policies by pruning branches where the upper bound cannot exceed the current best lower bound by more than ε. Upper bound V^U(b) assumes perfect type knowledge; lower bound V^L(b) uses best fixed-action policy. After H = O(log(1/ε)) depth, the gap between bounds shrinks below ε, enabling termination. When |K| >> |M|, the branching factor dominates and SARSOP's belief-space exploration becomes more efficient.

## Foundational Learning

- Concept: **Bayesian Inference for Discrete Latent Variables**
  - Why needed here: Core mechanism for updating beliefs over hidden user types given observed binary feedback.
  - Quick check question: Given prior q = (0.5, 0.5) and likelihoods P(k,m₁) = 0.9, P(k,m₂) = 0.2, what is the posterior after observing a positive response to k?

- Concept: **Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: Rec-APC is formally a POMDP; understanding belief-state MDPs and value iteration is essential for grasping why specialized algorithms outperform generic solvers in certain regimes.
  - Quick check question: Why does the belief state in Rec-APC remain on a lower-dimensional simplex rather than the full 2|M|+1 state space of the auxiliary POMDP construction?

- Concept: **Branch-and-Bound Pruning**
  - Why needed here: Algorithm 1's efficiency depends on maintaining valid upper/lower bounds and pruning provably suboptimal prefixes.
  - Quick check question: If V^Π ≤ Ṽ at some prefix Π, can we safely discard all extensions of Π? Why or why not?

## Architecture Onboarding

- Component map:
  - **Belief State Manager**: Stores current belief b ∈ Δ(M); initialized to prior q
  - **Bayesian Updater**: Computes τ(b,k) on positive feedback; returns None on negative feedback (session ends)
  - **Value Bound Calculator**: Computes V^U(b) (clairvoyant upper bound) and V^L(b) (best fixed-action lower bound)
  - **B&B Policy Search**: Explores policy prefixes via queue; prunes using bound comparisons; returns ε-optimal prefix ˜Π
  - **Convergence Detector**: Checks if belief is (c²/4, m)-concentrated; if so, switches to fixed-action policy for type m

- Critical path:
  1. Initialize belief to prior q
  2. For each round: B&B selects next category from current best prefix; observe feedback
  3. If positive: Bayesian update → check concentration → if concentrated, lock to myopic action
  4. If negative: Session terminates (churn event)

- Design tradeoffs:
  - **Algorithm 1 vs. SARSOP**: Algorithm 1 excels when |M| ≥ |K| (explores policy space); SARSOP excels when |K| >> |M| (explores belief space)
  - **Approximation ε vs. runtime**: Smaller ε requires deeper search (H ∝ log(1/ε)); quadratic impact on nodes explored
  - **Precomputation vs. online planning**: Current formulation assumes offline planning with known P, q; online adaptation to unknown P would require substantial extension

- Failure signatures:
  - No convergence after many rounds → suspect c(I) ≈ 0 or degenerate preference matrix
  - Runtime explosion on high-|K| instances → switch to SARSOP or increase ε
  - Value estimates diverge from actual outcomes → verify P matrix estimation and noise handling

- First 3 experiments:
  1. Replicate synthetic convergence experiments (Figure 1): Generate random instances, track uncertainty reduction over rounds, verify geometric convergence rates
  2. Runtime scaling sweep (Figure 2): Compare Algorithm 1 vs. SARSOP across varying |M|/|K| ratios; identify crossover point where SARSOP becomes preferable
  3. Real-data pipeline validation (Section 7): Apply spectral co-clustering to MovieLens, construct P and q, add Gaussian noise, verify Algorithm 1 maintains advantage over SARSOP

## Open Questions the Paper Calls Out

### Open Question 1
Does a provably optimal polynomial-time algorithm exist for computing the optimal policy, or is the Rec-APC problem computationally hard? The paper states they "still lack either a provably optimal polynomial-time algorithm for computing an optimal policy or a formal proof of hardness." The fundamental computational complexity class remains open.

### Open Question 2
Can the theoretical guarantees of convergence to pure exploitation be maintained if users are allowed to stay in the system after negative feedback? The current model assumes a "dichotomous structure" where users leave upon dislike, but in practice users may stay or provide feedback with varying intensity. The current proofs rely on deterministic belief updates from the specific like/stay vs. dislike/leave dynamic.

### Open Question 3
How can the efficiency of the branch-and-bound algorithm be improved for instances where the number of content categories far exceeds the number of user types? Algorithm 1 underperforms SARSOP when |K| >> |M| due to the branching factor being |K|. The current algorithm searches policy space by branching on categories, which scales poorly compared to belief-space methods when the action space is large.

## Limitations
- The theoretical convergence guarantees depend critically on the instance separator c(I) > 0, which requires well-separated preference matrices; convergence may be slow or fail when user types have similar preferences.
- The branch-and-bound algorithm's efficiency claims are based on synthetic data experiments with specific parameter choices; the crossover point where SARSOP becomes superior is theoretically predicted but not thoroughly mapped across the full parameter space.
- The noise robustness experiments on MovieLens data use only one noise level (σ=0.005), leaving questions about performance degradation under varying noise conditions.

## Confidence

- **High confidence**: The Bayesian update mechanism and value function formulation (Lemma 1) are mathematically sound and well-established POMDP theory.
- **Medium confidence**: The finite-time convergence claims and branch-and-bound pruning effectiveness rely on theoretical bounds that may not hold for all problem instances, particularly when c(I) is small.
- **Medium confidence**: The empirical runtime comparisons are internally consistent but may not generalize to all problem distributions or noise regimes.

## Next Checks

1. **Convergence rate validation**: Generate synthetic instances with varying degrees of preference matrix separation (different c(I) values). Measure actual uncertainty reduction over time and compare against the geometric convergence rates claimed in Corollary 1.

2. **Parameter space mapping**: Systematically sweep |M|/|K| ratios and instance separators c(I) to identify precise conditions where Algorithm 1 outperforms SARSOP versus vice versa. Include runtime measurements and optimality gap analysis.

3. **Noise sensitivity analysis**: Test Algorithm 1 on MovieLens-derived instances with multiple noise levels (σ ∈ {0.001, 0.005, 0.01, 0.05}). Measure performance degradation and identify noise thresholds where the algorithm's assumptions break down.