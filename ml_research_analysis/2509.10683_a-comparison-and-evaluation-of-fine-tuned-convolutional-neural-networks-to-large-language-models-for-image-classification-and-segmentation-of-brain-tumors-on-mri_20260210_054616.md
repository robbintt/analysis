---
ver: rpa2
title: A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to
  Large Language Models for Image Classification and Segmentation of Brain Tumors
  on MRI
arxiv_id: '2509.10683'
source_url: https://arxiv.org/abs/2509.10683
tags:
- segmentation
- glioma
- figure
- bounding
- tumor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the performance of general-purpose large language
  models (LLMs) and specialized convolutional neural networks (CNNs) for brain tumor
  classification and segmentation using the BraTS 2020 dataset. CNNs outperformed
  LLMs in both tasks, achieving higher accuracy and better segmentation results.
---

# A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI

## Quick Facts
- **arXiv ID**: 2509.10683
- **Source URL**: https://arxiv.org/abs/2509.10683
- **Reference count**: 17
- **Primary result**: CNNs outperformed general-purpose LLMs for brain tumor classification and segmentation on BraTS 2020 dataset

## Executive Summary
This study compared the performance of general-purpose large language models (LLMs) and specialized convolutional neural networks (CNNs) for brain tumor classification and segmentation using the BraTS 2020 dataset. CNNs achieved superior performance with 80% accuracy for tumor classification and Dice coefficient of 0.5942 for segmentation. The general-purpose LLM, LLaMA 3.2 Instruct, struggled with tumor localization and classification, often misclassifying tumors and clustering predictions near the image center. Fine-tuning the LLM showed only marginal improvements, indicating that LLMs, in their current form, are not well-suited for image-based medical tasks without significant architectural modifications or alternative training strategies.

## Method Summary
The study evaluated brain tumor classification and segmentation using the BraTS 2020 dataset with both CNNs and LLMs. The CNN was a specialized medical imaging model trained on the dataset, while the LLM (LlaMA 3.2 Instruct) was adapted for image analysis through fine-tuning. Performance was measured using accuracy for classification and Dice coefficient for segmentation. The LLM underwent limited fine-tuning attempts to improve its performance on medical imaging tasks, though these modifications yielded minimal improvement compared to the specialized CNN architecture.

## Key Results
- CNNs achieved 80% accuracy for tumor classification versus LLMs' poor performance
- CNN segmentation yielded Dice coefficient of 0.5942, significantly outperforming LLMs
- Fine-tuning the LLM provided only marginal improvements, suggesting fundamental limitations for medical image analysis

## Why This Works (Mechanism)
The study demonstrates that specialized architectures optimized for specific tasks outperform general-purpose models when applied to domain-specific problems. CNNs, designed with hierarchical feature extraction and spatial awareness, naturally excel at medical image analysis where precise localization and pattern recognition are critical. LLMs, despite their strong reasoning capabilities in text domains, lack the architectural components necessary for effective visual feature extraction and spatial reasoning in medical imaging contexts. The performance gap reflects both architectural mismatch and insufficient adaptation of LLMs to the unique characteristics of MRI data.

## Foundational Learning

**Convolutional Neural Networks**: Why needed - hierarchical feature extraction and spatial pattern recognition in images; Quick check - understand filter operations, pooling layers, and feature map generation

**Medical Image Segmentation**: Why needed - precise tumor boundary identification and volumetric analysis; Quick check - grasp Dice coefficient, Hausdorff distance, and volumetric metrics

**Large Language Model Adaptation**: Why needed - understanding how general-purpose models can be modified for specialized tasks; Quick check - know fine-tuning, prompt engineering, and domain adaptation techniques

**Brain Tumor MRI Analysis**: Why needed - domain-specific knowledge of tumor characteristics and imaging protocols; Quick check - understand T1, T2, FLAIR sequences and tumor subtypes

## Architecture Onboarding

**Component Map**: MRI images -> CNN feature extractor -> Classification head / Segmentation head OR MRI images -> LLM encoder -> Fine-tuned decoder -> Classification / Segmentation output

**Critical Path**: Data preprocessing -> Model architecture selection -> Training/fine-tuning -> Evaluation metrics computation -> Performance comparison

**Design Tradeoffs**: Specialized CNN architecture (high performance, limited flexibility) vs. general-purpose LLM (versatile but underperforming); architectural optimization vs. broad applicability

**Failure Signatures**: LLM predictions clustering at image center, misclassification of tumor types, poor segmentation boundaries; CNN overfitting on small datasets, sensitivity to noise

**3 First Experiments**: 1) Test hybrid model combining CNN feature extraction with LLM reasoning; 2) Implement multi-task learning for joint classification and segmentation; 3) Evaluate performance on additional medical imaging datasets

## Open Questions the Paper Calls Out
None

## Limitations
- The CNN used was specifically designed for medical imaging while the LLM was a general-purpose model, potentially creating unfair comparison
- Limited fine-tuning scope for the LLM may not reflect its full potential with more rigorous adaptation strategies
- BraTS 2020 dataset represents a specific subset of brain tumor cases that may not generalize to broader clinical scenarios
- Evaluation focused on quantitative metrics without deeper qualitative analysis of failure modes

## Confidence
- **High confidence**: CNNs outperform LLMs for brain tumor classification and segmentation on the tested dataset
- **Medium confidence**: Current LLMs are not well-suited for medical image analysis without significant architectural modifications
- **Medium confidence**: Fine-tuning alone provides limited improvement for LLMs in this domain

## Next Checks
1. Test hybrid models that combine CNN feature extraction with LLM reasoning capabilities to assess potential complementary benefits
2. Implement more extensive fine-tuning protocols including multi-task learning and domain-specific pretraining on medical imaging datasets
3. Evaluate model performance across multiple brain tumor datasets and imaging modalities to assess generalizability of findings