---
ver: rpa2
title: 'Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs
  with SPARQL through Natural Language'
arxiv_id: '2505.19971'
source_url: https://arxiv.org/abs/2505.19971
tags:
- language
- data
- sparql
- queries
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of creating natural language\
  \ interfaces for lexicographic data retrieval on knowledge graphs like Wikidata,\
  \ which are typically inaccessible to non-technical users due to the complexity\
  \ of SPARQL queries. The authors develop a multidimensional taxonomy to capture\
  \ the complexity of Wikidata\u2019s lexicographic data ontology and create a dataset\
  \ of over 1.2 million mappings from natural language utterances to SPARQL queries."
---

# Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language

## Quick Facts
- arXiv ID: 2505.19971
- Source URL: https://arxiv.org/abs/2505.19971
- Reference count: 40
- Primary result: GPT-3.5-Turbo achieves 0.57 pass@3 on generalization scenarios for natural language to SPARQL query conversion

## Executive Summary
This paper addresses the challenge of making lexicographic data on knowledge graphs accessible through natural language interfaces. Wikidata's complex lexicographic data ontology makes direct SPARQL query construction difficult for non-technical users. The authors develop a multidimensional taxonomy to capture this complexity and create a large dataset of natural language to SPARQL mappings. They evaluate multiple language models for query generation, finding that while smaller models perform well on familiar patterns, only GPT-3.5-Turbo demonstrates meaningful generalization to novel query structures.

## Method Summary
The authors first develop a multidimensional taxonomy capturing the complexity of Wikidata's lexicographic data ontology, identifying key dimensions such as query types, linguistic features, and data relationships. They then generate a dataset of over 1.2 million natural language utterances paired with corresponding SPARQL queries. Three models are evaluated: GPT-2, Phi-1.5, and GPT-3.5-Turbo, using pass@k metrics to measure performance. The evaluation includes both familiar query patterns and generalization scenarios with novel structures not seen during training.

## Key Results
- All models achieve high pass@1 scores (0.86-0.90) on familiar query patterns
- GPT-3.5-Turbo significantly outperforms smaller models on generalization (pass@3 of 0.57 vs 0.00-0.01)
- Model size and diverse pre-training are crucial for generalization capabilities
- Substantial challenges remain in achieving robust generalization for novel query structures

## Why This Works (Mechanism)
The approach works by leveraging large language models' ability to understand natural language semantics and map them to structured query representations. The multidimensional taxonomy effectively captures the ontological complexity of lexicographic data, enabling systematic dataset creation. GPT-3.5-Turbo's superior performance suggests that its larger parameter count and broader pre-training enable better pattern recognition and generalization to unseen query structures.

## Foundational Learning

**SPARQL query language**: Why needed - to retrieve data from knowledge graphs; Quick check - can you write a basic SELECT query?

**Knowledge graph ontologies**: Why needed - to understand data relationships and structure; Quick check - can you identify subject-predicate-object triples?

**Natural language processing**: Why needed - to map human language to formal queries; Quick check - can you parse a sentence into its grammatical components?

**Multidimensional taxonomies**: Why needed - to systematically categorize query complexity; Quick check - can you create a simple 2D taxonomy for a given domain?

**Pass@k evaluation metric**: Why needed - to measure model performance in generating correct queries; Quick check - can you explain why top-k evaluation matters for generation tasks?

## Architecture Onboarding

**Component map**: Natural Language Input -> Taxonomy Classification -> Query Pattern Matching -> SPARQL Generation -> Knowledge Graph Query Execution -> Results Presentation

**Critical path**: The most critical path is Natural Language Input to SPARQL Generation, as errors here propagate through the entire system. The taxonomy classification and query pattern matching stages must accurately capture user intent and map it to appropriate query structures.

**Design tradeoffs**: The authors chose to focus on English queries and Wikidata specifically, trading generalizability for depth of coverage in this domain. They opted for large language models over rule-based systems, accepting higher computational costs for better handling of linguistic variation.

**Failure signatures**: Common failures include incorrect semantic interpretation of natural language (e.g., misunderstanding negation or modality), inability to handle novel query patterns, and generation of syntactically invalid SPARQL queries. The system may also struggle with queries requiring cross-linguistic or cross-domain knowledge.

**3 first experiments**:
1. Test model performance on a held-out set of familiar query patterns to establish baseline capability
2. Evaluate generalization by testing on novel query structures not present in training data
3. Compare performance across different knowledge graphs to assess ontological transfer

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to English-language queries and Wikidata, raising questions about cross-linguistic and cross-graph generalizability
- Dataset creation relies on manually crafted mappings that may not capture full real-world query diversity
- Evaluation metrics focus primarily on pass@k scores, potentially missing nuances in query quality or semantic fidelity

## Confidence
- Model size and pre-training diversity drive generalization capability: High confidence
- GPT-3.5-Turbo achieves 0.57 pass@3 in generalization scenarios: Medium confidence
- Multidimensional taxonomy effectively captures Wikidata's lexicographic complexity: Medium confidence

## Next Checks
1. Evaluate the best-performing model on a held-out test set of truly novel query patterns not represented in training data, using human annotators to assess semantic equivalence between generated and expected SPARQL queries.

2. Conduct a comparative evaluation using alternative knowledge graphs (e.g., DBpedia, YAGO) to assess ontological transfer capabilities and identify whether the approach requires graph-specific fine-tuning.

3. Implement an ablation study testing whether the observed performance gains stem primarily from model scale, pre-training data diversity, or specific architectural features by comparing models with matched parameter counts but different training regimes.