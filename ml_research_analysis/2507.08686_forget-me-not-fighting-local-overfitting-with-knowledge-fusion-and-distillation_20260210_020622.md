---
ver: rpa2
title: 'Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and Distillation'
arxiv_id: '2507.08686'
source_url: https://arxiv.org/abs/2507.08686
tags:
- training
- test
- data
- overfitting
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies and addresses a phenomenon the authors call
  local overfitting, where deep models forget previously learned patterns in certain
  regions of the data space even without global overfitting. The authors propose a
  two-stage method: first, Knowledge Fusion (KF) combines checkpoints from mid-training
  using a forgetting metric to recover lost knowledge, and second, a distilled KF
  ensemble compresses this ensemble into a single model to maintain inference efficiency.'
---

# Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and Distillation

## Quick Facts
- arXiv ID: 2507.08686
- Source URL: https://arxiv.org/abs/2507.08686
- Authors: Uri Stern; Eli Corn; Daphna Weinshall
- Reference count: 40
- This paper identifies and addresses local overfitting, where deep models forget previously learned patterns in certain regions of the data space even without global overfitting.

## Executive Summary
This paper identifies and addresses a phenomenon the authors call local overfitting, where deep models forget previously learned patterns in certain regions of the data space even without global overfitting. The authors propose a two-stage method: first, Knowledge Fusion (KF) combines checkpoints from mid-training using a forgetting metric to recover lost knowledge, and second, a distilled KF ensemble compresses this ensemble into a single model to maintain inference efficiency. Extensive experiments show that KF consistently improves performance across datasets and architectures, with the distilled KF variant often outperforming both the original model and independently trained ensembles. The method achieves up to 15% error reduction in noisy label settings and delivers strong accuracy while reducing training and inference costs, making it a win-win solution for model performance and efficiency.

## Method Summary
The method consists of two stages: Knowledge Fusion and distillation. In the first stage, KF tracks a "forget fraction" metric during training to identify checkpoints that have forgotten previously learned regions of the data space. These checkpoints are then combined using an ensemble approach to recover the lost knowledge. In the second stage, a single distilled model is trained using the KF ensemble as the teacher, effectively compressing the ensemble's knowledge into a more efficient format. This approach addresses local overfitting by specifically targeting regions where the model has forgotten patterns, rather than focusing solely on global overfitting which is already well-studied.

## Key Results
- KF consistently improves performance across datasets and architectures
- Distilled KF variant often outperforms both the original model and independently trained ensembles
- Achieves up to 15% error reduction in noisy label settings
- Delivers strong accuracy while reducing training and inference costs

## Why This Works (Mechanism)
The paper addresses local overfitting by identifying that models can forget previously learned patterns in specific regions of the data space during training, even when global performance metrics appear satisfactory. The Knowledge Fusion method recovers this lost knowledge by combining checkpoints where different regions were learned, while distillation compresses this ensemble knowledge into a single efficient model. The effectiveness stems from explicitly targeting the forgetting phenomenon rather than relying on traditional regularization methods that address only global overfitting.

## Foundational Learning
- Local Overfitting: A phenomenon where models forget specific regions of the data space during training while maintaining global performance metrics. This is critical because traditional overfitting detection methods may miss these localized issues.
- Forget Fraction Metric: A measurement that quantifies how much a model has forgotten previously learned patterns in different regions of the data space during training. Quick check: Verify the metric correctly identifies when a model has lost knowledge in specific regions by comparing performance on previously mastered samples.
- Knowledge Fusion: The process of combining multiple model checkpoints to recover knowledge that has been forgotten in specific regions. Quick check: Confirm that combining checkpoints from different training stages improves performance on the regions where individual checkpoints have forgotten.
- Distillation: The compression of ensemble knowledge into a single model, which can paradoxically outperform the larger ensemble. Quick check: Test whether the distilled model maintains or improves upon the ensemble's performance while being more efficient.

## Architecture Onboarding

**Component Map**: Data → Model → Forget Fraction Monitor → KF Ensemble → Distilled Model → Inference

**Critical Path**: The critical path involves monitoring the forget fraction during training, selecting appropriate checkpoints for KF, creating the ensemble, and performing distillation to obtain the final efficient model.

**Design Tradeoffs**: The method trades increased training complexity (multiple checkpoints, ensemble creation) for improved final performance and efficiency. The key tradeoff is between the computational overhead during training and the benefits of recovered knowledge and reduced inference costs.

**Failure Signatures**: The method may fail if the forget fraction metric does not accurately identify regions of forgotten knowledge, if checkpoint selection is poor, or if distillation fails to effectively compress the ensemble knowledge.

**3 First Experiments**:
1. Test the forget fraction metric on a simple dataset to verify it correctly identifies forgetting patterns
2. Apply KF to a small CNN on CIFAR-10 to confirm knowledge recovery works as expected
3. Compare the distilled KF ensemble against a standard trained model to validate the efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical mechanism that causes the distilled single model to outperform the larger Knowledge Fusion (KF) ensemble teacher?
- Basis in paper: The authors note in Section VI-B that the superior performance of the student model is "surprising" and "counterintuitive," hypothesizing it is "likely due to the synergy" between checkpoint ensembling and the regularization effects of distillation.
- Why unresolved: The paper provides empirical evidence of the phenomenon but lacks a formal theoretical justification for why compressing the specific KF ensemble improves generalization.
- What evidence would resolve it: A theoretical analysis of the loss landscape demonstrating that distillation acts as a superior regularizer for the specific "forgotten" regions identified by the KF process.

### Open Question 2
- Question: Does the theoretical "forget time" derived for deep linear networks accurately predict the forgetting dynamics in modern deep non-linear architectures?
- Basis in paper: Section VII derives the "forget time" for over-parameterized deep linear networks, but the paper's main empirical results rely on complex, non-linear architectures (ConvNeXt, ViT) where these linear assumptions may not hold.
- Why unresolved: The mathematical derivation relies on spectral decomposition properties specific to linear operators, leaving the validity of these predictions for non-linear activation functions unproven.
- What evidence would resolve it: Empirical validation showing a strong correlation between the theoretical "forget time" (Equation 6) and the observed forgetting events in non-linear networks like ResNets or Transformers.

### Open Question 3
- Question: Is the "local overfitting" phenomenon and the effectiveness of Knowledge Fusion specific to image data, or does it generalize to other modalities like NLP?
- Basis in paper: The empirical evaluation in Section VI is restricted entirely to image classification datasets (CIFAR, TinyImageNet, ImageNet), leaving the behavior of the "forget fraction" in sequential or discrete data domains unknown.
- Why unresolved: The concept of local overfitting in "sub-regions of the data space" may manifest differently in high-dimensional text embeddings compared to image feature spaces.
- What evidence would resolve it: Experiments applying the forget fraction metric and KF to text classification or translation tasks to observe if the same forgetting patterns emerge during training.

## Limitations
- The concept of "local overfitting" is relatively new and may require further validation across diverse problem domains beyond image classification
- The forgetting metric used to identify problematic regions is not extensively validated for robustness across different architectures and datasets
- The method's effectiveness on extremely large-scale models and datasets remains untested

## Confidence

**Effectiveness of KF in recovering lost knowledge**: High
**Superiority of distilled KF ensemble over baseline models**: Medium
**Generalizability of the method across different architectures and datasets**: Medium
**Cost-efficiency claims (reduced training and inference costs)**: Low

## Next Checks
1. Test the KF method on larger-scale models (e.g., Vision Transformers, large language models) and datasets to assess scalability
2. Conduct ablation studies to isolate the impact of the forgetting metric and ensemble composition on final performance
3. Perform extensive runtime analysis comparing the full KF training process with standard training and inference to validate cost-efficiency claims