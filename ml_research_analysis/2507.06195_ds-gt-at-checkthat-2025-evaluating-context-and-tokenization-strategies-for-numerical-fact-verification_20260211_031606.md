---
ver: rpa2
title: 'DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for
  Numerical Fact Verification'
arxiv_id: '2507.06195'
source_url: https://arxiv.org/abs/2507.06195
tags:
- claims
- numerical
- evidence
- context
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how context length and tokenization strategies
  affect numerical fact verification performance. The authors evaluate these factors
  on the QuanTemp dataset using a hybrid retrieval pipeline with ModernBERT for veracity
  prediction.
---

# DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification

## Quick Facts
- arXiv ID: 2507.06195
- Source URL: https://arxiv.org/abs/2507.06195
- Reference count: 23
- 4th place in CheckThat! 2025 Task 3 with macro F1 of 0.57

## Executive Summary
This paper investigates how context length and tokenization strategies affect numerical fact verification performance. The authors evaluate these factors on the QuanTemp dataset using a hybrid retrieval pipeline with ModernBERT for veracity prediction. They find that neither longer context nor right-to-left tokenization improves performance, contrary to expectations. The best system achieves a macro F1 of 0.57, placing 4th among 10 submissions in the CheckThat! 2025 Task 3. The results indicate that evidence quality, rather than context size or tokenization method, is the primary bottleneck for numerical fact verification.

## Method Summary
The system uses a hybrid retrieval pipeline with claim decomposition via GPT-4o-mini, BM25 for initial retrieval of 50 documents per sub-claim, and a cross-encoder reranker to select top evidence. ModernBERT is fine-tuned for 3-class veracity classification (True/False/Conflicting) with either short context (1 evidence, 256 tokens) or long context (3 evidences, 1024 tokens). The model is trained with learning rate 2e-5 using cross-entropy loss, with the best configuration using 1 evidence per question and standard left-to-right tokenization.

## Key Results
- Best system achieved macro F1 of 0.57, placing 4th among 10 submissions
- Longer context window (3 evidences vs 1) did not improve performance
- Right-to-left tokenization degraded performance from 0.52 to 0.45-0.47 macro F1
- Training F1 (0.75) significantly exceeded validation F1 (0.56), indicating overfitting to weak evidence

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Retrieval for Evidence Grounding
- Claim: A two-step retrieval process is necessary to locate relevant numerical evidence within a large corpus before classification.
- Mechanism: The system first uses BM25 (sparse retrieval) to cast a wide net for candidate documents based on keyword matching. It then applies a cross-encoder reranker (`ms-marco-MiniLM-L-12-v2`) to score semantic relevance between the claim and the candidate snippets, selecting the top passages.
- Core assumption: Relevant numerical facts are present in the corpus, but lexical matching alone is insufficient to capture the semantic relationship between a claim and its evidence.
- Evidence anchors:
  - [abstract] "...building our own evidence retrieval pipeline."
  - [section 4.1] "We followed a standard two-step approach... sparse BM25 retrieval, followed by a reranking step."
  - [corpus] The neighbor paper "A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition" reinforces that retrieval is a distinct, critical stage.
- Break condition: If the top-k retrieved documents do not contain the specific numerical value or entity required to verify the claim, the subsequent classifier will hallucinate or default to the majority class.

### Mechanism 2: Claim Decomposition for Complex Queries
- Claim: Decomposing complex numerical claims into sub-questions improves retrieval precision.
- Mechanism: An LLM (GPT-4o-mini) breaks down a multifaceted claim into three smaller, atomic sub-claims. These sub-claims are used as queries for the retrieval system, aiming to surface evidence for specific components of the main claim.
- Core assumption: Complex claims contain multiple verifiable dimensions that are difficult to match against a single document; querying atomic units increases the probability of finding relevant evidence.
- Evidence anchors:
  - [section 4.1] "...perform with a claim decomposition by using GPT-4o-mini, which aims to split the original underlying claim into 3 separate smaller claims."
  - [corpus] "Fact in Fragments" supports this strategy, noting complex claims require deconstruction.
  - [corpus] Weak direct evidence in the provided corpus regarding the specific efficacy of *GPT-4o-mini* for this specific task, relying mostly on the paper's internal description.
- Break condition: If the decomposition logic introduces drift (e.g., altering the meaning of the original claim) or if the sub-queries become too granular to find matches in the evidence corpus, performance degrades.

### Mechanism 3: Context Capacity vs. Evidence Quality
- Claim: Increasing the context window (input length) does not improve veracity prediction if the retrieved evidence quality is poor.
- Mechanism: The authors utilize ModernBERT, which supports up to 8,192 tokens, to test if feeding more evidence snippets (1 vs 3) improves accuracy. The model attends to a larger context, but if the additional snippets are irrelevant (noise), the attention mechanism cannot extract signal.
- Core assumption: The bottleneck in numerical fact verification is the *relevance* of the retrieved text, not the model's capacity to process volume.
- Evidence anchors:
  - [abstract] "A longer context window does also not enhance veracity performance either, highlighting evidence quality as the dominant bottleneck."
  - [section 5] "Whilst training performance between short-context... and long-context... vary significantly, validation performance is similar."
  - [corpus] General consensus in neighbors (e.g., "ClaimIQ") suggests retrieval quality is a primary constraint.
- Break condition: If evidence retrieval were perfect (high precision), a longer context window might actually show improvements by providing more comprehensive background; the "bottleneck" diagnosis is conditional on the current retrieval performance.

## Foundational Learning

- Concept: **Sparse vs. Dense Retrieval**
  - Why needed here: The pipeline relies on BM25 (sparse) for initial recall and a cross-encoder (dense/semantic) for ranking. Understanding the difference is crucial for debugging why a relevant document might be missed.
  - Quick check question: Does the system use vector similarity for the first step (retrieval) or the second step (reranking)?

- Concept: **R2L (Right-to-Left) Tokenization**
  - Why needed here: The paper explicitly tests if changing how numbers are tokenized (processing digits from right to left) aids reasoning, a technique borrowed from arithmetic tasks.
  - Quick check question: Why might standard left-to-right tokenization obscure the magnitude of a number (e.g., distinguishing 100 from 1000) in a transformer model?

- Concept: **Macro F1 Score**
  - Why needed here: The dataset is imbalanced (57% False). Accuracy would be misleading; Macro F1 ensures the model is penalized for ignoring the minority "True" and "Conflicting" classes.
  - Quick check question: If a model predicts "False" for every single input, would Accuracy or Macro F1 better reflect the failure to detect "True" claims?

## Architecture Onboarding

- Component map:
  Input: Raw Claim -> Decomposition: GPT-4o-mini (splits claim → 3 sub-claims) -> Retrieval: BM25 (retrieves 50 docs per sub-claim) -> Reranking: Cross-Encoder (selects top 1-3 docs) -> Classification: ModernBERT (Veracity: True/False/Conflicting)

- Critical path: The **Retrieval → Reranking** step. The authors conclude that errors here propagate to the classifier. If the evidence snippet does not contain the number in question, the NLI model cannot verify it.

- Design tradeoffs:
  - **Long vs. Short Context**: The authors chose to stick with fewer evidences (Short Context) for the final submission despite ModernBERT's long context capabilities, because "more noise" (weak evidences) did not help.
  - **LoRA vs. Full Finetuning**: They tested PEFT (LoRA) but found significant pipeline overhead made it less efficient than expected.

- Failure signatures:
  - **High Training F1, Low Validation F1**: Indicates overfitting, driven by the model memorizing claim-evidence pairs rather than learning numerical reasoning.
  - **R2L Performance Drop**: If using R2L tokenization, expect a drop in F1 (0.38 train vs 0.50 standard) in this specific NLI context, contrary to arithmetic benchmarks.

- First 3 experiments:
  1. **Baseline Retrieval**: Run the provided BM25 + Reranker pipeline on the test set without decomposition to isolate the retrieval quality.
  2. **Context Ablation**: Train ModernBERT with 1 evidence (256 tokens) vs. 3 evidences (1024 tokens). Verify if the "evidence bottleneck" holds for your specific data split.
  3. **Error Analysis on "Conflicting"**: The model struggles most with the "Conflicting" class (F1 0.36-0.41). Manually inspect 20 false negatives to see if they require multi-hop reasoning or if the evidence was simply missing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid retrieval methods combining dense and sparse representations, alongside LLM-based reranking, overcome the evidence quality bottleneck?
- Basis in paper: [explicit] The authors state in Section 6 that replacing BM25 with hybrid retrieval and LLM-based reranking could enhance evidence quality, which is the current primary bottleneck.
- Why unresolved: The current study relied on a standard BM25 and cross-encoder pipeline, which yielded evidence of insufficient quality to benefit from longer context windows.
- What evidence would resolve it: A significant increase in macro F1 score and evidence relevance scores when dense-sparse retrieval and LLM rerankers are integrated into the pipeline.

### Open Question 2
- Question: Does explicit normalization of numbers and dates in claims and evidence improve the semantic matching capabilities of veracity classifiers?
- Basis in paper: [explicit] Section 6 proposes that normalizing numerical and temporal expressions may help language models better capture semantic equivalence in embedding space.
- Why unresolved: The current models processed claims and evidence without specific normalization, potentially hindering their ability to associate semantically similar numerical concepts.
- What evidence would resolve it: Comparative ablations showing improved performance on claims containing varied numerical formats (e.g., "1,000" vs. "one thousand") after applying normalization techniques.

### Open Question 3
- Question: Can ensembles of veracity classifiers leveraging diverse architectures improve robustness and reduce the generalization gap?
- Basis in paper: [explicit] Section 6 suggests leveraging ensembles of classifiers to integrate complementary strengths and improve robustness.
- Why unresolved: The best submission experienced a performance drop from validation to test sets, indicating that single models may lack robustness or overfit to the validation distribution.
- What evidence would resolve it: Higher and more consistent performance across validation and test splits when using an ensemble approach compared to any single constituent model.

## Limitations

- The conclusion about evidence quality being the bottleneck is based on experiments within a single hybrid retrieval framework, limiting generalizability to other retrieval methods.
- The impact of claim decomposition on retrieval quality is not directly measured or isolated.
- The paper lacks error analysis stratified by claim type (date vs. quantity vs. percentage), preventing identification of format-specific challenges.
- No ablation studies on retrieval hyperparameters (BM25 k1/b values, reranker threshold) means the pipeline's robustness remains unknown.

## Confidence

- **High Confidence:** The empirical finding that R2L tokenization does not improve NLI performance for numerical claims (contrary to arithmetic benchmarks). The training/validation F1 gap (0.75 vs. 0.56) clearly indicates overfitting to weak evidence.
- **Medium Confidence:** The assertion that "evidence quality is the bottleneck" is reasonable given the results, but it is a relative claim within the tested pipeline. Alternative retrieval strategies might shift the bottleneck elsewhere.
- **Low Confidence:** The specific impact of claim decomposition on retrieval quality, as the paper does not isolate or measure this effect independently.

## Next Checks

1. **Retrieval Quality Audit:** Manually annotate 50 randomly selected claims to verify whether the top-1 retrieved evidence snippet actually contains the numerical value or entity required to verify the claim. Calculate precision@k to quantify the retrieval bottleneck.
2. **Context Capacity Test:** Train the same ModernBERT model with 3 high-quality evidence snippets (curated to be relevant) versus 3 random snippets. If performance improves only with relevant evidence, it strengthens the "evidence quality" hypothesis.
3. **Tokenization Ablation with Alternative Models:** Test R2L tokenization on a non-NLI numerical reasoning task (e.g., DROP-style arithmetic QA) using the same ModernBERT model. If R2L improves arithmetic but not NLI, it confirms the domain-specificity of the technique.