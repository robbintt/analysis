---
ver: rpa2
title: 'Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert
  Knowledge into Algorithmic Planning'
arxiv_id: '2601.11479'
source_url: https://arxiv.org/abs/2601.11479
tags:
- allocation
- coverage
- advice
- health
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces LEG, a framework for upgrading health facilities
  in Ethiopia that jointly optimizes population coverage and alignment with expert
  guidance. The framework integrates a provable approximation algorithm for population
  coverage with LLM-driven iterative refinement, enabling human-AI alignment to ensure
  solutions reflect expert qualitative guidance while preserving coverage guarantees.
---

# Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning

## Quick Facts
- arXiv ID: 2601.11479
- Source URL: https://arxiv.org/abs/2601.11479
- Reference count: 40
- Key outcome: LEG framework jointly optimizes population coverage and expert alignment for health facility upgrades in Ethiopia

## Executive Summary
This paper introduces LEG, a framework that integrates LLMs with provable approximation algorithms to upgrade health facilities in Ethiopia. The approach combines classical submodular maximization with LLM-guided district allocation, enabling iterative refinement based on expert advice while preserving theoretical coverage guarantees. Experiments on real-world data from three Ethiopian regions demonstrate that LLM-guided iterations significantly improve alignment between algorithmic allocations and expert advice while maintaining high coverage efficiency.

## Method Summary
The LEG framework iterates between standard greedy allocation, LLM district-level allocation based on expert advice, and a GuidedGreedy algorithm that maps districts to specific grid cells while enforcing α-β guarantees. The process uses verbal feedback to update prompts across iterations, with a minimum allocation constraint and budget parameters (α, β) controlling the trade-off between coverage and alignment. The framework also extends to sequential budget rounds where prior allocations constrain future decisions.

## Key Results
- LLM-guided iterations achieve 0.10-0.20 higher advice alignment compared to purely quantitative feedback signals
- The framework maintains coverage guarantees (1 - e^{-αβ})f(OPT_b) even with LLM-directed deviations from greedy choices
- Short feedback windows (1 iteration) are sufficient for stable convergence in alignment improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The GuidedGreedy algorithm preserves theoretical coverage guarantees while allowing LLM-guided district allocation.
- Mechanism: The algorithm ensures that at least ⌈αb⌉ selected cells achieve marginal gains ≥ β times the unconstrained maximum. This guarantees f(S) ≥ (1 - e^{-αβ})f(OPT_b) even when LLM-directed selections deviate from pure greedy choices.
- Core assumption: The coverage function f is monotone submodular (population coverage satisfies this property).
- Evidence anchors:
  - [section 4.2, Theorem 4.1]: "f(S_limit) ≥ (1 - e^{-αβ})f(OPT_b)"
  - [section 4.1, Algorithm 2]: Lines 8-12 enforce the α-β constraint during selection
  - [corpus]: Weak - related papers focus on facility location optimization but not this specific hybrid guarantee mechanism
- Break condition: If α=0 or β=0, the guarantee collapses to 0; if coverage function is not submodular, the bound does not hold.

### Mechanism 2
- Claim: Verbal feedback improves alignment with expert advice better than purely quantitative feedback signals.
- Mechanism: Natural-language reflections allow the LLM to reason about qualitative trade-offs (e.g., "prioritize districts with poor health outcomes") that resist scalarization. The feedback is incorporated via prompt optimization: P_{i+1} = P_i + Feedback_i.
- Core assumption: The LLM can translate textual advice comparisons into meaningful allocation adjustments.
- Evidence anchors:
  - [section 5.2]: "Across all three regions, the verbal-feedback variant achieved notably higher advice alignment"
  - [Figure 5-7]: Shows verbal feedback achieving ~0.65-0.75 alignment vs. ~0.55-0.65 for quantitative across regions
  - [corpus]: Weak - no corpus papers test verbal vs. quantitative feedback in optimization
- Break condition: If advice is highly contradictory or incoherent, verbal reasoning may produce unstable allocations.

### Mechanism 3
- Claim: Short feedback windows (1-iteration history) are sufficient for stable convergence.
- Mechanism: The LLM internalizes historical patterns from the prompt's accumulated editable section, making explicit multi-step computation redundant. P_Editable accumulates all past feedback automatically.
- Core assumption: The LLM can infer temporal trends from compressed prompt history.
- Evidence anchors:
  - [section 5.4]: "after ten iterations, the observed differences between the two best variants (1-step and 3-step windows) were minimal"
  - [Figure 10]: Shows nearly identical performance curves for 1-step vs 3-step
  - [corpus]: No corpus evidence on feedback window length in LLM-optimization hybrids
- Break condition: If feedback requires complex multi-step reasoning (not tested in this paper), longer windows may be needed.

## Foundational Learning

- Concept: **Submodular function maximization**
  - Why needed here: The coverage function f is submodular; understanding why greedy gives (1-1/e) approximation is essential for interpreting Theorem 4.1.
  - Quick check question: Explain why adding a facility to a sparsely-covered area gives higher marginal gain than adding to a well-covered area.

- Concept: **Multi-objective optimization and Pareto frontiers**
  - Why needed here: The problem jointly optimizes coverage f(S) and alignment g(S); classical scalarization fails when preferences are qualitative.
  - Quick check question: Why can't we simply define a weighted objective w₁f(S) + w₂g(S)?

- Concept: **Human-AI alignment via language models**
  - Why needed here: The LLM bridges formal optimization and informal expert knowledge; understanding prompt engineering and feedback loops is critical.
  - Quick check question: What happens if the LLM's allocation suggestion violates the minimum district constraint?

## Architecture Onboarding

- Component map:
  - **Greedy()** -> **LLM District Allocator** -> **GuidedGreedy(α, β, b, V, d)** -> **Feedback Generator** -> **Prompt Optimizer**

- Critical path:
  1. Data preparation: Population grid, facility locations, walking accessibility (2-hour threshold)
  2. Advice encoding: Collect 20 expert sentences (5 per expert with intentional contradictions)
  3. Alignment function synthesis: Use LLM to generate g_eval(S) for evaluation
  4. Iteration loop (typically 10 iterations): Greedy → LLM → GuidedGreedy → Feedback → Prompt update
  5. Multi-round extension: For sequential budgets, pass existing allocations as minimum constraints

- Design tradeoffs:
  - α close to 1: Higher coverage, lower alignment (closer to pure greedy)
  - α close to 0: Higher alignment, lower coverage (more LLM control)
  - β < 1: Allows LLM to pick lower-marginal-gain cells aligned with advice
  - β = 1: Stricter adherence to greedy-quality selections

- Failure signatures:
  - LLM ignores minimum allocation constraint (mitigate with retry logic in Step 2)
  - Coverage drops below theoretical guarantee (check α-β parameterization)
  - Alignment oscillates without converging (increase prompt stability or reduce feedback verbosity)

- First 3 experiments:
  1. **Reproduce Figure 5-7 baseline**: Run verbal vs. quantitative feedback on one region with α=0, β=1; verify alignment improvement over 10 iterations
  2. **Vary α with fixed β=1**: Reproduce Figure 8; confirm coverage increases and alignment decreases as α increases
  3. **Test multi-round setting**: Simulate 3 budget rounds with Algorithm 3; verify cumulative allocation preserves guarantees

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the LEG framework perform when deployed with actual human experts rather than LLM-simulated expertise, particularly when expert advice contains ambiguities or evolves during the planning process?
- Basis in paper: [explicit] The paper states "To emulate multi-stakeholder guidance, we queried Gemini-2.5-Pro to generate 20 expert advice sentences... to mimic real decision environments."
- Why unresolved: Real expert guidance may differ in structure, specificity, and internal consistency from LLM-generated synthetic advice, potentially affecting convergence behavior and alignment metrics in ways the current experiments cannot capture.
- What evidence would resolve it: Deployment studies with actual Ethiopian health officials providing guidance, comparing performance metrics against synthetic experiments.

### Open Question 2
- Question: How does the multi-round online budget extension (Algorithm 3) perform empirically when applied to sequential multi-year planning scenarios with accumulating constraints?
- Basis in paper: [inferred] Algorithm 3 and Theorem 4.2 provide theoretical guarantees for online budgets, but all experimental validation (Sections 5.2–5.6) tests only single-round allocation without empirical validation of the sequential setting.
- Why unresolved: The theoretical guarantees for online budgets remain untested in practice, leaving uncertainty about actual performance when budgets arrive sequentially and prior allocations constrain future decisions.
- What evidence would resolve it: Experiments simulating multi-year scenarios with sequential budget arrivals, measuring whether coverage guarantees hold and how alignment evolves across rounds.

### Open Question 3
- Question: What principled methodology can guide stakeholders in selecting appropriate (α, β) trade-off parameters for different regional contexts?
- Basis in paper: [inferred] The paper notes "We anticipate that different regions in Ethiopia will select varying guarantee parameters" and demonstrates in Figure 9 that different α values produce substantially different spatial allocations, but offers no guidance on parameter selection.
- Why unresolved: Without selection heuristics, practitioners lack principled approaches to balance coverage guarantees against expert alignment based on regional characteristics.
- What evidence would resolve it: Sensitivity analysis across parameter combinations with domain expert input to develop context-aware selection guidelines.

### Open Question 4
- Question: How robust is the framework to variations in LLM model choice, prompt formulation, or quality of advice alignment evaluation?
- Basis in paper: [inferred] The framework relies on Gemini-2.5-Flash for iterative allocation and Gemini-2.5-Pro for advice generation and alignment scoring (Section 5.1), with prompt optimization depending on LLM-generated feedback quality, but no experiments test robustness to these design choices.
- Why unresolved: LLM outputs vary across models and prompts; understanding sensitivity to these choices is critical for reliable deployment.
- What evidence would resolve it: Ablation studies comparing performance across different LLM backbones, prompt templates, and alignment evaluation methods.

## Limitations
- Exact wording of synthetic advice sentences and LLM-based verifier not provided, making exact metric reproduction impossible
- GIS processing pipeline for converting raw data to grid structure underspecified
- Limited external validation since no corpus papers test verbal vs. quantitative feedback comparison

## Confidence

- **High confidence**: The theoretical coverage guarantee (1 - e^{-αβ})f(OPT_b) is mathematically sound given submodularity assumptions
- **Medium confidence**: Empirical results showing verbal feedback superiority are internally consistent across three regions
- **Low confidence**: Claim that 1-iteration feedback windows suffice for stable convergence has minimal supporting evidence

## Next Checks

1. **Verify coverage guarantee preservation**: Implement Algorithm 2 and confirm that for α=0.5, β=1.0, and b=20, the final allocation achieves at least (1 - e^{-0.5}) ≈ 0.39 fraction of the optimal coverage across multiple random seeds.

2. **Replicate verbal vs. quantitative feedback comparison**: Run both feedback variants on a single region (e.g., Afar) for 10 iterations with α=0, β=1.0, and verify that verbal feedback achieves >0.10 higher alignment score than quantitative feedback in later iterations.

3. **Test multi-round budget extension**: Simulate 3 sequential budget rounds with Algorithm 3, using cumulative allocations as minimum constraints. Verify that coverage grows monotonically and alignment remains stable across rounds.