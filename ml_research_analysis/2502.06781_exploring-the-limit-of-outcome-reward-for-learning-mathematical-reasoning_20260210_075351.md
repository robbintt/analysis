---
ver: rpa2
title: Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning
arxiv_id: '2502.06781'
source_url: https://arxiv.org/abs/2502.06781
tags:
- arxiv
- reasoning
- reward
- policy
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the limit of outcome reward-based reinforcement
  learning for mathematical reasoning tasks. The authors propose a new framework called
  OREAL that uses only binary outcome rewards (correct/incorrect) to train mathematical
  reasoning models.
---

# Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning

## Quick Facts
- arXiv ID: 2502.06781
- Source URL: https://arxiv.org/abs/2502.06781
- Reference count: 40
- Primary result: OREAL achieves 94.0% pass@1 on MATH-500 with a 7B model, matching previous 32B models

## Executive Summary
This paper presents OREAL (Outcome Reward Exploration for Automated Learning), a reinforcement learning framework that achieves state-of-the-art performance on mathematical reasoning tasks using only binary outcome rewards (correct/incorrect). The key insight is that behavior cloning on positive trajectories sampled through best-of-N generation, combined with a token-level reward model to identify important reasoning steps, is sufficient to learn high-quality mathematical reasoning policies. The framework theoretically proves that this approach converges to the KL-regularized optimal policy in binary feedback environments, addressing the challenge of sparse rewards in long reasoning chains.

## Method Summary
OREAL operates by first generating multiple candidate solutions for each problem using best-of-N sampling, then training a token-level reward model to identify important reasoning steps in successful trajectories. The framework performs behavior cloning on these positive trajectories, with special emphasis on tokens identified as important by the reward model. This approach leverages the theoretical guarantee that behavior cloning on positive trajectories is sufficient to learn the optimal policy under KL-regularization when only binary outcome rewards are available. The token-level reward model addresses the challenge of sparse rewards in long reasoning chains by providing denser supervision signals during training.

## Key Results
- OREAL-7B achieves 94.0% pass@1 accuracy on MATH-500, matching previous 32B models
- OREAL-32B surpasses all previous models with 95.0% pass@1 accuracy
- State-of-the-art performance achieved using only binary outcome rewards (correct/incorrect)

## Why This Works (Mechanism)
The framework works by theoretically proving that behavior cloning on positive trajectories from best-of-N sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. The token-level reward model identifies important tokens in reasoning trajectories, providing denser supervision signals that address the sparse reward problem inherent in long reasoning chains where only the final answer is evaluated.

## Foundational Learning
1. **KL-regularized reinforcement learning**: Why needed - provides theoretical foundation for behavior cloning convergence guarantees; Quick check - verify that the policy update maintains KL-divergence bounds
2. **Best-of-N sampling**: Why needed - generates high-quality positive trajectories for behavior cloning; Quick check - measure improvement in trajectory quality vs. N parameter
3. **Token-level reward modeling**: Why needed - addresses sparse rewards by identifying important reasoning steps; Quick check - evaluate reward model accuracy on held-out trajectories

## Architecture Onboarding
**Component map**: Problem Generator -> Best-of-N Sampler -> Token-Level Reward Model -> Behavior Cloning Trainer -> Policy Network

**Critical path**: The critical path flows from problem generation through best-of-N sampling to identify positive trajectories, then through the token-level reward model to weight important tokens, and finally to behavior cloning training of the policy network.

**Design tradeoffs**: The framework trades computational cost of multiple generations (best-of-N) for improved learning signal quality, and adds complexity of token-level reward modeling to address sparse rewards. This design prioritizes sample efficiency and theoretical guarantees over simplicity.

**Failure signatures**: Poor performance may indicate inadequate positive trajectory generation (insufficient N), inaccurate token-level reward modeling (misidentifying important tokens), or insufficient behavior cloning iterations. Validation should check each component independently.

**First experiments**:
1. Verify best-of-N sampling generates quality positive trajectories by comparing pass rates at different N values
2. Test token-level reward model accuracy on identifying important tokens in known good solutions
3. Perform ablation study comparing OREAL with standard behavior cloning on full trajectories

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on idealized assumptions about reward model accuracy and sampling quality that may not hold in practice
- Binary outcome rewards may not capture the full spectrum of intermediate reasoning quality
- Evaluation is primarily on MATH dataset, generalizability to other mathematical domains is unexplored

## Confidence
**High**: Experimental results demonstrating state-of-the-art performance on MATH-500 are robust and well-documented.

**Medium**: Theoretical analysis provides sound intuition but relies on idealized assumptions; token-level reward model effectiveness is empirically demonstrated but could benefit from additional analysis.

**Medium**: Claim that binary outcome rewards alone are sufficient is supported by results but may not extend to more complex mathematical domains.

## Next Checks
1. Conduct ablation studies removing the token-level reward model to quantify its exact contribution to performance gains
2. Evaluate OREAL on diverse mathematical reasoning datasets beyond MATH to assess generalizability
3. Perform sensitivity analysis on the best-of-N sampling parameter to determine how sampling budget affects performance across different model sizes and problem difficulties