---
ver: rpa2
title: 'J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge'
arxiv_id: '2505.11875'
source_url: https://arxiv.org/abs/2505.11875
tags:
- answ
- attempt
- arxiv
- ckpt
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether simple test-time scaling (STTS)
  can enhance the capabilities of LLM-as-a-judge systems. While existing approaches
  lack consistent scaling behavior under STTS, the authors propose a two-stage training
  method that combines supervised fine-tuning on a reflection-enhanced dataset with
  reinforcement learning using verifiable rewards.
---

# J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge

## Quick Facts
- **arXiv ID**: 2505.11875
- **Source URL**: https://arxiv.org/abs/2505.11875
- **Reference count**: 40
- **Primary result**: J1-7B achieves 4.8% improvement over state-of-the-art LLM-as-a-judge and demonstrates stronger scaling behavior under test-time scaling

## Executive Summary
This paper investigates simple test-time scaling (STTS) for LLM-as-a-judge systems, addressing the gap where existing approaches lack consistent scaling behavior. The authors propose a two-stage training method combining supervised fine-tuning on reflection-enhanced data with reinforcement learning using verifiable rewards. Their model J1-7B not only surpasses previous state-of-the-art by 4.8% but also demonstrates a 5.1% stronger scaling trend under STTS. The research reveals that effective STTS capability emerges predominantly during the RL phase rather than from SFT alone, and that incorporating reasoning-intensive data during cold-start training further improves STTS effectiveness.

## Method Summary
The authors develop a two-stage training framework for J1-7B. First, they employ supervised fine-tuning (SFT) on a reflection-enhanced dataset to establish baseline capabilities. Second, they apply reinforcement learning (RL) using verifiable rewards to enhance test-time scaling abilities. A key innovation is the cold-start training strategy that incorporates reasoning-intensive data to improve STTS effectiveness. The training pipeline leverages reflective reasoning strategies and structured reward signals to develop more reliable and scalable evaluation capabilities.

## Key Results
- J1-7B outperforms previous state-of-the-art LLM-as-a-judge by 4.8% on benchmark evaluations
- Demonstrates 5.1% stronger scaling trend under simple test-time scaling compared to baselines
- STTS capabilities emerge predominantly during RL phase rather than SFT, with reasoning-intensive cold-start training further enhancing effectiveness

## Why This Works (Mechanism)
The effectiveness stems from the two-stage training approach that separately addresses different aspects of judge capability development. The reflection-enhanced dataset during SFT establishes foundational evaluation skills, while the RL phase with verifiable rewards specifically targets the ability to leverage additional computation time effectively. The cold-start training with reasoning-intensive data primes the model to handle complex evaluation scenarios that benefit from test-time scaling. This structured approach allows the model to develop both baseline competence and the capacity to improve with additional computational resources.

## Foundational Learning

1. **Test-Time Scaling (STTS)**: The ability of models to improve performance with additional inference-time computation
   - Why needed: Enables more thorough evaluation of complex responses
   - Quick check: Does performance improve consistently as inference steps increase?

2. **Reflection-Enhanced Datasets**: Training data that includes explicit reasoning and evaluation steps
   - Why needed: Helps models learn structured evaluation approaches
   - Quick check: Are reflection traces present and of sufficient quality in the training data?

3. **Verifiable Rewards in RL**: Reward signals that can be automatically verified during training
   - Why needed: Enables stable RL training without extensive human feedback
   - Quick check: Are reward signals both meaningful and computationally efficient to verify?

## Architecture Onboarding

**Component Map**: Data Preprocessing -> SFT Training -> RL Training -> Evaluation

**Critical Path**: The two-stage training pipeline is critical, where SFT establishes baseline capabilities and RL specifically develops STTS effectiveness. The reflection-enhanced dataset and verifiable reward signals are essential components that enable this development.

**Design Tradeoffs**: The approach trades increased training complexity (two-stage process with specialized datasets) for improved scaling behavior and evaluation accuracy. The reliance on verifiable rewards enables more scalable training but may miss nuanced aspects of evaluation quality.

**Failure Signatures**: Poor scaling behavior under STTS suggests inadequate RL training or insufficiently challenging verifiable rewards. Suboptimal baseline performance indicates issues with the reflection-enhanced dataset quality or SFT process.

**3 First Experiments**:
1. Test scaling behavior by varying inference steps on a held-out evaluation set
2. Compare SFT-only vs. full two-stage training on baseline evaluation metrics
3. Ablation study removing reflection components from the training dataset

## Open Questions the Paper Calls Out

None

## Limitations

- Evaluation methodology relies heavily on simulated judge evaluations, with limited human validation studies
- Proprietary reflection-enhanced dataset and verifiable rewards framework limit reproducibility
- Cold-start training strategy requires domain-specific data collection that may not generalize across all evaluation domains

## Confidence

**High**: The observation that J1-7B demonstrates improved scaling behavior under STTS compared to baselines

**Medium**: The effectiveness of the two-stage training approach and the superiority over previous models

**Medium**: The claim that STTS capabilities emerge primarily during RL training

## Next Checks

1. Conduct extensive human evaluation studies across multiple domains to validate the correlation between simulated judge scores and human preferences, particularly for complex reasoning tasks

2. Test J1-7B's performance on adversarial prompts and edge cases to assess robustness and identify potential failure modes

3. Investigate the model's behavior on zero-shot and few-shot evaluation scenarios with different temperature settings to understand the practical limits of test-time scaling