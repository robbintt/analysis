---
ver: rpa2
title: 'DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders'
arxiv_id: '2512.13690'
source_url: https://arxiv.org/abs/2512.13690
tags:
- denoising
- diffusion
- steps
- video
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiffusionBrowser addresses the slow generation and limited interactivity
  of video diffusion models by enabling fast, multi-modal previews at any intermediate
  denoising step. It introduces a lightweight, model-agnostic decoder framework that
  predicts RGB and scene intrinsics (albedo, depth, normals) using multi-branch, modality-optimized
  heads trained on intermediate diffusion features.
---

# DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders

## Quick Facts
- **arXiv ID:** 2512.13690
- **Source URL:** https://arxiv.org/abs/2512.13690
- **Reference count:** 40
- **One-line primary result:** Enables <1s multi-modal video previews at any intermediate denoising step with PSNR gains up to 1.1 dB and strong user preference (74.6-76.9%) over baselines.

## Executive Summary
DiffusionBrowser introduces a lightweight, model-agnostic decoder framework that enables fast, interactive previews of video diffusion models at any intermediate denoising step. By predicting RGB and scene intrinsics (albedo, depth, normals) from intermediate diffusion features using multi-branch, modality-optimized heads, the method allows users to generate previews in under one second while preserving full model fidelity. The approach addresses the slow generation and limited interactivity of video diffusion models, supporting early termination and real-time steering through gradient-based feature modification.

## Method Summary
The method caches intermediate features from video diffusion models (specifically Wan 2.1 DiT) at various timesteps and transformer blocks during denoising. A 4-branch decoder with shared convolutional layers and modality-specific heads predicts six-channel outputs (base color, depth, normals, metallic, roughness, RGB) at ~208×120 resolution. Each branch is trained with L1 loss for most modalities and cosine loss for normals, combined with an ensemble MSE loss to mitigate the superposition problem where intermediate features encode multiple plausible futures. The framework also enables interactive steering via gradient-based feature optimization, allowing targeted modifications that propagate through subsequent denoising steps.

## Key Results
- PSNR gains of up to 1.1 dB over baselines at 10% denoising steps
- 4× faster inference enabling previews in under one second
- Strong user preference rates: 74.6% for content predictability, 72.9% for visual fidelity, 76.9% for scene clarity
- Stable geometric and structural signals emerge early in denoising, enabling interactive variation generation

## Why This Works (Mechanism)

### Mechanism 1
Scene intrinsics (depth, normals, albedo) emerge early in diffusion denoising, enabling fast preview generation. Linear probing experiments show predictive power for intrinsics saturates at 5th-15th of 50 timesteps and 10th-20th of 30 transformer blocks. Depth and normals are predictable earlier than RGB because they are lower-frequency signals representing coarse geometry. This works because diffusion models implicitly learn inverse rendering, encoding 3D scene structure despite training only on 2D data.

### Mechanism 2
Multi-branch decoders mitigate the "superposition problem" where intermediate features encode multiple plausible futures simultaneously. At high-noise timesteps, MSE-trained models predict posterior means that blur across modes. K=4 independent decoder branches, trained with branch-wise mode-seeking loss (L1 + perceptual) and ensemble MSE loss, encourage each branch to specialize in one plausible trajectory rather than averaging. This works because explicit multi-hypothesis decoding resolves blurry artifacts in high-motion/complex regions.

### Mechanism 3
Preview decoders enable real-time generation steering via gradient-based feature modification. Given frozen decoder D mapping features f_{t,b} to intrinsic maps, steering solves min_{f_{t,b}} L(D(f_{t,b}), y*) using Jacobian-based updates. This allows targeted modifications (color shifts, depth gradient enhancement, normal flipping) that propagate through subsequent denoising. This works because feature-space edits at intermediate steps meaningfully affect final outputs without requiring full re-generation.

## Foundational Learning

- **Scene Intrinsics Decomposition**: Decomposes scenes into lighting-independent components (base color, depth, normals, metallicity, roughness). Needed to understand the multi-channel outputs and why lower-frequency signals like albedo emerge earlier. Quick check: Can you explain why albedo (base color) is lower-frequency than RGB and emerges earlier in denoising?

- **Diffusion Denoising Trajectories (SDE/ODE formulation)**: Understanding how x_t evolves through timesteps and how posterior estimation p(x_0|x_t) becomes multimodal at high noise levels is essential for grasping the superposition problem. Quick check: At timestep t=200 of 1000, why might the estimated x̂_0 contain superimposed artifacts from multiple plausible clean states?

- **Multi-Hypothesis / Ensemble Decoding**: The K-branch architecture requires understanding how branch-wise and ensemble losses interact to balance mode specialization with mean accuracy. Quick check: What happens to branch diversity if you remove the ensemble loss term (λ_ens)?

## Architecture Onboarding

- **Component map:** Input features f_{t,b} -> 4 shared 3D conv layers -> K=4 parallel branches (each with 2 upscaling 3D conv layers) -> 6-channel output per branch -> Ensemble averaging -> Final preview

- **Critical path:** 1) Cache intermediate features f_{t,b} during base model denoising 2) Pass through shared conv layers 3) Each branch produces 6-channel output 4) Ensemble averaging for display 5) (Optional) Steering: compute gradient ∂L/∂f_{t,b}, update features, continue denoising

- **Design tradeoffs:** Branch count K: 4 branches balance mode coverage vs. compute; fewer risks collapse, more has diminishing returns. Decoder depth: 6 layers (4 shared + 2 upscaling) balances accuracy vs. latency (0.53s vs. 0.47s for linear probe). Preview resolution: ~208×120 keeps overhead minimal; upsampling for display.

- **Failure signatures:** Blurry patches in high-motion regions → insufficient branch diversity or too-early timestep. Steered intrinsics disappearing by t=0 → out-of-distribution features or weak base model 3D priors. Metallic/roughness predictions worse than other channels → less stable signal in features (Table 2 shows higher L1 error).

- **First 3 experiments:** 1) Linear probing baseline: Train single linear layer per modality at each block/timestep combination to replicate Figure 2 curves and validate feature quality. 2) Branch ablation: Compare K=1, K=2, K=4, K=8 on validation L1 error and visual mode separation using the toy tri-modal dataset. 3) Timestep sweep: Measure PSNR for each modality at 4%, 10%, 16%, 20% denoising steps to determine earliest viable preview point.

## Open Questions the Paper Calls Out

### Open Question 1
Can the multi-branch decoder architecture effectively mitigate the superposition problem in few-step distilled diffusion models, where limited NFEs cause exacerbated hallucinations? The authors demonstrated the superposition problem on a toy dataset and proposed multi-branch decoding as a solution for previews, but did not test whether this approach transfers to actual few-step distillation scenarios where quality degradation is well-documented.

### Open Question 2
How do intrinsic previews interact with text-driven conditioning, and can text prompts be incorporated into the preview generation process? The current framework decodes intrinsics from intermediate diffusion features without explicitly modeling how text conditioning influences the early emergence of geometry, layout, or appearance.

### Open Question 3
What causes steered intrinsics to dissipate during subsequent denoising steps, and can decoder architecture or training modifications improve steering persistence? The authors identified out-of-distribution features as a likely cause but did not validate this hypothesis or test whether deeper decoders, different training objectives, or in-distribution regularization could preserve steered modifications through the full denoising trajectory.

### Open Question 4
Can the multi-branch architecture scale to higher-resolution previews while maintaining mode separation quality? Current previews operate at roughly 208×120 resolution with 4 branches; it is unclear whether the mode-seeking benefits persist at higher resolutions where computational costs and feature complexity increase substantially.

## Limitations
- Multi-branch training setup reproducibility is uncertain due to unspecified feature extraction points and DiffusionRenderer pipeline details
- Steering mechanism is explicitly limited to early timesteps due to out-of-distribution feature states
- User study methodology lacks detail on participant selection, trial design, and statistical validation

## Confidence

- **High confidence**: The early emergence of scene intrinsics from diffusion features is well-supported by linear probing experiments showing predictable patterns at 5th-15th timesteps and 10th-20th blocks.
- **Medium confidence**: The multi-branch architecture's effectiveness against superposition is demonstrated convincingly in controlled toy experiments, but real-world validation is limited to qualitative comparisons.
- **Medium confidence**: Preview quality improvements are measured, but the comparison baseline may not represent state-of-the-art lightweight decoders, and user study preferences are methodologically under-specified.

## Next Checks

1. **Ablation study of ensemble loss**: Train MB decoder with and without the ensemble MSE term (λ_ens=0) on the same validation set to quantify the exact contribution of mode specialization vs. mean accuracy. Measure both PSNR and mode coverage metrics (e.g., KL divergence between branch predictions).

2. **Robustness across model architectures**: Apply the same MB decoder framework to a different base diffusion model (e.g., Stable Video Diffusion) using the same synthetic video dataset. Compare intrinsic prediction quality (L1/PSNR) and preview generation speed to test model-agnostic claims.

3. **Long-range steering evaluation**: Systematically test steering edits at multiple intermediate timesteps (10%, 20%, 30%, 40% denoising) and track feature dissipation rates through subsequent timesteps. Quantify how much steering signal survives to final output and identify the critical timestep beyond which edits become ineffective.