---
ver: rpa2
title: 'IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in
  Multimodal LLMs'
arxiv_id: '2504.15415'
source_url: https://arxiv.org/abs/2504.15415
tags:
- video
- image
- reasoning
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IV-Bench is the first benchmark for evaluating image-grounded video
  perception and reasoning in multimodal LLMs, containing 967 videos and 2,585 image-text
  queries across 13 tasks. State-of-the-art models achieved at most 28.9% accuracy,
  substantially underperforming on tasks requiring image context for video comprehension.
---

# IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs

## Quick Facts
- **arXiv ID:** 2504.15415
- **Source URL:** https://arxiv.org/abs/2504.15415
- **Reference count:** 21
- **Primary result:** First benchmark for image-grounded video perception and reasoning in multimodal LLMs, achieving maximum 28.9% accuracy across 967 videos and 2,585 image-text queries.

## Executive Summary
IV-Bench introduces a novel benchmark to evaluate how well multimodal large language models (MLLMs) can ground visual reasoning in both video content and external images. Unlike traditional video QA datasets, IV-Bench requires models to jointly process an externally sourced image and video to answer multiple-choice questions across 13 tasks spanning perception and reasoning. The benchmark reveals that current state-of-the-art models struggle significantly with this dual-modality grounding, with best performers achieving only 28.9% accuracy—substantially below human-level performance. The work identifies key architectural and inference patterns that influence performance, particularly the benefit of placing image context after video frames for larger models.

## Method Summary
The benchmark evaluates 27 multimodal LLMs on 967 videos (minimum 5 minutes each) paired with 2,585 image-text queries across 13 tasks (7 perception, 6 reasoning). Videos are uniformly sampled at 32 frames, and models process inputs in video-first order (video frames → image → question text). Performance is measured by accuracy on 10-option multiple-choice questions, with separate averages for perception and reasoning tasks. A simple data synthesis approach using existing video QA datasets (llava-video178k) was tested to assess whether format alignment alone could improve performance, but yielded only marginal gains, suggesting fundamental limitations in cross-modal grounding capabilities.

## Key Results
- State-of-the-art models achieved at most 28.9% accuracy on IV-Bench tasks
- Larger models (>26B parameters) significantly benefit from placing image contexts after video frames
- Smaller models (<10B parameters) show minimal improvement from image contexts
- Increasing frame number and video resolution positively impacts performance
- Simple data synthesis from existing video QA datasets yielded only marginal improvements

## Why This Works (Mechanism)

### Mechanism 1: Context Recency in Multimodal Inference
Large models leverage recency bias in attention mechanisms, with image contexts placed after video frames performing better because they avoid being "forgotten" by attention scores. Smaller models show minimal change because their attention mechanisms don't effectively utilize the image context regardless of position.

### Mechanism 2: Scaling Law for Token Allocation
Model capacity determines efficiency of visual token compression. Smaller models depend heavily on high frame counts to perceive video details, while larger models can extract complementary signals from both spatial and temporal dimensions, effectively trading spatial for temporal detail.

### Mechanism 3: Fundamental Generalization Gap vs. Data Alignment
Poor performance stems from fundamental inability to ground visual entities across modalities rather than data format issues. Simple synthesis pipelines aligned data structure but failed to teach underlying cross-modal retrieval and reasoning logic required for image-grounded video understanding.

## Foundational Learning

- **Visual Token Budgeting (Spatial vs. Temporal)**
  - Why needed here: Engineers must decide how to spend fixed context window on video (temporal density vs. spatial clarity)
  - Quick check question: If I halve the frame count but double the resolution, does the model still recognize a small text label (OCR) or a fast-moving object?

- **Cross-Modal Grounding (Image-to-Video Retrieval)**
  - Why needed here: Model must learn that an "Image" is a visual search query for specific object/event within video timeline
  - Quick check question: Does the model answer correctly if image is removed (text priors) or video is removed (image priors)? Both should fail.

- **Attention Dilution in Long Context**
  - Why needed here: 32-frame video generates hundreds of visual tokens, potentially diluting signal from single query image
  - Quick check question: If we move query image to middle of video token sequence, does accuracy drop compared to placing it at end?

## Architecture Onboarding

- **Component map:** Vision Encoder -> Visual Adapter/Projector -> LLM Backbone
- **Critical path:**
  1. Frame Sampling: Uniformly sample 32 frames from video
  2. Encoding: Encode video frames and external image separately
  3. Injection: Concatenate inputs in order: [Video_Tokens] [Image_Tokens] [Text_Query]
  4. Evaluation: Check if model grounds text answer on both modalities

- **Design tradeoffs:**
  - Frame Rate vs. Resolution: Higher resolution helps OCR/spatial tasks, higher frame rate helps temporal grounding
  - Inference Order: "Image-First" risks attention dilution, "Video-First" risks losing video context

- **Failure signatures:**
  - Random Guessing (10-15%): Model fails to process video or understand task
  - Text-Only Bias (20-30%): Model answers correctly without image (relying on common sense)
  - Modality Confusion: Model describes image content but fails to locate it in video

- **First 3 experiments:**
  1. Inference Pattern Test: Run model with "Video+Image" vs. "Image+Video" vs. "Text-Only" to isolate image contribution and ordering impact
  2. Token Budget Ablation: Fix total token count, compare (8 frames, 720p) vs. (64 frames, 240p) to profile model's preference for temporal density or spatial clarity
  3. Sanity Check (NLI Task): Evaluate only on Natural Language Inference subset; low accuracy indicates lack of basic visual alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training methodologies beyond simple data alignment are required to bridge the performance gap in image-grounded video reasoning?
- Basis: Section 4.4 concludes that simple data synthesis yielded only marginal improvements, suggesting challenges stem from "fundamental image-grounded video understanding ability demands"
- Evidence to resolve: Development of specialized training curriculum that enables models to achieve significantly higher accuracy (>50%) compared to baseline

### Open Question 2
- Question: How can "Temporal Reasoning" capabilities be specifically improved in MLLMs, given that it remains most challenging task?
- Basis: Section 4.2 notes Temporal Reasoning remains "especially challenging," and Conclusion states "future research should prioritize developing specialized mechanisms for video reasoning"
- Evidence to resolve: Architectural modification resulting in substantial accuracy increase specifically on "Temporal Reasoning" task subset

### Open Question 3
- Question: What architectural changes are necessary to enable smaller models (<10B parameters) to effectively utilize image contexts for video comprehension?
- Basis: Section 4.3.1 observes smaller models show "minimal benefit from incorporating images" while larger models benefit significantly
- Evidence to resolve: Smaller model (<10B) demonstrating performance parity with larger models on "Existence" or "NLI" tasks when provided with image contexts

### Open Question 4
- Question: What are underlying mechanisms causing "forgotten image" phenomenon when images are placed before video frames?
- Basis: Section 4.3.1 notes placing images after video frames yields superior performance for larger models, hypothesizing early images tend to be "forgotten" but exact cause is not confirmed
- Evidence to resolve: Analysis of attention maps or context utilization showing consistent attention weights on image tokens regardless of temporal position

## Limitations
- Opaque nature of underlying model architectures' video processing pipelines makes it difficult to attribute performance differences to specific architectural choices
- Simple data synthesis approach may not be optimal; more sophisticated techniques could yield different results
- Uniform 32-frame sampling strategy may not be optimal for all video types or tasks requiring fine-grained temporal analysis

## Confidence

- **High Confidence:** Current MLLMs significantly underperform on image-grounded video reasoning tasks (max 28.9% accuracy); larger models benefit from image context placement after video frames
- **Medium Confidence:** Attention dilution/recency bias explains why larger models benefit from video-first inference; poor performance due to fundamental generalization gaps rather than data format alignment
- **Low Confidence:** Specific scaling law for token allocation (smaller models preferring temporal cues, larger models preferring spatial detail); exact parameter thresholds (10B, 70B) are observational rather than theoretically derived

## Next Checks

1. **Ablation Study on Video-Only and Image-Only Inputs:** Run full IV-Bench evaluation with image removed (video-only) and video removed (image-only) for subset of models to quantify each modality's contribution and test cross-modal grounding requirement.

2. **Controlled Frame Sampling Experiment:** Systematically vary number of sampled frames (8, 16, 32, 64) at fixed resolution and measure accuracy on spatial vs. temporal tasks to directly test model's preference for temporal vs. spatial detail.

3. **Advanced Data Synthesis with Targeted Augmentation:** Implement synthesis pipeline that explicitly teaches cross-modal retrieval by generating synthetic images from video content and creating matching questions; compare performance gain against baseline synthetic data to determine if issue is fundamental understanding or data strategy.