---
ver: rpa2
title: 'FOSSIL: Regret-Minimizing Curriculum Learning for Metadata-Free and Low-Data
  Mpox Diagnosis'
arxiv_id: '2510.10041'
source_url: https://arxiv.org/abs/2510.10041
tags:
- curriculum
- learning
- difficulty
- across
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces FOSSIL, a regret-minimizing curriculum learning
  framework that adaptively weights training samples based on their estimated difficulty
  using softmax-based uncertainty. Applied to Mpox skin lesion diagnosis, FOSSIL significantly
  improves classification performance across diverse deep learning architectures without
  metadata, manual curation, or synthetic augmentation.
---

# FOSSIL: Regret-Minimizing Curriculum Learning for Metadata-Free and Low-Data Mpox Diagnosis

## Quick Facts
- arXiv ID: 2510.10041
- Source URL: https://arxiv.org/abs/2510.10041
- Authors: Sahng-Min Han; Minjae Kim; Jinho Cha; Se-woon Choe; Eunchan Daniel Cha; Jungwon Choi; Kyudong Jung
- Reference count: 40
- One-line primary result: FOSSIL achieves state-of-the-art Mpox classification (AUC = 0.9573) without metadata, manual curation, or synthetic augmentation

## Executive Summary
FOSSIL introduces a regret-minimizing curriculum learning framework that adaptively weights training samples based on estimated difficulty using softmax-based uncertainty. Applied to Mpox skin lesion diagnosis, it significantly improves classification performance across diverse deep learning architectures without metadata, manual curation, or synthetic augmentation. The method achieves state-of-the-art discrimination (AUC = 0.9573), calibration (ECE = 0.053), and robustness under real-world perturbations, while maintaining interpretability through localized activation patterns. Statistical validation confirms significant improvements over baseline training across all models tested.

## Method Summary
FOSSIL implements regret-minimizing curriculum learning through difficulty-aware sample weighting. For each training sample, difficulty is estimated as d_i = 1 - max_c p(c)_i using softmax confidence scores. Weights are assigned as w_i = exp(-d_i/T), where T is a temperature parameter controlling exploration-exploitation balance. The framework organizes data into four curriculum stages (Easy→Very Hard) based on 25th/50th/75th quantiles of difficulty scores. Training uses weighted loss L = (1/N) Σ w_i·ℓ_i(θ) with standard optimizers (Adam for CNNs, partial backbone freezing; SAM for transformers). The method was validated on MSLD v2 (804 images, binarized to Mpox vs non-Mpox) and MCSI external validation datasets using 5-fold stratified cross-validation.

## Key Results
- Achieved state-of-the-art Mpox classification performance (AUC = 0.9573) without metadata or synthetic augmentation
- Reduced Expected Calibration Error to 0.053, outperforming conventional baselines
- Demonstrated robustness to real-world perturbations including JPEG compression, Gaussian blur, and brightness/contrast shifts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Exponentially decaying sample weights minimize cumulative regret and stabilize gradient descent in small-data regimes.
- **Mechanism:** The framework assigns weight w_i = exp(-d_i/T) based on difficulty d_i, bounding the influence of high-difficulty samples to prevent optimizer destabilization. This minimizes an upper bound on cumulative regret R_T, prioritizing learning reliable features before adapting to harder ones.
- **Core assumption:** Sample difficulty correlates with gradient noise; the loss landscape allows stable convex approximation during weighted steps.
- **Evidence anchors:**
  - [abstract]: "...regret-minimizing weighting framework... weights that decrease exponentially with estimated difficulty."
  - [section]: "FOSSIL assigns weights to minimize an upper bound on RT... ensuring consistent convergence under imbalance or noise."
  - [corpus]: [arXiv:2509.13218] establishes the theoretical foundation of the regret-minimizing weighting rule.
- **Break condition:** Misspecified temperature T (too low) may ignore hard samples entirely, leading to underfitting.

### Mechanism 2
- **Claim:** Softmax confidence serves as a sufficient proxy for sample difficulty without requiring metadata.
- **Mechanism:** Difficulty defined as d_i = 1 - max_c p(c)_i quantifies model uncertainty. Unlike entropy (which showed fold-dependent skewness), this max-probability metric provided statistically distinct stratification for curriculum stages.
- **Core assumption:** Model's current predictive confidence aligns with the intrinsic "learnability" or ambiguity of the sample.
- **Evidence anchors:**
  - [abstract]: "Using softmax-based uncertainty as a continuous measure of difficulty."
  - [section]: "Mann–Whitney U test... confirmed the absence of class bias... validating the effectiveness of the quantile-based curriculum."
  - [corpus]: [arXiv:2507.14374] supports error-aware strategies, but FOSSIL validates softmax uncertainty as a robust metric.
- **Break condition:** Severely miscalibrated or random model at initialization makes initial difficulty scores meaningless, requiring burn-in period.

### Mechanism 3
- **Claim:** Difficulty-aware weighting acts as a regularizer to improve model calibration (ECE).
- **Mechanism:** Dampening loss contribution of "Very Hard" samples prevents overfitting to noise, resulting in flatter minima and probability outputs that better reflect true likelihoods, lowering Expected Calibration Error.
- **Core assumption:** "Hard" samples in low-data medical contexts are more likely confounding/outliers than high-information samples.
- **Evidence anchors:**
  - [abstract]: "reduced Expected Calibration Error to 0.053... outperforming conventional baselines."
  - [section]: "FOSSIL-trained models consistently outperformed... lower Expected Calibration Error (ECE)... [acting as] an effective regularization mechanism."
  - [corpus]: Weak direct evidence in corpus; this is a specific finding of this paper.
- **Break condition:** "Hard" samples containing critical rare features (e.g., rare Mpox presentation) might reduce sensitivity if down-weighted.

## Foundational Learning

- **Concept:** Regret Minimization (Online Learning)
  - **Why needed here:** Grounds weighting logic in minimizing R_T (difference between cumulative loss and hindsight optimum) rather than heuristics, explaining why weights are exponential.
  - **Quick check question:** Does the weighting scheme guarantee convergence to global minimum, or just a bound on loss relative to best fixed parameter?

- **Concept:** Model Calibration (ECE)
  - **Why needed here:** Clinical AI requires accurate confidence; high AUC with poor calibration is dangerous. FOSSIL explicitly optimizes for this.
  - **Quick check question:** If a model predicts "Positive" with 90% confidence on 100 images, how many should actually be positive for the model to be well-calibrated?

- **Concept:** Curriculum Learning (Self-Paced)
  - **Why needed here:** Method organizes data from Easy to Very Hard. Understanding difference between defined curricula (human-guided) and self-paced (model-guided) is crucial.
  - **Quick check question:** How does temperature parameter T control trade-off between exploring hard samples and exploiting easy ones?

## Architecture Onboarding

- **Component map:** Backbone -> FOSSIL weighting module -> Head
- **Critical path:**
  1. Forward pass → Softmax probabilities
  2. Compute difficulty d_i
  3. Compute weight w_i
  4. Backward pass using weighted loss L_FOSSIL = (1/N) Σ w_i·ℓ_i
- **Design tradeoffs:**
  - Arch Selection: ConvNeXt-T (Hybrid) generalized best; DeiT-S (Transformer) was unstable with SAM optimizer. Prefer ConvNeXt or DenseNet for stability.
  - Metric: Softmax-based difficulty more stable than Entropy, which was fold-dependent.
- **Failure signatures:**
  - Optimizer Conflict: SAM with FOSSIL on Transformers caused "severe degradation" (Table 8). Do not mix SAM with FOSSIL on ViTs without validation.
  - Fold Leakage: Difficulty scores must be computed per fold on training set only to prevent leakage.
- **First 3 experiments:**
  1. Baseline Sanity Check: Train DenseNet121 with standard Cross-Entropy vs. FOSSIL weighting. Confirm reduction in overfitting (train-val gap).
  2. Curriculum Validation: Visualize "Easy" vs. "Very Hard" samples. Verify "Easy" are canonical lesions and "Very Hard" are ambiguous/atypical cases.
  3. Ablation on T: Sweep temperature T (0.5, 1.0, 2.0) to observe shift in convergence speed vs. final AUC.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Would adaptive temperature scheduling strategies (e.g., dynamic decay or learning-rate-coupled adjustment of T) improve FOSSIL's convergence speed and final performance compared to fixed-temperature regimes?
- **Basis in paper:** [explicit] Section 4.3 identifies "adaptive temperature-scheduling strategies" as future work. Temperature parameter T remains fixed during training.
- **Why unresolved:** Paper uses fixed T without exploring how adaptive scheduling might better balance easy/hard sample emphasis throughout training.
- **What evidence would resolve it:** Comparative experiments with learning-rate-coupled T decay, cyclical T scheduling, or performance-based adaptive adjustment on Mpox classification task.

### Open Question 2
- **Question:** How does FOSSIL generalize across multi-institutional clinical settings with diverse imaging protocols, patient demographics, and acquisition equipment?
- **Basis in paper:** [explicit] Section 4.3 states "future work will include multi-institutional external validation." Current validation used MSLD v2 (Bangladesh) and MCSI (Kaggle), representing limited geographic diversity.
- **Why unresolved:** External validation demonstrates robustness on one alternative dataset, but true multi-institutional generalization remains untested.
- **What evidence would resolve it:** Prospective evaluation across 3+ geographically distinct clinical sites with varying skin tone distributions and lesion presentation patterns.

### Open Question 3
- **Question:** What causes transformer-based architectures (e.g., DeiT-S) to exhibit sensitivity to curriculum-optimizer interactions, and how can training strategies be adapted to maintain FOSSIL's benefits?
- **Basis in paper:** [inferred] Table 8 shows DeiT-S experienced "severe degradation under the SAM optimizer, suggesting that transformer-based models may be more sensitive to curriculum–optimizer interactions." No mitigation strategy proposed.
- **Why unresolved:** FOSSIL's benefits appear architecture-dependent with different optimizers, but optimal transformer configuration remains unclear.
- **What evidence would resolve it:** Systematic ablation study of FOSSIL across transformer architectures with and without SAM, analyzing gradient dynamics and weight distribution patterns.

### Open Question 4
- **Question:** What is the safety profile, clinical interpretability, and decision impact of FOSSIL-trained models when deployed in prospective clinical workflows?
- **Basis in paper:** [explicit] Section 4.3 states "prospective clinical trials should assess the safety, interpretability, and decision impact of models trained under the proposed weighting framework in real-world workflows."
- **Why unresolved:** All evaluation is retrospective; no prospective validation demonstrates real-world clinical utility or patient outcome improvements.
- **What evidence would resolve it:** Prospective clinical trial comparing FOSSIL-assisted vs. standard teledermatology screening, measuring diagnostic accuracy, time-to-diagnosis, clinician confidence, and patient outcomes.

## Limitations
- Temperature parameter T is not fully explored or specified, leaving a critical hyperparameter undefined for reproduction
- SAM optimizer instability with transformer architectures suggests architecture-specific constraints limiting generalizability
- Framework's performance on extremely small datasets (<100 samples) remains unverified despite claims of efficacy in "low-data regimes"

## Confidence

- **High Confidence:** The core regret-minimizing weighting mechanism and its empirical benefits on AUC, ECE, and robustness metrics are well-supported by statistical testing and ablation studies.
- **Medium Confidence:** The claim that FOSSIL generalizes beyond Mpox diagnosis to other biomedical domains is plausible but not directly validated beyond dermatology-adjacent tasks.
- **Low Confidence:** The assertion that FOSSIL is "metadata-free" is technically accurate but potentially misleading—it requires model-generated uncertainty scores, which constitute implicit metadata.

## Next Checks

1. **Temperature Sensitivity Analysis:** Systematically sweep T values (0.1 to 10.0) on a held-out dataset to identify optimal settings and characterize sensitivity.
2. **Cross-Domain Transfer:** Apply FOSSIL to a non-dermatological, low-data biomedical task (e.g., histopathology or genomics) to verify generalizability claims.
3. **Extreme Data Scarcity Test:** Evaluate FOSSIL on datasets with <100 samples to determine the lower bound of its effectiveness.