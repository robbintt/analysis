---
ver: rpa2
title: Discovering Process-Outcome Credit in Multi-Step LLM Reasoning
arxiv_id: '2602.01034'
source_url: https://arxiv.org/abs/2602.01034
tags:
- reasoning
- wang
- zhang
- step
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses reward sparsity in LLM reasoning by proposing
  a framework that provides dense step-wise supervision. It introduces Step-wise Marginal
  Information Gain (MIG) to quantify intrinsic value of each reasoning step via semantic
  likelihood increments, tracked against a monotonic historical watermark.
---

# Discovering Process-Outcome Credit in Multi-Step LLM Reasoning

## Quick Facts
- arXiv ID: 2602.01034
- Source URL: https://arxiv.org/abs/2602.01034
- Reference count: 40
- Primary result: Dense step-wise rewards via MIG improve LLM reasoning accuracy over GRPO while reducing reward sparsity issues

## Executive Summary
This paper addresses reward sparsity in multi-step LLM reasoning by introducing Step-wise Marginal Information Gain (MIG), a mechanism that provides dense supervision by quantifying the intrinsic value of reasoning steps through semantic likelihood increments tracked against a monotonic historical watermark. The framework combines this dense reward with outcome-oriented binary feedback through a decoupled masking strategy, while incorporating dual-gated self-correction to stabilize training on high-quality samples. Extensive experiments across textual and multimodal benchmarks show consistent improvements over GRPO in both sample efficiency and final accuracy, with superior out-of-distribution robustness and promising zero-shot transfer capabilities.

## Method Summary
The method computes dense rewards for reasoning steps by tracking likelihood increments of correct answers conditioned on reasoning prefixes, filtered through a Monotonic Historical Watermark that rewards only genuine breakthroughs. A decoupled optimization strategy applies separate loss terms to reasoning tokens (encouraging exploration) and full completions (enforcing correctness), while dual-gated self-correction distills successful trajectories only when both structural validity and factual correctness are verified. The approach is implemented with structured rollout parsing, step-aware likelihood probing, and hybrid optimization combining MIG, outcome, and gated SFT losses.

## Key Results
- MIG improves pass@1 accuracy on MATH benchmark by 6.1% over GRPO baseline
- Out-of-distribution robustness shows 2.4% improvement on CommonsenseQA and 4.7% on SV AMP
- Sample efficiency gains demonstrated through faster convergence across 8 benchmarks
- Superior performance on complex reasoning tasks (AIME 2025) while maintaining baseline performance on simpler tasks

## Why This Works (Mechanism)

### Mechanism 1: Step-wise Marginal Information Gain (MIG)
The MIG mechanism derives dense step-level rewards from the model's own probability landscape by computing length-normalized log-likelihood of ground truth answers conditioned on reasoning prefixes. A Monotonic Historical Watermark tracks maximum likelihood seen so far, rewarding only rectified increments that represent genuine semantic breakthroughs rather than position-based progress. This filters out redundant oscillations while crediting logical progress regardless of step position, solving credit assignment by rewarding semantic progress rather than token position.

### Mechanism 2: Decoupled Masking for Hybrid Optimization
The framework applies separate loss terms with distinct token-level masking to prevent interference between process exploration and outcome grounding. Dense MIG rewards guide reasoning expansion on CoT tokens while sparse binary outcome rewards enforce structural validity and correctness on full completions. This ensures the dense reward guides reasoning search space expansion while the sparse reward strictly bounds optimization, creating stable training dynamics rather than conflicting gradients.

### Mechanism 3: Dual-Gated SFT for Training Stabilization
Self-generated trajectories are distilled via SFT only when both structural validity and factual correctness are verified, preventing accumulation of logical hallucinations. The SFT loss activates only when format compliance and answer correctness gates both pass, ensuring high-fidelity data distillation from successful exploration paths only. This treats only successful self-generated paths as positive training samples, enabling quality-controlled knowledge transfer.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: The paper builds directly on GRPO as the baseline RL paradigm, using group-relative advantages to eliminate value networks. Understanding how GRPO computes advantages from grouped samples is essential to see how MIG modifies this.
  - Quick check question: Can you explain how GRPO estimates policy gradients without a learned value function, and why this creates reward sparsity for long reasoning chains?

- **Concept: Credit Assignment in Reinforcement Learning**
  - Why needed here: The core problem this paper addresses is assigning credit to specific reasoning steps when only the final outcome receives feedback. The MIG mechanism is a proposed solution to this fundamental RL challenge.
  - Quick check question: In a multi-step reasoning task where only the final answer receives a binary reward, why is it difficult for standard policy gradients to identify which intermediate steps contributed to success?

- **Concept: Likelihood as a Value Proxy**
  - Why needed here: The MIG mechanism assumes that P(y* | x, s1:k)—the probability of the correct answer given the reasoning prefix—correlates with reasoning quality. This is not trivially true and requires both theoretical justification and empirical validation.
  - Quick check question: Why might a reasoning step that increases the likelihood of a correct answer still be logically invalid, and how does the Monotonic Historical Watermark mitigate this risk?

## Architecture Onboarding

- **Component map:** Structured Rollout -> Step-Aware Probe -> Hybrid Optimizer
- **Critical path:** The HWM update loop in Algorithm 1 (lines 12-18) is the core logic—each step's likelihood is computed, compared against the running maximum, and only positive increments contribute to R(i)MIG. Errors here propagate to all downstream losses.
- **Design tradeoffs:**
  - Granularity vs. error accumulation: Case Study 2 shows MIG generates longer chains (19 steps vs. 5), which helps complex reasoning but increases arithmetic error surface area
  - Exploration vs. grounding: Pure MIG (without SFT) excels at Level 5 math (35.1%) but degrades foundational tasks; adding SFT recovers Level 1 (86.0%) at the cost of some peak performance
  - Answer equivalence handling: Equation 9's max-over-variants prevents false negatives but requires offline augmentation
- **Failure signatures:**
  - Reward hacking via format gaming: GRPO baseline degrades on SCQ5K-EN (62.0% vs. 64.0% base) by overfitting to answer formatting—MIG should prevent this but monitor for it
  - Cold start on hard tasks: If initial policy rarely produces correct outputs, SFT gate rarely activates; expect slow early progress on AIME-level problems
  - Negative transfer: GRPO shows negative transfer on MMStar (50.0% → 47.0%); verify MIG maintains positive transfer
- **First 3 experiments:**
  1. Reproduce ablation on MATH difficulty levels (Table 5): Train with and without SFT, verify the Level 1 vs. Level 5 tradeoff. This validates the hybrid optimization hypothesis.
  2. Validate ℓk as a value proxy (Appendix B protocol): Sample 100 GSM8K problems, compute MCTS values for intermediate states, measure correlation with ℓk. Target ρ > 0.6.
  3. Compare HWM vs. alternative aggregation (Table 4): Test Final-ℓT, Delta Sum, Position-Weighted Mean alongside HWM on a held-out set. Verify HWM achieves the best balance of correlation and granularity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Step-wise MIG framework effectively scale to larger base models (e.g., 70B+) without instability?
- **Basis in paper:** [explicit] Conclusion ("Future work will explore scaling our framework to larger base models.")
- **Why unresolved:** The study relies exclusively on Qwen2.5-Instruct-3B and Qwen2.5-VL-Instruct-3B architectures.
- **What evidence would resolve it:** Replicating the MIG training protocol on 70B or frontier-scale models, demonstrating maintained sample efficiency.

### Open Question 2
- **Question:** Does dynamic step-reward scaling mitigate the error accumulation caused by over-decomposition in simpler tasks?
- **Basis in paper:** [explicit] Appendix D.4.2 ("This suggests that future work could explore dynamic step-reward scaling, encouraging conciseness...")
- **Why unresolved:** The fixed MIG mechanism encourages high granularity (19 steps) in simple algebra, increasing calculation hallucinations compared to baselines (5 steps).
- **What evidence would resolve it:** An adaptive reward mechanism that reduces step density for procedural tasks, reducing error rates on benchmarks like SV AMP.

### Open Question 3
- **Question:** To what extent does Outcome-Gated SFT constrain the discovery of superior, non-human reasoning paths in abstract domains?
- **Basis in paper:** [inferred] Appendix D.1 and D.2 ("On the hardest problems (Level 5)... MIG w/o SFT achieves the highest performance... SFT data might introduce human biases.")
- **Why unresolved:** The ablation study reveals a trade-off where pure RL outperforms the full framework on the most difficult and abstract tasks, suggesting the SFT component acts as a "ceiling."
- **What evidence would resolve it:** A comparative analysis identifying specific reasoning strategies discovered by the pure RL model that are absent in SFT-augmented models.

## Limitations
- Core assumption that likelihood increments correlate with reasoning quality requires more extensive validation across diverse domains and model scales
- Decoupled masking strategy introduces three independent loss terms with unspecified hyperparameters (α, β, γ), creating potential sensitivity to tuning
- Cold start problem acknowledged for very hard problems where SFT gates rarely activate, potentially limiting learning on extreme out-of-distribution challenges

## Confidence
- **High Confidence:** The decoupled masking framework (Section 3.3) and its implementation details are well-specified. The core algorithm for computing MIG with the Monotonic Historical Watermark (Algorithm 1) is clearly described and reproducible.
- **Medium Confidence:** The empirical results showing consistent improvements over GRPO across multiple benchmarks (MATH, GSM8K, multimodal tasks). While results are compelling, the lack of hyperparameter sensitivity analysis and the limited validation of ℓk as a value proxy prevent high confidence.
- **Low Confidence:** The zero-shot transfer claims to MMStar and MathVista benchmarks, as these results are presented without detailed ablations or comparison to alternative transfer approaches.

## Next Checks
1. **Systematic Hyperparameter Sensitivity Analysis**: Conduct a grid search over α, β, γ values on a held-out validation set to determine optimal configurations and assess stability. Report variance in performance across different weight combinations.

2. **Extended Value Proxy Validation**: Replicate Appendix B's correlation study across at least 500 intermediate states spanning diverse reasoning domains (math, commonsense, multimodal). Include additional correlation metrics (Pearson, Kendall) and test whether the correlation holds for different model sizes.

3. **Cold Start Robustness Test**: Design an experiment where the initial policy is deliberately weakened (e.g., via weight perturbation) to simulate poor initialization. Measure whether the MIG signal alone can bootstrap learning without SFT contribution, and quantify the minimum accuracy threshold needed for SFT gates to activate.