---
ver: rpa2
title: Measuring Sparse Autoencoder Feature Sensitivity
arxiv_id: '2509.23717'
source_url: https://arxiv.org/abs/2509.23717
tags:
- feature
- sensitivity
- text
- activating
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a scalable method to measure feature sensitivity
  in Sparse Autoencoders (SAEs). Instead of relying on natural language explanations,
  the approach uses language models to generate text similar to a feature's activating
  examples and then measures how often the feature activates on this generated text.
---

# Measuring Sparse Autoencoder Feature Sensitivity

## Quick Facts
- **arXiv ID:** 2509.23717
- **Source URL:** https://arxiv.org/abs/2509.23717
- **Reference count:** 40
- **Primary result:** Many interpretable SAE features show poor sensitivity to semantically similar inputs, declining with SAE width

## Executive Summary
This paper introduces a scalable method to measure feature sensitivity in Sparse Autoencoders (SAEs) without relying on natural language explanations. The approach uses language models to generate text similar to a feature's activating examples and measures how often the feature activates on this generated text. This explanation-free method avoids potential errors from intermediate descriptions and directly evaluates whether features reliably activate on semantically similar inputs. The study finds that many interpretable SAE features have poor sensitivity—they fail to activate on generated text that humans judge as similar to original activating examples. Additionally, the research reveals that average feature sensitivity declines with increasing SAE width across 7 different SAE variants, identifying a new challenge for SAE scaling.

## Method Summary
The paper proposes a novel method for measuring feature sensitivity that bypasses natural language explanations entirely. Instead of using intermediate descriptions, the method generates text similar to a feature's activating examples using language models, then measures how often the feature activates on this generated text. This approach directly evaluates whether features reliably activate on semantically similar inputs without the potential errors introduced by human-written explanations. The method also incorporates human evaluation to validate that when features fail to activate on generated text, that text genuinely resembles the original activating examples, confirming the reliability of the measurement approach.

## Key Results
- Many interpretable SAE features fail to activate on generated text that humans judge as similar to original activating examples
- Average feature sensitivity declines with increasing SAE width across 7 different SAE variants
- Human evaluation validates that generated text genuinely resembles original activating examples when features fail to activate

## Why This Works (Mechanism)
The method works by creating a direct pipeline from feature activations to generated text similarity, eliminating the potential for errors that can occur when using intermediate natural language explanations. By using language models to generate text similar to a feature's activating examples and then measuring activation rates, the approach provides a more objective and scalable way to evaluate whether features capture consistent semantic patterns. The human validation component ensures that the generated text truly represents semantically similar inputs, confirming that low activation rates reflect genuine sensitivity issues rather than artifacts of the generation process.

## Foundational Learning

**Sparse Autoencoders (SAEs):** Neural network components that learn sparse representations by encouraging only a subset of neurons to activate for any given input. *Why needed:* SAEs are the primary model being evaluated for feature sensitivity. *Quick check:* Can identify which neurons activate for specific inputs and understand sparsity constraints.

**Feature Sensitivity:** The degree to which a learned feature reliably activates on semantically similar inputs. *Why needed:* Core concept being measured and the paper's main contribution. *Quick check:* Ability to explain why a feature should activate consistently across similar examples.

**Language Model Text Generation:** Using models like GPT to create synthetic text similar to given examples. *Why needed:* Key technique for creating test inputs without manual annotation. *Quick check:* Understanding how temperature, prompting, and model choice affect generated text quality.

## Architecture Onboarding

**Component Map:** SAE Layer -> Feature Activations -> Text Generation Model -> Generated Text -> Feature Sensitivity Measurement

**Critical Path:** Input text → SAE encoding → Feature activation → Generate similar text → Measure activation on generated text → Sensitivity score

**Design Tradeoffs:** The paper trades off the potential richness of human-written explanations against the scalability and objectivity of machine-generated text. This eliminates potential errors from intermediate descriptions but may miss nuanced semantic relationships that humans would capture.

**Failure Signatures:** Features that show high activation on original examples but low activation on generated similar text indicate poor sensitivity. This pattern suggests the feature is not capturing robust semantic patterns but rather specific surface-level characteristics.

**First Experiments:**
1. Measure baseline sensitivity of features on their original activating examples
2. Generate text using different temperature settings to test robustness to generation artifacts
3. Compare sensitivity scores across SAEs with varying width while holding other hyperparameters constant

## Open Questions the Paper Calls Out
None

## Limitations
- The study covers 7 SAE variants but does not systematically explore the full parameter space or control for confounding factors like training data distribution
- Human evaluation is limited to a small sample (300 features) and relies on pairwise comparisons that may not capture nuanced semantic relationships
- The assumption that failure to activate on generated text indicates poor sensitivity could reflect limitations in the text generation process or SAE training objective rather than inherent feature inadequacy

## Confidence
- **High:** Explanation-free methods avoid errors from intermediate descriptions
- **Medium:** Findings about sensitivity decline with SAE width (analysis covers 7 variants but lacks systematic exploration)
- **Low:** Generalizability across different SAE architectures, model sizes, and domains (study focuses on specific implementations without extensive ablation studies)

## Next Checks
1. Test sensitivity measurements across SAEs with controlled width variations while holding other hyperparameters constant to isolate the width effect
2. Evaluate feature sensitivity using alternative generation methods (different prompting strategies, temperature settings, or base models) to assess robustness to generation artifacts
3. Conduct systematic ablation studies comparing sensitivity scores when features are evaluated on human-written semantically similar examples versus machine-generated text to quantify potential generation bias