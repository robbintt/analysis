---
ver: rpa2
title: XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders
arxiv_id: '2512.13442'
source_url: https://arxiv.org/abs/2512.13442
tags:
- features
- data
- tabular
- learning
- interpretable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'XNNTab addresses the challenge of combining high predictive performance
  with interpretability in neural networks for tabular data. The method uses a two-stage
  approach: first, a standard neural network learns non-linear feature representations,
  then a sparse autoencoder (SAE) decomposes these into interpretable, monosemantic
  features.'
---

# XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders

## Quick Facts
- arXiv ID: 2512.13442
- Source URL: https://arxiv.org/abs/2512.13442
- Reference count: 0
- Primary result: XNNTab achieves comparable performance to black-box models (XGBoost, Random Forests) and state-of-the-art neural architectures (TabNet, NODE, T2G-Former) while providing interpretable feature decompositions

## Executive Summary
XNNTab addresses the challenge of combining high predictive performance with interpretability in neural networks for tabular data. The method uses a two-stage approach: first, a standard neural network learns non-linear feature representations, then a sparse autoencoder (SAE) decomposes these into interpretable, monosemantic features. Human-understandable semantics are assigned to each feature through rule-based classifiers that identify subsets of training instances that activate each feature.

Experiments on seven datasets show XNNTab outperforms interpretable models (Logistic Regression, Decision Trees) and achieves comparable performance to black-box models (XGBoost, Random Forests) and state-of-the-art neural architectures (TabNet, NODE, T2G-Former). On Adult dataset, XNNTab achieves macro F1 of 0.795, matching MLP and outperforming interpretable baselines. On Spambase, it reaches 0.948 F1. The learned features are interpretable through simple decision rules with average complexity of 2.4 terms per rule, making the model's predictions transparent through linear combinations of these features.

## Method Summary
XNNTab employs a four-step training procedure: (1) Train a multilayer perceptron (MLP) with cross-entropy loss plus L1 regularization on the penultimate layer to encourage sparsity; (2) Train a sparse autoencoder (SAE) to reconstruct the MLP's penultimate layer activations, enforcing sparsity on the latent representation; (3) Freeze the MLP and SAE weights, then fine-tune only the final classification layer using the reconstructed features; (4) Combine the SAE decoder and final classification layer into a single interpretable linear layer (W' = W·M^T). Interpretability is achieved by extracting Skope-rules from training instances that highly activate each SAE latent dimension, creating human-readable descriptions of feature semantics.

## Key Results
- XNNTab achieves macro F1 of 0.795 on Adult dataset, matching MLP performance while providing interpretable features
- On Spambase dataset, XNNTab reaches 0.948 F1, outperforming interpretable baselines (LR: 0.906, DT: 0.894)
- Average rule complexity is 2.4 terms per rule across datasets, with 2-4 features typically active per prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse autoencoders decompose polysemantic neural representations into interpretable monosemantic features.
- Mechanism: The SAE encoder maps MLP hidden activations to a higher-dimensional sparse latent space via `h_SAE = ReLU(M·h_l + b)`. Sparsity constraints force each latent dimension to activate only for specific input patterns, encouraging feature specialization rather than distributed representation.
- Core assumption: Polysemantic neurons can be linearly decomposed into monosemantic components without losing predictive information.
- Evidence anchors: [abstract] "XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE)" [Section 3.1] "By design, ĥ_l is a sparse linear combination of (latent) monosemantic dictionary features" [corpus] Neighbor paper "Transcoders Beat Sparse Autoencoders" suggests SAEs are actively debated for interpretability—mechanism not settled science
- Break condition: If MLP hidden representations are already near-linear or highly sparse, SAE provides marginal benefit. If reconstruction loss is high, monosemantic decomposition fails.

### Mechanism 2
- Claim: Linear combination of sparse features preserves interpretability while maintaining predictive performance.
- Mechanism: After training, the decoder weights (M^T) and final classification layer (W) are multiplied: `W' = W·M^T`. This creates a single linear layer from sparse features directly to predictions, where each weight indicates feature importance and direction (positive/negative class contribution).
- Core assumption: The composition of two linear transformations (decoder + classifier) can be collapsed without approximation error.
- Evidence anchors: [abstract] "making the overall model prediction intrinsically interpretable" [Section 3.2] "Therefore, we directly connect the hidden layer of the SAE to the output and set the weights of this layer to W' = W·M^T" [corpus] Weak direct evidence; neighbor papers don't discuss this specific linear collapse technique
- Break condition: If fine-tuning significantly changes feature semantics, pre-collapse weights no longer reflect learned representations. If SAE reconstruction is poor, linear approximation introduces error.

### Mechanism 3
- Claim: Rule-based classifiers on highly-activating instances assign human-readable semantics to latent features.
- Mechanism: For each SAE latent dimension j, identify instances with activations above threshold p (forming subset T_j). Train Skope-rules to describe T_j with conjunctive rules (e.g., "age > 44.5 AND IsActiveMember ≤ 0.5"). Rules capture the input conditions that trigger each monosemantic feature.
- Core assumption: High activations correspond to coherent, rule-describable input patterns rather than noisy or idiosyncratic combinations.
- Evidence anchors: [Section 3.3] "For each dimension in the latent representation of the SAE j ∈ {1, ...d_hid}, we identify the subset of training samples T_j which highly activate this feature" [Table 4] Shows extracted rules like "marital_status_Married is False and age ≤ 37.5 and educational_num < 12" [corpus] Neighbor paper "Binary Sparse Coding for Interpretability" notes SAE features may only be interpretable at high activation—supports thresholding approach
- Break condition: If no rules achieve high precision/recall for a feature, that feature remains uninterpretable. If rules are overly complex (>5 terms), interpretability degrades.

## Foundational Learning

- Concept: **Polysemantic vs. Monosemantic Representations**
  - Why needed here: The core premise is that standard neural neurons respond to multiple unrelated concepts (polysemantic), while SAE enforces single-concept neurons (monosemantic).
  - Quick check question: Can you explain why a single neuron activating for both "capital gains" and "education level" would hurt interpretability?

- Concept: **Sparsity-Interpretability Tradeoff**
  - Why needed here: L1 regularization forces few features to activate per sample, enabling simpler rule extraction but potentially losing information.
  - Quick check question: If 50 features activate for every input, can you write simple rules describing each feature?

- Concept: **Linear Layer Composition**
  - Why needed here: Understanding that `y = W·(M^T·h_SAE)` equals `y = (W·M^T)·h_SAE` is essential for grasping the final interpretable model.
  - Quick check question: Given W is [3×10] and M^T is [10×50], what is the shape of the combined weight matrix?

## Architecture Onboarding

- Component map:
  Input → MLP (θ_g) → h_l [penultimate layer]
                      ↓
         SAE Encoder (M, b) → h_SAE [sparse monosemantic features]
                      ↓
         SAE Decoder (M^T) → ĥ_l [reconstruction]
                      ↓
         Combined Linear (W' = W·M^T) → ŷ [prediction]
                      ↓
         Rule Extraction (Skope-rules on T_j) → Feature semantics

- Critical path:
  1. Train MLP to convergence (cross-entropy + L1)
  2. Extract h_l activations for all training samples
  3. Train SAE on h_l (reconstruction + sparsity loss)
  4. Freeze MLP and SAE; fine-tune W using ĥ_l
  5. Compute W' = W·M^T
  6. For each SAE neuron, find high-activation instances, extract rules

- Design tradeoffs:
  - **Expansion factor R** (SAE hidden size / input size): Higher R = more features, finer granularity, but more rules to interpret. Paper uses R∈{1,2,3}.
  - **Activation threshold p**: Higher p = stricter rule precision but fewer features get rules. Paper tests p∈{50%,60%,70%,80%,90%}.
  - **Rule complexity vs. coverage**: Precision=1.0 constraint ensures reliable rules but may leave features unexplained.

- Failure signatures:
  - SAE reconstruction loss > 0.5: Features don't capture MLP representations
  - <50% of features obtain rules: Model remains partially black-box
  - Rule recall < 0.3: Rules don't reliably describe activating instances
  - Performance drops >5% from MLP baseline: SAE bottleneck too severe

- First 3 experiments:
  1. **Baseline validation**: Train MLP alone on Adult dataset; confirm macro F1 ≈ 0.796. This isolates representation quality before SAE.
  2. **SAE reconstruction sanity check**: Train SAE on MLP activations; verify reconstruction loss < 0.1 and sparsity (>90% zeros in h_SAE). If fails, increase d_hid or adjust L1 coefficient.
  3. **Rule extraction coverage test**: For Spambase (smallest dataset), run full pipeline and measure: (a) fraction of features with rules at p=80%, (b) average rule length, (c) average recall. Target: >70% coverage, rule length ≤3, recall ≥0.4.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an automated method be developed to optimally select activation thresholds that balance rule recall with the fraction of extracted rules?
- Basis in paper: [explicit] The authors state in future work: "exploring automated approaches for selecting activation thresholds that balance rule recall with the fraction of extracted rules."
- Why unresolved: Current approach manually tests p ∈ {50%, 60%, 70%, 80%, 90%} and selects based on maximizing rule count, but optimal p varies by dataset (e.g., GE benefits from higher p, SB fluctuates unpredictably).
- What evidence would resolve it: A learned or adaptive threshold selection method that consistently maximizes both coverage and recall across diverse datasets without manual tuning.

### Open Question 2
- Question: What strategies can effectively reduce redundant rules through pruning or combining neurons?
- Basis in paper: [explicit] Listed as future work: "investigating strategies to reduce redundant rules such as pruning or combining neurons."
- Why unresolved: The paper shows some learned rules may capture similar patterns (e.g., multiple features activating for similar marital_status/age combinations in Adult dataset), increasing cognitive load for interpretation.
- What evidence would resolve it: A pruning or clustering method that reduces rule redundancy while maintaining predictive performance and coverage.

### Open Question 3
- Question: Can the SAE-based decomposition approach be extended to other specialized tabular neural architectures (e.g., TabR, FT-Transformer) to improve performance while maintaining interpretability?
- Basis in paper: [explicit] "The SAE approach could also be extended to other blackbox neural models for tabular data to further enhance performance."
- Why unresolved: XNNTab currently uses a standard MLP as the base model, which underperforms compared to state-of-the-art architectures like TabR on some datasets (e.g., Adult, Covertype).
- What evidence would resolve it: Demonstrating that SAE decomposition applied to attention-based or retrieval-augmented tabular models achieves comparable or better performance than the MLP baseline while preserving interpretability.

### Open Question 4
- Question: How does XNNTab's interpretability compare to other interpretable methods when evaluated with human users on real-world decision tasks?
- Basis in paper: [inferred] The paper evaluates interpretability through rule complexity metrics (average 2.4 terms per rule, number of active features) but does not include human evaluation of whether users can effectively understand and trust the explanations.
- Why unresolved: Quantitative metrics like rule length don't capture whether domain experts find the explanations actionable or trustworthy in practice.
- What evidence would resolve it: A user study comparing XNNTab's explanations to those from Logistic Regression, Decision Trees, and InterpreTabNet on task-specific criteria such as explanation usefulness, trust, and decision-making accuracy.

## Limitations
- The mechanism by which SAEs decompose polysemantic into monosemantic features remains theoretically contested in the literature
- Training procedure details are underspecified, particularly SAE loss formulation and fine-tuning hyperparameters
- The linear collapse assumption (W' = W·M^T) is not empirically validated with ablation studies

## Confidence
- **High confidence**: Predictive performance comparisons (F1 scores, rank preservation vs. baselines)
- **Medium confidence**: Interpretability mechanism (feature decomposition via SAE)
- **Low confidence**: Generalizability to non-tabular domains and robustness to dataset characteristics

## Next Checks
1. **Ablation on SAE necessity**: Train MLP directly to final layer (no SAE bottleneck) and compare performance to confirm SAE doesn't degrade predictive capability beyond acceptable limits
2. **Rule coverage validation**: Systematically vary p threshold and report fraction of features obtaining rules, average rule complexity, and coverage statistics to establish robustness of interpretability claims
3. **Cross-dataset generalization test**: Apply XNNTab to a held-out tabular dataset (not in original 7) to verify performance and interpretability claims generalize beyond the paper's experimental scope