---
ver: rpa2
title: 'CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable
  Elo Ratings'
arxiv_id: '2501.01257'
source_url: https://arxiv.org/abs/2501.01257
tags:
- problems
- code
- rating
- problem
- ratings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CodeElo, a competition-level code generation
  benchmark designed to evaluate the reasoning capabilities of large language models
  (LLMs). Existing benchmarks fall short due to the unavailability of private test
  cases, lack of support for special judges, and misaligned execution environments.
---

# CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings

## Quick Facts
- arXiv ID: 2501.01257
- Source URL: https://arxiv.org/abs/2501.01257
- Reference count: 35
- Key result: Introduces platform-integrated Elo rating system for LLMs on CodeForces problems; o1-mini achieves 1578, QwQ-32B-Preview achieves 1261

## Executive Summary
CodeElo addresses fundamental limitations in existing LLM code generation benchmarks by implementing direct submissions to the CodeForces platform, eliminating false positives through ground-truth evaluation. The benchmark evaluates 33 models (30 open-source, 3 proprietary) on 387 problems across 54 contests, producing human-comparable Elo ratings. Results show stark performance gaps: while o1-mini achieves near-expert human level (1578), most models struggle with even basic competitive programming problems, particularly those requiring dynamic programming or tree algorithms.

## Method Summary
CodeElo compiles competition-level problems from CodeForces, using an automated bot to submit model-generated solutions directly to the platform for official judgment. Models receive HTML-formatted problem descriptions and generate C++ solutions (max 4096 tokens) with chain-of-thought prompting. Each problem allows up to 8 submissions with penalties for failures. Elo ratings are calculated by determining each model's rank among human participants in each contest, then using binary search to find the expected rating that would produce that rank distribution. Final ratings are averaged across contests to reduce variance.

## Key Results
- Elo ratings: o1-mini (1578), QwQ-32B-Preview (1261), while most models fall below 1000 (bottom 25% of human participants)
- Language impact: C++ consistently outperforms Python for all models due to execution time constraints
- Algorithm weakness: Models show significant difficulty with dynamic programming and tree problems
- Performance ceiling: Even top models struggle with problems rated above 1700 difficulty

## Why This Works (Mechanism)

### Mechanism 1: Platform-Integrated Evaluation Eliminates False Positives
- Submitting model-generated code directly to CodeForces provides ground-truth evaluation that offline benchmarks cannot replicate
- An automated bot submits solutions to the official platform, receiving pass/fail status from hidden test cases and special judges
- Core assumption: CodeForces platform's test suite is comprehensive and representative of true problem difficulty

### Mechanism 2: Elo Rating Translation from Relative Ranking
- A model's Elo rating can be computed by positioning its contest performance within the distribution of human participant ratings
- For each contest, binary search solves for the expected rating r satisfying: m = Σ(1 / (1 + 10^(r-r_i)/400))
- Core assumption: Model performance within a single contest is independent and identically distributed across contests

### Mechanism 3: Language Selection Affects Performance on Time-Constrained Problems
- C++ produces higher Elo ratings than Python for most LLMs on competitive programming tasks
- Competitive programming problems enforce strict time limits; C++'s faster execution allows solutions with higher time complexity to pass
- Core assumption: Models can generate syntactically and semantically correct code in both languages

## Foundational Learning

- **Special Judges in Competitive Programming**: Why needed - ~30% of CodeForces problems have multiple valid outputs requiring custom validation logic rather than simple string comparison. Quick check - Given a problem asking to output "any valid graph with N nodes and M edges," would comparing against a single reference output work?

- **Elo Rating System Properties**: Why needed - Paper claims human-comparable ratings with "lower variance." Understanding that Elo ratings are zero-sum, depend on opponent strength, and require sufficient samples to stabilize is essential. Quick check - If a model beats a 1500-rated player and loses to a 1000-rated player in separate contests, what happens to its rating?

- **Time Complexity Constraints in Algorithmic Problems**: Why needed - Paper emphasizes that execution time is a "significantly critical factor." Solutions that are logically correct but algorithmically inefficient will fail regardless of code correctness. Quick check - A solution passes all sample test cases but fails on hidden tests. What are two likely causes beyond logical errors?

## Architecture Onboarding

- **Component map**: Problem scraper -> HTML parser -> Structured problem dataset -> Model inference -> Code extraction -> Submission bot -> CodeForces platform -> Platform response parser -> Per-contest rank calculation -> Elo calculator -> Aggregated ratings

- **Critical path**: 1) Contest selection (recent contests only) 2) Prompt construction with HTML-formatted problems 3) Model generates C++ solution 4) Bot submits within 1 minute 5) Parse "Accept" status 6) Calculate per-contest rank -> Elo via binary search 7) Aggregate across 54 contests for final rating (std dev ~50)

- **Design tradeoffs**: 8 submissions per problem cap reduces platform load but may underestimate capability; platform dependency provides ground truth but creates external dependency; C++ default elicits better performance but doesn't reflect typical LLM usage patterns

- **Failure signatures**: "Time Limit Exceeded" on logically correct solutions → algorithm too slow; high variance across contests (std dev 300-500 for weak models) → insufficient problem-solving consistency; zero solves on specific algorithm tags → fundamental capability gap

- **First 3 experiments**: 1) Replicate Elo calculation on single contest to verify rating matches expected percentile 2) Language ablation on 10 problems to quantify execution efficiency gap 3) Submission limit sensitivity test to estimate rating ceiling if unlimited submissions were allowed

## Open Questions the Paper Calls Out
None

## Limitations

- **Platform Dependency**: Core evaluation relies on automated submissions to CodeForces, creating ethical concerns and external dependency that may prevent independent validation
- **Sample Size Uncertainty**: 54 contests and 8-submission limits may systematically underestimate model capabilities and produce higher variance than desired
- **Language Representation Bias**: Default use of C++ creates disconnect from typical LLM usage patterns where Python dominates 95%+ of applications

## Confidence

- **High Confidence**: Platform-integrated evaluation effectively eliminates false positives; Elo rating calculation methodology is mathematically sound
- **Medium Confidence**: Specific Elo ratings for individual models may have higher uncertainty due to contest-level variance
- **Low Confidence**: Generalizability of C++ performance advantages to future model generations

## Next Checks

1. **Variance Sensitivity Analysis**: Replicate Elo calculation for top 5 models across different subsets of contests (25, 34, 44) to quantify how rating stability improves with sample size

2. **Unlimited Submissions Benchmark**: Remove 8-submission cap for subset of 10 problems to measure maximum achievable pass rate and compare resulting Elo ratings

3. **Cross-Platform Validation**: Implement same evaluation methodology on alternative platform (e.g., AtCoder) using identical problems and models to assess platform dependency