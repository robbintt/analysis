---
ver: rpa2
title: 'LLM4GNAS: A Large Language Model Based Toolkit for Graph Neural Architecture
  Search'
arxiv_id: '2502.10459'
source_url: https://arxiv.org/abs/2502.10459
tags:
- search
- graph
- llm4gnas
- neural
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM4GNAS is a toolkit for Graph Neural Architecture Search (GNAS)
  that leverages Large Language Models (LLMs) to automate the design of Graph Neural
  Networks (GNNs). It addresses the challenge of manual adaptation to new graph search
  spaces by using LLM prompts to guide the search process, reducing the need for manual
  intervention.
---

# LLM4GNAS: A Large Language Model Based Toolkit for Graph Neural Architecture Search

## Quick Facts
- **arXiv ID:** 2502.10459
- **Source URL:** https://arxiv.org/abs/2502.10459
- **Reference count:** 40
- **Primary result:** LLM4GNAS achieves 99.88% accuracy on NS dataset and 99.33% on Router dataset for link prediction, and 92.97% accuracy on ACM dataset for node classification.

## Executive Summary
LLM4GNAS is a toolkit that automates Graph Neural Network (GNN) architecture design using Large Language Models (LLMs) as controllers. The framework addresses the challenge of manual adaptation to new graph search spaces by using LLM prompts to guide the search process, significantly reducing the need for manual intervention. It supports both homogeneous and heterogeneous graphs and includes LLM-enhanced node augmentation and hyperparameter optimization, achieving state-of-the-art performance across various graph tasks.

## Method Summary
The LLM4GNAS framework uses LLMs to iteratively design GNN architectures by defining search spaces through text prompts. The LLM generates candidate architectures, which are evaluated on the target graph, with performance metrics fed back to refine subsequent proposals. The toolkit includes LLM-enhanced node augmentation for text-attributed graphs, where LLM-generated explanations enrich node features. It provides a programmable interface for defining custom search spaces and supports various graph tasks including node classification, link prediction, and graph classification.

## Key Results
- Achieves 99.88% accuracy on NS dataset and 99.33% on Router dataset for link prediction
- Attains 92.97% accuracy on ACM dataset for node classification
- Outperforms existing GNAS methods on both homogeneous and heterogeneous graphs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large Language Models (LLMs) can iteratively optimize Graph Neural Network (GNN) architectures by acting as a controller that generates, evaluates, and refines candidates based on performance feedback.
- **Mechanism:** The framework defines a search space via prompts ($P$). The LLM generates candidate architectures ($M$), which are evaluated on the target graph. The resulting performance metrics ($A$) are fed back into the LLM via a "reward prompt" ($P_F$), allowing the model to refine subsequent architecture proposals without manual code updates.
- **Core assumption:** The LLM possesses sufficient inherent knowledge of GNN design principles (e.g., aggregation strategies, skip connections) to interpret performance feedback and propose meaningful architectural mutations.
- **Evidence anchors:** [Abstract] "enabling the adaptation of GNAS methods to new search spaces through the modification of LLM prompts... reducing the need for manual intervention." [Section 2.3] "The process... involves iteratively designing GNNs... By continuously integrating the generated GNNs... and their evaluation results in the reward prompt $P_F$, the LLM converges fast."

### Mechanism 2
- **Claim:** Augmenting node features with embeddings or explanations generated by LLMs improves the accuracy of downstream GNNs, particularly for text-attributed graphs.
- **Mechanism:** For labeled data, the system uses LLMs to generate explanations and pseudo-labels (e.g., TAPE method), enriching the text attributes. A smaller language model is then fine-tuned to encode these enriched semantics into initial node embeddings, providing the GNN with higher-quality input features than raw text alone.
- **Core assumption:** Nodes in the graph possess textual attributes containing latent semantic information that standard initialization methods (e.g., random walks or raw bag-of-words) fail to capture fully.
- **Evidence anchors:** [Section 2.2] "LLM4GNAS adopts explanation-based enhancement methods... to augment textual attributes." [Section 4.4.1] "GCN with node augmentation (GCN+NA) improved accuracy from 83.62% to 93.53% [on PubMed]... demonstrating the benefit of the added node augmentation."

### Mechanism 3
- **Claim:** Defining the search space programmatically via text prompts allows for rapid adaptation to new graph types (e.g., homogeneous vs. heterogeneous) without rewriting the core search algorithm.
- **Mechanism:** Instead of hard-coding search operations, the user defines a `SearchSpaceBase` class containing text prompts that describe candidate operations and connections. The LLM reads these text descriptions to construct the search space dynamically.
- **Core assumption:** The architectural constraints and operations of a new graph domain can be effectively described in natural language for the LLM to parse and utilize.
- **Evidence anchors:** [Section 3.2] "This feature leverages prompt engineering to adapt... [the class includes] `operation_prompt`, `connection_prompt`... LLM4GNAS is able to access the new search space via its name." [Section 4.4.3] Reports that LLM4GNAS successfully navigated the search space defined by NAS-Bench-Graph, outperforming random and RL baselines.

## Foundational Learning

- **Concept:** **Graph Neural Architecture Search (GNAS)**
  - **Why needed here:** This is the core automation task. You must understand that GNAS is not just training a model, but searching for the model's structure (layers, aggregation types) itself.
  - **Quick check question:** How does the "search space" in GNAS differ from the "hyperparameters" in standard training?

- **Concept:** **Prompt Engineering for Code Generation**
  - **Why needed here:** The toolkit relies on prompts to define the search space and guide the LLM. Understanding how to structure these inputs (operation prompts vs. strategy prompts) is critical for extending the toolkit.
  - **Quick check question:** If you wanted to add a new custom layer to the search space, would you need to retrain the LLM or just modify the prompt?

- **Concept:** **Text-Attributed Graphs (TAGs)**
  - **Why needed here:** The node augmentation mechanism specifically targets graphs where nodes have text features.
  - **Quick check question:** Why might a standard GNN (like GCN) struggle with raw text features compared to LLM-augmented features?

## Architecture Onboarding

- **Component map:** AutoSolver -> SearchSpaceBase -> LLM Controller -> Evaluator
- **Critical path:**
  1.  **Setup:** Install dependencies (PyTorch, PYG) and configure API keys for the LLM (e.g., OpenAI).
  2.  **Configuration:** Select a search space (e.g., `AutoGEL`) and task type.
  3.  **Execution:** Initialize `AutoSolver` and call `.fit(dataset)`.
  4.  **Retrieval:** Access the `best_gnn` model artifact for prediction.

- **Design tradeoffs:**
  - **LLM Choice:** GPT-4 yields the best results (Table 5) but is slow and costly. Open-source models (LLAMA-2, GLM-3) are supported but show a performance drop of ~1-3%.
  - **Iteration Count:** The default is 15 iterations (10 architectures each). Reducing this lowers cost but risks missing the optimal architecture.
  - **Node Augmentation:** Adds significant computation upfront (generating LLM embeddings) but drastically improves accuracy on sparse/labeled datasets.

- **Failure signatures:**
  - **Hallucinated Syntax:** The LLM generates Python code for a GNN that contains syntax errors or undefined layer types.
  - **Dimension Mismatch:** The generated architecture fails to map input feature dimensions to output dimensions correctly.
  - **API Timeouts:** The search loop crashes due to network latency with the LLM provider.

- **First 3 experiments:**
  1.  **Hello World (Cora):** Run the exact code in Listing 1 on the Cora dataset to verify the pipeline and API integration.
  2.  **Ablation on Augmentation:** Run the same experiment with Node Augmentation disabled to quantify the contribution of the LLM-generated features.
  3.  **Custom Search Space:** Implement the `NewSearchSpace` class (Listing 2) by adding a single novel operation (e.g., a custom attention head) to verify you can extend the search space without modifying the core library.

## Open Questions the Paper Calls Out

- **Question:** How can the design of search strategy prompts be fully automated to further minimize human intervention?
  - **Basis in paper:** [explicit] The Conclusion states that future work will focus on "automating the design of search strategy prompts."
  - **Why unresolved:** Currently, the framework relies on user-defined or manually adjusted prompts (Section 3.2) to guide the LLM, which still requires domain knowledge to formulate effectively.
  - **What evidence would resolve it:** A module that dynamically generates and refines its own search prompts based on dataset characteristics without manual templates.

- **Question:** How can LLM-based GNAS methods be made transferable specifically for graph foundation models?
  - **Basis in paper:** [explicit] The Conclusion identifies the need for "developing transferable LLM-based GNAS methods specifically for graph foundation models."
  - **Why unresolved:** The current toolkit focuses on task-specific architecture search rather than transferring learned architectural knowledge to pre-trained graph foundation models.
  - **What evidence would resolve it:** A method demonstrating that architectures searched by LLM4GNAS can effectively fine-tune or adapt large graph foundation models across distinct domains.

- **Question:** Can the performance gap between proprietary models (e.g., GPT-4) and open-source models (e.g., LLAMA-2) be bridged?
  - **Basis in paper:** [inferred] Table 5 shows a significant accuracy drop when using LLAMA-2 (85.45% on Cora) compared to GPT-4 (89.22%), suggesting the method currently relies heavily on the capabilities of expensive, closed-source models.
  - **Why unresolved:** The paper does not explore prompt optimization or fine-tuning techniques specifically for smaller, open-source models to close this performance gap.
  - **What evidence would resolve it:** Experiments demonstrating that modified prompts or fine-tuned local LLMs can achieve parity with GPT-4 performance within the LLM4GNAS framework.

## Limitations

- **No source code or prompt templates:** The absence of source code or explicit prompt templates makes faithful reproduction challenging without reverse-engineering the LLM4GNAS pipeline.
- **Reliance on GPT-4:** The framework's performance depends heavily on GPT-4, introducing cost and non-determinism concerns as performance may vary with model updates or API rate limits.
- **Limited scalability validation:** The paper does not address scalability to very large graphs or real-time deployment scenarios, nor does it compare against the most recent GNAS methods post-2023.

## Confidence

- **High Confidence:** The core claim that LLM prompts can guide GNN architecture search (Mechanism 1) is well-supported by ablation results in Table 5 and the detailed description of the iterative feedback loop.
- **Medium Confidence:** The efficacy of LLM-enhanced node augmentation (Mechanism 2) is supported by the PubMed result (83.62% â†’ 93.53%) but relies on a single external citation for the TAPE method, and the generalizability to other text-attributed datasets is not thoroughly explored.
- **Low Confidence:** The programmatic search space definition (Mechanism 3) is logically sound and described in detail, but the claim of seamless adaptation to new graph types is primarily demonstrated via NAS-Bench-Graph, a synthetic benchmark, without validation on complex real-world heterogeneous graphs.

## Next Checks

1. **Prompt Template Reconstruction:** Reconstruct the exact operation, connection, and strategy prompts described in Section 3.2 and test their ability to generate valid GNN architectures on a small synthetic dataset.
2. **Ablation on Node Augmentation:** Run the node classification pipeline on Cora with and without the LLM-augmented features to quantify the accuracy gain and verify the implementation of the TAPE method.
3. **Cost-Performance Trade-off:** Execute a reduced iteration search (5 iterations instead of 15) on Citeseer and measure the degradation in accuracy versus the reduction in LLM API costs to establish a practical budget for deployment.