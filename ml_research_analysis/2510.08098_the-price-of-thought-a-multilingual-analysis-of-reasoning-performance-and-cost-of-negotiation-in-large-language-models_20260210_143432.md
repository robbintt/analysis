---
ver: rpa2
title: 'The Price of Thought: A Multilingual Analysis of Reasoning, Performance, and
  Cost of Negotiation in Large Language Models'
arxiv_id: '2510.08098'
source_url: https://arxiv.org/abs/2510.08098
tags:
- reasoning
- player
- game
- other
- proposal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically evaluates how explicit reasoning capabilities\
  \ affect LLM performance in multilingual negotiation tasks across three languages.\
  \ Using a self-play setup with three dialogue games, the research reveals that reasoning\
  \ dramatically improves negotiation outcomes\u2014GPT-5\u2019s performance increased\
  \ by 31.4% with reasoning enabled, but at nearly 400% higher computational cost."
---

# The Price of Thought: A Multilingual Analysis of Reasoning, Performance, and Cost of Negotiation in Large Language Models

## Quick Facts
- arXiv ID: 2510.08098
- Source URL: https://arxiv.org/abs/2510.08098
- Reference count: 40
- Primary result: Reasoning improves negotiation outcomes by 31.4% but increases cost nearly 400%

## Executive Summary
This study systematically evaluates how explicit reasoning capabilities affect LLM performance in multilingual negotiation tasks across three languages. Using a self-play setup with three dialogue games, the research reveals that reasoning dramatically improves negotiation outcomes—GPT-5’s performance increased by 31.4% with reasoning enabled, but at nearly 400% higher computational cost. A critical finding is that open-weight models consistently switch to English for internal reasoning traces even when negotiating in German or Italian, while commercial models maintain language consistency. This inconsistency impacts interpretability and trustworthiness. The research demonstrates that reasoning enables genuine strategic adaptation rather than surface-level pattern matching, with models showing improved handling of complex rules, value-based decisions, and collaborative outcomes when “thinking out loud” before acting.

## Method Summary
The study uses a self-play setup with three dialogue games (Deal or No Deal, Knapsack, and Ultimatum) to evaluate LLM negotiation performance across English, German, and Italian. Models negotiate with themselves using either reasoning-on or reasoning-off modes, where reasoning-on generates hidden intermediate traces before structured output. The evaluation framework measures both instruction-following compliance (% Played) and goal achievement (Quality Score) combined into a clemscore. Games include language-agnostic items but language-specific prompts and rules. Cost analysis tracks token generation differences between reasoning modes.

## Key Results
- GPT-5 with reasoning enabled shows 31.4% performance improvement but 4x computational cost increase
- Open-weight models consistently switch to English for internal reasoning traces across all non-English tasks
- Reasoning models follow a three-stage optimization strategy for knapsack problems: greedy selection, constraint satisfaction, and refinement
- Non-reasoning models frequently submit proposals that contradict their conversational agreements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit reasoning traces improve proposal-action alignment by enabling pre-commitment verification before output.
- Mechanism: When models generate internal reasoning before final output, they can self-verify that their proposal matches their stated strategy. Non-reasoning models frequently agree to a division in conversation but submit conflicting proposals to the game master.
- Core assumption: The reasoning trace serves as an intermediate representation that constrains final output, reducing action-plan divergence.
- Evidence anchors:
  - [section B.4]: "A major failure for non-reasoning models, particularly in the GPT-5 family, is agreeing to a specific division of items in conversation but then submitting an entirely different proposal to the game master"
  - [section B.4]: Table 6 shows reasoning models reduce "Wrong proposal" outcomes from 11-13 instances to 0-1
  - [corpus]: Limited direct corpus support; related work focuses on negotiation tactics, not mechanism-level alignment
- Break condition: If the reasoning trace is generated post-hoc or doesn't actually constrain the output layer, this mechanism degrades to mere correlation.

### Mechanism 2
- Claim: Language-consistent reasoning preserves cultural/contextual signals that improve strategic decisions in non-English tasks.
- Mechanism: Commercial models that maintain German/Italian in reasoning traces can leverage language-specific affordances (e.g., formal vs. informal address, cultural norms). Open-weight models switching to English lose these signals, potentially degrading trust and interpretability.
- Core assumption: The language of reasoning carries task-relevant semantic content, not just syntactic scaffolding.
- Evidence anchors:
  - [abstract]: "open-weight models consistently switch to English for their internal reasoning steps, even when negotiating in German or Italian... while a leading commercial model maintains language consistency"
  - [section 5.3]: Table 2 shows open-weight models have 0.06 (German) and 0.03 (Italian) reasoning language consistency vs. 1.00 for completions
  - [corpus]: Qi et al. (2025) found forcing models to reason in target language can reduce performance, suggesting complex tradeoff
- Break condition: If reasoning language is purely instrumental and all strategic content transfers across languages, this mechanism is aesthetic rather than functional.

### Mechanism 3
- Claim: Structured reasoning enables three-stage optimization (greedy selection, constraint satisfaction, refinement) for knapsack-style problems.
- Mechanism: Reasoning models decompose complex negotiations into: (1) value-to-weight ratio ranking, (2) filling to weight limit, (3) iterative substitution. This prevents premature commitment and allows counterparty modeling.
- Core assumption: The multi-step structure is emergent from reasoning training, not just longer generation time.
- Evidence anchors:
  - [section D.4]: "Reasoning models outperform others by following a three-stage plan: (1) applying a greedy heuristic... (2) filling the knapsack... (3) refining via substitutions"
  - [section D.4]: Figures 44-47 show reasoning models have higher initial disagreement but faster convergence
  - [corpus]: Limited corpus evidence on mechanism; related work focuses on negotiation outcomes, not decomposition patterns
- Break condition: If models achieve similar results with simple prompting extensions (CoT), the benefit is from decomposition rather than reasoning-specific training.

## Foundational Learning

- Concept: **Test-time compute scaling**
  - Why needed here: The paper frames reasoning as "scaling test time compute"—generating additional tokens before final output increases computational cost (4x for GPT-5) but improves performance.
  - Quick check question: Can you distinguish between more tokens from longer context vs. more tokens from explicit reasoning chains?

- Concept: **Pareto efficiency in multi-agent settings**
  - Why needed here: Negotiation quality is measured by how close outcomes are to Pareto-optimal allocations (no player can improve without harming the other).
  - Quick check question: Given two players with different value functions, what makes a division "Pareto optimal"?

- Concept: **Clembench evaluation framework**
  - Why needed here: Results use `% Played` (instruction following) and `Quality Score` (goal achievement), combined into `clemscore`. A model that aborts games scores 0 regardless of reasoning quality.
  - Quick check question: Why might a model with excellent strategic reasoning still score poorly on clemscore?

## Architecture Onboarding

- Component map:
  Game Master -> Self-play setup -> Reasoning trace generation -> Structured output validation -> Turn continuation or abort

- Critical path:
  1. Game instance generation (language-agnostic items, language-specific prompts)
  2. Model receives prompt with value function, rules, goal
  3. Reasoning model generates hidden trace, then structured action
  4. Game Master validates action; if invalid, penalty or abort
  5. Turns continue until agreement, timeout, or abort

- Design tradeoffs:
  - **Cost vs. performance**: 4x cost increase for 31% performance gain (GPT-5). Is this acceptable for your use case?
  - **Interpretability vs. consistency**: Open-weight models expose reasoning traces but in wrong language; commercial models maintain language but may not expose traces (GPT-5 doesn't return them)
  - **Instruction following vs. strategic depth**: Nemotron-9B shows good quality scores when games complete but high abort rates due to format violations

- Failure signatures:
  - **Premature proposals**: Model submits proposal before exchanging information (common in weak open-weight models)
  - **Proposal-strategy mismatch**: Agrees to one division, submits another (non-reasoning GPT-5)
  - **Reasoning loops**: Repeating same thought 3+ times; correlates with lower scores (Figure 7a)
  - **Language switch in reasoning**: Open-weight models use English internally even for German/Italian tasks

- First 3 experiments:
  1. **Baseline**: Run Deal or No Deal (semi-competitive mode, English) with reasoning off to establish `% Played` and quality baseline
  2. **Ablation**: Enable reasoning, measure cost increase and clemscore delta; check if improvement comes from better instruction following or better strategy
  3. **Cross-lingual**: Run German instance, inspect reasoning trace language (if accessible) to diagnose interpretability issues before deploying in multilingual settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can language-consistent reasoning be enforced in open-weight models without degrading task performance?
- Basis in paper: [explicit] The paper notes that "open-weight models consistently switch to English for their internal reasoning steps, even when negotiating in German or Italian" and cites Qi et al. (2025) finding that forcing models to reason in the target language can reduce performance.
- Why unresolved: The tension between interpretability (language-consistent traces) and performance remains unexplored—no intervention was tested.
- What evidence would resolve it: Fine-tuning or prompting experiments that enforce target-language reasoning with controlled performance comparisons.

### Open Question 2
- Question: Why does reasoning-enabled mode harm Llama-70B performance (negative margin) while benefiting other models?
- Basis in paper: [explicit] Table 1 shows Llama-70B with reasoning "On" has a negative margin (-27.3% overall), and the paper briefly speculates this "may be due to the effect of distillation" without investigation.
- Why unresolved: No ablation or analysis was conducted on distillation effects in reasoning-capable models.
- What evidence would resolve it: Controlled comparison of distilled vs. non-distilled variants with matched parameters on identical tasks.

### Open Question 3
- Question: How does reasoning language-switching affect performance on tasks requiring culture-specific or language-specific conceptual knowledge?
- Basis in paper: [inferred] The limitations section states: "For use cases that involve cultural or language-specific concepts or aspects, it can not be guaranteed that they will be well-understood in English (as most languages switch to it)."
- Why unresolved: The study used language-agnostic game instances; no culturally-grounded tasks were tested.
- What evidence would resolve it: Experiments with negotiation games containing culturally-bound items, idioms, or region-specific value systems.

### Open Question 4
- Question: What is the minimal effective reasoning budget for achieving substantial negotiation performance gains?
- Basis in paper: [inferred] The paper documents a ~400% cost increase for 31.4% performance gain but does not explore intermediate compute budgets or early stopping.
- Why unresolved: Only on/off reasoning modes were tested; no scaling curve was characterized.
- What evidence would resolve it: Experiments varying maximum reasoning token lengths or turn limits to map performance-cost tradeoffs.

## Limitations

- Self-play setup may not capture real-world negotiation dynamics with asymmetric information and capabilities
- Evaluation framework conflates reasoning quality with format compliance, potentially penalizing strategic but non-compliant models
- Cost analysis only considers per-token pricing without accounting for API overhead or context window effects

## Confidence

**High confidence**: The cost-performance tradeoff finding (31.4% improvement at 4x cost for GPT-5) is robust across multiple games and languages, supported by direct quantitative measurements in Table 3. **Medium confidence**: The claim that reasoning enables genuine strategic adaptation rather than surface pattern matching relies on behavioral observations (three-stage optimization) but lacks ablation studies to rule out alternative explanations like increased token count alone. **Low confidence**: The language consistency finding, while striking in descriptive statistics, has limited causal evidence—the study doesn't establish whether English reasoning traces actually harm performance or merely reduce interpretability.

## Next Checks

1. **Ablation on token count**: Run experiments with CoT prompting vs. reasoning models to isolate whether performance gains come from explicit reasoning training or simply generating more tokens before output
2. **Real-world deployment stress test**: Implement a mixed-language negotiation scenario where models must coordinate across language boundaries, measuring whether English reasoning traces create coordination failures
3. **Cost-effectiveness analysis**: Calculate total cost per Pareto-optimal outcome across different reasoning-cost combinations to determine the practical threshold where reasoning becomes economically viable