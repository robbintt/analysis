---
ver: rpa2
title: Dynamic Vision Mamba
arxiv_id: '2504.04787'
source_url: https://arxiv.org/abs/2504.04787
tags:
- token
- block
- pruning
- tokens
- dyvm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the inefficiency in Vision Mamba models caused
  by spatial redundancy at both token and block levels. The authors propose Dynamic
  Vision Mamba (DyVM), which customizes token pruning for Mamba's recurrent structure
  by rearranging pruned tokens before feeding them to the next block, avoiding training-inference
  inconsistency and extra computation.
---

# Dynamic Vision Mamba

## Quick Facts
- arXiv ID: 2504.04787
- Source URL: https://arxiv.org/abs/2504.04787
- Reference count: 40
- Primary result: Achieves 35.2% FLOPs reduction with only 1.7% accuracy loss on Vim-S

## Executive Summary
Dynamic Vision Mamba (DyVM) addresses spatial redundancy in Vision Mamba models through two complementary mechanisms: token pruning and dynamic block selection. The method customizes token pruning for Mamba's recurrent structure by rearranging pruned tokens before feeding them to the next block, avoiding training-inference inconsistency. Additionally, DyVM dynamically selects SSM blocks for each image based on computational complexity, reducing active blocks while maintaining accuracy. The approach generalizes across different Mamba vision model architectures and tasks, demonstrating consistent performance improvements on VideoMamba, MambaReg, and semantic segmentation tasks.

## Method Summary
DyVM operates through a token pruning mechanism that rearranges sequences to maintain training-inference consistency in recurrent SSMs, and a block selection mechanism that uses lightweight predictors to dynamically skip SSM blocks. The system employs a multi-term loss function combining classification, ratio constraints, and knowledge distillation to stabilize learning. Token predictors are placed at specific layers while block selectors operate at every layer, using the class token to determine computational complexity. The approach achieves efficiency gains through both token-level pruning (reducing FLOPs) and block-level skipping (improving throughput).

## Key Results
- 35.2% reduction in FLOPs with only 1.7% accuracy loss on Vim-S
- Demonstrates generalization across Vim, VideoMamba, and MambaReg architectures
- Achieves consistent performance improvements on semantic segmentation tasks
- Shows 1.36x throughput improvement by removing one SSM block despite marginal FLOP reduction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Token pruning in SSMs causes training-inference inconsistency because hidden state evolution depends on sequence position
- **Mechanism:** DyVM rearranges the sequence during training: retained tokens are moved to the front and pruned tokens to the back/masked, ensuring the i-th retained token receives the same evolution transformations during training as during inference
- **Core assumption:** Performance degradation stems from mismatch in recurrent state depth between masked training and physical removal at inference
- **Evidence anchors:** Abstract mentions rearranging pruned sequences; section 3.2 shows mismatch in evolution transformations (Equations 9 vs 10); MambaScope notes existing token reduction leads to information loss
- **Break condition:** If SSM architecture doesn't use recurrent hidden states (purely convolutional), rearrangement provides no benefit over standard masking

### Mechanism 2
- **Claim:** Inference throughput in Vision Mambas is bottlenecked by SSM block count rather than theoretical FLOPs
- **Mechanism:** DyVM employs lightweight "Block Selector" at every layer using class token to predict binary mask, dynamically skipping forward, backward, or both SSM blocks for specific samples
- **Core assumption:** Not all images require bidirectional context aggregation at every layer; "easy" samples can rely on fewer scans
- **Evidence anchors:** Abstract states inference speed is largely affected by number of SSM blocks; section 1, Figure 1b shows 1.36x throughput improvement with one block removed; DYNAMAX supports dynamic computing for Mamba
- **Break condition:** On hardware where SSM scanning isn't primary latency bottleneck, throughput gains may diminish

### Mechanism 3
- **Claim:** Joint supervision using distillation and ratio constraints is necessary to stabilize learning of dynamic pruning masks
- **Mechanism:** System uses multi-term loss: standard classification, MSE loss for token/block keeping ratios, and knowledge distillation losses (output KL-divergence and token-level MSE) to align pruned student with original teacher
- **Core assumption:** Unpruned backbone contains "dark knowledge" that guides pruned model better than ground truth labels alone
- **Evidence anchors:** Section 3.4 defines joint loss terms (Equations 23-27); table 7 shows distillation boosts accuracy by 0.3-0.4%; corpus neighbors focus on architecture/pruning efficiency rather than specific loss configurations
- **Break condition:** If teacher model is poorly trained or vastly different in capacity, distillation signals may conflict with ground truth classification loss, degrading convergence

## Foundational Learning

- **Concept: State Space Models (SSM) Recurrence**
  - **Why needed here:** Understanding the training-inference inconsistency problem requires grasping that Mamba accumulates information via recurrent hidden state $h_t$, where matrix $\bar{A}$ dictates history keeping
  - **Quick check question:** If you remove the 3rd token from a 5-token sequence, does the 4th token become the new 3rd input? How does this shift affect the hidden state it inherits?

- **Concept: Gumbel-Softmax / Concrete Distribution**
  - **Why needed here:** DyVM needs to make discrete decisions (Keep vs. Prune) to save computation, and standard backpropagation cannot flow through discrete "argmax"
  - **Quick check question:** How does the temperature parameter in Gumbel-Softmax control the trade-off between relaxed (probabilistic) mask and hard (discrete) mask during training?

- **Concept: Knowledge Distillation**
  - **Why needed here:** Aggressive pruning (35% FLOPs reduction) discards information, and distillation forces pruned model to mimic logits and intermediate features of original model
  - **Quick check question:** Why is MSE used for token distillation ($L_{dis\_token}$) while KL-Divergence is used for output distillation ($L_{dis\_out}$)?

## Architecture Onboarding

- **Component map:** Input Image -> Patches -> Mamba Blocks -> Token Predictor (at layers 6, 12, 18) -> Rearrangement Layer -> Block Selector (at every layer) -> Output
- **Critical path:**
  1. Input: Image → Patches
  2. Forward Pass: Pass through Mamba blocks
  3. Dynamic Decision: At configured layers, run Predictor
  4. Consistency Step: Rearrange sequence (Retained → Pruned) before passing to next SSM state update
  5. Block Skip: At every layer, check Block Selector mask to bypass Forward/Backward SSMs if allowed
- **Design tradeoffs:**
  - Token pruning reduces FLOPs significantly (compute bound); block pruning improves throughput significantly (latency bound)
  - Rearrangement cost: Moving memory costs bandwidth, claimed negligible compared to SSM computation but could be bottleneck on memory-constrained devices
  - Block selection is data-dependent, creating variable latency that might be problematic for real-time streaming systems requiring fixed cadence
- **Failure signatures:**
  - Inconsistency Crash: Standard masking instead of rearranging causes large accuracy gap between train and eval
  - Throughput Stagnation: Only token pruning barely improves FPS despite FLOPs drop
  - Collapse: Keeping ratio $\rho$ too low or weak distillation causes model to prune everything, resulting in 0% accuracy
- **First 3 experiments:**
  1. Consistency Validation: Train Vim-T with (A) Standard Token Masking and (B) DyVM Rearrangement, compare train-vs-eval accuracy gap on CIFAR/ImageNet subset
  2. Throughput Profiling: Integrate Block Selection with fixed "keep-all" tokens, measure FPS on A100/V100 while sweeping Block Ratio (1.0, 0.8, 0.6)
  3. Loss Ablation: Train full DyVM pipeline without Token Distillation loss ($L_{dis\_token}$), compare top-1 accuracy on ImageNet-1K against full model

## Open Questions the Paper Calls Out
- How can future Vision Mamba architectures be natively designed to minimize block redundancy without relying on external dynamic predictors?
- Can the token rearrangement strategy be effectively adapted for Vision Mamba models utilizing multi-directional scanning (e.g., Cross-Scan) without disrupting spatial dependencies?
- Is it possible to achieve memory reduction for dense prediction tasks using DyVM without overhead of restoring full spatial feature map?

## Limitations
- Effectiveness relies on assumption that training-inference inconsistency is primary bottleneck in Mamba pruning
- Rearrangement introduces memory bandwidth overhead not quantified in main paper
- Block selection assumes "easy" images can be reliably identified through class token features alone
- 35.2% FLOPs reduction with 1.7% accuracy loss reported only on Vim-S, performance on larger models may degrade faster

## Confidence
**High Confidence**: Core observation that SSM block count affects inference throughput more than FLOPs is well-supported by literature and hardware profiling studies
**Medium Confidence**: Distillation loss configuration and its contribution to accuracy recovery shows measurable improvements but relative importance of each term is unclear
**Low Confidence**: Generalization claims across different Mamba variants and tasks based on limited experiments without systematic ablation studies

## Next Checks
1. **Hardware profiling study**: Implement token rearrangement and measure memory bandwidth usage vs. compute savings on A100/V100, quantify break-even point where rearrangement overhead exceeds SSM computation savings
2. **Cross-task consistency validation**: Apply DyVM to dense prediction task (object detection or instance segmentation) and measure whether class-token-based block selection still correlates with computational complexity
3. **Component ablation across architectures**: Systematically disable token pruning and block selection independently on each tested architecture (Vim, VideoMamba, MambaReg, segmentation) to determine which component drives majority of efficiency gains