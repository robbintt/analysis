---
ver: rpa2
title: 'Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language
  Model Reasoning'
arxiv_id: '2511.07483'
source_url: https://arxiv.org/abs/2511.07483
tags:
- reward
- training
- reasoning
- arxiv
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces C2RM, a reward model that incorporates both
  correctness and confidence to guide small-scale language models in STEM reasoning
  tasks. The model penalizes low-confidence correct answers to encourage more robust
  reasoning, addressing the limitations of purely rule-based reinforcement learning
  that often rewards accidental correct answers from smaller models.
---

# Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning

## Quick Facts
- arXiv ID: 2511.07483
- Source URL: https://arxiv.org/abs/2511.07483
- Authors: Qianxi He; Qingyu Ren; Shanzhe Lei; Xuhong Wang; Yingchun Wang
- Reference count: 23
- Primary result: C2RM achieves 64.28% accuracy on JudgeBench, surpassing open-source reward models

## Executive Summary
This paper introduces C2RM, a reward model that incorporates both correctness and confidence to guide small-scale language models in STEM reasoning tasks. The model penalizes low-confidence correct answers to encourage more robust reasoning, addressing the limitations of purely rule-based reinforcement learning that often rewards accidental correct answers from smaller models. The approach uses a two-step data selection strategy to create high-quality training samples across science, math, and logic domains, training on approximately 20K contrastive pairs. Evaluation across multiple benchmarks shows C2RM achieves strong performance, surpassing open-source reward models with 64.28% accuracy on JudgeBench, maintaining longer and more comprehensive responses during RL training (245.1 tokens average vs 207.8), and improving policy model accuracy by 6.34% over baselines.

## Method Summary
C2RM addresses the problem of training small language models for STEM reasoning by creating a reward model that evaluates both correctness and confidence. The method uses a two-step data selection process: first filtering questions using a large model (Qwen2.5-72B-Instruct) to identify those with 40-70% accuracy, then generating 5 responses per question from three different models at temperature 0.7. Correctness is determined against ground truth, while confidence is estimated via consistency across multiple high-temperature rollouts. The model is trained on approximately 20K contrastive pairs using Qwen2.5-7B-Instruct fine-tuned for 2 epochs with learning rate 5e-6. The approach is evaluated on multiple benchmarks including Best-of-N accuracy on GPQA, MATH500, FOLIO, JudgeBench accuracy, and PPO RL policy accuracy with response length as a proxy for reasoning quality.

## Key Results
- C2RM achieves 64.28% accuracy on JudgeBench, surpassing open-source reward models
- Maintains longer and more comprehensive responses during RL training (245.1 tokens average vs 207.8)
- Improves policy model accuracy by 6.34% over baselines
- Ablation study confirms that incorporating both correctness and confidence dimensions is essential for optimal performance

## Why This Works (Mechanism)
C2RM works by recognizing that traditional reward models that only consider correctness can incentivize small language models to produce superficial or lucky correct answers without genuine reasoning. By incorporating confidence estimation through response consistency, the model can distinguish between answers that are correct due to sound reasoning versus those that are correct by chance. The confidence penalty ensures that even correct answers must demonstrate robustness across multiple sampling attempts to receive full reward. This creates a more nuanced training signal that encourages small models to develop genuine reasoning capabilities rather than relying on lucky guesses. The two-step data selection strategy ensures high-quality training samples that capture the complexity of real-world STEM reasoning while maintaining sufficient diversity to prevent overfitting to simple patterns.

## Foundational Learning

**Self-consistency estimation**: Measuring response agreement across multiple samples to estimate confidence. Needed to provide a proxy for answer reliability when ground truth confidence is unavailable. Quick check: Compare consistency scores against human-annotated confidence labels.

**Contrastive pair construction**: Creating training pairs that capture the relationship between correctness and confidence. Needed to teach the model to distinguish between different answer types (correct-confident vs correct-unconfident). Quick check: Verify pair distribution matches natural occurrence frequencies.

**Two-step data filtering**: Using a large model to filter questions before generating responses from multiple smaller models. Needed to ensure training data represents challenging but learnable problems. Quick check: Validate that filtered questions span appropriate difficulty range.

**Supervised reward fine-tuning**: Adapting a pretrained language model to output confidence-aware rewards. Needed to leverage existing language understanding while adding specialized reasoning capabilities. Quick check: Monitor reward calibration against known correct/incorrect examples.

## Architecture Onboarding

**Component map**: Question filter -> Response generator (3 models, 5 samples each) -> Correctness labeler -> Confidence estimator (self-consistency) -> Pair constructor -> Reward model fine-tuner -> RL policy trainer

**Critical path**: Data generation (filter + generate) -> Labeling (correctness + confidence) -> Pair construction -> Reward model training -> RL policy fine-tuning

**Design tradeoffs**: The approach trades computational cost of multiple response generations against improved reward quality. Alternative approaches could use single-pass confidence estimation or simpler filtering criteria, but these would sacrifice the nuanced understanding of answer reliability that C2RM provides.

**Failure signatures**: Low label diversity (skewed T&U/F&C distribution), reward hacking (response collapse during RL), or poor generalization to new domains would indicate problems with data quality, model architecture, or training procedure.

**First experiments**: 1) Verify consistency estimation correlates with actual answer reliability, 2) Test reward model calibration on held-out pairs, 3) Compare RL performance with and without confidence-aware rewards.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on self-consistency as a proxy for confidence may not hold for problems with multiple valid solution paths
- Two-step data selection process may introduce selection bias limiting generalization
- Effectiveness depends heavily on quality of consistency estimation process with unspecified parameters

## Confidence

**High Confidence**: Claims about improved response length (245.1 vs 207.8 tokens) and baseline comparison results (6.34% accuracy improvement) are supported by experimental data and ablation studies.

**Medium Confidence**: Claims about superiority of two-step data selection strategy are based on internal comparisons but lack direct comparison to simpler sampling approaches.

**Low Confidence**: Claims regarding universal applicability to other domains or model scales lack empirical validation beyond specific STEM tasks and small-to-medium language models tested.

## Next Checks

1. **Ablation on Confidence Estimation**: Systematically vary consistency threshold and rollout parameters to determine sensitivity of performance to these hyperparameters.

2. **Cross-Domain Transfer**: Evaluate C2RM on non-STEM reasoning tasks including commonsense reasoning and creative writing to assess generalizability.

3. **Comparison to Alternative Confidence Methods**: Implement and compare against alternative confidence estimation approaches such as entropy-based uncertainty or model uncertainty from Monte Carlo dropout.