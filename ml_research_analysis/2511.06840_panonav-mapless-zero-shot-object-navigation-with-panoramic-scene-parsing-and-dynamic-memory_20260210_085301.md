---
ver: rpa2
title: 'PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing
  and Dynamic Memory'
arxiv_id: '2511.06840'
source_url: https://arxiv.org/abs/2511.06840
tags:
- navigation
- object
- memory
- agent
- mapless
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PanoNav, a mapless zero-shot object navigation
  framework that uses only RGB images. It addresses the limitations of existing methods
  that rely on depth sensors or prebuilt maps, and those that make short-sighted decisions
  without historical context.
---

# PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory

## Quick Facts
- **arXiv ID:** 2511.06840
- **Source URL:** https://arxiv.org/abs/2511.06840
- **Reference count:** 10
- **Key outcome:** PanoNav achieves 43.5% SR and 23.7% SPL on HM3D, outperforming RGB-only mapless baselines.

## Executive Summary
This paper introduces PanoNav, a mapless zero-shot object navigation framework that uses only RGB images, addressing the limitations of depth-dependent and short-sighted navigation methods. PanoNav combines panoramic scene parsing using multi-view RGB and dot matrix representations with a dynamic memory-guided decision-making system to avoid local deadlocks. The method leverages large multimodal models for spatial reasoning and large language models for action selection, achieving state-of-the-art performance on the HM3D benchmark.

## Method Summary
PanoNav is an inference-only pipeline that takes 6 panoramic RGB images (60° intervals) as input and navigates to target objects in unseen environments without maps or depth sensors. The method converts RGB images to dot matrix representations using SCA preprocessing, then uses Qwen-2.5-VL to extract local spatial graphs and global summaries. These are combined with a bounded memory queue of previous global summaries and fed to DeepSeek-V3 for action selection. The PixNav controller executes low-level movements (MoveAhead, Turn, LookUp/Down, Stop) based on the LLM's decisions.

## Key Results
- Achieves 43.5% Success Rate (SR) and 23.7% Success-weighted Path Length (SPL) on HM3D benchmark
- Outperforms RGB-only baselines: PixNav (37.9% SR, 20.5% SPL) and ZSON (25.5% SR, 12.6% SPL)
- Ablation study shows importance of panoramic views (SR drops from 43.5% to 19.5% with 3 views)

## Why This Works (Mechanism)

### Mechanism 1: Panoramic Scene Parsing
The system converts raw RGB images into "dot matrix" representations to highlight planar positions, then feeds both raw images and dot matrices into an MLLM to generate spatial relation graphs describing object layouts. This dual-input approach enables spatial relationship reasoning in the absence of depth data.

### Mechanism 2: Dynamic Memory-Guided Decision-Making
A bounded text-based memory queue prevents local deadlocks by storing the last n global scene summaries. If an area resembles a target location but has already been summarized in memory, the LLM is prompted to select an unexplored direction instead of re-evaluating the same scene.

### Mechanism 3: Decoupled Visual Parsing and Decision-Making
The framework first forces an MLLM to output structured text (object lists, spatial relations), then feeds this text to a text-only LLM for action selection. This creates a "perceptual buffer" that reduces hallucination pressure during the action step.

## Foundational Learning

### Concept: Zero-Shot Object Navigation (ZSON)
**Why needed here:** This is the core task definition—navigating to an object class (e.g., "sofa") in an unseen environment without specific training for that environment.
**Quick check question:** How does ZSON differ from classical SLAM-based navigation? (Answer: No pre-built map or specific training data for the environment.)

### Concept: Semantic Deadlock
**Why needed here:** The paper specifically targets the failure mode where agents loop in semantically likely areas (e.g., living rooms) even when the target is absent.
**Quick check question:** Why would an agent get stuck in a room that doesn't contain the target? (Answer: The semantic prior suggests it should be there, and without memory, the agent re-evaluates the same scene as "promising" repeatedly.)

### Concept: Text-based Memory Queues
**Why needed here:** The paper replaces metric maps (coordinates) with a queue of textual descriptions.
**Quick check question:** What information is lost when replacing a 2D map with a text queue? (Answer: Precise geometry and distance between far-away points.)

## Architecture Onboarding

### Component map:
1. **Input:** 6x RGB Cameras (60° intervals)
2. **Preprocessing:** Scaffold (SCA) converter (RGB → Dot Matrix)
3. **Perception:** MLLM (Qwen-2.5-VL) → Local Descriptions + Global Summary
4. **State:** Dynamic Bounded Memory Queue (stores Global Summaries)
5. **Policy:** LLM (DeepSeek-V3) → selects high-level direction
6. **Control:** PixNav Controller → executes low-level movement

### Critical path:
The reliability of the Global Summary is critical. If the MLLM outputs a generic summary (e.g., "a room with furniture"), the Memory Queue will fail to distinguish visited rooms, causing deadlock.

### Design tradeoffs:
- **Panoramic vs Front-facing:** Table 3 shows dropping from 6 views to 3 views drops SR from 43.5% to 19.5%. The tradeoff is computational cost (processing 6 images) vs. blind spots.
- **Mapless vs Mapping:** Avoids the complexity of depth sensing and SLAM but relies entirely on the faithfulness of language descriptions.

### Failure signatures:
- **Repetitive Descriptions:** If the MLLM generates identical summaries for different rooms, the agent may stop exploring.
- **Hallucination:** The MLLM inventing objects that are not present (e.g., seeing a "chair" in a texture), causing the LLM to stop prematurely.

### First 3 experiments:
1. **Unit Test MLLM Output:** Feed images of a known room to the MLLM with the SCA dot matrix. Verify if "Spatial relationship" text accurately reflects the geometry.
2. **Ablate Memory:** Run 20 episodes with the Memory Queue disabled. Verify the agent exhibits circular trajectories (Fig 5 behavior) in rooms that look like target locations.
3. **Component Swap:** Replace the Dot Matrix input with plain RGB only (keeping the rest of the pipeline). Check if SR drops, isolating the value of the dot matrix encoding.

## Open Questions the Paper Calls Out
- How can multimodal information be effectively integrated into the memory queue to surpass the limitations of text-only global summaries in long-horizon navigation?
- Does the First-In-First-Out (FIFO) eviction policy of the Dynamic Bounded Memory Queue result in the loss of critical long-term spatial context in environments larger than the queue capacity?
- How robust is the RGB-only framework against real-world visual noise (e.g., motion blur, lighting changes) compared to the simulated HM3D environments?

## Limitations
- The effectiveness of dot matrix preprocessing over raw RGB input lacks direct ablation evidence.
- The assumption that global scene summaries alone can reliably distinguish between explored and unexplored regions has not been validated across environments with high semantic repetition.
- The optimal size of the bounded memory queue is not specified, and different values could significantly affect performance.

## Confidence

### Major Uncertainties
- **High confidence:** The architectural framework and experimental methodology are clearly specified and reproducible. The comparison against baseline methods is valid.
- **Medium confidence:** The core innovation of decoupling visual parsing from decision-making is sound, but the specific implementation details (prompt templates, memory queue size) are missing.
- **Low confidence:** The claimed advantage of dot matrix preprocessing over raw RGB input lacks direct ablation evidence.

## Next Checks
1. **Dot Matrix Ablation Test:** Replace dot matrix inputs with raw RGB only in the MLLM pipeline and measure the change in Success Rate to quantify the preprocessing contribution.
2. **Memory Queue Length Sweep:** Run experiments with memory queue sizes of 3, 5, 10, and 20 to identify the optimal configuration and understand sensitivity to this hyperparameter.
3. **Semantic Repetition Stress Test:** Design a test environment with multiple identical rooms and verify whether the text-based memory correctly distinguishes between them or incorrectly marks unvisited target-containing rooms as "already explored."