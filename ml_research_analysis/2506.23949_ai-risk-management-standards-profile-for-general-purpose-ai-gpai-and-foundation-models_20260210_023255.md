---
ver: rpa2
title: AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation
  Models
arxiv_id: '2506.23949'
source_url: https://arxiv.org/abs/2506.23949
tags:
- foundation
- gpai
- nist
- system
- guidance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This document provides targeted risk-management practices for developers
  of large-scale, state-of-the-art general-purpose AI (GPAI) models and foundation
  models. It adapts and builds on the NIST AI Risk Management Framework and ISO/IEC
  23894, focusing on the unique risks of GPAI/foundation models such as misuse potential,
  emergent properties, and large-scale societal impacts.
---

# AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models

## Quick Facts
- arXiv ID: 2506.23939
- Source URL: https://arxiv.org/abs/2506.23939
- Reference count: 28
- One-line primary result: Provides targeted risk-management practices for developers of large-scale, state-of-the-art general-purpose AI (GPAI) models and foundation models.

## Executive Summary
This document provides targeted risk-management practices for developers of large-scale, state-of-the-art general-purpose AI (GPAI) models and foundation models. It adapts and builds on the NIST AI Risk Management Framework and ISO/IEC 23894, focusing on the unique risks of GPAI/foundation models such as misuse potential, emergent properties, and large-scale societal impacts. The profile offers guidance across the AI lifecycle—governance, mapping, measuring, and managing—emphasizing high-priority steps like red-teaming, risk tolerance setting, and transparency. It aims to help developers maximize benefits and minimize negative impacts while supporting compliance with emerging regulations. The profile is intended for GPAI/foundation model developers but also benefits downstream developers, evaluators, and regulators.

## Method Summary
The profile was developed by adapting and building on the NIST AI Risk Management Framework and ISO/IEC 23894 to address the unique risks associated with general-purpose AI (GPAI) and foundation models. It provides guidance across the AI lifecycle, including governance, mapping, measuring, and managing, with a focus on high-priority steps such as red-teaming, risk tolerance setting, and transparency. The document emphasizes the importance of voluntary adoption by GPAI developers, as compliance is not mandated, and acknowledges the need for frequent updates to keep pace with the rapid evolution of AI technology.

## Key Results
- Provides comprehensive risk-management practices for GPAI and foundation model developers.
- Emphasizes high-priority steps like red-teaming, risk tolerance setting, and transparency.
- Aims to support compliance with emerging regulations and maximize benefits while minimizing negative impacts.

## Why This Works (Mechanism)
The profile works by adapting established risk management frameworks (NIST AI RMF and ISO/IEC 23894) to the unique challenges of GPAI and foundation models, such as misuse potential, emergent properties, and large-scale societal impacts. It provides structured guidance across the AI lifecycle, ensuring that developers address governance, mapping, measuring, and managing risks systematically. The emphasis on high-priority steps like red-teaming and transparency helps developers identify and mitigate risks early, while the focus on compliance supports alignment with emerging regulations.

## Foundational Learning
- **Red-teaming**: Critical for identifying vulnerabilities and misuse potential in AI models. *Why needed*: Helps uncover risks that may not be apparent during standard testing. *Quick check*: Ensure red-teaming is conducted by diverse teams with expertise in AI safety and security.
- **Risk tolerance setting**: Essential for defining acceptable levels of risk in AI deployment. *Why needed*: Provides a clear framework for decision-making and accountability. *Quick check*: Verify that risk tolerance levels are aligned with organizational values and regulatory requirements.
- **Transparency measures**: Necessary for building trust and enabling accountability. *Why needed*: Allows stakeholders to understand and assess the risks associated with AI models. *Quick check*: Confirm that transparency measures are clear, accessible, and regularly updated.
- **Governance frameworks**: Foundational for ensuring responsible AI development and deployment. *Why needed*: Provides oversight and accountability across the AI lifecycle. *Quick check*: Assess whether governance frameworks are integrated into all stages of AI development.
- **Compliance with emerging regulations**: Important for aligning with legal and ethical standards. *Why needed*: Ensures that AI development adheres to evolving regulatory requirements. *Quick check*: Monitor regulatory updates and adjust practices accordingly.

## Architecture Onboarding
- **Component map**: Governance -> Mapping -> Measuring -> Managing
- **Critical path**: Establish governance framework -> Conduct risk mapping -> Implement risk measurement -> Execute risk management strategies
- **Design tradeoffs**: Balancing innovation with risk mitigation, standardization with flexibility, and transparency with proprietary concerns
- **Failure signatures**: Ineffective risk identification, lack of stakeholder engagement, non-compliance with regulations, and inadequate transparency
- **First experiments**:
  1. Conduct a pilot red-teaming exercise on a small-scale GPAI model to identify potential vulnerabilities.
  2. Develop a prototype risk tolerance framework tailored to the specific use case of a foundation model.
  3. Create a transparency report for a GPAI model, outlining its capabilities, limitations, and associated risks.

## Open Questions the Paper Calls Out
None

## Limitations
- The document's effectiveness depends heavily on voluntary adoption by GPAI developers, as compliance is not mandated.
- The rapid evolution of AI technology may outpace the guidance, requiring frequent updates.
- Some risk management practices, such as red-teaming and transparency measures, may be difficult to standardize across diverse models and use cases.

## Confidence
- **High confidence**: The document provides comprehensive and structured guidance for GPAI developers, building on established risk management frameworks.
- **Medium confidence**: The profile's applicability across diverse AI models and regulatory environments is well-intentioned but may face practical challenges.
- **Low confidence**: The long-term effectiveness of the proposed practices in mitigating large-scale societal impacts remains uncertain.

## Next Checks
1. Conduct a longitudinal study to assess the impact of the profile on GPAI developers' risk management practices and outcomes over time.
2. Perform a comparative analysis of the profile's guidance against emerging international regulations to identify gaps and overlaps.
3. Develop case studies of GPAI models that have implemented the profile's recommendations to evaluate real-world effectiveness and challenges.