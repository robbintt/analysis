---
ver: rpa2
title: 'Predicting Startup Success Using Large Language Models: A Novel In-Context
  Learning Approach'
arxiv_id: '2601.16568'
source_url: https://arxiv.org/abs/2601.16568
tags:
- startup
- success
- startups
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel few-shot learning framework called
  kNN-ICL for predicting startup success in data-scarce environments typical of venture
  capital settings. The approach leverages large language models (LLMs) through in-context
  learning, where predictions are made by providing the model with relevant examples
  rather than training it on large datasets.
---

# Predicting Startup Success Using Large Language Models: A Novel In-Context Learning Approach

## Quick Facts
- **arXiv ID:** 2601.16568
- **Source URL:** https://arxiv.org/abs/2601.16568
- **Authors:** Abdurahman Maarouf; Alket Bakiaj; Stefan Feuerriegel
- **Reference count:** 13
- **Primary result:** kNN-ICL achieves 71.3% balanced accuracy on startup success prediction, outperforming supervised ML (63.1%) and vanilla ICL (69.6%)

## Executive Summary
This paper introduces kNN-ICL, a few-shot learning framework that leverages large language models for predicting startup success in data-scarce environments typical of venture capital settings. The approach uses in-context learning combined with k-nearest-neighbor retrieval to select relevant historical examples, balancing both structured attributes and unstructured textual descriptions. When evaluated on 4,034 US startups from Crunchbase, kNN-ICL achieved a balanced accuracy of 71.3%, demonstrating the effectiveness of in-context learning as a decision-making tool for VC firms with limited labeled data.

## Method Summary
The kNN-ICL framework retrieves k nearest neighbor startups based on cosine similarity of fused embeddings (structured features + text embeddings) and prompts an LLM with these examples to predict success. The method uses a hybrid embedding approach with α=0.5 scaling, stratified retrieval of k/2 SUCCESS and k/2 FAILURE cases, and Gemini 2.0 Flash for prediction. The framework is designed for binary classification of startup outcomes (SUCCESS/FAILURE) based on structured features and textual self-descriptions, evaluated using 5-repeated stratified shuffle splits with balanced accuracy as the primary metric.

## Key Results
- kNN-ICL achieves 71.3% balanced accuracy, outperforming supervised ML baselines (63.1%) and vanilla in-context learning (69.6%)
- Performance remains robust across different numbers of examples, with strong results even using as few as 50 examples
- The approach demonstrates effectiveness in data-scarce environments typical of venture capital decision-making

## Why This Works (Mechanism)
The kNN-ICL approach works by leveraging the LLM's ability to perform reasoning over contextually relevant examples rather than requiring extensive training data. By retrieving startups most similar to the target case and presenting them in context, the model can identify patterns and features associated with success. The fusion of structured and textual embeddings captures both quantitative attributes and qualitative descriptions, providing a comprehensive representation that enhances retrieval quality and prediction accuracy.

## Foundational Learning
- **In-context learning**: LLM's ability to perform tasks based on provided examples without parameter updates; needed for few-shot scenarios with limited data; quick check: verify model can correctly classify with only example prompts
- **k-nearest neighbors retrieval**: Finding similar instances based on distance metrics; needed to select relevant historical examples; quick check: examine similarity scores and retrieved examples for quality
- **Text embedding models**: Converting text to numerical vectors for similarity comparison; needed to represent startup descriptions; quick check: visualize embedding spaces for semantic coherence
- **Stratified sampling**: Ensuring balanced representation of classes in retrieved examples; needed to prevent majority class bias; quick check: verify k/2 split between SUCCESS and FAILURE examples
- **Fused embeddings**: Combining structured and unstructured data representations; needed for comprehensive similarity measurement; quick check: test performance with only structured or only textual embeddings
- **LLM prompting**: Constructing effective prompts with examples and instructions; needed to elicit accurate predictions; quick check: verify prompt structure matches algorithm specifications

## Architecture Onboarding

**Component Map:**
- Crunchbase data extraction -> Data preprocessing -> Embedding generation -> Fused vector creation -> kNN retrieval -> Prompt construction -> LLM prediction -> Performance evaluation

**Critical Path:**
Data extraction → Embedding generation → kNN retrieval → Prompt construction → LLM prediction → Performance evaluation

**Design Tradeoffs:**
- Embedding model selection (embedding-001) balances quality and efficiency
- α=0.5 fusion weighting assumes equal importance of structured and textual data
- k=50 represents a balance between context richness and prompt complexity
- Gemini 2.0 Flash chosen for speed versus more capable but slower models

**Failure Signatures:**
- Vanilla ICL dominated by majority class (FAILURE) indicates retrieval quality issues
- Context window overflow with large k suggests prompt engineering problems
- Memorization via company names reveals insufficient data anonymization
- Performance degradation with fewer examples indicates sensitivity to example quality

**First 3 Experiments:**
1. Test retrieval quality by examining top-k most similar startups for representative cases
2. Evaluate prompt effectiveness by testing with varying k values (10, 50, 100)
3. Compare performance with and without structured feature fusion to isolate contribution

## Open Questions the Paper Calls Out
- **Open Question 1:** Can kNN-ICL maintain robust performance when applied to heterogeneous proprietary datasets containing unstructured documents like pitch decks or business plans? The study relies solely on public Crunchbase profiles; the impact of richer, private, multi-modal data on the retrieval mechanism's effectiveness is untested.
- **Open Question 2:** How does kNN-ICL performance change when predicting continuous outcomes or alternative definitions of success (e.g., sustainable long-term operation) rather than binary exit events? The current evaluation is restricted to a binary label within a fixed five-year horizon.
- **Open Question 3:** Does increasing the context window size and the number of retrieved examples (k) beyond 50 strictly improve prediction accuracy, or does noise dilute the signal regardless of model capacity? Current experiments were limited by the saturation point observed within the specific model's context constraints.

## Limitations
- Evaluation relies on specific time window (2013-2015 startups, 2016-2020 outcomes) and geographic scope (US-based companies only)
- Multiple LLM API calls per prediction could pose scalability challenges for production deployment
- The binary definition of success (acquisition/IPO) doesn't capture broader range of potential startup trajectories

## Confidence
- **High confidence**: The kNN-ICL approach outperforms both supervised ML baselines and vanilla ICL on the reported metrics, with robust performance across different k values
- **Medium confidence**: The claim that in-context learning serves as an effective decision-making tool for VC firms, given the limited evaluation to a single dataset and specific prediction task
- **Medium confidence**: The superiority of fused embeddings over either structured or textual embeddings alone, as the specific contribution of each component isn't fully isolated

## Next Checks
1. Test the kNN-ICL framework on startups from different geographic regions and time periods to assess temporal and geographic generalizability
2. Conduct a cost-benefit analysis comparing the multiple LLM API calls per prediction against potential financial returns from improved decision-making
3. Perform ablation studies to quantify the relative contributions of structured features versus textual descriptions to the overall performance