---
ver: rpa2
title: 'Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in Large
  Language Models through Latent Semantic Alignment'
arxiv_id: '2510.24208'
source_url: https://arxiv.org/abs/2510.24208
tags:
- layer
- teacher
- knowledge
- transfer
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of transferring knowledge across
  large language models (LLMs) of different scales, which is hindered by "neural incompatibility"
  - architectural and parametric differences between models. The authors propose a
  semantics-first approach called SEMALIGN that uses layer activations (hidden states)
  as the medium for knowledge transfer rather than raw parameters.
---

# Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in Large Language Models through Latent Semantic Alignment

## Quick Facts
- **arXiv ID:** 2510.24208
- **Source URL:** https://arxiv.org/abs/2510.24208
- **Reference count:** 18
- **Primary result:** Latent semantic alignment consistently outperforms parameter-space transfer baselines across four benchmarks, achieving smaller performance gaps to teacher models.

## Executive Summary
This paper addresses the challenge of transferring knowledge across large language models (LLMs) of different scales, which is hindered by "neural incompatibility" - architectural and parametric differences between models. The authors propose a semantics-first approach called SEMALIGN that uses layer activations (hidden states) as the medium for knowledge transfer rather than raw parameters. The method achieves more stable transfer with smaller performance gaps to the teacher model while requiring only a small set of paired layers to be updated during training.

## Method Summary
SEMALIGN addresses cross-scale knowledge transfer by substituting activations for parameters as the transfer medium. The method involves three key steps: (1) using layer attribution to identify critical layers in the teacher model and pairing them with corresponding layers in the student model, (2) decomposing the teacher's layer outputs into semantic components via vocabulary-defined bases and recomposing them in the student's latent space to create supervisory signals, and (3) optimizing the student's paired layers to match these supervisory hidden states using cosine alignment losses. The approach focuses on steering a small set of paired layers, leveraging residual connections to induce broader behavioral alignment without full-network finetuning.

## Key Results
- Consistently outperforms parameter-space transfer baselines (SEEKING and LATEN) across MMLU, GSM8K, HumanEval, and MBPP benchmarks
- Achieves smaller performance gaps to the teacher model (averaging 1.44 points vs 3.43-3.92 for baselines)
- Particularly effective when transferring from code-specialized teachers
- Demonstrates more stable transfer with less variability across runs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Layer activations transfer more stably across model scales than raw parameters because latent representations exhibit higher cross-model similarity than weight spaces.
- **Mechanism:** The method substitutes activations for parameters as the transfer medium, bypassing "neural incompatibility" (architectural and parametric differences). CKA analysis shows high layer-wise similarity along the diagonal between teacher and student outputs, suggesting representation spaces share structure even when parameter spaces do not.
- **Core assumption:** Teacher-student pairs from the same model family share semantically organized latent spaces despite dimension mismatches.
- **Evidence anchors:** [abstract] "Instead of directly using the layer parameters, our approach takes activations as the medium of layer-wise knowledge transfer." [Section 5.3] Figure 4 shows high similarities between layer outputs "especially along the main diagonal," indicating compatibility when using outputs rather than parameters. [corpus] The closely related "Neural Incompatibility" paper (FMR 0.57) frames parameter-space transfer as fundamentally limited across scales, motivating activation-based alternatives.
- **Break condition:** If teacher and student come from architecturally divergent families (not sharing vocabulary or embedding space), semantic basis alignment may fail.

### Mechanism 2
- **Claim:** Decomposing hidden states into semantic components via vocabulary-defined bases enables dimension-preserving transfer across different latent space sizes.
- **Mechanism:** The method computes semantic coefficients via cosine projection onto teacher semantic bases (pseudoinverse of the LM-head), then recomposes with the same coefficients in the student's basis. This preserves the semantic "direction" while adapting to the student's dimensionality.
- **Core assumption:** Vocabulary tokens index consistent semantic directions across scales within the same model family; the pseudoinverse approximation of embeddings-to-representations is stable enough for transfer.
- **Evidence anchors:** [Section 3.1] "For each label on the LM vocabulary, there is an associated representation in the latent space, termed as 'semantic basis.'" [Section 4.2] Equations (1) and (2) formalize: decompose via `a = S_T^T h_T / ||h_T||`, recompose via `h̃_S = S_S a`. [corpus] No direct external validation of vocabulary-defined semantics in transfer; this is a novel operationalization. Evidence is internal to the paper's empirical validation.
- **Break condition:** If semantic bases are not index-aligned across models (vocabulary differences), coefficient transfer becomes meaningless.

### Mechanism 3
- **Claim:** Updating only a small set of paired layers suffices for behavioral alignment, because representation steering propagates through residual connections.
- **Mechanism:** The training objective combines layer-level cosine alignment (paired layer output → supervisory target) with output-level cosine alignment (final logits → label direction). Only the paired student layer's parameters update; the rest of the network is frozen. The supervisory signal is treated as constant.
- **Core assumption:** Steering intermediate representations induces downstream behavioral change without requiring full-network finetuning.
- **Evidence anchors:** [Section 4.3] "We adapt the student by updating only the parameters of its paired layer k while freezing all others." [Section 5.2] "steering a small set of paired layers is enough to induce broader behavioral alignment." [corpus] No external replication of this specific "sparse steering" claim; relies on paper's ablation results.
- **Break condition:** If critical knowledge is distributed across many layers (not localized), single-layer steering may underfit the teacher's behavior.

## Foundational Learning

- **Concept: Layer Attribution (Gradient × Activation)**
  - Why needed here: Determines which teacher layers carry task-relevant signal for pairing. Without attribution, layer selection is arbitrary.
  - Quick check question: Given a teacher model and a batch of task data, can you compute a scalar importance score per layer?

- **Concept: Semantic Bases / Vocabulary-Defined Semantics**
  - Why needed here: These bases anchor the decomposition-recomposition pipeline. Understanding that vocabulary tokens map to directions in latent space is prerequisite for aligning semantics across scales.
  - Quick check question: For a model with vocabulary size V and hidden dimension D, what is the shape of the semantic basis matrix S?

- **Concept: Cosine Similarity as Training Objective**
  - Why needed here: The method uses cosine-only losses for both intermediate and output alignment. Understanding why scale-invariant matching is appropriate (vs. MSE or cross-entropy) clarifies the design.
  - Quick check question: Why might cosine alignment produce more "conservative" transfer than cross-entropy-based finetuning?

## Architecture Onboarding

- **Component map:**
  1. Layer Attribution Module: Computes Layer Gradient × Activation scores for teacher layers
  2. Pairing Logic: Depth-aware mapping assigns student layers to teacher layers; handles non-integer depth ratios via interpolation
  3. Semantic Basis Cache: Precomputed pseudoinverse of LM-head for teacher and student; one-time computation per model
  4. Decomposition-Recomposition Engine: Projects teacher hidden states onto teacher bases, transfers coefficients, recomposes in student space
  5. Training Loop: Dual cosine losses (layer + output); updates only paired layer parameters via LoRA (rank 16 in experiments)

- **Critical path:** Attribution → Pairing → Basis Computation → Decomposition → Recomposition → Supervisory Target → Cosine Training. Errors in basis alignment or pairing strategy propagate directly to target quality.

- **Design tradeoffs:**
  - Conservative vs. Aggressive: Cosine-only matching yields stable transfer but may cap gains compared to methods that overshoot the teacher (SEEKING on GSM8K)
  - Layer Granularity: Current design pairs at layer level; sub-layer or head-level pairing (noted in limitations) could improve precision but increases complexity
  - Data Efficiency: Method uses small alignment sets (32–128 samples in experiments), trading coverage for cost

- **Failure signatures:**
  - Large teacher-student gap persists despite alignment → check if vocabulary/index alignment holds; semantic bases may be misaligned
  - Instability during training → verify gradient flow through frozen layers; ensure supervisory target is detached
  - Overshoot on specific tasks → cosine loss may be too weak; consider hybrid objectives (mentioned in future work)

- **First 3 experiments:**
  1. Sanity Check: Run CKA similarity between teacher and student layer outputs on held-out data (replicate Figure 4) to verify representation compatibility before transfer
  2. Ablation on Pairing Strategy: Compare depth-aware mapping vs. attribution-only selection to quantify the contribution of each pairing signal
  3. Cross-Family Test: Attempt SemAlign between models with different vocabularies (e.g., Llama → Mistral) to probe the break condition of semantic basis alignment

## Open Questions the Paper Calls Out

- **Open Question 1:** Does finer-grained semantic alignment at sub-layer or attention-head level improve transfer fidelity compared to layer-level alignment? [explicit] "Future directions include: (1) extending semantic alignment to finer granularity (sub-layer, attention head)" - Why unresolved: Current SemAlign operates at layer-level only; granularity effects on semantic preservation and transfer quality remain unexplored. What evidence would resolve it: Ablation experiments comparing layer-level vs. sub-layer vs. head-level alignment on same benchmarks, measuring both task performance and representational similarity metrics (e.g., CKA).

- **Open Question 2:** How robust is SemAlign across model families with fundamentally different architectures (e.g., Transformer vs. Mamba, or different attention mechanisms)? [explicit] "scaling analyses across families with larger architectural gaps to stress-test robustness" - Why unresolved: Experiments only cover Llama 2 variants (same architecture, different scales); semantic bases rely on shared vocabulary indices which may not transfer across fundamentally different architectures. What evidence would resolve it: Cross-family transfer experiments (e.g., Llama→Mistral, Transformer→SSM) with analysis of whether vocabulary-index alignment remains valid.

- **Open Question 3:** Why do all PKT methods hit a ceiling far below code-specialized teachers, and can latent semantic alignment breach this ceiling? [inferred] from results showing transferred models reach ~20 on HumanEval while specialized teachers reach 47-57, despite SemAlign outperforming baselines - Why unresolved: The paper observes the gap but does not diagnose whether it stems from limited training data, objective conservatism, or irrecoverable information loss during dimensionality reduction. What evidence would resolve it: Controlled experiments varying alignment data scale, training duration, and comparing aggressive vs. conservative objective formulations with the same teacher-student pairs.

## Limitations
- Semantic bases misalignment risk when transferring between models from different families with different vocabularies/embeddings
- Layer attribution may miss distributed knowledge patterns if teacher representations are highly diffuse
- Conservative cosine-only training objective may limit performance gains compared to cross-entropy-based approaches

## Confidence

- **High confidence:** Layer attribution identifies task-relevant teacher layers; CKA analysis showing high representation similarity along diagonal; performance consistently beating parameter-space baselines across four benchmarks
- **Medium confidence:** Semantic decomposition-recomposition preserves task-relevant directions; single-layer steering induces behavioral alignment; cosine-only training objective is optimal for stable transfer
- **Low confidence:** Cross-family transfer viability (different vocabularies/embeddings); scalability to much larger scale gaps (e.g., 70B→7B); long-term stability of latent steering effects

## Next Checks

1. **Semantic Basis Alignment Test:** Compute average cosine similarity between corresponding vocabulary token bases (teacher vs. student) before transfer. If mean similarity <0.7, investigate vocabulary misalignment as failure point.
2. **Ablation on Pairing Strategy:** Run transfer with (a) depth-aware pairing only, (b) attribution-based pairing only, (c) random pairing. Quantify contribution of each pairing signal to final performance.
3. **Cross-Family Transfer Attempt:** Attempt SemAlign transfer from Llama-13B-Chat to Mistral-7B-Chat (different vocabularies/embeddings). Measure performance degradation and identify whether semantic basis misalignment is the primary failure mode.