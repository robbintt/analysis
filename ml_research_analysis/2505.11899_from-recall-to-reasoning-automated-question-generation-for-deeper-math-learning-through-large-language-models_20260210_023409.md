---
ver: rpa2
title: 'From Recall to Reasoning: Automated Question Generation for Deeper Math Learning
  through Large Language Models'
arxiv_id: '2505.11899'
source_url: https://arxiv.org/abs/2505.11899
tags:
- generation
- question
- level
- genai
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explored using generative AI for creating high-quality
  math practice problems across varying cognitive depths. Initial testing showed GenAI
  could produce relevant questions with basic support, but quality and correctness
  improved when more context was provided.
---

# From Recall to Reasoning: Automated Question Generation for Deeper Math Learning through Large Language Models

## Quick Facts
- **arXiv ID:** 2505.11899
- **Source URL:** https://arxiv.org/abs/2505.11899
- **Reference count:** 34
- **Primary result:** Framework combining retrieval-augmented generation with Webb's Depth of Knowledge taxonomy improved question quality and depth alignment across multiple LLMs, particularly for higher-order thinking skills.

## Executive Summary
This study addresses the challenge of generating high-quality math practice problems that target different cognitive depths. Through two studies, the researchers developed and evaluated QG-DOK, a framework that combines retrieval-augmented generation with Webb's Depth of Knowledge taxonomy to create questions at four cognitive levels (Recall, Skills/Concepts, Strategic Thinking, Extended Thinking). The system significantly outperformed baseline approaches, particularly for higher-order thinking skills, with GPT-4o showing the most substantial improvements in appropriateness scores. While challenges remain with mid-level cognitive complexity and mathematical notation handling, the framework successfully generated questions with improved relevance, appropriateness, and depth alignment.

## Method Summary
The researchers developed QG-DOK by integrating retrieval-augmented generation with Webb's Depth of Knowledge taxonomy. The system converts textbooks and notes into vector chunks using text-embedding-ada-002, retrieves semantically similar chunks for a given query, and generates questions at specified DOK levels. The framework uses a prompt template that includes DOK definitions, examples, math context, and reasoning requirements. The approach was evaluated using three LLMs (GPT-4o, DeepSeek-V3, Gemini-1.5-Pro) with LLM-as-a-judge metrics for relevance and alignment, plus PINC scores for lexical diversity.

## Key Results
- DOK+RAG consistently outperformed DOK-only across all three evaluated LLMs
- GPT-4o showed the most significant gains in appropriateness scores (Level 1 improved from 0.80 to 0.91)
- High PINC scores (average 0.92) indicated strong lexical diversity while maintaining relevance
- The framework showed particular effectiveness for higher-order thinking skills (DOK Levels 3 & 4)
- Mid-level cognitive complexity (DOK Level 2) remained challenging with inconsistent alignment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing comprehensive source context significantly increases the factual correctness of generated mathematical solutions
- **Mechanism:** As reference material volume increases, the LLM relies less on parametric memory (prone to hallucinations) and more on provided context for deriving answers
- **Core assumption:** The model can successfully parse and prioritize provided context over internal weights
- **Evidence anchors:** Section 3 shows correctness improved from 60% to 100% with comprehensive context

### Mechanism 2
- **Claim:** Integrating Webb's Depth of Knowledge (DOK) into prompt structure steers model toward specific cognitive complexities
- **Mechanism:** Explicit DOK definitions and examples condition the LLM to align with specific pedagogical goals rather than defaulting to simple recall
- **Core assumption:** The LLM has sufficient internal reasoning capability to distinguish and synthesize questions at requested complexity levels
- **Evidence anchors:** Section 4 shows DOK+RAG outperformed DOK-only particularly for higher-order thinking skills

### Mechanism 3
- **Claim:** Grounding generation in vector database (RAG) improves relevance and appropriateness by forcing semantic alignment
- **Mechanism:** The system retrieves semantically similar chunks to serve as contextual grounding, preventing the model from drifting to tangential topics
- **Core assumption:** Semantic similarity effectively correlates with pedagogical relevance and mathematical integrity
- **Evidence anchors:** Section 4 shows DOK+RAG improved both relevance and appropriateness scores

## Foundational Learning

- **Concept:** Webb's Depth of Knowledge (DOK)
  - **Why needed here:** Identifies Bloom's Taxonomy as insufficient for math because it focuses on "cognitive process" rather than "task complexity"
  - **Quick check question:** Can you explain the difference between a "Level 2: Skills and Concepts" question and a "Level 3: Strategic Thinking" question in the context of a calculus problem?

- **Concept:** Retrieval-Augmented Generation (RAG)
  - **Why needed here:** Pivots from simple context injection to RAG architecture to solve "hallucination" and relevance issues found in Study 1
  - **Quick check question:** In a naive RAG pipeline, if a user asks for an "Intermediate Value Theorem" problem, which step determines the specific textbook excerpt included in the prompt?

- **Concept:** Vector Embeddings & Semantic Search
  - **Why needed here:** Relies on text-embedding-ada-002 to transform math text into vectors for finding "relevant" problems
  - **Quick check question:** Why might a standard text embedding model struggle to differentiate between a theorem's statement and its application if they use similar keywords?

## Architecture Onboarding

- **Component map:** Input Layer (User Query) -> Retrieval Layer (text-embedding-ada-002 Encoder -> Vector Database -> Semantic Search) -> Processing Layer (Prompt Construction) -> Generation Layer (LLM) -> Evaluation Layer (G-Eval + PINC)

- **Critical path:** Prompt engineering is the critical path - the prompt must explicitly describe DOK levels and provide examples alongside retrieved content to achieve high appropriateness scores

- **Design tradeoffs:**
  - Bloom vs. DOK: Trades common Bloom's Taxonomy for Webb's DOK, which is better for math complexity but introduces alignment challenges at Level 2
  - Naive RAG vs. Context Window: Trades dumping comprehensive context (100% correctness but potential relevance drop) vs. targeted RAG retrieval (higher relevance/alignment)
  - Temperature: Study 1 used Temperature 0 for reproducibility, while Study 2 used defaults (consistency vs. creativity tradeoff)

- **Failure signatures:**
  - Mid-Level Collapse: System fails to distinguish between DOK Level 2 and 3, often defaulting to simpler or harder extremes
  - Mathematical Hallucination: Model may output incorrect LaTeX notation or logic despite RAG
  - PINC Score Drift: High PINC scores might mask semantic repetition

- **First 3 experiments:**
  1. Context Scaling Reproduction: Replicate Study 1 by providing Minimal vs. Comprehensive context to verify 60% -> 100% correctness claim
  2. DOK Alignment Test: Generate 10 questions for "Intermediate Value Theorem" at each DOK level (1-4) using prompt templates
  3. RAG vs. No-RAG Benchmark: Compare appropriateness scores of questions generated with RAG retrieval vs. DOK prompt instructions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can a step-by-step framework derived from human educator design processes improve the accuracy of AI-driven question generation?
- **Basis in paper:** [explicit] The authors state: "Future work could explore how educators design problems to derive a step-by-step framework for AI-driven question generation."
- **Why unresolved:** Current prompts relying on simple descriptions of Depth of Knowledge (DOK) levels may be insufficient for consistent cognitive depth alignment
- **What evidence would resolve it:** A comparative study evaluating output quality between description-based prompting and a structured framework modeled on educator cognitive processes

### Open Question 2
- **Question:** What prompt engineering or model adjustments are required to stabilize the generation of mid-level cognitive complexity (DOK Level 2) questions?
- **Basis in paper:** [explicit] The study notes that "depth alignment at Level 2 was inconsistent across the models, suggesting that LLMs struggle with categorizing mid-level cognitive complexity."
- **Why unresolved:** Models performed well at high and low levels but fluctuated significantly at the "Skills and Concepts" level
- **What evidence would resolve it:** Successful generation of DOK Level 2 questions with high alignment scores across multiple LLMs using refined prompt strategies

### Open Question 3
- **Question:** Do the QG-DOK framework's positive results generalize to settings with diverse reference materials and multiple human evaluators?
- **Basis in paper:** [inferred] The authors acknowledge the constraint of using a "single reference content" and a "single human evaluator"
- **Why unresolved:** The robustness of relevance and appropriateness scores is unknown outside of the specific single-source context tested
- **What evidence would resolve it:** Replication using diverse mathematical textbooks and inter-rater reliability analysis among a panel of evaluators

## Limitations

- The framework's effectiveness is contingent on the quality of retrieved context chunks, with fragmentation of mathematical notation during chunking potentially undermining effectiveness
- Persistent challenges with mid-level cognitive complexity (DOK Level 2) where the model shows inconsistent alignment between Skills/Concepts and Strategic Thinking
- The evaluation methodology using LLM-as-a-judge (G-Eval) introduces potential circularity, as the same models being tested for reasoning capabilities are also assessing output quality

## Confidence

- **High Confidence:** The mechanism linking comprehensive context provision to improved factual correctness (Mechanism 1) is well-supported by Study 1 results showing 60% â†’ 100% accuracy improvement
- **Medium Confidence:** The superiority of DOK+RAG over DOK-only for higher-order thinking skills (DOK Levels 3 & 4) is demonstrated, but mid-level complexity issues suggest the framework's limitations are not fully resolved
- **Medium Confidence:** The claim that explicit DOK definitions in prompts better steer cognitive complexity than generic instructions, though inconsistent Level 2 performance indicates the steering mechanism is imperfect

## Next Checks

1. **Mid-Level Complexity Diagnostic Test:** Generate 50 questions specifically targeting DOK Level 2 across different mathematical domains. Have domain experts blind-grade these questions to quantify the exact failure rate and identify whether the model consistently confuses Level 2 with Levels 1 or 3.

2. **Chunking Integrity Validation:** Implement the vector database with varying chunk sizes (256, 512, 1024 tokens) and measure the correlation between chunk integrity (preserved mathematical notation) and question quality scores to isolate whether fragmentation is a primary failure mode.

3. **Cross-Subject Generalization Test:** Apply the QG-DOK framework to a non-mathematical subject (e.g., physics or history) using the same prompt templates. Compare DOK alignment scores across subjects to determine if the framework's effectiveness is domain-specific or generalizable.