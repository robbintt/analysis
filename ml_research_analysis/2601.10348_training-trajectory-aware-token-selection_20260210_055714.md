---
ver: rpa2
title: Training-Trajectory-Aware Token Selection
arxiv_id: '2601.10348'
source_url: https://arxiv.org/abs/2601.10348
tags:
- tokens
- training
- token
- distillation
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of continual reasoning distillation
  in language models, where standard fine-tuning often leads to performance degradation.
  The authors identify a phenomenon called "Imitation Shock," where training accuracy
  drops sharply early in training and then recovers, despite monotonically decreasing
  loss.
---

# Training-Trajectory-Aware Token Selection

## Quick Facts
- **arXiv ID:** 2601.10348
- **Source URL:** https://arxiv.org/abs/2601.10348
- **Reference count:** 40
- **Primary result:** T3S enables smaller models to surpass teachers on reasoning tasks by mitigating "Imitation Shock" during continual distillation

## Executive Summary
This paper introduces Training-Trajectory-Aware Token Selection (T3S) to address "Imitation Shock" in continual reasoning distillation, where training accuracy drops sharply early in training despite monotonic loss decrease. The authors identify "Imitation-Anchor Tokens" that dominate early optimization and suppress learning of important reasoning tokens. T3S uses training trajectory signals to identify and mask these anchor tokens during optimization, enabling more effective learning of reasoning capabilities. The method achieves consistent gains across autoregressive and diffusion language models, with Qwen3-8B surpassing its teacher DeepSeek-R1 on competitive reasoning benchmarks using only hundreds of examples.

## Method Summary
T3S addresses continual reasoning distillation by identifying and masking "Imitation-Anchor Tokens" that dominate early training optimization. The method tracks training trajectories to detect tokens whose loss decreases monotonically while accuracy drops sharply - a phenomenon termed "Imitation Shock." By masking these anchor tokens during training, T3S clears the optimization path for other reasoning-relevant tokens to be learned effectively. The approach is applied to both autoregressive models and diffusion language models, with token selection thresholds and masking schedules as key hyperparameters.

## Key Results
- Qwen3-8B surpasses teacher DeepSeek-R1 on competitive reasoning benchmarks with only hundreds of examples
- Qwen3-32B approaches Qwen3-235B in performance using T3S training
- LLaDA-2.0-Mini diffusion model exceeds its autoregressive baseline, achieving state-of-the-art performance among 16B-scale no-think models

## Why This Works (Mechanism)
T3S works by identifying and mitigating the dominance of "Imitation-Anchor Tokens" that capture early optimization focus during continual distillation. These tokens cause a sharp accuracy drop (Imitation Shock) despite monotonic loss decrease, effectively suppressing the learning of other reasoning-relevant tokens. By masking these anchor tokens based on training trajectory signals, T3S redirects optimization effort toward more important reasoning tokens that would otherwise be overshadowed. This allows the model to learn the intended reasoning patterns more effectively, even with limited training data.

## Foundational Learning
- **Imitation Shock**: Sharp early accuracy drop during distillation despite monotonic loss decrease - needed to understand the core problem T3S solves; quick check: training curves showing accuracy drop vs loss trend
- **Continual Reasoning Distillation**: Process of transferring reasoning capabilities from teacher to student models - needed context for the application domain; quick check: standard distillation vs continual distillation comparison
- **Training Trajectory Analysis**: Monitoring loss and accuracy evolution over training steps - needed to identify anchor tokens; quick check: token-level trajectory plots
- **Token-Level Optimization**: Understanding how individual tokens contribute to overall model behavior - needed to explain anchor token dominance; quick check: token importance ranking methods
- **Diffusion Language Models**: Alternative to autoregressive generation using iterative denoising - needed to show T3S applicability beyond standard models; quick check: diffusion vs autoregressive training dynamics

## Architecture Onboarding

**Component Map:** Data -> Model (autoregressive/diffusion) -> Token Selection Module -> Masked Training -> Performance Metrics

**Critical Path:** Training trajectory monitoring → Anchor token identification → Token masking → Optimized reasoning learning → Performance improvement

**Design Tradeoffs:** T3S trades additional hyperparameter tuning (selection thresholds, masking schedules) for improved reasoning performance. The method adds computational overhead for trajectory monitoring but enables effective learning with fewer examples. It requires careful balance between masking aggressive enough to clear optimization path but not so aggressive as to lose important information.

**Failure Signatures:** If anchor tokens are not properly identified, Imitation Shock persists and reasoning performance degrades. Over-aggressive masking may remove important tokens, causing training instability or poor convergence. Incorrect threshold settings may fail to distinguish true anchor tokens from genuinely important ones.

**First Experiments:**
1. Replicate Imitation Shock phenomenon on standard autoregressive model with teacher-student setup
2. Apply T3S with varying token selection thresholds to verify anchor token masking effectiveness
3. Compare T3S-trained diffusion model against autoregressive baseline on same reasoning tasks

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Causal mechanism linking anchor token dominance to performance degradation remains descriptive rather than mechanistically proven
- Method introduces additional hyperparameters requiring careful tuning, with sensitivity not extensively explored across scales
- Effectiveness on non-reasoning tasks and standard language modeling not evaluated

## Confidence

**High confidence:** The empirical observation of Imitation Shock is well-documented with clear training curves showing sharp accuracy drop followed by recovery despite monotonic loss decrease.

**Medium confidence:** The causal explanation linking Imitation-Anchor Tokens to the shock phenomenon is compelling but relies on correlation analysis rather than mechanistically proven causation.

**Medium confidence:** The claim that T3S enables smaller models to surpass their teacher with limited data is supported by results, but generalization is limited by lack of detailed teacher-student relationship and dataset composition information.

## Next Checks
1. Conduct ablation studies comparing T3S with alternative token selection strategies (random masking, attention-based importance, gradient magnitude) to isolate the contribution of training-trajectory-aware selection.

2. Test T3S on non-reasoning tasks including standard language modeling, summarization, and classification to evaluate generalizability beyond mathematical and coding reasoning benchmarks.

3. Perform hyperparameter sensitivity analysis across different model scales (1B, 8B, 32B, 70B) to determine robustness of token selection thresholds and masking schedules, reporting variance in performance gains.