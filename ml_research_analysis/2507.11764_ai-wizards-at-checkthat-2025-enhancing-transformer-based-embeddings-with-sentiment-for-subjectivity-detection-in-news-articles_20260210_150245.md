---
ver: rpa2
title: 'AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with
  Sentiment for Subjectivity Detection in News Articles'
arxiv_id: '2507.11764'
source_url: https://arxiv.org/abs/2507.11764
tags:
- sentiment
- subjectivity
- arabic
- english
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents AI Wizards\u2019 participation in the CLEF\
  \ 2025 CheckThat! Lab Task 1: Subjectivity Detection in News Articles."
---

# AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles

## Quick Facts
- **arXiv ID**: 2507.11764
- **Source URL**: https://arxiv.org/abs/2507.11764
- **Reference count**: 21
- **Primary result**: Achieved 1st place for Greek (Macro F1 = 0.51) and overall top rankings in CLEF 2025 CheckThat! Lab Task 1

## Executive Summary
This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1, focusing on subjectivity detection in news articles across multilingual and zero-shot settings. The team enhanced transformer-based classifiers by integrating sentiment scores from a pre-trained multilingual sentiment model with sentence representations. Their approach significantly improved subjective F1 scores, particularly in high-resource languages like English and Italian, while maintaining strong performance in zero-shot scenarios for unseen languages like Greek. The system employed mDeBERTaV3-base, ModernBERT-base, and Llama3.2-1B models with decision threshold calibration to address class imbalance.

## Method Summary
The approach uses transformer-based classifiers (primarily mDeBERTaV3-base) with sentiment feature integration through late fusion. A pre-trained multilingual sentiment model (twitter-xlm-roberta-base-sentiment) processes each sentence independently, generating three probability scores (positive, neutral, negative) that are concatenated with the transformer's [CLS] token embedding before classification. The system addresses class imbalance through post-hoc decision threshold calibration on the development set, optimizing for Macro F1 rather than using the default 0.5 threshold. Fine-tuning employed AdamW optimizer with learning rate 1e-5, batch size 16, and 6 epochs, with cross-entropy loss using class weights.

## Key Results
- Sentiment feature integration significantly boosted subjective F1 scores, particularly English (0.4843→0.5279) and Italian (0.6291→0.6804)
- Threshold calibration improved Macro F1 across languages, with Arabic jumping from 0.5538 to 0.5805
- Achieved 1st place ranking for Greek in zero-shot evaluation (Macro F1 = 0.51)
- Multilingual fine-tuning enabled effective zero-shot generalization to unseen languages

## Why This Works (Mechanism)

### Mechanism 1: Sentiment Feature Integration
Explicit sentiment signals improve subjective sentence detection by providing affective cues that correlate with subjectivity. The three-class sentiment model (trained on Twitter) generates probability scores that are concatenated with the [CLS] embedding, enriching the semantic representation before classification. This works best in high-resource languages where the sentiment model's domain gap is less severe.

### Mechanism 2: Threshold Calibration for Class Imbalance
Standard 0.5 thresholds favor majority classes in imbalanced datasets. Grid search on the development set (0.1-0.9) finds optimal thresholds that maximize Macro F1, shifting the decision boundary to better capture subjective instances. This assumes dev/test set distributions are similar.

### Mechanism 3: Multilingual Zero-Shot Transfer
Fine-tuning mDeBERTaV3 on multiple languages creates a shared embedding space for subjectivity detection. When applied to unseen languages like Greek, the model generalizes through aligned representations learned from the training languages, assuming structural subjectivity markers transfer across languages.

## Foundational Learning

- **Concept: Transformer [CLS] Token Pooling** - Extracts fixed-size sentence representation from variable-length inputs. Quick check: How does the model aggregate token embeddings into a single classification vector?
- **Concept: Feature-level Fusion (Concatenation)** - Appends sentiment scores to semantic embeddings before classification. Quick check: Why concatenate sentiment rather than use attention mechanisms?
- **Concept: Class Imbalance & Threshold Moving** - Corrects bias against minority classes post-training. Quick check: Why does lowering the threshold from 0.5 improve Macro F1 even if accuracy drops?

## Architecture Onboarding

- **Component map**: Raw Sentence → mDeBERTaV3 (→ [CLS]) → Sentiment Model (→ 3 scores) → Concat → Feed Forward → Softmax → Calibrated Threshold → Binary Label
- **Critical path**: Synchronization of sentiment model outputs with transformer embeddings. Sentiment model is frozen/inference-only, requiring careful alignment of batch inputs and outputs.
- **Design tradeoffs**: Late fusion (concatenation) is computationally cheaper than joint training but prevents lower-layer attention to sentiment signals. Translation was abandoned for native multilingual processing to preserve cultural nuance.
- **Failure signatures**: Arabic consistently underperforms due to domain mismatch between Twitter sentiment data and news text. Multilingual models sometimes underperform monolingual ones due to language interference.
- **First 3 experiments**: 1) Baseline vs. sentiment-augmented training on English data. 2) Threshold sensitivity analysis across 0.1-0.9 range. 3) Zero-shot evaluation comparing mDeBERTaV3 vs. sentiment-augmented version on unseen languages.

## Open Questions the Paper Calls Out

- Does incorporating granular emotional features (e.g., irony, anger) improve subjectivity detection more than standard positive/negative sentiment scores? The current study used a basic three-class sentiment model, but exploring fine-grained emotion could provide stronger signals.
- Can attention-based fusion mechanisms outperform simple concatenation for integrating sentiment features? The paper suggests attention-based fusion could allow dynamic weighting of semantic content against sentiment signals.
- Do larger Large Language Models (7B+ parameters) significantly outperform fine-tuned BERT-like architectures? The tested 1B Llama model underperformed, but it's unclear if this was due to model scale or architectural limitations.

## Limitations

- Sentiment model domain transfer from Twitter to formal news text creates performance gaps, particularly for non-English languages like Arabic
- Threshold calibration assumes development set distribution matches test set, which may not hold for multilingual and zero-shot scenarios
- Freezing the sentiment model limits adaptation to the specific subjectivity detection task

## Confidence

- **Sentiment feature integration improves subjective F1**: High confidence (direct empirical evidence across multiple languages)
- **Threshold calibration recovers minority class performance**: High confidence (clear before/after comparisons showing substantial improvements)
- **Multilingual fine-tuning enables zero-shot generalization**: Medium confidence (achieved 1st place for Greek, but lacks ablation studies comparing against other approaches)

## Next Checks

1. **Cross-domain sentiment validation**: Train a sentiment model specifically on news articles from target languages and compare its effectiveness against the Twitter-based model to validate domain mismatch explanations.

2. **Threshold stability analysis**: Evaluate calibrated thresholds across multiple random train/dev/test splits to determine whether optimal thresholds are stable or overfit to specific data partitions.

3. **Joint vs. late fusion comparison**: Implement a version where the sentiment model is fine-tuned jointly with the transformer and compare performance against the late fusion approach to test whether sentiment adaptation improves results.