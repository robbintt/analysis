---
ver: rpa2
title: 'Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering
  in Taiwan'
arxiv_id: '2508.01274'
source_url: https://arxiv.org/abs/2508.01274
tags:
- arxiv
- zhang
- audio
- chinese
- traditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-TW, the first benchmark for evaluating
  any-to-any multimodal models on Traditional Chinese, combining image-text and audio-text
  tasks from authentic proficiency tests. The dataset includes 900 multiple-choice
  questions, equally split between vision-based and audio-based subsets.
---

# Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering in Taiwan

## Quick Facts
- arXiv ID: 2508.01274
- Source URL: https://arxiv.org/abs/2508.01274
- Authors: Jui-Ming Yao; Bing-Cheng Xie; Sheng-Wei Peng; Hao-Yuan Chen; He-Rong Zheng; Bing-Jia Tan; Peter Shaojui Wang; Shun-Feng Su
- Reference count: 40
- Primary result: First benchmark for any-to-any multimodal models on Traditional Chinese, showing closed-source models outperform open-source ones, but open-source models can excel in audio tasks with significant latency advantages for end-to-end architectures

## Executive Summary
Multi-TW introduces the first benchmark for evaluating any-to-any multimodal models on Traditional Chinese, featuring 900 multiple-choice questions from authentic proficiency tests. The dataset is equally split between vision-based and audio-based tasks, enabling comprehensive evaluation of model capabilities across different input modalities. Experiments reveal that while closed-source models generally achieve higher accuracy, open-source models demonstrate competitive performance in audio tasks and offer substantial latency advantages through end-to-end processing pipelines.

## Method Summary
The Multi-TW benchmark consists of 900 multiple-choice questions (450 image-text, 450 audio-text) sourced from SC-TOP proficiency tests. Images are provided at 150 dpi PNG format and audio files are 128 kbps MP3 with average duration of 107.5 seconds. Models are evaluated zero-shot using a fixed prompt in Traditional Chinese instructing single-letter output (A, B, C, or D). Exact match accuracy is measured for both overall and subset performance, with inference latency recorded across preprocessing and model inference stages. The evaluation compares end-to-end any-to-any models against cascaded VLM+ASR pipelines.

## Key Results
- Closed-source models generally outperform open-source models on both image-text and audio-text tasks
- Open-source models can excel in audio tasks, with Qwen2.5-Omni-7B achieving 89.11% accuracy on audio versus 41.56% on images
- End-to-end any-to-any pipelines demonstrate 2-3x latency advantages over VLM+ASR cascaded systems (467-744 seconds vs 1,187-2,131 seconds)
- Simplified Chinese pre-trained models (Qwen, Baichuan) transfer effectively to Traditional Chinese tasks, achieving competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
End-to-end any-to-any architectures provide inference latency advantages over cascaded systems for audio-text tasks by processing raw waveforms directly into semantic tokens, avoiding the serialization overhead and intermediate data transfer associated with running a separate ASR model before a Vision-Language Model.

### Mechanism 2
Simplified Chinese pre-training offers transferable linguistic competence for Traditional Chinese tasks because Simplified and Traditional Chinese share significant lexical and structural DNA, allowing models to leverage shared representation to achieve >60% accuracy on Traditional benchmarks.

### Mechanism 3
Open-source multimodal models exhibit performance asymmetry, often excelling in audio tasks relative to visual tasks compared to closed-source counterparts, likely due to more mature audio encoders in open-source any-to-any models.

## Foundational Learning

- **Concept: Any-to-Any Multimodal Architecture** - Models that natively process multiple input types (audio/image) in a single forward pass rather than requiring cascaded systems (ASR + LLM). Quick check: Does the model accept raw audio and image simultaneously as input tokens, or require text transcription first?

- **Concept: Cross-Script Transfer (Simplified vs. Traditional Chinese)** - Explains why models trained primarily on Simplified Chinese data can function on Traditional Chinese benchmarks. Quick check: If a model is trained only on ASCII text, would you expect it to perform well on Cyrillic tasks?

- **Concept: Inference Latency vs. Accuracy Trade-offs** - The paper uniquely benchmarks both speed and accuracy, showing architectural choices directly impact real-world applicability. Quick check: For a real-time translation app, would you prioritize highest accuracy model or lowest latency local pipeline?

## Architecture Onboarding

- **Component map:** Raw waveforms and pixel data → Audio/Vision Encoders → Projector → LLM Backbone → Output
- **Critical path:** The Audio/Vision Encoder → Projector → LLM Backbone path is the primary determinant of both latency and "any-to-any" capability
- **Design tradeoffs:**
  - Cascaded (VLM + ASR): Higher potential accuracy but significantly higher latency (~2-3x slower)
  - Unified (Any-to-Any): Lower latency and streamlined deployment but currently lower visual performance in open-source variants
- **Failure signatures:**
  - Language Mismatch: Model outputs garbage or echoes the prompt
  - Modality Drop-out: High audio scores but near-random visual scores
- **First 3 experiments:**
  1. Baseline Latency Test: Run 100 samples through unified model vs cascaded pipeline to replicate latency gap
  2. Cross-Script Ablation: Evaluate Simplified Chinese model on Multi-TW with and without character conversion preprocessing
  3. Modality Stress Test: Specifically evaluate "Image-Based Q&A" vs "Paragraph Comprehension" (Audio) subsets to identify visual encoder bottleneck

## Open Questions the Paper Calls Out

1. To what extent does dedicated Traditional Chinese fine-tuning close the performance gap between open-source and closed-source multimodal models?

2. How does cross-lingual transfer capability influence the performance of Simplified Chinese-pretrained models on Traditional Chinese reasoning tasks?

3. Do end-to-end any-to-any architectures retain their latency advantage over cascaded VLM+ASR pipelines under parallelized or streaming inference conditions?

4. How does performance on Multi-TW's multiple-choice tasks correlate with capabilities in generative tasks and complex reasoning?

## Limitations

- The Multi-TW benchmark derives from SC-TOP language proficiency tests, which may have unique linguistic and cultural features not representative of general Traditional Chinese understanding
- Critical generation parameters (temperature, top_p, beam search width) and exact API versions for closed-source models are not reported
- Results are based on a single run on specific hardware configuration without variance measures or multi-run statistics

## Confidence

- End-to-end any-to-any architectures provide latency advantages: High
- Simplified Chinese pre-training transfers to Traditional Chinese: High
- Open-source models excel in audio relative to visual tasks: Medium

## Next Checks

1. Run the unified and cascaded pipelines with varying generation parameters (temperature 0.0, 0.7, 1.0; top_p 0.9, 1.0) to determine if accuracy differences are parameter-dependent

2. Evaluate the same models on VisTW and TCC-Bench to assess whether Multi-TW performance correlates with general Traditional Chinese multimodal capability

3. Create a parallel evaluation where Traditional Chinese text is automatically converted to Simplified Chinese before model input to quantify the exact contribution of Simplified-to-Traditional transfer mechanism