---
ver: rpa2
title: Pay Less Attention to Function Words for Free Robustness of Vision-Language
  Models
arxiv_id: '2512.07222'
source_url: https://arxiv.org/abs/2512.07222
tags:
- fare
- tecoa
- defense
- words
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the trade-off between robustness and performance
  in vision-language models (VLMs) under adversarial attacks. It proposes Function-word
  De-Attention (FDA), which reduces the impact of function words by calculating and
  subtracting cross-attention between function words and images from the original
  attention.
---

# Pay Less Attention to Function Words for Free Robustness of Vision-Language Models

## Quick Facts
- **arXiv ID**: 2512.07222
- **Source URL**: https://arxiv.org/abs/2512.07222
- **Reference count**: 40
- **Primary result**: FDA reduces average attack success rate by 18–53% with minimal (<1%) performance loss

## Executive Summary
This paper addresses the trade-off between robustness and performance in vision-language models (VLMs) under adversarial attacks. The authors propose Function-word De-Attention (FDA), which reduces the impact of function words by calculating and subtracting cross-attention between function words and images from the original attention. This improves alignment and robustness. Extensive experiments on 3 models, 2 tasks, and 3 datasets under 6 attacks show FDA reduces average attack success rate by 18–53% with minimal (<1%) performance loss, outperforms SOTA baselines, and generalizes well across settings.

## Method Summary
The paper proposes Function-word De-Attention (FDA), which improves robustness of VLMs by reducing the impact of function words during cross-attention. The method calculates the cross-attention between function words and images, then subtracts this from the original attention weights. This de-attention mechanism reduces the model's sensitivity to function words that often contribute to vulnerability under adversarial attacks, while maintaining performance on clean data. FDA is compatible with existing robustness methods and can be integrated into various VLM architectures.

## Key Results
- FDA reduces average attack success rate by 18–53% across multiple models and attack types
- Performance degradation on clean data is minimal (<1%)
- FDA outperforms state-of-the-art robustness baselines
- Method generalizes well across different VLMs, tasks, and datasets

## Why This Works (Mechanism)
The paper identifies that function words (articles, prepositions, conjunctions) create vulnerabilities in VLMs by causing spurious correlations during cross-attention with visual features. These words often have weak semantic meaning but can be exploited by adversarial attacks to manipulate model predictions. By de-attending function words, FDA reduces the model's reliance on these potentially misleading alignments, forcing the model to focus on content words that carry more meaningful semantic information. This mechanism improves both alignment between vision and language representations and robustness against adversarial perturbations targeting these function word vulnerabilities.

## Foundational Learning

**Vision-Language Models (VLMs)**: Neural architectures that process both visual and textual inputs simultaneously. Why needed: Understanding the basic architecture is essential for grasping how cross-attention operates between modalities. Quick check: Can you explain how a VLM differs from separate vision and language models?

**Cross-attention**: Mechanism where one modality attends to another to create aligned representations. Why needed: FDA specifically modifies this mechanism by de-attending function words. Quick check: What is the difference between self-attention and cross-attention in VLMs?

**Adversarial attacks on VLMs**: Techniques that manipulate inputs to cause model failures. Why needed: FDA's effectiveness is measured against these attacks. Quick check: How do white-box attacks differ from black-box attacks in the context of VLMs?

## Architecture Onboarding

**Component map**: Input text -> Tokenizer -> Function word detector -> Cross-attention calculation -> FDA de-attention subtraction -> Modified attention weights -> Model forward pass -> Output

**Critical path**: Function word detection → Cross-attention calculation → FDA subtraction → Modified attention → Model inference

**Design tradeoffs**: FDA trades minimal performance degradation (<1%) for significant robustness gains (18-53% attack success rate reduction). The method adds computational overhead for the additional attention calculations but provides compatibility with existing robustness methods.

**Failure signatures**: If FDA over-suppresses function word attention, the model may lose grammatical context; if under-suppressed, adversarial vulnerability remains. The sweet spot balances semantic preservation with robustness.

**First experiments**: 1) Measure baseline attack success rates on clean models; 2) Apply FDA and measure performance drop on clean data; 3) Compare attack success rates with and without FDA under various attack types.

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead of additional cross-attention calculations is not explicitly quantified
- Primary evaluation focuses on white-box attacks, leaving black-box attack effectiveness less explored
- Limited theoretical analysis of why function words specifically contribute to vulnerability

## Confidence
- **High confidence** in empirical results showing effectiveness across tested settings
- **Medium confidence** in generalizability to broader attack scenarios and real-world applications
- **Low confidence** in complete theoretical understanding of function-word vulnerability mechanism

## Next Checks
1. Measure and report the computational overhead (inference time and memory usage) introduced by FDA across different model sizes
2. Evaluate FDA's performance against black-box attacks and adaptive attacks specifically designed to circumvent function-word de-attention
3. Conduct ablation studies to identify which specific function words contribute most to vulnerability and whether the method can be optimized by focusing on a subset rather than all function words