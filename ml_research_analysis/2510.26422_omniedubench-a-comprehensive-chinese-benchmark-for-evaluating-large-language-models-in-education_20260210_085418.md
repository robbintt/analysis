---
ver: rpa2
title: 'OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language
  Models in Education'
arxiv_id: '2510.26422'
source_url: https://arxiv.org/abs/2510.26422
tags:
- knowledge
- dimension
- chinese
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OmniEduBench is a comprehensive Chinese educational benchmark designed
  to evaluate large language models across both knowledge and cultivation dimensions.
  The benchmark consists of 24.602K high-quality question-answer pairs covering 61
  subjects and 11 common exam question types.
---

# OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education

## Quick Facts
- **arXiv ID**: 2510.26422
- **Source URL**: https://arxiv.org/abs/2510.26422
- **Reference count**: 14
- **Primary result**: Benchmark reveals current LLMs trail human-level performance by nearly 30% in cultivation tasks, with Gemini-2.5 Pro achieving highest knowledge dimension accuracy at 62.78% and QWQ leading cultivation at 70.27%.

## Executive Summary
OmniEduBench is a comprehensive Chinese educational benchmark designed to evaluate large language models across both knowledge and cultivation dimensions. The benchmark consists of 24.602K high-quality question-answer pairs covering 61 subjects and 11 common exam question types. Knowledge dimension includes 18.121K pairs across 41 subjects, while cultivation dimension includes 6.481K pairs across 20 subjects. Experiments on 11 mainstream LLMs show significant performance gaps, with current models trailing human-level performance by nearly 30% in cultivation tasks. Even on the high-difficulty OmniEduBench HARD subset, the best model achieved less than 50% accuracy.

## Method Summary
OmniEduBench was constructed through a three-stage filtering process: 927K initial Q&A pairs from public (21K), private (106K), and LLM-synthesized (800K) sources were first filtered using QWQ-32B to retain incorrectly answered items (430K), then filtered again using Qwen3-235B (50K remaining), and finally expert-verified by 50 annotators and 5 senior reviewers to produce 24.602K final pairs. The benchmark employs dual evaluation: exact match for choice-based questions and LLM-assisted scoring for open-ended responses using Gemini-2.5 Pro as default scoring model. Questions are organized into 11 types across 61 subjects, partitioned into knowledge (41 subjects) and cultivation (20 subjects) dimensions.

## Key Results
- Gemini-2.5 Pro achieved highest knowledge dimension accuracy at 62.78%, while QWQ performed best in cultivation dimension at 70.27%
- GPT-4o underperformed on knowledge dimension (24.17%) compared to Qwen3-8B (43.86%), indicating weak transfer from English-centric training to Chinese exam formats
- Even on the high-difficulty OmniEduBench HARD subset (bottom 26% by difficulty), the best model achieved less than 50% accuracy
- Scoring model choice significantly impacts results, with GPT-4o-as-scorer deflating all results vs. Gemini-2.5 Pro-as-scorer by 5-10%

## Why This Works (Mechanism)

### Mechanism 1: Dual-Dimensional Evaluation Separates Knowledge from Pedagogical Reasoning
- Claim: Distinguishing knowledge mastery from cultivation capabilities reveals performance gaps that single-metric benchmarks mask.
- Mechanism: The benchmark partitions evaluation into (1) knowledge dimension—41 subjects with 11 question types testing factual recall and procedural problem-solving—and (2) cultivation dimension—20 subjects assessing empathy, moral reasoning, pedagogical judgment, and emotional support. Models must demonstrate different cognitive profiles across each.
- Core assumption: Educational competence requires both domain expertise and context-sensitive interpersonal reasoning; these are partially independent capabilities.
- Evidence anchors:
  - [abstract] Gemini-2.5 Pro leads knowledge (62.78%) while QWQ leads cultivation (70.27%), showing different model architectures excel at each dimension.
  - [section 3.2] "In the cultivation dimension, models generally perform better than in the knowledge dimension, which may be due to the fact that the cultivation tasks mainly consist of multiple-choice questions, making them simpler."
  - [corpus] EduEval similarly adopts hierarchical cognitive evaluation for Chinese K-12, but OmniEduBench uniquely adds cultivation-specific competencies.
- Break condition: If cultivation tasks are systematically easier due to format (multiple-choice) rather than genuine reasoning differences, the dimension comparison loses validity.

### Mechanism 2: Sequential Model Filtering Concentrates Difficulty at Capability Boundaries
- Claim: Cascading filters using progressively stronger models isolates questions near the frontier of current LLM capability.
- Mechanism: From 927K collected Q&A pairs, apply QWQ-32B first—retain only questions it answers incorrectly (430K). Then apply Qwen3-235B with same strategy, yielding 50K. Expert verification further reduces to 24.602K. This removes "easy" items that inflate scores and items with ambiguous answers.
- Core assumption: Questions that stump strong models represent genuine difficulty rather than noise, ambiguity, or annotation errors.
- Evidence anchors:
  - [section 2.2] "We first evaluated all questions using QWQ32B, retaining only those that the model answered incorrectly. This initial filtering resulted in a subset of 430K Q&A pairs. These questions then underwent a second filtering stage... ultimately yielding the final set of 50K."
  - [section 3.3] On OmniEduBench HARD (bottom 26% by difficulty), "even the best-performing model, Gemini, achieving less than 50% accuracy."
  - [corpus] Related Chinese benchmarks (C-Eval, CMMLU) don't report comparable dual-model filtering; this is a distinguishing quality-control mechanism.
- Break condition: If filtering removes valid easy items that test foundational knowledge, the benchmark becomes overly punitive and less diagnostic for weaker models.

### Mechanism 3: Hybrid Data Sourcing Balances Coverage, Contamination Control, and Scarcity Mitigation
- Claim: Combining three data sources—public, private, and LLM-synthesized—addresses coverage gaps while reducing data contamination risk.
- Mechanism: (1) Manual collection from public educational sites covers K-12 and professional exams; (2) private internal exam papers reduce risk of pre-training leakage; (3) LLM-generated cultivation scenarios fill gaps where real data is scarce, with expert consultation ensuring pedagogical fidelity.
- Core assumption: Each source compensates for another's blind spots without introducing systematic bias that inflates or deflates model performance.
- Evidence anchors:
  - [section 2.2] "We collected a total of 927K question–answer (Q&A) pairs, including 21K from publicly available data, 106K from private data, and 800K generated by LLMs."
  - [section 2.2, Table 2] Expert validation across 5 dimensions shows inter-rater agreement 0.83–0.90, indicating quality control succeeded across source types.
  - [corpus] CLUE (translated from English GLUE) carries source-language biases; OmniEduBench's native construction avoids this.
- Break condition: If LLM-synthesized cultivation data encodes biases from the generator model, evaluation may measure generator-model alignment rather than genuine pedagogical capability.

## Foundational Learning

- **Concept: Knowledge vs. Cultivation Dimensions in Educational AI**
  - Why needed here: Most existing benchmarks conflate factual knowledge with pedagogical reasoning, making it impossible to diagnose whether a model fails due to missing facts or missing situational judgment.
  - Quick check question: If a model correctly solves a calculus problem but suggests an inappropriate punishment for a distressed student, which dimension fails?

- **Concept: Dual-Machine Filtering for Difficulty Calibration**
  - Why needed here: Standard benchmarks often include many easy items that inflate average scores; sequential filtering by strong models concentrates evaluation near the capability frontier.
  - Quick check question: Why filter with QWQ-32B then Qwen3-235B sequentially, rather than using only the stronger model once?

- **Concept: LLM-Assisted Scoring for Open-Ended Responses**
  - Why needed here: Questions with multiple semantically equivalent valid answers (e.g., short explanations, essays) cannot be scored by exact string match; an LLM judge evaluates semantic equivalence.
  - Quick check question: If the scoring model is weaker than the model being evaluated, what systematic error pattern emerges?

## Architecture Onboarding

- **Component map:**
  - Data Ingestion: Public sources (XuekeNet, ZujuanNet) + Private sources (internal exams) + LLM synthesis (expert-guided generation)
  - Processing Pipeline: MinerU (PDF/Doc→Markdown) → Metadata extraction (subject, grade, type, tags) → Deduplication & cleaning
  - Filtering: Stage 1 (QWQ-32B, retain incorrect) → Stage 2 (Qwen3-235B, retain incorrect) → Expert verification (50 annotators, 5 senior reviewers)
  - Evaluation: (a) Exact match for choice-based questions; (b) LLM-assisted scoring (default: Gemini-2.5 Pro) for open-ended responses
  - Benchmark Structure: Knowledge (18.121K, 41 subjects, 11 question types) + Cultivation (6.481K, 20 subjects, mostly multiple-choice) + HARD subset (9.172K, bottom 26%)

- **Critical path:**
  1. Set up MinerU for document-to-markdown conversion; validate extraction quality on sample files.
  2. Deploy QWQ-32B and Qwen3-235B inference endpoints; implement filtering logic that retains only incorrectly answered items.
  3. Build two-path evaluation: exact-match scorer for choice questions, LLM-assisted scorer for open-ended.
  4. Establish expert review protocol with 5 quality dimensions (overall quality, clarity, option perplexity, accuracy, cultivation value).

- **Design tradeoffs:**
  - Coverage vs. depth: 61 subjects with 24.6K samples enables broad evaluation but limits per-subject statistical power.
  - Open-source vs. closed-source scoring: Gemini-2.5 Pro scoring improves accuracy but introduces API dependency; weaker scorers (GPT-4o) systematically deflate results.
  - Question-type diversity vs. evaluation consistency: 11 types mirror real exams but complicate automated scoring pipelines.

- **Failure signatures:**
  - GPT-4o underperforms Qwen3-8B on knowledge (24.17% vs. 43.86%), suggesting English-centric training has weak transfer to Chinese exam formats.
  - Few-shot prompting yields minimal average gains; some models (Qwen2.5-72B) show degraded performance, indicating inadequate in-context learning alignment during instruction tuning.
  - Qwen2.5-72B performs worst on HARD subset, showing parameter scale alone does not guarantee difficulty robustness.
  - Scoring model choice shifts absolute accuracy: GPT-4o-as-scorer deflates all results vs. Gemini-2.5 Pro-as-scorer (e.g., QWQ drops from 53.87% to 49.26%).

- **First 3 experiments:**
  1. Reproduce the primary result: evaluate a target model (e.g., Qwen3-8B) on both dimensions to establish baseline knowledge vs. cultivation gaps.
  2. Ablate filtering: compare full benchmark vs. single-stage-filtered subset to quantify difficulty concentration effect on HARD subset scores.
  3. Scorer sensitivity analysis: run evaluation on open-ended questions using three different LLM-assisted scorers (GPT-4o, Claude-4 Sonnet, Gemini-2.5 Pro) and report variance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs improve generalization and performance on the OmniEduBench HARD subset?
- Basis in paper: [explicit] The authors state on Page 7 that "further research is needed to enhance LLMs’ ability to generalize and maintain high performance on hard subsets."
- Why unresolved: Even the best-performing models (e.g., Gemini) achieve less than 50% accuracy on this high-difficulty subset, indicating a failure in sophisticated reasoning.
- Evidence: New training strategies or architectures that consistently lift HARD subset accuracy above 60%.

### Open Question 2
- Question: How can variance in LLM-assisted scoring be mitigated to ensure stable evaluation of open-ended educational questions?
- Basis in paper: [inferred] Table 6 demonstrates that using different scoring models (e.g., GPT-4o vs. Qwen3) significantly alters the accuracy rankings, indicating evaluation instability.
- Why unresolved: Subjective "cultivation" tasks lack standardized ground truth, forcing reliance on potentially biased proxy models for grading.
- Evidence: A scoring framework that exhibits high inter-model consistency and strong correlation with human expert judgments across all question types.

### Open Question 3
- Question: What sophisticated methods can effectively integrate few-shot examples to improve performance, given that simply increasing shot count yields limited gains?
- Basis in paper: [explicit] Page 7 notes that limited average improvement from increasing shots "highlight[s] the need for more sophisticated methods to integrate few-shot examples effectively."
- Why unresolved: Standard in-context learning appears insufficient for the complex reasoning required in this benchmark, potentially due to instruction-tuning limitations.
- Evidence: A prompting or training strategy that yields statistically significant improvements over zero-shot baselines across diverse model architectures.

## Limitations
- Cultivation tasks are dominated by multiple-choice format, which may inflate scores relative to more demanding open-ended pedagogical reasoning
- Dual-machine filtering may exclude valid foundational questions that serve as diagnostic baselines for weaker models
- Reliance on LLM-assisted scoring introduces up to 10% accuracy variance depending on scoring model choice, potentially conflating model capability with scorer bias

## Confidence
- **High confidence**: Knowledge dimension performance gaps between models (Gemini-2.5 Pro leading at 62.78%) are robust, as these rely on exact-match scoring with minimal scorer-dependent variance
- **Medium confidence**: Cultivation dimension results (QWQ at 70.27%) are moderately reliable but sensitive to scoring-model choice and the format simplicity of multiple-choice questions
- **Low confidence**: The claim that "current models trail human-level performance by nearly 30% in cultivation tasks" lacks direct human-baseline data; this extrapolation is based on relative model performance patterns rather than empirical human evaluation

## Next Checks
1. Conduct human evaluation on a stratified sample of 500 questions (250 knowledge, 250 cultivation) to establish ground-truth baseline performance and validate the claimed 30% human-model gap
2. Replicate the dual-machine filtering process with a third strong model (e.g., Claude-4) to test sensitivity of the final 24.6K sample selection to filtering order and model choice
3. Systematically vary the LLM scoring model across all open-ended questions (using at least three different models) and report variance in final accuracy to quantify scorer-induced uncertainty in cultivation dimension results