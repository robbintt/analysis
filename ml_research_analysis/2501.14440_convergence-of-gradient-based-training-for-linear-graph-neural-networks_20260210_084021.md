---
ver: rpa2
title: Convergence of gradient based training for linear Graph Neural Networks
arxiv_id: '2501.14440'
source_url: https://arxiv.org/abs/2501.14440
tags:
- gradient
- graph
- matrix
- convergence
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence properties of gradient-based
  training methods for linear graph neural networks (GNNs). The authors prove that
  gradient flow training with mean squared loss converges exponentially to the global
  minimum, with the convergence rate explicitly depending on initial weights and the
  graph shift operator.
---

# Convergence of gradient based training for linear Graph Neural Networks

## Quick Facts
- **arXiv ID**: 2501.14440
- **Source URL**: https://arxiv.org/abs/2501.14440
- **Reference count**: 40
- **Primary result**: Gradient flow training of linear GNNs converges exponentially to global minimum without requiring positive singular margin assumption

## Executive Summary
This paper provides a rigorous analysis of gradient-based training for linear graph neural networks (GNNs), proving that gradient flow with mean squared loss converges exponentially to the global minimum. The key innovation is establishing convergence without the restrictive positive singular margin assumption required by prior work. The convergence rate depends explicitly on initialization conditions and the graph shift operator, with the authors validating their theoretical findings through experiments on synthetic graph datasets and real-world climate data. The work bridges theoretical optimization guarantees with practical considerations for training GNNs on graph-structured data.

## Method Summary
The paper analyzes linear GNNs with H hidden layers using mean squared loss. The model performs feature propagation through graph aggregation using a shift operator S, with weight matrices W_1 through W_{H+1}. The authors prove exponential convergence of gradient flow training to the global minimum when initialized with W_1(0) as zero matrix and other weights as full-rank matrices with sufficiently large minimum singular values. They extend this to gradient descent with appropriate step sizes and discuss conditions for minimizing total weight energy. The analysis applies to both full-rank and low-rank feature matrices, with convergence rate depending on the smallest non-zero singular value of the aggregated feature matrix.

## Key Results
- Gradient flow training converges exponentially to global minimum at rate exp(-(1/m)βσ²_small((XS^H)_I)T)
- Convergence rate explicitly depends on choice of graph shift operator through σ_small((XS^H)_I)
- Under balanced initialization, gradient flow converges to global minimum that minimizes total weight energy
- Gradient descent also converges to global minimum with step size η < min{r/(M√p(H+1)), 1/(Mp), 2m/(βσ²_small)}

## Why This Works (Mechanism)

### Mechanism 1: Exponential Convergence via Specific Initialization Strategy
- **Claim**: Gradient flow training of linear GNNs converges exponentially to global minimum with initialization W_1(0)=0 and W_ℓ(0) full-rank with large σ_min
- **Mechanism**: Initialization creates neighborhood without sub-optimal critical points; gradient flow stays within this neighborhood with exponential loss decay
- **Core assumption**: Non-increasing hidden dimensions (d₁ ≥ d₂ ≥ ... ≥ d_{H+1}) and σ_min(W_{H+1}(0)) sufficiently large
- **Evidence anchors**: Abstract proves exponential convergence; Theorem 3.1 provides explicit bound; Chatterjee (arXiv:2203.16462) provides foundational local optimization result
- **Break condition**: Insufficient σ_min(W_{H+1}(0)) or violated full-rank condition leads to saddle points or divergence

### Mechanism 2: Convergence Rate Modulation via Graph Shift Operator
- **Claim**: Convergence rate depends explicitly on graph shift operator S through σ_small((XS^H)_I)
- **Mechanism**: Different shift operators produce different spectral properties in aggregated feature matrix, directly controlling exponential decay rate
- **Core assumption**: (XS^H)_I has non-trivial non-zero singular values
- **Evidence anchors**: Abstract mentions validation on synthetic datasets; Section 6.1.1 Figure 2 shows σ_small varies across operators; limited corpus support for GNN-specific dependencies
- **Break condition**: Very small σ_small((XS^H)_I) in sparse graphs with adjacency aggregation causes arbitrarily slow convergence

### Mechanism 3: Energy Minimization via Balanced Initialization
- **Claim**: Balanced initialization (W_ℓW_ℓ^T = W_{ℓ+1}^T W_{ℓ+1}) leads gradient flow to minimal-energy global minimum
- **Mechanism**: Balancedness preserved by gradient flow dynamics, combined with orthogonality constraints, restricts flow to manifold of balanced solutions
- **Core assumption**: W(0) satisfies balanced condition and W₁(0)U has zero entries for columns j > rank((XS^H)_I)
- **Evidence anchors**: Abstract discusses minimizing total weights; Theorem 4.4 states convergence under balanced conditions; Arora et al. (ICLR 2019) established similar results for deep linear networks
- **Break condition**: Unbalanced initialization or violated orthogonality constraints converges to global minimum but not minimal energy

## Foundational Learning

- **Concept: Gradient Flow vs. Gradient Descent**
  - **Why needed here**: Paper proves results for gradient flow (continuous-time) and extends to gradient descent (discrete-time); understanding distinction is essential for applying theoretical guarantees
  - **Quick check question**: Why does gradient descent require step size condition η < min{r/(M√p(H+1)), 1/(Mp), 2m/(βσ²_small)} while gradient flow does not?

- **Concept: Singular Value Decomposition and Non-Zero Singular Values**
  - **Why needed here**: Convergence rate depends on σ_small(M) - smallest non-zero singular value - rather than σ_min; critical when (XS^H)_I is rank-deficient
  - **Quick check question**: For matrix M with rank k < min(rows, cols), which singular value appears in convergence rate: σ_{k+1}(M) or σ_k(M)?

- **Concept: Graph Shift Operators and Their Spectral Properties**
  - **Why needed here**: Convergence rate varies by orders of magnitude depending on whether S is adjacency, Laplacian, or PageRank-style operator; impacts practical implementation
  - **Quick check question**: According to Figure 2, which shift operator consistently yields highest σ_small((XS^H)_I) across all tested graph models?

## Architecture Onboarding

- **Component map**:
  ```
  Input: X ∈ ℝ^{d_x × n}, S ∈ ℝ^{n × n}, Y ∈ ℝ^{d_y × n̄}
  Layer 1: Z₁ = W₁X → X₁ = Z₁S
  Layer 2: Z₂ = W₂X₁ → X₂ = Z₂S
  ...
  Layer H: Z_H = W_H X_{H-1} → X_H = Z_H S
  Output: Ŷ = W_{H+1} X_H (on labeled nodes)
  Loss: L(W) = (1/m)‖Ŷ_I - Y‖²_F where m = n̄ × d_y
  ```

- **Critical path**:
  1. Initialize W₁(0) = 0, W_ℓ(0) full-rank diagonal (e.g., identity scaled by factor a) for ℓ ≥ 2
  2. Verify σ_min(W_{H+1}(0)) satisfies lower bound from Remark 3.3
  3. Compute σ_small((XS^H)_I) to estimate expected convergence rate
  4. For gradient descent: set step size η < 2m/(βσ²_small((XS^H)_I))

- **Design tradeoffs**:
  - **Depth H**: Deeper networks capture multi-hop dependencies but require exponentially larger σ_min(W_{H+1}(0)) for guaranteed convergence
  - **Shift operator S**: Unnormalized Laplacian gives faster convergence (higher σ_small) but may require smaller gradient descent step sizes; normalized Laplacian more robust to graph structure variations
  - **Hidden dimensions**: Non-increasing dimensions required; bottleneck layers may introduce spurious local minima

- **Failure signatures**:
  - **Slow/stalled convergence**: Check if σ_small((XS^H)_I) is near-zero - may indicate poor shift operator choice
  - **Divergence in gradient descent**: Step size η too large; reduce by factor of 2
  - **Convergence to high-energy solution**: Initialization was not balanced; add explicit regularization or re-initialize

- **First 3 experiments**:
  1. **Baseline convergence test**: On your graph, compute σ_small((XS^H)_I) for each candidate shift operator. Choose operator with largest σ_small and verify exponential loss decay matches theoretical rate.
  2. **Initialization scale sensitivity**: Vary diagonal scale factor 'a' for W_{H+1}(0) and plot convergence rate vs. a². Confirm threshold behavior predicted by Remark 3.3.
  3. **Label fraction robustness**: Vary labeled fraction n̄/n from 10% to 90%. Plot σ_small((XS^H)_I) and observed convergence time. Verify transition behavior around n̄ ≈ d_x.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can convergence guarantees and exponential rates be extended to linear GNN architectures with bottleneck layers (where hidden dimensions decrease and then increase)?
  - **Basis**: Paper explicitly states focus on GNNs without bottleneck layer to avoid sub-local minimum
  - **Why unresolved**: Current proofs likely rely on non-decreasing dimensions to maintain rank properties
  - **What evidence would resolve it**: Formal proof extending Theorem 3.1 to architectures with decreasing dimensions, or counter-example showing failure

- **Open Question 2**: Do derived convergence properties hold for GNNs with non-linear activation functions (e.g., ReLU) or different loss functions (e.g., cross-entropy for classification)?
  - **Basis**: Analysis strictly limited to linear GNNs with mean squared loss
  - **Why unresolved**: Linearity of transformation WXS enables singular value-based bounds; non-linearities break this chain
  - **What evidence would resolve it**: Theoretical analysis of gradient dynamics for ReLU-GNNs or cross-entropy loss

- **Open Question 3**: What are specific convergence guarantees for normalized gradient flows in linear GNNs, and can trade-off between numerical stability and convergence speed be theoretically quantified?
  - **Basis**: Remark 6.1 discusses normalized gradient flows but notes loss of exponential convergence guarantee
  - **Why unresolved**: Normalizing gradient alters dynamics required to apply specific differential inequality
  - **What evidence would resolve it**: Theoretical derivation of convergence rate for normalized gradient flows or empirical study quantifying slowdown

- **Open Question 4**: How does theoretical initialization requirement (W_1(0) as zero matrix, W_ℓ(0) as full rank) impact early training dynamics compared to standard random initialization schemes?
  - **Basis**: Paper proposes specific initialization to prove convergence but doesn't compare extensively with standard practices
  - **Why unresolved**: While initialized weights converge, efficiency and practical implications vs. standard initializations not extensively analyzed
  - **What evidence would resolve it**: Comparative experiments analyzing time to convergence and generalization performance

## Limitations
- Theoretical initialization requirement (W_1(0) as zero matrix, W_ℓ(0) as full rank) may not align with standard deep learning practices
- Extension to stochastic gradient descent remains unexplored, limiting direct application to modern training regimes
- Balanced initialization condition for energy minimization lacks empirical validation and clear implementation guidance

## Confidence
- **High confidence**: Exponential convergence of gradient flow to global minimum (Theorem 3.1) - supported by rigorous proof and validated experiments
- **Medium confidence**: Explicit dependence of convergence rate on σ_small((XS^H)_I) - theoretically proven but practical sensitivity not fully characterized
- **Low confidence**: Energy minimization via balanced initialization (Theorem 4.4) - theoretical guarantee exists but lacks empirical validation and clear implementation guidance

## Next Checks
1. **Initialization robustness test**: Systematically vary diagonal scale factor 'a' in W_{H+1}(0) initialization and measure threshold at which convergence transitions from slow to exponential, comparing with theoretical bound from Remark 3.3.

2. **Saddle point verification**: For deep networks (H > 2) with non-increasing hidden dimensions, deliberately construct initializations that violate full-rank condition and empirically verify whether gradient flow encounters spurious critical points.

3. **Graph structure sensitivity**: Create pathological graph instances (star graphs, bipartite graphs) where certain shift operators produce extremely small σ_small((XS^H)_I) and measure actual convergence behavior versus theoretical rate prediction.