---
ver: rpa2
title: 'ToolRM: Outcome Reward Models for Tool-Calling Large Language Models'
arxiv_id: '2509.11963'
source_url: https://arxiv.org/abs/2509.11963
tags:
- tool
- arxiv
- reward
- preprint
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FC-RewardBench, the first benchmark for evaluating
  reward models on tool-calling tasks, and shows that existing models struggle in
  this domain. It proposes ToolRM, a suite of outcome reward models trained on synthetic
  data generated from open-weight LLMs, achieving up to 25% improvement over baselines.
---

# ToolRM: Outcome Reward Models for Tool-Calling Large Language Models

## Quick Facts
- **arXiv ID:** 2509.11963
- **Source URL:** https://arxiv.org/abs/2509.11963
- **Reference count:** 40
- **Key outcome:** Introduces FC-RewardBench benchmark and ToolRM outcome reward models, achieving up to 25% improvement over baselines for tool-calling tasks.

## Executive Summary
ToolRM introduces outcome reward models specifically designed for tool-calling tasks, addressing a critical gap where existing reward models struggle with tool-use scenarios. The paper creates FC-RewardBench, the first benchmark for evaluating reward models on tool-calling, and trains ToolRM using synthetic data generated from 11 open-weight LLMs. Results show up to 25% improvement over baselines, with ToolRM enabling better inference-time selection via Best-of-n sampling, effective data filtering for fine-tuning, and ground-truth-free policy optimization using RL. The approach demonstrates state-of-the-art performance on both the new benchmark and downstream tasks.

## Method Summary
The paper proposes outcome reward models trained on synthetic data generated from diverse open-weight LLMs, using a Bradley-Terry pairwise preference formulation with reward centering regularization. Data is obfuscated to prevent memorization of tool names, and models are trained to score final tool call sequences. Three model sizes (1.5B, 7B, 14B) are evaluated on FC-RewardBench and applied to Best-of-n inference, data filtering, and GRPO policy optimization.

## Key Results
- ToolRM models achieve up to 25% average improvement in downstream task performance compared to general-purpose baselines
- Best-of-n sampling with ToolRM-14B improves 0.6B model performance from 39.5% to 64.4% on FC-RewardBench
- ToolRM-1.5B outperforms the 120B-parameter gpt-oss baseline on FC-RewardBench despite being 80× smaller
- Ground-truth-free RL using ToolRM matches performance of ground-truth-dependent rewards on BFCL-v3

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reward centering with Bradley-Terry preference modeling produces calibrated scalar scores that rank tool-call quality reliably across diverse error types.
- **Mechanism:** The paper trains ORMs using the Bradley-Terry formulation (Eq. 1-2) with an added regularization term (Eq. 3: ηE[(r(x,y+) + r(x,y-))²]) that zero-centers rewards. This prevents reward drift and ensures the model learns relative preferences rather than absolute scores, which is critical when incorrect calls vary from subtle parameter errors to complete function mismatches.
- **Core assumption:** Assumes pairwise preference signals are sufficient supervision for learning nuanced tool-call correctness, without requiring explicit error categorization.
- **Evidence anchors:**
  - [abstract] "These models consistently outperform general-purpose baselines, achieving up to 25% average improvement in downstream task performance"
  - [section 3.2] Describes the complete reward modeling formulation with centering coefficient η=0.01
  - [corpus] Weak/no direct corpus evidence for this specific regularization technique in tool-calling RMs
- **Break condition:** Would fail if error types in deployment diverge significantly from the training distribution (see Table 1 error breakdown), or if the preference pairs lack diversity (the paper uses 11 models to mitigate this).

### Mechanism 2
- **Claim:** Synthetic data generated from a diverse pool of open-weight models captures realistic failure modes that hand-crafted negatives miss.
- **Mechanism:** The paper prompts 11 models (0.5B-32B parameters, including Qwen2.5, Granite, Mistral, SmolLM2) to generate tool calls, comparing outputs against ground truth. Only incorrect generations are retained as y-. This captures "natural variability and error patterns of real-world models" including "subtle and complex failure modes that are difficult to anticipate or enumerate manually."
- **Core assumption:** Assumes model-generated errors are representative of deployment errors, and that diversity in model architectures/scales translates to diversity in error types.
- **Evidence anchors:**
  - [abstract] "data synthesized from permissively licensed, open-weight LLMs"
  - [section 3.3] "This procedure enables the collection of data that reflects the natural variability and error patterns of real-world models"
  - [corpus] Neighbor paper "Nemotron-Research-Tool-N1" similarly uses synthetic trajectories, suggesting broader validity of this approach
- **Break condition:** Would fail if deployed models have systematic error patterns not represented in the 11-model pool. The paper mitigates via obfuscation (Mechanism 3) but doesn't test against proprietary model errors.

### Mechanism 3
- **Claim:** Obfuscating function and parameter names forces the reward model to attend to structural and semantic features rather than surface-level memorization.
- **Mechanism:** Before prompting models for data generation, the paper replaces "function and parameter names with randomly generated strings" and "reorders the keys in the function schema." This prevents the RM from learning spurious correlations (e.g., "get_weather" → high reward) and instead requires reasoning about parameter types, values, and schema compliance.
- **Core assumption:** Assumes tool-calling correctness depends on structural/semantic properties (parameter types, values, required fields) rather than surface form; assumes obfuscation preserves these properties.
- **Evidence anchors:**
  - [section 4] "we follow Lin et al. (2024) and obfuscate the data samples to avoid the model regurgitating its training data"
  - [Appendix A.9] Ablation shows removing obfuscation drops accuracy from 81.88% to 68.25% (13.63-point reduction)
  - [corpus] Weak/no direct corpus evidence for obfuscation in tool-calling RMs
- **Break condition:** Would fail if downstream tools have naming conventions that carry semantic information the RM needs (e.g., "dangerous_operation"). The paper doesn't analyze this tradeoff.

## Foundational Learning

- **Concept: Outcome Reward Models (ORMs) vs. Process Reward Models (PRMs)**
  - **Why needed here:** ToolRM is explicitly an ORM (scores only final tool calls, not intermediate reasoning). Understanding this distinction is critical because the paper argues ORMs scale more reliably than PRMs (citing Lin et al. 2025) and are more practical for tool-calling where intermediate supervision is scarce.
  - **Quick check question:** Given a multi-turn tool-calling conversation, would an ORM score each turn independently or only the final tool call sequence? (Answer: Only the final sequence.)

- **Concept: Best-of-n sampling with reward models**
  - **Why needed here:** The paper's main evaluation uses Best-of-n: sample n tool calls, score each with ToolRM, select highest-scoring. This is the primary mechanism for inference-time scaling. Understanding the cost-accuracy tradeoff is essential for practical deployment.
  - **Quick check question:** If Best-of-32 with ToolRM-14B improves a 0.6B model from 39.5% to 64.4%, what is the inference cost multiplier? (Answer: 32× generation + 32× reward scoring.)

- **Concept: Group Relative Policy Optimization (GRPO) for tool learning**
  - **Why needed here:** The paper uses GRPO (Section 5.3.2) for policy optimization, comparing ToolRM rewards against ground-truth-dependent rewards. Understanding GRPO's group-relative advantage estimation is critical for replicating the RL experiments.
  - **Quick check question:** In GRPO, how does ToolRM replace ground-truth rewards? (Answer: ToolRM scores replace R_correctness in the reward formulation R = R_format + R_correctness, enabling ground-truth-free RL.)

## Architecture Onboarding

- **Component map:**
  Training Data Generation Pipeline → Reward Model Training → ToolRM Checkpoints → Inference Applications

- **Critical path:**
  1. Data obfuscation (must occur before model prompting, or RM memorizes names)
  2. Pairwise preference curation (one incorrect per query to prevent over-representation)
  3. Reward centering coefficient (η=0.01, ablation shows 0.1 hurts performance)

- **Design tradeoffs:**
  - **Model size vs. efficiency:** ToolRM-14B achieves best accuracy but ToolRM-1.5B "surpasses the gpt-oss-120B model" on FC-RewardBench—smaller may be sufficient for filtering applications.
  - **Best-of-n vs. model scaling:** For SLMs (0.6B-8B), Best-of-32 with ToolRM outperforms training larger models. For 32B+ models, gains diminish (~2 points).
  - **Ground-truth-free RL:** R_ToolRM matches R_ToolRL performance without requiring ground-truth labels, but only tested on BFCL-v3; generalization to other benchmarks unknown.

- **Failure signatures:**
  - **Irrelevance errors increase:** Best-of-n with ToolRM increases irrelevance errors (185→210 in Table 8)—the RM "tends to favor producing function call outputs even in cases where no valid call can appropriately satisfy the user query."
  - **Diminishing returns for large models:** 32B+ generators see only ~2-point gains (Table 2, Figure 2).
  - **Position bias in LLM-as-Judge baselines:** The paper randomizes candidate order to mitigate, but doesn't test if ToolRM has similar biases.

- **First 3 experiments:**
  1. **Reproduce FC-RewardBench evaluation:** Download FC-RewardBench (1,500 pairs), evaluate an existing RM (e.g., Skywork-Reward-Llama-3.1-8B-v0.2) using pairwise accuracy. Expected: 45-65% accuracy per Figure 1.
  2. **Ablate obfuscation:** Train ToolRM-1.5B on non-obfuscated data. Expected: ~68% vs. 82% on FC-RewardBench (per Appendix A.9). This validates the generalization mechanism.
  3. **Test Best-of-n on a new benchmark:** Apply ToolRM-14B to RoTBench (Table 3) with n=32 on a Qwen-8B generator. Expected: Clean@32 should match or exceed Union@32, demonstrating noise robustness.

## Open Questions the Paper Calls Out
None

## Limitations
- **Benchmark coverage gaps:** FC-RewardBench focuses on single-turn tool calls with only 6 function types, missing complex multi-turn reasoning and broader API diversity.
- **Generalization to unseen tools:** While obfuscation prevents memorization of specific tool names, the 6 benchmark functions may not represent the full diversity of real-world APIs.
- **Trade-off between relevance and correctness:** Best-of-n sampling with ToolRM increases irrelevance errors (185→210 in Table 8), indicating the model sometimes favors making any call over no call when none would be appropriate.

## Confidence
- **High confidence:** FC-RewardBench construction methodology, pairwise preference modeling formulation, and synthetic data generation pipeline are reproducible and well-documented.
- **Medium confidence:** The Best-of-n inference benefits are robust for small-to-medium models (0.6B-8B) but diminish for 32B+ models. Ground-truth-free GRPO results show promise but are only validated on BFCL-v3.
- **Low confidence:** Claims about ToolRM enabling new capabilities (like better filtering for fine-tuning) are demonstrated on a single dataset. The paper doesn't provide systematic evaluation of how well ToolRM generalizes across different tool ecosystems or domains.

## Next Checks
1. **Cross-benchmark validation:** Evaluate ToolRM on ToolBench and WildToolBench using the same Best-of-n protocol to test generalization beyond FC-RewardBench's 6 functions.
2. **Multi-turn reasoning assessment:** Design a synthetic benchmark testing ToolRM's ability to score sequences of tool calls in multi-step reasoning scenarios, measuring whether the model can capture temporal dependencies and plan coherence.
3. **Real-world deployment study:** Partner with an API provider to test ToolRM on their actual production tool set (not the 6 benchmark functions) and measure end-to-end task success rates, including cases where no tool call is the correct answer.