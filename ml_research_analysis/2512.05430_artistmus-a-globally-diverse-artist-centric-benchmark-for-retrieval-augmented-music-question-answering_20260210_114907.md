---
ver: rpa2
title: 'ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented
  Music Question Answering'
arxiv_id: '2512.05430'
source_url: https://arxiv.org/abs/2512.05430
tags:
- music
- retrieval
- factual
- muswikidb
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of improving music-related\
  \ question answering by introducing MusWikiDB, a music-specific knowledge base of\
  \ 3.2M passages from 144K Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions\
  \ on 500 globally diverse artists. The authors evaluate retrieval-augmented generation\
  \ (RAG) for music QA, showing that it substantially improves factual accuracy\u2014\
  open-source models gain up to +56.8 percentage points (e.g., Qwen3 8B: 35.0\u2192\
  91.8), approaching proprietary model performance."
---

# ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering

## Quick Facts
- **arXiv ID**: 2512.05430
- **Source URL**: https://arxiv.org/abs/2512.05430
- **Reference count**: 0
- **Primary result**: RAG improves open-source model accuracy by up to +56.8 percentage points in music QA

## Executive Summary
This paper introduces MusWikiDB, a 3.2M passage music-specific knowledge base, and ArtistMus, a benchmark of 1,000 questions on 500 globally diverse artists. The authors evaluate retrieval-augmented generation (RAG) for music QA, demonstrating that open-source models improve factual accuracy by up to +56.8 percentage points (Qwen3 8B: 35.0→91.8) when augmented with retrieval. MusWikiDB yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general Wikipedia corpus. The findings show that domain-specific knowledge bases effectively bridge the gap between open-source and proprietary systems in music QA while enhancing contextual reasoning through RAG-style fine-tuning.

## Method Summary
The authors created MusWikiDB by extracting and segmenting 144K music-related Wikipedia pages into 3.2M passages (256 tokens each with 10% overlap). They constructed ArtistMus with 1,000 questions across 500 globally diverse artists, covering factual recall and contextual reasoning. The RAG pipeline uses BM25 for first-stage retrieval, BGE reranker for passage re-scoring, and LLMs (Llama 3, Qwen3, GPT-4o, Gemini 2.5 Flash) for answer generation. RAG-style fine-tuning was performed on (context, question, answer) triples. The system was evaluated on both factual and contextual question types, with passage length optimization and retrieval method comparisons.

## Key Results
- Open-source models achieve up to +56.8 percentage points factual accuracy improvement with RAG (Qwen3 8B: 35.0→91.8)
- MusWikiDB yields approximately 6 percentage points higher accuracy and 40% faster retrieval than general Wikipedia corpus
- RAG-style fine-tuning improves contextual reasoning from 85.6% to 93.0% accuracy
- BM25 + reranker outperforms dense retrieval (Contriever) for entity-centric music queries

## Why This Works (Mechanism)

### Mechanism 1: External Knowledge Grounding for Factual Recall
- **Claim**: Retrieval-augmented generation substantially improves factual accuracy in music QA by providing entity-specific information absent from model parameters.
- **Mechanism**: Factual questions require precise entity recall (e.g., album names) that is sparse in pretraining data. RAG retrieves relevant passages containing these entities, allowing models to extract answers directly from context rather than parametric memory.
- **Core assumption**: Models possess adequate reasoning capabilities to extract answers from retrieved context, but lack sufficient music-specific knowledge in their parameters.
- **Evidence anchors**: Open-source models gain up to +56.8 percentage points factual improvement; similar domain-specific QA improvements shown in MedBioLM and MUST-RAG.
- **Break condition**: If retrieved passages lack the specific entity, or if the model cannot follow instructions to integrate retrieved information.

### Mechanism 2: Domain-Specific Corpus Density
- **Claim**: A music-specialized knowledge base yields higher retrieval accuracy and faster retrieval than a general-purpose corpus.
- **Mechanism**: Filtering to music-relevant content (0.36B tokens vs 2.1B general Wikipedia) increases the density of relevant passages and reduces the search space, improving both precision and retrieval speed.
- **Core assumption**: Music QA queries predominantly require music-specific knowledge rather than broad cross-domain information.
- **Evidence anchors**: MusWikiDB yields approximately 6 percentage points higher accuracy and 40% faster retrieval than general Wikipedia; Figure 4 shows direct comparison of accuracy and retrieval time.
- **Break condition**: If queries require cross-domain knowledge not present in the filtered corpus.

### Mechanism 3: RAG-Style Fine-Tuning for Context Integration
- **Claim**: Training on (context, question, answer) triples improves the model's ability to leverage retrieved evidence for both factual recall and contextual reasoning.
- **Mechanism**: Fine-tuning explicitly teaches models how to integrate supporting passages into their reasoning process, rather than relying solely on parametric memory or ad-hoc context integration.
- **Core assumption**: Models can learn better context-integration strategies through supervised training on context-augmented examples.
- **Evidence anchors**: RAG-style fine-tuning achieves 93.0% contextual accuracy vs 85.6% with RAG inference alone (+7.4 pp); similar strategies explored in MedBioLM.
- **Break condition**: If fine-tuning data distribution doesn't match inference-time retrieval contexts, or if the model overfits to training passages.

## Foundational Learning

- **Concept**: Retrieval-Augmented Generation (RAG)
  - Why needed: Core paradigm for the entire system; understanding the distinction between parametric knowledge and external retrieval is essential.
  - Quick check question: Explain why RAG might be preferred over fine-tuning when knowledge needs frequent updates (e.g., new album releases).

- **Concept**: Sparse vs Dense Retrieval
  - Why needed: The paper compares BM25 (sparse/lexical) with Contriever (dense/embedding-based) retrieval; understanding this distinction informs retriever selection.
  - Quick check question: Why might BM25 outperform dense retrieval for entity-centric queries like artist names?

- **Concept**: Two-Stage Retrieval with Reranking
  - Why needed: The paper shows reranking provides +6.8 pp factual and +5.6 pp contextual improvements over first-stage retrieval alone.
  - Quick check question: What does a reranker do that the initial retriever cannot?

## Architecture Onboarding

- **Component map**: Question -> BM25 Retriever -> BGE Reranker -> LLM Backbone
- **Critical path**:
  1. Question → BM25 retrieves top-k passages (k=4 for 256-token passages, fixed 1024 token budget)
  2. Retrieved passages → BGE reranker scores and reorders
  3. Top passages + question → LLM generates answer (temperature=0)
- **Design tradeoffs**:
  - Passage size: 256 tokens optimal; shorter passages improve contextual accuracy, longer passages stabilize factual accuracy
  - Retrieval method: BM25 + reranker outperforms dense retrieval (Contriever) for entity-centric music queries
  - Corpus scope: Domain-specific MusWikiDB trades breadth for precision and speed vs general Wikipedia
- **Failure signatures**:
  - Instruction-following failures: ChatMusician achieved only 26.8% (near random) due to poor instruction adherence when integrating retrieved context
  - QA fine-tuning without context: Dropped contextual accuracy from 84.8% to 75.6%, suggesting overfitting to training answers without learning to reason
- **First 3 experiments**:
  1. Establish zero-shot baseline on ArtistMus for your chosen model (expect 30–40% factual, 75–85% contextual for 8B models)
  2. Add BM25 retrieval from MusWikiDB (expect +40–50 pp factual improvement)
  3. Add BGE reranker (expect additional +5–8 pp on both factual and contextual metrics)

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark Representativeness: The 1,000 questions on 500 artists may not fully capture the breadth of music-related knowledge, particularly for niche genres or contemporary artists with limited Wikipedia coverage.
- Retrieval Dependency: Substantial performance gains depend on the quality and coverage of MusWikiDB, creating potential blind spots for emerging artists or events.
- Model-Specific Generalization: Evaluation focuses on specific model families; results may not generalize to other architectures or smaller models not tested.

## Confidence
- **High Confidence**: The fundamental claim that RAG improves factual accuracy in music QA is strongly supported by consistent results across multiple models and comparison baselines.
- **Medium Confidence**: The claim that MusWikiDB provides 6 percentage points higher accuracy and 40% faster retrieval than general Wikipedia is supported by direct comparisons.
- **Medium Confidence**: The effectiveness of RAG-style fine-tuning is demonstrated through controlled experiments but requires further validation for generalization.

## Next Checks
1. **Cross-Domain Transfer**: Evaluate the same RAG pipeline on a non-music domain (e.g., film, literature) using domain-specific knowledge bases to test generalizability of observed performance gains.
2. **Temporal Robustness**: Test system performance on recent music releases and events not present in the MusWikiDB corpus to quantify degradation when external knowledge is unavailable.
3. **Model Architecture Sensitivity**: Compare performance across a broader range of model architectures to identify whether observed gains are architecture-dependent or represent general RAG benefits.