---
ver: rpa2
title: 'SlangDIT: Benchmarking LLMs in Interpretative Slang Translation'
arxiv_id: '2505.14181'
source_url: https://arxiv.org/abs/2505.14181
tags:
- slang
- translation
- sentence
- term
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of translating slang terms in
  context by introducing a new task called interpretative slang translation (SlangDIT),
  which involves detecting slang, providing cross-lingual explanations, and translating
  sentences accordingly. To support this task, the authors construct the SlangDIT
  dataset containing over 25k English-Chinese sentence pairs, with 7,818 labeled as
  polysemous slang terms.
---

# SlangDIT: Benchmarking LLMs in Interpretative Slang Translation

## Quick Facts
- arXiv ID: 2505.14181
- Source URL: https://arxiv.org/abs/2505.14181
- Reference count: 40
- Primary result: SlangOWL achieves 86.47% F1 on slang detection and significantly outperforms vanilla and SFT models in slang translation

## Executive Summary
This paper introduces SlangDIT, a new task and dataset for translating slang terms in context by combining slang detection, cross-lingual explanation, and translation. The authors construct a dataset of 25k+ English-Chinese sentence pairs with 7,818 labeled polysemous slang terms and propose SlangOWL, a deep thinking model that uses sequential task decomposition and chain-of-thought reasoning. Experiments demonstrate that SlangOWL significantly outperforms vanilla and supervised fine-tuned models across slang detection, explanation generation, and translation accuracy, validating the effectiveness of deep reasoning for handling polysemous slang.

## Method Summary
The approach involves training a deep thinking model (SlangOWL) that decomposes slang translation into three sequential subtasks: slang detection, cross-lingual explanation generation, and translation. The model is trained on reasoning traces distilled from DeepSeek-R1-Distill-Qwen-32B, which provides explicit intermediate thinking steps. The training pipeline uses LlamaFactory with DeepSpeed ZeRO-3, fine-tuning models of varying sizes (7B-14B) for three epochs with a learning rate of 1e-5. The dataset is constructed from movie subtitles filtered through multiple LLM stages and validated with GPT-4o for polysemy annotation.

## Key Results
- SlangOWL-14B achieves 86.47% F1 score on slang detection versus 78.21% for SFT-14B
- On polysemous slang (hard test), SlangOWL-14B reaches 89.60% F1 score
- Correct slang understanding yields 88.41% GoodT translations versus 18.63% when understanding is wrong
- SlangOWL-14B achieves 53.72 ROUGE-L score for explanations versus 26.26 for SFT-14B

## Why This Works (Mechanism)

### Mechanism 1: Sequential Task Decomposition with Prerequisite Dependencies
- Breaking slang translation into detection → explanation → translation improves final output quality
- The model learns explicit intermediate representations before generating translation, rather than direct mapping
- Core assumption: Detection and explanation quality directly causally impacts translation accuracy
- Evidence anchors: Abstract claim about accuracy improvement, Table 5 showing CSU vs WSU performance gap, limited external validation

### Mechanism 2: Deep Thinking via Long Chain-of-Thought Distillation
- Training on explicit multi-step reasoning traces improves handling of polysemous slang compared to direct SFT
- The model learns to generate internal "thought" tokens that analyze slang origin, meanings, and context
- Core assumption: Reasoning traces from DeepSeek-R1-Distill-Qwen-32B transfer effectively to smaller models
- Evidence anchors: Section 4 methodology, Table 3 performance comparison, consistent with broader chain-of-thought literature

### Mechanism 3: Polysemy-Aware Context Binding
- Explicitly labeling and reasoning about polysemous slang enables context-appropriate sense selection
- Hard test set contains polysemous terms (e.g., "Annie Oakley" as name vs. "sharpshooter" slang)
- Core assumption: Polysemy annotations are sufficiently accurate and training covers test variations
- Evidence anchors: Section 3.2 annotation methodology, Table 4 performance on polysemous terms, internal validation only

## Foundational Learning

- **Concept: Chain-of-Thought Reasoning in LLMs**
  - Why needed here: SlangOWL's "deep thinking" builds on o1-style reasoning; understanding why explicit intermediate steps improve output is essential
  - Quick check question: Can you explain why step-by-step reasoning might help disambiguate "carried a torch" (romantic vs. literal vs. ideological)?

- **Concept: Knowledge Distillation from Larger to Smaller Models**
  - Why needed here: Training data comes from DeepSeek-R1-Distill-Qwen-32B, deployed models are 7B-14B
  - Quick check question: What information might be lost when distilling reasoning traces from 32B to 7B models?

- **Concept: Multi-Task Learning with Sequential Dependencies**
  - Why needed here: Detection → explanation → translation pipeline requires joint optimization; errors propagate forward
  - Quick check question: If detection F1 is 60% and explanation ROUGE-L is 40%, what is the theoretical upper bound on translation quality?

## Architecture Onboarding

- **Component map:** Backbone LLMs (Llama-3.1-8B, Qwen2.5-7B, Qwen2.5-14B) → DeepSeek-R1-Distill-Qwen-32B (teacher) → DeepSpeed ZeRO-3 training → vLLM inference

- **Critical path:** 1) Generate reasoning traces with DeepSeek-R1-Distill-Qwen-32B using structured prompt 2) Fine-tune backbone LLMs on thought+output pairs for 3 epochs with lr=1e-5 3) Inference with vLLM using sampling (temp=0.1, repetition_penalty=1.05)

- **Design tradeoffs:** Distillation quality vs. cost (DeepSeek-R1-Distill-Qwen-32B traces are expensive but necessary); polysemy coverage (15% held out for hard test reducing training diversity); consensus filtering (two models improve precision but reduce recall)

- **Failure signatures:** Model outputs "No slang" for obvious slang (detection threshold too conservative); explanation is fluent but translation is literal (reasoning-output coupling is weak); polysemous slang always gets same translation regardless of context (polysemy reasoning not learned)

- **First 3 experiments:** 1) Reproduce SlangOWL-7B baseline on general test (verify F1 > 80%, ROUGE-L > 50%, BLEU > 23) 2) Ablate reasoning traces (train SFT-7B without thought data, expect ~8-10% F1 drop) 3) Test on 50 held-out polysemous examples from external source (assess generalization beyond movie subtitle domain)

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset relies heavily on GPT-4o for polysemy annotation without systematic human validation, raising concerns about annotation quality
- Claims of "state-of-the-art" performance lack external benchmarks for comparison in slang translation with polysemy handling
- The sequential task decomposition approach may not generalize well to slang domains beyond movie subtitles

## Confidence
- **High Confidence:** Sequential task decomposition demonstrably improves performance over vanilla models with strong ablation study evidence
- **Medium Confidence:** Deep thinking specifically helps with polysemy disambiguation, though mechanism isn't fully explained (counter-intuitive hard test performance)
- **Low Confidence:** Dataset quality and annotation process are insufficiently validated for claims about handling real-world slang complexity

## Next Checks
1. **Human Validation Study:** Have bilingual human experts evaluate 100 randomly selected translations from test set, comparing all model outputs and calculating inter-annotator agreement with Cohen's kappa

2. **Cross-Domain Generalization Test:** Evaluate SlangOWL on 500 slang examples from three domains not in training data (social media, technical documentation, spoken dialogue) and compare performance drop relative to movie subtitle domain

3. **Ablation of Intermediate Steps:** Create variants removing specific reasoning components (polysemy judgment, contextual meaning analysis) while keeping framework to measure additive performance degradation