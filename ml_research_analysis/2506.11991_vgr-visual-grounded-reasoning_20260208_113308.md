---
ver: rpa2
title: 'VGR: Visual Grounded Reasoning'
arxiv_id: '2506.11991'
source_url: https://arxiv.org/abs/2506.11991
tags:
- reasoning
- answer
- data
- image
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VGR, a novel multimodal reasoning framework
  that overcomes language bias in vision-language models by enabling autonomous visual
  grounding during reasoning. Unlike prior approaches that rely on text-only reasoning
  chains, VGR incorporates a selective feature replay mechanism allowing the model
  to retrieve and integrate relevant image regions on-demand via bounding box signals.
---

# VGR: Visual Grounded Reasoning

## Quick Facts
- arXiv ID: 2506.11991
- Source URL: https://arxiv.org/abs/2506.11991
- Authors: Jiacong Wang; Zijian Kang; Haochen Wang; Haiyong Jiang; Jiawen Li; Bohong Wu; Ya Wang; Jiao Ran; Xiao Liang; Chao Feng; Jun Xiao
- Reference count: 40
- Key outcome: VGR achieves +4.1 MMStar, +7.1 AI2D, +12.9 ChartQA using only 30% image tokens vs LLaVA-NeXT-7B baseline

## Executive Summary
VGR introduces a novel multimodal reasoning framework that overcomes language bias by enabling autonomous visual grounding during reasoning. Unlike prior approaches relying on text-only reasoning chains, VGR incorporates a selective feature replay mechanism allowing the model to retrieve and integrate relevant image regions on-demand via bounding box signals. To train this capability, the authors construct VGR-SFT, a large-scale dataset containing visual reasoning data with explicit region references. Experiments demonstrate that VGR achieves superior performance on multi-modal benchmarks while using significantly fewer image tokens.

## Method Summary
VGR is a multimodal reasoning framework that extends LLaVA architecture with selective visual grounding. The method uses AnyRes cropping to divide images into up to 16 patches, applies differential pooling (2×2 for snapshot/replay, 4×4 for local crops), and stores features in a unified feature map. During reasoning, the model outputs bounding box coordinates in the format `<sot>[x1,y1,x2,y2]<eot>`, which triggers retrieval of corresponding region features that are replayed to the LLM. The framework is trained via supervised fine-tuning on VGR-SFT, a dataset containing 158K samples with visual reasoning data and explicit region references. A detection head provides continuous spatial regression for bounding box coordinates.

## Key Results
- MMStar: +4.1 improvement over LLaVA-NeXT-7B baseline
- AI2D: +7.1 improvement over baseline
- ChartQA: +12.9 improvement over baseline
- Token efficiency: Uses only 30% of image tokens compared to baseline
- Detection loss ablation: MMStar 39.8→41.7, ChartQA 65.5→67.7, DocVQA 72.8→73.7

## Why This Works (Mechanism)

### Mechanism 1: Selective Feature Replay Reduces Token Redundancy
VGR stores visual features from high-resolution crops in a pooled feature map. During reasoning, when the model outputs `<sot>[x1,y1,x2,y2]<eot>`, the parser extracts the corresponding region features and feeds them back into the LLM. This allows the model to attend to arbitrary regions without pre-encoding all crops at full resolution, achieving 70% token reduction while expanding supported resolutions by 5×.

### Mechanism 2: Detection Loss Enables Continuous Spatial Regression
A small MLP detection head maps the `<eot>` token's hidden state to a 4-dimensional box (center, width, height). The detection loss combines L1 loss and GIoU loss with β=2, trained alongside cross-entropy on text tokens. This provides more precise localization than discrete token prediction, with ablation showing +2.2 MMStar and +2.2 ChartQA improvements.

### Mechanism 3: Grounding-Reasoning Interdependence
VGR-SFT data requires models to reference regions before describing their content. Ablations show removing either grounding or reasoning degrades performance, suggesting they reinforce each other. The mixed grounding-reasoning data transfers better than text-only CoT or grounding-only approaches, with full VGR achieving MMStar 41.7 vs 39.7 (w/o Grounding) and 39.3 (w/o Reasoning).

## Foundational Learning

- **AnyRes (Dynamic Resolution) Image Encoding**: VGR extends LLaVA's AnyRes from 4 to 16 crops and introduces differential pooling. Understanding how patches are divided, encoded, and pooled is essential for debugging feature retrieval.
  - Quick check: Given an image of 1008×672 pixels with p=336, how many 336×336 patches are generated, and which pooling factor applies to each type?

- **Bounding Box Parameterization (Center-Size Format)**: The detection loss uses (xc, yc, w, h) normalized to [0,1], while the replay signal uses corner coordinates (x1, y1, x2, y2). Understanding the conversion is necessary for implementing the parser.
  - Quick check: Convert normalized center-size box (0.5, 0.5, 0.2, 0.4) to corner format (x1, y1, x2, y2).

- **Supervised Fine-Tuning with Mixed Modalities**: VGR trains on image tokens, text tokens, and replayed region tokens simultaneously, but only text tokens contribute to cross-entropy loss. Understanding loss masking is critical for correct implementation.
  - Quick check: In a training sequence containing [image tokens][question tokens][replay signal][replayed image tokens][answer tokens], which spans should be masked from loss computation?

## Architecture Onboarding

- **Component map**: CLIP-ViT-L/14@336 → AnyRes crops → Adapter → Pooling → Feature map S → LLM (Vicuna-v1.5-7B/13B) → Selective Replay Parser → Detection Head (MLP)
- **Critical path**: Image → AnyRes crops → ViT → Adapter → Pooling → Feature map S (stored) → LLM input (144 tokens) → LLM generates text including `<sot>[coords]<eot>` → Parser detects `<eot>` → extracts coords → retrieves from S → 2×2 pooled replay tokens → append to LLM input → LLM continues generation
- **Design tradeoffs**: More crops vs. token budget (16 crops × 4×4 pooling = 720 tokens max); detection loss vs. pure tokenized boxes (regression adds MLP parameters); cold-start model quality vs. annotation efficiency (Qwen2.5-VL-72B slow but high-quality vs. InternVL3-14B faster but potential biases)
- **Failure signatures**: Empty/invalid replay signals (no features retrieved); off-image boxes (indexing errors); excessive replay calls (OOM/slowdown); detection head divergence (box predictions become nonsensical)
- **First 3 experiments**: (1) Reproduce baseline pooling ablation: 2×2 vs 2×2/4×4/2×2 pooling comparison; (2) Ablate replay vs grounding-only: quantify replay contribution on fine-grained benchmarks; (3) Test generalization to out-of-domain images: monitor for coordinate parsing failures and box prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does VGR generalize to alternative MLLM architectures beyond LLaVA? VGR is currently constrained to LLaVA architecture, exploring stronger visual encoders and LLMs could further enhance performance. All experiments use LLaVA-NeXT with CLIP-ViT-L/14 and Vicuna backbones. Architecture-specific design choices may not transfer directly to other architectures.

### Open Question 2
Can reinforcement learning enhance VGR's grounding-then-answering capability beyond SFT? Another avenue is integrating reinforcement learning (RL). With cold-start initialization and RL training, a more generalized and diverse reasoning process may be achievable. VGR relies entirely on supervised fine-tuning with rejection-sampled data.

### Open Question 3
How does VGR perform on non-rectangular or multiple disjoint regions of interest? VGR exclusively uses axis-aligned bounding boxes for grounding. The paper notes intentionally expanding bounding boxes "to encourage the trained model to retain contextual information," suggesting the rectangular format may inadequately capture irregularly-shaped or scattered relevant regions.

### Open Question 4
What causes the data type reversal effect where text-only reasoning data (LLaVA-CoT, MMPR) degrades performance relative to baseline? Table 5 shows LLaVA-CoT and MMPR (vanilla text reasoning datasets) yield worse results than baseline, while VGR's region-guided reasoning improves performance. The paper hypothesizes "accumulation of language bias during multimodal reasoning," but the mechanism is untested.

## Limitations
- Training data composition and quality remain underspecified, creating uncertainty about whether performance gains stem from model architecture or curated dataset quality
- Limited evidence of performance on out-of-distribution data or extreme aspect ratios, with resolution expansion claims not empirically validated across diverse image types
- Detection head sensitivity not fully analyzed, with sensitivity to hyperparameters like β=2 and MLP architecture choices remaining unclear

## Confidence
- **High Confidence**: Selective feature replay mechanism's basic implementation and token efficiency reduction (70% token savings while maintaining performance)
- **Medium Confidence**: Claim that grounding-reasoning interdependence improves multimodal understanding, though causal relationship not conclusively established
- **Low Confidence**: Assertion that VGR overcomes language bias in vision-language models, as direct comparisons against text-only reasoning baselines on the same data are not provided

## Next Checks
1. **Dataset Quality Verification**: Construct a minimal VGR-SFT subset (1K samples) and evaluate whether the model's performance scales linearly with dataset quality. Test whether VGR trained on random/noisy bounding boxes degrades similarly to the "w/o Grounding" ablation.
2. **Resolution Robustness Test**: Evaluate VGR on a diverse set of images with varying aspect ratios (panoramas, tall images, extreme crops) not present in training. Monitor for coordinate parsing failures, box prediction accuracy degradation, and performance drops.
3. **Detection Loss Sensitivity Analysis**: Systematically vary β (1.0, 2.0, 3.0) and MLP layer sizes while training on a fixed dataset subset. Compare performance and training stability to determine whether the reported β=2 configuration is optimal.