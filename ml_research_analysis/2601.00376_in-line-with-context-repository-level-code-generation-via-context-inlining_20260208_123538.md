---
ver: rpa2
title: 'In Line with Context: Repository-Level Code Generation via Context Inlining'
arxiv_id: '2601.00376'
source_url: https://arxiv.org/abs/2601.00376
tags:
- code
- context
- generation
- function
- inlinecoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'InlineCoder improves repository-level code generation by inlining
  the target function into its call graph. The method first generates a draft anchor
  function, then performs bidirectional inlining: upstream inlining embeds the draft
  into caller contexts, while downstream retrieval adds relevant callee functions.'
---

# In Line with Context: Repository-Level Code Generation via Context Inlining

## Quick Facts
- arXiv ID: 2601.00376
- Source URL: https://arxiv.org/abs/2601.00376
- Reference count: 40
- Improves repository-level code generation by inlining target functions into their call graph context

## Executive Summary
InlineCoder addresses repository-level code generation by transforming it into function-level generation through context inlining. The method generates a draft function, then performs bidirectional inlining: upstream inlining embeds the draft into caller contexts, while downstream retrieval adds relevant callee functions. This approach converts the challenging task of repository understanding into an easier function-level coding problem by making usage constraints explicit through inlined contexts.

## Method Summary
InlineCoder is a three-stage pipeline that improves repository-level code generation through context inlining. First, it generates a draft function using the target signature and repository imports/dependencies. Second, it performs bidirectional inlining: upstream inlining embeds the draft into caller contexts through parameter substitution and return normalization, while downstream retrieval fetches relevant callee functions based on AST analysis and LLM predictions. Third, it generates the final code using perplexity-based confidence scoring to guide how much the model should trust the draft versus revising it. The approach leverages call graph analysis, AST traversal, and perplexity scoring to create enriched prompts that make repository context explicit for function-level generation.

## Key Results
- Average relative gains of 29.73% in exact match, 20.82% in edit similarity, and 49.34% in BLEU on RepoExec benchmark
- Consistent improvements across three LLMs (DeepSeek-V3, Qwen3-Coder, GPT5-mini) on both DevEval and RepoExec benchmarks
- Ablation studies confirm each component's contribution: removing upstream inlining causes ~2.4 EM drop, removing downstream retrieval causes ~1.9 EM drop, and removing confidence scoring causes ~0.82 EM drop

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context inlining converts repository-level reasoning into function-level generation by embedding the target function within its execution context
- Mechanism: Four-step transformation (parameter substitution → return normalization → assignment redirection → inline expansion) injects the draft function directly into caller bodies, making usage constraints explicit (variable bindings, return types) without requiring the model to infer cross-file relationships
- Core assumption: The model better understands function requirements when it sees concrete usage sites rather than abstract function signatures
- Evidence anchors:
  - [abstract] "reframing the challenging repository understanding as an easier function-level coding task"
  - [section 3.3.1] "This inlining process ensures semantic equivalence while making the target function's intended behavior explicit in context"
  - [corpus] GRACE paper corroborates graph-guided hierarchical code fusion for repository context
- Break condition: Functions with no callers (isolated utilities) provide no upstream signal; Figure 9 shows No Context functions score higher but gain less from InlineCoder

### Mechanism 2
- Claim: Bidirectional retrieval anchored to a draft implementation improves relevance over similarity-based search
- Mechanism: Draft provides concrete API call signals (from AST parsing + LLM predictions), enabling targeted retrieval of actual dependencies rather than lexically similar but semantically unrelated snippets
- Core assumption: The draft's API calls, even if imperfectly implemented, indicate which repository functions are semantically relevant
- Evidence anchors:
  - [abstract] "This anchor drives a bidirectional inlining process"
  - [section 3.3.2] Equation 6-7 formalizes query construction from draft; "bridges the gap between a noisy, from-scratch draft and a concise, high-quality solution"
  - [corpus] Weak direct corpus evidence for draft-anchored retrieval specifically
- Break condition: If draft hallucinates non-existent APIs, retrieval yields empty results; Table 5 shows EM of ~30% on call statements indicates imperfect downstream matching

### Mechanism 3
- Claim: Perplexity-based confidence scoring reduces anchoring bias while preserving useful draft signals
- Mechanism: PPL thresholds (high <1.3, medium 1.3-2, low >2) map to natural language guidance that explicitly tells the model how much to trust vs. revise the draft
- Core assumption: Low perplexity correlates with draft correctness; explicit guidance prevents the model from ignoring enriched context and repeating draft errors
- Evidence anchors:
  - [section 3.4] Three-tier confidence mapping with specific prompts; Figure 8 case study shows confidence guidance enables error correction
  - [section 5.2] "w/o confidence" ablation shows -0.82 EM drop vs. full system
  - [corpus] No direct corpus evidence for PPL-based confidence in code generation
- Break condition: Thresholds tuned empirically (40%/40%/20% split); may not generalize across different model families or languages

## Foundational Learning

- Concept: **Call graph analysis and AST traversal**
  - Why needed here: Core infrastructure for identifying upstream callers and downstream callees; enables the bidirectional context extraction
  - Quick check question: Given a Python function `def foo(): bar()`, can you identify which tools would extract the call relationship and how you'd traverse to find callers of `foo`?

- Concept: **Perplexity as confidence proxy**
  - Why needed here: Determines whether the model should trust, refine, or regenerate the draft; critical for the confidence-guided generation stage
  - Quick check question: If a draft has PPL of 1.8, what guidance should be injected and what behavior should you expect from the final generation?

- Concept: **Function inlining semantics**
  - Why needed here: Upstream inlining requires correct parameter substitution, return handling, and scope management to preserve semantics
  - Quick check question: When inlining `def f(x): return x + 1` into `y = f(5)`, what transformations are needed to produce semantically equivalent inline code?

## Architecture Onboarding

- Component map:
  Input: function signature + repository access → [Draft Generator] → draft + API calls + PPL score → [Context Inliner] → upstream inlined context + downstream functions → [Context Integrator] → enhanced prompt with confidence guidance → [Final Generator] → completed function

- Critical path: Draft generation → downstream API extraction → upstream inlining → confidence-scored final generation. Draft quality gates downstream retrieval precision; upstream inlining enables return-type inference

- Design tradeoffs:
  - Two-stage generation (draft + final) increases latency but provides retrieval anchors
  - PPL thresholds empirically tuned; may need recalibration per model family
  - Substring-based retrieval (Equation 7) is simple but may miss renamed/imported functions

- Failure signatures:
  - Empty upstream context: function has no callers (check Figure 9 "No Context" category)
  - No downstream matches: draft uses non-existent APIs → verify against repository symbol table
  - PPL extreme values: very low PPL with wrong code indicates model overconfidence in incorrect patterns

- First 3 experiments:
  1. **Baseline sanity check**: Run Vanilla baseline on 20 samples, verify your evaluation pipeline produces EM/ES/BLEU scores matching reported ranges (DevEval EM ~11% for InlineCoder)
  2. **Component isolation**: Ablate upstream inlining only (w/o inline) on 50 samples; expect ~2.4 EM drop per Table 3
  3. **Cross-model validation**: Apply InlineCoder to a different code LLM not in the paper; if PPL thresholds produce uneven splits, recalibrate threshold boundaries

## Open Questions the Paper Calls Out
- The paper notes that evaluation is limited to Python repositories, which may limit generalizability to other programming languages, though the core methodology can be readily extended.

## Limitations
- Empirical evaluation limited to Python datasets without cross-language validation
- Draft-anchored retrieval mechanism lacks direct corpus evidence for superiority over similarity-based approaches
- Perplexity-based confidence scoring thresholds are empirically tuned without theoretical justification for generalizability

## Confidence

- **High Confidence**: The bidirectional inlining mechanism (upstream caller embedding + downstream callee retrieval) is well-supported by both implementation details and quantitative results showing consistent improvements across three different LLMs and two benchmarks.
- **Medium Confidence**: The draft generation quality claims are supported by the overall system performance but rely on an opaque GPT-5-mini placeholder and limited ablation showing only a 3.82% EM drop when removing draft components.
- **Low Confidence**: The perplexity-based confidence scoring mechanism lacks direct corpus evidence, with no comparative studies showing its advantage over alternative confidence estimation methods or simpler approaches like fixed revision prompts.

## Next Checks

1. **Cross-language generalization test**: Apply InlineCoder to a Java/C++ repository and measure whether the same perplexity thresholds (1.3, 2.0) and confidence mappings produce similar performance gains.

2. **Retrieval baseline comparison**: Implement a similarity-based retrieval baseline (cosine similarity on function embeddings) and compare its downstream matching precision against the draft-anchored approach on 50 samples.

3. **Perplexity threshold sensitivity**: Systematically vary PPL thresholds (±0.3 around current values) and measure the impact on EM/ES/BLEU scores to determine threshold robustness.