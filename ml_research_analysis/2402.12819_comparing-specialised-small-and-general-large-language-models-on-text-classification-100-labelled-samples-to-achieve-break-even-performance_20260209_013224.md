---
ver: rpa2
title: 'Comparing Specialised Small and General Large Language Models on Text Classification:
  100 Labelled Samples to Achieve Break-Even Performance'
arxiv_id: '2402.12819'
source_url: https://arxiv.org/abs/2402.12819
tags:
- performance
- samples
- learning
- in-context
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how many labelled training samples are
  needed for specialised small language models to outperform general large language
  models in text classification. Using eight language models and eight datasets, the
  research compares fine-tuning, instruction-tuning, prompting, and in-context learning
  across varying dataset sizes.
---

# Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance

## Quick Facts
- **arXiv ID:** 2402.12819
- **Source URL:** https://arxiv.org/abs/2402.12819
- **Authors:** Branislav Pecher; Ivan Srba; Maria Bielikova
- **Reference count:** 40
- **Primary result:** Specialized small models typically require only ~100 labeled samples to outperform general large models in text classification

## Executive Summary
This study investigates how many labeled training samples are needed for specialized small language models to outperform general large language models in text classification. Using eight language models and eight datasets, the research compares fine-tuning, instruction-tuning, prompting, and in-context learning across varying dataset sizes. The results show that specialized models typically require only around 100 labeled samples to match or surpass general models, though this varies significantly based on dataset characteristics. Binary datasets and tasks requiring complex language understanding need notably more samples. Performance variance strongly impacts comparisons, increasing required sample counts by 100-200% on average. The study also finds that larger models don't consistently yield better performance or lower variance, and 4-bit quantization has minimal impact on results. Based on these findings, the authors provide practical recommendations for choosing between specialized and general models based on annotation budget, computational resources, and task requirements.

## Method Summary
The study compares eight language models (BERT, RoBERTa, Flan-T5, GPT-4o-mini, LLaMA-2, LLaMA-3, Mistral, Zephyr) across eight text classification datasets using four approaches: fine-tuning, instruction-tuning, prompting, and in-context learning. Data is split 60/20/20 and subsampled exponentially from 1 sample per class to full dataset size. Fine-tuning uses standard hyperparameters (LR=1e-5, AdamW, 10 epochs), while instruction-tuning employs LoRA with 4-bit quantization. General models use zero-shot and few-shot prompting with context window management. Results are aggregated across 100 runs for fine-tuning and 20 runs for large language models, measuring F1 Macro with mean and standard deviation to identify break-even points where specialized models surpass general ones.

## Key Results
- Specialized models require approximately 100 labeled samples on average to match or exceed general models in multi-class classification tasks
- Binary classification datasets require significantly more samples (up to 4000-20,000) for specialized models to outperform general models
- Performance variance increases required sample counts by 100-200% on average when accounting for worst-case scenarios
- Larger models don't consistently provide better performance or lower variance across tasks
- 4-bit quantization has minimal impact on performance or variance for general models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Specialized small models (fine-tuned or instruction-tuned) can outperform general large models (prompting or in-context learning) with relatively few labeled samples (approx. 100 on average).
- **Mechanism:** Parameter updates (via fine-tuning or instruction-tuning) allow the model to adapt its weights specifically to the target distribution, effectively shifting the decision boundary. In contrast, general models rely on frozen priors and input conditioning (prompts), which may lack the precision for specific classification nuances without extensive context or prompt engineering.
- **Core assumption:** The specialized model architecture (e.g., BERT, RoBERTa) retains sufficient representational capacity to capture the task logic with limited gradient updates.
- **Evidence anchors:**
  - [abstract] "Specialised models often need only few samples (on average 100) to be on par or better..."
  - [section 3.1] "...fine-tuning approaches require on average between 10−500 labelled samples [to outperform zero-shot]."
  - [corpus] "Active Few-Shot Learning" supports the premise that FSL methods leverage limited data effectively, though this paper focuses on the break-even comparison specifically.
- **Break condition:** If the dataset was seen during the general model's pre-training (data leakage), the general model may remain competitive or superior, rendering the sample advantage moot [Section 3.1].

### Mechanism 2
- **Claim:** Performance variance (sensitivity to randomness) significantly inflates the number of samples required to guarantee specialized model superiority (the "Second Break-Even Point").
- **Mechanism:** In low-data regimes, randomness (seed selection, sample order, initialization) causes high fluctuation in performance metrics. To ensure reliable outperformance (worst-case specialized > best-case general), the sample count must increase to stabilize the variance, often by 100–200%.
- **Core assumption:** The observed variance is intrinsic to the learning methods and not solely an artifact of the specific experimental seeds.
- **Evidence anchors:**
  - [abstract] "...performance variance strongly impacts comparisons, increasing required sample counts by 100-200% on average."
  - [section 3.2] "Sensitivity to the effects of randomness... has a significant effect on the break-even points... increasing the number of required labelled samples by a significant margin."
  - [corpus] Corpus signals do not provide specific counter-evidence regarding variance inflation mechanisms.
- **Break condition:** If inference costs are ignored and "cherry-picked" seeds are allowed, the variance requirement could be bypassed (though the paper argues against this practice).

### Mechanism 3
- **Claim:** Binary classification tasks and tasks requiring complex language understanding require disproportionately more samples for specialized models to beat general ones compared to multi-class tasks.
- **Mechanism:** (Assumption) General Large Language Models (LLMs) possess strong priors for binary sentiment or linguistic acceptability due to pre-training objectives, making them robust baselines. Specialized models require more evidence (samples) to definitively shift these strong priors. Conversely, multi-class tasks may confuse general models (higher entropy), allowing specialized models to specialize faster with fewer samples.
- **Core assumption:** The difficulty is not solely defined by the number of classes but by the alignment between the pre-training distribution and the specific task boundary.
- **Evidence anchors:**
  - [section 3.1] "...fine-tuning on binary datasets requiring significantly more samples."
  - [section 3.1] "On multi-class datasets, fine-tuning requires only up to 100−200 labelled samples... On binary datasets the required samples are as high as 4000..."
  - [corpus] No specific corpus papers were found analyzing the binary vs. multi-class sample efficiency differential.
- **Break condition:** If the binary task involves highly idiosyncratic domain knowledge not present in the general model's pre-training, the sample requirement might drop.

## Foundational Learning

- **Concept: In-Context Learning (ICL) vs. Fine-Tuning**
  - **Why needed here:** The paper explicitly compares "General" approaches (Prompting/ICL) against "Specialized" approaches (Fine-Tuning/Instruction-Tuning). Understanding that ICL uses examples in the prompt context without weight updates, while Fine-Tuning updates model weights, is essential for interpreting the break-even points.
  - **Quick check question:** Does the model update its weights based on the 100 samples, or does it just look at them during inference?

- **Concept: Performance Variance (Sensitivity)**
  - **Why needed here:** The study emphasizes that average performance is misleading. The "Second Break-Even Point" dictates real-world viability by accounting for standard deviation across runs.
  - **Quick check question:** If I run the experiment with 3 different random seeds, will the ranking of Model A vs. Model B change?

- **Concept: Quantization (4-bit)**
  - **Why needed here:** The paper validates that using 4-bit quantized versions of general models (e.g., Mistral, Zephyr) does not significantly degrade performance or variance, which is critical for the compute-efficiency recommendations.
  - **Quick check question:** Can I run the large general model on a consumer GPU without losing the accuracy benefits described in the paper?

## Architecture Onboarding

- **Component map:** Data Subsampling (Stratified 60/20/20 split with exponential subsampling) -> Specialized Pipeline (BERT/RoBERTa -> Fine-Tuning/Instruction-Tuning -> Evaluation) -> General Pipeline (LLaMA/Mistral/GPT -> Prompting/ICL -> Evaluation) -> Analysis Module (F1 Macro calculation across 20-100 runs -> Break-Even Point Identification)

- **Critical path:**
  1. Select dataset and determine if binary or multi-class
  2. If annotation budget < 100: Prefer General LLM (Instruction-Tuned if compute allows, else ICL)
  3. If annotation budget > 100–200 (accounting for variance): Prefer Fine-Tuned Specialized Model
  4. If using General LLM: Apply 4-bit quantization to maximize compute efficiency without performance loss

- **Design tradeoffs:**
  - **Annotation vs. Compute:** The study frames the choice as a tradeoff. Specialized models trade annotation effort for lower inference compute (smaller model). General models trade high inference compute for zero annotation effort.
  - **Prompt Engineering vs. Sample Selection:** For general models, performance is highly sensitive to prompt format and in-context example quality. For specialized models, the "prompt" is fixed, but performance depends on sample quantity/quality.

- **Failure signatures:**
  - **Instability:** High standard deviation in ICL or Fine-Tuning with <50 samples [Section 3.2]
  - **Context Overflow:** General model performance drops abruptly when in-context examples exceed the model's context window [Section A.2]
  - **Overfitting:** Instruction-tuned models (specifically Flan-T5) may overfit quickly on larger subsets without step constraints [Section A.2]

- **First 3 experiments:**
  1. **Variance Baseline:** Run the target task using a General Model (LLaMA-3 or Mistral) with 0-shot and 4-shot ICL across 20 random seeds to establish the baseline variance and "First Break-Even" target
  2. **Sample Efficiency Scaling:** Fine-tune a Small Model (RoBERTa) on exponential subsets (10, 25, 50, 100 samples) to identify the exact intersection point where it surpasses the General Model's mean performance
  3. **Robustness Check (Second Break-Even):** Increase the training set size until the lower bound of the Small Model's performance (Mean - STD) exceeds the upper bound of the General Model's performance (Mean + STD)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do break-even points between specialised and general models differ for generation tasks (e.g., summarization, translation, question answering) compared to the text classification tasks studied?
- Basis in paper: [explicit] "This choice may limit the generalisation of our findings to other datasets, tasks and languages, such as the generation tasks, which are more representative for the general models and on which the fine-tuning cannot be used as easily"
- Why unresolved: The study focuses exclusively on classification datasets; generation tasks require different evaluation methodologies and may favor general models more strongly.
- What evidence would resolve it: Replicating the experimental methodology across representative generation tasks and comparing required sample sizes for specialised vs. general models.

### Open Question 2
- Question: What are robust methods for detecting and accounting for task contamination when datasets may have been included in large language model pre-training?
- Basis in paper: [explicit] "We are not sure whether the datasets we use in our experiments have been used to pre-train the models we use... this limitation is part of the recently recognised LLM validation crisis"
- Why unresolved: Training models from scratch to ensure clean evaluation is prohibitively expensive; systematic contamination detection methods remain underdeveloped.
- What evidence would resolve it: Developing and validating contamination detection techniques that can identify dataset exposure in pre-trained models without requiring access to training data.

### Open Question 3
- Question: Why do binary classification datasets require significantly more labelled samples (up to 20,000) compared to multi-class datasets (100-200) for specialised models to outperform general models?
- Basis in paper: [inferred] The paper observes this phenomenon consistently across all binary datasets (SST2, MRPC, CoLA, BoolQ) but does not provide a mechanistic explanation for why task type affects sample efficiency so dramatically.
- Why unresolved: The paper identifies the pattern but does not investigate underlying causes such as decision boundary complexity, class imbalance, or interaction with model architectures.
- What evidence would resolve it: Controlled experiments varying task characteristics independently (e.g., binary vs. multi-class with matched difficulty) and analysis of model representations during training.

## Limitations

- **Model Architecture Bias:** Results may not generalize beyond BERT/RoBERTa and LLaMA/Mistral architectures
- **Dataset Representativeness:** 8 datasets may not capture all real-world scenarios, particularly the extreme sample requirements for binary datasets
- **Implementation-Specific Factors:** Specific hyperparameters, quantization methods, and prompt templates may influence results

## Confidence

- **High Confidence:** The general finding that specialized small models typically require around 100 labeled samples to match or surpass general large models on multi-class text classification tasks
- **Medium Confidence:** The variance-related finding that performance fluctuations increase required sample counts by 100-200%
- **Low Confidence:** The claim that binary classification tasks require significantly more samples (up to 20,000) compared to multi-class tasks

## Next Checks

**Check 1: Cross-Architecture Validation**
Test the break-even hypothesis using alternative specialized model architectures (e.g., DeBERTa, Electra) and general model families (e.g., Claude, Gemini) to verify whether the ~100 sample threshold holds across different model designs.

**Check 2: Domain Transfer Experiment**
Evaluate whether the break-even points transfer across domains by training specialized models on one domain (e.g., news classification) and testing them on semantically similar but distinct domains (e.g., scientific abstracts), measuring both performance and required sample counts.

**Check 3: Real-World Annotation Study**
Conduct a user study where annotators create 100 labeled examples for a target task, then measure actual human effort versus the performance gains achieved, validating whether the theoretical sample efficiency translates to practical annotation budget constraints.