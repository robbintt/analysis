---
ver: rpa2
title: A Comparative Study of Retrieval Methods in Azure AI Search
arxiv_id: '2512.08078'
source_url: https://arxiv.org/abs/2512.08078
tags:
- retrieval
- semantic
- keyword
- search
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates retrieval strategies within Microsoft Azure's
  RAG framework to identify effective approaches for Early Case Assessment (ECA) in
  eDiscovery. We compared the performance of Azure AI Search's keyword, semantic,
  vector, hybrid, and hybrid-semantic retrieval methods, presenting the accuracy,
  relevance, and consistency of each method's AI-generated responses.
---

# A Comparative Study of Retrieval Methods in Azure AI Search

## Quick Facts
- arXiv ID: 2512.08078
- Source URL: https://arxiv.org/abs/2512.08078
- Reference count: 1
- Primary result: Retrieval quality is the primary determinant of downstream LLM performance in RAG systems

## Executive Summary
This study evaluates retrieval strategies within Microsoft Azure's RAG framework to identify effective approaches for Early Case Assessment (ECA) in eDiscovery. We compared the performance of Azure AI Search's keyword, semantic, vector, hybrid, and hybrid-semantic retrieval methods, presenting the accuracy, relevance, and consistency of each method's AI-generated responses. Our analysis shows that no single retrieval method performs consistently across all scenarios; keyword-based retrieval excels for structured data while vector retrieval captures semantic similarity. Hybrid semantic retrieval produced the most consistently relevant results but at higher computational cost. Many errors attributed to "LLM hallucination" were actually rooted in retrieval method design deficiencies, demonstrating that retrieval quality is the primary determinant of downstream model performance.

## Method Summary
The study uses the Jeb Bush email dataset (290,000+ documents) chunked into 2,000-token segments with 500-token overlap, creating 491,482 chunks. Azure AI Search indexes these chunks with text-embedding-ada-002 embeddings (1536 dimensions). Five retrieval methods are tested: keyword, vector, hybrid, semantic reranking, and hybrid-semantic. For each query, the top 50 chunks are retrieved and the top 5 are passed to GPT-4.1-mini (temperature=0) for generation. Nine specific prompts test different retrieval challenges, with manual qualitative assessment of accuracy, relevance, and consistency.

## Key Results
- No single retrieval method performs consistently across all scenarios
- Keyword-based retrieval excels for structured data with specific identifiers
- Vector retrieval captures semantic similarity but fails on alphanumeric terms
- Hybrid semantic retrieval produces the most consistently relevant results
- Many errors attributed to "LLM hallucination" were rooted in retrieval method design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval quality is the primary determinant of downstream LLM response accuracy in RAG systems.
- Mechanism: The retriever acts as a hard filter—only documents it surfaces can ground the LLM. If retrieval fails to locate relevant chunks, the generator has no path to correct answers regardless of model capability.
- Core assumption: The LLM cannot access information outside retrieved context; temperature=0 isolates retrieval effects from generation variability.
- Evidence anchors:
  - [abstract] "Many errors attributed to 'LLM hallucination' were actually rooted in retrieval method design deficiencies, demonstrating that retrieval quality is the primary determinant of downstream model performance."
  - [section] "Many errors attributed to 'LLM hallucination' were rooted in retrieval method design deficiencies, underscoring that retrieval quality is the primary determinant of downstream model performance."
  - [corpus] Related work on hybrid retrieval for hallucination mitigation (FMR=0.60) supports this finding but tests different systems.
- Break condition: If the LLM has access to external knowledge (e.g., web search tool use), retrieval is no longer the sole determinant.

### Mechanism 2
- Claim: Semantic reranking improves recall when initial retrieval returns relevant results in low positions, but cannot recover documents absent from the initial candidate set.
- Mechanism: Semantic rerankers reorder existing results using contextual similarity; they do not expand the search space. If the base retrieval misses relevant content entirely, reranking amplifies whatever is present.
- Core assumption: Reranking operates on a fixed candidate pool from prior retrieval; re-rankers have no independent document access.
- Evidence anchors:
  - [abstract] "Hybrid semantic retrieval produced the most consistently relevant results but at higher computational cost."
  - [section] P2 results: "The failure of the semantic method suggests that the keyword search did not return relevant information within its 50 results; because semantic reranking only reorders keyword search results, it cannot recover documents that keyword retrieval does not surface."
  - [corpus] Weak direct evidence; corpus papers focus on initial retrieval, not reranking behavior specifically.
- Break condition: If reranking is granted independent retrieval capability or expands the candidate pool, this constraint no longer applies.

### Mechanism 3
- Claim: Query structure and phrasing strongly influence which retrieval method succeeds, requiring method-query alignment.
- Mechanism: Keyword methods match exact lexical tokens; vector methods encode semantic meaning but lose specificity for rare terms or alphanumeric strings. No single method covers all query types.
- Core assumption: Embedding models do not preserve arbitrary alphanumeric sequences as semantically meaningful features.
- Evidence anchors:
  - [abstract] "No single retrieval method performs consistently across all scenarios; keyword-based retrieval excels for structured data while vector retrieval captures semantic similarity."
  - [section] P1: "The vector embedding of the prompt does not embed the alphanumeric case number as a meaningful semantic feature, causing the vector retrieval method to fail to locate the emails."
  - [section] P5: "By enclosing T-Squared in quotes, the keyword method now locates the correct information, while the vector method continues to fail."
  - [corpus] Facebook hybrid search paper (FMR=0.55) confirms keyword-embedding combination benefits for diverse query types.
- Break condition: If embedding models are trained to preserve alphanumeric identifiers or rare terms as distinct features, vector retrieval could handle these cases.

## Foundational Learning

- Concept: **RAG Architecture (Retriever + Generator)**
  - Why needed here: The entire paper evaluates how retriever choices affect generator output. Understanding this two-stage pipeline is prerequisite to interpreting results.
  - Quick check question: In a RAG system, can the LLM generate accurate information about documents the retriever never surfaced?

- Concept: **Lexical vs. Semantic Search**
  - Why needed here: The five retrieval methods differ primarily in how they combine lexical matching (keyword) with semantic understanding (vector/reranking). Knowing this distinction explains why P1 fails for vector but succeeds for keyword.
  - Quick check question: Would you expect keyword search to find documents discussing "citrus fruit forecasts" when your query is "orange production projections"?

- Concept: **Embedding Space and Cosine Similarity**
  - Why needed here: Vector retrieval embeds queries and documents into the same vector space; understanding that similarity is computed geometrically explains why rare terms and alphanumeric strings may not cluster usefully.
  - Quick check question: Why might an embedding model fail to distinguish "case no. TO 98-103033" from other alphanumeric strings?

## Architecture Onboarding

- Component map:
  Data Ingestion -> Chunking -> Embedding -> Index -> Retrieval -> Reranking -> LLM Generation -> Response

- Critical path:
  1. Query formulation → 2. Retrieval method selection → 3. Candidate retrieval (50 chunks) → 4. Optional reranking → 5. Top-k selection (5 chunks) → 6. LLM generation → 7. Response with citations
  - The paper shows steps 2-4 are where most errors originate.

- Design tradeoffs:
  - **Keyword**: Fast, exact match, misses semantics (P1 succeeds, P2 fails)
  - **Vector**: Semantic matching, poor on rare/alphanumeric terms (P2 succeeds, P1 fails)
  - **Hybrid**: Combines both, moderate cost, good coverage
  - **Semantic Reranking**: Improves relevance but can elevate misleading chunks (P7/P8 attribution error)
  - **Cost**: Hybrid-Semantic has highest computational cost; Keyword is lowest

- Failure signatures:
  - **Alphanumeric/rare terms fail in vector**: Check if query contains case numbers, IDs, nicknames (P1, P4)
  - **Conceptual queries fail in keyword**: Check if query requires semantic understanding (P2)
  - **Attribution errors after reranking**: Reranker elevated tangentially related content (P7-P9)
  - **Quoted terms succeed in keyword but not vector**: Exact phrase matching behavior differs (P5)

- First 3 experiments:
  1. **Run identical queries across all 5 retrieval methods** and compare which documents surface in top-5. Document which method finds ground-truth relevant chunks.
  2. **Test alphanumeric and rare-term queries** (like P1/P4) to validate vector retrieval failure mode in your own data; confirm keyword or hybrid recovers these.
  3. **Introduce a known-attribution-risk query** similar to P7: ask about a specific person's views when related but distinct individuals exist in the corpus. Verify whether citations reflect the correct source.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an agentic system accurately evaluate a natural language query to automatically select the optimal retrieval strategy (e.g., keyword vs. vector)?
- Basis in paper: [explicit] The authors state future work will explore using an agentic system to evaluate each query and select the retrieval strategy most likely to produce accurate results.
- Why unresolved: The current study evaluates static retrieval configurations manually; it does not test dynamic, query-dependent selection mechanisms.
- What evidence would resolve it: A comparative study where an agent selects the method per query, bench-marked against static single-method baselines for accuracy and latency.

### Open Question 2
- Question: Do the observed performance advantages of hybrid-semantic retrieval generalize to larger and more diverse legal datasets?
- Basis in paper: [explicit] The authors explicitly identify the need to benchmark retrieval strategies across larger and more diverse datasets to validate performance in real-world legal scenarios.
- Why unresolved: This study relied solely on the Jeb Bush email dataset (290,000 items), which may not represent the full complexity of other legal domains.
- What evidence would resolve it: Replicating the experimental methodology on distinct corpora (e.g., M&A contracts or litigation transcripts) and observing if hybrid-semantic retrieval maintains its superiority.

### Open Question 3
- Question: How can semantic reranking mechanisms be refined to prevent the elevation of tangential or misattributed content?
- Basis in paper: [inferred] The authors observed that semantic reranking elevated a text chunk containing George Bush's views, causing the LLM to misattribute them to Jeb Bush (Prompt P7/P8).
- Why unresolved: While the paper identifies this failure mode, it does not propose technical solutions to calibrate the reranker's sensitivity to entity-specific context.
- What evidence would resolve it: A modified reranking algorithm that successfully filters out high-similarity but contextually irrelevant chunks in entity-specific queries.

## Limitations

- Study relies on manual qualitative assessment of 9 prompts, introducing subjectivity
- Cannot isolate whether failures stem from embedding model limitations or ranker processing
- Specific configuration of Azure's semantic ranker is not disclosed
- Single dataset (Jeb Bush emails) may not generalize to other domains

## Confidence

- **High confidence**: Retrieval quality is the primary determinant of downstream LLM performance in RAG systems; keyword methods excel for structured data with specific identifiers while vector methods capture semantic similarity.
- **Medium confidence**: Hybrid semantic retrieval produces the most consistently relevant results across diverse query types; semantic reranking improves relevance when initial retrieval captures relevant content but cannot recover documents absent from the candidate pool.
- **Low confidence**: The attribution errors observed in P7-P9 are definitively caused by semantic reranking elevating tangentially related content rather than other factors like LLM generation artifacts.

## Next Checks

1. Test the alphanumeric/rare term failure mode (P1) with synthetic datasets containing various identifier formats to determine whether this is a universal vector embedding limitation or specific to certain encoding patterns.

2. Conduct a blind multi-reviewer assessment of the same 9 prompts to quantify inter-rater reliability and establish objective criteria for what constitutes "accurate" and "relevant" retrieval.

3. Compare Azure's semantic ranker behavior against open-source rerankers (e.g., Cohere, BGE) using identical candidate pools to determine whether the attribution issues in P7-P9 are implementation-specific or inherent to semantic reranking methodology.