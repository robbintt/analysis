---
ver: rpa2
title: Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers
arxiv_id: '2505.15239'
source_url: https://arxiv.org/abs/2505.15239
tags:
- neural
- loss
- collapse
- layers
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that neural collapse (NC) is globally optimal
  in deep residual networks (ResNets) and transformers, under certain regularization
  schemes. The authors show that as the depth of these architectures grows, their
  global optima converge to the optimal solution of an equivalent unconstrained features
  model (UFM), where NC emerges.
---

# Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers

## Quick Facts
- **arXiv ID:** 2505.15239
- **Source URL:** https://arxiv.org/abs/2505.15239
- **Reference count:** 40
- **Primary result:** Neural collapse is globally optimal in deep regularized ResNets and transformers, with convergence to UFM solutions as depth increases.

## Executive Summary
This paper proves that neural collapse (NC) emerges as a global optimum in deep residual networks and transformers under certain regularization schemes. The authors demonstrate that as network depth increases, these architectures converge to the optimal solution of an equivalent unconstrained features model (UFM), where NC naturally occurs. The theoretical framework applies to both cross-entropy and mean squared error loss functions with minimal assumptions on the data distribution. This work provides the first theoretical justification for using UFMs to analyze deep architectures, bridging the gap between feature learning in practical deep networks and the theoretically tractable UFM framework.

## Method Summary
The authors establish theoretical convergence results for deep regularized ResNets and transformers by showing that their global optima approach the UFM solution as depth grows. They analyze two main regularization schemes: ridge regularization on features and a slack variable approach that relaxes the orthogonality constraint. For both architectures, they prove that deep networks can approximate the UFM solution arbitrarily well given sufficient depth and appropriate width. The analysis relies on expressing the network outputs as solutions to optimization problems over intermediate features, then showing that these optimization problems converge to the UFM as depth increases. The theoretical framework is validated through experiments on CIFAR-10 and ImageNet for ResNets, and on sentiment analysis tasks for transformers, demonstrating that NC metrics improve with increasing depth.

## Key Results
- Deep ResNets and transformers converge to UFM solutions as depth increases under ridge regularization and slack variable regularization schemes
- The convergence holds for both cross-entropy and mean squared error loss functions
- Experimental results on CIFAR-10, ImageNet, and sentiment analysis tasks confirm that NC metrics improve with increasing depth
- The number of channels affects the approximation quality, with convergence requiring a lower bound on channel count

## Why This Works (Mechanism)
The theoretical mechanism relies on the recursive structure of deep networks, where each layer can be viewed as solving an optimization problem over intermediate features. As depth increases, these recursive optimizations progressively approach the UFM solution. For ResNets, the residual connections allow the network to maintain information flow while progressively refining features. For transformers, the self-attention mechanism enables non-local feature interactions that approximate the unconstrained optimization in UFM. The regularization schemes ensure that the optimization remains well-behaved and converges to meaningful solutions rather than trivial ones.

## Foundational Learning

**Unconstrained Features Model (UFM):** A theoretical framework where features are optimized without architectural constraints, serving as a benchmark for feature learning. Understanding UFM is crucial because it provides the target solution that deep networks approximate.

**Neural Collapse:** The phenomenon where features of the same class collapse to their mean while class means separate maximally. This is the key emergent behavior that the paper proves is globally optimal.

**Regularization Schemes:** Techniques like ridge regularization and slack variable methods that prevent trivial solutions and ensure convergence to meaningful optima. These are essential for making the theoretical analysis tractable.

**Global Optimality:** The property that a solution is optimal across all possible parameter configurations, not just locally. This stronger form of optimality is what the paper proves for deep networks.

**Approximation Theory:** The mathematical framework for analyzing how one function class can approximate another. This underlies the proof that deep networks can approximate UFM solutions.

## Architecture Onboarding

**Component Map:** Input features → Multiple ResNet/Transformer layers → Class means optimization → Output classification

**Critical Path:** The theoretical proof follows this sequence: define network output as optimization over features → show convergence of this optimization to UFM as depth increases → prove NC emerges at UFM optimum → validate experimentally

**Design Tradeoffs:** Depth vs. width tradeoff emerges, where deeper networks require fewer channels to achieve the same approximation quality. Regularization strength must balance between preventing trivial solutions and allowing sufficient flexibility.

**Failure Signatures:** If depth is insufficient or regularization is too weak, networks may not converge to NC. If channels are too few, approximation quality degrades regardless of depth.

**First Experiments:**
1. Train deep ResNets with varying depth on CIFAR-10 and measure NC metrics
2. Compare ridge regularization vs. slack variable regularization on convergence speed
3. Vary the number of channels to determine the minimum required for NC emergence

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical results assume infinite depth, which is impractical in real implementations
- Approximation quality depends on the number of channels, which is bounded in practice
- Results may not extend to other regularization schemes beyond those specifically analyzed
- Experimental validation is limited to a few datasets and may not generalize to all domains

## Confidence
- Theoretical framework soundness: High
- Practical applicability to finite-depth networks: Medium
- Generalization across different regularization schemes: Low
- Experimental validation comprehensiveness: Medium

## Next Checks
1. Evaluate the convergence to NC metrics on a wider range of datasets, including those with complex structures and noisy labels, to assess the universality of the findings.
2. Investigate the impact of different regularization schemes on the emergence of NC and the approximation quality of deep ResNets and transformers to the UFM.
3. Study the implications of NC for downstream tasks, such as robustness to adversarial examples and transfer learning performance, to understand the practical significance of NC in deep architectures.