---
ver: rpa2
title: 'DiSC-AMC: Token- and Parameter-Efficient Discretized Statistics In-Context
  Automatic Modulation Classification'
arxiv_id: '2510.00316'
source_url: https://arxiv.org/abs/2510.00316
tags:
- prompt
- accuracy
- modulation
- classification
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the computational inefficiency of large language
  model (LLM)-based automatic modulation classification (AMC) in wireless communication,
  which requires long prompts and large models. The authors propose DiSC-AMC, a token-
  and parameter-efficient method that discretizes higher-order statistics into compact
  symbolic tokens, prunes exemplar lists via a lightweight classifier, and enforces
  constrained label-only predictions.
---

# DiSC-AMC: Token- and Parameter-Efficient Discretized Statistics In-Context Automatic Modulation Classification

## Quick Facts
- arXiv ID: 2510.00316
- Source URL: https://arxiv.org/abs/2510.00316
- Reference count: 16
- Reduces prompt length and model size by more than half while maintaining competitive accuracy

## Executive Summary
This work addresses the computational inefficiency of large language model (LLM)-based automatic modulation classification (AMC) in wireless communication, which requires long prompts and large models. The authors propose DiSC-AMC, a token- and parameter-efficient method that discretizes higher-order statistics into compact symbolic tokens, prunes exemplar lists via a lightweight classifier, and enforces constrained label-only predictions. Evaluated on synthetic AMC with ten modulation types, DiSC-AMC achieves significant efficiency gains while maintaining competitive accuracy compared to traditional approaches.

## Method Summary
DiSC-AMC introduces a token- and parameter-efficient approach for LLM-based AMC by discretizing higher-order statistics into compact symbolic tokens. The method employs a lightweight classifier to prune exemplar lists, reducing computational overhead while maintaining classification accuracy. A key innovation is the constrained label-only prediction mechanism that eliminates the need for complex prompt engineering. The approach was evaluated on a synthetic AMC dataset with ten modulation types, demonstrating substantial reductions in both prompt length and model size requirements.

## Key Results
- Reduces prompt length and model size by more than half compared to traditional LLM-based AMC approaches
- Achieves 45.5% accuracy with a 5B-parameter Gemini-2.5-Flash model
- Outperforms a 7B baseline using prior approaches with only 5.2% accuracy

## Why This Works (Mechanism)
DiSC-AMC works by converting continuous statistical features into discrete symbolic tokens that are more efficiently processed by LLMs. The discretization preserves essential discriminative information while dramatically reducing token count. The pruning mechanism intelligently filters irrelevant exemplars, focusing the model's attention on the most informative samples. The constrained label-only prediction format simplifies the task for the LLM by removing ambiguity in output generation, allowing smaller models to achieve comparable performance to larger ones.

## Foundational Learning

**Higher-order statistics in AMC**: Statistical moments beyond mean and variance that capture signal characteristics. Needed to distinguish modulation types. Quick check: Compute kurtosis and skewness of different modulation schemes.

**Symbolic tokenization**: Converting numerical features into discrete tokens for efficient LLM processing. Required to reduce token count while preserving information. Quick check: Verify token distribution matches original feature space.

**Exemplar pruning strategies**: Methods to reduce training set size while maintaining classification performance. Essential for computational efficiency. Quick check: Measure accuracy drop with different pruning thresholds.

**Constrained generation**: Restricting LLM output to specific formats. Needed to ensure reliable classification predictions. Quick check: Test with and without constraints on output accuracy.

## Architecture Onboarding

Component map: Signal Processing -> Symbolic Discretization -> Lightweight Pruning -> LLM Prediction -> Output Constraint

Critical path: The symbolic discretization step is most critical as it directly impacts both efficiency and accuracy. Poor discretization leads to information loss and degraded performance.

Design tradeoffs: The method trades some classification accuracy for significant gains in computational efficiency. The choice of discretization granularity and pruning threshold represents the main optimization knobs.

Failure signatures: Performance degradation typically manifests as increased confusion between similar modulation types, particularly at lower SNR values. The lightweight classifier may also introduce errors if not properly calibrated.

First experiments:
1. Test discretization impact by varying the number of discrete levels while measuring accuracy and token count
2. Evaluate pruning effectiveness by comparing classification accuracy with different exemplar retention rates
3. Benchmark against classical ML methods (SVM, Random Forest) on the same dataset to establish relative performance

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Evaluation limited to synthetic dataset with ten modulation types, limiting real-world applicability
- Comparison against only one baseline (LLaMA-7B) provides insufficient context for claimed superiority
- Modest accuracy of 45.5% suggests fundamental limitations in the approach rather than just efficiency gains

## Confidence
- Efficiency improvements: Medium confidence (well-demonstrated but limited validation)
- Classification performance: Low confidence (modest accuracy, single dataset, limited baselines)

## Next Checks
1. Test on real-world radio datasets with varying SNR conditions and multipath fading to assess robustness
2. Compare against multiple state-of-the-art AMC approaches including both classical and deep learning methods to establish relative performance
3. Conduct ablation studies to quantify the impact of discretization granularity and pruning thresholds on accuracy versus efficiency trade-offs