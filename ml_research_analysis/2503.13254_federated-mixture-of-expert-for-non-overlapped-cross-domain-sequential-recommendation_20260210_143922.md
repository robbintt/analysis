---
ver: rpa2
title: Federated Mixture-of-Expert for Non-Overlapped Cross-Domain Sequential Recommendation
arxiv_id: '2503.13254'
source_url: https://arxiv.org/abs/2503.13254
tags:
- domain
- expert
- fmoe-cdsr
- local
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FMoE-CDSR, a federated learning framework
  for non-overlapped cross-domain sequential recommendation. The proposed method addresses
  the challenge of leveraging model parameters from other domains without requiring
  user overlap or sharing sensitive data like historical logs or embeddings.
---

# Federated Mixture-of-Expert for Non-Overlapped Cross-Domain Sequential Recommendation

## Quick Facts
- arXiv ID: 2503.13254
- Source URL: https://arxiv.org/abs/2503.13254
- Reference count: 29
- This paper introduces FMoE-CDSR, a federated learning framework for non-overlapped cross-domain sequential recommendation that achieves up to 13.62% MRR improvement over state-of-the-art methods.

## Executive Summary
This paper addresses the challenge of cross-domain sequential recommendation without user overlap or data sharing. The proposed FMoE-CDSR framework treats each domain's model as an expert and uses federated Mixture-of-Experts to adaptively aggregate knowledge across domains. The key innovation is freezing global expert parameters while training separate adaptation layers, preventing negative transfer across heterogeneous domains. Extensive experiments on Amazon datasets show significant improvements over baselines, with the method achieving 13.62% MRR and 23.07% NDCG@10 gains.

## Method Summary
FMoE-CDSR implements a federated learning framework where each domain trains a local sequential recommendation model with contrastive self-supervised objectives. Global experts from other domains are synchronized, frozen, and adapted via separate adapter parameters. A gate router dynamically fuses predictions using stop-gradient operations to ensure stable convergence. The framework consists of local domain expert learning, global expert adaptation through parameter synchronization, and dynamic knowledge fusion via the gate router. Training occurs over 40-60 federated rounds with 3 local epochs per round, maintaining gradient isolation between different experts within each domain.

## Key Results
- Achieves 13.62% MRR and 23.07% NDCG@10 improvements over state-of-the-art baselines on MBG dataset
- Shows 15% improvement in MRR (13.81 vs 15.88) when increasing source domain training epochs from 3 to 14
- Ablation studies confirm federated aggregation and MoE mechanism are essential, with freezing mechanism preventing negative transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing global expert parameters while training separate adaptation layers prevents negative transfer across heterogeneous domains
- Mechanism: When domain k downloads Experti from domain i, it freezes Experti's parameters but introduces trainable adaptation parameters. The contrastive and recommendation losses optimize only these adaptation parameters, allowing the global expert to fit domain k's distribution without corrupting its original knowledge.
- Core assumption: Domain-specific behavior patterns are sufficiently distinct that joint fine-tuning would cause interference
- Evidence anchors: [section 3.4] gradient isolation prevents interference; [section 4.5, Table 7] FMoE-CDSR-w/o Freeze drops from 6.63 to 6.52 MRR on SGH

### Mechanism 2
- Claim: The gate router's stop-gradient operation combined with softmax weighting enables dynamic expert selection without destabilizing individual expert convergence
- Mechanism: The gate takes concatenated expert outputs, computes importance weights via softmax network, and produces weighted prediction sum. Stop-gradient ensures each expert trains on its own loss independently.
- Core assumption: Different input sequences benefit from different expert combinations
- Evidence anchors: [section 3.5] stop-gradient ensures MoE doesn't affect expert convergence; [section 4.5, Table 6] removing gate router drops MBG performance from 13.62 to 13.53 MRR

### Mechanism 3
- Claim: Better-trained source domain experts transfer more effectively to target domains
- Mechanism: The federated paradigm creates a one-way dependency—domain k's performance depends on Experti's quality, but Experti trains independently. Longer training epochs in domain i produce more refined parameters that yield better target domain predictions.
- Core assumption: Sequential recommendation knowledge is partially transferable across non-overlapped domains
- Evidence anchors: [section 4.4, Table 5] "Local Expert + Global Expert(3 epochs)" achieves 13.81 MRR vs 15.88 for "(14 epochs)"—15% improvement from better source training

## Foundational Learning

- Concept: **Federated Learning with FedAvg**
  - Why needed here: FMoE-CDSR builds on FedAvg's client-server paradigm but addresses its limitation—simple parameter averaging loses domain-specific knowledge
  - Quick check question: Can you explain why FedAvg's parameter averaging would fail when merging a video recommendation model with an e-commerce model?

- Concept: **Mixture-of-Experts with Gating Networks**
  - Why needed here: The paper's core innovation is applying MoE to federated cross-domain transfer
  - Quick check question: If your gate router always outputs [0.33, 0.33, 0.33], what does this indicate about expert specialization?

- Concept: **Contrastive Learning for Sequential Recommendation**
  - Why needed here: Both local and global experts use augmentation contrastive objectives to enhance representation robustness
  - Quick check question: Why might random shuffling be a better augmentation for recommendation sequences than token masking in NLP?

## Architecture Onboarding

- Component map:
  ```
  [Domain k Local Data] 
        ↓
  [Embedding Layer: Sk, Sk,i, Sk,j + Position Encodings]
        ↓
  [Three Parallel Encoders]:
      Expertk (trainable) → zkT
      Expertk,i (frozen from ϕi) + Sk,i → zk,iT  
      Expertk,j (frozen from ϕj) + Sk,j → zk,jT
        ↓
  [Gate Router]: stop-gradient concat → Gatek(·) → gkT (softmax weights)
        ↓
  [Prediction Heads]: MLPk, MLPk,i, MLPk,j → okT+1, ok,iT+1, ok,jT+1
        ↓
  [Weighted Fusion]: ok,moeT+1 = WeightedSum(predictions, gkT)
        ↓
  [Losses]: Lkrec, Lkcon (local) | Lk,irec, Lk,icon (adaptation) | Lk,moerec (gate)
  ```

- Critical path:
  1. Initialize: Server creates Expertx0 caches for all domains
  2. Each round: Clients download other domains' experts as frozen global experts
  3. Local training: Update local expert + adaptation parameters + gate network (NOT global expert parameters)
  4. Upload: Send only local expert parameters to server
  5. Server aggregates and overwrites caches for next round

- Design tradeoffs:
  - Freezing vs. Fine-tuning global experts: Freezing prevents negative transfer but limits adaptation capacity—paper's ablation confirms freezing is net positive
  - Gate complexity vs. static weighting: Gate adds routing computation; ablation shows ~0.7% MRR improvement over uniform weights
  - Synchronization frequency: Paper uses 40-60 rounds with 3 local epochs; more frequent sync increases communication cost

- Failure signatures:
  - Gate collapse: If gkT converges to always selecting one expert, check learning rate separation
  - No improvement over local-only: Global expert adaptation parameters may be undertrained—verify Lk,icon and Lk,irec are optimizing
  - Performance drops after sync: Incoming global expert may be poorly trained; check source domain convergence

- First 3 experiments:
  1. **Baseline sanity check**: Run Local Expert variant to establish performance floor; compare against paper's 10.77 vs 13.62 MRR on MBG
  2. **Ablation path**: Remove one component at a time (gate router → freeze mechanism → contrastive loss) to verify each contributes as reported
  3. **Expert quality sensitivity**: Replicate Table 5 experiment—vary global expert pre-training epochs (3, 5, 10, 14) and measure target domain MRR

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can aggregation strategies be optimized for efficiency in this federated mixture-of-expert paradigm?
- **Basis in paper:** [explicit] The Conclusion states, "In the future, we plan to explore more efficient aggregation strategies."
- **Why unresolved:** The current implementation involves synchronizing full model parameters between the server and clients, which may be communication-heavy.
- **What evidence would resolve it:** A study comparing convergence speed and communication costs of various compression or sparsification techniques applied to the FMoE-CDSR framework.

### Open Question 2
- **Question:** Can FMoE-CDSR be successfully extended to non-sequential or industrial-scale recommendation scenarios?
- **Basis in paper:** [explicit] The Conclusion proposes to "extend our framework to broader recommendation scenarios."
- **Why unresolved:** The current experimental validation is restricted to sequential recommendation tasks using Amazon datasets.
- **What evidence would resolve it:** Evaluation of the framework on diverse tasks such as collaborative filtering, CTR prediction, or domains with significantly different data distributions.

### Open Question 3
- **Question:** How does performance and communication overhead scale when the number of participating domains significantly increases?
- **Basis in paper:** [inferred] The experiments are limited to scenarios with only three or four domains, and the method requires synchronizing separate expert models for every domain.
- **Why unresolved:** Downloading N-1 expert models to every client may become computationally and communicatively prohibitive as N grows.
- **What evidence would resolve it:** Experiments simulating environments with 10+ domains, analyzing bandwidth usage, local storage limits, and prediction accuracy.

## Limitations

- The framework assumes users have long enough sequences (≥10 interactions) for contrastive augmentation to be effective, potentially excluding cold-start users
- Experiments cover only e-commerce domains (Books, Movies, Games, Kitchen) with similar interaction patterns, limiting claims about cross-domain transfer between highly dissimilar domains
- The method requires synchronizing N-1 expert models to every client, which may become computationally and communicatively prohibitive as the number of domains grows

## Confidence

- **High**: Core federated MoE mechanism and parameter freezing preventing negative transfer (confirmed by ablation Table 7)
- **Medium**: Claim that gate router provides meaningful routing benefits (small but consistent gains in Table 6)
- **Medium**: Transfer quality dependency on source expert training (confirmed by epoch sensitivity in Table 5, but within narrow domain range)

## Next Checks

1. Test FMoE-CDSR on domains with minimal behavioral overlap (e.g., Fashion→Electronics) to verify transfer benefits hold beyond similar e-commerce categories
2. Implement ablation removing the gate router entirely (not just replacing with MLP) to measure routing complexity cost-benefit trade-off
3. Measure gate router entropy over training to verify it learns meaningful routing rather than collapsing to uniform weights