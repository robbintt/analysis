---
ver: rpa2
title: Uncovering inequalities in new knowledge learning by large language models
  across different languages
arxiv_id: '2503.04064'
source_url: https://arxiv.org/abs/2503.04064
tags:
- knowledge
- languages
- what
- llms
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates inequalities in large language models'
  (LLMs) ability to learn new knowledge across different languages. Using two datasets
  (fictional future knowledge and common-sense questions), experiments were conducted
  with GPT-4o-mini and Llama-3.1-8B under both in-context learning and fine-tuning
  settings.
---

# Uncovering inequalities in new knowledge learning by large language models across different languages

## Quick Facts
- arXiv ID: 2503.04064
- Source URL: https://arxiv.org/abs/2503.04064
- Reference count: 0
- LLMs learn new knowledge less effectively, transfer knowledge less easily, prioritize high-resource language knowledge, and resist errors less robustly in low-resource languages compared to high-resource languages.

## Executive Summary
This study investigates inequalities in large language models' (LLMs) ability to learn new knowledge across different languages. Using two datasets (fictional future knowledge and common-sense questions), experiments were conducted with GPT-4o-mini and Llama-3.1-8B under both in-context learning and fine-tuning settings. Results show that LLMs learn new knowledge less effectively, transfer knowledge less easily, prioritize high-resource language knowledge, and resist errors less robustly in low-resource languages compared to high-resource languages. These findings highlight persistent inequalities in LLMs' multilingual capabilities during the dynamic knowledge acquisition process.

## Method Summary
The study evaluated four dimensions of new knowledge learning (Effectiveness, Transferability, Prioritization, Robustness) across 17 languages (10 high-resource, 7 low-resource). Two datasets were used: a fictional dataset with 100 question-answer pairs about a future world, and a common-sense dataset with 50 QA pairs containing correct and incorrect answers. Experiments were conducted using GPT-4o-mini and Llama-3.1-8B under both in-context learning and fine-tuning settings, with accuracy measured by GPT-4o-mini as judge.

## Key Results
- Low-resource languages show systematically lower accuracy and slower convergence rates in new knowledge acquisition
- Knowledge acquired by LLMs transfers more easily to high-resource languages than to low-resource languages
- When English and Tamil provide contradictory answers, models prefer English 70%+ of the time regardless of query language
- Error resistance collapses rapidly in low-resource languages (40%→5% accuracy after 1 epoch in Turkmen)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-resource languages face systemic disadvantages in new knowledge acquisition due to under-representation in pre-training corpora.
- Mechanism: Languages comprising <0.1% of CommonCrawl data have insufficient neural representations, causing slower convergence, lower accuracy ceilings, and weaker error resistance.
- Core assumption: Pre-training data distribution creates durable linguistic hierarchies that persist through subsequent adaptation.
- Evidence anchors: Low-resource languages consistently face disadvantages across all four dimensions; languages accounting for <0.1% of data are considered low-resource; related work confirms performance gaps for low-resource languages.
- Break condition: If models pre-trained on balanced multilingual corpora still exhibit these gaps, mechanism is incomplete.

### Mechanism 2
- Claim: Language-specific neurons are under-trained for low-resource languages, limiting knowledge encoding capacity.
- Mechanism: LLMs contain language-specific regions that are under-trained for low-resource languages, reducing their ability to encode new knowledge efficiently.
- Core assumption: The language-specific/language-agnostic neuron hypothesis accurately describes LLM architecture.
- Evidence anchors: Inequalities may stem from under-training of neurons corresponding to low-resource languages; activation pattern differences across languages have been observed.
- Break condition: If interventions targeting language-specific neurons fail to reduce disparities, mechanism is incorrect.

### Mechanism 3
- Claim: Cross-lingual knowledge transfer is asymmetric, favoring high-resource target languages.
- Mechanism: Knowledge acquired in any language transfers more readily to high-resource languages because their neural representations are more robustly developed.
- Core assumption: Transfer quality depends on target language representation strength, not just source language.
- Evidence anchors: New knowledge acquired by LLMs can be more easily transferred to high-resource languages than to low-resource languages; knowledge transfers more seamlessly among linguistically similar high-resource languages.
- Break condition: If fine-tuning on low-resource target data equalizes transfer, mechanism may reflect insufficient adaptation rather than architectural constraints.

## Foundational Learning

- Concept: **Resource-level classification in NLP**
  - Why needed here: The paper defines low-resource as <0.1% of CommonCrawl data, not speaker population. Tamil (86.7M speakers) is low-resource; Italian (66.8M) is high-resource.
  - Quick check question: If a language has 50M speakers but minimal web presence, is it high or low resource for LLMs?

- Concept: **In-context learning vs. fine-tuning**
  - Why needed here: The paper evaluates both settings. In-context learning adds knowledge to prompts without parameter updates; fine-tuning modifies weights.
  - Quick check question: Which approach showed higher accuracy in transfer experiments, and why might that be?

- Concept: **Cross-lingual consistency**
  - Why needed here: Knowledge conflict experiments reveal that when English and Tamil provide contradictory answers, models prefer English 70%+ of the time regardless of query language.
  - Quick check question: What metric would you use to measure whether a model treats conflicting knowledge from two languages equally?

## Architecture Onboarding

- Component map: Pre-training corpus -> Language identification -> Language-specific neuron activation -> Knowledge encoding/transfer -> Response generation
- Critical path: 1. Language identification → 2. Language-specific neuron activation → 3. Knowledge encoding/transfer → 4. Response generation
- Design tradeoffs:
  - Fictional knowledge dataset avoids pre-training contamination but may not reflect real-world knowledge patterns
  - Translation via Google Translate introduces potential quality variation (back-translation showed 90-98% similarity)
  - 17 languages (10 high, 7 low) balances coverage with experiment cost
- Failure signatures:
  - Convergence but at lower accuracy ceiling (80% vs. 90%+)
  - Near-zero transfer to low-resource languages (19% accuracy for English→Zulu)
  - Rapid accuracy collapse under misinformation (40%→5% after 1 epoch in Turkmen)
  - High-resource preference in conflicts (70%+ consistency with high-resource answer)
- First 3 experiments:
  1. Replicate the effectiveness experiment: fine-tune GPT-4o-mini on fictional knowledge in a high-resource (e.g., Swedish) vs. low-resource (e.g., Welsh) language; plot accuracy over 12 epochs.
  2. Test transfer asymmetry: fine-tune on English, query in Zulu; fine-tune on Zulu, query in English. Compare accuracy deltas.
  3. Probe conflict prioritization: create 10 conflicting knowledge pairs between English and Tamil; query in a neutral third language (e.g., Chinese) and measure response alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the underlying neurological or architectural mechanisms driving disparities in how LLMs process and store new knowledge for low-resource versus high-resource languages?
- **Basis in paper:** The authors state, "Our study does not deeply explore the mechanisms behind these multilingual disparities," and suggest that inequalities may stem from "the under-training of neurons corresponding to low-resource languages."
- **Why unresolved:** This study focused on behavioral outcomes rather than internal model interpretability or neuron-level analysis.
- **What evidence would resolve it:** An analysis of language-specific versus language-agnostic neuron activation patterns during the fine-tuning or in-context learning phases.

### Open Question 2
- **Question:** How can developers construct efficient strategies to eliminate linguistic inequalities in new knowledge learning?
- **Basis in paper:** The authors acknowledge, "We have not yet proposed effective solutions to eliminate these inequalities," and note that existing methods like multilingual instruction tuning face "limitations in scalability and effectiveness."
- **Why unresolved:** The paper serves as an audit of the problem rather than a proposal for a technical fix.
- **What evidence would resolve it:** A novel training paradigm or data augmentation technique that results in statistically similar convergence speeds and robustness scores for both high- and low-resource languages.

### Open Question 3
- **Question:** Do the observed inequalities in learning fictional knowledge generalize to real-world, temporally evolving knowledge?
- **Basis in paper:** The authors note a limitation regarding their dataset: "We relied on fictional new knowledge. Collaboration with model developers in the future could help identify real-world examples of new knowledge for testing."
- **Why unresolved:** Because LLM pre-training datasets are undisclosed, the authors could not verify "newness" of real facts, forcing them to use hypothetical scenarios.
- **What evidence would resolve it:** Experiments using a "time-travel" dataset consisting of factual events that occurred strictly after the models' training cutoff dates.

## Limitations

- Pre-training data distribution is inferred from historical CommonCrawl statistics rather than directly measured for the specific models studied
- Language-specific neuron mechanisms remain hypothetical, based on related work rather than direct experimental validation
- Translation quality variation across languages could introduce confounds, though back-translation checks suggest minimal impact

## Confidence

- **High confidence**: Effectiveness findings - supported by consistent patterns across multiple experiments and datasets
- **Medium confidence**: Transfer asymmetry and prioritization claims - directional effects are clear but underlying mechanisms remain uncertain
- **Medium confidence**: Robustness findings - error resistance patterns are evident but speed of collapse varies considerably

## Next Checks

1. **Direct causal validation**: Pre-train or fine-tune a multilingual model on a balanced corpus where low-resource languages receive equal representation, then repeat the effectiveness and transfer experiments to test whether data distribution alone explains the disparities.

2. **Neuron-level intervention**: Use sparse autoencoders to identify and selectively enhance language-specific neurons for low-resource languages, then measure whether this intervention reduces the observed inequalities in knowledge acquisition and transfer.

3. **Real-world knowledge replication**: Validate the fictional knowledge findings using a carefully constructed real-world knowledge dataset with ground-truth verification independent of LLM-based evaluation, to ensure the effects generalize beyond synthetic scenarios.