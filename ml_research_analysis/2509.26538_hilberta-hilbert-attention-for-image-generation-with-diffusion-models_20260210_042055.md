---
ver: rpa2
title: 'HilbertA: Hilbert Attention for Image Generation with Diffusion Models'
arxiv_id: '2509.26538'
source_url: https://arxiv.org/abs/2509.26538
tags:
- attention
- hilberta
- image
- sparse
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HilbertA introduces a 2D-aware sparse attention mechanism for diffusion
  transformers that preserves spatial locality while ensuring GPU memory efficiency.
  It reorders image tokens along Hilbert curves to maintain spatial neighborhoods
  in contiguous memory, partitions tokens into tiles for local attention, and employs
  a sliding schedule across layers to enable long-range information propagation without
  uncoalesced memory access.
---

# HilbertA: Hilbert Attention for Image Generation with Diffusion Models

## Quick Facts
- arXiv ID: 2509.26538
- Source URL: https://arxiv.org/abs/2509.26538
- Reference count: 33
- Primary result: Up to 2.3× attention and 1.51× end-to-end speedup on 1024×1024 and 2048×2048 images while maintaining comparable quality

## Executive Summary
HilbertA introduces a 2D-aware sparse attention mechanism for diffusion transformers that preserves spatial locality while ensuring GPU memory efficiency. It reorders image tokens along Hilbert curves to maintain spatial neighborhoods in contiguous memory, partitions tokens into tiles for local attention, and employs a sliding schedule across layers to enable long-range information propagation without uncoalesced memory access. Additionally, it incorporates a small central shared region to enhance cross-tile communication and positional awareness. Implemented in Triton, HilbertA achieves significant speedups while maintaining image quality comparable to or surpassing baselines.

## Method Summary
HilbertA reorders image tokens along Hilbert curves to achieve contiguous memory layout while preserving spatial neighborhoods, partitions tokens into tiles for local attention, and employs a sliding schedule across layers to enable long-range information propagation without repeated or uncoalesced memory access. The method also incorporates a small central shared region to enhance cross-tile communication and positional awareness. Implemented in Triton, it achieves up to 2.3× attention speedup and 1.10× end-to-end acceleration at 1024×1024 resolution, and up to 4.17× attention and 1.51× end-to-end speedup at 2048×2048, while maintaining image quality comparable to or surpassing baselines.

## Key Results
- Up to 2.3× attention speedup and 1.10× end-to-end acceleration at 1024×1024 resolution
- Up to 4.17× attention and 1.51× end-to-end speedup at 2048×2048 resolution
- Maintains image quality comparable to or surpassing dense attention baselines (FID, LPIPS, CLIP-I metrics)

## Why This Works (Mechanism)

### Mechanism 1: Hilbert-Curve Token Reordering
Reordering image tokens along a Hilbert curve preserves 2D spatial locality while enabling contiguous GPU memory access. The Hilbert curve is a space-filling fractal that maps 2D grid coordinates to 1D sequence positions. Unlike row-major or Morton (Z-order) traversals, the Hilbert curve minimizes both Edge Average Stretch (EAS)—the average sequence separation of spatial neighbors—and Geometric Distortion Error (GDE)—the mismatch between sequence and true spatial distances after rescaling. This dual property ensures that tokens within any contiguous sequence segment form spatially coherent 2D neighborhoods.

### Mechanism 2: Tile-and-Slide Information Propagation
Partitioning the Hilbert-ordered sequence into tiles for local attention, combined with a fixed-offset sliding schedule across layers, enables cross-tile information exchange without uncoalesced memory access. Tiles are defined as contiguous non-overlapping segments of the reordered sequence. At each layer ℓ, the attention window for each token shifts by Δ = N_T / L tokens (where N_T is tile size, L is sliding cycle length). After L layers, the window has advanced by one full tile. Tokens entering a new tile act as "messengers," carrying aggregated information from their prior tile. This creates an Effective Receptive Field (ERF) that grows by one tile per layer, eventually covering the full sequence after T layers (where T = N / N_T).

### Mechanism 3: Central Shared Region for RoPE Anchoring
A fixed, centrally-located shared region that all tiles attend to provides both global information relay and positional grounding under Rotary Positional Embeddings. The shared region is positioned at the image center and attended to by every tile alongside local attention. It serves two roles: (1) as a broadcast/aggregation hub for cross-tile context, and (2) as a positional reference point that implicitly encodes each tile's location relative to a fixed anchor. Under RoPE—which encodes relative positions—this shared region ensures tiles have a common reference, improving structural consistency.

## Foundational Learning

- **Space-filling curves (Hilbert, Morton/Z-order)**: Understanding why Hilbert curves outperform alternatives requires grasping how these curves map 2D grids to 1D sequences and the tradeoffs between locality preservation and implementation complexity. Quick check: Given a 4×4 grid, sketch the Hilbert curve traversal and compare it to row-major order. Which better preserves the neighbors of cell (1,1)?

- **Coalesced vs. uncoalesced GPU memory access**: The core efficiency claim hinges on avoiding uncoalesced reads. Without this concept, one cannot evaluate why CLEAR's overlapping windows underperform despite higher theoretical sparsity. Quick check: In GPU terminology, what makes a memory access pattern "coalesced," and why does strided access (e.g., reading every 64th element) degrade performance?

- **Effective Receptive Field (ERF) in stacked attention layers**: The sliding mechanism's theoretical guarantee depends on ERF growth through sequential attention. Understanding ERF clarifies why local attention can achieve global coverage given sufficient depth. Quick check: If each attention layer restricts queries to attend to a window of size W tokens, how many layers are needed for a token at position 0 to indirectly attend to position 4W?

## Architecture Onboarding

- **Component map**: Reordering stage -> Tiling stage -> Sliding stage -> Shared region integration -> Triton kernel execution
- **Critical path**: 1. Precompute Hilbert bijection for target resolution (offline, one-time). 2. At inference start: gather image tokens along Hilbert order. 3. Per denoising step: run Triton sparse attention kernel across all transformer layers, with per-layer sliding offset. 4. After final layer: scatter tokens back to original raster order for VAE decoding.
- **Design tradeoffs**: Tile count vs. quality: 16 tiles → higher sparsity (94%), faster, but lower FID/CLIP-I. 4 tiles → lower sparsity (75%), slower, but better quality. Shared region size: Larger shared regions improve coherence but reduce sparsity. Sliding cycle length: L=2 vs. L=4 yields negligible quality differences.
- **Failure signatures**: Tile-boundary artifacts: Faint seams on flat backgrounds. Global incoherence: Mismatched lighting across regions. No speedup despite sparsity: Check for uncoalesced access in the kernel.
- **First 3 experiments**: 1. Sanity check: On a 256×256 image, visualize the Hilbert reordering by plotting token indices as colors. 2. Micro-benchmark: Measure attention latency for tile sizes [64, 128, 256, 512, 1024] at fixed sequence length 4096. 3. Ablation on shared region: Generate 100 images at 1024×1024 with shared region sizes [0, 64, 256, 1024].

## Open Questions the Paper Calls Out

### Open Question 1
Can a 3D Hilbert curve extension effectively preserve both spatial locality and temporal continuity in video diffusion transformers while maintaining contiguous memory access? The paper regards this extension as a promising direction for future work for 3D Hilbert curves in video generation, but extending to 3D spatiotemporal volumes introduces additional complexity in tile partitioning and requires elevating the shared region to three dimensions under 3D RoPE.

### Open Question 2
What is the optimal strategy for placing and sizing shared regions to eliminate boundary artifacts while preserving memory efficiency? The current fixed central shared region does not fully prevent cross-tile mismatches at boundaries. The paper suggests mitigations including distributing shared regions along edges or using interleaved tiling, but does not validate these.

### Open Question 3
Does HilbertA generalize to diffusion transformer architectures beyond Flux.1-dev, and do the efficiency-quality trade-offs remain consistent across different model designs? All experiments use Flux.1-dev only. The interaction between Hilbert reordering and architectural specifics may affect the locality-preservation and memory-access benefits differently across models.

## Limitations

- **Uncoalesced Memory Access Risks**: The paper claims contiguous memory access via Hilbert reordering, but lacks low-level GPU profiling to confirm actual memory coalescing patterns. Implementation-dependent behavior across GPU architectures remains unverified.

- **Shared Region Design Gaps**: The central shared region's placement in Hilbert order is not precisely defined. For rectangular inputs, the center mapping to Hilbert indices is ambiguous. The assumption that a single central region suffices for global coordination lacks empirical validation across diverse image types.

- **Long-Range Propagation Assumptions**: The tile-and-slide mechanism assumes information propagates adequately through sequential tile relays. However, the paper does not provide empirical analysis of Effective Receptive Field growth across layers or demonstrate that cross-tile communication is sufficient for maintaining global coherence in complex scenes.

## Confidence

**High Confidence**: The theoretical framework for Hilbert curve locality preservation (Edge Average Stretch, Geometric Distortion Error) is well-established in spatial indexing literature.

**Medium Confidence**: The reported efficiency gains are based on implemented Triton kernels, but lack independent verification. The trade-off between tile count and quality is empirically demonstrated but could vary with different model architectures.

**Low Confidence**: The shared region mechanism's effectiveness for positional awareness under RoPE is theoretically justified but lacks comparative ablation studies. The claim that a single central region provides sufficient global coordination for all image types is weakly supported.

## Next Checks

1. **Memory Access Pattern Profiling**: Use Nsight Compute or equivalent profiling tools to measure actual memory coalescing efficiency of the HilbertA kernel. Compare uncoalesced read/write percentages against dense attention baseline and CLEAR.

2. **ERF Growth Analysis**: Instrument the model to visualize Effective Receptive Field expansion across layers for different tile counts (4 vs. 16). Track how many layers are needed for a token to indirectly attend to all other tokens and identify any bottlenecks in information propagation.

3. **Shared Region Ablation Study**: Generate systematic ablations varying shared region size [0, 64, 256, 1024] and position (center vs. corner vs. distributed). Evaluate FID, LPIPS, and CLIP-I on COCO Val-5k for each configuration. Test on structurally diverse images to determine if a single central region provides sufficient global coordination.