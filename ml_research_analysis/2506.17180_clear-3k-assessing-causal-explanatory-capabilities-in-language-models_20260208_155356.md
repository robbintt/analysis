---
ver: rpa2
title: 'CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models'
arxiv_id: '2506.17180'
source_url: https://arxiv.org/abs/2506.17180
tags:
- causal
- explanatory
- 'true'
- language
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLEAR-3K, a dataset of 3,000 assertion-reasoning
  questions designed to assess whether language models can distinguish between semantic
  relatedness and genuine causal explanatory relationships. The dataset spans eight
  subjects across grades 9-12, with questions reformulated into a causal explanation
  task that asks models to determine if a reason explains an assertion.
---

# CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models

## Quick Facts
- arXiv ID: 2506.17180
- Source URL: https://arxiv.org/abs/2506.17180
- Authors: Naiming Liu; Richard Baraniuk; Shashank Sonkar
- Reference count: 38
- Key outcome: Language models frequently confuse semantic similarity with genuine causal explanation, with MCC performance plateauing at just 0.55 even for the largest models

## Executive Summary
CLEAR-3K introduces a 3,000-question dataset designed to test whether language models can distinguish between semantic relatedness and genuine causal explanatory relationships. The dataset spans eight subjects across grades 9-12, reformulated into a binary causal explanation task. Evaluation of 21 models (0.5B–72B parameters) reveals that models consistently rely on lexical and semantic overlap as a proxy for causal inference, leading to systematic errors. As parameter size increases, models shift from being overly skeptical to excessively permissive, yet the balanced performance metric (MCC) plateaus at 0.55, suggesting fundamental limitations in current architectures' ability to reason about causation.

## Method Summary
The study evaluates 21 language models across five families (LLaMA3, Qwen2.5, Qwen3, Gemma3, Phi-4) on CLEAR-3K, a dataset of 3,008 assertion-reasoning questions from high school curricula. Questions are reformulated from a 4-category assertion-reasoning format into a binary causal explanation task where models must determine if a reason causally explains an assertion. Models are evaluated using zero-shot inference with a JSON prompt format, measuring Explanatory Accuracy, Rejection Accuracy, and Matthews Correlation Coefficient (MCC). The study also analyzes semantic similarity between assertion-reason pairs to diagnose whether models rely on surface-level features rather than genuine causal inference.

## Key Results
- MCC performance plateaus at 0.55 across model families, even for the largest 70B-72B parameter models
- Models systematically confuse semantic similarity with causal explanation, with similarity explaining ~10% of prediction error variance
- As parameter size increases, models shift from excessive skepticism (small models: 12% explanatory / 87% rejection) to excessive permissiveness (large models: 85% explanatory / 64% rejection)
- Factual verification works well (94% rejection when facts are false) but fails to distinguish valid from invalid explanations among true statements

## Why This Works (Mechanism)

### Mechanism 1: Semantic Similarity Proxy
- Claim: Models substitute surface-level semantic overlap for genuine causal inference when evaluating explanatory relationships.
- Mechanism: Higher lexical/semantic similarity between assertion and reason increases probability of "yes" predictions regardless of actual causal validity. Statistical analysis shows similarity explains ~10% of error variance (pseudo R² = 0.097).
- Core assumption: Models lack internal causal world models and default to distributional associations learned during pretraining.
- Evidence anchors:
  - [abstract] "language models frequently confuse semantic similarity with causality, relying on lexical and semantic overlap instead of inferring actual causal explanatory relationships"
  - [section 4.3] Confusion matrix shows 0.590 similarity for true positives vs. 0.547 for true negatives; false positives show 0.571 vs. 0.547 for correct rejections
  - [corpus] Related work confirms LLMs "rely on superficial correlations rather than genuine understanding" (arXiv:2509.17380)
- Break condition: If models developed explicit causal reasoning modules or were trained on counterfactual examples that decouple similarity from causation, this proxy mechanism would weaken.

### Mechanism 2: Scale-Dependent Response Bias Shift
- Claim: Increasing parameter size shifts models from excessive skepticism to excessive permissiveness without improving causal discrimination.
- Mechanism: Larger models become more sensitive to semantic relationships, increasing true positive AND false positive rates. MCC plateaus because gains in explanatory accuracy are offset by losses in rejection accuracy.
- Core assumption: Scaling amplifies pattern-matching capabilities without fundamentally altering the architecture's causal inference mechanism.
- Evidence anchors:
  - [abstract] "as parameter size increases, models tend to shift from being overly skeptical about causal relationships to being excessively permissive"
  - [section 4.2.1] LLaMA3-1B: 12% explanatory / 87% rejection → LLaMA3-70B: 85% explanatory / 64% rejection; MCC stays ~0.50–0.55
  - [corpus] Weak direct evidence on bias shift mechanism; corpus focuses on causal reasoning benchmarks, not scale dynamics
- Break condition: If scaling laws for causal reasoning diverge from semantic sensitivity, or if training objectives explicitly penalize false positives, the shift pattern could change.

### Mechanism 3: Factual Verification Gate
- Claim: Models successfully reject explanatory relationships when either statement contains factual errors, but this doesn't extend to distinguishing valid from invalid explanations among true statements.
- Mechanism: Models have learned that false statements cannot validly explain—factual inconsistency serves as reliable rejection signal. However, when both statements are true, models lack discriminative signal beyond semantic overlap.
- Core assumption: Factual verification and causal discrimination are partially decoupled capabilities in current architectures.
- Evidence anchors:
  - [section 3.1] "This pattern suggests that while language models can detect factual inconsistencies, they fundamentally rely on semantic similarity as a proxy for causal relationships"
  - [section 4.2.2] Qwen3-14B achieves 94% rejection when A or R is false; drops to distinguishing valid/invalid explanations among true statements with MCC ~0.55
  - [corpus] Related work on counterfactual reasoning (arXiv:2503.08051) suggests causal precedence principles are not inherently captured
- Break condition: If factual verification and causal reasoning share underlying representations, improving one might transfer to the other; current evidence suggests limited transfer.

## Foundational Learning

- **Concept: Matthews Correlation Coefficient (MCC)**
  - Why needed here: MCC is the primary evaluation metric because it handles class imbalance (non-explanatory cases outnumber explanatory 3:1) and accounts for true negatives, unlike F1. Understanding why MCC plateaus at 0.55 is essential for interpreting results.
  - Quick check question: Why would accuracy be misleading for this task given the answer distribution?

- **Concept: Correlation vs. Causation in Language**
  - Why needed here: The core finding is that models confuse semantic correlation (shared vocabulary, topical relatedness) with causal explanation. Distinguishing these requires understanding that "A and B are related" ≠ "B explains A."
  - Quick check question: Give an example where two statements are semantically related but lack explanatory relationship.

- **Concept: Assertion-Reasoning Question Format**
  - Why needed here: The dataset adapts an educational assessment format with four categories (a-d). The Causal Explanation Task reformulates this into binary classification, collapsing (b), (c), (d) into "no"—understanding this mapping is critical for interpreting results.
  - Quick check question: Why does the reformulation treat categories (b), (c), and (d) identically?

## Architecture Onboarding

- **Component map:**
  - Input: Assertion-Reason pairs (3,008 total across 8 subjects, grades 9-12)
  - Task reformulation: Binary "Does R explain A?" (yes/no) vs. original 4-way classification
  - Evaluation layer: Three metrics—Explanatory Accuracy, Rejection Accuracy, MCC
  - Analysis layer: Semantic similarity scoring between A-R pairs to diagnose reliance on surface features

- **Critical path:**
  1. Both statements true → model must distinguish semantic correlation from causation (hardest case)
  2. Either statement false → model should reject (factual verification gate)
  3. Semantic similarity analysis → reveals whether predictions correlate with surface features

- **Design tradeoffs:**
  - Smaller models: High rejection rates (conservative) → miss valid explanations
  - Larger models: High acceptance rates (permissive) → accept invalid explanations
  - Neither achieves balanced performance; MCC ~0.55 represents current ceiling

- **Failure signatures:**
  - Systematic correlation between semantic similarity and predictions (higher similarity → "yes")
  - Large gap between rejection accuracy when facts are false (90%+) vs. discrimination when both true (MCC 0.55)
  - a/b confusion in traditional format: misclassifying "both true, no explanation" as "both true, explains"

- **First 3 experiments:**
  1. **Baseline replication:** Run the Causal Explanation Task on Qwen3-14B or Phi-4-14B on CLEAR-3K to verify MCC ~0.55 and reproduce the semantic similarity correlation pattern
  2. **Counterfactual augmentation:** Test whether adding training examples with high semantic similarity but no causal relationship reduces false positive rates (tests if proxy mechanism can be weakened)
  3. **Subject-specific probing:** Evaluate performance breakdown by subject (Biology MCC=0.32 vs. Economics MCC=0.58) to identify whether domain knowledge or reasoning style affects causal discrimination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training innovations can decouple causal inference from semantic similarity to break the observed 0.55 MCC plateau?
- Basis in paper: [explicit] The conclusion states that current scaling approaches are insufficient and future progress requires innovations specifically targeting causal inference.
- Why unresolved: The evaluation shows that increasing parameter size merely shifts model bias from skepticism to permissiveness without improving the fundamental ability to distinguish causation from correlation.
- What evidence would resolve it: A modified model architecture or objective function that achieves significantly higher MCC (e.g., >0.7) on CLEAR-3K without relying on lexical overlap heuristics.

### Open Question 2
- Question: Can fine-tuning strategies be developed to balance the trade-off where smaller models are overly skeptical and larger models are excessively permissive?
- Basis in paper: [inferred] Results show a consistent shift in bias from rejection to acceptance as parameter size increases, yet the overall MCC remains low.
- Why unresolved: It is unclear if this bias shift is an inherent property of model capacity or a byproduct of current pre-training objectives that favor semantic association.
- What evidence would resolve it: A training regime that enables smaller models to achieve high Explanatory Accuracy or constrains larger models to maintain high Rejection Accuracy.

### Open Question 3
- Question: To what extent does the difficulty hierarchy observed in open-source models (e.g., Biology being harder than Math) result from training data distribution versus inherent domain complexity?
- Basis in paper: [inferred] The appendix notes significant performance variance across subjects, with Biology (MCC 0.32) proving harder than Math (MCC 0.53).
- Why unresolved: While the authors suggest multi-factor causality in biology makes it harder, they do not control for the volume of domain-specific pre-training data.
- What evidence would resolve it: Evaluating models trained on controlled corpora to isolate the effect of domain knowledge density from the complexity of causal reasoning required.

## Limitations
- The semantic similarity analysis shows correlation with model predictions but lacks a validated causal model explaining why models default to this proxy
- All evaluations use zero-shot inference; fine-tuning or few-shot approaches might yield different patterns
- Dataset covers only high school curriculum topics; results may not generalize to professional domains or novel scientific concepts

## Confidence
- High confidence: Models show systematic reliance on semantic similarity as a proxy for causal explanation; MCC plateau at 0.55 represents real architectural limitation
- Medium confidence: Scale-dependent shift from skepticism to permissiveness is consistently observed, but underlying mechanisms need further investigation
- Medium confidence: Factual verification gate hypothesis is supported by performance patterns but requires controlled experiments to confirm causal decoupling

## Next Checks
1. **Causal intervention experiment:** Manipulate semantic similarity while holding causal validity constant in synthetic assertion-reason pairs to establish whether similarity directly causes prediction changes, or merely correlates with them
2. **Cross-domain generalization test:** Evaluate CLEAR-3K performance on professional/complex scientific domains to determine if the semantic-similarity proxy behavior generalizes beyond high school curriculum
3. **Architectural ablation study:** Compare CLEAR-3K performance across model families with different attention mechanisms, activation functions, or training objectives to identify whether the semantic-similarity bias stems from specific architectural choices or is universal to current transformer designs