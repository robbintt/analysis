---
ver: rpa2
title: 'Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic'
arxiv_id: '2506.23875'
source_url: https://arxiv.org/abs/2506.23875
tags:
- order
- latexit
- permutation
- target
- orders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of discovering a learning-friendly
  order for target token sequences in Transformer decoders, particularly for arithmetic
  tasks. The core method idea leverages the observation that neural networks learn
  easier samples earlier in training: it trains a Transformer on a mixture of target
  sequences in different orders and identifies orders with faster loss drops as learning-friendly.'
---

# Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic

## Quick Facts
- arXiv ID: 2506.23875
- Source URL: https://arxiv.org/abs/2506.23875
- Reference count: 20
- This paper proposes a method to discover learning-friendly token orders for Transformer decoders by exploiting early-stage loss drops during training.

## Executive Summary
This paper addresses the problem of discovering a learning-friendly order for target token sequences in Transformer decoders, particularly for arithmetic tasks. The core method idea leverages the observation that neural networks learn easier samples earlier in training: it trains a Transformer on a mixture of target sequences in different orders and identifies orders with faster loss drops as learning-friendly. To handle the factorial growth of the permutation space, the paper proposes a two-stage hierarchical approach combining global block-level exploration with local refinement within blocks. Experiments on four order-sensitive arithmetic tasks (ReLU, Square-19, Index, and multiplication) demonstrate that this method successfully discovers learning-friendly orders out of billions of candidates, improving success rates from approximately 10% to near 100%. Notably, on the multiplication task, the method rediscovered the reverse-digit order previously reported in prior studies. The approach is effective for target lengths up to 13 tokens and works across challenging initialization scenarios, including fully random permutations and more structured block-restricted settings.

## Method Summary
The method trains a Transformer on a dataset where target sequences are presented in multiple different permutations. After a short training period (1-2 epochs), it evaluates the validation loss for each permutation individually. Permutations with the fastest early loss drops are considered "learning-friendly." To scale to long sequences, a two-stage hierarchical search is used: a global stage swaps blocks of tokens to find coarse structure, and a local stage refines ordering within blocks. The discovered order is then used to retrain a fresh model, achieving near-perfect success rates on order-sensitive arithmetic tasks.

## Key Results
- The method discovers learning-friendly orders for ReLU, Square-19, Index, and multiplication tasks, improving success rates from ~10% to near 100%.
- On the multiplication task, the method rediscovered the reverse-digit order previously reported in prior work.
- The hierarchical search successfully identifies optimal orders from billions of candidates using only thousands of evaluations.
- Learning-friendly orders yield sparser attention maps, but optimizing for sparsity directly fails to discover correct orders.

## Why This Works (Mechanism)

### Mechanism 1
Training on mixed-order data allows early-stage loss to identify learning-friendly token orderings. Deep networks exhibit "spectral bias"—they fit simple patterns before memorizing noise. When a Transformer is trained on sequences in multiple permutations simultaneously, the model preferentially learns orders where dependencies are local and causal. Loss drops faster for these orders during the first 1-2 epochs, creating a detectable signal. Core assumption: the training dynamics that produce easy-to-hard learning on instances generalize to easy-to-hard orderings of the same instance. Evidence: validation loss profiling after 1-2 epochs identifies forward order as easiest for ReLU task. Break condition: if orders differ only in late-token dependencies, early loss may not discriminate.

### Mechanism 2
A two-stage hierarchical search can identify optimal orders from billions of candidates using only thousands of evaluations. The global stage swaps token blocks (not individual tokens), reducing an L! space to roughly (K+1)! for depth K. This finds coarse structure (e.g., "end-of-sequence tokens should come last"). The local stage then permutes within blocks, refining token-level ordering. Each stage uses the same loss-profiling procedure. Core assumption: learning-friendly orders have contiguous blocks that are themselves order-friendly. Evidence: hierarchical search flow diagram shows block-level swapping followed by intra-block refinement. Break condition: if the optimal order is highly non-local, block-based search may prune it early.

### Mechanism 3
Learning-friendly orders yield sparser attention maps, but optimizing for sparsity directly fails to discover correct orders. When token order respects causal dependencies, each prediction attends primarily to a small set of predecessors. This reduces attention entropy. However, gradient-based optimization of a soft permutation matrix toward lower entropy collapses to local minima or causes information leakage from future tokens. Core assumption: sparsity is a symptom, not a driver, of learnability. Evidence: forward orders consistently yield lower attention entropy than reverse orders. Break condition: if tasks require dense attention, sparsity may not differentiate orders.

## Foundational Learning

- **Autoregressive decoding with causal masking**
  - Why needed here: The paper reorders decoder input tokens; you must understand that each token can only attend to earlier positions in the sequence.
  - Quick check question: If you reverse a target sequence, which token now has access to the most information during generation?

- **Symmetric group and permutation matrices**
  - Why needed here: The search space is formalized as $S_L$, the set of all L! permutations. A permutation $\pi$ is implemented via matrix multiplication $YP$.
  - Quick check question: For a sequence of length 4, how many distinct orderings exist? If you split it into 2 blocks of 2, how many block-level orderings are there?

- **Non-injective functions and ambiguity**
  - Why needed here: The paper's tasks (ReLU, Square-19) use non-injective recurrence—later values don't uniquely determine earlier ones. This makes reverse/random orders hard to learn.
  - Quick check question: If $y_i = \text{ReLU}(x_i + y_{i-1})$ and you know $y_i = 5$, can you uniquely recover $y_{i-1}$? Why or why not?

## Architecture Onboarding

- **Component map:**
  - Mixed-order dataset constructor -> Early-loss profiler -> Hierarchical search controller -> Retraining stage

- **Critical path:**
  1. Choose initial permutation set (random $P_r$ or block-restricted $P_b$)
  2. Run global stage: for depth $k = 1 \to K$, split sequences into $k$ blocks, evaluate $k!$ orderings per candidate, keep top survivors
  3. Run local stage: for block sizes $l = 2 \to \lfloor L/2 \rfloor$, refine intra-block orders
  4. Retrain final model on discovered order; evaluate success rate

- **Design tradeoffs:**
  - Batch size vs. discrimination: Larger batches (128 used) stabilize loss estimates but reduce per-order signal specificity
  - Depth K vs. compute: Global stage evaluates $(K+1)!$ permutations; deeper search covers more space but scales factorially
  - Block restriction: Initializing with $P_b$ enables scaling to $L=40$+, but assumes the true order respects some block structure

- **Failure signatures:**
  - All permutations have similar loss: Task may not be order-sensitive, or sequence too short
  - Discovered order changes every run: Loss signal too weak; increase epochs or validation samples
  - Retraining fails on discovered order: Order is locally optimal for early loss but not globally learnable

- **First 3 experiments:**
  1. Reproduce ReLU task (L=13): Train mixed-order model with $P_g$ (1 forward + 127 random). Confirm forward order achieves lowest validation loss after 1 epoch.
  2. Ablate search depth: Compare $K=2$ vs $K=6$ on Square-19 task. Measure final success rate after retraining.
  3. Test on real multiplication: Apply pipeline to Prod task with $L=10$. Verify that discovered order matches least-significant-digit-first.

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed hierarchical search method be adapted to handle target sequences of variable lengths rather than fixed-length sequences? Basis: The authors state in the Conclusion, "The extension to longer sequences and target sequences at a variable length will be future work." Why unresolved: The current formulation defines the permutation space $\Sigma_L$ and the hierarchical block splitting based on a fixed length $L$, which breaks down when $|Y|$ varies across samples. What evidence would resolve it: A modified framework that successfully discovers learning-friendly orders on a dataset with varying target lengths without requiring a fixed permutation matrix size.

### Open Question 2
Does the relationship between early-stage loss drop and learning-friendly orders hold for natural language reasoning tasks, or is it specific to the rigid structure of arithmetic? Basis: While the introduction generalizes to "chain of thought" in Transformers, all experiments are restricted to synthetic arithmetic tasks. Why unresolved: The "learning-friendly" property in arithmetic relies on explicit recursive dependencies; it is unclear if the loss-drop heuristic can identify semantic coherence in natural language where syntax is more flexible. What evidence would resolve it: Experiments applying the method to a textual reasoning dataset where the "ground truth" order is semantically defined.

### Open Question 3
To what extent does the discovered optimal order depend on the specific type of positional encoding used by the Transformer? Basis: The paper utilizes trainable random positional embeddings and cites prior work showing that relative-position embeddings significantly alter generalization in arithmetic. Why unresolved: If the model uses relative positional encodings, it may inherently learn to handle position dependencies better, potentially changing or flattening the landscape of "learning-friendly" orders. What evidence would resolve it: A comparison of the discovered orders and success rates when training identical models equipped with absolute vs. relative positional encodings.

## Limitations
- Generalizability beyond synthetic arithmetic tasks: Validated only on four synthetic order-sensitive tasks, not on real-world applications.
- Sensitivity to architectural choices: Experiments use a small GPT-2; effectiveness may degrade for larger models or different architectures.
- Reproducibility of early-loss profiling: Method relies on detecting small differences in validation loss after only 1-2 epochs, which could be noisy.

## Confidence
- **Claim: Learning-friendly orders can be discovered automatically for order-sensitive arithmetic tasks.** Confidence: **High**. Experimental results show consistent success rates improving from ~10% to near 100%.
- **Claim: The two-stage hierarchical search is necessary and effective for scaling to billions of permutations.** Confidence: **Medium**. The approach is novel and effective for tested tasks, but no ablation is provided for alternative search strategies.
- **Claim: Sparsity is a symptom, not a driver, of learnability.** Confidence: **Low**. While optimizing for sparsity via soft permutations fails, it doesn't rule out other sparsity-based objectives.

## Next Checks
1. Reproduce the ReLU task (L=13) with ablation on profiling epochs. Run the loss-profiling stage for E=1, 2, and 3 epochs and compare the ranking of permutations.
2. Test the method on a real-world order-sensitive task. Apply the pipeline to a structured prediction task such as sorting or addition of multi-digit numbers in standard order.
3. Scale the hierarchical search to L=20+ with variable block sizes. Run the global stage with K=8 on a task like Square-19 (L=20) and measure success rate.