---
ver: rpa2
title: 'Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text
  and Image'
arxiv_id: '2512.16899'
source_url: https://arxiv.org/abs/2512.16899
tags:
- image
- response
- text
- reasoning
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Multimodal RewardBench 2 (MMRB2) addresses the lack of comprehensive\
  \ evaluation for reward models on omni models handling interleaved text and images.\
  \ The benchmark spans four tasks\u2014text-to-image generation, image editing, interleaved\
  \ generation, and multimodal reasoning\u2014with 1,000 expert-annotated preference\
  \ pairs per task."
---

# Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image

## Quick Facts
- arXiv ID: 2512.16899
- Source URL: https://arxiv.org/abs/2512.16899
- Reference count: 40
- Primary result: MMRB2 benchmark shows latest models achieve 66-80% judge-human agreement on multimodal preference pairs, compared to >90% for humans.

## Executive Summary
Multimodal RewardBench 2 (MMRB2) addresses the lack of comprehensive evaluation for reward models on omni models handling interleaved text and images. The benchmark spans four tasks—text-to-image generation, image editing, interleaved generation, and multimodal reasoning—with 1,000 expert-annotated preference pairs per task. MMRB2 uses challenging prompts, responses from state-of-the-art models and agents, and high-consensus preference pairs curated via ensemble filtering. Evaluation of existing judges shows that the latest Gemini 3 Pro achieves 75-80% accuracy, while GPT-5 and Gemini 2.5 Pro reach 66-75%, compared to >90% for humans. GPT-4o, commonly used for evaluation, attains only 59% accuracy. The best open-source model, Qwen3-VL-32B, matches Gemini 2.5 Flash at 64%. MMRB2 performance strongly correlates with downstream task success, validating its utility. Analysis reveals judges agree more with humans on different-model pairs than same-model pairs, and show bias toward image-containing responses in multimodal reasoning. MMRB2 establishes a challenging benchmark for advancing omni reward modeling.

## Method Summary
MMRB2 constructs a benchmark for evaluating omni reward models through a multi-stage pipeline: (1) prompt curation from 21 source tasks via stratified sampling to create 4 task categories with 1,000 prompts each, (2) response generation from 7-11 state-of-the-art models plus specialized agents for interleaved and reasoning tasks, (3) ensemble filtering using 9 diverse judges to remove easy pairs (90%+ agreement threshold), (4) human annotation by 3 annotators using 7-point Likert scales, (5) pair construction with quality filters (rating spread ≤4, midpoint exclusion), and (6) judge evaluation using positional consistent dual evaluation where each pair is evaluated in both orderings. The primary metric is judge-human agreement accuracy computed from binary preference comparisons.

## Key Results
- Latest models achieve 75-80% accuracy on MMRB2, compared to >90% for humans
- GPT-4o, commonly used for evaluation, attains only 59% accuracy
- Best open-source model (Qwen3-VL-32B) matches Gemini 2.5 Flash at 64%
- Judges perform 5-13% better on different-model pairs than same-model pairs
- Strong correlation (>0.8 Pearson's r) between MMRB2 performance and downstream task success via Best-of-N sampling

## Why This Works (Mechanism)

### Mechanism 1: Ensemble Filtering for Hard-Example Curation
- Claim: Filtering out easy preference pairs where 90%+ of diverse judges agree focuses human annotation on informative comparisons that better discriminate reward model quality.
- Mechanism: Nine multimodal judges (API + open-source) evaluate each pair in both orderings. Pairs with ≥90% majority agreement are discarded before human review, leaving only ambiguous cases where fine-grained judgment matters.
- Core assumption: High-agreement pairs provide minimal signal for differentiating reward model capabilities; disagreement correlates with task difficulty.
- Evidence anchors:
  - "We define easy pairs as those where the majority label appears in at least 90% of all judge evaluations across both orderings, and discard them because they provide little signal"
  - After filtering, "inter-annotator agreement on these filtered pairs is high: 95.6% overall"
  - Related work Omni-Reward notes modality imbalance in existing RMs, supporting need for targeted hard-example curation
- Break condition: If filtered pairs still show high model agreement (>85%), the filtering threshold may be too permissive or judge diversity is insufficient.

### Mechanism 2: Positional Consistent Dual Evaluation Mitigates Order Bias
- Claim: Evaluating each pair in both original and swapped orders reduces systematic position bias where MLLM judges prefer the first-presented response.
- Mechanism: Each (A, B) comparison yields two independent judgments—forward and reverse—doubling evaluation instances while penalizing judges with position-dependent preferences.
- Core assumption: Position bias is symmetric (preference for first slot regardless of content), not tied to specific response features.
- Evidence anchors:
  - "Position bias is common problem; models have a systematic preference for the first item in a pair"
  - "Both forward and reverse judgments are retained as independent data points, doubling judge-human comparison instances"
  - No direct corpus evidence on this specific technique; related benchmarks don't address position bias systematically
- Break condition: If a judge's accuracy diverges significantly between forward and reverse orders (beyond random variance), the judge may have content-dependent biases beyond position.

### Mechanism 3: Best-of-N Correlation Validates Downstream Utility
- Claim: MMRB2 accuracy predicts downstream task performance because reward models that align better with human preference make better selections from candidate pools.
- Mechanism: For each query, generate N=8 candidates; use the reward model to select the best via knockout tournament; measure selected response quality with task-specific metrics. Strong correlation (>0.8 Pearson's r) confirms benchmark validity.
- Core assumption: Human preference alignment on benchmark pairs transfers to preference alignment on downstream task outputs.
- Evidence anchors:
  - "Performance on MMRB2 strongly correlates with downstream task success using Best-of-N sampling"
  - "A good reward model can give great gains on downstream performance, even with the simple best-of-N sampling"
  - MMMG benchmark similarly validates through human-aligned evaluation, supporting correlation-based validation approaches
- Break condition: If correlation weakens (<0.6) for new task domains or model families, the benchmark may not generalize or may have distribution shift.

## Foundational Learning

- Concept: **Reward Models as Preference Classifiers**
  - Why needed here: MMRB2 evaluates models that judge which of two multimodal responses better aligns with human preference; understanding RM training objectives is prerequisite.
  - Quick check question: Can you explain why a reward model trained on SD 2.1-era outputs might underperform on frontier model outputs?

- Concept: **Interleaved Multimodal Sequences**
  - Why needed here: Omni models generate arbitrary text-image sequences; evaluation must handle variable modality orderings, not just text-to-image or image-to-text.
  - Quick check question: How would you represent a response containing 3 images interspersed with 4 text segments for pairwise comparison?

- Concept: **Position Bias in LLM-as-Judge**
  - Why needed here: MMRB2 explicitly addresses this through dual evaluation; practitioners must understand why naive single-pass judging is unreliable.
  - Quick check question: If GPT-4o prefers response A 70% of the time when A is shown first, but only 45% when B is shown first, what is the position bias magnitude?

## Architecture Onboarding

- Component map:
  Prompt curation layer -> Response generation layer -> Ensemble filtering layer -> Human annotation layer -> Pair construction -> Evaluation layer

- Critical path:
  1. Generate diverse prompts from existing benchmarks (avoid train-test leakage via test-split-only sampling)
  2. Generate candidate responses from frontier models; use agents where native models fail
  3. Apply ensemble filtering to surface challenging pairs
  4. Collect 3-way human annotations with task-specific rubrics
  5. Filter by agreement spread (≤4 on 7-point scale) and midpoint proximity
  6. Evaluate judges via dual ordering; compute binary agreement with human majority

- Design tradeoffs:
  - Filtering threshold: 90% catches easy pairs but may introduce judge ensemble bias; lower threshold increases data volume but adds annotation noise
  - Annotation redundancy: 3 annotators balances cost vs. reliability; higher redundancy improves agreement detection
  - Same-model vs. different-model pairs: Same-model pairs (57.4%) test fine-grained discrimination; different-model pairs (42.6%) test capability-gap detection—judges perform 5-13% better on different-model pairs

- Failure signatures:
  - Low inter-annotator agreement (<90% after filtering): Indicates ambiguous task definitions or rubric issues
  - High judge-judge correlation with low human agreement: Judges share systematic biases (e.g., image-content bias: judges prefer responses with images by 27.7-49.3%)
  - Test-time scaling flatlines: Qwen3-VL shows no improvement with more samples; GPT/Gemini gain only 0.8-1.2%

- First 3 experiments:
  1. Baseline judge evaluation: Run Qwen2.5-VL-7B and Gemini 3 Pro on all 4 tasks; verify accuracy gaps match reported values (50.4-64.1% vs. 74.4-79.5%)
  2. Position bias ablation: Evaluate a subset with single-pass (no swap) vs. dual evaluation; quantify accuracy difference
  3. Best-of-N correlation check: Use Qwen3-VL-32B as selector on GenAI-Bench with N=8; confirm correlation coefficient >0.8 with MMRB2 score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What alternative test-time scaling strategies can effectively improve multimodal reward model accuracy beyond simple majority voting?
- Basis in paper: [explicit] The authors conclude that "test-time scaling provides only modest returns... suggesting that alternative scaling methods are needed" (p. 12).
- Why unresolved: The paper demonstrates that majority voting provides negligible gains (<1.2%) for multimodal judges, unlike in text-only domains, leaving a gap for effective scaling methods.
- What evidence would resolve it: Demonstrating a new inference strategy (e.g., reasoning-based verification) that yields significant accuracy gains (>5%) on MMRB2 without increasing model size.

### Open Question 2
- Question: How can multimodal judges be debiased to fairly evaluate reasoning paths that do not include images?
- Basis in paper: [explicit] The analysis reveals a "strong bias toward responses that include images," with performance gaps of 27.7–49.3% when the preferred response is text-only (p. 3, 11).
- Why unresolved: Even the best models prefer visually augmented reasoning trajectories regardless of correctness, failing to neutrally assess "thinking with images" vs. text-only reasoning.
- What evidence would resolve it: Identifying training protocols or prompting techniques that reduce the accuracy gap between image-containing and text-only preferred pairs to under 10%.

### Open Question 3
- Question: How do current reward modeling capabilities generalize to multi-turn agentic interactions and interleaved generation?
- Basis in paper: [explicit] The authors explicitly identify "multi-turn and agentic interaction trajectories" as a limitation and "natural next step" for the current single-turn benchmark (p. 12, Appendix D).
- Why unresolved: MMRB2 evaluates isolated prompt-response pairs, but omni-models increasingly operate as agents over multiple turns where consistency and planning are critical.
- What evidence would resolve it: Extending the MMRB2 framework to sequential tasks and correlating single-turn judge accuracy with multi-turn trajectory success rates.

## Limitations
- 90% ensemble agreement filtering threshold may introduce systematic biases if filtering judges share common preferences
- Validation limited to single Best-of-N sampling scenario; generalization to other reward modeling applications remains unproven
- Reasoning task subset relatively small (200 pairs) and may not fully represent complex interleaved reasoning scenarios

## Confidence

- **High confidence**: Benchmark construction methodology (ensemble filtering, dual evaluation, human annotation pipeline) is clearly specified and reproducible. Reported human accuracy of 90%+ serves as strong validation of preference pair quality.
- **Medium confidence**: Claim that MMRB2 captures "omni" model evaluation is well-supported by task diversity, but reasoning task subset may not fully represent complex scenarios.
- **Low confidence**: Performance gap between Gemini 3 Pro (75-80%) and GPT-4o (59%) on multimodal reasoning suggests task-specific limitations, but without detailed error analysis, it's unclear whether this reflects genuine capability differences or benchmark-specific artifacts.

## Next Checks
1. **Filter bias validation**: Analyze distribution of excluded pairs from 90% filtering stage to identify systematic patterns (e.g., image-only vs. text-only comparisons, modality ordering) that might indicate judge ensemble bias.
2. **Cross-task generalization**: Evaluate MMRB2-trained judges on distinct multimodal reasoning benchmark (e.g., MMMG or NExT-OMNI) to verify whether performance correlations hold beyond benchmark's own downstream tasks.
3. **Fine-grained error analysis**: Conduct stratified analysis of judge errors on same-model vs. different-model pairs to quantify whether 5-13% performance gap is consistent across model capability levels or specific to certain model pairs.