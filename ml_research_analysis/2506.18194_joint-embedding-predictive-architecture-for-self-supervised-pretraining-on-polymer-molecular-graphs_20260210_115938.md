---
ver: rpa2
title: Joint Embedding Predictive Architecture for self-supervised pretraining on
  polymer molecular graphs
arxiv_id: '2506.18194'
source_url: https://arxiv.org/abs/2506.18194
tags:
- polymer
- data
- target
- pretraining
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of predicting polymer properties
  with limited labeled data by proposing a self-supervised learning approach based
  on Joint Embedding Predictive Architectures (JEPA) applied to polymer molecular
  graphs. JEPA operates by predicting target subgraph embeddings from context subgraph
  embeddings in an embedding space, avoiding the noise and overfitting issues associated
  with reconstructing inputs directly.
---

# Joint Embedding Predictive Architecture for self-supervised pretraining on polymer molecular graphs

## Quick Facts
- arXiv ID: 2506.18194
- Source URL: https://arxiv.org/abs/2506.18194
- Authors: Francesco Piccoli; Gabriel Vogel; Jana M. Weber
- Reference count: 40
- Primary result: JEPA improves polymer property prediction in low-data regimes, with R² improvements up to 39.8% for 0.4% labeled data

## Executive Summary
This paper addresses the challenge of predicting polymer properties with limited labeled data by proposing a self-supervised learning approach based on Joint Embedding Predictive Architectures (JEPA) applied to polymer molecular graphs. JEPA operates by predicting target subgraph embeddings from context subgraph embeddings in an embedding space, avoiding the noise and overfitting issues associated with reconstructing inputs directly. The method was tested on conjugated copolymer and diblock copolymer datasets, showing consistent performance improvements, particularly in low-data regimes. For instance, in the smallest labeled data scenario (0.4% of data), R² improved by 39.8%. The approach outperformed simpler models like random forests in most tested scenarios and demonstrated effective transfer learning across different polymer chemical spaces.

## Method Summary
The proposed method applies Joint Embedding Predictive Architectures to polymer molecular graphs for self-supervised pretraining. JEPA works by sampling context and target subgraphs from polymer molecular graphs, encoding them into embedding spaces, and training a predictor to estimate target embeddings from context embeddings. The model is trained using a contrastive loss that pulls together matching context-target pairs while pushing apart non-matching pairs. After pretraining, the context encoder is fine-tuned on labeled data for property prediction tasks. The approach specifically uses random-walk algorithms for subgraph sampling with context sizes of 60% and target sizes of 10% of the full graph, avoiding direct input reconstruction to prevent noise and overfitting issues common in autoencoders.

## Key Results
- JEPA achieved R² improvements of up to 39.8% in the lowest data regime (0.4% labeled data)
- Outperformed random forest models in most tested scenarios on conjugated and diblock copolymer datasets
- Demonstrated effective transfer learning capabilities across different polymer chemical spaces
- Optimal performance achieved with 60% context size and 10% target size for subgraph sampling

## Why This Works (Mechanism)
JEPA avoids the noise and overfitting issues of direct input reconstruction by operating in an embedding space where it predicts target subgraph representations from context subgraph representations. This indirect prediction approach is more robust to local variations in molecular graphs while capturing essential structural relationships. The self-supervised pretraining allows the model to learn rich molecular representations from unlabeled data before fine-tuning on limited labeled examples, making it particularly effective in low-data regimes where traditional supervised methods struggle.

## Foundational Learning
- **Polymer molecular graphs**: Represent polymers as graphs where nodes are atoms and edges are bonds - needed to capture chemical structure relationships for property prediction
- **Self-supervised learning**: Training on unlabeled data to learn representations before fine-tuning on labeled data - needed when labeled data is scarce and expensive to obtain
- **Contrastive learning**: Training objective that pulls together similar pairs and pushes apart dissimilar pairs in embedding space - needed to create meaningful representations without labels
- **Subgraph sampling**: Extracting context and target portions of molecular graphs - needed to create prediction tasks that capture local-to-global structural relationships

## Architecture Onboarding

**Component Map**: Molecular Graph -> Subgraph Sampler -> Context Encoder + Target Encoder -> Predictor -> Contrastive Loss -> Joint Embedding Space

**Critical Path**: Subgraph Sampler → Context Encoder → Predictor → Contrastive Loss (for pretraining); Context Encoder → Property Predictor → Task Loss (for fine-tuning)

**Design Tradeoffs**: Avoids input reconstruction noise but requires careful subgraph sampling strategy; balances representation learning quality against computational complexity of subgraph operations

**Failure Signatures**: Poor performance in low-data regimes indicates inadequate pretraining; inconsistent results across chemical spaces suggest suboptimal subgraph sampling parameters

**First Experiments**:
1. Test different context/target size ratios (40/20, 60/10, 80/5) on a small polymer subset
2. Compare pretraining with and without contrastive loss on a validation set
3. Evaluate transfer learning from conjugated to diblock copolymers with varying chemical similarity

## Open Questions the Paper Calls Out
None

## Limitations
- Results validated on small datasets (72 and 51 polymers) limiting generalizability
- Optimal subgraphing hyperparameters not systematically explored across all scenarios
- Comparison limited to random forests without state-of-the-art molecular GNN baselines
- Transfer learning experiments limited to one pair of moderately similar chemical spaces

## Confidence
- High confidence in JEPA's effectiveness for small labeled datasets and superiority over random forests in most tested scenarios
- Medium confidence in optimal subgraphing strategy (60% context, 10% target) due to limited systematic exploration
- Medium confidence in transfer learning benefits across chemical spaces, given limited chemical diversity tested

## Next Checks
1. Test JEPA on larger, more diverse polymer datasets (>500 polymers) to assess scalability and generalizability across broader chemical spaces
2. Compare JEPA performance against state-of-the-art molecular graph neural networks (MPNNs, GNNs with attention mechanisms) rather than just random forests
3. Systematically explore sensitivity of JEPA performance to subgraphing hyperparameters (context/target ratios, random-walk parameters) across different polymer classes