---
ver: rpa2
title: 'From Solo to Symphony: Orchestrating Multi-Agent Collaboration with Single-Agent
  Demos'
arxiv_id: '2511.02762'
source_url: https://arxiv.org/abs/2511.02762
tags:
- solo
- learning
- cooperative
- policy
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of training multi-agent reinforcement\
  \ learning (MARL) systems, which is inefficient when agents must learn to cooperate\
  \ from scratch. The key insight is that solo demonstrations\u2014where agents learn\
  \ individual tasks\u2014are far easier to obtain than multi-agent data but are underexplored\
  \ for accelerating cooperative learning."
---

# From Solo to Symphony: Orchestrating Multi-Agent Collaboration with Single-Agent Demos

## Quick Facts
- arXiv ID: 2511.02762
- Source URL: https://arxiv.org/abs/2511.02762
- Reference count: 40
- Primary result: Solo-to-Collaborative RL (SoCo) framework significantly improves MARL training efficiency by leveraging solo demonstrations, achieving over 20% better performance on 5-agent Spread and ~84% improvement on 3-agent MultiHalfCheetah

## Executive Summary
This paper addresses the challenge of training multi-agent reinforcement learning (MARL) systems by introducing a novel framework that leverages single-agent demonstrations to accelerate cooperative learning. The key insight is that solo demonstrations, which are easier to obtain than multi-agent data, can be effectively used to bootstrap multi-agent training through a policy fusion approach. The proposed Solo-to-Collaborative RL (SoCo) framework pretrains a shared policy from solo demonstrations via imitation learning, then adapts it during multi-agent training using a gating selector and action editor. Experiments across nine tasks demonstrate significant improvements in training efficiency and final performance compared to baseline algorithms.

## Method Summary
The SoCo framework addresses the inefficiency of training multi-agent systems from scratch by leveraging single-agent demonstrations. It consists of two main phases: first, pretraining a shared solo policy using imitation learning from demonstrations; second, adapting this policy during multi-agent training through a policy fusion module. This module employs a gating selector (inspired by Mixture-of-Experts) to choose appropriate solo actions and an action editor to refine them for cooperative scenarios. The approach effectively handles observation mismatch and domain shift between solo and multi-agent settings, enabling faster convergence and improved performance across various cooperative tasks.

## Key Results
- On 5-agent Spread task, SoCo outperforms baseline algorithms by over 20% in final performance
- On 3-agent MultiHalfCheetah, SoCo achieves ~84% improvement in final performance
- Across nine tasks spanning four cooperative scenarios, SoCo demonstrates consistent improvements in training efficiency and competitive or superior final performance

## Why This Works (Mechanism)
The mechanism works by leveraging the knowledge encoded in solo demonstrations through imitation learning, then adapting it to the cooperative setting using a policy fusion module. The gating selector identifies which solo actions are most relevant for the current multi-agent state, while the action editor refines these actions to account for coordination requirements. This approach effectively bridges the gap between individual task performance and cooperative behavior, addressing the observation mismatch and domain shift that typically hinder direct transfer of solo policies to multi-agent scenarios.

## Foundational Learning

**Imitation Learning**: Why needed - To extract useful behavior patterns from solo demonstrations; Quick check - Verify solo policy can solve individual tasks reliably before multi-agent adaptation

**Policy Fusion Architecture**: Why needed - To combine solo knowledge with multi-agent coordination; Quick check - Test gating selector's ability to choose appropriate actions across different scenarios

**Domain Adaptation**: Why needed - To handle observation mismatch between solo and multi-agent settings; Quick check - Measure performance degradation when transferring policies between domains

**Mixture-of-Experts (MoE)**: Why needed - To selectively combine multiple policy outputs; Quick check - Evaluate gating selector's accuracy in choosing optimal actions

**Multi-Agent Coordination**: Why needed - To enable effective collaboration among agents; Quick check - Assess team performance metrics in cooperative tasks

## Architecture Onboarding

**Component Map**: Solo Demonstrations -> Pretrained Solo Policy -> Policy Fusion Module (Gating Selector + Action Editor) -> Adapted Multi-Agent Policy

**Critical Path**: The policy fusion module is the critical component, as it directly determines how effectively solo knowledge transfers to cooperative settings. The gating selector and action editor work in tandem to select and refine solo actions for multi-agent scenarios.

**Design Tradeoffs**: The shared policy architecture simplifies pretraining but may limit performance in heterogeneous agent environments. The policy fusion module adds complexity but enables effective transfer from solo to collaborative settings.

**Failure Signatures**: Performance degradation occurs when solo behaviors are fundamentally incompatible with cooperative requirements, or when the observation space mismatch is too large for the action editor to bridge effectively.

**First Experiments**:
1. Test gating selector's action selection accuracy across different task scenarios
2. Evaluate action editor's effectiveness in handling observation mismatch
3. Measure performance with ablated versions (gating selector only vs action editor only)

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The shared policy architecture may not capture agent-specific behaviors in heterogeneous multi-agent environments
- The framework's performance on larger agent populations beyond tested scenarios remains uncertain
- The policy fusion module introduces additional hyperparameters that may require careful tuning for different tasks

## Confidence
- High confidence in the experimental methodology and reproducibility of results
- Medium confidence in the generalizability of solo-to-collaborative transfer across diverse multi-agent tasks
- Medium confidence in the scalability of the approach to larger agent populations beyond the tested scenarios

## Next Checks
1. Conduct ablation studies removing either the gating selector or action editor components to quantify their individual contributions to performance gains and identify potential redundancy.

2. Test the framework on heterogeneous multi-agent environments with distinct agent capabilities to evaluate the shared policy approach's limitations and potential adaptations needed.

3. Implement a systematic analysis of scenarios where solo demonstrations fail to transfer effectively, identifying specific characteristics of tasks that challenge the solo-to-collaborative paradigm.