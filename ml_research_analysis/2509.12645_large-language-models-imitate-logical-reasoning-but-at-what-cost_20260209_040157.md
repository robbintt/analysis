---
ver: rpa2
title: Large Language Models Imitate Logical Reasoning, but at what Cost?
arxiv_id: '2509.12645'
source_url: https://arxiv.org/abs/2509.12645
tags:
- reasoning
- query
- https
- flops
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the reasoning capability of frontier Large
  Language Models (LLMs) over an 18-month period, testing their accuracy and faithfulness
  to reasoning strategies on the PrOntoQA benchmark. The research reveals that improvements
  in LLM performance from 2023 to 2024 are primarily due to hidden Chain of Thought
  prompting, while the introduction of thinking models in 2025 enabled near-perfect
  accuracy.
---

# Large Language Models Imitate Logical Reasoning, but at what Cost?

## Quick Facts
- arXiv ID: 2509.12645
- Source URL: https://arxiv.org/abs/2509.12645
- Reference count: 40
- Frontier LLMs improved logical reasoning accuracy from 2023-2025 primarily through hidden Chain of Thought prompting, while neuro-symbolic approaches using small LLMs and Z3 solvers achieved near-perfect accuracy at significantly lower computational cost.

## Executive Summary
This study evaluates the reasoning capability of frontier Large Language Models (LLMs) over an 18-month period, testing their accuracy and faithfulness to reasoning strategies on the PrOntoQA benchmark. The research reveals that improvements in LLM performance from 2023 to 2024 are primarily due to hidden Chain of Thought prompting, while the introduction of thinking models in 2025 enabled near-perfect accuracy. Additionally, a neuro-symbolic approach combining smaller LLMs with the Z3 SMT solver significantly reduces computational costs while maintaining high performance, demonstrating the potential for LLMs to interface with external reasoning tools.

## Method Summary
The study evaluates 9 frontier LLMs across 3 time periods (2023, 2024, 2025) on the PrOntoQA logical reasoning benchmark using 6 different prompt types including normal, chain-of-thought, and strategy-specific prompts. A neuro-symbolic approach uses small LLMs (<15B parameters) to translate natural language problems into First-Order Logic, which is then processed by the Z3 SMT solver. The research measures accuracy, reasoning completeness, faithfulness to specific strategies, and computational cost using the approximation that inference FLOPs equal 2 × Active Parameters × Total Tokens.

## Key Results
- 2024 models showed significantly higher token generation than 2023 models, suggesting hidden Chain of Thought prompting as the primary driver of performance improvements
- 2025 thinking models achieved near-perfect accuracy but at substantially higher computational cost
- Neuro-symbolic approach combining small LLMs with Z3 solver achieved >99% accuracy at approximately 20% of the computational cost of frontier models
- Faithfulness to specific reasoning strategies remained low across all models, indicating that LLMs often guess correct answers without following proper logical steps

## Why This Works (Mechanism)

### Mechanism 1
Performance improvements in frontier models (2024-2025) appear driven by increased inference-time compute (hidden or explicit Chain of Thought) rather than fundamental architectural leaps in logical deduction. The paper suggests that 2024 models utilize "hidden" system prompts or training behaviors that force step-by-step reasoning (CoT), evidenced by high completion token counts even in "Normal" conditions. By 2025, "thinking" models explicitly scale the number of generated tokens to simulate reasoning paths. Generating intermediate steps allows the model to resolve dependencies iteratively rather than in a single forward pass, effectively trading compute for accuracy.

### Mechanism 2
Neuro-symbolic architectures achieve near-perfect accuracy by offloading logical verification to an external SMT solver (Z3), bypassing the LLM's probabilistic deduction weaknesses. A smaller LLM (<15B parameters) functions purely as a semantic parser, translating natural language into standardized First-Order Logic (FOL). The Z3 solver then deterministically checks the satisfiability of the query. This avoids the "imitation" errors inherent to pure LLM reasoning, as translation is a simpler pattern recognition task for LLMs than deductive reasoning.

### Mechanism 3
Computational cost (FLOPs) scales linearly with the total number of generated tokens and active parameters, making output token reduction the primary lever for efficiency. The study validates the approximation that Inference FLOPs ≈ 2 × (Active Parameters) × (Total Tokens). "Thinking" models are computationally expensive because they generate thousands of tokens. Neuro-symbolic models reduce cost by reducing the token count (translation only) and parameter count (smaller model).

## Foundational Learning

**Faithfulness vs. Imitation in Reasoning**
Why needed here: The paper critiques LLMs for "imitating" reasoning. Engineers must distinguish between a model getting the right answer (Accuracy) and using the correct logical steps (Faithfulness). A model might guess "True" correctly but hallucinate the proof steps.
Quick check question: Given a model's output, can you map its stated steps back to the "Golden Chain of Thought" to verify it didn't just guess the answer?

**Satisfiability Modulo Theories (SMT)**
Why needed here: To understand the neuro-symbolic approach, one must understand that Z3 is not "generating" an answer but checking if a set of logical statements (axioms + negated query) can coexist. If they cannot (UNSAT), the query is true.
Quick check question: If Z3 returns "UNSAT" for a set of constraints including the negation of a query, what does that imply about the truth of the original query?

**Inference Compute (FLOPs)**
Why needed here: The paper optimizes for cost. You need to understand the 2Nn formula to estimate the economic viability of deploying a "Thinking" model versus a Neuro-symbolic pipeline.
Quick check question: If Model A (100B params) generates 100 tokens and Model B (10B params) generates 500 tokens, which is cheaper to run?

## Architecture Onboarding

**Component map:**
Natural language problem → Small LLM (translation) → Parser/Validator → Z3 SMT Solver → Result Interpreter

**Critical path:**
1. Prompt Engineering: Constructing the prompt to force the LLM to output strict logic
2. Translation: LLM generates the formal representation
3. Verification (Optional but recommended): Feed the query and its negation to Z3. If both return sat, the translation is flawed (contradiction). Trigger the "Repair" prompt
4. Solving: Z3 executes the proof search

**Design tradeoffs:**
- Frontier "Thinking" Models: High accuracy (~100%), extremely high cost (1000+ tokens/query), "Black Box" reasoning (low faithfulness/interpretability)
- Neuro-Symbolic (Small LLM + Z3): High accuracy (~99-100%), low cost (~200 tokens/query), high interpretability (formal proof exists)
- Risk: Neuro-symbolic systems are brittle to parsing errors; Frontier models are brittle to hallucination

**Failure signatures:**
- Contamination: Model scores 100% on standard benchmarks but fails when entity names are changed
- Parser Breakage: LLM outputs "Cats are mammals" instead of "For all x, if x is Cat, then x is Mammal," causing Z3 to reject the syntax
- Logical Inconsistency: The LLM translates a query such that it is both true and false depending on interpretation; Z3 returns unknown or the consistency check fails

**First 3 experiments:**
1. Baseline Token Audit: Run a "Normal" prompt (no CoT instruction) on your target model. If output tokens > 50, suspect hidden CoT/hidden prompts
2. Translation Accuracy Test: Ask a small LLM (e.g., Llama 3.1 8B) to translate 10 logical statements into the standardized format defined in Appendix D. Manually check parsing success rate
3. Cost/Performance Pivot: Replicate the "New Variables" test from Appendix C. Compare the cost (FLOPs) of a frontier model vs. a local small model + Z3 on these unseen variations to verify true reasoning vs. memorization

## Open Questions the Paper Calls Out

**Can the neuro-symbolic architecture maintain high fidelity when applied to real-world problems outside of synthetic benchmarks?**
The study was limited to PrOntoQA "Steamroller" problems, which the authors classify as "toy examples" easily solvable by SMT solvers. Successful implementation on complex, non-synthetic datasets (e.g., legal reasoning or formal software verification) would demonstrate the translation remains accurate when formalizing messy natural language.

**How does the performance of smaller LLMs (<15B parameters) degrade when translating problems for more sophisticated Automated Theorem Proving (ATP) strategies?**
The current study utilized basic reasoning strategies (bottom-up, top-down) and limited problem complexity; it is unknown if smaller models can successfully translate the nuances required for advanced proof search heuristics. Benchmarking translation accuracy on high-complexity datasets (e.g., TPTP) and measuring resulting solver success rates would provide evidence.

**Can the pattern recognition capabilities of LLMs be reliably leveraged to interface with external tools other than SMT solvers to reduce computational costs?**
The paper demonstrated cost reduction only for the specific case of logical deduction via Z3; the generalizability of this "translation then execution" paradigm to other symbolic engines is untested. Identifying a distinct symbolic domain (e.g., computer algebra systems or physics engines) and demonstrating effective translation would resolve this question.

## Limitations

The exact configuration of the PrOntoQA generator for the "False Ontology" setting is not fully specified, potentially affecting model performance comparisons across time periods. The claim that 2024-2025 performance improvements are primarily due to hidden CoT prompting relies on indirect evidence and assumptions about model behavior without access to actual system prompts. Faithfulness measurement depends on manually constructed "Golden Chain of Thought" sequences and automated regex parsing, introducing potential reproducibility issues.

## Confidence

**High Confidence:**
- Neuro-symbolic approach consistently achieves high accuracy (>99%) and reduces computational costs by approximately 80% compared to frontier models
- The approximation formula for inference FLOPs (2 × Active Parameters × Total Tokens) is validated within 10% accuracy across all experiments
- Comparative analysis showing 2024 models generate significantly more tokens than 2023 models is supported by direct measurements

**Medium Confidence:**
- The claim that 2024-2025 performance improvements are primarily due to hidden CoT prompting rather than architectural advances
- The interpretation that frontier models are "imitating" reasoning rather than truly reasoning

**Low Confidence:**
- The exact mechanisms by which frontier models achieve high accuracy without explicit CoT instructions

## Next Checks

1. **CoT Isolation Experiment:** Design a controlled experiment where frontier models are forced to output only the final answer (via strict token limits or logit biases) to test whether performance degrades to 2023 levels, confirming that hidden CoT is the primary driver of improvements.

2. **Translation Robustness Test:** Systematically evaluate the neuro-symbolic pipeline's performance when the LLM encounters edge cases in natural language parsing, such as ambiguous negations or complex quantifier structures, to identify failure modes in the translation step.

3. **Cross-Domain Generalization:** Apply the neuro-symbolic approach to a different logical reasoning benchmark (e.g., a set theory or graph-based reasoning task) to verify that the Z3 solver provides consistent advantages across domains, not just PrOntoQA.