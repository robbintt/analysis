---
ver: rpa2
title: Transformative or Conservative? Conservation laws for ResNets and Transformers
arxiv_id: '2506.06194'
source_url: https://arxiv.org/abs/2506.06194
tags:
- conservation
- laws
- span
- such
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper derives and analyzes conservation laws for modern neural
  network architectures, focusing on convolutional ResNets and Transformers. The authors
  first establish that conservation laws for gradient flow with weight decay are determined
  by their equivalents without weight decay.
---

# Transformative or Conservative? Conservation laws for ResNets and Transformers

## Quick Facts
- **arXiv ID**: 2506.06194
- **Source URL**: https://arxiv.org/abs/2506.06194
- **Reference count**: 40
- **Primary result**: Derives conservation laws for gradient flow in modern architectures, showing they persist approximately under discrete SGD with O(step-size²) error

## Executive Summary
This paper establishes conservation laws for modern neural network architectures including convolutional ResNets and Transformers by analyzing gradient flow dynamics. The authors prove that conservation laws with weight decay are fully determined by their equivalents without weight decay, then systematically derive new conservation laws for key building blocks like shallow convolutions, self-attention, and classification layers. A novel contribution is showing that conservation laws depending only on single blocks exactly match those of the isolated block, enabling modular analysis. The paper also demonstrates that these conservation principles persist under discrete SGD dynamics with an error bound scaling as O(step-size²), validated through experiments on CIFAR-10 and IMDb datasets.

## Method Summary
The method involves analyzing gradient flow dynamics θ̇(t) = -∇L_Z(θ(t)) to derive conservation laws, then examining how these laws persist under discrete SGD updates θ_{k+1} = θ_k - τ∇L_Z(θ_k). The authors derive specific conservation law formulas for convolutional blocks (h(θ) = Σ_k||u_{k,j}||² - Σ_i||v_{j,i}||²), attention layers (||Q_j||²_F - ||K_j||²_F), and classification heads. Experiments track the relative conservation error |(h(θ_k) - h(θ_0))/h(θ_0)| over 50 training steps using SGD without momentum or weight decay, with learning rates varying between 10⁻³ and 5×10⁻³. The theoretical error bound O(τ²) is validated by plotting error on log-log scales and verifying the slope matches theoretical predictions.

## Key Results
- Conservation laws for gradient flow with weight decay are fully determined by their equivalents without weight decay (Structure Theorem)
- Conservation laws depending only on single blocks exactly match those of the isolated block (Theorem 4.6)
- No conservation laws exist for overlapping parameter blocks spanning multiple residual connections (Theorem 4.7)
- Discrete SGD approximately preserves continuous conservation laws with O(τ²) error, confirmed on CIFAR-10 (ResNet-18) and IMDb (Transformer)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Conservation laws in gradient flow with weight decay are fully determined by their equivalents without weight decay.
- **Mechanism**: The Structure Theorem (Theorem 2.1) shows any conserved function h(t,θ) for weight-decay dynamics can be expressed as H(θ·exp(∫₀ᵗλ(s)ds)), reducing analysis from D+1 variables to D parameters.
- **Core assumption**: For every θ, there exists a dataset Z such that ∇L_Z(θ) = 0 (satisfied for classical losses).
- **Break condition**: Fails if loss landscape has no stationary points reachable by data.

### Mechanism 2
- **Claim**: Conservation laws depending only on a single block's parameters exactly match those of the isolated block.
- **Mechanism**: Proposition 4.3 characterizes "block" conservation laws via subspace R_{θ_T}(W_{g,ℓ}). Theorem 4.6 proves global network's block-dependent conservation laws coincide with shallow network's laws. Skip connections don't alter conservation structure.
- **Core assumption**: Block satisfies open-map property and non-degenerate weight conditions.
- **Break condition**: Fails if block parameters are degenerate or if overlapping blocks across residual connections.

### Mechanism 3
- **Claim**: Discrete SGD approximately preserves continuous gradient-flow conservation laws with O(τ²) error.
- **Mechanism**: Taylor expansion of h(θ_{k+1}) - h(θ_k) yields second-order term involving Hessian and gradient. Proposition 5.1 bounds accumulated error as C_h·C_L·Στ_i².
- **Core assumption**: Bounded Hessian of conservation law and bounded expected gradient norm throughout training.
- **Break condition**: Fails if gradients explode or conservation law has unbounded curvature.

## Foundational Learning

- **Concept**: Gradient flow as continuous-time limit of gradient descent
  - **Why needed here**: Core results derive from ODE θ̇(t) = -∇L_Z(θ(t)), the τ→0 limit of discrete updates.
  - **Quick check**: Can you explain why gradient flow is useful for analysis even though we train with discrete steps?

- **Concept**: Lie algebras and orthogonality characterization of conservation laws
  - **Why needed here**: Proposition 2.9 establishes h is conserved iff ∇h(θ) ⊥ Lie(W_{g,ℓ})(θ). Determines number of independent conservation laws.
  - **Quick check**: Given a set of vector fields, how would you compute the dimension of the Lie algebra they generate?

- **Concept**: Residual connections and their mathematical structure
  - **Why needed here**: Paper proves skip connections preserve conservation law structure (g̃(θ,x) = x + g(θ,x) has same laws as g).
  - **Quick check**: Why does adding a skip connection not change ∂_θg?

## Architecture Onboarding

- **Component map**:
  - Input layer: Standard feature representation x ∈ R^m (may be reshaped for Transformers)
  - Residual blocks: q blocks, each with parameters θ_l; can be (a) 2-layer conv ReLU, (b) 2-layer dense ReLU, or (c) single-head attention
  - Classification head: Optional softmax layer with its own conservation laws (column sums)
  - Conservation law tracker: Computes h_j(θ) = Σ_k||u_{k,j}||² - Σ_i||v_{j,i}||² for conv blocks; ||Q_j||²_F - ||K_j||²_F for attention

- **Critical path**:
  1. Identify which blocks in your architecture are covered (conv-ReLU, dense-ReLU, single-head attention)
  2. Map parameters to conservation law formulas (Theorem 3.6 for conv, Corollary 3.9 for attention, Proposition 3.11 for classification)
  3. Initialize tracking at θ₀; monitor |h(θ_k) - h(θ₀)|/|h(θ₀)| during training
  4. Verify error scales as τ²k for constant step-size (Figure 1 pattern)

- **Design tradeoffs**:
  - Multi-head attention: Only partial characterization available (Corollary 3.10); completeness is open
  - Normalization layers (LayerNorm, BatchNorm): Not covered in this analysis
  - Overlapping blocks (parameters spanning two residual connections): Theorem 4.7 shows no conservation laws exist for these parameter subsets

- **Failure signatures**:
  - Conservation error grows linearly with k instead of τ²k → likely violation of bounded gradient assumption or incorrect conservation formula
  - Zero conservation error initially then sudden growth → parameter entered degenerate region (e.g., filter collapse)
  - Different error scaling for different learning rates → check if batch size or other hyperparameters affect gradient variance

- **First 3 experiments**:
  1. **Reproduce CIFAR-10 conservation tracking**: Train ResNet-18 with SGD (no momentum/weight decay), track h(θ_T) = Σ_j h_j for first residual block. Verify slope ∝ τ² across learning rates 1e-3 to 5e-3.
  2. **Test attention layer conservation on synthetic data**: Single-head attention layer, track ||Q||²_F - ||K||²_F and ||V||²_F - ||O||²_F. Confirm preservation under varying sequence lengths N≥2.
  3. **Probe break conditions**: Introduce overlapping parameter blocks (cross-block parameters) and verify no conservation laws exist numerically by computing rank of span{∇L_Z(θ) : Z}.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Are the identified conservation laws for multi-head attention mechanisms complete, or do additional laws exist?
- **Basis**: Section 3.4 states that while conservation laws for single heads are complete, "determining whether this set of conservation laws is complete remains an open problem" for multi-head attention.
- **Why unresolved**: The interaction between multiple heads creates a complex parameter structure where the dimension of the associated Lie algebra is difficult to fully characterize theoretically.
- **What evidence would resolve it**: A formal proof matching the number of independent laws to the dimension of the parameter space minus the Lie algebra dimension, or an explicit derivation of new independent conserved quantities.

### Open Question 2
- **Question**: How do transformer normalization layers (e.g., LayerNorm) and max-pooling layers affect the existence and form of conservation laws?
- **Basis**: The Conclusion lists these specific architectural components as "not currently accounted for" and identifies their integration as "promising avenues for future research."
- **Why unresolved**: These layers introduce different parameterization structures and non-smoothness (in the case of max-pooling) that fall outside the analysis of the smooth, linear/attention blocks covered in the paper.
- **What evidence would resolve it**: Extending the theoretical framework (Definition 2.4 and Proposition 2.7) to derive conservation laws specifically for networks incorporating these normalization and pooling operations.

### Open Question 3
- **Question**: Can the approximate conservation guarantees for discrete SGD be established under weaker assumptions than uniformly bounded gradients?
- **Basis**: Section 5.1 notes that the bounded gradient assumption required for the O(τ²) error bound is "restrictive" for non-convex deep networks and generally only holds for smooth convex losses.
- **Why unresolved**: The current proof relies on a global constant bounding the gradient variance; without this, the error accumulation over iterations cannot be theoretically controlled.
- **What evidence would resolve it**: Deriving error bounds that depend on local smoothness properties or specific trajectory characteristics of SGD, rather than global gradient bounds.

## Limitations
- Analysis is limited to specific building blocks (shallow conv/dense/attention layers) and doesn't cover normalization layers or complex skip connection patterns
- Completeness proofs rely on specific matrix rank conditions (open-map property) that may not hold for degenerate parameter configurations
- SGD error analysis assumes bounded Hessians and gradients throughout training, which may break down in practice

## Confidence
- **High**: The weight-decay Structure Theorem (Theorem 2.1) - general result about gradient flow dynamics with no architecture-specific assumptions
- **Medium**: Conservation laws for convolutional/dense blocks - while formulas are derived correctly, completeness depends on the open-map property requiring empirical verification
- **Medium**: Multi-head attention characterization - Corollary 3.10 provides partial result, but completeness remains open as noted by authors
- **High**: SGD error bounds - Proposition 5.1 is standard Taylor expansion result, though applicability depends on boundedness assumptions holding throughout training

## Next Checks
1. **Verify open-map property empirically**: For a ResNet-18 on CIFAR-10, compute the rank of the Jacobian of g̃ at initialization across multiple seeds to confirm it matches the theoretical maximum rank required for completeness.

2. **Test multi-head attention**: Extend the synthetic data experiment to 2-head attention. Verify whether the conservation laws from Corollary 3.10 hold and whether additional conservation laws emerge beyond the single-head case.

3. **Stress-test error bounds**: Intentionally train with aggressive learning rates (τ > 5e-3) and observe whether the conservation error still scales as τ² or if it exhibits linear growth, indicating violation of the bounded gradient assumption.