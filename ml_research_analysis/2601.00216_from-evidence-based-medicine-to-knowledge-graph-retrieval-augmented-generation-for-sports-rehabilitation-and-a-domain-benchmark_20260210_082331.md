---
ver: rpa2
title: 'From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation
  for Sports Rehabilitation and a Domain Benchmark'
arxiv_id: '2601.00216'
source_url: https://arxiv.org/abs/2601.00216
tags:
- evidence
- retrieval
- arxiv
- answer
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents SR-RAG, an EBM-adapted GraphRAG framework
  for sports rehabilitation that addresses two key gaps: lack of PICO alignment between
  queries and retrieved evidence, and absence of evidence hierarchy considerations
  during reranking. The framework integrates the PICO framework into knowledge graph
  construction and retrieval, and proposes a Bayesian-inspired reranking algorithm
  (BETR) that calibrates ranking scores by evidence grade without introducing predefined
  weights.'
---

# From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark

## Quick Facts
- arXiv ID: 2601.00216
- Source URL: https://arxiv.org/abs/2601.00216
- Reference count: 40
- Primary result: SR-RAG framework achieves 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy on sports rehabilitation QA

## Executive Summary
This study presents SR-RAG, an EBM-adapted GraphRAG framework for sports rehabilitation that addresses two key gaps: lack of PICO alignment between queries and retrieved evidence, and absence of evidence hierarchy considerations during reranking. The framework integrates the PICO framework into knowledge graph construction and retrieval, and proposes a Bayesian-inspired reranking algorithm (BETR) that calibrates ranking scores by evidence grade without introducing predefined weights. A knowledge graph of 357,844 nodes and 371,226 edges was constructed, and a reusable benchmark of 1,637 QA pairs was released. The system achieved strong performance metrics and received high ratings from expert clinician evaluation, demonstrating that the proposed EBM adaptation strategy substantially improves retrieval and answer quality and is readily transferable to other clinical domains.

## Method Summary
SR-RAG adapts the Youtu-GraphRAG architecture by integrating PICO framework into knowledge graph construction and retrieval, and introducing a Bayesian-inspired reranking algorithm (BETR). The framework constructs a knowledge graph with 357,844 nodes and 371,226 edges using PICO-extended schema, implements PICO-guided Hypothetical Document Embeddings for retrieval, employs dual-track retrieval with separate handling of Grade A (guidelines) and B-E evidence, and applies BETR calibration to rerank results based on evidence hierarchy. The system was evaluated on a benchmark of 1,637 QA pairs using multiple metrics including nugget coverage, answer faithfulness, semantic similarity, and PICOT match accuracy, with expert clinician assessment across five dimensions.

## Key Results
- Achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy
- Expert clinician evaluation rated the system 4.66-4.84 across five dimensions
- Ablation studies showed BETR improves nugget coverage by 8.2% over standard reranking
- PICO-guided HyDE improves PICOT match accuracy from 0.723 to 0.788
- Schema adaptation improves PICOT match accuracy from 0.701 to 0.788

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining knowledge graph construction to the PICO framework reduces retrieval drift and improves answer alignment
- Mechanism: By defining the graph schema strictly using PICO-related entity types rather than generic medical terms, the extraction agent forces unstructured text into a structure that mirrors clinical reasoning
- Core assumption: Clinical questions can be mapped to PICO structure and the extraction agent can accurately resolve text to these specific entity types
- Evidence anchors: Integrating PICO framework into knowledge graph construction and retrieval; replacing schema with generic terms caused significant drop in PICOT match accuracy (0.788 to 0.701)

### Mechanism 2
- Claim: Bayesian-inspired reranking (BETR) improves reliability of evidence selection by learning to penalize lower-quality evidence grades
- Mechanism: BETR learns "grade bias" parameters via Maximum A Posteriori estimation, treating semantic score as likelihood and evidence hierarchy as prior
- Core assumption: Training pairs allow model to disentangle semantic relevance from evidence grade, with monotonic relationship between grade and utility
- Evidence anchors: BETR calibrates ranking scores by evidence grade without predefined weights; learned grade biases were strictly monotonically decreasing consistent with EBM principles

### Mechanism 3
- Claim: PICO-guided Hypothetical Document Embeddings bridges semantic gap between short user queries and dense technical literature
- Mechanism: LLM generates hypothetical document using PICO soft constraints, creating retrieval target that is semantically rich but factually conservative
- Core assumption: LLM can generate hypothetical document semantically close to ground truth without hallucinating details
- Evidence anchors: PICO soft constraints incorporated into HyDE prompts; removing HyDE drops PICOT match accuracy (0.788 to 0.723)

## Foundational Learning

- Concept: **PICO Framework (EBM)**
  - Why needed here: This is the ontological backbone of the entire system (graph schema, HyDE prompts, evaluation)
  - Quick check question: In the query "Is HIIT better than MICT for heart patients?", which part is the 'Comparator'?

- Concept: **Maximum A Posteriori (MAP) Estimation**
  - Why needed here: Required to understand the BETR reranking algorithm
  - Quick check question: In BETR, what serves as the "prior" and what serves as the "likelihood" in the scoring function?

- Concept: **GraphRAG vs. Vector RAG**
  - Why needed here: Understanding that retrieval traverses entity relationships (edges), not just vector similarity
  - Quick check question: Why would a vector search fail to distinguish between "exercise for children" and "exercise for adults" if phrasing is similar, whereas GraphRAG with PICO schema might succeed?

## Architecture Onboarding

- Component map: PDFs -> Docling (Markdown) -> LLM-aware Chunker -> PICO-Schema Graph Constructor -> Query Decomposer -> [PICO-HyDE Generator, Graph Retriever, Dense Retriever] -> RRF Fusion -> ColBERT (Coarse) -> Cross-Encoder (Fine) -> BETR Calibrator -> Top-K Selection -> LLM (DeepSeek-V3) -> Structured Output

- Critical path: The BETR Calibrator is the critical novelty. The system relies on the metadata field `evidence_grade` being present for every chunk during the reranking phase.

- Design tradeoffs:
  - Strict Schema vs. Coverage: PICO-extended schema improves precision but may increase false negatives for literature that doesn't fit neat PICO entities
  - Dual-Track Retrieval: Separating Grade A (Guidelines) from B-E prevents dilution but requires managing two retrieval indices

- Failure signatures:
  - "Population Drift" in Output: If PICO-guided HyDE hallucinates a population or graph retrieval matches wrong Population node
  - "Over-penalization" in BETR: If learned biases are too aggressive, system might ignore relevant RCT in favor of generic Guideline

- First 3 experiments:
  1. Ablate BETR: Disable $u_t$ term to verify impact of evidence hierarchy on nugget coverage vs semantic similarity
  2. Schema Stress Test: Run queries where PICO elements are implicit to see if Query Decomposer and HyDE can resolve them
  3. Visualize Grade Biases: Extract learned $u_t$ values to confirm they strictly follow A > B > C > D > E hierarchy

## Open Questions the Paper Calls Out

- Can BETR be effectively extended to incorporate within-study quality assessments (e.g., GRADE systems) in addition to cross-study evidence hierarchies? The authors note this represents a promising direction since current approach focuses on macro-level study types rather than internal validity.

- Does implementing ontology normalization significantly reduce retrieval noise and improve accuracy in the sports rehabilitation knowledge graph compared to current long-tailed distribution? The authors identify absence of ontology normalization as a limitation and plan to explore finer-grained ontology alignment.

- Are the learned grade bias parameters of BETR robust across clinical domains with different evidence distributions, or do they require re-calibration? The authors claim adaptation strategy is "readily transferable to other clinical domains," but parameters were learned solely from sports rehabilitation literature.

## Limitations
- Framework's reliance on PICO structure may limit coverage for complex clinical scenarios that don't fit neatly into PICO categories
- BETR assumes monotonic relationship between evidence grade and utility that may not always hold in practice
- System requires extensive manual annotation of evidence grades across corpus, creating scalability barrier

## Confidence
- High confidence in framework's performance on sports rehabilitation QA tasks
- Medium confidence in generalizability beyond structured clinical queries due to PICO schema limitations
- Medium confidence in scalability claims given manual annotation requirements
- Medium confidence in automated evaluation reliability using LLM-as-judge metrics
- Low confidence in extraction accuracy without error analysis of PICO entity resolution

## Next Checks
1. Test framework on clinical queries with non-PICO elements (social determinants, patient preferences) to assess schema limitations
2. Conduct ablation studies comparing BETR's learned calibration against fixed heuristic weighting schemes across multiple clinical domains
3. Measure end-to-end performance degradation when evidence grades are missing or mislabeled in corpus metadata