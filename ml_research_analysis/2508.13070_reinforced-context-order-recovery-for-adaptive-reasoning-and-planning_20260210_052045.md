---
ver: rpa2
title: Reinforced Context Order Recovery for Adaptive Reasoning and Planning
arxiv_id: '2508.13070'
source_url: https://arxiv.org/abs/2508.13070
tags:
- recor
- order
- token
- training
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that standard causal and diffusion language
  models struggle with complex reasoning tasks because they generate tokens in a fixed
  (left-to-right) or random order, rather than an adaptive order that would make the
  problem tractable. To address this, the authors propose Reinforced Context Order
  Recovery (ReCOR), a reinforcement learning framework that learns to adaptively select
  the next token to generate based on the current context.
---

# Reinforced Context Order Recovery for Adaptive Reasoning and Planning

## Quick Facts
- **arXiv ID:** 2508.13070
- **Source URL:** https://arxiv.org/abs/2508.13070
- **Reference count:** 40
- **One-line primary result:** ReCOR outperforms standard causal and diffusion models on arithmetic and logic reasoning tasks by adaptively selecting generation order based on token prediction difficulty.

## Executive Summary
Standard causal and diffusion language models generate tokens in fixed or random orders, which can be inefficient for complex reasoning tasks. This paper introduces Reinforced Context Order Recovery (ReCOR), a reinforcement learning framework that learns to adaptively select the next token to generate based on current context difficulty. ReCOR uses self-supervised rewards derived from token prediction confidence to guide order selection during both training and inference, addressing distribution shift issues common in inference-only adaptive methods. Experiments demonstrate consistent improvements over strong baselines on arithmetic, Sudoku, and Zebra puzzle datasets.

## Method Summary
ReCOR casts generation order recovery as a Markov Decision Process where actions are selecting positions to fill and rewards come from token predictor confidence. The framework jointly trains an order policy (via Soft Q-Learning) and token predictor (via language modeling) using a multi-stream transformer architecture. The token predictor's probability scores serve as self-supervised rewards, allowing the system to learn without ground-truth order annotations. This joint optimization ensures training and inference distributions align, avoiding the pitfalls of models that train on random orders but infer adaptively.

## Key Results
- ReCOR consistently outperforms standard left-to-right baselines, adaptive masked diffusion models, and even oracle models with ground-truth orders on reasoning benchmarks
- The method demonstrates strong scaling properties, with performance improving as more compute is allocated to order queries
- Joint training of order and token models proves critical, as models trained with random orders fail when used with adaptive inference

## Why This Works (Mechanism)

### Mechanism 1: Predictive V-Information for Hardness Estimation
ReCOR uses Predictive V-information to quantify computational hardness of predicting tokens given current context. The framework prioritizes tokens where the model has highest probability of success, using solved "easy" tokens as stepping stones for harder ones. This greedy approach to maximizing immediate information gain leads to globally solvable solution paths. The core assumption is that token prediction difficulty varies significantly based on context.

### Mechanism 2: RL-based Order Policy via Self-Supervision
The framework uses reinforcement learning to recover optimal generation order without human annotations. The action is selecting the next position to fill, while the reward comes from the log-probability of correct tokens predicted by the concurrent token model. This creates a feedback loop where the order policy learns to select positions the token predictor can currently solve reliably. The assumption is that the token predictor learns faster than the order policy, providing stable reward signals.

### Mechanism 3: Training-Inference Distribution Alignment
Jointly optimizing order policy and token model eliminates distribution shift issues that plague adaptive inference methods. Standard Masked Diffusion Models train on randomly masked contexts rarely seeing clean intermediate states needed for hard reasoning steps. ReCOR samples actions from the current policy during training, ensuring the token predictor is trained on the exact sub-problems encountered during adaptive inference, effectively curriculum learning the sub-problems.

## Foundational Learning

- **Concept: Predictive V-information**
  - **Why needed here:** Provides mathematical justification for why "easy-first" generation is superior under computational constraints
  - **Quick check question:** How does V-information differ from standard Shannon entropy in the context of computational constraints?

- **Concept: Soft Q-Learning (SQL)**
  - **Why needed here:** Used to train the order policy with entropy regularization
  - **Quick check question:** Why does the paper suggest using Binary Cross-Entropy (BCE) loss for sparse rewards instead of Mean Squared Error (MSE)?

- **Concept: Two-Stream Attention (XLNet-style)**
  - **Why needed here:** Architecture uses separate streams to prevent predictions from "cheating" by attending to their own future tokens
  - **Quick check question:** In the multi-stream architecture, why can't the token query stream attend to itself?

## Architecture Onboarding

- **Component map:** Backbone (GPT-2 style Transformer) -> Main Stream (encodes context) -> Token Query Stream (predicts tokens) -> Order Query Stream (predicts Q-values)
- **Critical path:** Rollout (sample mini-batch and actions) -> Reward Generation (forward pass through token predictor) -> Optimization (update token predictor and policy)
- **Design tradeoffs:** Compute vs. Accuracy (more queries improve performance but increase FLOPs), Sparse vs. Dense Reward (sparse is more robust but provides less gradient signal)
- **Failure signatures:** Mode Collapse (policy gets stuck selecting same positions), Distribution Mismatch (training uses uniform exploration but inference is greedy)
- **First 3 experiments:** Synthetic Autoregression (verify model learns to reverse order), Reward Ablation (compare sparse vs. dense rewards on Sudoku), Inference Compute Scaling (compare ReCOR vs. MDM on Multiplication)

## Open Questions the Paper Calls Out

- Can ReCOR maintain its efficiency and performance advantages when scaled to general-purpose, large-scale language modeling tasks beyond synthetic reasoning puzzles?
- Can the reinforcement learning framework be extended to recover other unobserved latent variables in the generation process, aside from token order?
- Do more advanced reinforcement learning algorithms significantly improve the stability or sample efficiency of the order prediction policy?

## Limitations

- Experiments are restricted to synthetic and structured reasoning tasks (Sudoku, Zebra puzzles, arithmetic), leaving scalability to open-ended tasks unexplored
- Architecture implementation details are incomplete, particularly regarding how learnable embeddings map to discrete action spaces
- The framework's reliance on token predictor confidence as reward assumes well-calibrated probabilities without providing explicit calibration analysis

## Confidence

- **High Confidence:** ReCOR improves performance over standard models, joint training is critical, adaptive ordering is superior to fixed ordering
- **Medium Confidence:** V-information provides theoretical justification, compute efficiency gains are significant
- **Low Confidence:** Sparse rewards are more robust than dense rewards, ReCOR scales to real-world applications

## Next Checks

1. **Reward Signal Calibration and Robustness:** Verify token predictor probability scores are well-calibrated by plotting Reliability Diagrams and comparing policy accuracy using sparse vs. calibrated dense rewards

2. **Open-Ended Reasoning Task:** Test ReCOR on mathematical proof steps or code generation tasks (e.g., HumanEval) to measure exact match accuracy and analyze generated reasoning chains

3. **Compute Efficiency Benchmark:** Instrument implementations to count FLOPs per step and plot total FLOPs vs. accuracy for varying input lengths to quantify O(M²) vs. O(M³) scaling advantage