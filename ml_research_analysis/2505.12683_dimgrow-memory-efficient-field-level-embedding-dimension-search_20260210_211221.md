---
ver: rpa2
title: 'DimGrow: Memory-Efficient Field-level Embedding Dimension Search'
arxiv_id: '2505.12683'
source_url: https://arxiv.org/abs/2505.12683
tags:
- embedding
- dimension
- feature
- search
- dimensions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of field-level embedding dimension
  search in recommendation systems, where different feature fields require different
  embedding dimensionalities. The key limitation of existing methods is their reliance
  on memory-intensive SuperNets that enumerate all possible dimension combinations.
---

# DimGrow: Memory-Efficient Field-level Embedding Dimension Search

## Quick Facts
- arXiv ID: 2505.12683
- Source URL: https://arxiv.org/abs/2505.12683
- Authors: Yihong Huang; Chen Chu
- Reference count: 40
- Key outcome: DimGrow reduces GPU memory usage by up to 23% during embedding dimension search while maintaining competitive performance

## Executive Summary
This paper addresses the challenge of field-level embedding dimension search in recommendation systems, where different feature fields require different embedding dimensionalities. The key limitation of existing methods is their reliance on memory-intensive SuperNets that enumerate all possible dimension combinations. The authors propose DimGrow, a lightweight approach that starts training from one dimension per feature field and progressively expands or shrinks dimensions based on learned importance scores. Experimental results on three public datasets show that DimGrow achieves competitive or superior performance compared to existing methods while significantly reducing memory consumption during the search phase.

## Method Summary
DimGrow introduces a novel approach to field-level embedding dimension search by starting with minimal initial dimensions and adaptively adjusting them during training. The method employs a Shuffle Gate mechanism that evaluates dimension importance by measuring the impact of shuffling each dimension across samples. Important dimensions maintain high gate values while unimportant ones are pruned. The search process begins with one dimension per feature field and progressively expands or shrinks dimensions based on learned importance scores, eliminating the need for memory-intensive SuperNets that enumerate all possible dimension combinations.

## Key Results
- DimGrow reduces GPU memory usage by up to 23% on large datasets compared to SuperNet-based methods
- The method maintains constant memory footprint even when scaling to higher dimensions
- Performance matches or exceeds existing methods on Aliccp, Avazu, and Criteo datasets
- Search process is faster than traditional methods on larger datasets

## Why This Works (Mechanism)
DimGrow works by addressing the fundamental inefficiency of SuperNet-based dimension search methods. Traditional approaches require enumerating all possible dimension combinations, leading to exponential memory growth. DimGrow's progressive approach starts small and adapts dimensions based on actual feature importance, measured through the Shuffle Gate mechanism. This allows the system to focus computational resources only on dimensions that contribute meaningfully to prediction accuracy.

## Foundational Learning

**SuperNets**: Why needed - Traditional approach for neural architecture search that trains all possible architectures simultaneously. Quick check - Memory scales exponentially with number of dimension options.

**Field-level embeddings**: Why needed - Different feature fields in recommendation systems have varying importance and require different dimensionalities. Quick check - Static dimension assignment leads to suboptimal performance or wasted resources.

**Shuffle Gate mechanism**: Why needed - Provides differentiable importance scores for dimension pruning without requiring separate validation phases. Quick check - Dimension importance measured by impact of shuffling across samples.

## Architecture Onboarding

**Component Map**: Input features -> Embedding layers (progressive dimensions) -> Shuffle Gate evaluation -> Dimension adjustment -> Prediction head

**Critical Path**: Feature input → Embedding lookup → Shuffle Gate importance scoring → Dimension pruning/expanding → Prediction output

**Design Tradeoffs**: Memory efficiency vs. search completeness - DimGrow sacrifices exhaustive search coverage for practical memory constraints. Adaptive vs. static initialization - Progressive approach allows learning optimal dimensions rather than manual specification.

**Failure Signatures**: Memory overflow during search phase indicates SuperNet limitations. Suboptimal performance suggests incorrect dimension initialization or insufficient pruning sensitivity.

**First Experiments**: 1) Baseline comparison with static dimension assignment on small dataset. 2) Memory profiling comparison between DimGrow and SuperNet approach. 3) Ablation study on Shuffle Gate sensitivity parameters.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited ablation studies to validate Shuffle Gate mechanism effectiveness
- Incomplete computational complexity analysis lacking training time overhead details
- Limited dataset coverage potentially restricting generalizability
- No discussion of inference runtime latency implications

## Confidence

| Claim | Confidence |
|-------|------------|
| Memory reduction up to 23% | Medium |
| Competitive performance | Medium |
| Faster search on large datasets | Medium |
| Constant memory scaling | Medium |

## Next Checks

1) Conduct extensive ablation studies comparing Shuffle Gate against alternative dimension importance scoring mechanisms to quantify its contribution to overall performance gains.

2) Evaluate DimGrow's performance on industrial-scale datasets with heterogeneous feature distributions and higher cardinality fields to assess scalability beyond the current experimental scope.

3) Measure and report the computational overhead during both training and inference phases to provide complete resource utilization analysis for practical deployment considerations.