---
ver: rpa2
title: The Effect of Label Noise on the Information Content of Neural Representations
arxiv_id: '2510.06401'
source_url: https://arxiv.org/abs/2510.06401
tags:
- representations
- information
- noise
- label
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how label noise affects the information
  content of neural network hidden representations using a task-agnostic statistical
  measure called Information Imbalance (II). II quantifies the relative information
  between two feature spaces by analyzing how well relative distances between samples
  are preserved when transitioning between spaces.
---

# The Effect of Label Noise on the Information Content of Neural Representations

## Quick Facts
- arXiv ID: 2510.06401
- Source URL: https://arxiv.org/abs/2510.06401
- Reference count: 40
- Primary result: Label noise primarily degrades information in the last layer of neural networks while preserving feature encoding within hidden representations.

## Executive Summary
This study investigates how label noise affects the information content of neural network hidden representations using Information Imbalance (II), a task-agnostic statistical measure that quantifies relative information between feature spaces. The authors establish a theoretical connection between II and conditional mutual information, computing explicit lower bounds for II in Gaussian models. Through experiments on MNIST and CIFAR-10 with varying label noise levels, they demonstrate that II captures the double descent phenomenon in test error and reveals that overparameterized networks develop robust hidden representations regardless of label noise, while underparameterized networks benefit from noisy labels.

## Method Summary
The authors employ Information Imbalance (II) as their primary analytical tool, which measures how well relative distances between samples are preserved when transitioning between feature spaces. They establish theoretical connections between II and conditional mutual information, providing explicit lower bounds for II in Gaussian models including one-dimensional embeddings and denoising models. Empirically, they analyze single-layer fully connected networks on MNIST and convolutional networks on CIFAR-10, systematically varying label noise levels. The study examines information flow across different network layers, comparing representations from independently trained networks and analyzing how information degradation correlates with network performance.

## Key Results
- The double descent phenomenon in test error is mirrored by a double descent in II between independent network representations.
- In underparameterized regimes, noisy labels yield more informative representations than clean labels, while overparameterized regimes show approximately equal informativeness regardless of label noise.
- Label noise primarily degrades information in the last layer (between hidden and pre-softmax representations), while feature encoding within hidden representations remains relatively preserved.

## Why This Works (Mechanism)
The effectiveness of Information Imbalance stems from its ability to capture the relative information content between feature spaces without requiring task-specific labels. By measuring how well distance relationships between samples are preserved across representations, II provides a task-agnostic metric that reflects the underlying information structure. The connection to conditional mutual information provides theoretical grounding, while the empirical validation across different architectures and noise levels demonstrates its practical utility for understanding generalization in neural networks.

## Foundational Learning
- Information Imbalance (II): A statistical measure quantifying relative information between feature spaces by analyzing distance preservation. Why needed: Provides task-agnostic evaluation of representation quality. Quick check: Can be computed between any two feature representations without requiring labels.
- Conditional Mutual Information: Measures information shared between variables given a third variable. Why needed: Establishes theoretical foundation for II's effectiveness. Quick check: Related to II through theoretical bounds derived in Gaussian models.
- Double Descent Phenomenon: Test error decreases, increases, then decreases again as model capacity increases. Why needed: Provides context for understanding how representation quality evolves with network size. Quick check: Observed both in test error and II between representations.
- Gaussian Models: Theoretical framework used to derive explicit bounds for II. Why needed: Enables analytical tractability and provides baseline understanding. Quick check: One-dimensional embedding and denoising models provide explicit lower bounds.
- Label Noise Robustness: The ability of representations to maintain information content despite corrupted labels. Why needed: Central question of how training data quality affects learned representations. Quick check: Overparameterized networks show similar II regardless of noise level.

## Architecture Onboarding
Component map: Input data -> Hidden layers -> Pre-softmax layer -> Output
Critical path: Data flows through hidden layers where information is encoded, then through the final transformation to logits. The critical information bottleneck appears to be between hidden representations and pre-softmax outputs.
Design tradeoffs: The study focuses on fully connected networks for MNIST and convolutional networks for CIFAR-10, representing different architectural approaches to image classification. The choice of single-layer networks for MNIST simplifies analysis while still capturing essential phenomena.
Failure signatures: When label noise dominates, information degradation is most pronounced in the transition from hidden to pre-softmax representations. Overparameterized networks fail to show the clean/noisy distinction in representation quality.
First experiments: 1) Measure II between hidden representations and pre-softmax outputs across different noise levels. 2) Compare II values between independently trained networks with same architecture. 3) Analyze how II changes across different network depths.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical framework relies heavily on Gaussian assumptions, limiting applicability to non-Gaussian data distributions.
- Focus on MNIST and CIFAR-10 datasets raises questions about generalizability to larger-scale problems.
- The study examines only fully connected and convolutional architectures, leaving uncertainty about applicability to other architectures like transformers.

## Confidence
High: The connection between II and conditional mutual information, observed robustness in overparameterized networks, and correlation between information loss and performance.
Medium: The claim that II serves as an effective unsupervised tool for understanding generalization, and the specific mechanisms by which label noise affects different network layers.
Low: Generalizability to more complex architectures and larger-scale datasets beyond MNIST and CIFAR-10.

## Next Checks
1. Test the II metric's effectiveness on larger-scale datasets and more diverse model architectures, particularly transformers and attention-based networks.
2. Conduct ablation studies to isolate the specific mechanisms by which label noise affects different network layers and their information content.
3. Perform theoretical analysis to strengthen the connection between II and conditional mutual information, particularly in non-Gaussian settings and complex representations.