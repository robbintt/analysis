---
ver: rpa2
title: 'WikiTermBase: An AI-Augmented Term Base to Standardize Arabic Translation
  on Wikipedia'
arxiv_id: '2505.20369'
source_url: https://arxiv.org/abs/2505.20369
tags:
- term
- arabic
- language
- wikipedia
- terms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents WikiTermBase, an open-source AI-augmented term
  base for standardizing Arabic translation on Wikipedia. It addresses the lack of
  terminology consistency in Arabic Wikipedia, where translators spend significant
  time manually searching for technical terms.
---

# WikiTermBase: An AI-Augmented Term Base to Standardize Arabic Translation on Wikipedia

## Quick Facts
- arXiv ID: 2505.20369
- Source URL: https://arxiv.org/abs/2505.20369
- Reference count: 3
- Primary result: 90% positive user ratings, ~20% time savings for Arabic Wikipedia translation

## Executive Summary
WikiTermBase addresses terminology inconsistency in Arabic Wikipedia by providing an open-source term base that aggregates over 900,000 terms from multiple sources and uses LLMs for semantic grouping. The system ranks Arabic equivalents by morphological frequency across dictionaries and maps terms to Wiktionary sense definitions using Claude Sonnet 3.7 and GPT-4. User testing with 80 editors showed significant improvements in translation speed and standardization, with editors reporting approximately 20% time savings per article.

## Method Summary
The method compiles dictionary data from ~50 digitized sources into a MariaDB database on Toolforge, with terms preprocessed to remove diacritics and special characters. Wiktionary serves as the reference layer with unique sense IDs extracted from definitions. LLMs map Arabic term definitions to these sense IDs, enabling sense-disambiguated retrieval. Users query the system directly from Wikipedia's editing interface via SQL, receiving ranked Arabic equivalents grouped by morphological frequency and semantic similarity. The approach combines automated semantic mapping with frequency-based ranking to surface the most commonly accepted translations.

## Key Results
- 90% of 80 tested editors rated the tool positively for improving translation speed and standardization
- Users estimated ~20% time savings per 250-word article translated
- System successfully aggregates over 900,000 terms from multiple sources with morphological and semantic grouping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frequency-based morphological aggregation increases translation consistency by surfacing the most commonly agreed-upon equivalent across lexicographic sources.
- Mechanism: Terms are grouped by morphological form after preprocessing (diacritics, special characters removed). The system recommends the most frequent form across all dictionaries—in some cases, over 20 sources agree on a standard equivalent.
- Core assumption: Morphological frequency across dictionaries correlates with appropriate standardization for Wikipedia translation contexts.
- Evidence anchors:
  - [abstract] "Terms are mapped by morphological frequency and semantic similarity"
  - [section] "The user is recommended the most frequent form across all dictionaries. In some cases, over 20 sources will agree on the standard equivalent."
  - [corpus] Weak direct validation; neighbor papers address Arabic NLP challenges but not frequency-based term ranking specifically.
- Break condition: If dictionaries systematically bias toward non-standard or archaic forms, frequency ranking may reinforce poor choices.

### Mechanism 2
- Claim: LLM-based semantic grouping enables sense-disambiguated term retrieval, improving relevance over purely morphological matching.
- Mechanism: Wiktionary sense definitions are extracted, assigned unique IDs, and mapped to Arabic equivalents using LLMs (Claude Sonnet 3.7, GPT-4). A term like "scale" has at least five unique senses, some sharing Arabic equivalents.
- Core assumption: LLM semantic mapping accuracy is sufficient for production use; evaluation is ongoing ("tentative results are promising").
- Evidence anchors:
  - [abstract] "uses LLMs for semantic grouping"
  - [section] "LLMs, including Claude Sonnet 3.7 and GPT-4, were used for semantic grouping... A term like 'scale' has at least five unique senses in our term base"
  - [corpus] Related work (Hala, SHAMI-MT) demonstrates LLM viability for Arabic tasks but does not validate this specific semantic mapping approach.
- Break condition: If LLM mapping error rates exceed acceptable thresholds for technical terminology, users may receive incorrect sense recommendations.

### Mechanism 3
- Claim: Direct integration into Wikipedia's editing interface via SQL queries reduces context-switching overhead, contributing to reported time savings.
- Mechanism: The term base is accessed through SQL queries sent to a MariaDB instance hosted on Toolforge. Users query from within Wikipedia's editing interface without external tool navigation.
- Core assumption: Reduced context-switching directly causes the ~20% time savings reported by users; no controlled experiment isolates this variable.
- Evidence anchors:
  - [abstract] "The tool integrates directly into Wikipedia's editing interface via SQL queries to a MariaDB database"
  - [section] "Users can query the term base directly from within Wikipedia's editing interface... Users estimated that the tool saved them about 20% of the time needed to translate a one page (250 word) long article."
  - [corpus] No corpus validation; integration-ergonomics effects are domain-specific.
- Break condition: If network latency or database load degrades query response times, workflow friction may negate integration benefits.

## Foundational Learning

- Concept: **Term base (terminology database)**
  - Why needed here: The entire system is a term base; understanding how term bases support consistency and time-saving is prerequisite to evaluating the approach.
  - Quick check question: Can you explain how a term base differs from a general-purpose dictionary in a translation workflow?

- Concept: **Morphological frequency analysis**
  - Why needed here: Arabic has rich morphology; the system groups terms morphologically before ranking by frequency.
  - Quick check question: Given two Arabic word forms that differ only by diacritics, should they be counted as the same term for frequency purposes?

- Concept: **Word sense disambiguation (WSD)**
  - Why needed here: The system maps terms to Wiktionary sense IDs; understanding sense granularity is essential for interpreting results.
  - Quick check question: Why might the word "scale" require five or more distinct sense IDs in a term base?

## Architecture Onboarding

- Component map:
  Source data (50 digitized dictionaries) -> Processing layer (LLM semantic mapping) -> Reference layer (Wiktionary sense IDs) -> Storage layer (MariaDB on Toolforge) -> Integration layer (SQL queries from Wikipedia editor) -> Output (ranked Arabic equivalents)

- Critical path:
  1. User submits query from Wikipedia editor
  2. SQL query retrieves exact matches + similar entries (ranked by edit distance)
  3. Results grouped by unique source-language term (morphological ID)
  4. Senses ranked by frequency in database
  5. Arabic equivalents retrieved per sense ID (LLM-mapped)
  6. Output ranked by statistical analysis after preprocessing cleanup

- Design tradeoffs:
  - Wiktionary chosen for granularity and openness, but sense definitions may inconsistent quality
  - LLM mapping enables scalability but introduces unquantified error rates (evaluation ongoing)
  - MariaDB/Toolforge provides Wikimedia ecosystem integration; future Wikidata migration uncertain due to data scale
  - ~20% of 950K terms are duplicates—cleanup strategy affects storage and retrieval

- Failure signatures:
  - Multi-word search identified as improvement area (user feedback)
  - LLM semantic mapping accuracy not yet fully evaluated
  - User contribution mechanism not yet implemented
  - Potential Wikidata migration blocked by scale and community willingness

- First 3 experiments:
  1. **Single-term precision test**: Query 20 technical terms with known standard translations; measure whether frequency-ranked recommendation matches expected standard.
  2. **Sense disambiguation validation**: For polysemous terms (e.g., "scale," "absorb"), compare LLM-assigned sense IDs against human expert judgments; calculate accuracy.
  3. **Integration latency benchmark**: Measure end-to-end query response time from Wikipedia editor under typical load; identify if latency exceeds acceptable workflow thresholds (~2 seconds).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the quantitative accuracy of the LLM-based semantic mapping compared to human-annotated standards?
- Basis in paper: [explicit] The authors state they are "currently going through a multi-step evaluation of the LLM accuracy for semantic mapping" and that results are currently only "tentative."
- Why unresolved: The paper presents the tool and initial user feedback but the specific technical evaluation of the AI's grouping precision is an ongoing process not concluded in the text.
- What evidence would resolve it: A published error analysis or accuracy metric (e.g., F1 score) derived from the multi-step evaluation mentioned by the authors.

### Open Question 2
- Question: Is it feasible to migrate the massive dataset into Wikidata's Lexicographic Data extension without losing functionality?
- Basis in paper: [explicit] The paper notes that a future migration to Wikidata is "contingent on the feasibility of transferring our massive data set into the Lexicographic Data extension."
- Why unresolved: The authors explicitly list the technical feasibility of handling such a scale within Wikidata's specific infrastructure as an unresolved condition.
- What evidence would resolve it: A successful proof-of-concept migration or a technical report detailing the compatibility of the WikiTermBase schema with Wikidata's lexicographical extension.

### Open Question 3
- Question: How can the database be effectively opened for user contributions without compromising data quality?
- Basis in paper: [explicit] The results section identifies "opening up the database for user contributions" as one of the "main areas of improvement" derived from user feedback.
- Why unresolved: The current system relies on compiled dictionaries and AI mapping; a mechanism for dynamic user input is desired but not yet implemented or described.
- What evidence would resolve it: The design and deployment of a contribution interface that includes moderation or validation workflows for new terms.

## Limitations
- LLM semantic mapping accuracy not formally evaluated - only "tentative results are promising"
- ~20% time savings figure comes from self-reported user estimates rather than controlled experiments
- Multi-word term searches currently fail and need improvement
- Potential Wikidata migration path uncertain due to data scale and community adoption

## Confidence
- **High confidence**: Database architecture, SQL integration methodology, frequency-based ranking mechanism
- **Medium confidence**: User-reported time savings and satisfaction metrics, system usability
- **Low confidence**: LLM semantic mapping accuracy, long-term scalability with Wikidata integration

## Next Checks
1. **Controlled time-motion study**: Conduct A/B testing where Arabic Wikipedia editors translate articles with and without WikiTermBase, measuring actual time differences and error rates.

2. **LLM mapping accuracy audit**: Sample 100 polysemous terms and have bilingual experts verify whether LLM-assigned sense IDs match correct semantic interpretations.

3. **Multi-word query performance test**: Implement phrase-level search capability and evaluate precision/recall for compound technical terms common in Wikipedia articles.