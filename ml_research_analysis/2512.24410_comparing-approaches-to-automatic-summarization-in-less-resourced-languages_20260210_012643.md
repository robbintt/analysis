---
ver: rpa2
title: Comparing Approaches to Automatic Summarization in Less-Resourced Languages
arxiv_id: '2512.24410'
source_url: https://arxiv.org/abs/2512.24410
tags:
- lr-sum
- languages
- language
- summarization
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study compares various approaches to automatic summarization\
  \ for less-resourced languages, finding that multilingual mT5 fine-tuning significantly\
  \ outperforms zero-shot LLM prompting. Three data augmentation strategies\u2014\
  extractive summarization, self-training, and back-summarization\u2014were tested\
  \ with Wikipedia articles, showing modest improvements over individual language\
  \ baselines but not surpassing multilingual transfer performance."
---

# Comparing Approaches to Automatic Summarization in Less-Resourced Languages

## Quick Facts
- **arXiv ID:** 2512.24410
- **Source URL:** https://arxiv.org/abs/2512.24410
- **Reference count:** 0
- **Primary result:** Multilingual mT5 fine-tuning outperforms zero-shot LLM prompting for most less-resourced language summarization tasks.

## Executive Summary
This study systematically compares approaches to automatic summarization for less-resourced languages, evaluating fine-tuned mT5 models against zero-shot LLM prompting, data augmentation strategies, and translation-based pipelines. The results demonstrate that multilingual fine-tuning of mT5 significantly outperforms individual language fine-tuning with data augmentation and zero-shot LLM performance for most metrics. While data augmentation strategies (extractive summarization, self-training, and back-summarization) showed modest improvements over individual language baselines, they did not close the performance gap with multilingual transfer. The work also reveals that smaller LLMs frequently produce English output when summarizing non-Roman script languages, while reference-based metrics (ROUGE, BERTScore) proved more reliable than reference-free LLM judges for less-resourced languages.

## Method Summary
The study compares summarization approaches across 18 less-resourced languages using LR-Sum and XL-Sum datasets. Individual language fine-tuning used up to 6k synthetic Wikipedia-based examples per language through three augmentation strategies: extractive summarization with LexRank, self-training with mT5-generated summaries, and back-summarization creating synthetic documents from extractive summaries. Multilingual fine-tuning used upsampling to balance language representation. Zero-shot LLM prompting tested both smaller models (Mixtral 8x7b, Llama 3 8b, Aya-101) and larger models (Gemma-3 27b, Llama 3.3 70b, Aya-Expanse 32b) with explicit language prompts. Evaluation used ROUGE-1/2/L (with mT5 tokenizer), BERTScore, and M-Prometheus, measuring English content percentage via CLD3.

## Key Results
- Multilingual mT5 fine-tuning outperformed zero-shot LLM prompting for most metrics across all tested languages
- Data augmentation improved individual language fine-tuning but did not surpass multilingual transfer performance
- Smaller LLMs produced significant English output for non-Roman scripts (up to 75.4% for Georgian with Mixtral)
- Reference-based metrics (ROUGE, BERTScore) showed more consistent results than M-Prometheus for less-resourced languages
- The translation-summarize-translation pipeline performed worse than direct summarization

## Why This Works (Mechanism)

### Mechanism 1
Multilingual fine-tuning of mT5 outperforms single-language fine-tuning with data augmentation for most less-resourced languages because multilingual training enables cross-lingual transfer where patterns learned from relatively higher-resource languages in the joint training set inform summarization for lower-resource languages. Upsampling balances language representation without artificially inflating low-resource example frequency. The core assumption is that the summarization task shares learnable structure across languages that can transfer despite typological differences.

### Mechanism 2
Data augmentation via extractive, self-training, and back-summarization approaches improves single-language fine-tuning but does not close the gap to multilingual transfer because synthetic document-summary pairs from Wikipedia expand training data quantity while introducing noise from automatic generation. Extractive-training uses LexRank to select salient sentences; self-training uses model-generated summaries; back-summarization generates synthetic documents from extractive summaries. All increase example counts but vary in synthetic data quality, with Armenian back-summarization showing degraded performance due to high novelty scores (97.33) indicating irrelevant word generation.

### Mechanism 3
Smaller LLMs (7-13B parameters) produce substantial English output when summarizing less-resourced languages, particularly those with non-Roman scripts, due to training data imbalance. Pretraining corpora are dominated by higher-resourced languages, causing models to revert to English when prompted in underrepresented languages—either refusing the task, summarizing in English, or switching mid-generation. This effect intensifies for scripts with less pretraining representation, with Aya-101 being the notable exception due to its multilingual instruction training.

## Foundational Learning

- **mT5 (multilingual Text-to-Text Transfer Transformer):** Why needed here: The strongest results come from fine-tuning mT5-base, not from LLMs. Understanding encoder-decoder seq2seq architecture vs. decoder-only LLMs explains why smaller fine-tuned models outperform larger prompted ones for this task. Quick check question: Given the paper's hyperparameters (beam size 4, max target length 512, learning rate 5e-4), would you expect mT5-large to improve results proportionally, or might compute budget favor more epochs on mT5-base?

- **ROUGE/BERTScore/M-Prometheus evaluation tradeoffs:** Why needed here: The paper shows M-Prometheus favors LLM outputs while reference-based metrics favor mT5. Understanding why LLM judges may be unreliable for less-resourced languages prevents overconfident evaluation. Quick check question: If M-Prometheus is trained primarily on 6 higher-resource languages, why might its scores show higher variance for Armenian summaries than for English?

- **Cross-lingual transfer in multilingual models:** Why needed here: Multilingual fine-tuning with upsampling outperforms single-language training even with augmentation. Understanding transfer mechanics helps predict which language combinations will benefit most. Quick check question: Would adding a typologically similar higher-resource language (e.g., Persian for Kurdish) provide more transfer benefit than adding a higher-resource but typologically distant language (e.g., English)?

## Architecture Onboarding

- **Component map:** LR-Sum + XL-Sum datasets -> Wikipedia articles (≥5 sentences, filtered using Segment Any Text) -> Three augmentation strategies (Extractive→LexRank; Self-Training→mT5 inference; Back-Summarization→reverse seq2seq model) -> mT5-base fine-tuning (3 epochs, batch size 32, upsampling factor 0.5 for multilingual) -> Zero-shot LLMs (Mixtral 8x7b, Llama 3 8b, Aya-101, Gemma-3 27b, Llama 3.3 70b, Aya-Expanse 32b) via Ollama/vLLM -> Post-process LLM output to remove English commentary (pattern matching + CLD3 language ID) -> ROUGE-1/2/L (mT5 tokenizer), BERTScore, M-Prometheus evaluation

- **Critical path:** 1. Filter Wikipedia articles → augment training data (max 6k synthetic per language) 2. Fine-tune mT5 multilingually with upsampling 3. Run LLM inference with explicit language prompts 4. Post-process LLM output to remove English commentary (pattern matching + CLD3 language ID) 5. Evaluate with multiple metrics—report ROUGE-L and BERTScore as primary

- **Design tradeoffs:** Individual vs. multilingual fine-tuning: Individual allows language-specific optimization; multilingual provides transfer benefit. Augmentation quantity vs. quality: More synthetic data helps until noise dominates (see Armenian back-summarization failure). LLM size vs. language fidelity: Larger LLMs add English commentary; smaller LLMs refuse or switch entirely. Reference-based vs. LLM-as-judge: M-Prometheus favors LLM outputs; ROUGE/BERTScore favor mT5.

- **Failure signatures:** High English percentage in LLM output (>30%) → likely non-Roman script, consider Aya models or pipeline approaches. ROUGE scores improve but novelty scores spike (>50%) → synthetic data may be introducing irrelevant generation. M-Prometheus disagrees strongly with ROUGE/BERTScore → verify language is in M-Prometheus training set.

- **First 3 experiments:** 1. Baseline reproduction: Fine-tune mT5-base multilingually on combined LR-Sum + XL-Sum data with paper's hyperparameters. Verify ROUGE-L scores are within ±2 points of reported values for 3 test languages. 2. Ablation on augmentation strategy: For a single language (e.g., Sorani Kurdish), compare all three augmentation approaches with identical synthetic data quantities (3k examples). Measure ROUGE-L and novelty scores to identify best strategy per language characteristics. 3. LLM language fidelity test: Run zero-shot summarization with Mixtral, Llama 3, and Aya-101 on 100 examples each for 3 scripts (Roman: Macedonian; non-Roman: Georgian, Amharic). Measure English percentage via CLD3 to confirm paper's script-correlation finding.

## Open Questions the Paper Calls Out

- **How can LLM-based evaluators like M-Prometheus be calibrated to reliably assess summary quality in less-resourced languages without exhibiting a bias toward high-resource model outputs?** The authors state that M-Prometheus "appeared less reliable when evaluating less-resourced language summaries" and "tends to favor larger LLM model output," suggesting a bias in the judge itself. The study identifies this as a limitation because the LLM judge was trained primarily on high-resourced languages, leading to higher variance and poor alignment with reference-based metrics for low-resource languages. Development of a correlation analysis between LLM judge scores and human evaluations specifically for low-resource languages, or the release of a fine-tuned evaluator with reduced high-resource bias, would resolve this question.

- **Do automated metrics (ROUGE, BERTScore) accurately reflect human judgments of summary quality for the specific less-resourced languages studied (e.g., Armenian, Khmer, Sorani Kurdish)?** The paper explicitly lists the reliance on automated metrics as a limitation, noting that "human evaluation can be expensive and especially difficult for less-resourced languages." The study relies entirely on automated proxies for quality because recruiting speakers for thousands of summary evaluations was cost-prohibitive. A human evaluation study involving native speakers rating the factual consistency and fluency of the generated summaries compared to the automated scores would resolve this question.

- **Can specific prompting strategies or constrained decoding methods prevent smaller LLMs from defaulting to English when summarizing non-Roman script languages?** The authors found that "Smaller LLMs like Mixtral and Llama 3 produced significant amounts of English output, particularly for languages with non-Roman scripts." The paper identifies the issue (language switching/refusals) but does not test interventions to enforce target language adherence in smaller models. An ablation study testing various prompt engineering techniques (e.g., few-shotting, grammar constraints) and measuring the reduction in English output using language identification tools would resolve this question.

## Limitations

- Limited language scope (18 total languages) constrains generalizability to the broader universe of less-resourced languages
- Reliance on Wikipedia as sole synthetic data source introduces potential domain mismatch with news-based summarization tasks
- Evaluation methodology's dependence on reference-based metrics and LLM judges presents inherent limitations for less-resourced languages
- No systematic investigation of augmentation quantity effects or extremely low-resource language performance

## Confidence

- **High Confidence:** Multilingual mT5 fine-tuning outperforming zero-shot LLM prompting for most less-resourced languages
- **Medium Confidence:** Data augmentation improving single-language fine-tuning but not closing the gap to multilingual transfer
- **Medium Confidence:** Smaller LLMs producing substantial English output for non-Roman scripts
- **Low Confidence:** M-Prometheus reliability for less-resourced languages

## Next Checks

1. **Reproduce core results:** Fine-tune mT5-base multilingually on combined LR-Sum + XL-Sum data using the specified hyperparameters. Compare ROUGE-L scores against the paper's reported values for 3 test languages (e.g., Sorani Kurdish, Khmer, Armenian) with 500-bootstrap standard errors.

2. **Augmentation strategy ablation:** For a single language (e.g., Sorani Kurdish), systematically compare all three augmentation approaches (extractive, self-training, back-summarization) using identical synthetic data quantities (3k examples). Measure both ROUGE-L and novelty scores to identify which strategy works best for different language characteristics.

3. **LLM language fidelity validation:** Run zero-shot summarization with Mixtral, Llama 3, and Aya-101 on 100 examples each for 3 scripts (Roman: Macedonian; non-Roman: Georgian, Amharic). Quantify English percentage using CLD3 to confirm the paper's finding that non-Roman scripts correlate with higher English contamination in smaller LLMs.