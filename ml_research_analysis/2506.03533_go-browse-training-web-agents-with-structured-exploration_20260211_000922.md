---
ver: rpa2
title: 'Go-Browse: Training Web Agents with Structured Exploration'
arxiv_id: '2506.03533'
source_url: https://arxiv.org/abs/2506.03533
tags:
- tasks
- page
- go-browse
- exploration
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Go-Browse, a method for automatically collecting
  web agent training data through structured exploration. The core idea is to treat
  website exploration as a graph traversal problem, building a map of discovered URLs
  and reusing information across exploration episodes.
---

# Go-Browse: Training Web Agents with Structured Exploration

## Quick Facts
- arXiv ID: 2506.03533
- Source URL: https://arxiv.org/abs/2506.03533
- Reference count: 40
- Achieves 21.7% success rate on WebArena tasks, surpassing state-of-the-art for sub-10B models by 2.9%

## Executive Summary
Go-Browse addresses the inefficiency of web agent training data collection by treating website exploration as a graph traversal problem. Instead of independent exploration episodes, it builds a persistent graph of discovered URLs and reuses navigation effort across episodes through a frontier-based approach. The method achieves 21.7% success rate on WebArena tasks when fine-tuning a 7B parameter model, outperforming previous state-of-the-art approaches and GPT-4o-mini. The key innovation lies in decoupling navigation from local task solving through "prefixed sampling," enabling weaker models to generate high-quality training data by starting from relevant webpages rather than the homepage.

## Method Summary
Go-Browse collects training data through structured exploration by maintaining a graph G=(V,E) of URLs and trajectories with an exploration frontier F. The outer loop selects URLs from F, while the inner loop generates data using NavExplorer/PageExplorer agents to propose tasks, a FeasibilityChecker to validate them, and Solvers to generate training traces. The system uses prefixed sampling (starting from relevant webpages) and unprefixed sampling (starting from homepage) to balance navigation learning with local interaction skills. After collecting 10K successful trajectories across 100 URLs, a 7B parameter language model is fine-tuned using supervised learning on this dataset.

## Key Results
- 21.7% success rate on WebArena tasks, surpassing previous sub-10B state-of-the-art by 2.9%
- Outperforms GPT-4o-mini by 2.4% on WebArena
- Demonstrates higher dataset diversity compared to previous methods
- Shows particular strength on deeper, longer-horizon tasks requiring multiple navigation steps

## Why This Works (Mechanism)

### Mechanism 1
Treating website exploration as graph traversal improves data collection efficiency by building a persistent graph of URLs and reusing information across episodes. The system maintains an exploration frontier of discovered but unexplored pages, resetting future episodes to these frontier nodes rather than the homepage. This reuses navigation effort and ensures broader coverage. The mechanism breaks down in SPAs where URLs don't uniquely determine visible state.

### Mechanism 2
Decoupling global navigation from local task solving via prefixed sampling enables weaker models to generate high-quality training data. By starting tasks from specific webpages rather than the homepage, the system isolates the difficulty of local interaction from global navigation. This bootstrapping effect allows 7B models to succeed at complex tasks they couldn't reach from scratch. The mechanism may fail if deployment requires discovering pages from the homepage.

### Mechanism 3
Agent-driven task proposal grounded in dynamic interaction yields more feasible tasks than static instruction generation. Go-Browse uses web agents to interact with pages, clicking buttons and traversing links to identify executable tasks. This grounds proposals in actual environment capabilities rather than hallucinating from static screenshots. The approach may be limited if the exploration agent itself is brittle or prone to loops.

## Foundational Learning

- **Graph Traversal & Frontier-Based Exploration**: Essential for understanding the BFS-like traversal logic where frontier represents nodes to be explored and visited set represents processed nodes. Quick check: If an agent discovers links A, B, and C from homepage, adds them to frontier, then explores A to find D, what is the frontier state before processing D?

- **The ReAct Pattern (Reason + Act)**: Agents use this paradigm outputting chain-of-thought and executable function calls. Training data consists of (state, thought, action) tuples. Quick check: In a ReAct trace, what's the difference between "thought" and "action" fields?

- **Supervised Fine-Tuning (SFT) vs. Reinforcement Learning (RL)**: The approach uses SFT on successful trajectories only, teaching imitation rather than optimization. Quick check: Why might training on both successful and failed trajectories be detrimental for SFT?

## Architecture Onboarding

- **Component map**: Graph Manager -> Outer Loop (Coordinator) -> Inner Loop (Data Generation Pipeline: NavExplorer/PageExplorer -> FeasibilityChecker -> Solvers)
- **Critical path**: The FeasibilityChecker is the bottleneck, consuming most compute through strong model evaluations and VLM judgments. Dataset quality depends almost entirely on this filter's precision.
- **Design tradeoffs**: Prefixed sampling (cheap, high local success) vs. unprefixed sampling (hard, teaches navigation); Exploration budget (15 steps) balances coverage vs. cost.
- **Failure signatures**: Infinite loops in NavExplorer stall frontier; empty datasets result from strict VLM-judges or weak FeasibilityCheckers; state desynchronization occurs when URLs point to changed page states.
- **First 3 experiments**: 1) Validate Feasibility Checker on curated tasks measuring precision/recall; 2) Compare Go-Browse vs. Random Walk on controlled website for unique URLs discovered; 3) Prefixed Sampling Ablation testing weak model success rates with/without prefixed starts.

## Open Questions the Paper Calls Out

- Incorporating unsuccessful trajectories through RL-based objectives could improve results beyond current SFT-only approach
- Scaling to open web outside controlled benchmarks may reveal limitations of current graph traversal logic
- Increasing model size beyond 7B parameters may unlock greater performance improvements
- Computational cost comparison with interaction-first methods like NNetNav remains unevaluated

## Limitations

- URL-state assumption breaks down for SPAs where same URL represents different visible states
- Dataset quality depends heavily on expensive strong model feasibility checking, limiting scaling
- Evaluation focuses on WebArena benchmark which may not represent real-world web complexity

## Confidence

- High confidence: Graph traversal mechanism and navigation-local task decoupling are well-supported by ablation studies
- Medium confidence: Superiority of agent-driven task proposal has qualitative support but lacks direct quantitative comparison
- Low confidence: Claim about higher dataset diversity lacks rigorous quantification and comparison

## Next Checks

1. Test Go-Browse on real-world SPA (Gmail or React dashboard) to measure URL-state misalignment effects on exploration efficiency
2. Conduct direct comparison between agent-driven and static instruction generation on same websites, measuring task feasibility rates and diversity metrics
3. Evaluate 7B model on held-out WebArena subset requiring deep navigation (>5 clicks) to verify advantage for longer-horizon tasks