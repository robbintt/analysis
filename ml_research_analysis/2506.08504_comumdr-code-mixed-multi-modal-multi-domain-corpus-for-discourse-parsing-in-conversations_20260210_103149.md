---
ver: rpa2
title: 'CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing
  in conversations'
arxiv_id: '2506.08504'
source_url: https://arxiv.org/abs/2506.08504
tags:
- discourse
- relation
- comumdr
- link
- parsing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoMuMDR, a large-scale code-mixed (Hindi-English)
  multimodal (text+audio) multi-domain corpus for discourse parsing in conversations.
  It contains 799 dialogues from customer call center interactions across five domains,
  annotated with nine discourse relation types.
---

# CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations

## Quick Facts
- **arXiv ID**: 2506.08504
- **Source URL**: https://arxiv.org/abs/2506.08504
- **Reference count**: 40
- **Primary result**: State-of-the-art discourse parsing models perform significantly worse on code-mixed (Hindi-English) multimodal conversations compared to English-only datasets, highlighting challenges of multi-domain code-mixing.

## Executive Summary
This paper introduces CoMuMDR, a large-scale code-mixed (Hindi-English) multimodal (text+audio) multi-domain corpus for discourse parsing in conversations. It contains 799 dialogues from customer call center interactions across five domains, annotated with nine discourse relation types. Experiments with state-of-the-art discourse parsing models show significantly lower performance on CoMuMDR compared to existing English-only datasets (STAC, Molweni), highlighting the challenges of multi-domain code-mixed conversations. GPT-4o also underperforms, emphasizing the need for specialized models. The corpus aims to support research in realistic, multilingual dialogue understanding.

## Method Summary
The CoMuMDR corpus consists of 799 dialogues (8,811 utterances) from customer call center conversations across five domains. Audio is processed through ASR (WavLM + KenLM) and diarization (Titanet) models, with manual corrections using a "Diarization Continuation" label. Five baseline discourse parsing models are evaluated: Deep Sequential, Hierarchical, Structure-aware, SSP-BERT+SCIJE, and SDDP, using either English-only (GloVe/RoBERTa) or multilingual (`paraphrase-xlm-r-multilingual-v1`) embeddings. Models are trained on 639 dialogues, validated on 79, and tested on 81. The primary evaluation metric is F1-score for both link prediction and relation classification.

## Key Results
- State-of-the-art models show significantly lower performance on CoMuMDR compared to English-only datasets like STAC and Molweni.
- SSP-BERT+SCIJE with multilingual embeddings achieves the best performance but still lags behind its English-only counterparts.
- GPT-4o underperforms specialized models, emphasizing the need for domain-specific approaches.
- Error analysis reveals structural constraint violations and semantic representation mismatches as key failure modes.

## Why This Works (Mechanism)

### Mechanism 1: Real-World Noise Propagation
Introducing realistic, noisy inputs (ASR errors, overlapping speech, diarization failures) disrupts parsers trained on clean text, exposing brittleness in SoTA models. The corpus utilizes ASR and diarization models (Titanet) on raw audio. Errors introduced here (e.g., incorrectly split utterances) propagate downstream, forcing models to resolve structural ambiguities that are pre-cleaned in datasets like STAC.

### Mechanism 2: Semantic Representation Mismatch
Standard English or generic multilingual embeddings fail to capture the nuanced semantics of code-mixed "Hinglish," degrading relation classification. Models often rely on English-specific features (e.g., RoBERTa) or generic multilingual vectors. When processing Hindi-English code-mixing, the semantic vector space misaligns with the syntactic reality, preventing the model from distinguishing subtle relations like "Contrast" vs. "Correction."

### Mechanism 3: Structural Constraint Violation
Enforcing tree-structural assumptions on dialogue data that naturally forms Directed Acyclic Graphs (DAGs) reduces parsing accuracy. Models like SDDP convert discourse graphs to Minimum Spanning Trees (MST) for efficient decoding. In customer call centers, utterances often link to multiple future nodes (DAG structure). Forcing a tree structure discards valid discourse links, causing information loss.

## Foundational Learning

**Discourse Parsing & SDRT**
- Why needed: This is the core task. You must understand that the goal is to link Elementary Discourse Units (EDUs) and classify the relationship (e.g., Continuation, Contrast).
- Quick check: What is the difference between an EDU and a full utterance in the context of this paper?

**Speaker Diarization**
- Why needed: The paper highlights that errors in splitting audio by speaker (diarization) directly impact the quality of the discourse annotations.
- Quick check: Why did the authors need to add a "Diarization Continuation" label during annotation?

**Code-Mixing (Hinglish)**
- Why needed: The dataset is not monolingual. Understanding that Hindi and English are mixed within sentences is crucial for selecting the right embedding models.
- Quick check: Why would a standard English BERT model struggle with the input text in CoMuMDR?

## Architecture Onboarding

**Component map**: Raw Audio (8kHz) -> Preprocessing: ASR (WavLM + KenLM) + Diarization (Titanet) -> Text: Anonymized Transcriptions -> Feature Extraction: Embeddings (RoBERTa/XLM-R) -> Model: Encoder (Hierarchical/Sequential) + Decoder (Link/Relation Classifier).

**Critical path**: The transition from Audio features to Text embeddings. This is where the "multi-modal" advantage is currently underutilized (as noted in Future Directions) and where the code-mixing challenge is most acute.

**Design tradeoffs**:
- Tree vs. DAG Decoding: SDDP uses tree constraints for efficiency but loses structural accuracy (valid for STAC, invalid for CoMuMDR).
- Joint vs. Sequential Prediction: SDDP predicts link+relation simultaneously (harder), while SSP-BERT+SCIJE separates them (easier to debug).

**Failure signatures**:
- High Link, Low Relation F1: Indicates the model finds that things are connected but fails to classify how (common in Table 3 for CoMuMDR).
- Zero F1 on Rare Classes: Models collapse on low-support labels like "Correction" or "Conditional" (Table 4).
- Diarization Drift: Unnatural jumps in speaker turns or "overlapping" text segments in the input data.

**First 3 experiments**:
1. Baseline Replication: Run SSP-BERT+SCIJE with `paraphrase-xlm-r-multilingual-v1` embeddings to verify the ~0.56 Link+Relation F1 score reported in Table 3.
2. Ablation on Noise: Manually inspect "Diarization Continuation" instances to see if correcting the text span improves the model's ability to predict the relation.
3. Embedding Swap: Compare standard multilingual embeddings vs. a Hinglish-specific fine-tuned model (if available) specifically on the "Acknowledgment" vs "Question-Answer" confusion highlighted in Section 4.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How can audio modality features (acoustic and prosodic) be effectively integrated into neural discourse parsing architectures to improve performance on code-mixed conversations?
- Basis in paper: [explicit] The paper states in the conclusion: "In the future, we plan to develop more advanced models incorporating audio modality information."
- Why unresolved: The baseline experiments reported in the paper utilized only the transcribed text (unimodal), despite the corpus being multi-modal. The specific contribution of audio signals to resolving discourse ambiguity in code-mixed settings remains unquantified.
- What evidence would resolve it: A multi-modal model evaluated on CoMuMDR that demonstrates a statistically significant improvement in F1-scores for link and relation prediction over text-only baselines.

**Open Question 2**
- Question: How can structured decoding algorithms, such as SDDP, be adapted to handle discourse structures that form Directed Acyclic Graphs (DAGs) rather than strictly trees?
- Basis in paper: [inferred] The error analysis notes that SDDP performs poorly on CoMuMDR because it "assumes the discourse relations to form a tree... while most of the discourse relations in CoMuMDR cannot adhere to tree structures."
- Why unresolved: Current structured decoders often rely on tree constraints (e.g., Maximum Spanning Tree) for efficiency. Adapting these theoretical frameworks to handle the re-entrant nodes common in realistic conversations without a massive increase in computational complexity is an open modeling challenge.
- What evidence would resolve it: A modified parsing algorithm that enforces DAG constraints during decoding and achieves higher accuracy than current tree-based or local classification models on the CoMuMDR test set.

**Open Question 3**
- Question: To what extent do automatic speech recognition (ASR) and speaker diarization errors degrade discourse parsing performance, and can models be made robust to these input uncertainties?
- Basis in paper: [inferred] The authors note the "motivation... is to create a practical... system that handles audio conversations and is robust to transcription and diarization errors" and highlight "imperfect diarization" as a key limitation.
- Why unresolved: The paper manually corrected diarization errors during annotation (using a "Diarization Continuation" label). Therefore, the baseline results reflect performance on corrected text rather than the raw, noisy output of a real-world ASR pipeline.
- What evidence would resolve it: An evaluation of baseline models using raw, uncorrected ASR and diarization transcripts, followed by the demonstration of a noise-robust model that mitigates the performance drop.

## Limitations
- The corpus relies on ASR and diarization models that introduce transcription and segmentation errors, but quantitative error rates for these components are not provided.
- Only text and embeddings are publicly released, limiting reproducibility of the multi-modal aspects.
- The claim about DAG structures being fundamentally different from STAC/Molweni tree structures is asserted but not empirically validated with graph analysis metrics.

## Confidence
- Code-mixing semantic representation mismatch: Medium confidence - Supported by embedding comparisons showing RoBERTa fails on Hinglish, but no specialized Hinglish model was actually tested for comparison.
- Structural constraint violation: Low confidence - The paper claims DAG vs tree mismatch but provides no quantitative analysis of graph density or comparison of structural metrics between datasets.
- Real-world noise propagation: Medium confidence - ASR/diarization errors are documented, but the magnitude of their impact versus other factors (e.g., linguistic complexity) is not isolated through controlled experiments.

## Next Checks
1. Noise ablation study: Manually correct 50 randomly selected "Diarization Continuation" instances and re-evaluate model performance to quantify the impact of transcription noise on discourse parsing accuracy.
2. Graph structure analysis: Compute and compare graph density, average node degree, and DAG vs tree metrics between CoMuMDR and STAC/Molweni to empirically validate the structural constraint violation hypothesis.
3. Specialized embedding evaluation: Fine-tune a multilingual model on a small Hindi-English code-mixed corpus and compare its performance on CoMuMDR against the baseline `paraphrase-xlm-r-multilingual-v1` embeddings to isolate the semantic representation effect.