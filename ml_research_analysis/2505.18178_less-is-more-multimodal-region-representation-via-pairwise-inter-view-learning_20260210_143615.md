---
ver: rpa2
title: 'Less is More: Multimodal Region Representation via Pairwise Inter-view Learning'
arxiv_id: '2505.18178'
source_url: https://arxiv.org/abs/2505.18178
tags:
- information
- cookie
- learning
- modalities
- shared
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning comprehensive region
  representations by extending multimodal region representation learning (RRL) to
  more than two modalities. The core method, CooKIE, introduces a pairwise inter-view
  learning approach that captures both shared and unique information across multiple
  geospatial data sources without explicitly modeling high-order dependencies, thereby
  reducing computational complexity.
---

# Less is More: Multimodal Region Representation via Pairwise Inter-view Learning

## Quick Facts
- arXiv ID: 2505.18178
- Source URL: https://arxiv.org/abs/2505.18178
- Authors: Min Namgung; Yijun Lin; JangHyeon Lee; Yao-Yi Chiang
- Reference count: 40
- One-line primary result: CooKIE achieves up to 8.5% improvement in R2 for population density prediction while reducing parameters by 56.7% and FLOPs by 63% compared to state-of-the-art RRL methods.

## Executive Summary
This paper addresses the challenge of learning comprehensive region representations by extending multimodal region representation learning (RRL) to more than two modalities. The core method, CooKIE, introduces a pairwise inter-view learning approach that captures both shared and unique information across multiple geospatial data sources without explicitly modeling high-order dependencies, thereby reducing computational complexity. Evaluated on three regression tasks and a land use classification task in New York City and Delhi, CooKIE outperforms state-of-the-art RRL methods, achieving up to 8.5% improvement in R2 for population density prediction and reducing parameter counts by up to 56.7% and FLOPs by up to 63%.

## Method Summary
CooKIE learns multimodal region representations through a two-stage process. First, it pretrains modality-specific encoders using intra-view contrastive learning to ensure distinctiveness across regions. Then, it jointly trains using pairwise inter-view learning that captures both shared and unique information across modalities. The method estimates shared information using InfoNCE (lower bound) and unique information using CLUB (upper bound) for MI estimation. The final representation is a concatenation of all learned embeddings, which are then used for downstream tasks like regression and classification.

## Key Results
- Achieved up to 8.5% improvement in R2 for population density prediction compared to state-of-the-art methods
- Reduced parameter counts by up to 56.7% and FLOPs by up to 63% compared to GFactorCL
- Demonstrated effectiveness across three regression tasks (population density, crime rate, greenness) and one land use classification task in NYC and Delhi

## Why This Works (Mechanism)

### Mechanism 1
Pairwise inter-view learning captures high-order dependencies without explicitly modeling conditional mutual information terms. Instead of computing CMI for every conditional shared term, CooKIE computes only unconditional pairwise shared information. Based on interaction information theory, each pairwise term inherently contains both high-order information and conditional pairwise information—high-order dependencies appear redundantly across pairs but without added computational cost. The downstream ML predictor can filter redundant high-order information that appears multiple times across pairwise embeddings.

### Mechanism 2
Factorizing multimodal data into shared and unique components preserves task-relevant modality-specific information that contrastive learning alone discards. Unique information is estimated as I(z_i; y | z_{M\{z_i}}) using InfoNCE for task-relevant MI and CLUB to minimize task-irrelevant dependencies. Shared information captures common semantics while removing task-irrelevant correlation via conditional CLUB. This factorization is crucial because modality-specific details can explain region characteristics that shared information alone cannot capture.

### Mechanism 3
Two-stage training (intra-view pretraining → pairwise inter-view joint training) produces distinctive yet aligned region embeddings. Intra-view contrastive learning first makes embeddings distinguishable across regions using NCE loss with augmented positives and cross-region negatives. Inter-view learning then aligns modalities while preserving unique information. The combined objective balances both distinctiveness within modalities and alignment across modalities.

## Foundational Learning

- **Mutual Information (MI) Estimation via Bounds**
  - Why needed here: CooKIE's entire formulation relies on tractable MI estimation since exact computation is infeasible for high-dimensional embeddings
  - Quick check question: Can you explain why InfoNCE provides a lower bound and CLUB an upper bound on MI, and when each is appropriate?

- **Contrastive Learning (InfoNCE/NCE Loss)**
  - Why needed here: Core building block for both intra-view (region distinctiveness) and inter-view (modality alignment) learning
  - Quick check question: Given a batch of N embeddings, how would you construct positive and negative pairs for intra-view vs. inter-view contrastive learning?

- **Interaction Information (Multivariate MI Extension)**
  - Why needed here: Theoretical justification that pairwise terms capture high-order dependencies without explicit conditioning
  - Quick check question: For three random variables X, Y, Z, how does I(X;Y;Z) relate to conditional mutual information I(X;Y|Z)?

## Architecture Onboarding

- **Component map**: Encoders -> Intra-view module -> Inter-view module -> Fusion -> Predictor
- **Critical path**: 1. Pretrain encoders with intra-view NCE only 2. Joint training with combined L = αL_intra + L_inter_s + L_inter_u 3. Extract embeddings, train downstream predictor
- **Design tradeoffs**: Pairwise vs. GFactorCL scales O(m²) in objectives vs O(2^m). Trade-off is redundancy vs. explicit factorization. Embedding size 512; larger may capture more but increases FLOPs. Augmentation strategy must preserve task-relevant semantics.
- **Failure signatures**: High variance across folds suggests insufficient intra-view pretraining or poor augmentation. GFactorCL outperforming CooKIE may indicate redundancy is harmful for this specific task/modality combination. Close-to-zero unique information suggests augmentations may be too aggressive.
- **First 3 experiments**:
  1. Ablation on α: Vary α ∈ {0.1, 0.5, 1.0, 2.0} to find optimal intra/inter-view balance on held-out validation set
  2. Modality dropout: Train with all 4 modalities, test with 3 (remove each one) to verify robustness and identify critical modalities per task
  3. Augmentation sensitivity: Test each augmentation type independently to validate optimal unimodal augmentation assumption

## Open Questions the Paper Calls Out

- **Question**: How does CooKIE's performance scale when applied to diverse geospatial data types beyond POI, AOI, satellite imagery, and building footprints?
- **Question**: Does the redundant capture of high-order information in pairwise embeddings hinder optimization or embedding quality as the number of modalities increases significantly?
- **Question**: Can the static CooKIE framework be effectively adapted to handle spatiotemporal dynamics for time-sensitive urban prediction tasks?

## Limitations

- Mathematical proof gap exists for the claim that pairwise terms inherently capture high-order dependencies without explicit modeling
- Heavy reliance on optimal unimodal augmentation assumption, which may not hold in practice
- Limited evaluation to only four specific modalities, leaving generalization to other data types uncertain
- Key hyperparameters (α, τ, augmentation probabilities) not specified, making reproducibility challenging

## Confidence

- **High Confidence**: The core architectural design and empirical improvements are well-supported by the experiments
- **Medium Confidence**: The theoretical justification for pairwise learning is plausible but not rigorously proven
- **Low Confidence**: The generalizability to other geographic regions, modalities, or task types is not established

## Next Checks

1. **Mathematical Rigor Validation**: Formally prove or disprove the claim that pairwise inter-view terms {S_{12}, S_{13}, S_{23}} inherently capture high-order dependencies {S_{12|3}, S_{13|2}, S_{23|1}, S_{123}} using interaction information theory

2. **Augmentation Sensitivity Analysis**: Systematically vary augmentation parameters and measure their impact on CooKIE's performance to quantify robustness to suboptimal augmentations

3. **Cross-Dataset Generalization Test**: Apply CooKIE to a third, geographically and socioeconomically distinct dataset to assess whether improvements are consistent across different contexts