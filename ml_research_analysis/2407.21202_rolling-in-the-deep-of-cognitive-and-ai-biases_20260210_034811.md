---
ver: rpa2
title: Rolling in the deep of cognitive and AI biases
arxiv_id: '2407.21202'
source_url: https://arxiv.org/abs/2407.21202
tags:
- biases
- human
- bias
- cognitive
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HAI-ROLL, a framework mapping human cognitive
  heuristics to AI biases across the AI lifecycle. By identifying how human decision-making
  shortcuts (e.g., representativeness, availability, anchoring) influence AI design
  and deployment, it reveals hidden pathways through which cognitive biases manifest
  as computational biases (e.g., representation, measurement, algorithmic, deployment).
---

# Rolling in the deep of cognitive and AI biases

## Quick Facts
- arXiv ID: 2407.21202
- Source URL: https://arxiv.org/abs/2407.21202
- Reference count: 40
- One-line primary result: HAI-ROLL framework maps human cognitive heuristics to AI biases across the AI lifecycle to support sociotechnical fairness.

## Executive Summary
This paper proposes HAI-ROLL, a framework mapping human cognitive heuristics to AI biases across the AI lifecycle. By identifying how human decision-making shortcuts influence AI design and deployment, it reveals hidden pathways through which cognitive biases manifest as computational biases. The framework aims to foster a sociotechnical view of AI fairness, supporting bias-aware decision-making and offering practical guidance for developers and policymakers.

## Method Summary
The paper develops a conceptual framework through interdisciplinary literature synthesis, combining cognitive science (Kahneman, Tversky) with AI fairness literature. It identifies harmful actions at different AI lifecycle phases and traces how human cognitive heuristics (representativeness, availability, anchoring, affect) lead to computational biases (representation, measurement, algorithmic, evaluation, deployment). No datasets, code, or empirical validation are provided.

## Key Results
- Proposes HAI-ROLL framework mapping four cognitive heuristics to six AI computational biases across three lifecycle phases
- Identifies specific harmful actions (e.g., HA.P1 for non-representative sampling) that bridge human heuristics to technical biases
- Suggests framework can inform regulatory approaches like the EU AI Act's risk management process

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cognitive heuristics migrate into computational biases through specific "Harmful Actions" at different AI lifecycle stages
- **Mechanism:** Transitive mapping where human mental shortcuts trigger technical errors resulting in computational biases
- **Core assumption:** AI biases are sociotechnical symptoms of human decision-making constraints
- **Evidence anchors:** [abstract] identifies harmful human actions influencing AI lifecycle; [section 4] details mappings like [HA.P1] linking Availability to Representation bias
- **Break condition:** If AI system is fully autonomous in data selection and architecture design

### Mechanism 2
- **Claim:** Human cognitive biases are amplified in LLMs via RLHF
- **Mechanism:** Human annotators rate outputs based on subjective heuristics, training reward models to prefer agreeable over factual responses
- **Core assumption:** Human feedback reflects subjective values rather than objective truth
- **Evidence anchors:** [section 4.2] notes RLHF may reinforce emotionally appealing responses; [section 1] mentions alignment processes can reinforce cultural biases
- **Break condition:** If RLHF rating rubrics are strictly objective and audited

### Mechanism 3
- **Claim:** Deployment Bias arises from users applying Automation Bias (Anchoring)
- **Mechanism:** Users anchor high trust to AI outputs, leading to rubber-stamping decisions without critical evaluation
- **Core assumption:** Users perceive AI as objective authorities, overlooking sociotechnical limitations
- **Evidence anchors:** [section 4.3] identifies [HA.D1] where decision makers rely on AI due to automation bias; [section 3] defines Anchoring as relying on initial information
- **Break condition:** If AI outputs include mandatory justification prompts or confidence warnings

## Foundational Learning

- **Concept:** System 1 vs. System 2 Thinking (Dual Process Theory)
  - **Why needed here:** Categorizes human heuristics as "System 1" thinking essential for understanding Harmful Actions
  - **Quick check question:** Can you distinguish between pattern matching (System 1) versus logical deliberation (System 2) in data sampling?

- **Concept:** Sociotechnical Systems
  - **Why needed here:** Framework argues against viewing AI as purely technical
  - **Quick check question:** If an algorithm is mathematically fair but deployed in workflow encouraging user laziness, is the system fair?

- **Concept:** The AI Lifecycle Phases (Pre-, In-, Post-processing)
  - **Why needed here:** HAI-ROLL maps biases to specific phases
  - **Quick check question:** During which phase does Evaluation Bias typically originate, and is it strictly confined to that phase?

## Architecture Onboarding

- **Component map:** Cognitive Layer (4 heuristics) → Operational Layer (3 lifecycle phases with Harmful Actions) → Computational Layer (6 AI Biases)

- **Critical path:**
  1. Audit: Select a lifecycle phase
  2. Trace: Identify human action
  3. Map: Connect to heuristic
  4. Predict: Identify resulting computational bias

- **Design tradeoffs:** Tradeoff between computational efficiency and cognitive debiasing; simpler models preferred due to Availability heuristic but may increase Algorithmic Bias

- **Failure signatures:**
  - Rubber-stamping: High "Blind-Accept Rate" in deployment
  - Sycophancy: RLHF models prioritizing agreeableness over truth
  - Proxy error: Using zip codes to infer ethnicity, resulting in skewed outcomes

- **First 3 experiments:**
  1. Cognitive Bias Impact Assessment (CBIA): Document which heuristics impact specific AI pipeline stages before coding
  2. Controlled Design Vignettes: Compare developer decisions with and without cognitive priming about Availability bias
  3. Reward Model Auditing (RLHF): Analyze preference for verbosity/agreeableness vs. factual accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Which specific human heuristics have the strongest causal impact on computational biases at different AI lifecycle stages?
- **Basis in paper:** [explicit] Page 3 poses Research Question 2 about which heuristics affect critical AI biases and when
- **Why unresolved:** Framework lacks empirical data to quantify "fairness intensities" or rank influence of specific heuristics
- **What evidence:** Quantitative results measuring effect size of specific cognitive biases on model unfairness

### Open Question 2
- **Question:** Can the proposed "Heuristic → Harmful Action → AI Bias" chains be empirically validated?
- **Basis in paper:** [explicit] Page 11 states future steps should include empirical validation through controlled experiments
- **Why unresolved:** Framework relies on literature synthesis rather than experimental proof of causality
- **What evidence:** Experimental studies comparing decision-making with and without biasing influences

### Open Question 3
- **Question:** How can the framework be operationalized into concrete regulatory tools like Cognitive Bias Impact Assessment?
- **Basis in paper:** [explicit] Page 10 suggests CBIA could be appended to EU AI Act's risk management
- **Why unresolved:** Translating theory into auditable legal standards requires domain-specific case studies and consensus on thresholds
- **What evidence:** Validated pilot protocol demonstrating successful CBIA implementation

## Limitations
- Framework is primarily theoretical without empirical validation through controlled experiments or real-world case studies
- Specific mapping between cognitive heuristics and AI biases relies on qualitative reasoning rather than quantitative evidence
- Applicability across different AI domains and cultural contexts remains untested

## Confidence
- **High Confidence**: Conceptual mapping between human cognitive biases and AI lifecycle phases is well-founded and aligns with established dual-process theory
- **Medium Confidence**: Specific identification of harmful actions and their connection to resulting computational biases is logically coherent but requires empirical validation
- **Low Confidence**: Framework's predictive power and practical utility in real-world AI development contexts has not been demonstrated

## Next Checks
1. Conduct systematic literature review to identify documented cases where specific cognitive heuristics have demonstrably led to measurable AI biases
2. Design and implement controlled experiments where AI practitioners are exposed to cognitive bias awareness training versus control groups
3. Apply HAI-ROLL framework to a real-world AI system case study, documenting all identified harmful actions, heuristics, and resulting biases, then compare predictions against actual outcomes