---
ver: rpa2
title: Generating Diverse Q&A Benchmarks for RAG Evaluation with DataMorgana
arxiv_id: '2501.12789'
source_url: https://arxiv.org/abs/2501.12789
tags:
- questions
- question
- diversity
- datamorgana
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DataMorgana addresses the challenge of generating diverse synthetic
  Q&A benchmarks for RAG evaluation by introducing a configurable, two-stage approach.
  It allows users to define detailed categorizations of questions and end-users, specifying
  their characteristics and distribution probabilities within the benchmark.
---

# Generating Diverse Q&A Benchmarks for RAG Evaluation with DataMorgana

## Quick Facts
- **arXiv ID**: 2501.12789
- **Source URL**: https://arxiv.org/abs/2501.12789
- **Reference count**: 38
- **Primary result**: DataMorgana achieves higher diversity in synthetic Q&A benchmarks (NDG 2.536 on COVID-QA) compared to baselines while maintaining quality.

## Executive Summary
DataMorgana addresses the challenge of generating diverse synthetic Q&A benchmarks for RAG evaluation by introducing a configurable, two-stage approach. It allows users to define detailed categorizations of questions and end-users, specifying their characteristics and distribution probabilities within the benchmark. This enables the generation of highly diverse Q&A pairs through combinatorial combinations of user and question categories. Experimental results demonstrate DataMorgana's superiority in achieving higher diversity compared to existing methods while maintaining question quality through a filtering stage.

## Method Summary
DataMorgana uses a two-stage approach: first, users define question and user categorizations in a JSON configuration file, specifying category names, probabilities, and natural-language descriptions. Second, the system generates Q&A pairs by sampling categories, selecting documents, instantiating prompts with category descriptions, and invoking an LLM to generate candidates. A filtering stage verifies constraint satisfaction, context-freeness, and document faithfulness before selecting final outputs. The method leverages Claude-3.5 Sonnet v2 with k=3 candidates per document.

## Key Results
- On COVID-QA corpus: NDG 2.536 vs Vanilla 1.517, Know Your RAG 2.358, DeepEval 2.415
- Lower SRS scores: 0.372 vs Vanilla 0.920, Know Your RAG 0.613, DeepEval 0.644
- Similar improvements observed on Wikipedia corpus
- Generates questions with greater lexical, syntactic, and semantic diversity across various user personas

## Why This Works (Mechanism)

### Mechanism 1
Joint combinations of user and question categorizations produce higher lexical, syntactic, and semantic diversity than single-taxonomy or unconstrained approaches. Multiple categorizations (factuality, premise, phrasing, linguistic variation; user expertise) are sampled per generation, instantiating different prompts that steer the LLM away from its default output distribution.

### Mechanism 2
Natural-language category descriptions enable precise control over question style without code changes, allowing non-technical users to specify distributions. A JSON configuration maps category names, probabilities, and free-text descriptions into prompt templates that the LLM interprets at inference time.

### Mechanism 3
A lightweight generate-then-filter stage maintains individual question quality while preserving aggregate diversity. Generate k candidates per document, filter for constraint satisfaction, faithfulness, and category adherence, then sample from valid outputs to prevent quality collapse while retaining diversity.

## Foundational Learning

- **Concept**: RAG evaluation benchmarks
  - Why needed: DataMorgana generates synthetic Q&A to evaluate retrieval and generation components; understanding what makes a benchmark valid grounds why diversity matters
  - Quick check: Can you explain why a benchmark with high per-question quality but low diversity may mislead RAG system comparisons?

- **Concept**: LLM generation bias and prompt steering
  - Why needed: The core mechanism relies on countering LLM tendencies toward "obvious responses" via explicit constraints in prompts
  - Quick check: What happens to output diversity if you reuse the exact same prompt across many documents without any category variation?

- **Concept**: Diversity metrics (N-gram diversity, compression ratio, homogenization score)
  - Why needed: The paper quantifies improvements using lexical, syntactic, and semantic measures; interpreting these metrics is necessary to evaluate whether changes to configuration actually improve diversity
  - Quick check: Why might a benchmark with simpler vocabulary score lower on lexical diversity metrics even if it is more varied in user intent?

## Architecture Onboarding

- **Component map**: Configuration JSON -> Category sampling -> Prompt instantiation -> LLM generation -> Filtering -> Output
- **Critical path**: Configuration definition → category sampling → prompt instantiation → LLM generation → filtering → output. Errors in category definitions or overly restrictive filters directly degrade diversity or yield.
- **Design tradeoffs**: More categorizations increase combinatorial diversity but raise configuration complexity and risk of conflicting constraints. Higher k improves filtering options but increases LLM cost. Lightweight pipeline prioritizes speed over potentially richer control.
- **Failure signatures**: Low syntactic diversity (high PoS-CR) indicates categories may be too similar; high self-repetition (SRS) suggests probability distribution too concentrated; high filter rejection rate indicates descriptions conflict with document content.
- **First 3 experiments**: 1) Run DataMorgana with default categorizations on small corpus (20 documents); measure NDG, PoS-CR, embeddings-HS against Vanilla baseline. 2) Ablate user categorizations only; compare diversity delta. 3) Vary k (1, 3, 5) and measure trade-off between filtering yield and diversity.

## Open Questions the Paper Calls Out

- **New diversity metrics**: How should new diversity metrics be designed to properly value simpler terminology alongside sophisticated vocabulary? Authors note current metrics penalize questions with simpler terminology and leave this for future work.

- **Category consistency enforcement**: Does enforcing consistency among user and question categories improve generation quality, or can LLMs handle inconsistencies autonomously? Authors are not enforcing consistency in current release and will wait for early beta testers to experiment.

- **Correlation with RAG evaluation outcomes**: How well does higher benchmark diversity correlate with improved RAG system evaluation outcomes? Authors demonstrate diversity improvements but do not validate whether these benchmarks lead to better RAG evaluation compared to less diverse alternatives.

## Limitations

- Exact filtering criteria and implementation details are not disclosed, limiting reproducibility
- Diversity improvements demonstrated only on two corpora (COVID-QA and Wikipedia), limiting generalizability
- Reliance on single LLM (Claude-3.5 Sonnet v2) means results may not transfer to other models or domains
- No systematic validation of how category consistency affects generation quality

## Confidence

- **High Confidence**: The mechanism of combining user and question categorizations to improve diversity is clearly described and experimentally validated
- **Medium Confidence**: The effectiveness of natural-language category descriptions in steering LLM output is plausible but lacks external validation
- **Low Confidence**: The robustness of the filtering stage and its impact on diversity are not independently verified

## Next Checks

1. **Ablation of user categorizations**: Run DataMorgana with and without user categorizations on the same corpus; quantify the marginal contribution of user diversity to overall metric improvements.

2. **Filter strictness sweep**: Vary the filtering acceptance threshold and measure the trade-off between diversity gains and filtering yield; identify the point where diversity plateaus or declines.

3. **Cross-LLM replication**: Repeat the generation pipeline using a different LLM (e.g., GPT-4) on the same corpus and configuration; compare diversity metrics to assess model dependence.