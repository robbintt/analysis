---
ver: rpa2
title: Structured and Abstractive Reasoning on Multi-modal Relational Knowledge Images
arxiv_id: '2510.21828'
source_url: https://arxiv.org/abs/2510.21828
tags:
- task
- data
- star
- answer
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of structured and abstractive
  reasoning on multi-modal relational knowledge (MMRK) images, which contain complex
  semantic relations between entities. The authors introduce an automatic STAR data
  engine that synthesizes images with MMRK and generates instruction data with reliable
  chain-of-thought reasoning.
---

# Structured and Abstractive Reasoning on Multi-modal Relational Knowledge Images

## Quick Facts
- arXiv ID: 2510.21828
- Source URL: https://arxiv.org/abs/2510.21828
- Reference count: 26
- Key outcome: Smaller MLLMs (3B-34B) significantly outperform GPT-4o on STAR tasks after two-stage training on synthetic MMRK images.

## Executive Summary
This paper tackles the challenge of structured and abstractive reasoning on multi-modal relational knowledge images (MMRK), where entities are linked via visual and textual relations. The authors introduce an automatic STAR data engine that synthesizes MMRK images from knowledge graphs and generates instruction data with reliable chain-of-thought reasoning. They propose a two-stage training framework (supervised fine-tuning + preference alignment) to enhance MLLM STAR capabilities. Experiments on 5 open-source MLLMs show that smaller models significantly outperform GPT-4o on STAR tasks after training, demonstrating effective transfer learning and scalability. Ablation studies confirm the importance of chain-of-thought prompts and multi-modal entity information for performance.

## Method Summary
The authors introduce a two-stage training framework for enhancing MLLM STAR capabilities. Stage 1 uses supervised fine-tuning (LoRA) on synthetic MMRK images and CoT instruction data. Stage 2 applies preference alignment (DPO/ORPO) using failure cases from Stage 1 to correct hallucinations. The STAR data engine samples subgraphs (max 9 entities) from MMKGs, visualizes them via GraphViz, and generates instruction data with CoT reasoning. The approach is evaluated on 8 reasoning tasks across 5 open-source MLLMs (3B to 34B parameters).

## Key Results
- Smaller MLLMs (3B-34B) significantly outperform GPT-4o on STAR tasks after two-stage training
- Two-stage training (SFT + PA) improves reasoning accuracy and reduces hallucinations
- CoT prompts and multi-modal entity information are critical for performance
- Models demonstrate improved commonsense knowledge retention across domains (except science)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training on GraphViz-visualized MMRK images develops "visual grounding" for abstract relational logic absent in natural image training.
- **Mechanism:** The data engine converts symbolic triples (Entity A → Relation → Entity B) into visual graphs using GraphViz. By training on this mapping, the MLLM learns to associate visual topologies (arrows, nodes) with semantic logic.
- **Core assumption:** GraphViz produces standard, unambiguous visual representations of relations.
- **Evidence anchors:** Section 3.2 Step 4: "We visualize each processed subgraph... V ← GraphViz(KG′′)." The abstract mentions the automatic STAR data synthesis engine.
- **Break condition:** If visual layout varies wildly (e.g., curved vs. straight edges), the model may fail to generalize the visual syntax of relations.

### Mechanism 2
- **Claim:** Two-stage training works because SFT establishes surface-level competency while PA targets specific reasoning hallucinations.
- **Mechanism:** Stage 1 (SFT) floods the model with valid reasoning paths (CoT), but primarily optimizes for likelihood, potentially reinforcing plausible but incorrect reasoning. Stage 2 (PA/DPO) explicitly contrasts correct "gold" answers against the model's own Stage 1 failure modes, sharpening the decision boundary for complex logic.
- **Core assumption:** Stage 1 failure cases contain systematic biases that can be modeled as "unpreferred" samples.
- **Evidence anchors:** Section 5.6: "PA training... corrects a substantial proportion of erroneous outputs... significantly reduces hallucinations." The abstract mentions the two-stage framework.
- **Break condition:** If Stage 2 "unpreferred" data is too noisy, preference optimization may destabilize general capabilities.

### Mechanism 3
- **Claim:** Textual modality serves as the primary anchor for reasoning, while visual modality provides entity disambiguation.
- **Mechanism:** In MMRK images, entities contain both image and text labels. The model relies on text to parse semantic relations while using images to ground specific entity instances.
- **Core assumption:** The MLLM's OCR capability is robust enough to read entity labels within synthetic graph layouts.
- **Evidence anchors:** Section 5.5 Table 2: "Omitting entity texts leads to a greater decline, suggesting that textual information is particularly critical." Figure 6(1) shows accuracy drops in "w/o ent. texts" vs "full dataset."
- **Break condition:** If synthetic images have low resolution or fonts that induce OCR errors, reasoning chains will break due to missing text tokens.

## Foundational Learning

- **Concept: Multi-Modal Knowledge Graphs (MMKG)**
  - **Why needed here:** This is the raw fuel for the system. Unlike standard KGs, MMKGs link entities to images. You must understand that the system samples subgraphs from this structure to create visual input.
  - **Quick check question:** Can you distinguish between a standard triple (Subject, Predicate, Object) and the visual representation of that triple as a node-edge-node image?

- **Concept: Preference Alignment (DPO/ORPO)**
  - **Why needed here:** The paper claims SFT is insufficient for "hard cases." Understanding how DPO contrasts a "chosen" answer against a "rejected" answer is vital to grasping the Stage 2 performance boost.
  - **Quick check question:** How does the loss function in Stage 2 differ from the next-token prediction loss of Stage 1?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - **Why needed here:** The paper evaluates not just the final answer but the *quality* of the reasoning path. The training data synthesizes CoT to guide the model.
  - **Quick check question:** Why would removing CoT from the training data (as done in the ablation study) degrade final answer accuracy?

## Architecture Onboarding

- **Component map:**
  1. Source: MMKGs (VisualSem, FB15K-237)
  2. Data Engine: Sampler → GraphViz (Visualizer) → Template Filler (Instruction Synthesis)
  3. Model: MLLM Backbone (e.g., Qwen2.5-VL, LLaVA)
  4. Trainer: Stage 1 (LoRA SFT) → Stage 2 (DPO/ORPO)

- **Critical path:**
  The **Instruction Synthesis** step. Because current MLLMs struggle to generate reliable CoT for these abstract images, the system must generate the CoT text *before* visualization, using the raw graph data. If this "gold" CoT is low quality, the Preference Alignment in Stage 2 fails.

- **Design tradeoffs:**
  - Single-task vs. Multi-task: Training on the "Full" mix of 8 tasks yields better generalization than single-task training due to positive transfer (Section 5.3)
  - Modality: The system trades off computational simplicity (removing images) for accuracy. The ablation study proves keeping both text and images in nodes is necessary for peak performance

- **Failure signatures:**
  - Hallucination: The model invents relations or entities not present in the visual graph (common in Zero-shot and Stage 1)
  - Counting Errors: Fluctuating accuracy in Entity/Relation Counting tasks (Task 1 & 2) suggests the model struggles with exact enumeration on the visual structure

- **First 3 experiments:**
  1. Verify the Data Engine: Run the pipeline on a small subset of VisualSem. Visually inspect 5-10 generated MMRK images to ensure GraphViz renders text and images clearly
  2. Baseline vs. SFT: Compare Zero-shot accuracy vs. Stage 1 (SFT) on the validation set for a specific task (e.g., Entity Counting) to confirm the "capability unlocking" effect
  3. Ablation Check: Re-run Stage 1 training with CoT prompts stripped. Compare the "CoT Score" to verify the claim that CoT guidance improves final answer accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What methodologies can overcome the diminishing returns of data scaling for simple structured tasks (e.g., counting) without relying on increasing backbone model size?
- **Basis in paper:** Section 5.4: "To surpass current limitations, it is necessary to utilize more powerful backbones, as further scaling of training data alone yields diminishing returns for certain task types."
- **Why unresolved:** While the paper demonstrates that increasing data improves CoT quality, it shows that answer accuracy for counting tasks fluctuates and is bottlenecked by model architecture rather than data volume.
- **What evidence would resolve it:** A study showing architectural modifications (e.g., specific attention mechanisms) or curriculum learning strategies that allow existing backbones to achieve linear performance gains with increased data on counting tasks.

### Open Question 2
- **Question:** Why does the two-stage training framework result in a performance drop in the "Science" domain while improving commonsense knowledge in other domains?
- **Basis in paper:** Section 5.6 and Figure 6(3): "models... demonstrate improved performance across all domains except science."
- **Why unresolved:** The authors observe this negative transfer but do not provide a theoretical explanation for why reasoning capabilities conflict specifically with scientific knowledge retention.
- **What evidence would resolve it:** An analysis of the embedding space or attention patterns during Science tasks to identify interference between the learned STAR reasoning mechanisms and pre-existing scientific knowledge representations.

### Open Question 3
- **Question:** To what extent is the model's reasoning capability dependent on visual structure processing versus OCR of node labels?
- **Basis in paper:** Section 5.5 ablation studies show that removing entity text causes a greater performance drop than removing entity images, leading the authors to conclude "textual information is particularly critical."
- **Why unresolved:** This suggests the model may be relying heavily on reading text rather than interpreting the relational edges and spatial layout of the MMRK images, leaving the efficacy of pure visual reasoning ambiguous.
- **What evidence would resolve it:** Experiments using abstract symbols or meaningless glyphs in place of text labels within the graph structure to isolate the model's ability to reason purely from the visual topology.

### Open Question 4
- **Question:** How does MLLM performance degrade or scale when processing MMRK images that exceed the 9-entity complexity limit?
- **Basis in paper:** Section 3.2 Step 2: "To control data complexity, we limit each subgraph to a maximum of 9 entities."
- **Why unresolved:** Real-world knowledge graphs often involve dense clusters of entities; the performance of the STAR framework on larger, more complex visual inputs remains untested.
- **What evidence would resolve it:** Experiments measuring accuracy and inference latency on synthesized graphs with increasing entity counts (e.g., 15, 25, 50 entities) to map the failure boundaries of the current approach.

## Limitations
- The focus on synthetic data limits evaluation of real-world robustness to naturally occurring MMRK structures
- The approach does not address scalability of manual labeling or data synthesis costs at larger scales
- The paper focuses on English-language MMKGs, leaving multilingual performance unclear

## Confidence
- **High Confidence:** The two-stage training framework (SFT + Preference Alignment) effectively improves STAR performance; ablation results on the importance of CoT and entity texts are reliable
- **Medium Confidence:** The scalability of the approach to real-world MMRK images and generalization to new reasoning tasks is plausible but not conclusively demonstrated
- **Low Confidence:** Claims about robustness to real-world MMRK images and the effectiveness of the preference alignment stage for unseen error types are speculative without further empirical support

## Next Checks
1. **Data Engine Validation:** Generate a small set of MMRK images using the described pipeline and visually inspect them for clarity, label readability, and layout consistency
2. **Transfer Learning Test:** After Stage 1 training, evaluate on a held-out task or domain not seen during training to test generalization beyond the 8 seed tasks
3. **Preference Alignment Robustness:** Manually review a sample of preference pairs (gold vs. model wrong outputs) to ensure failure cases are systematic and not random