---
ver: rpa2
title: 'Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen''s
  Kappa and Semantic Similarity for Qualitative Research Validation'
arxiv_id: '2512.20352'
source_url: https://arxiv.org/abs/2512.20352
tags:
- themes
- runs
- reliability
- thematic
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multi-perspective validation framework for
  LLM-based thematic analysis combining Cohen's Kappa and cosine similarity metrics.
  The framework uses ensemble validation with configurable seeds (1-6) and temperature
  (0.0-2.0) to produce multiple independent analysis runs.
---

# Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation

## Quick Facts
- arXiv ID: 2512.20352
- Source URL: https://arxiv.org/abs/2512.20352
- Authors: Nilesh Jain; Seyi Adeyinka; Leor Roseman; Aza Allsop
- Reference count: 21
- Key outcome: Framework achieves κ>0.80 agreement across three LLMs with 50-83% consensus theme consistency

## Executive Summary
This paper presents a novel framework for validating LLM-based thematic analysis by combining Cohen's Kappa for categorical agreement with cosine similarity for semantic equivalence. The framework uses ensemble validation with 6 independent runs per model using fixed seeds and configurable temperatures to produce multiple analysis perspectives. Tested on a psychedelic art therapy interview transcript across three LLMs, the framework achieved very high reliability (κ=0.907 for Gemini 2.5 Pro) and successfully extracted consensus themes with 50-83% consistency, demonstrating that AI-assisted thematic analysis can achieve reliability levels comparable to traditional multi-coder approaches while significantly reducing time and cost.

## Method Summary
The framework performs thematic analysis using ensemble validation with 6 independent runs per LLM, each configured with fixed seeds [42, 123, 456, 789, 1011, 1213] at temperature 0.7. It computes pairwise Cohen's Kappa values across all runs for inter-rater agreement and cosine similarity using all-MiniLM-L6-v2 embeddings for semantic equivalence. The consensus extraction algorithm dynamically detects theme structures across runs, clusters themes with cosine similarity >0.70 into equivalence classes, and retains themes appearing in ≥50% of runs. The approach was tested on a 28,377-character ketamine art therapy interview transcript across Gemini 2.5 Pro, GPT-4o, and Claude 3.5 Sonnet.

## Key Results
- Gemini 2.5 Pro achieved highest reliability (κ=0.907, cosine=95.3%)
- GPT-4o achieved κ=0.853, cosine=92.6%
- Claude 3.5 Sonnet achieved κ=0.842, cosine=92.1%
- All models achieved very high agreement (κ>0.80)
- Framework extracted consensus themes with 50-83% consistency across runs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple independent LLM runs with fixed seeds produce stable reliability estimates analogous to K-fold cross-validation.
- Mechanism: The framework runs 6 independent analyses per model using fixed seeds [42, 123, 456, 789, 1011, 1213] at temperature 0.7, generating 15 pairwise comparisons (n(n-1)/2). This reduces standard error by ~41% compared to 3-run configurations.
- Core assumption: Seeds introduce controlled, reproducible variation that captures meaningful interpretive possibilities without diverging into incoherence.
- Evidence anchors: [abstract]: "ensemble validation with configurable seeds (1-6) and temperature (0.0-2.0) to produce multiple independent analysis runs"; [section 3.1]: "Six runs enable 15 pairwise comparisons... representing a 41% reduction in variability"
- Break condition: If pairwise kappa values fall below 0.40 or show variance >0.40 in kappa range, the ensemble fails to validate reliability.

### Mechanism 2
- Claim: Combining Cohen's Kappa with cosine similarity captures both categorical agreement and semantic equivalence that either metric alone misses.
- Mechanism: Kappa computes theme presence/absence agreement accounting for chance. Cosine similarity uses all-MiniLM-L6-v2 embeddings (384-dimensional) to capture semantic equivalence—"perfectionist barriers" vs. "creative blocks from self-criticism" achieve high cosine despite low lexical overlap.
- Core assumption: Themes phrased differently can express identical concepts; kappa's requirement for exact matches is too restrictive for qualitative interpretation.
- Evidence anchors: [abstract]: "combining Cohen's Kappa (κ) for inter-rater agreement and cosine similarity for semantic consistency"; [section 3.2]: "Cosine Similarity. Captures semantic equivalence beyond exact matches"
- Break condition: If kappa and cosine diverge substantially (e.g., κ < 0.50 but cosine > 0.90), this signals systematic categorization disagreement requiring human review.

### Mechanism 3
- Claim: Structure-agnostic consensus extraction enables theme identification across arbitrary JSON schemas without predefined templates.
- Mechanism: Dynamic schema detection identifies common array fields across runs, computes pairwise cosine similarity between all theme descriptions, clusters themes with similarity >0.70 into equivalence classes, and retains themes appearing in ≥50% of runs.
- Core assumption: LLM outputs contain identifiable theme structures; 50% threshold balances filtering spurious themes while preserving valid interpretive variation.
- Evidence anchors: [section 3.3]: "structure-agnostic consensus extraction working with any JSON format"; [section 4.3]: "Dynamic Schema Detection. Analyzes LLM outputs to identify common array fields across runs"
- Break condition: If robust JSON parsing fails (rate <95%), or no themes achieve 50% threshold across runs, consensus extraction fails.

## Foundational Learning

- **Cohen's Kappa Interpretation (Landis-Koch criteria)**:
  - Why needed here: The paper claims "almost perfect agreement" based on κ > 0.80; understanding this scale is essential for interpreting results.
  - Quick check question: Would κ = 0.65 indicate "substantial" or "moderate" agreement?

- **Embedding-based semantic similarity**:
  - Why needed here: Cosine similarity on 384-dimensional embeddings validates themes beyond lexical matching; understanding why embeddings outperform string overlap is critical.
  - Quick check question: Why does "IFS Integration" and "Internal Family Systems work" achieve high cosine similarity despite zero lexical overlap?

- **Ensemble variance reduction**:
  - Why needed here: The paper claims 6 runs provide "substantially more stable estimates" than 2-3 runs; understanding the statistical rationale prevents over- or under-investment.
  - Quick check question: What happens to standard error when increasing from 3 to 6 independent runs?

## Architecture Onboarding

- **Component map**: Client-side preprocessing (Next.js 14, React) -> Multi-provider API layer -> Embedding engine (Transformers.js) -> Consensus extractor -> Error handler
- **Critical path**: 1. Preprocess transcript (chunk if >1M tokens with 20% overlap) 2. Execute N runs with seeds via multi-provider API 3. Strip markdown code fences, parse JSON outputs 4. Compute embeddings for each theme description (limit: 10 themes/run) 5. Calculate 15 pairwise kappa values and cosine similarities 6. Cluster themes (similarity >0.70), filter by ≥50% occurrence 7. Report consensus themes with consistency percentages
- **Design tradeoffs**: 6 runs vs. cost: 41% variance reduction justified for high-stakes research; reduce to 3-4 runs for exploratory work; 50% threshold: Lower (33%) for exploratory analysis, raise (67%) for clinical/conservative applications; Client-side embeddings: Preserves privacy but limits computation; sampling required for >10 theme pairs; Temperature 0.7: Balances determinism with interpretive variation; T < 0.5 for structured data, T > 1.0 for creative exploration
- **Failure signatures**: κ < 0.40: Poor agreement—ensemble approach invalid; Kappa range > 0.40: High variance indicates unstable runs; Cosine < 80%: Low semantic convergence warrants prompt revision; JSON parse failures > 5%: LLM output format inconsistent; No consensus themes at 50%: Threshold too high or data lacks coherent themes
- **First 3 experiments**: 1. Baseline validation: Run framework on a transcript with manually coded ground truth; verify κ > 0.80 and consensus themes match human-coded themes 2. Temperature sweep: Test T = 0.0, 0.7, 1.5 on same transcript; document impact on kappa range and consensus theme count 3. Cross-model ensemble: Run identical analysis on Gemini + GPT-4o + Claude; identify model-invariant themes (appear in all 3 models) vs. model-specific interpretations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the agreement level between AI-generated consensus themes and human-coded themes on identical qualitative datasets?
- Basis in paper: [explicit] Future Work states: "Comparison against human coders on identical datasets, measuring kappa agreement between AI consensus themes and human-coded themes. Zhang et al. [12] achieved 0.76 similarity; our inter-run consistency (0.92-0.95) suggests potential for high human-AI agreement."
- Why unresolved: The study only measured inter-LLM reliability, not human-AI agreement. Without this comparison, it remains unclear whether high LLM consistency translates to valid thematic analysis that matches human interpretation.
- What evidence would resolve it: Conduct studies using identical transcripts coded by both human experts and the LLM ensemble framework, computing Cohen's Kappa between AI consensus themes and human-derived themes.

### Open Question 2
- Question: How does the framework's reliability perform across diverse datasets, domains, and non-English languages?
- Basis in paper: [explicit] Limitations states: "generalization requires evaluation across diverse domains (clinical, educational, organizational), data types (interviews, focus groups, surveys), and languages" and "boundary conditions remain to be established."
- Why unresolved: All empirical results derive from a single ketamine art therapy interview transcript (28,377 characters). Whether κ>0.80 reliability holds for other data types, domains, or languages is unknown.
- What evidence would resolve it: Systematic evaluation across multiple datasets (clinical interviews, focus groups, surveys), domains (healthcare, education, organizational), and languages (Spanish, Chinese, etc.) with reported reliability metrics for each.

### Open Question 3
- Question: Can thematic saturation metrics dynamically determine optimal run counts instead of using a fixed N=6 runs?
- Basis in paper: [explicit] Future Work states: "Implementing thematic saturation metrics [17, 18] to determine optimal run counts dynamically. If new themes cease emerging after N runs, stop analysis rather than using fixed N=6."
- Why unresolved: The current framework uses a fixed 6-run protocol based on statistical rationale, but this may be excessive for simple datasets or insufficient for complex ones.
- What evidence would resolve it: Implement and test adaptive stopping criteria based on thematic saturation, comparing resulting reliability metrics and theme quality against fixed-run baselines.

## Limitations
- Performance on diverse transcript types remains untested beyond psychedelic therapy interviews
- Reproducibility depends heavily on the undisclosed prompt template, which could significantly affect results
- The 50% consensus threshold is empirically chosen without systematic optimization across different data types

## Confidence
- **High Confidence**: The mathematical validity of combining Cohen's Kappa with cosine similarity for dual metric validation; the statistical benefit of 6-run ensembles over 3-run configurations
- **Medium Confidence**: The claim that this framework achieves "reliability levels comparable to traditional multi-coder approaches" based on a single case study
- **Low Confidence**: The generalizability of the 0.70 cosine similarity threshold and 50% consensus requirement across different qualitative analysis contexts

## Next Checks
1. **Cross-domain validation**: Test the framework on transcripts from different qualitative research domains (medical interviews, consumer feedback, organizational studies) to assess generalizability
2. **Ground truth comparison**: Compare AI-generated consensus themes against manually coded themes from the same transcript by multiple human researchers to validate accuracy claims
3. **Threshold sensitivity analysis**: Systematically vary the cosine similarity threshold (0.50-0.90) and consensus percentage (33%-67%) to identify optimal parameter combinations for different use cases