---
ver: rpa2
title: Scaling Search-Augmented LLM Reasoning via Adaptive Information Control
arxiv_id: '2602.01672'
source_url: https://arxiv.org/abs/2602.01672
tags:
- information
- search
- control
- answer
- think
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for adaptive information control
  in search-augmented reasoning agents. The core idea is to regulate information acquisition
  using a formal measure of information utility that quantifies the marginal value
  of retrieved evidence under the current reasoning state.
---

# Scaling Search-Augmented LLM Reasoning via Adaptive Information Control

## Quick Facts
- arXiv ID: 2602.01672
- Source URL: https://arxiv.org/abs/2602.01672
- Reference count: 40
- Key outcome: Introduces adaptive information control framework for search-augmented reasoning agents using formal information utility measures, showing 9.4% (7B) and 8.6% (3B) performance gains over RL baselines across seven benchmarks

## Executive Summary
This paper presents a framework for adaptive information control in search-augmented reasoning agents. The core innovation is regulating information acquisition using a formal measure of information utility that quantifies the marginal value of retrieved evidence under the current reasoning state. This utility guides two control mechanisms: retrieval continuation control (deciding when to stop or continue retrieval) and granularity control (selectively expanding hierarchical information). The framework is integrated with reinforcement learning using an annealed control strategy that gradually removes external control, allowing the model to internalize effective behaviors. Experiments across seven benchmarks with models ranging from 3B to 7B parameters show consistent improvements, with average performance gains of 9.4% (7B) and 8.6% (3B) over strong RL baselines. The approach also demonstrates better training stability and computational efficiency compared to retrieval-free and retrieval-based methods without explicit control.

## Method Summary
The framework introduces adaptive information control for search-augmented reasoning agents, using a formal measure of information utility to quantify the marginal value of retrieved evidence. This utility guides two control mechanisms: retrieval continuation control (deciding when to stop or continue retrieval) and granularity control (selectively expanding hierarchical information). The system integrates with reinforcement learning using an annealed control strategy that gradually removes external control, allowing the model to internalize effective behaviors. The approach is evaluated across seven benchmarks with models ranging from 3B to 7B parameters, demonstrating consistent performance improvements and better training stability compared to retrieval-free and retrieval-based methods without explicit control.

## Key Results
- Average performance gains of 9.4% for 7B parameter models and 8.6% for 3B parameter models over strong RL baselines
- Consistent improvements across seven different benchmarks
- Better training stability and computational efficiency compared to retrieval-free and retrieval-based methods without explicit control

## Why This Works (Mechanism)
The framework works by introducing a formal information utility measure that quantifies the marginal value of retrieved evidence under the current reasoning state. This utility guides two control mechanisms: retrieval continuation control determines when to stop or continue retrieval based on the diminishing returns of additional information, while granularity control selectively expands hierarchical information at appropriate levels of detail. The annealed control strategy gradually transitions from external to internalized control, allowing the model to learn when and how to retrieve information autonomously. This adaptive approach prevents both under-retrieval (missing critical evidence) and over-retrieval (wasting computation on low-value information), resulting in more efficient and effective reasoning.

## Foundational Learning
- Information utility theory: Needed to quantify the marginal value of retrieved evidence in reasoning contexts; Quick check: Verify that the utility measure captures diminishing returns as more information is retrieved
- Reinforcement learning with annealed control: Needed to gradually transition from external control to model-internalized behaviors; Quick check: Ensure the annealing schedule appropriately balances exploration and exploitation
- Hierarchical information representation: Needed to support selective expansion at different levels of detail; Quick check: Validate that the granularity control mechanism correctly identifies optimal expansion points
- Search-augmented reasoning: Needed to understand how external information retrieval integrates with internal reasoning processes; Quick check: Confirm that retrieved evidence is appropriately weighted in the final reasoning output
- Marginal value estimation: Needed to determine when additional information retrieval becomes less valuable than continuing reasoning; Quick check: Test that the system correctly identifies diminishing returns in information acquisition

## Architecture Onboarding

**Component Map:**
Search Engine -> Information Utility Module -> Retrieval Continuation Control -> Granularity Control -> Reasoning Model -> RL Policy (Annealed)

**Critical Path:**
1. Initial reasoning state is established
2. Search engine retrieves candidate evidence
3. Information utility module evaluates marginal value of evidence
4. Retrieval continuation control decides whether to continue or stop
5. Granularity control determines expansion level for hierarchical information
6. Reasoning model processes information and updates state
7. RL policy (initially guided, then internalized) optimizes control decisions

**Design Tradeoffs:**
- Granularity vs. efficiency: Finer-grained control provides better optimization but increases computational overhead
- Annealing speed vs. stability: Faster annealing reduces external control sooner but may compromise learning stability
- Information utility precision vs. computational cost: More sophisticated utility measures improve decision quality but increase computation time
- Search depth vs. relevance: Deeper searches may find more relevant evidence but risk retrieving low-value information

**Failure Signatures:**
- Premature termination: Utility measure fails to recognize value in additional retrieval
- Over-retrieval: Utility measure incorrectly estimates high marginal value for low-quality evidence
- Granularity mismatch: Control mechanism expands information at wrong hierarchical levels
- Annealing failure: Premature internalization of suboptimal control policies

**First 3 Experiments:**
1. Baseline comparison: Test performance without adaptive control versus with control enabled
2. Utility ablation: Compare different formulations of information utility measure
3. Annealing schedule variation: Test different rates of transitioning from external to internalized control

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical grounding for information utility measure lacks rigorous mathematical justification
- Adaptive search module may exhibit brittleness on out-of-distribution datasets
- Annealing schedule is heuristic rather than theoretically derived
- Lacks detailed qualitative analysis of what evidence types are most valuable

## Confidence
High confidence: Experimental methodology is sound with appropriate baselines, ablation studies, and multiple benchmarks showing consistent improvements across model scales. Computational efficiency gains and training stability improvements are well-documented.

Medium confidence: Effectiveness of the adaptive information control framework itself, as theoretical foundations could benefit from more rigorous treatment. Specific architectural choices (utility formulation, annealing schedule) appear effective but may not be optimal.

Low confidence: Generalizability to much larger models (13B+), different reasoning domains, or real-world applications with varying retrieval costs and quality constraints.

## Next Checks
1. Conduct cross-domain transfer experiments using models trained on one benchmark and evaluated on out-of-distribution reasoning tasks to assess robustness and generalizability of learned control policies.

2. Perform ablation studies specifically isolating the contribution of the information utility measure versus the control mechanisms, testing alternative utility formulations to establish whether the proposed measure is critical or if simpler approaches would suffice.

3. Implement a live deployment test where the system operates with realistic retrieval latency and quality constraints, measuring both performance and computational costs compared to standard retrieval baselines under these practical conditions.