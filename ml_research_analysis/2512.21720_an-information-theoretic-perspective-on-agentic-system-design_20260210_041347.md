---
ver: rpa2
title: An Information Theoretic Perspective on Agentic System Design
arxiv_id: '2512.21720'
source_url: https://arxiv.org/abs/2512.21720
tags:
- information
- compressor
- predictor
- research
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an information-theoretic framework for designing
  compressor-predictor agentic systems, where smaller compressor LMs distill context
  into summaries consumed by larger predictor LMs. The authors develop a practical
  mutual information estimator that quantifies compression quality without requiring
  task-specific evaluation.
---

# An Information Theoretic Perspective on Agentic System Design

## Quick Facts
- arXiv ID: 2512.21720
- Source URL: https://arxiv.org/abs/2512.21720
- Reference count: 40
- Key outcome: MI-based estimator quantifies compression quality without task-specific evaluation; larger compressors achieve up to 5.5× more information per token and yield greater accuracy gains than larger predictors

## Executive Summary
This paper introduces an information-theoretic framework for designing compressor-predictor agentic systems, where smaller compressor LMs distill context into summaries consumed by larger predictor LMs. The authors develop a practical mutual information estimator that quantifies compression quality without requiring task-specific evaluation. Empirical results across five datasets demonstrate that larger compressors not only improve accuracy but also achieve greater token efficiency, conveying up to 5.5× more information per token than smaller models. Scaling compressors proves substantially more effective than scaling predictors, with compressor model family and size outweighing predictor size in importance. A Deep Research application validates these principles, achieving 99% of frontier-LM accuracy at 26% of API costs using local 3B parameter compressors.

## Method Summary
The framework treats compressor-predictor systems through an information-theoretic lens, using mutual information (MI) between original context and compression as a task-agnostic quality metric. The MI estimator uses a Monte Carlo approach with log probabilities from proxy LMs to approximate I(X;Z|Q). The authors evaluate five datasets (LongHealth, FinanceBench, QASPER, WildChat, FineWeb) using compressors ranging from 1.5B to 7B parameters and predictors from 8B to 405B parameters. They measure MI, accuracy, perplexity, bit efficiency (MI/token), and FLOPs-per-generation to establish scaling laws and rate-distortion relationships. A Deep Research application demonstrates practical deployment with parallel local compressors feeding a cloud predictor.

## Key Results
- Larger compressors achieve up to 5.5× more information per token and demonstrate sublinear FLOPs-per-generation scaling
- Scaling compressors yields larger accuracy gains than scaling predictors (Qwen-2.5 1B→7B: +60% accuracy; predictor 70B→405B: +12%)
- Deep Research application achieves 99% of frontier-LM accuracy at 26% of API costs using local 3B parameter compressors
- Four design principles emerge: front-load compute into local compressors, optimize for information density, expect model family differences in scaling, and recognize sublinear compute growth

## Why This Works (Mechanism)

### Mechanism 1: Information-Theoretic Quality Estimation
Treating the compressor as a noisy channel, MI quantifies how much task-relevant information a compression retains. The Monte Carlo estimator approximates this via log probabilities from a proxy LM (≈7-8B), providing a task-agnostic proxy for compressor efficacy that strongly predicts downstream predictor performance. This avoids full end-to-end sweeps for each compressor-predictor pairing.

### Mechanism 2: Scaling Efficiency via Sublinear Compute Growth
Larger compressors emit fewer tokens while preserving more information, yielding sublinear FLOPs-per-generation growth with model size. Increased capacity improves information density (bits per token), so fewer tokens are needed for equivalent or higher MI. The reduction in output tokens offsets increased parameter count.

### Mechanism 3: Front-Load Compute into Compressors
The compressor is an information bottleneck. Increasing bottleneck capacity preserves more relevant information, reducing predictor burden; predictor gains saturate once capacity suffices. Scaling compressor size yields larger accuracy gains than scaling predictor size beyond ~8-70B parameters.

## Foundational Learning

- **Concept**: Mutual information (MI) between two random variables.
  - Why needed here: MI quantifies how much information a compression retains; the framework relies on estimating and maximizing it.
  - Quick check question: If X and Z are independent, what is I(X; Z)? (Answer: 0)

- **Concept**: Rate-distortion theory.
  - Why needed here: Guides trade-offs between compression rate (bits/token) and distortion (accuracy loss).
  - Quick check question: As rate increases, what typically happens to distortion? (Answer: It decreases toward an irreducible floor)

- **Concept**: Monte Carlo estimation.
  - Why needed here: The MI estimator uses Monte Carlo sampling to approximate intractable expectations.
  - Quick check question: As samples increase, what happens to estimator variance? (Answer: Variance decreases)

## Architecture Onboarding

- **Component map**: Context X → Compressor LM → Summary Z → Predictor LM → Answer Y; MI Estimator measures I(X;Z|Q) using proxy model log probs

- **Critical path**: 1) Define compression prompt (query-specific/agnostic) 2) Compress X → Z 3) Estimate I(X; Z|Q) 4) Predict Z → Y 5) Evaluate accuracy/perplexity; use MI as task-agnostic proxy 6) Deep Research: predictor decomposes task → compressors process sources → predictor synthesizes

- **Design tradeoffs**: Compressor size vs memory (larger = more bit-efficient but needs more RAM); prompt conciseness (shifts absolute tokens; scaling trends persist); proxy model choice (introduces fixed MI offset; scaling trends robust)

- **Failure signatures**: High perplexity despite high MI (compressor-predictor style mismatch or irreducible error); MI saturates, accuracy stalls (hit error floor from noise or predictor limits); small negative MI (Monte Carlo variance; clip to zero); omission errors (key details excluded ~30% of errors)

- **First 3 experiments**: 1) Replicate MI estimation on small QA subset (N=20, M=20) with 3B vs 7B compressors; correlate MI with accuracy 2) Ablate compressor vs predictor scaling: fix predictor, vary compressor; then fix compressor, vary predictor 3) Measure token counts and FLOPs-per-generation across compressor sizes; verify sublinear scaling for Qwen-2.5

## Open Questions the Paper Calls Out

- **Open Question 1**: Can rate-distortion principles be operationalized into training objectives to optimize compressor-predictor communication? The authors suggest training objectives based on rate-distortion analysis represent another avenue to optimize compressor-predictor communication, but do not explore fine-tuning or training methodologies.

- **Open Question 2**: Do mixture-of-experts (MoE) models exhibit distinct scaling laws compared to dense models when acting as compressors? The paper notes MoE models may exhibit different scaling behaviors since their compute cost depends on activated experts rather than total parameter count, but scaling laws are derived only from dense architectures.

- **Open Question 3**: Can mutual information metrics effectively guide dynamic routing strategies or fallback decisions in multi-turn agentic workflows? The authors suggest information-theoretic principles could guide compressor routing strategies and fallback decisions for remote full-context processing, but focus on single-round workflows.

## Limitations

- MI estimator relies on proxy models whose calibration at 1-3B scales is uncertain, potentially yielding noisy estimates for small compressors
- Model family differences introduce variability in token efficiency trends, complicating direct comparisons across architectures
- Framework assumes predictor capacity suffices beyond ~8-70B parameters, which may not hold for tasks requiring extremely complex multi-step reasoning

## Confidence

- **High confidence**: Mutual information as task-agnostic proxy for compression quality (strong empirical correlation with accuracy across multiple datasets)
- **Medium confidence**: Sublinear FLOPs scaling with compressor size (demonstrated for Qwen-2.5 but not all model families)
- **Medium confidence**: Front-loading compute into compressors (clear scaling advantage shown, but predictor capacity assumptions unverified for all task types)
- **Low confidence**: Generalizability to domains outside healthcare, finance, research, and chat (experiments limited to 5 datasets)

## Next Checks

1. **Cross-domain robustness**: Test MI estimator and scaling principles on domains with different information densities (legal documents, scientific literature, code) to verify the 5.5× information density claim holds across varied content types.

2. **Extreme capacity scenarios**: Evaluate systems with both extremely large compressors (>20B) and extremely large predictors (>400B) to test the saturation hypothesis and determine if predictor scaling eventually overtakes compressor scaling for complex reasoning tasks.

3. **Fully local deployment**: Implement the Deep Research application using only local models (compressors and predictors) to validate cost savings when API access is unavailable and assess performance degradation from using smaller predictor LMs.