---
ver: rpa2
title: Joint-stochastic-approximation Autoencoders with Application to Semi-supervised
  Learning
arxiv_id: '2505.18558'
source_url: https://arxiv.org/abs/2505.18558
tags:
- learning
- data
- semi-supervised
- discrete
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Joint-stochastic-approximation (JSA) autoencoders,
  a new family of deep generative models for semi-supervised learning. JSA autoencoders
  directly maximize data log-likelihood while minimizing the KL divergence between
  the posterior and inference model, addressing limitations of existing methods like
  VAEs and GANs in handling discrete variables and indirect likelihood optimization.
---

# Joint-stochastic-approximation Autoencoders with Application to Semi-supervised Learning

## Quick Facts
- arXiv ID: 2505.18558
- Source URL: https://arxiv.org/abs/2505.18558
- Reference count: 19
- Discrete latent variable models achieve competitive performance on MNIST and SVHN semi-supervised classification for the first time

## Executive Summary
This paper introduces Joint-stochastic-approximation (JSA) autoencoders, a new family of deep generative models for semi-supervised learning. JSA autoencoders directly maximize data log-likelihood while minimizing the KL divergence between the posterior and inference model, addressing limitations of existing methods like VAEs and GANs in handling discrete variables and indirect likelihood optimization. The method employs stochastic approximation to jointly optimize generative and inference models. Experiments demonstrate JSA autoencoders' superiority in robustness to structure mismatch and consistent handling of discrete and continuous variables.

## Method Summary
JSA autoencoders use joint stochastic approximation to simultaneously optimize generative and inference models. The approach directly maximizes data log-likelihood and minimizes inclusive KL divergence between the posterior and inference model. A key innovation is the use of Metropolis Independence Sampling (MIS) to sample from the posterior during training, enabling exact likelihood optimization even with discrete latent variables. The method jointly updates parameters θ (generative) and φ (inference) using stochastic approximation updates based on sampled minibatches from both supervised and unsupervised data.

## Key Results
- JAE with discrete latent space achieves competitive performance on MNIST and SVHN semi-supervised classification, marking first successful application of discrete latent models in challenging semi-supervised tasks
- JAE demonstrates superior robustness to structure mismatches between encoder and decoder compared to VAEs
- JAE successfully handles both discrete and continuous latent variables through unified framework

## Why This Works (Mechanism)
JSA autoencoders work by directly optimizing the data log-likelihood through stochastic approximation, avoiding the indirect optimization path of VAEs. The key insight is that joint optimization of generative and inference models can be achieved through simultaneous parameter updates based on samples from both models. The MIS sampling procedure provides unbiased samples from the true posterior, enabling exact likelihood computation even with discrete latent variables. This direct optimization approach eliminates the need for reparameterization tricks and handles discrete latent spaces naturally.

## Foundational Learning
**Metropolis Independence Sampling**: Why needed - provides exact posterior samples for discrete latent variables where reparameterization fails. Quick check - monitor acceptance ratio during training; should stabilize above 0.1.
**Stochastic Approximation**: Why needed - enables joint optimization of incompatible objectives (likelihood maximization vs KL minimization). Quick check - verify parameter updates follow SA convergence criteria with decreasing learning rates.
**Joint parameter updates**: Why needed - allows simultaneous optimization of generative and inference models without alternating optimization. Quick check - track both θ and φ trajectories to ensure convergence.

## Architecture Onboarding

**Component map:**
- Generative model pθ(x, y, h) = p(y)p(h)pθ(x|h): Decoder network mapping latent codes to observations
- Inference model qφ(y, h|x) = qφ(y|x)qφ(h|x): Encoder network with factorized outputs
- MIS sampler: Metropolis Independence Sampler using qφ as proposal for pθ posterior
- Joint parameter vector λ = (θ, φ)T updated via SA

**Critical path:**
1. Initialize θ, φ via Xavier initialization
2. For each minibatch:
   - Draw unsupervised samples via MIS from p(x)pθ(y, h|x) using qφ proposals
   - Draw supervised samples from p(x, y)pθ(h|x, y)
   - Compute gradients of log-likelihood and inclusive KL
   - Update parameters via SA with learning rate γt
3. Monitor acceptance ratio in MIS (target >0.1); if too low, improve qφ

**Design tradeoffs:**
- Discrete vs continuous latent: Discrete (Bernoulli) removes reparameterization issues but increases mixing time; continuous enables faster convergence but requires reparameterization tricks
- MIS moves: More moves reduce variance but increase compute; paper uses 10-step warmup per datapoint
- Encoder capacity: Lower capacity speeds training but increases mismatch cost (JAE more robust than VAE here)

**Failure signatures:**
- MIS acceptance ratio → 0: qφ is poor proposal; increase encoder capacity or warmup steps
- KL divergence plateaus above zero: Structure mismatch; JAE can still converge but efficiency degrades
- Mode collapse in generation: Unlike GANs, JAE shouldn't exhibit this; check MIS implementation

**First 3 experiments:**
1. Factor analysis synthetic test (Section 4.1): Verify JAE handles encoder-decoder mismatch better than VAE; track both KL divergences
2. GMM clustering (Section 4.2): Test discrete+continuous latent mixture; confirm JAE discovers 16 discrete modes plus local variance
3. Low-label MNIST (100 labels): Replicate Table 1 result with 60d Bernoulli prior; target <2% error rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can JSA autoencoders be adapted to achieve competitive performance on high-complexity natural image datasets like CIFAR-10, given the significant performance gap observed compared to GAN-based methods?
- Basis in paper: [inferred] Table 1 reports that JAE with Bernoulli latent space yields a 44.8% error rate on CIFAR-10, whereas state-of-the-art methods like BadGAN achieve approximately 14.4%.
- Why unresolved: The paper demonstrates success on MNIST and SVHN but does not explain the failure mode on CIFAR-10 or propose architectural adjustments for more complex image distributions.
- What evidence would resolve it: A study showing JAE matching state-of-the-art error rates on CIFAR-10 or analysis explaining why the current MCMC-based sampling struggles with the multi-modality of natural images.

### Open Question 2
- Question: How does the computational overhead of the iterative Metropolis Independence Sampler (MIS) compare to single-pass variational methods, particularly as the dimensionality of the data increases?
- Basis in paper: [inferred] Page 4 details the use of MIS with "multiple moves" to reduce fluctuation, and Page 5 notes that the algorithm relies on repeatedly executing Markov transitions to approximate expectations.
- Why unresolved: While the paper claims theoretical consistency, it does not quantify the wall-clock training time or the number of burn-in steps required relative to standard backpropagation in VAEs.
- What evidence would resolve it: A comparative benchmark of training time and mixing efficiency between JAE and VAE baselines on high-dimensional datasets.

### Open Question 3
- Question: Does the JSA learning mechanism effectively resolve the "training collapse" (latent variable neglect) in text generation tasks for longer, more complex sequences?
- Basis in paper: [inferred] Page 3 identifies the collapse issue in VAEs, and Page 7 shows superior reconstruction for Context-Free Grammar (CFG) sequences, but experiments are limited to short sequences (max length 12).
- Why unresolved: The experiments are restricted to synthetic, short symbolic sequences; it remains unverified if JAE maintains latent code utilization for natural language with long-term dependencies.
- What evidence would resolve it: Experiments applying JAE to standard text benchmarks (e.g., language modeling) demonstrating that the latent code actively influences the decoder's output distribution.

## Limitations
- **Scalability concerns**: Discrete Bernoulli latent representation requires exponentially many dimensions for complex distributions, with 220 dimensions representing a practical limit for SVHN
- **Implementation complexity**: Joint stochastic approximation framework requires careful tuning of multiple hyperparameters including learning rates, MIS parameters, and warmup schedules
- **Structural mismatch vulnerability**: While JAE is more robust than VAEs, severe mismatches between generative and inference models can still degrade performance

## Confidence
**High confidence** in the fundamental algorithmic contribution: The JSA framework for joint optimization of generative and inference models is mathematically sound, with clear connections to stochastic approximation theory and evidence of effective implementation.

**Medium confidence** in empirical performance claims: While the paper demonstrates competitive results on MNIST and SVHN, the comparison set is limited. The claim of "first successful application of discrete latent variable models" in challenging semi-supervised tasks is plausible but difficult to verify comprehensively across all prior work.

**Low confidence** in scalability assertions: The paper provides limited evidence regarding performance on larger-scale datasets or more complex image distributions beyond the standard benchmarks.

## Next Checks
1. **Architecture sensitivity analysis**: Systematically vary encoder/decoder network depths and widths on MNIST to quantify performance degradation under structural mismatches, directly testing the claimed robustness advantage over VAEs.
2. **Latent space ablation study**: Compare JAE performance using different latent dimensionalities (e.g., 30, 60, 120 dimensions) on both MNIST and SVHN to establish the relationship between latent capacity and classification accuracy.
3. **Mixture latent evaluation**: Implement the discrete+continuous latent mixture model proposed in Section 3.3 and evaluate on a synthetic mixture dataset to verify the theoretical advantages of hybrid latent representations for modeling complex distributions.