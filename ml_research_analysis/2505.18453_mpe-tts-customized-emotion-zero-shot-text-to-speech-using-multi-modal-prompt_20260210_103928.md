---
ver: rpa2
title: 'MPE-TTS: Customized Emotion Zero-Shot Text-To-Speech Using Multi-Modal Prompt'
arxiv_id: '2505.18453'
source_url: https://arxiv.org/abs/2505.18453
tags:
- speech
- emotion
- prosody
- prompt
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MPE-TTS is a zero-shot text-to-speech system that enables flexible
  emotion customization using multi-modal prompts (text, image, or speech). The method
  disentangles speech into content, timbre, emotion, and prosody using a hierarchical
  approach.
---

# MPE-TTS: Customized Emotion Zero-Shot Text-To-Speech Using Multi-Modal Prompt

## Quick Facts
- arXiv ID: 2505.18453
- Source URL: https://arxiv.org/abs/2505.18453
- Authors: Zhichao Wu; Yueteng Kang; Songjun Cao; Long Ma; Qiulin Li; Qun Yang
- Reference count: 0
- One-line primary result: MPE-TTS enables flexible emotion customization using multi-modal prompts (text, image, or speech) while achieving superior emotion accuracy and speaker preservation compared to baselines.

## Executive Summary
MPE-TTS introduces a zero-shot text-to-speech system that enables flexible emotion customization through multi-modal prompts. The method disentangles speech into content, timbre, emotion, and prosody using a hierarchical approach with a multi-modal prompt emotion encoder (MPEE) based on Emotion2Vec. It employs an LLM-like prosody predictor with emotion consistency loss to preserve emotional characteristics in generated speech, using a diffusion-based acoustic model to generate mel-spectrograms. The system was evaluated on the MEAD-TTS dataset and demonstrated superior performance compared to baselines like Meta-StyleSpeech, GenerSpeech, and MM-TTS.

## Method Summary
MPE-TTS is a zero-shot TTS system that enables emotion customization through multi-modal prompts (text, image, or speech). The method disentangles speech hierarchically into content, timbre, emotion, and prosody. It uses a multi-modal prompt emotion encoder (MPEE) that extracts emotion features from any prompt type by aligning text and image prompts to a pre-trained speech emotion space via Emotion2Vec. An LLM-like prosody predictor with emotion consistency loss preserves emotional nuance in prosody, while a diffusion-based acoustic model generates the target mel-spectrogram. The system was trained on LibriTTS (585h) for pretraining and MEAD-TTS (36h) for fine-tuning, using three-stage training: MPEE training, acoustic model training, and prosody predictor training.

## Key Results
- MPE-TTS achieved WER improvements of 5.6-8.4% and emotion accuracy improvements of 13-19% over competitors like Meta-StyleSpeech, GenerSpeech, and MM-TTS.
- Subjective evaluations showed higher MOS (3.73 vs 3.55), ESMOS (4.05 vs 3.75), and SSMOS (3.73 vs 3.55) scores when using speech prompts.
- Ablation studies confirmed the effectiveness of MPEE and emotion consistency loss in improving emotion preservation.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Disentangling speech into hierarchical granularity enables independent control of emotion/timbre and prosody/content.
- **Mechanism:** The system separates features using specific bottlenecks and input isolation. Timbre is isolated by extracting a global vector from a random sentence of the same speaker. Prosody is isolated by extracting features exclusively from the low 20 bins of the mel-spectrogram, which contain rhythm/energy data with minimal timbre/content information.
- **Core assumption:** The low-frequency bins (0-20) of a mel-spectrogram sufficiently capture prosody while rejecting timbre and phonemic content.
- **Evidence anchors:** [section 2.1] "We define the timbre and emotion features as coarse-grained... extract the global speaker vector from a random sentence... to disentangle." [section 2.1] "VQ-based prosody encoder... based on the low 20 bins... which contain almost complete prosody and significantly less timbre."
- **Break condition:** If the low-bin assumption fails and timbre leaks into the prosody encoder, the system will fail to separate speaker identity from speaking style.

### Mechanism 2
- **Claim:** Aligning text and image prompts to a pre-trained speech emotion space allows flexible multi-modal control without training a TTS model from scratch on non-speech data.
- **Mechanism:** The system uses a frozen Emotion2Vec model as the speech anchor. It trains lightweight adapter layers on top of frozen CLIP encoders (for text/image) using Mean Squared Error (MSE) loss to map text/image embeddings onto the Emotion2Vec feature space.
- **Core assumption:** The pre-trained Emotion2Vec model provides a robust, universal "ground truth" for emotion that is superior to training a new joint space from scratch.
- **Evidence anchors:** [section 2.2] "We employ the Mean Squared Error (MSE) loss function to train the text emotion encoder and the image emotion encoder... [mapping] $E_t, E_i$ to $E_s$." [abstract] "Multi-modal prompt emotion encoder to extract emotion features from any prompt type."
- **Break condition:** If the Emotion2Vec space lacks granularity for subtle emotions, the text/image adapters will map to a coarse representation, resulting in generic outputs.

### Mechanism 3
- **Claim:** An autoregressive (LLM-like) predictor with an auxiliary consistency loss preserves emotional nuance in prosody better than direct prediction.
- **Mechanism:** Instead of predicting prosody directly from text alone, an 8-layer Transformer predicts prosody tokens conditioned on content, timbre, and emotion. An Emotion Consistency Loss (ECL) forces a classifier on the predicted prosody to match the target emotion label, acting as a regularizer.
- **Core assumption:** Prosody can be effectively discretized into VQ tokens suitable for a language model architecture.
- **Evidence anchors:** [section 2.3] "We introduce an LLM-like model... to fit the distribution of prosody... [with] Emotion Consistency Loss... to preserve emotion information." [corpus] Neighbor papers (e.g., "PROEMO") support the trend of prompt-driven intensity control, validating the focus on fine-grained emotional prosody.
- **Break condition:** If the ECL weight is too high, the predictor may prioritize emotion classification accuracy over natural speech fluency, resulting in robotic intonation.

## Foundational Learning

- **Concept:** **Vector Quantization (VQ) in Prosody**
  - **Why needed here:** The prosody encoder compresses continuous acoustic features into discrete codes to train the LLM-like predictor.
  - **Quick check question:** Does the codebook size restrict the diversity of possible speaking styles?

- **Concept:** **Diffusion Probabilistic Models**
  - **Why needed here:** The acoustic decoder uses a diffusion process (U-net) to generate the mel-spectrogram, favored for high-fidelity generation over simpler upsampling methods.
  - **Quick check question:** How does the inference speed of the diffusion decoder compare to the LLM prosody predictor?

- **Concept:** **Feature Disentanglement**
  - **Why needed here:** The core value proposition relies on changing emotion without changing the speaker (timbre).
  - **Quick check question:** If you swap the Timbre Encoder input but keep the Emotion Prompt the same, should the output voice sound different?

## Architecture Onboarding

- **Component map:** User Prompt → MPEE (CLIP + Emotion2Vec + Adapters) → Emotion Code → LLM Prosody Predictor (Transformer) → VQ Prosody Tokens → Diffusion Decoder (U-net) → Mel-spectrogram → Vocoder → Speech

- **Critical path:**
  1. **Training:** Acoustic Model is trained first → Prosody Encoder is extracted → Prosody Predictor is trained using the Encoder's outputs.
  2. **Inference:** User provides Prompt → MPEE extracts Emotion Code → LLM Predictor generates Prosody tokens → Diffusion Model generates Mel.

- **Design tradeoffs:**
  - The use of an LLM-like predictor (autoregressive) likely improves naturalness but introduces sequential latency compared to non-autoregressive flow-matching models.
  - Freezing CLIP/Emotion2Vec reduces trainable parameters but limits the model's ability to fine-tune representations for the specific TTS domain.

- **Failure signatures:**
  - **High WER (Word Error Rate):** Indicates the Content Encoder is leaking style information or the Duration Predictor is miscalibrated.
  - **Low ESMOS (Emotion Similarity):** If this drops but accuracy is high, the model is capturing the category but not the *intensity* or nuance.
  - **Speaker Leakage:** If changing the text prompt changes the perceived speaker voice, the disentanglement bottleneck is too wide.

- **First 3 experiments:**
  1. **Verify Disentanglement:** Generate speech using Speaker A's timbre and Speaker B's emotion prompt. Measure Speaker Similarity (SSMOS) and Emotion Similarity (ESMOS) to ensure they are independent.
  2. **Ablate ECL:** Run inference with $Loss_{ECL} = 0$ to confirm the drop in emotion accuracy reported in Table 1/2.
  3. **Cross-Modal Consistency:** Input "Happy" as text, image, and speech prompt for the same target text. Verify that the output prosody and emotion metrics remain roughly stable across modalities.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does simultaneously fusing multiple prompt modalities (e.g., text and image) improve emotion consistency and naturalness compared to single-modality inputs? The authors state they "have not yet explored the impact of multi-modal fusion features on zero-shot generation," limiting the system to selecting a single modality per inference.

- **Open Question 2:** Can the proposed hierarchical disentangling strategy maintain stability and performance when scaled to significantly larger datasets and model parameters? The authors acknowledge that "constraints in emotional data and computational resources" limit the current system and suggest that larger scales could enhance generation quality.

- **Open Question 3:** Does restricting prosody extraction to the low 20 frequency bins of the mel-spectrogram result in the loss of critical high-frequency emotional nuances? The method assumes the low 20 bins contain "almost complete prosody," but this cutoff may discard timbral or spectral details that convey specific emotions like "whispering" or "urgency."

## Limitations

- The paper lacks complete training details including learning rates, batch sizes, codebook dimensions, and exact loss weights, making faithful reproduction difficult.
- The VQ-based prosody encoder's low-bin assumption is theoretically sound but not empirically validated for all speakers and speaking styles across diverse emotional expressions.
- The use of frozen pre-trained models (Emotion2Vec, CLIP) may create a domain gap that limits the system's ability to handle truly unseen speakers, emotions, or fine-grained emotional nuances.

## Confidence

**High Confidence (4/5):** The hierarchical disentanglement approach is technically sound and well-supported by ablation studies showing MPEE and ECL improve performance.

**Medium Confidence (3/5):** The subjective evaluation results showing MPEE-TTS superiority are promising but may be influenced by the specific evaluation protocol or rater selection.

**Low Confidence (2/5):** The paper does not provide sufficient evidence that the system can handle fine-grained emotional nuances or intensity variations beyond the discrete categories in MEAD-TTS.

## Next Checks

1. **Disentanglement Verification:** Generate speech using Speaker A's timbre and Speaker B's emotion prompt, then measure Speaker Similarity (SSMOS) and Emotion Similarity (ESMOS). The outputs should maintain high speaker similarity while accurately conveying the target emotion.

2. **ECL Ablation Test:** Run inference with $Loss_{ECL} = 0$ and compare emotion accuracy (EACC) and emotion similarity (ESMOS) to the full model. This will confirm whether the Emotion Consistency Loss is truly necessary for emotion preservation.

3. **Cross-Modal Robustness Test:** Input "Happy" as text, image, and speech prompt for the same target text and measure the variation in output prosody and emotion metrics (ESMOS, EACC). The results should show stable performance across modalities if the MPEE is truly effective at unifying emotion representation.