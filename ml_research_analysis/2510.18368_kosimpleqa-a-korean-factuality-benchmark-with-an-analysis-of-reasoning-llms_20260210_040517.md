---
ver: rpa2
title: 'KoSimpleQA: A Korean Factuality Benchmark with an Analysis of Reasoning LLMs'
arxiv_id: '2510.18368'
source_url: https://arxiv.org/abs/2510.18368
tags:
- korean
- kosimpleqa
- llms
- simpleqa
- cultural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KoSimpleQA, the first factuality benchmark
  designed specifically for Korean cultural knowledge. The dataset consists of 1,000
  short, unambiguous questions grounded in Korean cultural contexts, collected through
  a two-stage quality control process involving crowdsourcing and expert validation.
---

# KoSimpleQA: A Korean Factuality Benchmark with an Analysis of Reasoning LLMs

## Quick Facts
- arXiv ID: 2510.18368
- Source URL: https://arxiv.org/abs/2510.18368
- Reference count: 6
- Introduces first factuality benchmark specifically for Korean cultural knowledge with 1,000 questions

## Executive Summary
This paper introduces KoSimpleQA, the first factuality benchmark designed specifically for Korean cultural knowledge. The dataset consists of 1,000 short, unambiguous questions grounded in Korean cultural contexts, collected through a two-stage quality control process involving crowdsourcing and expert validation. A comprehensive evaluation of diverse open-source LLMs shows that even the strongest models achieve only 33.7% correct answers, highlighting the benchmark's difficulty. Performance rankings on KoSimpleQA differ substantially from English benchmarks, indicating the unique cultural dimension it captures. Analysis of reasoning models reveals that engaging reasoning capabilities helps models better elicit latent knowledge and increases abstention when uncertain, suggesting reasoning can positively impact factual reliability.

## Method Summary
KoSimpleQA evaluates LLM factuality on Korean cultural knowledge using 1,000 short, fact-seeking questions with unambiguous single answers. The dataset was collected via Selectstar and Flitto platforms, culturally grounded in Korea, with knowledge cutoff December 31, 2023. Each question includes up to 4 source verification links. Evaluation follows SimpleQA framework using metrics: Correct (CO), Not Attempted (NA), Incorrect (IN), Correct Given Attempted (CGA), and F-score. Models are evaluated with temperature=1.0, top_p=1.0, max_tokens=2,048. Reference evaluation code is available at https://github.com/openai/simple-evals.

## Key Results
- Korean Community LLMs (HCX SEED 14B, kanana 8B, EXAONE 32B) outperformed Multilingual LLMs (Gemma 27B, Llama 3.1 70B, Qwen 32B) on KoSimpleQA
- Even strongest models achieved only 33.7% correct answers, demonstrating benchmark difficulty
- Performance rankings differ substantially from English SimpleQA, highlighting cultural knowledge dimension
- Reasoning models showed increased abstention rates while preserving or improving correct answers

## Why This Works (Mechanism)

### Mechanism 1: Culturally-Grounded Benchmark Divergence
- Claim: Models trained predominantly on Korean data are systematically underestimated by English-centric benchmarks, and cultural grounding in benchmarks reveals distinct capability dimensions not captured by translated evaluations.
- Mechanism: Training data language distribution shapes latent knowledge organization; when evaluation questions are culturally aligned with training distribution, models can better retrieve relevant knowledge.
- Core assumption: Cultural knowledge is not transferable via translation alone—it requires exposure to language-community-specific contexts during training.
- Evidence anchors: [abstract] "Performance rankings on KoSimpleQA differ substantially from those on the English SimpleQA, highlighting the unique value of our dataset." [Section 3.2] "This contrast indicates that KoSimpleQA captures aspects that are not evaluated by existing benchmarks such as SimpleQA."

### Mechanism 2: Reasoning-Induced Abstention and Knowledge Elicitation
- Claim: Engaging reasoning capabilities increases model abstention when uncertain while preserving or improving correct answer rates, suggesting reasoning helps models both recognize knowledge boundaries and better access latent knowledge.
- Mechanism: Extended "thinking" generation provides additional compute for self-consistency checking and uncertainty estimation.
- Core assumption: The abstention behavior reflects genuine uncertainty calibration rather than format failure, and correct answer improvements stem from better knowledge access rather than chance.
- Evidence anchors: [abstract] "Engaging reasoning capabilities in the factual QA task can both help models better elicit their latent knowledge and improve their ability to abstain when uncertain." [Section 3.3] "A particularly notable trend is that the rate of not attempted responses increased sharply, while correct responses were largely preserved or slightly improved."

### Mechanism 3: Catastrophic Forgetting in Reasoning Fine-Tuning
- Claim: Reasoning-specific fine-tuning can degrade factual reliability if training sacrifices factual knowledge retention for reasoning behavior acquisition.
- Mechanism: EXAONE Deep showed substantial performance drops vs. base instruct models, suggesting the fine-tuning process for reasoning capabilities may have overwritten or suppressed factually-grounded parameters.
- Core assumption: The performance drop is attributable to the reasoning fine-tuning process specifically, not other factors like smaller effective capacity or different prompting requirements.
- Evidence anchors: [Section 3.3] "EXAONE Deep exhibited a substantial drop in performance compared to its base instruct version... suggesting that catastrophic forgetting may have occurred, likely compromising factual reliability for reasoning-specific behavior."

## Foundational Learning

- Concept: **SimpleQA-Style Evaluation Paradigm**
  - Why needed here: KoSimpleQA inherits the design principle of short, unambiguous, fact-seeking questions that are challenging yet easy to grade.
  - Quick check question: Can you explain why unambiguous answers are critical for reliable automated grading in factuality benchmarks?

- Concept: **Linguistic Competence vs. Cultural Knowledge**
  - Why needed here: The paper explicitly distinguishes fluent language generation from culturally-grounded knowledge.
  - Quick check question: If you translate SimpleQA questions to Korean, why would this not adequately test Korean cultural knowledge?

- Concept: **Reasoning Model Behavior Patterns**
  - Why needed here: The paper analyzes "thinking mode" behavior—extended reasoning generation that can lead to abstention.
  - Quick check question: When a reasoning model increases its "not attempted" rate, what two possible interpretations should you consider?

## Architecture Onboarding

- Component map:
  Dataset Core -> Quality Control Pipeline -> Evaluation Framework -> Model Categories -> Reasoning Analysis

- Critical path:
  1. Load KoSimpleQA questions (1,000 items with source links for verification)
  2. Configure generation parameters (temperature=1.0, top_p=1.0, max_tokens=2,048 per SimpleQA protocol)
  3. Run inference on target models, recording responses
  4. Grade responses using the defined metrics (exact match against unambiguous answers)
  5. For reasoning models, track thinking-mode behavior including token-limited abstentions

- Design tradeoffs:
  - **Short-answer constraint**: Ensures reliable grading but does not evaluate long-form factual generation quality
  - **Temporal cutoff (Dec 31, 2023)**: Avoids rapidly-changing facts but may exclude relevant recent cultural developments
  - **Difficulty filtering via closed-source model failures**: Ensures benchmark challenge but may bias toward certain failure modes

- Failure signatures:
  - **High incorrect + low abstention**: Model hallucinates confidently rather than refusing—indicates poor uncertainty calibration
  - **Large performance gap vs. English SimpleQA**: May indicate training data language imbalance or cultural knowledge gaps
  - **Reasoning model: sharp accuracy drop vs. base**: Potential catastrophic forgetting from reasoning fine-tuning
  - **Reasoning model: high abstention with low correct**: Thinking mode failing to elicit knowledge, possibly stuck in reasoning loops

- First 3 experiments:
  1. **Baseline establishment**: Run Korean Community LLMs (HCX SEED 14B, kanana 8B, EXAONE 32B) and Multilingual LLMs (Gemma 27B, Qwen 32B) on KoSimpleQA to reproduce the Korean vs. multilingual performance gap reported in Table 1.
  2. **Cross-benchmark ranking analysis**: Evaluate the same models on both KoSimpleQA and English SimpleQA to verify the ranking divergence phenomenon—confirm Korean models excel on Korean benchmark while multilingual models excel on English benchmark.
  3. **Reasoning abstention analysis**: Compare base instruct vs. thinking-enabled variants for Qwen3 and HCX SEED, specifically measuring changes in (a) not-attempted rates, (b) correct answer preservation, and (c) incorrect answer reduction to validate the reasoning-induced calibration hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does performance on short-form factuality benchmarks like KoSimpleQA predict the ability to generate longer, fact-rich responses in Korean cultural contexts?
- Basis in paper: [explicit] The Limitations section states that SimpleQA-style benchmarks "do not capture whether the ability to generate factual short answers translates to producing longer, fact-rich responses. Bridging this gap remains an open direction for future research."
- Why unresolved: The current evaluation paradigm only assesses single-answer fact-seeking questions, leaving the relationship to extended generation untested.
- What evidence would resolve it: A correlational study comparing model performance on KoSimpleQA against performance on a long-form Korean generation benchmark requiring factual accuracy across multiple sentences or paragraphs.

### Open Question 2
- Question: Why does reasoning fine-tuning cause catastrophic forgetting of factual knowledge in some model families (EXAONE Deep) but not others (Qwen3 think, HCX SEED think)?
- Basis in paper: [inferred] The paper reports that EXAONE Deep showed "a substantial drop in performance" with "catastrophic forgetting" while other reasoning models "either maintained or improved their overall performance," but does not investigate the cause of this discrepancy.
- Why unresolved: The training data, architecture differences, and fine-tuning methodologies across these models are not analyzed, leaving the source of divergent behavior unclear.
- What evidence would resolve it: A controlled comparison of reasoning training procedures across model families, isolating factors such as training data composition, learning rates, and whether reasoning capabilities are built-in versus added via fine-tuning.

### Open Question 3
- Question: What mechanisms enable reasoning models to increase abstention rates on uncertain questions while preserving or improving correct answer rates?
- Basis in paper: [inferred] The analysis section observes that "the rate of not attempted responses increased sharply, while correct responses were largely preserved or slightly improved," but does not explain how reasoning enables this beneficial calibration.
- Why unresolved: The paper documents the phenomenon but does not probe whether it stems from extended thinking loops, internal confidence estimation, or other factors.
- What evidence would resolve it: An analysis of reasoning traces correlating specific reasoning behaviors (e.g., self-correction, source verification statements, hedging language) with final abstention decisions, across multiple reasoning model architectures.

## Limitations

- **Grading Methodology Uncertainty**: The paper relies on SimpleQA evaluation metrics but does not specify whether answers are graded via exact string matching, pattern matching, or through a grader model.
- **Temporal Knowledge Constraints**: With a knowledge cutoff of December 31, 2023, the benchmark may not capture recent cultural developments or rapidly evolving knowledge domains.
- **Reasoning Model Evaluation Complexity**: The analysis of reasoning models reveals that extended thinking generation can lead to token-limited abstentions, but does not fully disentangle whether these abstentions reflect genuine uncertainty calibration versus model format limitations.

## Confidence

- **High Confidence**: The core finding that culturally-grounded benchmarks reveal different capability dimensions than translated benchmarks.
- **Medium Confidence**: The mechanism that reasoning capabilities help models elicit latent knowledge and improve uncertainty calibration.
- **Low Confidence**: The catastrophic forgetting claim for EXAONE Deep.

## Next Checks

1. **Reproduce Cross-Benchmark Ranking Divergence**: Evaluate the same set of Korean and multilingual models on both KoSimpleQA and English SimpleQA using identical evaluation protocols. Confirm that models trained primarily on Korean data show reversed performance rankings compared to multilingual models, validating the cultural grounding hypothesis.

2. **Validate Reasoning Calibration Mechanism**: Conduct ablation studies comparing reasoning models under different token budgets (e.g., max_tokens=1024 vs 2048). If reasoning-induced abstention is genuine uncertainty calibration, reducing token budgets should maintain correct answer rates while reducing abstentions.

3. **Test Cultural Transferability Hypothesis**: Translate a subset of KoSimpleQA questions to English and evaluate both Korean and multilingual models. If cultural knowledge is not transferable via translation, models should show systematically different performance on translated questions versus questions originally written in English, even when controlling for linguistic complexity.