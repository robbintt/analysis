---
ver: rpa2
title: 'Never Start from Scratch: Expediting On-Device LLM Personalization via Explainable
  Model Selection'
arxiv_id: '2504.13938'
source_url: https://arxiv.org/abs/2504.13938
tags:
- data
- language
- pllm
- arxiv
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: XPerT enables efficient on-device LLM personalization by selecting
  and fine-tuning pre-personalized models from the cloud, reducing computation costs
  by 83% and improving data efficiency by 51%. It achieves this through explainable
  model selection, where language style differences between cached personalized models
  and base models are quantified using orthogonal embedding vectors.
---

# Never Start from Scratch: Expediting On-Device LLM Personalization via Explainable Model Selection

## Quick Facts
- arXiv ID: 2504.13938
- Source URL: https://arxiv.org/abs/2504.13938
- Reference count: 40
- Enables efficient on-device LLM personalization by selecting and fine-tuning pre-personalized models from the cloud, reducing computation costs by 83% and improving data efficiency by 51%

## Executive Summary
XPerT introduces a novel approach for on-device LLM personalization that avoids starting from scratch by leveraging a library of pre-personalized models stored in the cloud. The method uses explainable model selection based on orthogonal embedding vectors to quantify language style differences between cached personalized models and base models. By comparing these vectors locally with user's personal data, XPerT enables accurate model selection without downloading full models, significantly reducing communication and computation costs while maintaining high selection accuracy across multiple smartphone models and LLM families.

## Method Summary
XPerT addresses the challenge of on-device LLM personalization by implementing a two-stage process: model selection and local fine-tuning. Instead of downloading and fine-tuning base models from scratch, the system selects the most suitable pre-personalized model from a cached library using orthogonal embedding vectors that capture language style differences. These vectors are compared locally with the user's personal data to identify the best matching model. The selected model is then fine-tuned on-device using the user's data, significantly reducing both communication costs (up to 96.5%) and computation requirements (up to 71.4%) compared to traditional approaches while maintaining selection accuracy of up to 96%.

## Key Results
- Reduces computation costs by 83% and improves data efficiency by 51%
- Achieves up to 96.5% reduction in communication costs and 71.4% reduction in computation costs
- Maintains high selection accuracy of up to 96% across multiple smartphone models and LLM families

## Why This Works (Mechanism)
The system works by leveraging orthogonal embedding vectors that capture the distinct language style differences between pre-personalized models and their base versions. These vectors serve as efficient representations that can be compared locally with user data without requiring full model downloads. The orthogonal nature ensures that the differences between models are preserved and can be accurately measured, enabling precise model selection. By selecting the most appropriate pre-personalized model first, the subsequent fine-tuning requires significantly less data and computation, as the model is already closer to the desired personalized state.

## Foundational Learning
- **Orthogonal embedding vectors**: Mathematical constructs that preserve the distinct differences between model variants; needed for accurate style comparison without full model downloads
- **On-device fine-tuning**: Process of adapting pre-trained models using local data; required to maintain privacy and reduce latency
- **Model selection via vector comparison**: Technique for choosing optimal pre-personalized model; essential for minimizing fine-tuning costs
- **Language style quantification**: Method of measuring differences in writing patterns; critical for accurate model matching
- **Cloud-based model caching**: Strategy for storing pre-personalized models; necessary for quick access and reduced communication overhead

## Architecture Onboarding

**Component Map**: User Data -> Local Vector Extraction -> Cloud Model Library -> Vector Comparison -> Selected Model -> On-Device Fine-Tuning -> Personalized Model

**Critical Path**: The core workflow involves extracting orthogonal embedding vectors from user data, comparing these vectors against cached model vectors in the cloud, selecting the best-matching model, and performing local fine-tuning to achieve personalization.

**Design Tradeoffs**: The approach trades initial cloud storage requirements and vector computation overhead for significant reductions in communication and computation costs during personalization. The orthogonal embedding representation adds complexity but enables the privacy-preserving local selection mechanism.

**Failure Signatures**: Poor selection accuracy may occur with sparse or noisy user data, insufficient diversity in the cached model library, or when language style differences are too subtle to capture with orthogonal vectors. Model selection failures cascade into suboptimal fine-tuning results.

**3 First Experiments**:
1. Verify orthogonal embedding vector computation accurately captures language style differences across different LLM families
2. Test selection accuracy with varying amounts of user data (sparse vs. rich)
3. Measure communication cost reduction when selecting models versus downloading full base models

## Open Questions the Paper Calls Out
None

## Limitations
- Performance may degrade with highly sparse or noisy personal data
- Generalization across different personalization domains beyond tested cases is uncertain
- Real-world scalability across diverse device models and network conditions needs validation

## Confidence

**Major Claim Clusters:**
- On-device model selection and fine-tuning efficiency: High confidence
- Cost reduction (communication and computation) and data efficiency: Medium confidence
- Scalability and generalizability across domains and devices: Low confidence

## Next Checks
1. Test the orthogonal embedding-based selection on a broader set of personalization domains (e.g., technical writing, medical queries) to assess generalizability
2. Evaluate the method's performance with highly sparse or noisy user data to determine robustness
3. Conduct real-world field tests on diverse device models and network conditions to verify scalability and practical cost savings