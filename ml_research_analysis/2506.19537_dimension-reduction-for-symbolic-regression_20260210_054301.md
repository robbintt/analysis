---
ver: rpa2
title: Dimension Reduction for Symbolic Regression
arxiv_id: '2506.19537'
source_url: https://arxiv.org/abs/2506.19537
tags:
- regression
- symbolic
- search
- beam
- substitutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an iterative dimension reduction technique
  for symbolic regression that systematically searches for valid variable substitutions
  to simplify regression problems. The core idea is to enumerate small expression
  DAGs as candidate substitutions and validate them using functional dependence measures
  (CODEC and KMAC).
---

# Dimension Reduction for Symbolic Regression

## Quick Facts
- arXiv ID: 2506.19537
- Source URL: https://arxiv.org/abs/2506.19537
- Reference count: 15
- Key outcome: Iterative dimension reduction via systematic variable substitutions improves symbolic regression recovery rates by ~50% across diverse algorithms

## Executive Summary
This paper introduces an iterative dimension reduction technique for symbolic regression that systematically searches for valid variable substitutions to simplify regression problems. The method uses a beam search to enumerate small expression DAGs as candidate substitutions and validates them using functional dependence measures (CODEC and KMAC). By transforming the regression problem to a new coordinate system with fewer variables, the approach significantly reduces the search space for downstream symbolic regressors. Experiments on 880 Wikipedia equations and 114 Feynman formulas show the method reduces variables by ~50% on average and improves recovery rates from 58% to 69% for UDFS and from 6% to 34% for polynomial regression.

## Method Summary
The method performs iterative dimension reduction by systematically searching for valid variable substitutions. It uses beam search to explore a tree of possible substitutions, where each node represents a transformed dataset with fewer variables. Candidate substitutions are enumerated as small expression DAGs and validated using CODEC or KMAC functional dependence measures. The approach handles both input substitutions (replacing variable combinations with a single variable) and out-input substitutions (involving the output variable). Valid substitutions are applied iteratively to reduce problem dimensionality, with the most reduced problem then solved by any symbolic regression algorithm. The final expression is reconstructed by combining the regressor's output with the sequence of substitutions found.

## Key Results
- Reduces variables by ~50% on average across 880 Wikipedia equations and 114 Feynman formulas
- Improves recovery rates from 58% to 69% for UDFS and from 6% to 34% for polynomial regression
- Consistently increases Jaccard similarity between recovered and ground-truth expressions even when exact recovery fails
- Outperforms baseline volume measure for dependence testing across all tested noise levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iteratively identifying variable substitutions reduces symbolic regression search complexity.
- Mechanism: A beam search enumerates small expression DAGs as candidate substitutions (input and out-input types). Each substitution transforms the regression problem to a new coordinate system with fewer variables, shrinking the search space dimension for downstream symbolic regressors.
- Core assumption: The true underlying formula contains variables that appear only in fixed combinations.
- Evidence anchors: [abstract] "The core idea is to enumerate small expression DAGs as candidate substitutions and validate them using functional dependence measures..." [section] "The resulting iterative dimension reduction procedure can be used with any symbolic regression approach."
- Break condition: The ground-truth expression contains variables that do not group into any simple functional form that can be discovered by the limited DAG search.

### Mechanism 2
- Claim: Validating substitutions with CODEC and KMAC functional dependence measures reliably identifies valid substitutions from finite samples.
- Mechanism: Instead of relying on hard-coded predicates, the method uses statistical measures of functional dependence. A high dependence score (close to 1) between the transformed variables and the output suggests that a valid functional relationship has been preserved, indicating a successful substitution.
- Core assumption: The functional dependence measures converge to 1 for noiseless functional relationships and 0 for independent variables, given sufficient samples.
- Evidence anchors: [abstract] "...validate them using functional dependence measures (CODEC and KMAC)." [section] "For checking if a possible substitution is a valid substitution, we build on recent work by Chatterjee (2021) and generalizations..."
- Break condition: High noise levels or insufficient sample sizes cause dependence scores to be unreliable, leading to spurious or missed substitutions.

### Mechanism 3
- Claim: Out-input substitutions allow discovering more complex input substitutions indirectly.
- Mechanism: Some valid substitutions are too complex to be found directly. An out-input substitution creates an auxiliary regression problem. Solving this yields a function. A computer algebra system then solves the equation for the output variable, effectively recovering the complex input substitution.
- Core assumption: The target formula's structure can be decomposed using a simple out-input substitution.
- Evidence anchors: [abstract] "The approach can handle both input substitutions... and out-input substitutions (involving the output variable)." [section] "Conceptually, a simple out-input substitution h is used in an auxiliary symbolic regression problem to find a complex input substitution g."
- Break condition: The equation cannot be solved symbolically for the output variable by the CAS.

## Foundational Learning

- **Concept: Beam Search**
  - Why needed here: The search space for possible substitutions is exponential. Beam search is a heuristic that explores a graph by expanding the most promising nodes in a limited set (the "beam"), balancing search quality with computational cost.
  - Quick check question: If you have a beam width of 3 and 5 candidate substitutions with scores [0.9, 0.8, 0.7, 0.6, 0.5], which nodes will be explored in the next step?

- **Concept: Directed Acyclic Graphs (DAGs) for Symbolic Expressions**
  - Why needed here: Expression trees can be inefficient for expressions with repeated sub-expressions. DAGs allow common sub-expressions to be represented by a single node, enabling more compact and efficient enumeration of the search space.
  - Quick check question: Draw the expression tree and the corresponding DAG for a + b * c - b * c. Which is more efficient?

- **Concept: Functional Dependence Measures (CODEC, KMAC)**
  - Why needed here: These are the statistical tests that power the substitution search. Understanding that they provide a scalar score indicating how likely a functional relationship exists between variables is key to understanding how valid substitutions are ranked.
  - Quick check question: If CODEC(X, Y) ≈ 1, what does that imply about the relationship between random variables X and Y? What if CODEC(X, Y) ≈ 0?

## Architecture Onboarding

- **Component map:** Beam Search Orchestrator -> Expression DAG Enumerator -> Dependence Tester -> Problem Transformer -> Symbolic Regressor (External) -> CAS (SymPy)

- **Critical path:**
  1. Input data (x, y) enters the Beam Search.
  2. DAG Enumerator proposes a set of candidate substitutions.
  3. For each candidate, the data is transformed, and the Dependence Tester computes a score.
  4. The Beam Search Orchestrator selects the top-scoring substitutions (beam width).
  5. Steps 2-4 repeat iteratively on reduced problems until no further substitution is found.
  6. The path leading to the highest-scoring reduced problem is selected.
  7. The Symbolic Regressor runs on the final reduced problem to find f_g.
  8. The final expression f is reconstructed by combining f_g with the sequence of substitutions found.

- **Design tradeoffs:**
  - Beam Size vs. Search Quality: Larger beam size finds better substitutions but increases runtime significantly. The paper finds diminishing returns beyond beam size 1 for their benchmarks.
  - Search Space vs. Substitution Complexity: The paper limits the DAG search to very small graphs (1 intermediary node). This limits the complexity of substitutions that can be found directly.
  - Direct vs. Indirect Search: Out-input substitutions add power by finding complex substitutions indirectly but introduce a dependence on the CAS for symbolic inversion.

- **Failure signatures:**
  - Stagnant Search: Reduction rate is zero or near-zero. Likely cause: expression DAG search space is too small, or noise levels are too high for dependence measures.
  - Runtime Explosion: Search is too slow. Likely cause: beam size is too large or dependence testing is the bottleneck (e.g., using KMAC over CODEC).
  - Spurious Substitutions: Recovery rates drop. Likely cause: dependence measure threshold is too permissive, identifying false functional relationships.

- **First 3 experiments:**
  1. Re-run the Reduction Rate Ablation: Reproduce the experiment comparing CODEC, KMAC, and the baseline volume measure on the Feynman dataset at different noise levels.
  2. Validate Single Substitution Step: Take a known formula with a known valid substitution (e.g., Washburn's equation). Run a single step of the search and confirm that the algorithm correctly identifies and scores the substitution higher than random candidates.
  3. End-to-End Recovery Test: Run the full pipeline (Beam Search + UDFS) on a small subset of the Wikipedia equations dataset and compare the recovery rate to running UDFS alone.

## Open Questions the Paper Calls Out
- Question: Does expanding the expression DAG search space beyond single-intermediary nodes yield valid, complex substitutions that are currently being missed, and does this justify the computational cost?
- Basis in paper: [explicit] The "Implementation Details" section states the authors "keep the search space small" by restricting the search to DAGs "with at most one intermediary node and one output node."
- Why unresolved: This restriction is a heuristic choice for efficiency; the potential performance gains (or lack thereof) from exploring deeper expression trees remain unquantified.
- What evidence would resolve it: An ablation study measuring the discovery rate of valid substitutions and total runtime when the maximum DAG depth is increased.

## Limitations
- The method's performance critically depends on the ability of functional dependence measures (CODEC/KMAC) to detect valid relationships in finite, potentially noisy samples.
- The search is limited to very small DAGs (k=1), which may miss complex but valid substitutions.
- The reliance on a computer algebra system (CAS) for out-input substitutions introduces another potential failure point if symbolic inversion fails.

## Confidence
- **High Confidence:** The general framework of using beam search to find valid variable substitutions and the resulting reduction in problem dimensionality. The experiments demonstrate consistent improvement in recovery rates across multiple symbolic regression algorithms.
- **Medium Confidence:** The effectiveness of CODEC/KMAC measures for validating substitutions in the presence of noise. While the paper shows they outperform a baseline, the exact thresholds and robustness across diverse formulas need further validation.
- **Low Confidence:** The scalability of the method to very large problems or the ability to find substitutions involving complex interactions beyond the limited DAG search space.

## Next Checks
1. **Stress Test Dependence Measures:** Systematically vary noise levels and sample sizes on synthetic formulas and measure how CODEC/KMAC performance degrades compared to exact symbolic verification.
2. **Explore Larger DAGs:** Modify the search to allow slightly larger DAGs (k=2 or 3) and evaluate if the additional computational cost is justified by improved recovery rates on a subset of the benchmarks.
3. **Analyze Failure Cases:** For formulas where the method fails to recover the ground truth, analyze the substitution search tree to identify why valid substitutions were missed (e.g., too complex for DAG search, dependence measures failed).