---
ver: rpa2
title: 'Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional
  Reasoning'
arxiv_id: '2601.15160'
source_url: https://arxiv.org/abs/2601.15160
tags:
- reasoning
- reward
- compositional
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a post-training pipeline that uses knowledge
  graphs as implicit reward models to enable compositional reasoning in large language
  models. The method combines supervised fine-tuning with reinforcement learning,
  where path-derived rewards from knowledge graph triples encourage models to compose
  intermediate axioms rather than focus solely on final answers.
---

# Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning

## Quick Facts
- **arXiv ID:** 2601.15160
- **Source URL:** https://arxiv.org/abs/2601.15160
- **Reference count:** 32
- **Primary result:** A 14B model trained with KG-derived path rewards outperforms much larger frontier models on complex medical multi-hop reasoning

## Executive Summary
This work introduces a post-training pipeline that uses knowledge graphs as implicit reward models to enable compositional reasoning in large language models. The method combines supervised fine-tuning with reinforcement learning, where path-derived rewards from knowledge graph triples encourage models to compose intermediate axioms rather than focus solely on final answers. Training a 14B model on short-hop reasoning paths and evaluating on complex multi-hop queries shows that the approach significantly improves accuracy, especially on the most difficult tasks, outperforming much larger frontier models. The method is robust to adversarial perturbations and demonstrates that grounding reasoning in structured knowledge is a scalable path to intelligent reasoning.

## Method Summary
The method employs a two-stage training pipeline: first supervised fine-tuning (SFT) on 19,660 multi-hop medical QA tasks with ground-truth KG paths and reasoning traces, then reinforcement learning with Group Relative Policy Optimization (GRPO) on 5,000 tasks using a composite reward signal. The reward combines binary correctness with path alignment, measuring token overlap between the model's reasoning trace and ground-truth KG path entities. The approach uses a 14B Qwen3 model with LoRA adapters for SFT, then full-parameter updates during RL. The key innovation is using KG paths as implicit reward models that provide process-level supervision, enabling compositional generalization from training on 1-3 hop paths to solving 4-5 hop queries.

## Key Results
- SFT+RL model achieves 82.20% accuracy on ICD-Bench, outperforming SFT-only (70.86%) and much larger frontier models
- Path alignment reward alone achieves 79.29%, showing process supervision is effective even without outcome rewards
- Generalization gap widens with complexity: +7.5% on 4-hop and +11.1% on 5-hop questions vs. SFT-only
- Asymmetric negative reinforcement (β=-1 for incorrect, α=0.1 for correct) provides stable learning dynamics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Path-derived rewards from KG triples provide process-level supervision that enables compositional generalization beyond training complexity.
- **Mechanism:** The path alignment reward R_path measures token overlap between the model's reasoning trace and ground-truth KG path entities (coverage = |T(r) ∩ T(P)| / |T(P)|). A minimum-hit constraint (≥2 distinct path entities) discourages trivial matches. This creates dense feedback on intermediate reasoning steps rather than sparse outcome-only signals.
- **Core assumption:** KG paths represent verifiable, complete logical chains; models that align with these paths are reasoning correctly rather than exploiting surface patterns.
- **Evidence anchors:**
  - [abstract] "path-derived rewards act as a 'compositional bridge', enabling our model to significantly outperform much larger models"
  - [Section 5.1] SFT+RL shows +7.5% on 4-hop and +11.1% on 5-hop questions vs. SFT-only; generalization gap widens as hop length increases
  - [corpus] Related work (DPRM, arXiv:2511.08364) explores dual implicit process reward models for multi-hop QA, suggesting process supervision is an active research direction but not yet standardized
- **Break condition:** If KG paths are incomplete, contain errors, or don't represent the minimal logical chain, path alignment may reward incorrect reasoning or penalize valid alternative paths.

### Mechanism 2
- **Claim:** SFT provides necessary axiomatic grounding; RL alone is insufficient for compositional reasoning at small model scales.
- **Mechanism:** The base model lacks domain knowledge. Zero-RL (GRPO directly on base model) with 5k–24k examples fails to match SFT-only performance (70.86%). SFT first installs atomic facts and reasoning patterns; RL then amplifies compositional logic to connect these patterns.
- **Core assumption:** Compositional ability emerges from amplifying existing knowledge rather than acquiring both knowledge and composition simultaneously.
- **Evidence anchors:**
  - [Section 4.2] "Zero-RL approach... does not consistently outperform SFT-only training... 5k examples yields the strongest results among all other settings"
  - [Section 2.1] Aligns with Chu et al. (2025): "SFT memorizes, RL generalizes"
  - [corpus] Plan Then Retrieve (arXiv:2510.20691) combines RL with KG reasoning but uses KG for retrieval planning, not as a reward signal—different mechanism
- **Break condition:** If SFT data quality is poor or lacks coverage of core axioms, RL may amplify incorrect foundations; if the base model is already sufficiently capable, SFT may not add marginal value.

### Mechanism 3
- **Claim:** Asymmetric negative reinforcement combined with path alignment provides stable learning dynamics for compositional reasoning.
- **Mechanism:** Binary reward uses asymmetric penalty (α=0.1 for correct, β=−1.0 for incorrect) per Zhu et al. (2025). This encourages exploration of correct alternate paths while penalizing wrong answers strongly. Combined with R_path, the total reward R_total = R_bin + R_path balances outcome correctness with process alignment.
- **Core assumption:** Stronger penalties for incorrect answers than rewards for correct ones stabilize policy optimization in multi-hop reasoning tasks.
- **Evidence anchors:**
  - [Table 4] Path alignment + negative binary achieves 82.20% vs. path alignment + normal binary at 68.03%
  - [Section 4.4] "asymmetric design ensures stable learning by reinforcing exploration of correct alternate paths"
  - [corpus] Corpus signals show limited direct evidence on asymmetric reward design in KG-RL; this appears to be a novel contribution of this work
- **Break condition:** If penalty is too high, the model may become overly conservative and fail to explore; if too low, reward hacking on path tokens without valid reasoning may occur.

## Foundational Learning

- **Concept:** Knowledge Graph Triple Representation (head, relation, tail)
  - **Why needed here:** The entire reward mechanism depends on representing domain knowledge as traversable paths of triples. Understanding that (SMARCB1 mutation, causes, RTPS) is a single hop, and multi-hop paths chain these, is essential.
  - **Quick check question:** Given triples (A, treats, B) and (B, causes, C), what 2-hop inference connects A to C?

- **Concept:** Group Relative Policy Optimization (GRPO)
  - **Why needed here:** The RL stage uses GRPO, which normalizes advantages at the group level and eliminates the need for a separate critic. Understanding how group-level normalization affects reward scaling is critical for debugging.
  - **Quick check question:** How does GRPO differ from standard PPO in handling advantage estimation?

- **Concept:** Compositional Generalization
  - **Why needed here:** The paper's central claim is that training on 1-3 hop paths enables 4-5 hop reasoning. This requires understanding composition as combining learned primitives into novel sequences, not memorizing longer chains.
  - **Quick check question:** If a model sees (A→B) and (B→C) paths during training, what type of generalization is required to solve (A→B→C→D)?

## Architecture Onboarding

- **Component map:** Base LLM (Qwen3 14B) → SFT Stage (LoRA, r=16, α=16, lr=2e-4) → SFT Checkpoint → RL Stage (GRPO, lr=8e-6, temp=0.6, G=2) → Final SFT+RL Model

- **Critical path:** The path alignment reward extraction is the most fragile component. It requires: (1) parsing `<think`> blocks from model output, (2) tokenizing and normalizing, (3) matching against ground-truth path tokens from the KG. Errors in any step produce noisy rewards that destabilize training.

- **Design tradeoffs:**
  - RL data budget (5k) vs. SFT budget (19.66k): Authors found larger RL budgets yield diminishing returns and instability; small but high-quality RL is more efficient
  - LoRA-only RL updates (66.75% accuracy) vs. full-parameter updates (82.20%): Restricted updates insufficient for compositional reasoning
  - Combining all rewards (similarity + thinking quality + path + binary) collapsed to 55.21%: Over-optimization with conflicting signals causes failure

- **Failure signatures:**
  - Accuracy plateaus or degrades after initial RL epochs → check reward hacking (repetition penalty ϕ_rep=1.15 should help)
  - Model generates fluent reasoning but wrong answers → R_bin weight may be too low; increase β penalty
  - High accuracy on 2-hop but collapse on 5-hop → SFT coverage may be insufficient; R_path minimum-hit constraint may be too permissive

- **First 3 experiments:**
  1. **Reproduce Zero-RL baseline:** Apply GRPO directly to base model with 5k examples using R_bin only. Verify that accuracy stays below SFT-only (~65% vs. 70.86%). This confirms SFT is a prerequisite.
  2. **Ablate reward components:** Train SFT→RL with R_path only (no R_bin), then R_bin only (no R_path), then both. Expected: R_path only (~79.29%), R_bin only (~79.54%), both (~82.20%). Confirms synergy.
  3. **Hop generalization test:** Evaluate on held-out 2-hop, 3-hop, 4-hop, 5-hop splits separately. Verify the gap between SFT-only and SFT+RL widens with hop count (Figure 3 pattern). If gap is uniform, the mechanism may not be compositional generalization.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can knowledge graph-derived reward signals enable compositional reasoning in domains beyond medicine, such as legal reasoning, organic chemistry, or engineering?
- **Open Question 2:** What is the theoretical limit of compositional generalization—can models trained on short paths (1-3 hops) generalize to arbitrarily long reasoning chains (6+ hops)?
- **Open Question 3:** How robust is the path-aligned reward signal to incompleteness or noise in the underlying knowledge graph?

## Limitations
- The approach is validated only in the medical domain using UMLS, limiting generalizability to other knowledge structures
- Absolute accuracy on 5-hop queries remains moderate (~55-65%), suggesting the method still struggles with maximum complexity
- The paper assumes high-quality, complete KG paths; real-world KGs often contain missing or incorrect triples that could mislead reward computation

## Confidence
- **High Confidence:** SFT+RL pipeline with path alignment rewards outperforms both SFT-only and larger frontier models; asymmetric negative reinforcement is crucial for stable learning
- **Medium Confidence:** Path-derived rewards enable compositional generalization from 1-3 hop training to 4-5 hop testing; mechanism may also reflect improved memorization
- **Low Confidence:** Knowledge graphs are "implicit reward models" is somewhat metaphorical; scalability claim based on single domain and model scale

## Next Checks
1. **Cross-Domain Generalization Test:** Apply the SFT+RL pipeline with path-derived rewards to a non-medical KG domain (e.g., Wikipedia-based commonsense reasoning) and evaluate whether the same compositional generalization benefits emerge.

2. **Path Coverage Ablation:** Systematically vary the completeness of ground-truth KG paths used for reward computation (e.g., using 50%, 75%, 100% of path tokens) and measure the impact on final accuracy.

3. **Alternative Process Supervision Comparison:** Replace KG-derived path alignment with other forms of process supervision (e.g., chain-of-thought alignment from human-annotated reasoning traces) while keeping the same SFT+RL framework.