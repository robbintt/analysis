---
ver: rpa2
title: 'Evaluating LLMs'' Multilingual Capabilities for Bengali: Benchmark Creation
  and Performance Analysis'
arxiv_id: '2507.23248'
source_url: https://arxiv.org/abs/2507.23248
tags:
- bengali
- datasets
- english
- across
- tokenization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of standardized evaluation benchmarks
  for Bengali NLP by translating and curating eight English benchmark datasets into
  Bengali. The authors evaluate ten recent open-source multilingual LLMs on these
  translated datasets using metrics such as accuracy, Response Error Rate (RER), Response
  Adherence Rate (RAR), and LLM-Judge.
---

# Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis

## Quick Facts
- arXiv ID: 2507.23248
- Source URL: https://arxiv.org/abs/2507.23248
- Reference count: 5
- Primary result: Bengali NLP benchmarks translated from English reveal consistent performance gaps across multilingual LLMs, with tokenization inefficiency and limited language exposure as key limiting factors.

## Executive Summary
This paper addresses the lack of standardized evaluation benchmarks for Bengali NLP by translating eight English benchmark datasets into Bengali and evaluating ten recent open-source multilingual LLMs. The authors find consistent performance gaps for Bengali compared to English across all models, with smaller models and certain families (particularly Mistral) showing larger drops. Tokenization analysis reveals that Bengali inputs generate significantly more tokens per instance and per word than English, and excessive tokenization correlates with lower accuracy. The study also finds that models with explicit multilingual pretraining show smaller performance gaps, while larger models partially compensate for tokenization inefficiency. These findings highlight critical limitations in current multilingual models and underscore the need for improved datasets and evaluation methodologies for underrepresented languages.

## Method Summary
The authors translated 20 English NLP benchmark datasets to Bengali using GPT-4o-mini, selecting 8 for evaluation (HELLASWAG, WINOGRANDE, CommonsenseQA, BoolQ, OpenBookQA, ARC, GSM8K-Main, MMLU). They evaluated 10 multilingual LLMs (LLaMA 3.1 8B/70B, LLaMA 3.2 3B, LLaMA 3.3 70B, Qwen 2.5 7B/72B, Mistral 7B/24B, DeepSeek-R1 14B/70B) on these translated datasets using zero-shot inference. Performance was measured using Accuracy, Response Error Rate (RER), Response Adherence Rate (RAR), and LLM-Judge. The study also conducted tokenization analysis comparing Bengali vs English token counts (ATPR, ATPW, ABPT, ANSL) and examined the relationship between tokenization efficiency and model accuracy.

## Key Results
- Bengali inputs generate significantly more tokens per instance and per word than English, with excessive tokenization correlating with lower model accuracy
- All evaluated models show consistent performance gaps for Bengali compared to English, with larger gaps for smaller models and certain families like Mistral
- Models with explicit multilingual pretraining (like DeepSeek) demonstrate greater robustness to Bengali tasks with smaller performance gaps
- English tokens have higher average bytes per token, implying greater resource demands compared to Bengali

## Why This Works (Mechanism)

### Mechanism 1
Excessive tokenization of Bengali text correlates with lower model accuracy on downstream tasks. Bengali's alphasyllabary script produces fragmented subword tokens under BPE/WordPiece schemes, increasing sequence length and introducing noise that degrades attention patterns and representation learning. Core assumption: Token fragmentation directly harms semantic coherence in hidden states (not conclusively proven; correlation observed). Evidence: [abstract] "Tokenization analysis reveals that Bengali inputs generate significantly more tokens per instance and per word than English, and excessive tokenization correlates with lower accuracy"; [section 3.4] Figure 7a shows negative correlation between ATPR and LLM-Judge scores; [corpus] "Evaluating Subword Tokenization Techniques for Bengali" (FMR 0.52) confirms standard tokenizers underperform on Bengali.

### Mechanism 2
Models with explicit multilingual pretraining (Bengali tokens in corpus) show smaller performance gaps between English and Bengali. Pretraining exposure creates language-specific subword embeddings and attention patterns, reducing out-of-distribution token sequences at inference. Core assumption: Documented multilingual support implies sufficient Bengali representation (exact token counts not disclosed for most models). Evidence: [abstract] "Models like DeepSeek demonstrate greater robustness" with lower standard deviation across languages; [section 3.3] Table 3 indicates Qwen and DeepSeek have Bengali in pretraining; [corpus] Weak direct evidence on pretraining composition.

### Mechanism 3
Larger model scale partially compensates for tokenization inefficiency and limited language exposure. Greater parameter count provides capacity to learn noisy inter-token relationships from fragmented sequences, acting as a buffer against representation poverty. Core assumption: Scale benefits generalize across architectural families. Evidence: [section 3.3] Figure 3b shows smaller models (3B-8B) have larger accuracy drops in Bengali; 70B models show smaller gaps; [corpus] No direct corpus evidence on scale Ã— language interaction.

## Foundational Learning

- Concept: Subword tokenization (BPE/WordPiece/Unigram)
  - Why needed here: The paper's central finding hinges on how these algorithms fragment Bengali script differently than Latin script. Without understanding merge operations and vocabulary construction, the tokenization metrics (ATPR, ATPW) lack context.
  - Quick check question: Given the vocabulary ["ing", "go", "##ing", "going"], how would "going" tokenize under BPE vs. WordPiece?

- Concept: Cross-lingual transfer in transformers
  - Why needed here: The performance gap analysis assumes some shared representation space between English and Bengali; understanding how multilingual models align languages informs why DeepSeek shows robustness.
  - Quick check question: If a model has never seen Bengali tokens during pretraining, what mechanisms (if any) allow it to process Bengali input?

- Concept: Evaluation metrics for generative models (accuracy vs. semantic similarity)
  - Why needed here: The paper uses Accuracy, RER, RAR, and LLM-Judge; each captures different failure modes. RER measures format adherence, while LLM-Judge measures semantic equivalence.
  - Quick check question: A model outputs "The answer is affirmative" when the ground truth is "yes." Which metric penalizes this, and which might accept it?

## Architecture Onboarding

- Component map: Tokenization layer -> Embedding layer -> Transformer backbone -> Output head

- Critical path:
  1. Translate English benchmarks -> Bengali via GPT-4o-mini (Section 2.2)
  2. Tokenize with each model's tokenizer -> compute ATPR, ATPW, ABPT (Section 3.4)
  3. Run zero-shot inference -> collect responses
  4. Evaluate with Accuracy, RER/RAR, LLM-Judge -> compare EN vs. BN gaps (Section 3.2)

- Design tradeoffs:
  - Translation-based benchmarks vs. native Bengali: Faster creation but risks translation artifacts; paper acknowledges this limitation (Section 5)
  - LLM-Judge vs. human evaluation: Scalable but introduces judge bias; paper flags this as a limitation
  - Zero-shot vs. few-shot evaluation: Paper uses zero-shot for consistency; few-shot might reduce RER

- Failure signatures:
  - High RER (>0.5) with moderate LLM-Judge: Model understands content but ignores format instructions (common in Mistral-7B for Bengali per Table 5)
  - Low accuracy + low RER: Model follows format but answers incorrectly (representation/understanding failure)
  - Large ATPR gap (BN >> EN) with small ABPT: Tokenizer fragmentation without byte-efficiency gains

- First 3 experiments:
  1. Reproduce tokenization analysis on a single dataset (e.g., BoolQ): Compute ATPR/ATPW for English vs. Bengali using Qwen-2.5-7B's tokenizer to validate fragmentation pattern.
  2. Ablate model scale within one family: Compare LLaMA-3.1-8B vs. LLaMA-3.1-70B on Bengali ARC-E to confirm scale compensation effect.
  3. Test tokenizer swap: Run DeepSeek-R1-14B with LLaMA-3.1 tokenizer (if feasible) to isolate tokenizer vs. architecture contribution to robustness.

## Open Questions the Paper Calls Out

- How does model performance differ when evaluated on manually validated or natively curated Bengali datasets versus the machine-translated benchmarks presented here? The authors acknowledge their translated datasets "were not manually validated" and list "manual dataset validation" as a priority for future efforts in the Conclusion.

- Can flexible, semantically-aware evaluation metrics effectively mitigate the false negatives inherent in exact-match accuracy for generative Bengali tasks? The Limitations section notes that "strict rules may penalize valid answers that do not conform to a narrow format" and calls for "more flexible evaluation criteria."

- Does optimizing tokenization specifically for Bengali script to reduce "tokens per row" causally improve downstream accuracy for smaller models? The paper identifies an "inverse relationship between tokenization efficiency and LLM accuracy" and notes that excessive tokens introduce noise, yet it does not test a fix.

## Limitations
- Reliance on GPT-4o-mini for translating English benchmarks to Bengali introduces uncertainty about translation quality and potential artifacts affecting evaluation
- Use of LLM-Judge as a semantic similarity metric, while scalable, introduces potential bias without quantified correlation to human evaluation
- Claims about pretraining data composition and its impact on Bengali performance are largely inferential without direct corpus analysis

## Confidence

**High Confidence** - The observed performance gaps between English and Bengali across all models are well-supported by the data. The tokenization analysis showing higher ATPR/ATPW for Bengali is empirically robust. The correlation between model scale and reduced performance gaps is consistently observed across model families.

**Medium Confidence** - The explanations for performance gaps (tokenization inefficiency, limited pretraining exposure, scale compensation) are mechanistically plausible but not conclusively proven. The DeepSeek robustness observation is supported by data but lacks rigorous ablation to isolate contributing factors.

**Low Confidence** - The specific mechanisms linking tokenization fragmentation to accuracy degradation remain theoretical. Claims about pretraining data composition and its impact on Bengali performance are largely inferential without direct corpus analysis.

## Next Checks

1. **Tokenizer Ablation Study** - Run the same Bengali benchmarks using a tokenizer specifically optimized for Bengali grapheme clusters (e.g., from the "Evaluating Subword Tokenization Techniques for Bengali" corpus paper). Compare ATPR, ATPW, and downstream accuracy to test whether tokenization inefficiency is the primary driver of performance gaps.

2. **Human Evaluation Correlation** - Select 100 random Bengali model outputs across 3 model families (high/medium/low performers) and have native Bengali speakers evaluate semantic correctness and format adherence. Compute correlation coefficients with LLM-Judge scores to validate the automated evaluation methodology.

3. **Translation Quality Analysis** - Identify any existing native Bengali versions of the benchmark datasets. Translate a subset back to English and compare with original English versions using semantic similarity metrics. Quantify potential translation drift that could bias results.