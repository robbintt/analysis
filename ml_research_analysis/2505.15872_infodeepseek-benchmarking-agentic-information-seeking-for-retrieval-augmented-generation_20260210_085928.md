---
ver: rpa2
title: 'InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented
  Generation'
arxiv_id: '2505.15872'
source_url: https://arxiv.org/abs/2505.15872
tags:
- information
- answer
- question
- search
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfoDeepSeek introduces a new benchmark for evaluating agentic
  information seeking in real-world, dynamic web environments. It addresses the limitations
  of existing RAG benchmarks, which are confined to static settings with limited corpora
  and simple queries.
---

# InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2505.15872
- Source URL: https://arxiv.org/abs/2505.15872
- Reference count: 40
- Key outcome: Introduces a new benchmark for evaluating agentic information seeking in real-world, dynamic web environments, addressing limitations of existing RAG benchmarks.

## Executive Summary
InfoDeepSeek presents a novel benchmark for evaluating agentic information seeking in Retrieval-Augmented Generation systems. The benchmark addresses limitations of existing RAG benchmarks by focusing on real-world, dynamic web environments with challenging queries requiring multi-hop reasoning and long-tail knowledge. The paper introduces a comprehensive evaluation framework with fine-grained metrics and demonstrates significant performance gaps across different models and search engines, highlighting the challenges of agentic information seeking and offering actionable insights for future research.

## Method Summary
The benchmark uses 245 manually curated questions designed to be determinate, difficult, and diverse across domains, languages, and attributes. A three-stage agentic RAG pipeline is implemented: Retrieval (LLM core + tool library), Augmentation (evidence filtration), and Generation (final synthesis). The framework includes multi-turn planning/reflection with search tools and memory. Evaluation uses LLM judges with separate prompts for false-premise questions and human verification. Maximum retrieval steps are set to 5 by default.

## Key Results
- Significant performance gaps exist across different LLMs and search engines
- Retrieval interference affects 40-80% of queries, where web retrieval degrades performance
- Google search engine consistently outperforms DuckDuckGo and Bing across all models
- Agentic information seeking remains challenging, with accuracy rates ranging from 10-35% depending on configuration

## Why This Works (Mechanism)

### Mechanism 1: Difficulty-Induced Agentic Activation
The dataset construction explicitly filters out queries solvable by single-turn search or parametric knowledge, forcing agents to utilize iterative planning and multi-hop reasoning. This activates latent planning capabilities only when simple retrieval heuristics fail.

### Mechanism 2: Retrieval Interference
Accessing external web content can degrade performance when noisy or conflicting search results override the model's correct parametric knowledge. Low-quality or tangentially relevant results appear to dilute the model's internal confidence, leading to hallucination or error.

### Mechanism 3: Search Engine as a Capability Multiplier
The quality of the search engine API acts as a hard ceiling on agent performance, often outweighing minor differences in model reasoning capability. Better retrieval reduces the reasoning load required to find and synthesize evidence.

## Foundational Learning

- **Agentic RAG Loop (Plan-Act-Reflect)**: Why needed: Unlike static RAG, this system requires the model to act as an autonomous agent, deciding when to search and what to search for next based on previous results. Quick check: Can you trace how an agent updates its plan if the first search query returns irrelevant results?

- **Multi-hop Reasoning**: Why needed: The benchmark is constructed of "multi-hop" queries where the answer requires synthesizing information from multiple distinct documents or steps. Quick check: If document A contains "X is the CEO" and document B contains "The CEO lives in Y," can the system link these to answer "Where does X live?"

- **Evidence Filtration (Augmentation)**: Why needed: The "Augmentation Stage" is critical because web content is noisy. The agent must distinguish between relevant evidence and "distracting information." Quick check: How does the system handle a webpage that contains the right keywords but false or outdated facts?

## Architecture Onboarding

- **Component map**: Retrieval Stage (LLM + Tool Library + Memory) -> Augmentation Stage (Selection function) -> Generation Stage (LLM synthesis)

- **Critical path**:
  1. Query Planning: Agent analyzes query and generates initial search keywords
  2. Tool Execution: Search engine returns snippets/links; Browser extracts content
  3. Reflection: Agent reviews Memory; decides to stop, search again, or browse specific links
  4. Evidence Selection: Agent filters collected content into top-k relevant pieces
  5. Answer Generation: Final response produced based only on selected evidence

- **Design tradeoffs**:
  - DuckDuckGo is free/open but yields ~50% lower accuracy than Google
  - Increasing max steps T improves accuracy but drastically increases token cost and latency
  - Larger k (top-k evidence) adds context but risks introducing noise that degrades answer quality

- **Failure signatures**:
  - High IC, Low ACC: Poor filtering or "Lost in the Middle" phenomenon
  - Retrieval Interference: Model answers correctly with 0-shot but fails with Search
  - Redundant Looping: Agent generates same search query repeatedly

- **First 3 experiments**:
  1. Baseline Sanity Check: Run 245 queries with GPT-4o and DuckDuckGo
  2. Search Engine Ablation: Keep model fixed, swap search engine (DuckDuckGo -> Google)
  3. Interference Analysis: Compare "Parametric Only" vs. "Agentic" mode on subset

## Open Questions the Paper Calls Out

### Open Question 1
How can agent architectures be modified to implement "knowledge consistency checks" or confidence scoring to mitigate retrieval interference, where irrelevant external information degrades the model's correct internal knowledge? The paper quantifies interference rates (40-80%) but does not implement specific architectural solutions to preserve internal knowledge confidence.

### Open Question 2
Can an automated data generation pipeline be developed to construct challenging queries that satisfy the criteria of determinacy and difficulty without the high cost of manual annotation and multi-stage verification? The current methodology relies on complex manual pipeline limiting dataset scale and speed of updates.

### Open Question 3
What specific training or prompting strategies can improve the Effective Evidence Utilization (EEU) of agents to better filter long-tail noise and distracting information? While the paper identifies that models struggle to extract useful evidence from vast observations, it does not propose solutions for the augmentation stage's failure to prioritize top-k evidence effectively.

## Limitations
- Benchmark relies on manual curation introducing potential bias in query selection and difficulty filtering
- Performance heavily influenced by API access quality of search engines rather than purely measuring agentic reasoning
- Does not provide detailed error analysis for why specific queries fail

## Confidence

- **High Confidence**: Existence of significant performance gaps across models and search engines; observation that retrieval interference affects 40-80% of queries; general finding that agentic information seeking remains challenging

- **Medium Confidence**: Specific accuracy percentages reported; relative ranking of models across different search engines; effectiveness of three-stage pipeline architecture

- **Low Confidence**: Generalizability of 245-query benchmark to all real-world scenarios; absolute performance differences between models when controlled for search engine quality; long-term stability of LLM judges' evaluation consistency

## Next Checks

1. **Cross-Dataset Generalization**: Test the agentic pipeline on a separate, independently curated information-seeking dataset to verify whether performance patterns hold beyond InfoDeepSeek benchmark.

2. **Controlled Search Engine Experiment**: Implement a fair comparison using the same search engine (Google) for all models while controlling for API costs and rate limits to isolate agent capabilities from retrieval quality differences.

3. **Error Analysis Framework**: Conduct detailed failure mode analysis by categorizing query failures into planning errors, retrieval failures, evidence selection mistakes, and generation hallucinations to identify specific architectural weaknesses.