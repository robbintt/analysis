---
ver: rpa2
title: Monte Carlo Sampling for Analyzing In-Context Examples
arxiv_id: '2503.22002'
source_url: https://arxiv.org/abs/2503.22002
tags:
- performance
- examples
- in-context
- each
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a Monte Carlo sampling method to analyze the
  impact of the number of in-context examples while controlling for effects from example
  selection and ordering. They find that performance plateaus at previously suggested
  numbers of examples (k=4, k=8) are not consistently observable across different
  permutations, indicating that guidance on the number of examples may not generalize
  across different example sets and orderings.
---

# Monte Carlo Sampling for Analyzing In-Context Examples

## Quick Facts
- arXiv ID: 2503.22002
- Source URL: https://arxiv.org/abs/2503.22002
- Reference count: 34
- Authors: Stephanie Schoch; Yangfeng Ji
- Primary result: Performance plateaus at k=4 and k=8 are not consistently observable across permutations, and Monte Carlo example selection trades robustness for performance

## Executive Summary
This paper introduces a Monte Carlo sampling method to analyze in-context learning (ICL) performance while controlling for confounding effects from example selection and ordering. The authors find that previously observed performance plateaus at k=4 and k=8 are artifacts of averaging across permutations, as individual permutations show highly variable performance up to k=20. They also discover that one-shot performance can vary significantly depending on the selected example, sometimes outperforming and sometimes underperforming zero-shot settings. When using Monte Carlo sampling for example selection to increase robustness, they observe more stable performance but unexpectedly lower performance than random selection, suggesting a tradeoff between performance and robustness.

## Method Summary
The method uses Monte Carlo sampling to analyze the impact of the number of in-context examples (k) while explicitly accounting for effects from example selection and ordering. For each dataset, they sample a subset S_K of 20 examples from the training set, then generate P=20 random permutations of this subset. For each permutation, they compute model accuracy at each incremental k from 0 to K=20. This process is repeated for 5 trials with different sampled subsets. Performance is aggregated by averaging across permutations and trials. For example selection analysis, they compute Z-scores for each example across permutations and select top/bottom 6 examples based on Z>1 or Z<-1 thresholds.

## Key Results
- Performance plateaus at k=4 and k=8 are not consistently observable within individual permutations
- Individual permutations exhibit highly variable performance up to k=20
- One-shot performance can vary significantly, sometimes outperforming and sometimes underperforming zero-shot settings
- Monte Carlo-based example selection improves robustness but results in lower average performance than random selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Monte Carlo sampling with permutation averaging reduces confounding from ordering and example selection when analyzing k-shot performance.
- Mechanism: By sampling a fixed subset S_K and computing performance at each incremental k across multiple random permutations π_p, then averaging, the method estimates E_π[V_M(S_k^π)]—the expected performance for k examples marginalized over orderings. This eliminates the need to exhaustively evaluate nC_k × k! combinations.
- Core assumption: Permutations are uniformly distributed and P=20 permutations provides sufficient coverage of ordering space for stable estimates.
- Evidence anchors:
  - [abstract] "Monte Carlo sampling-based method to study the impact of number of examples while explicitly accounting for effects from order and selected examples"
  - [section 3] "the use of permutations reduces the influence of ordering, and the averaging across multiple trials reduces the influence of selected examples"
  - [corpus] No direct corpus evidence on this specific ICL application of MC sampling; related corpus papers address MC for Bayesian inference (arXiv:2502.06601) but not ICL.
- Break condition: If permutation coverage is insufficient for high-variance datasets, estimates may remain noisy; authors use P=20 which may not converge for all model/task pairs.

### Mechanism 2
- Claim: Averaged performance curves mask high variance in individual permutations, leading to misleading "plateau" guidance.
- Mechanism: When plotting average accuracy across permutations, erratic individual trajectories smooth into apparent plateaus. However, individual permutations show continued volatility through k=20, meaning optimal k for any specific example set and ordering varies widely.
- Core assumption: The variance observed in individual permutations reflects real sensitivity rather than measurement noise.
- Evidence anchors:
  - [abstract] "while performance plateaus appear when averaging across permutations, individual permutations exhibit highly variable performance"
  - [section 5.1] "performance plateaus by k=4 and k=8 examples are not observable within individual permutations...we continue to observe erratic performance changes up through k=20"
  - [corpus] Problem-Solving Logic Guided Curriculum ICL (arXiv:2502.15401) notes ordering sensitivity but doesn't address this specific averaging artifact.
- Break condition: If variance is primarily from evaluation set sampling (n=256), not ICL sensitivity, the conclusion overstates brittleness.

### Mechanism 3
- Claim: Monte Carlo-derived Z-scores for example selection increase robustness but paradoxically reduce peak performance vs. random selection.
- Mechanism: Examples with consistently high (or low) average accuracy across permutations exhibit lower variance across orderings and k values. However, selecting these "stable" examples performs worse than random—a potential robustness-performance tradeoff.
- Core assumption: High Z-score examples are genuinely more robust rather than artifacts of limited sampling.
- Evidence anchors:
  - [abstract] "while performance is robust to ordering and number of examples, there is an unexpected performance degradation compared to random sampling"
  - [section 5.2] "the random selection exhibited the highest performance for k > 1...raises the question of whether there is an existing performance vs. robustness tradeoff"
  - [corpus] No corpus papers address this specific tradeoff in ICL example selection.
- Break condition: If random selection benefits from diversity that MC selection filters out, the tradeoff is real; if MC selection simply identifies mediocre examples, the mechanism is selection failure, not tradeoff.

## Foundational Learning

- Concept: **In-Context Learning (ICL) sensitivity factors**
  - Why needed here: The paper's entire contribution depends on understanding that ICL is brittle to three factors: which examples (selection), their order (permutation), and how many (k). Without this background, the MC sampling motivation is unclear.
  - Quick check question: Given 4 candidate examples for a 2-shot prompt, how many distinct ordered prompts are possible?

- Concept: **Monte Carlo estimation of expected values**
  - Why needed here: The method approximates E_π[V_M(S_k^π)] via sampling rather than exhaustive enumeration. Understanding MC as variance-reduction via averaging is essential.
  - Quick check question: Why does MC sampling with P=20 permutations provide a better estimate of expected performance than a single random ordering?

- Concept: **Performance-robustness tradeoffs in selection**
  - Why needed here: The negative result (robust but lower-performing selection) is only interpretable if you understand that selection criteria can optimize different objectives.
  - Quick check question: In what deployment scenarios would you prefer a lower-performing but more robust example selection strategy?

## Architecture Onboarding

- Component map:
  Training Data (D_trn) → Subset Sampler (S_K, 5 trials) → Permutation Generator (π_p, P=20)
                                                                    ↓
                            Performance Aggregator ← Model Inference Loop (k=0 to K)
                                    ↓
                         Average Performance µ_k per k (output)

  Extension for selection: Z-score Calculator → High/Low Set Identification → Re-evaluation

- Critical path:
  1. Subset sampling (determines candidate pool)
  2. Permutation generation (controls ordering coverage)
  3. Incremental inference at each k (generates raw measurements)
  4. Aggregation across permutations and trials (produces final estimates)

- Design tradeoffs:
  - P (permutations) vs. compute: P=20 chosen for tractability; higher P reduces variance but increases cost linearly
  - K (max examples) vs. context limits: K=20 fits most context windows but may not capture long-horizon effects
  - Evaluation set size (256) vs. measurement noise: Smaller sets increase inference speed but may introduce sampling variance

- Failure signatures:
  - High standard deviation across trials → insufficient permutation coverage or inherently unstable task
  - One-shot consistently below zero-shot → potentially harmful example characteristics (not investigated in paper)
  - MC-selected examples underperforming random → selection criterion captures robustness at expense of peak performance

- First 3 experiments:
  1. Replicate the MC sampling procedure on a single dataset (e.g., SST-2) with a smaller model, plotting both individual permutation curves and averages to verify the averaging-masks-variance finding.
  2. Vary P (5, 10, 20, 50) on one model/task pair to assess convergence of performance estimates and determine minimum P for stable results.
  3. Implement the Z-score selection method on a held-out portion of training data, comparing random vs. high-Z vs. low-Z subsets to reproduce the robustness-performance tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific qualities of in-context examples cause significant performance degradation in one-shot settings compared to zero-shot, and do these qualities persist in k-shot settings?
- Basis in paper: [explicit] The authors explicitly state, "This contradiction raises the question of what qualities of in-context examples can lead to such significant performance degradation in one-shot settings, and whether these have any impact when used within a k-shot setting."
- Why unresolved: The paper identifies that specific examples cause performance to drop below zero-shot baselines, but does not conduct a qualitative analysis (e.g., linguistic features, noise, label correctness) to characterize these examples.
- What evidence would resolve it: A detailed error analysis or feature attribution study correlating specific example attributes (length, sentiment, ambiguity) with the magnitude of performance drop relative to the zero-shot baseline.

### Open Question 2
- Question: Can example selection methods be designed to simultaneously maximize performance and robustness, overcoming the tradeoff discovered in Monte Carlo-based selection?
- Basis in paper: [explicit] The authors ask "whether there is an existing performance vs. robustness tradeoff, and... how methods can identify examples that possess these qualities that also lead to higher performance gains."
- Why unresolved: The authors found that selecting examples using Monte Carlo sampling increased robustness (low variance) but resulted in lower average accuracy compared to random sampling. It is unclear if this tradeoff is inherent to the method or a solvable optimization problem.
- What evidence would resolve it: The development of a selection algorithm that utilizes marginal contributions or valuation scores but retains or exceeds the accuracy of random baselines while maintaining low variance across permutations.

### Open Question 3
- Question: Do the observed phenomena of erratic performance plateaus and the failure of valuation-based selection persist in models with parameters significantly larger than 13B?
- Basis in paper: [inferred] The authors acknowledge a limitation that "results are reported on models up to 13B parameters due to constraints posed by our available computational resources," while hypothesizing that results should generalize.
- Why unresolved: In-context learning dynamics often change qualitatively with scale (emergent abilities). It is possible that larger models (e.g., 70B+) exhibit more consistent plateaus or respond differently to valuation-based selection, resolving the brittleness observed in smaller models.
- What evidence would resolve it: Replicating the Monte Carlo sampling experiments (specifically the variance across permutations and the selection ablation) on models like Llama-2-70B or Llama-3 to verify if the variance decreases or if the selection method becomes effective.

## Limitations

- Evaluation Set Subsampling: The study subsamples 256 validation instances per dataset but doesn't report whether this subsampling was stratified or random, nor how much variance this introduces.
- Permutation Sufficiency: With P=20 permutations per trial, the authors acknowledge this is a trade-off with computational cost, but the convergence properties are not analyzed.
- Prompt Template Specificity: The study uses "default prompts" from the evaluation harness without specifying the exact format, limiting reproducibility.

## Confidence

**High Confidence**:
- Monte Carlo sampling effectively reduces confounding from ordering and example selection when analyzing k-shot performance
- Individual permutations show highly variable performance that averaging masks
- One-shot performance can vary significantly depending on selected example

**Medium Confidence**:
- Performance plateaus at k=4 and k=8 are not consistently observable within individual permutations
- Monte Carlo-based example selection improves robustness to ordering and number of examples
- There exists a performance-robustness tradeoff in example selection

**Low Confidence**:
- The specific numerical thresholds (Z>1, Z<-1 for selecting 6 examples) are optimal
- P=20 permutations provides sufficient coverage for all datasets and models
- The tradeoff between performance and robustness generalizes beyond the studied datasets

## Next Checks

1. **Permutation Coverage Analysis**: Run the MC sampling procedure with varying numbers of permutations (P=5, 10, 20, 50) on a single dataset/model pair. Plot the coefficient of variation in performance estimates across trials to identify the point of diminishing returns and determine minimum P for stable results.

2. **Validation Set Stability Test**: Subsample the validation set multiple times (e.g., 10 different 256-instance subsets) for one dataset and recompute all reported metrics. This quantifies how much the findings depend on the specific validation subsample chosen.

3. **Prompt Template Sensitivity**: Implement two different prompt templates (e.g., different label formats or instruction phrasings) and rerun the MC analysis on one dataset. Compare whether the same patterns of sensitivity to k, ordering, and example selection emerge, or if effects are prompt-dependent.