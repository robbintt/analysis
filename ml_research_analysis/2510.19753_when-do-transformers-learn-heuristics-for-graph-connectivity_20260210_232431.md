---
ver: rpa2
title: When Do Transformers Learn Heuristics for Graph Connectivity?
arxiv_id: '2510.19753'
source_url: https://arxiv.org/abs/2510.19753
tags:
- graphs
- layer
- training
- capacity
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transformers often fail to learn robust algorithms, instead overfitting
  brittle heuristics that don't generalize. This paper explains why, using graph connectivity
  as a testbed.
---

# When Do Transformers Learn Heuristics for Graph Connectivity?

## Quick Facts
- **arXiv ID:** 2510.19753
- **Source URL:** https://arxiv.org/abs/2510.19753
- **Reference count:** 40
- **Primary result:** L-layer Transformers have exact capacity 3^L for graph connectivity; restricting training to within-capacity graphs suppresses heuristics and improves OOD generalization.

## Executive Summary
Transformers often fail to learn robust algorithms, instead overfitting brittle heuristics that don't generalize. This paper explains why, using graph connectivity as a testbed. It introduces the disentangled Transformer, a simplified architecture that makes analysis tractable, and proves a precise capacity bound: an L-layer model can only solve graphs with diameter ≤ 3^L. Training dynamics reveal a sharp dichotomy—within-capacity graphs promote an algorithmic channel (matrix powering), while beyond-capacity graphs drive a heuristic channel (degree counting). Experiments confirm this exact threshold and show that restricting training data to within-capacity graphs suppresses heuristics and improves generalization. This data lever strategy even transfers to standard Transformers. The key takeaway: model capacity and training data distribution jointly determine whether Transformers learn algorithms or shortcuts, and simple data filtering can steer them toward generalizable solutions.

## Method Summary
The paper studies graph connectivity prediction using two Transformer variants. The disentangled Transformer uses a simplified architecture where each layer appends attention outputs to the hidden state, enabling tractable analysis. Standard Transformers use pre-norm, ReLU activation, and no positional encoding. Training uses Erdős-Rényi graphs, with key experiments filtering data to have diameter ≤ 3^L ("data lever"). The method tracks I-channel vs J-channel energy shares via Frobenius norm projection and evaluates OOD generalization on 2Chain and 2Clique distributions.

## Key Results
- Proved that L-layer disentangled Transformers have exact capacity 3^L for graph connectivity
- Demonstrated that training data distribution determines whether models learn algorithms (I-channel) or heuristics (J-channel)
- Showed that restricting training to within-capacity graphs suppresses heuristics and improves OOD generalization
- Transferred the data lever strategy to standard Transformers, achieving similar generalization improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: An L-layer disentangled Transformer can solve graph connectivity exactly for graphs with diameter ≤ 3^L, and this bound is tight.
- Mechanism: The architecture implements matrix powering via repeated attention-based composition. Each layer triples the reachable distance: layer ℓ can propagate information across paths of length up to 3^ℓ hops. The readout sums powers of the adjacency matrix A^k, which computes reachability for paths within that radius.
- Core assumption: Non-negative weights (W^ℓ ≥ 0) and sufficient nodes (n ≥ (7/3)·3^L + 2).
- Evidence anchors:
  - [abstract] "prove that an L-layer model has capacity to solve for graphs with diameters up to exactly 3^L, implementing an algorithm equivalent to computing powers of the adjacency matrix"
  - [section 4.2] Theorem 4.4 proves the upper bound; Lemma B.1 and B.2 construct counterexamples exceeding capacity.
  - [corpus] Weak corpus support—no related papers validate this specific 3^L scaling law.
- Break condition: If weights allow negative entries, or if graph diameter exceeds 3^L, the mechanism fails to provide correct connectivity.

### Mechanism 2
- Claim: Learned weights decompose into an algorithmic channel (A_ℓ ⊗ I_n) and a heuristic channel (B_ℓ ⊗ J_n).
- Mechanism: Under layerwise permutation equivariance, each weight block W^ℓ must lie in span{I_n, J_n}. The I_n-channel implements local multi-hop composition (matrix powers). The J_n-channel broadcasts degree statistics: J_n x = (1^⊤x)1 aggregates node degrees, producing global shortcuts based on degree-counting rather than actual connectivity.
- Core assumption: Layerwise attention equivariance (Attn(Ph(I⊗P^⊤); W) = P·Attn(h; W)·(I⊗P^⊤)).
- Evidence anchors:
  - [section 4.3] Theorem 4.6 derives the decomposition W_ℓ = A_ℓ ⊗ I_n + B_ℓ ⊗ J_n from equivariance.
  - [section 5.1] Figure 3 shows trained weights converge to this decomposition empirically.
  - [corpus] No corpus papers confirm this specific channel decomposition.
- Break condition: If the model lacks equivariance (e.g., asymmetric initialization, irregular architectures), the decomposition may not hold.

### Mechanism 3
- Claim: Training data distribution determines which channel dominates: within-capacity graphs promote the algorithmic channel; beyond-capacity graphs promote the heuristic channel.
- Mechanism: Population gradients on the J-channel balance two forces: (1) penalty from false positives on cross-component pairs in disconnected graphs, and (2) reward from correctly identifying connected pairs. When training contains many beyond-capacity connected graphs, the reward dominates, pushing B_ℓ > 0 (heuristic wins). When training is restricted to within-capacity disconnected graphs, the penalty dominates, forcing B_ℓ → 0 (algorithm wins).
- Core assumption: Sufficient fraction of within-capacity disconnected graphs to generate cross-component penalties; Bernoulli cross-entropy loss.
- Evidence anchors:
  - [section 4.3] Equation (5) formalizes the gradient balance; Theorem C.5 proves B_ℓ = 0 at KKT points under cross-component penalty dominance.
  - [section 5.2] Figure 4 shows restricting to within-capacity data pushes A-channel share to ~100%.
  - [corpus] Related work "Transformers Can Learn Connectivity in Some Graphs but Not Others" corroborates difficulty scaling, but doesn't explain the gradient mechanism.
- Break condition: If training data contains too few disconnected within-capacity graphs, or too many connected beyond-capacity graphs, the heuristic channel dominates and OOD generalization fails.

## Foundational Learning

- **Graph Connectivity via Matrix Powers**:
  - Why needed here: The entire theoretical framework rests on computing transitive closure via A^k where k ≥ diameter(G). Without understanding that reachability = non-zero entries in A^n (with self-loops), the capacity bound and algorithmic channel are opaque.
  - Quick check question: Given adjacency matrix A with self-loops, what does (A^3)_ij > 0 tell you about nodes i and j?

- **Permutation Equivariance in Neural Networks**:
  - Why needed here: The decomposition into I_n and J_n channels emerges from demanding equivariance under node relabeling. Understanding this symmetry constraint is essential to grasp why weights take this specific form.
  - Quick check question: If you permute node indices via matrix P, what must hold for a function f to be equivariant: f(PAP^⊤) = ?

- **Gradient-Based Channel Selection**:
  - Why needed here: The "data lever" works because gradients differentially reinforce channels based on data distribution. Understanding how loss gradients on cross-component vs. within-component pairs steer learning is critical.
  - Quick check question: On a disconnected graph, why does predicting a connection between different components produce a gradient signal that penalizes the J-channel?

## Architecture Onboarding

- **Component map**: A+I_n -> Attn(h;W) -> h_ℓ -> W_O -> R
- **Critical path**:
  1. Generate training graphs from ER(n, p) distribution
  2. Filter to within-capacity: diam(G) ≤ 3^L (the "data lever")
  3. Train with cross-entropy loss on connectivity prediction
  4. Monitor I-channel vs. J-channel energy share via projection
  5. Evaluate OOD on 2Chain and 2Clique distributions
- **Design tradeoffs**:
  - Depth L vs. capacity: Each layer triples reachable distance (3^L), but increases hidden dimension exponentially (2^(ℓ+1)·n)
  - Disentangled vs. standard: Disentangled enables tractable analysis; standard requires more data but theory transfers (Figure 7)
  - Filtering strictness: diam(G) ≤ 3^L gives best generalization; too strict (≤ 2^L) hurts; too loose promotes heuristics
- **Failure signatures**:
  - High training accuracy, near-zero OOD accuracy → model learned degree heuristic
  - J-channel energy share > 50% → heuristic dominated
  - Max perfect path length plateauing below 3^L → capacity not reached, check weight initialization/learning rate
- **First 3 experiments**:
  1. Replicate Figure 2: Train 2-layer disentangled transformer on ER(n=24), plot accuracy vs. path length. Expect sharp drop at 3^2 = 9.
  2. Replicate Figure 4: Train 1-layer model on ER(n=8) with/without diameter filtering. Track I/J channel shares over training.
  3. Transfer test (Figure 7): Train 2-layer standard Transformer on filtered vs. unfiltered ER(n=20), evaluate on 2Chain(n=20, k=10). Expect filtered model generalizes, untrained fails.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "data lever" strategy (restricting training data to within-capacity instances) generalize to other algorithmic reasoning tasks beyond graph connectivity?
- Basis in paper: [explicit] The paper introduces connectivity as a "testbed" to explain a broader phenomenon where models fail to learn algorithms, implying the ultimate goal is to apply these insights to general algorithmic reasoning.
- Why unresolved: The empirical validation of the data lever is confined to the specific task of graph connectivity.
- What evidence would resolve it: Applying the capacity-based data filtering method to other tasks with formal algorithmic solutions (e.g., arithmetic, sorting) and observing similar suppression of heuristics.

### Open Question 2
- Question: Can the exact capacity bound ($3^L$) and the algorithmic/heuristic channel decomposition be rigorously proven for standard Transformers without relying on the simplifying assumptions of the disentangled architecture?
- Basis in paper: [inferred] The theoretical proofs rely on a simplified "disentangled Transformer" and non-negative weights, though experiments suggest the phenomena transfer to standard architectures.
- Why unresolved: The theoretical gap between the tractable disentangled model and the complex dynamics of standard dense Transformers remains unbridged by formal proof.
- What evidence would resolve it: A formal proof extending the $3^L$ capacity theorem and channel decomposition to standard multi-head attention mechanisms.

### Open Question 3
- Question: How does the algorithm-heuristic dichotomy change when applied to directed graphs or graph problems with higher complexity (outside of deterministic logspace)?
- Basis in paper: [inferred] The paper notes connectivity is a "natural starting point" because it sits low in the complexity hierarchy (class $L$), but leaves open whether these dynamics persist for harder problems.
- Why unresolved: The theoretical analysis and capacity bounds are derived specifically for undirected connectivity and matrix powering.
- What evidence would resolve it: Analysis of training dynamics on directed acyclic graphs or shortest path problems to determine if the $3^L$ scaling law and degree-counting heuristics persist.

## Limitations

- Theoretical capacity bound (3^L) relies on simplified disentangled architecture assumptions
- Channel decomposition mechanism (A⊗I_n + B⊗J_n) may not hold under all training conditions or for standard Transformers
- Data-lever hypothesis has medium theoretical grounding despite strong empirical support
- Transferability to standard Transformers shows qualitative match but quantitative thresholds may differ

## Confidence

- Theoretical capacity bound (3^L): High
- Channel decomposition (A⊗I_n + B⊗J_n): Medium
- Data-lever mechanism: High (empirical), Medium (theoretical)
- Standard Transformer transferability: Medium
- Cross-layer weight coupling hypothesis: Low (suggested but not tested)

## Next Checks

1. **Gradient channel attribution**: During training, compute and visualize the contribution of I-channel vs J-channel gradients to parameter updates. Verify that cross-component penalties dominate in within-capacity disconnected graphs.

2. **Perturbation sensitivity**: Apply small random perturbations to trained weights and measure changes in I/J channel energy shares. Determine if the channel structure is robust or coincidental.

3. **Transfer learning test**: Train disentangled models on within-capacity data, then fine-tune on standard Transformer architecture. Measure if the algorithmic channel persists and improves OOD generalization compared to training from scratch.