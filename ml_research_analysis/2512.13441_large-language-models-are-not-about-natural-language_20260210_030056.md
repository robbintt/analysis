---
ver: rpa2
title: Large language models are not about natural language
arxiv_id: '2512.13441'
source_url: https://arxiv.org/abs/2512.13441
tags:
- language
- llms
- human
- languages
- impossible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The commentary argues that Large Language Models (LLMs) are fundamentally
  unsuited for understanding human language because they operate as probabilistic
  models requiring vast amounts of training data, while human language is generated
  through a recursive, hierarchical, mind-internal computational system. Unlike human
  children who can develop language with minimal input and distinguish possible from
  impossible languages, LLMs cannot make this distinction and process language through
  linear pattern detection.
---

# Large language models are not about natural language

## Quick Facts
- arXiv ID: 2512.13441
- Source URL: https://arxiv.org/abs/2512.13441
- Authors: Johan J. Bolhuis; Andrea Moro; Stephen Crain; Sandiway Fong
- Reference count: 0
- LLMs operate on linear patterns while human language requires hierarchical computation

## Executive Summary
The commentary argues that Large Language Models fundamentally misunderstand human language by treating it as a statistical pattern recognition problem rather than a hierarchical generative system. While LLMs require vast amounts of data to detect surface-level regularities, human children acquire language from minimal input using an innate computational system that generates hierarchical structures. The authors demonstrate that LLMs cannot distinguish between possible (grammatical) and impossible (ungrammatical) languages, failing to replicate the selective processing observed in human neural systems. This architectural mismatch means LLMs, despite their capabilities, cannot inform our understanding of the human language faculty.

## Method Summary
The paper reviews existing literature comparing LLM behavior with human language acquisition and processing. It cites studies showing LLMs perform equally well on forward and reversed text, and on structured versus randomly shuffled sequences. The authors reference neuroimaging studies demonstrating human selective processing of possible versus impossible languages in Broca's area, contrasting this with LLM uniform processing. The method is primarily a critical synthesis of experimental findings rather than original empirical work.

## Key Results
- LLMs require trillions of tokens while human children learn language from minimal input
- LLMs process reversed English text as effectively as natural text, failing to distinguish possible from impossible languages
- Human language involves hierarchical thought structures, not linear word sequences

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Linear Sequence Analysis
- Claim: LLMs function by detecting statistical regularities in linear word strings, not by building hierarchical structures.
- Mechanism: Models compute transition probabilities across flattened sequences, traceable to Markov's 1913 analysis of letter patterns. The system accumulates statistical associations across trillions of parameters without constructing the generative structures that determine meaning.
- Core assumption: Surface-level statistical patterns are sufficient for producing coherent output.
- Evidence anchors:
  - [abstract] "they are probabilistic models that require a vast amount of data to analyse externalized strings of words"
  - [section] "already in 1913, Markov (1913/2006) published a probabilistic analysis of vowels and consonants confirming a strong bias in favor of an alternating vowel/consonant pattern"
  - [corpus] Weak—neighboring papers address LLM applications, not architectural critiques
- Break condition: When tasks require distinguishing grammatical possibility from impossibility, or when meaning depends on hierarchical constituency.

### Mechanism 2: Hierarchical Generative Computation (Human Faculty)
- Claim: Human language operates via a mind-internal recursive system that generates hierarchical thought structures before externalization.
- Mechanism: A computational system applies recursive operations to build nested structures; these structures, not surface strings, determine meaning. The system grows from minimal input using innate constraints.
- Core assumption: Meaning arises from hierarchical structure, not from word sequences.
- Evidence anchors:
  - [abstract] "human language is underpinned by a mind-internal computational system that recursively generates hierarchical thought structures"
  - [section] "The nature of those structures determines meaning (semantics) (Everaert et al., 2015)"
  - [corpus] Weak—not substantively addressed in corpus neighbors
- Break condition: Assumption: This mechanism would fail if deprived of innate structural priors.

### Mechanism 3: Selective Processing of Possible vs. Impossible Languages
- Claim: Human neural architecture selectively processes only hierarchically-structured languages; impossible (linear-only) languages trigger inhibition in Broca's area.
- Mechanism: Convergent fMRI evidence shows selective activation patterns: Broca's area network activates for hierarchical rules but is inhibited for linear-only rules. LLMs lack this selectivity.
- Core assumption: The brain has domain-specific constraints on what counts as a learnable language.
- Evidence anchors:
  - [section] "showing a selective inhibition of a network involving Broca's area for impossible languages only" (Musso et al., 2003; Tettamanti et al., 2002)
  - [section] "LLMs did not distinguish between normal English and backward reversed English text" (Luo et al., 2024; Ziv et al., 2025)
  - [corpus] Weak—no direct replication or refutation in corpus
- Break condition: When a system lacks architectural priors that constrain learnability to possible languages.

## Foundational Learning

- **Concept: Poverty of the Stimulus**
  - Why needed here: Explains why humans acquire language from minimal input while LLMs require trillions of tokens.
  - Quick check question: Can you explain how children produce sentences they've never heard, without explicit instruction?

- **Concept: Hierarchical vs. Linear Syntax**
  - Why needed here: Core distinction between how LLMs process (linear sequences) and how human language is structured (nested constituents).
  - Quick check question: Why can't "the cat the dog chased barked" be parsed by adjacent-word relationships alone?

- **Concept: Possible vs. Impossible Languages**
  - Why needed here: Experimental paradigm that reveals architectural differences between human and machine language processing.
  - Quick check question: What makes a rule "impossible" for human acquisition, and why would an LLM still learn it?

## Architecture Onboarding

- **Component map:**
  - LLM path: Input tokens → embedding layer → attention over linear positions → probability distribution over next token
  - Human path (proposed): Minimal input → innate computational system → recursive hierarchical structure → externalization as linear string

- **Critical path:** Understanding that LLMs lack the structural priors that constrain human learning to possible languages. The paper argues this is not a gap that more data can fill.

- **Design tradeoffs:**
  - LLM: Architectural simplicity + massive data → broad pattern matching but no structural understanding
  - Human: Innate complexity + minimal data → constrained learnability with generative capacity
  - Energy: ~20W (human brain) vs. 70MW for 100K GPUs (xAI Memphis installation)

- **Failure signatures:**
  - LLM performs equivalently on forward and reversed text (Luo et al., 2024)
  - LLM shows minimal difference between structured and random sequences (Bowers, 2025 critique of Kallini et al.)
  - No selective inhibition for impossible languages

- **First 3 experiments:**
  1. Replicate Luo et al.: Test your model on forward vs. reversed scientific text; compute perplexity ratio.
  2. Hierarchical dependency test: Construct sentences where meaning depends on non-adjacent hierarchical relations; compare model accuracy vs. linear-baseline models.
  3. Impossible language learning: Train on an artificial language with linear-only rules (e.g., fixed-position agreement); compare learning curves to hierarchical-rule languages.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Large Language Models be constrained to selectively reject "impossible" languages (e.g., linear or reversed sequences) in a manner comparable to human neurological inhibition?
- Basis in paper: [inferred] The authors highlight that LLMs process reversed English text as effectively as natural text (Luo et al., 2024; Ziv et al., 2025), failing to replicate the human brain's distinct response to impossible languages observed in Broca's area (Musso et al., 2003).
- Why unresolved: Current architectures appear to lack the specific inductive biases that prevent humans from acquiring non-hierarchical linguistic patterns.
- What evidence would resolve it: The development of a model that fails to learn or categorically rejects impossible languages while retaining fluency in natural languages, potentially verified by mapping model representations against human fMRI data.

### Open Question 2
- Question: Can hierarchical syntactic structures that determine semantic meaning emerge from models trained exclusively on linear "flattened" strings of words?
- Basis in paper: [inferred] The paper posits a fundamental disconnect between LLMs analyzing linear word strings and the human mind's generation of hierarchical thought structures (Everaert et al., 2015).
- Why unresolved: It is unclear whether statistical approximation of word sequences can ever yield the generative, recursive computational system described by the Strong Minimalist Thesis.
- What evidence would resolve it: Formal proofs or causal interventions demonstrating that an LLM constructs and manipulates explicit hierarchical trees (rather than surface statistics) to resolve semantic ambiguities.

### Open Question 3
- Question: Can statistical models replicate the distinct, stage-like developmental trajectories of child language acquisition without innate structural priors?
- Basis in paper: [inferred] The authors note that children produce structures not found in adult input, whereas LLMs "assimilate linguistic patterns" in a way that bears "no semblance" to human developmental stages (Page 2).
- Why unresolved: The "Poverty of the Stimulus" argument suggests certain linguistic knowledge is unlearnable from input alone, yet LLMs rely entirely on massive input volumes.
- What evidence would resolve it: A learning model that, when trained on a child-sized corpus, exhibits the same systematic errors and stage-wise transitions (e.g., over-regularization) observed in human development.

## Limitations

- Architecture specificity gap: Claims about "LLMs" generalize from specific models without testing architectural variations
- Evidence weight imbalance: Heavy reliance on Luo et al. (2024) and Ziv et al. (2025) without independent verification
- Neuroscientific generalization risk: Extending neural activation patterns to computational architecture claims

## Confidence

- **High confidence**: The core distinction between probabilistic linear pattern detection and hierarchical generative computation as different mechanisms for language processing
- **Medium confidence**: That LLMs fail to distinguish possible from impossible languages in the specific experimental paradigms cited
- **Medium confidence**: The poverty of the stimulus argument explaining why human children need less data than LLMs

## Next Checks

1. **Replicate the forward/backward text distinction**: Using multiple LLM architectures (GPT-2, GPT-3 variants, LLaMA variants), evaluate on both normal and character-reversed scientific text from arXiv or PubMed. Compute perplexity ratios and statistical significance across models to test whether the claimed insensitivity is universal or model-dependent.

2. **Test hierarchical vs linear rule learning**: Create artificial languages with hierarchical dependencies (nested structures) versus linear-only rules (adjacent position dependencies). Train identical LLM architectures on both types and measure learning curves, final accuracy, and whether the models can generalize beyond training patterns. Compare against human learning data from Musso et al. (2003).

3. **Impossible language selective inhibition test**: Design impossible languages that violate universal constraints (e.g., center-embedding beyond human processing limits, or non-context-free patterns). Train LLMs on these languages and measure whether they can learn them as effectively as possible languages, contrasting with the selective neural inhibition observed in human fMRI studies.