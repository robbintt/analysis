---
ver: rpa2
title: Investigating Recent Large Language Models for Vietnamese Machine Reading Comprehension
arxiv_id: '2503.18062'
source_url: https://arxiv.org/abs/2503.18062
tags:
- llama
- language
- performance
- gemma
- vietnamese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates fine-tuning recent large language models
  (LLMs) for Vietnamese machine reading comprehension (MRC) tasks. The authors fine-tune
  Llama 3 (8B) and Gemma (7B) models on the ViMMRC dataset using Quantized Low-Rank
  Adaptation (QLoRA), achieving state-of-the-art performance.
---

# Investigating Recent Large Language Models for Vietnamese Machine Reading Comprehension

## Quick Facts
- arXiv ID: 2503.18062
- Source URL: https://arxiv.org/abs/2503.18062
- Reference count: 21
- Primary result: Llama 3 (8B) and Gemma (7B) fine-tuned with QLoRA achieve state-of-the-art accuracy on Vietnamese MRC

## Executive Summary
This study investigates fine-tuning recent large language models for Vietnamese machine reading comprehension using the ViMMRC dataset. The authors demonstrate that Llama 3 (8B) and Gemma (7B) models, when fine-tuned using Quantized Low-Rank Adaptation (QLoRA), achieve state-of-the-art performance despite being significantly smaller than GPT-3 and GPT-3.5. The work shows that parameter-efficient fine-tuning methods enable effective adaptation of multilingual LLMs for low-resource languages like Vietnamese. The fine-tuned models outperform both traditional BERT-based approaches and larger general-purpose models, highlighting the effectiveness of specialized fine-tuning over raw model size.

## Method Summary
The authors fine-tune Llama 3 (8B) and Gemma (7B) base models using QLoRA on the ViMMRC dataset through the Unsloth implementation. Models are trained with 4-bit quantization, low-rank adapters, and a structured chat template format for multiple-choice questions. Training uses AdamW optimizer with cosine learning rate scheduling (Llama 3: LR 2e-4, Gemma: LR 5e-5), batch size 64, 3 epochs, and sequence length 8192. The structured prompt format separates reference passages, questions, and answer options (A-D), with models expected to output single-letter answers.

## Key Results
- Fine-tuned Llama 3 and Gemma models achieve state-of-the-art accuracy on ViMMRC dataset
- Base models outperform their instruction-tuned counterparts (87.11% vs 84.94% for Gemma)
- Despite being smaller than GPT-3/GPT-3.5, fine-tuned models surpass both traditional BERT approaches and larger models
- Models show strong generalization across different grade levels (1-5) in Vietnamese educational materials

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QLoRA fine-tuning enables smaller parameter-efficient models to outperform much larger general-purpose models on specialized Vietnamese MRC tasks.
- Mechanism: Quantized Low-Rank Adaptation reduces trainable parameters by freezing the base model weights and injecting low-rank adapter matrices. This concentrates learning on task-specific adaptations (Vietnamese reading comprehension) while preserving the pretrained knowledge base, allowing a 7-8B parameter model to specialize beyond a 175B model's general capabilities.
- Core assumption: The pretrained model has sufficient multilingual representations that can be efficiently redirected for Vietnamese comprehension through targeted adapter updates.
- Evidence anchors:
  - [abstract] "By utilizing Quantized Low-Rank Adaptation (QLoRA), we efficiently fine-tune these models... Although our fine-tuned models are smaller than GPT-3 and GPT-3.5, they outperform both traditional BERT-based approaches and these larger models."
  - [section III.D] "QLoRA is a parameter-efficient method for adapting pre-trained language models by utilizing quantization and low-rank adaptation. This technique reduces the computational cost and memory footprint."
  - [corpus] Weak/missing - no corpus neighbors directly validate QLoRA's mechanism for Vietnamese; related work focuses on dataset creation rather than fine-tuning methodology.
- Break condition: If the pretrained model lacks sufficient Vietnamese token coverage or cross-lingual transfer capability, low-rank adapters may be insufficient to bridge the gap.

### Mechanism 2
- Claim: Base models outperform instruction-tuned variants when fine-tuned for Vietnamese MRC because instruction-tuning optimizes for objectives misaligned with reading comprehension.
- Mechanism: Instruction-tuned models undergo additional training for conversational abilities and safety/toxicity mitigation. For Gemma, the authors note instruction-tuning uses "exclusively English data," lacking multilingual data. This creates interference—the instruction-tuning shifts the model's response distribution away from the precise answer-selection behavior needed for multiple-choice MRC.
- Core assumption: The instruction-tuning process either degrades cross-lingual transfer (Gemma) or optimizes for capabilities orthogonal to MRC accuracy (Llama 3).
- Evidence anchors:
  - [section IV.C] "The fine-tuned base LLMs achieve better accuracy compared to their instruction-tuned counterparts... we speculate that this is because the instructed Gemma is trained exclusively on English data, lacking the multi-language data necessary."
  - [section IV.C] "The base Llama 3 model also surpasses its instruction-tuned version, though the difference is marginal... the instructed Llama is primarily optimized for chat abilities and toxicity mitigation."
  - [corpus] Not addressed in corpus neighbors.
- Break condition: If the instruction-tuned version included substantial multilingual instruction data, this performance gap would likely shrink or reverse.

### Mechanism 3
- Claim: Structured chat templates with explicit reference-question-option formatting reduce ambiguity in model outputs and improve answer extraction reliability.
- Mechanism: The template (Table II) separates context (reference passage), query (question + options), and expected output format (single letter A/B/C/D). This constrained output space reduces the model's need to generate free-form explanations and focuses computation on answer selection.
- Core assumption: The model can reliably follow the template format and map its internal comprehension to a single-token letter output.
- Evidence anchors:
  - [section III.D] "Our implementation employs a structured chat template (Table II) to prompt the model for answering multiple-choice questions... This structured format helps streamline the interaction and ensures that the model's responses are aligned with the user's expectations."
  - [section IV.E] Error analysis shows failures on reasoning and figurative language, not output formatting—suggesting template compliance is achieved.
  - [corpus] Not directly addressed.
- Break condition: If the model struggles with instruction-following in Vietnamese, template adherence could degrade, requiring constrained decoding.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA/QLoRA)**
  - Why needed here: This is the core technique enabling the paper's results. Understanding that LoRA adds trainable rank-decomposition matrices to frozen weights, and QLoRA adds 4-bit quantization, is essential before implementation.
  - Quick check question: Can you explain why LoRA reduces trainable parameters from O(d²) to O(dr) where d is hidden dimension and r is rank?

- **Concept: Base vs. Instruction-Tuned Model Selection**
  - Why needed here: The paper's counterintuitive finding that base models outperform instruction-tuned variants for this task informs model selection strategy for similar low-resource language adaptations.
  - Quick check question: What two reasons does the paper give for why instruction-tuned Gemma underperforms the base version?

- **Concept: Cross-Lingual Transfer in Multilingual LLMs**
  - Why needed here: Llama 3's 5% non-English pretraining data and its implications for Vietnamese performance underpin the paper's feasibility. Understanding transfer limitations helps set realistic expectations.
  - Quick check question: Why might a model with 5% non-English pretraining data still struggle with Vietnamese-specific tasks?

## Architecture Onboarding

- **Component map:**
  Input: ViMMRC dataset (2,783 questions across 417 texts, grades 1-5, train/dev/test split: 1,975/294/514)
  Base models: Llama 3 (8B) with 128k tokenizer, Gemma (7B) optimized for GPU
  Fine-tuning layer: QLoRA via Unsloth implementation (low-rank adapters on 4-bit quantized base)
  Prompting: Structured chat template with reference, question, 4 options (A-D)
  Output: Single-token classification (A, B, C, or D)

- **Critical path:**
  1. Load pretrained model with 4-bit quantization
  2. Attach LoRA adapters (rank not specified in paper—check Unsloth defaults)
  3. Format dataset using chat template (Table II)
  4. Train for 3 epochs with cosine LR scheduler
  5. Evaluate on test set with accuracy metric

- **Design tradeoffs:**
  - Base vs. Instruction-tuned: Base models show higher accuracy (87.11% Gemma Base vs. 84.94% Gemma Instructed) but may require more prompt engineering for deployment
  - Quantization precision vs. performance: Paper acknowledges "quantization techniques may introduce subtle performance trade-offs" (Section IV.F)
  - Model size vs. inference cost: 7-8B models enable resource-constrained deployment but may have weaker reasoning than larger models (evidenced by error analysis on inference questions)

- **Failure signatures:**
  - Reasoning failures: Models select plausible-but-incorrect answers requiring multi-step inference (e.g., inferring father's love from gift-giving behavior)
  - Poetic form recognition failures: Models conflate syllable-count patterns (selecting "six-eight" instead of "four syllables")
  - Grammatical role confusion: Misclassifying pronouns as nouns in syntactic analysis questions
  - Bias persistence: Unlike BERTology's "all of the above" bias, fine-tuned models show different error patterns—useful for debugging

- **First 3 experiments:**
  1. **Reproduce baseline comparison**: Fine-tune Llama 3 Base on ViMMRC using reported hyperparameters (batch 64, epochs 3, LR 2e-4, cosine schedule). Target: ~84.93% accuracy on test set.
  2. **Ablate model variant**: Run same experiment with Llama 3 Instructed and Gemma Base/Instructed. Verify the base > instructed pattern holds. If not, check template compatibility and learning rate sensitivity.
  3. **Error analysis on failure modes**: Evaluate on subset of questions requiring reasoning (like Table VII samples). Quantify error types to determine if Chain-of-Thought prompting (suggested future work) addresses these specific patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does Chain-of-Thought (CoT) prompting mitigate the reasoning errors identified in the error analysis?
- Basis in paper: [explicit] The authors propose implementing CoT prompting in future work to enhance reasoning capabilities, specifically addressing observed failures in identifying poetic forms and character traits.
- Why unresolved: The current study utilized standard fine-tuning via QLoRA without employing specialized prompting strategies to handle the complex reasoning tasks where the models currently struggle.
- What evidence would resolve it: An ablation study comparing the baseline QLoRA models against CoT-enhanced versions on the specific "reasoning-required" samples identified in Table VII.

### Open Question 2
- Question: How robust are these fine-tuned models when evaluated on advanced educational materials found in ViMMRC 2.0?
- Basis in paper: [explicit] The authors identify the exclusion of ViMMRC 2.0 (which extends to 12th grade) as a limitation and explicitly propose expanding evaluation to this dataset as future work.
- Why unresolved: The current experiments are restricted to texts for grades 1–5, leaving the models' performance on more complex, higher-level reading comprehension untested.
- What evidence would resolve it: Benchmarking the fine-tuned Llama 3 and Gemma models on the ViMMRC 2.0 test set to measure accuracy degradation relative to the primary school dataset.

### Open Question 3
- Question: What are the specific accuracy trade-offs introduced by using 4-bit quantization (QLoRA) versus full-parameter fine-tuning?
- Basis in paper: [explicit] The authors acknowledge that while quantization alleviates resource constraints, it "may introduce subtle performance trade-offs" that were not isolated in the current results.
- Why unresolved: The paper reports results for the QLoRA approach but does not provide a comparison to non-quantized baselines, making it unclear if the efficiency gains came at the cost of peak accuracy.
- What evidence would resolve it: A comparative analysis of accuracy scores between the 4-bit quantized models and their 16-bit full-precision counterparts on the same ViMMRC test set.

## Limitations
- Study focuses exclusively on multiple-choice format, limiting applicability to open-ended comprehension tasks
- Critical QLoRA hyperparameters (rank, alpha, target layers) are not reported, making exact reproduction challenging
- Models show systematic failures on reasoning and figurative language questions requiring deeper comprehension
- No evaluation against GPT-4 or other contemporary frontier models to establish absolute performance ceilings

## Confidence

**High Confidence (Reason: Claims are directly supported by experimental results with clear comparisons)**
- Fine-tuned Llama 3 and Gemma models achieve state-of-the-art performance on ViMMRC
- Base models outperform their instruction-tuned variants for this task
- QLoRA enables efficient fine-tuning of large models with reduced computational cost
- Models generalize across different grade levels in the dataset

**Medium Confidence (Reason: Mechanistic explanations are plausible but partially inferred from results)**
- Instruction-tuned models underperform due to English-only training data and chat optimization
- Structured chat templates improve answer extraction reliability
- 7-8B models can outperform 175B models through task specialization

**Low Confidence (Reason: Claims lack direct experimental validation or are speculative)**
- The models' error patterns indicate specific Vietnamese language challenges (inferred from error analysis)
- Bias mitigation effectiveness is implied but not systematically measured
- Resource efficiency claims are theoretical based on QLoRA properties rather than measured power consumption

## Next Checks

1. **Reproduce the base vs. instruction-tuned comparison**: Fine-tune both Llama 3 Base and Instructed variants on ViMMRC using the same hyperparameters. Verify the reported 84.93% vs 82.91% accuracy difference holds. If not reproducible, investigate whether template formatting or learning rate tuning affects this relationship.

2. **Evaluate reasoning failure modes**: Create a diagnostic test set from ViMMRC containing only questions requiring multi-step inference (similar to Table VII examples). Measure accuracy specifically on these reasoning questions to quantify whether Chain-of-Thought prompting could address this systematic weakness.

3. **Test Vietnamese tokenization robustness**: Run the fine-tuned models on Vietnamese text containing rare characters, diacritics, and compound words not present in the training data. Measure performance degradation to assess whether tokenization limitations constrain model generalization to broader Vietnamese NLP tasks.