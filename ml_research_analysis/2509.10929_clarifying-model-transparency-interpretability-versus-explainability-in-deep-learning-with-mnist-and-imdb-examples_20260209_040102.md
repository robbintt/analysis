---
ver: rpa2
title: 'Clarifying Model Transparency: Interpretability versus Explainability in Deep
  Learning with MNIST and IMDB Examples'
arxiv_id: '2509.10929'
source_url: https://arxiv.org/abs/2509.10929
tags:
- local
- explanations
- global
- mnist
- interpretability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper distinguishes between model interpretability (global\
  \ understanding of a model's internal logic) and explainability (post-hoc techniques\
  \ to justify specific predictions) in deep learning. Using MNIST digit recognition\
  \ and IMDB sentiment analysis as case studies, the work demonstrates that post-hoc\
  \ XAI methods like SHAP and LIME can generate local explanations for individual\
  \ predictions\u2014e.g., identifying critical pixels for a CNN's digit classification\
  \ or influential words for sentiment labeling\u2014even when the underlying model\
  \ is not globally interpretable."
---

# Clarifying Model Transparency: Interpretability versus Explainability in Deep Learning with MNIST and IMDB Examples

## Quick Facts
- arXiv ID: 2509.10929
- Source URL: https://arxiv.org/abs/2509.10929
- Authors: Mitali Raj
- Reference count: 7
- Primary result: Post-hoc XAI methods like SHAP and LIME provide local explanations for individual predictions in complex deep learning models, distinguishing between global interpretability and local explainability.

## Executive Summary
This paper distinguishes between model interpretability (global understanding of a model's internal logic) and explainability (post-hoc techniques to justify specific predictions) in deep learning. Using MNIST digit recognition and IMDB sentiment analysis as case studies, the work demonstrates that post-hoc XAI methods like SHAP and LIME can generate local explanations for individual predictions—e.g., identifying critical pixels for a CNN's digit classification or influential words for sentiment labeling—even when the underlying model is not globally interpretable. These local insights improve trust and comprehension without making the entire model transparent. The key outcome is that explainability provides actionable justifications for specific outputs in complex models, but does not equate to global interpretability, underscoring the need for both approaches in building trustworthy AI systems.

## Method Summary
The paper trains standard deep learning models (CNN for MNIST, LSTM/Transformer for IMDB) and applies post-hoc explainability tools including SHAP, LIME, and attention visualization. For MNIST, SHAP generates pixel attribution heatmaps while LIME uses superpixel perturbations to identify critical features. For IMDB, LIME analyzes word importance through text perturbations, and attention mechanisms reveal which tokens the model emphasized. The methods produce local explanations for individual predictions without requiring global model transparency. The work compares these approaches to demonstrate how explainability tools can make complex models more trustworthy through instance-level justifications.

## Key Results
- Post-hoc XAI methods like SHAP and LIME can generate local explanations for individual predictions in complex models
- Local explainability (justifying specific predictions) is achievable even when global interpretability is impossible
- Different XAI methods (perturbation-based vs. gradient-based vs. attention-based) can provide complementary insights for the same model

## Why This Works (Mechanism)

### Mechanism 1: Local Approximation via Perturbation-based Attribution
- Claim: Post-hoc methods like LIME can generate locally faithful explanations for individual predictions by approximating complex decision boundaries with simpler, interpretable surrogate models.
- Mechanism: LIME perturbs the input (e.g., toggling superpixels in images, removing words in text), observes prediction probability changes, and trains a sparse linear model on these perturbations to identify which input features most influenced the specific prediction.
- Core assumption: The complex model's decision boundary can be meaningfully approximated by a simpler model in a local neighborhood around a specific instance.
- Evidence anchors:
  - [abstract] "post-hoc XAI methods like SHAP and LIME can generate local explanations for individual predictions—e.g., identifying critical pixels for a CNN's digit classification or influential words for sentiment labeling"
  - [section IV.A] "LIME can locally approximate the CNN's decision boundary for a given image by introducing perturbations (like toggling superpixels) and training a simpler, understandable model"
  - [corpus] ABE framework paper (arXiv:2505.06258) describes attribution algorithms as "essential for enhancing the interpretability and trustworthiness of deep learning models by identifying key features driving model decisions"
- Break condition: If the local neighborhood is too large or the surrogate model class is misspecified, explanations may not faithfully reflect the original model's reasoning.

### Mechanism 2: Shapley-value Based Feature Importance
- Claim: SHAP provides theoretically grounded feature attribution by computing each feature's marginal contribution to the prediction across all possible feature coalitions.
- Mechanism: SHAP borrows from cooperative game theory to assign importance scores, calculating how much each feature (pixel, word) contributes to pushing the prediction away from the baseline prediction, averaged over all possible orderings of feature inclusion.
- Core assumption: Features can be meaningfully removed or masked, and the resulting prediction changes reflect genuine feature importance rather than artifacts of the masking procedure.
- Evidence anchors:
  - [abstract] "identifying critical pixels for a CNN's digit classification or influential words for sentiment labeling—even when the underlying model is not globally interpretable"
  - [section IV.A, Fig. 1] SHAP heatmap showing "red areas contribute positively to the prediction '7', while blue areas contribute negatively"
  - [corpus] Bond default risk paper (arXiv:2502.19615) notes "Interpretability analysis methods for artificial intelligence models, such as LIME and SHAP, are widely used, though they primarily serve as post-model for analyzing model outputs"
- Break condition: Computationally intractable for exact computation on high-dimensional inputs; approximations may introduce variance or bias.

### Mechanism 3: Attention as Proxy for Feature Importance
- Claim: In Transformer architectures, attention weights can reveal which input tokens the model prioritizes during processing, providing a form of built-in explanation.
- Mechanism: Self-attention mechanisms compute learned weights indicating how much each token attends to every other token; visualizing these weights (especially from the classification token to input tokens) shows which words influenced the final sentiment prediction.
- Core assumption: Higher attention weights correspond to greater influence on the prediction—a correlation that may not always hold.
- Evidence anchors:
  - [section IV.B] "When utilizing a Transformer, attention weights can reveal which words the model emphasized... For a review classified as 'positive', strong attention might fall on terms like 'excellent,' 'brilliant,' or 'loved.'"
  - [abstract] "word-level importance can clarify an IMDB sentiment outcome"
  - [corpus] LLM sentiment analysis paper (arXiv:2503.11948) addresses "Interpretability remains a key difficulty in sentiment analysis with Large Language Models" and applies SHAP to enhance understanding
- Break condition: Attention weights indicate where the model looks, not necessarily what reasoning it applies; attention can be diffuse or misleading for explaining predictions.

## Foundational Learning

- Concept: **Global vs. Local Model Understanding**
  - Why needed here: The paper's core argument depends on distinguishing global interpretability (understanding the entire model's logic) from local explainability (explaining individual predictions).
  - Quick check question: Can you explain why a decision tree is globally interpretable while a CNN with SHAP explanations is only locally explainable?

- Concept: **Perturbation-based Attribution**
  - Why needed here: LIME and certain SHAP variants work by systematically perturbing inputs and measuring output changes—understanding this is essential for debugging explanation quality.
  - Quick check question: If you remove the word "excellent" from a positive review and the prediction probability barely changes, what does that suggest about that word's importance?

- Concept: **Gradient-based Attribution**
  - Why needed here: Methods like Grad-CAM compute feature importance via backpropagation gradients, which requires understanding how gradients flow through neural networks.
  - Quick check question: Why might gradient-based methods produce noisy explanations for ReLU networks compared to smooth activation functions?

## Architecture Onboarding

- Component map: Input layer (images, text) -> Model backbone (CNN for MNIST, LSTM/Transformer for IMDB) -> XAI wrapper (LIME, SHAP, Grad-CAM, Attention Visualization) -> Explanation output (heatmaps for images, word importance scores for text) -> Evaluation layer (human assessment, faithfulness metrics)

- Critical path: 1. Train or load pre-trained model on target task (MNIST classification, IMDB sentiment) 2. Select XAI method appropriate to model type and explanation needs 3. For each instance requiring explanation: run inference, then apply XAI method 4. Visualize/communicate results (heatmaps, ranked word lists) 5. Validate that explanations align with domain knowledge (manual check)

- Design tradeoffs:
  - **LIME vs. SHAP**: LIME is faster but may be less stable; SHAP has theoretical guarantees but higher computational cost
  - **Perturbation vs. Gradient**: Perturbation methods are model-agnostic but slower; gradient methods are fast but model-specific
  - **Performance vs. Transparency**: Paper explicitly notes "High-accuracy DL models frequently lack global interpretability. Simpler, more interpretable alternatives might compromise predictive power."

- Failure signatures:
  - Explanations inconsistent with domain knowledge (e.g., highlighting background pixels in medical images)
  - High variance in explanations across multiple runs on same input
  - Explanations that change dramatically with small input perturbations
  - Contradictory explanations from different XAI methods on same prediction

- First 3 experiments:
  1. **Baseline LIME on MNIST**: Train a simple CNN on MNIST, apply LIME to explain 10 correctly classified and 10 misclassified digits; compare which features are highlighted—misclassifications should show "wrong" features being prioritized.
  2. **SHAP comparison across model complexity**: Train both a simple logistic regression and a deep CNN on MNIST; apply SHAP to both and compare explanation coherence—simpler model should have more consistent explanations.
  3. **Attention visualization sanity check on IMDB**: Fine-tune a Transformer on IMDB, visualize attention for reviews with clear sentiment words vs. ambiguous reviews; verify that clear cases show expected attention patterns (but note paper's warning: local explanations don't prove global understanding).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can post-hoc explanation methods advance from identifying correlational feature importance to providing insights grounded in causal inference?
- Basis in paper: [explicit] The authors list "Advancement of Causal Explanations" as a key future direction, noting the need to progress beyond correlational importance.
- Why unresolved: Current techniques like SHAP and LIME identify associations between features and outputs but do not necessarily isolate the causal mechanisms driving the model's logic.
- What evidence would resolve it: New XAI frameworks that can distinguish causal drivers from spurious correlations within a model's decision boundary.

### Open Question 2
- Question: How can we rigorously validate that local explanations generated by tools like LIME or SHAP genuinely mirror the model's internal reasoning?
- Basis in paper: [explicit] The paper cites "Veracity of Explanations" as a primary challenge, questioning whether justifications truly reflect the model's actual reasoning process.
- Why unresolved: Post-hoc methods often rely on surrogate models or approximations, creating a risk that the explanation is an artifact of the approximation rather than the original model.
- What evidence would resolve it: Development of standardized metrics or "sanity checks" that can quantify the fidelity of an explanation against the ground-truth behavior of the model.

### Open Question 3
- Question: Can novel deep learning architectures be developed that achieve high predictive performance without sacrificing inherent global transparency?
- Basis in paper: [explicit] The authors identify the "Performance vs. Transparency Trade-off" as a major limitation and call for the "Development of Inherently Interpretable Deep Learning Systems."
- Why unresolved: Currently, high performance on complex tasks (like IMDB sentiment analysis) typically requires deep, opaque architectures (e.g., Transformers) that defy global interpretation.
- What evidence would resolve it: A deep learning model that matches state-of-the-art accuracy on standard benchmarks while allowing complete human inspection of its operational logic.

## Limitations
- No quantitative validation of explanation quality or faithfulness metrics
- Unspecified background datasets for SHAP reference distributions
- No comparison of different XAI methods on same instances
- Attention-based explanations lack rigorous correlation testing with actual influence

## Confidence

**Major uncertainties:**
- No quantitative validation of explanation quality or faithfulness metrics
- Unspecified background datasets for SHAP reference distributions
- No comparison of different XAI methods on same instances
- Attention-based explanations lack rigorous correlation testing with actual influence

## Next Checks
1. Conduct ablation studies varying SHAP background dataset size (10→50→100 samples) to measure explanation stability
2. Compare LIME explanations across multiple random seeds on same instances to quantify variance
3. Test attention-based explanations against gradient-based methods (Grad-CAM) on same IMDB reviews to assess consistency