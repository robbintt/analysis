---
ver: rpa2
title: Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory
  Accelerators
arxiv_id: '2601.21737'
source_url: https://arxiv.org/abs/2601.21737
tags:
- quantization
- accuracy
- ieee
- latency
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a mixed-precision training and compilation
  framework for RRAM-based Computing-in-Memory (CIM) accelerators. The framework leverages
  mixed-precision quantization-aware training with a reinforcement learning-based
  optimizer (CIM-AQ) to determine optimal bit-width assignments for weights and activations
  across neural network layers.
---

# Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators

## Quick Facts
- arXiv ID: 2601.21737
- Source URL: https://arxiv.org/abs/2601.21737
- Reference count: 40
- Key outcome: Achieves up to 2.48× speedup over 8-bit quantization compilers on ResNet-18, VGG-16, and ViT-B/32 with minimal accuracy loss (as low as 0.086%).

## Executive Summary
This work introduces a mixed-precision training and compilation framework for RRAM-based Computing-in-Memory (CIM) accelerators. The framework leverages mixed-precision quantization-aware training with a reinforcement learning-based optimizer (CIM-AQ) to determine optimal bit-width assignments for weights and activations across neural network layers. This addresses the challenge of mapping high-precision models to low-resolution crossbars without incurring excessive latency from bit slicing. The framework includes a compiler that maps the optimized models to CIM targets using TVM, with CIM-specific optimizations.

## Method Summary
The framework consists of a reinforcement learning-based optimizer (CIM-AQ) that searches for optimal per-layer bit-width configurations, and a TVM-based compiler that maps the resulting mixed-precision models to CIM hardware. CIM-AQ extends the HAQ framework with Brevitas for quantization-aware training, using a DDPG agent to iteratively select bit-widths based on a reward function balancing latency and accuracy. The compiler processes ONNX models through Relay IR, applies CIM-specific optimization passes (QNNFuse, partitioning, staging buffers), and lowers to CIM API calls.

## Key Results
- Achieves up to 2.48× speedup over 8-bit quantization compilers
- Minimal accuracy loss (as low as 0.086%) on ResNet-18, VGG-16, and ViT-B/32
- Introduces effective bit-width constraints that improve search efficiency and performance
- Open-sourced framework available at https://github.com/jmkle/cim-aq

## Why This Works (Mechanism)

### Mechanism 1
Reinforcement learning with a custom reward function can efficiently search the massive mixed-precision quantization space for CIM targets. A DDPG agent iteratively selects bit-width configurations per layer, with a reward function that penalizes accuracy drops below a target threshold while rewarding latency speedup over an 8-bit baseline. Constraints reduce the search space by enforcing input/output layer requirements and weight alignment to cell resolution.

### Mechanism 2
Reducing activation bit-width provides more direct latency reduction than reducing weight bit-width on CIM architectures. Activation bit-width directly determines MVM cycles (each bit requires one cycle with 1-bit DACs), while weight bit-width affects latency indirectly through bit-slicing overhead. The RL agent learns to quantize activations more aggressively in early layers with fewer weights and more MVMs.

### Mechanism 3
TVM-based compilation with CIM-specific passes enables efficient mapping of mixed-precision models to crossbar operations. The compiler detects QDQ patterns, infers bit-widths via custom passes, merges floating-point residuals into integer operations, partitions CPU/CIM operations, and injects staging buffers with CIM API calls during lowering.

## Foundational Learning

- **Concept: Differential pair mapping on RRAM crossbars**
  - Why needed: Weights are stored as (g+, g−) cell pairs; essential for interpreting bit-slicing overhead and the latency model
  - Quick check: Why does a single N-column crossbar require 2N cells in differential mapping?

- **Concept: Fake quantization in QAT**
  - Why needed: Brevitas applies quantization effects during forward pass while allowing gradient flow; framework relies on this for accurate MPQ search
  - Quick check: What happens to gradients during the fake quantization forward pass?

- **Concept: DDPG actor-critic architecture**
  - Why needed: CIM-AQ extends HAQ which uses DDPG; understanding actor vs. critic separation clarifies how optimizer learns bit-width policies
  - Quick check: Why is DDPG considered "off-policy," and how does the replay buffer support this?

## Architecture Onboarding

- **Component map:** CIM-AQ (RL optimizer) -> Brevitas (QAT backend) -> ONNX model -> TVM compiler (Relay IR) -> CIM lowering -> Runtime

- **Critical path:**
  1. Define hardware parameters (crossbar size, cell resolution, DAC resolution, timing)
  2. Run CIM-AQ for ~600 episodes on reduced dataset to find MPQ policy
  3. Full 30-epoch QAT on ImageNet with discovered bit-widths
  4. Export to ONNX, compile through TVM pipeline
  5. Deploy to simulator/hardware

- **Design tradeoffs:**
  - Speedup vs. accuracy: S/AL score captures this; VGG-16 achieves 28.9, ViT-B/32 only 1.03
  - Constraint strictness: "Both constraints" yields best S/AL but may limit peak speedup
  - Cell resolution: 2-bit cells enable finer-grained speedup steps; 4-bit cells require larger weight reductions

- **Failure signatures:**
  - Large accuracy drops on ViT-B/32 (2.14%) suggest transformer attention layers are under-optimized
  - If speedup <1.5×, check whether constraints are forcing high bit-widths on first/last layers
  - Compilation errors likely stem from unsupported layer types requiring custom axis labeling

- **First 3 experiments:**
  1. Reproduce ResNet-18 with both constraints on 4-bit cells; target S/AL >2.9. Verify bit-width distribution matches Figure 7 pattern.
  2. Ablate the reward function: set γ=0 (ignore accuracy above target) and observe whether accuracy degrades or speedup increases.
  3. Test a new architecture (e.g., MobileNetV2) without constraints; measure whether CIM-AQ still finds viable MPQ configurations or requires constraint tuning.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can crossbar non-idealities (e.g., device variability, conductance drift) be effectively integrated into the mixed-precision quantization-aware training process?
  - Basis: Conclusion states future work will extend framework to include crossbar non-idealities
  - Why unresolved: Current framework assumes ideal hardware behavior, which may not hold on physical RRAM devices
  - Evidence needed: Updated training framework with non-ideal behavior injected into forward pass and resulting accuracy/robustness metrics

- **Open Question 2:** How does the framework scale to multi-core or tiled CIM architectures regarding the latency cost model and compilation strategy?
  - Basis: Methodology explicitly assumes "single CIM core" for latency cost modeling
  - Why unresolved: Current latency model doesn't account for inter-core communication, data routing overhead, or crossbar-level parallelism
  - Evidence needed: Extension of analytical model to include interconnect costs and compilation results mapping large models onto tiled, multi-core accelerator simulation

- **Open Question 3:** To what extent do the simulated speedups and accuracy metrics correlate with performance on physical RRAM hardware?
  - Basis: Evaluation conducted using an "open-source crossbar simulator" rather than physical measurements
  - Why unresolved: Simulators abstract away circuit-level parasitics and device defects that can degrade actual performance
  - Evidence needed: Deployment of compiled mixed-precision models onto fabricated RRAM chip or device-accurate SPICE-level simulation

## Limitations

- Limited architectural diversity in evaluation (primarily CNNs and one ViT variant)
- Hardware assumptions about crossbar non-idealities not yet implemented
- TVM compiler implementation details remain underspecified
- Reward function hyperparameters may require tuning for different neural architectures

## Confidence

- **High Confidence:** The fundamental premise that mixed-precision quantization can reduce CIM latency while preserving accuracy
- **Medium Confidence:** The RL-based search methodology due to limited architectural diversity and unspecified DDPG hyperparameters
- **Medium Confidence:** The TVM compilation pipeline due to insufficient implementation details for custom CIM lowering passes

## Next Checks

1. **Reward Function Sensitivity:** Perform ablation studies varying α, β, γ hyperparameters across different architectures (CNNs vs. transformers) to determine optimal reward function tuning for each domain.

2. **Crossbar Non-ideality Integration:** Implement and evaluate the framework with realistic crossbar variability and drift models to validate robustness under hardware imperfections.

3. **Compiler Fidelity Verification:** Test the TVM-based compiler with custom layer types and verify generated CIM API calls against hardware simulator specifications to ensure correct compilation.