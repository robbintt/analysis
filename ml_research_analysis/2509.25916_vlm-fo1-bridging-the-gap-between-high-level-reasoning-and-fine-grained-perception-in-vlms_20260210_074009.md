---
ver: rpa2
title: 'VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception
  in VLMs'
arxiv_id: '2509.25916'
source_url: https://arxiv.org/abs/2509.25916
tags:
- arxiv
- region
- visual
- vision
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLM-FO1 introduces a plug-and-play framework that transforms object-centric
  perception from brittle coordinate generation into robust feature retrieval, addressing
  VLMs' weakness in fine-grained localization. The method employs a Hybrid Fine-grained
  Region Encoder with dual vision encoders to produce region tokens rich in semantic
  and spatial detail, enabling the LLM to reason about and ground language in specific
  visual regions.
---

# VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs

## Quick Facts
- **arXiv ID:** 2509.25916
- **Source URL:** https://arxiv.org/abs/2509.25916
- **Reference count:** 40
- **Primary result:** Achieves 44.4 mAP on COCO object detection, surpassing much larger VLMs while preserving general visual reasoning capabilities

## Executive Summary
VLM-FO1 addresses the fundamental tension in vision-language models between high-level reasoning and fine-grained perception by reframing object detection as a feature retrieval task rather than coordinate generation. The method employs a hybrid fine-grained region encoder that combines semantic features from a frozen primary vision encoder with high-resolution spatial details from an auxiliary encoder, enabling the LLM to reason about and ground language in specific visual regions. Through a two-stage decoupled training strategy, VLM-FO1 achieves state-of-the-art performance across diverse benchmarks while maintaining the base model's general visual understanding capabilities.

## Method Summary
VLM-FO1 integrates with any pre-trained VLM by adding a hybrid fine-grained region encoder (HFRE) that fuses features from both the base model's vision encoder and a high-resolution auxiliary encoder (DaViT-Large). The system uses an external Omni Proposal Network (OPN) to generate bounding boxes, which are processed through region-of-interest alignment (RoIAlign) and fused with positional embeddings. The two-stage training strategy first aligns the region tokens to the LLM's embedding space with the base VLM frozen, then refines perception through mixed data including negative rejection samples while gently unfreezing components.

## Key Results
- Achieves 44.4 mAP on COCO object detection (over 20-point improvement over baseline)
- Demonstrates 59.0% accuracy on COCOText OCR and 80.1 on Ferret Bench referring reasoning
- Outperforms much larger models while preserving general VLM capabilities (64.6 vs 64.5 on OpenCompass benchmarks)
- Maintains plug-and-play integration with any pre-trained VLM architecture

## Why This Works (Mechanism)

### Mechanism 1
Reframing object detection from numerical regression to discrete retrieval significantly reduces error brittleness. Instead of forcing the LLM to generate precise floating-point coordinates, the system uses an external detector to propose regions and retrieves these using special tokens, converting a hard generation problem into a robust selection task.

### Mechanism 2
Hybrid feature encoding preserves high-level reasoning while enabling fine-grained perception. The HFRE fuses features from the base VLM's semantically-rich encoder and a high-resolution auxiliary encoder, preventing the perception-reasoning trade-off where improving vision often degrades language understanding.

### Mechanism 3
Two-stage decoupled training enables plug-and-play integration without catastrophic forgetting. By freezing the base VLM in Stage 1 and only training projection layers, the model aligns new region tokens to the existing embedding space. Stage 2 then gently refines perception while retaining general capabilities.

## Foundational Learning

- **Concept: RoIAlign (Region of Interest Alignment)**
  - **Why needed here:** Extracts features from specific bounding boxes proposed by the OPN without losing spatial precision due to quantization errors, critical for the "fine-grained" aspect of the paper.
  - **Quick check question:** Can you explain why RoIAlign uses bilinear interpolation rather than integer rounding (as in RoIPool) when mapping proposal boxes to the feature map grid?

- **Concept: Positional Embeddings (Sine-Cosine)**
  - **Why needed here:** The LLM processes tokens sequentially and needs explicit spatial signals added to region tokens since the sequence order is arbitrary and does not correspond to image coordinates.
  - **Quick check question:** If you shuffle the input order of the region tokens, should the output prediction change? Why or why not based on the paper's architecture?

- **Concept: Negative Mining / Rejection Sampling**
  - **Why needed here:** Standard VLMs are trained to be helpful and often hallucinate objects to match text prompts. To perform accurate detection where absence is valid, the model must be explicitly trained on examples requiring "none" responses.
  - **Quick check question:** In Section 4.1, what percentage of detection data is reserved for rejection samples, and what specific failure mode does this address?

## Architecture Onboarding

- **Component map:** Image + Text Prompt -> Omni Proposal Network -> Dual-Vision Encoder (Primary + Auxiliary) -> HFRE (RoIAlign + Fusion) -> Region-Language Connector -> LLM (Qwen2.5 LLM)
- **Critical path:** The flow relies on the OmDet-Turbo generating valid boxes, which are fed into RoIAlign modules operating on feature maps from both Vision Encoders. The resulting vectors are fused, projected by the Region-Language Connector into the LLM's dimension, and prepended to the text sequence.
- **Design tradeoffs:** The system runs a full object detector plus a VLM, which is slower than pure VLM regression but solves the precision problem. The proposal network is decoupled, allowing swapping for specialized detectors without retraining the entire VLM.
- **Failure signatures:**
  - Hallucinated Detections: Model detects objects mentioned in prompt but not in image (insufficient Stage 2 rejection sampling)
  - Low Recall: Model misses small objects (OPN proposal limit too low or failed to propose region)
  - Semantic Drift: Model loses general chat ability (primary vision encoder unfrozen or general data omitted from Stage 2)
- **First 3 experiments:**
  1. Integration Test (Stage 1 alignment): Train only the MLP Connector with VLM frozen. Verify LLM can accurately describe content of specific `<regionN>` token (Region Classification).
  2. Ablation on Feature Sources: Run inference using only Auxiliary vs. only Primary features for region tokens. Compare COCO mAP to validate necessity of Hybrid fusion.
  3. Negative Rejection Test: Prompt with "Find the [object_not_in_image]" on images. Measure False Positive rate to ensure Stage 2 rejection training was effective.

## Open Questions the Paper Calls Out

### Open Question 1
Can the Hybrid Fine-grained Region Encoder (HFRE) be effectively integrated into other VLM architectures (e.g., InternVL or LLaVA) with different embedding spaces, or is the alignment training specific to the Qwen2.5-VL backbone?

### Open Question 2
How does the model's object detection recall degrade in highly cluttered environments where the number of foreground objects significantly exceeds the fixed input limit of 100 proposals?

### Open Question 3
What is the computational overhead and inference latency introduced by the Dual-Vision Encoder and HFRE compared to standard coordinate-generation VLMs?

## Limitations
- Critical dependency on custom OmDet-Turbo object proposal network with unspecified training details and hyperparameters
- Multiple unspecified training configurations including epochs, batch sizes, and learning rate schedules
- Evaluation scope limited to perception-specific benchmarks without detailed breakdown of individual task performance or reasoning capability analysis

## Confidence

**High Confidence:**
- Core mechanism of reframing coordinate regression as feature retrieval is well-supported by architecture description and ablation studies
- Hybrid encoder design validated by ablation showing 67.65 vs 66.15/65.89 for individual encoders

**Medium Confidence:**
- Two-stage training strategy's effectiveness in preventing catastrophic forgetting supported by Table 7 comparisons but lacks detailed training dynamics
- Generalization claims across diverse benchmarks assume consistent OPN performance without cross-dataset analysis

**Low Confidence:**
- Generalization claims assume OPN performs consistently across domains without supporting analysis

## Next Checks

1. **OPN Independence Validation:** Train VLM-FO1 with alternative proposal networks (Grounding DINO, YOLO variants) while keeping all other components constant. Measure performance degradation to quantify OPN dependency and identify minimum recall threshold for effective feature retrieval.

2. **Negative Sample Impact Analysis:** Conduct ablation experiments removing the 20% rejection samples from Stage 2 training. Compare hallucination rates on OVDEval hard negatives and COCO detection precision to empirically validate the necessity of explicit negative mining.

3. **General Capability Degradation Test:** Perform fine-grained analysis of VLM-FO1's performance on individual OpenCompass tasks (specifically MMBench, MMStar) during training progression. Track task-specific scores to identify hidden degradation in reasoning capabilities masked by aggregate metrics.