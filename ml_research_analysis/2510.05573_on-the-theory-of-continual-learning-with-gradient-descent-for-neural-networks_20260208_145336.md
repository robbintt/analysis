---
ver: rpa2
title: On the Theory of Continual Learning with Gradient Descent for Neural Networks
arxiv_id: '2510.05573'
source_url: https://arxiv.org/abs/2510.05573
tags:
- learning
- task
- loss
- tasks
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes gradient-based continual learning in one-hidden-layer
  neural networks, focusing on train and test-time forgetting. It considers a stream
  of independent XOR cluster tasks with orthogonal means and Gaussian noise, using
  gradient descent without explicit regularization.
---

# On the Theory of Continual Learning with Gradient Descent for Neural Networks

## Quick Facts
- arXiv ID: 2510.05573
- Source URL: https://arxiv.org/abs/2510.05573
- Authors: Hossein Taheri; Avishek Ghosh; Arya Mazumdar
- Reference count: 40
- One-line primary result: With n = Θ̃(d²K) samples per task, m = Θ(d⁸K⁴) hidden neurons, and T = Θ(d²) iterations, both train and test errors remain small across all tasks in continual learning with gradient descent for one-hidden-layer neural networks.

## Executive Summary
This paper provides the first theoretical analysis of gradient-based continual learning in neural networks, focusing on train and test-time forgetting. The authors analyze a one-hidden-layer neural network trained with full-batch gradient descent on a stream of independent XOR cluster tasks with orthogonal means and Gaussian noise. They derive closed-form bounds showing that increasing sample size and network width can reduce forgetting, and prove that both train and test errors remain small across all tasks when sufficient samples, width, and iterations are used.

## Method Summary
The paper analyzes continual learning on XOR cluster tasks using a one-hidden-layer neural network with quadratic activation. The model uses full-batch gradient descent with fixed second-layer coefficients, operating in the lazy/kernel regime where weights stay close to initialization. The analysis derives forgetting bounds based on concentration inequalities and algorithmic stability arguments, showing how sample size, network width, and iteration count affect forgetting rates. Experiments validate these theoretical insights across various settings using both synthetic and real datasets.

## Key Results
- Train-time forgetting bound scales as O(1/√n) with sample size and O(ηT√(K−k)/(d√n)) with task position
- Test-time forgetting decomposes into train-time forgetting plus a generalization gap that shrinks with width
- With n = Θ̃(d²K) samples, m = Θ(d⁸K⁴) neurons, and T = Θ(d²) iterations, both train and test errors remain small across all tasks
- Width scaling is conservative; experiments show benefits plateau around m = 10³–10⁴ for d=50–75

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing sample size per task reduces train-time forgetting by concentrating gradient signal toward task-relevant directions.
- Mechanism: With n = Θ̃(d²K) samples, the empirical average of gradient updates converges to its expectation, reducing noise that interferes with previously learned tasks. The bound shows forgetting scales as O(1/√n).
- Core assumption: Tasks have orthogonal mean vectors (µₖ⁺ ⊥ µⱼ⁺ for k≠j), preventing direct gradient interference in the infinite-width limit.
- Evidence anchors:
  - [abstract]: "bounds on the rate of forgetting during train and test-time in terms of the number of iterations, the sample size..."
  - [section 2.2, Theorem 1]: Forgetting bound includes term ηT√(K−k)/(d√n)
  - [corpus]: Related work on overparameterized continual learning (arxiv 2502.10442) confirms sample complexity matters but lacks closed-form rates.
- Break condition: If tasks share non-orthogonal structure (µₖ⁺ not orthogonal to µⱼ⁺), gradient updates from later tasks directly corrupt earlier task solutions.

### Mechanism 2
- Claim: Over-parameterization (large width m) enables the lazy/kernel regime where weights stay near initialization, limiting forgetting.
- Mechanism: With m = Θ(d⁸K⁴) neurons, the network operates in the Neural Tangent Kernel regime. Gradient updates are linearized around initialization, so weight distance ‖w_T − w₀‖ scales as O(T/√m), keeping feature representations stable across tasks.
- Core assumption: Quadratic activation φ(t) = t²/2 and early stopping with T = Θ(d²) iterations prevent weights from exiting the linearization region.
- Evidence anchors:
  - [section 1.2]: "increasing the width enables the network to operate in the lazy/kernel regime where...features remain constant during training"
  - [section C.3, Eq. 19]: Finite-width error scales as O(η²K²T²/√m)
  - [corpus]: Multiple neighbors (Graldi et al. 2024, Guha & Lakshman 2024) empirically observe width helps but with diminishing returns—consistent with √m scaling.
- Break condition: If T grows too large (beyond Θ(d²)) or learning rate η is too large, weights exit the kernel regime and feature learning occurs, potentially increasing forgetting (as noted in Section 4).

### Mechanism 3
- Claim: Test-time forgetting decomposes into train-time forgetting plus a delayed generalization gap that shrinks with width and cumulative training loss.
- Mechanism: By algorithmic stability arguments, the generalization gap F_gen_k,K scales as O(ηT·e^(ηT·K/√m)/n). Larger width stabilizes the learning dynamics, reducing sensitivity to any single training example.
- Core assumption: Loss function is 1-Lipschitz and 1-smooth (for Theorem 3), or self-bounded (for improved Theorem 4 bounds).
- Evidence anchors:
  - [section 2.1.2, Eq. 3]: Explicit decomposition F_ts = F_tr + F_gen
  - [section 2.2, Theorem 3]: "F_gen_k,K ≲ ηT·e^(ηT(K−k+1)/√m) / n"
  - [corpus]: Corpus evidence is weak for this specific decomposition; related works focus on linear models rather than neural net stability bounds.
- Break condition: If the loss function lacks smoothness (e.g., 0-1 loss) or the network width is insufficient, stability-based bounds become vacuous.

## Foundational Learning

- Concept: **Neural Tangent Kernel (NTK) regime**
  - Why needed here: The entire theoretical analysis assumes m→∞ where the network linearizes, enabling closed-form weight evolution analysis.
  - Quick check question: Can you explain why the NTK regime requires ‖w − w₀‖/√m → 0 during training?

- Concept: **Algorithmic stability for generalization**
  - Why needed here: Test-time forgetting bounds rely on uniform stability arguments (Bousquet & Elisseeff 2002) adapted to continual learning.
  - Quick check question: How does leave-one-out stability of the final iterate relate to the generalization gap?

- Concept: **Sub-exponential concentration (Bernstein's inequality)**
  - Why needed here: Proving that gradient updates concentrate around their expectation requires bounding sums of sub-exponential random variables (the (x_i^T x_j)² terms).
  - Quick check question: For what values of n does the empirical gradient mean converge to the population gradient within O(1/d)?

## Architecture Onboarding

- Component map:
  - Input: d-dimensional XOR cluster data with orthogonal task means
  - Hidden layer: m neurons with quadratic activation (t²/2)
  - Output: Fixed second-layer coefficients a_i ∈ {±1}
  - Loss: Hinge loss f(u) = max{1−u, 0} for training analysis; logistic loss compatible with improved generalization bounds

- Critical path:
  1. Initialize w⁽⁰⁾ ~ N(0, I_p) where p = md
  2. For each task k: run T full-batch GD steps on n samples
  3. Track weight evolution: w_K ≈ w₀ + (ηT/√m)·Σ_k A_k·z where A_k captures task-k gradient statistics
  4. Bound forgetting by analyzing how A_j (j>k) perturbs task-k output

- Design tradeoffs:
  - **Width vs. iterations**: m = Θ(d⁸K⁴) is extremely conservative; experiments show benefits plateau around m = 10³–10⁴ for d=50–75
  - **Early stopping vs. convergence**: T = Θ(d²) ensures train loss → 0 but larger T increases forgetting; Figure 4 shows T=4000 causes more drift than T=2000
  - **Sample size vs. task count**: n scales linearly with K (n = Θ̃(d²K)), so more tasks require proportionally more data per task

- Failure signatures:
  - Catastrophic forgetting spike when width is too small (m < d⁴): Figure 5 shows m=100 fails while m≥1000 succeeds
  - Generalization collapse if T >> d²: weights exit kernel regime, stability bounds break
  - Non-orthogonal tasks: theory provides no guarantees; empirical performance degrades significantly

- First 3 experiments:
  1. **Vary sample size**: Fix K=3, m=1000, T=200, d=50. Run n∈{1000, 2500, 5000}. Expected result: forgetting F_tr_1,3 decreases as O(1/√n). Verify against Figure 1.
  2. **Vary width**: Fix K=3, n=5000, T=200, d=75. Run m∈{100, 300, 1000, 3000, 10000}. Expected result: test error improves then plateaus. Check if finite-width error term O(T²/√m) matches empirical slope.
  3. **Stress test orthogonality**: Generate tasks with μ_k·μ_j = cos(θ) for θ∈{0°, 45°, 90°}. Expected result: forgetting increases as orthogonality decreases; theory's θ=90° case should match empirical best case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can analogous forgetting guarantees be derived for stochastic gradient descent (SGD) with mini-batches, and how does optimization noise interact with catastrophic forgetting?
- Basis in paper: [explicit] "An immediate next step is to analyze other training methodologies, such as (mini-batch) stochastic gradient descent, where additional noise may interact with forgetting."
- Why unresolved: The paper only analyzes full-batch gradient descent. SGD introduces gradient noise that could either help or harm forgetting through implicit regularization or destabilization effects.
- What evidence would resolve it: Theoretical bounds on forgetting under mini-batch SGD, or experiments showing whether SGD noise amplifies or mitigates forgetting compared to full-batch GD.

### Open Question 2
- Question: Can similar theoretical guarantees be extended to the feature-learning regime where step-sizes are large, weights move substantially from initialization, and early stopping is avoided?
- Basis in paper: [explicit] "Extending the theory to the feature-learning regime... remains a challenging and exciting problem."
- Why unresolved: The analysis relies on the lazy/kernel regime where weights stay close to initialization; feature learning involves non-linear dynamics that break the linearization arguments used in the proofs.
- What evidence would resolve it: Bounds on forgetting that hold for large step-sizes without early stopping, or demonstrations that the lazy-regime requirements can be relaxed.

### Open Question 3
- Question: Are the polynomial dependencies in the derived sample and width complexities (n = Θ̃(d²K), m = Θ(d⁸K⁴)) tight, or can significantly tighter bounds be achieved?
- Basis in paper: [inferred] The bounds have high polynomial dependencies, particularly the d⁸K⁴ width requirement, and experiments show benefits diminish as m grows, suggesting possible overestimation.
- Why unresolved: The proof techniques require union bounds and concentration arguments that may be loose; empirical results suggest smaller widths suffice in practice.
- What evidence would resolve it: Refined analysis yielding lower polynomial exponents, or construction of counterexamples showing the current dependencies are necessary.

### Open Question 4
- Question: How does task correlation (non-orthogonal task structure) affect forgetting rates and the conditions for successful continual learning?
- Basis in paper: [explicit] "Although our analysis can be extended to the more general case where the mean vectors are not orthogonal between tasks, this is beyond the scope of the present work."
- Why unresolved: Orthogonality simplifies the analysis by ensuring task gradients do not interfere; correlated tasks introduce interference terms that complicate the weight evolution analysis.
- What evidence would resolve it: Forgetting bounds expressed in terms of task similarity measures, or experiments showing how correlation impacts the sample/width requirements.

## Limitations
- Theoretical guarantees critically depend on orthogonal task means, limiting practical applicability to real-world scenarios where task relationships are rarely perfectly orthogonal
- Width scaling constants are extremely conservative (m = Θ(d⁸K⁴)) compared to empirical results (m ≈ 20d), suggesting the theoretical framework may be loose
- The analysis assumes fixed learning rate and doesn't explore how different learning rates affect the trade-off between convergence speed and forgetting

## Confidence
- **High confidence**: Train-time forgetting bounds (Theorem 1) - these follow directly from concentration inequalities and have experimental validation
- **Medium confidence**: Generalization gap decomposition (Theorem 3) - the stability-based argument is sound but relies on strong smoothness assumptions
- **Medium confidence**: Width requirement scaling - theoretical bounds are rigorous but likely pessimistic compared to practical performance

## Next Checks
1. **Non-orthogonal task experiment**: Systematically vary the angle between task means (0° to 90°) and measure how forgetting scales. This would validate whether the orthogonality assumption is truly necessary or just simplifies analysis.

2. **Width scaling study**: Run experiments with m ∈ {d, d², d⁴, d⁸} for fixed d and K. Plot forgetting vs 1/√m to verify the theoretical scaling relationship and identify where diminishing returns begin.

3. **Early stopping verification**: Test whether T = Θ(d²) iterations are indeed optimal. Run experiments with T ∈ {d, d², d³} and measure both train loss convergence and forgetting to identify the sweet spot where early stopping prevents kernel regime breakdown.