---
ver: rpa2
title: Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making
arxiv_id: '2601.01522'
source_url: https://arxiv.org/abs/2601.01522
tags:
- cost
- decision
- prior
- framework
- screen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a Bayesian framework for orchestrating multiple
  LLMs in cost-aware sequential decision-making, addressing the limitations of current
  discriminative approaches. The core innovation is eliciting likelihood functions
  via contrastive prompting, aggregating across diverse models using robust statistics,
  and applying Bayesian updating to enable cost-sensitive action selection and principled
  information gathering.
---

# Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making

## Quick Facts
- **arXiv ID**: 2601.01522
- **Source URL**: https://arxiv.org/abs/2601.01522
- **Reference count**: 40
- **Primary result**: 34% cost reduction ($294K savings) over single-LLM baselines in synthetic hiring with 5 LLMs.

## Executive Summary
This paper presents a Bayesian framework for orchestrating multiple LLMs in cost-aware sequential decision-making, specifically for hiring decisions. The key innovation is using contrastive prompting to elicit likelihood functions from diverse LLMs, then aggregating these via robust statistics (median) and updating beliefs through Bayesian inference. The framework enables cost-sensitive action selection and principled information gathering via value of information analysis. Experiments on 1,000 synthetic resumes show 34% cost reduction, improved demographic parity by 45%, and better-calibrated beliefs compared to discriminative baselines.

## Method Summary
The framework elicits likelihood functions p(x|s) for each candidate resume x and quality state s via contrastive prompting across 5 LLMs. Each LLM scores how typical the resume is for each state (0-10 scale), normalized to likelihoods. These are aggregated using median across models. Bayesian updating computes posterior beliefs about candidate quality, and cost-aware action selection chooses optimal decisions (reject/phone screen/interview) by minimizing expected cost. Disagreement-triggered screening activates phone screens when uncertainty is high, quantified via coefficient of variation and value of information analysis.

## Key Results
- 34% cost reduction ($294,000 savings) compared to best single-LLM baseline
- Improved demographic parity by 45% (reduced max group difference in interview rates)
- Better-calibrated beliefs (Expected Calibration Error 0.09 vs 0.18 for baselines)
- Ablation analysis: multi-LLM aggregation contributes 51% of savings, sequential updating 43%, disagreement-triggered screening 20%

## Why This Works (Mechanism)
The framework succeeds by directly modeling p(x|s) rather than discriminative p(s|x), avoiding prior-data mismatch that plagues discriminative approaches. Bayesian updating with ensemble likelihoods captures epistemic uncertainty naturally, enabling cost-sensitive decisions that account for both action costs and outcome uncertainties. The contrastive elicitation prompts LLMs to reason about state-conditional distributions rather than memorizing decision boundaries, improving calibration and generalization. Multi-LLM aggregation reduces individual model biases while maintaining robustness through median-based combination.

## Foundational Learning
- **Likelihood elicitation via contrastive prompting**: Prompt LLMs to score how typical evidence is for each state, then normalize scores to approximate p(x|s). Needed because discriminative models p(s|x) suffer from prior mismatch. Quick check: verify elicited likelihoods sum to 1 across states for individual evidence.
- **Bayesian updating with asymmetric costs**: Compute expected cost of each action given posterior beliefs, then select action minimizing expected cost. Needed to make optimal decisions under uncertainty. Quick check: confirm posterior concentrates on true state as evidence accumulates.
- **Value of information analysis for screening**: Trigger additional evidence collection (phone screen) when expected cost reduction exceeds collection cost. Needed to avoid both premature decisions and unnecessary expenses. Quick check: measure reduction in decision uncertainty after screening.

## Architecture Onboarding

**Component Map**: Synthetic resume generation -> Likelihood elicitation (5 LLMs) -> Median aggregation -> Bayesian updating -> Cost-aware action selection -> VOI-based screening

**Critical Path**: Evidence → Likelihood elicitation → Aggregation → Bayesian update → Action selection. The bottleneck is LLM response time and aggregation latency.

**Design Tradeoffs**: Median aggregation provides robustness to outliers but discards information about likelihood shape. Sequential updating enables VOI-based screening but requires maintaining and updating beliefs. Contrastive prompting improves calibration but needs careful prompt engineering.

**Failure Signatures**: Systematic over-interviewing indicates prior mismatch or miscalibrated likelihoods. High disagreement with low VOI suggests ambiguous evidence or poorly separated states. Degraded performance with single LLM reveals ensemble dependency.

**First 3 Experiments**:
1. Run ablation with single LLM (no aggregation) - expect 51% cost increase
2. Test uniform prior - expect 41% cost increase, confirming prior sensitivity
3. Disable VOI screening - measure cost impact and decision accuracy changes

## Open Questions the Paper Calls Out

**Open Question 1**: How many LLM queries are needed to estimate likelihoods p(x|s) within ε error, and how does this scale with state space size |S| and observation complexity? (Basis: Section 4.7 lists theoretical foundations as open research direction)

**Open Question 2**: How should likelihood elicitation and Bayesian updating be adapted for hierarchical or continuous state spaces rather than discrete 4-state models? (Basis: Section 4.7 notes current framework uses discrete states but many domains have hierarchical or continuous latent variables)

**Open Question 3**: Do likelihood elicitation prompts transfer across domains (medical triage, credit underwriting) without domain-specific engineering? (Basis: Section 4.7 asks how well prompts transfer to other domains)

**Open Question 4**: How should LLM likelihoods be combined with human expert assessments within the Bayesian framework? (Basis: Section 4.7 lists integration with human judgment as unexplored)

## Limitations

- Prompt specification ambiguity: Exact contrastive prompt templates for different evidence types are not provided, introducing uncertainty in reproducing exact likelihood distributions
- Cost parameter calibration: Framework performance is sensitive to asymmetric cost parameters that may not generalize across domains
- Synthetic data generation process: Resume generation relies on LLM-produced text validated via expert review, but criteria for manual review and removal are not fully specified

## Confidence

- **High confidence**: Core theoretical framework (Bayesian updating with likelihood elicitation, cost-aware action selection, VOI-based screening) is mathematically sound and well-defined
- **Medium confidence**: Empirical results showing 34% cost reduction and 45% demographic parity improvement are internally consistent but depend on synthetic data and specific cost parameters
- **Low confidence**: Relative contribution of each component (51% aggregation, 43% updating, 20% screening) from ablations should be interpreted cautiously due to synthetic setup limitations

## Next Checks

1. **Prompt replication test**: Implement contrastive prompting with minor variations in wording and measure variance in likelihood scores across 5 trials per resume to establish sensitivity to prompt phrasing

2. **Cost parameter sensitivity**: Sweep interview cost C_interview from $1K to $5K and measure how optimal action policy and total cost change, identifying thresholds where framework behavior qualitatively shifts

3. **Real-world transfer**: Apply framework to a small set of real resumes with human-annotated ground truth from a partnering company to validate whether cost savings and demographic parity improvements hold outside synthetic domain