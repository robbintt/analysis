---
ver: rpa2
title: 'HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence
  from Reading to Discovery'
arxiv_id: '2512.22899'
source_url: https://arxiv.org/abs/2512.22899
tags:
- scientific
- literature
- across
- reasoning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HiSciBench introduces a hierarchical, multi-disciplinary benchmark
  for evaluating scientific intelligence across five progressive cognitive levels
  (L1-L5) spanning six scientific disciplines. The benchmark contains 8,735 instances
  testing fundamental knowledge, literature parsing, question answering, review generation,
  and scientific discovery, with multimodal and cross-lingual support.
---

# HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery

## Quick Facts
- arXiv ID: 2512.22899
- Source URL: https://arxiv.org/abs/2512.22899
- Reference count: 40
- Introduces hierarchical benchmark spanning five cognitive levels across six scientific disciplines

## Executive Summary
HiSciBench introduces a hierarchical, multi-disciplinary benchmark for evaluating scientific intelligence across five progressive cognitive levels (L1-L5) spanning six scientific disciplines. The benchmark contains 8,735 instances testing fundamental knowledge, literature parsing, question answering, review generation, and scientific discovery, with multimodal and cross-lingual support. Comprehensive evaluation of 18 state-of-the-art models revealed significant performance gaps: while GPT-5 achieves 69% accuracy on basic literacy tasks, performance drops to 25% on discovery-level challenges, with particularly low citation verifiability (17-22%). The benchmark exposes critical weaknesses in higher-order reasoning, multimodal integration, and epistemic grounding, providing actionable insights for developing more capable and reliable scientific AI systems.

## Method Summary
The benchmark evaluates scientific intelligence through five hierarchical levels: L1 (Scientific Literacy) with 1,200 multiple-choice questions, L2 (Literature Parsing) with 629 OCR samples and translation pairs, L3 (Literature QA) with 5,514 monolingual and 629 cross-lingual QA pairs, L4 (Review Generation) with 60 review topics using LLM-as-Judge for content quality, and L5 (Discovery) with 74 code-generation tasks measuring execution success. The 8,735 instances span six disciplines and test both text-only and vision-language models, with metrics ranging from accuracy and BLEU scores to citation verifiability and code execution success rates.

## Key Results
- GPT-5 achieves 69% accuracy on L1 scientific literacy but performance drops to 25% on L5 discovery tasks
- Vision-language models show 12-17 BLEU point degradation compared to text-only models on translation tasks
- General LLMs achieve 88-99% content quality scores but only 17-22% citation verifiability due to 81% citation fabrication
- SurveyX achieves 71.4% citation verifiability compared to GPT-5's 19.3%, demonstrating grounding improvement potential

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structuring scientific evaluation hierarchically reveals performance degradation patterns that flat benchmarks miss.
- Mechanism: By decomposing scientific capability into sequential cognitive stages (L1→L5), the benchmark creates dependency-aware failure attribution. Models that fail at L4 or L5 often have compound weaknesses traceable to earlier levels—insufficient factual grounding (L1) propagates into synthesis errors (L4) and discovery failures (L5).
- Core assumption: Scientific intelligence is genuinely hierarchical rather than a collection of independent skills.
- Evidence anchors: [abstract] "performance declines sharply to 25% on discovery-level challenges"; [section III.B] "scientific research expertise develops through distinct cognitive stages"; [corpus] Related work (SciEvalKit, MASTER) similarly adopts hierarchical evaluation.

### Mechanism 2
- Claim: Multimodal integration introduces semantic noise that degrades linguistic reasoning performance.
- Mechanism: Vision-language models process visual and textual information through shared representations, but scientific documents contain dense symbolic content where visual parsing errors propagate into downstream reasoning. Text-only inputs consistently outperform vision-language inputs on translation tasks (37-49 BLEU vs. 12-17 BLEU gap).
- Core assumption: Visual and textual modalities in scientific documents should reinforce rather than interfere with each other.
- Evidence anchors: [section IV.D, Q1] "A consistent degradation is observed when visual information is introduced alongside text"; [section IV.C, L2 results] "Text-only models consistently outperform their vision-based counterparts"; [corpus] Limited direct corroboration.

### Mechanism 3
- Claim: Surface-level fluency masks epistemic grounding failures, creating an 80% factuality gap in generated content.
- Mechanism: LLMs optimize for coherent generation rather than verifiable truth. In L4 literature review tasks, models achieve 88-99% content quality scores but only 17-22% citation verifiability. This occurs because: (1) models generate plausible-sounding but fabricated citations (81% of unverifiable references), (2) even real citations suffer 97% metadata inaccuracy, and (3) 89% of claims diverge from actual source content.
- Core assumption: Content quality and factual grounding are separable dimensions; high fluency does not imply high accuracy.
- Evidence anchors: [section IV.D, Q2-Q3] "striking disparity between self-assessed content quality and citation reliability... severe 80% factuality gap"; [section IV.C, L4 results] "Citation fabrication accounts for 81% of unverifiable references... Faithfulness deficit produces an 89% unfaithfulness rate"; [corpus] FlowSearch and related agent systems emphasize retrieval augmentation.

## Foundational Learning

- **Bloom's Taxonomy of Cognitive Domains**
  - Why needed here: The entire L1-L5 hierarchy maps to Bloom's cognitive levels (Remember → Understand → Apply → Analyze → Evaluate → Create). Understanding why models fail at L5 requires recognizing that discovery demands synthesis + evaluation + creation, not just recall.
  - Quick check question: Can you explain why a model achieving 84% on mathematics literacy (L1) might still fail at scientific discovery (L5) involving mathematical reasoning?

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The 17-22% citation verifiability for general LLMs versus SurveyX's 71.4% directly illustrates why grounding external knowledge matters. Understanding this gap requires knowing how RAG architectures differ from pure generation.
  - Quick check question: Why does SurveyX achieve 3.5× higher citation verifiability than GPT-5 despite lower content scores in some dimensions?

- **Cross-Modal Grounding**
  - Why needed here: The 12-17 BLEU degradation when adding visual inputs to translation tasks reflects fundamental multimodal alignment challenges. Debugging this requires understanding how vision-language models bind visual features to semantic concepts.
  - Quick check question: If a model correctly extracts text from a scientific figure but mistranslates a formula, is the failure in OCR, semantic parsing, or cross-lingual transfer?

## Architecture Onboarding

- **Component map:** L1 (Scientific Literacy) → L2 (Literature Parsing) → L3 (Literature QA) → L4 (Review Generation) → L5 (Discovery)
- **Critical path:** Model receives input (text, image, or both) based on task level → Vision-language models process PDF/diagram → OCR/parsing → reasoning → For L4: Model generates review → content evaluation → citation verification → For L5: Model generates code → execution → output validation → Metrics aggregated by level, discipline, and modality
- **Design tradeoffs:** Coverage vs. depth (L3 dominates dataset 63% to emphasize literature reasoning; L4/L5 remain small 0.7%/0.8% due to annotation cost); Automated vs. human evaluation (L4 uses LLM-as-Judge for scalability but risks same overconfidence it measures); Multimodal ambition vs. current capability (including vision tasks exposes text-vs-vision performance gap, useful for diagnosis)
- **Failure signatures:** Modality degradation (BLEU drops 12-17 points when switching from text to vision input); Citation hallucination (81% of general-model citations are fabricated; 97% have metadata errors); Incomplete solutions (L5: Code executes but produces wrong output structure); Reasoning-logic mismatch (L5: Syntactically correct code with semantic errors)
- **First 3 experiments:** Baseline text-only evaluation (Run DeepSeek-R1 or Qwen3-32B on L1-L3 text tasks to establish non-multimodal performance ceiling); Citation grounding intervention (Implement RAG for L4 using Semantic Scholar API; compare verifiability rates against baseline GPT-5); Cross-modal error analysis (For L2.1 OCR failures on mathematics papers, isolate whether errors stem from formula rendering, symbol recognition, or layout understanding)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the "factuality gap" between high content quality and low citation verifiability in literature synthesis be effectively closed?
- **Basis in paper:** [explicit] The authors explicitly identify an "80% factuality gap" in Section IV-D (Q2), where models achieve ~99% content quality scores but only 17–22% citation verifiability.
- **Why unresolved:** General-purpose LLMs prioritize surface-level fluency and coherence over epistemic grounding, leading to fabricated or unfaithful references that current training objectives do not penalize sufficiently.
- **What evidence would resolve it:** The development of models that achieve greater than 80% citation verifiability on L4 tasks while maintaining high content quality, potentially via retrieval-augmented architectures.

### Open Question 2
- **Question:** Why does the introduction of visual context in current multimodal models degrade semantic translation fidelity compared to text-only inputs?
- **Basis in paper:** [explicit] Section IV-D (Q1) and Figure 4 show that GPT-5’s performance on cross-lingual translation drops by 12–17 BLEU points when switching from text-only to vision-language inputs.
- **Why unresolved:** This suggests that current fusion mechanisms fail to align visual features with textual semantics, causing visual input to act as noise rather than complementary context.
- **What evidence would resolve it:** Identifying training strategies or architectures where vision-language models consistently outperform text-only baselines on document-level translation tasks.

### Open Question 3
- **Question:** How can models be improved to align generated code with high-level scientific intent to prevent "logically correct but scientifically wrong" solutions?
- **Basis in paper:** [explicit] Section IV-D (Q4) details failure modes in L5 where models produce syntactically valid code that executes the wrong logic (e.g., reducing along the wrong tensor axis).
- **Why unresolved:** Models lack robust spatiotemporal reasoning and conceptual synthesis capabilities, creating a disconnect between understanding a scientific goal and implementing the correct computational workflow.
- **What evidence would resolve it:** A significant increase in L5 success rates (currently ~25%) driven by improvements in semantic understanding rather than just coding proficiency.

## Limitations
- The 8,735-instance size may not fully capture scientific domain complexity, particularly in L4/L5 where annotation costs limited sample sizes
- Multimodal evaluation relies on current vision-language model capabilities that may improve rapidly, potentially changing the text-vs-vision degradation patterns observed
- LLM-as-Judge evaluation for L4 content quality introduces potential circularity, as the same model families being evaluated may also serve as judges

## Confidence
- **High Confidence:** Hierarchical performance degradation patterns (L1 84% → L5 25%) and citation hallucination statistics (81% fabricated citations, 97% metadata errors) are directly measurable and robust across tested models
- **Medium Confidence:** Cross-modal interference mechanism is well-supported but may be architecture-dependent; improvements in vision-language pretraining could mitigate the observed degradation
- **Medium Confidence:** The 80% factuality gap assumes LLM-as-Judge accuracy; alternative human evaluation might yield different content quality assessments, though citation verifiability remains independently verifiable

## Next Checks
1. **Hierarchical Dependency Validation:** Test whether models achieving high L5 scores maintain corresponding L1-L3 performance. If successful L5 solutions emerge without strong L1-L3 foundations, the cognitive dependency assumption requires revision.

2. **RAG Grounding Intervention:** Implement SurveyX-style retrieval augmentation for L4 and measure citation verifiability improvement. This directly tests whether the 17-22% verifiability gap is architecture-limited rather than fundamental.

3. **Cross-Modal Architecture Analysis:** Compare vision-language model performance on L2.1 (OCR) versus L2.2 (translation) to isolate whether degradation stems from formula recognition, layout parsing, or semantic binding. This determines whether current multimodal failures are solvable through better pretraining or indicate deeper architectural constraints.