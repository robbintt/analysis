---
ver: rpa2
title: Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual
  Projector
arxiv_id: '2510.12287'
source_url: https://arxiv.org/abs/2510.12287
tags:
- logos
- text
- hallucination
- projector
- logo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Logo hallucination occurs when VLMs output brand names from symbol-only
  logos lacking visible text, revealing that these models rely on learned symbolic
  priors rather than genuine glyph perception. To investigate, the authors systematically
  categorize logos (pure symbol, hybrid, pure text), apply nine structured perturbations,
  and perform projector-level embedding ablation on LLaVA.
---

# Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector

## Quick Facts
- arXiv ID: 2510.12287
- Source URL: https://arxiv.org/abs/2510.12287
- Authors: Sifan Li; Hongkai Chen; Yujun Cai; Qingwen Ye; Liyang Chen; Junsong Yuan; Yiwei Wang
- Reference count: 10
- One-line primary result: VLMs hallucinate brand names from symbol-only logos due to semantic entanglement in visual projector embeddings, with targeted ablation reducing errors by ~30% while preserving OCR accuracy.

## Executive Summary
Vision language models (VLMs) frequently output brand names when shown symbol-only logos, revealing a fundamental limitation in their multimodal reasoning. Through systematic experimentation across four leading VLMs, this work demonstrates that such hallucinations arise from semantic entanglement in the visual projector, where symbolic visual features are mapped to text-like embeddings. The authors show that hallucination is robust to most visual perturbations but highly sensitive to occlusion, and that targeted ablation of a small projector subspace can significantly reduce these errors while preserving genuine OCR capabilities.

## Method Summary
The authors systematically investigated logo hallucination across four VLMs using LogoDet-3K dataset with curated splits (pure symbol, hybrid, pure text) and nine structured perturbations. They performed projector-level embedding ablation on LLaVA-1.6, extracting mean-pooled projector outputs and fitting L1-regularized logistic regression probes to identify hallucination-predictive dimensions. The top-32 dimensions were targeted for ablation at inference, reducing hallucination by ~30% while maintaining OCR accuracy. The study employed Exact text accuracy (Acctext) on pure-text logos and hallucination rate (Hall) on pure-symbol logos as primary metrics.

## Key Results
- VLMs consistently hallucinate brand names from pure-symbol logos lacking visible text across four leading models
- Hallucination persists under most distortions (blur, flips, rotations, inversion, sharpening) but drops sharply with occlusion
- Targeted ablation of 32 projector dimensions reduces hallucination by 29.6% while preserving OCR accuracy (3.2% drop)
- Circular logos show highest hallucination rates; irregular logos show lowest

## Why This Works (Mechanism)

### Mechanism 1: Semantic Entanglement in Projector Embeddings
- Claim: Logo hallucination occurs because visual projectors map symbolic visual features into text-like embedding space, causing models to conflate brand symbols with written text.
- Mechanism: During training, the projector learns to align logo visuals with their textual name embeddings in the LLM's space. At inference, even text-free logos activate text-recognition pathways because their visual features are mapped close to brand-name embeddings.
- Core assumption: The projector creates shortcuts that bypass genuine glyph perception, treating iconic symbols as "textual" based on learned associations.
- Evidence anchors:
  - [abstract] "VLMs often rely on symbolic priors rather than genuine glyph perception, particularly for iconic circular logos"
  - [section 3.3] Describes projector mapping Z = Proj(Vision(image)) where visual tokens are projected into LLM embedding space
  - [corpus] Related work (BASIC, Attention Guided Alignment) confirms visual-text alignment challenges in projector designs
- Break condition: If models performed true OCR, they would not output brand names for pure-symbol logos lacking any visible text.

### Mechanism 2: Low-Dimensional Hallucination Subspace
- Claim: Hallucination is concentrated in a small subset of projector dimensions, not distributed across the full embedding space.
- Mechanism: Sparse logistic regression probes identify specific dimensions (k=32 out of d dimensions) that strongly predict hallucination. Targeted ablation of these dimensions reduces hallucination by ~30% while preserving OCR accuracy, whereas random ablation has negligible effect.
- Core assumption: Hallucination arises from specific embedding directions that encode spurious symbol-to-text mappings.
- Evidence anchors:
  - [abstract] "Embedding analysis shows hallucination is localized to a small projector subspace; targeted ablation reduces errors by ~30%"
  - [section 4.3] Shows targeted ablation reduces hallucination by 29.6% while accuracy drops only 3.2%; random placebo produces <1% change
  - [corpus] VIB-Probe work on detecting hallucinations via information bottleneck is conceptually related but does not address subspace localization directly
- Break condition: If hallucination were uniformly distributed across projector dimensions, targeted dimension ablation would not produce selective effects.

### Mechanism 3: Prior-Based Recognition Resilience
- Claim: Hallucinations persist under most visual perturbations because models rely on learned brand priors rather than pixel-level glyph analysis.
- Mechanism: Logo recognition triggers through high-level symbolic associations (especially for circular/iconic logos). Most perturbations preserve global structure, so priors remain activated. Occlusion disrupts this by removing key identifying regions.
- Core assumption: VLMs recognize logos through holistic shape priors rather than character-by-character reading.
- Evidence anchors:
  - [abstract] "hallucinations persist even under strong distortions, with occlusion exposing the sharpest weaknesses"
  - [section 4.2] Occlusion accounts for largest share of failures across all models; other perturbations cause uniform, smaller drops
  - [corpus] Related work on visual object hallucination (Comprehensive Analysis) documents similar robustness to perturbations
- Break condition: If models relied on genuine glyph perception, text-like perturbations would differentially affect text vs. symbol logos.

## Foundational Learning

- Concept: **Visual Projector Function in VLMs**
  - Why needed here: The projector maps vision encoder outputs into LLM token embedding space; this is where semantic entanglement occurs.
  - Quick check question: Can you explain why a visual projector must output text-like embeddings for the LLM to process them?

- Concept: **Embedding Space Geometry and Subspace Operations**
  - Why needed here: Understanding how ablation removes specific dimensions from projector outputs requires knowing what embedding dimensions represent.
  - Quick check question: What does it mean geometrically when we "zero out" specific dimensions in an embedding vector?

- Concept: **Hallucination vs. Grounding in Multimodal Models**
  - Why needed here: The paper defines hallucination as outputs not grounded in visual evidence; distinguishing prior-based inference from perception is central.
  - Quick check question: When a model outputs "Nike" for a swoosh logo with no text, is this hallucination or correct recognition? Why does the distinction matter?

## Architecture Onboarding

- Component map: Vision Encoder → Visual Projector → LLM Backbone
- Critical path: Vision Encoder → Projector (embedding entanglement occurs here) → Pooled representation (z̄) → LLM decoding (hallucinated brand names emerge here)
- Design tradeoffs:
  - Strong visual-text alignment enables multimodal reasoning but creates symbol-to-text conflation risk
  - Dense projector embeddings improve general capabilities but entangle modalities; sparse/disentangled designs may reduce hallucination but could hurt overall performance
  - OCR-guided decoding could gate text outputs but requires explicit text detection as precondition
- Failure signatures:
  - Model outputs brand names for pure-symbol logos (no visible text present)
  - Hallucination persists under prompt engineering ("extract only visible text") and constrained decoding
  - Circular logos show elevated hallucination rates; irregular logos show lowest rates
  - Occlusion causes disproportionate failure vs. other perturbations
- First 3 experiments:
  1. Replicate subspace identification: Train sparse logistic regression probe on LLaVA-1.6 projector outputs using pure-symbol logos; verify top-k dimensions predict hallucination.
  2. Ablation validation: Zero out identified dimensions (k=32) at inference; measure hallucination reduction on held-out symbol logos and OCR accuracy preservation on text logos.
  3. Occlusion sensitivity analysis: Apply graduated occlusion masks (10%, 20%, 30% area) to circular vs. irregular logos; test whether hallucination drops correlate with shape category as predicted.

## Open Questions the Paper Calls Out

- Can projector architectures be explicitly designed or regularized to disentangle symbolic visual features from text-triggering embeddings while preserving genuine OCR capabilities?
- Can OCR-guided decoding mechanisms effectively gate brand-name generation by requiring character-level verification before emitting textual tokens?
- Do the projector-level hallucination subspaces identified in logo analysis generalize to natural scenes, documents, and video?
- Do VLMs systematically hallucinate emotional or value associations (luxury, affordability, sophistication) from visual symbols in addition to textual content?

## Limitations
- Study limited to LLaVA-1.6 13B architecture; generalization to other projector designs (e.g., Q-Former) unverified
- Symbolic priors mechanism assumes causal link between projector embeddings and hallucination without ruling out alternative explanations
- Dimension count (k=32) appears arbitrary without sensitivity analysis or clear selection criteria
- Does not explore whether robustness to perturbations stems from model architecture or logo dataset distribution

## Confidence
- Confidence is **Medium** for core claim of projector entanglement: ablation evidence is compelling but limited to single architecture
- Confidence is **High** for perturbation robustness finding: experimental design is rigorous and occlusion effect is consistent
- Confidence is **Medium** for subspace localization claim: selective ablation effects observed but dimension selection lacks sensitivity analysis

## Next Checks
1. **Architectural Generalization Test**: Replicate ablation experiment on at least two other VLM architectures (e.g., CLIP-based models, BLIP-2) to verify hallucination subspace localization holds across projector designs.
2. **Temporal Stability Analysis**: Track hallucination rates and subspace composition across different model checkpoints during training to reveal whether projector entanglement emerges gradually or is a fixed artifact.
3. **Cross-Domain Symbol Recognition**: Test pure-symbol logos across domains (e.g., UI mockups, standalone images) to determine if hallucination is triggered by visual context or intrinsic symbol properties.