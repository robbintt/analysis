---
ver: rpa2
title: Analysis of heart failure patient trajectories using sequence modeling
arxiv_id: '2511.16839'
source_url: https://arxiv.org/abs/2511.16839
tags:
- clinical
- after
- one-year
- initial
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A systematic ablation study was conducted to empirically evaluate
  the performance of six sequence models (BERT, XLNet, ModernBERT, Llama, Mamba, Mamba2)
  across three one-year prediction tasks (clinical instability, mortality after initial
  HF hospitalization, mortality after latest hospitalization) using a large Swedish
  heart failure cohort (N=42,820). Models were compared across various settings including
  input tokenization, model architecture configurations (context length and model
  size), temporal preprocessing techniques, and data availability.
---

# Analysis of heart failure patient trajectories using sequence modeling

## Quick Facts
- arXiv ID: 2511.16839
- Source URL: https://arxiv.org/abs/2511.16839
- Reference count: 40
- Primary result: Llama consistently outperformed Transformers and Mamba models across three heart failure prediction tasks, achieving superior performance with 25% less training data and shorter context lengths.

## Executive Summary
This systematic ablation study evaluates six sequence models (BERT, XLNet, ModernBERT, Llama, Mamba, Mamba2) for predicting clinical outcomes in heart failure patients using Swedish EHR data. The study tests model performance across three one-year prediction tasks: clinical instability after initial HF hospitalization, mortality after initial HF hospitalization, and mortality after latest hospitalization. Llama emerges as the top-performing architecture, consistently achieving the highest AUPRC and best calibration while requiring less training data than alternatives. The analysis reveals that extended context length drives performance gains more effectively than increased model complexity, with Llama-Small at C=512 being the recommended configuration.

## Method Summary
The study uses a Swedish heart failure cohort (N=42,820) with structured EHR data including diagnoses (ICD-10), vital signs, laboratories, medications (ATC), procedures (KVÅ), and demographics. Patient sequences are tokenized using ICD-10 level 3 codes with 10-bin discretization for continuous variables. Six sequence models are evaluated: three Transformer-based (BERT, XLNet, ModernBERT) and three modern architectures (Llama, Mamba, Mamba2). Models are pre-trained on either masked language modeling or next-token prediction objectives, then fine-tuned on binary classification tasks. The study conducts extensive ablation experiments varying context length (128-1024), model size (Small/Medium), temporal preprocessing, and data availability.

## Key Results
- Llama consistently achieved the highest predictive discrimination (AUPRC) across all tasks and ablation conditions
- Mamba-based models outperformed traditional Transformers, particularly at longer contexts
- Llama and Mamba achieved superior performance using 25% less training data than other models
- Small-sized models (Llama-Small) often matched or exceeded medium-sized performance with reduced overfitting risk
- Extended context length (C=512) provided greater performance gains than increasing model complexity

## Why This Works (Mechanism)

### Mechanism 1: Extended Context Length Drives Performance
Extended context length (C=512) captures more patient history tokens, enabling better contextualization of clinical events. The study shows small-sized Llama at C=512 outperforms medium-sized models at shorter contexts, indicating that data quantity (more tokens) matters more than parameter count for this domain.

### Mechanism 2: Next-Token Prediction Superiority
NTP objectives (used by Llama, Mamba) process tokens autoregressively, learning sequential dependencies that align with temporal patient trajectories. MLM (used by BERT) masks random tokens bidirectionally, which may disrupt the causal structure of clinical events.

### Mechanism 3: Architectural Efficiency Differences
Llama and Mamba incorporate design improvements (RoPE embeddings, pre-normalization, RMSNorm, gating mechanisms) that enable more efficient gradient flow and parameter utilization compared to traditional Transformers.

## Foundational Learning

- **State Space Models (SSMs) vs. Attention Mechanisms**: Mamba uses SSMs with linear complexity O(n); Transformers use attention with quadratic O(n²). Understanding this explains why Mamba was expected to excel at long contexts but Llama won at short ones.
  - Quick check: Can you explain why quadratic attention costs might be acceptable at C≤512 but problematic at C≥4096?

- **Pre-training Objectives for Sequential Data**: The paper compares MLM (random token masking) vs. NTP (next-token prediction). This distinction matters because clinical events have inherent temporal structure.
  - Quick check: Why would predicting the next token in a patient sequence differ fundamentally from predicting a randomly masked token?

- **Tokenization Granularity for Clinical Codes**: The paper tests vocabulary sizes by varying ICD-10 code levels (i=3 vs. i=4) and discretization bins (b=5 vs. b=10). Finer isn't always better—it depends on trajectory position.
  - Quick check: What information might be lost when using ICD-10 category level (I50) vs. diagnosis level (I509)?

## Architecture Onboarding

- **Component map**: Input layer (tokenized sequences) -> Backbone (Llama/Mamba/Transformer) -> Classification head (2-layer feedforward) -> Output (binary prediction)

- **Critical path**: 1) Extract EHR data → tokenize with Vb=10, i=3 vocabulary 2) Construct patient sequences → pad/truncate to C=512 3) Pre-train Llama-Small on NTP objective 4) Fine-tune on clinical task

- **Design tradeoffs**: C=512 vs. C=1024 (longer helps mortality prediction but increases cost); Small vs. Medium (Small often matches Medium with less overfitting); Aggregation vs. truncation (aggregation preserves more history)

- **Failure signatures**: Large vocabulary (Vb=10, i=4) underperforms (likely overfitting); Including PRO degrades performance (KVÅ limitations); Medium models at C=256 show negative gains over Small (overfitting indicator)

- **First 3 experiments**: 1) Train Llama-Small with C=512, Vb=10, i=3 on all tasks; verify AUPRC ranges 2) Compare Llama-Small at C=128, 256, 512; confirm performance scales with context 3) Train Llama on 75% data with all concepts vs. 100% data with DX+VIT+LAB only; verify 75%/all-concepts wins

## Open Questions the Paper Calls Out

- Will the superior performance of Llama and Mamba models persist when applied to geographically diverse cohorts or non-heart failure disease populations? (lacks external validation)

- Does the Mamba architecture surpass Llama in predictive discrimination or efficiency when processing patient histories significantly longer than 512 tokens? (long context lengths not fully evaluated)

- Can the fusion of unstructured clinical text or cardiac imaging data with structured EHR tokens improve the predictive performance of these sequence models? (future work should focus on multi-modal fusion)

## Limitations

- Single-cohort study restricted to Swedish HF patients without external validation
- Absence of clinical notes and unstructured data limits real-world applicability
- Modest sample size (N=42,820) and three-task scope constrain generalizability
- No ablation studies on pretraining dataset size to verify efficiency claims
- Limited exploration of clinical interpretability and explainability

## Confidence

- **High confidence**: Llama and Mamba consistently outperform Transformers; extended context length provides more benefit than model complexity; NTP outperforms MLM; Llama-Small at C=512 is recommended configuration
- **Medium confidence**: 25% data efficiency claims may not generalize to other domains; i=3 granularity superiority needs validation across coding systems; architectural differences at short contexts may shift at longer contexts
- **Low confidence**: Exact bin boundaries for discretization not specified; clinical significance of AUPRC gains not established; efficiency claims weakly supported by corpus evidence

## Next Checks

1. **Vocabulary sensitivity analysis**: Replicate using alternative granularities (i=4 for ICD-10, different bin counts) to verify i=3 superiority across coding systems

2. **Pretraining scale validation**: Train on progressively smaller pretraining datasets (25%, 50%, 75%, 100%) to empirically verify 25% data efficiency advantage

3. **Clinical interpretability assessment**: Compare attention patterns or state-space activations across architectures and validate against clinical domain expertise for explainability advantages