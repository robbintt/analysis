---
ver: rpa2
title: LLMs Can Teach Themselves to Better Predict the Future
arxiv_id: '2502.05253'
source_url: https://arxiv.org/abs/2502.05253
tags:
- arxiv
- forecasting
- base
- phi-4
- deepseek-r1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a self-play based fine-tuning method that
  improves LLM forecasting accuracy by 7-10% without human-curated data. The approach
  generates reasoning traces via model self-play, ranks them by proximity to actual
  outcomes, and applies Direct Preference Optimization (DPO) to refine forecasting
  capabilities.
---

# LLMs Can Teach Themselves to Better Predict the Future

## Quick Facts
- arXiv ID: 2502.05253
- Source URL: https://arxiv.org/abs/2502.05253
- Reference count: 27
- Primary result: Self-play fine-tuning improves LLM forecasting accuracy by 7-10% using outcome-based ranking

## Executive Summary
This paper introduces a novel self-play based fine-tuning method that enables large language models to improve their forecasting accuracy without requiring human-curated data. The approach generates reasoning traces through model self-play, ranks them based on proximity to actual outcomes, and applies Direct Preference Optimization (DPO) to refine the model's predictive capabilities. Tested on Phi-4 14B and DeepSeek-R1 14B models, the method achieved Brier scores of 0.200 and 0.197 respectively, matching the performance of much larger GPT-4o models.

The key innovation lies in leveraging outcome-based rankings rather than traditional supervised learning or human preference data. By generating synthetic reasoning traces and using the actual outcomes to rank them, the method creates a self-supervised learning loop that improves forecasting performance. The approach is particularly valuable because it doesn't require expensive human annotation while still achieving competitive results against state-of-the-art models.

## Method Summary
The method employs a self-play mechanism where the LLM generates multiple reasoning traces for forecasting questions, then ranks these traces based on their proximity to actual outcomes. The top-ranked traces are used to fine-tune the model using Direct Preference Optimization (DPO), creating a feedback loop that improves forecasting accuracy over time. The approach generates diverse reasoning paths without human intervention, using outcome-based ranking as the preference signal rather than human judgments. The method was tested on multiple datasets including AGORA, LiveBench, and MMLU, comparing performance against both base models and control groups with randomized labels.

## Key Results
- Achieved mean Brier scores of 0.200 (Phi-4 14B) and 0.197 (DeepSeek-R1 14B)
- Performance matched GPT-4o despite using 14B parameter models versus GPT-4o's much larger architecture
- Statistically significant 7-10% improvement over base models and control groups with randomized labels
- Demonstrated effectiveness across multiple forecasting benchmarks (AGORA, LiveBench, MMLU)

## Why This Works (Mechanism)
The method works by creating a self-supervised learning loop where the model generates its own training data through reasoning traces, then uses actual outcomes to rank these traces and extract preference signals. This approach leverages the model's existing reasoning capabilities while using outcome-based feedback to refine prediction accuracy. The DPO fine-tuning process aligns the model's probability distributions with the ranked preferences, effectively teaching the model to generate reasoning that leads to more accurate predictions.

## Foundational Learning
- **Direct Preference Optimization (DPO)**: A fine-tuning method that optimizes model parameters based on pairwise preference data, crucial for incorporating the outcome-based rankings into the model's learning process
- **Self-play in LLMs**: A technique where models generate synthetic data through interaction with themselves, enabling self-supervised learning without human annotation
- **Brier Score**: A proper scoring rule for probabilistic forecasts that measures the mean squared difference between predicted probabilities and actual outcomes, used to evaluate forecasting accuracy
- **Reasoning trace generation**: The process of creating step-by-step logical paths for predictions, which provides the raw material for outcome-based ranking
- **Outcome-based ranking**: Using actual results to rank model-generated predictions, creating a ground truth signal for preference learning
- **Self-supervised learning**: Learning from automatically generated labels rather than human annotation, essential for scaling the method without manual effort

## Architecture Onboarding

**Component map**: Data Generator -> Reasoning Trace Generator -> Outcome Comparator -> Ranker -> DPO Optimizer -> Fine-tuned Model

**Critical path**: The method critically depends on the quality of reasoning trace generation and the accuracy of outcome-based ranking. Poor trace generation leads to incoherent reasoning paths that cannot be effectively ranked, while inaccurate ranking signals result in suboptimal preference learning.

**Design tradeoffs**: The approach trades model diversity for computational efficiency by generating multiple traces per question rather than using more sophisticated search strategies. This choice prioritizes scalability over potentially finding optimal reasoning paths.

**Failure signatures**: 
- Inconsistent reasoning traces that don't follow logical progression
- Ranking failures when multiple traces are equally plausible
- Overfitting to the self-play data distribution
- Degradation in performance on questions outside the training distribution

**First experiments**:
1. Generate reasoning traces for simple binary forecasting questions to validate trace coherence
2. Test outcome-based ranking with synthetic data where ground truth is known
3. Evaluate DPO fine-tuning impact on a small subset before full-scale training

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims compared to GPT-4o are misleading due to different model architectures and scales
- Method relies heavily on generating coherent reasoning traces, which may not scale well to complex forecasting tasks
- Evaluation focuses on specific benchmark datasets, limiting conclusions about real-world applications
- Doesn't address overfitting risks to the self-play data distribution
- Assumes ground truth availability for outcome-based ranking, which isn't always practical

## Confidence
- Claims about 7-10% Brier score improvements: Medium
- Claims about matching GPT-4o performance: Low
- Claims about method generalizability: Medium

## Next Checks
1. Test the method on longer-horizon forecasting tasks (3+ months) to assess temporal generalization limits
2. Evaluate performance when only partial outcomes are available or when multiple plausible scenarios exist
3. Implement ablation studies removing the reasoning trace generation step to isolate the contribution of outcome-based ranking versus additional information