---
ver: rpa2
title: Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought
  Correctness Perception Distillation
arxiv_id: '2509.05602'
source_url: https://arxiv.org/abs/2509.05602
tags:
- answer
- rationale
- reasoning
- student
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation

## Quick Facts
- **arXiv ID:** 2509.05602
- **Source URL:** https://arxiv.org/abs/2509.05602
- **Reference count:** 40
- **Primary result:** CoPeD achieves strong performance on BIG-Bench Hard with improved faithfulness and soundness of rationales.

## Executive Summary
This paper addresses the problem of spurious correlations in chain-of-thought distillation from large language models (LLMs) to small language models (SLMs). The authors propose CoPeD (Correctness Perception Distillation), a framework that trains SLMs to predict answers based on correct rationales and revise them when incorrect. The method uses a dual-task training setup and a correctness-aware weighted loss to mitigate the impact of noisy teacher-generated data. Experiments on BIG-Bench Hard and other datasets show that CoPeD improves both in-distribution and out-of-distribution generalization while enhancing the faithfulness and soundness of generated rationales.

## Method Summary
The method trains a student SLM using a dual-task framework: answer prediction with correct rationales and rationale correction for erroneous ones. It employs a combined loss function with correctness-aware weighting, where training samples are dynamically weighted based on the combined loss of rationale generation and answer prediction. The approach uses LoRA fine-tuning on LLaMA2-7B, with a warm-up period before applying weighted loss. The framework aims to force the student model to condition its predictions on the reasoning path rather than superficial question-answer correlations.

## Key Results
- CoPeD achieves state-of-the-art performance on BIG-Bench Hard reasoning tasks.
- The method improves both faithfulness and soundness of generated rationales compared to baseline approaches.
- CoPeD demonstrates strong out-of-distribution generalization on BB-sub, AGIEval, ARC-E, and ARC-C datasets.
- The dual-task training setup and correctness-aware weighted loss work synergistically to improve performance.

## Why This Works (Mechanism)

### Mechanism 1: Dual-Task Training for Causal Decoupling (CoPeD-T)
- Claim: Multi-task training that separates answer prediction from rationale correction forces the student to condition predictions on reasoning paths, breaking spurious correlations.
- Mechanism: Model trained on two conditional tasks using status tokens: `P(A|Q, R_correct)` when rationale correct, `P(R_correct|Q, R_erroneous)` when rationale incorrect. This compels the model to treat rationale as causal mediator.
- Core assumptions: Status tokens are strong signals for task distinction; student model can learn both generation and revision without catastrophic interference.
- Evidence anchors: Abstract states model "predicts answers based on correct rationales and revise them when they are incorrect." Section 2.2 notes framework "encourages the model to rely on valid reasoning paths... rather than superficial question–answer correlations."
- Break condition: Benefits collapse if status tokens are ignored or model relies solely on question Q for both tasks.

### Mechanism 2: Confidence-Weighted Loss for Noise Robustness (CoPeD-L)
- Claim: Dynamically re-weighting samples based on rationale quality and alignment mitigates noise from imperfect teacher data.
- Mechanism: Confidence weight w_i computed from combined score of overall difficulty and misalignment, where difficulty ≈ L_r + L_a and misalignment ≈ |L_r - L_a|. Final loss is Σ w_i * (L_r + L_a).
- Core assumptions: Loss values L_r and L_a are reliable proxies for rationale quality and alignment; warm-up period with uniform weights is necessary before reliable weighting.
- Evidence anchors: Abstract notes "dynamically adjusts the contribution of each training instance based on the combined loss of the rationale and the answer." Section 2.3 defines correctness-aware weighted loss with w_i calculation.
- Break condition: Mechanism fails if correct but complex rationale has high initial loss and is incorrectly down-weighted, or if confidently incorrect rationale is erroneously up-weighted.

### Mechanism 3: Synergistic Regularization (CoPeD-TL)
- Claim: Combination of dual-task setup and weighted loss provides synergistic regularization improving both IND and OOD generalization.
- Mechanism: Dual-task setup defines valid reasoning processes while weighted loss acts as dynamic filter prioritizing high-quality examples. Synergy effective for OOD generalization by encouraging robust, transferable reasoning patterns.
- Core assumptions: Two mechanisms are complementary without conflicting gradients; learned reasoning patterns are fundamental enough to generalize to OOD tasks.
- Evidence anchors: Table 1 shows CoPeD-TL consistently outperforms both CoPeD-T and CoPeD-L across all datasets, often by significant margin (e.g., +6.7% over CoPeD-T on BBH-test). Figure 5 shows strong OOD performance even with reduced training data.
- Break condition: Synergy may not hold if OOD data noise is fundamentally different from IND noise model was tuned on.

## Foundational Learning

- **Concept: Spurious Correlation (Shortcut Learning)**
  - Why needed here: Core problem addressed - models can achieve high accuracy by learning shallow mappings from question keywords to answers without underlying causal logic.
  - Quick check question: A model trained on VQA task achieves 95% accuracy but fails completely when dominant object in image is changed. What likely happened?

- **Concept: Faithfulness vs. Soundness**
  - Why needed here: Paper's key evaluation metrics. Faithfulness measures if generated rationale supports model's predicted answer. Soundness measures if rationale supports ground-truth answer. Improving both is goal.
  - Quick check question: Model predicts "A" but generates rationale for "B". Is this rationale faithful? Is it sound?

- **Concept: Chain-of-Thought (CoT) Distillation**
  - Why needed here: Paper operates within this paradigm - not just teaching student answer but teaching reasoning process (CoT) generated by teacher model. Understanding this distinction vital.
  - Quick check question: In standard fine-tuning, you train on (input, output). In CoT distillation, what is additional component included in training data?

## Architecture Onboarding

- **Component map:**
  Teacher LLM -> Data Processor -> Student SLM -> Multi-Task Loss -> Confidence Weighting Module -> Weighted Loss Optimizer

- **Critical path:**
  1. Data Generation: Use teacher LLM to create raw dataset. Quality of this data is major bottleneck.
  2. Dual-Task Construction: Format data with status tokens (rst, rsf) to create training set for both tasks.
  3. Warm-up Training: Train for n epochs with uniform weights (w_i=1) to establish baseline reasoning capability.
  4. Weighted Fine-Tuning: Enable confidence-weighted loss to refine model on high-quality, aligned examples.

- **Design tradeoffs:**
  1. Heuristic Labeling Quality: Entire method hinges on "correct answer = correct rationale" assumption. Known source of error. Weighted loss is mitigation, not fix.
  2. Task Balance (α): Paper finds α=0.5 (equal weight) best, but may need tuning. Higher α focuses more on correction, might improve reasoning quality at expense of answer accuracy.
  3. Temperature (τ): τ parameter in softmax weighting controls how sharply model focuses on low-loss samples. Critical hyperparameter to tune.

- **Failure signatures:**
  1. Incoherent Rationale: Model generates rationale that is grammatical but logically irrelevant to question, indicating failure to learn causal structure.
  2. Overfitting to Rationale Format: Model learns to generate "Therefore, the answer is..." without valid preceding argument.
  3. No Improvement from Weighting: If performance with weighted loss same or worse than uniform weighting, suggests loss values are not good proxies for quality.

- **First 3 experiments:**
  1. Ablation on Task Balance (α): Train with α ∈ [0.3, 0.5, 0.7] to find optimal balance between answer prediction and rationale correction for specific student model.
  2. Ablation on Temperature (τ): Sweep τ ∈ [1, 2.5, 5, 10] to find best weighting sharpness. Value too low may cause instability.
  3. OOD Generalization Test: Train on primary dataset (e.g., BBH) and evaluate on clearly OOD dataset (e.g., AGIEval) to confirm method improves generalization, not just memorization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can small language models (SLMs) be enhanced to effectively verify correctness of their own generated rationales during inference?
- Basis in paper: Section 6 (Limitations) states student model "currently struggles to effectively validate whether the rationale derived from its reasoning is indeed correct."
- Why unresolved: Paper's analysis (Appendix D) shows using status token as output does not significantly improve performance, suggesting student lacks internal capacity to distinguish valid reasoning paths from flawed ones autonomously.
- What evidence would resolve it: Method allowing SLMs to self-correct without external teacher intervention, resulting in higher accuracy on verification tasks (e.g., improved performance in "output-correction" setting in Figure 7).

### Open Question 2
- Question: Can correctness-aware weighting mechanism be refined to better capture subtle inconsistencies between rationales and answers?
- Basis in paper: Section 6 notes "current weighting mechanism is relatively simple and could be further refined to better capture subtle inconsistencies."
- Why unresolved: Existing formula relies on simple loss discrepancy (|L_r - L_a|), which may fail to detect semantic misalignment or hallucination when loss values are deceptively low.
- What evidence would resolve it: New weighting function that correlates more strongly with human evaluation of reasoning alignment than current softmax-based loss metric.

### Open Question 3
- Question: How can framework reduce dependence on heuristic correctness labels to minimize noisy supervision?
- Basis in paper: Section 6 acknowledges approach "depends on correctness labels heuristically derived from whether predicted answer matches ground truth," which may introduce noise in edge cases.
- Why unresolved: Correct answer does not guarantee correct rationale, and using answer as proxy forces model to handle potentially misleading training signals.
- What evidence would resolve it: Distillation framework that incorporates independent rationale verification (e.g., using separate verifier model or logical entailment checks) to assign labels, outperforming current accuracy-based proxy.

## Limitations

- The method relies heavily on the heuristic assumption that correct answers imply correct rationales, which can introduce significant noise into the training data.
- The effectiveness of the correctness-aware weighted loss depends on the reliability of loss values as proxies for rationale quality and alignment, which is not directly validated.
- The paper lacks comparison to more direct methods for mitigating spurious correlations such as adversarial training or counterfactual data augmentation.

## Confidence

- **High Confidence:** Identification of spurious correlations as key problem in LLM-to-SLM distillation; validity of dual-task framework as novel approach.
- **Medium Confidence:** Effectiveness of correctness-aware weighted loss for noise mitigation; synergy between dual-task and weighted loss components.
- **Low Confidence:** Robustness to highly noisy data; general superiority for all types of OOD generalization.

## Next Checks

1. **Ablation on Data Quality:** Train CoPeD model on subset of data where rationales have been manually verified as correct/incorrect. Compare performance to original model trained on heuristically labeled data. This directly tests impact of labeling assumption.

2. **Comparison to Counterfactual Data:** Generate counterfactual version of BBH test set where key words in questions are replaced (e.g., "sunlight" -> "moonlight"). Evaluate baseline and CoPeD models on this set to measure robustness to specific type of spurious correlation method is designed to address.

3. **Hyperparameter Sensitivity Analysis:** Systematically vary temperature parameter τ and task balance α across wider range than reported. Plot learning curves to identify stability and sensitivity of method to these critical hyperparameters.