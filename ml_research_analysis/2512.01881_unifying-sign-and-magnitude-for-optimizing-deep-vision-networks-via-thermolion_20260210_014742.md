---
ver: rpa2
title: Unifying Sign and Magnitude for Optimizing Deep Vision Networks via ThermoLion
arxiv_id: '2512.01881'
source_url: https://arxiv.org/abs/2512.01881
tags:
- gradient
- optimizer
- noise
- adam
- thermolion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitations of existing deep learning optimizers
  that either rely on gradient magnitude (AdamW) or gradient sign (Lion), which perform
  poorly in different regimes of signal-to-noise ratio (SNR) during training. ThermoLion
  introduces a unified framework that dynamically modulates the update rule based
  on local SNR estimates, interpolating between sign-based exploration and magnitude-based
  exploitation phases.
---

# Unifying Sign and Magnitude for Optimizing Deep Vision Networks via ThermoLion

## Quick Facts
- **arXiv ID:** 2512.01881
- **Source URL:** https://arxiv.org/abs/2512.01881
- **Reference count:** 25
- **One-line primary result:** Achieves 87.98% accuracy on GTSRB (vs. 67.38% for Adam) and 74.60% on CIFAR-10 (vs. 54.60% for Adam) while maintaining only 1% overhead.

## Executive Summary
ThermoLion introduces a unified optimizer that dynamically modulates gradient updates based on local Signal-to-Noise Ratio (SNR), interpolating between sign-based exploration and magnitude-based exploitation. It incorporates a momentum alignment mechanism to accelerate convergence on stable trajectories and couples noise injection to the phase state for implicit annealing. Tested across 12 vision datasets, it significantly outperforms state-of-the-art optimizers while maintaining computational efficiency.

## Method Summary
ThermoLion computes SNR as the ratio of momentum to variance, gates this through tanh to determine phase (Gas/Solid), and blends sign-based and magnitude-based updates accordingly. It boosts step size when momentum and gradient align, and injects noise scaled by the inverse of the phase gate. The method uses standard Adam-style EMA buffers with specific hyperparameters: β1=0.9, β2=0.99, η=1e-3, weight_decay=0.01, c=2.0, α=0.5.

## Key Results
- Achieves 87.98% accuracy on GTSRB (vs. 67.38% for Adam baseline)
- Achieves 74.60% accuracy on CIFAR-10 (vs. 54.60% for Adam baseline)
- Maintains only 1% computational overhead compared to Adam

## Why This Works (Mechanism)

### Mechanism 1: SNR-Gated Phase Transition
Dynamically interpolates between sign-based and magnitude-based updates based on local SNR to prevent noise amplification and precision loss. Uses element-wise SNR ratio mapped to gate via tanh, switching between Gas Phase (low SNR) and Solid Phase (high SNR).

### Mechanism 2: Constructive Interference (Momentum Alignment)
Accelerates updates when historical momentum aligns with current gradient by computing an Alignment Factor that boosts step size when signs match, indicating confident trajectories.

### Mechanism 3: Thermodynamic Noise Injection
Coupling noise injection to local phase state allows exploration in high-entropy regimes and smooth convergence in low-entropy regimes without manual schedule tuning. Noise variance scales with inverse of phase gate.

## Foundational Learning

- **Concept: Signal-to-Noise Ratio (SNR) in Gradient Descent**
  - Why needed here: ThermoLion relies on estimating SNR to drive its core switching logic between sign-based and magnitude-based updates
  - Quick check question: In a mini-batch update, does a large gradient norm always indicate a steep slope, or could it indicate high variance in the data?

- **Concept: Exponential Moving Average (EMA)**
  - Why needed here: The algorithm uses EMA to track momentum and variance, which are critical for the SNR estimate that drives the gating mechanism
  - Quick check question: If β1 = 0.9, roughly how many recent steps is the optimizer averaging over to estimate the momentum?

- **Concept: Quantization (1-bit vs. 32-bit)**
  - Why needed here: The paper frames the trade-off as "Gas Phase" (sign-based/1-bit) vs. "Solid Phase" (magnitude-based/32-bit)
  - Quick check question: What specific piece of information is lost when you apply the sign() function to a gradient vector?

## Architecture Onboarding

- **Component map:** Gradient g_t → SNR Estimator → Phase Gate → Alignment Scaler → Update Composer → Thermal Injector → Parameter Update
- **Critical path:** The calculation of ρ_t and the subsequent tanh projection is the critical path for the control flow, happening element-wise before the final update vector is assembled
- **Design tradeoffs:** Fixed c=2.0 scaling factor to prevent step sizes from shrinking too fast during Solid Phase; adds vector operations (tanh, comparisons) to standard Adam update loop
- **Failure signatures:** Oscillation if λ_t rapidly flips between 0 and 1; divergence in Gas Phase if boost factor is too aggressive; stalling if v_t overestimates variance
- **First 3 experiments:** Phase Visualization (log λ_t distribution across layers), Ablation on Alignment (run with A_t=0 vs full ThermoLion), Stress Test on GTSRB/CIFAR-100 (fixed learning rate to verify implicit annealing)

## Open Questions the Paper Calls Out

- **Open Question 1:** Does ThermoLion maintain advantages on large-scale architectures (Transformers, ResNets) and full datasets, given current experiments use moderate ConvNet and 5,000-sample subset
- **Open Question 2:** How does SNR-gated transition influence sharpness of minima found, and does it inherently promote flat minima
- **Open Question 3:** Can heuristic SNR gating mechanism be formally derived from probabilistic model of gradient field to provide theoretical convergence guarantees
- **Open Question 4:** Is fixed scaling factor c=2.0 universally optimal, or does optimal balance require task-specific tuning in low-SNR regimes

## Limitations
- Lack of ablation studies isolating contribution of each mechanism (SNR gating, momentum alignment, thermal noise injection)
- Claim that alignment factor accelerates convergence lacks direct comparison with and without boost factor
- Thermal noise injection lacks external validation beyond this paper

## Confidence
- **High:** Core SNR-based phase transition mechanism is mathematically sound with consistent empirical improvements
- **Medium:** Momentum alignment mechanism's contribution to convergence speed requires targeted ablation studies
- **Medium:** Thermal noise injection as implicit annealing is theoretically justified but lacks external validation

## Next Checks
1. **Mechanism Isolation Ablation:** Run CIFAR-10 experiments with ThermoLion full, without A_t boost, without thermal noise injection, and baseline Adam to quantify specific contributions
2. **Phase Transition Analysis:** Track distribution of λ_t values across different network layers and training epochs to verify local adaptation and identify oscillation patterns
3. **Robustness Under Noise:** Evaluate ThermoLion on deliberately corrupted CIFAR-10 (varying levels of label/noise injection) to test SNR gating's robustness advantages