---
ver: rpa2
title: Multimodal Machine Translation with Visual Scene Graph Pruning
arxiv_id: '2505.19507'
source_url: https://arxiv.org/abs/2505.19507
tags:
- scene
- visual
- translation
- graph
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of redundant visual information
  in multimodal machine translation (MMT), where excessive visual entities overwhelm
  the text content and introduce noise. The authors propose a novel visual Scene Graph
  Pruning (PSG) approach that leverages language scene graph information to guide
  the pruning of redundant nodes in visual scene graphs, thereby reducing noise in
  downstream translation tasks.
---

# Multimodal Machine Translation with Visual Scene Graph Pruning

## Quick Facts
- arXiv ID: 2505.19507
- Source URL: https://arxiv.org/abs/2505.19507
- Authors: Chenyu Lu; Shiliang Sun; Jing Zhao; Nan Zhang; Tengfei Song; Hao Yang
- Reference count: 31
- Primary result: Achieves 37.11 BLEU on Multi30K English-German benchmark, surpassing previous best by +1.14 BLEU

## Executive Summary
This paper addresses the problem of redundant visual information in multimodal machine translation (MMT), where excessive visual entities overwhelm the text content and introduce noise. The authors propose a novel visual Scene Graph Pruning (PSG) approach that leverages language scene graph information to guide the pruning of redundant nodes in visual scene graphs, thereby reducing noise in downstream translation tasks. The PSG model simultaneously generates visual and language scene graphs to guide the pruning process, effectively bridging the structural heterogeneity between visual and language modalities. Extensive experiments on multilingual datasets (Multi30K, AmbigCaps, and CoMMuTE) demonstrate the effectiveness of PSG, achieving new state-of-the-art results.

## Method Summary
The proposed PSG approach uses a cross-modal attention pruning mechanism that leverages language scene graph information to guide the pruning of redundant nodes in visual scene graphs. The model simultaneously generates visual and language scene graphs using CLIP for entity/relation representation, and applies a Graph Convolutional Network (GCN) to perform message passing between entities. A multi-step KL-constrained pruning loss gradually reduces the number of visual entities from an average of 36 to approximately 3.7, aligning with the density of language scene graphs. The final translation model combines multimodal (L_mmt), pruning (L_prune), and text-only NMT (L_nmt) losses to prevent over-pruning and maintain translation quality.

## Key Results
- Achieves 37.11 BLEU score on Multi30K English-German benchmark, surpassing previous best by +1.14 BLEU and +0.84 METEOR
- Sets new state-of-the-art results on AmbigCaps (48.87 BLEU) and CoMMuTE (34.28 BLEU)
- Shows consistent improvements across multiple language pairs: En-Fr (52.80 BLEU), En-Tr (34.81 BLEU), and Tr-En (40.01 BLEU)
- Ablation studies confirm effectiveness of pruning module and text-only NMT constraint

## Why This Works (Mechanism)
The approach works by addressing the fundamental mismatch between rich visual scene graphs and sparse language scene graphs. Visual scene graphs typically contain many more entities (avg 36) than language scene graphs (avg 3.7), creating noise in translation. By using language scene graph structure to guide visual scene graph pruning, the model effectively filters out redundant visual information while preserving critical visual cues. The cross-modal attention pruning mechanism learns which visual entities are most relevant for translation by comparing visual and language scene graph structures, allowing the model to focus on truly useful visual information rather than being overwhelmed by excessive detail.

## Foundational Learning

**Visual Scene Graphs**: Structured representations of images as graphs with entities (objects, attributes) and relations between them. Needed to provide structured visual input for translation; quick check: verify VSGN extracts ~36 entities per image.

**Language Scene Graphs**: Similar graph structures derived from textual descriptions, capturing entities and their relationships. Needed as pruning guidance for visual scene graphs; quick check: Stanford LSGP should produce ~3.7 entities per caption.

**Graph Convolutional Networks (GCNs)**: Neural networks that operate on graph-structured data through message passing between nodes. Needed to aggregate information across visual scene graph entities; quick check: verify entity embeddings are updated through multiple message passing steps.

**Cross-modal Attention Pruning**: Mechanism that uses attention scores between visual and language entities to determine which visual entities to retain. Needed to bridge structural differences between modalities; quick check: pruning should reduce visual entities from 36 to ~3.7 while maintaining translation quality.

**Multi-step KL-constrained Pruning**: Progressive pruning approach with Kullback-Leibler divergence regularization to gradually reduce visual entities. Needed to prevent abrupt information loss during training; quick check: pruning steps should show gradual reduction in visual entity count.

## Architecture Onboarding

**Component Map**: CLIP -> VSGN (visual SG) -> PSG -> Transformer -> Translation; CLIP -> Stanford LSGP (language SG) -> PSG -> Transformer

**Critical Path**: Visual Scene Graph Extraction (VSGN) → Language Scene Graph Extraction (LSGP) → Cross-modal Attention Pruning (PSG) → Multimodal Translation (Transformer)

**Design Tradeoffs**: The model trades increased computational complexity (dual scene graph generation, GCN message passing, multi-step pruning) for improved translation quality by reducing visual noise. The addition of text-only NMT loss prevents over-pruning but requires careful balancing with multimodal loss.

**Failure Signatures**: 
- Over-pruning: BLEU score drops significantly below text-only baseline (41.02 for En-De)
- Under-pruning: Retained visual entities remain high (~30+) with minimal BLEU improvement
- Training instability: BLEU fluctuates or diverges during training

**First Experiments**:
1. Verify scene graph extraction produces expected entity counts (visual ~36, language ~3.7)
2. Test cross-modal attention pruning with fixed λ to observe immediate impact on visual entities
3. Train with combined loss (L_mmt + L_prune + L_nmt) and monitor BLEU progression

## Open Questions the Paper Calls Out
None

## Limitations
- The exact implementations of VSGN and Stanford LSGP models are unspecified, including model versions and checkpoints
- The CLIP variant used for visual entity representation is not specified, which affects reproducibility
- The GCN architecture details (number of layers for message passing) are not provided
- The multi-step pruning intensity schedule is only vaguely described as "incrementally increased"

## Confidence

**High Confidence**: The overall approach and experimental setup are well-specified and demonstrate consistent improvements across multiple benchmarks and language pairs.

**Medium Confidence**: The pruning mechanism and loss formulation are specified, but training dynamics details are limited, which could affect reproducibility.

**Low Confidence**: Critical components like VSGN, Stanford LSGP, and CLIP variant are unspecified, which are essential for reproducing the scene graphs.

## Next Checks

1. **Scene Graph Extraction Verification**: Reproduce visual and language scene graphs using specified models and verify average retained visual entities (~3.7) matches Table 1 results.

2. **Text-Only NMT Baseline Check**: Ensure text-only NMT loss is active from training start and verify BLEU score reaches 41.02 for En-De on text-only benchmark.

3. **Ablation Study Replication**: Replicate ablation studies to isolate contributions of pruning module and text-only NMT constraint, confirming robustness of reported improvements.