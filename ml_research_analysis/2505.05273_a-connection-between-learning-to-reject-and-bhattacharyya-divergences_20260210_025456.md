---
ver: rpa2
title: A Connection Between Learning to Reject and Bhattacharyya Divergences
arxiv_id: '2505.05273'
source_url: https://arxiv.org/abs/2505.05273
tags:
- learning
- divergence
- density
- joint
- rejection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper connects learning to reject with Bhattacharyya divergences.
  The problem addressed is optimal rejection in classification, where a model can
  abstain from making predictions on uncertain inputs.
---

# A Connection Between Learning to Reject and Bhattacharyya Divergences

## Quick Facts
- arXiv ID: 2505.05273
- Source URL: https://arxiv.org/abs/2505.05273
- Reference count: 30
- One-line primary result: Rejection via Bhattacharyya divergence is less aggressive than Chow's Rule (KL divergence) when using a modified log-loss and joint ideal distribution.

## Executive Summary
This paper establishes a theoretical connection between learning to reject in classification and Bhattacharyya divergences. The key insight is that rejection can be formulated as thresholding the density ratio between an ideal distribution and the true data distribution. By learning a joint ideal distribution over inputs and labels (rather than just the marginal input distribution as in Chow's Rule), the resulting rejector corresponds to thresholding the skewed Bhattacharyya divergence between estimated and ground-truth class probabilities under a modified log-loss. This approach is provably less aggressive than Chow's Rule, rejecting fewer instances.

## Method Summary
The method learns a joint ideal distribution Q over X√óY by minimizing expected loss under Q plus Œª¬∑KL(Q||P), where P is the true data distribution. The rejector is obtained by marginalizing to Qj(x), computing the density ratio œÅj(x)=dQj/dPx, and thresholding at œÑ. Under a modified log-loss (log(Œ∑y(x)/Œ∑*y(x))), this joint density ratio rejector corresponds to thresholding the skewed Bhattacharyya divergence B_{1-1/Œª}(Œ∑*(x)||Œ∑(x)). The approach contrasts with Chow's Rule, which uses marginal ideal distribution and thresholds KL divergence.

## Key Results
- Learning a joint ideal distribution over X√óY produces a less aggressive rejection criterion than learning only over X
- Under modified log-loss, the joint rejector corresponds to thresholding the skewed Bhattacharyya divergence
- Bhattacharyya-based rejection (B_{1-1/Œª}) is provably less aggressive than KL-based Chow's Rule (‚â§ (1/Œª)¬∑KL bound)

## Why This Works (Mechanism)

### Mechanism 1: Joint Ideal Distribution Optimization
Learning a joint ideal distribution over inputs and labels (X√óY) produces a less aggressive rejection criterion than learning only over the marginal input distribution X. The joint optimization learns Q‚àà‚ñ≥(X√óY) by minimizing expected loss under Q plus Œª¬∑KL(Q||P). After marginalizing Y to obtain Qj(x), the density ratio œÅj(x)=dQj/dPx yields the rejector. Jensen's inequality guarantees Z¬∑œÅ(x)‚â§Zj¬∑œÅj(x), meaning whenever the joint rejector rejects, the marginal must also reject (appropriately scaled).

### Mechanism 2: Density Ratio Thresholding Framework
Rejection decisions can be formulated as thresholding the density ratio between an ideal distribution Q and the true data distribution P. The rejector r(x)=ùïÄ[œÅ(x)‚â§œÑ] where œÅ=dQ/dP. For KL divergence dissimilarity, the marginal case gives œÅ(x)=(1/Z)¬∑exp(‚àíE_{Y~Œ∑*(x)}[‚Ñì(Y,h(x))]/Œª); the joint case gives œÅj(x)=(1/Zj)¬∑E_{Y~Œ∑*[exp(‚àí‚Ñì(Y,h(x))/Œª)]. Monotonicity of exp(‚àíz/Œª) connects this to Chow's Rule.

### Mechanism 3: Modified Log-Loss and Bhattacharyya Connection
Under a modified log-loss Àú‚Ñì_log(y,h(x))=‚àílog(Œ∑y(x)/Œ∑*y(x)), the joint density ratio rejector corresponds to thresholding the skewed Bhattacharyya divergence between estimated and ground-truth class probabilities. With this loss, œÅj(x)=(1/Zj)¬∑C_{1/Œª}(Œ∑(x)||Œ∑*(x)), yielding rejection criterion r(x)=ùïÄ[B_{1‚àí1/Œª}(Œ∑*(x)||Œ∑(x))‚â•Œ∫j]. Lemma 2 proves B_{1‚àí1/Œª}‚â§(1/Œª)¬∑KL, establishing that Bhattacharyya-based rejection is provably less aggressive than KL-based Chow's Rule.

## Foundational Learning

- **Concept: Chow's Rule (Optimal Rejection)** - Classical baseline for optimal rejection based on conditional risk thresholding. The entire paper positions its contributions as an alternative to Chow's Rule.
  - Quick check: Given classifier h with conditional risk E_{Y~Œ∑*(x)}[‚Ñì(Y,h(x))], what is the optimal rejection decision under cost c?

- **Concept: f-divergences and KL Divergence** - The paper uses KL divergence as the dissimilarity D(Q||P) in ideal distribution optimization. Understanding f-divergence structure is essential for extending beyond KL.
  - Quick check: Write the formula for KL(P||Q). Why does the paper use KL(Q||P) direction in Eq. 3?

- **Concept: Bhattacharyya Coefficient and Skewed Divergence** - The main theoretical contribution connects rejection to BŒ≤(Q||P)=‚àílog CŒ≤(Q||P). You must understand CŒ≤(Q||P)=‚à´Q(z)^Œ≤ P(z)^{1‚àíŒ≤}dz and its boundedness in [0,1].
  - Quick check: For Œ≤=1/2, what geometric interpretation does the Bhattacharyya coefficient have? What happens as Œ≤‚Üí0 or Œ≤‚Üí1?

## Architecture Onboarding

- **Component map**: Fixed classifier h: X‚Üí‚Ñù^L ‚Üí softmax ‚Üí Œ∑(x)‚àà‚ñ≥(Y) ‚Üí [Ground-truth Œ∑*(x)] ‚Üí [Divergence computer] ‚Üí [Threshold comparator] ‚Üí r(x)‚àà{0,1}
- **Critical path**: Given input x, obtain Œ∑(x)=softmax(h(x)) and Œ∑*(x); for marginal/Chow: compute KL(Œ∑*||Œ∑); for joint/Bhattacharyya: compute C_{1/Œª}(Œ∑(x)||Œ∑*(x)), then B_{1‚àí1/Œª}(Œ∑*(x)||Œ∑(x)); reject if divergence‚â•threshold
- **Design tradeoffs**:
  1. Marginal vs. joint ideal distribution: Marginal recovers Chow's Rule (KL-based, more aggressive); joint yields Bhattacharyya-based (provably less aggressive per Lemma 2)
  2. Œª parameter: In marginal case, Œª only scales threshold; in joint case, Œª controls skew Œ≤=1‚àí1/Œª, changing rejection boundary shape
  3. Loss function: Standard log-loss ‚Üí threshold c‚Ä≤(x) varies with x; modified Àú‚Ñì_log ‚Üí constant threshold Œ∫
- **Failure signatures**:
  1. Over-aggressive rejection: Using Chow's Rule when fewer rejections are desired‚Äîswitch to joint/Bhattacharyya
  2. Invalid Œª: Œª‚â§1 produces invalid Bhattacharyya skew (Œ≤‚â•1); ensure Œª>1
  3. Poor Œ∑* estimation: Misestimated ground-truth posterior corrupts divergence calculations entirely
- **First 3 experiments**:
  1. Reproduce Chow's Rule baseline: Implement marginal ideal distribution with standard log-loss; verify KL(Œ∑*||Œ∑)‚â•c‚Ä≤(x) matches optimal rejection from Eq. 2
  2. Compare aggressiveness: Apply both marginal and joint rejectors with modified Àú‚Ñì_log to identical test sets; verify r(dr)(x;Œª¬∑Œ∫)‚â•r(dr)j(x;Œ∫) per Lemma 2
  3. Ablate Œª in joint case: Sweep Œª‚àà{1.5,2,5,10} and measure how rejection boundaries shift beyond simple threshold scaling (observe skew Œ≤=1‚àí1/Œª effect)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the derived joint density ratio rejector when the true Bayes posterior Œ∑* is replaced by an imperfect estimate?
- Basis in paper: Remark 1 notes the method relies on Œ∑* for the modified log-loss, equating it to a cascade with a Bayes-optimal teacher, which is unavailable in practice.
- Why unresolved: The theoretical optimality relies on exact ground-truth probabilities, but real-world applications must use approximations.
- What evidence would resolve it: Empirical analysis showing the degradation of rejection performance as the estimate of Œ∑* diverges from the true distribution.

### Open Question 2
- Question: Does the "less aggressive" rejection behavior of the Bhattacharyya method result in superior selective classification performance compared to Chow's Rule?
- Basis in paper: Lemma 1 and Section 3 establish that the joint rejector rejects fewer instances than the marginal rejector, but the utility of this property is not validated.
- Why unresolved: The paper proves a theoretical difference in rejection boundaries but does not demonstrate that this difference improves the risk-coverage trade-off.
- What evidence would resolve it: Benchmark experiments comparing the risk-coverage curves of the proposed Bhattacharyya rejector against standard Chow's Rule baselines.

### Open Question 3
- Question: How should the regularization parameter Œª be selected to optimize the skew of the Bhattacharyya divergence for specific datasets?
- Basis in paper: Theorem 5 shows Œª controls the divergence skew (1 - 1/Œª), impacting the rejection boundary, whereas in the marginal case it only scales the threshold.
- Why unresolved: The paper derives the mathematical relationship but offers no guidance on tuning Œª for the joint distribution case.
- What evidence would resolve it: A sensitivity analysis illustrating how varying Œª affects the balance between coverage and empirical risk.

## Limitations

- **Unobservable Œ∑***: The ground-truth posterior Œ∑*(x) is fundamentally inaccessible in practice, requiring approximations that introduce error.
- **No empirical validation**: The paper provides no experiments or ablation studies, making real-world performance unknown.
- **Scalability concerns**: High-dimensional density ratio estimation for œÅ and œÅj may be unstable or computationally expensive.

## Confidence

- **High**: The theoretical derivation connecting joint ideal distribution optimization to Bhattacharyya divergence (Theorem 5, Lemma 2)
- **Medium**: The claim that Bhattacharyya-based rejection is "less aggressive" than Chow's Rule (Lemma 2 provides bound but not operational characterization)
- **Low**: Practical implementation details (density ratio estimation, Œª selection, Œ∑* approximation)

## Next Checks

1. **Theoretical verification**: Re-derive Lemma 1 and Lemma 2 independently to confirm that joint rejector ‚äÜ marginal rejector and B_{1-1/Œª} ‚â§ (1/Œª)¬∑KL
2. **Implementation experiment**: Implement both Chow's Rule and joint/Bhattacharyya rejectors on CIFAR-10 with a pretrained ResNet; verify Lemma 1 prediction on actual data
3. **Approximation study**: Compare rejection performance when Œ∑* is approximated by: (a) one-hot labels, (b) soft teacher model outputs, (c) ensemble averaging