---
ver: rpa2
title: Catoni Contextual Bandits are Robust to Heavy-tailed Rewards
arxiv_id: '2502.02486'
source_url: https://arxiv.org/abs/2502.02486
tags:
- bound
- variance
- where
- lemma
- catoni
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a robust contextual bandit algorithm using
  Catoni's estimator to handle heavy-tailed rewards with general function approximation.
  The key insight is that by applying the Catoni estimator to estimate excess loss,
  the algorithm can achieve regret bounds that depend polynomially on cumulative reward
  variance while only scaling logarithmically with reward range R.
---

# Catoni Contextual Bandits are Robust to Heavy-tailed Rewards

## Quick Facts
- **arXiv ID:** 2502.02486
- **Source URL:** https://arxiv.org/abs/2502.02486
- **Reference count:** 40
- **Primary result:** Robust contextual bandit algorithms using Catoni's estimator achieve variance-dependent regret bounds that depend polynomially on cumulative reward variance while only scaling logarithmically on reward range R.

## Executive Summary
This paper develops a robust contextual bandit algorithm using Catoni's estimator to handle heavy-tailed rewards with general function approximation. The key insight is that by applying the Catoni estimator to estimate excess loss, the algorithm can achieve regret bounds that depend polynomially on cumulative reward variance while only scaling logarithmically with reward range R. The authors propose two algorithms: Catoni-OFUL for the known-variance case and Variance-Agnostic Catoni Bandit (VACB) for the unknown-variance case, both achieving variance-dependent regret bounds.

## Method Summary
The paper addresses contextual bandits with heavy-tailed rewards where rewards have bounded range R but potentially unbounded variance relative to R. The core approach uses Catoni's robust estimator to replace standard Hoeffding-style concentration, enabling sub-Gaussian concentration for variables with bounded variance but unbounded range. For known variance, Catoni-OFUL uses variance-weighted regression with a min-max optimization framework. For unknown variance, VACB employs a peeling-based approach with aggregate variance estimation using Catoni on squared residuals. Both algorithms maintain confidence sets via weighted loss minimization and use optimism for action selection.

## Key Results
- Catoni-OFUL achieves regret of form $\tilde{O}(\sqrt{\sum_{t=1}^T \sigma_t^2 \cdot \text{dim}_1(T) \cdot \log N(F, \nu) + \text{dim}_1(T) \cdot \log N(F, \nu)})$ for known variance
- VACB achieves regret of $\tilde{O}(\sqrt{\sum_{t=1}^T \sigma_t^2 \cdot \log N(F, \nu)} \cdot \text{dim}_1(T) + \text{dim}_1(T) \cdot (\log N(F, \nu))^{3/4}(\sqrt{c_\eta} + \sigma_\eta))$ for unknown variance
- The approach provides matching lower bounds showing variance dependence is optimal
- Particularly valuable when worst-case reward range is much larger than variance, which occurs frequently with heavy-tailed distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Catoni estimator enables robust concentration of excess loss estimates under heavy-tailed noise with only logarithmic dependence on the reward range.
- Mechanism: The Catoni estimator (defined via Ψ function in Eq. 2) replaces standard Hoeffding-style concentration, which would scale polynomially with R. By using the influence function Ψ(x) = sign(x)·log(1 + |x| + x²/2), the estimator achieves sub-Gaussian concentration for variables with bounded variance but unbounded range. The key is that the excess loss decomposition isolates the heavy-tailed term Iᵢ = (f(xᵢ) - f'(xᵢ))(f'(xᵢ) - yᵢ)/σ̄²ᵢ, which the Catoni estimator robustifies.
- Core assumption: Noise has bounded variance (E[η²_t] ≤ σ²_t) and bounded range (|η_t| ≤ R), but R can be much larger than σ.
- Evidence anchors:
  - [abstract] "regret bound that depends only on the cumulative reward variance and logarithmically on the reward range R"
  - [section 3.2, Lemma 2] Shows concentration error bounded by O(θV/t + ι²/θt) where θ is adaptive to sample variance
  - [corpus] Related work "Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards" addresses similar (1+ε)-moment settings
- Break condition: If variance is infinite or noise is adversarial rather than stochastic, the Catoni concentration guarantees fail.

### Mechanism 2
- Claim: Variance-weighted confidence sets constructed via Catoni excess loss estimation contain the true function f* with high probability while achieving dimension-dependent scaling.
- Mechanism: The confidence set F_t := {f ∈ F_{t-1} : Σᵢ(f(xᵢ) - f̂_t(xᵢ))²/σ̄²ᵢ ≤ β̂²_t} uses weights σ̄ᵢ = max(α, σᵢ, √(4ιL_f D_F(...))). The min-max estimator f̂_t = argmin_f̂ max_{f'} L_t(f̂, f') ensures that even under heavy-tailed noise, the excess loss L_t concentrates around the true excess risk R_t, and the confidence set shrinks at variance-dependent rates.
- Core assumption: Realizability (f* ∈ F) and the function class F has finite eluder dimension and covering number.
- Evidence anchors:
  - [abstract] "When the variance of the reward at each round is known, we use a variance-weighted regression approach"
  - [section 3.3, Lemma 3] Proves L_t(f̂_t) ≥ 2/3 V_t(f̂_t, f*) - 1/3 β̂²_t ensuring f* ∈ F_t
  - [corpus] "Variance-Aware Feel-Good Thompson Sampling" explores variance-dependent bounds via alternative mechanisms
- Break condition: If the function class is misspecified (f* ∉ F) or the eluder dimension grows super-linearly with T, regret bounds degrade.

### Mechanism 3
- Claim: A peeling-based approach with aggregate variance estimation avoids per-round variance estimation while maintaining variance-dependent regret.
- Mechanism: VACB partitions samples into L levels by uncertainty D^l_t(x). At each level, weights w_i = 2^l D^l_i(x_i) substitute for unknown σ_i in normalizing losses. The aggregated variance estimator V̂ar^l_t uses Catoni on squared residuals (y_i - f̂^{l}_{t-1}(x_i))²/w²_i, providing 2-approximation to Σ σ²_i/w²_i without requiring a separate variance function class.
- Core assumption: Fourth moment bounds exist (Var[η²_t | F_t] ≤ c_η Var[η_t | F_t]).
- Evidence anchors:
  - [abstract] "we further propose a careful peeling-based algorithm and remove the need for cumbersome variance estimation"
  - [section 4.2, Lemma 5] Proves 1/2 Σσ²_i/w²_i ≤ V̂ar^l_t ≤ 3/2 Σσ²_i/w²_i + O(ι(σ²_η + c_η))
  - [corpus] Limited direct corpus support for peeling + Catoni combination; this appears novel
- Break condition: If fourth moments are unbounded or c_η is extremely large, the variance estimator accuracy degrades, affecting regret.

## Foundational Learning

- Concept: **Eluder Dimension**
  - Why needed here: Quantifies how quickly the function class F can be learned; appears in regret bounds as dim_1,T(F), capturing the intrinsic complexity of non-linear function approximation beyond linear case.
  - Quick check question: Can you explain why the weighted eluder coefficient D²_F in Definition 2 measures "how much in-sample error bounds out-of-sample error"?

- Concept: **Covering Number N(F, ν)**
  - Why needed here: Captures the complexity of the function class for uniform convergence; the regret scales as Õ(√(Σσ²_t · dim · log N(F,υ))) where log N captures the "size" of F for finite-sample analysis.
  - Quick check question: If F is a class of neural networks with bounded weights, how would you estimate or bound N(F, ν)?

- Concept: **Catoni's M-estimator**
  - Why needed here: This is the core robust statistics tool enabling variance-dependent concentration without sub-Gaussian assumptions; understanding the influence function Ψ is essential for implementation.
  - Quick check question: Why does Ψ(x) = sign(x)·log(1 + |x| + x²/2) provide better heavy-tailed robustness than quadratic loss or Huber loss?

## Architecture Onboarding

- Component map:
  1. Weight computation module: Computes σ̄_t = max(α, σ_t, √(4ιL_f D_F(...))) for known variance; computes w_t = 2^l D^l_t(x_t) for unknown variance
  2. Catoni estimator module: Solves f(x) = 0 for Σ Ψ(θ(Z_i - x)) = 0 given samples {Z_i} and parameter θ
  3. Min-max optimizer: Solves f̂_t = argmin_{f̂} max_{f'} L_t(f̂, f') to find robust estimator
  4. Confidence set constructor: Maintains F_t based on weighted squared loss constraints
  5. Action selector: Chooses x_t = argmax_{x∈X_t} max_{f∈F_{t-1}} f(x) via optimism principle
  6. Peeling controller (VACB only): Manages L levels, uncertainty thresholds, and variance estimator updates

- Critical path:
  1. Observe context/decision set X_t
  2. Compute uncertainty D_F(x) for candidate actions
  3. Select action via optimism (or peeling rule for VACB)
  4. Observe reward y_t
  5. Update weights and run Catoni-based optimization
  6. Update confidence set F_t
  7. For VACB: update level assignments and variance estimator

- Design tradeoffs:
  - Known vs unknown variance: Catoni-OFUL has tighter bounds (Õ(√Σσ²_t · dim · log N)) but requires σ_t; VACB handles unknown variance with O(dim · (log N)^{3/4}) overhead
  - Optimization efficiency vs regret: Min-max formulation (Eq. 3) is computationally challenging; Algorithm 3 (Appendix D) provides alternative with candidate sets but same regret
  - Covering number vs discretization error: Smaller ν improves approximation but increases log N; paper recommends ν = O(1/L_f^{12} R^4 T^{10})

- Failure signatures:
  - Regret scales as Õ(R · dim) instead of Õ(√Σσ²_t · dim): Catoni estimator θ parameter may be misspecified
  - Confidence set F_t becomes empty: β̂_t threshold too tight or covering ν too coarse
  - VACB variance estimator diverges: Check Assumption 1 on fourth moments; c_η may be unbounded
  - Action selection inefficient: Eluder dimension computation requires solving sup over F×F

- First 3 experiments:
  1. Linear bandit validation: Implement Catoni-OFUL for F = {θ^T x : ||θ||₂ ≤ 1} with synthetic heavy-tailed noise (e.g., Pareto rewards with finite variance), compare regret against weighted-OFUL baselines across varying R/σ ratios
  2. Ablation on Catoni parameter θ: Test sensitivity of the adaptive θ_t(f, f') = 2ι/√(V_t + ε²) by comparing against fixed-θ variants; measure regret degradation when θ is mis-specified by 10×
  3. VACB peeling granularity: For unknown variance setting, sweep L ∈ {5, 10, 15, 20} levels and measure (a) regret accumulation, (b) variance estimator accuracy |V̂ar^l_t - Σσ²_i/w²_i|, (c) computational overhead per round

## Open Questions the Paper Calls Out

- **Can the worse dependence on eluder dimension in the unknown-variance case (VACB algorithm) be improved to match the known-variance bound?**
  - The conclusion states: "When the per-round variance is unknown... the algorithm also obtains a variance-based bound depending on R logarithmically, but has a worse dependence on the eluder dimension. Improving this is left as a future direction."
  - The peeling technique introduces additional dimension factors that don't appear in the known-variance analysis.

- **Can the Catoni-based approach be extended to handle adversarial corruption in rewards, beyond heavy-tailed noise?**
  - The conclusion suggests: "Since the Catoni estimator is a general device from robust statistics, it might also be useful to investigate if it enables us to handle other forms of noise, such as adversarial corruption."
  - Adversarial corruption differs fundamentally from stochastic heavy-tailed noise; Catoni's guarantees rely on variance bounds that may not hold under adversarial perturbations.

- **Can these results be extended from contextual bandits to general Markov Decision Processes (MDPs)?**
  - The conclusion states: "It would also be interesting to extend the results to general MDPs."
  - MDPs introduce temporal dependencies and value function estimation complexities not present in bandits; the eluder dimension analysis may require different techniques.

## Limitations

- The paper presents a purely theoretical analysis with no empirical validation, making practical utility uncertain
- The computationally intensive min-max optimization in Eq. 3 may be difficult to implement efficiently for realistic function classes like neural networks
- The variance-agnostic approach relies on fourth-moment bounds that may be difficult to verify in practice
- The peeling approach introduces O((log N)^{3/4}) overhead, making it potentially impractical for large function classes

## Confidence

- **High confidence** in the variance-dependent regret bounds (Õ(√Σσ²_t · dim · log N)) - the analysis is rigorous and builds on established techniques
- **Medium confidence** in the practical utility - the computational complexity and need for variance-related assumptions may limit applicability
- **Low confidence** in implementation details - critical components like the min-max optimization are referenced but not fully specified

## Next Checks

1. **Computational feasibility**: Implement the min-max optimization for a simple linear function class and measure runtime vs regret trade-offs
2. **Fourth-moment sensitivity**: Test VACB performance on synthetic data with varying c_η values to understand robustness to fourth-moment assumptions
3. **Comparison to alternatives**: Implement and compare against existing heavy-tailed bandit approaches (like UCB-M and Truncated-UCB) to quantify the benefit of variance-dependent bounds