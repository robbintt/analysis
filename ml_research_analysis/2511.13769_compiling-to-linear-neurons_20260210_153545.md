---
ver: rpa2
title: Compiling to linear neurons
arxiv_id: '2511.13769'
source_url: https://arxiv.org/abs/2511.13769
tags:
- compiler
- neurons
- linear
- neural
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Cajal, a typed, higher-order, and linear programming
  language that compiles to linear neurons, enabling direct programming of neural
  networks with discrete structures. The core innovation is a compiler correctness
  theorem that preserves the behavior of Cajal programs when translated to linear
  neurons, allowing discrete algorithms to be expressed in a differentiable form compatible
  with gradient-based learning.
---

# Compiling to linear neurons

## Quick Facts
- **arXiv ID:** 2511.13769
- **Source URL:** https://arxiv.org/abs/2511.13769
- **Reference count:** 40
- **Primary result:** Cajal enables direct programming of neural networks with discrete structures, yielding faster learning and greater data efficiency compared to indirect programming approaches.

## Executive Summary
This paper introduces Cajal, a typed, higher-order, and linear programming language that compiles to linear neurons, enabling direct programming of neural networks with discrete structures. The core innovation is a compiler correctness theorem that preserves the behavior of Cajal programs when translated to linear neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. The experiments demonstrate that directly programming neural networks using Cajal yields significant benefits: networks learn faster and with greater data efficiency compared to traditional indirect programming approaches. In a conditional image transformation task, directly programmed networks solved the problem while indirectly programmed networks failed. In conditional classification tasks (AND, OR, XOR), directly programmed networks consistently outperformed indirect models, with the performance advantage correlating to the strength of the compiler correctness theorem satisfied by the compiler. Additionally, directly programming networks facilitates debugging by exposing intermediate computation states.

## Method Summary
The paper presents a compiler that translates a typed, higher-order, linear programming language (Cajal) into linear neurons. The key mechanism involves creating soft-branching structures where discrete algorithmic components (like conditionals) are compiled into differentiable equivalents using interpolation. For the conditional image transformation task, the direct model consists of three sub-networks: a condition network that detects whether the input is "1", an identity network that preserves the input, and a transform network that converts non-"1" digits to "1". These are combined using coefficients derived from the condition network's output, creating a differentiable branching structure. The compiler correctness theorem ensures that this translation preserves the original algorithm's behavior, enabling the network to learn discrete algorithms through gradient descent.

## Key Results
- Directly programmed networks solved a conditional image transformation task where indirectly programmed networks failed, with indirect models collapsing to constant output.
- In conditional classification tasks (AND, OR, XOR), directly programmed networks consistently outperformed indirect models, with performance gains correlating to compiler theorem strength.
- Directly programmed networks learned faster and with greater data efficiency compared to indirect programming approaches.
- The compiler correctness theorem was verified to preserve algorithm behavior when translated to linear neurons.

## Why This Works (Mechanism)
The compiler correctness theorem is the foundational mechanism that enables this approach. By formally proving that Cajal programs compile to linear neurons while preserving their behavior, the system can express discrete algorithms (like conditionals and loops) in a differentiable form. This allows gradient-based learning to optimize parameters within a structured framework that maintains the discrete algorithm's semantics. The soft-branching mechanism (using interpolation coefficients) enables differentiable execution of discrete control flow, while the type system and linearity constraints ensure that the compiled network maintains the algorithm's logical structure during training.

## Foundational Learning
- **Linear Types:** Ensure resources (neurons) are used exactly once, preventing information loss and enabling precise compilation to linear neurons. Needed for maintaining algorithm structure during translation.
- **Compiler Correctness Theorem:** Formally proves that Cajal programs' behavior is preserved when compiled to linear neurons. Quick check: Verify that the compiled network produces the same output as the original Cajal program on test inputs.
- **Soft-Branching:** Differentiable approximation of discrete control flow using interpolation coefficients. Quick check: Confirm that the interpolation coefficients properly activate the intended sub-networks based on conditions.
- **Higher-Order Programming in Neural Networks:** Enables functions as first-class values, allowing complex algorithmic structures to be expressed in differentiable form. Quick check: Test nested function calls in Cajal and verify correct compilation.

## Architecture Onboarding
**Component Map:** Condition Network (784→400→2) → Coefficients α → [Identity Path, Transform Path] → Combined Output
**Critical Path:** Input → Condition Network → Softmax → α₁,α₂ → Identity(x)·α₁ + Transform(x)·α₂ → Output
**Design Tradeoffs:** Direct programming trades architectural flexibility for guaranteed algorithm preservation, while indirect programming allows arbitrary network structures but loses discrete algorithm semantics.
**Failure Signatures:** Indirect models collapse to constant output; direct models fail if condition network cannot properly differentiate classes early in training.
**First Experiments:** 1) Implement conditional image transformation with both direct and indirect architectures; 2) Test soft-branching with synthetic interpolation tasks; 3) Verify compiler correctness theorem preservation on simple Cajal programs.

## Open Questions the Paper Calls Out
None

## Limitations
- The compiler's practical implementation details are not fully specified, requiring assumptions about weight initialization and normalization.
- The approach is demonstrated on relatively simple tasks (MNIST-based) and may not scale to more complex algorithms.
- The theoretical guarantees depend on specific structural constraints that may limit architectural flexibility.

## Confidence
- **High Confidence:** The theoretical contribution regarding Cajal's compiler correctness theorem and its role in preserving discrete algorithm behavior in differentiable form.
- **Medium Confidence:** The empirical demonstration that directly programmed networks outperform indirect approaches on the conditional image transformation task, given the specific architecture and task setup described.
- **Medium Confidence:** The claim about debugging benefits through visibility of intermediate states, as this is somewhat qualitative and depends on implementation details.

## Next Checks
1. Implement the conditional image transformation task with both direct (Cajal-style) and indirect (MLP) architectures, verifying the indirect baseline fails to solve the task while the direct approach succeeds.
2. Conduct ablation studies removing the compiler correctness theorem's guarantees (e.g., by breaking the structural constraints) to confirm its necessity for the observed performance gains.
3. Test the conditional classification tasks (AND, OR, XOR) to verify the correlation between compiler theorem strength and performance advantage, using the same direct/indirect programming paradigm.