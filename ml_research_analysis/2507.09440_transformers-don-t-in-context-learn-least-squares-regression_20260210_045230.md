---
ver: rpa2
title: Transformers Don't In-Context Learn Least Squares Regression
arxiv_id: '2507.09440'
source_url: https://arxiv.org/abs/2507.09440
tags:
- prompts
- regression
- subspace
- training
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformers perform in-context learning
  (ICL) for linear regression tasks, challenging the prevailing hypothesis that transformers
  implement algorithms like Ordinary Least Squares (OLS) regression during inference.
  Through a series of out-of-distribution generalization experiments, the authors
  demonstrate that transformers fail to generalize when input or weight vectors fall
  outside the subspace seen during pre-training, a behavior inconsistent with classical
  regression rules that should generalize regardless of input subspace.
---

# Transformers Don't In-Context Learn Least Squares Regression

## Quick Facts
- arXiv ID: 2507.09440
- Source URL: https://arxiv.org/abs/2507.09440
- Reference count: 17
- Primary result: Transformers trained for ICL fail to generalize after shifts in the prompt distribution, inconsistent with OLS regression.

## Executive Summary
This paper investigates how transformers perform in-context learning (ICL) for linear regression tasks, challenging the prevailing hypothesis that transformers implement algorithms like Ordinary Least Squares (OLS) regression during inference. Through a series of out-of-distribution generalization experiments, the authors demonstrate that transformers fail to generalize when input or weight vectors fall outside the subspace seen during pre-training, a behavior inconsistent with classical regression rules that should generalize regardless of input subspace. Notably, even in-distribution, transformers achieve error rates orders of magnitude worse than OLS. The authors identify a "spectral signature" in the transformer's residual stream representations: in-distribution prompts exhibit a stable low-dimensional structure with rapidly decaying singular values and consistent principal directions, while out-of-distribution prompts show flatter spectra and unstable directions. This signature correlates strongly with performance and can reliably detect distribution shifts.

## Method Summary
The authors trained GPT-2-style transformers (12 layers, 8 heads, hidden dim 256) on synthetic linear regression tasks where they must predict y = w^T x from example pairs without gradient updates. They used curriculum training starting with reduced dimensionality and fewer examples, then systematically varied the input and weight vector subspaces. The key experiments involved training transformers on restricted subspaces (10-dimensional out of 20 total) and testing generalization to orthogonal subspaces. They extracted residual-stream representations before the final readout head and performed SVD analysis to identify spectral signatures that distinguish in-distribution from out-of-distribution prompts.

## Key Results
- Transformers trained on restricted subspaces fail to generalize to orthogonal subspaces, while OLS regression would perform identically across all subspaces.
- Even in-distribution, transformers achieve error rates orders of magnitude worse than OLS in the overdetermined regime.
- In-distribution prompts produce stable spectral signatures with consistent top two singular vectors and rapidly decaying singular values, while OOD prompts show flat spectra and unstable directions.
- The spectral signature can detect distribution shifts with 94% accuracy for in-distribution prompts versus 19% for OOD prompts.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer ICL binds to training distribution geometry rather than implementing a distribution-agnostic learning algorithm.
- Mechanism: When trained on regression prompts with inputs or weights restricted to a subspace D∥, the model learns representations that encode this subspace structure. Predictions depend on projecting inputs onto learned canonical directions that align with D∥. When OOD inputs arrive, their representations lack alignment with these fixed directions, causing prediction degradation.
- Core assumption: The model memorizes subspace-specific patterns rather than learning a general regression rule.
- Evidence anchors:
  - [abstract] "transformers trained for ICL fail to generalize after shifts in the prompt distribution, a behaviour that is inconsistent with the notion of transformers implementing algorithms such as OLS"
  - [Section 4.1, Figure 2-3] Shows transformers trained on D∥ perform well on D∥ but fail on D⊥ (orthogonal subspace) and D□ (full space); error increases smoothly as test inputs blend toward OOD components.
  - [corpus] Related work (Goddard et al., 2024) also finds ICL generalization gaps under subspace shifts, supporting the distribution-dependence claim.

### Mechanism 2
- Claim: In-distribution prompts produce a stable "spectral signature"—two dominant singular vectors consistent across prompts—that predicts low loss and detects distribution shift.
- Mechanism: Residual-stream representations Zp of in-distribution prompts exhibit rapidly decaying singular values with stable top-two right singular vectors. The model's readout head primarily attends to projections along these canonical directions. OOD prompts produce flatter spectra with unstable directions, breaking the alignment with the readout head.
- Core assumption: The readout head has learned to depend on the canonical subspace spanned by these top singular vectors.
- Evidence anchors:
  - [abstract] "Inputs from the same distribution as the training data produce representations with a unique spectral signature: inputs from this distribution tend to have the same top two singular vectors."
  - [Section 5.2, Figure 5, Table 1] Cosine similarity between prompt singular vectors and canonical vectors is ~0.95+ for first two components (in-distribution) vs high variability (OOD); Gaussian fit to Cp,:2 yields 94% in-region for S∥ vs 19% for S⊥.
  - [corpus] No directly contradicting evidence found; spectral analysis of ICL representations is a novel contribution.

### Mechanism 3
- Claim: Transformers do not implement OLS or gradient descent even in-distribution; they achieve orders-of-magnitude higher asymptotic error.
- Mechanism: Rather than computing (X^T X)^{-1} X^T y, the transformer learns a subspace-constrained solution that approximates ridge-like regularization toward training subspace statistics. This explains both the in-distribution error gap and OOD failure.
- Core assumption: The learning objective optimizes for training-distribution performance without inducing algorithmic generalization.
- Evidence anchors:
  - [Section 4.1, Figure 2] "once we enter the overdetermined regime, the OLS baseline achieves an error that is several orders of magnitude lower than that of the transformer models"
  - [Section 4.1] Blending experiment shows transformer error increases as OOD component grows, while projected-OLS baseline behaves differently—indicating the transformer is not simply running OLS on the training-subspace component.

## Foundational Learning

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The paper investigates ICL mechanisms in transformers; understanding that ICL means solving new tasks from prompt examples without gradient updates is prerequisite.
  - Quick check question: Given a transformer trained on task A, can it solve task B at inference time using only examples in the prompt? Under what conditions does this fail?

- Concept: **Ordinary Least Squares (OLS) and Ridge Regression**
  - Why needed here: The paper compares transformer ICL against classical regression baselines; understanding OLS guarantees (distribution-agnostic, optimal under Gauss-Markov conditions) clarifies why transformer behavior is surprising.
  - Quick check question: Why does OLS generalize across input subspaces while transformers trained on subspace-restricted data do not?

- Concept: **Singular Value Decomposition (SVD) and Spectral Analysis**
  - Why needed here: The paper's diagnostic tool is spectral signature detection via SVD of residual-stream representations; understanding singular values/vectors is necessary to interpret Figures 4-5 and the Cp metric.
  - Quick check question: If a representation matrix Z has singular values [10, 8, 0.1, 0.05, ...], what does this imply about its effective dimensionality?

## Architecture Onboarding

- Component map:
  GPT-2-style transformer -> Residual stream -> Readout head -> Scalar prediction

- Critical path:
  1. Prompt tokens → embedding layer
  2. 12 transformer blocks (attention + MLP, residual connections)
  3. Residual stream representations z(t_i) collected before readout
  4. Readout head produces scalar prediction for each position
  5. MSE loss against ground-truth w^T x_i

- Design tradeoffs:
  - Training subspace diversity: More diverse training (e.g., multiple scales) improves OOD robustness (Appendix D) but may slow convergence
  - Curriculum schedule (Appendix C): Starting with reduced dimensionality and fewer examples helps early training but constrains final generalization
  - Noise in labels (Appendix B): Training with label noise degrades in-distribution performance to ridge-like levels

- Failure signatures:
  - Spectral signature deviation: If top-two Cp values fall outside 95% confidence region of canonical distribution, expect degraded predictions
  - Subspace misalignment: If test inputs have significant components orthogonal to training subspace, error scales with misalignment (Figure 3)
  - Scale mismatch: Input magnitude outside training scale range causes rapid error growth (Appendix D)

- First 3 experiments:
  1. **Subspace restriction test**: Train transformer on D∥ (10-dim subspace of 20-dim space), evaluate on D∥, D⊥, D□. Compare MSE vs OLS. Expected: transformer matches OLS on D∥ (but with higher asymptotic error), fails on D⊥ and D□.
  2. **Spectral signature extraction**: Collect residual-stream representations Zp for batches from each distribution. Compute SVD, extract singular value spectra and cosine similarities against canonical V*^T. Expected: in-distribution shows steep spectral decay and stable top-2 vectors; OOD shows flat spectra and unstable directions.
  3. **OOD detection via Cp metric**: Fit bivariate Gaussian to Cp,:2 from S∥. Compute percentage of holdout S∥ and S⊥ prompts within 95% confidence region. Expected: >90% for S∥, <25% for S⊥.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do spectral signatures generalize from synthetic linear regression tasks to richer, real-world ICL tasks such as language translation or code generation?
- Basis in paper: [explicit] The authors state: "the extent to which these findings generalize to richer, real-world ICL tasks (e.g. language-translation, code generation) remains an open question."
- Why unresolved: Experiments focused only on synthetic linear regression with GPT-2-style architectures; only preliminary analysis on real LLMs (Qwen3-4B on Caesar ciphers) was provided.
- What evidence would resolve it: Demonstration of consistent spectral signature patterns in pretrained LLMs across diverse real-world ICL tasks, with correlations between signature presence and task performance.

### Open Question 2
- Question: How does the spectral signature emerge and stabilize throughout the pretraining process?
- Basis in paper: [explicit] Authors propose as future work: "Training Dynamics of the spectral signature to track how it evolves and stabilizes over training. This could involve examining gradient-updates and how they relate to the residual-stream representation."
- Why unresolved: Current analysis examines only final trained models, not the trajectory of representation geometry during training.
- What evidence would resolve it: Longitudinal analysis of residual-stream representations at multiple training checkpoints, correlating signature emergence with loss curves and gradient dynamics.

### Open Question 3
- Question: How do specific architectural components (layer normalization, attention heads, MLP layers) interact with the formation of spectral signatures?
- Basis in paper: [explicit] The authors note: "we have not yet fully characterized how it emerges during pretraining or how it interacts with specific architectural components (e.g. layer normalization, attention heads, MLP layers)."
- Why unresolved: The study uses standard GPT-2 architecture without ablations or component-level analysis.
- What evidence would resolve it: Ablation studies varying individual components and measuring impact on spectral signature formation and ICL generalization performance.

## Limitations

- The paper's findings are based on synthetic regression tasks with controlled distributions that may not generalize to real-world transformer behaviors.
- The 20-dimensional input space is significantly smaller than typical transformer applications, raising questions about scalability.
- The curriculum training schedule (starting with reduced dimensionality and fewer examples) may induce artifacts that wouldn't appear in standard training regimes.

## Confidence

**High Confidence**: The empirical finding that transformers achieve orders-of-magnitude worse MSE than OLS in-distribution, and that this gap persists across training runs, is robustly demonstrated. The subspace generalization failure (D⊥ and D□ experiments) is clearly reproducible.

**Medium Confidence**: The spectral signature mechanism and its diagnostic utility. While the SVD analysis shows clear differences between distributions, the claim that this signature directly explains the performance gap requires further mechanistic validation. The Cp metric's effectiveness as an OOD detector is demonstrated but needs broader validation.

**Low Confidence**: The assertion that transformers don't implement any form of gradient descent or iterative refinement. The paper shows transformers fail to generalize like OLS, but doesn't conclusively rule out other optimization-like mechanisms that might be distribution-dependent.

## Next Checks

1. **Scale Sensitivity Analysis**: Replicate the blending experiment (Figure 3) but systematically vary the scale of input vectors across orders of magnitude. This would test whether the distribution-dependence is truly geometric or merely scale-dependent, and whether the spectral signature breaks down predictably with scale shifts.

2. **Cross-Architecture Spectral Comparison**: Apply the same SVD-based spectral analysis to transformers trained with different architectures (e.g., different MLP sizes, attention patterns, or even non-transformer models like RNNs) on the same regression tasks. This would test whether the spectral signature is a general property of sequence models or specific to the GPT-2 architecture used.

3. **Fine-tuning Transfer Experiment**: Take a transformer trained on D∥ and fine-tune it on D⊥ (without catastrophic forgetting of D∥). Measure whether the spectral signature adapts to the new subspace and whether MSE on D⊥ improves relative to zero-shot OOD performance. This would test whether the distribution-dependence is a fixed property or can be learned away with exposure.