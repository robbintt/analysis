---
ver: rpa2
title: 'RecoWorld: Building Simulated Environments for Agentic Recommender Systems'
arxiv_id: '2509.10397'
source_url: https://arxiv.org/abs/2509.10397
tags:
- user
- arxiv
- recommender
- agentic
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RecoWorld presents a simulated environment framework for training
  and evaluating agentic recommender systems. It uses a dual-view architecture where
  a simulated user and an agentic recommender engage in multi-turn interactions aimed
  at maximizing user retention.
---

# RecoWorld: Building Simulated Environments for Agentic Recommender Systems

## Quick Facts
- arXiv ID: 2509.10397
- Source URL: https://arxiv.org/abs/2509.10397
- Reference count: 40
- Primary result: Framework for training agentic recommenders using simulated user-instructor interactions to optimize retention

## Executive Summary
RecoWorld introduces a simulated environment framework for training and evaluating agentic recommender systems through collaborative interactions with LLM-based simulated users. The system uses a dual-view architecture where a simulated user and an agentic recommender engage in multi-turn interactions aimed at maximizing user retention. The framework supports diverse content representations and enables safe testing of recommendation strategies without impacting real users. RecoWorld represents an important first step toward collaborative recommender systems where users provide instructions and recommenders respond, optimizing both retention and engagement through a dynamic feedback loop.

## Method Summary
RecoWorld employs a dual-view architecture featuring an LLM-based user simulator and an agentic recommender that interact over multi-turn sessions. The user simulator processes recommended items, updates its mindset, and generates reflective instructions when disengagement is sensed. The agentic recommender adapts recommendations using these instructions and reasoning traces, creating a dynamic feedback loop. The environment supports three content representation options: text-based (using models like Llama4, Qwen3), multimodal (Qwen3-Omni, Gemini-2.5-Pro), and semantic ID modeling. User preferences are modeled through dynamic memory that filters historical behaviors based on learned importance scores and updates latent states using temporal functions. The system operates as a Markov Decision Process where trajectory-level engagement metrics serve as reward signals for reinforcement learning algorithms like PPO or DPO.

## Key Results
- Framework enables safe testing of recommendation strategies without impacting real users
- Supports instruction-driven multi-turn interactions for collaborative recommendation
- Three content representation options: text-based, multimodal, and semantic ID modeling
- Dynamic memory system maintains evolving user preference states across long interaction histories

## Why This Works (Mechanism)

### Mechanism 1: Instruction-Driven Dual-View Feedback Loop
- Claim: A simulated user and agentic recommender can collaboratively improve recommendation quality through iterative, instruction-based interactions.
- Mechanism: The user simulator processes recommended items, generates reflective instructions when disengagement is sensed, and the recommender adapts its output by incorporating these instructions and reasoning traces. This creates a dynamic feedback loop where explicit user signals directly shape subsequent recommendations.
- Core assumption: LLMs can accurately simulate realistic user disengagement triggers and generate actionable feedback instructions.
- Evidence anchors: [abstract] "The user simulator reviews recommended items, updates its mindset, and when sensing potential user disengagement, generates reflective instructions." [section 2] "A successful simulator triggers accurate instructions that guide the recommender, while an effective recommender responds with item lists that enhance user satisfaction."

### Mechanism 2: Session-Level Reward Signals from Multi-Turn Trajectories
- Claim: Trajectory-level engagement metrics (e.g., total time spent, clicks, turn count) provide richer reward signals than single-action metrics for training agentic recommenders.
- Mechanism: Each multi-turn interaction generates a trajectory. Session-level statistics are aggregated and used as pseudo-rewards for reinforcement learning. This encourages policies that optimize for long-term retention rather than immediate clicks.
- Core assumption: Session-level metrics from simulated users correlate with real-world retention outcomes.
- Evidence anchors: [abstract] "generates engagement trajectories as reward signals for reinforcement learning" [section 2] "We use trajectory-level interaction metrics such as total time spent as reward signals to measure the effectiveness of agentic RecSys."

### Mechanism 3: Dynamic Memory and Evolving Preference Modeling
- Claim: User simulators can maintain and update preference states across long interaction histories by filtering relevant memories and modeling mindset shifts.
- Mechanism: The simulator maintains engagement memory (interaction-wise and session-wise), dynamically filters past behaviors based on learned importance scores, and updates latent preference states using temporal functions. This enables personalized, context-aware responses over time.
- Core assumption: The learned importance function and temporal update rules generalize across diverse user profiles.
- Evidence anchors: [section 3] "We define an engagement memory M_u^t... where alpha_k is the learned importance score conditioned on current context C_t." [section 3] "User interests are not static but continuously evolve... z_t = g(z_{t-1}, M_u^t, C_t)."

## Foundational Learning

- **Markov Decision Processes (MDPs)**: RecoWorld frames user-recommender interactions as an MDP with states (user mindset), actions (recommendations), transitions (simulated by LLM), and rewards (engagement metrics). *Quick check*: Can you explain how a policy maps states to actions in an MDP, and how reward signals guide policy updates?

- **LLM-based User Simulation**: The entire simulator relies on LLMs to predict user actions, generate instructions, and update mindsets across multi-turn sessions. *Quick check*: How would you validate that an LLM's simulated user behavior matches real user patterns?

- **Multi-Agent Simulation Dynamics**: RecoWorld supports multi-agent setups where N simulated users interact and influence each other, requiring understanding of state update rules and environmental dynamics. *Quick check*: In a network of simulated users, how do neighbor interactions affect individual state updates over time?

## Architecture Onboarding

- **Component map**: User Simulator -> Agentic Recommender -> Reward Generator -> Multi-Agent Orchestrator
- **Critical path**: 
  1. Initialize user simulator with profile, context, and engagement history
  2. Serve initial recommendation list to simulator
  3. Simulator processes items, outputs actions and potentially instructions
  4. If instruction issued, agentic recommender reconfigures and returns updated list
  5. Repeat until session ends; aggregate trajectory metrics into reward signal
  6. Use reward to update recommender policy via RL (e.g., PPO, DPO)

- **Design tradeoffs**:
  - Text-based vs. multimodal vs. semantic ID modeling: Text is flexible but may miss nuance; multimodal is richer but costlier and harder to compress; semantic IDs are compact but require continual pre-training
  - Explicit vs. implicit instructions: Explicit instructions are interpretable but sparse; implicit signals require inference but are abundant
  - Simulated vs. real user validation: Simulation is scalable but may diverge from reality; human annotators are gold standard but expensive

- **Failure signatures**:
  - Simulator generates incoherent or overly generic instructions
  - Session rewards plateau despite policy changes (reward hacking)
  - Memory grows unbounded, causing latency or noise in preference updates
  - Multi-agent simulations collapse into uniform behavior (lack of diversity)

- **First 3 experiments**:
  1. Validate simulator realism by comparing simulated session trajectories against held-out real user data using session-level metrics (time spent, click-through rate)
  2. Ablate instruction-following: run with instructions enabled vs. disabled to measure impact on retention metrics
  3. Test multi-agent population scaling: simulate 10, 100, 1000 users and analyze whether collective feedback stabilizes or diverges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does high NDCG paired with low user retention in RecoWorld reliably indicate suboptimal content diversity, while low NDCG with high retention signals successful exploration?
- Basis in paper: [explicit] The authors explicitly hypothesize on Page 4 that "(b) high NDCG with low user retention reveals suboptimal recommendations... and (c) low NDCG with high retention likely suggests effective exploration."
- Why unresolved: The paper presents a blueprint framework and does not include experimental results or data to validate these specific correlations between offline metrics and the simulator's trajectory-level rewards.
- What evidence would resolve it: Empirical analysis of simulation logs showing the correlation coefficients between NDCG scores, retention metrics (time spent), and diversity measures across different agent policies.

### Open Question 2
- Question: To what degree do the multi-turn interaction trajectories generated by LLM-based user simulators correlate with actual human annotator behavior?
- Basis in paper: [explicit] Page 4 states, "Our paper does not present experimental results; instead, we outline evaluation designs," and proposes comparing session-level statistics between simulated users and human annotators as a "sanity check."
- Why unresolved: The framework defines the mechanism for human validation but leaves the execution of these comparison studies to future work to prove the simulator's fidelity.
- What evidence would resolve it: A study benchmarking the KL divergence or cosine similarity between action distributions (clicks, skips, instructions) of simulated agents versus human annotators on identical recommendation lists.

### Open Question 3
- Question: How effectively can an agentic recommender infer latent user intent from vague instructions (e.g., "I'm bored") compared to specific requests?
- Basis in paper: [explicit] Page 7 notes that "ambiguous instructions require more sophisticated handling" than lexical or semantic matching and envisions a "reasoning-intensive retrieval model" as a future necessity.
- Why unresolved: The paper identifies this as a distinct challenge but does not propose a specific architecture or benchmark results for resolving high-ambiguity user feedback.
- What evidence would resolve it: Ablation studies within RecoWorld comparing the success rates of standard retrieval versus reasoning-enhanced models in satisfying users who issue ambiguous instructions.

## Limitations
- Framework remains largely theoretical with minimal empirical validation of core mechanisms
- Key assumptions about simulator realism and reward correlation are stated but not tested
- Critical implementation details (prompts, reward formulas, training hyperparameters) are omitted
- No experimental results demonstrating effectiveness of the proposed architecture

## Confidence
- **Mechanism 1 (Instruction-Driven Feedback Loop)**: Medium confidence - conceptually sound but simulator accuracy unverified
- **Mechanism 2 (Session-Level Rewards)**: Medium confidence - RL formulation valid but real-world correlation unproven
- **Mechanism 3 (Dynamic Memory)**: Low confidence - formulation specified but no validation of preference evolution accuracy
- **Overall Framework**: Low confidence - architectural blueprint without empirical demonstration

## Next Checks
1. **Simulator Reality Validation**: Compare simulated session trajectories against held-out real user interaction data using session-level metrics (time spent, click-through rate, session duration distribution)
2. **Instruction Impact Isolation**: Run controlled experiments with instructions enabled vs. disabled to measure quantifiable impact on retention metrics across multiple user profiles
3. **Multi-Agent Population Scaling Test**: Simulate increasing population sizes (10, 100, 1000 users) and analyze whether collective feedback dynamics stabilize or diverge, measuring diversity and convergence properties