---
ver: rpa2
title: Constructing and Evaluating Declarative RAG Pipelines in PyTerrier
arxiv_id: '2506.10802'
source_url: https://arxiv.org/abs/2506.10802
tags:
- retrieval
- https
- pyterrier
- pipelines
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PyTerrier-RAG, an extension of the PyTerrier
  platform that enables declarative construction and evaluation of Retrieval-Augmented
  Generation (RAG) pipelines. The framework supports both sequential and iterative
  RAG architectures, providing access to standard QA datasets, evaluation measures
  (Exact Match, F1, ROUGE), and state-of-the-art LLM readers including Fusion-in-Decoder
  models.
---

# Constructing and Evaluating Declarative RAG Pipelines in PyTerrier

## Quick Facts
- **arXiv ID:** 2506.10802
- **Source URL:** https://arxiv.org/abs/2506.10802
- **Reference count:** 40
- **Primary result:** Introduces PyTerrier-RAG framework for declarative construction and evaluation of RAG pipelines

## Executive Summary
This paper presents PyTerrier-RAG, an extension of the PyTerrier IR platform that enables researchers to declaratively construct and evaluate Retrieval-Augmented Generation pipelines. The framework supports both sequential (retriever → reranker → reader) and iterative (IRCoT) RAG architectures, providing access to standard QA datasets, evaluation metrics (Exact Match, F1, ROUGE), and state-of-the-art LLM readers including Fusion-in-Decoder models. By leveraging PyTerrier's operator notation and extending its data model with RAG-specific types, researchers can easily compose pipelines combining sparse, learned-sparse, and dense retrievers with various neural rankers without writing glue code.

## Method Summary
PyTerrier-RAG extends the PyTerrier data model with RAG-specific types (Q_c for context-bearing queries, A for answers) and provides transformers for each pipeline stage. The framework uses Python operator overloading to create declarative pipelines, where components like retrievers, rerankers, concatenators, and readers are connected via the `>>` operator. Prefix-computation optimizations identify shared pipeline stages across multiple experiments to reduce redundant computation. The system supports various retrieval approaches (BM25, SPLADE, E5) and reader backends (HuggingFace, OpenAI, T5-FiD) with built-in evaluation using standard QA metrics.

## Key Results
- Provides declarative framework for constructing RAG pipelines without configuration files
- Supports prefix-computation optimization for efficient multi-pipeline experiments
- Implements iterative IRCoT approach for complex reasoning tasks
- Offers pre-built indices for common corpora via HuggingFace
- Eliminates integration friction between pipeline stages through type-safe data flow

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The framework reduces integration friction and enhances reproducibility by abstracting data exchange between pipeline stages into relational algebra operators.
- **Mechanism:** Python operator overloading (e.g., `>>` for "then") enforces strict type flow (e.g., $Q \to R \to A$). This transforms imperative glue code into declarative expressions, ensuring that the output schema of one component (e.g., a Retriever) automatically matches the input schema of the next (e.g., a Reranker).
- **Core assumption:** The user's custom components conform to the PyTerrier data model, returning Pandas DataFrames with required attributes (e.g., `qid`, `docno`, `score`).
- **Evidence anchors:**
  - [Section 3] "PyTerrier suggests a declarative manner... several operators are defined on transformers... implemented through the use of operator overloading."
  - [Abstract] "...leveraging its operator notation, researchers can easily compose pipelines..."

### Mechanism 2
- **Claim:** Evaluation efficiency is improved by identifying and caching shared pipeline prefixes across multiple experimental configurations.
- **Mechanism:** The `pt.Experiment` API examines the declarative structure of the pipelines being compared. If multiple pipelines share an initial retrieval stage (e.g., identical BM25 retrieval), the system executes this "prefix" only once, caches the result, and reuses it for downstream variants (e.g., different LLM readers).
- **Core assumption:** The shared prefix components are deterministic and side-effect free.
- **Evidence anchors:**
  - [Section 4.5] "...prefix-computation [30]. This examines the pipelines... identifies any common prefix... and applies that prefix to the input topics only once."
  - [Corpus] Neighbors confirm "Precomputation and Caching" is a recognized strategy for pipeline efficiency in PyTerrier (arXiv:2504.09984).

### Mechanism 3
- **Claim:** Standardizing RAG-specific data types enables modular swapping of retrieval and generation components without re-engineering the pipeline architecture.
- **Mechanism:** By extending the base IR types ($Q, R$) with RAG-specific types like Context-bearing queries ($Q_c$) and Answers ($A$), the framework formalizes transformations (e.g., $R \to Q_c$ via a *Concatenator*, $Q_c \to A$ via a *Reader*). This decouples the *retrieval* of documents from the *preparation* of the LLM prompt.
- **Core assumption:** The reader component can function effectively on the aggregated context ($Q_c$), though the paper notes exceptions like Fusion-in-Decoder (FiD) which require the raw ranking ($R$).
- **Evidence anchors:**
  - [Section 4.1] "Building on these, we add datatypes... $Q_c$ which contains context... Creation of RAG Context: $R \to Q_c$."
  - [Section 4.2] "Retrieval output is aggregated by a separate Concatenator class... So a sequential RAG pipeline is as follows..."

## Foundational Learning

- **Concept: PyTerrier Data Model (Transformers & Relations)**
  - **Why needed here:** The entire framework relies on understanding that data flows as "relations" (Pandas DataFrames) with specific schemas (columns like `qid`, `docno`, `text`). Without this, operator overloading (`>>`) appears magical rather than structural.
  - **Quick check question:** If a retriever outputs a dataframe with columns `[qid, docno, score]`, what columns must a reranker output to be valid in the pipeline?

- **Concept: Sequential vs. Iterative RAG**
  - **Why needed here:** The paper explicitly contrasts simple pipelines (Retriever $\to$ Reader) with complex ones (IRCoT) where the LLM's output influences the next retrieval step. Distinguishing these is necessary to select the correct topology (linear vs. loop).
  - **Quick check question:** In an IRCoT pipeline, what component determines when the iterative loop should stop?

- **Concept: Declarative Experimentation**
  - **Why needed here:** The paper champions `pt.Experiment` as a replacement for manual loops and config files. Users must understand that they define the *system* and the *dataset*, and the framework handles the execution and metric calculation (F1, EM).
  - **Quick check question:** When calling `pt.Experiment`, does the user write a loop to iterate over the test queries, or does the API handle the iteration internally?

## Architecture Onboarding

- **Component map:** Query $\to$ Retriever (get top-k) $\to$ Reranker (refine top-k) $\to$ Concatenator (format text) $\to$ Reader (generate answer)
- **Critical path:** Query $\to$ Retriever (get top-k) $\to$ Reranker (refine top-k) $\to$ Concatenator (format text) $\to$ Reader (generate answer). *Note: The Reranker is optional but recommended in the paper's examples.*
- **Design tradeoffs:**
  - **Config vs. Code:** PyTerrier-RAG eschews YAML/JSON config files for pure Python declarations. This maximizes flexibility (branching logic, loops) but requires engineering effort over configuration.
  - **General vs. Specialized Readers:** Using the standard `Reader` (accepting $Q_c$) is easier but may limit performance compared to specialized architectures like FiD (which accepts $R$ to encode documents individually).
  - **Memory vs. Speed:** Prefix-computation caches intermediate results, speeding up ablation studies but increasing memory usage.
- **Failure signatures:**
  - **`KeyError` on 'qanswer':** The Reader transformer failed to generate a response, or the column was renamed incorrectly.
  - **Schema mismatch:** Connecting a dense retriever output directly to a component expecting sparse index statistics without the correct adapter.
  - **FiD TypeError:** Passing a concatenated string ($Q_c$) to a FiD reader which expects a ranked list ($R$).
- **First 3 experiments:**
  1.  **Baseline Sequential RAG:** Load a pre-built index (e.g., `ragwiki-terrier`) $\to$ BM25 $\to$ OpenAI Reader. Evaluate on Natural Questions to establish a baseline F1 score.
  2.  **Retriever Ablation:** Keep the Reader fixed (e.g., Flan-T5). Compare BM25 vs. SPLADE vs. E5 dense retrieval to isolate the impact of retrieval quality on answer accuracy.
  3.  **Iterative Reasoning:** Implement an IRCoT loop using a small LLM (e.g., Llama-3.1-8B) on a multi-hop dataset (HotPotQA) to compare performance against the sequential baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the "lossy" consumption of information by LLMs affect the translation of retrieval relevance scores into generation quality across different retriever types?
- **Basis in paper:** [explicit] The authors state in the conclusion that "easily expressing different retrieval pipelines is of particular importance" because "the nature of LLMs is that consumption of information is lossy, and differ from previous expectations in the IR about ordering documents."
- **Why unresolved:** While the framework provides the capacity to investigate this, the paper does not present experimental findings quantifying how this "lossy" nature impacts the optimal choice or configuration of sparse vs. dense retrievers.
- **What evidence would resolve it:** Empirical analysis within PyTerrier-RAG correlating traditional retrieval relevance metrics with answer quality metrics (EM, F1) specifically for pipelines utilizing different retrieval architectures.

### Open Question 2
- **Question:** Can complex, graph-based iterative RAG methods be implemented with the same declarative succinctness as the demonstrated sequential pipelines?
- **Basis in paper:** [inferred] The paper mentions that complex methods like REANO and TRACE "can be expressed as PyTerrier-RAG pipelines," but only demonstrates the code for simpler sequential and IRCoT pipelines.
- **Why unresolved:** It is unclear if the current operator notation and data model extensions can represent complex entity-relationship reasoning without resorting to imperative code that breaks the declarative abstraction.
- **What evidence would resolve it:** A code implementation and complexity analysis of a graph-based method (e.g., REANO) in PyTerrier-RAG compared to its native implementation.

### Open Question 3
- **Question:** Which specific datasets and reader architectures should be prioritized for future implementation to maximize the framework's utility?
- **Basis in paper:** [explicit] The conclusion states: "In future work, we will continue to supplement PyTerrier-RAG with more datasets and more reader implementations."
- **Why unresolved:** The paper lists current datasets and readers but leaves the specific roadmap for future expansions undefined.
- **What evidence would resolve it:** A gap analysis of the framework's current coverage against state-of-the-art RAG benchmarks (e.g., newer multi-hop datasets) and newly released LLM architectures.

## Limitations

- No empirical results or ablation studies demonstrating framework's performance improvements over existing RAG implementations
- Effectiveness with specialized readers like FiD (which require raw rankings rather than aggregated context) mentioned but not demonstrated
- Prefix-computation optimization's practical runtime impact remains unverified without benchmarks

## Confidence

- **High confidence:** The declarative pipeline construction mechanism using operator overloading and type-safe data flow (Mechanism 1) is well-supported by the PyTerrier foundation and clearly specified.
- **Medium confidence:** The prefix-computation optimization (Mechanism 2) is theoretically sound based on PyTerrier's architecture, but its practical impact remains unverified without runtime benchmarks.
- **Medium confidence:** The modular component swapping enabled by RAG-specific data types (Mechanism 3) is logically coherent, though the exception for FiD readers suggests the generalization has practical limitations.

## Next Checks

1. Implement and time a multi-pipeline ablation study using prefix-computation vs. naive execution to measure actual runtime improvements.
2. Compare F1/EM scores of sequential RAG pipelines using different retriever combinations (BM25, SPLADE, E5) on Natural Questions to validate retriever impact claims.
3. Construct and evaluate an IRCoT pipeline on HotPotQA to demonstrate iterative reasoning performance against the sequential baseline.