---
ver: rpa2
title: Quantifying Self-Awareness of Knowledge in Large Language Models
arxiv_id: '2509.15339'
source_url: https://arxiv.org/abs/2509.15339
tags:
- hallucination
- self-awareness
- question
- information
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for quantifying self-awareness
  in large language models by distinguishing it from question-awareness. The authors
  argue that hallucination detection often conflates model-side introspection with
  question-side shortcuts, leading to overestimated performance.
---

# Quantifying Self-Awareness of Knowledge in Large Language Models

## Quick Facts
- arXiv ID: 2509.15339
- Source URL: https://arxiv.org/abs/2509.15339
- Reference count: 40
- One-line primary result: Introduces AQE metric to isolate self-awareness from question-awareness and SCAO method to enhance model-side introspection in hallucination detection

## Executive Summary
This paper addresses a fundamental challenge in hallucination detection: distinguishing true model-side self-awareness from question-side shortcuts. The authors demonstrate that existing methods conflate these two sources of signal, leading to overestimated performance. They propose the Approximate Question-side Effect (AQE) metric to quantify the contribution of question-awareness and introduce SCAO (Semantic Compression by Answering in One Word) to enhance genuine self-awareness. Their analysis reveals that much of the reported hallucination detection performance stems from exploiting superficial question patterns rather than true introspection. SCAO achieves strong and consistent performance, especially in settings with reduced question-side cues, demonstrating its effectiveness in fostering genuine self-awareness in LLMs.

## Method Summary
The framework quantifies self-awareness by isolating question-side effects from true model introspection. AQE uses sBERT embeddings of questions to estimate the contribution of question-awareness through a separate model θ'. Three hallucination prediction approaches are evaluated: confidence-based thresholding, hidden-state probing with DNNs, and aggregation. SCAO enhances self-awareness by forcing one-word answers, which shifts confidence patterns toward knowledge retrieval rather than question repetition. The method is tested across multiple datasets (ParaRel, Mintaka, HaluEval, HotpotQA, SimpleQuestion, Explain) with LLaMA-3 models, using AUROC and accuracy as primary metrics. Refined datasets control for question-side shortcuts by removing binary questions and using out-of-domain splits.

## Key Results
- AQE scores of 0.65-0.82 across datasets indicate substantial question-side shortcut contribution
- SCAO achieves up to 0.83 AUROC in refined settings, outperforming hidden-state probing
- Under refinement (+type and +domain), probe performance drops sharply (e.g., ParaRel: 0.89→0.70) while SCAO remains stable
- Conf(SCAO) shows superior generalization to out-of-domain settings compared to hidden-state probes

## Why This Works (Mechanism)

### Mechanism 1: AQE isolates question-side shortcuts from true self-awareness
The AQE metric quantifies the contribution of question-awareness by using sBERT to predict correctness without access to the LLM's knowledge. This reveals how much performance comes from exploiting question-side patterns (domain, type, broken annotations) versus genuine model-side introspection.

### Mechanism 2: Confidence scores under one-word prompting reduce question-side contamination
Constraining output to one word forces the model to retrieve entity-level knowledge rather than exploit grammatical shortcuts. This makes confidence scores more reflective of true model-side uncertainty, as the first token shifts from repeating the question entity to retrieving related knowledge.

### Mechanism 3: Hidden-state probing overfits to dataset-specific question patterns
High-dimensional hidden states encode both question-side and model-side information, but trained probes preferentially exploit the easier, more stable question-side signals within a dataset. This leads to performance drops when shortcuts are removed through refinement.

## Foundational Learning

- **Shapley value / marginal contribution**: Why needed - AQE is framed as isolating one factor's effect from a mixed signal. Quick check - If you swap the order of feature attribution, does AQE remain invariant?
- **Dense retrieval calibration (threshold-based relevance)**: Why needed - SCAO analogizes LLM confidence to MIPS range search, where a threshold determines if knowledge is "in the database." Quick check - How does a confidence threshold differ from a probability threshold in classification?
- **Hidden-state probing and linear separability**: Why needed - The framework assumes hidden states encode attributes like "truthfulness" in linearly separable directions. Quick check - What does it mean if a linear probe achieves 90% accuracy but a non-linear probe achieves 95%?

## Architecture Onboarding

- **Component map**: Target LLM θ (LLaMA-3) -> generates answers and provides hidden states/confidence; Reference model θ' (sBERT) -> embeds questions for AQE; Probe module ϕ (DNN/threshold) -> predicts correctness from hidden states or confidence; Refinement pipeline -> filters datasets to control shortcuts
- **Critical path**: 1) Prepare dataset D = {(question, answer_label)}; 2) Extract hidden state s from θ and s' from sBERT; 3) Compute AQE: train ϕ' on s'→k; 4) Train ϕ on s (or confidence) → k; compute A(ϕ(sM)) = AUROC - AQE_auc; 5) Apply SCAO: modify prompt with "You must answer in only one word"
- **Design tradeoffs**: Confidence vs. hidden state (lower-dimensional but more robust vs. richer but prone to overfitting); SCAO applicability (best for entity-based factoid QA, less effective for multi-word answers); AQE validity (depends on sBERT approximating LLM's question-side encoding)
- **Failure signatures**: AQE_auc ≈ AUROC(ϕ) indicates zero self-awareness contribution; SCAO confidence does not shift indicates instruction-following failure; probe performance collapses under refinement but confidence-based method remains stable indicates shortcut exploitation
- **First 3 experiments**: 1) Compute AQE baseline on Mintaka and ParaRel; 2) Apply +type and +domain refinements; 3) Compare Conf(SCAO) vs. Probe DNN on ParaRel +domain

## Open Questions the Paper Calls Out
- **Open Question 1**: How can the definition and measurement of self-awareness be extended beyond factoid knowledge-recall tasks to more complex settings such as multi-step reasoning and long-form text comprehension? (Section 6 mentions this remains an important direction)
- **Open Question 2**: To what extent does the choice of the external proxy model (e.g., sBERT) in calculating the Approximate Question-side Effect (AQE) influence the estimation of the self-awareness contribution? (The validity depends on sBERT capturing similar question-side information as LLM)
- **Open Question 3**: Does the Semantic Compression by Answering in One Word (SCAO) method introduce new failure modes when the correct answer requires semantic nuance or multiple tokens to express accurately? (Section C.2 acknowledges the model often chooses entity components as the first token)

## Limitations
- AQE metric relies on strong assumption that sBERT embeddings capture approximately the same question-side information as LLM hidden states, which is never directly validated
- SCAO's one-word prompting mechanism is supported by qualitative analysis but not systematically tested for robustness across model sizes, instruction adherence, or domain complexity
- Probe overfitting claim is inferred from performance gaps between original and refined datasets but lacks targeted ablation experiments to confirm shortcut exploitation

## Confidence
- **High**: AQE quantifies a measurable signal (question-side contribution) using sBERT embeddings; the metric is reproducible given the specification
- **Medium**: SCAO improves robustness under refinement, as shown by AUROC gains in out-of-domain settings; the mechanism is plausible but the instruction-following assumption is not stress-tested
- **Medium**: Probe overfitting is the most likely explanation for performance drops under refinement, given the documented shortcut patterns, but not conclusively proven without targeted ablations

## Next Checks
1. **AQE separability validation**: Train a probe on s' (sBERT) and a probe on s_M (LLM hidden states with question information removed or masked). Compare their combined performance to the full probe on s. If the gap is small, the AQE separability assumption is supported.
2. **SCAO instruction-following robustness**: Test SCAO across different model sizes (e.g., 7B, 13B, 70B) and instruction phrasings. Measure the proportion of prompts where the first token still repeats the question entity. If this proportion remains high despite one-word instruction, the mechanism is compromised.
3. **Probe shortcut ablation**: Design a control experiment where question-side shortcuts are preserved but model-side knowledge is degraded (e.g., via knowledge retrieval failure or prompt injection). If probe performance remains high while confidence-based performance drops, it confirms that the probe is exploiting shortcuts rather than self-awareness.