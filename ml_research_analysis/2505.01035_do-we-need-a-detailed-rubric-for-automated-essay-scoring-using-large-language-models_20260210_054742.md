---
ver: rpa2
title: Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language
  Models?
arxiv_id: '2505.01035'
source_url: https://arxiv.org/abs/2505.01035
tags:
- rubric
- scoring
- essay
- language
- rubrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines whether detailed rubrics are necessary for
  automated essay scoring (AES) using large language models (LLMs). The research compares
  scoring accuracy across three rubric conditions (detailed, simplified, and none)
  using four different LLMs on the TOEFL11 dataset.
---

# Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?

## Quick Facts
- arXiv ID: 2505.01035
- Source URL: https://arxiv.org/abs/2505.01035
- Authors: Lui Yoshida
- Reference count: 39
- Primary result: Simplified rubrics maintain scoring accuracy while reducing token usage by ~40% for most LLMs, though model-specific effects vary

## Executive Summary
This study investigates whether detailed rubrics are necessary for automated essay scoring using large language models. Through controlled experiments across four different LLMs on the TOEFL11 dataset, the research demonstrates that simplified rubrics can achieve comparable scoring accuracy to detailed rubrics while significantly reducing token usage. The findings reveal important model-specific variations, with some models performing better with simplified prompts while others show degradation with increased rubric detail. This suggests a practical path toward more efficient AES systems without sacrificing scoring quality for most models.

## Method Summary
The study compares three rubric conditions (detailed, simplified, and none) across four LLMs (Claude 3.5 Haiku, GPT-4o-mini, Gemini 1.5 Flash, Llama 3 70B Instruct) using the TOEFL11 dataset containing 12,100 essays across 8 prompts. Essays were scored on a 5-point scale by each model under each rubric condition, then converted to 3-point scores for comparison against expert ratings. Scoring accuracy was measured using Quadratic Weighted Kappa (QWK), with statistical significance assessed via paired bootstrap testing (1,000 iterations) and Holm's correction. Token efficiency was also measured to quantify prompt length impact.

## Key Results
- Three out of four models maintained similar scoring accuracy with simplified rubrics compared to detailed ones
- Token usage was reduced by approximately 40% when using simplified rubrics
- Gemini 1.5 Flash showed decreased performance with more detailed rubrics, exhibiting a systematic bias toward higher scores
- All rubric conditions significantly outperformed the no-rubric baseline

## Why This Works (Mechanism)

### Mechanism 1
Modern LLMs have internalized essay evaluation criteria during pre-training, so simplified rubrics act as lightweight pointers to this existing knowledge rather than requiring complete specification. The 78-word simplified rubric provides sufficient anchoring for models to retrieve relevant evaluation schemas. This works because the target evaluation domain (essay scoring) is sufficiently represented in pre-training data.

### Mechanism 2
For some models, longer prompts degrade inference quality due to prompt-length sensitivity. Gemini 1.5 Flash showed systematic bias toward higher scores as rubric detail increased, suggesting the detailed rubric caused it to lose calibration with the scoring task. This may occur through reduced attention to specific instructions or shifts in output distribution as context window fills.

### Mechanism 3
Explicit rubrics constrain model behavior to the target task by providing task grounding. Without a rubric, models default to generic or internally-learned scoring schemas that may not align with specific evaluation frameworks like TOEFL's 3-level scheme. Any rubric—simplified or detailed—anchors the model to the intended scoring criteria.

## Foundational Learning

- **Quadratic Weighted Kappa (QWK):** Standard metric for AES agreement between machine and human scores, accounting for ordinal scale and penalizing larger disagreements more heavily. Why needed: QWK better captures ordinal scoring agreement than raw accuracy.
- **Token economics in LLM APIs:** Direct impact on cost and environmental footprint. Why needed: The study's practical contribution is reducing token usage (~40% savings) while maintaining accuracy.
- **Bootstrap significance testing with multiple comparisons:** Used to assess statistical significance of QWK differences. Why needed: The paper uses paired bootstrap with 1,000 resamples and Holm's correction to properly handle multiple comparisons.

## Architecture Onboarding

- **Component map:**
  ```
  Prompt Template
  ├── Instruction (task description, role assignment)
  ├── Essay Prompt (writing prompt given to test-takers)
  ├── Response (essay text to score)
  ├── Assessment Method (rubric: Full | Simplified | None)
  └── Output Format (structured rating extraction)

  Evaluation Pipeline
  ├── LLM API call → raw score (0-5)
  ├── Score conversion → 3-level (Low/Medium/High)
  ├── QWK calculation vs expert labels
  └── Bootstrap CI + significance testing
  ```

- **Critical path:** Rubric selection → prompt construction → API call → score mapping → QWK computation
- **Design tradeoffs:**
  - Full rubric: Maximum specification, ~2× tokens, may degrade some models
  - Simplified rubric: ~1.2× tokens, comparable accuracy for most models, faster/cheaper
  - No rubric: Baseline tokens, significantly worse accuracy—not recommended
- **Failure signatures:**
  - Model systematically over-scores essays (check confusion matrix for high-score bias)
  - QWK drops as rubric detail increases (sign of prompt-length sensitivity)
  - Large variance across prompts/models (QWK ~0.6 suggests limited practical readiness)
- **First 3 experiments:**
  1. Replicate on held-out prompts from TOEFL11 to confirm generalization
  2. Apply same rubric conditions to ASAP or CLC-FCE datasets to test domain transfer
  3. Test prompt-length thresholds for Gemini 1.5 Flash to identify degradation point

## Open Questions the Paper Calls Out
1. Do simplified rubrics maintain scoring accuracy across diverse essay datasets such as ASAP or CLC-FCE?
2. How do "reasoning" models (e.g., OpenAI o1) perform on AES tasks when rubric detail is reduced?
3. What specific factors cause models like Gemini 1.5 Flash to suffer performance degradation when detailed rubrics are applied?

## Limitations
- Study is limited to TOEFL11 dataset (L2 learners, specific prompts), requiring validation on other datasets like ASAP or CLC-FCE
- The full rubric text is not provided, limiting exact replication
- Does not explore the impact of rubric simplification on nuanced scoring criteria
- Model-specific sensitivity to prompt length needs further investigation across more architectures

## Confidence
- **High Confidence:** Simplified rubrics achieve comparable accuracy to detailed rubrics for most models
- **Medium Confidence:** Model-specific sensitivity to prompt length, particularly for Gemini 1.5 Flash
- **Medium Confidence:** Token efficiency improvement (~40% reduction) and its practical significance

## Next Checks
1. Cross-Dataset Validation: Apply simplified rubric approach to other standardized essay datasets (e.g., ASAP, CLC-FCE) to test domain transferability
2. Model Sensitivity Analysis: Conduct systematic study of prompt-length effects across broader range of LLM architectures, including reasoning models
3. Edge Case Evaluation: Test simplified rubric on essays with highly specialized content (technical, creative, or low-resource language) to identify boundary conditions where detailed rubrics become necessary