---
ver: rpa2
title: 'Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision'
arxiv_id: '2509.14234'
source_url: https://arxiv.org/abs/2509.14234
tags:
- synthesis
- rollouts
- arxiv
- reference
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces Compute as Teacher (CaT), a framework that\
  \ uses inference compute to generate reference-free supervision for reinforcement\
  \ learning, especially in non-verifiable domains like healthcare guidance where\
  \ human annotations are scarce. CaT aggregates multiple parallel rollouts from a\
  \ model into a pseudo-reference using a synthesis method and derives rewards via\
  \ self-proposed rubrics\u2014binary, auditable criteria generated from the pseudo-reference\
  \ and scored by an LLM judge."
---

# Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision
## Quick Facts
- arXiv ID: 2509.14234
- Source URL: https://arxiv.org/abs/2509.14234
- Reference count: 36
- Introduces a framework using inference compute to generate reference-free supervision for RL in non-verifiable domains

## Executive Summary
Compute as Teacher (CaT) addresses the challenge of reinforcement learning in non-verifiable domains like healthcare guidance where human annotations are scarce. The framework generates pseudo-references from multiple parallel model rollouts and derives rewards through LLM-judged self-proposed rubrics. CaT achieves up to 30% relative improvement on HealthBench while using 9× less inference compute than baseline synthesis methods. The approach bridges the gap between verifiable and non-verifiable domains by creating an auditable feedback loop without requiring ground truth labels.

## Method Summary
CaT uses multiple parallel rollouts from a policy to generate a pseudo-reference through synthesis, then creates self-proposed rubrics that are scored by an LLM judge to derive rewards. The framework operates without ground truth labels, making it suitable for domains where human annotation is expensive or impossible. On HealthBench, CaT matches or exceeds inference-time synthesis quality while reducing test-time compute by 9×. The method demonstrates versatility across both verifiable (MATH-500) and non-verifiable domains, outperforming supervised fine-tuning and selection baselines through its synthesis-based approach that reconciles partial reasoning across rollouts.

## Key Results
- Achieves up to 30% relative improvement over initial policy on HealthBench
- Uses 9× less test-time compute than inference-time synthesis methods
- Matches best baselines on MATH-500 while demonstrating strong performance in non-verifiable domains

## Why This Works (Mechanism)
CaT leverages the reasoning diversity across multiple parallel rollouts to synthesize a pseudo-reference that captures the best elements of each attempt. The self-proposed rubrics create binary, auditable criteria that can be consistently scored by an LLM judge, providing stable reward signals for RL. This approach overcomes the limitations of judge-only feedback by providing structured, reference-based supervision without requiring human annotations. The synthesis method effectively aggregates partial reasoning from multiple attempts, while the rubric mechanism ensures rewards are both interpretable and reproducible.

## Foundational Learning
- **Inference-time compute utilization**: Needed because CaT turns computational resources during inference into supervisory signals rather than just using them for better predictions. Quick check: Can the framework generate meaningful pseudo-references from parallel rollouts?
- **Reinforcement learning without ground truth**: Required for domains where human annotations are expensive or impossible. Quick check: Does the reward signal remain stable and meaningful without reference labels?
- **LLM-based synthesis and judging**: Essential for creating and evaluating pseudo-references at scale. Quick check: Can the LLM consistently score self-proposed rubrics across diverse outputs?
- **Binary rubric design**: Provides auditable, interpretable feedback that enables consistent RL training. Quick check: Are the rubrics specific enough to guide policy improvement while remaining generalizable?

## Architecture Onboarding
**Component map**: Policy -> Multiple parallel rollouts -> Synthesis module -> Pseudo-reference -> Rubric generator -> LLM judge -> Reward signal -> RL optimizer -> Updated policy
**Critical path**: The synthesis and rubric generation stages are critical, as they directly determine the quality of the pseudo-reference and reward signal that drives learning.
**Design tradeoffs**: CaT trades inference compute for supervision quality, using parallel rollouts to generate synthetic references instead of relying on expensive human annotations. The binary rubric approach sacrifices nuance for consistency and interpretability.
**Failure signatures**: Poor policy reasoning leads to uninformative rollouts that cannot be meaningfully synthesized. Judge miscalibration causes inconsistent rewards. Limited rollout diversity causes premature convergence of the synthesis method.
**First experiments**: 1) Run CaT with varying numbers of parallel rollouts to find the optimal tradeoff between compute and supervision quality. 2) Test rubric generation with different levels of specificity to assess impact on reward stability. 3) Compare CaT performance when starting from policies of varying initial quality.

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Gains plateau as rollouts converge, limiting effectiveness in later training stages
- Performance depends heavily on base model quality for reasoning and synthesis
- Judge calibration sensitivity may affect reward consistency across domains

## Confidence
- High confidence in verifiable domain results (MATH-500) due to ground truth validation
- Medium confidence in non-verifiable domain improvements (HealthBench) due to reliance on synthetic references
- High confidence in 9× compute reduction claim due to direct measurability
- Medium confidence in generalizability across diverse domains due to limited validation scope

## Next Checks
1. Test CaT's performance when starting from a very weak initial policy to assess robustness to poor reasoning diversity
2. Conduct ablation studies on judge calibration and rubric specificity to quantify their impact on reward consistency and policy improvement
3. Evaluate CaT on a broader set of non-verifiable domains with varying complexity and ambiguity to test scalability and generalizability