---
ver: rpa2
title: 'Med-CRAFT: Automated Construction of Interpretable and Multi-Hop Video Workloads
  via Knowledge Graph Traversal'
arxiv_id: '2512.01045'
source_url: https://arxiv.org/abs/2512.01045
tags:
- video
- graph
- visual
- reasoning
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Med-CRAFT, a neuro-symbolic data engineering
  framework that automates the construction of interpretable, multi-hop video benchmarks
  for medical domains. By extracting structured visual primitives from raw video and
  organizing them into a dynamic Spatiotemporal Knowledge Graph, the system generates
  query-answer pairs through deterministic graph traversal, ensuring rigorous logical
  provenance and avoiding hallucinations common in black-box generative approaches.
---

# Med-CRAFT: Automated Construction of Interpretable and Multi-Hop Video Workloads via Knowledge Graph Traversal

## Quick Facts
- arXiv ID: 2512.01045
- Source URL: https://arxiv.org/abs/2512.01045
- Authors: Shenxi Liu; Kan Li; Mingyang Zhao; Yuhang Tian; Shoujun Zhou; Bin Li
- Reference count: 15
- Primary result: Introduces Med-CRAFT, a neuro-symbolic framework that automates interpretable, multi-hop medical video benchmark construction via knowledge graph traversal, achieving complexity comparable to expert-curated datasets.

## Executive Summary
Med-CRAFT presents a neuro-symbolic data engineering framework that transforms raw medical video into structured, interpretable benchmarks. The system extracts visual primitives from video and organizes them into a dynamic Spatiotemporal Knowledge Graph, then generates query-answer pairs through deterministic graph traversal. This approach ensures rigorous logical provenance and avoids hallucinations common in black-box generative methods. The framework is instantiated in M3-Med-Auto, a large-scale medical video reasoning benchmark exhibiting fine-grained temporal selectivity and multi-hop logical complexity.

## Method Summary
Med-CRAFT operates through a three-layer pipeline: (1) Visual Extraction - Grounding DINO detection with ASR/OCR prompts, CLIP-ViT encoding, and Hungarian bipartite matching to create 3D spatiotemporal tubelets; (2) Graph Construction - Nodes from tubelets, edges via spatiotemporal overlap, trajectory DTW, and Video-CLIP semantics, with Qwen3-VL predicate labeling; (3) Query Synthesis - DFS traversal to extract paths, Qwen3-VL question generation, and GPT-4V adversarial validation. The system is evaluated on temporal IoU metrics and logic alignment analysis.

## Key Results
- Query workloads exhibit complexity comparable to expert-curated medical video datasets
- High correlation between prescribed graph topology and reasoning steps of state-of-the-art MLLMs
- Demonstrates fine-grained temporal selectivity and multi-hop logical complexity in M3-Med-Auto benchmark
- Validates capability to encode verifiable logic into visual-linguistic benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transforming raw video into a Spatiotemporal Knowledge Graph enforces logical provenance that purely generative models lack.
- **Mechanism:** Visual primitives extracted from video become nodes, linked via spatiotemporal association and semantic similarity as edges. This converts reasoning into verifiable graph topology properties rather than probabilistic outputs.
- **Core assumption:** Visual co-occurrence and trajectory similarity (GIoU, DTW) are valid proxies for semantic relationship discovery in medical procedures.
- **Evidence anchors:** Abstract mentions extracting structured visual primitives into a dynamic Spatiotemporal Knowledge Graph. Section 3.2 discusses quantifying pairwise spatiotemporal and semantic affinities for logic-weighted edges. Neighbor paper "BifrostRAG" supports dual-graph approaches for multi-hop QA.
- **Break condition:** If object detector fails to capture fine-grained medical instruments, the resulting graph will have missing nodes, causing logical chains to fail before query synthesis.

### Mechanism 2
- **Claim:** Generating queries via deterministic graph traversal (DFS) eliminates hallucination by restricting the solution space to valid visual paths.
- **Mechanism:** Instead of asking LLM to "guess" questions, the system traverses the KG and extracts structured paths. The LLM only translates these specific logic chains into natural language, making the path itself the truth.
- **Core assumption:** DFS paths effectively map to human-understandable multi-hop reasoning chains.
- **Evidence anchors:** Abstract formalizes benchmark synthesis as deterministic graph traversal. Section 3.3 describes executing deterministic traversal algorithms to extract valid reasoning paths. "M3KG-RAG" corroborates structured graph traversal for multi-hop reasoning.
- **Break condition:** If graph contains spurious edges between unrelated tools appearing in same frame, DFS will produce valid graph paths that are logically nonsensical in real world.

### Mechanism 3
- **Claim:** Adversarial validation by a "Judge" model filters out low-quality or ambiguous generated questions.
- **Mechanism:** GPT-4V acts as critic, scoring generated questions against video content. This feedback loop catches errors introduced during "Natural Language-ization" step.
- **Core assumption:** Judge model possesses superior reasoning capabilities compared to generator and can reliably discern semantic ambiguity.
- **Evidence anchors:** Section 3.3 states a more powerful MLLM (GPT-4V) acts as independent "Judge" and only qualified questions are included. Abstract mentions validating capability to encode verifiable logic. Limited direct evidence in corpus for this specific adversarial loop.
- **Break condition:** If Judge model suffers from same "semantic gap" as generator, it may pass hallucinated questions, admitting noise into final benchmark.

## Foundational Learning

- **Concept: Spatiotemporal Association (Tracking)**
  - **Why needed here:** Architecture relies on converting 2D bounding boxes into 3D "tubelets" (trajectories). Without understanding Bipartite Matching and Hungarian Algorithms, cannot debug why objects fail to link across frames.
  - **Quick check question:** Given two bounding boxes in adjacent frames with low IoU but high visual similarity, how does the system decide they are the same object?

- **Concept: Knowledge Graph Construction (Nodes/Edges)**
  - **Why needed here:** Core innovation is graph layer. Must understand how to define entities (nodes) from visual primitives and relations (edges) from interaction metrics (GIoU, DTW).
  - **Quick check question:** What is the difference between semantic edge (similarity of features) and spatiotemporal edge (overlap in time/space)?

- **Concept: Neuro-symbolic AI**
  - **Why needed here:** Hybrid system using neural networks (DINO, CLIP) for perception but symbolic logic (Graph Traversal) for reasoning. Understanding this dichotomy is key to knowing which parts are probabilistic and which are deterministic.
  - **Quick check question:** Where does the "neural" part end and the "symbolic" part begin in the Med-CRAFT pipeline?

## Architecture Onboarding

- **Component map:**
  1. **Visual Extraction Layer (Pixel):** Raw Video → Grounding DINO + CLIP → Bipartite Matching → Spatiotemporal Tubelets
  2. **Graph Construction Layer (Semantic):** Tubelets → Nodes. Calculate $S_{st}$ (Overlap) & $S_{traj}$ (Trajectory) → Establish Edges → Medical Knowledge Graph
  3. **Query Synthesis Layer (Logic):** Graph → DFS Traversal → Logical Path → LLM (Qwen3-VL) → Natural Language Question

- **Critical path:** The Entity Relationship Modeling (Section 3.2). If edges connecting "cotton swab" to "iodine" are weak (low priority score), DFS will never find multi-hop path "cotton swab → dips in → iodine," and benchmark will lack complex questions.

- **Design tradeoffs:**
  - **Real vs. Synthetic:** Uses real video (high domain fidelity, noisy tracking) vs. simulation (perfect tracking, low visual realism)
  - **Determinism vs. Diversity:** DFS ensures valid logic but may miss diverse linguistic phrasings that pure LLM might hallucinate. System trades linguistic diversity for logical verifiability.

- **Failure signatures:**
  - **Fragmented Tubelets:** Objects appearing/disappearing rapidly (Visual Layer failure)
  - **Disconnected Graph:** Valid medical steps exist but not linked (Graph Layer failure; threshold too high)
  - **Unanswerable Questions:** Question asks "Why did X happen?" but graph path only shows "X happened" (Synthesis Layer failure)

- **First 3 experiments:**
  1. **Visual Probe:** Run Visual Extraction Layer on single video. Visualize "Spatiotemporal Tubelets" as overlays. Do bounding boxes persist through occlusion?
  2. **Graph Topology Analysis:** Generate KG and print degree of connectivity. Is graph fully connected, or are there isolated "islands" of instruments?
  3. **Hop-Count Validation:** Generate 10 "Complex" (multi-hop) queries. Manually verify if ground-truth video segment contains full causal chain described in question.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can Med-CRAFT pipeline effectively integrate disparate medical modalities, such as CT scans and electronic health records (EHR), into the knowledge graph to evolve benchmarks from visual perception tasks to comprehensive clinical diagnosis? [explicit] Authors state in Future Work, "We plan to integrate disparate medical modalities... evolving the benchmark from visual perception to comprehensive clinical diagnosis." Why unresolved: Current architecture relies heavily on visual primitives from video streams. Integrating static, non-visual data like EHRs requires new alignment mechanisms. What evidence would resolve it: Modified M3-Med-Auto dataset including diagnostic queries answerable only by cross-referencing visual surgical actions with pre-operative EHR data.

- **Open Question 2:** How can model performance metrics be utilized to automatically identify "weak paths" in the Spatiotemporal Knowledge Graph to generate adversarial samples for continuous model improvement? [explicit] Authors propose an "Active Learning Feedback Loop" to "close the loop by using the performance metrics of evaluated models to automatically identify 'weak paths'." Why unresolved: Current pipeline operates linearly (Video → KG → Query). Mapping model's failure back to specific deficient graph topology requires robust attribution mechanism not defined in current system. What evidence would resolve it: Demonstrated feedback mechanism where low performance scores on specific query types trigger targeted regeneration of graph edges related to those failures.

- **Open Question 3:** Can a privacy-preserving federated execution of the Med-CRAFT pipeline be implemented to allow hospitals to generate standardized benchmarks locally without sharing raw patient video data? [explicit] Authors identify "Privacy-Preserving Federated Synthesis" as future direction to address sensitivity of medical data. Why unresolved: Pipeline depends on high-quality visual extraction and global graph construction. Federated learning typically involves sharing gradients or model weights, whereas Med-CRAFT relies on deterministic graph traversal of raw data instances, making it difficult to construct coherent global benchmark without centralizing visual features. What evidence would resolve it: Prototype system where distinct medical institutions generate statistically correlated benchmark workloads while retaining local storage of underlying raw video files.

## Limitations
- Dual-threshold (spatial + semantic) approach for edge construction may fail when fine-grained medical instruments share similar visual features but have distinct functions
- Adversarial validation loop depends heavily on GPT-4V's reasoning capabilities without establishing it as ground truth
- "Scalability" claim based on dataset size rather than computational efficiency metrics

## Confidence
- **High confidence:** Core mechanism of converting video to KG via spatiotemporal tracking and semantic similarity is well-grounded in established computer vision literature
- **Medium confidence:** Claim that DFS traversal produces logically coherent multi-hop questions is supported by internal evaluations but lacks external validation on established benchmarks
- **Low confidence:** Assertion that this approach eliminates hallucination entirely is overstated, as generator LLM still introduces linguistic variability that could contain subtle semantic errors

## Next Checks
1. **Edge quality audit:** Manually inspect 50 randomly selected KG edges to verify that spatiotemporal overlap + semantic similarity accurately captures true medical instrument interactions
2. **Cross-domain generalization:** Apply exact same pipeline to non-medical procedural videos (e.g., cooking or assembly tasks) and measure whether logical complexity remains consistent
3. **Hallucination stress test:** Design adversarial queries where KG contains ambiguous or missing information, then measure whether system gracefully declines to generate questions versus producing confident but incorrect answers