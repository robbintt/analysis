---
ver: rpa2
title: 'RLP: Reinforcement as a Pretraining Objective'
arxiv_id: '2510.01265'
source_url: https://arxiv.org/abs/2510.01265
tags:
- pretraining
- reasoning
- reinforcement
- math
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RLP (Reinforcement Learning as a Pretraining
  Objective), a method that integrates reinforcement learning directly into pretraining
  by treating chain-of-thought reasoning as an exploratory action. The key idea is
  to reward thoughts based on the information gain they provide for predicting future
  tokens, creating a verifier-free dense reward signal that can be applied at every
  position in ordinary text.
---

# RLP: Reinforcement as a Pretraining Objective

## Quick Facts
- arXiv ID: 2510.01265
- Source URL: https://arxiv.org/abs/2510.01265
- Reference count: 30
- Key outcome: RLP improves Qwen3-1.7B-Base average by 19% on 8 math-and-science benchmarks, scaling to 61.32% on Nemotron-Nano-12B-v2

## Executive Summary
RLP (Reinforcement Learning as a Pretraining Objective) integrates reinforcement learning directly into pretraining by treating chain-of-thought reasoning as an exploratory action. The key innovation is a verifier-free dense reward signal based on information gain for predicting future tokens, applied at every position in ordinary text. This approach enables models to learn independent thinking behavior earlier in training, bridging the gap between next-token prediction and chain-of-thought reasoning.

The method achieves significant performance gains across math and science benchmarks, with improvements compounding when identical post-training is applied. Scaling experiments demonstrate effectiveness from 1.7B to 12B parameters, showing 23% improvement in scientific reasoning tasks. The approach uses a shared transformer backbone for both thought generation and prediction, stabilized by an EMA baseline and PPO-style clipped surrogate objective.

## Method Summary
RLP treats chain-of-thought generation as an exploratory action with intrinsic rewards based on information gain. At each position, the model samples a latent CoT thought and predicts the next token conditioned on it. The reward is the increase in log-likelihood compared to a "no-think" baseline computed by an EMA teacher. Training uses a clipped surrogate objective applied only to thought tokens, preserving the base model's knowledge while optimizing the reasoning policy. The method operates on diverse corpora including reasoning-focused datasets and general pretraining data, requiring 170M-1B tokens for training.

## Key Results
- 19% improvement in overall average across 8 math-and-science benchmarks for Qwen3-1.7B-Base
- Scaling to Nemotron-Nano-12B-v2 increases overall average from 42.81% to 61.32%
- 23% improvement in scientific reasoning average when scaling to 12B parameters
- Largest improvements on reasoning-heavy tasks like AIME25 and MMLU-Pro

## Why This Works (Mechanism)

### Mechanism 1
Treating Chain-of-Thought (CoT) generation as an exploratory action with an intrinsic reward signal incentivizes the model to generate useful reasoning traces. At each position, the model samples a latent CoT, then predicts the next token conditioning on both the context and CoT. The reward is the information gain: the increase in log-likelihood of the observed token compared to a "no-think" baseline. This works because improving the log-likelihood of the ground-truth next token via internal reasoning correlates with the emergence of robust reasoning capabilities. If generated thoughts don't causally improve prediction, the expected reward becomes zero or negative, forcing the policy to abandon CoT generation.

### Mechanism 2
An Exponential Moving Average (EMA) of the model weights serves as a stable, moving "no-think" baseline to calculate rewards without requiring a separate value function model. A copy of the model parameters is updated slowly from the training weights, generating the baseline score without a CoT. The difference isolates the contribution of the current thought. The EMA represents a competent "average" policy that is current enough to be relevant but lagged enough to prevent reward hacking. If the EMA tracks too fast, the reward collapses to 0 and learning stops.

### Mechanism 3
Updating weights only on thought tokens via a clipped surrogate objective (PPO-style) stabilizes the reinforcement signal while preserving the base model's knowledge. The loss function applies gradients strictly to the tokens of the sampled thought, treating the reward term as a constant. This decouples the reasoning process (policy update) from the prediction task (fixed target). The log-likelihood ratio is a sufficient signal to optimize the thought generation policy without needing to differentiate through the "reasoned scorer." If gradients flow through the reward calculation, the model may learn to manipulate the score function directly rather than generating better thoughts.

## Foundational Learning

- **Concept: Policy Gradient (REINFORCE)**
  - **Why needed here:** RLP is fundamentally an RL algorithm where the "policy" is the distribution over CoT tokens.
  - **Quick check question:** Can you explain why we need a baseline (the EMA model) to reduce variance in REINFORCE?

- **Concept: PPO / Clipped Surrogate Objective**
  - **Why needed here:** The paper uses a clipped objective to prevent the thought policy from changing too drastically in a single update step.
  - **Quick check question:** What happens to the trust region if the importance ratio ρ becomes very large?

- **Concept: Cross-Entropy & Information Gain**
  - **Why needed here:** The reward is mathematically defined as the reduction in cross-entropy.
  - **Quick check question:** Does a positive reward r(c_t) imply the reasoned model assigns higher or lower probability to the observed token compared to the baseline?

## Architecture Onboarding

- **Component map:** Context -> Shared Core (Thought Policy + Reasoned Predictor) -> EMA Baseline -> Reward Engine (log-likelihood comparator)

- **Critical path:**
  1. Sample context x_{<t}
  2. Sample thought c_t from π_θ
  3. Score with CoT: Compute log-prob of ground truth x_t given (x_{<t}, c_t) using θ
  4. Score Baseline: Compute log-prob of x_t given x_{<t} using φ (no grad)
  5. Compute Advantage: A = r(c_t) - baseline (group-relative)
  6. Update: Backpropagate clipped loss through c_t tokens only

- **Design tradeoffs:**
  - Rollout Count (G): Higher G reduces variance but increases compute linearly (G=16 optimal)
  - CoT Length: Longer thoughts allow more "thinking time" but increase memory usage (2048 tokens sufficient)
  - KL Penalty (β): Paper sets β=0, finding clipping and EMA provide sufficient stability

- **Failure signatures:**
  - Thought Collapse: Generated CoTs become empty or single-token if reward signal is too weak
  - Reward Hacking: Model generates CoTs that repeat context or ground truth to artificially boost S_pred
  - Training Instability: Loss spikes if learning rate is too high relative to EMA decay rate

- **First 3 experiments:**
  1. **Sanity Check (Overfit):** Run RLP on tiny dataset (100 examples) with G=1. Verify model achieves positive reward and predicts training tokens perfectly.
  2. **Ablation on Rollouts:** Compare performance with G ∈ {1, 4, 16} to verify group-relative advantages stabilize training.
  3. **Compute-Matched Baseline:** Compare RLP (170M tokens) vs. Continuous Pretraining (170M tokens) to isolate value of RL objective vs. just more data.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the relative performance gain of RLP persist or diminish when scaling to model sizes significantly larger than 12B parameters and pretraining budgets exceeding 1B tokens?
- **Basis in paper:** The paper claims "strong scalability" but validates only up to Nemotron-Nano-12B-v2 and 1B token pretraining run.
- **Why unresolved:** Scaling laws for reinforcement pretraining objectives may differ from standard next-token prediction, and benefits at 1.7B–12B scales may not linearly transfer to 70B+ models.
- **What evidence would resolve it:** Evaluation of RLP on a 70B+ parameter model trained on trillions of tokens, comparing the relative margin over standard pretraining baselines.

### Open Question 2
- **Question:** To what extent do the sampled chain-of-thought traces correspond to interpretable semantic reasoning versus statistical artifacts that exploit the information gain reward?
- **Basis in paper:** Section 2.1 notes that without EMA lag, the objective invites "degenerate strategies," and the reward is based solely on "predictive usefulness" rather than logical validity.
- **Why unresolved:** The paper evaluates downstream accuracy but does not provide qualitative analysis of CoT traces generated during unsupervised pretraining phase.
- **What evidence would resolve it:** A human or model-based evaluation of internal CoT samples for logical coherence and factual grounding independent of next-token prediction accuracy.

### Open Question 3
- **Question:** How can the optimal completion length (thought channel size) be determined adaptively without extensive empirical sweeps?
- **Basis in paper:** Figure 2b demonstrates extreme sensitivity to completion length (performance jumps from 11.5% to 42.17%), suggesting method relies heavily on tuning this hyperparameter.
- **Why unresolved:** The paper uses fixed length of 2048 based on grid search but offers no mechanism to predict or adapt this length for new domains or model architectures.
- **What evidence would resolve it:** A dynamic stopping mechanism for thought generation that matches performance of best fixed-length baseline.

## Limitations
- Performance gains are primarily demonstrated on math and science benchmarks, with unclear effectiveness on general language understanding tasks
- The method requires extensive hyperparameter tuning (rollout count, completion length, EMA decay) that may not generalize across domains
- The information-gain reward assumes correlation between next-token prediction improvement and genuine reasoning capability, which may not always hold

## Confidence

**High Confidence:** The core technical contribution—integrating RL pretraining with CoT as exploratory actions and dense information-gain rewards—is well-specified and reproducible. The architectural details (shared core, EMA baseline, clipped surrogate loss) are clearly described.

**Medium Confidence:** The reported performance improvements (19% average gain on 8 benchmarks, scaling to 12B parameters) are likely reproducible given the detailed experimental setup, though exact numbers may vary with implementation details like clipping bounds and sampling parameters.

**Low Confidence:** Claims about the method's effectiveness on general language understanding beyond math/science, or its superiority over all existing pretraining approaches, are not sufficiently validated by the current experimental scope.

## Next Checks

**Validation Check 1:** Implement the exact RLP training procedure (with G=16 rollouts, τ=0.999 EMA, and completion length=2048) on Qwen3-1.7B-Base using only the Nemotron-Crossthink dataset for 170M tokens. Verify that the model achieves positive average rewards and improved performance on at least one benchmark (e.g., MATH500) compared to the base model.

**Validation Check 2:** Conduct an ablation study comparing RLP with: (a) standard next-token prediction pretraining on the same data volume, and (b) a "no-think" control where the CoT channel is disabled but all other components (including the reward calculation) are kept. This isolates the contribution of the RL objective from additional training data or architectural complexity.

**Validation Check 3:** Test the model's behavior on out-of-distribution reasoning tasks not included in the training corpus. Evaluate on a non-mathematical reasoning benchmark (e.g., StrategyQA or CommonsenseQA) to assess whether the CoT reasoning capability generalizes beyond mathematical domains, or whether it's overfitting to math-specific patterns.