---
ver: rpa2
title: 'GraphicBench: A Planning Benchmark for Graphic Design with Language Agents'
arxiv_id: '2504.11571'
source_url: https://arxiv.org/abs/2504.11571
tags:
- design
- expert
- layername
- description
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can plan graphic design workflows that integrate
  explicit and implicit constraints, but execution fails due to spatial reasoning,
  inter-expert dependency coordination, and action retrieval issues. Experiments with
  six LLMs on a 1,079-query benchmark show strong workflow planning (up to 0.972 design
  pass rate) but poor execution outcomes (success rates as low as 15.8%, fidelity
  0.093, VQA pass rate 22.56%).
---

# GraphicBench: A Planning Benchmark for Graphic Design with Language Agents

## Quick Facts
- **arXiv ID**: 2504.11571
- **Source URL**: https://arxiv.org/abs/2504.11571
- **Reference count**: 40
- **Primary result**: Large language models can plan graphic design workflows that integrate explicit and implicit constraints, but execution fails due to spatial reasoning, inter-expert dependency coordination, and action retrieval issues.

## Executive Summary
GraphicBench evaluates large language models' ability to plan and execute graphic design workflows. The benchmark introduces a multi-agent framework where specialized design experts (Photo Editor, Vector Graphic Editor, Layout Designer) work under a supervisor to create design plans. While models demonstrate strong workflow planning capabilities (up to 0.972 design pass rate), execution suffers with low success rates (as low as 15.8%) due to spatial reasoning failures, invalid action retrieval, and broken dependencies between expert agents.

## Method Summary
The benchmark uses a hierarchical multi-agent architecture where a supervisor agent first generates a design outline from user queries, then recruits three specialized expert agents to create individual workflows. These workflows are integrated into a unified plan and mapped to 46 predefined actions with specific parameters. The system is evaluated on 1,079 queries across six LLMs, measuring design pass rates, fidelity, and visual question answering performance.

## Key Results
- Strong workflow planning capabilities with design pass rates up to 0.972
- Poor execution outcomes with success rates as low as 15.8%
- Fidelity scores as low as 0.093 indicate significant gaps between planned and actual outputs
- Visual question answering pass rates of only 22.56% reveal execution quality issues
- 53% of failures stem from broken dependencies between expert agents

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Multi-Agent Planning Decomposition
Breaking complex graphic design tasks into specialized expert roles improves workflow planning coherence and constraint integration compared to monolithic single-agent approaches. A supervisor agent parses user queries into design outlines, recruits specialized expert agents, assigns high-level goals, and integrates individual workflows into a cohesive plan.

### Mechanism 2: Constraint Integration via Design Outline Generation
Explicit design outline generation as an intermediate step enables integration of both explicit user constraints and implicit commonsense constraints before planning. The supervisor agent produces a structured design outline that extracts explicit constraints from user queries and infers implicit constraints through commonsense reasoning.

### Mechanism 3: Action Retrieval from Fixed Tool Vocabulary
Mapping workflow steps to predefined actions with explicit parameter schemas enables executable plan generation but creates retrieval bottlenecks when models hallucinate invalid actions. Each expert agent maps workflow steps to valid actions and infers parameter values including spatial coordinates, dimensions, and RGB values.

## Foundational Learning

- **Concept: Hierarchical Task Decomposition**
  - Why needed here: Understanding how complex tasks are broken into subtasks assigned to specialized agents is essential for diagnosing where coordination breaks down (53% of errors stem from dependency issues)
  - Quick check question: Can you identify at least two scenarios where sequential expert handoffs create dependency risks in this architecture?

- **Concept: Spatial Reasoning in LLMs**
  - Why needed here: The paper identifies spatial reasoning as a primary failure mode—models generate correct high-level plans but fail to produce accurate coordinate values for positioning elements
  - Quick check question: How would you distinguish between a planning failure (wrong sequence of actions) and a spatial reasoning failure (correct sequence but wrong coordinates)?

- **Concept: Action Space Grounding**
  - Why needed here: The gap between natural language workflow descriptions and executable tool calls requires mapping abstract intentions to concrete API parameters—a fundamental challenge in tool-using agents
  - Quick check question: What information would an agent need to correctly infer the `posX` and `posY` parameters for centering an element in a document?

## Architecture Onboarding

- **Component map**: User Query → Design Outline → Expert Recruitment → Workflow Generation per Expert → Supervised Workflow Integration → Action Retrieval with Parameters → Sequential Execution → Design Output

- **Critical path**: The Workflow Supervision step (resolving inter-expert dependencies) is the highest-risk coordination point based on error analysis

- **Design tradeoffs**:
  - Fixed 46-action vocabulary ensures executability but limits expressiveness; dynamic action generation could expand capabilities but risks invalid code
  - Three-expert specialization aligns with real design tools but creates handoff complexity; fewer experts reduce coordination overhead but lose domain focus
  - Supervisor-based integration provides explicit dependency management but creates single point of failure; distributed coordination could be more robust but harder to debug

- **Failure signatures**:
  - Dependency errors (53%): Workflow references file that wasn't saved by previous expert, or duplicate actions across experts
  - Invalid actions (up to 23%): Model hallucinates action name not in vocabulary
  - Spatial positioning failures: Elements placed outside document boundaries or overlapping incorrectly
  - Incomplete execution: Workflow terminates early after one expert's actions

- **First 3 experiments**:
  1. Spatial reasoning probe: Isolate coordinate inference by providing gold action sequences and measuring parameter accuracy
  2. Dependency tracing: Add explicit file-existence checks between expert handoffs to quantify preventable dependency errors
  3. Action vocabulary expansion test: Manually add 10 commonly-hallucinated actions to measure error reduction

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do Visual Language Models (VLMs) perform on GraphicBench compared to the text-only LLMs evaluated in the paper?
- **Basis in paper**: The Limitations section states, "a key complementary study still remains – evaluating GRAPHICBENCH with visual language models."
- **Why unresolved**: Current experiments rely on LLMs processing image captions rather than raw visual data, potentially missing visual nuances required for spatial reasoning.
- **What evidence would resolve it**: Benchmarking VLMs (e.g., GPT-4V) using raw image inputs on the GraphicBench tasks and comparing fidelity and spatial reasoning error rates against text-only baselines.

### Open Question 2
- **Question**: Can dynamic action generation improve execution success rates over the fixed set of 46 manually defined actions?
- **Basis in paper**: The Limitations section notes the action set is fixed and suggests "future works could investigate automated methods for dynamically generating and retrieving actions."
- **Why unresolved**: The current manual definition of actions limits the agent's flexibility and scope within the Adobe CC environment.
- **What evidence would resolve it**: Implementing an automated code generation module for actions and comparing the execution success rate and action coverage against the current static setup.

### Open Question 3
- **Question**: Does incorporating interactive clarification loops improve planning for vague user queries?
- **Basis in paper**: The Limitations section highlights the assumption of explicit specifications and proposes exploring scenarios where "models seek clarification or request additional details."
- **Why unresolved**: The current framework assumes user queries are relatively complete, whereas real-world requests are often high-level and underspecified.
- **What evidence would resolve it**: Modifying the framework to include a user-simulation clarification step and measuring the increase in design pass rates for ambiguous queries.

### Open Question 4
- **Question**: How can multi-agent architectures be modified to resolve the global dependency errors that account for 53% of execution failures?
- **Basis in paper**: The Error Analysis shows the majority of failures stem from "dependency issues," specifically global dependencies between experts (e.g., file handoffs).
- **Why unresolved**: The current "Workflow Supervision" step fails to effectively track and validate the state of artifacts transferred between the Photo Editor, Vector Editor, and Layout Designer.
- **What evidence would resolve it**: Introducing a shared memory or state-verification mechanism between agents and measuring the reduction in global dependency errors.

## Limitations
- The benchmark evaluates a fixed vocabulary of 46 actions aligned with Adobe Creative Cloud scripting, but doesn't explore whether alternative action representations could improve execution success rates
- Error analysis shows 53% of failures stem from broken dependencies between expert agents, yet the supervisor's dependency resolution strategy remains underspecified
- The benchmark's evaluation metrics assume ground truth comparisons are feasible and comprehensive, but the methodology for establishing these ground truths isn't detailed

## Confidence

- **High confidence**: Workflow planning capabilities (design pass rate 0.972) are well-established through systematic evaluation across six LLMs with consistent results
- **Medium confidence**: The three-Expert specialization architecture is justified by real design tool usage patterns, though the optimal number of experts and their boundaries remain open questions
- **Low confidence**: The supervisor's implicit constraint inference mechanism is asserted but not empirically validated—there's limited evidence that LLMs can reliably infer "commonsense" design constraints that align with human expectations

## Next Checks

1. **Ground truth verification**: Audit a random sample of 50 ground truth workflows to assess annotation consistency and identify systematic biases in the benchmark construction
2. **Expert boundary stress test**: Create tasks requiring simultaneous multi-expert coordination to test whether the three-expert decomposition creates unnecessary coordination overhead
3. **Spatial reasoning isolation**: Implement a controlled experiment where action sequences are held constant while only coordinate parameters vary, to quantify the contribution of spatial reasoning failures versus planning failures