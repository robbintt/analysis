---
ver: rpa2
title: Revisiting Bayesian Model Averaging in the Era of Foundation Models
arxiv_id: '2505.21857'
source_url: https://arxiv.org/abs/2505.21857
tags:
- weights
- training
- classification
- datasets
- posterior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work revisits Bayesian model averaging (BMA) to ensemble pre-trained
  and/or lightly-finetuned foundation models for image and text classification. To
  make BMA tractable, the authors freeze foundation model parameters and train only
  lightweight linear classifiers, treating them as the learnable components in BMA.
---

# Revisiting Bayesian Model Averaging in the Era of Foundation Models

## Quick Facts
- arXiv ID: 2505.21857
- Source URL: https://arxiv.org/abs/2505.21857
- Reference count: 40
- Primary result: BMA and OMA improve foundation model ensembling for image and text classification, with OMA outperforming output averaging by up to 3.11% on GLUE benchmarks

## Executive Summary
This work revisits Bayesian model averaging (BMA) for ensemble learning with foundation models, addressing the computational intractability of traditional BMA through a practical framework. By freezing foundation model parameters and training only lightweight linear classifiers, the authors enable tractable posterior computation over model weights. They propose both a Bayesian approach (BMA) that accounts for model complexity and an optimizable approach (OMA) that directly minimizes prediction entropy. Experiments show significant improvements over simple output averaging across ImageNet variants and GLUE benchmarks while requiring minimal computational resources.

## Method Summary
The authors freeze foundation model parameters and train linear classifiers as learnable components, enabling tractable Bayesian model averaging through Laplace approximation. For BMA, they compute model posteriors using block-diagonal Hessian approximation to estimate marginal likelihoods, which balance data fit against model complexity. OMA directly optimizes ensemble weights by minimizing expected prediction entropy on validation data. The framework supports both zero-shot and MAP-trained models, with BMA favoring simpler models through its complexity penalty while OMA provides a computationally efficient alternative that doesn't require training labels.

## Key Results
- BMA improves over output averaging by up to 3.78% on ImageNet-V2
- OMA combining zero-shot and MAP estimates outperforms single models and reaches comparable accuracy to fully finetuned methods
- OMA improves over output averaging by up to 3.11% on GLUE benchmarks
- The approach requires only linear classifiers to be trained, making it accessible with modest GPU resources

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Freezing foundation model parameters and training only linear classifiers enables tractable Bayesian model averaging with principled posterior weights.
- **Mechanism:** The Laplace approximation converts an intractable marginal likelihood integral into a closed-form computation by approximating the posterior as Gaussian around the MAP estimate. The log marginal likelihood simplifies to three terms: data fit under MAP weights, L2 penalty on weights, and a complexity penalty via the Hessian log-determinant.
- **Core assumption:** The block-diagonal Hessian approximation sufficiently captures model uncertainty for computing posteriors.
- **Evidence anchors:** Section 3.1.1 and 3.1.2 detail the Laplace approximation and block-diagonal Hessian approach; related work on Bayesian last-layer methods supports this approximation.
- **Break condition:** If off-diagonal Hessian elements become significant for a given model-dataset pair, posterior weights may mis-rank model utility.

### Mechanism 2
- **Claim:** Direct optimization of ensemble weights by minimizing expected prediction entropy (OMA) improves performance, especially under distribution shift where training-based posteriors are unreliable.
- **Mechanism:** Lemma 2.1 proves that the entropy of model-averaged predictions is always ≤ any single model's prediction entropy. OMA exploits this by treating ensemble weights β as optimizable parameters, minimizing the objective L(β) = expected cross-entropy + regularization.
- **Core assumption:** Lower expected entropy on validation data correlates with better generalization.
- **Evidence anchors:** Section 3.2 describes the OMA optimization; Table 2 shows OMA improves over output averaging on all OOD datasets.
- **Break condition:** If validation set labels are unavailable and the entropy minimization drives weights toward overconfident-but-wrong models, OMA will degrade.

### Mechanism 3
- **Claim:** BMA's marginal likelihood formulation provides a built-in complexity penalty that prevents overfitting to training data, unlike simple output averaging.
- **Mechanism:** The log marginal likelihood contains three terms: (1) log-likelihood rewarding data fit, (2) L2 penalty on MAP weights, (3) log|H·Sα + I| penalizing model complexity via the Hessian.
- **Core assumption:** The complexity penalty from the Hessian term meaningfully captures model capacity relevant to generalization for frozen feature extractors.
- **Evidence anchors:** Section 3.2 discusses BMA vs OMA tradeoffs; Fig. 2 shows learned posterior weights for ImageNet-1K.
- **Break condition:** If all candidate models have similar complexity, the complexity penalty term may not differentiate them meaningfully.

## Foundational Learning

- **Concept: Laplace Approximation**
  - Why needed here: Core technique for computing approximate Bayesian posteriors over linear classifiers; enables tractable marginal likelihood estimation for millions of parameters.
  - Quick check question: Can you explain why approximating the posterior as Gaussian around the MAP estimate enables closed-form marginal likelihood computation?

- **Concept: Marginal Likelihood (Model Evidence)**
  - Why needed here: Determines posterior model probabilities that weight ensemble predictions; balances fit vs. complexity automatically.
  - Quick check question: Why does marginal likelihood integrate over parameters rather than using point estimates?

- **Concept: Entropy of Predictive Distributions**
  - Why needed here: Foundation for OMA's objective function; lower entropy indicates more confident predictions.
  - Quick check question: Why does Lemma 2.1 guarantee that BMA predictions have entropy ≤ any single model's predictions?

## Architecture Onboarding

- **Component map:** Feature extractors (frozen OpenCLIP models ϕ₁...ϕₗ) → Linear classifiers (w₁...wₗ) → Hessian computation (block-diagonal) → Posterior weight calculator → OMA optimizer

- **Critical path:**
  1. Preprocess all data through each feature extractor (9-24 hours for ImageNet-1K on RTX 4090)
  2. Train linear classifiers via MAP (≤2 mins)
  3. Compute block-diagonal Hessians (~3 hours for ImageNet-1K per model)
  4. Compute posterior weights and/or run OMA optimization

- **Design tradeoffs:**
  - BMA vs. OMA: BMA accounts for model complexity but requires training labels; OMA works unsupervised but may overfit to confident-but-wrong models
  - Block-diagonal vs. full Hessian: Block-diagonal fits in memory but ignores inter-class correlations; full Hessian is intractable for large C
  - Number of models L: More models increase inference cost linearly but improve robustness

- **Failure signatures:**
  - BMA underperforms output averaging → Large distribution shift; training-based posteriors invalid for test data
  - OMA weights concentrate on single model → Over-regularization or homogeneous model pool
  - Memory errors during Hessian computation → Reduce subsampling ratio or use smaller batch sizes

- **First 3 experiments:**
  1. Reproduce BMA on ImageNet-1K with 2-3 OpenCLIP models; verify posterior weights match Fig. 2 qualitatively
  2. Compare BMA vs. OMA vs. output averaging on ImageNet-V2 (small shift) and ImageNet-R (large shift); confirm OMA advantage on OOD
  3. Ablate prior variance α (per Supplementary G.1) to understand sensitivity of posterior weights to regularization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the BMA framework perform when applied to modalities beyond vision (e.g., audio, multimodal, or structured data) with pre-trained foundation models?
- Basis in paper: [explicit] The author states: "it is hard to employ our BMA framework beyond the image domain, where we do not know how useful the features of the pre-trained foundation models are."
- Why unresolved: The paper only demonstrates BMA on image classification with OpenCLIP models; feature quality and linear classifier effectiveness in other domains remains unexplored.
- What evidence would resolve it: Empirical evaluation of BMA with foundation models from other modalities (e.g., audio encoders like Whisper, multimodal models) on corresponding classification benchmarks.

### Open Question 2
- Question: How does mixture-of-experts (ME) compare to BMA for foundation model ensembling in this setting?
- Basis in paper: [explicit] The supplementary material states: "it is an intriguing future direction to test ME in the same setting."
- Why unresolved: The paper uses BMA but notes the conceptual similarity to ME; no empirical comparison was conducted.
- What evidence would resolve it: Direct comparison of BMA vs. ME formulation using the same foundation models and datasets, measuring classification accuracy and computational cost.

### Open Question 3
- Question: Can OMA be extended to explicitly incorporate epistemic uncertainty without sacrificing its computational efficiency?
- Basis in paper: [explicit] Section 3.2 notes: "Unlike BMA, OMA does not explicitly account for epistemic uncertainty (model uncertainty), which could affect robustness since epistemic uncertainty can be attributed to limited or shifted training data."
- Why unresolved: OMA optimizes weights to minimize entropy but lacks the principled uncertainty quantification that BMA provides through marginal likelihood.
- What evidence would resolve it: A modified OMA objective that includes epistemic uncertainty terms, evaluated on distribution shift benchmarks to assess robustness improvements.

## Limitations
- The block-diagonal Hessian approximation's effectiveness for foundation model ensembling lacks direct validation
- OMA's unsupervised weight optimization may overfit to confident-but-wrong models under extreme distribution shift
- The choice of 8 specific OpenCLIP models as representative foundation models may not generalize to all model architectures

## Confidence

- **High confidence**: BMA improves over output averaging on ImageNet-1K (up to +3.78%); OMA improves over output averaging on GLUE (up to +3.11%); the Laplace approximation framework is mathematically sound.
- **Medium confidence**: OMA's advantage on OOD datasets (ObjectNet +2.49%, Img-R -1.18%) requires more distribution shift analysis; the complexity penalty's practical importance for foundation model ensembling is theoretically justified but not empirically validated against simpler alternatives.
- **Low confidence**: The choice of 8 specific OpenCLIP models as representative foundation models; whether block-diagonal Hessian suffices for all model architectures.

## Next Checks

1. **Block-diagonal vs. full Hessian validation**: Compute full Hessian on a small subset of ImageNet-1K (e.g., 100 examples, 5 classes) and compare posterior weights to block-diagonal approximation. Quantify the rank correlation and weight differences.

2. **OMA robustness to distribution shift**: Evaluate OMA on ImageNet-R with varying validation set contamination (mix in some training-domain examples) to test sensitivity to validation distribution shift. Monitor if weights concentrate on overconfident models.

3. **Ablation of complexity penalty**: Implement BMA without the log|H·Sα + I| term (only data fit + L2) and compare ImageNet-1K performance. This isolates whether the complexity penalty meaningfully impacts model selection beyond simple likelihood weighting.