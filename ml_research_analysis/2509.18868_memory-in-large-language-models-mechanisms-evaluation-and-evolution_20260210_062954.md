---
ver: rpa2
title: 'Memory in Large Language Models: Mechanisms, Evaluation and Evolution'
arxiv_id: '2509.18868'
source_url: https://arxiv.org/abs/2509.18868
tags:
- memory
- evaluation
- knowledge
- retrieval
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for evaluating and governing
  memory in large language models (LLMs). The authors define LLM memory as a persistent
  state written during pretraining, finetuning, or inference that can later be addressed
  and stably influences outputs.
---

# Memory in Large Language Models: Mechanisms, Evaluation and Evolution

## Quick Facts
- **arXiv ID:** 2509.18868
- **Source URL:** https://arxiv.org/abs/2509.18868
- **Reference count:** 40
- **Primary result:** A unified evaluation and governance framework for LLM memory that bridges theoretical mechanisms with practical engineering requirements, enabling standardized evaluation and safe memory management across the full LLM lifecycle.

## Executive Summary
This paper presents a comprehensive framework for understanding and managing memory in large language models (LLMs). The authors propose a four-way taxonomy (parametric, contextual, external, and procedural/episodic) and introduce a "memory quadruple" characterization system. The core contribution is a three-setting evaluation protocol that decouples model capability from information availability, enabling apples-to-apples comparisons across different memory mechanisms. The framework also proposes DMM-Gov, a governance system for coordinating memory updating and forgetting through continued pretraining, parameter-efficient finetuning, model editing, and retrieval-augmented generation.

## Method Summary
The method centers on a three-setting evaluation protocol (parametric-only, offline retrieval, online retrieval) that isolates model capability from information availability on identical data slices. This protocol enables standardized comparison across memory types by running experiments under controlled conditions. For governance, DMM-Gov coordinates multiple updating mechanisms—continued pretraining, parameter-efficient finetuning, model editing (ROME, MEND, MEMIT, SERAC), and RAG—into an auditable loop covering admission thresholds, progressive rollout, online monitoring, reversible rollback, and change audit certificates. The framework builds layered metrics for each memory type: parametric memory uses closed-book recall and edit differentials; contextual memory focuses on position curves and mid-sequence performance; external memory evaluates answer correctness alongside snippet-level attribution; procedural/episodic memory examines cross-session consistency.

## Key Results
- Introduces a unified "memory quadruple" framework (storage location, persistence, write/access path, controllability) for characterizing LLM memory types
- Proposes a three-setting evaluation protocol that decouples model capability from information availability on the same data slice
- Identifies critical phenomena including "Lost in the Middle" for contextual memory and the need to separate correctness from faithfulness in RAG evaluation
- Proposes DMM-Gov as a comprehensive governance framework coordinating multiple memory updating mechanisms
- Highlights open research questions around causal localization, Pareto-optimal editing, and retrieval vs. context window tradeoffs

## Why This Works (Mechanism)

### Mechanism 1: FFN as Key-Value Parametric Memory
The paper surveys evidence suggesting mid-layer MLPs function as associative memories where key vectors correspond to input contexts and value vectors predict subsequent tokens. By isolating these causal mediators, methods like ROME can apply rank-one updates to overwrite specific associations. This localization enables "surgical" edits without full retraining, assuming knowledge is sufficiently localized rather than distributed diffusely.

### Mechanism 2: Positional Calibration in Contextual Memory
Standard attention mechanisms dilute focus on mid-sequence tokens, creating a "Lost in the Middle" phenomenon where performance drops in the center of long contexts. The U-shaped performance curve (high at start/end, low in middle) is identified as a structural artifact of current Transformer attention rather than a data anomaly, requiring structured reordering or "anchor-guided" prompting to recover utility.

### Mechanism 3: Retrieval as Non-Parametric Override
External memory (RAG) improves timeliness and factuality only if retrieval quality is decoupled from generation faithfulness during evaluation. Retrieval systems inject evidence into the context window, which the model must then "read" and prioritize over stale parametric knowledge. The "source-first" evaluation approach ensures retrieval recall sets the performance ceiling.

## Foundational Learning

- **Concept: Causal Tracing / Mediation Analysis**
  - **Why needed here:** To move beyond correlation and understand where specific facts are processed within layers to enable precise model editing
  - **Quick check question:** Can you describe the difference between a "clean run," a "corrupted run," and a "corrupted-with-restoration run" in identifying a knowledge storage site?

- **Concept: Position-Performance Curves (Lost in the Middle)**
  - **Why needed here:** To diagnose why an LLM fails to utilize a document placed in the center of a long prompt, distinguishing between context length limits and attention allocation failures
  - **Quick check question:** If accuracy follows a U-shaped curve relative to context position, what does that imply about standard attention mechanisms?

- **Concept: Groundedness vs. Correctness**
  - **Why needed here:** Crucial for evaluating RAG; a model might give a factually correct answer based on luck while ignoring or hallucinating the citation, which is a governance failure
  - **Quick check question:** Why is a high "Answer Accuracy" score potentially misleading if "Citation Recall" is low in a high-stakes domain?

## Architecture Onboarding

- **Component map:** Parametric (Weights/FFN layers) -> Contextual (Activation/KV Cache) -> External (RAG Pipelines: Retriever -> Reranker -> Generator) -> Procedural (Session logs/Timelines: Event Store)

- **Critical path:** The Write -> Read -> Inhibit/Update loop. You must map how data enters the system (Pretraining/Finetuning), how it is accessed (Inference/Attention), and how it is modified (Model Editing/RAG overrides)

- **Design tradeoffs:**
  - Parametric vs. External: Speed vs. Timeliness. Weights are fast but static; RAG is slower but updateable
  - Locality vs. Generality: Aggressive model editing risks damaging unrelated capabilities

- **Failure signatures:**
  - Hallucination: Generation unsupported by context or parameters (Low FActScore)
  - Mid-sequence Drop: Failure to retrieve "needles" placed in the middle of the haystack
  - Catastrophic Forgetting: Drawdown in general performance after model editing

- **First 3 experiments:**
  1. **Position Sensitivity Stress Test:** Run "Needle in a Haystack" retrieval task across varying context lengths (4k to 128k) to map position-performance curve
  2. **RAG Attribution Audit:** Evaluate RAG pipeline separating Retrieval Recall from Groundedness
  3. **Model Edit Delta:** Apply single fact edit (e.g., ROME) and measure "Edit Success Rate" vs. "Locality" to validate "small-step editing" principle

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Do minimal identifiability conditions for "knowledge loci" exist in LLMs without strong structural priors, and can they be reproduced across model families?
**Basis in paper:** Section 6.1, "Open Proposition A"
**Why unresolved:** Current localization evidence relies on intervention heuristics and correlational observations rather than a unified, falsifiable causal paradigm
**What evidence would resolve it:** Reproducible studies across diverse architectures establishing falsifiable mediation effects between specific weight updates and output changes

### Open Question 2
**Question:** Can combinations of causally constrained editing and verifiable unlearning attain a provable Pareto frontier balancing effectiveness, locality, and scalability?
**Basis in paper:** Section 6.3, "Open Proposition C"
**Why unresolved:** Current methods often trigger neighborhood spillover or downstream regression, and "as if never learned" remains unstable under adversarial distributions
**What evidence would resolve it:** Demonstration of editing method maximizing ESR while maintaining strict locality and minimal drawdown on adversarial test sets

### Open Question 3
**Question:** Under fixed latency and cost budgets, can high-quality retrieval combined with small-window replay systematically outperform ultra-long-context direct reading?
**Basis in paper:** Section 6.4, "Open Proposition D"
**Why unresolved:** While "visible ≠ usable" is established, specific boundary conditions and cross-domain transferability remain unidentified
**What evidence would resolve it:** Unified protocol comparing RAG-based replay against ultra-long-context models on RULER benchmark, controlling for token budgets and latency

## Limitations

- **Scalability Constraints:** Three-setting evaluation protocol requires 3x compute resources, making it infeasible for very large models (>100B parameters)
- **Open-Ended Memory Types:** Procedural/episodic memory category remains least developed with only two primary evaluation metrics
- **Implementation Ambiguity:** Key governance mechanisms lack precise specifications, with "Minimum Identifiability" conditions presented as open proposition rather than concrete implementation

## Confidence

**High Confidence (⭑⭑⭑):** The four-way taxonomy and memory quadruple framework are well-grounded in existing literature and provide clear coordinate system

**Medium Confidence (⭑⭑):** Mechanistic hypothesis about FFNs functioning as addressable key-value parametric memory is supported by causal tracing studies, though localization across architectures remains uncertain

**Low Confidence (⭑):** Governance framework's practical effectiveness in production environments hasn't been validated at scale; coordination of multiple updating mechanisms remains largely theoretical

## Next Checks

1. **Position-Performance Validation:** Implement "Needle in a Haystack" stress test across multiple context window sizes (4k, 16k, 128k) to empirically verify U-shaped performance curve and quantify mid-sequence drop

2. **Cross-Architecture Edit Stability:** Apply ROME/MEMIT model editing to three different LLM architectures (GPT, Llama, Mistral) and measure edit success rate and locality preservation across 50+ factual edits

3. **Temporal Consistency Audit:** Implement DMM-Gov governance loop on production RAG system and track correlation between update frequency, answer freshness, and hallucination rates over 30-day period with 1000+ daily interactions