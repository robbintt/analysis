---
ver: rpa2
title: 'HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local
  Guidance'
arxiv_id: '2508.19076'
source_url: https://arxiv.org/abs/2508.19076
tags:
- milestone
- task
- action
- agent
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HiPlan, a hierarchical planning framework
  that addresses the challenge of long-horizon decision-making for LLM-based agents.
  HiPlan decomposes complex tasks into milestone action guides for global direction
  and step-wise hints for detailed actions, leveraging a milestone library constructed
  from expert demonstrations.
---

# HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance

## Quick Facts
- **arXiv ID:** 2508.19076
- **Source URL:** https://arxiv.org/abs/2508.19076
- **Reference count:** 28
- **Key outcome:** HiPlan achieves up to 32% absolute improvement in task success rates on ALFWorld and WebShop benchmarks through hierarchical milestone-based planning with adaptive guidance

## Executive Summary
HiPlan addresses long-horizon decision-making for LLM-based agents by introducing a hierarchical planning framework that decomposes complex tasks into milestone action guides and step-wise hints. The system leverages a milestone library constructed from expert demonstrations, retrieving similar task and milestone-level experiences during execution to generate adaptive guidance. This approach maintains strategic coherence while enabling local flexibility, resulting in significant improvements in task completion efficiency and success rates across multiple benchmarks.

## Method Summary
HiPlan operates through a dual-phase approach: offline library construction and online hierarchical execution. The offline phase segments expert trajectories into semantically meaningful milestones using GPT-4o, creating a vector database indexed by task and milestone embeddings. During online execution, the system first retrieves similar tasks to generate a static Milestone Action Guide, then dynamically retrieves similar milestone segments to produce step-wise hints that bridge the gap between current observations and target milestones. The LLM agent (Mixtral-8x22B or LLaMA-3.3-70B) generates actions based on historical context, global guide, and local hints.

## Key Results
- Achieves up to 32% absolute improvement in task success rates compared to strong baselines
- Reduces task completion steps by 28-37% compared to single-level planning approaches
- Demonstrates consistent performance gains across ALFWorld and WebShop benchmarks with different LLM architectures

## Why This Works (Mechanism)

### Mechanism 1: Intermediate-Granularity Retrieval
Retrieving milestone-level trajectory segments balances the generalizability of task-level plans with the specificity of action-level traces, reducing noise while preserving actionable context. This approach segments expert trajectories into semantically meaningful chunks and indexes them, allowing the system to retrieve the specific segment relevant to the current sub-goal rather than a full noisy trajectory or isolated action.

### Mechanism 2: Hierarchical Global-Local Synergy
Decoupling planning into a static Milestone Action Guide (Global) and dynamic Step-wise Hints (Local) resolves the conflict between strategic coherence and tactical adaptability. The Global Guide prevents the agent from drifting by locking in a high-level roadmap, while Local Hints allow handling environmental stochastics without constant full re-planning.

### Mechanism 3: Explicit Gap Bridging via Context Injection
Performance improves because Step-wise Hints explicitly prompt-condition the LLM on the gap between current observation and milestone, rather than just asking for the next action. This forces the model to ground its reasoning in retrieved expert trajectory segments, analyzing the current state context and providing corrections based on successful past cases.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - **Why needed here:** The paper explicitly formalizes the environment as a POMDP, explaining why the agent cannot see the full state and requires memory and guidance
  - **Quick check question:** Can you explain why the policy depends on the history of observations and actions rather than just the current state?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** HiPlan is essentially a dual-level RAG system for agents, requiring understanding of vector similarity and embedding spaces for the Milestone Library construction
  - **Quick check question:** How does the system determine which milestone from the library is relevant to the current task state?

- **Concept: Hierarchical Planning (HL/LL)**
  - **Why needed here:** The architecture splits planning into High-Level (Milestones) and Low-Level (Hints), similar to the options framework
  - **Quick check question:** What happens if the High-Level planner produces a milestone that is physically impossible in the current environment?

## Architecture Onboarding

- **Component map:** Expert Demos -> LLM Segmenter -> Milestone Library (Vector DB) -> Task Retriever -> LLM Planner -> Milestone Action Guide; Obs + Guide -> Milestone Retriever -> LLM Hinter -> Hint -> Agent Policy -> Action
- **Critical path:** The quality of the Milestone Library. If offline segmentation creates noisy or non-atomic milestones, online retrieval will fail to provide useful Step-wise Hints
- **Design tradeoffs:** Granularity tradeoffs (too few milestones = loss of guidance; too many = retrieval noise), Retrieval Scope (M=2 tasks, P=2 milestones)
- **Failure signatures:** Semantic Misidentification (visually similar but wrong objects), Premature Transition (advancing before completion), Inefficient Exploration (looping)
- **First 3 experiments:**
  1. Reproduce Ablation: Run HiPlan-Milestone (Global only) vs. HiPlan (Global+Local) on ALFWorld subset to verify synergy
  2. Stress Test Retrieval: Inject distractor milestone into library to test hallucination resistance
  3. Segmentation Sensitivity: Use GPT-3.5 instead of GPT-4o for offline segmentation to measure library quality impact

## Open Questions the Paper Calls Out

- **Open Question 1:** How does HiPlan perform in embodied robotics or multimodal environments compared to current text-based simulations? The paper suggests extending the framework to broader tasks and domains to assess generalizability and scalability.

- **Open Question 2:** Can step-wise hint generation evolve to autonomously summarize and abstract experiences for cross-task knowledge transfer? The paper plans to investigate methods for summarizing and abstracting experience to enable effective cross-task knowledge transfer.

- **Open Question 3:** How sensitive is HiPlan to the quality and availability of offline expert demonstrations used to construct the milestone library? The paper assumes access to successful task demonstrations but doesn't analyze performance degradation with sparse or noisy data.

## Limitations

- Performance gains heavily depend on the quality of expert trajectory library and milestone segmentation process, both insufficiently specified
- Limited scope to only two benchmarks (ALFWorld, WebShop) raises questions about generalizability to other task types
- Systematic weakness in semantic misidentification when objects share functional categories (e.g., confusing tomatoes with apples)

## Confidence

- **High confidence:** Hierarchical architecture design and basic mechanism descriptions (framework structure, retrieval processes); ablation studies showing global-local synergy
- **Medium confidence:** Performance claims and comparative results; uncertain due to limited benchmark scope and unclear expert data provenance
- **Low confidence:** Claims about milestone library construction quality and milestone completion detection heuristics; critical components lack sufficient detail

## Next Checks

1. **Library Quality Stress Test:** Replace GPT-4o milestone segmenter with GPT-3.5 and measure performance degradation to isolate whether library quality drives results

2. **Retrieval Noise Sensitivity:** Systematically vary the number of retrieved tasks (M) and milestones (P) beyond M=2, P=2 values to identify optimal retrieval scope and noise thresholds

3. **Cross-Environment Generalizability:** Evaluate HiPlan on a third benchmark with different task characteristics (e.g., text-only reasoning tasks) to test whether hierarchical retrieval benefits extend beyond visual-spatial environments