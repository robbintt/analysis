---
ver: rpa2
title: 'R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability'
arxiv_id: '2511.17367'
source_url: https://arxiv.org/abs/2511.17367
tags:
- policy
- evader
- pursuers
- pursuit
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of computing real-time pursuit
  strategies in graph-based pursuit-evasion games under partial observability, where
  traditional methods become computationally expensive. The authors propose a novel
  approach that combines theoretical analysis of dynamic programming (DP) algorithms
  with reinforcement learning (RL) to create robust, generalizable pursuit policies.
---

# R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability

## Quick Facts
- **arXiv ID:** 2511.17367
- **Source URL:** https://arxiv.org/abs/2511.17367
- **Reference count:** 23
- **Primary result:** Zero-shot generalization to unseen real-world graphs, consistently outperforming standard game RL methods like PSRO

## Executive Summary
This paper addresses computing real-time pursuit strategies in graph-based pursuit-evasion games under partial observability, where traditional methods become computationally expensive. The authors propose a novel approach combining theoretical analysis of dynamic programming (DP) algorithms with reinforcement learning (RL) to create robust, generalizable pursuit policies. They prove DP algorithms remain optimal under asynchronous moves by the evader and extend these strategies to partially observable settings using a belief preservation mechanism. The method integrates this mechanism into the Equilibrium Policy Generalization (EPG) framework, enabling cross-graph RL training against asynchronous-move DP evaders.

## Method Summary
The method computes distance tables D offline for each training graph using DP algorithm, then trains a GNN policy using SAC with KL-divergence policy guidance against asynchronous-move DP evaders. Belief preservation maintains evader location uncertainty without exponential history enumeration. The GNN encoder uses 6-layer masked self-attention while the decoder uses a pointer network. Training involves 30k episodes on synthetic graphs plus 70k on urban graphs, with the policy achieving zero-shot generalization to unseen real-world graphs.

## Key Results
- R2PS achieves 0.73-1.00 success rates across 10 test graphs despite never training on them
- R2PS outperforms PSRO (which trains directly on test graphs) against all evader types
- GNN inference scales effectively with graph complexity (O(n²m) vs Õ(n^(m+1)) for DP recomputation)

## Why This Works (Mechanism)

### Mechanism 1
The DP algorithm produces optimal strategies against asynchronous-move evaders who can predict pursuer actions. The distance table D encodes minimax worst-case capture timesteps via backwards induction from terminal states. Lemma 1 proves D(np, ne) = min[max[D(sp, se)]] + 1, which means the evader's informational advantage is already accounted for in the distance computation.

### Mechanism 2
Belief preservation maintains a sufficient approximation of evader location uncertainty without exponential history enumeration. Equations (4) and (7) propagate belief through a one-step neighborhood expansion followed by observation-based pruning. The position set Pos contracts to observed nodes when detection occurs, and expands to Neighbor(Pos) \ observed_nodes otherwise.

### Mechanism 3
Cross-graph RL against optimal DP evaders produces policies that generalize zero-shot to unseen real-world graphs. The training exposes the GNN policy to diverse graph topologies with provably optimal adversary responses. The KL-regularized loss anchors exploration near DP reference actions while SAC entropy regularization prevents collapse.

## Foundational Learning

- **Two-player zero-sum Markov games and Nash equilibrium**: Understanding minimax equilibrium concepts is essential for interpreting optimality proofs and value function V*. Quick check: In a zero-sum game, if the pursuer can guarantee capture in ≤10 steps, what can you conclude about the evader's best-case survival time?

- **Graph Neural Networks with attention mechanisms**: The policy representation uses masked self-attention to encode graph structure. Understanding how attention weights aggregate neighbor information clarifies why O(n²m) inference is achievable. Quick check: If you double the number of nodes n in a graph while keeping average degree constant, how does GNN inference time scale?

- **Belief states in partially observable environments**: The belief preservation mechanism is a POMDP belief update without explicit value iteration. Understanding that belief is a sufficient statistic for optimal action selection clarifies why this approximation works. Quick check: If the observation range is 0 (no detections after initialization), how does the belief distribution evolve over 5 timesteps?

## Architecture Onboarding

- **Component map**: Preprocess (Algorithm 1 computes D) -> Belief State Module (Equations 4,7) -> GNN Encoder (6-layer self-attention) -> Decoder + Pointer Network (queries to action distribution) -> Training Loop (SAC + KL guidance)

- **Critical path**: 1) Generate training graphs → compute D tables → derive DP policies; 2) For each training iteration: sample graph, sample initial state, run episode with current πθ vs ν*; 3) Update belief state each timestep using observations; 4) Compute loss L(θ) = JSAC + β · DKLDP→πθ, backprop; 5) Repeat until convergence

- **Design tradeoffs**: Observation range 2 (minimal for hardest training; policies improve with more sensors at test time); Uniform ν in belief updates (conservative; known opponent policy improves success rates); β=0.1 guidance weight (balances DP anchoring vs RL exploration); GNN vs recomputing D (O(n²m) vs Õ(n^(m+1)) inference)

- **Failure signatures**: Pursuers cluster at "rest points" (DPPos without belief-averaging); Success rate drops on specific graphs (Hollywood Walk of Fame, Sagrada Familia, The Bund have structures unfavorable to pursuit); Belief update infrequency degrades success rates 30-50%

- **First 3 experiments**: 1) Run DPbelief pursuer on Grid Map with observation range 6; expect ~100% success rate; 2) Compare DPPos vs DPbelief on Scotland-Yard Map with observation range 2; expect ~0.44 vs 0.63 success rate; 3) Train R2PS on synthetic set only, evaluate on Eiffel Tower graph; expect success rate ~1.00 against DPsync, ~1.00 against DPasync

## Open Questions the Paper Calls Out

### Open Question 1
Can the belief preservation mechanism and resulting R2PS policy effectively adapt to dynamic graph topologies where edges are added or removed during an active pursuit episode? The Introduction motivates research by citing scenarios like "traffic jams" where edges are "frequently removed and added," but experimental evaluation focuses exclusively on generalization to unseen static graphs.

### Open Question 2
Can the performance gap between "Original" (uniform distribution) and "Known Opponent" belief updates be closed by integrating an online learning component to estimate the evader's policy ν? Section 3.2 states ν(v) is set to uniform distribution by default, while Table 4 demonstrates that utilizing known opponent information significantly improves success rates.

### Open Question 3
How does the R2PS framework perform in symmetric settings where the evader also suffers from partial observability, rather than possessing the assumed global information? Section 2.1 explicitly defines the problem such that "the evader could still obtain the global information" to avoid PSPACE-hardness associated with both players having partial information.

## Limitations
- Performance may degrade on test graphs containing structural motifs absent from training distribution
- Belief mechanism may accumulate error over long horizons, particularly when observation range is minimal
- Approach is tailored for specific asymmetric information structure (strong evader, weak pursuer)

## Confidence

**High confidence**: Cross-graph RL achieves robust zero-shot generalization; DP algorithms remain optimal against asynchronous-move evaders; belief preservation improves over position-only strategies.

**Medium confidence**: Belief mechanism maintains sufficient accuracy for long-term planning; training distribution adequately covers test graph structural space.

**Low confidence**: The approach generalizes to evaders with non-stationary or deceptive strategies; performance scales indefinitely with graph complexity.

## Next Checks

1. **Structural coverage analysis**: Quantify graph topology statistics (diameter distribution, clustering coefficients, degree distributions) in training vs test sets to formally assess coverage.

2. **Belief error accumulation**: Track belief divergence from ground truth over episode length for varying observation ranges to establish error bounds.

3. **Adversarial evader testing**: Evaluate performance against evaders using non-stationary or deceptive strategies to test robustness beyond the DP benchmark.