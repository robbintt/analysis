---
ver: rpa2
title: 'MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large
  Video Models'
arxiv_id: '2509.08538'
source_url: https://arxiv.org/abs/2509.08538
tags:
- video
- hallucination
- binary
- accuracy
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MESH, a novel benchmark designed to evaluate
  hallucinations in large video models (LVMs) by mimicking human video understanding.
  MESH employs a question-answering framework with binary and multi-choice formats,
  incorporating target and trap instances across three domains: setting, character,
  and stage.'
---

# MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models

## Quick Facts
- arXiv ID: 2509.08538
- Source URL: https://arxiv.org/abs/2509.08538
- Reference count: 40
- Primary result: Introduces MESH benchmark to evaluate hallucinations in LVMs by mimicking human video understanding through question-answering with target and trap instances across setting, character, and stage domains.

## Executive Summary
This paper introduces MESH, a novel benchmark designed to evaluate hallucinations in large video models (LVMs) by mimicking human video understanding. MESH employs a question-answering framework with binary and multi-choice formats, incorporating target and trap instances across three domains: setting, character, and stage. Experiments show that while LVMs excel at recognizing basic objects and coarse features, their susceptibility to hallucinations increases significantly when handling fine details or aligning multiple actions involving various subjects in longer videos. The results demonstrate that MESH effectively differentiates LVMs based on their ability to leverage multi-frame tokens for detecting fine features and actions, with stronger models showing better performance in video question answering and captioning tasks.

## Method Summary
MESH evaluates LVMs using a question-answering framework that mimics human video understanding through a bottom-up approach. The benchmark categorizes video comprehension into three domains: setting (objects/environment), characters (physical traits), and stage (actions/dialogue). For each domain, it generates questions with target (ground truth) and trap (negative) instances. The evaluation uses both binary (Yes/No) and multi-choice formats, with trap generation employing space-aware object selection and semantic perturbations. Questions are categorized by difficulty (basic vs. advanced) and the benchmark includes detailed metrics like Option Balance and Correct Option Balance to measure performance beyond simple accuracy.

## Key Results
- LVMs show higher hallucination rates when processing fine details (e.g., character features) compared to basic objects
- Multi-choice questions provide more effective differentiation than binary questions, reducing "yes bias" in model responses
- Longer videos (32 frames vs 8 frames) increase hallucination susceptibility, particularly for fine-grained character features
- Strong LVMs leverage multi-frame tokens effectively for detecting fine features and actions, showing better performance in video question answering and captioning tasks

## Why This Works (Mechanism)

### Mechanism 1: Bottom-Up Perception Decomposition
The benchmark deconstructs "mise-en-scène" into three layers: Setting (objects/environment), Character (physical traits), and Stage (actions/dialogue). By forcing the model to answer questions at each layer independently, the framework separates failures in basic object detection from failures in complex temporal binding. This hierarchical approach aligns with human cognitive processing and allows isolation of hallucination sources.

### Mechanism 2: Semantically Constrained Negative Sampling (Traps)
Instead of random negatives, MESH generates "trap" instances using specific perturbations: space-aware object selection, character feature substitution, and action swapping. This forces models to rely on precise visual discrimination rather than statistical likelihood. Traps are semantically plausible but factually absent, exposing models that rely on semantic correlations rather than visual evidence.

### Mechanism 3: Multi-Choice Contrastive Stress Testing
The framework uses multi-choice questions (1 target, 3 traps) to reduce "positive ratio" bias and mitigate ambiguity inherent in binary judgments. By requiring the model to select the most correct option among plausible distractors, the evaluation forces comparative reasoning and reduces the impact of subjective feature interpretation.

## Foundational Learning

- **Concept: Subject-Action Binding**
  - **Why needed here:** Models fail at binding actors to actions (e.g., detecting "a man" and "opening a door" but failing to confirm this man opened this door).
  - **Quick check question:** Given a video of two people, if Person A speaks and Person B moves, can you identify who is acting, or do you just list the actions "speaking" and "moving"?

- **Concept: Token Reduction & Pooling in Vision Encoders**
  - **Why needed here:** The paper links hallucinations in "Character" tasks to aggressive token pooling (2x2 or 4x4 layers), which sacrifices fine details (glasses, pockets) for efficiency.
  - **Quick check question:** If you compress a 64-frame video into a fixed set of visual tokens, which features are retained (motion, color) and which are likely lost (texture, small accessories)?

- **Concept: Mise-en-scène (Visual Composition)**
  - **Why needed here:** This is the theoretical basis for the benchmark. Understanding that video is composed of Setting (context), Characters (agents), and Stage (actions) is necessary to interpret the "Basic" vs. "Advanced" difficulty splits.
  - **Quick check question:** In a scene of a cooking show, distinguish the Setting (kitchen appliances) from the Stage (chopping vegetables) and the Character (the chef's uniform).

## Architecture Onboarding

- **Component map:** TVQA+ (video clips + bounding boxes + subtitles) -> GPT-4o/YOLO (feature extraction) -> Human verification -> Hallucinator Tuples generation -> Binary and MC evaluation

- **Critical path:** The Space-Centered Selection (Eq. 2 & 3). Ensuring traps are semantically linked (e.g., "bed" is a trap only if the video is not in a bedroom) is the step that differentiates this from random noise benchmarks.

- **Design tradeoffs:**
  - **Fine vs. Coarse Granularity:** The "Character" tasks range from Coarse (Gender/Shirt Color) to Fine (Pocket presence, Collar type). Tradeoff: Finer features increase difficulty but introduce annotation ambiguity.
  - **Manual vs. LLM Annotation:** GPT-4o is used for initial feature extraction to scale, but manual verification is required to prevent "LLM hallucinations" infecting the hallucination benchmark.

- **Failure signatures:**
  - **High Positive Ratio (>80%) in Binary:** Indicates the model has a strong "Yes" bias and is failing to reject traps.
  - **High COV (Correct Option Balance):** Indicates the model favors specific options (A, B, C, D) regardless of content.
  - **Performance Drop on "Origin" (64 frames):** A sharp drop from Short (8 frames) to Origin suggests the model cannot aggregate information over long contexts and gets distracted by noise.

- **First 3 experiments:**
  1. **Binary vs. MC Validation:** Run a baseline LVM (e.g., LLaVA-Video-7B) on the "Setting" subset. Compare Binary Accuracy vs. MC Accuracy to verify the "yes-bias" hypothesis.
  2. **Trap Category Ablation:** Evaluate performance on "Action Out of Video" (AO) vs. "Similar Action" (SA) in the Stage subset. Expect significantly lower accuracy on SA to confirm models struggle with fine-grained semantic distinctions.
  3. **Frame Length Sensitivity:** Test the "Character" subset using Short (8f) vs. Long (32f) clips. If accuracy drops significantly on fine-grained features for Long clips, it confirms the "longer context introduces noise" finding.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the inclusion of the audio modality affect the evaluation of "Stage" hallucinations, particularly regarding dialogue attribution and audio-visual synchrony?
- **Basis in paper:** [explicit] Appendix A states, "Our benchmark specifically examines human understanding of the visual components of videos, excluding audio."
- **Why unresolved:** The current benchmark relies solely on visual cues to identify speakers and actions, ignoring the multimodal nature of human video comprehension where audio is critical for validating "Stage" elements like dialogue.
- **What evidence would resolve it:** An extension of the MESH framework incorporating audio tracks to verify speaker identity and action consistency, comparing model performance against the visual-only baseline.

### Open Question 2
- **Question:** What architectural modifications are necessary to balance aggressive token reduction with the preservation of fine-grained visual details in long videos?
- **Basis in paper:** [explicit] Section 4.2.2 notes that "Aggressive token reduction via pooling... sacrifices detail for efficiency," resulting in performance drops in fine-grained character tasks as video length increases.
- **Why unresolved:** While the paper identifies that downsampling and pooling hurt detail retention, it does not propose or test architectural alternatives that could maintain high fidelity over long temporal sequences.
- **What evidence would resolve it:** Evaluating LVMs with dynamic token sampling or hierarchical compression architectures on the MESH benchmark to see if they maintain accuracy in "Fine" and "Stage" categories without losing efficiency.

### Open Question 3
- **Question:** Can the granular hallucination categories identified by MESH (e.g., Similar Action vs. Mixed in Video) be utilized to effectively fine-tune LVMs to reduce specific hallucination types?
- **Basis in paper:** [inferred] The paper measures hallucinations but does not explore using the diagnostic data for model improvement. The results show specific failure modes that could serve as training signals.
- **Why unresolved:** It remains unclear if the specific trap instances generated for evaluation can be repurposed as contrastive examples to train more robust video models.
- **What evidence would resolve it:** An experiment where a baseline LVM is fine-tuned using MESH's "trap" instances as negative examples, demonstrating a statistically significant reduction in the targeted hallucination categories.

## Limitations

- **Annotation subjectivity in fine-grained features** presents a critical validity concern, particularly for "Fine" category questions where human annotators may disagree on subtle distinctions.
- **Token reduction effects may be conflated with architectural differences** since the paper compares different LVMs rather than conducting controlled ablation studies on a single architecture.
- **Corpus citation weakness** limits external validation, with only 8 related works cited despite claiming novelty in diagnostic decomposition and trap generation.

## Confidence

- **High Confidence**: The core finding that LVMs hallucinate more on fine details than coarse features is well-supported by systematic progression from Setting → Character → Stage tasks.
- **Medium Confidence**: The claim that multi-choice format reduces "yes bias" is plausible but relies heavily on internal metrics rather than external benchmarks.
- **Low Confidence**: The hierarchical bottom-up processing claim lacks strong empirical validation, as the paper doesn't demonstrate models actually process information in this sequence.

## Next Checks

1. **Annotator Agreement Analysis**: Re-annotate 100 randomly selected fine-grained Character questions with multiple human annotators and calculate Fleiss' kappa to establish whether the 8.5% error rate reflects genuine ambiguity or inconsistent labeling standards.

2. **Controlled Token Reduction Ablation**: Take a single LVM architecture and systematically vary the pooling layer sizes (1x1, 2x2, 4x4) while keeping all other parameters constant to isolate the effect of token reduction on fine feature detection.

3. **Cross-Dataset Generalization Test**: Evaluate the same MESH benchmark questions on a completely different video dataset (e.g., Kinetics, Something-Something) using the same LVMs to test whether hallucination patterns are dataset-specific or represent fundamental model limitations.