---
ver: rpa2
title: 'UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous
  Temperature Forecasting'
arxiv_id: '2512.10866'
source_url: https://arxiv.org/abs/2512.10866
tags:
- linear
- test
- nlinear
- forecasting
- dlinear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates linear models (Linear, NLinear, DLinear) against
  Transformer-family architectures (Transformer, Informer, Autoformer) for long-horizon
  exogenous-only temperature forecasting. Linear models consistently outperformed
  Transformers, with NLinear achieving the best accuracy (MAE 0.2461, MSE 0.5220 on
  the official test set).
---

# UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting

## Quick Facts
- arXiv ID: 2512.10866
- Source URL: https://arxiv.org/abs/2512.10866
- Authors: Ruslan Gokhman
- Reference count: 18
- Primary result: Linear models outperformed Transformers for long-horizon exogenous temperature forecasting; NLinear achieved best accuracy (MAE 0.2461, MSE 0.5220 on official test set)

## Executive Summary
This study compares linear models (Linear, NLinear, DLinear) against Transformer-family architectures (Transformer, Informer, Autoformer) for long-horizon exogenous-only temperature forecasting. Linear models consistently outperformed Transformers, with NLinear achieving the best overall accuracy. The results demonstrate that simple, interpretable linear models remain strong baselines for time series forecasting in challenging exogenous-only settings, highlighting the limited benefit of complex architectures for this task.

## Method Summary
The study evaluates six models on the Smart Buildings Control Suite dataset for 96-step ahead temperature forecasting using only exogenous inputs. Models include three linear variants (Linear, NLinear with centering, DLinear with trend-seasonal decomposition) and three Transformers (Transformer, Informer, Autoformer). Training uses z-score normalization with training-set statistics, Adam optimizer, and early stopping on validation MAE. The task is exogenous-only, preventing autoregressive conditioning at inference.

## Key Results
- NLinear achieved best accuracy (MAE 0.2461, MSE 0.5220 on official test set)
- Linear models reached MAE 0.2862 and MSE 0.5634 overall
- Transformers failed to generalize (MAE 0.8342, MSE 1.4371)
- Transformers showed severe overfitting (training MSE 0.0330 → test MSE 1.4371)

## Why This Works (Mechanism)

### Mechanism 1: Input centering improves robustness to nonstationarity
NLinear centers input window by subtracting last timestep, applies linear projection, then adds back. This removes local level information before learning, forcing the model to predict changes rather than absolute values. Assumes nonstationary series where local mean shifts over time. Evidence: NLinear achieved best accuracy; DLinear paper notes drifting trends as key challenge.

### Mechanism 2: Explicit trend-seasonal decomposition captures multi-scale patterns
DLinear applies moving average to extract trend component, subtracts to get seasonal residual, then learns separate linear projections for each component before recombining. Assumes additive decomposition of temperature series into trend plus seasonal fluctuations. Evidence: DLinear achieved second-best test performance (MAE 0.2811); HydroFusion-LMF notes multi-scale seasonal cycles as key challenges.

### Mechanism 3: Transformers overfit with limited data and exogenous-only constraints
Transformers have high capacity via multi-head attention but overfit when training data is limited and exogenous-only constraints prevent autoregressive conditioning. Assumes exogenous-only setting provides weaker signal than autoregressive settings. Evidence: Transformer training MSE 0.0330 → test MSE 1.4371; similar pattern reported in PG&E Challenge for load forecasting.

## Foundational Learning

- **Nonstationarity and distribution shift in time series**: Why needed - Indoor temperature exhibits regime changes. Quick check - Given series drifting upward over 6 months, why might model trained on early data fail on later data even if daily patterns unchanged?

- **Additive decomposition of time series**: Why needed - DLinear's effectiveness depends on additive decomposition assumption. Quick check - If temperature series has multiplicative seasonality, would DLinear's moving-average trend extraction still be appropriate?

- **Generalization gap as overfitting diagnostic**: Why needed - Transformer's training MSE vs test MSE degradation (43×) indicates overfitting. Quick check - Model achieves near-zero training loss but 10× higher validation loss. What are three possible causes, and which is most likely given exogenous-only constraint?

## Architecture Onboarding

- **Component map**: Exogenous window (96 steps) → z-score normalization → model-specific preprocessing → linear/Transformer model → 96-step forecast

- **Critical path**: 1) Data loading with exogenous-only constraint 2) Normalization using training statistics 3) Model training with early stopping 4) Inference and metric computation in normalized space

- **Design tradeoffs**: NLinear vs DLinear - simpler centering vs explicit decomposition; Linear vs Transformer - interpretability vs flexibility; assumption that 96/96 horizon is appropriate

- **Failure signatures**: Training loss near zero but test loss >> validation loss (overfitting); linear models outperforming complex architectures on test but not train (generalization issue); MAE degrading from validation to test (distribution shift)

- **First 3 experiments**: 1) Replicate NLinear baseline with identical preprocessing 2) Ablate centering operation (vanilla Linear vs NLinear) 3) Test Transformer with aggressive regularization

## Open Questions the Paper Calls Out
- Can integrating modest nonlinearity or transfer learning improve performance over pure linear baselines as datasets grow?
- Does performance gap between linear models and Transformers remain statistically significant when accounting for variance across multiple random seeds?
- Do linear models retain advantage over Transformers when evaluated on probabilistic metrics (KL divergence) rather than point estimates?

## Limitations
- Results based on single dataset (SBCS) and specific 96/96 setup; external validation needed
- Does not explore hyperparameter sensitivity in depth or specify optimal configurations
- Does not investigate whether linear models would maintain advantage with larger datasets or lagged target access

## Confidence

**High Confidence**: Linear models outperformed Transformers on SBCS test set (MAE 0.2461 vs 0.8342); Transformer overfitting demonstrated (training MSE 0.0330 → test MSE 1.4371)

**Medium Confidence**: Linear models as strong baselines for time series forecasting beyond SBCS (supported by corpus evidence from energy forecasting challenges)

**Low Confidence**: Attribution of linear model success primarily to centering mechanism lacks direct ablation studies

## Next Checks
1. Ablation study on centering: Train NLinear variants with and without centering to quantify exact contribution
2. Cross-dataset replication: Apply NLinear to energy load forecasting and traffic prediction datasets
3. Transformer capacity scaling: Systematically vary Transformer architecture while monitoring train-test gap