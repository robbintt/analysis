---
ver: rpa2
title: 'Technical Report: Full-Stack Fine-Tuning for the Q Programming Language'
arxiv_id: '2508.06813'
source_url: https://arxiv.org/abs/2508.06813
tags:
- evaluation
- training
- dataset
- pass
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a full-stack fine-tuning pipeline for adapting
  large language models to the Q programming language, a niche language used in quantitative
  finance. The authors construct a LeetCode-style dataset, perform domain-adaptive
  pretraining, supervised fine-tuning, and reinforcement learning on Qwen-2.5 models
  across five parameter scales.
---

# Technical Report: Full-Stack Fine-Tuning for the Q Programming Language

## Quick Facts
- **arXiv ID**: 2508.06813
- **Source URL**: https://arxiv.org/abs/2508.06813
- **Reference count**: 35
- **Primary result**: Qwen-2.5 models achieve 59% pass@1 accuracy on Q programming tasks, surpassing Claude Opus-4 by 29.5%

## Executive Summary
This paper presents a full-stack fine-tuning pipeline for adapting large language models to the Q programming language, a niche language used in quantitative finance. The authors construct a LeetCode-style dataset, perform domain-adaptive pretraining, supervised fine-tuning, and reinforcement learning on Qwen-2.5 models across five parameter scales. Their best model achieves 59% pass@1 accuracy on the Q benchmark, surpassing Claude Opus-4 by 29.5% and outperforming GPT-4.1 across all model sizes. The work provides a reproducible blueprint for specializing LLMs to low-resource domains with rigorous evaluation frameworks.

## Method Summary
The authors develop a three-stage fine-tuning pipeline for Qwen-2.5 models to specialize them for the Q programming language. First, they perform domain-adaptive pretraining on a filtered corpus of ~1.6M tokens from GitHub Q repositories and Kx documentation using next-token prediction. Second, they conduct supervised fine-tuning on a custom Q-LeetCode dataset (542 train/136 test problems) covering four task types: Description-to-Q, Python-to-Q, Q-to-Python, and function completion. Third, they apply reinforcement learning with GRPO, optimizing for test case execution rewards with perfect bonus incentives. The pipeline is evaluated across five model scales (1.5B to 32B parameters) using pass@k accuracy metrics with a Q interpreter for execution verification.

## Key Results
- Best model achieves 59% pass@1 accuracy, 29.5% improvement over Claude Opus-4
- All five model sizes (1.5B to 32B) outperform GPT-4.1 on Q tasks
- Pass@8 reaches 74% for 32B model, demonstrating strong multi-attempt performance
- Small models (1.5B/3B) fail to benefit from RL, showing capacity limitations

## Why This Works (Mechanism)
The three-stage fine-tuning approach progressively builds Q language competence through increasing specialization. Domain-adaptive pretraining establishes basic Q syntax and idioms from authentic codebases. SFT then teaches problem-solving patterns through instruction-based examples across multiple task types. RL with test case rewards aligns model outputs with actual program correctness rather than just syntactic similarity. The combination addresses the fundamental challenge of low-resource domains by creating synthetic training data through model-in-the-loop generation while maintaining strict solution-test separation to prevent reward hacking.

## Foundational Learning
**Q Programming Language**: Array-based language for financial analytics with APL heritage; required for understanding domain-specific syntax and idioms.
*Why needed*: Target language for specialization; all evaluation and training data use Q syntax.
*Quick check*: Can the model generate syntactically correct Q expressions for basic array operations?

**LeetCode-style Problem Format**: Structured programming challenges with descriptions, test cases, and solution verification.
*Why needed*: Provides standardized benchmark format for evaluating programming language models.
*Quick check*: Does the model correctly parse problem descriptions and generate code that passes provided test cases?

**GRPO (Group Relative Policy Optimization)**: Reinforcement learning algorithm that optimizes policy gradients using group-relative advantage estimation.
*Why needed*: Enables alignment of model outputs with execution-based rewards rather than just likelihood.
*Quick check*: Can the model improve test case success rates through iterative RL training?

**Pass@k Metric**: Evaluation metric measuring if any of k model samples pass all test cases.
*Why needed*: Accounts for stochastic sampling in language model generation and provides robust performance measurement.
*Quick check*: Does increasing k substantially improve overall success rates?

## Architecture Onboarding
**Component Map**: Q Interpreter -> Evaluation Harness -> Pretraining Corpus -> Q-LeetCode Dataset -> SFT Stage -> RL Stage -> Final Model
**Critical Path**: Q-LeetCode dataset generation → SFT training → RL optimization → pass@k evaluation
**Design Tradeoffs**: Model-in-the-loop dataset generation enables low-resource domain adaptation but introduces potential bias; multi-task SFT provides broader capability than single-task specialization.
**Failure Signatures**: Small models fail to improve during RL (capacity limitations); improper solution-test separation causes reward hacking; pretraining overfitting occurs rapidly on large models.
**First Experiments**: 1) Verify Q interpreter correctly executes basic Q programs with expected outputs. 2) Test pass@k evaluation on a small set of Python-to-Q translations. 3) Run domain-adaptive pretraining for 100 steps and verify loss decreases.

## Open Questions the Paper Calls Out
None

## Limitations
- Extremely low-resource domain (1.6M pretraining tokens) may not generalize to other niche languages
- Model-in-the-loop dataset construction introduces potential bias through iterative quality scoring
- Reinforcement learning shows clear overfitting on small models (1.5B/3B), suggesting fundamental capacity constraints

## Confidence
- **High confidence**: Training pipeline specification, evaluation methodology, baseline comparisons
- **Medium confidence**: Dataset quality and construction methodology, pretraining corpus filtering criteria
- **Low confidence**: Generalizability to other low-resource programming languages, optimal hyperparameter selection

## Next Checks
1. **Cross-domain validation**: Apply the same three-stage fine-tuning pipeline to another niche programming language (e.g., KDB+/Q's APL-derived predecessor) with similar data scarcity to test methodology transferability
2. **Dataset bias analysis**: Reconstruct the model-in-the-loop dataset generation with transparent scoring thresholds and analyze the distribution of problem difficulty, solution length, and test case complexity to quantify potential selection bias
3. **Scaling law validation**: Systematically evaluate model performance across all five scales (1.5B through 32B) on held-out validation sets during each training stage to map the relationship between model capacity and learning efficiency in low-resource domains