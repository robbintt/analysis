---
ver: rpa2
title: 'NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning
  Tasks'
arxiv_id: '2507.01921'
source_url: https://arxiv.org/abs/2507.01921
tags:
- reasoning
- engineering
- force
- training
- science
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work conducts a systematic study of distilling reasoning capabilities
  from a strong teacher model to a weaker student model. The authors curate a high-quality
  dataset called NaturalThoughts by selecting reasoning traces from DeepSeek-R1 based
  on questions from NaturalReasoning.
---

# NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks

## Quick Facts
- **arXiv ID:** 2507.01921
- **Source URL:** https://arxiv.org/abs/2507.01921
- **Reference count:** 30
- **Primary result:** Data scaling with random sampling is a strong baseline, but difficulty-based selection of diverse reasoning strategies is more sample-efficient for distilling reasoning capabilities from DeepSeek-R1 to smaller student models.

## Executive Summary
This work systematically studies distilling reasoning capabilities from a strong teacher model (DeepSeek-R1) to a weaker student model through supervised fine-tuning on curated reasoning traces. The authors create NaturalThoughts, a high-quality dataset selected from DeepSeek-R1's reasoning traces on questions from NaturalReasoning, using multiple filtering strategies including difficulty (model disagreement) and reasoning strategy diversity. They find that while scaling up data size with random sampling provides strong baseline performance, selecting difficult examples requiring diverse reasoning strategies yields better sample efficiency. Additionally, they propose mixed System-1 and System-2 distillation to enable inference-time compute control, allowing the student model to adapt its reasoning strategy based on input difficulty.

## Method Summary
The method involves generating reasoning traces from DeepSeek-R1 on questions from the NaturalReasoning dataset (2.8M questions), then annotating these traces for reasoning strategies and verbosity using Llama-3.1-70B-Instruct. The dataset is filtered using various selection strategies: random sampling, reasoning strategy diversity, semantic embedding diversity via HDBSCAN, model disagreement for difficulty estimation, and trace length. The student models (Llama-3.1-8B-Instruct or Qwen-2.5-7B-Instruct) are trained via supervised fine-tuning with masked question loss, where only reasoning traces and final answers are used. For mixed distillation, System-1 (answer-only) and System-2 (full reasoning + answer) examples are combined with difficulty-based mixing, enabling inference-time control through explicit instructions.

## Key Results
- Data scaling with random sampling is a strong baseline, but difficulty-based selection improves sample efficiency
- Selecting examples requiring diverse reasoning strategies (self-verification, backtracking, exploration) outperforms random selection and topic-based diversity
- Mixed System-1/System-2 distillation enables inference-time compute control while maintaining high accuracy
- NaturalThoughts outperforms existing reasoning datasets on STEM reasoning benchmarks including GPQA-D, MMLU-Pro, and SuperGPQA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Difficulty-based selection improves sample efficiency in reasoning distillation
- **Mechanism:** Questions where teacher models disagree (DeepSeek-R1 vs. Llama-3.3-70B) proxy for difficulty; these require more advanced reasoning traces, providing richer learning signal per sample
- **Core assumption:** Model disagreement correlates with question difficulty and reasoning complexity
- **Evidence anchors:**
  - [abstract]: "selecting difficult examples that require more diverse reasoning strategies is more sample-efficient to transfer the teacher model's reasoning skills"
  - [section 5.1, Table 1]: Models Disagree subset at 10k achieves 39.7±0.4 GPQA-D vs. 37.6±0.6 for random
  - [corpus]: Related work (arXiv:2510.03988) confirms teacher data selection significantly impacts distillation quality
- **Break condition:** If disagreement primarily reflects teacher quirks rather than genuine difficulty, selection may introduce noise

### Mechanism 2
- **Claim:** Reasoning strategy diversity outperforms question-level diversity for transfer
- **Mechanism:** Filtering for diverse meta-reasoning strategies (self-verification, backtracking, exploration) teaches student varied problem-solving primitives rather than domain knowledge
- **Core assumption:** Annotated strategies (by Llama-3.1-70B-Instruct) meaningfully capture distinct reasoning patterns
- **Evidence anchors:**
  - [section 3.1]: "prompt Llama-3.1-70B-Instruct to identify the 'meta-reasoning' strategies throughout the thinking process"
  - [section 5.1, Table 1]: Reasoning Strategies (10k) achieves 38.8±0.5 GPQA-D vs. Topics (32.7±0.5) and Semantic Embeddings (39.4±0.5)
  - [corpus]: Weak direct evidence; related papers focus on trace quality over strategy diversity
- **Break condition:** If strategy annotations are noisy or strategies don't transfer across domains, filtering provides no benefit

### Mechanism 3
- **Claim:** Mixed System-1/System-2 distillation enables inference-time compute control
- **Mechanism:** Training with difficulty-conditioned mix (full CoT for hard, answer-only for easy) teaches student to adapt reasoning depth; explicit instructions ("Think carefully" vs. "Answer directly") provide inference-time steering
- **Core assumption:** Student can learn to calibrate effort based on implicit question difficulty signals
- **Evidence anchors:**
  - [section 5.3, Table 4]: Difficulty-based mixing achieves 38.9±0.7 accuracy in Think mode vs. 37.6±0.6 for pure System-2, with 36% fewer System-2 training examples
  - [section 5.3]: "difficulty-based mixing method achieves an accuracy of 38.9%, representing a 1.3% improvement over System-2 distillation"
  - [corpus]: KaVa (arXiv:2510.02312) compresses reasoning traces for efficiency, supporting efficiency-accuracy tradeoff
- **Break condition:** If question difficulty at inference differs from training distribution, adaptive behavior may misfire

## Foundational Learning

- **Concept: Supervised Fine-tuning (SFT)**
  - Why needed here: Distillation relies on SFT to transfer teacher's reasoning traces to student
  - Quick check question: Can you explain why SFT on reasoning traces might outperform RL alone for smaller models?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The paper distinguishes System-2 (full CoT) from System-1 (answer-only) and studies what makes CoT traces effective for transfer
  - Quick check question: What properties of CoT traces might make them more transferable than direct answers?

- **Concept: Knowledge Distillation (Teacher-Student)**
  - Why needed here: The entire framework is strong-to-weak distillation; understanding capacity gaps and transfer efficiency is essential
  - Quick check question: Why might a smaller student model struggle to replicate all teacher behaviors?

## Architecture Onboarding

- **Component map:**
  NaturalReasoning (2.8M questions) -> Teacher (DeepSeek-R1) -> NaturalThoughts seed -> Annotation (Llama-3.1-70B: domains, strategies, verbosity) -> Selection filters -> Student SFT (Llama-3.1-8B or Qwen-2.5-7B) -> Evaluation (GPQA-D, MATH-500, MMLU-Pro, SuperGPQA)
  Mixed distillation branch: System-1 (final answer only) vs. System-2 (full reasoning + answer)

- **Critical path:**
  1. Generate reasoning traces with DeepSeek-R1 (16,384 max tokens)
  2. Annotate traces for strategies and verbosity (Llama-3.1-70B-Instruct)
  3. Filter by difficulty (Models Disagree) or diversity (Reasoning Strategies)
  4. Train student with masked question loss (only train on reasoning + answer)
  5. For mixed distillation: label easy/hard, mix System-1 and System-2 responses

- **Design tradeoffs:**
  - Random sampling (simple, strong baseline, requires more data) vs. curated selection (sample-efficient, requires annotation overhead)
  - System-2 only (highest accuracy, inefficient) vs. System-1 only (efficient, lower accuracy) vs. mixed (controllable, complexity in difficulty estimation)
  - Scaling to 500k examples (better performance, compute cost) vs. curated 10k (faster iteration, may plateau)

- **Failure signatures:**
  - Performance plateaus despite scaling -> check data quality; NaturalThoughts assumes high-quality source questions
  - Mixed distillation fails to adapt -> difficulty proxy (model disagreement) may not generalize to target domain
  - Student overgenerates tokens -> System-2 distillation without efficiency controls; need mixed training
  - Poor generalization to math despite STEM gains -> paper notes source data has limited math coverage

- **First 3 experiments:**
  1. **Replicate random vs. selection at 10k scale** on your target domain: Train on random 10k vs. Models Disagree 10k; expect 1-3% gap on reasoning benchmarks
  2. **Ablate difficulty proxy:** Compare model disagreement vs. response length vs. verbosity scoring; identify which best predicts your target task performance
  3. **Pilot mixed System-1/System-2:** Train with p=0.4 System-2 random mixing; evaluate No-Think, Adaptive-Think, and Think modes to verify controllability

## Open Questions the Paper Calls Out

- **On-policy vs. off-policy distillation:** The paper uses off-policy SFT, but on-policy distillation (where student generates partial contexts for teacher evaluation) is another common approach. The findings on data selection strategies may not transfer to on-policy settings, requiring verification through experiments comparing the same selection methods in both paradigms.

- **Reinforcement learning interaction:** The implications of curated reasoning data selection could be further explored by conducting reinforcement learning following the SFT distillation stage, examining how different selection strategies affect subsequent RL training and final performance.

- **Optimal System-1/System-2 ratio:** The paper tested only three random mixing ratios (0.2, 0.4, 0.6 System-2) and difficulty-based mixing, but the optimal ratio likely depends on task type and model architecture, warranting a systematic sweep across multiple benchmarks.

- **Domain-specific math performance:** The paper notes lagging improvements on mathematical tasks due to source data distribution misalignment with math benchmarks, suggesting that domain-aware curation strategies specifically for mathematical reasoning could improve performance on MATH-500 and related evaluations.

## Limitations

- The paper assumes model disagreement between DeepSeek-R1 and Llama-3.3-70B serves as a reliable proxy for question difficulty, which may not generalize to domains where the teacher model has inherent biases or blind spots.

- The strategy diversity claims rely heavily on automated annotation by Llama-3.1-70B-Instruct, whose consistency and domain coverage remain uncertain.

- The mixed System-1/System-2 approach assumes inference-time difficulty can be estimated from training-time patterns, but real-world inputs may not follow the same distribution, potentially limiting adaptive behavior.

- Access restrictions to NaturalReasoning data could limit reproducibility and external validation.

## Confidence

- **High confidence:** Data scaling baseline effectiveness, mixed distillation enabling inference-time compute control
- **Medium confidence:** Difficulty-based selection improving sample efficiency, reasoning strategy diversity benefits
- **Low confidence:** Generalization of adaptive behavior across domains, robustness of automated strategy annotations

## Next Checks

1. **Validate difficulty proxy:** Replicate the Models Disagree vs. Random comparison on a held-out subset of your target domain, then test generalization to new domains (e.g., code reasoning or humanities) to verify difficulty proxy transferability.

2. **Ablate strategy annotations:** Compare performance of reasoning strategy-based selection against simpler heuristics like trace length or semantic diversity, using 10K-sample experiments to isolate the contribution of strategy diversity from annotation noise.

3. **Test adaptive behavior robustness:** Evaluate the mixed System-1/System-2 model on domains with different difficulty distributions than training data, measuring whether explicit inference-time instructions ("Think carefully"/"Answer directly") still produce correct mode switching.