---
ver: rpa2
title: 'OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal
  Large Models'
arxiv_id: '2507.13993'
source_url: https://arxiv.org/abs/2507.13993
tags:
- fracture
- report
- clinical
- medical
- diagnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OrthoInsight is a multimodal deep learning framework for automated
  rib fracture diagnosis and reporting from CT images. It combines YOLOv9 for fracture
  detection, a medical knowledge graph for clinical context retrieval, and a fine-tuned
  LLaVA model for report generation.
---

# OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models

## Quick Facts
- **arXiv ID:** 2507.13993
- **Source URL:** https://arxiv.org/abs/2507.13993
- **Reference count:** 40
- **Primary result:** Multi-modal deep learning framework that achieves 4.28/5 average score on automated rib fracture diagnosis and report generation from CT images

## Executive Summary
OrthoInsight is a multimodal deep learning framework for automated rib fracture diagnosis and reporting from CT images. It combines YOLOv9 for fracture detection, a medical knowledge graph for clinical context retrieval, and a fine-tuned LLaVA model for report generation. Evaluated on 28,675 annotated CT images, OrthoInsight achieves an average score of 4.28 across diagnostic accuracy, content completeness, logical coherence, and clinical guidance value, outperforming models like GPT-4 and Claude-3. The framework accurately detects and classifies rib fractures while generating detailed, clinically actionable diagnostic reports, supporting radiologists in decision-making and improving diagnostic efficiency.

## Method Summary
OrthoInsight integrates three main components: a YOLOv9 object detection model for identifying and classifying rib fractures in CT images, a medical knowledge graph for retrieving relevant clinical guidelines, and a fine-tuned LLaVA model for generating diagnostic reports. The system processes CT images through YOLOv9 to detect fracture locations and types, retrieves relevant medical knowledge using a BERT-based encoder, and then generates comprehensive diagnostic reports using the LLaVA model fine-tuned with LoRA adapters. The framework was trained on a dataset of 28,675 CT images with corresponding expert-written reports, achieving superior performance compared to baseline models in both detection accuracy and report quality.

## Key Results
- Achieved 4.28 average score across four diagnostic metrics on 28,675 CT images
- YOLOv9 detection achieved mAP@0.5 of 97% with Recall of 89%
- Outperformed GPT-4 and Claude-3 in both fracture detection and report generation
- Successfully classified five fracture types: nondisplaced, displaced, multiple, flail chest, and rib nonunion

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** YOLOv9-based detection provides structured fracture localization and type classification that grounds downstream report generation.
- **Mechanism:** The YOLOv9 model extracts spatial features from preprocessed CT images (512×512, contrast-enhanced) and learns to predict bounding boxes and fracture subtype labels (displaced, nondisplaced, multiple, flail chest, rib nonunion), creating a structured input for the report generation module.

### Mechanism 2
- **Claim:** Medical knowledge graph retrieval provides contextual evidence that improves report clinical relevance.
- **Mechanism:** The system uses a BERT-based encoder to match YOLO-detected fracture features and partial report content against a curated orthopedic knowledge graph (42k triples), retrieving relevant clinical guidelines and treatment recommendations that are incorporated into the final diagnostic report.

### Mechanism 3
- **Claim:** LLaVA fine-tuning with LoRA adapters enables efficient adaptation to medical domain while preserving general reasoning capabilities.
- **Mechanism:** The LLaVA-1.5-7B model is fine-tuned using LoRA (rank=64, alpha=16) on paired data of YOLO outputs, retrieved knowledge, and expert reports, allowing the model to generate coherent, clinically accurate diagnostic reports while maintaining its multimodal reasoning abilities.

## Foundational Learning

### YOLOv9 for Medical Object Detection
- **Why needed:** Provides accurate localization and classification of rib fractures in CT images
- **Quick check:** Verify mAP@0.5 > 97% on validation set

### Medical Knowledge Graph Construction
- **Why needed:** Supplies clinical context and treatment guidelines for report generation
- **Quick check:** Ensure retrieval similarity scores exceed 0.7 for relevant queries

### LLaVA Multimodal Fine-tuning
- **Why needed:** Enables generation of coherent diagnostic reports from visual and textual inputs
- **Quick check:** Monitor instruction tuning loss during LoRA fine-tuning

### BERT-based Text Retrieval
- **Why needed:** Matches fracture descriptions to relevant medical knowledge
- **Quick check:** Validate that retrieved triples are clinically relevant to input queries

### Multi-Modal Report Generation
- **Why needed:** Combines visual detection, clinical knowledge, and language generation
- **Quick check:** Assess report coherence and completeness across all four evaluation metrics

## Architecture Onboarding

### Component Map
YOLOv9 Detection -> Knowledge Graph Retrieval -> LLaVA Report Generation

### Critical Path
1. CT image preprocessing (resize to 512×512, contrast enhancement)
2. YOLOv9 fracture detection and classification
3. Knowledge graph retrieval using BERT encoder
4. LLaVA report generation with LoRA fine-tuning

### Design Tradeoffs
- **Detection precision vs. speed:** YOLOv9 offers real-time performance but may miss subtle fractures
- **Knowledge graph depth vs. retrieval speed:** Larger graphs provide more context but increase latency
- **LLaVA model size vs. fine-tuning efficiency:** 7B parameter model balances performance with practical fine-tuning requirements

### Failure Signatures
- **Detection failure:** Low Recall (<89%) leads to incomplete reports missing actual fractures
- **Knowledge mismatch:** BERT encoder retrieves irrelevant guidelines, producing clinically inaccurate reports
- **Generation incoherence:** LoRA fine-tuning fails to produce logically coherent reports despite accurate detection

### Three First Experiments
1. **Detection validation:** Train YOLOv9 on public rib fracture dataset (e.g., RibFrac) using specified hyperparameters, verify mAP@0.5 > 97%
2. **Knowledge retrieval test:** Construct simplified medical knowledge graph, test BERT-based retrieval with YOLO-generated queries
3. **LLaVA fine-tuning baseline:** Fine-tune LLaVA-1.5-7B on synthetic instruction dataset, evaluate report quality against GPT-4 baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does OrthoInsight's performance generalize across multi-institutional datasets with varying CT acquisition protocols and patient demographics?
- **Basis in paper:** [explicit] The conclusion explicitly states future work will focus on "expanding dataset diversity."
- **Why unresolved:** The current study relies on a single third-party repository (28.7k images) with reports written by experts from one institution (West China Hospital).
- **What evidence would resolve it:** Evaluation of Diagnostic Accuracy and Clinical Guidance Value on external, multi-center datasets.

### Open Question 2
- **Question:** Can the model maintain high performance when adapted to other complex anatomical structures or musculoskeletal injuries without architectural redesign?
- **Basis in paper:** [explicit] The authors list "exploring broader applications in medical imaging" as a primary goal for future research.
- **Why unresolved:** The framework is specialized for rib fractures, utilizing a specific medical knowledge graph and YOLOv9 training tailored to rib annotations.
- **What evidence would resolve it:** Successful application of the OrthoInsight pipeline (YOLO + KG + LLaVA) to other injury types, such as skull or long-bone fractures.

### Open Question 3
- **Question:** Does the reliance on GPT-4o for data augmentation introduce systematic biases or hallucinations that limit the student model's performance?
- **Basis in paper:** [inferred] Section IV.B notes that training targets were expanded using GPT-4o, and Section IV.C uses GPT-4o for evaluation.
- **Why unresolved:** The transfer of knowledge from the teacher (GPT-4o) to the student (LLaVA) risks propagating specific model artifacts or stylistic biases present in the synthetic reports.
- **What evidence would resolve it:** A comparative analysis of model performance when trained solely on human-verified reports versus the GPT-4o augmented dataset.

### Open Question 4
- **Question:** How robust is the system to noise or artifacts in raw clinical CT scans that have not undergone the study's specific preprocessing?
- **Basis in paper:** [inferred] The methodology relies on standardized preprocessing (resizing to 512x512, contrast enhancement) and Section IX mentions "enhancing model robustness" as future work.
- **Why unresolved:** Real-world deployment often involves uncurated, noisy images; the current evaluation uses preprocessed, cleaned data.
- **What evidence would resolve it:** Stress-testing the model on unprocessed or artifact-heavy CT scans to measure the degradation in Diagnostic Accuracy.

## Limitations

- **Dataset Dependency**: The framework's performance is heavily tied to a proprietary dataset of 28,675 CT images and corresponding expert reports that were created specifically for this study. Without access to these data, independent validation is impossible, representing a fundamental reproducibility barrier.
- **Knowledge Graph Specificity**: The 42,000-triple medical knowledge graph is described as "curated" but its construction methodology, source materials, and exact format are not specified. This limits understanding of how generalizable the retrieval component is to other anatomical regions or clinical contexts.
- **LLM Performance Variation**: The reported 4.28 average score across diagnostic metrics was evaluated using GPT-4o as a judge, but results may vary significantly with different evaluation LLMs or human expert assessment. The methodology for metric calculation (particularly "clinical guidance value") lacks sufficient detail for independent verification.

## Confidence

- **High Confidence**: The technical integration of YOLOv9 for detection and LLaVA for report generation is clearly specified with hyperparameters and follows established deep learning practices.
- **Medium Confidence**: The end-to-end workflow from detection to report generation is logically coherent, though the knowledge graph retrieval step could be more rigorously validated.
- **Low Confidence**: The reported performance metrics (4.28 average score) cannot be independently verified without the proprietary dataset and evaluation framework.

## Next Checks

1. **Detection Module Validation**: Train YOLOv9 on a public rib fracture dataset (e.g., RibFrac) using the specified hyperparameters (150 epochs, batch 16, LR 0.01, 640×640 input). Verify that mAP@0.5 exceeds 97% and that Recall remains above 89% to prevent error propagation.

2. **Knowledge Graph Construction**: Build a simplified medical knowledge graph using publicly available orthopedic textbooks and clinical guidelines (minimum 5,000 triples). Test BERT-based retrieval with YOLO-generated fracture descriptions to ensure retrieved context is relevant and actionable.

3. **LLaVA Fine-tuning Replication**: Using the LoRA configuration (rank=64, alpha=16), fine-tune LLaVA-1.5-7B on a small synthetic instruction dataset pairing simulated YOLO outputs with medical text. Evaluate report quality using a consistent rubric and compare against GPT-4/Claude baselines to assess relative performance.