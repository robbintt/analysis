---
ver: rpa2
title: Markov Chain Estimation with In-Context Learning
arxiv_id: '2508.03934'
source_url: https://arxiv.org/abs/2508.03934
tags:
- training
- markov
- transition
- transformer
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We investigate whether transformers trained on next-token prediction
  can learn to estimate Markov chain transition probabilities in-context rather than
  memorizing training patterns. We train transformers on Markov chains with randomly
  sampled transition matrices and evaluate their ability to generalize to unseen matrices.
---

# Markov Chain Estimation with In-Context Learning

## Quick Facts
- arXiv ID: 2508.03934
- Source URL: https://arxiv.org/abs/2508.03934
- Reference count: 19
- Key outcome: Transformers can learn to estimate Markov chain transition probabilities in-context rather than memorizing training patterns when above certain model and dataset size thresholds

## Executive Summary
This paper investigates whether transformers trained on next-token prediction can learn to estimate Markov chain transition probabilities in-context rather than memorizing training patterns. The authors train transformers on Markov chains with randomly sampled transition matrices and evaluate their ability to generalize to unseen matrices. They find that above certain thresholds in model size and training set size, transformers learn to estimate transition probabilities from context instead of memorizing them. Two state encoding schemes - permutation-based and random orthogonal embeddings - are proposed to enable training on a single Markov chain while maintaining generalization to chains with different numbers of states or transition statistics.

## Method Summary
The researchers train transformers on sequences generated from Markov chains with randomly sampled transition matrices. During training, the model observes sequences and learns to predict the next token, which implicitly requires learning the underlying transition probabilities. The key innovation involves using either permutation-based or random orthogonal embeddings for state representation, which allows the model to generalize to chains with different numbers of states than seen during training. The orthogonal embedding approach is particularly effective, enabling transformers to handle chains with up to twice as many states as seen during training while maintaining robustness to changes in transition probability distributions compared to standard learned embeddings.

## Key Results
- Above certain thresholds in model size and training set size, transformers learn to estimate transition probabilities in-context rather than memorizing them
- The orthogonal embedding approach enables transformers to handle chains with up to twice as many states as seen during training
- The proposed method shows better robustness to changes in transition probability distributions compared to standard learned embeddings

## Why This Works (Mechanism)
The mechanism relies on transformers learning to extract transition probability information from the context of observed sequences rather than memorizing specific transition patterns. When using appropriate state encodings (particularly orthogonal embeddings), the model develops representations that capture the statistical structure of transitions rather than specific state identities. This allows generalization to new Markov chains with different state counts or transition distributions. The in-context learning capability emerges when the model capacity and training data are sufficient to learn the underlying probability estimation task rather than overfitting to specific training examples.

## Foundational Learning
- **Markov Chain Properties**: Understanding first-order Markov chains and transition matrices is essential because the entire problem setup depends on these stochastic processes. Quick check: Can you explain what it means for a process to have the Markov property?
- **Transformer Architecture**: Knowledge of self-attention mechanisms and next-token prediction is crucial since the model must learn to predict future states from context. Quick check: How does self-attention help capture dependencies in sequential data?
- **State Representation**: Understanding how states can be embedded and represented in neural networks is necessary to grasp why orthogonal embeddings enable better generalization. Quick check: What makes orthogonal embeddings different from standard learned embeddings?
- **Generalization in Machine Learning**: Familiarity with the concept of generalization versus memorization is important to understand the core research question. Quick check: How can you distinguish between a model that memorizes versus one that learns generalizable patterns?

## Architecture Onboarding

**Component Map**: Data Generator -> Markov Chain Sampler -> Transformer Model -> Loss Function -> Parameter Updates

**Critical Path**: The critical path for in-context learning of transition probabilities involves the transformer's self-attention mechanism processing the context sequence, extracting statistical patterns from state transitions, and using this information to estimate probabilities for unseen chains. The orthogonal state embeddings play a crucial role by providing a representation space that captures transition structure rather than specific state identities.

**Design Tradeoffs**: The choice between permutation-based and orthogonal embeddings represents a key tradeoff. Permutation-based embeddings are simpler but less flexible, while orthogonal embeddings provide better generalization at the cost of more complex initialization. The researchers also trade off model size and training data quantity against generalization capability - larger models and more data enable better in-context learning but increase computational costs.

**Failure Signatures**: Models fail to learn in-context estimation when they are too small relative to the complexity of the transition matrices, when trained on insufficient data, or when using standard learned embeddings that encourage memorization of specific state identities. Performance degradation is typically seen as inability to generalize to chains with different numbers of states or transition distributions.

**First Experiments**:
1. Train a small transformer on a single Markov chain with standard embeddings and test generalization to a chain with different transition probabilities
2. Increase model size and training data quantity while keeping the same setup to observe emergence of in-context learning
3. Replace standard embeddings with orthogonal embeddings and test generalization to chains with 1.5x, 2x, and 3x the number of states

## Open Questions the Paper Calls Out
The paper leaves several open questions regarding the scalability and robustness of the proposed approach. Major uncertainties remain around performance when scaling to state spaces significantly larger than training data (beyond 2x), the behavior on highly complex transition patterns such as highly skewed distributions or long-range dependencies, and whether similar in-context learning capabilities extend to higher-order Markov chains or other stochastic processes. Additionally, the convergence speed and computational efficiency when scaling to larger state spaces or longer sequences are not characterized.

## Limitations
- Results demonstrate successful generalization for Markov chains with up to twice the number of states seen during training, but performance degradation when scaling beyond this factor is unclear
- The study focuses on first-order Markov chains, leaving open questions about whether similar in-context learning capabilities extend to higher-order chains or other stochastic processes
- The experimental methodology relies on synthetic data generation with controlled parameters, which may not fully capture challenges of real-world applications where Markov chain structure might be less well-defined or corrupted by noise

## Confidence
- **High**: Transformers can learn to estimate transition probabilities in-context rather than pure memorization, given sufficient model and dataset size
- **Medium**: The orthogonal embedding approach enables generalization to chains with up to twice as many states
- **Medium**: The proposed method shows better robustness to changes in transition probability distributions compared to standard embeddings
- **Low**: Scalability beyond twice the training state count, performance on highly complex transition patterns

## Next Checks
1. Evaluate performance degradation when scaling to state spaces 4x, 8x, and 16x larger than training data, measuring both accuracy and computational costs
2. Test the approach on higher-order Markov chains (order 2, 3, and 4) to assess whether in-context learning generalizes beyond first-order dependencies
3. Introduce structured noise patterns and corrupted transition matrices to evaluate robustness under realistic conditions where Markov assumptions may be violated