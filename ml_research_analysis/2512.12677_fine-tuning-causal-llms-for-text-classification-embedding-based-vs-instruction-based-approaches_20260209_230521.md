---
ver: rpa2
title: 'Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based
  Approaches'
arxiv_id: '2512.12677'
source_url: https://arxiv.org/abs/2512.12677
tags:
- classification
- label
- text
- fine-tuning
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study compares two parameter-efficient strategies for fine-tuning\
  \ decoder-only LLMs for text classification under resource constraints. The embedding-based\
  \ approach attaches a classification head to the LLM\u2019s final token embedding,\
  \ while the instruction-based approach reformulates classification as a prompt-response\
  \ generation task."
---

# Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches

## Quick Facts
- arXiv ID: 2512.12677
- Source URL: https://arxiv.org/abs/2512.12677
- Authors: Amirhossein Yousefiramandi; Ciaran Cooney
- Reference count: 10
- Primary result: Embedding-based approach significantly outperforms instruction-based method for single-GPU LLM text classification

## Executive Summary
This study compares two parameter-efficient strategies for fine-tuning decoder-only LLMs for text classification under single-GPU constraints. The embedding-based approach attaches a classification head to the LLM's final token embedding, while the instruction-based approach reformulates classification as prompt-response generation. Both methods leverage 4-bit quantization and LoRA adapters to enable training models up to 8B parameters on a single GPU. Experiments on proprietary and public patent datasets show the embedding-based method achieves significantly higher F1-scores (0.86 vs 0.76 on CLV, 0.785 vs 0.724 on WIPO) while using fewer trainable parameters and producing more calibrated probabilities.

## Method Summary
The study implements two approaches for fine-tuning decoder-only LLMs for text classification under single-GPU constraints. The embedding-based approach adds a classification head to the final token embedding and fine-tunes with LoRA rank-8/16 adapters. The instruction-based approach reformulates classification as a prompt-response generation task using LoRA rank-64 adapters. Both methods use 4-bit NF4 quantization for base model weights, with the embedding approach using 8-bit AdamW and the instruction approach using 32-bit Paged AdamW. Training uses gradient accumulation to achieve effective batch sizes of 8, with linear and cosine schedulers respectively. The embedding approach trains up to 20 epochs with early stopping, while the instruction approach trains for 5 epochs.

## Key Results
- Embedding-based approach achieves 0.86 F1 on CLV dataset vs 0.76 for instruction-based
- Embedding-based approach achieves 0.785 F1 on WIPO dataset vs 0.724 for instruction-based
- Embedding-based method uses significantly fewer trainable parameters (5.6M-42M vs 45.1M-167.8M)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The final token embedding in decoder-only LLMs can serve as an effective sequence representation for classification, analogous to BERT's [CLS] token.
- **Mechanism:** Causal LLMs process input autoregressively with left-to-right attention, so each token can only attend to preceding tokens. The last token's hidden state has implicitly attended to the entire sequence through successive attention layers, making it a summary representation. Attaching a linear classification head to this embedding allows direct class prediction without generation.
- **Core assumption:** The final token embedding contains sufficient task-relevant information aggregated from all prior tokens; the quality of this representation depends on the model's pre-training and the LoRA adaptation quality.
- **Evidence anchors:**
  - [abstract] "attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation)"
  - [Page 2] "Because causal LLMs process input autoregressively, the last token's state inherently attends to all previous tokens in the sequence. Thus, using that state as a summary vector allows the classifier to consider the full context of the input."
  - [corpus] Related work on PEFT strategies (arXiv:2505.07162, arXiv:2503.01131) supports parameter-efficient classification heads but doesn't directly validate the final-token mechanism.
- **Break condition:** If the input sequence exceeds the model's context window and is truncated mid-sequence, the final token may not have visibility over the most relevant content; performance degrades on very long documents.

### Mechanism 2
- **Claim:** Combining 4-bit quantization with LoRA adapters enables training models up to 8B parameters on a single GPU with minimal accuracy loss.
- **Mechanism:** Quantization compresses frozen base weights to 4-bit precision (NF4), reducing memory by ~8x. LoRA injects trainable low-rank matrices into attention and feed-forward layers while keeping base weights frozen. Gradients backpropagate through quantized weights into LoRA adapters only. This decouples memory-intensive base model storage from trainable parameter optimization.
- **Core assumption:** The quantization error introduced by 4-bit compression does not catastrophically distort the representation space for the target task; low-rank adapters have sufficient expressivity for task adaptation.
- **Evidence anchors:**
  - [abstract] "To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA)"
  - [Page 2-3] "This compresses model memory usage to roughly one-eighth of the 32-bit size, allowing even 8B parameter models to fit in GPU memory."
  - [corpus] Related paper on layer-wise fine-tuning (arXiv:2510.00268) shows similar PEFT benefits for classification tasks.
- **Break condition:** If the target task requires significant changes to the base representations (e.g., very different domain from pre-training), low-rank adaptation may be insufficient; full fine-tuning would be needed.

### Mechanism 3
- **Claim:** Direct optimization of class posteriors (embedding-based) yields more calibrated probabilities and faster inference than optimizing token likelihoods (instruction-based).
- **Mechanism:** Embedding-based classification uses cross-entropy loss directly over C class probabilities, producing a proper categorical distribution. Instruction-based training optimizes next-token prediction over the label text tokens; probability calibration depends on tokenizer verbalization and may require constrained decoding. The embedding approach requires only one forward pass plus a linear projection; instruction-based requires autoregressive token generation.
- **Core assumption:** Calibrated probabilities correlate with downstream utility (e.g., thresholding for multi-label); the verbalizer mapping from classes to text tokens does not introduce ambiguity or tokenization artifacts.
- **Evidence anchors:**
  - [Page 5, Section 2.3.1] "Approach 1 yields a calibrated distribution over C classes... Approach 2 yields a short generated sequence, which may require constrained decoding and post-processing."
  - [Page 11] "The embedding-based approach consistently produced more calibrated probability outputs, with clear confidence levels that correlated well with prediction correctness."
  - [corpus] Work on instruction tuning limitations (arXiv:2402.05119) notes calibration issues, supporting this observation.
- **Break condition:** If labels have semantic relationships not captured by independent class probabilities (e.g., hierarchical labels requiring structured outputs), the simple softmax/sigmoid formulation may be insufficient; constrained generation may be preferable despite calibration tradeoffs.

## Foundational Learning

- **Concept: Causal (Autoregressive) Attention Mask**
  - **Why needed here:** Decoder-only LLMs use causal masking where each token only attends to previous tokens. This differs fundamentally from BERT's bidirectional attention and explains why we use the final token (not an arbitrary position) for sequence representation.
  - **Quick check question:** If you truncate an input sequence to 512 tokens and take the embedding at position 256, what information is that embedding missing?

- **Concept: LoRA Rank and Target Layers**
  - **Why needed here:** The paper uses rank-8 and rank-16 LoRA adapters on attention projections (Q, K, V, O) and feed-forward layers. Understanding how rank affects expressivity vs. parameter count is critical for resource-constrained tuning.
  - **Quick check question:** Why might a rank-8 LoRA work well for single-label classification but rank-16 perform better on extreme multi-label tasks (200+ labels)?

- **Concept: Quantization-Aware Training vs. Post-Hoc Quantization**
  - **Why needed here:** QLoRA uses 4-bit quantization for frozen weights but trains LoRA adapters in higher precision. This is not training the quantized weights directly—it's training adapters that compensate for quantization effects.
  - **Quick check question:** If you observe performance degradation after quantization, should you increase LoRA rank or switch to higher-bit quantization first?

## Architecture Onboarding

- **Component map:**
Input → Tokenizer → 4-bit Quantized Base LLM (frozen) → Final Token Hidden State → Classification Head (trainable) + LoRA Adapters (trainable)

- **Critical path:**
1. Load model with 4-bit quantization (BitsAndBytes, NF4)
2. Inject LoRA adapters (target: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj)
3. For embedding approach: Attach classification head (Linear: hidden_dim → num_classes)
4. Configure optimizer (8-bit AdamW recommended for memory)
5. Use gradient accumulation if batch size < desired effective batch
6. For multi-label: Use BCE loss with sigmoid; for single-label: Cross-entropy with softmax

- **Design tradeoffs:**
- Embedding-based: Faster training/inference, fewer parameters, calibrated probabilities, but requires adding a task-specific head
- Instruction-based: More flexible (same model can handle multiple tasks via prompts), no architectural changes, but slower, more parameters, fragile to prompt variations
- LoRA rank 8 vs 16: ~2x parameter difference; rank-16 shows modest improvement on complex multi-label tasks

- **Failure signatures:**
- Llama-3.2-1B-Instruct underperforms base Llama-3.2-1B on embedding approach (0.601 vs 0.824 F1)—instruction-tuned models may have representations less suited to direct classification
- Instruction-based models produce malformed label outputs (wrong delimiter, extra text)—requires constrained decoding or robust post-processing
- Very low throughput on 7B+ models (<1 sample/sec)—indicates memory pressure or suboptimal batch configuration

- **First 3 experiments:**
1. **Baseline check:** Train embedding-based classifier on Llama-3.2-3B with rank-8 LoRA; verify you achieve F1 ≈ 0.86 on CLV-equivalent data. If significantly lower, check learning rate (target: 1-2×10⁻⁴) and LoRA target layers.
2. **Ablation on LoRA rank:** Compare rank-8 vs rank-16 on multi-label task. Expect <2% F1 difference; if larger, rank-8 may be underfitting.
3. **Calibration comparison:** Train both approaches on same data; plot reliability diagrams. Embedding-based should show better calibration (confidence closer to accuracy). If not, check sigmoid threshold for multi-label or softmax temperature.

## Open Questions the Paper Calls Out
None

## Limitations
- Proprietary CLV dataset specifics are unavailable, limiting external validation
- Instruction-based approach performance may be artificially constrained by suboptimal prompt templates and verbalizer mappings
- Study focuses on patent classification tasks; results may not generalize to other domains

## Confidence
High: Performance claims are well-supported by experiments on two datasets with clear metrics
Medium: Architectural choices (LoRA rank, quantization strategy) are justified but some hyperparameter details are unclear
Low: Proprietary dataset limits reproducibility and generalizability of findings

## Next Checks
1. Verify 4-bit quantization is properly applied before training to prevent out-of-memory errors
2. Compare calibration curves between embedding-based and instruction-based approaches on same dataset
3. Test embedding-based approach on non-patent classification tasks to assess domain generalization