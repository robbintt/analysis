---
ver: rpa2
title: 'AquiLLM: a RAG Tool for Capturing Tacit Knowledge in Research Groups'
arxiv_id: '2508.05648'
source_url: https://arxiv.org/abs/2508.05648
tags:
- aquillm
- research
- groups
- knowledge
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AquiLLM is a lightweight, modular retrieval-augmented generation
  (RAG) system designed specifically for research groups to capture and retrieve tacit
  knowledge from informal and private sources. The system integrates varied document
  types including emails, meeting notes, and training materials while supporting configurable
  privacy settings and single-tenant deployment.
---

# AquiLLM: a RAG Tool for Capturing Tacit Knowledge in Research Groups

## Quick Facts
- arXiv ID: 2508.05648
- Source URL: https://arxiv.org/abs/2508.05648
- Reference count: 16
- Primary result: Lightweight, modular RAG system for research groups to capture and retrieve tacit knowledge from internal documents

## Executive Summary
AquiLLM is a single-tenant, self-hosted RAG system designed specifically for research groups to capture and retrieve tacit knowledge from informal and private sources. Built on Django and PostgreSQL with pgvector, it integrates varied document types including emails, meeting notes, and training materials while supporting configurable privacy settings. The system employs a tool-calling architecture where the LLM controls search functions, enabling iterative multi-step retrieval rather than single-pass approaches. Deployed in beta with an astronomy group at UCLA and environmental scientists, AquiLLM has proven useful for onboarding new members and querying documentation while maintaining data sovereignty through local deployment options.

## Method Summary
The system uses a Django backend with PostgreSQL and pgvector for vector search, avoiding LLM integration frameworks to reduce maintenance overhead and enable full infrastructure control. Documents are chunked and embedded on save(), stored as TextChunk records with vectors. An LLM abstraction layer with tool calling capabilities allows the model to explore collections for information retrieval. The architecture includes a WebSocket chat interface and supports single-tenant deployment via Docker containers. Hybrid search combines vector and trigram matching with reranking, and the system can use either cloud LLM APIs or local inference through Ollama.

## Key Results
- Successfully deployed in beta with astronomy group at UCLA and environmental scientists
- Enabled new members to understand past decisions and find documentation through natural language queries
- Demonstrated effectiveness of tool-calling RAG approach for iterative, multi-step information retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic vector search overcomes terminology fragmentation in heterogeneous research document corpora.
- Mechanism: Text embeddings map semantically similar content to nearby vector space positions, enabling retrieval of conceptually related passages even when users lack precise domain-specific vocabulary.
- Core assumption: The embedding model sufficiently captures semantic relationships within the domain's technical language.
- Evidence anchors:
  - [section I.A] Traditional search tools depend on exact keyword matches and do not handle variation in terminology or document formats.
  - [section I.B] Researchers can pose natural language questions without knowing exact terminology, and the system can identify relevant information across documents that use different vocabulary.
  - [corpus] Weak direct evidence—neighbor paper "Leveraging Large Language Models for Tacit Knowledge Discovery in Organizational Contexts" addresses similar tacit knowledge challenges but does not validate embedding effectiveness for this use case.
- Break condition: Embedding model fails to capture domain-specific jargon; retrieval quality degrades to random for novel technical terms.

### Mechanism 2
- Claim: Tool-calling architecture enables iterative, multi-step retrieval that outperforms single-pass RAG pipelines.
- Mechanism: Rather than appending vector search results directly to the LLM prompt, AquiLLM exposes search functions as tools the LLM can invoke. This allows the model to issue multiple queries, refine searches based on intermediate results, and synthesize across collections dynamically.
- Core assumption: The LLM has sufficient reasoning capability to plan and execute effective search strategies.
- Evidence anchors:
  - [section II.A] Unlike many RAG tools, AquiLLM uses tool calling to give control of search functions to the LLM... The more sophisticated approach used in AquiLLM allows the model to explore the collection in order to find the information it needs.
  - [section II.C.2] The WebSocket consumer responsible for the front end chat interface then provides TextChunk's search functionality to the LLM using the LLMTool decorator, completing the RAG pipeline.
  - [corpus] No comparative evidence available—tool-calling vs. single-pass RAG comparison not present in corpus neighbors.
- Break condition: LLM tool-calling overhead exceeds latency budget; model enters infinite query loops; tool schema ambiguity causes malformed calls.

### Mechanism 3
- Claim: Single-tenant, self-hosted deployment with local inference options addresses research group privacy constraints that block adoption of cloud RAG services.
- Mechanism: By packaging all components (PostgreSQL, MinIO, optional Ollama) in Docker containers deployable via single script, AquiLLM keeps data within group-controlled infrastructure. The LLM abstraction layer prevents vendor lock-in, supporting both cloud APIs and fully local models.
- Core assumption: Research groups have sufficient infrastructure access (even a single Linux machine) and basic sysadmin capability.
- Evidence anchors:
  - [abstract] Most current RAG-LLM systems are oriented toward public documents and overlook the privacy concerns of internal research materials.
  - [section I.C] Many research groups prefer to maintain complete control over their infrastructure and data... A RAG system for research groups must align with this self-hosted ethos.
  - [section III] Deployed to a Jetstream2 instance using the packaged deployment scripts, and accessed over the public internet.
  - [corpus] No corpus validation of privacy-driven adoption claims—neighboring papers do not address deployment model preferences.
- Break condition: Group lacks any dedicated hardware; local model inference quality insufficient for domain tasks; maintenance burden exceeds group capacity despite simplification efforts.

## Foundational Learning

- Concept: Vector embeddings and similarity search
  - Why needed here: Core retrieval mechanism; understanding cosine similarity vs. Euclidean distance, chunking strategies, and embedding model selection is prerequisite to debugging retrieval quality.
  - Quick check question: Given two text chunks with embedding vectors [0.8, 0.6] and [0.6, 0.8], what is their cosine similarity?

- Concept: LLM tool calling / function calling
  - Why needed here: AquiLLM's distinguishing architectural choice; requires understanding how to define tool schemas, handle tool call responses, and manage conversation state with interleaved tool invocations.
  - Quick check question: What is the difference between appending RAG context to a prompt vs. giving an LLM a search tool it can call?

- Concept: Django ORM and migrations
  - Why needed here: AquiLLM's RAG logic is implemented entirely through Django models (Collection, Document, TextChunk); contributors must navigate model relationships and constraints.
  - Quick check question: In Django, how would you ensure that deleting a Document also deletes all associated TextChunk records?

## Architecture Onboarding

- Component map:
  - Django backend -> PostgreSQL with pgvector -> MinIO object storage -> LLM abstraction layer -> Frontend with React components

- Critical path: Document upload → chunking/embedding in Document.save() → vector storage in TextChunk → user query → LLM invokes search tool → hybrid search (vector + trigram) with reranking → LLM synthesizes response

- Design tradeoffs:
  - Single-tenant monolith (Django) limits horizontal scalability but reduces deployment complexity for small groups
  - No LangChain dependency increases custom code but avoids external framework churn and lock-in
  - pgvector adds vector capability to PostgreSQL rather than requiring separate vector DB, consolidating data management
  - Tool-calling RAG is more powerful but higher latency than single-pass retrieval

- Failure signatures:
  - Empty or poor-quality retrieval: Check embedding model configuration, chunk size parameters, pgvector index status
  - Tool call errors: Verify LLMTool decorator applied correctly, Pydantic schema generation, provider-specific tool format
  - Authentication failures: Validate social provider configuration in django-allauth settings
  - Slow queries: Check vector index existence, reranking overhead, unnecessary full-table scans

- First 3 experiments:
  1. Deploy minimal local instance using provided bash script, ingest 5-10 PDFs, verify vector search returns semantically relevant chunks for test queries.
  2. Test tool-calling behavior: submit multi-hop question requiring synthesis across documents; trace LLM query sequence in logs.
  3. Configure local Ollama model, compare retrieval quality and latency against cloud LLM API; assess whether local inference meets group accuracy requirements.

## Open Questions the Paper Calls Out

- How effectively can AquiLLM capture and retrieve tacit knowledge from strictly informal, multimodal sources such as meeting audio or transcripts?
  - Basis in paper: [explicit] The authors note that a second beta group is "in the process of collecting recordings... to investigate its utility on such informal data," and the conclusion lists expanding multimodal capabilities as future work.
  - Why unresolved: Current reported findings are limited to formal papers, meeting notes, and transcripts; the system's utility for raw audio or transient informal data remains untested in the text.
  - What evidence would resolve it: Evaluation results from the environmental science group's audio data showing successful retrieval of insights from training sessions.

- Does the implementation of LLM tool-calling for search control provide significant advantages in retrieval accuracy over standard RAG methods?
  - Basis in paper: [inferred] The paper claims the tool-calling approach is "more sophisticated" than basic search-string appending, allowing the model to "explore the collection," but offers no comparative performance data.
  - Why unresolved: The benefits are posited theoretically and architecturally but lack empirical validation (e.g., precision/recall metrics or user success rates) against the "more basic RAG tools" mentioned.
  - What evidence would resolve it: Quantitative benchmarks comparing AquiLLM's tool-calling retrieval against standard vector search retrieval on the same document corpus.

- To what extent does AquiLLM generalize to research domains with different data management workflows outside of physics and environmental science?
  - Basis in paper: [explicit] The conclusion states, "Future work will... evaluate its use across diverse scientific domains."
  - Why unresolved: The paper only presents beta deployments for an astronomy group and an environmental science group, leaving its efficacy in other domains (e.g., humanities, wet labs) unknown.
  - What evidence would resolve it: Deployment case studies and user feedback from distinct disciplines demonstrating successful integration with their specific documentation habits.

## Limitations
- Single-tenant monolith design has not been tested beyond beta deployments with two research teams
- Performance under concurrent usage or with large document collections remains unknown
- Absence of explicit quantitative metrics for retrieval quality or user satisfaction limits objective assessment

## Confidence
- **High**: Architectural choices addressing privacy and infrastructure control align with documented research community preferences
- **Medium**: Tool-calling RAG mechanism is sound but lacks comparative evidence against single-pass approaches
- **Low**: Claims about utility in onboarding and knowledge capture are qualitative observations from limited beta deployments without systematic measurement

## Next Checks
1. Deploy AquiLLM to a third research group with at least 50GB of heterogeneous internal documents; measure query latency, retrieval accuracy (via manual relevance judgment), and concurrent user capacity over one month.
2. Conduct a controlled experiment comparing tool-calling RAG against single-pass RAG with identical document collections and query sets; measure precision, recall, and user-perceived answer quality.
3. Perform a maintenance burden assessment: track time spent on infrastructure updates, model API changes, and debugging over a six-month period; compare against projected effort for an equivalent LangChain-based system.