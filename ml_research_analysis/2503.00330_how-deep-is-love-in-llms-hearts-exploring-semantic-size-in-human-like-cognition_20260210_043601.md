---
ver: rpa2
title: How Deep is Love in LLMs' Hearts? Exploring Semantic Size in Human-like Cognition
arxiv_id: '2503.00330'
source_url: https://arxiv.org/abs/2503.00330
tags:
- semantic
- size
- llms
- small
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how large language models (LLMs) understand
  semantic size, the perceived magnitude of concepts, by comparing their performance
  with human cognition. The research employs three approaches: (1) metaphorical reasoning,
  where LLMs associate abstract words with concrete objects of varying sizes, (2)
  probing internal representations to assess how well semantic size is encoded, and
  (3) testing biases toward attention-grabbing headlines with larger semantic sizes
  in a shopping scenario.'
---

# How Deep is Love in LLMs' Hearts? Exploring Semantic Size in Human-like Cognition

## Quick Facts
- arXiv ID: 2503.00330
- Source URL: https://arxiv.org/abs/2503.00330
- Reference count: 40
- Primary result: Multi-modal LLMs outperform text-only models in understanding semantic size, demonstrating more human-like cognition in metaphorical reasoning and headline selection.

## Executive Summary
This study investigates how large language models understand semantic size—the perceived magnitude of concepts—by comparing their performance with human cognition. Through three experimental approaches, researchers found that multi-modal trained LLMs (MLLMs) demonstrate superior understanding of semantic size compared to text-only models. MLLMs showed better performance in metaphorical associations, headline selection, and internal representation of semantic size concepts. The research suggests that multi-modal training enhances LLMs' cognitive abilities and brings them closer to human-like understanding of abstract concepts.

## Method Summary
The research employed three main experimental approaches to evaluate semantic size understanding. First, they tested metaphorical reasoning by asking models to associate abstract words with concrete objects of varying sizes. Second, they probed internal representations to assess how well semantic size was encoded in model embeddings. Third, they tested biases toward attention-grabbing headlines with larger semantic sizes in shopping scenarios. The study compared multi-modal trained LLMs against text-only models across these tasks, using both quantitative metrics and qualitative analysis of model certainty and reasoning patterns.

## Key Results
- MLLMs achieved higher classification accuracy in semantic size tasks compared to text-only models
- MLLMs demonstrated stronger metaphorical associations between abstract concepts and concrete objects of varying sizes
- MLLMs showed higher certainty in their choices and better headline selection in shopping scenarios
- Text-only models struggled particularly with abstract concepts, highlighting limitations in their semantic understanding

## Why This Works (Mechanism)
The mechanism underlying semantic size understanding appears to rely on the integration of multiple sensory modalities. When models are trained on both text and visual data, they develop richer representations that capture the physical and perceptual dimensions of concepts. This multi-modal grounding allows models to better understand the relative magnitude of abstract ideas by connecting them to concrete, size-associated experiences. The visual component provides crucial context for understanding scale and proportion that text alone cannot fully convey.

## Foundational Learning
- Semantic Size: The perceived magnitude of concepts, crucial for understanding abstract ideas through concrete associations
- Why needed: Provides framework for measuring how models interpret relative magnitude of concepts
- Quick check: Can be tested through metaphor association tasks and headline selection experiments

- Multi-modal Training: Models trained on both text and visual data
- Why needed: Enables richer concept representations through cross-modal grounding
- Quick check: Compare performance against text-only models on visual reasoning tasks

- Embodied Cognition: Understanding through physical and sensory experiences
- Why needed: Explains why multi-modal training improves semantic understanding
- Quick check: Evaluate model performance on tasks requiring spatial and physical reasoning

## Architecture Onboarding

Component Map: Text Encoder -> Multi-modal Fusion Layer -> Semantic Size Classifier -> Output Layer

Critical Path: Input Processing -> Multi-modal Integration -> Semantic Size Representation -> Decision Making

Design Tradeoffs:
- Text-only vs. Multi-modal training: Speed vs. accuracy in semantic understanding
- Model size vs. performance: Larger models show better semantic size comprehension
- Training data diversity vs. specialization: Broader exposure improves generalization

Failure Signatures:
- Over-reliance on literal interpretations rather than metaphorical understanding
- Inability to distinguish subtle differences in semantic magnitude
- Confusion with abstract concepts lacking clear size associations

First Experiments:
1. Metaphor association task: Compare model responses for abstract concepts paired with objects of different sizes
2. Internal representation analysis: Examine embedding distances for concepts with varying semantic sizes
3. Headline selection task: Evaluate model choices in shopping scenarios with size-related headlines

## Open Questions the Paper Calls Out
None

## Limitations
- Unclear whether performance differences stem specifically from multi-modal training or other factors like model architecture
- Limited evaluation scope focused mainly on metaphorical reasoning and headline selection
- Challenges with abstract concepts suggest models still lack deep understanding of human-like cognition
- Results may not generalize to other aspects of cognitive reasoning beyond semantic size

## Confidence
- Major claim (MLLMs demonstrate more human-like understanding): Medium
- Multi-modal training impact on semantic size: Medium
- Real-world multi-modal experiences crucial for cognition: Low

## Next Checks
1. Conduct controlled experiments comparing MLLMs with text-only models of identical architecture and scale to isolate the specific impact of multi-modal training on semantic size understanding
2. Expand the evaluation to include a broader range of cognitive tasks beyond metaphorical reasoning and headline selection, such as spatial reasoning, temporal understanding, and emotional cognition
3. Investigate the relationship between model performance on semantic size tasks and performance on tasks requiring embodied cognition, such as physical interaction simulations or visual grounding tasks