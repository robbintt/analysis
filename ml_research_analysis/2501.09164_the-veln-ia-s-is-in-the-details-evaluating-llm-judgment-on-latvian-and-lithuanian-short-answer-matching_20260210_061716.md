---
ver: rpa2
title: 'The Veln(ia)s is in the Details: Evaluating LLM Judgment on Latvian and Lithuanian
  Short Answer Matching'
arxiv_id: '2501.09164'
source_url: https://arxiv.org/abs/2501.09164
tags:
- answer
- answers
- matched
- wang
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces novel datasets for evaluating large language
  models (LLMs) on short answer matching tasks in Latvian and Lithuanian. The authors
  generated 502 Latvian and 690 Lithuanian question-answer pairs from Wikipedia, then
  created matched and non-matched answers using alteration rules that introduce subtle
  semantic changes.
---

# The Veln(ia)s is in the Details: Evaluating LLM Judgment on Latvian and Lithuanian Short Answer Matching

## Quick Facts
- arXiv ID: 2501.09164
- Source URL: https://arxiv.org/abs/2501.09164
- Reference count: 32
- Introduces novel datasets for short answer matching in Latvian and Lithuanian with 502 Latvian and 690 Lithuanian question-answer pairs

## Executive Summary
This paper introduces novel datasets for evaluating large language models (LLMs) on short answer matching tasks in Latvian and Lithuanian. The authors generated question-answer pairs from Wikipedia and created matched and non-matched answers using alteration rules that introduce subtle semantic changes. A subset was manually verified for quality. The datasets were used to benchmark several LLMs including LLaMa3.1, Mistral, EuroLLM, and QWEN2.5 in both zero-shot and few-shot settings. Results showed that larger models like QWEN2.5 72b and LLaMa3.1 70b achieved near-perfect performance in distinguishing matched from non-matched answers.

## Method Summary
The study constructed datasets of 502 Latvian and 690 Lithuanian question-answer pairs from Wikipedia articles. For each correct answer, the authors generated non-matched versions using automated alteration rules that introduced subtle semantic changes, such as adding/removing negations, replacing words with semantically similar alternatives, or changing temporal references. A subset of the dataset was manually verified for quality. The datasets were used to evaluate several LLMs including LLaMa3.1 (8b, 70b), Mistral (7b, 12b), EuroLLM (9b), and QWEN2.5 (7b, 72b) in both zero-shot and few-shot settings, testing their ability to distinguish matched from non-matched answers.

## Key Results
- Larger models (QWEN2.5 72b, LLaMa3.1 70b) achieved near-perfect performance in distinguishing matched from non-matched answers
- QWEN2.5 7b and Mistral 7b achieved performance comparable to 70b models in zero-shot settings
- Mistral Nemo 12b underperformed particularly on Lithuanian tasks despite being a larger model
- Few-shot learning benefits varied across models, with some improving significantly while others showed minimal gains

## Why This Works (Mechanism)
The study demonstrates that LLMs can effectively distinguish between semantically equivalent and non-equivalent short answers in Latvian and Lithuanian when provided with sufficient model capacity and appropriate prompting strategies. The automated alteration rules successfully create challenging test cases that require nuanced understanding of language-specific semantic relationships, negation handling, and contextual meaning preservation.

## Foundational Learning
- Question-answer pair generation from Wikipedia - needed for creating authentic test data; quick check: verify factual accuracy of generated pairs
- Semantic alteration rule design - needed to create challenging non-matched examples; quick check: validate that alterations create genuine semantic differences
- Zero-shot vs few-shot evaluation protocols - needed to assess model generalization; quick check: ensure consistent prompt formatting across conditions
- Cross-linguistic evaluation methodology - needed for fair comparison between Latvian and Lithuanian; quick check: verify language-specific tokenization and processing

## Architecture Onboarding

**Component map:** Dataset generation -> Model evaluation -> Performance analysis -> Result validation

**Critical path:** High-quality dataset construction is essential as it directly determines the validity of all downstream model evaluations and performance conclusions.

**Design tradeoffs:** The study chose automated alteration rules for scalability over manual creation of non-matched answers, trading potential ecological validity for dataset size and consistency.

**Failure signatures:** Poor model performance indicates either insufficient model capacity for the language pair, ineffective few-shot examples, or that the semantic alterations created examples too subtle for the model to detect reliably.

**3 first experiments:**
1. Validate that automated alterations successfully create non-equivalent answers by having humans verify a sample
2. Test whether few-shot examples improve performance uniformly across all models or only specific architectures
3. Compare model performance on original vs altered answers to quantify the difficulty introduced by the alteration rules

## Open Questions the Paper Calls Out
None

## Limitations
- Limited dataset size (502 Latvian and 690 Lithuanian pairs) may not provide sufficient statistical power
- Automated alteration rules may not accurately reflect real-world semantic mismatches
- Evaluation focuses exclusively on short-answer matching, not addressing more complex reasoning tasks

## Confidence
- High confidence in claims about large model performance (QWEN2.5 72b, LLaMa3.1 70b) achieving near-perfect results
- Medium confidence in claims about smaller model performance and few-shot learning effectiveness due to observed variance
- Medium confidence in the finding that QWEN2.5 7b and Mistral 7b achieved comparable performance to 70b models in zero-shot settings

## Next Checks
1. Expand the dataset size by an order of magnitude to include diverse question types and answer lengths
2. Conduct human evaluation studies comparing automatically generated non-matched answers against naturally occurring incorrect responses
3. Test additional model architectures including smaller specialized models and emerging multilingual models to validate generalizability of findings