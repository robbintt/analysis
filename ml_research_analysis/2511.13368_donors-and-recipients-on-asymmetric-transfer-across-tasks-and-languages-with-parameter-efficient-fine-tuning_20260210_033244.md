---
ver: rpa2
title: 'Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with
  Parameter-Efficient Fine-Tuning'
arxiv_id: '2511.13368'
source_url: https://arxiv.org/abs/2511.13368
tags:
- b-instruct
- qwen2
- llama-3
- transfer
- truthfulqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how parameter-efficient fine-tuning on one
  task-language pair affects performance across all other task-language combinations.
  Using a controlled grid of four benchmarks and eleven languages, the authors fine-tune
  each model on a single source cell and measure transfer as percentage-point changes
  across target pairs, decomposing results into matched-task cross-language, matched-language
  cross-task, and cross-task cross-language regimes.
---

# Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning

## Quick Facts
- **arXiv ID**: 2511.13368
- **Source URL**: https://arxiv.org/abs/2511.13368
- **Reference count**: 40
- **Primary result**: Parameter-efficient fine-tuning exhibits pronounced asymmetry—matched-task cross-language transfer yields reliable gains, while off-task regimes produce smaller improvements and higher degradation risk.

## Executive Summary
This study examines how parameter-efficient fine-tuning on one task-language pair affects performance across all other task-language combinations. Using a controlled grid of four benchmarks and eleven languages, the authors fine-tune each model on a single source cell and measure transfer as percentage-point changes across target pairs, decomposing results into matched-task cross-language, matched-language cross-task, and cross-task cross-language regimes. The results show a pronounced asymmetry: matched-task cross-language transfer yields reliable gains with high win rates and low harm rates, while off-task regimes produce smaller improvements and higher degradation risk. A stable donor-recipient hierarchy emerges, with high-resource languages and broad semantic tasks acting as efficient hubs, and low-resource languages as fragile recipients. Transfer magnitudes are driven primarily by target-side properties, and while coarse regularities are stable across model families, fine-grained donor rankings are not. These findings provide risk-aware fine-tuning heuristics that prioritize matched-task sources and flag high-harm donors to maximize downstream gains.

## Method Summary
The paper constructs a 4×11 transfer grid (4 benchmarks × 11 languages) and evaluates parameter-efficient fine-tuning using LoRA adapters (rank 32, α 64) on each source cell. For each of 9 instruction-tuned models (Llama 3, Qwen 2.5, Gemma 3 at multiple scales), they establish zero-shot baselines on all task-language cells, then fine-tune on each source cell with fixed hyperparameters (AdamW, lr 5e-5, 10% warmup, 3 epochs max, early stopping). Transfer is measured as percentage-point change (Δ) versus baseline, with win rate (Δ > 0) and harm rate (Δ < -1.0 pp) computed per regime. Variance decomposition via linear mixed-effects models isolates contributions of model, source, and target properties, while Consistency Index (Kendall τ) measures ranking stability across model families.

## Key Results
- Matched-task cross-language transfer produces reliable gains (Δ > 0 in 67% of cases) with low harm rates (≤10% in most cases), while off-task regimes show smaller improvements and higher degradation risk.
- Target-side properties dominate transfer success—target language explains 75.4% of variance in matched-task regime, while source and model contributions are minimal.
- Stable donor-recipient hierarchies emerge: high-resource languages and broad semantic tasks act as efficient hubs, while low-resource languages are fragile recipients with elevated harm rates.
- Syntactic similarity between source and target languages predicts cross-lingual transfer more strongly than lexical overlap.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer success under single-source LoRA is dominated by target-side receptivity, not model architecture or source selection.
- Mechanism: Fine-tuning exports low-rank updates that are absorbed or rejected based on how well the target task-language representation aligns with the adapter subspace. High-resource languages and broad semantic tasks have richer overlapping subspaces, enabling them to "catch" incoming signals. Fragile targets (low-resource languages, specialized tasks) have weaker coverage, causing signal loss or interference.
- Core assumption: The adapter learns a task-language-specific direction in parameter space that generalizes only if the target representation lies near that direction.
- Evidence anchors:
  - [abstract] "variance decomposition shows target-side properties dominate transfer success (75.4% variance explained in matched-task regime), while model family and scale have minimal impact"
  - [Section 4.2] "the target language explains 75.4% of the variance, while the residual is minimal (11.6%)"
  - [corpus] Related work on cross-lingual knowledge barriers (Chua et al., 2025) finds models align surface representations yet fail to propagate task knowledge without explicit multilingual fine-tuning; consistent with receptivity-limited transfer.
- Break condition: If multi-source fine-tuning or full fine-tuning is used instead of single-source LoRA, the target-dominance pattern may weaken (Appendix A.4 shows LoRA tracks full fine-tuning but residual variance increases in off-task regimes).

### Mechanism 2
- Claim: Cross-task transfer is directed and asymmetric—some tasks (e.g., TruthfulQA) act as strong donors but fragile recipients, while others (e.g., Global-MMLU) are universal recipients but poor donors.
- Mechanism: Tasks that teach generalizable "meta-skills" (e.g., resisting imitative falsehoods) produce updates that improve reasoning across tasks. Conversely, tasks that rely on broad knowledge absorption can integrate signals from many sources but do not export distinctive transferable patterns.
- Core assumption: Task updates encode different kinds of knowledge—some are "additive priors," others are "sinks" that absorb but do not re-emit useful structure.
- Evidence anchors:
  - [abstract] "Languages and tasks form stable donor-recipient hierarchies, with high-resource languages and broad semantic knowledge tasks acting as hubs"
  - [Section 4.1.3] "TruthfulQA emerges as the strongest donor but the most fragile recipient, improving other tasks while being vulnerable to negative interference itself, whereas Global-MMLU absorbs substantial gains from all other tasks yet contributes almost negligible transfer benefits"
  - [corpus] Cross-lingual transfer work (Malkin et al., 2022) documents asymmetric donor roles in zero-shot transfer; conceptually similar directional structure.
- Break condition: If a task is fine-tuned with multi-task mixing or regularization, its donor/recipient profile may shift toward symmetry.

### Mechanism 3
- Claim: Syntactic similarity between source and target languages predicts cross-lingual transfer more strongly than lexical or inventory overlap.
- Mechanism: LoRA updates capture abstract reasoning templates tied to grammatical structure (e.g., word order, dependency patterns). When source and target share syntax, the template transfers efficiently; lexical differences are less disruptive because the adapter modifies higher-level circuitry rather than surface embeddings.
- Core assumption: Parameter-efficient fine-tuning operates on structural representations that are shared across syntactically similar languages.
- Evidence anchors:
  - [Section 5.1] "syntactic distance is a strong negative predictor of transfer, whereas inventory distance shows little systematic effect"
  - [Appendix A.7] Spearman ρ = -0.605 (p < 0.001) for syntactic distance; inventory distance not significant
  - [corpus] Related work on language fusion for cross-lingual transfer emphasizes leveraging shared representation spaces but does not isolate syntax; this paper provides finer-grained evidence.
- Break condition: If full fine-tuning is used, lexical overlap may become more predictive as embeddings are directly updated.

## Foundational Learning

- **Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
  - Why needed here: The entire experimental design assumes fixed-rank adapters; understanding how low-rank updates constrain what can transfer is essential.
  - Quick check question: Can you explain why a rank-32 adapter might limit both positive transfer and catastrophic forgetting compared to full fine-tuning?

- **Variance Decomposition (Mixed-Effects Models)**
  - Why needed here: The paper's central claim—target-side dominance—rests on partitioning variance across model, source, and target components.
  - Quick check question: If target explains 75% of variance and model explains 4%, what does that imply for choosing between Llama 3 vs. Qwen 2.5 for a Bengali task?

- **Win Rate / Harm Rate Metrics**
  - Why needed here: The paper evaluates transfer not just by mean gain but by how often fine-tuning helps vs. hurts each target cell.
  - Quick check question: Why might a regime with positive mean Δ still be risky if harm rate is high?

## Architecture Onboarding

- **Component map**: Transfer Grid (4 benchmarks × 11 languages = 44 cells) -> Transfer Regimes (MT–CL, ML–CT, CT–CL) -> Donor/Recipient Scores (aggregated Δ per language/task) -> Stability Analysis (variance decomposition + Consistency Index)

- **Critical path**:
  1. Establish baseline performance for model m on all 44 cells.
  2. Fine-tune m on source cell (d_src, ℓ_src) with fixed LoRA recipe (r=32, α=64, 3 epochs, early stopping on validation loss).
  3. Evaluate fine-tuned model on all non-source target cells; compute Δ, win rate, harm rate.
  4. Aggregate into donor/recipient matrices; run variance decomposition.

- **Design tradeoffs**:
  - **Rank selection**: Higher r increases MT–CL gains but raises harm rate; r=32 offers best trade-off in ablation.
  - **Single-source vs. multi-source**: Single-source isolates transfer effects but may underrepresent real-world multi-task scenarios.
  - **Machine-translated vs. curated benchmarks**: Global-MMLU (curated) shows smaller gains and higher harm rates than translated benchmarks—benchmark construction affects measured transfer.

- **Failure signatures**:
  - **Elevated harm rates on low-resource targets**: Hindi and Bengali show higher harm rates and lower win rates (Table 9).
  - **Negative off-task impact from TruthfulQA on small models**: Some Qwen and Llama runs show Δ_off-task < 0 (Table 14).
  - **Inconsistent fine-grained rankings**: Low Consistency Index (τ ≈ 0.03–0.13) means ranking specific best donors is unstable across models.

- **First 3 experiments**:
  1. **Replicate MT–CL transfer for one language pair**: Fine-tune Llama-3.1-8B on ARC-Challenge in English; evaluate on ARC-Challenge in Spanish and Bengali. Verify that Δ is positive for Spanish and smaller/variable for Bengali.
  2. **Check donor asymmetry**: Fine-tune on TruthfulQA (English) and Global-MMLU (English); compare Δ on other tasks. Confirm TruthfulQA exports larger positive transfer.
  3. **Test syntactic-distance prediction**: For a fixed source (e.g., French), compute Δ for targets German (syntactically close) and Hindi (syntactically distant); verify larger gains for German.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the target-dominated variance decomposition (where target language explains ~75% of variance in MT–CL transfer) persist at model scales beyond 8B parameters?
- Basis in paper: [explicit] "Future work should (i) extend this framework to larger models and additional architectures to test whether target-dominated variance persists at larger scales"
- Why unresolved: The study only tests models up to 8B (Llama-3.1-8B, Qwen2.5-7B), and scaling may alter how transfer dynamics distribute between source, target, and model components.
- What evidence would resolve it: Replicating the mixed-effects variance decomposition on larger models (e.g., 70B, 405B) using the same transfer grid methodology.

### Open Question 2
- Question: How does multi-source composition (simultaneous fine-tuning on multiple task–language pairs) reshape the donor–recipient hierarchy observed under single-source adaptation?
- Basis in paper: [explicit] "Future work should... (ii) move beyond single-source PEFT to study multi-source composition"
- Why unresolved: The study isolates single-source effects; real-world fine-tuning often mixes sources, which could amplify, attenuate, or restructure the asymmetric transfer patterns observed.
- What evidence would resolve it: Controlled experiments varying the number and combination of source cells while measuring whether hub donor/recipient roles remain stable or shift.

### Open Question 3
- Question: Does the donor–recipient hierarchy transfer to open-ended generative tasks with human or preference-based evaluation?
- Basis in paper: [explicit] "Future work should... (iii) broaden the evaluation suite to generative and open-ended tasks with human or preference-based assessment, to test whether the donor–recipient hierarchy we observe on multiple-choice benchmarks carries over to other settings"
- Why unresolved: All benchmarks in this study use multiple-choice or short-form classification; generative tasks may involve different transfer dynamics (e.g., fluency, coherence vs. discrete correctness).
- What evidence would resolve it: Applying the transfer grid methodology to generative benchmarks (e.g., summarization, dialogue) with human or model-based preference judgments.

### Open Question 4
- Question: Do few-shot prompting or Chain-of-Thought decoding strategies alter the transfer patterns and harm rates observed under zero-shot evaluation?
- Basis in paper: [inferred] From Limitations: "our evaluation protocol fixes decoding in a zero-shot setting, but few-shot, Chain-of-Thought (CoT) prompting, or alternative decoding strategies could yield different outcomes"
- Why unresolved: In-context examples or reasoning chains may buffer against negative transfer or change how task-specific knowledge propagates across languages.
- What evidence would resolve it: Repeating transfer measurements with few-shot and CoT prompting regimes, comparing harm rates and win rates to the zero-shot baseline.

## Limitations
- Limited to four specific benchmarks; stability of donor-recipient hierarchies across different task types remains unknown.
- Single-source fine-tuning assumption may understate interference effects present in real-world multi-task scenarios.
- Machine-translated vs. curated benchmarks show different transfer patterns, raising questions about generalization to naturally occurring data.

## Confidence

**High Confidence**: Matched-task cross-language transfer produces reliable gains with low harm rates; variance decomposition showing target-side dominance is methodologically sound and consistent.

**Medium Confidence**: Coarse donor-recipient hierarchies are stable (high-resource languages as hubs, broad semantic tasks as universal recipients) but fine-grained rankings show low consistency across model families.

**Low Confidence**: Specific donor rankings for individual task-language pairs are not stable across models (Consistency Index τ ≈ 0.03–0.13).

## Next Checks

1. **Cross-domain stability test**: Replicate the transfer grid with a different set of benchmarks (e.g., mathematical reasoning, code generation, or medical QA) to verify whether the observed donor-recipient hierarchies and asymmetry patterns generalize beyond the current task set.

2. **Multi-source fine-tuning validation**: Conduct experiments with multi-task fine-tuning (e.g., mixing sources from different tasks) to determine whether the observed asymmetry patterns persist or whether interference effects become more pronounced when multiple adapters are applied.

3. **Low-resource language expansion**: Extend the language coverage to include typologically distant languages (e.g., Mandarin, Arabic, Swahili) to test whether the syntactic similarity hypothesis holds across language families and whether high-resource language dominance persists for structurally different languages.