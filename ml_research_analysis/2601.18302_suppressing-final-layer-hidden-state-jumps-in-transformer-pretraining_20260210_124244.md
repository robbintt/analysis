---
ver: rpa2
title: Suppressing Final Layer Hidden State Jumps in Transformer Pretraining
arxiv_id: '2601.18302'
source_url: https://arxiv.org/abs/2601.18302
tags:
- layer
- jreg
- baseline
- layers
- jump
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates a phenomenon in Transformer language models
  where the final layer exhibits disproportionately large changes in hidden state
  angular distance ("jumps") compared to middle layers. The authors introduce a quantitative
  metric to measure this jump strength and demonstrate its prevalence across multiple
  open-weight models and its amplification during pre-training.
---

# Suppressing Final Layer Hidden State Jumps in Transformer Pretraining

## Quick Facts
- arXiv ID: 2601.18302
- Source URL: https://arxiv.org/abs/2601.18302
- Reference count: 40
- Primary result: JREG reduces final-layer hidden state jumps, increases middle-layer effectiveness, and improves downstream performance in Llama-based models

## Executive Summary
This paper identifies a phenomenon in Transformer language models where final layers exhibit disproportionately large changes in hidden state angular distance compared to middle layers, termed "hidden state jumps." The authors quantify this using a metric called Jump Rate and demonstrate its prevalence across multiple open-weight models and its amplification during pre-training. To address this imbalance, they propose Jump-Suppressing Regularizer (JREG), a simple training-time regularization technique that penalizes large hidden state displacements in final layers. Experiments on three sizes of Llama-based models (170M, 1B, and 3.4B parameters) show that JREG consistently reduces jump rates, increases middle layer effectiveness, and improves downstream task performance compared to baseline models without altering the architecture.

## Method Summary
The authors introduce Jump Rate as a quantitative metric to measure the angular distance changes between consecutive layers, normalized by layer depth. JREG is implemented as a regularization term added to the pre-training loss that penalizes large angular displacements in final layers while preserving the model's representational capacity. The method is evaluated through controlled experiments comparing JREG-applied models against standard pre-trained baselines across multiple model scales, with downstream performance measured on standard benchmarks after both pre-training and supervised fine-tuning.

## Key Results
- JREG consistently reduces Jump Rate in final layers across all tested model sizes (170M, 1B, 3.4B parameters)
- Middle layers show increased effectiveness and capability utilization when final-layer jumps are suppressed
- Downstream task performance improves significantly with JREG, with gains persisting after supervised fine-tuning
- The improvements are statistically significant and demonstrate robustness across model scales

## Why This Works (Mechanism)
The mechanism relies on the observation that excessive hidden state jumps in final layers create an imbalance in how different layers contribute to the model's overall capability. By suppressing these jumps through regularization, JREG encourages a more balanced distribution of representational changes across layers, preventing the final layer from becoming a bottleneck or performing disproportionate work. This balanced approach allows middle layers to contribute more effectively to the model's final output, leading to improved utilization of the entire network depth.

## Foundational Learning
- **Angular Distance**: A measure of the angle between two vectors in high-dimensional space; crucial for understanding how hidden states change between layers and detecting disproportionate jumps
  - Why needed: Provides the geometric basis for quantifying hidden state displacements
  - Quick check: Calculate cosine similarity between consecutive layer outputs

- **Jump Rate Metric**: A normalized measure of angular distance changes between consecutive layers, used to quantify the "jump" phenomenon
  - Why needed: Enables systematic comparison of hidden state dynamics across different model architectures and scales
  - Quick check: Verify that Jump Rate increases monotonically from middle to final layers in baseline models

- **Regularization in Pre-training**: Techniques that modify the training objective to prevent undesirable behaviors or encourage specific properties
  - Why needed: Forms the foundation for understanding how JREG modifies standard pre-training
  - Quick check: Confirm that JREG adds a penalty term to the standard cross-entropy loss

## Architecture Onboarding

**Component Map:**
Input Embeddings -> Encoder Stack (Middle Layers) -> Final Layers -> Output Head

**Critical Path:**
Input sequence → Embedding layer → Middle Transformer layers → Final Transformer layer → Output projection → Prediction

**Design Tradeoffs:**
The primary tradeoff is between suppressing final-layer jumps (which improves middle-layer utilization) and maintaining sufficient representational capacity in the final layer for task-specific processing. JREG must balance these competing needs without degrading the model's ability to learn complex patterns.

**Failure Signatures:**
- Excessive suppression could lead to underfitting or inability to learn complex representations
- Insufficient suppression would fail to address the imbalance problem
- Improper hyperparameter tuning could result in either vanishing gradients or unstable training

**Three First Experiments:**
1. Measure baseline Jump Rate across layers in a pre-trained model to establish the existence of the phenomenon
2. Apply JREG with varying regularization strengths to find the optimal balance point
3. Compare middle-layer probe performance between JREG-applied and baseline models to verify increased middle-layer effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis is limited to Llama-based models of three specific sizes (170M, 1B, 3.4B parameters) and may not generalize to all Transformer architectures
- The study focuses exclusively on angular distance measurements without exploring other representational metrics
- Downstream evaluations cover a limited set of tasks and do not extensively test domain-specific or multi-modal applications

## Confidence

**High confidence:**
- Experimental methodology for measuring jump strength is rigorous and reproducible
- Statistical significance of downstream improvements is well-established

**Medium confidence:**
- Interpretation that jump suppression leads to "more balanced capability usage" is plausible but not definitively proven
- Claim that this is a "universal phenomenon" is supported by tested sample but may not generalize to all architectures

## Next Checks
1. Test JREG across diverse model families (BERT, GPT, Vision Transformers) and training conditions to assess architectural and data dependence
2. Conduct ablation studies comparing JREG against other regularization techniques to isolate whether jump suppression specifically drives improvements
3. Implement intermediate-layer capability probes to empirically verify whether jump suppression correlates with more uniform capability distribution across layers