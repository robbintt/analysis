---
ver: rpa2
title: Imperceptible Jailbreaking against Large Language Models
arxiv_id: '2510.05025'
source_url: https://arxiv.org/abs/2510.05025
tags:
- imperceptible
- variation
- jailbreaks
- malicious
- selectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces imperceptible jailbreaks that exploit invisible
  Unicode variation selectors to bypass LLM safety alignment without visible modifications.
  The method appends invisible variation selectors to malicious prompts, altering
  their tokenization while maintaining visual indistinguishability.
---

# Imperceptible Jailbreaking against Large Language Models

## Quick Facts
- arXiv ID: 2510.05025
- Source URL: https://arxiv.org/abs/2510.05025
- Authors: Kuofeng Gao; Yiming Li; Chao Du; Xin Wang; Xingjun Ma; Shu-Tao Xia; Tianyu Pang
- Reference count: 33
- Key outcome: Introduces invisible Unicode variation selectors to bypass LLM safety alignment, achieving up to 100% attack success rates against four aligned models

## Executive Summary
This paper presents a novel jailbreaking technique that exploits invisible Unicode variation selectors to bypass safety alignment in large language models without any visible modifications to prompts. By appending these invisible characters, the method alters tokenization patterns while maintaining visual indistinguishability, making the attacks imperceptible to human users. The researchers developed a chain-of-search optimization pipeline to systematically discover effective invisible suffixes that maximize the likelihood of generating harmful responses.

The approach demonstrates superior performance compared to traditional visible jailbreak methods, achieving near-perfect success rates across multiple aligned LLMs. The method also generalizes to prompt injection attacks, suggesting broader implications for LLM security. The paper highlights how subtle manipulation of tokenization through invisible characters can effectively circumvent safety mechanisms designed to prevent harmful outputs.

## Method Summary
The proposed method leverages Unicode variation selectors (U+FE00-U+FE0F) as imperceptible perturbations to jailbreak aligned LLMs. These invisible characters modify how prompts are tokenized without changing their visual appearance, exploiting the gap between human perception and machine processing. The researchers developed a chain-of-search pipeline that iteratively appends variation selectors to malicious prompts, optimizing for maximum harmful response generation while maintaining imperceptibility. This approach systematically explores the space of invisible character combinations to find suffixes that successfully bypass safety alignment mechanisms.

## Key Results
- Achieves up to 100% attack success rates against four aligned LLMs
- Outperforms traditional visible jailbreak methods in terms of both effectiveness and stealth
- Successfully generalizes the approach to prompt injection attacks
- Demonstrates that imperceptible modifications can effectively bypass safety mechanisms

## Why This Works (Mechanism)
The attack exploits the fundamental difference between human perception and machine tokenization. While humans cannot perceive Unicode variation selectors, these characters significantly alter how LLMs tokenize and process input text. By strategically appending these invisible characters, attackers can manipulate the internal representation of prompts without changing their visible appearance, effectively bypassing safety filters that rely on visible content analysis.

## Foundational Learning
**Unicode variation selectors**: Special characters that modify glyph presentation without visible change - needed to understand the attack vector; quick check: verify they're invisible in standard text editors
**LLM tokenization**: Process of converting text into numerical representations - needed to understand how invisible characters affect processing; quick check: compare token sequences with/without variation selectors
**Safety alignment mechanisms**: Techniques used to prevent harmful outputs - needed to understand what the attack bypasses; quick check: review typical alignment strategies like RLHF
**Chain-of-search optimization**: Iterative search process for finding effective attack patterns - needed to understand the methodology; quick check: trace how the search algorithm improves success rates
**Prompt injection attacks**: Techniques for manipulating LLM behavior - needed to understand attack generalization; quick check: verify the method works beyond simple jailbreaks

## Architecture Onboarding

**Component map**: Input prompt -> Variation selector append -> Tokenization -> LLM processing -> Output generation
**Critical path**: The optimization pipeline that iteratively appends invisible characters and evaluates success rates
**Design tradeoffs**: Imperceptibility vs. attack effectiveness - more variation selectors may increase success but risk detection through other means
**Failure signatures**: Visible prompts that still trigger safety mechanisms despite invisible modifications
**First experiments**: 1) Baseline testing without variation selectors, 2) Single variation selector impact assessment, 3) Multi-selector optimization evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness depends on specific tokenization behavior of targeted LLMs
- May not transfer across different model architectures or fine-tuning approaches
- Vulnerable to input sanitization or Unicode normalization in real-world applications

## Confidence
- High Confidence: The core mechanism of using invisible variation selectors to alter tokenization is technically sound
- Medium Confidence: Attack success rates against specific tested models are reliable, but generalizability remains uncertain
- Medium Confidence: Comparison to traditional methods is valid for tested scenarios, but broader benchmarking needed

## Next Checks
1. Test attack transferability across different LLM architectures (open vs. closed-source models)
2. Evaluate defensive strategies including Unicode normalization and input sanitization
3. Benchmark computational efficiency of the chain-of-search optimization under time constraints