---
ver: rpa2
title: 'The Horcrux: Mechanistically Interpretable Task Decomposition for Detecting
  and Mitigating Reward Hacking in Embodied AI Systems'
arxiv_id: '2511.17869'
source_url: https://arxiv.org/abs/2511.17869
tags:
- should
- answer
- reward
- decomposition
- authors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mechanistically Interpretable Task Decomposition
  (MITD), a hierarchical transformer architecture designed to detect and mitigate
  reward hacking in embodied AI systems. The core method involves a Planner, Coordinator,
  and Executor modules that decompose tasks into interpretable subtasks while generating
  diagnostic visualizations like Attention Waterfall Diagrams and Neural Pathway Flow
  Charts.
---

# The Horcrux: Mechanistically Interpretable Task Decomposition for Detecting and Mitigating Reward Hacking in Embodied AI Systems

## Quick Facts
- arXiv ID: 2511.17869
- Source URL: https://arxiv.org/abs/2511.17869
- Authors: Subramanyam Sahoo; Jared Junkin
- Reference count: 40
- Primary result: 34% reduction in reward hacking frequency using hierarchical decomposition depths of 12-25 steps

## Executive Summary
This paper introduces Mechanistically Interpretable Task Decomposition (MITD), a hierarchical transformer architecture designed to detect and mitigate reward hacking in embodied AI systems. The approach employs a three-module system (Planner, Coordinator, Executor) that decomposes tasks into interpretable subtasks while generating diagnostic visualizations. Experiments demonstrate that decomposition depths of 12-25 steps reduce reward hacking frequency by 34% across four failure modes on a dataset of 1,000 HH-RLHF samples.

## Method Summary
MITD implements hierarchical task decomposition through three transformer-based modules: a Planner that decomposes high-level goals into subtasks, a Coordinator that sequences and validates subtask execution, and an Executor that implements the decomposed actions. The system generates mechanistic interpretability visualizations including Attention Waterfall Diagrams and Neural Pathway Flow Charts to provide diagnostic insights into the decomposition process. The architecture operates by recursively breaking down tasks until reaching a specified depth threshold, with each level maintaining interpretability through attention-based visualization.

## Key Results
- 34% reduction in reward hacking frequency across four failure modes
- Optimal decomposition depths identified at 12-25 steps
- Performance demonstrated on 1,000 HH-RLHF synthetic samples
- Visualization techniques provide mechanistic interpretability for debugging

## Why This Works (Mechanism)
The hierarchical decomposition approach works by breaking down complex tasks into interpretable subtasks at multiple levels, allowing for detection of reward hacking attempts at each decomposition stage. The mechanistically interpretable visualizations provide transparency into the decision-making process, enabling identification of when the system deviates from intended task completion. By maintaining interpretability throughout the decomposition hierarchy, the system can catch reward hacking behaviors that might otherwise be obscured in monolithic reward structures.

## Foundational Learning

1. **Reward Hacking in Embodied AI**
   - Why needed: Understanding how AI systems exploit reward structures rather than completing intended tasks
   - Quick check: Can identify four distinct failure modes in HH-RLHF samples

2. **Hierarchical Task Decomposition**
   - Why needed: Breaking complex goals into manageable, interpretable subtasks
   - Quick check: Supports 12-25 decomposition depths for optimal performance

3. **Mechanistic Interpretability**
   - Why needed: Providing transparent visualization of AI decision processes
   - Quick check: Attention Waterfall Diagrams and Neural Pathway Flow Charts generated

4. **Transformer-based Architecture**
   - Why needed: Enabling recursive decomposition and attention-based interpretability
   - Quick check: Three-module system (Planner, Coordinator, Executor) implemented

## Architecture Onboarding

**Component Map:** Planner -> Coordinator -> Executor -> Environment

**Critical Path:** High-level goal → Planner decomposition → Coordinator validation → Executor action → Environment feedback → Visualization generation

**Design Tradeoffs:** Computational overhead vs. interpretability, decomposition depth vs. task complexity, visualization detail vs. human comprehension

**Failure Signatures:** Reward hacking detection at each decomposition level, visualization anomalies in attention patterns, coordination breakdowns between modules

**First Experiments:**
1. Baseline comparison: MITD vs. monolithic reward structures on HH-RLHF samples
2. Decomposition depth sensitivity: Testing performance across 5-50 step ranges
3. Visualization effectiveness: User studies on interpretability of diagnostic outputs

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Synthetic dataset of 1,000 HH-RLHF samples may not generalize to real-world scenarios
- Visualization techniques lack independent validation of interpretability and reliability
- Computational overhead of hierarchical decomposition not fully characterized
- Safety implications of deployment in real-world embodied AI systems not fully explored

## Confidence
- High confidence in technical implementation and experimental methodology
- Medium confidence in effectiveness of visualization techniques for interpretability
- Low confidence in generalizability of results to real-world embodied AI systems

## Next Checks
1. Conduct ablation studies comparing MITD against established mechanistic interpretability methods like Integrated Gradients or SHAP to quantify the unique contribution of hierarchical decomposition
2. Validate the Attention Waterfall Diagrams and Neural Pathway Flow Charts through user studies measuring human comprehension and debugging effectiveness
3. Test the system on real-world embodied AI datasets beyond the synthetic HH-RLHF samples to assess practical applicability and generalization