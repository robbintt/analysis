---
ver: rpa2
title: 'Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of
  GAN'
arxiv_id: '2504.15099'
source_url: https://arxiv.org/abs/2504.15099
tags:
- discriminator
- learning
- step
- training
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fast-Slow Co-advancing Optimizer (FSCO),
  a novel reinforcement learning-based optimizer for GAN training that addresses the
  sensitivity to hyperparameters and training instability. FSCO employs a DDPG agent
  to dynamically control the discriminator's learning rate based on the loss difference
  between generator and discriminator, creating a more harmonious adversarial training
  process.
---

# Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN

## Quick Facts
- **arXiv ID:** 2504.15099
- **Source URL:** https://arxiv.org/abs/2504.15099
- **Reference count:** 6
- **Primary result:** FSCO uses DDPG to dynamically adjust discriminator learning rate, stabilizing GAN training across MNIST, ANIME, and Ganyu datasets while expanding hyperparameter tolerance.

## Executive Summary
This paper introduces Fast-Slow Co-advancing Optimizer (FSCO), a reinforcement learning-based method that transforms GAN training from static hyperparameter tuning to a feedback-controlled process. By employing a DDPG agent to dynamically modulate the discriminator's learning rate based on the loss difference between generator and discriminator, FSCO addresses the fundamental instability in adversarial training. The approach creates a more harmonious adversarial process where the discriminator "waits" for the generator to catch up, preventing the discriminator from becoming too strong too quickly. Experiments on three datasets demonstrate FSCO's effectiveness in stabilizing training and reducing hyperparameter sensitivity, though it cannot overcome fundamental architectural limitations and requires careful selection of the discriminator's base learning rate.

## Method Summary
FSCO implements a DDPG agent that observes the GAN's training state and outputs a continuous multiplier $u(t) \in (0, 1)$ to scale the discriminator's base learning rate. The agent is rewarded based on the negative absolute difference between generator and discriminator losses, incentivizing actions that maintain balance between the two networks. During training, the GAN produces generator and discriminator losses, the reward is calculated, and the DDPG agent predicts the learning rate multiplier for the next step. This multiplier is applied to the discriminator's optimizer, and the transition is stored in an experience replay buffer for agent training. The method uses soft updates for network stability and random sampling from the replay buffer to handle the non-stationary nature of GAN training.

## Key Results
- FSCO stabilizes GAN training across three datasets (MNIST 28×28, ANIME 64×64, Ganyu 128×128) by preventing discriminator dominance
- The method expands the effective hyperparameter range for discriminator-generator coordination, reducing sensitivity to initial learning rate selection
- FSCO transforms GAN training from fixed-step hyperparameter tuning to a feedback-controlled process, though it cannot fundamentally enhance network capabilities or prevent overfitting

## Why This Works (Mechanism)

### Mechanism 1: Feedback-Controlled Discriminator Pacing
The method treats GAN training as a control problem rather than static optimization. A DDPG agent observes training state and outputs a multiplier $u(t) \in (0, 1)$ that scales the discriminator's base learning rate down to an effective rate. If the discriminator learns too fast, the agent suppresses its learning rate, forcing it to "wait" for the generator. This prevents the discriminator from becoming too strong and overpowering the generator, maintaining a harmonious training equilibrium. The core assumption is that instability stems from imbalance in learning speeds, with the discriminator converging too quickly to distinguish real from fake, providing no usable gradient for the generator.

### Mechanism 2: Loss-Difference Reward Signaling
The DDPG agent is rewarded based on $Reward(t) = - |G_{loss}(t) - D_{loss}(t)|$, incentivizing the agent to minimize the gap between generator and discriminator losses. If the discriminator loss drops too low (indicating it is winning), the reward drops, prompting the agent to reduce the discriminator's learning rate. This creates a self-correcting signal for learning rate adjustment. The core assumption is that a small absolute difference between generator and discriminator losses correlates with stable, productive adversarial training rather than both networks failing simultaneously.

### Mechanism 3: Experience Replay for Stabilizing the Optimizer
GAN training is non-stationary as the data distribution changes with generator improvement. By storing state-action-reward transitions in a replay buffer and sampling randomly, the DDPG agent learns a smoother policy for setting learning rates rather than overfitting to recent mini-batch results. The agent learns a generalized mapping from state to optimal multiplier. The core assumption is that state dynamics of the GAN are sufficiently Markovian or consistent for the agent to learn this mapping effectively.

## Foundational Learning

- **Concept: Adversarial Dynamics (Min-Max Game)**
  - Why needed here: FSCO solves the failure mode where the discriminator wins too early, causing the generator to stop learning. Understanding this zero-sum nature is required to see why slowing down the winner is valid.
  - Quick check question: Why would a perfect discriminator (100% accuracy) actually be a failure case for GAN training?

- **Concept: Actor-Critic Methods (DDPG)**
  - Why needed here: The core of FSCO is a DDPG agent. The "Actor" outputs the continuous action (learning rate multiplier), while the "Critic" estimates if that action will lead to stable training (reward).
  - Quick check question: In this context, does the DDPG Agent output a discrete class label or a continuous scalar value?

- **Concept: Learning Rate Sensitivity**
  - Why needed here: The paper explicitly targets "sensitivity to hyperparameters." A fixed learning rate can be too fast for one network and too slow for another, and finding this "sweet spot" manually is computationally expensive.
  - Quick check question: According to the paper, does FSCO replace the learning rate entirely, or does it modulate a base learning rate?

## Architecture Onboarding

- **Component map:** GAN Environment -> State Vector (losses, step sizes) -> DDPG Agent (Actor + Critic) -> Action (multiplier) -> Actuator (updates D's learning rate) -> GAN Environment

- **Critical path:**
  1. Train GAN for one batch/epoch -> Get $G_{loss}, D_{loss}$
  2. Calculate Reward $R = -|D_{loss} - G_{loss}|$
  3. Feed State to DDPG Actor -> Output Multiplier $u(t)$
  4. Apply Multiplier to D's optimizer
  5. Store transition $(S, A, R, S')$ in Replay Buffer
  6. Update DDPG networks (Soft update targets)

- **Design tradeoffs:**
  - Stability vs. Speed: The agent often suppresses the discriminator's learning rate to near zero to ensure stability, which may prolong training time compared to perfectly tuned manual schedule
  - Overhead: Introduces computational overhead of running a DDPG network alongside the GAN

- **Failure signatures:**
  - Overfitting: The paper explicitly notes that while training stabilizes, the generator may still overfit to the training set
  - Zero Step-Size Lock: The agent might learn that setting learning rate to 0 is safest way to maximize reward, effectively halting discriminator training
  - Base Rate Dependence: FSCO scales a base rate; if the base rate is absurdly wrong, the agent's modulation range may be insufficient to fix it

- **First 3 experiments:**
  1. Vanilla vs. FSCO on MNIST: Replicate 28×28 experiment, plot discriminator learning rate over time, verify it drops when generator loss is high
  2. Base LR Sensitivity Sweep: Test if FSCO allows wider range of base learning rates (e.g., try $\eta_D = 0.001, 0.01, 0.1$) without mode collapse
  3. Reward Engineering: Modify reward function (e.g., remove absolute value, add term for image quality) to see if "harmonious" equilibrium correlates with visual quality

## Open Questions the Paper Calls Out

- **Open Question 1:** How can FSCO be adapted to successfully train GANs on high-resolution datasets (e.g., 512×512) where current discriminator implementation fails to learn features? The authors state experiments on 512×512 network were unsuccessful because discriminator was "almost unable to learn features," limiting applicability.

- **Open Question 2:** Can the FSCO reward mechanism be modified to mitigate severe overfitting observed in later stages of training? The authors note that while gaming process remains stable, "overfitting phenomena appeared in the later stages" across multiple experiments.

- **Open Question 3:** Can the dependency on careful manual selection of discriminator's base learning rate be eliminated or fully automated? The paper concludes FSCO "requires careful selection of the discriminator's base learning rate" and cannot theoretically determine optimal fixed step size.

## Limitations
- Cannot overcome fundamental limitations of network architecture—if generator lacks capacity, FSCO only delays rather than prevents failure
- Requires careful selection of discriminator's base learning rate, reducing but not solving black-box tuning difficulty
- Severe overfitting observed in later stages of training despite stabilized adversarial dynamics

## Confidence

- **High confidence:** The core mechanism (DDPG agent modulating discriminator learning rate based on loss difference) is clearly specified and internally consistent with GAN training dynamics
- **Medium confidence:** Experimental results on three datasets demonstrate stability improvements and expanded hyperparameter ranges, but lack of quantitative metrics limits generalizability claims
- **Low confidence:** Claims about "harmonious" training reducing hyperparameter tuning time are supported qualitatively but lack systematic comparison to manual tuning efforts or alternative stabilization methods

## Next Checks

1. Implement the same DCGAN with FSCO on MNIST using only specified hyperparameters and document whether discriminator learning rate actually decreases when generator loss increases
2. Test the method's sensitivity to base discriminator learning rate by sweeping across 10× the specified range and documenting failure points
3. Measure image quality metrics (FID, if possible, or simple reconstruction error) alongside loss curves to verify that "harmonious" equilibrium correlates with actual image fidelity rather than just numerical loss balance