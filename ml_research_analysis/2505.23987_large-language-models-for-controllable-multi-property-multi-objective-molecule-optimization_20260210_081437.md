---
ver: rpa2
title: Large Language Models for Controllable Multi-property Multi-objective Molecule
  Optimization
arxiv_id: '2505.23987'
source_url: https://arxiv.org/abs/2505.23987
tags:
- shot
- tasks
- llms
- properties
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces C-MuMOInstruct, the first large-scale instruction-tuning
  dataset for controllable multi-property molecule optimization with property-specific
  objectives. It enables models to selectively improve specific molecular properties
  while maintaining others at desirable levels, reflecting real-world drug design
  needs.
---

# Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization

## Quick Facts
- arXiv ID: 2505.23987
- Source URL: https://arxiv.org/abs/2505.23987
- Reference count: 40
- Key outcome: GeLLM4O-Cs achieve up to 126% higher success rate than baselines on controllable multi-property molecule optimization

## Executive Summary
This paper introduces C-MuMOInstruct, the first large-scale instruction-tuning dataset for controllable multi-property molecule optimization with explicit property-specific objectives. The dataset enables large language models to learn targeted structural modification strategies for improving specific molecular properties while maintaining others at desirable levels. Leveraging this dataset, the authors develop GeLLM4O-Cs, a family of instruction-tuned LLMs including specialist and generalist variants that outperform strong baselines on both in-distribution and out-of-distribution tasks.

## Method Summary
The authors construct C-MuMOInstruct from molecule pairs satisfying similarity, improvement, and stability constraints, then fine-tune Mistral-7B and Llama3.1-8B with LoRA (rank 16) to create specialist models for individual property combinations and generalist models trained across multiple combinations. Training uses 256,185 curated pairs with 30 instruction templates per task, employing beam search inference to generate 20 candidates per molecule. Evaluation spans 5 in-distribution and 5 out-of-distribution tasks with success rate as the primary metric.

## Key Results
- GeLLM4O-Cs achieve up to 126% higher success rate than best general-purpose LLMs
- Generalist models demonstrate 35% better success rate than best LLMs on out-of-distribution tasks
- Specialist models excel on narrow tasks but generalist variants enable zero-shot generalization to novel property combinations

## Why This Works (Mechanism)

### Mechanism 1: Property-Specific Objective Encoding via Instruction Tuning
Instruction tuning on molecule pairs with explicit property-specific thresholds enables LLMs to learn targeted structural modification strategies that distinguish between "improve" vs. "maintain" objectives. The model learns to associate natural language directives with specific structural modifications through pairwise training.

### Mechanism 2: Cross-Task Knowledge Transfer via Multi-Property Combinations
Generalist models trained across diverse property combinations enable transfer of shared chemical semantics and modification strategies to novel property combinations during inference, supporting zero-shot generalization to unseen tasks.

### Mechanism 3: Implicit Structure-Property Mapping via Pairwise Examples
Training on (Mx, My) pairs with similarity and property constraints enables the model to implicitly encode how structural modifications map to property changes without explicit property predictors.

## Foundational Learning

- **Multi-objective optimization with Pareto trade-offs**: Core task involves simultaneously optimizing multiple properties that often conflict. Quick check: Given a molecule with sub-optimal BBBP (0.6, threshold 0.8) and near-optimal QED (0.85, threshold 0.8), what constraints should guide optimization?

- **SMILES molecular representation and canonicalization**: All molecules are represented as SMILES strings. Quick check: What does SMILES "C#Cc1ccc(C2CC3CCC(C2C(=O)OC)N3C)cc1" represent, and how would you determine if two SMILES strings describe the same molecule?

- **ADMET properties in drug discovery**: The 10 properties (BBBP, hERG, DRD2, etc.) are standard ADMET endpoints. Quick check: Why is hERG inhibition typically minimized, and what structural features increase hERG binding risk?

## Architecture Onboarding

- **Component map**: C-MuMOInstruct dataset (256,185 pairs) -> LoRA fine-tuning (rank 16) -> GeLLM4O-C specialist/generalist models -> Beam search inference (width 20) -> Evaluation metrics

- **Critical path**: Data curation (Algorithm A1) -> Instruction tuning (LoRA) -> Inference (beam search) -> Evaluation (SR, Sim, RI)

- **Design tradeoffs**: Specialist vs. Generalist (narrow excellence vs. OOD generalization), training scale (â‰¤100 pairs/task), similarity threshold (Tanimoto > 0.6)

- **Failure signatures**: Low validity rate (<90%), poor generalization to OOD, property conflict-induced degradation (BDPQ), overfitting to instruction phrasing

- **First 3 experiments**:
  1. Reproduce IND baseline: Train BPQ specialist, target SR > 78%
  2. Ablate training data scale: Vary pairs-per-task limits (50, 100, 200)
  3. Stress-test OOD generalization: Evaluate P(10) on synthetic contradictory objectives

## Open Questions the Paper Calls Out

- **Open Question 1**: How can an iterative feedback mechanism be designed to allow GeLLM4O-C to refine molecules over multiple steps until all pharmaceutical thresholds are met? The current framework is single-step, and designing feedback mechanisms is noted as future work.

- **Open Question 2**: How can negative transfer caused by conflicting objectives in multi-task training be mitigated? The generalist model underperforms on tasks with competing objectives, but specific solutions are not proposed.

- **Open Question 3**: Does training on experimentally validated wet-lab data improve optimization accuracy compared to computational predictors? The paper suggests this as a promising direction given potential inaccuracies in empirical predictors.

## Limitations
- Dependence on external property predictors with unspecified versions/configurations creates reproducibility gaps
- Pairwise training may struggle with novel scaffolds requiring large structural changes beyond Tanimoto 0.6 similarity
- Generalist models show objective conflicts suggesting limitations in simultaneously optimizing incompatible property combinations

## Confidence
- **High confidence**: C-MuMOInstruct dataset construction and specialist model performance on in-distribution tasks
- **Medium confidence**: Generalist zero-shot OOD generalization claims (limited to 5 OOD tasks)
- **Low confidence**: Mechanism claims about implicit structure-property mapping (weak corpus support)

## Next Checks
1. Reproduce property score consistency: Recompute all property scores using multiple ADMET predictor versions to quantify scoring variance
2. Analyze failure mode patterns: Categorize failures by property type and compare model explanations to identify systematic reasoning gaps
3. Test scaffold generalization limits: Evaluate models on out-of-distribution scaffolds requiring >0.6 Tanimoto similarity changes