---
ver: rpa2
title: 'Artificial Intelligence in the Food Industry: Food Waste Estimation based
  on Computer Vision, a Brief Case Study in a University Dining Hall'
arxiv_id: '2507.14662'
source_url: https://arxiv.org/abs/2507.14662
tags:
- food
- waste
- segmentation
- consumption
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents a computer vision-based framework for estimating
  food waste in university dining halls by analyzing RGB images of meals before and
  after consumption. Five Iranian dishes were selected, and semantic segmentation
  was performed using four models: U-Net, U-Net++, and their lightweight variants.'
---

# Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall

## Quick Facts
- **arXiv ID**: 2507.14662
- **Source URL**: https://arxiv.org/abs/2507.14662
- **Reference count**: 14
- **Primary result**: Computer vision framework for food waste estimation in university dining halls achieves >90% pixel accuracy across five Iranian dishes using semantic segmentation models

## Executive Summary
This study presents a computer vision-based framework for estimating food waste in university dining halls by analyzing RGB images of meals before and after consumption. Five Iranian dishes were selected, and semantic segmentation was performed using four models: U-Net, U-Net++, and their lightweight variants. A custom capped dynamic inverse-frequency loss and AdamW optimizer were employed, with models trained on a dataset of 848 images (including augmented data) and evaluated on 245 test images. All models achieved strong performance, with at least one model exceeding 90% Distributional Pixel Accuracy (DPA) for each dish. Lightweight models demonstrated faster inference speeds while maintaining competitive accuracy. The approach offers a scalable, contactless solution for real-time food waste monitoring in institutional settings, though it is limited by reliance on 2D imaging and a constrained food variety.

## Method Summary
The research employed a computer vision approach using semantic segmentation to estimate food waste in university dining halls. Five Iranian dishes were selected as the target foods. Four deep learning models were utilized: U-Net, U-Net++, and their lightweight variants. A custom capped dynamic inverse-frequency loss function was designed to address class imbalance, combined with the AdamW optimizer. The dataset consisted of 848 images (including augmented data) for training and 245 images for testing. Images captured plates with food before and after consumption, allowing the models to segment and quantify waste portions.

## Key Results
- All four models achieved Distributional Pixel Accuracy (DPA) scores exceeding 90% for at least one of the five tested dishes
- Lightweight model variants demonstrated significantly faster inference speeds while maintaining competitive accuracy with their full-sized counterparts
- U-Net and U-Net++ architectures showed superior performance in segmenting complex food textures and shapes compared to traditional thresholding methods
- The custom capped dynamic inverse-frequency loss function effectively addressed class imbalance challenges in the food waste segmentation task

## Why This Works (Mechanism)
The computer vision approach works by leveraging semantic segmentation to classify each pixel in meal images as either food or waste. By comparing pre-consumption and post-consumption images, the system can calculate the volume of food waste through pixel-wise differences. The use of deep learning models, particularly U-Net architectures, enables effective feature extraction from food textures and shapes, allowing accurate distinction between different food types and waste portions. The custom loss function addresses the inherent class imbalance where waste regions typically occupy smaller pixel areas than intact food, ensuring the model doesn't bias toward the majority class during training.

## Foundational Learning
**Semantic Segmentation**: The process of classifying each pixel in an image into predefined categories. Why needed: Essential for distinguishing between food and waste at the pixel level to calculate accurate waste volumes. Quick check: Verify model can correctly label individual pixels in test images.

**U-Net Architecture**: A convolutional neural network designed for biomedical image segmentation with an encoder-decoder structure and skip connections. Why needed: Provides effective feature extraction and precise localization for food waste boundaries. Quick check: Confirm encoder captures relevant features while decoder reconstructs spatial information.

**Class Imbalance Handling**: Techniques to address situations where some classes have significantly fewer samples than others. Why needed: Waste regions typically represent minority classes in food images, requiring special treatment to prevent model bias. Quick check: Monitor class-specific performance metrics during training.

**Data Augmentation**: Techniques to artificially expand dataset size by applying transformations like rotations, flips, and color adjustments. Why needed: Increases model robustness and generalization by exposing it to varied representations of the same food items. Quick check: Validate model performance on augmented vs. non-augmented test sets.

**Distributional Pixel Accuracy (DPA)**: A metric measuring the percentage of correctly classified pixels across all classes. Why needed: Provides quantitative assessment of segmentation quality for food waste estimation. Quick check: Compare DPA scores across different model architectures and food types.

## Architecture Onboarding

**Component Map**: Image Acquisition -> Preprocessing -> Semantic Segmentation -> Post-processing -> Waste Volume Calculation

**Critical Path**: The most time-sensitive components are the semantic segmentation inference and waste volume calculation. Preprocessing must be efficient to avoid bottlenecks, while post-processing should be minimal as the primary output comes from the segmentation model.

**Design Tradeoffs**: The study balanced model accuracy against inference speed by developing lightweight variants of U-Net and U-Net++. While heavier models achieved higher accuracy, lightweight versions provided acceptable performance with faster processing times, crucial for real-time deployment. The custom loss function tradeoff involved increased training complexity for better handling of class imbalance.

**Failure Signatures**: Model performance degradation may occur with: (1) Poor lighting conditions affecting image quality, (2) Mixed dishes with overlapping food types, (3) Novel food items outside the training distribution, (4) Occlusions from utensils or plates, (5) Significant perspective distortion from camera angle changes.

**First 3 Experiments**:
1. Test baseline performance on held-out validation set with varying lighting conditions to assess robustness
2. Evaluate inference time and accuracy tradeoff between full-sized and lightweight model variants on the same test set
3. Conduct ablation study removing the custom capped dynamic inverse-frequency loss to quantify its contribution to performance

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to only five Iranian dishes, restricting generalizability across diverse food types and cultural contexts
- Reliance on 2D RGB imaging introduces inherent accuracy limitations, as depth information and 3D volume calculations cannot be directly obtained
- Dataset, while augmented, remains relatively small with 848 training images, potentially limiting model robustness to real-world variability

## Confidence
- Model Performance Claims (High): The reported high DPA scores and comparative analysis between model variants are well-supported by the presented methodology and results.
- Generalization Claims (Medium): While the approach shows promise, confidence is limited by the narrow scope of food types and controlled experimental conditions.
- Real-world Applicability (Low): Claims about scalability and practical implementation require further validation beyond the laboratory setting.

## Next Checks
1. Expand dataset testing to include at least 15-20 diverse food types from multiple cuisines to assess model robustness across different meal compositions.
2. Conduct field validation in actual dining hall settings over extended periods to evaluate performance under varying lighting, crowd conditions, and camera placements.
3. Implement ground-truth validation using volume measurements from actual waste disposal to verify the correlation between pixel-level accuracy and real-world waste quantification.