---
ver: rpa2
title: 'FlowDPS: Flow-Driven Posterior Sampling for Inverse Problems'
arxiv_id: '2503.08136'
source_url: https://arxiv.org/abs/2503.08136
tags:
- flow
- inverse
- flowdps
- diffusion
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FlowDPS, a novel flow-based posterior sampling
  method for solving inverse problems. The method leverages flow matching models to
  perform posterior sampling by decomposing the flow ODE into clean image and noise
  estimation components.
---

# FlowDPS: Flow-Driven Posterior Sampling for Inverse Problems

## Quick Facts
- arXiv ID: 2503.08136
- Source URL: https://arxiv.org/abs/2503.08136
- Authors: Jeongsol Kim; Bryan Sangwoo Kim; Jong Chul Ye
- Reference count: 40
- Primary result: Introduces FlowDPS, a flow-based posterior sampling method that achieves state-of-the-art results on four linear inverse problems without additional training

## Executive Summary
FlowDPS introduces a novel flow-based posterior sampling method for solving inverse problems by leveraging flow matching models. The method decomposes the flow ODE into clean image and noise estimation components, which are then manipulated through likelihood gradient integration for data consistency and stochastic noise addition for generative quality. This approach can be seamlessly integrated with latent flow models like Stable Diffusion 3.0 and demonstrates superior reconstruction quality on linear inverse problems including super-resolution and deblurring tasks across multiple datasets.

## Method Summary
FlowDPS addresses inverse problems by performing posterior sampling using pre-trained flow matching models. The core innovation lies in decomposing the flow ordinary differential equation (ODE) into two components: one for estimating clean images and another for estimating noise. These components are manipulated separately - the clean image component is guided by likelihood gradients to ensure data consistency with the observed measurements, while stochastic noise is added to maintain the generative quality of the samples. This approach allows FlowDPS to leverage existing pre-trained flow models without requiring additional training, making it highly practical and efficient for solving various inverse problems.

## Key Results
- Achieves state-of-the-art performance on super-resolution and deblurring tasks across AFHQ, FFHQ, and DIV2K datasets
- Outperforms existing LDM-based and flow-based solvers in PSNR, SSIM, FID, and LPIPS metrics
- Demonstrates superior reconstruction quality for high-resolution images with severe degradations without requiring additional training

## Why This Works (Mechanism)
The method works by leveraging the mathematical structure of flow matching models to perform posterior sampling. By decomposing the flow ODE into clean image and noise estimation components, FlowDPS can separately optimize for data consistency (through likelihood gradient integration) and generative quality (through stochastic noise addition). This decomposition allows the method to maintain the probabilistic nature of the flow model while ensuring the reconstructed images are consistent with the observed measurements, effectively bridging the gap between generative modeling and inverse problem solving.

## Foundational Learning
- **Flow Matching Models**: Neural networks trained to estimate the probability flow between distributions - needed to understand the underlying architecture FlowDPS builds upon; quick check: can you explain the difference between score-based models and flow matching models?
- **Inverse Problems**: Mathematical frameworks for recovering signals from incomplete or corrupted measurements - essential for understanding the problem domain; quick check: can you formulate a simple inverse problem mathematically?
- **ODE Solvers**: Numerical methods for solving ordinary differential equations - critical for understanding how flow models operate; quick check: what are the trade-offs between Euler and higher-order ODE solvers?
- **Likelihood Gradients**: Gradients of the probability density with respect to observations - needed for data consistency enforcement; quick check: can you derive the likelihood gradient for a simple Gaussian noise model?
- **Posterior Sampling**: Drawing samples from the posterior distribution - the ultimate goal of FlowDPS; quick check: how does posterior sampling differ from MAP estimation?

## Architecture Onboarding

**Component Map**
Flow Matching Model -> ODE Decomposition -> Likelihood Gradient Integration -> Stochastic Noise Addition -> Posterior Samples

**Critical Path**
Degraded Image → Flow Matching Model → Clean Image and Noise Components → Likelihood Gradient Update → Stochastic Noise Addition → Reconstructed Image

**Design Tradeoffs**
- Uses pre-trained models (fast, practical) vs. training specialized models (potentially better performance)
- Decomposes ODE (flexible, interpretable) vs. end-to-end training (simpler, potentially more optimized)
- Integrates likelihood gradients (data consistency) vs. pure generative sampling (easier, less constrained)

**Failure Signatures**
- Poor reconstruction quality when degradation models differ significantly from flow model assumptions
- Mode collapse or unrealistic outputs when stochastic noise parameters are mis-tuned
- Computational bottlenecks when scaling to very high-resolution images due to ODE solver requirements

**3 First Experiments**
1. Test FlowDPS on synthetic super-resolution with varying degradation levels to establish baseline performance
2. Compare reconstruction quality with and without likelihood gradient integration to quantify its contribution
3. Evaluate sensitivity to stochastic noise parameters by varying noise scales and observing output quality

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily dependent on quality and generalization of pre-trained flow models
- Decomposition approach may not generalize well to non-linear inverse problems
- Integration of likelihood gradients and stochastic noise may require significant adaptation for complex inverse problems

## Confidence
- **High**: Claims regarding superior performance on tested linear inverse problems are well-supported by quantitative metrics
- **Medium**: Claims about state-of-the-art results without additional training assume sufficient quality of pre-trained models
- **Low**: Generalizability to non-linear inverse problems and robustness to distribution shifts remain unproven

## Next Checks
1. Test FlowDPS on non-linear inverse problems (e.g., compressive sensing with non-Gaussian noise) to evaluate generalizability beyond linear degradations
2. Conduct ablation studies on the contribution of each component (clean image estimation, noise estimation, likelihood gradient integration, stochastic noise addition)
3. Evaluate FlowDPS on out-of-distribution datasets and real-world corrupted images to assess robustness and practical applicability