---
ver: rpa2
title: Evaluating the performance and fragility of large language models on the self-assessment
  for neurological surgeons
arxiv_id: '2505.23477'
source_url: https://arxiv.org/abs/2505.23477
tags:
- llms
- questions
- accuracy
- medical
- open-source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated 28 large language models on 2,904 neurosurgery
  board-style questions from the CNS-SANS benchmark and tested their resilience to
  distracting, irrelevant information. Six models achieved board-passing performance,
  with top models scoring over 15.7% above the passing threshold.
---

# Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons

## Quick Facts
- arXiv ID: 2505.23477
- Source URL: https://arxiv.org/abs/2505.23477
- Reference count: 0
- Top models scored over 15.7% above the passing threshold on neurosurgery board exam questions

## Executive Summary
This study evaluated 28 large language models on 2,904 neurosurgery board-style questions from the CNS-SANS benchmark, testing their performance with and without distracting, irrelevant information. Six models achieved board-passing performance, with top models scoring significantly above the passing threshold. However, all models experienced substantial accuracy drops when extraneous distractors were added, with some declining by as much as 20.4%. Proprietary models showed greater resilience than open-source variants, but even they were affected. Fragility was most pronounced in Neuropathology questions for general open-source models. These findings highlight that while LLMs can answer neurosurgical questions at expert levels, their performance is highly vulnerable to irrelevant content, underscoring the need for improved robustness before clinical deployment.

## Method Summary
The study evaluated 28 LLMs (19 general open-source, 3 medical open-source, 6 proprietary) on 2,904 text-only CNS-SANS neurosurgery board questions. Distractors were generated by GPT-4o, which extracted clinical terms from incorrect answer choices and created non-clinical sentences using those terms in polysemous contexts. Models were evaluated using temperature=0 and exact string matching on chain-of-thought outputs. Accuracy was compared under clean and distractor conditions to measure performance degradation.

## Key Results
- Six models achieved board-passing performance, with top models scoring over 15.7% above the passing threshold
- All models experienced significant accuracy drops when extraneous distractors were added, ranging from 18.6% to 20.4%
- Proprietary models showed higher baseline performance and greater resilience compared to open-source models
- Neuropathology questions exhibited the highest accuracy loss for general open-source models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit attention diffusion when semantically familiar but contextually irrelevant terms appear in prompts, degrading task performance.
- Mechanism: Polysemous medical terms activate clinical semantic clusters during attention, causing the model to distribute computational weight toward noise rather than task-relevant tokens.
- Core assumption: The attention mechanism lacks explicit salience filtering for clinically irrelevant but semantically related content.
- Evidence anchors:
  - The framework incorporated simple, irrelevant distractor statements containing polysemous words with clinical meanings used in non-clinical contexts to determine the extent to which such distractions degrade model performance.
  - For instance, we might add in the sentence 'The patient's zodiac sign is Cancer,' in which 'Cancer' refers to the astrological sign rather than the malignancy.
  - Vishwanath et al. 2025 (cited in paper) found similar distraction effects with 18% accuracy drops.

### Mechanism 2
- Claim: Proprietary models demonstrate greater resilience to distractors due to scale and training data diversity, not medical domain knowledge alone.
- Mechanism: Larger models with broader training corpora may develop more robust attention patterns that can contextualize polysemous terms against broader semantic priors.
- Core assumption: Proprietary models' robustness stems from architectural scale and pretraining diversity rather than domain-specific adaptation.
- Evidence anchors:
  - Proprietary models showed higher baseline performance and greater resilience compared to open-source models.
  - The proprietary models significantly outperformed both open-source models (P = .0280 compared to medical open-source; P < 10-4 compared to general open-source).
  - Although medical fine-tuning improves content familiarity, it does not reliably teach models to suppress misleading or non-salient information.

### Mechanism 3
- Claim: Neuropathology questions exhibit disproportionate accuracy degradation because they rely on abstract, polysemous terminology that is inherently more confusable.
- Mechanism: Neuropathology language often involves terms with multiple clinical and non-clinical meanings, creating greater interference when distractors activate competing semantic pathways.
- Core assumption: Domain-specific semantic complexity correlates with vulnerability to polysemous distractors.
- Evidence anchors:
  - The accuracy loss for Neuropathology was significantly higher than that of other sections only for the general open-source models (P < 10-4 compared to Functional...).
  - This may reflect the abstract, clinically inconsequent, and often polysemous language used in neuropathology, which challenges even experienced clinicians.

## Foundational Learning

- Concept: **Polysemy and semantic interference**
  - Why needed here: Understanding how words with multiple meanings can activate incorrect semantic pathways is essential for diagnosing why distractors work.
  - Quick check question: If a model sees "The patient is a Leo with a mass," which semantic frame for "Leo" should dominate—astrological or anatomical?

- Concept: **Attention distribution in transformer models**
  - Why needed here: The vulnerability to distractors is fundamentally an attention allocation problem—models weight irrelevant tokens too heavily.
  - Quick check question: What happens to token attention weights when a semantically familiar distractor sentence is prepended to a clinical question?

- Concept: **Adversarial robustness vs. benchmark accuracy**
  - Why needed here: High clean accuracy does not imply robust performance; these are independent capabilities that must be evaluated separately.
  - Quick check question: A model scores 85% on clean questions and 70% with distractors—is it "safe" for clinical deployment?

## Architecture Onboarding

- Component map:
  - Benchmark dataset (2,904 CNS-SANS questions) -> Distractor generator (GPT-4o with polysemous terms) -> Evaluation pipeline (temperature=0, exact string match) -> Model tiers (general open-source, medical open-source, proprietary)

- Critical path:
  1. Extract question + incorrect answer options
  2. Identify clinical terms from distractors
  3. Generate polysemous non-clinical sentences
  4. Embed into original question
  5. Evaluate accuracy delta

- Design tradeoffs:
  - Clean benchmark performance vs. real-world robustness (high clean scores do not predict distraction resilience)
  - Medical fine-tuning vs. generalization (domain adaptation may increase sensitivity to clinical terms, worsening distraction effects)
  - Exact string matching vs. semantic evaluation (underestimates partial competence but ensures reproducibility)

- Failure signatures:
  - Accuracy drop >10% when single distractor sentence added
  - Neuropathology section disproportionately affected (semantic ambiguity marker)
  - Open-source models clustering at high accuracy loss / low baseline accuracy (Spearman ρ = -0.66)

- First 3 experiments:
  1. Replicate distractor injection on a different medical benchmark (e.g., USMLE) to test generalizability of fragility.
  2. Vary distractor position (prepend vs. append vs. mid-insertion) to map attention sensitivity to token location.
  3. Fine-tune a small open-source model with adversarial noise augmentation (distractor + non-distractor pairs) and measure resilience gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What training-time or architectural interventions can effectively bolster LLM resilience to in-text distractions without compromising baseline medical knowledge accuracy?
- Basis in paper: Authors state "future systems may require technical and architectural innovations such as adversarial noise-aware fine-tuning and retrieval-augmented prompting restricted to curated templates" and call for "developing novel mitigation strategies aimed at bolstering LLM resilience."
- Why unresolved: The paper demonstrates vulnerability but does not test any mitigation strategies; prior work cited shows fine-tuning alone does not confer resistance.
- What evidence would resolve it: Comparative evaluation of models trained with adversarial noise augmentation, attention mechanisms prioritizing clinical relevance, or curated retrieval systems on the same distraction benchmark.

### Open Question 2
- Question: How does distractor vulnerability change when multimodal inputs (images) are included in neurosurgical board questions?
- Basis in paper: Authors note "roughly one-quarter of the original SANS corpus—those items containing images—was excluded; multimodal LLMs may yield different robustness profiles."
- Why unresolved: Vision-language models may integrate visual and textual cues differently, potentially amplifying or mitigating text-based distraction effects—this was not tested.
- What evidence would resolve it: Evaluation of multimodal LLMs on the excluded image-containing SANS questions with and without distractors, comparing accuracy degradation to text-only results.

### Open Question 3
- Question: Does vulnerability generalize to other distractor types common in real clinical documentation, such as copy-paste redundancy, transcription errors, or longer digressions?
- Basis in paper: "Only a single distractor style was examined, whereas real-world clinical notes may feature longer digressions, copy-and-paste redundancy and transcription errors."
- Why unresolved: The polysemous-word distractor paradigm may not capture the full spectrum of noise in EHR-derived text; severity could differ substantially.
- What evidence would resolve it: Systematic evaluation using multiple distractor categories on the same LLMs, quantifying relative accuracy declines across distractor types.

### Open Question 4
- Question: Why are Neuropathology questions disproportionately susceptible to distractors in open-source models, and does this reflect semantic properties amenable to targeted robustness improvements?
- Basis in paper: Authors observed that "accuracy loss for Neuropathology was significantly higher than that of other sections only for the general open-source models" and suggest "abstract, clinically inconsequent, and often polysemous language" may play a role, but do not test mechanisms.
- Why unresolved: The underlying cause—semantic ambiguity, training data gaps, or attention distribution—remains unexplored.
- What evidence would resolve it: Attention analysis on Neuropathology vs. more robust sections; targeted fine-tuning experiments with augmented neuropathology examples.

## Limitations

- The study tested only text-based questions, excluding the 1,061 image-containing questions from CNS-SANS, limiting generalizability to the full board exam
- Exact string matching for evaluation may underestimate partial knowledge and fail to capture semantic understanding
- Proprietary models were accessed via APIs, introducing potential confounding from unstated preprocessing or optimization
- Only a single distractor style was examined, whereas real-world clinical notes may feature longer digressions, copy-and-paste redundancy, and transcription errors

## Confidence

- **High confidence**: LLMs experience significant accuracy degradation when exposed to irrelevant distractors (18.6-20.4% drops observed across models)
- **Medium confidence**: Proprietary models demonstrate greater resilience than open-source variants, given the lack of transparency about proprietary model architectures and training data
- **Medium confidence**: Polysemy and attention diffusion explain the mechanism of distraction vulnerability, as this is theoretically sound but not directly measured in the study

## Next Checks

1. Replicate the distraction experiment on a different medical benchmark (e.g., USMLE) to test whether the fragility pattern generalizes beyond neurosurgery
2. Vary distractor position (prepend vs. append vs. mid-insertion) to map attention sensitivity to token location and test the attention diffusion hypothesis
3. Fine-tune a small open-source model with adversarial noise augmentation (distractor + non-distractor pairs) and measure whether resilience can be improved without scaling up model size