---
ver: rpa2
title: 'Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to
  Use Instead'
arxiv_id: '2510.01624'
source_url: https://arxiv.org/abs/2510.01624
tags:
- performance
- training
- pass
- examples
- after
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work identifies that high post-SFT performance is not reliably
  predictive of subsequent RL gains, often due to overfitting or dataset bias toward
  simpler or homogeneous examples. To address this, the authors propose two new predictors:
  generalization loss on held-out reasoning examples and Pass@k accuracy at large
  k values.'
---

# Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead

## Quick Facts
- **arXiv ID**: 2510.01624
- **Source URL**: https://arxiv.org/abs/2510.01624
- **Reference count**: 28
- **Primary result**: High post-SFT scores often mislead RL gains due to overfitting; proposed metrics (generalization loss, large-k Pass@k) double prediction accuracy.

## Executive Summary
This work reveals that high post-SFT (supervised fine-tuning) performance is not a reliable predictor of downstream reinforcement learning (RL) gains, often because SFT overfits to simpler or homogeneous examples in the training data. The authors propose two alternative predictors—generalization loss on held-out reasoning examples and Pass@k accuracy at large k values—that significantly improve prediction accuracy (up to 2x in R² and Spearman's correlation) compared to using post-SFT scores alone. These insights help optimize SFT training and data selection before costly RL fine-tuning, improving both efficiency and final model performance. An enhanced evaluation tool will be open-sourced.

## Method Summary
The authors identify that post-SFT scores can be misleading due to overfitting and dataset bias toward easier examples. To address this, they propose two new predictors: generalization loss, which measures performance drop on held-out reasoning examples, and Pass@k accuracy at large k values, which assesses robustness across many candidates. These metrics are shown to better forecast RL gains by focusing on reasoning diversity and generalization, rather than just SFT accuracy. The methodology is validated across multiple reasoning datasets, with improvements in prediction accuracy over naive post-SFT evaluation.

## Key Results
- High post-SFT scores are not reliably predictive of RL gains due to overfitting or bias toward simpler examples.
- Proposed metrics (generalization loss and large-k Pass@k) improve prediction accuracy by up to 2x in R² and Spearman's correlation.
- The findings enable more effective SFT training and data selection before committing to expensive RL fine-tuning.

## Why This Works (Mechanism)
The paper's proposed metrics address the core issue that post-SFT performance often overfits to the training distribution, especially when the data is skewed toward easier or homogeneous reasoning examples. By focusing on generalization loss and Pass@k at large k, the metrics capture a model's ability to handle diverse, harder reasoning problems, which are more indicative of RL gains. This shift from simple accuracy to reasoning robustness and diversity provides a more accurate signal for downstream RL performance.

## Foundational Learning
- **Overfitting in SFT**: Why needed: Prevents models from generalizing to unseen, harder examples. Quick check: Compare train/test performance gaps.
- **Dataset bias**: Why needed: Skewed data can inflate SFT scores without real reasoning gains. Quick check: Analyze example difficulty and diversity in training data.
- **Pass@k metric**: Why needed: Evaluates model robustness across multiple candidates, important for reasoning tasks. Quick check: Vary k and observe stability of Pass@k.
- **Generalization loss**: Why needed: Quantifies drop in performance on held-out reasoning sets, indicating overfitting. Quick check: Compute loss on in-distribution vs. out-of-distribution reasoning examples.
- **RL gains prediction**: Why needed: Guides efficient resource allocation before costly RL fine-tuning. Quick check: Correlate proposed metrics with actual RL improvements.
- **Held-out reasoning sets**: Why needed: Provide unbiased evaluation of model's reasoning capabilities. Quick check: Ensure held-out set contains diverse, challenging examples.

## Architecture Onboarding
- **Component map**: SFT training -> Post-SFT evaluation (traditional metrics) -> RL fine-tuning; Proposed: SFT training -> Generalization loss & Pass@k evaluation -> RL fine-tuning.
- **Critical path**: Efficient prediction of RL gains to minimize wasted compute on suboptimal SFT models.
- **Design tradeoffs**: Accuracy vs. computational cost of evaluation; robustness vs. sensitivity to dataset composition.
- **Failure signatures**: Overfitting to easy examples, poor generalization to held-out reasoning tasks, mismatch between post-SFT and RL performance.
- **Three first experiments**:
  1. Compare post-SFT accuracy vs. generalization loss on held-out reasoning examples.
  2. Evaluate Pass@k stability across a range of k values.
  3. Test predictive power of proposed metrics against actual RL gains on diverse reasoning datasets.

## Open Questions the Paper Calls Out
None

## Limitations
- Proposed metrics' gains depend heavily on dataset nature and task distribution; may not generalize to all domains.
- Applicability to non-reasoning tasks (e.g., code generation, classification) remains untested.
- Sensitivity of generalization loss and Pass@k to held-out set construction and choice of k is not fully explored.

## Confidence
- **Empirical results**: Medium (internally consistent, but external validity limited)
- **Applicability to non-reasoning tasks**: Low (not covered)
- **Robustness of proposed metrics**: Medium (depends on dataset and task specifics)

## Next Checks
1. Test the proposed metrics on non-reasoning tasks (e.g., code synthesis, multi-label classification) to assess cross-domain applicability.
2. Conduct ablation studies varying the held-out set composition (difficulty, domain diversity) to determine sensitivity of generalization loss to dataset bias.
3. Evaluate the stability and predictive power of Pass@k across a range of k values, including analysis of diminishing returns, to establish robust operational thresholds.