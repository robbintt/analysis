---
ver: rpa2
title: 'Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas'
arxiv_id: '2602.01418'
source_url: https://arxiv.org/abs/2602.01418
tags:
- pape
- position
- encoding
- attention
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Parabolic Position Encoding (PaPE), a new
  position encoding method for vision transformers based on learnable parabolas applied
  to relative token positions. PaPE is designed from five key principles: translation
  invariance, rotation invariance, distance decay, directionality, and context awareness.'
---

# Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas

## Quick Facts
- arXiv ID: 2602.01418
- Source URL: https://arxiv.org/abs/2602.01418
- Reference count: 27
- Primary result: Parabolic Position Encoding (PaPE) achieves top performance on 7 out of 8 datasets across 4 vision modalities

## Executive Summary
This paper introduces Parabolic Position Encoding (PaPE), a novel position encoding method for vision transformers based on learnable parabolas applied to relative token positions. The method is designed around five key principles: translation invariance, rotation invariance, distance decay, directionality, and context awareness. PaPE uses a quadratic term to encode distance and a linear term to encode direction, making it compatible with efficient attention kernels through separate query-key transformations.

Evaluated across 8 diverse datasets spanning images, point clouds, videos, and event cameras, PaPE achieves the highest accuracy on 7 out of 8 datasets with an average accuracy of 66.3%. Notably, PaPE demonstrates exceptional classification extrapolation capabilities, improving accuracy by up to 10.5% over the next-best position encoding when scaling beyond training resolution on ImageNet-1K. The method adds only modest computational overhead while providing significant performance gains across diverse vision tasks.

## Method Summary
PaPE encodes relative positions between tokens using a combination of quadratic and linear learnable parameters. The quadratic term captures distance information through a parabola-like function, while the linear term encodes directional information. This design allows the model to learn how positional relationships should influence attention based on the specific task and data modality. The encoding is applied to relative token positions, making it translation and rotation invariant by design. By separating the distance and direction components, PaPE maintains compatibility with efficient attention mechanisms that require distinct query and key transformations.

## Key Results
- Achieved top performance on 7 out of 8 datasets across 4 vision modalities (images, point clouds, videos, event cameras)
- Average accuracy of 66.3% across all evaluated datasets
- Demonstrated 10.5% improvement in classification extrapolation capabilities when scaling beyond training resolution on ImageNet-1K

## Why This Works (Mechanism)
PaPE's effectiveness stems from its principled design that directly encodes geometric relationships between tokens. The parabolic function naturally captures how the importance of positional information should decay with distance, while the linear component preserves directional information. This dual-component approach allows the model to learn task-specific patterns of how position should influence attention. The relative positioning ensures translation and rotation invariance, making the encoding applicable across diverse vision tasks without requiring dataset-specific modifications. The separation into distance and direction components also enables efficient computation through standard attention mechanisms.

## Foundational Learning
- **Vision Transformers**: Understanding how transformers process visual data through token attention mechanisms. Needed to grasp why position encoding is critical for vision tasks where spatial relationships matter.
- **Relative Position Encoding**: The concept of encoding positions based on relationships between tokens rather than absolute coordinates. Quick check: Compare absolute vs relative encoding in simple attention examples.
- **Parabolic Functions**: Mathematical understanding of quadratic functions and their properties. Quick check: Verify how parabola shape affects distance decay in position encoding.
- **Attention Mechanisms**: How attention weights are computed and how position encodings influence these weights. Quick check: Trace how position encodings modify attention scores in a simple transformer layer.
- **Translation/Rotation Invariance**: Understanding geometric transformations and their impact on vision tasks. Quick check: Demonstrate how different position encodings handle image shifts or rotations.
- **Efficient Attention Kernels**: Knowledge of how attention computations can be optimized and what constraints exist. Quick check: Examine how separate query-key transformations enable efficient attention.

## Architecture Onboarding

**Component Map**: Input tokens → Relative position calculation → PaPE encoding (quadratic + linear) → Attention computation → Output features

**Critical Path**: The critical path flows from input tokens through relative position calculation, through the parabolic encoding function, and into the attention mechanism. The encoding must be computed before attention can be applied, making it a bottleneck for the overall processing pipeline.

**Design Tradeoffs**: PaPE trades increased parameter count for improved positional awareness. The learnable quadratic and linear components add parameters but enable task-specific adaptation of positional importance. The separation into distance and direction components enables compatibility with efficient attention but may limit the expressiveness of the encoding compared to more complex formulations.

**Failure Signatures**: Potential failure modes include overfitting to training resolution (though PaPE shows better extrapolation), inability to capture complex positional patterns that require higher-order terms, and computational overhead in scenarios with extremely large numbers of tokens where position encoding computation dominates.

**Three First Experiments**:
1. Compare PaPE performance against standard absolute and relative position encodings on a simple image classification task
2. Test PaPE's extrapolation capabilities by training on low-resolution images and evaluating on high-resolution images
3. Measure the actual computational overhead of PaPE compared to baseline position encodings on different hardware platforms

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on classification tasks, potentially limiting generalizability to other vision applications
- Exceptional extrapolation capabilities demonstrated on a single dataset (ImageNet-1K), raising questions about consistency across different vision modalities
- Computational overhead claims lack detailed quantitative analysis across different hardware configurations and batch sizes

## Confidence
- **High**: Fundamental design principles and mathematical formulation are clearly presented and logically consistent
- **Medium**: Empirical performance claims are supported by experimental results but limited by focus on classification tasks
- **Low**: Computational efficiency claims lack sufficient quantitative analysis across diverse hardware configurations

## Next Checks
1. Evaluate PaPE on non-classification vision tasks such as object detection, segmentation, and instance segmentation to assess generalizability beyond classification
2. Conduct systematic ablation studies measuring the actual computational overhead (FLOPs, memory usage, inference time) across different hardware platforms and batch sizes
3. Test the extrapolation capabilities on additional datasets beyond ImageNet-1K, particularly in other vision modalities like point clouds and videos, to verify if the 10.5% improvement is consistent across tasks