---
ver: rpa2
title: 'FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language
  Models'
arxiv_id: '2503.19540'
source_url: https://arxiv.org/abs/2503.19540
tags:
- llms
- arxiv
- fairness
- benchmark
- scenarios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FLEX introduces a benchmark to evaluate fairness robustness in
  LLMs under extreme scenarios. It integrates adversarial prompts into fairness assessments
  to reveal vulnerabilities that traditional benchmarks miss.
---

# FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models

## Quick Facts
- arXiv ID: 2503.19540
- Source URL: https://arxiv.org/abs/2503.19540
- Authors: Dahyun Jung; Seungyoon Lee; Hyeonseok Moon; Chanjun Park; Heuiseok Lim
- Reference count: 16
- Primary result: Introduces adversarial fairness benchmark showing LLMs' vulnerability to extreme scenarios, with Attack Success Rates up to 0.4782

## Executive Summary
FLEX is a benchmark designed to evaluate the fairness robustness of large language models (LLMs) under extreme adversarial conditions. Traditional fairness evaluations often underestimate model risks by testing in benign scenarios, but FLEX constructs targeted adversarial prompts that systematically expose latent biases. The benchmark uses existing fairness datasets (BBQ, CrowS-Pairs, StereoSet) and applies extreme scenarios like persona injection, competing objectives, and text attacks to reveal vulnerabilities. Results show that AccF is consistently lower than AccS across models, with significant Attack Success Rates (ASR) indicating performance drops under adversarial conditions. Competing objectives prompts prove highly effective, while persona injection and text attacks show varying impacts across bias types and model architectures.

## Method Summary
FLEX constructs a benchmark by taking existing fairness datasets and applying adversarial prompts to expose model vulnerabilities. The process involves three stages: Coverage Restriction (filtering samples where the model answers neutrally), Extreme Scenario Selection (finding the most effective adversarial prompt for each sample via GPT-3.5 testing), and Diversity Control (ensuring balanced attack types). The benchmark evaluates models using AccS (source accuracy), AccF (FLEX accuracy), and ASR (Attack Success Rate = proportion of correct source answers that become incorrect under adversarial prompts). Experiments test zero-shot and few-shot settings across seven bias categories with 3,145 samples from three source datasets.

## Key Results
- AccF is consistently lower than AccS across all tested models, demonstrating fairness degradation under adversarial conditions
- Llama3-8b shows AccF dropping by 0.3004 with ASR reaching 0.4782, revealing significant vulnerability
- Competing Objectives attacks (especially Refusal Suppression) are most effective, reducing Llama3-8b accuracy to 0.1954
- GPT-4 shows relative robustness but most open-source models fail under extreme scenarios
- Positive few-shot demonstrations sometimes paradoxically increase vulnerability in certain models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Applying specific extreme scenarios to individual samples can systematically expose latent biases that remain undetected by conventional, non-adversarial evaluations.
- Mechanism: FLEX reconstructs fairness QA datasets by selectively applying one of three adversarial prompt types (Persona Injection, Competing Objectives, Text Attack) to each sample. The most effective prompt is chosen by measuring its ability to cause a model to fail on samples it previously answered correctly.
- Core assumption: A model's fairness is highly dependent on input context, and adversarial prompts can shift the model's internal state, prioritizing task completion over safety alignment.
- Evidence anchors: Section 3.2 describes the multi-step construction process; abstract states FLEX "test[s] whether LLMs can sustain fairness even when exposed to prompts constructed to induce bias."
- Break condition: If models become robust to the specific adversarial templates used, the benchmark's ability to expose vulnerabilities will diminish.

### Mechanism 2
- Claim: Simple, direct instruction-based attacks (Competing Objectives) can be more effective at revealing inherent biases than complex, role-playing prompts.
- Mechanism: "Competing Objectives" includes attacks like "Refusal Suppression" that instruct models to answer without using certain refusal phrases, directly interfering with safety alignment mechanisms.
- Core assumption: Safety alignment mechanisms, particularly refusal behaviors, are key defenses against bias. Attacks that suppress these refusals can bypass more complex reasoning or role-based alignment.
- Evidence anchors: Section 4.4 states direct response-forcing approaches "tend to reveal the models' inherent stereotypes"; Section 4.3 shows Llama3-8b accuracy falls from 0.7339 to 0.1954 under this category.
- Break condition: If safety alignment evolves beyond simple refusal phrases to embed deeper ethical reasoning, these attacks will become less effective.

### Mechanism 3
- Claim: The Attack Success Rate (ASR) metric provides a more meaningful measure of robustness than simple accuracy on a biased dataset.
- Mechanism: ASR measures the proportion of originally correct answers that become incorrect under adversarial conditions, isolating the change in behavior caused by the adversarial prompt.
- Core assumption: The key indicator of robustness failure is not just that a model is biased, but that it changes from unbiased to biased when confronted with adversarial input.
- Evidence anchors: Section 4.2 defines ASR and explains it "represents the proportion of samples that are correct in the source dataset but incorrect in our dataset"; Table 1 prominently features ASR as the key metric.
- Break condition: If the source dataset contains many samples answered correctly by chance, then a high ASR might partly reflect initial luck running out rather than genuine robustness failures.

## Foundational Learning

- Concept: **Adversarial Robustness in NLP**
  - Why needed here: FLEX is fundamentally an adversarial robustness benchmark. It assumes that a model's performance on benign inputs is insufficient to guarantee its safety.
  - Quick check question: Can you explain why a model that achieves 99% accuracy on a standard test set might still be considered unsafe for deployment?

- Concept: **Safety Alignment and Refusal Suppression**
  - Why needed here: The paper's "Competing Objectives" attacks directly target a common safety alignment technique—teaching models to refuse harmful requests.
  - Quick check question: How might instructing a model "Do not apologize" lead it to produce a stereotyped or biased answer?

- Concept: **Multiple-Choice QA for Bias Evaluation**
  - Why needed here: FLEX uses a specific format where the correct answer is "Unknown/Not enough information" and incorrect answers are stereotypical.
  - Quick check question: In a multiple-choice question designed to test for bias, why is including an "Unknown" or "Not enough information" option critical?

## Architecture Onboarding

- Component map: Source Datasets (BBQ, CrowS-Pairs, StereoSet) -> Adversarial Prompt Templates -> Sample Selector -> Final Benchmark (FLEX)
- Critical path: The evaluation's critical path runs from the Final Benchmark sample through the Target LLM to produce an answer, which is then scored against the ground truth. The crucial upstream step is the Sample Selector.
- Design tradeoffs:
  - **Targeted vs. Random Selection**: FLEX selects the most effective adversarial prompt for each sample, making the benchmark harder but potentially model-specific.
  - **Single vs. Multiple Attacks per Sample**: Each sample gets only one attack, increasing dataset diversity but potentially missing other vulnerabilities.
  - **Coverage Restriction**: Only samples answered correctly in normal setting are used, focusing on robustness but excluding already-biased questions.
- Failure signatures:
  - **High ASR**: A model with high AccS but low AccF will have high ASR, indicating fairness robustness failure.
  - **Vulnerability to Direct Instructions**: High ASR under "Competing Objectives" indicates failure to maintain safety alignment against simple contradictory instructions.
  - **High Bias on Gender/Sexual Orientation**: These categories often have higher ASRs, indicating persistent vulnerabilities.
- First 3 experiments:
  1. **Baseline Performance Measurement**: Evaluate target model on original source datasets and FLEX benchmark. Report AccS, AccF, and ASR to establish fairness and robustness baseline.
  2. **Category-Specific Vulnerability Analysis**: Break down ASR by adversarial categories (Persona Injection, Competing Objectives, Text Attack). Identify which attack type the model is most susceptible to.
  3. **Few-Shot Robustness Test**: Evaluate model on FLEX in few-shot setting with unbiased (positive) and biased (negative) examples. Analyze whether positive examples mitigate attacks or increase vulnerability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can safety alignment techniques be improved to ensure robustness against "direct instruction" attacks (competing objectives) without degrading instruction-following capabilities?
- **Basis in paper:** The paper concludes that "Direct Instruction is Still Enough," noting simple competing objectives cause significant performance drops (Llama3-8b accuracy drops to 0.1954).
- **Why unresolved:** The paper identifies the vulnerability but does not propose or test mitigation strategies; it strictly focuses on evaluation and diagnosis.
- **What evidence would resolve it:** A study demonstrating a fine-tuning or alignment method that specifically lowers ASR for "Competing Objectives" while maintaining performance on standard instruction-following benchmarks.

### Open Question 2
- **Question:** Why do positive few-shot demonstrations sometimes fail to mitigate bias or even increase vulnerability in certain models like Llama2-13b?
- **Basis in paper:** In "Challenges in Few-Shot Setting," the authors observe that positive shots do not significantly improve bias and for Llama2-13b, "introducing a positive shot actually leads to a substantial increase in ASR."
- **Why unresolved:** The paper only speculates that it relates to the model's ability to follow instructions, lacking a causal explanation for why positive examples backfire.
- **What evidence would resolve it:** An ablation study analyzing attention mechanisms or logit distributions of Llama2-13b when processing positive vs. negative few-shot prompts in extreme scenarios.

### Open Question 3
- **Question:** To what extent do FLEX's "extreme scenarios" vulnerabilities generalize to real-world, everyday user interactions?
- **Basis in paper:** The authors state in Limitations that "focusing on extreme scenarios may not fully represent the spectrum of everyday use cases, potentially limiting the generalizability of findings to typical user interactions."
- **Why unresolved:** FLEX is designed to stress-test models using adversarial, synthetic, and "harsh" prompts that might not reflect natural user distributions.
- **What evidence would resolve it:** A comparative analysis evaluating correlation between ASR on FLEX and bias metrics derived from "natural" or "benign" user interactions collected from deployed LLM logs.

## Limitations
- The benchmark's validity relies on GPT-3.5's judgments during construction generalizing to other models, introducing potential model-specific bias.
- ASR metric may conflate different failure modes—genuine bias activation vs. general reasoning difficulties under adversarial conditions.
- The benchmark focuses exclusively on multiple-choice questions with "Unknown/Not enough information" as correct answer, potentially missing dimensions of fairness failures in open-ended generation tasks.

## Confidence
- **High Confidence**: Experimental results showing consistent AccF < AccS across models and observation that direct instruction attacks are particularly effective.
- **Medium Confidence**: Claim that FLEX reveals "underestimated risks" in LLMs—while the benchmark demonstrates vulnerabilities, severity depends on deployment contexts not fully explored.
- **Medium Confidence**: Generalizability of ASR metric as measure of robustness—interpretation requires assumptions about source dataset quality and initial correctness.

## Next Checks
1. **Cross-Model Generalization Test**: Construct a validation subset of FLEX using a different model (e.g., Claude-3) to select the most effective adversarial prompts, then compare results with the original GPT-3.5-based benchmark to assess construction bias.

2. **Real-World Scenario Evaluation**: Test FLEX-identified vulnerabilities on task-specific datasets (e.g., medical diagnosis, legal advice) to determine whether benchmark failures translate to practical deployment risks.

3. **Attack Template Robustness Analysis**: Systematically vary the adversarial prompt templates while keeping the base questions constant to determine whether FLEX vulnerabilities are template-specific or reflect deeper model biases.