---
ver: rpa2
title: Improving Model Classification by Optimizing the Training Dataset
arxiv_id: '2507.16729'
source_url: https://arxiv.org/abs/2507.16729
tags:
- coreset
- data
- classification
- dataset
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates how coreset construction
  parameters influence classification performance, beyond traditional loss approximation.
  The authors propose tuning deterministic sampling ratios, class-wise sample allocation,
  and weight handling strategies to improve classification metrics such as F1 score
  and balanced accuracy.
---

# Improving Model Classification by Optimizing the Training Dataset

## Quick Facts
- **arXiv ID:** 2507.16729
- **Source URL:** https://arxiv.org/abs/2507.16729
- **Reference count:** 40
- **Primary result:** Tuned coresets outperform vanilla coresets and often full-dataset training on F1 score and balanced accuracy.

## Executive Summary
This paper challenges the conventional wisdom that coresets (weighted data subsets) should be optimized purely for loss approximation. Instead, it proposes a systematic framework to tune coreset construction parameters—deterministic sampling ratios, class-wise sample allocation, and weight handling—to improve classification metrics like F1 score and balanced accuracy. Through experiments on multiple datasets and classifiers (logistic regression, SVM, XGBoost), the authors demonstrate that thoughtfully designed coresets can not only reduce dataset size but also enhance model generalization and mitigate data imbalances, often surpassing full-dataset performance.

## Method Summary
The method builds on standard coreset theory but diverges by optimizing for classification quality rather than strict loss bounds. First, sensitivity scores (importance weights) are computed for each training point using methods like Leverage, Lewis, or Unified f-SVD. Then, a grid search tunes three parameters: deterministic inclusion of high-probability points, manual class-wise sample allocation (e.g., 50/50 regardless of dataset imbalance), and weight assignment strategies (`keep`, `inv`, `prop`). Optionally, active sampling iteratively adds uncertain points identified by a trained model. The coreset maximizing validation F1 is selected for final training.

## Key Results
- Tuned coresets consistently outperform vanilla coresets (proportional sampling) on F1 and balanced accuracy.
- In many cases, tuned coresets surpass full-dataset training performance.
- On highly imbalanced datasets (e.g., CreditCard with 0.17% positive class), manual class allocation significantly improves minority class recall and overall F1.
- Active sampling further refines coreset quality by adding hard-to-classify examples missed by sensitivity bounds.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Deterministic inclusion of high-sensitivity points improves data diversity and downstream classification compared to pure importance sampling.
- **Mechanism:** Standard importance sampling often assigns extreme probabilities to a small subset of points, causing them to be sampled multiple times (duplicates). This reduces the number of unique training instances, harming generalization even if training loss is approximated. By deterministically including points above a probability threshold (without duplication) and re-normalizing weights, the coreset retains more unique information.
- **Core assumption:** The paper assumes that duplicate high-sensitivity points contribute less to classification generalization than a broader set of unique points, even if those duplicates are theoretically optimal for loss approximation.
- **Evidence anchors:**
  - [abstract]: Mentions "deterministic sampling" as a tunable parameter to enhance downstream quality.
  - [section 2.2]: States that "points with very high probability tend to be sampled more than once. This leads to low diversity... classification metrics often degrade significantly."
  - [corpus]: Related work (e.g., "Stable Coresets via Posterior Sampling") suggests aligning loss landscapes helps, supporting the need for better distribution matching than simple loss approximation provides.
- **Break condition:** If the determinism threshold is set too high, the coreset may exhaust its budget on "forced" points, leaving insufficient samples for the "long tail" of the data distribution, potentially hurting robustness.

### Mechanism 2
- **Claim:** Manually adjusting class-wise sample allocation mitigates data imbalance effects better than proportional allocation.
- **Mechanism:** Proportional sampling reproduces the class imbalance of the full dataset in the coreset. For classifiers like Logistic Regression or SVM, this can bias the decision boundary toward the majority class. By manually allocating more samples to minority classes (e.g., 50/50 split regardless of original ratio), the model learns a more balanced boundary, directly improving metrics like F1 and Balanced Accuracy.
- **Core assumption:** This assumes the user has access to validation data that reflects the desired operational characteristics (e.g., caring about minority class performance) to tune the allocation ratios.
- **Evidence anchors:**
  - [abstract]: Proposes "class-wise sample allocation" to enhance classification quality.
  - [section 2.3]: Notes that proportional sampling "tends to overrepresent large classes and may under-sample informative or rare classes."
  - [section 4]: Experiments on the highly imbalanced CreditCard dataset (0.17% positive class) show significant gains in Balanced Accuracy and F1.
- **Break condition:** If the dataset is severely imbalanced and the model is prone to overfitting, oversampling the minority class without sufficient unique data (using duplication) might cause the model to memorize specific noise patterns in the minority class.

### Mechanism 3
- **Claim:** Active sampling refines the coreset to target model-specific uncertainty gaps missed by theoretical sensitivity bounds.
- **Mechanism:** Sensitivity scores (used for coresets) approximate the loss function generically, often bounding the "worst-case" error. They do not explicitly optimize for the specific decision boundary of the final trained classifier. An active sampling step uses the model trained on the initial coreset to identify points it is uncertain about (hard examples) and adds them to the set, directly patching holes in the decision boundary.
- **Core assumption:** This assumes the active sampling loop has access to a pool of unlabeled data (or remaining training data) and that the current model's uncertainty correlates with missing information critical for the test distribution.
- **Evidence anchors:**
  - [abstract]: Mentions "refinement via active sampling" as a key component.
  - [section 3]: Argues that sensitivity bounds are often loose upper bounds, creating a gap between the coreset distribution and the "true" distribution the model needs.
  - [corpus]: Related work on "Coreset Selection via LLM-based Concept Bottlenecks" highlights that standard selection often requires refinement to match model-specific needs.
- **Break condition:** If the "patience" parameter (stopping criterion) is not tuned correctly, active sampling may add noisy outliers that degrade the generalization learned from the initial coreset.

## Foundational Learning

- **Concept:** **Coresets & Sensitivity Scores**
  - **Why needed here:** The paper builds upon, but diverges from, standard coreset theory. You must understand that a coreset is typically a weighted subset designed to approximate the *loss* of the full dataset (Eq. 2). The paper argues this is insufficient for *classification metrics*.
  - **Quick check question:** Can you explain why a coreset that perfectly approximates Logistic Loss might still yield a poor F1 score?
- **Concept:** **Imbalanced Classification Metrics (F1 vs. Accuracy)**
  - **Why needed here:** The central thesis is optimizing for F1 and Balanced Accuracy rather than just loss. You need to understand that "Accuracy" can be misleading on imbalanced datasets (e.g., 99% accuracy by guessing "negative" on a 99% negative dataset).
  - **Quick check question:** On a dataset with 1% positive samples, why might increasing the Recall of the positive class lower the overall Accuracy but increase the F1 score?
- **Concept:** **Importance Sampling vs. Deterministic Selection**
  - **Why needed here:** The paper introduces a hybrid sampling method. You need to know the difference between sampling with replacement (probabilistic) and forcing inclusion (deterministic) to understand the weight adjustment mechanisms (keep, inv, prop).
  - **Quick check question:** If a point is deterministically included in a coreset, how should its weight theoretically change compared to if it were sampled with probability $p$?

## Architecture Onboarding

- **Component map:**
  1. **Sensitivity Engine:** Computes importance scores (e.g., Unified, Leverage) for the training data (referencing Section 4.1/4.2 types).
  2. **Allocation Manager:** Splits the total coreset budget $m$ across classes (implements Section 2.3).
  3. **Hybrid Sampler:** Selects points based on a threshold (deterministic) + probability (probabilistic) (implements Section 2.2).
  4. **Weight Normalizer:** Assigns final weights based on strategy (`keep`, `inv`, `prop`) (Section 2.2.1).
  5. **Active Refiner (Optional):** Iterative loop that trains a model, queries uncertain points, and adds them to the coreset (Algorithm 1).

- **Critical path:**
  1. Calculate sensitivities for the full dataset.
  2. **Grid Search:** Iterate through [Class Ratio $\times$ Deterministic Threshold $\times$ Weight Strategy].
  3. Construct candidate coresets and train a proxy model.
  4. Evaluate on Validation set (Optimize for F1).
  5. (Optional) Run Active Sampling to boost performance on hard examples.

- **Design tradeoffs:**
  - **Computational Cost vs. Quality:** The "Tuned" approach requires a grid search over sampling parameters. This is computationally more expensive than "Vanilla" coreset generation but cheaper than training the final large model multiple times.
  - **Loss Approximation vs. Classification:** The paper explicitly trades off strict theoretical loss guarantees (Eq. 2) for empirical classification gains (F1) by altering class distributions.

- **Failure signatures:**
  - **Metric Collapse:** F1 score is 0 or near 0. *Likely cause:* Class allocation is severely skewed, or deterministic threshold excludes all minority class points.
  - **Overfitting to Validation:** Tuned coreset performs amazingly on validation but fails on test. *Likely cause:* The grid search space was too narrow or overfitted to the specific validation split.
  - **Active Sampling Divergence:** Active loop runs indefinitely without improvement. *Likely cause:* The "patience" parameter $\rho$ is set too high, or the uncertainty metric is identifying outliers rather than informative hard examples.

- **First 3 experiments:**
  1. **Sanity Check (Vanilla vs. Full):** Reproduce Table 1/2 results on a small dataset (e.g., A9A). Verify that Vanilla coreset approximates loss but lags in F1.
  2. **Class Allocation Ablation:** Fix deterministic threshold. Run a sweep only on class ratios (e.g., from proportional to 50/50). Plot F1 vs. Class Ratio to verify the "U-curve" of optimal allocation.
  3. **Active Sampling Integration:** Take the best configuration from step 2. Run Algorithm 1. Observe if the F1 score strictly increases or if "patience" is triggered early.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can meta-learning or reinforcement learning strategies automate the selection of optimal coreset parameters?
- Basis in paper: [explicit] The conclusion states that future work could explore these strategies to replace the computationally demanding grid search process currently used.
- Why unresolved: The current framework relies on manual grid search to tune parameters like deterministic ratios and class allocation.
- What evidence would resolve it: A system using meta-learning that selects parameters achieving equivalent or better F1 scores than grid search with reduced computational cost.

### Open Question 2
- Question: Does the class-wise allocation and tuning framework extend effectively to multiclass classification datasets?
- Basis in paper: [explicit] The conclusion identifies extending the approach to multiclass datasets as a promising avenue for further development.
- Why unresolved: The empirical evaluation is restricted to binary classification tasks (e.g., Adult, CreditCard, IEEE).
- What evidence would resolve it: Successful application of the tuning protocol on datasets with >2 classes, showing improvements in macro-averaged F1 scores.

### Open Question 3
- Question: Is there a formal theoretical guarantee that a coreset designed for logistic regression suffices for gradient boosting decision trees (XGBoost)?
- Basis in paper: [inferred] Section 4.3 utilizes a logistic regression coreset for XGBoost based on Proposition 1, which is supported only by empirical validation and intuition rather than formal proof.
- Why unresolved: The paper admits to approaching this cross-model application from a "practical point of view" due to the difficulty of defining point contributions for XGBoost.
- What evidence would resolve it: A mathematical proof demonstrating that the sensitivity bounds for logistic regression provide approximation guarantees for the gradient boosting loss.

## Limitations
- The performance gains heavily depend on the specific implementation of sensitivity calculation methods, which are not fully detailed and rely on external libraries (DataHeroes).
- The "Active Sampling" component is mentioned but lacks details on the uncertainty metric used for sample selection, limiting assessment of its contribution.
- The paper assumes access to a validation set for tuning parameters, which may not always be available in real-world scenarios.

## Confidence
- **High confidence** in the core claim that tuned coresets improve F1 and Balanced Accuracy over vanilla coresets and, in many cases, full-dataset training. This is supported by systematic experiments across multiple datasets and classifiers.
- **Medium confidence** in the specific mechanisms (deterministic sampling, class allocation, active sampling) driving the improvements. While the paper provides theoretical reasoning and empirical evidence, the lack of full algorithmic details for some components limits independent verification.
- **Low confidence** in the generalizability of the active sampling results, as the query strategy is not specified and its contribution is not isolated in the ablation studies.

## Next Checks
1. **Sensitivity Implementation Verification:** Reproduce the sensitivity scores (Unified, Leverage, Lewis) on a small dataset using the described methods and verify they align with expected importance rankings.
2. **Active Sampling Isolation:** Conduct an ablation study on a dataset where active sampling is used, training with and without the active refinement step to quantify its specific contribution to F1 score gains.
3. **Validation-Free Tuning:** Test the tuned coreset approach on a dataset where no separate validation set is available, using techniques like cross-validation or a held-out test set for tuning, to assess robustness to parameter selection.