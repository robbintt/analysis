---
ver: rpa2
title: 'MultiAIGCD: A Comprehensive dataset for AI Generated Code Detection Covering
  Multiple Languages, Models,Prompts, and Scenarios'
arxiv_id: '2507.21693'
source_url: https://arxiv.org/abs/2507.21693
tags:
- code
- python
- problem
- llms
- ai-generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiAIGCD, a comprehensive dataset for AI-generated
  code detection covering three programming languages (Python, Java, Go), six LLMs
  (including reasoning models like OpenAI o3-mini), three prompting approaches, and
  three usage scenarios (code generation, runtime error fixing, and output correction).
  The dataset contains 32,148 human-written and 121,271 AI-generated code snippets
  across 800 programming problems from CodeNet.
---

# MultiAIGCD: A Comprehensive dataset for AI Generated Code Detection Covering Multiple Languages, Models,Prompts, and Scenarios

## Quick Facts
- **arXiv ID:** 2507.21693
- **Source URL:** https://arxiv.org/abs/2507.21693
- **Reference count:** 7
- **Key outcome:** MultiAIGCD dataset contains 32,148 human-written and 121,271 AI-generated code snippets across 800 programming problems from CodeNet, covering three languages, six LLMs, three prompting approaches, and three usage scenarios.

## Executive Summary
This paper introduces MultiAIGCD, a comprehensive dataset for AI-generated code detection covering three programming languages (Python, Java, Go), six LLMs (including reasoning models like OpenAI o3-mini), three prompting approaches, and three usage scenarios (code generation, runtime error fixing, and output correction). Through qualitative analysis, the study reveals distinct coding style differences between human-written and AI-generated code, with human code being longer and containing more comments and functions. Benchmark experiments on three state-of-the-art detection models show that OpenAI's Ada embeddings consistently yield the highest prediction accuracy, with strong performance in detecting code generated from problem definitions but significantly lower accuracy for AI-fixed code samples.

## Method Summary
The dataset was constructed using IBM CodeNet's Python Benchmark subset (800 problems) with up to 5 human code submissions per problem status (accepted, runtime error, wrong answer) in Python, Java, and Go. Six LLMs (Llama-3.3-70B, Qwen2.5-Coder-32B, GPT-4o, Claude 3.5 Sonnet v2, DeepSeek-V3, o3-mini) generated AI code using three prompt types (Lazy, Role, Rephrase & Respond) for three scenarios: from-scratch generation, runtime error fixing, and output correction. Detection models included SVM with text-embedding-ada-002, SVM with CodeT5+ embeddings, and fine-tuned CodeBERTa. The dataset was split 80/10/10 by problem with no leakage, and syntax validation was applied to both human and AI code.

## Key Results
- Detection accuracy drops significantly for AI-fixed code samples (ScenarioRuntime/Output) compared to from-scratch generation (ScenarioScratch)
- OpenAI's Ada embeddings achieve the highest prediction accuracy across all scenarios
- Cross-language detection performance collapses when training on two languages and testing on the third (Go F1 drops to 0.0712)
- Human-written code is longer, contains more comments and functions, while LLM code is structurally more concise

## Why This Works (Mechanism)

### Mechanism 1: Stylistic Separability in Generation-from-Scratch
Code embedding models map code into vector spaces where LLMs consistently cluster together away from human submissions due to distinct stylistic artifacts like blank line frequency, comment density, and structural verbosity. This occurs because LLMs produce fewer outliers in length and structure compared to the high variance of human coding styles.

### Mechanism 2: Signal Dilution via Constrained Editing
When LLMs fix runtime errors or incorrect outputs, the detection signal degrades because the model preserves the existing human-authored structure, limiting the expression of native LLM stylistic priors. The input context constrains the output space, causing the LLM to retain the "human" statistical profile of the original code.

### Mechanism 3: Cross-Language Feature Misalignment
Detection features learned for high-resource languages (Python, Java) fail to transfer to languages with different syntax patterns (Go), suggesting embeddings capture language-specific syntax rather than universal "AI-ness." Models trained on Python/Java learn to detect specific token probabilities or syntactic biases of those languages.

## Foundational Learning

- **Code Stylometry & Embeddings:** Understanding that embeddings capture syntax, variable naming, and structure—not just logic—is key to interpreting results. *Quick check:* How might an embedding vector for a "verbose" human solution differ from a "concise" LLM solution in PCA space?

- **Zero-shot vs. Fine-tuned Detection:** Distinguishing between using a fixed feature extractor and updating model weights is necessary to replicate the baseline. *Quick check:* Why would a fine-tuned model (CodeBERTa) generally outperform a static embedding + SVM in the "Scratch" scenario but struggle similarly in cross-language tests?

- **The "Correction" vs. "Generation" Distribution Shift:** Understanding that "fixing" changes the input-output distribution is crucial for the core negative result. *Quick check:* If an LLM rewrites 90% of a codebase to fix a bug, does it remain a "fixing" scenario or shift back to "generation"?

## Architecture Onboarding

- **Component map:** Source Data (CodeNet) -> Generation Engine (6 LLMs + 3 Prompts) -> Preprocessing (AST parsing + filtering) -> Feature Extraction (CodeBERTa/ADA/CodeT5+) -> Classifier (SVM/CodeBERTa head)

- **Critical path:** The post-processing pipeline is critical; filtering unparsable code is required to prevent the detector from simply learning "LLMs generate broken syntax."

- **Design tradeoffs:** CodeBERTa offers higher peak accuracy but requires GPU fine-tuning, while SVM+Ada is faster/cheaper but less robust. "Role" prompting increases latency and missing responses but may alter the stylistic signature.

- **Failure signatures:** High False Negatives in "Fix" Scenarios (classifying AI-fixed code as human), Go Language Collapse (random guesses for Go when trained only on Python/Java).

- **First 3 experiments:**
  1. Train on Scratch (Python), test on Runtime (Python) to confirm the drop isn't just due to data scarcity but the mechanism of editing.
  2. Replicate the "w/o Llama" experiment to see if the detector relies on shared LLM features or model-specific quirks.
  3. Inspect false positives in the "Fix" scenario to see if the model detects remaining human scaffolding rather than AI edits.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can detection models effectively identify "blended codes" where LLMs are used to generate only a portion of the codebase?
- **Basis:** The Conclusion states plans to cover more usage scenarios such as blended codes.
- **Why unresolved:** The current dataset focuses exclusively on fully AI-generated or AI-fixed snippets, not mixed-authorship scenarios.

### Open Question 2
- **Question:** What are the realistic LLM usage scenarios and prompting strategies employed by students and developers in practice?
- **Basis:** The Conclusion notes plans to conduct user studies across students and developers.
- **Why unresolved:** The study uses researcher-defined prompts (Lazy, Role, Rephrase) which may not reflect actual user prompts in educational or industrial settings.

### Open Question 3
- **Question:** How can the significant performance gap in detecting AI-fixed code versus code generated from scratch be reduced?
- **Basis:** While the paper reports high F1 scores for code generated from scratch, it notes significant accuracy drops for AI-fixed code without proposing a method to close this gap.
- **Why unresolved:** Current detection models struggle to distinguish the AI's stylistic fingerprint when it modifies existing human-written logic.

### Open Question 4
- **Question:** How robust are other state-of-the-art detection models against the cross-language and cross-model scenarios presented in MultiAIGCD?
- **Basis:** The Limitations section states that assessing other models will be an important extension.
- **Why unresolved:** The study only benchmarks SVMAda, SVMT5+, and CodeBERTa, leaving the performance of the broader landscape unknown.

## Limitations
- Detection accuracy significantly degrades in correction scenarios, suggesting the mechanism relies heavily on code generation rather than minimal editing patterns.
- Cross-language performance collapse indicates the "AI signature" may be language-dependent rather than universal.
- The paper doesn't explore whether models can adapt to mimic human coding styles or whether defensive prompting strategies could evade detection.

## Confidence
- **High Confidence:** The stylistic differences between human and AI-generated code are well-established. The methodology for dataset construction and benchmark results are reproducible.
- **Medium Confidence:** The proposed mechanisms are logically sound but require further empirical validation across different model families and coding paradigms.
- **Low Confidence:** The generalizability of detection across all programming scenarios remains uncertain, particularly for collaborative coding where human and AI contributions are mixed.

## Next Checks
1. **Boundary Condition Test:** Systematically vary the percentage of code rewritten during "fixing" scenarios (10%, 50%, 90%) to determine the threshold where detection performance shifts from "fixing" to "generation" classification.
2. **Style Transfer Evaluation:** Fine-tune LLMs with human-style prompts and measure detection accuracy to quantify how much stylistic mimicry degrades detection performance.
3. **Cross-Embedding Generalization:** Train detection models on one embedding type (e.g., CodeBERTa) and test on another (e.g., ADA) to assess whether the "AI signature" transfers across different code representation spaces.