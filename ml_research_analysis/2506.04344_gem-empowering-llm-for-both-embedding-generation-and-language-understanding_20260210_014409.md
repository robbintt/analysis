---
ver: rpa2
title: 'GEM: Empowering LLM for both Embedding Generation and Language Understanding'
arxiv_id: '2506.04344'
source_url: https://arxiv.org/abs/2506.04344
tags:
- text
- special
- embedding
- language
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GEM, a method that enables large decoder-only
  language models (LLMs) to generate high-quality text embeddings while preserving
  their original generation and reasoning capabilities. The approach inserts special
  tokens into input text and manipulates attention masks to encourage the model to
  compress information into these tokens.
---

# GEM: Empowering LLM for both Embedding Generation and Language Understanding

## Quick Facts
- **arXiv ID**: 2506.04344
- **Source URL**: https://arxiv.org/abs/2506.04344
- **Reference count**: 10
- **Primary result**: GEM enables LLMs to generate high-quality embeddings while preserving generation capabilities with minimal training data

## Executive Summary
GEM introduces a method that enables large decoder-only language models to generate high-quality text embeddings while maintaining their original text generation and reasoning capabilities. The approach works by inserting special tokens into input text and manipulating attention masks to encourage information compression into these tokens. Applied to Llama and Mistral models (1B-8B parameters), GEM significantly improves performance on the Massive Text Embedding Benchmark while adding minimal computational overhead and preserving language understanding capabilities.

## Method Summary
GEM works by inserting special tokens into input text and manipulating attention masks to encourage the model to compress information into these tokens. The method uses mixed next-token prediction loss with optional special tokens and contrastive learning to improve embedding quality. The approach is designed to be computationally efficient, requiring as little as 32,000 training rows compared to larger datasets needed by alternative methods. The technique is applied to decoder-only models like Llama and Mistral across different parameter sizes (1B-8B).

## Key Results
- Significantly improves performance on Massive Text Embedding Benchmark (MTEB) across multiple model sizes
- Maintains original generation and reasoning capabilities with only +0.3% impact on MMLU
- Requires minimal training data (32,000 rows) compared to alternative approaches
- Successfully applied to both Llama and Mistral models (1B-8B parameters)

## Why This Works (Mechanism)
GEM works by leveraging the inherent sequence modeling capabilities of decoder-only LLMs and redirecting them toward embedding generation. The key insight is that by inserting special tokens and manipulating attention masks, the model is encouraged to compress semantic information into these tokens during the generation process. This allows the model to produce meaningful embeddings while maintaining its ability to perform next-token prediction for text generation. The mixed loss function balances between traditional language modeling and embedding-specific objectives, while contrastive learning further refines the embedding space.

## Foundational Learning
- **Attention mask manipulation**: Needed to control information flow and encourage compression into special tokens; quick check: verify attention weights concentrate on special tokens during inference
- **Special token insertion**: Required to create dedicated embedding generation points within the sequence; quick check: confirm special tokens appear at consistent positions across inputs
- **Contrastive learning**: Used to refine embedding quality by pulling similar items together and pushing dissimilar items apart; quick check: measure embedding space clustering quality
- **Mixed loss functions**: Balances traditional language modeling with embedding-specific objectives; quick check: monitor both next-token prediction accuracy and embedding similarity metrics
- **Decoder-only architecture constraints**: Understanding how these models differ from encoder-decoder architectures in handling bidirectional context; quick check: compare attention patterns with encoder-based models
- **Embedding quality metrics**: Spearman correlation and other benchmarks for evaluating embedding effectiveness; quick check: validate against multiple embedding evaluation datasets

## Architecture Onboarding

**Component Map**: Input Text -> Special Token Insertion -> Attention Mask Manipulation -> Mixed Loss Training -> Embedding Generation

**Critical Path**: The critical path involves the special token insertion and attention mask manipulation, which directly enable the model to generate meaningful embeddings while maintaining generation capabilities. This is followed by the mixed loss training that balances embedding quality with generation performance.

**Design Tradeoffs**: The approach trades off some model capacity for dual functionality, but the experimental results suggest this tradeoff is minimal. The use of special tokens adds slight overhead to generation but enables embedding capabilities. The small training dataset requirement is a significant advantage over alternatives that need much larger corpora.

**Failure Signatures**: 
- If attention masks are not properly configured, the model may fail to compress information effectively into special tokens
- Insufficient training data may lead to poor embedding quality or overfitting
- Incorrect loss weighting could degrade either generation or embedding performance
- Special token placement issues could result in inconsistent embeddings across different input lengths

**3 First Experiments**:
1. Test embedding quality with varying numbers of special tokens (1, 2, 4) to find optimal configuration
2. Compare attention patterns with and without mask manipulation to verify information compression
3. Evaluate embedding stability across different input lengths and domains

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to English-only datasets, limiting generalizability to multilingual contexts
- Absolute embedding quality metrics not provided, making practical utility difficult to assess
- Small training dataset (32,000 examples) raises questions about scalability and potential overfitting
- MMLU-based evaluation of language understanding preservation may miss subtle degradation in reasoning capabilities

## Confidence
- **High confidence**: Core technical contribution of enabling embedding generation while maintaining generation capabilities is well-supported by experimental results
- **Medium confidence**: Computational efficiency claims are supported but limited by comparison to only one baseline
- **Medium confidence**: Preservation of language understanding capabilities demonstrated on MMLU but may not generalize to other reasoning benchmarks

## Next Checks
1. **Multilingual Evaluation**: Test GEM's performance on multilingual benchmarks to assess cross-lingual embedding quality and determine if attention mask manipulation generalizes across languages

2. **Long-form Generation Analysis**: Evaluate impact on extended text generation tasks to identify potential degradation in generation quality not apparent in standard MMLU evaluations

3. **Scaling Behavior**: Conduct experiments with varying training set sizes (10K, 100K, 1M examples) to characterize the relationship between training data volume and embedding quality