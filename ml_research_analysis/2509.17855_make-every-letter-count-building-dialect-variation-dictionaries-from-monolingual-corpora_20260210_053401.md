---
ver: rpa2
title: 'Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual
  Corpora'
arxiv_id: '2509.17855'
source_url: https://arxiv.org/abs/2509.17855
tags:
- term
- german
- inflected
- translation
- bavarian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces DIALEMMA, a novel framework for creating
  dialect variation dictionaries from monolingual corpora without requiring parallel
  data. Using Bavarian Wikipedia as a case study, the authors extract and manually
  annotate 100K German-Bavarian word pairs, forming a ground truth dataset of 11K
  direct translations and 7K inflected variants.
---

# Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual Corpora

## Quick Facts
- arXiv ID: 2509.17855
- Source URL: https://arxiv.org/abs/2509.17855
- Reference count: 40
- Primary result: Introduced DIALEMMA framework extracting 100K German-Bavarian word pairs from monolingual corpora; evaluated 9 LLMs showing scale predicts dialect judgment/translation capability

## Executive Summary
This study introduces DIALEMMA, a novel framework for creating dialect variation dictionaries from monolingual corpora without requiring parallel data. Using Bavarian Wikipedia as a case study, the authors extract and manually annotate 100K German-Bavarian word pairs, forming a ground truth dataset of 11K direct translations and 7K inflected variants. They evaluate nine state-of-the-art LLMs on two tasks: judging whether Bavarian terms are translations, inflected variants, or unrelated to German lemmas (macro-F1 scores 0.25-0.57), and translating Bavarian terms to German (accuracy 0.06-0.59). Results show larger models outperform smaller ones, with Mistral-123b achieving the best judgment performance (macro-F1 0.567) and Gemma3-27b the best translation accuracy (0.586).

## Method Summary
The DIALEMMA pipeline extracts dialect-standard word pairs from monolingual corpora through k-nearest neighbor filtering based on Levenshtein distance. German lemmas are extracted and POS-tagged using spaCy, then compared against Bavarian vocabulary tokens. For each German lemma, the k=10 Bavarian candidates with lowest edit distance are retrieved, breaking ties randomly. Human annotators classify these pairs as translations ("yes"), inflected variants ("inflected"), or unrelated ("no"). The resulting dataset is used to evaluate LLMs on judgment (3-way classification) and translation (dialect→standard) tasks, with experiments ablating context and prompt language.

## Key Results
- Larger LLMs consistently outperform smaller ones on both dialect judgment (Mistral-123b: macro-F1 0.567) and translation (Gemma3-27b: accuracy 0.586)
- Context improves translation accuracy but reduces variant recognition performance
- English prompts generally outperform German prompts for most models
- Models struggle most with distinguishing translations from inflected variants and perform worse on lexically distant word pairs
- Instruction-following errors vary significantly across models (0.1% for Gemma3-27b judgment vs 76.4% for Mistral-7b translation)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexical similarity filtering reduces dialect-lemma matching from intractable to tractable without parallel corpora.
- Mechanism: DIALEMMA exploits high cognate overlap between related varieties. By computing Levenshtein distance between standard lemmas and dialect vocabulary, then retaining only the k nearest neighbors per lemma, the annotation search space contracts from O(|V₁| × |V₂|) to O(k × |V₁|). Human annotators then verify the reduced candidate set.
- Core assumption: Dialect-standard translation pairs exhibit lower edit distances than unrelated word pairs on average.
- Evidence anchors: Abstract states 100K German-Bavarian word pairs were extracted; section 3.1 describes k-NN extraction based on Levenshtein distance.

### Mechanism 2
- Claim: Model scale predicts dialect judgment and translation capability, with emergent distinction between translation vs. inflected variant classes.
- Mechanism: Larger LLMs within the same family show higher macro-F1 and accuracy. Confusion matrices reveal smaller models default to predicting "no" class, while larger models shift probability mass toward "yes" and "inflected," suggesting scale-dependent morphological awareness.
- Core assumption: Scale improves internal representations of morphological and orthographic variation patterns.
- Evidence anchors: Abstract reports Mistral-123b best judgment (macro-F1 0.567) and Gemma3-27b best translation accuracy (0.586); section 5.1 confusion matrices show probability mass shifts.

### Mechanism 3
- Claim: Context provides disambiguating signal for translation but introduces noise for variant classification.
- Mechanism: Usage examples (±50 characters around dialect term) improve translation accuracy but reduce judgment macro-F1. Translation benefits from semantic context; variant detection suffers, possibly because context introduces competing hypotheses or distracts from orthographic focus.
- Core assumption: Translation relies on semantic/contextual cues; variant detection requires focused orthographic comparison.
- Evidence anchors: Abstract states "Providing context improves translation but reduces variant recognition"; section 5.3 Figure 3 shows positive Δ for translation, negative Δ for judgment.

## Foundational Learning

- Concept: **Levenshtein distance and string similarity metrics**
  - Why needed here: Core to DIALEMMA's candidate extraction; models perform worse at higher LD (Figure 5 shows accuracy drops from ~0.5 at LD=1 to ~0.3 at LD=8).
  - Quick check question: Given German "zweisprachig" and Bavarian "zwaasprochig" (LD=3), what's the normalized edit distance? Would this pair likely pass a k=10 nearest-neighbor filter?

- Concept: **Dialect orthographic variation vs. lexical replacement**
  - Why needed here: DIALEMMA targets cognate spelling variation, not lexical divergence (e.g., regional word choices). This scopes the problem and defines failure modes.
  - Quick check question: If Bavarian uses "Marmelade" where German uses "Konfitüre," would DIALEMMA capture this pair? Why or why not?

- Concept: **Instruction-following error (IF Error) in LLM evaluation**
  - Why needed here: Models like Llama4-17b show 43.8% IF error on judgment and 76.4% (Mistral-7b) on translation, conflating task performance with instruction adherence.
  - Quick check question: If a model outputs "Yes, this is a translation" instead of just "yes," is this an IF error? How would you handle this in evaluation?

## Architecture Onboarding

- Component map:
  1. **Lemmatizer/POS tagger** (spaCy de_core_news_lg) → standardizes L1 (German) vocabulary to lemmas with POS_max
  2. **Vocabulary extractor** → V^L1 (lemmas), V^L2 (all unique dialect tokens, no frequency filtering)
  3. **Shared token filter** → removes tokens appearing in both V^L1 and V^L2
  4. **k-NN extractor** → for each lemma, retrieves k=10 dialect candidates by LD; breaks ties randomly
  5. **Context retriever** → extracts c=3 usage examples (±50 characters) per candidate
  6. **Human annotator interface** → presents lemma, POS_max, candidates, contexts; collects "yes"/"inflected"/"no" labels
  7. **LLM evaluator** → tests judgment (3-way classification) and translation (dialect→standard)

- Critical path: Lemmatization quality → vocabulary extraction → LD-based k-NN → annotation quality. If lemmatization mis-tags (e.g., adjectives tagged as adverbs, noted in Limitations), candidates will be mismatched.

- Design tradeoffs:
  - k=10 balances recall (capturing variants) vs. annotation burden; lower k reduces workload but may miss true variants.
  - No frequency filtering on V^L2 preserves rare spellings but increases noise.
  - English prompts outperform German prompts; trade-off between task-language alignment and model-language alignment.

- Failure signatures:
  - **High IF Error with low performance** (Mistral-7b translation: 76.4% IF error, 0.058 accuracy) → model doesn't understand task format; try prompt simplification or few-shot examples.
  - **Low IF Error with low performance** (Gemma3-27b judgment: 0.1% IF error, 0.418 macro-F1) → model follows instructions but lacks dialect knowledge; consider fine-tuning or retrieval augmentation.
  - **Performance degrades with LD** → expected; monitor LD distribution in your target data.

- First 3 experiments:
  1. **Baseline replication**: Apply DIALEMMA to a different dialect-standard pair (e.g., Alemannic-German Wikipedia) using identical parameters (k=10, c=3). Compare annotation yield and LD distributions.
  2. **Prompt ablation**: Test the best-performing model (Mistral-123b for judgment, Gemma3-27b for translation) with the full prompt pool (22 judgment, 21 translation prompts) on a held-out split. Report variance in macro-F1 and accuracy.
  3. **Context window sensitivity**: Vary context length (0, 25, 50, 100 characters) for translation task only. Plot accuracy vs. context length to identify the optimal window before noise dominates.

## Open Questions the Paper Calls Out

- Can fine-tuning LLMs substantially improve their ability to identify dialect–standard translation pairs and perform dialect-to-standard translation?
  - Basis: The conclusion states plans to fine-tune LLMs for identifying translation pairs and as translators.
  - Why unresolved: This study only evaluates zero-shot performance of instruction-tuned LLMs;