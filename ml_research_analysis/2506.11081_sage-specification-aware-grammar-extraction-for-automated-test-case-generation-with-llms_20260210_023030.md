---
ver: rpa2
title: SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation
  with LLMs
arxiv_id: '2506.11081'
source_url: https://arxiv.org/abs/2506.11081
tags:
- test
- generation
- grammar
- cases
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of automated test case generation
  for competitive programming, where generating valid and general grammars from natural
  language specifications remains challenging, especially under limited supervision.
  The proposed SAGE framework addresses this by first fine-tuning an open-source LLM
  (DeepSeek-R1-Distill-Qwen-14B) with supervised learning to perform specification-to-grammar
  translation, then applying Group Relative Policy Optimization (GRPO) to enhance
  grammar validity and generality using verifiable reward signals.
---

# SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation with LLMs

## Quick Facts
- **arXiv ID**: 2506.11081
- **Source URL**: https://arxiv.org/abs/2506.11081
- **Reference count**: 40
- **Primary result**: Achieves 96.66% set-based validity, 95.92% set-based generality, and 80.67% set-based effectiveness, improving over prior work by 15.92 percentage points in grammar validity and 12.34 percentage points in test effectiveness

## Executive Summary
SAGE tackles the challenge of automated test case generation for competitive programming by translating natural language specifications into Context-Free Grammars with Counters (CCFGs). The framework combines supervised fine-tuning of an open-source LLM with Group Relative Policy Optimization (GRPO) to enhance grammar validity and generality. By leveraging verifiable reward signals based on test case generation from the produced grammars, SAGE achieves state-of-the-art performance, outperforming 17 open and closed-source LLMs. The approach demonstrates significant improvements in grammar quality and test effectiveness through its two-phase training methodology.

## Method Summary
SAGE first fine-tunes DeepSeek-R1-Distill-Qwen-14B on 960 specification-CCFG pairs using cross-entropy loss, then applies GRPO with rewards computed from element-based validity and generality (measured by sampling k=5 test cases). The framework uses prompts with role instructions, few-shot examples, and Chain-of-Thought reasoning. For non-fine-tuned models, iterative feedback up to 5 turns corrects syntactic and semantic errors. The complete pipeline includes specification processing, SFT+GRPO-tuned model inference, CCFG validation, and Las Vegas constrained sampling for test generation.

## Key Results
- Achieves 96.66% set-based validity, 95.92% set-based generality, and 80.67% set-based effectiveness
- Outperforms 17 open and closed-source LLMs by 15.92 percentage points in grammar validity
- Improves test effectiveness by 12.34 percentage points over prior work
- GRPO fine-tuning provides noticeable gains, especially in generality (96.75%→94.34% element-based)

## Why This Works (Mechanism)

### Mechanism 1: Supervised Fine-Tuning for Specification-to-Grammar Translation
Fine-tuning an LLM on specification-to-CCFG pairs enables it to learn grammar generation patterns that generalize to unseen specifications. Cross-entropy loss training on prompt-response pairs with role instructions, few-shot examples, and CoT reasoning allows the model to learn the translation task. The approach assumes the model can generalize from limited supervision (~960 pairs) to diverse competitive programming specifications.

### Mechanism 2: GRPO with Verifiable Validity and Generality Rewards
Reinforcement learning with verifiable reward signals improves grammar quality beyond SFT alone. GRPO extends PPO by computing group-relative advantages across structurally similar problem groups, using rewards R = RV × RG that multiply element-based validity (ratio of valid test cases) by element-based generality (ratio of ground-truth test cases the grammar can parse).

### Mechanism 3: Iterative Feedback for Error Correction
Multi-turn feedback enables progressive correction of syntactic and semantic grammar errors for models not already fine-tuned. A validator identifies error categories (null grammar, unbracketed counter variables, missing variable references, node overflow, invalid non-terminals) which are fed back as corrective prompts for regeneration, up to 5 iterations.

## Foundational Learning

- **Concept**: Context-Free Grammars with Counters (CCFGs)
  - Why needed here: CCFGs extend CFGs by storing and reusing counter values during derivation, enabling grammars to express logical/numerical constraints implicit in specifications.
  - Quick check question: Given production `<T_i> → <T_{i-1}> <s> a_i` with `<T_1> → a_1`, how does the counter `i` control the number of space-separated values generated?

- **Concept**: Validity vs. Generality Trade-off
  - Why needed here: These are the dual objectives that guide RL rewards and evaluation. Validity = generated test cases conform to specification. Generality = grammar can produce all valid test cases.
  - Quick check question: If a grammar generates only valid test cases but cannot produce edge cases like minimum/maximum constraint values, which metric suffers?

- **Concept**: Group Relative Policy Optimization (GRPO)
  - Why needed here: GRPO extends PPO by normalizing advantages within groups of structurally similar tasks, encouraging generalization across problem classes.
  - Quick check question: How does GRPO's group-relative advantage Â⁽ᵍ⁾ₜ = Âₜ − E[t′∈g][Âₜ′] differ from standard PPO advantage estimation, and why might this help grammar generalization?

## Architecture Onboarding

- **Component map**: Natural language specification → Prompt construction → SFT fine-tuning → GRPO training → CCFG output → Validator → Test generation → (optional) Iterative feedback loop

- **Critical path**:
  1. Construct prompt with specification + few-shot examples + CoT instructions
  2. Generate grammar via SFT+GRPO-tuned model
  3. Validate well-formedness (malformed → reward=0, or trigger feedback loop)
  4. Sample k=5 test cases to compute validity and generality rewards
  5. Generate test cases via constrained sampling with backtracking

- **Design tradeoffs**:
  - Open-source (14B) vs. closed-source LLMs: Open-source enables SFT+RL but closed-source models benefit more from iterative feedback
  - Prompt components: Few-shot examples > rules > CoT for impact; fine-tuned models can omit explicit rules (learned implicitly)
  - k=5 samples for reward: Computational efficiency vs. reward signal reliability
  - 5-iteration feedback cap: Diminishing returns for already-well-performing models

- **Failure signatures**:
  - Node overflow error: Production generates values exceeding constraints (e.g., `[0-9][0-9][0-9]` when 1≤n≤10⁹)
  - Counter parse errors: Invalid regex patterns (`[0-9]+`) or unsupported symbolic expressions (`{n-1}`)
  - Numeric literal misinterpretation: `10⁵` parsed as `105` rather than `100000`
  - Validity stagnation across iterations: Model cannot resolve fundamental well-formedness issues

- **First 3 experiments**:
  1. Prompt ablation study: Test 1-shot vs. 5-shot with/without rules/CoT to isolate prompting contribution
  2. Component ablation: Compare base model → +SFT → +GRPO progression to quantify each component's contribution
  3. Error taxonomy analysis: Run inference on held-out specifications, categorize failures by error type, identify highest-impact failure modes

## Open Questions the Paper Calls Out

- **Can incorporating buggy reference implementations improve the targeting of generated test cases toward exposing faults?**
  - Basis: Authors state plans to "incorporate incorrect or buggy reference code to guide the generation of targeted test cases"
  - Why unresolved: Current SAGE generates grammars solely from specifications without leveraging known buggy code patterns
  - What evidence would resolve it: Experiments comparing fault detection rate between specification-only SAGE and variant conditioning on buggy code

- **Can effectiveness-guided reinforcement learning produce high-quality grammars without labeled specification-grammar pairs?**
  - Basis: Authors mention plans to "explore effectiveness-guided RL even with unlabeled specifications"
  - Why unresolved: Current GRPO relies on ground-truth grammars to compute validity and generality rewards
  - What evidence would resolve it: Demonstrating RL-only variant achieves comparable metrics using only test execution outcomes as rewards

- **Would more expressive grammar formalisms beyond CCFGs enable handling of more complex constraint types?**
  - Basis: Future work includes plans to "extend SAGE to more expressive grammar representations"
  - Why unresolved: Case studies reveal failure patterns with symbolic counter expressions and misinterpretation of exponential notation
  - What evidence would resolve it: Comparative experiments showing improved handling of complex constraints using extended formalisms

## Limitations
- Heavy dependence on provided CodeContests dataset with 1,200 annotated specifications, creating barrier to generalization
- GRPO rewards rely on sampling only k=5 test cases, which may provide noisy signals for complex grammars
- CCFG formalism introduces complexity in parsing and validation that may not generalize to specifications requiring richer constraint representations

## Confidence
- **High confidence**: SFT fine-tuning on specification-to-grammar pairs improves grammar validity (proven by ablation showing base → SFT gain)
- **Medium confidence**: GRPO with verifiable rewards improves generality (ablation shows benefit, but reward sampling at k=5 introduces uncertainty)
- **Medium confidence**: Iterative feedback corrects syntactic errors for non-fine-tuned models (demonstrated on 271 benchmarks, but model-dependent)
- **Low confidence**: Generalization to specifications requiring constraints beyond CCFGs (no evidence provided)

## Next Checks
1. **Reward signal reliability test**: Vary k from 5 to 20 test samples and measure stability of validity/generality rewards during GRPO training to quantify noise in RL signals
2. **Out-of-distribution robustness**: Apply SAGE to specifications from different competitive programming domains to test generalization beyond the CodeContests distribution
3. **Constraint complexity scaling**: Systematically evaluate grammar performance on specifications with increasingly complex constraints to identify the CCFG formalism's limits