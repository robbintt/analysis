---
ver: rpa2
title: 'CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large
  Language Models'
arxiv_id: '2509.15027'
source_url: https://arxiv.org/abs/2509.15027
tags:
- texts
- argument
- text
- computational
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CLEAR, a comprehensive linguistic evaluation\
  \ pipeline for analyzing LLM-based text rewriting, specifically focusing on argument\
  \ improvement. The authors develop 57 metrics mapped to four linguistic levels\u2014\
  lexical, syntactic, semantic, and pragmatic\u2014and apply them across five argumentation\
  \ corpora using six different LLMs and five prompting techniques."
---

# CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models

## Quick Facts
- arXiv ID: 2509.15027
- Source URL: https://arxiv.org/abs/2509.15027
- Reference count: 24
- This paper introduces CLEAR, a comprehensive linguistic evaluation pipeline for analyzing LLM-based text rewriting, specifically focusing on argument improvement.

## Executive Summary
This paper introduces CLEAR, a comprehensive linguistic evaluation pipeline for analyzing LLM-based text rewriting, specifically focusing on argument improvement. The authors develop 57 metrics mapped to four linguistic levels—lexical, syntactic, semantic, and pragmatic—and apply them across five argumentation corpora using six different LLMs and five prompting techniques. The key findings reveal that LLMs tend to improve argumentative texts by shortening them while simultaneously increasing average word length and merging sentences. The analysis shows an overall increase in persuasion and coherence dimensions, with models refining existing text structure rather than making significant semantic changes. The study also investigates length and positivity biases, finding that models generally make texts more neutral in tone and show no strong preference for specific text lengths.

## Method Summary
The CLEAR pipeline evaluates argument improvement by processing original and LLM-rewritten texts through 57 linguistic metrics across four levels. The method uses five corpora (Essays, Microtexts EN/DE, ArgRewrite V.2), six LLMs (bloomz-560m, bloomz-3b, Phi-3-mini/medium-4k-instruct, OLMo-7B-0724-Instruct, Llama-3.1-Nemotron-70B-Instruct), and five prompting techniques. Metrics are computed using tools including LexicalRichness, LinguaF, spaCy dependency parsing, BERTAlign, Feng-Hirst RST parser, GRUEN, and sentiment classifiers. The pipeline measures changes across lexical (TTR, word length, readability), syntactic (dependency tags, transformations), semantic (GRUEN, RST depth), and pragmatic (coherence, persuasion) dimensions, with correlation analysis for bias detection.

## Key Results
- LLMs improve arguments by shortening texts (≈4.66% to 37.39% decrease) while increasing average word length and merging sentences
- Improved texts show increased persuasion and coherence scores despite decreased RST tree depth (structural simplification)
- Models neutralize sentiment, shifting polarity toward neutral regardless of original sentiment
- Merge operations dominate transformations (strongest correlation ≈0.64 with original length)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs improve arguments primarily through condensation—reducing text length while increasing information density via longer words and sentence merging.
- Mechanism: When prompted for improvement, models perform merge operations (n:1 sentence transformations) and increase 4–6 syllable word frequency while decreasing shorter words. This raises token-to-type ratio and compresses content without expanding it.
- Core assumption: Higher information density and consolidated structure correlate with improved argumentative quality as measured by persuasion and coherence metrics.
- Evidence anchors:
  - [abstract] "the models perform ArgImp by shortening the texts while simultaneously increasing average word length and merging sentences"
  - [section 5.1] "On the lexical level we note that models, on average, decreased text length (≈4.66% to 37.39% decrease)"
  - [section 5.3] "merge has a strong correlation (≈0.64) as well as delete (≈0.27)"
  - [corpus] Weak direct corpus support—neighbor papers focus on rewriting but not specifically on condensation mechanisms.
- Break condition: If input texts are already highly compressed (e.g., Microtexts corpus), this mechanism reverses—models expand instead (≈40.18% length increase).

### Mechanism 2
- Claim: LLMs simplify rhetorical structure while preserving or enhancing persuasive effectiveness.
- Mechanism: Models reduce Rhetorical Structure Theory (RST) parse tree depth, indicating simpler discourse organization. Despite structural simplification, pragmatic scores (coherence, persuasion) increase, suggesting that readability improvements outweigh complexity loss.
- Core assumption: Shallow rhetorical trees correspond to easier comprehension without sacrificing argumentative strength.
- Evidence anchors:
  - [section 5.1] "LLMs consistently decrease the depth of the RST parse tree... A shallow RST tree indicates that the texts are less complex and easier to understand"
  - [section 5.1] "On the pragmatic level we note an increase for both persuasiveness and coherence for all models on all datasets"
  - [corpus] Neighbor paper "Dr Genre" addresses text rewriting but does not evaluate rhetorical structure specifically.
- Break condition: For very short inputs (Microtexts), RST depth increases instead—models elaborate structure when input is under-developed.

### Mechanism 3
- Claim: LLMs neutralize sentiment rather than exhibiting positivity bias in argument improvement.
- Mechanism: Models shift polarity toward neutral regardless of initial sentiment. Original texts averaged +13.18% polarity; improved texts averaged +11.39%. Median shift was -14.59% (toward negative/neutral).
- Core assumption: Neutral tone is perceived as more professional or credible in argumentative contexts.
- Evidence anchors:
  - [section 5.4] "Overall, the model tends to move the improved texts towards a more neutral tone"
  - [section 5.1] "All models perform similarly in terms of sentiment changes... for all English texts the polarity decreases"
  - [corpus] Neighbor "A Comprehensive Analysis of LLM Outputs" mentions bias but does not address argument-specific neutrality.
- Break condition: For German Microtexts, sentiment shifted strongly positive—language-specific or corpus-specific behavior may override neutrality tendency.

## Foundational Learning

- Concept: **Four Linguistic Levels Framework**
  - Why needed here: The CLEAR pipeline maps 57 metrics to lexical, syntactic, semantic, and pragmatic levels. Understanding this taxonomy is prerequisite to interpreting any evaluation output.
  - Quick check question: Given a metric measuring "number of coordinating noun phrases," which linguistic level does it belong to?

- Concept: **Rhetorical Structure Theory (RST) Parsing**
  - Why needed here: RST tree depth serves as the primary semantic complexity indicator. Deeper trees = more complex discourse relations.
  - Quick check question: If an improved argument has RST depth change of -21.17%, did its discourse structure become more or less complex?

- Concept: **BERTAlign Sentence Transformation Types**
  - Why needed here: The pipeline categorizes changes as merge, split, copy, delete, fusion, or add. "Merge" dominated across datasets, indicating condensation strategy.
  - Quick check question: What transformation type has the strongest correlation with original text length, and what does this imply about model behavior on longer inputs?

## Architecture Onboarding

- Component map:
  Input corpora (Essays, Microtexts EN/DE, ArgRewrite V.2) → 6 LLMs × 5 prompting techniques → CLEAR pipeline (57 metrics) → 4 linguistic levels + bias analysis + argument component classification

- Critical path:
  1. Prompt model with argument + improvement instruction
  2. Extract improved text (wrapped in @ symbols)
  3. Run all 57 metrics on original vs. improved
  4. Map metrics to linguistic levels
  5. Compute delta scores and correlations
  6. Optional: Manual preference analysis (2 blinded reviewers)

- Design tradeoffs:
  - RST parser choice: Feng & Hirst (2014) chosen over Maekawa et al. (2024) due to lower computational cost despite marginal accuracy difference (58.2 vs 60.0 F1)
  - Argument component classifier: Stab & Gurevych (2014) used for accessibility despite newer approaches existing—accuracy 0.77
  - Manual evaluation limited to small subset due to scale (34,200 combinations)

- Failure signatures:
  - Very small models (bloomz-560m, bloomz-3b) produced "barely legible texts"—omit from analysis
  - Hallucinated references when original text contained citations (no RAG in setup)
  - Logical flaws in original arguments sometimes preserved without elaboration or removal

- First 3 experiments:
  1. Replicate lexical-level analysis on new argument corpus: Measure length change, word length distribution, and token-to-type ratio delta using LexicalRichness library.
  2. Test neutrality hypothesis on domain-specific corpus (e.g., legal arguments): Run sentiment analysis on original vs. improved texts to verify if neutrality tendency holds across domains.
  3. Ablate prompting technique: Compare 3-shot vs. Branch-Solve-Merge on same model/dataset to isolate prompt effect on merge vs. delete operation frequency.

## Open Questions the Paper Calls Out

- **Question**: How do LLM-rewritten arguments perform in terms of extrinsic, reader-focused effectiveness compared to the original texts?
  - Basis in paper: [explicit] The authors explicitly state in the Limitations section that they "do not incorporate user studies to evaluate the perceived impact of the improvements."
  - Why unresolved: The current study focuses solely on intrinsic, text-centric linguistic qualities, leaving the actual persuasive impact on human readers untested.
  - What evidence would resolve it: Results from human subject studies measuring persuasion rates and perceived quality of LLM-rewritten arguments versus originals.

- **Question**: Can LLMs be guided to detect and correct logical fallacies or factually weak premises during the argument improvement process?
  - Basis in paper: [inferred] The manual analysis revealed that "Llama 3.1 does not appear to check for logical quality of arguments" and failed to remove illogical points (e.g., GPS tracking causing cars to drive down stairs).
  - Why unresolved: The study found models refine structure and style but lack the capacity to evaluate or improve the logical soundness or dialectical reasonableness of the content.
  - What evidence would resolve it: An evaluation of LLM performance on an ArgImp task where the input texts contain specific, annotated logical fallacies.

- **Question**: How can evaluation pipelines like CLEAR be effectively expanded to assess higher-order argumentative traits such as prompt adherence and content organization?
  - Basis in paper: [explicit] The authors note that "Higher-order traits such as prompt adherence, content and overall organization require a more complex evaluation... which is beyond the scope of this work."
  - Why unresolved: Current metrics primarily capture linguistic structure and surface-level coherence, failing to quantify if the argument addresses the correct topic or organizes ideas effectively.
  - What evidence would resolve it: The integration and validation of new automated metrics specifically designed to score content relevance and macro-structure into the CLEAR pipeline.

## Limitations
- The study relies heavily on automatic metrics without extensive manual validation, with manual assessment limited to a small subset of 20 samples from 3 essays with only 2 blinded reviewers.
- The pipeline does not systematically address hallucination risks, noting that models sometimes add factually incorrect information or hallucinate references when original texts contained citations.
- The focus on English and German texts limits generalizability to other languages.

## Confidence

**High Confidence**: The observation that LLMs consistently shorten texts while increasing word length and performing merge operations is well-supported by corpus-wide metrics across multiple datasets and models. The neutrality bias finding (texts shifting toward neutral sentiment) shows consistent patterns across English texts with clear numerical evidence.

**Medium Confidence**: The claim that structural simplification (decreased RST depth) correlates with improved persuasion and coherence is supported but requires careful interpretation—the causal relationship between simpler discourse structure and argumentative quality isn't fully established. The assertion that LLMs "refine existing structure rather than making significant semantic changes" is based on delta comparisons but doesn't rule out subtle semantic shifts.

**Low Confidence**: The specific mechanism explanations (e.g., why models prefer merge over delete operations, or why neutrality is preferred) remain speculative. The study identifies patterns but doesn't test underlying motivations for model behavior.

## Next Checks

1. **Manual Validation Expansion**: Replicate the manual preference analysis on a larger sample (e.g., 100 samples across diverse argument types) with multiple reviewers to establish inter-rater reliability and validate automatic metric correlations.

2. **Hallucination Risk Assessment**: Design a systematic evaluation of factual accuracy preservation by embedding verifiable claims in argumentative texts and measuring hallucination rates across different model sizes and prompting techniques.

3. **Cross-Domain Generalization Test**: Apply the CLEAR pipeline to non-argumentative text types (e.g., technical documentation, narrative prose) to determine if the observed condensation and neutrality patterns are specific to argumentative writing or represent general LLM behavior.