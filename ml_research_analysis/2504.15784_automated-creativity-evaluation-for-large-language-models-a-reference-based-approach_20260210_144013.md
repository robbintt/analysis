---
ver: rpa2
title: 'Automated Creativity Evaluation for Large Language Models: A Reference-Based
  Approach'
arxiv_id: '2504.15784'
source_url: https://arxiv.org/abs/2504.15784
tags:
- creativity
- evaluation
- test
- creative
- stories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an automated creativity evaluation method
  for large language models (LLMs) using the Torrance Test of Creative Writing (TTCW).
  The approach employs a reference-based Likert-style scoring mechanism, comparing
  LLM-generated stories to high-quality reference texts across 14 binary creativity
  tests.
---

# Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach

## Quick Facts
- arXiv ID: 2504.15784
- Source URL: https://arxiv.org/abs/2504.15784
- Authors: Ruizhe Li; Chiwei Zhu; Benfeng Xu; Xiaorui Wang; Zhendong Mao
- Reference count: 17
- One-line primary result: Reference-based evaluation improves automated creativity assessment alignment with human judgments by 15% (0.75 pairwise accuracy)

## Executive Summary
This paper introduces an automated method for evaluating LLM-generated creative writing using reference-based comparative scoring. The approach employs the Torrance Test of Creative Writing (TTCW) framework with 14 creativity tests, comparing candidate stories against high-quality reference texts using a Likert-style scoring system. The method uses an analyze-rate prompting strategy that requires explicit reasoning before scoring, and incorporates positional bias mitigation through alternating candidate/reference ordering. Experiments demonstrate significant improvements in alignment with human judgments compared to baseline evaluation methods, achieving 0.75 pairwise accuracy on creativity assessments.

## Method Summary
The framework evaluates LLM-generated stories by comparing them pairwise against high-quality reference texts from *The New Yorker*. For each of 14 TTCW tests, the method employs a 5-point Likert scale (-2 to +2) and uses an analyze-rate prompting strategy that forces explicit reasoning before scoring. Each test is conducted twice with swapped positions to mitigate positional bias, and the final score is derived from the difference between forward and reverse assessments. The method converts comparative judgments to binary pass/fail per test (cutoff at -2), then aggregates passed tests for a final creativity score (0-14). The approach demonstrates improved alignment with human judgments while maintaining robustness across different scoring scales.

## Key Results
- Achieved 0.75 pairwise accuracy (+15% improvement) over baseline evaluation methods
- Spearman correlation with human judgments ranged from 0.22 to 0.49 across different models
- 5-point Likert scale outperformed both 3-point and 7-point alternatives in LLM evaluation
- Reference-based approach maintained effectiveness when applied to 10-point rating scales

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reference-based comparison anchors LLM evaluators to concrete quality standards, reducing score drift and improving alignment with human judgment.
- Mechanism: Instead of evaluating candidate texts in isolation, the evaluator compares the candidate against a high-quality reference text using a relative Likert scale. The reference provides an implicit quality baseline that constrains the evaluation space.
- Core assumption: High-quality human-authored texts represent a consistent creativity standard that LLMs can meaningfully compare against.
- Evidence anchors:
  - [abstract]: "Our method employs a reference-based Likert-style approach, scoring generated creative texts relative to high-quality reference texts across various tests."
  - [section 5.3]: "Removing either component led to a decrease in ranking similarity and evaluation stability, highlighting the complementary roles of both strategies."
  - [corpus]: Related work (BARTScore, BERTScore) validates reference-based evaluation for text generation, though creativity-specific validation remains limited.

### Mechanism 2
- Claim: The analyze-rate prompting strategy forces explicit reasoning before scoring, improving evaluation consistency.
- Mechanism: Rather than directly outputting a rating, the LLM is instructed to "think step by step" and analyze both texts against specific creativity criteria before providing a final verdict.
- Core assumption: LLMs produce more reliable judgments when required to verbalize analysis, reducing random variation in outputs.
- Evidence anchors:
  - [section 3.3]: "Instead of directly assigning a rating, the model is first prompted to analyze the sample according to the evaluation criteria before providing a final score."
  - [section 5.3]: Ablation shows removing analyze-rate reduces Spearman correlation from 0.22 to 0.16 (Qwen) and 0.49 to 0.45 (Claude 3.5).
  - [corpus]: Related work (Chiang and Lee, 2023) demonstrates similar gains with GPT models on evaluation tasks.

### Mechanism 3
- Claim: Positional bias mitigation through alternating candidate/reference ordering improves score reliability.
- Mechanism: Each test is conducted twice with swapped positions (candidate vs. reference order). The final score is derived from the difference between the two assessments, canceling out position-dependent biases.
- Core assumption: Positional bias is symmetric and can be canceled through averaging.
- Evidence anchors:
  - [section 3.2]: "To minimize positional bias, the sequence of the candidate and reference texts is alternated, and each test is conducted twice."
  - [section 3.2 formula]: Score is calculated as the difference between forward and reverse order assessments.

## Foundational Learning

- Concept: **Torrance Tests of Creative Thinking (TTCT)**
  - Why needed here: TTCW adapts TTCT principles to writing evaluation. Understanding the original framework (divergent thinking, fluency/flexibility/originality/elaboration dimensions) clarifies why these 14 specific tests were chosen.
  - Quick check question: Can you explain why "divergent thinking" tasks differ from "convergent thinking" tasks in creativity assessment?

- Concept: **Likert Scale Design**
  - Why needed here: The paper uses a 5-point Likert scale (-2 to +2) and empirically tests alternatives. Understanding scale granularity tradeoffs helps interpret why 5-point outperformed 3-point and 7-point options.
  - Quick check question: Why might a 7-point scale perform worse than a 5-point scale when using LLMs as evaluators?

- Concept: **Pairwise Ranking vs. Absolute Scoring**
  - Why needed here: The method converts comparative judgments to binary pass/fail per test, then aggregates. Understanding ranking metrics (Spearman, Kendall's tau, pairwise accuracy) is essential for interpreting results.
  - Quick check question: Why is pairwise accuracy often more interpretable than Spearman correlation for small datasets?

## Architecture Onboarding

- Component map:
  Reference Story → Plot Extraction → LLM Candidate Generation
                                              ↓
  Reference + Candidate → TTCW Test Prompts (×14 tests)
                                              ↓
  Analyze-Rate Prompting → Likert Score (forward order)
  Analyze-Rate Prompting → Likert Score (reverse order)
                                              ↓
  Score Difference → Binary Label (cutoff threshold)
                                              ↓
  Aggregate Passed Tests → Final Creativity Score

- Critical path: The reference quality is the linchpin. Poor references will collapse the entire evaluation framework since all comparisons become unreliable.

- Design tradeoffs:
  - **Cutoff score (-2 vs. 0)**: Lower cutoffs increase pass rates, potentially reducing discrimination; empirical search found -2 optimal for this dataset
  - **Scale granularity**: 5-point balances expressiveness with LLM reliability; 7-point introduced noise
  - **Reference selection**: Human-authored vs. model-generated references—latter enables broader applicability but may lower quality ceiling

- Failure signatures:
  - All candidates receive identical scores (likely reference too superior or cutoff too permissive)
  - High variance between forward/reverse order scores (positional bias not canceling)
  - Negative Spearman correlation with human rankings (fundamental misalignment in evaluation criteria)

- First 3 experiments:
  1. Reproduce baseline vs. reference-based comparison on a single TTCW dimension to validate the core mechanism
  2. Ablate analyze-rate prompting to quantify its contribution isolated from reference-based scoring
  3. Test generalization using GPT-4-generated references (per Section 5.4) to assess robustness when human references unavailable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the evaluation framework be adapted for unrestricted article-level evaluations where high-quality reference texts are unavailable?
- Basis in paper: [explicit] Section 7 (Limitation) explicitly states that the method's "reliance on reference stories... may restrict its scalability for unrestricted article-level evaluations."
- Why unresolved: The current methodology fundamentally relies on pairwise comparison against a "ground truth" text to function, creating a dependency that must be broken or altered for open-ended use cases.

### Open Question 2
- Question: How can the system effectively distinguish relative rankings among candidate texts when all candidates are significantly inferior to the reference?
- Basis in paper: [explicit] Section 7 (Limitation) notes the method "may not be suitable when all candidate texts are far inferior to the reference," as this results in uniformly negative labels.
- Why unresolved: The binary scoring logic based on a cutoff prevents the model from differentiating between "bad" and "very bad" when the distance to the reference is large.

### Open Question 3
- Question: To what extent does the selection of reference texts bias the evaluation towards specific writing styles or cultural norms?
- Basis in paper: [explicit] Section 8 (Potential Risks) identifies the risk of "amplifying biases in reference texts, which could favor certain styles or cultural norms."
- Why unresolved: The paper uses *The New Yorker* as a reference but does not quantify how this specific stylistic choice influences the definition of "creative" used by the evaluator.

## Limitations

- Reference dependency fundamentally limits scalability to domains without readily available high-quality reference texts
- Cannot distinguish among candidates when all are significantly inferior to reference standard
- Limited to story evaluation domain; generalization to other creative modalities remains unproven

## Confidence

- **High confidence**: The core mechanism of reference-based comparative evaluation (Mechanism 1) is well-supported by experimental results showing consistent improvements over baseline methods.
- **Medium confidence**: The analyze-rate prompting strategy (Mechanism 2) shows positive effects in ablation studies, but the magnitude of improvement varies across models and could be influenced by implementation details.
- **Medium confidence**: Positional bias mitigation (Mechanism 3) is supported by theoretical reasoning and implementation details, though empirical validation of its necessity is limited.

## Next Checks

1. **Cross-domain generalization**: Test the framework on non-story creative tasks (poetry generation, code generation, visual art descriptions) to validate whether reference-based evaluation transfers beyond the tested domain.

2. **Reference quality sensitivity**: Systematically vary reference quality (using both high-quality human texts and LLM-generated references) to quantify how reference selection impacts evaluation reliability and discrimination capability.

3. **Human evaluator comparison**: Conduct controlled studies comparing automated reference-based scores with direct human pairwise comparisons on the same candidate-reference pairs to validate that the LLM evaluator's judgments align with human comparative reasoning rather than relying on absolute quality judgments.