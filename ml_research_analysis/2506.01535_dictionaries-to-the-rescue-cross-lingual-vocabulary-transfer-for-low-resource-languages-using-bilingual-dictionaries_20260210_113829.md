---
ver: rpa2
title: 'Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource
  Languages Using Bilingual Dictionaries'
arxiv_id: '2506.01535'
source_url: https://arxiv.org/abs/2506.01535
tags:
- language
- languages
- lapt
- target
- subwords
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a dictionary-based method for cross-lingual
  vocabulary transfer to adapt pre-trained language models to low-resource languages.
  The approach leverages bilingual dictionaries to map target subwords to source subwords
  iteratively by progressively removing mapped subwords from the target tokenizer,
  allowing shorter subwords to be mapped in subsequent iterations.
---

# Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource Languages Using Bilingual Dictionaries

## Quick Facts
- arXiv ID: 2506.01535
- Source URL: https://arxiv.org/abs/2506.01535
- Reference count: 33
- Key outcome: Dictionary-based cross-lingual vocabulary transfer method achieves higher F1 scores and lower perplexity than existing approaches for low-resource languages

## Executive Summary
This paper proposes a novel method for adapting pre-trained language models to low-resource languages using bilingual dictionaries instead of parallel corpora. The approach iteratively maps target subwords to source subwords by progressively removing mapped subwords from the target tokenizer, enabling finer-grained coverage. Experiments on seven languages show this method outperforms existing approaches like FOCUS and WECHSEL, particularly for languages with different scripts and limited resources.

## Method Summary
The method trains a byte-level BPE tokenizer on dictionary entries, then iteratively aligns target subwords to source subwords using fast_align. Mapped subwords are progressively removed from the target vocabulary, allowing shorter subwords to be mapped in subsequent iterations. Target embeddings are initialized as weighted averages of aligned source embeddings, then optionally refined through language-adaptive pre-training (LAPT) with MLM or CLM loss. The approach requires only a bilingual dictionary and small target corpus, making it suitable for truly low-resource scenarios.

## Key Results
- Achieves higher F1 scores on WikiANN NER dataset compared to FOCUS baseline across all seven target languages
- Shows lower word-length-normalized perplexity than FOCUS for both MLM and CLM models
- Coverage varies significantly by language (77-97%), with morphologically rich languages showing lower coverage

## Why This Works (Mechanism)

### Mechanism 1
Iterative subword removal from the target BPE tokenizer enables progressively finer-grained mapping coverage. Since BPE tokenizers merge frequent subword pairs during training, removing a mapped subword forces the tokenizer to fall back to its constituent shorter subwords in subsequent iterations. This allows the algorithm to map both long and short subwords via dictionary alignments. Limited corpus evidence shows performance drops without removal, supporting the mechanism. The loop terminates when no new subwords are mapped in an iteration.

### Mechanism 2
Treating dictionary entry-definition pairs as parallel corpora enables subword-level alignment for embedding transfer. The method uses fast_align (IBM Model 2) to align target subwords to source subwords based on co-occurrence statistics across tokenized dictionary pairs. Alignment quality depends on dictionary quality and tokenization consistency. Corpus papers on vocabulary overlap suggest token overlap alone is insufficient; alignment quality matters. Alignment fails when dictionary entries are multi-word phrases with no clear word-to-word correspondence.

### Mechanism 3
Weighted averaging of source subword embeddings, weighted by alignment counts, transfers semantic knowledge to target subwords. For each target subword, its embedding is initialized as a weighted average of aligned source embeddings, where weights are normalized co-occurrence counts. Special tokens, digits, and punctuation are copied directly. Prior work uses similar initialization strategies but requires more data or overlap. Unmapped target subwords receive the UNK embedding or random initialization, potentially degrading performance if coverage is low.

## Foundational Learning

- **Byte-level BPE tokenization**: Needed to prevent OOV issues with limited dictionary data. Quick check: Can you explain why byte-level BPE prevents OOV issues compared to character-level or word-level tokenization?

- **IBM Model 2 / fast_align alignment**: Needed for statistical subword correspondence estimation. Quick check: What does fast_align optimize, and why might it struggle with very short dictionary entries?

- **Language-Adaptive Pre-Training (LAPT)**: Needed to fine-tune models to target language distribution after embedding initialization. Quick check: Why does the paper note LAPT may play an "insignificant role" for MLM NER but substantially reduces perplexity for CLMs?

## Architecture Onboarding

- Component map: Tokenizer Training -> Subword Mapping -> Embedding Initialization -> LAPT

- Critical path: (1) Obtain bilingual dictionary; (2) Train byte-level BPE tokenizer on dictionary entries; (3) Run iterative mapping until convergence; (4) Initialize embeddings using weighted averages; (5) Replace source vocabulary with target vocabulary; (6) Optionally run LAPT with limited target corpus

- Design tradeoffs: Vocabulary replacement vs. expansion (replacement better for target language performance); Monolingual vs. multilingual source model (monolingual better for distant languages); Removal iteration depth (more iterations improve coverage but may introduce noise for CLMs)

- Failure signatures: High perplexity and low F1 indicate unmapped tokens or poor alignment quality; OOD outliers in perplexity distribution; Script mismatch without byte-level BPE

- First 3 experiments: (1) Coverage diagnostic after Algorithm 1; (2) Ablation without removal step; (3) Source model comparison between monolingual and multilingual sources

## Open Questions the Paper Calls Out

1. **Inflectional morphology handling**: The paper notes dictionaries generally "have only the unmarked form and not the inflected form," making handling inflection challenging for fusional languages. Experiments with dictionaries augmented with morphological inflection generators could resolve this.

2. **Alignment algorithm refinement**: Section 6 states "There is room for performance improvement by refining the alignment method or hyperparameter tuning." Comparative studies replacing fast_align with neural alignment models could test this.

3. **Morpheme-aware tokenization**: The paper notes "morpheme-aware tokenization... can be helpful" but restricted to BPE for comparability. Evaluation using morphological tokenizers like MorphBPE on agglutinative languages could test this.

## Limitations

- **Coverage dependency**: Effectiveness fundamentally depends on dictionary coverage and quality, creating inherent ceiling for morphologically rich languages
- **Perplexity outlier problem**: Extreme outliers persist even after LAPT, suggesting potential generalization issues
- **Source model sensitivity**: Method's effectiveness varies significantly based on source model choice, which isn't systematically explored

## Confidence

**High Confidence (4/5)**: Core iterative mapping mechanism with subword removal is well-supported by ablation studies
**Medium Confidence (3/5)**: Performance claims vs. baselines are supported but limited to specific datasets and comparisons
**Low Confidence (2/5)**: LAPT's marginal benefit beyond embedding initialization isn't systematically explored

## Next Checks

1. **Morphology-Aware Dictionary Extension**: Create extended evaluation with dictionary entries augmented with inflected forms for morphologically rich languages to test whether coverage limitation is primary bottleneck

2. **Perplexity Outlier Analysis**: Systematically investigate extreme perplexity outliers by categorizing them by linguistic features and testing whether targeted data filtering reduces outlier frequency

3. **Source Model Systematic Comparison**: Compare multiple source models across all seven target languages to measure correlation between source-target language similarity and performance, validating source model recommendations