---
ver: rpa2
title: What Really Counts? Examining Step and Token Level Attribution in Multilingual
  CoT Reasoning
arxiv_id: '2511.15886'
source_url: https://arxiv.org/abs/2511.15886
tags:
- reasoning
- steps
- step
- attribution
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the attribution patterns underlying Chain-of-Thought
  (CoT) reasoning in multilingual large language models (LLMs). Using both step-level
  and token-level attribution methods, the research analyzes how different reasoning
  steps contribute to final answers across five languages: English, French, German,
  Bengali, and Chinese.'
---

# What Really Counts? Examining Step and Token Level Attribution in Multilingual CoT Reasoning

## Quick Facts
- arXiv ID: 2511.15886
- Source URL: https://arxiv.org/abs/2511.15886
- Authors: Jeremias Ferrao; Ezgi Basar; Khondoker Ittehadul Islam; Mahrokh Hassani
- Reference count: 8
- Key outcome: Structured Chain-of-Thought prompting significantly improves accuracy for high-resource Latin-script languages (English, French) but shows minimal benefit for low-resource Bengali, with final reasoning steps consistently receiving the highest attribution scores.

## Executive Summary
This study investigates attribution patterns in multilingual Chain-of-Thought (CoT) reasoning by analyzing both step-level and token-level contributions to final answers. Using the Qwen2.5 1.5B-Instruct model across five languages (English, French, German, Bengali, Chinese), the research reveals that structured CoT significantly improves accuracy for high-resource Latin-script languages while showing minimal benefit for low-resource Bengali. The analysis demonstrates that final reasoning steps consistently receive the highest importance scores, particularly in incorrect predictions, suggesting potential overreliance on final steps in the reasoning process.

The token-level attribution experiments with controlled perturbations (negation and distractor conditions) show decreased accuracy and less coherent attribution patterns, indicating that CoT reasoning may be more fragile than previously understood. These findings highlight limitations in CoT prompting's multilingual robustness and interpretability, with excessive emphasis on final steps and sensitivity to input modifications potentially indicating unreliable reasoning processes across different language contexts.

## Method Summary
The study employs a mixed-methods approach combining structured CoT prompting with two attribution techniques: ContextCite for step-level attribution and Saliency for token-level attribution. Experiments use the Qwen2.5 1.5B-Instruct model on the MGSM multilingual math word problem benchmark across five languages. The methodology compares structured CoT against baseline approaches, measuring accuracy improvements and analyzing attribution patterns. Step-wise analysis quantifies importance scores across reasoning steps, while token-level experiments introduce controlled perturbations (negation and distractor conditions) to assess robustness of attribution patterns.

## Key Results
- Structured CoT improves accuracy for high-resource Latin-script languages (English, French) but shows minimal benefit for low-resource Bengali
- Final reasoning step consistently receives highest importance scores, particularly in incorrect predictions
- Token-level attribution with controlled perturbations (negation, distractor) shows decreased accuracy and less coherent attribution patterns
- Attribution slopes are steeper for high-resource languages compared to Bengali, indicating different reasoning dynamics across language groups

## Why This Works (Mechanism)
Chain-of-Thought reasoning works by decomposing complex problems into intermediate reasoning steps, allowing models to build toward solutions incrementally. The attribution analysis reveals that models tend to weight later reasoning steps more heavily than earlier ones, particularly in final answer generation. This step-wise decomposition appears to help high-resource language models by providing structured context, but the effectiveness diminishes for low-resource languages where the model may struggle with both language understanding and mathematical reasoning simultaneously.

## Foundational Learning

**Chain-of-Thought Prompting**
- Why needed: Enables complex reasoning by breaking down problems into intermediate steps
- Quick check: Compare accuracy on multi-step problems with and without CoT prompting

**Attribution Methods**
- Why needed: Quantifies which model components contribute most to predictions
- Quick check: Verify attribution scores sum to meaningful values across test cases

**Multilingual Model Performance**
- Why needed: Understands how language resource availability affects reasoning capabilities
- Quick check: Compare performance across high-resource vs low-resource language pairs

**Saliency Attribution**
- Why needed: Identifies which input tokens most influence model predictions
- Quick check: Test with token masking to verify attribution relevance

**ContextCite Attribution**
- Why needed: Measures importance of intermediate reasoning steps in final predictions
- Quick check: Validate step importance scores correlate with reasoning accuracy

## Architecture Onboarding

**Component Map**
Prompt Generator -> Qwen2.5 1.5B-Instruct Model -> Attribution Analyzer -> Performance Evaluator

**Critical Path**
Structured CoT Prompt → Model Reasoning → Step Attribution (ContextCite) → Token Attribution (Saliency) → Accuracy Assessment

**Design Tradeoffs**
Model size (1.5B) vs. reasoning capability: Smaller models may show clearer attribution patterns but limited performance
Language coverage vs. depth: Five languages provide diversity but may miss important patterns in other language families

**Failure Signatures**
Excessive emphasis on final reasoning steps indicates potential overreliance on last-step reasoning
Decreased attribution coherence under perturbation suggests fragile reasoning patterns
Low structured generation compliance for Bengali indicates language-specific prompting challenges

**First Experiments**
1. Compare attribution patterns across different model sizes (7B, 72B) using same MGSM benchmark
2. Test cross-lingual prompting by using English CoT examples for Bengali queries
3. Apply alternative attribution methods (Shapley values, Integrated Gradients) to verify pattern robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does employing cross-lingual prompting strategies, such as using English CoT examples for Bengali queries, improve attribution balance and reasoning accuracy?
- Basis in paper: [explicit] The conclusion suggests "specifically investigating cross-lingual prompting strategies... could offer practical methods to enhance multilingual reasoning capabilities."
- Why unresolved: The current experiments relied on native-language few-shot examples, which resulted in minimal accuracy improvements and low structured generation compliance for Bengali.
- Evidence: A comparison of attribution slopes and accuracy scores when running the MGSM Bengali test set with English few-shot prompts versus native Bengali prompts.

### Open Question 2
- Question: Do the observed attribution patterns—specifically the excessive emphasis on final reasoning steps in incorrect predictions—persist in larger model architectures?
- Basis in paper: [explicit] The authors state in the Limitations section that "findings are based on a single 1.5B parameter model, and may not extend to larger architectures."
- Why unresolved: Prior research (Lanham et al. 2023) suggests faithfulness to CoT depends on model scale, but this specific multilingual attribution trend has not been validated on larger models.
- Evidence: Replication of the ContextCite step-wise analysis on larger variants of the Qwen model family (e.g., 7B, 72B) using the same MGSM benchmark.

### Open Question 3
- Question: Are the identified attribution patterns artifacts of the specific algorithms used (ContextCite and Saliency), or are they consistent across diverse interpretability techniques?
- Basis in paper: [explicit] The Limitations section notes that "reliance on specific attribution methods necessitates confirmation with alternative techniques."
- Why unresolved: Different attribution methods (e.g., Shapley values, Integrated Gradients) may assign importance scores differently, potentially altering the conclusion that the final step is universally dominant.
- Evidence: Applying alternative token-level and step-level attribution libraries to the generated outputs to verify if the high importance assigned to the final step is robust across methods.

## Limitations
- Analysis constrained to single 1.5B parameter model, limiting generalizability to larger architectures
- Reliance on only five languages with uneven high-resource vs low-resource representation creates sampling bias
- MGSM benchmark's focus on math word problems may not capture full spectrum of CoT reasoning capabilities
- Attribution methods employed may not fully capture complex interdependencies between reasoning steps and tokens

## Confidence
- **High Confidence**: Structured CoT improves accuracy for high-resource languages, well-supported by empirical data and consistent with prior literature
- **Medium Confidence**: Final reasoning steps receiving highest importance scores requires cautious interpretation due to attribution method limitations
- **Low Confidence**: Claims about CoT prompting's unreliability in multilingual contexts limited by narrow language scope and model size constraints

## Next Checks
1. Replicate attribution analysis across multiple model sizes (7B+ parameters) and architectures to assess scalability of findings
2. Expand language coverage to include additional low-resource languages with diverse scripts (Arabic, Hindi, Swahili) to better understand multilingual robustness patterns
3. Implement ablation studies on different CoT step structures to determine optimal reasoning depth and complexity across language groups