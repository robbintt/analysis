---
ver: rpa2
title: Are Hallucinations Bad Estimations?
arxiv_id: '2509.21473'
source_url: https://arxiv.org/abs/2509.21473
tags:
- hallucination
- loss
- definition
- probability
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides a novel theoretical perspective on hallucinations\
  \ in generative models, framing them as a fundamental misalignment between the loss-minimizing\
  \ training objective and human-acceptable outputs. The authors formalize this as\
  \ \u03B4-hallucination, where an estimator's output fails to align with any plausible\
  \ input cause."
---

# Are Hallucinations Bad Estimations?
## Quick Facts
- arXiv ID: 2509.21473
- Source URL: https://arxiv.org/abs/2509.21473
- Reference count: 4
- This paper proves that minimizing expected loss cannot eliminate hallucinations in generative models

## Executive Summary
This paper reframes hallucinations in generative models as a fundamental misalignment between loss-minimizing training objectives and human-acceptable outputs. The authors formalize this as δ-hallucination, proving that even optimal estimators minimizing expected loss can still hallucinate. Through theoretical analysis and controlled experiments across coin-flipping, open-ended QA, and text-to-image generation tasks, they demonstrate that training to minimize loss does not reduce hallucination rates, establishing hallucination as an intrinsic property of probabilistic estimation rather than a solvable bug.

## Method Summary
The authors validate their theoretical framework through three controlled experiments. First, they train an 8-layer transformer on synthetic coin-flipping data to predict total heads from coin labels, showing training loss decreases while conditional probability of estimates under latent labels does not consistently increase. Second, they fine-tune Qwen language models using LLaMA-Factory with LoRA for 2-4 epochs, computing resemblance to TruthfulQA incorrect answers via Gestalt Pattern Matching. Third, they fine-tune Stable Diffusion v1.5 UNet on AFHQ dataset, fit per-class GMMs on CLIP embeddings, and measure hallucination rate as the fraction of generated images outside the Hallucination Density Rejection (HCDR) threshold. Across all settings, they consistently observe that loss minimization does not reduce hallucination rates.

## Key Results
- Optimal estimators minimizing expected loss can still produce hallucinated outputs
- Theoretical lower bound on δ-hallucination probability is derived and validated
- Consistent empirical evidence across synthetic, language, and vision tasks shows loss reduction does not eliminate hallucination

## Why This Works (Mechanism)
The paper establishes that hallucination is not a training artifact but a structural property of estimation under loss-minimization. The mechanism relies on the fundamental mismatch between mathematical optimality (minimizing expected loss) and human judgment of correctness. When the loss function penalizes incorrect outputs differently than human evaluators would, the model optimizes for the wrong objective. This creates a systematic gap where the most probable outputs under the model's learned distribution may not align with any plausible input cause, resulting in δ-hallucination regardless of model capacity or training data quality.

## Foundational Learning
- **δ-hallucination**: A formal definition of hallucination where model outputs fail to align with any plausible input cause. Needed to establish a rigorous mathematical framework for analyzing hallucinations. Quick check: Verify that outputs can be classified as hallucinated or not using the δ threshold.
- **Conditional probability under latent labels**: Measuring the probability of estimates given true underlying causes. Needed to evaluate whether models learn to produce plausible outputs conditioned on actual inputs. Quick check: Compare conditional probability metrics across training epochs.
- **Hallucination Density Rejection (HCDR)**: A threshold-based method for detecting hallucinated images using GMMs on CLIP embeddings. Needed to quantify hallucination rates in image generation. Quick check: Visualize density histograms with threshold line to confirm ~90% coverage.

## Architecture Onboarding
- **Component map**: Coin labels -> 8-layer transformer -> Predicted heads; Questions -> LoRA-tuned Qwen -> Answers; Text prompts -> SD UNet -> Images -> CLIP embeddings -> GMM density scores
- **Critical path**: Data generation → Model training → Metric computation → Analysis. The core insight is that training loss decreases along the entire path while hallucination metrics remain stable or increase.
- **Design tradeoffs**: Standard loss minimization prioritizes mathematical optimality over human alignment, sacrificing hallucination resistance for predictive accuracy.
- **Failure signatures**: Training loss consistently decreases while conditional probability metrics plateau or decrease; HCDR thresholds maintain consistent coverage despite lower training loss.
- **First experiment**: Run coin flipping experiment to verify disconnect between loss reduction and conditional probability under latent labels
- **Second experiment**: Implement HCDR calculation with visual verification of threshold coverage on AFHQ held-out data
- **Third experiment**: Test QA experiment with multiple learning rates and seeds to assess robustness of resemblance metrics

## Open Questions the Paper Calls Out
- Can tighter lower bounds on δ-hallucination probability be derived under more relaxed distributional assumptions than those in Theorem 6.1 (identical means and independence of conditional means)?
- How do different loss functions (beyond quadratic and cross-entropy) affect δ-hallucination rates, and can alternative objectives explicitly minimize hallucination while maintaining predictive performance?
- Can practical "alignment-oriented training schemes"—such as HDR-guided sampling or mixed-objective fine-tuning—effectively reduce δ-hallucination without sacrificing model utility?

## Limitations
- Missing detailed experimental hyperparameters (learning rates, batch sizes, LoRA configurations)
- Unspecified GMM parameters (number of components, covariance type) for image experiments
- Lack of implementation details for proposed alternative training objectives and alignment schemes

## Confidence
- High confidence in the theoretical framework and its core mathematical proofs
- Medium confidence in the experimental validation due to missing implementation details
- Medium confidence in the generalizability of results across different generative model types

## Next Checks
1. Re-run the coin flipping experiment with explicit latent label conditioning to verify the disconnect between training loss reduction and conditional probability metrics
2. Implement the HCDR calculation with visual verification of the 10th percentile threshold coverage on held-out AFHQ data
3. Test the QA experiment with at least 4 different learning rates and 4 seeds to assess robustness of the resemblance metric to TruthfulQA incorrect answers