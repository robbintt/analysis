---
ver: rpa2
title: 'It''s All About the Confidence: An Unsupervised Approach for Multilingual
  Historical Entity Linking using Large Language Models'
arxiv_id: '2601.08500'
source_url: https://arxiv.org/abs/2601.08500
tags:
- entity
- candidate
- mhel-llamo
- entities
- historical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MHEL-LLaMo, an unsupervised ensemble approach
  for multilingual historical entity linking that combines a bi-encoder for candidate
  retrieval with an LLM for NIL prediction and candidate selection. The system uses
  confidence scores from the bi-encoder to filter easy samples and apply the LLM only
  for hard cases, reducing computational costs while preventing hallucinations.
---

# It's All About the Confidence: An Unsupervised Approach for Multilingual Historical Entity Linking using Large Language Models

## Quick Facts
- **arXiv ID**: 2601.08500
- **Source URL**: https://arxiv.org/abs/2601.08500
- **Reference count**: 24
- **Primary result**: MHEL-LLaMo achieves F1 scores up to 0.723 on English (HIPE-2020), 0.662 on French (NewsEye), and 0.698 on Italian (MHERCL), outperforming state-of-the-art models by up to 27% without requiring fine-tuning.

## Executive Summary
This paper introduces MHEL-LLaMo, an unsupervised ensemble approach for multilingual historical entity linking that combines a bi-encoder for candidate retrieval with a large language model (LLM) for NIL prediction and candidate selection. The system leverages confidence scores from the bi-encoder to filter easy samples and apply the LLM only for hard cases, reducing computational costs while preventing hallucinations. Evaluated on four benchmarks across six European languages from the 19th-20th centuries, MHEL-LLaMo demonstrates significant performance improvements over state-of-the-art models, achieving F1 scores up to 27% higher than existing approaches without requiring fine-tuning.

## Method Summary
MHEL-LLaMo operates through a two-stage process: first, a bi-encoder retrieves candidate entities from Wikipedia for each mention in the text, then an LLM performs NIL prediction and candidate ranking for low-confidence cases. The system uses confidence thresholding to determine when to invoke the LLM, applying it only to mentions where the bi-encoder's confidence falls below a certain threshold. This adaptive approach reduces computational costs while maintaining high accuracy. The LLM is prompted with chained queries that first predict whether a mention should be linked to an entity (NIL prediction) and then ranks the candidates if linking is appropriate. The system is evaluated across four historical datasets in six European languages (English, French, German, Italian, Spanish, and Swedish) covering texts from the 19th and 20th centuries.

## Key Results
- MHEL-LLaMo achieves F1 scores up to 0.723 on English (HIPE-2020), 0.662 on French (NewsEye), and 0.698 on Italian (MHERCL)
- Outperforms state-of-the-art models by up to 27% without requiring fine-tuning
- The adaptive threshold mechanism proves particularly effective on datasets with high NIL rates and popular entities
- Prompt chaining improves NIL prediction accuracy compared to single-step approaches

## Why This Works (Mechanism)
The system's effectiveness stems from its confidence-based filtering approach that combines the speed of bi-encoders with the reasoning capabilities of LLMs. By using the bi-encoder's confidence scores to identify difficult cases, the system avoids the computational expense of running LLM inference on all mentions while still capturing the nuanced understanding needed for challenging entity linking decisions. The prompt chaining approach breaks down the complex entity linking task into manageable sub-tasks (NIL prediction followed by ranking), allowing the LLM to focus on specific aspects of the problem and improving overall accuracy.

## Foundational Learning
- **Entity Linking**: The task of connecting textual mentions to their corresponding entities in a knowledge base (why needed: core problem being solved; quick check: can the system correctly link "Paris" to the city entity)
- **Bi-encoder architecture**: A neural model that encodes mentions and entities separately for efficient candidate retrieval (why needed: enables fast candidate generation; quick check: does retrieval produce relevant candidates within top-10)
- **NIL prediction**: Determining when a mention does not correspond to any known entity (why needed: prevents incorrect linking; quick check: can the system correctly identify novel entities or misspellings)
- **Confidence thresholding**: Using probability scores to filter cases requiring additional processing (why needed: balances accuracy and efficiency; quick check: what threshold value maximizes F1 score)
- **Prompt chaining**: Breaking complex tasks into sequential prompts for LLMs (why needed: improves task decomposition and accuracy; quick check: does chained prompting outperform single-step approaches)
- **Historical multilingual processing**: Handling language variation and temporal changes in entity references (why needed: addresses the specific challenges of historical texts; quick check: can the system handle obsolete terminology)

## Architecture Onboarding

**Component Map**: Text mentions -> Bi-encoder -> Confidence scores -> Filter (high confidence -> output; low confidence -> LLM) -> LLM NIL prediction and ranking -> Final entity selection

**Critical Path**: The core pipeline follows: candidate retrieval via bi-encoder → confidence thresholding → LLM processing for low-confidence cases → final entity selection. The LLM serves as the final arbiter for difficult cases identified by low confidence scores.

**Design Tradeoffs**: The system trades computational efficiency for accuracy by using confidence thresholds to limit LLM usage. This creates a potential coverage gap where difficult but important mentions might be missed if their confidence scores are artificially low. The use of a fine-tuned bi-encoder for retrieval partially undermines the "unsupervised" claim but provides better performance than fully unsupervised alternatives.

**Failure Signatures**: The system may struggle with rare entities lacking strong Wikipedia representation, novel entities not present in training data, or mentions where the bi-encoder produces falsely high confidence scores. Performance may degrade on datasets with low NIL rates where the confidence threshold mechanism is less effective.

**First Experiments**:
1. Measure the distribution of confidence scores across different datasets to understand the threshold selection process
2. Compare retrieval accuracy of the fine-tuned bi-encoder against an unsupervised baseline
3. Evaluate NIL prediction accuracy separately from entity linking accuracy to isolate LLM performance

## Open Questions the Paper Calls Out
None

## Limitations
- The reliance on confidence thresholding may systematically exclude harder cases that could benefit from LLM processing, creating a coverage-efficiency tradeoff
- Performance advantages are particularly pronounced on datasets with high NIL rates and popular entities, suggesting reduced effectiveness for rare or emerging entities
- The use of in-domain fine-tuned bi-encoders for retrieval partially undermines the "unsupervised" nature of the system
- LLM-based ranking and NIL prediction remains computationally expensive compared to fully unsupervised alternatives

## Confidence
- **High Confidence**: The F1 score improvements over baselines (up to 27%) on the tested benchmarks are well-supported by quantitative results
- **Medium Confidence**: The claim about "preventing hallucinations" is supported by methodology but lacks systematic evaluation of LLM output quality across all failure modes
- **Medium Confidence**: The assertion that the approach works "without requiring fine-tuning" is partially undermined by the use of fine-tuned bi-encoders for candidate retrieval

## Next Checks
1. Test MHEL-LLaMo on datasets with low NIL rates and rare entities to evaluate whether performance gains persist outside the current evaluation scope
2. Conduct ablation studies comparing the fine-tuned bi-encoder against unsupervised retrieval methods to quantify the true cost of the "unsupervised" claim
3. Measure computational overhead and runtime differences between the full MHEL-LLaMo pipeline and baselines to validate the efficiency claims beyond accuracy metrics