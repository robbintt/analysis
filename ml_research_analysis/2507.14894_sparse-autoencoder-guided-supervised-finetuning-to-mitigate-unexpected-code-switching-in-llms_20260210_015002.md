---
ver: rpa2
title: Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching
  in LLMs
arxiv_id: '2507.14894'
source_url: https://arxiv.org/abs/2507.14894
tags:
- code-switching
- language
- unexpected
- sasft
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unexpected code-switching in
  multilingual Large Language Models (LLMs), where models inappropriately switch to
  unexpected languages during generation, reducing readability and usability. The
  authors provide the first in-depth mechanistic analysis using sparse autoencoders
  (SAEs), discovering that unexpected code-switching is closely related to unusually
  high pre-activation values of irrelevant language-specific features.
---

# Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs

## Quick Facts
- **arXiv ID:** 2507.14894
- **Source URL:** https://arxiv.org/abs/2507.14894
- **Reference count:** 40
- **Key outcome:** First in-depth mechanistic analysis of unexpected code-switching using sparse autoencoders (SAEs), discovering high pre-activation values of language-specific features precede code-switching. Proposes SASFT method reducing code-switching by >50% across five models while maintaining multilingual performance.

## Executive Summary
This paper addresses unexpected code-switching in multilingual LLMs, where models inappropriately switch to unintended languages during generation. Through SAE analysis, the authors discover that code-switching correlates with unusually high pre-activation values of irrelevant language-specific features. They propose Sparse Autoencoder-guided Supervised Finetuning (SASFT), which adds an auxiliary loss to keep these pre-activation values below thresholds during training. Experiments on Gemma-2, Llama-3.1, and Qwen3 series across Chinese, Russian, and Korean demonstrate consistent code-switching reduction (>50%) while maintaining or improving multilingual benchmark performance.

## Method Summary
SASFT adds an auxiliary loss to standard supervised finetuning that constrains pre-activation values of language-specific features. The method identifies language-specific SAE features using a monolinguality metric, estimates baseline pre-activation values from training data, and adds L_reduce = E[Σ ReLU(f_s(x) - α_j)] to the cross-entropy loss. Training uses λ=0.05 regularization, applies to the last 2 transformer layers, and uses top 2 language-specific features per language. The approach requires pre-trained SAEs, multilingual SFT datasets, and code-switching detection via script identification.

## Key Results
- SASFT reduces code-switching ratio by >50% across five models (Gemma-2, Llama-3.1, Qwen3 series) in four out of five cases, with complete elimination in four cases
- Korean shows best reduction (>90%) while Russian shows least (53%), suggesting language-specific effectiveness
- SASFT maintains or improves multilingual benchmark performance on six tasks (MMLU, HumanEval, Flores-200, HellaSwag, LogiQA, IFEval)
- Pre-estimated baseline α_j outperforms zero-baseline (α=0), showing 69-87% vs 24-47% reduction on Qwen3-1.7B
- Multi-layer (last 2 layers) and multi-feature (top 2) intervention outperforms single-layer/single-feature approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unexpected code-switching is preceded by abnormally high pre-activation values of language-specific features
- Mechanism: As the model generates tokens leading up to a code-switch event, the pre-activation f(x) of the target language's specific features rises gradually. The token immediately before code-switching exhibits elevated pre-activation, causing the model to interpret subsequent generation in that language context.
- Core assumption: Language-specific SAE features capture monolingual behavior sufficiently to predict and control code-switching (assumption: these features are causally responsible, not just correlated).
- Evidence anchors:
  - [abstract]: "unexpected code-switching...correlates with unusually high pre-activation values of language-specific features"
  - [Section 3.2.1, Figure 3]: Pre-activation values increase in tokens preceding code-switching across all five tested models
  - [Section 3.2.2, Figure 4]: Ablating Chinese features with directional ablation reduces code-switching ratio in proportion to ablation coefficient λ
  - [corpus]: Related work on "Language steering in latent space" suggests latent-space language directions can steer generation, supporting the directional intervention premise (weak direct evidence for pre-activation specifically)
- Break condition: If pre-activation patterns do not consistently precede code-switching in your target model/language pair, this mechanism may not apply; verify with SAE analysis first

### Mechanism 2
- Claim: Training with auxiliary loss on pre-activation values teaches the model to self-regulate language features
- Mechanism: During SFT, the auxiliary loss L_reduce = E[∑ ReLU(f_s(x) - α_j)] penalizes residual streams where irrelevant language features exceed their baseline pre-activation α_j. Over training, the model learns to suppress these features proactively during generation, rather than requiring inference-time intervention.
- Core assumption: The pre-estimated average pre-activation α_j is a meaningful baseline; setting it to zero is suboptimal because negative pre-activations carry semantic information.
- Evidence anchors:
  - [Section 4.2, Eq. 8-9]: Formal definition of auxiliary loss combining ReLU threshold with cross-entropy
  - [Table 3]: SASFT with learned α_j outperforms SASFTzero (α=0), showing importance of proper baseline
  - [Section 5.3, Figure 6]: Multi-layer application outperforms single-layer, suggesting language features operate across layers
  - [corpus]: No direct corpus evidence for this specific auxiliary loss formulation
- Break condition: If your training data has highly varied language distributions or α_j estimates are unstable, the threshold may need retuning; monitor for over-suppression causing fluency degradation

### Mechanism 3
- Claim: Multi-layer, multi-feature intervention is necessary because language-specific features are distributed
- Mechanism: Single-layer intervention shows decreasing effectiveness moving toward earlier layers (Figure 6). Language features appear across multiple transformer layers; constraining only the final layer is insufficient because earlier layers continue propagating language signals.
- Core assumption: Top-ranked language-specific features (by monolinguality metric ν) are the primary drivers of code-switching.
- Evidence anchors:
  - [Section 5.3, Figure 6]: Multi-layer (dashed lines) consistently outperforms single-layer (solid lines) across all models
  - [Section 5.3, Figure 7]: Multi-feature intervention shows lower variance and better reduction than single-feature
  - [Section 4.1, Eq. 7]: Monolinguality metric ν^L_s = μ^L_s - γ^L_s for ranking language-specific features
  - [corpus]: No corpus papers directly address multi-layer feature distribution for code-switching
- Break condition: If computational budget limits multi-layer SAE access, prioritize final 2 layers as done in main experiments; earlier layers show diminishing returns

## Foundational Learning

- Concept: Sparse Autoencoders (SAEs) decompose activations into interpretable feature directions
  - Why needed here: SAEs are the diagnostic tool for identifying language-specific features; without understanding SAE encoder/decoder structure (W_enc, W_dec, ReLU activation), you cannot extract or manipulate the features SASFT relies on
  - Quick check question: Given a residual stream x and trained SAE, can you compute the pre-activation f(x) and explain why it differs from the feature activation a(x)?

- Concept: Pre-activation vs. activation values
  - Why needed here: The paper explicitly uses pre-activation f(x) = W_enc·x + b_enc rather than post-ReLU activation a(x), because negative pre-activations have meaningful semantic content (negative projections along feature directions)
  - Quick check question: Why would setting α_j = 0 be problematic if pre-activations can be negative?

- Concept: Directional ablation in residual stream
  - Why needed here: Understanding how to modify residual streams by subtracting feature directions (x' ← x - λd) is essential for both the feasibility study (inference-time ablation) and conceptualizing what SASFT learns to do internally
  - Quick check question: If you ablate a language feature with λ too high, what capability degradation might you expect?

## Architecture Onboarding

- Component map: SAE infrastructure -> Language feature identification -> Pre-activation baseline estimator -> Modified SFT trainer -> Code-switching detector
- Critical path:
  1. Train or obtain SAEs for your base model (requires residual stream data)
  2. Collect multilingual corpus, compute ν for all features per language
  3. Select language-specific features (paper uses top 2)
  4. Estimate α_j baselines from training data
  5. Run SASFT with L_training = L_CE + λ·L_reduce on last 2 layers
  6. Evaluate code-switching ratio + multilingual benchmarks
- Design tradeoffs:
  - Layer selection: More layers = better reduction but higher compute and potential over-constraint
  - Feature count: More features = more stable but risk of suppressing useful multilingual representations
  - λ hyperparameter: Too high harms fluency; too low has weak effect (paper uses 0.05 via grid search)
  - α_j estimation: Pre-estimated vs. zero baseline trades accuracy for simplicity
- Failure signatures:
  - Code-switching persists: Check if SAE features are truly language-specific (high ν); may need different features
  - Multilingual benchmark degradation: λ may be too high or features too broad; reduce regularization strength
  - Training instability: L_reduce scale may dominate L_CE; normalize or reduce λ
  - Korean works better than Russian/Chinese (observed in paper): Likely due to script distinctiveness and model's language capability asymmetry
- First 3 experiments:
  1. Reproduce feasibility study (Section 3.2): On your model, visualize pre-activation trajectory before code-switching events; confirm correlation
  2. Single-language SASFT pilot: Train on one target language (e.g., Chinese) with last 2 layers + top 2 features; compare code-switching ratio to SFT baseline
  3. Ablate α_j setting: Compare SASFT (pre-estimated α) vs. SASFTzero (α=0) on one model; verify Table 3 finding that proper baseline matters

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness claims depend heavily on quality and representativeness of proprietary SFT datasets that are unavailable for verification
- Korean language shows systematically better results than Russian and Chinese, suggesting method may work unevenly across language types
- Central causal claim (pre-activation values drive code-switching) rests on mechanistic analysis rather than definitive ablation studies proving causality

## Confidence
- **High:** Code-switching reduction claims (consistent results across multiple models and languages)
- **Medium:** Mechanistic explanation (plausible but not definitively proven)
- **Low:** Generalizability to other languages and multilingual settings beyond tested scope

## Next Checks
1. **Causality verification through ablation:** Design ablation study where you randomly select non-language-specific features and apply the same pre-activation constraint. If SASFT only works for language-specific features (high ν), this would strengthen the causal claim. Compare reduction effectiveness between language-specific and non-specific features.

2. **Generalization to additional language pairs:** Test SASFT on languages with different script types (e.g., Arabic, Japanese, Hindi) and language families (e.g., French, German, Spanish). Include at least one language pair that frequently code-switches naturally (like Hindi-English) to test whether SASFT distinguishes between natural and unexpected code-switching.

3. **Long-form generation stability:** Evaluate code-switching behavior in extended generation tasks (500+ tokens) rather than just prompt-based generation. This tests whether the pre-activation constraints remain effective over longer sequences and whether the method introduces any accumulation effects or degradation in long-form fluency.