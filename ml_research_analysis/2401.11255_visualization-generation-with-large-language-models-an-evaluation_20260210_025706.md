---
ver: rpa2
title: 'Visualization Generation with Large Language Models: An Evaluation'
arxiv_id: '2401.11255'
source_url: https://arxiv.org/abs/2401.11255
tags:
- visualization
- data
- llms
- prompt
- nl2vis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) for generating
  Vega-Lite visualizations from natural language queries using the nvBench dataset.
  Six representative LLMs are tested with eight prompt strategies, including few-shot,
  zero-shot, and reasoning-oriented approaches.
---

# Visualization Generation with Large Language Models: An Evaluation

## Quick Facts
- **arXiv ID:** 2401.11255
- **Source URL:** https://arxiv.org/abs/2401.11255
- **Reference count:** 40
- **Primary result:** Large language models vary significantly in their ability to generate Vega-Lite visualizations from natural language, with few-shot prompting generally outperforming other strategies.

## Executive Summary
This paper evaluates six representative large language models for generating Vega-Lite visualizations from natural language queries using the nvBench dataset. The study tests eight prompt strategies, including few-shot, zero-shot, and reasoning-oriented approaches. Results show significant variation across models, prompts, and chart types, with few-shot prompting generally outperforming others. The evaluation reveals that simpler chart types do not always yield better results, and higher-performing models do not consistently excel across all prompts. The study also identifies limitations in the nvBench benchmark, highlighting the need for improved evaluation frameworks.

## Method Summary
The study evaluates six open-source LLMs (Qwen2-7B, Llama3-8B, Mistral-7B, Gemma-7b, GLM4-9B, DeepSeek-R1-8B) using the nvBench dataset filtered to remove multi-table SQL needs and grouping line charts. Eight prompt strategies are tested: Zero-shot, Few-shot, Zero-shot-CoT, PS-Plus-CoT, Auto-CoT, Least-to-Most, Self-Refine, and Self-Consistency. Performance is measured by three metrics: Vis Accuracy (mark type match), Validity (JSON renders successfully), and Legality (data attributes match ground truth). Prompts enforce Vega-Lite v5 format with specific data URL formatting.

## Key Results
- Few-shot prompting significantly outperforms zero-shot approaches, achieving higher legality rates (26% vs 6% average)
- Chart complexity does not directly correlate with performance - pie charts underperform despite being simpler due to shorter, less informative queries
- Least-to-Most decomposition yields the strongest results on Llama3 by enforcing explicit procedural segmentation
- Self-Consistency achieves highest legality (37.06%) but at low computational efficiency due to multiple generations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Few-shot prompting improves NL2VIS performance by providing direct grammar exemplification, whereas reasoning-based strategies fail to bridge the declarative syntax gap.
- **Mechanism:** LLMs pre-trained on natural language and imperative code often lack sufficient exposure to declarative visualization grammars. Few-shot examples ground generation in correct syntactic patterns, while reasoning prompts add abstract steps without resolving syntax knowledge gaps.
- **Core assumption:** The bottleneck in NL2VIS tasks is representational understanding of target grammar, not logical reasoning capability.
- **Evidence anchors:** Section V-A (a) shows few-shot compensating for insufficient Vega-Lite grammar knowledge, while reasoning-oriented prompts fail to bridge the syntactic gap.

### Mechanism 2
- **Claim:** Structured problem decomposition (Least-to-Most) enhances performance in multi-stage visualization tasks by enforcing explicit procedural segmentation.
- **Mechanism:** Visualization generation is inherently sequential (schema → filter → encode). Least-to-Most forcing models to solve sub-problems in order maintains coherence, while CoT produces redundant or misaligned intermediate steps.
- **Core assumption:** The visualization generation task aligns better with hierarchical, stage-wise reasoning than with single-step or loosely structured thought chains.
- **Evidence anchors:** Section V-A (b) demonstrates Least-to-Most providing explicit stage-wise guidance, yielding strongest results on Llama3.

### Mechanism 3
- **Claim:** Visualization accuracy is influenced more by semantic density of user queries than by inherent chart type complexity.
- **Mechanism:** Shorter queries (common in pie chart tasks) provide limited context for LLMs to infer intent, leading to incorrect mappings. Longer queries provide richer constraints, reducing ambiguity.
- **Core assumption:** nvBench's query length distribution correlates with chart types, creating confounding where "simple" charts are paired with "sparse" natural language inputs.
- **Evidence anchors:** Section V-A (d) shows pie chart queries are generally shorter (15.65 words) versus bar charts (25.67) and stacked bar (28.33).

## Foundational Learning

- **Concept: Declarative vs. Imperative Visualization**
  - **Why needed here:** The paper evaluates Vega-Lite (declarative JSON) rather than Python (imperative). The "syntax gap" mechanism relies on understanding that LLMs struggle more with strict, nested structure of declarative grammars than procedural code.
  - **Quick check question:** Can you distinguish between describing *how* to draw a chart (imperative loops) vs. *what* the chart encoding is (declarative JSON)?

- **Concept: NL2VIS Evaluation Metrics (Validity vs. Legality)**
  - **Why needed here:** The paper separates "validity" (does the code run/render?) from "legality" (does it answer the question and match data attributes?). Understanding this distinction is required to interpret baseline results.
  - **Quick check question:** If an LLM generates a valid bar chart that renders successfully but uses wrong data column for X-axis, is this a failure of validity or legality?

- **Concept: Prompt Strategy Categories**
  - **Why needed here:** The paper classifies prompts into In-Context Learning (Few-shot), Chain-of-Thought, and Decomposition. Knowing these categories is prerequisite to understanding why "more reasoning" (CoT) failed compared to "more examples" (Few-shot).
  - **Quick check question:** Does "Zero-shot-CoT" rely on providing examples or adding reasoning instructions like "Let's think step by step"?

## Architecture Onboarding

- **Component map:** Input Preprocessor → Prompt Constructor → LLM Engine → Evaluation Pipeline (Validator → Comparator)
- **Critical path:** The **Prompt Constructor** is the highest-leverage component. Shifting from Zero-shot to Few-shot changes legality from ~6% to ~26% on average, dwarfing impact of model selection alone.
- **Design tradeoffs:**
  - Few-shot vs. Zero-shot: Few-shot provides higher accuracy (legality) but requires curated examples and consumes context window tokens; Zero-shot is flexible but suffers from "grammar hallucinations."
  - Self-Consistency: Yields highest legality (37.06%) but has low computational efficiency due to multiple generations and majority voting.
- **Failure signatures:**
  - Format Errors: Model outputs reasoning text inside JSON block (common in CoT strategies)
  - Encoding Misgeneralization: Using x/y encodings for Pie charts (which typically use theta/color) because model over-generalizes from Cartesian chart training data
  - Control Illusion: Strong models (like Llama3) misinterpreting Self-Refine prompts by treating feedback examples as tasks to execute
- **First 3 experiments:**
  1. Sanity Check (Prompt Ablation): Run same 50 queries through system using Zero-shot vs. Few-shot prompts. Measure delta in *Validity* to confirm "grammar knowledge" hypothesis.
  2. Chart Type Stress Test: Isolate "Pie Chart" queries and run with Zero-shot vs. Few-shot. Verify if Few-shot examples specifically correcting theta encoding improve legality.
  3. Decomposition Validation: Implement Least-to-Most prompt and check if intermediate steps (Filter → Aggregate) appear explicitly in trace, correlating this with higher validity rates compared to Zero-shot-CoT.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do general-purpose LLMs compare to specialized, fine-tuned models (like ncNet) when jointly assessing accuracy, generalizability, and resource efficiency?
- **Basis in paper:** The authors note that fine-tuned models achieve high accuracy but lack generalizability, whereas LLMs were evaluated using zero/few-shot prompts, making direct comparison difficult without unified framework.
- **Why unresolved:** The study evaluated only general-purpose LLMs and did not fine-tune them or directly benchmark against specialized models using standardized cost-benefit analysis.
- **What evidence would resolve it:** A comparative study measuring inference costs and accuracy drops on out-of-domain datasets for both model types.

### Open Question 2
- **Question:** Can LLM-based NL2VIS approaches be effectively generalized to imperative visualization grammars like D3.js, rather than just declarative ones like Vega-Lite?
- **Basis in paper:** The authors identify need to explore applicability in broader visualization paradigms, noting that while Vega-Lite is popular, D3 offers distinct advantages but presents more challenges.
- **Why unresolved:** The evaluation scope was strictly limited to Vega-Lite specification format.
- **What evidence would resolve it:** An evaluation of LLM performance on generating D3 code for same natural language queries used in nvBench.

### Open Question 3
- **Question:** Does a multi-round interaction paradigm significantly improve accuracy and utility of generated visualizations compared to one-round paradigm evaluated?
- **Basis in paper:** The authors state that current evaluation focused on one-round generation to simulate end-to-end tasks, leaving potential of iterative refinement unexplored.
- **Why unresolved:** The experimental design explicitly excluded multi-round interactions to simplify baseline evaluation process.
- **What evidence would resolve it:** User studies measuring success rate of visualization tasks when users are allowed iterative natural language feedback with model.

### Open Question 4
- **Question:** How can benchmark evaluation be augmented with human-centered perspectives to identify deeper issues that static metrics miss?
- **Basis in paper:** The paper suggests that benchmark-driven methodologies may be too coarse-grained and proposes incorporating crowd-sourcing evaluations.
- **Why unresolved:** The study relied entirely on automated metrics (validity, legality, accuracy) derived from nvBench dataset.
- **What evidence would resolve it:** A framework combining automated metrics with human ratings for "usefulness" or "insightfulness" of generated charts.

## Limitations

- The study relies on nvBench dataset, which contains significant noise including mismatched queries, inappropriate visual mappings, and incorrect underlying data, introducing uncertainty about whether performance differences reflect genuine capability gaps or dataset artifacts.
- The exact prompt templates were not provided in main paper text, making faithful reproduction challenging and limiting replicability of specific findings.
- The evaluation focuses exclusively on open-source models with maximum 9B parameters, leaving uncertainty about how findings generalize to frontier models or proprietary systems.

## Confidence

- **High confidence:** The core finding that few-shot prompting outperforms zero-shot approaches for NL2VIS tasks. This is well-supported by systematic ablation studies and clear mechanism of syntax grounding.
- **Medium confidence:** The relative performance ordering of different prompt strategies (Least-to-Most > Self-Consistency > others). While statistically supported, some differences are small and may be dataset-dependent.
- **Medium confidence:** The claim that query length affects performance more than chart complexity. This is supported by correlation data but requires controlled experiments to establish causation definitively.
- **Low confidence:** Generalizability of specific numerical results to other datasets or domains beyond nvBench. The authors note the dataset has unique characteristics that may not transfer.

## Next Checks

1. **Controlled ablation on query length:** Create a balanced subset of nvBench where pie charts and bar charts have matched query lengths, then re-run few-shot vs zero-shot comparisons to isolate effect of query semantics from chart type.

2. **Cross-dataset validation:** Apply best-performing prompt strategy (few-shot) to an independent NL2VIS benchmark like VisWiser or DeepVisBench to test whether performance hierarchy holds across different data distributions.

3. **Syntax gap isolation:** Design a minimal synthetic dataset where all queries are semantically equivalent but vary only in syntactic complexity (different ways to specify same chart). Test whether few-shot examples consistently outperform zero-shot across this controlled grammar variation.