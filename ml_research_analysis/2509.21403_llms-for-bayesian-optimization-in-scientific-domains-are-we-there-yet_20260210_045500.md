---
ver: rpa2
title: 'LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?'
arxiv_id: '2509.21403'
source_url: https://arxiv.org/abs/2509.21403
tags:
- genes
- llmnn
- gene
- hits
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper evaluates whether instruction-tuned large language models
  (LLMs) can perform effective in-context experimental design for scientific tasks.
  Using BioDiscoveryAgent with both open-source and closed-source LLMs, it finds that
  LLMs show no sensitivity to experimental feedback: replacing true outcomes with
  randomly permuted labels has no impact on performance.'
---

# LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?

## Quick Facts
- arXiv ID: 2509.21403
- Source URL: https://arxiv.org/abs/2509.21403
- Authors: Rushil Gupta; Jason Hartford; Bang Liu
- Reference count: 28
- Primary result: LLMs show no sensitivity to experimental feedback; hybrid LLMNN outperforms pure LLM and classical methods.

## Executive Summary
This paper critically evaluates whether instruction-tuned LLMs can perform effective in-context experimental design for scientific tasks. Using BioDiscoveryAgent with Llama-3.1-8B, Qwen-2-7B, and Claude 3.5 Sonnet across gene perturbation and molecular property optimization benchmarks, the authors find that LLMs do not leverage experimental feedback. A randomization ablation shows that replacing true outcomes with permuted labels has no impact on performance. The paper introduces LLM-guided Nearest Neighbour (LLMNN), a hybrid method combining LLM prior knowledge with nearest-neighbor sampling in embedding space, which achieves competitive or superior performance without requiring significant in-context adaptation.

## Method Summary
The paper evaluates LLM-based experimental design using BioDiscoveryAgent with both open-source and closed-source LLMs. The core method is LLM-guided Nearest Neighbour (LLMNN), which prompts an LLM to propose cluster centers in the candidate space, then retrieves nearest unexplored neighbors in embedding space to form batches. The process iterates with feedback from the oracle. Classical baselines include Linear UCB and Gaussian Process optimization. The key ablation is BDA-Rand, where true outcomes are replaced with randomly permuted labels to test feedback sensitivity. Datasets include 5 gene perturbation datasets (>18k genes each) and 3 molecular property datasets, with embeddings from Achilles (genes) and MolFormer-XL (molecules).

## Key Results
- LLMs show no sensitivity to experimental feedback: BDA-Rand performs comparably to BDA across all datasets.
- Classical methods (Linear UCB, GP) consistently outperform pure LLM agents when both have access to the same embeddings.
- LLMNN achieves competitive or superior performance across domains, outperforming random centroid baselines by >50%.
- Claude 3.5 Sonnet BDA underperforms compared to LLMNN with smaller Llama-3.1-8B.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Current instruction-tuned LLMs do not perform genuine in-context experimental design when prompted with experimental history.
- Mechanism: LLMs rely primarily on prior knowledge encoded during pretraining to select candidates, treating appended feedback as passive context rather than updating an implicit posterior to guide subsequent selections.
- Core assumption: The randomization ablation (BDA-Rand) correctly isolates feedback sensitivity by breaking the action-outcome correspondence while preserving marginal distributions.
- Evidence anchors:
  - [abstract] "LLM-based agents show no sensitivity to experimental feedback: replacing true outcomes with randomly permuted labels has no impact on performance."
  - [section 5, Table 1] BDA and BDA-Rand perform comparably across all 5 gene datasets for Llama-3.1-8B, Qwen-2-7B, and Claude 3.5 Sonnet.
  - [corpus] Corpus signals include work on "In-Context Multi-Objective Optimization" (FMR 0.611), suggesting active research on in-context optimization, but this paper finds current LLMs do not achieve this for experimental design.
- Break condition: If LLMs were performing true posterior updating, performance should degrade significantly when feedback is randomized—it does not.

### Mechanism 2
- Claim: Hybrid methods that decouple prior-based reasoning (LLM) from batch acquisition (nearest-neighbor sampling) outperform pure LLM-based or pure classical approaches in scientific experimental design.
- Mechanism: LLM proposes diverse cluster centers using domain priors; nearest-neighbor sampling in embedding space exploits the inductive bias that similar candidates have similar properties, forming coherent batches around promising regions.
- Core assumption: The embedding space meaningfully captures functional similarity (genes/molecules with similar embeddings have similar properties).
- Evidence anchors:
  - [abstract] "LLMNN achieves competitive or superior performance across domains without requiring significant in-context adaptation."
  - [section 7, Table 3] LLMNN (Llama-3.1-8B) outperforms Claude 3.5 Sonnet BDA on 3/4 gene datasets despite using a smaller model and simpler tools.
  - [section 7, Tables 5-6] Ablation with random centroids shows >50% performance drop, confirming LLM guidance is critical.
  - [corpus] Related work on hybrid approaches is limited in the corpus; corpus signals focus on pure BO or pure LLM methods.
- Break condition: If nearest-neighbor sampling alone drove performance, random centroids would match LLM-guided centroids—they do not.

### Mechanism 3
- Claim: Classical methods (Linear UCB, GP) outperform pure LLM agents when both have access to the same embedding-based priors.
- Mechanism: Classical methods explicitly model uncertainty and balance exploration-exploitation via acquisition functions; LLMs lack explicit mechanisms for posterior updating and acquisition, relying on implicit priors without adaptive refinement.
- Core assumption: Embeddings provided to classical methods capture comparable prior information to what LLMs encode internally.
- Evidence anchors:
  - [section 5, Table 2] Linear UCB or GP outperforms BDA on all 5 gene datasets for both Llama-3.1-8B and Qwen-2-7B.
  - [section 7, Table 3] Classical baselines match or exceed Claude 3.5 Sonnet BDA on 4/4 datasets where comparable.
  - [corpus] Corpus includes BO-focused papers (e.g., "Honegumi: An Interface for Accelerating the Adoption of Bayesian Optimization") but limited direct comparisons to LLM agents.
- Break condition: If LLMs effectively used feedback, they should match or exceed classical methods given their richer priors—they do not.

## Foundational Learning

- Concept: Bayesian Optimization (BO) for Experimental Design
  - Why needed here: The paper frames scientific experimental design as a sequential decision problem under budget constraints; understanding BO principles (surrogate models, acquisition functions) is essential to interpret why classical methods outperform LLMs.
  - Quick check question: Can you explain how Expected Improvement balances exploration and exploitation in BO?

- Concept: In-Context Learning in LLMs
  - Why needed here: The paper critically evaluates whether LLMs can adapt behavior based on experimental history appended to prompts without parameter updates.
  - Quick check question: What is the difference between in-context learning (prompt-based adaptation) and fine-tuning (parameter updates)?

- Concept: Bandit Algorithms (Linear UCB)
  - Why needed here: Linear UCB is a key baseline; it models the trade-off between exploring uncertain regions and exploiting known good candidates via confidence bounds.
  - Quick check question: How does the upper confidence bound in Linear UCB encourage exploration of less-sampled candidates?

## Architecture Onboarding

- Component map:
  - Candidate Memory -> LLM Prompt Engine -> Nearest Neighbor Sampler -> Feedback Loop

- Critical path:
  1. Initialize with task instructions and candidate space → 2. LLM proposes nc cluster centers → 3. Retrieve nearest unexplored neighbors → 4. Submit batch to oracle → 5. Update memory with outcomes → 6. Append feedback to prompt → 7. Repeat for N rounds

- Design tradeoffs:
  - Equal budget allocation across centers (simple but potentially suboptimal) vs. adaptive allocation based on hit likelihood (more complex)
  - Pure LLM selection vs. LLM+NN hybrid: hybrid outperforms but is largely exploitative
  - Domain-specific embeddings (Achilles, MolFormer) vs. general-purpose (OpenAI): domain-specific may capture better similarity but reduces generality

- Failure signatures:
  - BDA-Rand matches BDA → LLM is ignoring feedback; consider hybrid or classical methods
  - LLMNN underperforms on molecular datasets → embedding similarity may not capture property-relevant features; consider alternative embeddings or acquisition strategies
  - Classical methods consistently outperform LLM approaches → LLM priors alone insufficient; integrate explicit posterior updating

- First 3 experiments:
  1. Replicate BDA vs. BDA-Rand comparison on a subset of gene datasets to confirm feedback insensitivity in your LLM backbone
  2. Implement LLMNN with your choice of embeddings and compare against Linear UCB and GP baselines on the same datasets
  3. Ablate embedding choice: compare domain-specific (Achilles/MolFormer) vs. general-purpose (OpenAI text-embedding-3-large) to assess sensitivity to embedding quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive budget allocation schemes that use probabilistic models (e.g., Gaussian Processes) to weight cluster centers improve LLMNN's performance over uniform allocation?
- Basis in paper: [explicit] "more complex schemes can be explored to adaptively allocate more budget to centers that have a higher probability of detecting hits. For example, a probabilistic model like GP could be used to determine the hit likelihood"
- Why unresolved: LLMNN currently allocates equal budget to each cluster center regardless of uncertainty or expected value, limiting exploitation of promising regions.
- What evidence would resolve it: Experiments comparing LLMNN with GP-weighted budget allocation on gene perturbation and molecular datasets.

### Open Question 2
- Question: What domain-specific inductive biases beyond nearest-neighbor similarity would improve performance in molecular property optimization tasks?
- Basis in paper: [explicit] "the inductive bias that similar candidates have similar properties is clearly not the best bias on molecular domains, as the classic exploration approaches maintain a strong performance"
- Why unresolved: Classical baselines outperform LLMNN on 2/3 molecular datasets, suggesting the embedding-based similarity assumption is insufficient for chemical space.
- What evidence would resolve it: Systematic comparison of alternative inductive biases (e.g., functional group similarity, scaffold hopping) integrated into hybrid methods.

### Open Question 3
- Question: What architectural or training modifications would enable LLMs to genuinely perform in-context posterior updating for experimental design?
- Basis in paper: [inferred] The paper shows LLMs trained on next-token prediction and RLHF "do not leverage experimental feedback," raising the question of whether different training paradigms could enable true in-context Bayesian reasoning.
- Why unresolved: It remains unclear whether the failure is fundamental to current training objectives or whether specialized fine-tuning could induce proper feedback integration.
- What evidence would resolve it: Evaluation of LLMs trained with explicit Bayesian inference objectives on the randomized-feedback ablation from this paper.

## Limitations

- The randomization ablation assumes breaking action-outcome correspondence fully isolates feedback sensitivity, but potential confounding from implicit prior feedback mechanisms cannot be ruled out.
- Embedding quality's impact on nearest-neighbor sampling effectiveness is not fully characterized; performance drops may reflect embedding inadequacy rather than LLM guidance failure.
- Threshold definitions for gene perturbation hits lack the explicit percentile specification used for molecular datasets, potentially affecting cross-domain comparisons.

## Confidence

- High: Classical methods (Linear UCB, GP) outperform pure LLM agents when both use the same embeddings.
- High: LLMNN's superior performance over random centroids confirms the value of LLM-proposed cluster centers.
- Medium: The claim that current LLMs cannot perform in-context experimental design is well-supported by BDA-Rand results.
- Low: The exact mechanism by which embeddings capture property-relevant similarity is not fully validated.

## Next Checks

1. Implement BDA vs. BDA-Rand ablation on a held-out gene dataset to verify feedback insensitivity in your LLM backbone.
2. Ablate embedding choice (domain-specific vs. general-purpose) within LLMNN to assess sensitivity to embedding quality.
3. Test LLMNN with adaptive budget allocation (probabilistic assignment based on predicted hit likelihood) versus equal allocation to probe whether exploitation is the primary driver of performance.