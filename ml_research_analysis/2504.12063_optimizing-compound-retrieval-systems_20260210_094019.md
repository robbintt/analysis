---
ver: rpa2
title: Optimizing Compound Retrieval Systems
arxiv_id: '2504.12063'
source_url: https://arxiv.org/abs/2504.12063
tags:
- retrieval
- ranking
- compound
- systems
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes compound retrieval systems as a generalization
  of the cascading retrieval paradigm, allowing multiple prediction models to interact
  in novel ways beyond sequential top-K re-ranking. The authors introduce a framework
  that optimizes both the selection of model predictions (via a probabilistic policy)
  and their aggregation (via a learnable scoring function) to balance effectiveness
  and efficiency.
---

# Optimizing Compound Retrieval Systems

## Quick Facts
- arXiv ID: 2504.12063
- Source URL: https://arxiv.org/abs/2504.12063
- Reference count: 40
- One-line primary result: Compound retrieval systems outperform cascading baselines with 10x efficiency gains over PRP while maintaining comparable nDCG

## Executive Summary
This paper introduces compound retrieval systems as a generalization of the cascading retrieval paradigm, enabling multiple prediction models to interact in novel ways beyond traditional sequential top-K re-ranking. The framework optimizes both the selection of model predictions through a probabilistic policy and their aggregation via a learnable scoring function, achieving a balance between effectiveness and efficiency. Applied to combining BM25 with LLM-based relevance predictions, the system discovers novel strategies like selective pairwise comparisons that surpass traditional cascading approaches.

## Method Summary
The compound retrieval framework introduces a probabilistic policy to select which model predictions to use and a learnable scoring function to aggregate these predictions effectively. This approach generalizes cascading retrieval by allowing non-sequential model interactions and optimizing the trade-off between retrieval effectiveness and computational efficiency. The method supports both supervised fine-tuning and self-supervised learning approaches, with experiments demonstrating significant improvements over traditional cascading baselines on the TREC-DL dataset.

## Key Results
- Compound systems achieve 10x efficiency gains over PRP while maintaining comparable nDCG performance
- The framework discovers novel retrieval strategies including selective pairwise comparisons
- Self-supervised optimization closely matches PRP's distillation performance

## Why This Works (Mechanism)
The compound retrieval framework succeeds by moving beyond the limitations of sequential cascading, where each stage must process all candidate documents before passing a subset to the next stage. Instead, it optimizes both which models to use for each document and how to combine their predictions, allowing for more flexible and efficient retrieval strategies. The probabilistic policy enables selective application of expensive models only when beneficial, while the learnable aggregation function can capture complex interactions between different model predictions.

## Foundational Learning
- Probabilistic policies in retrieval: Needed for selective model application to balance cost and effectiveness; quick check: verify policy learns to skip expensive models on clearly non-relevant documents
- Learnable scoring functions: Required to optimally combine heterogeneous model predictions; quick check: ensure aggregation captures complementary strengths of BM25 and LLM predictions
- Self-supervised distillation: Important for training without expensive relevance labels; quick check: compare self-supervised vs supervised performance on held-out data

## Architecture Onboarding

**Component Map:** BM25 scores -> Probabilistic selection policy -> LLM predictions (pointwise/pairwise) -> Learnable aggregation function -> Final ranking

**Critical Path:** Document retrieval via BM25 → Model selection policy → LLM scoring (when selected) → Aggregated scoring → Ranked results

**Design Tradeoffs:** The framework trades implementation simplicity for flexibility and potential performance gains. While traditional cascading is straightforward to implement and debug, the compound approach requires optimizing additional hyperparameters and managing more complex interactions between components.

**Failure Signatures:** Poor performance may manifest as either: 1) The selection policy consistently choosing expensive models, negating efficiency gains, or 2) The aggregation function failing to properly combine heterogeneous predictions, resulting in suboptimal rankings despite individual model quality.

**First Experiments:**
1. Compare compound system against simple cascading baseline with identical underlying models
2. Test different model selection strategies (random vs learned policy) with fixed aggregation
3. Evaluate the impact of self-supervised vs supervised learning on final retrieval performance

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability beyond TREC-DL dataset remains uncertain
- Training process may be computationally intensive despite inference efficiency gains
- Framework relies on relatively standard model combinations rather than novel underlying models

## Confidence

**High Confidence:**
- Compound retrieval framework is mathematically sound and implementable
- Method successfully optimizes both model selection and aggregation
- Framework supports both supervised and self-supervised learning approaches
- Experimental results on TREC-DL are reproducible

**Medium Confidence:**
- 10x efficiency gains over PRP maintain across different datasets
- Novel strategies generalize beyond specific experimental setup
- Self-supervised optimization consistently matches PRP's distillation performance
- Framework effectiveness translates to production environments

**Low Confidence:**
- Framework outperforms all possible cascading baseline variations
- Compound approach always discovers superior strategies to human-designed pipelines
- Method's benefits scale linearly with number of combined models
- Framework addresses all challenges in compound AI system optimization

## Next Checks
1. **Cross-dataset validation**: Evaluate on multiple IR datasets (MS MARCO, Robust04, ClueWeb) to assess generalizability across domains and document collections

2. **Ablation studies on optimization components**: Systematically disable model selection policy or aggregation function separately to quantify individual contributions and test framework benefits with different underlying models

3. **Production deployment simulation**: Create realistic simulation accounting for variable query loads, model serving costs, and latency constraints to evaluate practical deployment scenarios