---
ver: rpa2
title: Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start
  Scenarios
arxiv_id: '2507.00330'
source_url: https://arxiv.org/abs/2507.00330
tags:
- selection
- instances
- select
- instance
- verbalizer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of selecting both verbalizers
  and few-shot instances in cold-start settings where no labeled data exists, aiming
  to improve prompt-based learning performance. The proposed method, COLD SELECT,
  jointly selects verbalizers and instances by modeling data diversity through shared
  embedding spaces, clustering, and a selection-based optimization framework that
  balances cohesion, separation, and impurity.
---

# Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios

## Quick Facts
- **arXiv ID:** 2507.00330
- **Source URL:** https://arxiv.org/abs/2507.00330
- **Reference count:** 40
- **Primary result:** COLD SELECT improves prompt-based learning accuracy in cold-start settings by jointly selecting instances and verbalizers through diversity modeling.

## Executive Summary
This paper introduces COLD SELECT, a method for joint instance and verbalizer selection in cold-start prompt-based learning. The approach addresses the challenge of selecting both few-shot instances and verbalizer tokens without any labeled data. COLD SELECT leverages a shared embedding space to model the dependency between instances and verbalizers, then applies dimensionality reduction, clustering, and a selection optimization framework to maximize data diversity and minimize labeling uncertainty. Experiments on eight benchmark datasets demonstrate improved accuracy over existing methods, particularly in low-resource scenarios.

## Method Summary
COLD SELECT operates by first extracting embeddings for both instances (using h[MASK] from a PLM) and vocabulary tokens (using pre-softmax outputs). These embeddings are combined and reduced using PCA, then clustered with KMeans and refined using negative silhouette loss. The method then iteratively selects clusters based on a scoring function that balances cohesion, separation, and impurity. After labeling the most representative instance in each selected cluster, the verbalizer token is determined as the nearest vocabulary token. This process continues until the labeling budget is exhausted, producing both a set of labeled instances and corresponding verbalizer tokens for each class.

## Key Results
- COLD SELECT achieves up to 1.9% higher accuracy than state-of-the-art methods like PATRON on IMDB with smaller labeling budgets
- The method requires fewer labeled instances to reach competitive accuracy compared to existing approaches
- COLD SELECT demonstrates effectiveness across diverse tasks including sentiment analysis, news classification, and topic classification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Jointly selecting instances and verbalizers in a shared embedding space improves prompt-based learning by leveraging their dependency.
- **Mechanism:** Classification probability depends on the dot product between h[MASK] and verbalizer token embeddings. When these are close in the shared space, classification is more confident. COLD SELECT maps both into the same space, enabling joint selection that respects this geometric relationship.
- **Core assumption:** Pre-softmax token embeddings and h[MASK] embeddings from the same PLM occupy comparable semantic spaces.
- **Evidence anchors:** [abstract] Existing studies overlook the dependency between instances and verbalizers; [Section 4.2] Eq. (2) shows classification probability is maximized when verbalizer and h[MASK] embeddings have high cosine similarity.

### Mechanism 2
- **Claim:** Dimensionality reduction followed by clustering and silhouette-based refinement captures data diversity.
- **Mechanism:** PCA reduces high-dimensional embeddings to preserve variance. KMeans groups similar embeddings. Negative silhouette loss optimizes for well-separated, cohesive clusters. Token-only clusters are discarded; instance-only clusters are reassigned.
- **Core assumption:** The data distribution contains cluster structure that aligns with class boundaries.
- **Evidence anchors:** [Section 4.3] Dimensionality reduction using PCA enhances computational efficiency; [Table 5] Cohesion + separation + impurity together yield 87.37% on IMDB vs. 80.50% for cohesion alone.

### Mechanism 3
- **Claim:** Minimizing labeling uncertainty by balancing cohesion, separation, and impurity optimizes budget usage.
- **Mechanism:** At each step, COLD SELECT scores clusters by E[cohesion + separation + impurity]. High-cohesion clusters are prioritized early. High-separation avoids redundancy. High-impurity clusters are targeted for annotation to reduce uncertainty.
- **Core assumption:** Dense, well-separated clusters correlate with predictable labeling outcomes.
- **Evidence anchors:** [abstract] By optimizing for minimal uncertainty and maximal diversity; [Section 4.4] Eq. (11-16) define the selection policy; [Table 2-4] COLD SELECT achieves up to 1.9% higher accuracy than PATRON on IMDB.

## Foundational Learning

- **Concept:** Prompt-based learning with masked language models (MLMs)
  - **Why needed here:** COLD SELECT operates on PLMs trained with MLM objective. You must understand how templates transform inputs into cloze-style tasks and how verbalizers map predicted tokens to class labels.
  - **Quick check question:** Given a sentiment template `<S>. It was [MASK].`, what does the model predict at the [MASK] position for a positive review, and how does a verbalizer convert that to a class label?

- **Concept:** Clustering validity metrics (silhouette score, cohesion, separation)
  - **Why needed here:** The method uses negative silhouette loss for cluster refinement and defines its own cohesion/separation metrics for selection.
  - **Quick check question:** For a point i in cluster C, what does a silhouette score near +1 indicate vs. near -1?

- **Concept:** Active learning under cold-start constraints
  - **Why needed here:** COLD SELECT is fundamentally a cold-start active learning method with a fixed labeling budget.
  - **Quick check question:** In a cold-start setting with no labeled data, why might pure uncertainty sampling (e.g., entropy-based) fail to select useful instances?

## Architecture Onboarding

- **Component map:** Embedding Extraction -> Dimensionality Reduction -> Clustering -> Cluster Refinement -> Selection Loop -> Output
- **Critical path:** Embedding quality → PCA variance retention → cluster-label alignment → selection metric design → labeling policy correctness
- **Design tradeoffs:**
  - K choice: Too few clusters → coarse selection; too many → sparse clusters. Paper uses K=40 but does not justify.
  - PCA components: Must preserve enough variance to separate classes; paper does not specify number.
  - Static vs. dynamic centroids: Dynamic updates adapt to label information but introduce path-dependency.
- **Failure signatures:**
  - Token-only clusters dominate: Indicates embeddings are not well-aligned between instances and vocabulary.
  - No accuracy gain over Random-g: Suggests cohesion/separation/impurity scoring is not informative.
  - Budget exhaustion with low accuracy on diverse datasets: May indicate multi-class impurity is harder to reduce.
- **First 3 experiments:**
  1. Reproduce IMDB baseline (B=32) with RoBERTa-base: Compare accuracy against Random and PATRON. Verify that your clustering produces mixed clusters.
  2. Ablate one metric at a time: Run cohesion-only, cohesion+separation, cohesion+impurity, and full (cohesion+separation+impurity) on IMDB.
  3. Visualize clusters before and after refinement: Project PCA-reduced embeddings to 2D (e.g., t-SNE) and color by true labels.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive non-linear transformations (e.g., kernel PCA) improve cluster separability over the linear PCA method used in COLD SELECT?
- **Basis in paper:** [explicit] Section 7 states that PCA assumes linear separability which may introduce biases, and suggests exploring kernel PCA or deep representation learning.
- **Why unresolved:** The current methodology relies on linear projection, which may fail to disentangle complex, non-linear relationships in high-dimensional PLM embedding spaces.
- **What evidence would resolve it:** Empirical comparisons showing improved accuracy and silhouette scores when using non-linear reduction techniques versus standard PCA on the same benchmarks.

### Open Question 2
- **Question:** Can alternative clustering methods like DBSCAN or hierarchical clustering enhance robustness compared to the KMeans approach?
- **Basis in paper:** [explicit] Section 7 notes that KMeans depends on initialization and may not capture complex structures, suggesting hierarchical clustering or DBSCAN as alternatives.
- **Why unresolved:** KMeans assumes spherical clusters of similar size; if the data diversity modeling requires capturing irregularly shaped clusters, current performance may be suboptimal.
- **What evidence would resolve it:** Ablation studies comparing the convergence speed and classification accuracy of COLD SELECT when KMeans is swapped with density-based clustering algorithms.

### Open Question 3
- **Question:** Is COLD SELECT effective in multilingual and multimodal cold-start scenarios?
- **Basis in paper:** [explicit] Section 6 and Section 7 explicitly identify extending the method to multilingual and multimodal datasets as a future research direction.
- **Why unresolved:** The current experiments are restricted to English text classification.
- **What evidence would resolve it:** Results from applying the framework to multilingual benchmarks (e.g., XNLI) or multimodal tasks.

## Limitations
- **Linear PCA assumption:** The method assumes linear separability which may not hold for complex data distributions
- **KMeans initialization sensitivity:** The clustering approach depends on KMeans initialization, which may not capture complex cluster structures
- **English text limitation:** Current experiments are restricted to English text classification, leaving multilingual and multimodal effectiveness unexplored

## Confidence

| Claim | Evidence Level |
|-------|----------------|
| Joint instance and verbalizer selection improves accuracy | High - Demonstrated across 8 benchmark datasets |
| Diversity modeling through clustering is effective | Medium - Supported by ablation studies but limited to KMeans |
| Cold-start scenario performance | High - Explicit comparison against baselines in zero-shot setting |

## Next Checks

1. **Reproduce the IMDB baseline** with B=32 labeling budget using RoBERTa-base and verify that COLD SELECT outperforms Random and PATRON baselines by the claimed margin.
2. **Validate the clustering quality** by visualizing the PCA-reduced embeddings before and after refinement, checking for cluster-to-class alignment.
3. **Test the sensitivity to K** by running experiments with different numbers of clusters (e.g., K=20, 40, 60) to understand the impact on final accuracy.