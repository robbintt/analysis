---
ver: rpa2
title: Tail Distribution of Regret in Optimistic Reinforcement Learning
arxiv_id: '2511.18247'
source_url: https://arxiv.org/abs/2511.18247
tags:
- regret
- tail
- bounds
- algorithm
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the tail distribution of regret in episodic
  tabular reinforcement learning with a UCBVI-type algorithm. The authors derive instance-dependent
  sub-Gaussian and sub-Weibull tail bounds for the cumulative regret over K episodes,
  going beyond standard expected regret or single high-probability quantile bounds.
---

# Tail Distribution of Regret in Optimistic Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.18247
- Source URL: https://arxiv.org/abs/2511.18247
- Reference count: 12
- One-line primary result: Derives instance-dependent sub-Gaussian and sub-Weibull tail bounds for cumulative regret in episodic tabular RL with UCBVI-type algorithm

## Executive Summary
This paper analyzes the tail distribution of regret in episodic tabular reinforcement learning with a UCBVI-type algorithm. The authors derive instance-dependent sub-Gaussian and sub-Weibull tail bounds for cumulative regret over K episodes, going beyond standard expected regret or single high-probability quantile bounds. They study two exploration-bonus schedules: one depending on the total number of episodes K, and one depending only on the current episode index. For both settings, the regret exhibits a two-regime structure: sub-Gaussian tails for moderate thresholds and sub-Weibull tails for larger deviations.

## Method Summary
The method analyzes a UCBVI-type algorithm in finite-horizon tabular MDPs with unknown transitions. The algorithm maintains optimistic Q-value estimates using exploration bonuses that scale as ‚àö((S ln 2 + ŒºK^Œ±)/n) for K-dependent or ‚àö((S ln 2 + Œº(k+1)^Œ±)/n) for K-independent variants. The analysis decomposes regret into a burn-in phase and a concentration phase, applying martingale concentration techniques to derive two-regime tail bounds. The key parameter Œ± ‚àà [0,1] controls the tradeoff between expected regret and tail decay rate.

## Key Results
- Regret exhibits two-regime structure: sub-Gaussian tails for moderate deviations and sub-Weibull tails beyond an instance-dependent threshold
- Expected regret scales as √ï(H‚Å∂SA(S+K^Œ±)/gap*) where gap* is the global Q*-gap
- K-dependent bonuses yield simpler exponential decay exp(-√ï(K^Œ±)) while K-independent bonuses are anytime but have more complex thresholds
- Transition thresholds identified as √ï(H^{5/2}K^{(1+Œ±)/2}) for K-dependent and √ï(H^{(5-Œ±)/(2-Œ±)}K^{1/(2-Œ±)}) for K-independent

## Why This Works (Mechanism)

### Mechanism 1: Two-Regime Tail Decomposition
The regret distribution exhibits sub-Gaussian tails for moderate deviations and transitions to sub-Weibull tails beyond an instance-dependent threshold. The proof decomposes regret R_K = R_K^(0) + R_K^(1) into a burn-in phase (bounded trivially) and a concentration phase. Under the "good event" GÃÑ_K, the centered regret RÃÉ_K = R_K^(1) - m_K^(mb) is a sum of bounded martingale differences with |Œ∑ÃÑ_{h+1}^k| ‚â§ 5H¬≤. Azuma-Hoeffding yields sub-Gaussian concentration with variance proxy ~50H‚ÅµK. Beyond the transition threshold, the residual "bad event" probability Œ¥_K^(mb)(Œ≥) dominates, decaying as exp(-√ï(K^Œ±)) for K-dependent bonuses or exp(-√ï(x^Œ±/H^Œ±)) for K-independent bonuses‚Äîprecisely sub-Weibull decay.

### Mechanism 2: Aggressive Exploration Bonuses Enable Tail Control
Bonuses scaling as ‚àö((S ln 2 + ŒºK^Œ±)/n) rather than the standard ‚àö(ln(K)/n) enable non-trivial tail bounds at the cost of polynomial dependence on K in expected regret. The K^Œ± term acts as a "global exploration budget"‚Äîlarger Œ± makes bonuses persist even after moderate visitation counts, ensuring sustained exploration. This directly controls the failure probability Œ¥_K^(mb)(Œ≥) ‚àù exp(-2ŒºK^Œ±) via Weissman's inequality applied to transition kernel estimation. The tradeoff: expected regret m_K^(mb) ‚àù K^Œ±/gap* increases with Œ±, but tail decay accelerates.

### Mechanism 3: Good-Event Conditioning with Martingale Residual
The tail probability admits a clean decomposition Pr(R_K ‚â• x) ‚â§ Pr((GÃÑ_K)^c) + Pr(R_K^(1)ùüô{GÃÑ_K} ‚â• x - H‚åàK^Œ≥‚åâ) separating model estimation failures from policy regret fluctuations. Define the "good event" G_k = ‚à©_{h,s,a}{||ƒ•P_h^k(¬∑|s,a) - P_h^*(¬∑|s,a)||_1 ‚â§ 2b_h^k(n_h^k(s,a))/(H-h-1)}. Under G_k, optimism holds: Q_h^k(s,a) ‚â• Q_h^*(s,a) by backward induction. The bad event probability is bounded via union bound + Weissman concentration over all (s,a,h,k). Conditioned on GÃÑ_K = ‚à©_k G_k, the regret decomposes into the deterministic baseline m_K^(mb) (from summing clipped bonuses) minus a martingale term, enabling Azuma-Hoeffding application.

## Foundational Learning

- **Sub-Gaussian vs. Sub-Weibull distributions**
  - Why needed here: The paper's core result is characterizing when regret tails transition from sub-Gaussian (Pr(X ‚â• x) ‚â§ exp(-cx¬≤)) to sub-Weibull (Pr(X ‚â• x) ‚â§ exp(-cx^Œ∏) for Œ∏ < 2). Understanding this distinction is essential for interpreting Corollary 1.
  - Quick check question: If Pr(R_K ‚â• x) ‚â§ exp(-cx^{1.5}), is this sub-Gaussian, sub-Weibull, or neither?

- **Optimism in UCBVI (Upper Confidence Bound Value Iteration)**
  - Why needed here: Algorithm 1 computes Q_h^k(s,a) = min{r_h(s,a) + ƒ•P_h^k V_{h+1}^k + b_h^k(n), H-h} where the bonus ensures Q_h^k ‚â• Q_h^* with high probability. This is the foundation enabling gap-dependent regret analysis.
  - Quick check question: Why does the min operator clip at H-h rather than leaving the sum unbounded?

- **Gap-dependent regret bounds**
  - Why needed here: The instance-dependent baseline m_K^(mb) ‚àù 1/gap* means regret scales inversely with the minimum gap between optimal and suboptimal actions. Larger gaps ‚Üí faster identification ‚Üí lower regret.
  - Quick check question: In a bandit with arm means [0.9, 0.5], what is the gap? How does regret scale with this gap?

## Architecture Onboarding

- **Component map**: Bonus scheduler -> Empirical model -> Optimistic planner -> Policy executor -> Visit counter
- **Critical path**: Bonus scale Œ± ‚Üí exploration aggressiveness ‚Üí bad-event probability Œ¥_K^(mb) ‚Üí tail decay rate. The Œ± parameter is the single tuning knob controlling the expected-regret/tail-decay Pareto frontier.
- **Design tradeoffs**:
  - K-dependent vs. K-independent bonuses: K-dependent requires knowing total episodes K in advance (impractical for streaming) but yields simpler exp(-√ï(K^Œ±)) tails; K-independent is anytime but has more complex threshold √ï(H^{(5-Œ±)/(2-Œ±)}K^{1/(2-Œ±)})
  - Œ± selection: Œ± ‚Üí 0 minimizes expected regret but yields heavy tails; Œ± ‚Üí 1 gives lightest tails but √ï(K) expected regret. Paper does not prescribe optimal Œ±‚Äîcontext-dependent
- **Failure signatures**:
  - Optimism failure: If Pr((GÃÑ_K)^c) is underestimated (e.g., bonus constant Œº too small), the algorithm may select suboptimal actions confidently, causing regret spikes beyond predicted tails
  - Gap violation: If gap* is misspecified (true gap smaller than assumed), m_K^(mb) underestimates baseline, shifting the entire tail distribution rightward
  - Numerical instability: When n_h^k(s,a) = 0, bonus is set to ‚àû (Line 3), ensuring exploration of unvisited state-action pairs
- **First 3 experiments**:
  1. Validate two-regime structure on a simple MDP: Construct a 3-state, 2-action MDP with known gap*. Run Algorithm 1 with Œ± ‚àà {0.25, 0.5, 0.75} for K=10^4 episodes, repeat 1000 times. Plot empirical Pr(R_K ‚â• x) vs. theoretical bounds from Corollary 1. Verify sub-Gaussian decay up to threshold, then regime change.
  2. Ablate bonus schedules: Compare K-dependent vs. K-independent bonuses on the same MDP. Measure: (a) empirical expected regret, (b) tail decay rate via log-log regression on Pr(R_K ‚â• x), (c) transition threshold location. Confirm K-independent achieves comparable tails without advance knowledge of K.
  3. Sensitivity to gap misspecification: Run with assumed gap*_assumed varying from 0.5 √ó gap*_true to 2 √ó gap*_true. Plot calibration: theoretical Pr(R_K ‚â• x) vs. empirical frequency. Identify regime where bounds remain valid (should be when gap*_assumed ‚â§ gap*_true).

## Open Questions the Paper Calls Out

- **Are the derived sub-Gaussian and sub-Weibull tail bounds tight, or can they be improved with a sharper analysis?**
  - Basis in paper: The paper states "We derive instance-dependent sub-Gaussian and sub-Weibull tail bounds" but only provides upper bounds without corresponding lower bounds.
  - Why unresolved: No information-theoretic lower bounds on the tail distribution are established to certify optimality of the rates.
  - What evidence would resolve it: A matching lower bound showing that for some MDP instance, the regret tail must be at least sub-Gaussian/sub-Weibull with the same exponents and transition thresholds.

- **Can tail-regret guarantees be extended to algorithms beyond UCBVI, such as Thompson Sampling, PSRL, or model-free methods like optimistic Q-learning?**
  - Basis in paper: The authors note their work is "one of the first comprehensive tail-regret guarantees for a standard optimistic algorithm in episodic reinforcement learning" and analyze only a UCBVI-type algorithm.
  - Why unresolved: Different exploration mechanisms (posterior sampling vs. optimism) may induce fundamentally different tail behaviors.
  - What evidence would resolve it: Tail bounds for Thompson Sampling or Q-learning, potentially revealing different tail structures than the two-regime sub-Gaussian/sub-Weibull pattern.

- **What is the tail behavior of regret in MDPs where the global Q*-gap is arbitrarily small or zero?**
  - Basis in paper: The analysis critically depends on gap* > 0, and the expected regret bound √ï(H‚Å∂SA(S+K^Œ±)/gap*) grows unboundedly as gap* ‚Üí 0.
  - Why unresolved: The current instance-dependent analysis provides no meaningful bounds for near-gapless MDPs, yet many practical problems have small gaps.
  - What evidence would resolve it: Gap-free or minimax tail bounds that degrade gracefully when gap* is small, or a demonstration that heavy tails emerge in gap-free settings.

## Limitations
- The analysis critically depends on the global Q*-gap assumption (gap* > 0), which may not hold in problems with multiple optimal policies yielding identical values
- The paper provides limited empirical validation‚Äîno concrete MDP instances or experimental results are included to verify the theoretical tail bounds or calibrate the constants
- The bonus inflation parameter Œº is only required to be positive, with no guidance on optimal selection for balancing exploration and tail control in practice

## Confidence
- **High Confidence**: The mechanism of good-event conditioning with martingale decomposition (Mechanism 3) and the derivation of sub-Gaussian tails via Azuma-Hoeffding (Corollary 1 for moderate deviations) are standard techniques in bandit and RL theory, well-established in the literature
- **Medium Confidence**: The two-regime tail structure and transition thresholds (Mechanism 1) are mathematically sound given the assumptions, but their practical relevance depends on whether the specified MDPs actually exhibit the required gap* and whether the constants are tight
- **Low Confidence**: The claim that K-independent bonuses achieve comparable tail bounds without advance knowledge of K (Mechanism 2) is theoretically derived but lacks empirical verification. The practical tradeoff between Œ± selection and performance is not quantified

## Next Checks
1. **Validate Two-Regime Structure**: Implement Algorithm 1 on a simple chain MDP with known gap*. Run 1000 trials, plot empirical Pr(R_K ‚â• x) vs. x, and verify the transition from sub-Gaussian to sub-Weibull decay at the predicted threshold.
2. **Ablate Bonus Schedules**: Compare K-dependent vs. K-independent bonuses on the same MDP. Measure empirical expected regret, tail decay rate (via log-log regression), and transition threshold location to confirm K-independent achieves comparable tails without advance K knowledge.
3. **Sensitivity to Gap Misspecification**: Run with assumed gap* values varying from 0.5√ó to 2√ó the true gap*. Plot calibration: theoretical Pr(R_K ‚â• x) vs. empirical frequency. Identify the regime where bounds remain valid (when assumed gap* ‚â§ true gap*).