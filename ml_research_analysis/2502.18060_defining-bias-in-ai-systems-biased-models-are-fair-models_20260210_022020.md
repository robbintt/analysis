---
ver: rpa2
title: 'Defining bias in AI-systems: Biased models are fair models'
arxiv_id: '2502.18060'
source_url: https://arxiv.org/abs/2502.18060
tags:
- bias
- fairness
- what
- mitigation
- term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper clarifies the ambiguous use of the term "bias" in AI
  discussions by distinguishing between technical bias (an architectural concept in
  neural networks) and social bias (unequal treatment based on outcome-irrelevant
  characteristics). It argues that "unbiased" does not equate to "fair," as true fairness
  requires accommodating individual differences rather than enforcing uniform treatment.
---

# Defining bias in AI-systems: Biased models are fair models

## Quick Facts
- arXiv ID: 2502.18060
- Source URL: https://arxiv.org/abs/2502.18060
- Reference count: 7
- Key outcome: This paper clarifies the ambiguous use of the term "bias" in AI discussions by distinguishing between technical bias (an architectural concept in neural networks) and social bias (unequal treatment based on outcome-irrelevant characteristics).

## Executive Summary
This position paper argues that the term "bias" in AI discourse conflates three distinct concepts—architectural bias (a technical parameter in neural networks), statistical bias (from the bias-variance tradeoff), and social bias (discrimination based on irrelevant characteristics). The authors contend that conflating these meanings undermines effective fairness interventions. They propose that true fairness requires accommodating individual differences rather than enforcing uniform treatment, arguing that "unbiased" does not equate to "fair." The paper distinguishes between harmful discrimination (Capital D Discrimination) based on outcome-irrelevant features and equitable differentiation based on outcome-relevant characteristics.

## Method Summary
This is a conceptual position paper that synthesizes existing literature to clarify the terminology around bias in AI systems. The authors use illustrative examples (facial recognition disparities, children's curriculum design, cucumber vs. apple classification) to demonstrate their framework. They propose a three-way taxonomy distinguishing technical bias, discrimination, and equitable differentiation, but provide no empirical methodology or validation data. The approach is primarily theoretical argumentation supported by literature references.

## Key Results
- Technical bias in neural networks (the constant term in neuron computations) is a neutral architectural concept, not inherently harmful
- "Unbiased" models can produce unfair outcomes when they ignore outcome-relevant differences between individuals or groups
- The distinction between fair and unfair differentiation depends on whether classification is based on outcome-relevant versus outcome-irrelevant characteristics

## Why This Works (Mechanism)

### Mechanism 1: Semantic Disambiguation of "Bias"
- Claim: Conflating technical and social definitions of "bias" undermines effective fairness interventions.
- Mechanism: The paper separates "bias" into three distinct concepts—(1) architectural bias (a neutral constant term that shifts decision boundaries in neural networks), (2) statistical bias (from the bias-variance tradeoff), and (3) sociological bias (preferential treatment impacting outcomes). By treating these as analytically distinct, practitioners can target interventions appropriately.
- Core assumption: Clarifying terminology will lead to more effective mitigation strategies rather than treating all "bias" as uniformly harmful.

### Mechanism 2: Relevance-Based Differentiation Criterion
- Claim: The ethical valence of differentiation depends on whether classification dimensions are outcome-relevant.
- Mechanism: The paper proposes that "discriminating between" (value-neutral classification) becomes "Capital D Discrimination" when distinctions rely on irrelevant characteristics. The cucumber/apple example illustrates that using relevant features (color, shape) for classification is ethically unproblematic, while using irrelevant features (e.g., gender for hiring when only qualifications matter) constitutes harmful discrimination.
- Core assumption: There exists objective, task-specific criteria for determining "relevance" that stakeholders can agree upon.

### Mechanism 3: Equity-as-Differentiated Treatment
- Claim: True fairness (equity) requires accommodating differences rather than applying uniform treatment.
- Mechanism: The paper argues that an unbiased model treating all inputs equally can produce unfair outcomes. Using the children's curriculum example, even a perfectly representative dataset produces a "one-size-fits-all" standard that disadvantages children with different needs. Equity requires recognizing outcome-relevant differences and adjusting treatment accordingly.
- Core assumption: Accommodating differences improves outcomes for disadvantaged groups without creating new forms of unfairness.

## Foundational Learning

- Concept: **Architectural Bias in Neural Networks**
  - Why needed here: The paper's central argument requires understanding that "bias" in ML is a neutral technical term—a constant added to weighted inputs before activation—distinct from social meanings.
  - Quick check question: Can you explain why removing the bias term from a neural network would impair its ability to learn certain decision boundaries?

- Concept: **Equity vs. Equality Distinction**
  - Why needed here: The paper's normative claim hinges on differentiating "same treatment" (equality) from "fair treatment accounting for different needs" (equity).
  - Quick check question: Give an example where equal treatment produces unequal outcomes—would adding accommodation constitute "good bias" under this framework?

- Concept: **Sampling Bias and Representativeness**
  - Why needed here: The paper uses sampling bias examples to show that even representative, "unbiased" data can produce unfair models if the underlying framework ignores relevant differences.
  - Quick check question: If a dataset is perfectly representative of a population, can the resulting model still produce unfair outcomes? Under what conditions?

## Architecture Onboarding

- Component map:
  - Technical Bias Layer (b term in neuron computations)
  - Relevance Determination Module (identifies outcome-relevant vs. irrelevant features)
  - Differentiation/Discrimination Classifier (distinguishes fair vs. unfair differentiation)
  - Equity Adjustment Layer (applies differentiated treatment based on relevant differences)

- Critical path:
  1. Define task and intended outcomes explicitly
  2. Identify which input features are outcome-relevant (requires domain expertise)
  3. Ensure model can differentiate based on relevant features
  4. Detect if model uses irrelevant features for classification
  5. Implement accommodations for outcome-relevant differences where equity demands it

- Design tradeoffs:
  - **Precision vs. Contested Relevance**: Formally defining "relevant" features enables automated fairness checks but may miss context-dependent nuances
  - **Equal Treatment vs. Equity**: Uniform treatment is simpler to implement but may perpetuate disadvantages; differentiated treatment is more complex and risks perceived unfairness
  - **Technical Mitigation vs. Social Intervention**: Technical debiasing cannot address structural inequities in what data represents

- Failure signatures:
  - **Conflation failure**: Treating all "bias" as harmful leads to removing legitimate differentiation (e.g., blinding models to relevant features)
  - **Relevance misclassification**: Incorrectly labeling outcome-relevant features as "sensitive" prevents equitable accommodation
  - **False equality**: Achieving statistical parity across groups while ignoring that different groups have different baseline needs

- First 3 experiments:
  1. **Vocabulary Audit**: Document all uses of "bias" in your codebase, documentation, and discussions—categorize each as technical, statistical, or social. Flag conflation instances.
  2. **Relevance Mapping**: For a specific model task, explicitly list (a) outcome-relevant features, (b) irrelevant features that could cause harm if used, and (c) features whose relevance is contested. Validate with domain experts.
  3. **Equality-vs-Equity Probe**: Take a model trained for "equal treatment" and identify one group that would benefit from differentiated accommodation. Simulate what "good bias" (equity adjustment) would look like and measure outcome changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can "outcome-relevant" versus "outcome-irrelevant" characteristics be operationally defined and detected in AI systems?
- Basis in paper: The authors propose that the distinction between harmful discrimination and equitable differentiation "is whether the distinction in question is made along relevant or irrelevant dimensions," but provide no method for determining relevance in practice.
- Why unresolved: The paper establishes the conceptual distinction but offers no systematic criteria, thresholds, or detection methods for identifying which features are legitimately relevant to a given task.
- What evidence would resolve it: A validated framework or rubric that can classify features as relevant/irrelevant for specific AI applications, tested across multiple domains with inter-rater reliability metrics.

### Open Question 2
- Question: How can bias mitigation strategies be redesigned to preserve "equitable differentiation" (good bias) while eliminating discriminatory bias?
- Basis in paper: The paper argues that "efforts that conflate technical and social bias or treat fairness merely as 'equal treatment' risk perpetuating inequalities rather than correcting them."
- Why unresolved: Current mitigation approaches (e.g., the 17 methods evaluated in Chen et al. 2023) aim to equalize treatment across groups, which the authors argue may undermine true equity.
- What evidence would resolve it: Comparative studies showing that mitigation methods preserving outcome-relevant distinctions produce measurably fairer outcomes for marginalized groups than current equalization approaches.

### Open Question 3
- Question: What metrics can distinguish between harmful discrimination and fair differentiation in AI model outputs?
- Basis in paper: The paper's conclusion calls for "distinguishing between harmful discrimination and equitable differentiation," yet no measurement approach is proposed for this distinction in deployed systems.
- Why unresolved: Existing fairness metrics (demographic parity, equalized odds) measure statistical parity, not whether differential treatment is justified by relevant factors.
- What evidence would resolve it: Development and validation of evaluation metrics that correlate with human judgments of equitable vs. discriminatory outcomes across diverse scenarios.

## Limitations
- The paper provides no operational definition or methodology for determining whether a feature is "outcome-relevant" versus "outcome-irrelevant," which is central to distinguishing fair from unfair discrimination.
- No empirical validation is provided to show that this conceptual distinction improves fairness outcomes compared to existing technical bias mitigation approaches.
- The framework doesn't address how to handle cases where relevance determination itself is contested or changes across contexts.

## Confidence
- **High confidence**: The technical distinction between architectural bias (neutral parameter) and social bias (harmful discrimination) is well-established and clearly articulated
- **Medium confidence**: The equity-vs-equality framework and its implications for fairness interventions are logically sound but lack empirical support
- **Low confidence**: The operationalization of "outcome-relevant features" as a criterion for distinguishing fair from unfair differentiation, given the absence of clear methodology

## Next Checks
1. **Relevance Determination Validation**: Apply the framework to three diverse AI systems (e.g., hiring, loan approval, medical diagnosis) and have domain experts independently assess whether the "outcome-relevant" vs "outcome-irrelevant" classification aligns with actual fairness concerns
2. **Empirical Impact Assessment**: Compare model outcomes when applying this framework's guidance versus standard technical bias mitigation—measure whether equity-adjusted models produce better outcomes for disadvantaged groups without creating new inequities
3. **Stakeholder Perception Study**: Survey affected populations about whether differentiated treatment based on "outcome-relevant" features is perceived as fair or discriminatory, testing the social acceptability of the equity framework