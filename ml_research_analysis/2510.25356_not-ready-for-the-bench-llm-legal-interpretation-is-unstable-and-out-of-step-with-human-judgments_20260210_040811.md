---
ver: rpa2
title: 'Not ready for the bench: LLM legal interpretation is unstable and out of step
  with human judgments'
arxiv_id: '2510.25356'
source_url: https://arxiv.org/abs/2510.25356
tags:
- judgment
- question
- human
- legal
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates whether large language models (LLMs) can\
  \ reliably determine the \u2018ordinary meaning\u2019 of legal terms as part of\
  \ legal interpretation. Using 138 insurance contract scenarios, the authors query\
  \ 15 models across 9 systematically varied question formats."
---

# Not ready for the bench: LLM legal interpretation is unstable and out of step with human judgments

## Quick Facts
- arXiv ID: 2510.25356
- Source URL: https://arxiv.org/abs/2510.25356
- Authors: Abhishek Purushothama; Junghyun Min; Brandon Waldon; Nathan Schneider
- Reference count: 29
- Primary result: Models show high instability across prompt variants and poor correlation with human judgments, except for largest instruction-tuned models

## Executive Summary
This study investigates whether large language models can reliably determine the 'ordinary meaning' of legal terms in insurance contracts. Using 138 real-world insurance scenarios and 15 different models, the authors systematically test model performance across nine question variants. The research reveals that current LLMs are highly unstable and poorly aligned with human legal interpretation, with many models showing little sensitivity to scenario content and giving identical answers regardless of context.

The findings demonstrate that model outputs vary dramatically across both model type and question phrasing, with only the largest instruction-tuned models achieving moderate correlation with human judgments. However, this correlation is highly dependent on specific question variants and often unreliable. The study concludes that current LLMs are not ready for deployment in legal interpretation tasks due to their instability and misalignment with human judgment.

## Method Summary
The researchers used 138 real-world insurance contract scenarios from a 2023 legal interpretation study. They tested 15 different LLMs across nine systematically varied question formats designed to probe different aspects of legal interpretation. The evaluation measured model consistency by testing how judgments shifted across question variants, and compared model outputs to human judgments using correlation metrics. The study employed fixed temperature=0 settings and focused exclusively on binary-choice question-answering tasks, using the best-performing model version for each system based on MMLU benchmarks.

## Key Results
- Models showed extreme instability, with many giving identical answers across different scenarios
- Only the largest instruction-tuned models achieved moderate correlation with human judgment (R² up to 0.60)
- Model judgments varied widely across both model type and question phrasing
- Token probabilities were often biased or misaligned with human interpretation
- Correlation with human judgment was highly variant-dependent and unreliable

## Why This Works (Mechanism)
None provided

## Foundational Learning
None provided

## Architecture Onboarding
None provided

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can chain-of-thought prompting or "reasoning" model architectures mitigate the instability and prompt sensitivity observed in standard LLMs?
- Basis in paper: The authors explicitly state in the Limitations section that none of the evaluated models are considered "reasoning" or "thinking" models and that they did not use chain-of-thought prompting, suggesting these methods might offer "a lower estimation of model ability."
- Why unresolved: The study establishes a baseline for standard instruction-tuned models but does not test if intermediate reasoning steps help the model decouple the answer from superficial prompt phrasing.
- What evidence would resolve it: Re-evaluating the 138 insurance scenarios using reasoning-optimized models or standard models with chain-of-thought prompts to measure if variance across the 9 question variants decreases.

### Open Question 2
- Question: To what extent does data contamination (training on the evaluation scenarios) inflate the correlation scores of large models like GPT-4?
- Basis in paper: Section 4.3 notes that "we have not ruled out the possibility of data contamination" and Section B highlights that the evaluation data was public before the cutoff dates of the best-performing models.
- Why unresolved: It is unclear if the moderate correlation (R² up to 0.60) observed in GPT-4 reflects genuine semantic understanding or merely memorization of the specific insurance scenarios from the 2023 dataset.
- What evidence would resolve it: Replicating the experiment on newly generated, held-out legal scenarios that post-date the models' training cutoffs to see if the correlation persists.

### Open Question 3
- Question: Do alternative prompting strategies, such as eliciting arguments or examples, result in more stable and human-aligned interpretations than binary QA?
- Basis in paper: The Limitations section states that the binary-choice QA task "does not represent other mechanisms of using LMs for legal interpretation, such as producing arguments for and against an interpretation... or eliciting examples."
- Why unresolved: The paper proves that binary "direct queries" are unstable, but it remains unknown if forcing the model to articulate its reasoning changes the distribution of judgments or reduces the "stopped clock" bias.
- What evidence would resolve it: Designing a comparative study where models first generate arguments or usage examples before rendering a judgment, and measuring the correlation with human judgments across prompt variants.

## Limitations
- Analysis relies on a relatively small corpus of 138 insurance contract scenarios that may not capture full legal complexity
- Comparison with human judgments is constrained by availability of only one labeled human judgment per scenario
- Study focuses exclusively on English-language models and insurance contracts, limiting generalizability
- Fixed temperature=0 settings may not fully capture range of model behavior

## Confidence
- High confidence: Models produce highly inconsistent outputs across question variants and show poor alignment with human judgments in most cases
- Medium confidence: Only the largest instruction-tuned models achieve moderate correlation with human judgment, but this correlation is variant-dependent and unreliable
- Low confidence: The broader claim that LLMs are "not ready for the bench" due to stability and alignment issues, as this requires extrapolation beyond the specific insurance contract domain studied

## Next Checks
1. Replicate the study using a larger and more diverse corpus of legal texts spanning multiple legal domains and jurisdictions
2. Collect multiple human judgments per scenario to establish inter-annotator agreement and assess the reliability of the human baseline
3. Test model performance across a wider range of temperature settings and prompt engineering techniques to establish bounds on model variability and identify conditions that maximize stability