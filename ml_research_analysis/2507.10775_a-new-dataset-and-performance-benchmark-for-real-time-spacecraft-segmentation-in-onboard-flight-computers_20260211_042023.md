---
ver: rpa2
title: A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation
  in Onboard Flight Computers
arxiv_id: '2507.10775'
source_url: https://arxiv.org/abs/2507.10775
tags:
- spacecraft
- images
- segmentation
- dataset
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the critical need for autonomous spacecraft\
  \ inspection systems in resource-constrained orbital environments by introducing\
  \ the Spacecraft With Masks (SWiM) dataset\u2014the first large-scale, annotated\
  \ collection of nearly 64,000 spacecraft images specifically designed for real-time\
  \ segmentation on flight computers. The dataset combines real spacecraft models\
  \ with synthetic backgrounds generated through NASA's TTALOS pipeline and incorporates\
  \ realistic noise and distortions."
---

# A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers

## Quick Facts
- arXiv ID: 2507.10775
- Source URL: https://arxiv.org/abs/2507.10775
- Reference count: 23
- Spacecraft segmentation model achieves 0.92 Dice score, 0.69-1.07 Hausdorff distance, and 0.5s inference time on resource-constrained flight computers

## Executive Summary
This work introduces SWiM, the first large-scale dataset for real-time spacecraft segmentation on flight computers, addressing the critical need for autonomous inspection systems in orbital environments. The dataset combines real spacecraft models with synthetic backgrounds and realistic noise to train models that can operate within strict hardware constraints (4-core CPU, <4GB RAM, <0.95s inference). The authors establish performance benchmarks using YOLOv8 and YOLOv11 segmentation models, achieving a Dice score of 0.92 and inference time of approximately 0.5 seconds on NASA's target hardware.

## Method Summary
The method involves training YOLOv8-nano and YOLOv11-nano segmentation models on the SWiM dataset, which contains nearly 64,000 spacecraft images with synthetic backgrounds and realistic noise. The models are exported to ONNX format with quantization techniques and tested in a Docker container limited to 3GB RAM and 3 CPU cores on Intel Celeron N3350 hardware. The training process uses default Ultralytics hyperparameters, with images resized to 1280×1024 and masks in YOLO polygon format.

## Key Results
- Achieved 0.92 Dice score and 0.69-1.07 Hausdorff distance on test set
- Maintained inference time of approximately 0.5 seconds on target hardware
- Successfully operated within resource constraints: 4-core CPU, <4GB RAM, <0.95s inference

## Why This Works (Mechanism)

### Mechanism 1: Domain Randomization via Synthetic-Real Composition
Superimposing real spacecraft models onto diverse synthetic backgrounds combined with noise injection forces the model to learn structural invariance rather than overfitting to specific background contexts. This acts as a regularizer, reducing the domain gap between simulation and physical deployment.

### Mechanism 2: Compute-Constrained Architecture Selection (Nano + Quantization)
YOLO Nano variants reduce layer depth and width, lowering FLOP count. Exporting to ONNX and running on restricted Docker container ensures the inference graph is optimized for the specific instruction set of the target CPU, minimizing memory bandwidth bottlenecks.

### Mechanism 3: Dual-Metric Optimization for Boundary Precision
Optimizing for both Dice coefficient and Hausdorff distance ensures the model is accurate for both "bulk" detection and precise edge localization required for proximity operations. Dice is tolerant of minor boundary shifts while Hausdorff penalizes the single worst deviation.

## Foundational Learning

- **Instance Segmentation vs. Semantic Segmentation**
  - Why needed: The paper uses YOLOv8-seg which outputs polygon masks for individual objects, distinct from semantic segmentation which labels pixels by class.
  - Quick check: Does the model output a single background/foreground mask, or distinct polygons for each detected spacecraft?

- **Model Quantization (FP32 to INT8)**
  - Why needed: The deployment relies on quantization to fit within 4GB RAM. Understanding how reducing precision affects model accuracy vs. latency is crucial.
  - Quick check: How does reducing numeric precision specifically help the model run on a CPU-only 4GB RAM flight computer?

- **Hausdorff Distance**
  - Why needed: This is the primary "boundary-aware" metric used to verify safety. It is distinct from IoU/Dice and critical for understanding the "worst-case" error scenarios.
  - Quick check: If a predicted mask is perfect except for one pixel that is 10 pixels away from the ground truth, which metric (Dice or Hausdorff) will penalize this error more severely?

## Architecture Onboarding

- **Component map:** Input: 640×640 RGB Image → Backbone: CSPDarknet (Nano) → Neck: PANet → Heads: 4 Decoupled Heads (Classification, Localization, Objectness, Segmentation) → Output: Bounding Boxes + Prototype Masks

- **Critical path:** 1. Data Ingestion (SWiM Dataset) → Augmentation (Noise/Blur) → 2. Training (YOLOv8/v11 Nano on GPU) → 3. Export (.pt to .onnx format) → 4. Quantization (Optimization for CPU) → 5. Deployment (Docker container restricted to 3 Cores/3GB RAM on Intel Celeron)

- **Design tradeoffs:** The Nano model was chosen over Small (7M params) because inference time exceeded 1s constraint. Synthetic vs. real validation uses synthetic TTALOS images for diversity but relies on "real" dataset components for physical validity.

- **Failure signatures:** High Hausdorff, High Dice indicates model finds object but struggles with precise boundaries. Latency spikes may indicate memory leaks in ONNX runtime or background processes consuming limited CPU cores.

- **First 3 experiments:**
  1. Baseline Reproduction: Train YOLOv8 Nano on "SWiM Baseline" dataset and verify Dice score approximates 0.92 without augmentations
  2. Latency Profiling: Deploy exported ONNX model in Docker container limited to 3GB RAM and 3 CPU cores; verify inference <0.95s
  3. Noise Sensitivity Test: Evaluate model on held-out images with injected Gaussian noise and motion blur not present in training set

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but raises several implicit ones through its methodology and results.

## Limitations
- Lack of specification for training hyperparameters (epochs, learning rate, batch size) makes exact reproduction challenging
- Performance on actual flight hardware beyond controlled Docker simulation remains untested
- No validation that noise/distortion profiles in synthetic data accurately match real flight camera characteristics

## Confidence
- **High Confidence:** Real-time inference capability on constrained hardware (verified through Docker container testing)
- **Medium Confidence:** Generalization to physical deployment (based on synthetic validation but lacks flight testing)
- **Medium Confidence:** Dataset sufficiency for spacecraft segmentation (no comparison to alternative datasets)

## Next Checks
1. Test model performance on actual flight hardware (not just simulated Docker container) to validate real-world latency and accuracy
2. Evaluate model robustness to lighting conditions not present in training data (e.g., direct sun exposure, shadow transitions)
3. Conduct ablation study removing synthetic data augmentation to quantify its contribution to the 0.92 Dice score