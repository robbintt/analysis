---
ver: rpa2
title: 'ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction'
arxiv_id: '2511.12214'
source_url: https://arxiv.org/abs/2511.12214
tags:
- trajectory
- graph
- prediction
- expert
- virtual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses pedestrian trajectory prediction by proposing
  ViTE, a framework that combines a Virtual Graph and an Expert Router. The Virtual
  Graph introduces dynamic virtual nodes to efficiently capture high-order interactions
  without deep GNN stacks, while the Expert Router adaptively selects interaction
  experts based on social context using a Mixture-of-Experts design.
---

# ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction

## Quick Facts
- arXiv ID: 2511.12214
- Source URL: https://arxiv.org/abs/2511.12214
- Reference count: 16
- Primary result: State-of-the-art ADE/FDE of 0.20/0.32 on ETH/UCY with efficient virtual graph and MoE routing

## Executive Summary
This paper addresses pedestrian trajectory prediction by proposing ViTE, a framework that combines a Virtual Graph and an Expert Router. The Virtual Graph introduces dynamic virtual nodes to efficiently capture high-order interactions without deep GNN stacks, while the Expert Router adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This allows the model to flexibly integrate diverse relational patterns. Experiments on ETH/UCY, NBA, and SDD datasets demonstrate state-of-the-art performance, with the method achieving the lowest average ADE/FDE of 0.20/0.32 on ETH/UCY and competitive results on other benchmarks. The approach is also shown to be efficient, with minimal parameter count and computational complexity compared to existing methods.

## Method Summary
ViTE predicts pedestrian trajectories by modeling social interactions through a two-stage approach. First, a virtual graph with dynamic virtual nodes reduces effective resistance, enabling high-order interaction capture without deep GNN stacks. Second, a mixture-of-experts router adaptively selects between one-hop (local) and high-order (virtual graph) interaction experts based on per-node social context. The model uses sparse k-NN graphs with relational attention to capture local interactions efficiently. Training involves optimizing both trajectory prediction loss and an importance loss to prevent expert collapse, with diverse predictions generated through parallel decoder heads.

## Key Results
- Achieves lowest average ADE/FDE of 0.20/0.32 on ETH/UCY benchmark
- Outperforms state-of-the-art methods including Transformer-based and GNN-based approaches
- Demonstrates efficient performance with minimal parameters and computational complexity
- Shows strong results on diverse datasets including NBA SportVU and Stanford Drone Dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Virtual nodes reduce graph effective resistance, enabling high-order interaction capture without deep GNN stacks.
- **Mechanism:** Dynamic virtual nodes act as global aggregation hubs in a two-stage message-passing scheme: (1) real nodes aggregate to virtual nodes via attention; (2) virtual nodes broadcast refined representations back. This short-circuits multi-hop paths that would otherwise require 4+ GNN layers.
- **Core assumption:** High-order dependencies can be approximated through a small set of learnable bottleneck nodes rather than explicit multi-hop propagation.
- **Evidence anchors:**
  - [abstract] "Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks"
  - [section] Figure 3 shows effective resistance reduction from Rae = 4.0 → 1.2; Table 6 shows 2-layer GCN (0.22 ADE) underperforms vs. virtual graph (0.20 ADE) with fewer parameters
  - [corpus] Related work (HighGraph, PCHGCN) confirms deep GNNs face under-reaching vs. over-smoothing trade-offs; no corpus papers contradict virtual node approach
- **Break condition:** If virtual node count is too low (1 node) or too high (5+ nodes), capacity mismatch degrades performance (Supplementary Figure 1).

### Mechanism 2
- **Claim:** MoE-based routing adaptively selects interaction experts based on per-node social context, improving over fixed fusion.
- **Mechanism:** A gating network computes soft routing weights per node; Top-P thresholding (p=0.5–0.6) activates only confident experts. One-hop expert handles local interactions; high-order expert (via virtual graph) captures global dependencies. Outputs are renormalized and fused.
- **Core assumption:** Different nodes require different interaction scales; scene complexity varies spatially across agents.
- **Evidence anchors:**
  - [abstract] "Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design"
  - [section] Table 4: One-hop only (0.22 ADE) < High-order only (0.23 ADE) < Mixed (0.25 ADE) < MoE adaptive (0.20 ADE); Figure 6 shows expert weights vary by scenario complexity
  - [corpus] No corpus papers apply MoE to trajectory interaction routing; NLP/vision MoE success cited but not replicated in this domain
- **Break condition:** If importance loss (L_imp) is removed, routing collapse may occur where one expert dominates.

### Mechanism 3
- **Claim:** Sparse k-NN graphs with relational attention capture local interactions efficiently while maintaining scalability.
- **Mechanism:** Graph connectivity determined by feature-similarity k-NN; relational transformer with sparse attention encodes edge features alongside node pairs. This preserves pairwise relational structure without dense attention's O(N²) cost.
- **Core assumption:** Local interactions are the dominant signal; k-NN sufficiently approximates true interaction topology.
- **Evidence anchors:**
  - [section] "dynamically determine the graph connectivity via a k-nearest neighbor strategy based on feature similarity"; "relational transformer (RT) layer equipped with sparse attention"
  - [corpus] ST-GAT, Social-STGCNN use similar sparse graph constructions; validated across baselines
- **Break condition:** If k is too small, disconnected components prevent message flow; if too large, noise overwhelms signal.

## Foundational Learning

- **Concept: Graph Neural Networks & Message Passing**
  - **Why needed here:** ViTE's core is message passing between real nodes and virtual nodes; understanding aggregation, update functions, and receptive fields is essential.
  - **Quick check question:** Can you explain how a 2-layer GCN's receptive field differs from a 4-layer GCN, and why over-smoothing occurs at depth?

- **Concept: Mixture-of-Experts (MoE) & Gating Networks**
  - **Why needed here:** The Expert Router uses soft gating + Top-P selection; understanding load balancing and routing collapse is critical for debugging.
  - **Quick check question:** What happens if all inputs route to the same expert, and how does an auxiliary load-balancing loss mitigate this?

- **Concept: Effective Resistance in Graphs**
  - **Why needed here:** The paper uses effective resistance (Eq. 1) to theoretically motivate virtual nodes; this connects graph structure to communication efficiency.
  - **Quick check question:** Does adding a virtual node between two distant nodes increase or decrease their effective resistance, and what does this imply for message passing?

## Architecture Onboarding

- **Component map:** Input Encoder → Relational Transformer → (One-Hop Expert ∥ High-Order Expert) → Expert Router → Trajectory Decoder

- **Critical path:** Input → RT embeddings → (One-Hop Expert ∥ High-Order Expert) → Expert Router fusion → Decoder → predictions

- **Design tradeoffs:**
  - Virtual node count: 3 optimal for ETH/UCY; 1–2 for simpler scenes (NBA, SDD). Too many → noise; too few → under-capacity.
  - Top-P threshold: 0.5–0.6; lower → sparser but risk under-reaching; higher → compute waste.
  - Expert depth: One-hop uses 2-layer GCN; Virtual Graph uses 1-layer GAT+1-layer GCN. Deeper not better (Table 6: 4-layer GCN underperforms 2-layer).

- **Failure signatures:**
  - Collision predictions in converging scenarios (Supplementary Figure 3): model lacks explicit collision-avoidance loss.
  - Routing collapse: one expert dominates (check L_imp term; should stabilize after warmup).
  - Over-smoothing in ablation: if virtual graph removed and deep GCN used, node representations homogenize.

- **First 3 experiments:**
  1. **Sanity check:** Run on ETH subset with only one-hop expert (disable high-order). Expect ADE ~0.22. If much worse, check graph connectivity (k-NN) and edge feature encoding.
  2. **Virtual node ablation:** Vary virtual node count {1, 2, 3, 4, 5} on UNIV subset. Plot ADE/FDE; expect U-shaped curve with optimum at 3. If monotonic, check virtual node initialization diversity (D_vn).
  3. **Router analysis:** Log expert weights per scenario. In simple scenes (HOTEL), one-hop should dominate (>0.7); in crowded scenes (UNIV), high-order should increase (>0.4). If static, check gating network learning rate and noise injection (ε term).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the integration of visual scene semantics (e.g., obstacles, road structures) into the ViTE framework improve prediction accuracy and social compliance?
- **Basis in paper:** [explicit] (Conclusion states: "Future work will incorporate contextual image information to further enhance scene understanding.")
- **Why unresolved:** The current model relies solely on trajectory coordinates and interaction graphs, lacking explicit environmental constraints or visual cues that dictate movement.
- **What evidence would resolve it:** Experiments on scene-annotated datasets (e.g., SDD) comparing the baseline ViTE against a variant fusing visual features, showing reduced off-road predictions.

### Open Question 2
- **Question:** Can explicit collision-aware objectives or intent modeling effectively mitigate trajectory collision artifacts in highly convergent scenarios?
- **Basis in paper:** [explicit] (Supplementary Material "Failure Case Discussion" notes: "future work could explore collision-aware objectives or intent-aware prediction...")
- **Why unresolved:** The current loss function overfits to historical motion in ambiguous cases, failing to anticipate social negotiation in dense, converging crowds.
- **What evidence would resolve it:** Quantitative analysis of collision rates in predicted trajectories for converging-agent scenarios, comparing standard training against collision-penalized training.

### Open Question 3
- **Question:** Can a dynamic Mixture-of-Experts architecture that adjusts expert granularity or quantity based on real-time scene complexity outperform the current fixed design?
- **Basis in paper:** [explicit] (Conclusion states: "We also aim to explore more flexible expert architectures that dynamically adjust their granularity or number...")
- **Why unresolved:** The current ViTE implementation uses a fixed number of experts and virtual nodes, which may be suboptimal across varying crowd densities.
- **What evidence would resolve it:** Efficiency and accuracy comparisons (ADE/FDE vs. MACs) across sparse and dense dataset subsets using a variable-capacity router.

## Limitations

- **Scalability to larger graphs**: While the virtual graph approach reduces effective resistance, the computational efficiency gains are primarily demonstrated on moderate-sized pedestrian datasets. The method's performance on larger-scale crowd scenarios (1000+ agents) remains untested.
- **Importance loss hyperparameter**: The weight λ for the importance loss (L_imp) is not specified in the paper, which is critical for preventing expert collapse. This could significantly impact reproducibility.
- **Collision handling**: The model lacks explicit collision-avoidance mechanisms, as evidenced by failure cases in converging scenarios. This limits its applicability to dense crowd navigation.

## Confidence

- **High confidence**: The virtual graph mechanism for reducing effective resistance and the basic MoE routing framework are well-supported by theoretical grounding and ablation studies.
- **Medium confidence**: The adaptive routing benefits are demonstrated empirically but rely on unpublished hyperparameter choices (λ) that could affect stability.
- **Medium confidence**: The claimed efficiency gains are supported by parameter comparisons, but runtime measurements are not provided for direct comparison.

## Next Checks

1. **Virtual node sensitivity**: Systematically vary virtual node count (1-5) on ETH/UCY and measure ADE/FDE to confirm the U-shaped performance curve claimed in the paper.
2. **Expert routing stability**: Monitor expert weight distributions during training across different datasets to verify that the importance loss prevents collapse and that routing adapts to scene complexity.
3. **Collision scenario testing**: Design test cases with converging trajectories where collision avoidance is critical and evaluate whether ViTE produces physically plausible predictions or requires additional safety constraints.