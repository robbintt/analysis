---
ver: rpa2
title: 'Revisiting Group Relative Policy Optimization: Insights into On-Policy and
  Off-Policy Training'
arxiv_id: '2505.22257'
source_url: https://arxiv.org/abs/2505.22257
tags:
- grpo
- policy
- off-policy
- optimization
- on-policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes and extends Group Relative Policy Optimization
  (GRPO) for both on-policy and off-policy reinforcement learning settings. The authors
  derive theoretical bounds on policy improvement for both regimes and introduce an
  off-policy GRPO variant that reduces communication overhead during training.
---

# Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training

## Quick Facts
- **arXiv ID:** 2505.22257
- **Source URL:** https://arxiv.org/abs/2505.22257
- **Reference count:** 18
- **Primary result:** Off-policy GRPO achieves comparable or superior performance to on-policy GRPO on GSM8K and Deepscaler datasets while reducing communication overhead

## Executive Summary
This paper analyzes and extends Group Relative Policy Optimization (GRPO) for both on-policy and off-policy reinforcement learning settings. The authors derive theoretical bounds on policy improvement for both regimes and introduce an off-policy GRPO variant that reduces communication overhead during training. Their theoretical analysis provides justification for zero-variance masking techniques previously proposed in the literature. Experiments on GSM8K and Deepscaler datasets show that off-policy GRPO either outperforms or matches on-policy performance while potentially reducing computational costs.

## Method Summary
The paper proposes an off-policy variant of GRPO that samples from a delayed policy π_{k-v} instead of the current policy π_k, reducing communication overhead by updating the vLLM server only every v iterations. The method uses clipped importance sampling objectives with bounded total variation penalties, providing theoretical guarantees for policy improvement. Zero-variance masking is applied to remove prompts with fully correct or incorrect responses to prevent training instability. Experiments use iterative GRPO with group size G=16, temperature 0.1-0.7, and compare on-policy (v=1, i=1) against off-policy (v=10, i=1) configurations on models up to 1.5B parameters.

## Key Results
- Off-policy GRPO matches or exceeds on-policy performance on GSM8K and Deepscaler datasets
- Zero-variance masking stabilizes training, improving GSM8K Pass@1 from ~45% to 50%
- Off-policy training reduces communication overhead by updating vLLM server every 10 iterations instead of every iteration
- Theoretical bounds show improvement guarantees when sampling policy stays close to current policy

## Why This Works (Mechanism)

### Mechanism 1: Off-Policy Advantage Estimation with Bounded Deviation
- **Claim:** Off-policy GRPO can match or exceed on-policy performance when the sampling policy α remains close to the current policy π_k.
- **Mechanism:** The expected reward improvement J(π) - J(π_k) is lower-bounded by the advantage-weighted importance sampling objective L_α(π) minus penalty terms proportional to TV(π, α) and TV(π_k, α). When α = π_{k-v} for small v, the total variation penalties remain bounded, allowing the advantage signal to drive improvement.
- **Core assumption:** The off-policy α satisfies TV(α, π_k) ≤ δ (stays in the vicinity of current policy) and the reward variance under α is non-zero (M_{α,r,0} < ∞).
- **Evidence anchors:**
  - [abstract] "Our results show that off-policy GRPO either significantly outperforms or performs on par with its on-policy counterpart."
  - [Section 3.1, Theorem 1] Provides the formal lower bound involving total variation terms.
  - [Section 5.2, Tables 2-3] Aime24 and Math500 show off-policy (v=10, i=1) achieving 30.47% and 84.74% mean vs on-policy's 30.22% and 85.19%.
  - [corpus] RePO paper confirms replay-enhanced (off-policy) approaches improve data efficiency over standard GRPO.
- **Break condition:** If v is too large, π_{k-v} diverges significantly from π_k, causing TV(π_k, α) to dominate the lower bound and eliminating improvement guarantees.

### Mechanism 2: Zero-Variance Masking Prevents Lower-Bound Collapse
- **Claim:** Filtering prompts with fully correct or fully incorrect responses stabilizes GRPO training by preventing the variance penalty term from exploding.
- **Mechanism:** The improvement lower bound contains the term (1 - σ_{α,r,ε}(x))/σ_{α,r,ε}(x). For verifiable (binary) rewards, when success probability p approaches 0 or 1, variance σ² = p(1-p) approaches 0, causing this term to diverge and the negative penalty to dominate. Masking these samples removes the divergence.
- **Core assumption:** The reward follows a Bernoulli distribution (verifiable/correctness rewards), and fully correct/incorrect samples have zero empirical variance in a group.
- **Evidence anchors:**
  - [Section 3.1, Figure 1] Explicit plot showing the penalty term diverges as variance approaches zero.
  - [Section 5.1, Figure 2b] Masking zero-variance samples improves GSM8K Pass@1 from ~45% (unstable) to 50% (stable).
  - [corpus] Corpus evidence is weak—DAPO is cited but not present in neighbors; no direct replication in corpus.
- **Break condition:** If masking removes too many samples, effective batch size decreases sharply, potentially introducing high variance in gradient estimates.

### Mechanism 3: Clipped Surrogate Constrains Policy Ratio Drift
- **Claim:** The clipped off-policy objective L^c_α(π) ensures the importance ratio π/α remains within [π_k/α - ε, π_k/α + ε], providing a practical relaxation of the KL constraint.
- **Mechanism:** The clipping function f_ε(r, r', a) = min(ra, clip(r, max(r'-ε, 0), r'+ε)a) bounds how far π can deviate from both the off-policy α and current policy π_k. For positive advantages, π/α is clipped to at most π_k/α + ε; for negative advantages, to at least max(π_k/α - ε, 0).
- **Core assumption:** Small learning rate and small v allow approximating π_k/π_{k-v} ≈ 1, simplifying the clipping implementation.
- **Evidence anchors:**
  - [Section 3.2, Equation 6] Defines the clipped off-policy GRPO objective explicitly.
  - [Section 3.2] "The clipping ensures that the ratio π/α remains bounded and is a relaxation of the KL (or the total variation distance)."
  - [corpus] SOUP and DaGRPO papers confirm clipping/ratio control variants improve GRPO stability.
- **Break condition:** If ε is set too small, policy updates become overly conservative, slowing convergence; if too large, the trust region guarantee degrades.

## Foundational Learning

- **Concept:** Importance Sampling in Off-Policy RL
  - **Why needed here:** Off-policy GRPO computes expectations under α but optimizes π, requiring the π(y|x)/α(y|x) ratio throughout.
  - **Quick check question:** Given samples from policy β, how do you estimate E_{x~π}[f(x)]?

- **Concept:** Total Variation Distance and KL Divergence Relationship
  - **Why needed here:** The theoretical bounds use TV distance; Pinsker's inequality (TV ≤ √(KL/2)) justifies using KL constraints as a proxy.
  - **Quick check question:** State Pinsker's inequality connecting TV and KL.

- **Concept:** Advantage Function as Standardized Reward
  - **Why needed here:** GRPO replaces the learned critic in PPO with group-based reward standardization (A = (r - μ)/σ).
  - **Quick check question:** Why does GRPO not need a critic network unlike PPO?

## Architecture Onboarding

- **Component map:** vLLM inference server -> Training workers -> Reward verifier -> Advantage estimator -> Objective evaluator
- **Critical path:**
  1. Sample batch D_b from prompt distribution ρ_X
  2. If k mod v == 0: update π_θ^{old} on vLLM server (synchronization point)
  3. Generate G outputs per prompt via vLLM
  4. Compute rewards and advantages (can mask zero-variance samples)
  5. Update π_θ via gradient ascent on clipped objective (i iterations)
  6. Swap π_ref ← π_θ at epoch boundaries

- **Design tradeoffs:**
  - **v (server update frequency):** Larger v reduces communication overhead but increases TV(π_k, α) penalty risk
  - **i (SGD iterations per batch):** Original GRPO uses i>1 for sample reuse; this work uses i=1 with v>1 for cleaner off-policy semantics
  - **ε (clip range):** Controls conservatism of updates; paper doesn't ablate this explicitly
  - **Zero-variance masking:** Stabilizes training but reduces effective sample count

- **Failure signatures:**
  - Training instability with Pass@1 oscillating or dropping → check if zero-variance masking is enabled
  - No improvement despite many iterations → v may be too large; policy drift exceeded trust region
  - Slow convergence with off-policy GRPO → ε may be too restrictive or learning rate mismatched
  - Communication bottleneck in multi-node → v=1 forces full weight sync every iteration

- **First 3 experiments:**
  1. **Ablate v on small model:** Train Qwen-0.5B on GSM8K with v∈{1,5,10,20} (i=1 fixed) to find the point where off-policy performance degrades
  2. **Validate zero-variance masking:** Compare on-policy GRPO with and without masking on GSM8K; plot training curves for stability
  3. **Profile communication overhead:** Measure time spent in vLLM weight sync vs. training for different v values on a multi-GPU setup to quantify speedup potential

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does off-policy GRPO maintain its stability and communication efficiency advantages in multi-node distributed training setups with models larger than 1.5B parameters?
- Basis in paper: [explicit] The authors state in the limitations that they "reserve to scale up our experiments to larger training runs in future work," noting that single-node setups "cannot fully demonstrate the potential speedups."
- Why unresolved: Resource constraints limited the experiments to a single node with 8 GPUs, preventing the observation of inter-node communication overhead reduction which is the primary theoretical benefit of the off-policy variant.
- What evidence would resolve it: Benchmarks showing wall-clock time and performance retention when training models >7B parameters across multiple nodes using the proposed off-policy serving strategy.

### Open Question 2
- Question: How sensitive is the algorithm's convergence to the update interval $v$, and what are the bounds beyond which the policy divergence breaks the theoretical improvement guarantees?
- Basis in paper: [inferred] The paper guarantees improvement "as long as $v$ is not too large" and relies on the approximation that the sampling policy $\alpha$ stays in the vicinity of the current policy $\pi_k$.
- Why unresolved: While $v=10$ was used in experiments, the paper does not provide an empirical sensitivity analysis or theoretical upper bound for $v$ relative to the learning rate.
- What evidence would resolve it: An ablation study varying $v$ and the learning rate to identify the "breaking point" where the off-policy advantage estimation degrades performance compared to on-policy GRPO.

### Open Question 3
- Question: Can the theoretical justification for zero-variance masking be effectively extended to continuous or dense reward functions where variance approaches but does not equal zero?
- Basis in paper: [inferred] The analysis of the exploding term $\frac{1-\sigma}{\sigma}$ and the masking strategy is discussed primarily in the context of verifiable (binary) rewards.
- Why unresolved: It is unclear if masking only "zero variance" samples is sufficient to stabilize the lower bound for non-sparse reward landscapes typical in standard RLHF.
- What evidence would resolve it: Experiments applying the masking technique to continuous reward tasks (e.g., preference modeling) to see if stability is maintained without discarding excessive data.

## Limitations
- The theoretical bounds rely on assumptions about bounded total variation and variance that may not hold in practice
- Zero-variance masking's effectiveness is supported by weak empirical evidence in the corpus
- The approximation π_k/π_{k-v} ≈ 1 is assumed but not explicitly validated across all experiments

## Confidence

- **High confidence**: The core claim that off-policy GRPO can match on-policy performance (supported by direct experimental results on GSM8K and Deepscaler)
- **Medium confidence**: The theoretical bounds providing justification for zero-variance masking (partially supported; the mechanism is sound but empirical validation is limited)
- **Medium confidence**: The communication overhead reduction claim (plausible given the v=10 server update frequency, but not directly measured)

## Next Checks

1. **Ablate v on GSM8K with Qwen-0.5B:** Systematically test v∈{1,5,10,20} to identify the threshold where off-policy performance degrades due to policy drift
2. **Validate zero-variance masking independently:** Run controlled experiments comparing on-policy GRPO with and without masking on GSM8K, measuring both stability (variance across runs) and final performance
3. **Profile communication overhead:** Measure wall-clock time for different v values in a multi-GPU setup to quantify the claimed speedup from reduced server synchronization