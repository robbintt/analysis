---
ver: rpa2
title: 'The Alignment Waltz: Jointly Training Agents to Collaborate for Safety'
arxiv_id: '2510.08240'
source_url: https://arxiv.org/abs/2510.08240
tags:
- agent
- feedback
- conversation
- training
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: W altzRL introduces a multi-agent reinforcement learning framework
  that formulates safety alignment as a collaborative positive-sum game between a
  conversation agent and a feedback agent. The method employs a Dynamic Improvement
  Reward that evolves based on how well the conversation agent incorporates feedback,
  enabling adaptive improvements to unsafe or overrefusing responses.
---

# The Alignment Waltz: Jointly Training Agents to Collaborate for Safety

## Quick Facts
- arXiv ID: 2510.08240
- Source URL: https://arxiv.org/abs/2510.08240
- Reference count: 40
- Key outcome: Introduces a multi-agent reinforcement learning framework that jointly trains conversation and feedback agents to improve LLM safety while maintaining helpfulness

## Executive Summary
WaltzRL introduces a novel multi-agent reinforcement learning framework that formulates safety alignment as a collaborative positive-sum game between a conversation agent and a feedback agent. The method employs a Dynamic Improvement Reward that evolves based on how well the conversation agent incorporates feedback, enabling adaptive improvements to unsafe or overrefusing responses. Across five diverse datasets, WaltzRL significantly reduces unsafe responses and overrefusals compared to baselines while preserving general capabilities. The approach advances the Pareto front between helpfulness and harmlessness, demonstrating that specialized safety-focused feedback agents can enhance LLM safety without degrading helpfulness.

## Method Summary
WaltzRL is a multi-agent reinforcement learning framework that trains two specialized agents: a conversation agent that generates responses and a feedback agent that evaluates safety. The framework formulates safety alignment as a positive-sum game where both agents receive rewards when unsafe responses are successfully improved. The Dynamic Improvement Reward mechanism provides adaptive feedback based on whether the conversation agent successfully incorporates the feedback agent's suggestions. The feedback agent is trained to identify safety issues and propose specific improvements, while the conversation agent learns to generate safer responses by incorporating this feedback. The method employs a shared value function to stabilize training and uses separate value functions for each agent to guide their learning objectives.

## Key Results
- Reduces unsafe responses from 39.0% to 4.6% on WildJailbreak dataset compared to direct preference optimization
- Decreases overrefusals from 45.3% to 9.9% on OR-Bench dataset while maintaining general capabilities
- Advances the Pareto front between helpfulness and harmlessness across five diverse safety datasets

## Why This Works (Mechanism)
The effectiveness of WaltzRL stems from its collaborative multi-agent framework that creates a feedback loop between specialized agents. The Dynamic Improvement Reward mechanism provides continuous, adaptive feedback that evolves as the conversation agent improves, creating a self-reinforcing learning process. By training both agents jointly, the framework ensures that the feedback agent learns to provide actionable, targeted suggestions rather than generic warnings. The positive-sum game formulation aligns the incentives of both agents toward the shared goal of improving safety without sacrificing helpfulness. The specialized architecture allows each agent to focus on its core competency - the conversation agent on generating natural responses and the feedback agent on identifying and addressing safety issues.

## Foundational Learning

**Reinforcement Learning**: Why needed - provides the framework for training agents through reward signals rather than supervised labels; Quick check - verify that reward shaping appropriately balances safety improvements with response quality.

**Multi-Agent Systems**: Why needed - enables collaborative learning between specialized agents with complementary capabilities; Quick check - ensure proper credit assignment and reward distribution between agents.

**Dynamic Reward Functions**: Why needed - allows the reward structure to evolve as the conversation agent improves, preventing reward hacking; Quick check - monitor reward stability and convergence across training epochs.

**Feedback Loop Optimization**: Why needed - creates a continuous improvement cycle where feedback quality improves as the conversation agent learns; Quick check - measure correlation between feedback quality and safety improvements over time.

## Architecture Onboarding

**Component Map**: Conversation Agent -> Safety Evaluator -> Dynamic Improvement Reward -> Value Functions -> Feedback Agent

**Critical Path**: The core training loop flows from the conversation agent generating responses, through the feedback agent providing safety evaluations, to the dynamic reward calculation, and finally to the value functions that guide both agents' learning updates.

**Design Tradeoffs**: The framework trades increased computational complexity (training two specialized agents) for improved safety alignment performance. The use of synthetic safety datasets reduces data collection costs but may limit real-world generalizability.

**Failure Signatures**: Training instability may occur if the Dynamic Improvement Reward becomes too aggressive or if the feedback agent's evaluations become inconsistent. Overfitting to synthetic safety patterns could reduce effectiveness on real-world unsafe scenarios.

**First Experiments**:
1. Test joint training convergence with varying reward scales for the Dynamic Improvement Reward
2. Evaluate the sensitivity of safety improvements to feedback agent accuracy thresholds
3. Measure the impact of shared versus separate value functions on training stability

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Reliance on synthetically generated safety datasets may not capture real-world safety complexity
- Computational overhead of training specialized feedback agents may limit scalability
- Focus on binary safety/unsafety classification may oversimplify nuanced safety considerations

## Confidence
High confidence in technical implementation and experimental methodology; Medium confidence in scalability claims and real-world applicability; Low confidence in long-term generalization to unseen safety scenarios.

## Next Checks
1. Test WaltzRL's performance on safety benchmarks not used during training to evaluate true generalization capabilities
2. Conduct human evaluation studies to assess the quality and naturalness of conversation agent responses in safety-critical domains
3. Analyze the computational overhead and training time requirements when scaling to larger language models and more diverse safety scenarios