---
ver: rpa2
title: 'Language, Culture, and Ideology: Personalizing Offensiveness Detection in
  Political Tweets with Reasoning LLMs'
arxiv_id: '2510.02351'
source_url: https://arxiv.org/abs/2510.02351
tags:
- language
- conservative
- reasoning
- left
- political
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated whether reasoning-enhanced large language
  models (LLMs) could personalize offensive language detection by simulating diverse
  political and cultural perspectives. Using a multilingual dataset of political tweets,
  models were prompted to classify content as offensive or non-offensive from the
  viewpoints of various ideological personas (far-right, conservative, progressive,
  centrist) across English, Polish, and Russian contexts.
---

# Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs

## Quick Facts
- **arXiv ID:** 2510.02351
- **Source URL:** https://arxiv.org/abs/2510.02351
- **Reference count:** 40
- **Primary result:** Reasoning-enabled LLMs can simulate diverse ideological perspectives for personalized offensive language detection across multiple languages, with cross-language consistency and ideological differentiation outperforming non-reasoning models.

## Executive Summary
This study investigates whether reasoning-enhanced large language models can personalize offensive language detection by simulating diverse political and cultural perspectives. Using a multilingual dataset of political tweets, models were prompted to classify content as offensive or non-offensive from the viewpoints of various ideological personas (far-right, conservative, progressive, centrist) across English, Polish, and Russian contexts. Results show that large reasoning-capable models (e.g., DeepSeek-R1, o4-mini) achieve higher cross-language consistency and better ideological differentiation than non-reasoning models, which produce more uniform outputs. Reasoning-enabled models also generate interpretable justifications and align judgments with ideological framing.

## Method Summary
The study used a subset of 297 tweets from the MD-Agreement dataset (US 2020 elections domain) translated into Polish and Russian. Eight LLMs were evaluated, including reasoning-enabled models (DeepSeek-R1, o4-mini) and non-reasoning models (DeepSeek-V3, GPT-4.1-mini, Qwen3-8B). Models received prompts with detailed persona profiles (political group, nationality, age, sex, outlook) and classified tweets as offensive or non-offensive. Reasoning models sampled 5 responses per prompt; non-reasoning models output probabilities. Classification consistency was estimated using Wald confidence intervals (α=0.10), with samples having estimated probabilities near 0.5 excluded. Cross-Language Consistency (CLC) and Inter-Group Differentiation (IGD) metrics were computed to evaluate personalization performance.

## Key Results
- Reasoning-enabled models (DeepSeek-R1, o4-mini) achieved IGD scores of 100.03 and 75.69 respectively, showing strong ideological differentiation
- DeepSeek-V3 (non-reasoning) showed minimal ideological differentiation with IGD of 1.58
- Cross-language correlations within political groups remained stable (0.79-0.93), indicating consistent ideological framing across languages
- Reasoning models generated interpretable justifications aligned with provided persona profiles

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit reasoning traces enable models to maintain coherent ideological perspectives across varied inputs.
- **Mechanism:** Reasoning-capable models generate intermediate explanatory steps before classification, which appear to anchor the model's interpretation to the provided persona profile. This multi-step process allows the model to reference the ideological framing repeatedly during generation, reducing drift toward consensus judgments.
- **Core assumption:** The intermediate reasoning steps genuinely reflect perspective-taking rather than post-hoc rationalization of predetermined outputs.
- **Evidence anchors:**
  - [abstract] "Reasoning-enabled models also generated interpretable justifications and aligned judgments with ideological framing."
  - [Section V-A] "DeepSeek-R1 frequently explores multiple hypothetical scenarios to justify its decisions, enhancing interpretability."
  - [corpus] Related work on reasoning traces improving classification consistency (Wei et al., 2022, cited in paper) supports this mechanism, though corpus signals lack direct replications of this specific personalization task.
- **Break condition:** If reasoning traces are truncated or the model defaults to template responses (as observed with o4-mini's "templated conclusions"), ideological differentiation degrades.

### Mechanism 2
- **Claim:** Cross-linguistic ideological consistency emerges from language-invariant conceptual representations in reasoning layers.
- **Mechanism:** When models process ideological framing in prompts, they encode the conceptual structure (e.g., far-right vs. progressive values) in intermediate representations that persist across linguistic surface forms. Reasoning steps reinforce this conceptual anchoring before language-specific outputs are generated.
- **Core assumption:** The correlation patterns observed reflect genuine ideological coherence rather than artifacts of training data overlap between languages.
- **Evidence anchors:**
  - [Section V-A] "Correlations within political groups remain stable across languages (EN, PL, RU), suggesting model judgments are consistent for a given ideology regardless of language."
  - [Figure 2] DeepSeek-R1 shows internal coherence of 0.79–0.93 within political groups across languages.
  - [corpus] Weak direct evidence—neighbor papers focus on bias detection but not specifically on cross-linguistic ideological consistency mechanisms.
- **Break condition:** When reasoning outputs default to English (observed in 86% of DeepSeek-R1 responses regardless of prompt language), the mechanism may fail for lower-resource languages where conceptual representations are less robust.

### Mechanism 3
- **Claim:** Scale amplifies reasoning's effectiveness for personalization, but scale alone is insufficient without explicit reasoning.
- **Mechanism:** Larger models possess more granular representations of ideological concepts and social norms. When combined with reasoning mechanisms, this capacity enables explicit comparison of input content against persona-specific thresholds. Non-reasoning large models (e.g., DeepSeek-V3) default to aggregate training distribution patterns, producing uniformly positive correlations regardless of persona.
- **Core assumption:** The observed performance differences stem from reasoning architecture rather than other architectural variations between tested models.
- **Evidence anchors:**
  - [Section V-B] DeepSeek-V3 (large, non-reasoning) shows "minimal ideological differentiation" with IGD of 1.58 vs. 100.03 for DeepSeek-R1.
  - [Table I] GPT-4.1-mini achieves IGD of only 8.09 despite its size, while reasoning-enabled Qwen3-8B achieves 32.23 with fewer parameters.
  - [corpus] Related work (Section VI, reference 43) suggests reasoning enhances classification consistency, but corpus lacks independent validation of scale × reasoning interactions.
- **Break condition:** Smaller reasoning models (e.g., Qwen3-8B) show degraded performance—22.6% samples excluded due to low confidence vs. 9.3% for larger reasoning models.

## Foundational Learning

- **Concept: Persona-based prompting with ideological profiles**
  - **Why needed here:** The framework requires constructing detailed persona descriptions that include not just political affiliation but also outlook, values, and demographic context. Simplified labels (e.g., "conservative") produce weaker differentiation.
  - **Quick check question:** Can you articulate why the persona prompt includes "outlook" and "values" fields rather than just political group labels?

- **Concept: Binary classification with confidence estimation**
  - **Why needed here:** The study uses 5-fold sampling to estimate latent classification probabilities, then applies Wald confidence intervals to filter uncertain predictions. This approach distinguishes model uncertainty from genuine ideological disagreement.
  - **Quick check question:** Given a model that outputs [1, 0, 1, 1, 0] for five samples of the same input, what is the estimated probability p̂ and should this sample be excluded at α = 0.10?

- **Concept: Cross-Language Consistency (CLC) and Inter-Group Differentiation (IGD) metrics**
  - **Why needed here:** Standard accuracy metrics fail here because there's no ground truth—the goal is personalization, not correctness. CLC measures stability across languages; IGD measures ideological separation.
  - **Quick check question:** If a model produces identical outputs for all political groups, what would its IGD score be?

## Architecture Onboarding

- **Component map:** Persona profile construction → Prompt formatting → LLM inference (reasoning-enabled or standard) → 5-fold response sampling → Probability estimation → Confidence filtering (Wald CI) → Correlation matrix construction → CLC/IGD metric computation
- **Critical path:** 1. Ensure persona profiles include political group, nationality, age, sex, and outlook description (see Figure 1) 2. Use reasoning-enabled models with sampling (temperature > 0) if forced by API; collect 5 responses per prompt 3. Filter samples where estimated p̂ ∈ {0.4, 0.6} before computing correlation matrices 4. Compute CLC (lower = more consistent) and IGD (higher = better differentiation)
- **Design tradeoffs:** - Sampling vs. temperature = 0: Reasoning models often disallow temperature control, forcing sampling. This introduces variance but enables confidence estimation. Alternative: use logprobs from non-reasoning models for single-pass probability estimation. - Binary vs. graded offensiveness: Binary classification simplifies analysis but loses nuance. The paper acknowledges this limitation. - English-dominant reasoning: Accept that reasoning traces may default to English even for non-English prompts, or implement language-specific prompting strategies (untested in this work).
- **Failure signatures:** - Uniformly high positive correlations across all persona combinations → model lacks personalization capability (see DeepSeek-V3, Figure 5) - High exclusion rate (>15%) → model produces inconsistent outputs; consider increasing sample count or switching models - Reasoning outputs in wrong language → expected behavior for current reasoning models; document as limitation
- **First 3 experiments:** 1. Baseline validation: Run DeepSeek-R1 and DeepSeek-V3 on a 50-tweet subset with 2 personas. Confirm R1 shows higher IGD than V3 before scaling up. 2. Persona depth test: Compare simplified prompts ("You are a progressive") vs. full persona profiles. Measure IGD delta to quantify the value of detailed outlook descriptions. 3. Language ablation: Run English-only prompts with all nationality variants removed. Compare CLC to full multilingual setup to isolate the effect of explicit nationality cues vs. language alone.

## Open Questions the Paper Calls Out
- Can the multilingual alignment gap in reasoning outputs be reduced so models generate reasoning in the same language as the prompt rather than defaulting to English? (Authors observe 86% English reasoning even for Polish/Russian prompts)
- Would expanding beyond binary classification to capture gradations of offensiveness, intent, and context improve the ecological validity of personalized detection? (Paper acknowledges binary framing ignores gradations of harm, context, or intent)

## Limitations
- Absence of ground truth labels for offensive content makes it impossible to validate whether personalized judgments align with actual offensiveness
- English-language dominance of reasoning traces (86% of outputs) raises questions about validity for non-English contexts
- Persona construction relies on stereotypical characterizations that may not capture ideological nuance or individual variation within groups

## Confidence
- **High Confidence:** Cross-language correlation stability within political groups
- **Medium Confidence:** Reasoning mechanism effectiveness for ideological differentiation
- **Medium Confidence:** Scale × reasoning interaction
- **Low Confidence:** Interpretation of English-dominant reasoning traces in multilingual context

## Next Checks
1. **Ablation study on reasoning components:** Remove reasoning traces from reasoning-capable models and re-run classification to isolate the contribution of explicit reasoning versus model scale.
2. **Ground truth correlation analysis:** Manually annotate a subset of tweets with actual offensive content labels and measure alignment between model predictions and ground truth across different ideological personas.
3. **Cross-linguistic reasoning trace analysis:** Analyze the semantic content of reasoning traces in non-English languages to quantify the frequency and quality of language-appropriate reasoning versus English defaults.