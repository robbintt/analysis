---
ver: rpa2
title: 'LatPhon: Lightweight Multilingual G2P for Romance Languages and English'
arxiv_id: '2509.03300'
source_url: https://arxiv.org/abs/2509.03300
tags:
- languages
- multilingual
- vowel
- error
- italian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LatPhon, a compact multilingual grapheme-to-phoneme
  (G2P) conversion model covering six Latin-script languages: English, Spanish, French,
  Italian, Portuguese, and Romanian. Built as a 7.5 million-parameter Transformer,
  it achieves a mean phoneme error rate (PER) of 3.5% on the public ipa-dict corpus,
  outperforming the byte-level ByT5 baseline (5.4%) and approaching language-specific
  WFSTs (3.2%).'
---

# LatPhon: Lightweight Multilingual G2P for Romance Languages and English

## Quick Facts
- arXiv ID: 2509.03300
- Source URL: https://arxiv.org/abs/2509.03300
- Authors: Luis Felipe Chary; Miguel Arjona Ramirez
- Reference count: 11
- Key outcome: 7.5M-parameter Transformer achieves 3.5% mean PER across 6 languages, outperforming 580M-parameter ByT5 baseline (5.4%) while being 77× smaller

## Executive Summary
This paper introduces LatPhon, a compact multilingual grapheme-to-phoneme (G2P) conversion model covering six Latin-script languages: English, Spanish, French, Italian, Portuguese, and Romanian. Built as a 7.5 million-parameter Transformer, it achieves a mean phoneme error rate (PER) of 3.5% on the public ipa-dict corpus, outperforming the byte-level ByT5 baseline (5.4%) and approaching language-specific WFSTs (3.2%). The model uses a 4-layer encoder-decoder architecture with 256-dimensional embeddings, 8-head attention, and a learned 6-way language-ID embedding. Ablation studies show that removing the language-ID token degrades PER by 21.6 percentage points on average, confirming its importance for cross-lingual sharing. Inference is fast (about 31 words per second) and the final model occupies only 30 MB, making on-device deployment feasible. The work demonstrates that a single, lightweight, multilingual G2P model can serve as a universal front-end for speech processing pipelines while remaining compact and accurate.

## Method Summary
LatPhon is a 4-layer encoder-decoder Transformer with 256-d embeddings, 8-head attention, rotary positional encodings, and a learned 6-way language-ID embedding prepended to input sequences. The model maps graphemes to a shared 109-symbol IPA output vocabulary and is trained autoregressively using teacher forcing on the ipa-dict corpus with 100k steps, batch size 64, AdamW optimizer, and cosine learning rate decay. Total parameters: 7.5M; model size: 30 MB; inference speed: ~31 words/second.

## Key Results
- 3.5% mean PER across 6 languages, outperforming 580M-parameter ByT5 (5.4%) while being 77× smaller
- Language-ID embedding ablation degrades PER by 21.6 percentage points (p < 10^-6), confirming its critical role
- Model size of 30 MB enables on-device deployment; inference speed of 31 words/second supports real-time applications
- Single multilingual model approaches per-language WFST performance (3.5% vs 3.2% PER) with unified maintenance

## Why This Works (Mechanism)

### Mechanism 1: Language-ID Conditional Disambiguation
A learned language-ID embedding prepended to the input sequence enables a single Transformer to route grapheme-phoneme mappings through language-appropriate pathways, preventing cross-lingual interference. The 6-way language-ID embedding conditions the self-attention mechanism, shifting attention patterns to activate correct phoneme predictions when encountering ambiguous orthographic symbols like "a" in different languages.

### Mechanism 2: Shared IPA Vocabulary Compression
Mapping all languages to a unified 109-symbol IPA output vocabulary reduces output space complexity by orders of magnitude, concentrating model capacity on phonetic distinctions rather than lexical memorization. The decoder softmax operates over ~100 IPA symbols instead of tens of thousands of word or byte tokens, cutting computational cost dramatically.

### Mechanism 3: Right-Sized Transformer for Bounded Task Complexity
A 4-layer, 7.5M parameter autoregressive Transformer matches the task complexity of dictionary-based G2P without requiring pre-training or knowledge distillation. G2P has bounded complexity—finite grapheme sequences map to finite phoneme sequences—unlike open-ended language modeling. The small model capacity matches this complexity, avoiding both underfitting and the overfitting that larger models risk on limited data.

## Foundational Learning

- **Teacher Forcing in Sequence-to-Sequence Training**
  - Why needed here: The model trains by feeding ground-truth previous phonemes to the decoder, but uses autoregressive prediction at inference
  - Quick check question: During training with teacher forcing, what does the decoder receive as input at timestep t, and how does this differ from inference?

- **Autoregressive vs. Non-Autoregressive Decoding**
  - Why needed here: The paper explicitly contrasts its autoregressive approach with LiteG2P's non-autoregressive architecture; understanding this tradeoff is essential for architecture decisions
  - Quick check question: In an autoregressive decoder, why must phoneme prediction at position t wait for position t-1, and what throughput implication does this create?

- **Rotary Positional Encodings (RoPE)**
  - Why needed here: The architecture uses rotary positional encodings rather than absolute sinusoidal embeddings
  - Quick check question: How does RoPE encode relative position information differently from additive sinusoidal encodings, and why might this benefit variable-length G2P sequences?

## Architecture Onboarding

- **Component map**: Input graphemes + language-ID token → 4-layer encoder (256-d, 8-head, RoPE) → 4-layer decoder (256-d, 8-head, autoregressive) → 109-symbol softmax

- **Critical path**: Map input graphemes to 256-d character embeddings → Prepend learned language-ID embedding → Apply rotary position encodings → Encode through 4-layer Transformer encoder → Decode phoneme-by-phoneme autoregressively (teacher forcing during training, greedy/beam at inference) → Project to 109-symbol softmax for phoneme prediction

- **Design tradeoffs**: Autoregressive vs. NAR (LatPhon prioritizes accuracy over speed; LiteG2P achieves faster inference via NAR but requires teacher distillation); Single multilingual model vs. per-language WFSTs (0.3% absolute PER gap traded for unified maintenance); Small model vs. ByT5 (77× smaller but better PER, though ByT5 covers more scripts)

- **Failure signatures**: Italian PER (5.8%) >> other languages due to data scarcity (6,108 training pairs vs. 594,899 for Spanish); Consonant gemination errors in Italian (e.g., "petto" → /"pEto/ instead of /"pEtto/); English vowel/stress errors due to irregular orthography; Without language-ID: PER jumps to 25.1%

- **First 3 experiments**: 1) Reproduce language-ID ablation: Remove language-ID token, retrain, verify ~21.6 percentage point PER degradation; 2) Italian data scaling test: Augment Italian training data, measure PER improvement; 3) Cross-language transfer probe: Hold out one Romance language during training, evaluate zero-shot PER

## Open Questions the Paper Calls Out

- **Joint training with end-to-end acoustic models**: The authors plan to explore joint training of the G2P module with end-to-end acoustic models to investigate if co-adaptation can further improve synthesis quality, as current evaluation is in isolation using PER.

- **Synthetic data augmentation for low-resource languages**: To bolster performance on low-resource languages like Italian, the authors will investigate synthetic data augmentation techniques, as small training sets correlate with higher error rates.

- **Generalization to out-of-domain vocabulary**: The model's handling of neologisms, modern slang, or rare proper nouns not found in standard lexicons remains untested, as evaluation relies on standard dictionary entries.

## Limitations

- Data imbalance: Italian training data is dramatically smaller (6,108 pairs) than other languages, resulting in significantly higher Italian PER (5.8%)
- Limited vocabulary coverage: The 109-symbol IPA vocabulary excludes certain phonemes (e.g., Arabic pharyngeal fricatives), limiting language coverage
- Out-of-vocabulary performance untested: The model's handling of proper nouns, neologisms, or modern slang remains unverified as evaluation uses standard dictionary entries

## Confidence

**High Confidence (4/5):**
- Language-ID embedding critical for multilingual performance (ablation shows 21.6 percentage point PER degradation with p < 10^-6)
- Model achieves 3.5% mean PER, outperforming ByT5 baseline (5.4%) while being 77× smaller
- Inference speed (31 words/second) sufficient for real-time applications
- 30 MB model size enables on-device deployment

**Medium Confidence (3/5):**
- Claims about shared IPA vocabulary reducing soft-max complexity are theoretically sound but not directly validated against alternatives
- Architecture design choices (4 layers, 256-d embeddings) appear well-matched to task complexity but optimal hyperparameters aren't extensively explored
- Claims about "universal front-end" applicability assume similar orthographic-to-phonetic complexity across languages

**Low Confidence (2/5):**
- Generalization to unseen vocabulary (neologisms, proper nouns) untested
- Zero-shot cross-language transfer capabilities not evaluated
- Performance on languages outside the tested Romance/English set remains unknown

## Next Checks

1. **Language-ID Ablation Reproduction**: Remove the 6-way language-ID embedding, retrain on identical data, and verify the ~21.6 percentage point PER degradation to confirm the conditioning mechanism is causal and essential.

2. **Italian Data Scaling Experiment**: Systematically increase Italian training data through synthetic generation or additional lexicons, measuring whether PER approaches other languages or if architectural modifications are needed for consonant gemination patterns.

3. **Cross-Language Transfer Probe**: Hold out one Romance language during training, then evaluate zero-shot PER on the held-out language to test whether shared IPA vocabulary enables meaningful transfer or if language-ID embedding prevents it.