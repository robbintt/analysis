---
ver: rpa2
title: 'CLAPS: Posterior-Aware Conformal Intervals via Last-Layer Laplace'
arxiv_id: '2512.01384'
source_url: https://arxiv.org/abs/2512.01384
tags:
- claps
- conformal
- posterior
- intervals
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CLAPS combines a Last-Layer Laplace Approximation with split-conformal\
  \ calibration to produce posterior-aware conformal intervals. It defines a two-sided\
  \ posterior CDF score that aligns the conformity metric with the full predictive\
  \ distribution, yielding narrower intervals at fixed coverage\u2014especially on\
  \ small to medium tabular datasets where epistemic uncertainty is nontrivial."
---

# CLAPS: Posterior-Aware Conformal Intervals via Last-Layer Laplace

## Quick Facts
- arXiv ID: 2512.01384
- Source URL: https://arxiv.org/abs/2512.01384
- Authors: Dongseok Kim, Hyoungsun Choi, Mohamed Jismy Aashik Rasool, Gisung Oh
- Reference count: 40
- Primary result: Combines Last-Layer Laplace Approximation with split-conformal calibration to produce narrower intervals at fixed coverage, especially on small to medium tabular datasets

## Executive Summary
CLAPS introduces a method that fuses Last-Layer Laplace Approximation (LLLA) with split-conformal prediction to generate posterior-aware conformal intervals. By defining a two-sided CDF score that aligns the conformity metric with the full predictive distribution, CLAPS achieves narrower intervals at the same target coverage compared to residual-based scores—particularly when epistemic uncertainty is nontrivial. The method includes lightweight diagnostics to separate aleatoric and epistemic uncertainty components, guiding method selection across different data regimes.

## Method Summary
CLAPS fits a Laplace posterior over the last linear layer of a pretrained MLP, capturing epistemic uncertainty via the posterior covariance. The predictive distribution combines this with aleatoric noise to form Gaussian posteriors. A two-sided CDF score measures how typical each observation is under this posterior. Split-conformal calibration then guarantees marginal coverage by selecting a threshold via rank rule. The resulting intervals inherit efficiency from the posterior's tail structure while maintaining coverage regardless of posterior correctness.

## Key Results
- CLAPS achieves nominal 90% coverage with narrower intervals than residual-based CP on small to medium datasets (Airfoil, kin8nm, CASP)
- Posterior contraction diagnostics confirm epistemic uncertainty diminishes as dataset size grows, explaining regime-dependent performance
- Scale-learning methods (Norm-CP, CQR) outperform CLAPS on large, heteroscedastic datasets (YearPredictionMSD) where posterior uncertainty collapses

## Why This Works (Mechanism)

### Mechanism 1: Posterior Shape Alignment via Two-Sided CDF Scoring
The two-sided CDF score `s(x,y) = min{F_post(y|x), 1-F_post(y|x)}` measures how "typical" `y` is under the full Gaussian predictive, producing nested superlevel sets that coincide with highest-posterior-density intervals. This yields narrower intervals at fixed coverage when the predictive posterior captures meaningful tail structure. Break condition: when posterior collapse occurs (`r(x) → 0`) or heteroscedasticity dominates without adequate posterior capture.

### Mechanism 2: Last-Layer Laplace Approximation Provides Lightweight Epistemic Uncertainty
LLLA provides closed-form Gaussian posterior over the final linear head with variance decomposed as `v̂(x) = σ² + φ(x)ᵀΣφ(x)`. The epistemic component `φ(x)ᵀΣφ(x)` varies with `x`, adapting interval width. Core assumption: most train-time parameter variability concentrates in the last layer. Break condition: if the backbone is misspecified or epistemic uncertainty resides primarily in earlier layers, the last-layer posterior will understate total uncertainty.

### Mechanism 3: Split-Conformal Calibration Guarantees Coverage Independent of Score Correctness
Regardless of posterior calibration, split-conformal rank calibration guarantees finite-sample marginal coverage `≥ 1-α` under exchangeability. The threshold `t` is determined by sorting calibration scores and selecting the `(m+1)α`-quantile. Break condition: under distribution shift or non-exchangeability, coverage guarantees may not hold.

## Foundational Learning

- **Concept: Split-Conformal Prediction**
  - Why needed: This is the calibration backbone that guarantees coverage for any measurable score
  - Quick check: Given calibration scores `{s₁,...,s_m}` sorted ascending, what is the threshold `t` that ensures `≥ 1-α` coverage under exchangeability?

- **Concept: Laplace Approximation in Bayesian Neural Networks**
  - Why needed: LLLA is the source of the posterior predictive and its epistemic uncertainty component
  - Quick check: Why does the Laplace approximation use a Gaussian to approximate the posterior, and what does the Hessian encode?

- **Concept: Aleatoric vs. Epistemic Uncertainty Decomposition**
  - Why needed: The diagnostics (`r(x)`, `tr(Σ)`, `ρ`) and method selection hinge on separating noise from parameter uncertainty
  - Quick check: On a dataset with 500K training points, would you expect `r(x)` to be large or small? Why?

## Architecture Onboarding

- **Component map:** Pretrained MLP backbone `φ(·)` → Last-Layer Laplace head (w_MAP, Σ, σ̂²) → Two-sided CDF score computation → Split-conformal calibration → Prediction intervals

- **Critical path:**
  1. Train backbone (or load pretrained)
  2. Fit LLLA head on D_tr → w_MAP, Σ, σ̂² (O(nd²) for HᵀH, O(d³) for Cholesky)
  3. Compute calibration scores on D_cal (O(md²))
  4. Apply conformal threshold; interval construction is O(d) per query

- **Design tradeoffs:**
  - Width vs. robustness: CLAPS optimizes for width under nested-score family; CTI trades width for higher coverage
  - Posterior fidelity vs. cost: Diagonal Laplace is faster but less expressive; full covariance captures input-dependent epistemic variance at O(d²) per query
  - Scale-learning vs. posterior-aware: CQR/Norm-CP learn input-dependent scales explicitly; CLAPS derives them from LLLA

- **Failure signatures:**
  - Posterior collapse: tr(Σ) → 0, r(x) → 0; intervals become homoscedastic and CLAPS loses advantage
  - Strong heteroscedasticity with low epistemic signal: ρ is large, r(x) small → scale-learning methods outperform
  - Backbone misspecification: LLLA underestimates uncertainty if features are poor

- **First 3 experiments:**
  1. Baseline replication on Airfoil/kin8nm: Implement CLAPS with 2-layer MLP (128 hidden units). Compare coverage and width against CP, Norm-CP, CQR. Expect CLAPS to have the narrowest intervals at ~0.90 coverage.
  2. Posterior contraction diagnostic: On each dataset, compute r(x), tr(Σ), and ρ on calibration data. Reproduce Table 4 and Table 5 patterns. Confirm YearPrediction shows near-zero r(x) and significant ρ.
  3. Ablation on λ and σ̂² estimator: Vary λ ∈ {0.1, 0.3, 1, 3, 10} and compare evidence-based vs. residual-based σ̂². Verify coverage is insensitive while width changes modestly.

## Open Questions the Paper Calls Out

- Can a hybrid conformal score interpolating between posterior-aware and scale-aware components improve efficiency across diverse data regimes?
  - The authors state that "a hybrid formulation that interpolates between a posterior-aware term and a learned scale term using r is a promising direction for future work."

- How can CLAPS be extended to guarantee coverage under covariate shift or non-exchangeable data distributions?
  - The paper suggests extending CLAPS to "localized coverage under covariate shift and non-exchangeable settings" as a necessary step to broaden applicability.

- Do structured factorizations (e.g., KFAC) or subnetwork Laplace approximations offer better interval efficiency than the last-layer approach?
  - The authors ask whether "moving beyond a single last-layer approximation via structured factorizations... could further improve tail fidelity."

## Limitations

- Coverage guarantees are marginal and distribution-dependent; strong covariate shift would invalidate claims
- MLP training details are underspecified, requiring reasonable defaults that may affect backbone quality
- Posterior contraction diagnostics assume no catastrophic forgetting in the backbone, which could invalidate LLLA epistemic estimates

## Confidence

- **High:** Split-conformal coverage guarantees, theorem statements on oracle efficiency and posterior contraction, empirical coverage results on benchmark datasets
- **Medium:** Comparative width results across methods (sensitive to training hyperparameters and dataset splits)
- **Low:** Claims about method selection in Table 5 for datasets not directly benchmarked (Boston Housing, Wine Quality)

## Next Checks

1. Implement CLAPS on Boston Housing and Wine Quality; verify Table 5's predicted superiority over scale-learning methods when r(x) is high
2. Test robustness to backbone misspecification by corrupting 20% of labels; measure degradation in posterior contraction metrics
3. Compare CLAPS to a diagonal Laplace variant to quantify cost-benefit of full covariance epistemic uncertainty