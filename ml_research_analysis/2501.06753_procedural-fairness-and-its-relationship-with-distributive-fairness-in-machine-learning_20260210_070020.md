---
ver: rpa2
title: Procedural Fairness and Its Relationship with Distributive Fairness in Machine
  Learning
arxiv_id: '2501.06753'
source_url: https://arxiv.org/abs/2501.06753
tags:
- fairness
- distributive
- procedural
- dataset
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the under-explored area of procedural fairness
  in machine learning (ML), which examines the fairness of the decision-making process,
  in contrast to the more commonly studied distributive fairness that focuses on decision
  outcomes. The authors propose a novel method to achieve procedural fairness during
  the ML model training phase by optimizing a procedural fairness metric, GPFF AE,
  using a regularization approach.
---

# Procedural Fairness and Its Relationship with Distributive Fairness in Machine Learning

## Quick Facts
- arXiv ID: 2501.06753
- Source URL: https://arxiv.org/abs/2501.06753
- Reference count: 12
- Key outcome: Introduces GPFF AE regularization approach to optimize procedural fairness during ML training, showing significant improvements across seven datasets while revealing the relationship between procedural and distributive fairness

## Executive Summary
This paper addresses the under-explored area of procedural fairness in machine learning, examining fairness in the decision-making process rather than just outcomes. The authors propose a novel method to achieve procedural fairness during ML model training by optimizing a procedural fairness metric, GPFF AE, using regularization. Through comprehensive experiments on synthetic and real-world datasets, they demonstrate that optimizing procedural fairness not only ensures fair decision-making processes but also improves distributive fairness by mitigating biases. The study reveals that dataset bias and procedural unfairness are primary sources of distributive unfairness, and that optimizing distributive fairness can come at the cost of procedural fairness.

## Method Summary
The authors propose a regularization approach to optimize procedural fairness during ML model training. They introduce the GPFF AE metric to measure procedural fairness and incorporate it as a regularization term in the model's objective function. This method is evaluated across one synthetic and six real-world datasets using binary classification tasks. The experimental setup compares models trained with procedural fairness regularization against baseline models and those optimized for distributive fairness metrics, examining the trade-offs and relationships between these two fairness concepts.

## Key Results
- The GPFF AE regularization approach significantly improves procedural fairness across all tested datasets
- Procedural fairness optimization also improves distributive fairness by mitigating dataset and model-introduced biases
- Optimizing for distributive fairness achieves fair outcomes but may compromise procedural fairness, favoring disadvantaged groups to counterbalance dataset biases

## Why This Works (Mechanism)
The regularization approach works by directly incorporating procedural fairness considerations into the model training objective, rather than treating fairness as a post-processing step. By optimizing the GPFF AE metric during training, the model learns to make decisions that are fair in their process, not just in their outcomes. This approach addresses both dataset bias (pre-existing unfairness in training data) and procedural unfairness (unfairness introduced by the model's decision-making process). The mechanism ensures that fairness considerations are integrated throughout the learning process, rather than being applied as an afterthought, leading to more robust and comprehensive fairness improvements.

## Foundational Learning

**Procedural Fairness**: Understanding fairness in the decision-making process itself (why needed: provides theoretical foundation for the study's focus beyond outcome fairness; quick check: Can identify procedural fairness metrics and their importance)

**Distributive Fairness**: Knowledge of fairness in decision outcomes (why needed: establishes the comparison baseline and relationship being studied; quick check: Can explain common distributive fairness metrics)

**Regularization in ML**: Understanding how regularization terms modify model training objectives (why needed: critical for understanding the proposed method's mechanism; quick check: Can describe how regularization affects optimization)

**Bias in ML**: Understanding different sources of bias (dataset bias, model bias) (why needed: essential for interpreting results and trade-offs; quick check: Can identify and distinguish between bias types)

**Binary Classification**: Understanding binary classification tasks and metrics (why needed: context for experimental setup; quick check: Can describe performance metrics for binary classification)

## Architecture Onboarding

Component map: Data Preprocessing -> Model Training (with GPFF AE regularization) -> Fairness Evaluation (procedural and distributive metrics)

Critical path: The key innovation is the integration of procedural fairness regularization during the training phase, where the GPFF AE metric is incorporated as a penalty term in the loss function, directly influencing how the model learns to make decisions.

Design tradeoffs: The approach trades computational complexity during training (additional regularization term) for improved fairness outcomes, and faces the classic fairness-accuracy trade-off that must be balanced based on application requirements.

Failure signatures: The method may fail when procedural fairness metrics are poorly defined for a domain, when computational resources are limited for the additional regularization, or when the relationship between procedural and distributive fairness is not well understood for the specific application context.

First experiments:
1. Implement the GPFF AE regularization on a simple synthetic dataset to verify the mechanism works as expected
2. Apply the approach to a standard benchmark dataset (like Adult Income) to compare with existing fairness methods
3. Conduct ablation studies varying the weight of the procedural fairness regularization term to find optimal trade-offs

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation based on limited dataset diversity (one synthetic, six real-world) may restrict generalizability
- Focus on binary classification tasks leaves unclear applicability to regression or multi-class problems
- Trade-offs between procedural and distributive fairness identified empirically but lack theoretical guarantees
- Computational cost analysis for large-scale applications is insufficient

## Confidence

High confidence: Core finding that GPFF AE regularization improves procedural fairness across multiple datasets

Medium confidence: Relationship between procedural and distributive fairness as identified through empirical observations

Medium confidence: Practical implications and trade-offs given limited scope of datasets and problem types

## Next Checks

1. Evaluate the GPFF AE regularization approach across diverse ML tasks (regression, multi-class classification) and larger, more varied datasets to test generalizability

2. Conduct ablation studies comparing different procedural fairness metrics and regularization techniques to establish robustness of the approach

3. Perform theoretical analysis to characterize the relationship between procedural and distributive fairness under various optimization objectives, moving beyond empirical observations