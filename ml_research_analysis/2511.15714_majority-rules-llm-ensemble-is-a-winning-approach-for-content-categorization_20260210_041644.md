---
ver: rpa2
title: 'Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization'
arxiv_id: '2511.15714'
source_url: https://arxiv.org/abs/2511.15714
tags:
- ensemble
- categorization
- llms
- performance
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that ensemble-based large language models
  (eLLMs) substantially outperform individual LLMs in hierarchical text categorization,
  achieving up to 65% improvement in F1-score. By combining predictions from multiple
  diverse LLMs under a collective decision-making framework, eLLMs mitigate hallucinations,
  reduce category inflation, and improve classification accuracy on the IAB taxonomy.
---

# Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization

## Quick Facts
- **arXiv ID:** 2511.15714
- **Source URL:** https://arxiv.org/abs/2511.15714
- **Reference count:** 16
- **Primary result:** eLLM ensembles achieve up to 65% improvement in F1-score over individual LLMs for hierarchical text categorization.

## Executive Summary
This study demonstrates that ensemble-based large language models (eLLMs) substantially outperform individual LLMs in hierarchical text categorization, achieving up to 65% improvement in F1-score. By combining predictions from multiple diverse LLMs under a collective decision-making framework, eLLMs mitigate hallucinations, reduce category inflation, and improve classification accuracy on the IAB taxonomy. Experiments with 10 LLMs on 8,660 documents show that even small ensembles surpass the best single model, with larger ensembles approaching human-expert performance. The approach offers a scalable, robust solution for taxonomy-based classification, though it requires increased computational cost.

## Method Summary
The framework employs zero-shot prompting with embedded taxonomy constraints, using stepwise hierarchical prompting (Tier-1→Tier-4) to navigate the IAB 2.2 taxonomy. Multiple LLMs generate raw category predictions, which are aggregated using the Collective Decision Making (CDM) algorithm. This algorithm computes relevance scores combining popularity (how many models selected a category), importance (hierarchical depth), and proximity (semantic closeness). Categories below consensus threshold τ=0.65 are filtered out, producing the final category set.

## Key Results
- eLLM ensembles achieve F1-score of 0.92 with 10 LLMs, compared to 0.55 for the best single model (+67% improvement)
- Even 2-LLM ensembles improve F1 by 33% over single models
- Consensus thresholding at τ=0.65 optimizes precision (1.00) and recall (0.80) trade-off
- eLLMs reduce category inflation and hallucinations compared to individual models

## Why This Works (Mechanism)

### Mechanism 1: Statistical Error Cancellation
Aggregating predictions from multiple diverse LLMs suppresses individual model errors through statistical cancellation. Each LLM produces independent predictions, and the CDM algorithm filters out categories that lack multi-model support. The independence assumption requires diverse architectures/training paradigms to prevent systematic shared errors.

### Mechanism 2: Consensus Thresholding
The CDM requires categories to exceed relevance score threshold τ before inclusion. Hallucinated categories typically appear in only one model's output (low popularity), resulting in R(c) below τ. This consensus requirement eliminates spurious predictions that lack broader support.

### Mechanism 3: Hierarchical Depth-Weighting
The relevance scoring prioritizes deeper taxonomy categories by assigning higher importance weights to categories with greater hierarchical depth. This counteracts models' tendency toward safe, high-level predictions and favors specific categories with multi-model agreement.

## Foundational Learning

- **Zero-shot taxonomy classification:** The framework operates without task-specific fine-tuning, relying on LLMs' native categorization capabilities. Quick check: Why might a model assign "Books and Literature" to a fantasy excerpt but miss "Young Adult Literature"?

- **Condorcet Jury Theorem:** Theoretical foundation explaining why majority voting works—if each model exceeds random accuracy and errors are independent, larger ensembles converge toward correct decisions. Quick check: Why does the theorem require independent errors, and what happens if all ensemble members share the same training data?

- **Hierarchical classification constraints:** IAB taxonomy requires parent-child consistency; naive ensembles could predict children without parents or vice versa. Quick check: If models predict {Fiction, Young Adult Literature}, which is the parent? Does your aggregation handle this?

## Architecture Onboarding

- **Component map:** Input Text → [LLM₁, LLM₂, ..., LLMₖ] → Raw Category Sets C₁...Cₖ → CDM Aggregator → Threshold Filter (τ) → Final Category Set

- **Critical path:**
  1. Prompt engineering: System prompt must include full valid category list to constrain outputs
  2. Model selection: Prioritize architectural diversity over just scale
  3. Threshold calibration: Run sweep on held-out annotated data (paper found τ=0.65 optimal)

- **Design tradeoffs:**
  - Ensemble size vs. cost: 2-model gives +33% F1; 10-model gives +67% but 5× inference cost
  - Precision vs. recall: Higher τ increases precision (1.00 at τ≥0.75) but drops recall (0.45 at τ=0.95)
  - Proprietary vs. open models: Open-source cheaper but may lack diversity; hybrid ensembles recommended

- **Failure signatures:**
  - Category inflation (IR > 1): Models over-predicting; increase τ
  - High hallucination rate: Invalid categories passing threshold; audit prompt constraints
  - Low recall (<0.7): Valid categories filtered; decrease τ or add more ensemble members

- **First 3 experiments:**
  1. Replicate threshold sweep on your own labeled data to calibrate optimal τ for your domain
  2. Test 2-model vs. 5-model ensemble on cost-per-correct-prediction basis
  3. Ablate depth-weighting (set Ic=1 for all categories) to verify hierarchical scoring contributes to your task

## Open Questions the Paper Calls Out

- **Minimal ensemble size:** What is the minimal ensemble size and specific model composition required to achieve near-maximal accuracy while minimizing inference cost? The paper identifies cost-performance optimization as necessary next step.

- **Adaptive ensembles:** Can adaptive ensembles dynamically select specific LLMs based on domain specialization or task complexity? The authors call for systems that select LLMs based on contextual relevance rather than using a static set.

- **Taxonomy scalability:** Does the eLLM framework maintain its performance advantage when applied to significantly larger or denser taxonomies? The authors note the IAB taxonomy is "sparse" compared to others like DMOZ (750,000 nodes).

## Limitations

- The ensemble framework requires access to multiple LLM APIs, creating cost and dependency constraints that scale linearly with ensemble size
- The independence assumption for error cancellation lacks direct validation and could fail if models share systematic biases
- The optimal consensus threshold τ=0.65 is derived from a single corpus and may require domain-specific retuning

## Confidence

- **High Confidence:** The core mechanism of CDM aggregation improving over single models is well-supported by systematic experiments
- **Medium Confidence:** The independence assumption for error cancellation lacks direct validation; the depth-weighting heuristic's effectiveness is asserted but not independently verified
- **Low Confidence:** Claims about hallucination rate reduction are supported only by indirect evidence; no direct measurement of hallucination frequency pre/post-ensemble is provided

## Next Checks

1. **Error Correlation Analysis:** Compute pairwise error overlap between ensemble models on a held-out validation set. If correlation exceeds 0.7 for common failure modes, the independence assumption breaks.

2. **Cross-Domain Threshold Calibration:** Apply the same ensemble framework to a different text categorization task and verify whether τ=0.65 remains optimal or requires domain-specific tuning.

3. **Hallucination Quantification:** Implement a systematic hallucination detection pipeline that flags categories not present in the ground truth taxonomy. Measure hallucination rate reduction specifically attributable to ensemble aggregation.