---
ver: rpa2
title: 'GEM: A Gym for Agentic LLMs'
arxiv_id: '2510.01051'
source_url: https://arxiv.org/abs/2510.01051
tags:
- learning
- environments
- training
- arxiv
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GEM provides a unified framework for training and evaluating agentic
  LLMs through multi-turn interactions with complex environments. It addresses the
  gap in standardized, flexible environments needed for reinforcement learning with
  language models, particularly for long-horizon tasks.
---

# GEM: A Gym for Agentic LLMs

## Quick Facts
- arXiv ID: 2510.01051
- Source URL: https://arxiv.org/abs/2510.01051
- Reference count: 40
- Primary result: Provides a unified framework for training and evaluating agentic LLMs through multi-turn interactions with complex environments, with 24 diverse environments and a Return Batch Normalization (ReBN) algorithm that improves multi-turn REINFORCE performance.

## Executive Summary
GEM provides a unified framework for training and evaluating agentic LLMs through multi-turn interactions with complex environments. It addresses the gap in standardized, flexible environments needed for reinforcement learning with language models, particularly for long-horizon tasks. The framework includes 24 diverse environments, support for tools like Python, search, and MCP, asynchronous vectorized execution for efficiency, and wrappers for extensibility. A key contribution is the Return Batch Normalization (ReBN) algorithm, which improves multi-turn REINFORCE by normalizing returns over the entire batch. Experiments show ReBN consistently outperforms vanilla REINFORCE and matches or exceeds PPO and GRPO across environments. The framework integrates with five major RL training libraries, supports vision-language tasks, and enables standardized evaluation of strong LLMs on tool use and terminal interaction tasks. GEM aims to accelerate agentic LLM research by providing a clean, decoupled infrastructure for training and evaluation.

## Method Summary
GEM introduces a unified framework for training agentic LLMs through multi-turn interactions with complex environments. The core innovation is Return Batch Normalization (ReBN), which normalizes returns across entire batches before computing policy gradients, enabling stable learning with sparse rewards and dense per-turn feedback. The framework supports 24 diverse environments including games, reasoning tasks, code generation, math problems, and QA, with tool integration (Python, search, MCP) and vision-language capabilities. Environments use a response-level action formulation where complete LLM responses are treated as single actions, enabling tractable multi-turn RL. The system provides asynchronous vectorized execution for efficiency and integrates with five major RL training libraries (Oat, Verl, OpenRLHF, ROLL, RL2). Training uses standard RL hyperparemeters with batch size B, learning rate 1e-6, and discount factors tuned per environment type (γ < 1 for efficiency-sensitive tasks).

## Key Results
- ReBN consistently improves vanilla REINFORCE by normalizing returns over entire batches, providing both positive and negative gradient signals even with sparse 0/1 rewards
- Setting γ < 1 naturally encourages agents to solve tasks in fewer turns, recovering optimal strategies like binary search without explicit turn limits
- Response-level action formulation enables tractable multi-turn RL with proper per-turn credit assignment, avoiding token-level episodes spanning thousands of steps
- ReBN matches or exceeds PPO and GRPO across environments while being compatible with dense per-turn rewards and arbitrary discount factors
- The framework supports standardized evaluation of strong LLMs on tool use and terminal interaction tasks with clean, decoupled infrastructure

## Why This Works (Mechanism)

### Mechanism 1: Return Batch Normalization (ReBN)
ReBN normalizes per-transition returns G_t over entire batches using A_ReBN,t = (G_t - mean(G)) / std(G). This creates negative gradients for below-average returns, crucial for exploration and efficient learning in multi-turn settings. Requires sufficient batch size and diverse trajectories. Evidence: consistently outperforms vanilla REINFORCE in math/QA and all other settings.

### Mechanism 2: Discount Factor γ < 1 for Efficiency Incentives
Lower discount factors reduce present value of future rewards, creating pressure to reach terminal rewards quickly. This recovers optimal strategies (e.g., binary search) without explicit turn limits. Incompatible with GRPO's trajectory-level formulation. Evidence: γ = 0.9 discovers binary search in ~5-6 turns while γ = 0.999 exhausts trials.

### Mechanism 3: Response-Level Action Formulation for Multi-Turn Credit Assignment
Treating complete LLM responses as single actions enables tractable multi-turn RL with proper per-turn credit assignment. Avoids token-level episodes while preserving turn-level advantage estimation. Evidence: REINFORCE outperforms GRPO in multi-turn environments (Sudoku, Minesweeper).

## Foundational Learning

- **Concept: Policy Gradient Methods (REINFORCE)**
  - Why needed here: ReBN builds directly on REINFORCE's Monte Carlo return estimation. Without understanding ∇θ log π(a|s) × G_t, the normalization mechanism is opaque.
  - Quick check question: Can you explain why REINFORCE uses log probabilities rather than raw probabilities in the gradient?

- **Concept: Credit Assignment in RL**
  - Why needed here: The paper's central critique of GRPO is its poor credit assignment—all turns share the same advantage. Understanding temporal credit assignment clarifies why ReBN matters.
  - Quick check question: In a 10-turn game, why would giving the same reward signal to all 10 actions be suboptimal?

- **Concept: Discount Factor (γ) and Return Calculation**
  - Why needed here: Section 4.2's binary search result hinges on understanding how γ affects the value of future rewards.
  - Quick check question: If γ = 0.9 and a reward of 1 arrives 5 turns later, what's its present value?

## Architecture Onboarding

- **Component map:**
  - gem.core.Env → Base environment class (reset, step)
  - gem.envs.registration.register() → Environment registry
  - Observation wrappers → Control state representation (full history vs. recent output)
  - Tool wrappers → Python/Search/MCP integration layers
  - Async vectorized execution → Parallel environment stepping with autoreset

- **Critical path:**
  1. Define task (inherit from Env or use existing dataset integration)
  2. Add tools via wrappers if multi-turn behavior desired
  3. Register environment with `register("namespace:name", class_path, **kwargs)`
  4. Configure ReBN in training loop (normalize returns over batch before gradient step)
  5. Set γ based on efficiency requirements (γ < 1 for speed incentives, γ ≈ 1 for outcome-focused tasks)

- **Design tradeoffs:**
  - GRPO vs. REINFORCE+ReBN: GRPO simpler for single-turn, ReBN necessary for dense rewards and γ < 1
  - Observation wrapper choice: Full history provides more context but hits token limits; recent-only loses long-term dependencies
  - Batch size vs. normalization quality: Smaller batches train faster but ReBN statistics become noisy

- **Failure signatures:**
  - Tool non-use: If ReBN not applied with 0/1 rewards, agents may never learn to call tools
  - Myopic behavior: γ too low → premature commitment without exploration
  - Convergence instability: Vanilla REINFORCE without normalization shows high variance
  - GRPO on multi-turn: Flat advantage estimation causes slow learning in dense-reward environments

- **First 3 experiments:**
  1. **Replicate γ study:** Train on `game:GuessTheNumber` with γ ∈ {0.9, 0.99, 0.999}. Verify that γ = 0.9 discovers binary search in ~5-6 turns while γ = 0.999 exhausts trials.
  2. **ReBN ablation:** Compare vanilla REINFORCE vs. ReBN on `math:Orz57K+Tool`. Track tool usage rate and episode length. Expect ReBN to induce tool use.
  3. **Cross-framework integration test:** Run the same environment (e.g., `rg:LetterCounting`) through two different training frameworks (Oat vs. OpenRLHF). Learning curves should align within stochastic variance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the learning of accurate critic networks be stabilized for multi-turn PPO in complex environments where it currently underperforms?
- Basis in paper: Section 4.1 notes that while PPO performs well in some tasks, it is "deemed difficult to robustly learn an accurate critic," explicitly "inviting future works to go in this direction."
- Why unresolved: The paper identifies the failure mode but proposes ReBN as a critic-free alternative rather than solving the instability of critic learning in multi-turn settings.
- What evidence would resolve it: An algorithmic modification or architectural change that allows PPO's critic to converge robustly in all GEM environments, matching or exceeding ReBN without the computational overhead of extensive rollouts.

### Open Question 2
- Question: Can multi-agent reinforcement learning effectively co-evolve user and assistant agents to achieve scalable, autonomous learning in conversational tasks?
- Basis in paper: Section D.3 states that the observation of user model strength impacting assistant performance "motivate[s] us to develop multi-agent RL to co-evolve user and assistant agents."
- Why unresolved: The paper evaluates static pairs of existing models but does not implement or validate a training loop where both agents update their policies simultaneously.
- What evidence would resolve it: A training curriculum showing that a co-evolved user/assistant pair achieves higher success rates or discovers more robust strategies than an assistant trained against a fixed, scripted user.

### Open Question 3
- Question: Does Return Batch Normalization (ReBN) maintain stability compared to PPO in environments with extremely long horizons (e.g., >100 turns)?
- Basis in paper: Section 2 mentions GEM supports tasks "over 100 turns," and Section 3.2 defines ReBN using Monte Carlo returns, which theoretically suffer from high variance in long trajectories compared to Temporal Difference methods used in PPO.
- Why unresolved: The paper demonstrates ReBN's success on included benchmarks but does not explicitly analyze how return estimator variance scales with episode length in longest-horizon tasks.
- What evidence would resolve it: A comparative analysis of gradient variance and learning stability between ReBN and PPO specifically on tasks exceeding 100 turns.

## Limitations

- Critical hyperparameters like batch size B are underspecified, directly affecting ReBN normalization reliability and gradient magnitudes
- Empirical validation predominantly uses small Qwen3 models (1.7B/4B) and relatively short training horizons (500 steps), leaving scaling uncertainty
- Binary search efficiency results depend on specific discount factor choices (γ=0.9) without sensitivity analysis or justification beyond empirical observation
- Lack of detailed reward shaping specifications per environment makes it difficult to assess whether performance gains are due to ReBN or implicit reward engineering

## Confidence

**High Confidence:** Response-level action formulation for multi-turn RL is well-grounded in standard practice. Critique of GRPO's trajectory-level advantage estimation is theoretically sound and supported by binary search experiment.

**Medium Confidence:** ReBN's effectiveness in tested environments is demonstrated, but lack of batch size specification and reward detail prevents confident generalization. Mechanism is plausible given related work on global advantage normalization.

**Low Confidence:** Claim that ReBN "matches or exceeds" PPO and GRPO across all environments is overstated. PPO struggles with long-horizon tasks due to known critic instability limitations rather than ReBN-specific advantages.

## Next Checks

1. **Batch Size Sensitivity Analysis:** Systematically evaluate ReBN performance across batch sizes B ∈ {32, 64, 128, 256} on `game:GuessTheNumber` with γ=0.9. Track both convergence speed and final performance to identify minimum viable batch size for stable normalization.

2. **Reward Shaping Ablation:** Implement three variants of `math:Orz57K+Tool`: (a) sparse 0/1 rewards with ReBN, (b) sparse 0/1 rewards without ReBN, (c) dense rewards (±1 per turn + tool success signals) with ReBN. Compare tool usage rates and episode lengths.

3. **Scaling Study:** Extend training to Qwen3-7B and Qwen3-14B models on representative environments (`game:Minesweeper`, `math:Orz57K`, `rg:LetterCounting`). Measure compute efficiency and final performance to assess whether ReBN benefits scale with model capacity.