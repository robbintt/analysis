---
ver: rpa2
title: Efficient Seq2seq Coreference Resolution Using Entity Representations
arxiv_id: '2510.14504'
source_url: https://arxiv.org/abs/2510.14504
tags:
- coreference
- incremental
- ontonotes
- computational
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes an efficient incremental coreference resolution
  method using compressed entity representations. The method extracts and re-organizes
  entity-level tokens while discarding most other input tokens, enabling faster processing
  in incremental settings like dialogue.
---

# Efficient Seq2seq Coreference Resolution Using Entity Representations

## Quick Facts
- **arXiv ID:** 2510.14504
- **Source URL:** https://arxiv.org/abs/2510.14504
- **Authors:** Matt Grenander; Shay B. Cohen; Mark Steedman
- **Reference count:** 21
- **Primary result:** Achieves 0.6 F1 points below full-prefix incremental baseline on OntoNotes while compressing input by 1.8x

## Executive Summary
This work proposes an efficient incremental coreference resolution method that compresses input by extracting entity-level tokens and discarding most other tokens. The approach maintains strong performance while reducing GPU memory usage by 1.9x compared to non-incremental models. The method particularly excels on LitBank, surpassing contemporary approaches and matching state-of-the-art performance. The key innovation is representing entities with compressed `<e><m>mention1</m>...<m>mentionK</m>|clusterID</e>` tokens, enabling faster processing in incremental settings like dialogue.

## Method Summary
The method processes text chunk-by-chunk using a seq2seq framework where only entity-level tokens are retained after each step. Entities are wrapped in `<e>...</e>` tokens containing all mentions and their cluster IDs, with recently-updated entities moved to the rightmost position to signal salience. The model uses T0-3B with Token Action schema, trained for 30 epochs on 4x NVIDIA RTX A6000 GPUs. Input consists of compressed entity blocks, a 100-token context window, and the current target chunk, enabling efficient incremental processing while maintaining coreference resolution performance.

## Key Results
- Best model achieves 0.6 F1 points below full-prefix incremental baseline on OntoNotes while compressing input length by 1.8x
- On LitBank, model surpasses contemporary methods and matches state-of-the-art performance
- Reduces GPU memory usage by 1.9x compared to non-incremental models
- Particularly struggles with named entities and definite noun phrases in OntoNotes due to lack of singleton annotation

## Why This Works (Mechanism)

### Mechanism 1: Entity-Centric Compression Reduces Redundancy While Preserving Coreference Signal
The method maintains performance by extracting only mention spans and cluster identities, discarding all other tokens. This works because coreference resolution primarily depends on mention spans and cluster identities rather than surrounding context. The compressed representation `<e><m>mention1</m>...<m>mentionK</m>|clusterID</e>` encodes sufficient information for resolving future coreferences, with discourse context beyond mentions being largely redundant for this task.

### Mechanism 2: Recency-Based Reordering Improves Entity Salience
Recently-mentioned entities are promoted to the rightmost position in the representation, reflecting the cognitive theory that recently-mentioned entities are more salient. This reordering signals to the model which entities are likely to be relevant for coreference decisions. The effect is modest but measurable, lifting CoNLL F1 by 0.18-0.45 points, suggesting the model learns to attend more strongly to rightmost entities.

### Mechanism 3: Incremental Processing Tradeoffs Favor Singleton-Annotated Datasets
The gap between incremental and non-incremental models is caused by OntoNotes' lack of singleton annotation, which forces early anaphoricity decisions without recovery options. In incremental settings, models must decide if a span is a mention before seeing later text. OntoNotes only annotates mentions that co-refer, creating noise when models encounter would-be singletons. LitBank's singleton annotation mitigates this by providing more accurate mention detection patterns.

## Foundational Learning

- **Seq2seq Coreference Formulation (Token Action)**: The base architecture uses T5-style models outputting annotated text with mention boundaries and cluster IDs. Understanding how `<m>mention|clusterID</m>` encoding works is prerequisite to grasping how compression preserves coreference information.

- **Incremental vs. Batch Processing**: The paper's central contribution is making seq2seq coreference work incrementally, where text is processed chunk-by-chunk without revising previous predictions. Understanding why existing methods requiring full output re-encoding each step are inefficient is crucial.

- **Coreference Evaluation Metrics (MUC, BÂ³, CEAFe)**: The paper analyzes performance through these metrics' lenses, with CEAFe's entity-focused recall revealing unique failure modes. Understanding which metric would be most affected if a model misses entire entity clusters rather than individual mentions is important.

## Architecture Onboarding

- **Component map**: Input Layer (chunk + entity memory + context) -> Encoder (T0) -> Decoder (T0) -> Memory Manager (extract mentions, update clusters, reorder, prune) -> Inference Constraints (modified vocabulary for valid syntax)

- **Critical path**: Initialize with empty entity memory; for each chunk, concatenate entity_blocks + context + target_chunk; run T0 encoder-decoder; extract mentions and assign/update cluster IDs; update entity memory (append new mentions, reorder recently-updated entities rightward); trim context to fixed 100-token window; repeat for next chunk.

- **Design tradeoffs**: Context length (0 vs. 50 vs. 100 vs. 200 tokens) - 100 is optimal; entity ordering provides small boost (0.18-0.45 F1) but adds complexity; chunk size fixed at 100 tokens per sentence boundary.

- **Failure signatures**: Pronoun resolution errors with speaker-dependent references lacking context; long-range named entity misses (40% of incremental-only errors); CEAFe recall crash indicating missing entire entities rather than mentions.

- **First 3 experiments**: 1) Reproduce Full-Prefix baseline using T0-3B with Token Action schema on OntoNotes (30 epochs) to validate 79.6 CoNLL F1 baseline. 2) Ablate context length (0, 50, 100, 200 tokens) with Entity-Centric representation to find efficiency frontier. 3) Test on LitBank to verify singleton annotation mitigates incremental gaps.

## Open Questions the Paper Calls Out

- **Can a model trained to explicitly predict mention likelihood (singletons) bridge the performance gap between incremental and non-incremental settings on datasets like OntoNotes?** The authors hypothesize that OntoNotes' lack of singleton annotation fundamentally limits incremental performance, but attempts using NER labels or pseudosingletons failed to close the gap.

- **Does integrating speaker tags or dialogue metadata into the Entity-Centric representation improve resolution of ambiguous pronouns that rely on narrative context?** The model fails to link context-dependent pronouns like "us" without speaker information, suggesting adding speaker tags may offer a simple solution.

- **Would a salience-based ordering heuristic (e.g., grammatical role or semantic focus) outperform the current recency-based re-ordering strategy?** The current recency ordering provides only marginal gains (0.18 F1), raising questions about whether more sophisticated salience measures could improve performance.

## Limitations

- OntoNotes annotation artifacts create unresolvable performance gaps due to lack of singleton annotation, forcing early anaphoricity decisions without recovery options
- Context window optimization remains empirical without theoretical justification for optimal 100-token setting
- Entity recency reordering effect is modest (0.18-0.45 F1) for added complexity

## Confidence

- **High Confidence**: Claims about memory efficiency gains (1.9x reduction) and basic compression ratios (1.8x input length reduction) are well-supported by direct measurements and ablation studies
- **Medium Confidence**: Performance comparisons on LitBank and singleton annotation effects are reasonably supported but limited by single dataset evaluation
- **Low Confidence**: Claims about entity recency reordering significantly improving salience are weakly supported with only modest F1 gains

## Next Checks

1. **Multi-dataset validation on singleton-rich corpora**: Evaluate the approach on additional singleton-annotated datasets beyond LitBank (e.g., PreCo, GAP) to determine whether performance improvements generalize and whether OntoNotes limitation is dataset-specific.

2. **Attention mechanism analysis**: Conduct attention weight analysis to verify the model actually attends more strongly to recently-promoted entities and that this explains observed performance gains from recency ordering.

3. **Context window generalization study**: Systematically test the approach across diverse document lengths and genres to determine whether the 100-token optimal window is truly universal or varies significantly with document characteristics.