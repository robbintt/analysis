---
ver: rpa2
title: 'MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information
  Gain in Semantic Space'
arxiv_id: '2504.13835'
source_url: https://arxiv.org/abs/2504.13835
tags:
- data
- information
- label
- quality
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MIG proposes an information-based method for automatic data selection
  in instruction tuning. It constructs a label graph to model the semantic space and
  quantifies dataset information by balancing quality and diversity through an upper-convex
  information score function with information propagation.
---

# MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space

## Quick Facts
- **arXiv ID:** 2504.13835
- **Source URL:** https://arxiv.org/abs/2504.13835
- **Reference count:** 40
- **Primary result:** MIG achieves +5.73% improvement on AlpacaEval and +6.89% on WildBench compared to full Tulu3 SFT model while using only 5% of the dataset

## Executive Summary
MIG introduces an information-theoretic framework for automatic data selection in instruction tuning that operates in "semantic space" rather than embedding space. The method constructs a label graph to model semantic relationships and quantifies dataset information through a submodular information score function that balances quality and diversity. By iteratively selecting data points that maximize information gain on this graph, MIG achieves state-of-the-art performance across multiple benchmarks while using significantly less data than traditional full-dataset training approaches.

## Method Summary
MIG automatically selects high-quality, diverse instruction-tuning data by constructing a label graph where nodes represent semantic categories and edges capture textual similarity. The method integrates instance-level quality scores (DEITA) into the graph information stream, allowing high-quality samples to inject more information into relevant labels. Using a submodular information measure with a diminishing-returns function, MIG greedily selects data points that maximize marginal information gain, ensuring balanced coverage across semantic categories while prioritizing quality.

## Key Results
- Achieves +5.73% improvement on AlpacaEval and +6.89% on WildBench versus full Tulu3 SFT model
- Uses only 5% of the dataset (50K samples) compared to full 939K Tulu3 training
- Demonstrates >100x speedup over embedding-based pairwise selection methods
- Shows consistent superiority across multiple base models (Llama3.1-8B, Qwen2.5-7B, Mistral-7B)

## Why This Works (Mechanism)

### Mechanism 1: Submodular Information Maximization
The greedy selection strategy is effective because the defined dataset information function $E(D)$ is submodular, providing theoretical guarantees that iterative selection approximates the optimal subset. By applying a concave function $\phi$ to accumulated label information, the system exhibits diminishing returns - as specific labels become saturated, marginal gains decrease, forcing selection of under-represented labels to maximize total score. This naturally enforces the desired balance between quality (intensity) and diversity (spread).

### Mechanism 2: Semantic Graph Propagation
Modeling semantic space as a graph with information propagation provides more robust diversity representation than independent label counting. Raw data points map to labels, but information propagates across graph edges based on textual similarity, distributing a data point's value across semantically related neighbors. This prevents redundancy that's obvious in semantic space but distinct in literal label space, ensuring truly diverse coverage rather than superficial label variety.

### Mechanism 3: Quality-Weighted Label Injection
Integrating instance-level quality scores directly into the graph information stream biases selection toward high-utility samples while maintaining diversity constraints. Data points inject information into their associated labels proportional to quality scores, meaning high-quality data provides a larger "pulse" of information. This makes quality samples more likely to be selected even if their labels are somewhat saturated, provided marginal gain remains positive.

## Foundational Learning

- **Concept: Submodular Function Maximization**
  - **Why needed here:** The entire selection logic relies on submodularity to justify the greedy algorithm as an approximation of an NP-hard problem
  - **Quick check question:** Why does a "diminishing returns" (concave) function naturally enforce diversity in a selection set?

- **Concept: Semantic Space vs. Embedding Space**
  - **Why needed here:** The paper explicitly critiques prior work for operating in "embedding space" rather than "semantic space" - understanding this distinction is key to understanding the architecture
  - **Quick check question:** How does MIG define the relationship between two data points compared to k-Means clustering on embeddings?

- **Concept: Facility Location Functions**
  - **Why needed here:** The paper compares its information measure to "facility location functions" used in prior diversity work
  - **Quick check question:** What is the computational complexity disadvantage of facility location functions that MIG aims to solve?

## Architecture Onboarding

- **Component map:** Data Pool -> Annotators (Tagger + Scorer) -> Graph Builder -> MIG Sampler -> SFT Trainer
- **Critical path:** The definition of the **Label Graph** (Node count and Edge threshold). If the graph is too sparse, diversity suffers; if too dense, the signal is diluted. The paper identifies a "sweet spot" (e.g., 4531 nodes for Tulu3) which must be tuned via grid search for new datasets.
- **Design tradeoffs:**
  - **Label Granularity:** More labels = finer diversity control but higher computational cost and potential for noise (outlier tags). Fewer labels = risk of collapsing distinct concepts.
  - **Scorer Dependency:** MIG is agnostic to the scorer, but performance is upper-bounded by the scorer's accuracy. The paper found DEITA scores superior to IFD or Tag Count.
- **Failure signatures:**
  - **Performance Plateau:** If the model fails to improve on specific benchmarks (e.g., Math/Coding), check if the Label Graph has sufficient granularity in those domains or if the quality scorer penalizes the brevity typical of those tasks.
  - **High Variance:** If selected data is repetitive, check the Information Score function $\phi$ (Eq 12/13); the derivative might be decaying too slowly, failing to penalize redundancy.
- **First 3 experiments:**
  1. **Hyper-parameter Grid Search:** Run MIG on a validation slice varying the *Node Count* and *Edge Threshold* to find the optimal graph topology before full selection.
  2. **Ablation on $\phi(x)$:** Compare $\phi(x) = 1 - e^{-\alpha x}$ vs $\phi(x) = x^{\alpha}$ on a small budget (e.g., 10K samples) to determine which diminishing return curve fits the data distribution better.
  3. **Efficiency Benchmark:** Profile the sampler on the full dataset to confirm the >100x speedup over embedding-based pairwise methods holds in your infrastructure.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the optimal structure of the label graph, specifically the node count (label set size) and edge density (similarity threshold), be automatically determined for a given data pool without extensive empirical search? The authors note there is no universally optimal solution, as the ideal label graph depends on the characteristics of the data pool.

- **Open Question 2:** Can the information score function $\phi$ be effectively customized for individual labels rather than applied globally, and would this enhance the flexibility of the selection process? Future work could focus on developing methods to automatically determine the parameters in MIG, such as customizing the information score function for each label.

- **Open Question 3:** To what extent does the reliance on external quality scorers (e.g., DEITA scores) limit the method's ability to capture intrinsic data quality in domains where such scorers are unavailable or biased? The paper does not propose a method to generate $s_i$ internally or correct for scorer bias.

## Limitations

- **Semantic Graph Dependency:** The method assumes InsTagger-generated labels adequately capture semantic distinctions, which may fail for domains with subtle task variations or nuanced instructions.
- **Scorer Reliability:** Performance is upper-bounded by the quality scorer's accuracy - if DEITA's judgment doesn't align with human preferences for specific task types (e.g., coding or reasoning), MIG optimizes for the wrong objective.
- **Hyperparameter Sensitivity:** The graph construction requires careful tuning of threshold T and node count that may not generalize across domains without domain-specific adjustment.

## Confidence

- **High Confidence:** The submodularity framework and greedy approximation algorithm are mathematically sound and well-established. The reported efficiency gains (>100x speedup) are plausible given the computational complexity reduction.
- **Medium Confidence:** The experimental results showing consistent outperformance across multiple benchmarks are compelling, but evaluation relies on AlpacaEval's preference judgments, which have known limitations.
- **Low Confidence:** The assertion that MIG operates in "semantic space" rather than "embedding space" is somewhat semantic itself - the method still depends on embeddings for graph construction.

## Next Checks

1. **Domain Transfer Test:** Apply MIG to a dataset from a different domain (e.g., scientific literature or code documentation) with domain-specific annotators to verify the graph construction and selection logic generalizes beyond conversational data.

2. **Scorer Robustness Test:** Replace DEITA with an alternative quality scorer (e.g., GPT-4V-based scoring) and retrain the same model family to determine if performance gains persist when the quality signal changes.

3. **Label Granularity Stress Test:** Systematically vary the InsTagger's output granularity (from broad to fine-grained labels) on a fixed dataset to identify the optimal label resolution for different task types and verify the claimed tradeoff between diversity control and computational cost.