---
ver: rpa2
title: 'Heart Disease Prediction: A Comparative Study of Optimisers Performance in
  Deep Neural Networks'
arxiv_id: '2509.08499'
source_url: https://arxiv.org/abs/2509.08499
tags:
- learning
- training
- convergence
- optimizer
- optimizers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the performance of 10 optimizers in training
  a deep neural network for heart disease prediction using a publicly available dataset.
  The key outcome is that RMSProp demonstrated the most balanced performance across
  key metrics: it achieved the highest precision (0.765), strong recall (0.827), and
  a solid AUC score (0.841), along with faster convergence (18 epochs).'
---

# Heart Disease Prediction: A Comparative Study of Optimisers Performance in Deep Neural Networks

## Quick Facts
- arXiv ID: 2509.08499
- Source URL: https://arxiv.org/abs/2509.08499
- Reference count: 40
- A comparative analysis of 10 optimizers for heart disease prediction using deep neural networks

## Executive Summary
This study systematically evaluates the performance of 10 different optimizers in training deep neural networks for heart disease prediction using a publicly available dataset. The research employs a comprehensive methodology including data preprocessing, 10-fold cross-validation, dropout regularization, and early stopping to compare optimizer performance across multiple metrics including accuracy, precision, recall, F1-score, ROC-AUC, and convergence speed. The study reveals significant trade-offs between convergence speed and stability among different optimizers, with RMSProp demonstrating the most balanced performance overall.

## Method Summary
The study utilized a publicly available heart disease dataset from the UCI Machine Learning Repository containing 303 samples with 13 features. The methodology included data preprocessing with normalization and handling of missing values, implementation of 10-fold cross-validation to ensure robust performance evaluation, and incorporation of dropout regularization and early stopping to prevent overfitting. A deep neural network architecture with 3 hidden layers (8-16-8 neurons) was employed, and 10 different optimizers were systematically compared including SGD, Adam, Adamax, Nadam, Adagrad, Adadelta, RMSProp, FTRL, AMSGrad, and NadamAX. Performance metrics included accuracy, precision, recall, F1-score, ROC-AUC, and convergence speed measured by epochs to optimal performance.

## Key Results
- RMSProp achieved the highest precision (0.765), strong recall (0.827), and solid AUC score (0.841) with fastest convergence (18 epochs)
- Adagrad and Adadelta showed higher stability but slower convergence compared to other optimizers
- The final model with dropout regularization, hyperparameter tuning, and early stopping achieved improved ROC-AUC score of 92%

## Why This Works (Mechanism)
The study demonstrates that different optimizers have distinct convergence characteristics and stability profiles when training deep neural networks for medical diagnosis tasks. RMSProp's adaptive learning rate mechanism allows it to navigate the optimization landscape effectively, achieving both fast convergence and stable performance across different folds. The trade-off between speed and stability observed in optimizers like Adagrad and Adadelta highlights the importance of balancing exploration of the parameter space with exploitation of promising regions. The combination of dropout regularization, hyperparameter tuning, and early stopping creates a robust framework that mitigates overfitting while allowing optimizers to find optimal solutions.

## Foundational Learning
- **Optimizer mechanics**: Understanding how different optimization algorithms update weights during training is crucial for selecting appropriate methods for specific tasks - quick check: verify weight update equations for each optimizer
- **Cross-validation**: Essential for obtaining reliable performance estimates and preventing overfitting in medical datasets - quick check: ensure folds maintain class distribution
- **Regularization techniques**: Dropout and early stopping prevent overfitting and improve model generalization - quick check: monitor validation loss during training
- **Performance metrics**: Different metrics (precision, recall, AUC) provide complementary views of model performance, especially important in imbalanced medical datasets - quick check: calculate confusion matrix for threshold analysis
- **Hyperparameter tuning**: Systematic exploration of learning rates and other parameters significantly impacts optimizer performance - quick check: use grid or random search methodology
- **Convergence analysis**: Understanding training dynamics helps identify optimal stopping points and optimizer characteristics - quick check: plot training/validation curves

## Architecture Onboarding
- **Component map**: Dataset -> Preprocessing -> 10-Fold CV -> DNN (3 hidden layers: 8-16-8 neurons) -> Optimizer (10 variants) -> Performance Metrics
- **Critical path**: Data preprocessing → Model training with optimizer → Cross-validation → Performance evaluation → Hyperparameter tuning → Final model with dropout + early stopping
- **Design tradeoffs**: Speed vs. stability (Adagrad/Adadelta slower but more stable vs. RMSProp faster but potentially less stable), complexity vs. interpretability (deep network vs. simpler models)
- **Failure signatures**: Overfitting indicated by divergence between training and validation loss, optimizer instability shown by erratic loss curves, poor convergence indicated by plateaued loss
- **First experiments**: 1) Compare training/validation loss curves for each optimizer, 2) Analyze precision-recall trade-offs across different threshold settings, 3) Evaluate model performance on temporal splits if temporal data available

## Open Questions the Paper Calls Out
None

## Limitations
- Relatively small dataset size (303 samples) may limit generalizability and increase overfitting risk
- Single dataset focus may not capture full variability of heart disease presentations across different populations
- Limited hyperparameter tuning for each optimizer, leaving potential for improved performance unexplored

## Confidence
- High confidence: Comparative performance of different optimizers based on reported metrics
- Medium confidence: Impact of dropout regularization and early stopping on model performance
- Medium confidence: Claim of superiority for healthcare applications given limited validation scope

## Next Checks
1. Test the model on multiple heart disease datasets from different sources to assess generalizability
2. Conduct more comprehensive hyperparameter search for each optimizer to determine if performance differences persist
3. Perform cross-validation with temporal splits if temporal data is available to ensure model performs well on future data points