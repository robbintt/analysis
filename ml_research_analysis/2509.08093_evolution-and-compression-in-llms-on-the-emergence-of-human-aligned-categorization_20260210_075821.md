---
ver: rpa2
title: 'Evolution and compression in LLMs: On the emergence of human-aligned categorization'
arxiv_id: '2509.08093'
source_url: https://arxiv.org/abs/2509.08093
tags:
- color
- systems
- llms
- naming
- iicll
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether large language models (LLMs) can
  develop human-like semantic categories, focusing on color naming. Using a cognitively-motivated
  framework based on the Information Bottleneck principle, the authors replicate two
  influential human studies: an English color-naming task and a cultural transmission
  experiment via Iterated in-Context Language Learning (IICLL).'
---

# Evolution and compression in LLMs: On the emergence of human-aligned categorization

## Quick Facts
- arXiv ID: 2509.08093
- Source URL: https://arxiv.org/abs/2509.08093
- Authors: Nathaniel Imel; Noga Zaslavsky
- Reference count: 40
- Most LLMs fail to capture English color system, but larger, instruction-tuned models achieve higher alignment and efficiency

## Executive Summary
This study investigates whether large language models can develop human-like semantic categories, focusing on color naming. Using a cognitively-motivated framework based on the Information Bottleneck principle, the authors replicate two influential human studies: an English color-naming task and a cultural transmission experiment via Iterated in-Context Language Learning (IICLL). The findings show that most LLMs fail to capture the English color system, but larger, instruction-tuned models achieve higher alignment and efficiency. Through IICLL, LLMs iteratively restructure random color systems toward greater IB-efficiency and human-alignment, with Gemini 2.0 uniquely recapitulating the wide range of near-optimal tradeoffs observed in humans.

## Method Summary
The study employs two main experimental paradigms. First, models are prompted to name all 330 World Color Survey color chips using English color terms, measuring alignment to human English naming and Information Bottleneck efficiency. Second, IICLL simulates cultural transmission by iteratively training models on sampled stimulus-label pairs from previous generations, starting from random color partitions. The framework uses constrained decoding, sliding conversation windows, and monitors for degenerate collapse. Models are evaluated on their ability to sustain structured systems and converge toward human-like IB-efficient tradeoffs.

## Key Results
- Most LLMs fail to reproduce coherent color vocabularies when prompted with 330 WCS color chips
- Larger, instruction-tuned models (e.g., Gemini 2.0, Gemma 3 27B) closely approximate human English color naming and achieve near-optimal IB tradeoffs
- Through IICLL, LLMs iteratively restructure random color systems toward greater IB-efficiency and human-alignment, with Gemini 2.0 uniquely recapitulating the wide range of near-optimal tradeoffs observed in humans

## Why This Works (Mechanism)

### Mechanism 1: Information Bottleneck Efficiency Emergence
- Claim: LLMs develop IB-efficient semantic systems through implicit optimization of compression-accuracy tradeoffs, despite not being trained for this objective
- Mechanism: Models implicitly balance minimizing lexicon complexity I(M;W) against maximizing accuracy I(W;U), converging toward solutions on the theoretical IB bound
- Core assumption: IB-efficiency may be an intrinsic property that emerges to support intelligent categorization behavior, present in both humans and sufficiently capable LLMs
- Evidence anchors:
  - [abstract] "These findings demonstrate how human-aligned semantic categories can emerge in LLMs via the same fundamental principle that underlies semantic efficiency in humans"
  - [section 4.2] "Over generations, LLM systems become (i) more efficient in their mapping of stimuli to terms, (ii) more similar to naturally occurring human color systems"
  - [corpus] "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning" applies Information Bottleneck framework to compare compression-meaning tradeoffs between humans and LLMs

### Mechanism 2: Iterated Learning Reveals Inductive Biases
- Claim: Cultural transmission simulation via iterated learning exposes models' intrinsic priors toward structured, human-aligned categorization
- Mechanism: Multi-generation learning chains where each generation receives sampled stimulus-label pairs from previous generation; under Bayesian assumptions, this Markov chain converges to learners' prior distribution p(L) over category systems
- Core assumption: Models with strong in-context learning can generalize from limited training examples (6× vocabulary size) to full 330-chip stimulus space, revealing underlying structural biases
- Evidence anchors:
  - [abstract] "To test whether these LLMs simply mimic patterns in their training data or actually exhibit a human-like inductive bias toward IB-efficiency, we simulate cultural evolution of pseudo color-naming systems in LLMs via IICLL"
  - [section 4.2] "Gemini develops color naming systems that converge to a similar range of near-optimal IB solutions as the typological patterns of the WCS languages"
  - [corpus] No corpus papers directly address iterated learning paradigms for bias elicitation in LLMs

### Mechanism 3: Scale and Instruction Tuning Jointly Enable Category Formation
- Claim: Human-aligned categorization requires both sufficient model scale AND instruction tuning; neither alone is sufficient
- Mechanism: Larger models capture complex category boundaries in high-dimensional semantic space; instruction tuning shapes priors toward coherent, structured, human-aligned outputs rather than unstructured text prediction
- Core assumption: Category alignment emerges from interaction of representational capacity and training objective alignment with human communication patterns
- Evidence anchors:
  - [abstract] "larger, instruction-tuned models (e.g., Gemini 2.0, Gemma 3 27B) closely approximate human English color naming and achieve near-optimal IB tradeoffs"
  - [section 4.1] "many state-of-the-art pretrained LLMs struggle to reproduce any coherent color vocabulary"
  - [corpus] "Human-Aligned Bench" evaluates fine-grained reasoning alignment between MLLMs and humans

## Foundational Learning

- Concept: **Information Bottleneck Tradeoff (IB Principle)**
  - Why needed here: Core theoretical framework explaining why human semantic systems achieve near-optimal compression; directly operationalized in evaluation metrics
  - Quick check question: Given complexity I(M;W)=3.2 bits and accuracy I(W;U)=2.1 bits, can you calculate the IB objective Fβ[q] for β=2 and explain what varying β represents?

- Concept: **Iterated Learning Convergence**
  - Why needed here: Theoretical foundation for why IICLL reveals inductive biases; explains connection between transmission dynamics and prior distributions
  - Quick check question: Under what conditions does iterated learning converge to learners' prior p(L), and what does rapid convergence in 4 generations suggest about bias strength?

- Concept: **In-Context Learning Capacity and Generalization**
  - Why needed here: IICLL requires models to integrate dozens of training examples and generalize across full 330-stimulus space; capacity differences explain model hierarchy
  - Quick check question: Why must the k=14 condition (84 training examples) be considered an "extreme-case sanity check," and what does failure on this condition indicate about model limitations?

## Architecture Onboarding

- Component map:
  - **Stimulus encoding**: 330 WCS color chips encoded as sRGB triples [0-1] for text models; 10×10 pixel patches for multimodal models
  - **Prompt architecture**: Training block (6× vocabulary size examples) + sliding conversation window (10 interactions) + current query
  - **Constrained decoding**: Log-probability scoring over allowed vocabulary list; Gemini API controlled generation for closed models
  - **Iteration loop**: Sample pairs from generation t → construct training set → run production across all stimuli → detect degenerate collapse

- Critical path:
  1. **Comprehension validation**: Verify model retrieves correct labels from training examples (~80% accuracy threshold); unstable models rejected
  2. **Production phase**: Query all 330 stimuli in randomized order; track consistency with training data
  3. **Generation transition**: Sample 6× vocabulary pairs uniformly from produced system; initialize next generation
  4. **Convergence monitoring**: Track IB trajectory; terminate at 13 generations or on degenerate collapse (single-term system)

- Design tradeoffs:
  - **Sliding window size**: 0→frequent degenerate collapse; 10→stable convergence; 20-50→no significant improvement over 10
  - **Input modality**: sRGB outperforms CIELAB for text models (degraded performance in CIELAB replication); images only help below ~3 bits complexity
  - **Vocabulary size constraints**: k=2-6 stable across capable models; k=14 differentiates Gemini (sustains structure) from others (immediate collapse)
  - **Sampling strategy**: Uniform sampling (not need-based prior) to match human experiment protocol

- Failure signatures:
  - **Degenerate collapse**: All 330 stimuli mapped to single term; indicates insufficient in-context capacity or coherence (small models)
  - **Incoherent boundaries**: Mode maps show random/noisy category partitions; indicates lack of perceptual grounding (base models)
  - **Low-complexity trap**: Systems converge to minimal-complexity solutions on IB bound, missing full complexity range of human languages (most models except Gemini)
  - **Comprehension failure**: <80% accuracy retrieving training labels; indicates model cannot reliably integrate in-context examples

- First 3 experiments:
  1. **English naming baseline across model families**: Prompt each model with all 330 WCS chips using 14 basic English color terms; measure English-alignment (1-NID), IB-efficiency (ε), and complexity to establish capability hierarchy
  2. **Single-generation IICLL stability test**: Initialize k=3 chain with random partitions from Xu et al. (2013); run one complete generation; verify model maintains non-degenerate structure and >80% training consistency
  3. **Scale ablation within single family**: Compare Gemma 3 1B, 4B, 12B, 27B (all instruction-tuned) on k=4 condition to isolate parameter count effects on achievable complexity range and IB-efficiency trajectory

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does integrating functional communicative pressure into the IICLL framework result in the emergence of more robust or human-aligned semantic systems?
- **Basis in paper:** [explicit] The authors state, "an important future direction is to extend our IICLL framework to incorporate communication as a selective pressure," noting that current work relies solely on cultural transmission.
- **Why unresolved:** The current study demonstrates that transmission alone is sufficient for efficiency, but it does not test the combined effect of transmission and communicative utility, which is central to human language evolution.
- **What evidence would resolve it:** Experiments where IICLL chains are selected based on successful information transfer (communicative accuracy) rather than just reproduction, observing if systems converge more rapidly or differently to the IB bound.

### Open Question 2
- **Question:** What are the precise origins of the inductive bias toward IB-efficiency observed in LLMs (e.g., training data distribution vs. instruction-tuning)?
- **Basis in paper:** [explicit] The authors ask, "how might this bias emerge from properties of the training data, instruction-tuning, or model size," and identify investigating this as an important direction.
- **Why unresolved:** The study establishes a correlation between model size/tuning and efficiency but does not isolate the causal mechanisms creating the bias.
- **What evidence would resolve it:** Controlled ablation studies comparing models trained on different data distributions (e.g., random text vs. natural text) or comparing base models before and after specific fine-tuning stages.

### Open Question 3
- **Question:** Does the capacity to evolve human-aligned, IB-efficient categories via IICLL generalize to semantic domains beyond color?
- **Basis in paper:** [explicit] The authors list "extending our analyses across more languages and semantic domains" as a future direction, noting that while Shepard circles provided initial evidence, a full-scale analysis is needed.
- **Why unresolved:** Color is a uniquely well-documented domain with specific perceptual grounding; it remains unproven if LLMs can evolve efficient categories for abstract or less perceptually grounded domains (e.g., emotions, temporal concepts) using this method.
- **What evidence would resolve it:** Applying the IICLL methodology to diverse semantic domains and measuring convergence toward theoretical IB optima for those specific spaces.

## Limitations

- Generalization beyond color domain: Color provides a continuous perceptual space with relatively simple geometric properties, which may make category formation easier than in discrete or culturally-specific domains
- Training data contamination concerns: Cannot completely exclude that models learned partial color categorization patterns during pretraining
- In-context learning as proxy for intrinsic bias: IICLL reveals what models can do with limited examples but may conflate architectural priors with optimization dynamics specific to in-context learning

## Confidence

- **High Confidence**: Information Bottleneck efficiency emergence mechanism, Iterated Learning convergence theory, Scale and instruction tuning joint requirements
- **Medium Confidence**: Generalization across color naming domains, Role of IB framework in explaining human-LLM parallels, In-context learning capacity constraints
- **Low Confidence**: Domain generalization beyond color, Training data contamination effects, IB framework limitations

## Next Checks

1. **Cross-domain category emergence**: Apply IICLL methodology to non-perceptual semantic domains (e.g., kinship systems, spatial prepositions) to test whether IB-efficient human-aligned categories emerge across qualitatively different semantic spaces

2. **Architectural ablation study**: Systematically vary attention patterns, embedding dimensionality, and layer count in controlled experiments to isolate which architectural features most strongly predict human-aligned category formation capacity

3. **Out-of-distribution color spaces**: Test IICLL on color systems using perceptually uniform spaces (e.g., CIELAB) or non-spectral stimuli (e.g., texture, shape) to determine whether human-aligned category emergence depends on specific perceptual regularities in the WCS color space