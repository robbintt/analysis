---
ver: rpa2
title: 'Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation'
arxiv_id: '2506.07617'
source_url: https://arxiv.org/abs/2506.07617
tags:
- dialect
- hutsul
- translation
- ukrainian
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first effort to adapt large language models
  (LLMs) to the Ukrainian Hutsul dialect, a low-resource and morphologically complex
  dialect spoken in the Carpathian Highlands. The authors created a parallel corpus
  of 9,852 Hutsul-to-standard Ukrainian sentence pairs and a dictionary of 7,320 dialectal
  word mappings, then expanded the corpus with 52,142 synthetic examples using an
  advanced Retrieval-Augmented Generation (RAG) pipeline.
---

# Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation

## Quick Facts
- arXiv ID: 2506.07617
- Source URL: https://arxiv.org/abs/2506.07617
- Reference count: 13
- Primary result: Fine-tuned Mistral-7B achieved BLEU 74.35, chrF++ 81.89, and dialectal quality score 3.60/5 on Hutsul-to-standard Ukrainian translation, outperforming GPT-4o baseline

## Executive Summary
This paper presents the first effort to adapt large language models to the Ukrainian Hutsul dialect, a low-resource and morphologically complex dialect spoken in the Carpathian Highlands. The authors created a parallel corpus of 9,852 Hutsul-to-standard Ukrainian sentence pairs and a dictionary of 7,320 dialectal word mappings, then expanded the corpus with 52,142 synthetic examples using an advanced Retrieval-Augmented Generation (RAG) pipeline. They fine-tuned multiple open-source LLMs (Mistral-7B and LLaMA-3.1-8B) using LoRA for the task of standard-to-dialect translation. Evaluation using BLEU, chrF++, TER, and GPT-4o-based judgment showed that fine-tuned models outperformed the zero-shot GPT-4o baseline across both automatic and LLM-evaluated metrics.

## Method Summary
The authors created a parallel corpus of 9,852 Hutsul-to-standard Ukrainian sentence pairs from literary sources and web scraping, plus a dictionary of 7,320 dialectal word mappings. They expanded the corpus with 52,142 synthetic examples using a RAG pipeline that retrieves semantically similar dialectal sentences from a source corpus ("Дiдо Иванчiк"), then applies explicitly extracted grammar rules (phonological shifts, morphological alternations) via GPT-4o to generate new standard-to-dialect pairs. After synthetic generation, they filtered pairs using alignment metrics (U-src, U-tgt, X) computed by fast_align. Finally, they fine-tuned Mistral-7B-Instruct and LLaMA-3.1-8B using LoRA on the combined manual and synthetic corpus for 3 epochs.

## Key Results
- Fine-tuned Mistral model achieved BLEU 74.35, chrF++ 81.89, and dialectal quality score 3.60/5
- Outperformed zero-shot GPT-4o baseline (BLEU 56.64, chrF++ 65.90, dialect quality 3.22/5)
- Manual-only training yielded BLEU 72.65 and chrF++ 80.73, showing synthetic data provides valuable coverage
- LoRA fine-tuning enabled 7B-parameter models to adapt effectively with limited computational resources

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieval-Augmented Generation with structured linguistic rules produces usable synthetic parallel data for dialectal translation when authentic corpora are scarce.
- **Mechanism:** A RAG pipeline retrieves semantically similar dialectal sentences from a source corpus ("Дiдо Иванчiк"), then GPT-4o applies explicitly extracted grammar rules (phonological shifts, morphological alternations) to generate new standard-to-dialect pairs. The retrieved examples provide lexical and syntactic templates while rules constrain transformations.
- **Core assumption:** The dialect's grammatical transformations can be captured as explicit rules and the source corpus's vocabulary coverage is sufficient for the target domain.
- **Evidence anchors:** [abstract] "proposing an advanced Retrieval-Augmented Generation (RAG) pipeline to generate synthetic parallel translation pairs, expanding the corpus with 52142 examples"; [Section 3.3] Describes grammar rule extraction, indexing via text-embedding-3-large, and prompt construction with top-3 retrieved examples; [corpus] Related work on dialect MT uses similar augmentation strategies but without the RAG-grammar hybrid approach.
- **Break condition:** If the source corpus lacks lexical diversity (e.g., no technology/politics vocabulary), synthetic data will not generalize to uncovered domains.

### Mechanism 2
- **Claim:** Alignment-based filtering removes structurally divergent synthetic pairs, improving dataset quality beyond raw generation output.
- **Mechanism:** After synthetic generation, compute alignment metrics (U-src, U-tgt for unaligned characters; X for crossing alignments) using fast_align. Filter pairs exceeding empirical thresholds (U-src < 0.1, U-tgt < 0.1, X < 0.2). This removes hallucinated or structurally mismatched translations.
- **Core assumption:** Lower alignment divergence correlates with translation quality for closely related language varieties.
- **Evidence anchors:** [Section 3.3] Table 1 shows filtered synthetic data improved from U-src=0.139 to 0.005, U-tgt=0.136 to 0.005, X=0.033 to 0.019; [Section 3.3] "This procedure removed inconsistent examples, reducing the number of reorderings, and improving alignment"; [corpus] Limited direct corpus evidence; alignment-based filtering for dialect data is not widely documented.
- **Break condition:** If synthetic generation produces fluent but semantically incorrect outputs with good surface alignment, filtering will not catch these errors.

### Mechanism 3
- **Claim:** LoRA fine-tuning on combined manual + synthetic data enables 7B-parameter models to outperform zero-shot GPT-4o on dialectal translation metrics.
- **Mechanism:** Parameter-efficient fine-tuning (LoRA) adapts Mistral-7B-Instruct and LLaMA-3.1-8B using 3 epochs on the expanded corpus. Synthetic data provides scale (~52K pairs) while manual data (~10K pairs) anchors quality. The instruction-tuned base models already support Ukrainian, reducing adaptation distance.
- **Core assumption:** The dialect is sufficiently similar to standard Ukrainian that tokenizer fallback and existing multilingual representations transfer effectively.
- **Evidence anchors:** [abstract] "fine-tuned models outperformed the zero-shot GPT-4o baseline across both automatic and LLM-evaluated metrics"; [Section 5.3, Table 2] Mistral (manual + synthetic): BLEU 74.35, chrF++ 81.89, Dialect 3.60 vs. GPT-4o: BLEU 56.64, chrF++ 65.90, Dialect 3.22; [corpus] "Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese" shows similar findings with Aya23 models.
- **Break condition:** If base model lacks sufficient Ukrainian representation or tokenizer coverage, fine-tuning may fail to generalize.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** The synthetic data pipeline depends on retrieving dialectal examples to guide generation. Without understanding RAG architecture (embedding models, similarity search, context windowing), you cannot debug or extend the data generation system.
  - **Quick check question:** Can you explain why the authors chose text-embedding-3-large over a simpler TF-IDF retrieval approach for a morphologically rich dialect?

- **Concept: Word Alignment in Machine Translation**
  - **Why needed here:** The filtering mechanism uses alignment statistics (U-src, U-tgt, X) to detect low-quality synthetic pairs. Understanding IBM Model 2 / fast_align basics helps interpret why these metrics signal quality.
  - **Quick check question:** Why would crossing alignments (X) indicate potential problems in a standard-to-dialect pair where word reordering is expected?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - **Why needed here:** The entire adaptation strategy hinges on LoRA making 7B models trainable on limited GPU resources while preserving base capabilities.
  - **Quick check question:** What happens to dialect-specific adaptations if LoRA rank is set too low versus too high?

## Architecture Onboarding

- **Component map:** Manual corpus (9,852 pairs from literary sources + web scraping) → Dictionary (7,320 word mappings) → Grammar rule extraction (GPT-4o + external sources) → RAG index (text-embedding-3-large) → Candidate selection from UberText → Prompt construction → Generation (GPT-4o) → Sequence similarity filtering (difflib, threshold 0.45) → Alignment metrics (fast_align) → Threshold filtering → LoRA fine-tuning (Mistral-7B / LLaMA-3.1-8B, 3 epochs, combined corpus) → Evaluation (BLEU, chrF++, TER via sacreBLEU + GPT-4o judgment)

- **Critical path:** Grammar rule quality → RAG retrieval relevance → Synthetic generation accuracy → Alignment filtering thresholds → Training data composition → Model selection. Errors propagate forward; poor rules cannot be fixed by more data.

- **Design tradeoffs:**
  - Manual-only vs. manual+synthetic: Manual ensures authenticity but limits scale; synthetic expands coverage but may introduce artifacts (authors note dialect score slightly lower for synthetic-trained models in some configurations)
  - GPT-4o as judge vs. human evaluation: No human annotators available; GPT-4o may bias toward standard Ukrainian patterns
  - Strict vs. lenient filtering thresholds: Stricter filtering (lower U-src/U-tgt) improves alignment but reduces corpus size

- **Failure signatures:**
  - High BLEU but low dialect score: Model copying standard Ukrainian with minimal dialectal transformation
  - High crossing alignment (X) in synthetic data: Prompt not providing sufficient dialectal context
  - Adequacy >> dialect scores: Model preserving meaning but failing phonological/morphological transformations
  - chrF++ >> BLEU: Character-level matches but poor n-gram precision (common with morphological variation)

- **First 3 experiments:**
  1. **Ablate synthetic data ratio:** Train with 0%, 25%, 50%, 75%, 100% synthetic data mixed with manual data to find optimal balance; monitor all metrics.
  2. **Filter threshold sweep:** Vary U-src/U-tgt thresholds (0.05, 0.1, 0.15) and measure impact on final model BLEU/chrF++ and dialect quality scores.
  3. **Cross-dialect transfer test:** Apply the same pipeline to a related Ukrainian dialect (Boyko or Lemko) using only the grammar rules and dictionary, without new manual corpus, to assess pipeline generalizability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would human evaluation by native Hutsul speakers compare to GPT-4o-based LLM evaluation for dialectal translation quality assessment?
- Basis in paper: [explicit] The authors state in the Limitations section that "GPT-4o is not explicitly fine-tuned for dialectal assessment, and its preferences may still align with standard Ukrainian and human evaluation would provide much more reliable assessments."
- Why unresolved: The authors lacked access to human annotators and relied entirely on automatic metrics and LLM-based judgment.
- What evidence would resolve it: A human evaluation study with fluent Hutsul speakers rating model outputs on fluency, adequacy, and dialectal quality, compared against GPT-4o ratings.

### Open Question 2
- Question: Can the proposed RAG-based synthetic data generation pipeline generalize to other Ukrainian dialects (e.g., Boyko, Lemko) or other low-resource languages?
- Basis in paper: [explicit] The Limitations section notes that "our current methods are tailored to Hutsul, a relatively well-documented dialect within the Ukrainian language. Extension to other dialects or usage of the same approach for other low-resource languages will require adaptation of both the data pipeline and prompting strategies."
- Why unresolved: The approach was only tested on Hutsul, which has more available written sources than many other dialects.
- What evidence would resolve it: Replication studies applying the same methodology to other dialects or low-resource languages, documenting required adaptations and comparative performance.

### Open Question 3
- Question: How can synthetic data generation be improved to achieve better dialectal quality scores while maintaining high automatic metrics?
- Basis in paper: [explicit] The authors observe that "both LLaMA and Mistral trained on combined synthetic and manually annotated data show strong scores on automatic metrics but slightly underperform on dialectal quality, highlighting the limitations of our method of generating synthetic data."
- Why unresolved: Synthetic data improves BLEU/chrF++ but introduces stylistic inconsistencies that reduce perceived dialectal authenticity.
- What evidence would resolve it: Ablation studies comparing different synthetic data generation strategies, measuring both automatic metrics and dialectal quality scores.

### Open Question 4
- Question: How can the approach be extended to handle modern domains (politics, technology, news) where Hutsul lexicon is sparse or absent?
- Basis in paper: [explicit] The authors acknowledge that "synthetic data reflects the lexical and topical range of the source corpus, which lacks modern domains such as aviation, technology, news and politics" and that "lexical coverage in these areas remains quite sparse or absent."
- Why unresolved: The source materials are primarily literary and folkloric, with no modern domain coverage.
- What evidence would resolve it: Development and evaluation of domain-adapted models or lexicon expansion techniques for modern terminology in dialectal form.

## Limitations
- Limited dialect coverage: The corpus focuses on Hutsul dialect, which may not generalize to other Ukrainian dialects or low-resource language varieties
- GPT-4o as sole judge: Absence of human evaluation means dialect quality judgments may be biased toward standard Ukrainian patterns
- Synthetic data artifacts: Filtering cannot catch semantically correct but dialectally inappropriate translations

## Confidence
- **High confidence:** The pipeline of RAG generation + alignment filtering producing usable synthetic data for dialect translation
- **Medium confidence:** LoRA fine-tuning on the expanded corpus outperforms zero-shot GPT-4o
- **Medium confidence:** The three mechanisms (RAG generation, alignment filtering, LoRA fine-tuning) are sufficient for effective adaptation

## Next Checks
1. **Human evaluation on dialect quality:** Recruit native Hutsul speakers to rate model outputs on dialect authenticity, grammatical correctness, and naturalness, comparing against GPT-4o's LLM judgments to identify potential biases.
2. **Cross-dialect transfer experiment:** Apply the entire pipeline (grammar extraction, RAG generation, filtering, fine-tuning) to another Ukrainian dialect like Boyko or Lemko using only the grammar rules and dictionary, without new manual corpus, to assess pipeline generalizability.
3. **Synthetic data composition ablation:** Systematically vary the ratio of synthetic to manual data (0%, 25%, 50%, 75%, 100% synthetic mixed with manual) across training runs to identify the optimal balance between coverage and quality, monitoring all evaluation metrics.