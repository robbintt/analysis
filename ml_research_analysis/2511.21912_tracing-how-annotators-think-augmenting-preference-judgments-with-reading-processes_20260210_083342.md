---
ver: rpa2
title: 'Tracing How Annotators Think: Augmenting Preference Judgments with Reading
  Processes'
arxiv_id: '2511.21912'
source_url: https://arxiv.org/abs/2511.21912
tags:
- reading
- annotators
- annotation
- annotator
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for capturing annotator reading
  processes during preference annotation tasks by using mouse tracking to record fine-grained
  reading behaviors. The authors created a dataset, PreferRead, containing 1,000 preference
  annotation items each annotated by three participants, capturing both preference
  judgments and detailed mouse-tracked reading behaviors including re-reading patterns,
  reading paths, and word-level gaze durations.
---

# Tracing How Annotators Think: Augmenting Preference Judgments with Reading Processes

## Quick Facts
- arXiv ID: 2511.21912
- Source URL: https://arxiv.org/abs/2511.21912
- Reference count: 0
- This paper introduces a novel method for capturing annotator reading processes during preference annotation tasks using mouse tracking to record fine-grained reading behaviors.

## Executive Summary
This paper presents PreferRead, a novel dataset and methodology that captures annotator reading processes during preference annotation tasks using mouse tracking. The authors developed a web interface where annotators reveal blurred text by hovering, creating a proxy for gaze tracking that records fine-grained reading behaviors including re-reading patterns, reading paths, and word-level gaze durations. The dataset contains 1,000 preference annotation items each annotated by three participants, enabling analysis of how reading behaviors correlate with annotation quality. The results reveal that annotators re-read their chosen responses most frequently, rarely revisit prompts, and often skip the latter portions of rejected responses, suggesting cognitive economy in decision-making.

## Method Summary
The authors created a web-based annotation interface using JSPsych that reveals blurred text only when annotators hover over it, enabling mouse tracking as a proxy for gaze tracking. The interface records character-level mouse enter/exit events with timestamps, which are then aggregated into word-level duration vectors. Events are filtered to exclude durations below 160ms or above 4000ms, normalized via z-score binning, and used to extract behavioral metrics including re-reading flags, path length, loop detection, and word coverage. The dataset contains 1,000 preference annotation items with three annotators per item, totaling 3,000 preference judgments with associated reading process data.

## Key Results
- Annotators re-read their chosen responses most frequently (38.4% of re-reads vs 25.4% for rejected), with 74% returning to chosen response last
- Re-reading behavior is significantly associated with higher inter-annotator agreement (χ²(1) = 11.25, p = 0.001)
- Longer reading paths and times correlate with lower agreement, with disagreeing annotators showing M=2.56 path length vs M=2.42 for agreeing annotators (p=0.001)
- Reading path loops between responses are significantly more common among disagreeing annotators (χ²(1) = 9.42, p=0.002)

## Why This Works (Mechanism)

### Mechanism 1: Mouse Tracking Approximates Gaze Attention
- Claim: Mouse cursor position during text revelation provides a scalable proxy for eye-tracking gaze estimation in reading tasks.
- Mechanism: Users must mouse over blurred text to reveal it, creating a time series of hovered characters that maps to attention allocation. Character-level events are consolidated into word-level durations, filtered to valid reading windows (160–4000ms), and normalized via z-score binning.
- Core assumption: Mouse movements reflect genuine reading attention, not arbitrary cursor behavior.
- Evidence anchors:
  - [abstract] "fine-grained annotator reading behaviors obtained from mouse tracking"
  - [Section 3.1] Describes consolidation of character-level events into word-level vectors, filtering by duration bounds, and binning normalization following Wilcox et al. (2024)
  - [corpus] Weak direct corpus support; related papers focus on annotation quality incentives rather than behavioral capture methods
- Break condition: If users game the interface (e.g., sweeping mouse without reading), duration filters may fail to exclude non-reading behavior.

### Mechanism 2: Re-reading Correlates with Higher Annotation Agreement
- Claim: Annotators who re-read sections—particularly their chosen response—show higher inter-annotator agreement.
- Mechanism: Re-reading functions as a confirmation check before submission. Annotators disproportionately re-read chosen responses (38.4% vs. 25.4% for rejected), and 74% of re-readers return to chosen response last.
- Core assumption: Re-reading indicates deliberate verification rather than confusion.
- Evidence anchors:
  - [abstract] "re-reading is associated with higher inter-annotator agreement"
  - [Section 4.3.2] Chi-square test: χ²(1) = 11.25, p = 0.001 for re-reading and agreement
  - [corpus] Neighbor papers on annotation quality focus on incentives and demographics, not behavioral signals—no direct corroboration
- Break condition: If re-reading is driven by confusion rather than confirmation, the association with agreement may invert for complex texts.

### Mechanism 3: Extended Reading Paths Signal Indecision and Lower Agreement
- Claim: Longer reading paths (more transitions between prompt, chosen, and rejected sections) and looping between responses correlate with disagreement.
- Mechanism: Back-and-forth traversal indicates difficulty choosing; indecisive annotators produce less stable labels that align less with others. Path length M=2.56 for disagreeing annotators vs. M=2.42 for agreeing (p=0.001).
- Core assumption: Path length proxies internal decision certainty.
- Evidence anchors:
  - [abstract] "long reading paths and times are associated with lower agreement"
  - [Section 4.3.2] Looping between responses significantly more common among disagreeing annotators (χ²(1) = 9.42, p=0.002)
  - [Section 4.3.3] Continuous metrics confirm slower reading times (338ms/word vs. 315ms/word) for disagreeing annotators
  - [corpus] No direct corpus validation; neighboring work on disagreement (Dsouza & Kovatchev 2025) addresses task design, not behavioral paths
- Break condition: If long paths reflect thorough comparison rather than indecision (e.g., expert annotators), the negative correlation may not generalize.

## Foundational Learning

- Concept: **Eye-tracking proxies in reading research**
  - Why needed here: Mouse tracking builds on validation that cursor position correlates with gaze; understanding fixation filtering (160–4000ms bounds) requires knowing what eye-tracking excludes.
  - Quick check question: Why might a 50ms mouse hover be excluded from analysis, and what cognitive process does a 2000ms hover likely represent?

- Concept: **Inter-annotator agreement metrics (Krippendorff's alpha)**
  - Why needed here: The paper reports α=0.25 as "fair agreement"; interpreting whether this indicates noisy data or inherent subjectivity requires understanding ordinal agreement baselines.
  - Quick check question: If three annotators label 100 items and exactly two agree on each item, what would you expect the alpha to be?

- Concept: **Preference annotation in RLHF pipelines**
  - Why needed here: The task design (choosing between two model responses) mirrors reward model training data collection; knowing why preference is preferred over rating informs why reading behavior matters for alignment quality.
  - Quick check question: Why might preference judgments provide "stronger signals" than absolute ratings for training reward models?

## Architecture Onboarding

- Component map:
  - **JSPsych-based frontend**: Renders blurred text, captures mouse enter/exit events per character span
  - **Event aggregation layer**: Converts raw mouse events → word-level duration vectors
  - **Filtering pipeline**: Applies 160–4000ms bounds, z-score normalization, 5-level binning
  - **Metric extraction module**: Computes re-read flags, path length, loop detection, word coverage
  - **Storage**: Per-trial time series + aggregated word-level gaze estimates

- Critical path:
  1. User mouses over text → events logged with timestamps
  2. Character durations summed to word durations
  3. Invalid durations filtered
  4. Metrics (re-read, path, coverage) extracted for analysis
  5. Agreement correlation computed post-hoc

- Design tradeoffs:
  - **Blur-based revelation vs. naturalistic reading**: Increases task demands but enables gaze proxy; participants report slower reading (7 negative comments in pilot)
  - **1-second threshold for "reading" a section**: Avoids false positives from cursor drift but may miss rapid skimming
  - **Three annotators per item**: Balances cost against agreement signal granularity

- Failure signatures:
  - Word coverage <10%: Likely accidental advance or disengaged annotator (1.9% of trials excluded)
  - No re-reading + very short path + low coverage: Flag as potential low-quality annotation
  - Excessive looping (>2 cycles): May indicate confusing item or unreliable annotator

- First 3 experiments:
  1. **Replicate agreement correlation on new domain**: Apply interface to summarization or safety annotation; test if re-reading still predicts agreement when task differs.
  2. **Reading-behavior-weighted training**: Use word-level gaze estimates as attention weights in preference model; compare reward model performance vs. uniform attention baseline.
  3. **Adaptive interface**: Detect low-coverage or no-re-read trials in real-time; prompt annotator to confirm decision before submission; measure impact on final agreement rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reading process signals (e.g., re-reading patterns, word-level gaze durations) be explicitly integrated into preference model training and RLHF pipelines to improve reward model performance?
- Basis in paper: [explicit] Authors state: "Future research can also explore methods to explicitly integrate reading signals, such as word-level scores and reading strategies, into preference model training and reinforcement learning paradigms."
- Why unresolved: PreferRead provides the dataset and behavioral metrics, but no experiments were conducted on actually using these signals in model training or alignment.
- What evidence would resolve it: Experiments training reward models with auxiliary objectives based on reading behaviors, comparing performance against baselines without such signals.

### Open Question 2
- Question: Can reading behaviors reliably distinguish between sources of annotator disagreement (differing opinions vs. low confidence vs. annotation quality issues)?
- Basis in paper: [explicit] Authors state: "A difficult question in modeling annotator disagreement in subjective tasks like preference selection is whether the disagreement stems from differing opinions, low annotator confidence, or inconsistencies in annotation quality."
- Why unresolved: PreferRead shows correlations (e.g., looping with disagreement, re-reading with agreement), but these associations do not establish causal attribution to specific sources of disagreement.
- What evidence would resolve it: Studies combining reading metrics with explicit confidence ratings and controlled annotation quality manipulations to disentangle sources.

### Open Question 3
- Question: Does combining reading process data with annotator demographic information improve modeling of pluralistic opinions compared to either signal alone?
- Basis in paper: [explicit] Authors state: "Using reading patterns in conjunction with demographic data may enhance pluralistic opinion modeling."
- Why unresolved: PreferRead collected demographic information but analyzed reading behaviors independently; no joint modeling was performed.
- What evidence would resolve it: Comparative experiments modeling disagreement using demographics only, reading behaviors only, and both combined.

### Open Question 4
- Question: Do the observed relationships between reading behaviors and annotator agreement generalize to other complex, subjective annotation tasks beyond preference selection?
- Basis in paper: [inferred] Authors note their methods "can be adapted to capture reading behaviors in other challenging and subjective tasks" such as law and medicine, but only preference annotation was studied. The generalizability of findings (e.g., re-reading associated with higher agreement) remains untested.
- What evidence would resolve it: Replication studies applying the same mouse-tracking paradigm to tasks like sentiment annotation, legal document review, or medical coding.

## Limitations

The correlation between re-reading and higher agreement assumes re-reading reflects deliberate verification rather than confusion or comprehension difficulty. The negative association between long reading paths and agreement may invert for expert annotators who systematically compare responses. The blur-based interface likely slowed reading speeds and increased cognitive load, potentially altering natural reading patterns.

## Confidence

**High confidence**: The technical implementation of mouse tracking as a gaze proxy (character-to-word aggregation, duration filtering, normalization) is methodologically sound. The statistical tests for association between reading behaviors and agreement are correctly applied.

**Medium confidence**: The behavioral mechanisms (re-reading as verification, path length as indecision) are plausible but not definitively proven. The absence of eye-tracking ground truth means mouse tracking correlations are inferred rather than directly validated.

**Low confidence**: The generalizability of findings across domains, annotation types, and annotator populations remains untested. The small number of negative usability comments (7) in pilot testing provides weak evidence about interface burden.

## Next Checks

1. **Cross-domain replication**: Apply the interface to summarization preference annotation and safety labeling tasks. Test whether re-reading continues to predict agreement when reading material and decision complexity differ from the original dataset.

2. **Expert vs. novice comparison**: Recruit expert annotators (e.g., professional editors, subject matter experts) and compare their reading patterns to crowdworkers. Test whether long paths correlate with disagreement or thorough evaluation in expert populations.

3. **Attention-weighted model training**: Use the word-level gaze estimates as attention weights in a preference model. Compare reward model performance against a baseline that treats all words equally. Measure whether behavioral signals improve alignment quality beyond traditional preference data alone.