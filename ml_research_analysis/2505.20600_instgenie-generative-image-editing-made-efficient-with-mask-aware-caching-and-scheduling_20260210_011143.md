---
ver: rpa2
title: 'InstGenIE: Generative Image Editing Made Efficient with Mask-aware Caching
  and Scheduling'
arxiv_id: '2505.20600'
source_url: https://arxiv.org/abs/2505.20600
tags:
- image
- serving
- instgenie
- editing
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InstGenIE is an efficient system for serving generative image editing
  requests. The key insight is that masks in image editing specify which regions to
  modify, allowing the system to reuse cached intermediate activations from unmasked
  regions to avoid redundant computations.
---

# InstGenIE: Generative Image Editing Made Efficient with Mask-aware Caching and Scheduling

## Quick Facts
- **arXiv ID:** 2505.20600
- **Source URL:** https://arxiv.org/abs/2505.20600
- **Reference count:** 40
- **Key result:** Achieves up to 3× higher throughput and 14.7× lower average request latency for generative image editing

## Executive Summary
InstGenIE is an efficient system for serving generative image editing requests using diffusion models. The key insight is that masks in image editing specify which regions to modify, allowing the system to reuse cached intermediate activations from unmasked regions to avoid redundant computations. The system addresses challenges of cache loading overhead, queuing delays, and load imbalance from heterogeneous masks through three key designs: a bubble-free pipeline scheme that overlaps cache loading with computation, continuous batching adapted for diffusion models to reduce queuing latency, and a mask-aware load balancing strategy that routes requests based on mask size. Evaluation shows InstGenIE outperforms state-of-the-art systems while maintaining image quality.

## Method Summary
InstGenIE implements mask-aware computation by reusing cached intermediate activations (specifically the output Y matrix) for unmasked tokens across requests. The system uses a dynamic programming algorithm to decide per-block whether to use cached activations, ensuring a bubble-free pipeline by overlapping PCIe loading with masked-token computation using CUDA streams. Continuous batching allows requests to join running batches after each denoising step, and CPU-intensive preprocessing is disaggregated into separate processes. A mask-aware load balancing scheduler routes requests based on mask size using regression models to estimate latency and greedy routing algorithms.

## Key Results
- Achieves up to 3× higher throughput compared to state-of-the-art systems
- Reduces average request latency by up to 14.7× while maintaining image quality
- Outperforms Diffusers, FISEdit, and TeaCache across SD2.1, SDXL, and Flux models
- Maintains image quality metrics (CLIP, FID, SSIM) comparable to baseline systems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Reusing cached activations for unmasked tokens reduces computational load without degrading image quality.
- **Mechanism**: Tokens are categorized as masked and unmasked. Token-wise operations (linear projection, feedforward, LayerNorm) compute independently per token. Though attention creates inter-token dependencies, analysis shows unmasked token activations remain highly similar across requests, and masked tokens primarily attend to other masked tokens. The system caches activations from the output Y matrix for unmasked tokens and replenishes them rather than recomputing.
- **Core assumption**: Unmasked token activations exhibit high similarity across editing requests for the same template, and attention patterns support selective masked-only computation without distortion.
- **Evidence anchors**:
  - [abstract]: "reusing cached intermediate activations from previous inferences"
  - [section §3.1, Fig. 5-6]: Cosine similarity analysis shows high similarity for unmasked tokens; attention visualization confirms masked tokens attend primarily to other masked tokens (3○), with minimal cross-attention (2○, 4○)
  - [corpus]: H2-Cache, FreqCa, and OmniCache explore diffusion caching but without mask-aware specialization
- **Break condition**: When mask ratio approaches 1.0 (no unmasked tokens), or for global edits like style transfer where mask locality doesn't apply. Also breaks if template reuse is rare in the workload.

### Mechanism 2
- **Claim**: A bubble-free pipeline eliminates loading overhead by selectively applying caching only to transformer blocks where loading latency ≤ computation latency.
- **Mechanism**: Cached activations (GiB-scale) are stored in host memory, not GPU HBM. Two CUDA streams run concurrently: computation stream processes masked tokens while cache load stream fetches unmasked activations. A dynamic programming algorithm (Algo. 1) decides per-block whether to use cache—skipping cache for blocks where loading would create bubbles.
- **Core assumption**: Loading latency and computation latency scale differently with mask ratio; optimal block-level caching decisions can be computed with negligible overhead.
- **Evidence anchors**:
  - [abstract]: "bubble-free pipeline scheme that overlaps computation with cache loading"
  - [section §4.2, Fig. 9]: Visual comparison of naive, strawman, and bubble-free pipelines; Algo. 1 provides O(N) DP formulation
  - [corpus]: Token Caching paper addresses overlapping concerns for DiT but without pipeline optimization
- **Break condition**: When mask ratio is very small, loading latency exceeds computation for most blocks, forcing fallback to full computation. Also breaks under severe PCIe bandwidth constraints.

### Mechanism 3
- **Claim**: Continuous batching with CPU/GPU process disaggregation reduces P95 tail latency by 29% compared to naive continuous batching.
- **Mechanism**: Diffusion models use iterative denoising (e.g., 50 steps). New requests join the running batch after each denoising step, not after batch completion. CPU-intensive preprocessing/postprocessing are moved to separate processes, preventing interruptions to the GPU-dedicated main process.
- **Core assumption**: The iterative denoising structure is sufficiently analogous to LLM decoding for continuous batching to transfer, and CPU operations are the primary interruption source.
- **Evidence anchors**:
  - [abstract]: "allowing newly arrived requests to join the running batch in just one step of denoising computation"
  - [section §4.3, Fig. 10, §6.4]: Disaggregation reduces P95 latency by 29%; requests interrupted up to 8 times with naive approach
  - [corpus]: Weak direct evidence—LLM systems (vLLM, Orca) use continuous batching but diffusion-specific adaptation is novel here
- **Break condition**: When preprocessing overhead is negligible compared to denoising, disaggregation provides minimal benefit. Also breaks if denoising step counts vary drastically across requests.

## Foundational Learning

- **Concept**: Transformer block structure in diffusion models
  - **Why needed here**: Understanding which operations are token-wise vs. attention-dependent determines what can be cached and reused.
  - **Quick check question**: Given a latent of shape (B, C, H, W), what shape does it become for transformer processing, and which operations create inter-token dependencies?

- **Concept**: CUDA streams and memory hierarchy
  - **Why needed here**: The pipeline optimization requires overlapping host-to-device memory transfers with GPU computation using separate streams.
  - **Quick check question**: Why must cached activations be stored in host memory rather than GPU HBM, and what determines whether a block should use cached activations?

- **Concept**: Continuous batching vs. static batching
  - **Why needed here**: Understanding the latency/throughput tradeoff of when new requests can join a running batch.
  - **Quick check question**: In static batching, when can a new request join? In InstGenIE's continuous batching, when can it join?

## Architecture Onboarding

- **Component map**: Request arrival → Scheduler (regression model estimates load, greedy routing) → Worker queue → Cache engine (async load from host/disk) → Model executor (DP decides per-block cache vs. compute, pipeline execution) → Post-processing → Output

- **Critical path**: Request arrival → Scheduler (regression model estimates load, greedy routing) → Worker queue → Cache engine (async load from host/disk) → Model executor (DP decides per-block cache vs. compute, pipeline execution) → Post-processing → Output

- **Design tradeoffs**:
  - Cache on Y matrix vs. K/V matrices: Y caching is 2× smaller with similar speedup
  - Per-block caching decisions: More blocks cached → lower compute but higher loading overhead
  - Max batch size: Larger batches improve throughput but increase memory pressure

- **Failure signatures**:
  - Image distortion: Cached activations don't match current template → verify template ID before cache reuse
  - GPU OOM: Batch size too large or cache not offloaded → check host memory eviction
  - Load imbalance hotspots: P95 latency spikes under high RPS → verify mask-aware scheduling is active
  - Pipeline bubbles: Inference latency doesn't scale with mask ratio → check DP algorithm output

- **First 3 experiments**:
  1. **Baseline latency/quality comparison**: Run InstGenIE vs. Diffusers/FISEdit/TeaCache with SDXL on H800 at RPS 1-4, measure avg latency, P95 latency, CLIP score, FID, SSIM.
  2. **Mask ratio scaling validation**: Vary mask ratio from 0.1 to 0.9, measure inference latency to confirm linear relationship per Table 1 analysis.
  3. **Ablation on batching strategy**: Compare static batching vs. naive continuous batching vs. disaggregated continuous batching on Flux at RPS 0.5, measure P95 latency and interruption counts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can mask-aware acceleration be adapted for global image editing tasks like style transfer, where masks may be absent or cover the entire image, negating current caching benefits?
- Basis in paper: [explicit] The Discussion section states that for tasks like style transfer, "the benefits of mask-aware computation and load balance will diminish."
- Why unresolved: The current design fundamentally relies on sparsity (unmasked regions) to skip computation; global edits lack this exploitable sparsity.
- Evidence would resolve it: A modification of the InstGenIE scheduler or kernel that maintains performance parity for full-image edits compared to standard diffusion serving.

### Open Question 2
- Question: Can GPU Streaming Multiprocessor (SM) utilization be optimized for InstGenIE during single-request inference or small mask ratios to outperform full-token caching methods like TeaCache?
- Basis in paper: [inferred] Section 6.2 notes that InstGenIE achieves lower throughput than TeaCache at batch size 1 because mask-guided selection reduces token count, leading to "limited GPU streaming multiprocessor (SM) utilization."
- Why unresolved: Reducing the number of computed tokens creates a sparse workload that underutilizes the GPU hardware when batching is not possible.
- Evidence would resolve it: Kernel optimizations or dynamic batching techniques that improve SM occupancy for small mask ratios in single-request scenarios.

### Open Question 3
- Question: Can InstGenIE's host-memory caching strategy be effectively combined with multi-GPU tensor parallelism (e.g., DistriFusion) to accelerate high-resolution editing without sacrificing throughput?
- Basis in paper: [inferred] Section 2.4 reviews multi-GPU approaches like DistriFusion, but InstGenIE's evaluation (Section 6.1) explicitly uses a "one GPU per worker" configuration.
- Why unresolved: The system relies on PCIe bandwidth to load activations from host memory; combining this with the high-bandwidth inter-GPU communication required for tensor parallelism introduces complex bandwidth contention.
- Evidence would resolve it: An evaluation of InstGenIE running on a single large request split across multiple GPUs, analyzing the trade-off between communication and cache loading overhead.

## Limitations
- Effectiveness diminishes significantly when mask ratios approach 0% or 100%, where caching provides minimal benefit or the entire image must be recomputed
- Performance depends heavily on template reuse frequency, which isn't quantified in the evaluation
- Cache management overhead may offset benefits under memory-constrained conditions or with smaller batch sizes

## Confidence
- **High Confidence**: The core mechanism of reusing unmasked token activations is well-supported by attention analysis and experimental results
- **Medium Confidence**: The bubble-free pipeline design and continuous batching improvements are theoretically sound but evaluated on specific hardware configurations
- **Low Confidence**: The load balancing strategy's effectiveness depends on accurate latency estimation, but regression model accuracy isn't validated

## Next Checks
1. **Mask Ratio Sensitivity Analysis**: Evaluate InstGenIE performance across a continuous spectrum of mask ratios (0-100%) on synthetic datasets to identify the operational range where the system provides net benefit versus overhead.

2. **Template Reuse Frequency Impact**: Measure cache hit rates and overall system performance with varying degrees of template reuse to quantify the dependency on workload characteristics.

3. **Hardware Sensitivity Testing**: Compare InstGenIE performance on different GPU architectures (e.g., RTX 4090 vs. H800) and under PCIe bandwidth throttling to assess robustness across deployment scenarios.