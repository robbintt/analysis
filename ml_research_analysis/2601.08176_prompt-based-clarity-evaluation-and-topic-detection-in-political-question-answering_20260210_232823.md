---
ver: rpa2
title: Prompt-Based Clarity Evaluation and Topic Detection in Political Question Answering
arxiv_id: '2601.08176'
source_url: https://arxiv.org/abs/2601.08176
tags:
- clarity
- evasion
- prompting
- topic
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of automatically evaluating
  clarity and topic detection in political question answering, where large language
  models (LLMs) must not only provide factually correct answers but also clearly address
  the intent of the question. The authors evaluate the impact of prompt design on
  automatic clarity evaluation using the CLARITY dataset from the SemEval 2026 shared
  task, comparing GPT-5.2 against a GPT-3.5 baseline under three prompting strategies:
  simple prompting, chain-of-thought prompting, and chain-of-thought with few-shot
  examples.'
---

# Prompt-Based Clarity Evaluation and Topic Detection in Political Question Answering

## Quick Facts
- **arXiv ID:** 2601.08176
- **Source URL:** https://arxiv.org/abs/2601.08176
- **Reference count:** 28
- **Primary result:** GPT-5.2 with chain-of-thought prompting achieves 63% clarity accuracy vs 56% for GPT-3.5 baseline

## Executive Summary
This paper investigates how prompt design affects automatic clarity evaluation and topic detection in political question answering. Using the CLARITY dataset from SemEval 2026, the authors compare GPT-5.2 against a GPT-3.5 baseline under three prompting strategies: simple prompting, chain-of-thought (CoT), and CoT with few-shot examples. Results demonstrate that reasoning-based prompts significantly improve high-level clarity classification (56% to 63%) and topic identification (60% to 74%), but show uneven benefits for fine-grained evasion classification. The findings reveal that prompt design yields task-specific performance impacts, with clarity and topic tasks benefiting more consistently than nuanced evasion subtype detection.

## Method Summary
The study evaluates automatic clarity and topic detection using the CLARITY dataset (2,061 unique question-answer pairs after deduplication). Three prompting strategies are tested: simple prompting, chain-of-thought, and chain-of-thought with few-shot examples. GPT-5.2 is compared against the provided GPT-3.5 baseline. Models generate predictions for clarity labels (Clear Reply, Ambivalent, Clear Non-Reply), evasion labels (Implicit, Dodging, General, Deflection, Partial/Half-Answer, Clarification), and topic labels. Evaluation uses accuracy, precision/recall/F1 per class, and hierarchical exact match against human annotations. Experiments are conducted in Google Colab with fixed evaluation pipelines.

## Key Results
- GPT-5.2 achieves 63% clarity accuracy with CoT + few-shot prompting vs 56% for GPT-3.5 baseline
- Topic identification improves from 60% to 74% with reasoning-based prompting
- Evasion classification shows unstable improvements, peaking at 34% accuracy with CoT alone
- Chain-of-thought prompting consistently outperforms simple prompting across all tasks
- Few-shot examples help clarity but slightly hurt evasion classification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Chain-of-thought prompting improves high-level clarity classification by forcing explicit reasoning about question-answer alignment before label assignment.
- **Mechanism:** The structured decision procedure (identify core request → assess answer sufficiency → assign labels) decomposes a subjective judgment into intermediate reasoning steps, reducing annotation noise from direct label prediction.
- **Core assumption:** Models better approximate human judgment when explicitly guided through the inference process humans implicitly perform.
- **Evidence anchors:**
  - [abstract]: "GPT-5.2 consistently outperforms the GPT-3.5 baseline on clarity prediction, with accuracy improving from 56% to 63% under chain-of-thought with few-shot prompting."
  - [Section V-A, Table III]: Clarity accuracy rises from 0.56 (GPT-3.5 baseline) to 0.60 (CoT) to 0.64 (CoT + few-shot).
  - [corpus]: SQuARE paper (FMR=0.61) demonstrates similar gains from sequential reasoning in QA, supporting the mechanism that structured reasoning improves complex QA tasks.

### Mechanism 2
- **Claim:** Topic identification benefits disproportionately from reasoning prompts because topic assignment requires identifying latent semantic intent across political domain vocabulary.
- **Mechanism:** Reasoning-based prompts cause the model to first identify "what the question is primarily asking about" before topic assignment, reducing errors from surface-level keyword matching in semantically overlapping domains.
- **Core assumption:** Political questions have a single dominant topic amenable to stepwise identification; multi-topic questions degrade this mechanism.
- **Evidence anchors:**
  - [abstract]: "Topic identification accuracy also improves from 60% to 74% with reasoning-based prompting."
  - [Section V-D, Table VI]: ChatGPT with simple prompt achieves 60%; CoT prompting achieves 74%.
  - [corpus]: Iterative Topic Taxonomy Induction paper (arxiv:2510.15125) addresses similar topic induction challenges in political advertising.

### Mechanism 3
- **Claim:** Fine-grained evasion classification shows unstable improvements because evasion subtypes require pragmatic and discourse-level cues that prompt-based reasoning alone does not reliably elicit.
- **Mechanism:** Evasion categories (Implicit, Dodging, General, Deflection, Partial/Half-Answer) represent subtle conversational strategies where boundaries are context-dependent and annotator agreement is inherently lower.
- **Core assumption:** Evasion distinctions require world knowledge and speaker intent modeling beyond textual patterns; prompts cannot easily encode this.
- **Evidence anchors:**
  - [abstract]: "Improvements for fine-grained evasion classification are less stable, indicating that prompt design yields uneven benefits across hierarchical evaluation tasks."
  - [Section V-C, Table V]: Evasion accuracy peaks at 0.34 with CoT alone, but drops to 0.32 with CoT + few-shot. Class-wise F1 ranges from 0.04 (Partial/Half-Answer) to 0.59 (Clarification).
  - [Section VI-B]: "Evasion prediction exhibits greater sensitivity to prompt design and does not show monotonic improvement with increased prompt complexity."

## Foundational Learning

- **Concept: Hierarchical label taxonomies with conditional dependencies**
  - **Why needed here:** Clarity labels (Clear Reply, Ambivalent, Clear Non-Reply) condition valid evasion labels; incorrect parent predictions propagate to child labels, explaining low hierarchical exact match.
  - **Quick check question:** If a model predicts "Clear Reply" but assigns evasion label "Deflection," is this a valid labeling? (Answer: No—Clear Reply should map to "Explicit" or similar non-evasive subtypes.)

- **Concept: Prompt sensitivity and non-monotonic performance gains**
  - **Why needed here:** Adding few-shot examples helps clarity but slightly hurts evasion; engineers must test prompt variations per task rather than assuming "more complexity = better."
  - **Quick check question:** If CoT + few-shot improves Task A by 7 points but hurts Task B by 2 points, what should you do before deployment? (Answer: Task-specific prompt optimization, possibly using different prompts for different tasks.)

- **Concept: Human agreement ceiling in subjective annotation**
  - **Why needed here:** Clarity and evasion judgments involve inherent subjectivity; model performance is bounded by annotator agreement. The paper notes three annotators but does not report inter-annotator agreement.
  - **Quick check question:** If human annotators agree on evasion labels 60% of the time, what is the realistic upper bound for model accuracy? (Answer: Approximately 60%, assuming models approximate human judgment patterns.)

## Architecture Onboarding

- **Component map:** Question-answer pairs -> Prompt layer (Simple/CoT/CoT+few-shot) -> GPT-5.2 model -> Prediction extraction -> Evaluation metrics
- **Critical path:**
  1. Data preprocessing (deduplication, CSV conversion)
  2. Prompt template construction per strategy
  3. Model inference with fixed temperature/settings
  4. Prediction extraction and alignment with ground truth
  5. Metric computation across clarity, evasion, and topic dimensions
- **Design tradeoffs:**
  - Single vs. multi-prompt systems: Using one prompt for all three tasks simplifies deployment but may sacrifice evasion accuracy; task-specific prompts require orchestration overhead.
  - Few-shot inclusion: Helps clarity (64% vs. 60%) but slightly hurts evasion (32% vs. 34%); decision depends on which task is primary.
  - Model version: GPT-5.2 outperforms GPT-3.5 baseline, but cost and latency tradeoffs are not addressed.
- **Failure signatures:**
  - Ambivalent vs. Clear Non-Reply confusion: Most clarity errors occur at this boundary.
  - Evasion subtype confusion: Implicit, General, Dodging show low F1 (0.09–0.20), suggesting models detect "something evasive" but cannot reliably subtype.
  - Topic overlap errors: Economy vs. Healthcare confusion in semantically related domains.
- **First 3 experiments:**
  1. Baseline replication: Run simple prompting on GPT-5.2 with the CLARITY dataset; verify clarity accuracy ~0.59 and evasion accuracy ~0.28 per Table III.
  2. CoT-only ablation: Test CoT prompting without few-shot examples; expect clarity ~0.60, evasion ~0.34. Compare per-class F1 to identify which evasion types benefit most.
  3. Task-specific prompt optimization: Create separate prompts optimized for clarity-only and evasion-only tasks; measure whether decoupling improves evasion accuracy beyond the 0.34 ceiling.

## Open Questions the Paper Calls Out
None

## Limitations
- Study depends on unreleased GPT-5.2 model, restricting independent verification
- Few-shot examples used in CoT+Few-shot condition are not provided, creating ambiguity about optimal prompt construction
- Paper lacks reported inter-annotator agreement metrics for the CLARITY dataset, making it impossible to establish human performance ceiling
- Instability in fine-grained evasion classification may reflect limitations in the annotation scheme itself rather than purely model capabilities

## Confidence

- **High Confidence:** GPT-5.2 consistently outperforms GPT-3.5 baseline on clarity prediction (accuracy improving from 56% to 63%) and topic identification (60% to 74%) under chain-of-thought prompting.
- **Medium Confidence:** The claim that evasion classification shows unstable improvements is supported by the data, but underlying reasons remain unclear without inter-annotator agreement data.
- **Low Confidence:** The specific few-shot examples and their exact impact on performance cannot be verified without the examples being provided.

## Next Checks
1. **Inter-annotator Agreement Measurement:** Calculate and report Fleiss' kappa or similar agreement metrics among the three annotators used to create the CLARITY dataset, establishing the human performance ceiling for clarity and evasion judgments.
2. **Few-shot Example Transparency:** Provide the exact few-shot examples used in the CoT+Few-shot condition, or conduct ablation studies showing how different example sets affect performance across tasks.
3. **Task-specific Prompt Optimization:** Implement separate prompt templates optimized for each task (clarity-only, evasion-only, topic-only) rather than a single unified prompt, measuring whether this specialization improves evasion classification accuracy beyond the current 0.34 ceiling.