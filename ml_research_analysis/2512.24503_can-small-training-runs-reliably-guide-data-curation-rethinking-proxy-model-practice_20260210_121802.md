---
ver: rpa2
title: Can Small Training Runs Reliably Guide Data Curation? Rethinking Proxy-Model
  Practice
arxiv_id: '2512.24503'
source_url: https://arxiv.org/abs/2512.24503
tags:
- learning
- training
- data
- proxy
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of data quality assessment for
  large language model pretraining, where small proxy models are typically used to
  evaluate candidate datasets. The key issue identified is that using identical hyperparameter
  configurations across datasets for "fair" comparison is flawed, as the optimal training
  configuration is inherently data-dependent.
---

# Can Small Training Runs Reliably Guide Data Curation? Rethinking Proxy-Model Practice

## Quick Facts
- arXiv ID: 2512.24503
- Source URL: https://arxiv.org/abs/2512.24503
- Reference count: 40
- Primary result: Tiny learning rates (10^-5 to 10^-6) dramatically improve proxy model reliability for dataset ranking in LLM pretraining

## Executive Summary
This paper addresses a fundamental challenge in LLM data curation: using small proxy models to evaluate candidate datasets before expensive large-scale pretraining. The authors identify a critical flaw in standard practice - using identical hyperparameters across datasets creates misleading comparisons because optimal configurations are inherently data-dependent. They propose a simple yet effective solution: training proxy models with very small learning rates. Theoretically, they prove this approach preserves dataset rankings for random-feature models in the infinite-width limit. Empirically, across 23 data recipes spanning four curation dimensions and three model families, tiny learning rates achieved Spearman correlations exceeding 0.95 for dataset rankings, dramatically outperforming standard hyperparameter approaches.

## Method Summary
The authors propose training proxy models with extremely small learning rates (10^-5 to 10^-6) as a simple patch to improve dataset ranking reliability. They prove theoretically that for random-feature models in the infinite-width limit, tiny learning rates preserve dataset rankings according to optimal achievable loss. The empirical validation involved 23 data recipes across four data curation dimensions (deduplication, data mixing, filtering, ordering) and three model families (GPT2, Pythia, OPT). They compared tiny learning rates against standard learning rate approaches and hyperparameter tuning, measuring dataset ranking consistency through Spearman rank correlation and top-k decision regret metrics.

## Key Results
- Spearman rank correlation for dataset rankings improved to over 0.95 across 253 data recipe pairs
- Top-k decision regret was minimized compared to standard learning rate approaches
- Tiny learning rates achieved comparable or superior performance to expensive hyperparameter tuning while requiring only modest additional computational overhead

## Why This Works (Mechanism)
The mechanism relies on the observation that optimal hyperparameter configurations are inherently data-dependent. Standard practice of using identical hyperparameters across datasets creates unfair comparisons because different datasets have different optimal configurations. By using extremely small learning rates, the proxy models enter a regime where the relative performance ordering of datasets becomes more stable and transferable to larger models. This happens because tiny learning rates reduce the sensitivity to specific data characteristics, allowing the model to better capture fundamental dataset quality differences rather than artifacts of particular hyperparameter choices.

## Foundational Learning

**Random-feature models** - Simplified neural network models where only the last layer is trained, while earlier layers are fixed. *Why needed:* Provides tractable theoretical framework for analyzing dataset ranking preservation. *Quick check:* Verify understanding of how random features differ from standard neural networks.

**Infinite-width limit** - Theoretical regime where neural network width approaches infinity. *Why needed:* Enables exact analysis of model behavior and convergence properties. *Quick check:* Understand how infinite-width relates to Gaussian process behavior.

**Dataset ranking consistency** - The degree to which proxy model evaluations preserve the ordering of datasets when evaluated on larger models. *Why needed:* Core metric for evaluating proxy model reliability. *Quick check:* Can you explain why ranking consistency matters more than absolute performance?

## Architecture Onboarding

**Component map:** Data curation pipeline -> Proxy model training -> Dataset ranking -> Large model pretraining

**Critical path:** Dataset evaluation (proxy) → Ranking preservation → Large model selection

**Design tradeoffs:** Simple patch (tiny learning rates) vs. complex hyperparameter tuning - tiny rates require more training steps but dramatically improve reliability

**Failure signatures:** Low Spearman correlation (<0.7) indicates proxy model is not capturing meaningful dataset differences; high variance across runs suggests instability

**3 first experiments:**
1. Train proxy models with tiny learning rates on a small set of diverse datasets and measure ranking consistency
2. Compare tiny learning rates against standard learning rate with identical hyperparameters across datasets
3. Test transferability of tiny learning rate rankings to hyperparameter-tuned larger models

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Theoretical analysis relies on random-feature models in infinite-width limit, which may not fully capture practical finite-width networks
- Empirical validation primarily on language modeling tasks, generalization to other domains uncertain
- Computational overhead increases due to more training steps needed at tiny learning rates

## Confidence

**High confidence** in the empirical finding that tiny learning rates improve dataset ranking consistency

**Medium confidence** in theoretical generalization from random-feature models to practical networks

**Medium confidence** in cross-model-family transferability given limited architectural diversity

## Next Checks

1. Test the tiny learning rate approach on non-language tasks (vision, speech, multimodal) to assess domain transferability

2. Conduct ablation studies varying initial learning rate ranges to quantify sensitivity to initialization

3. Evaluate whether alternative simple patches (e.g., early stopping criteria, batch size adjustments) could achieve similar improvements without requiring many additional training steps