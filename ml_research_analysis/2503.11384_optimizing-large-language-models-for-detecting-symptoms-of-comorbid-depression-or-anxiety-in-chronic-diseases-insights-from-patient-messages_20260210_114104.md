---
ver: rpa2
title: 'Optimizing Large Language Models for Detecting Symptoms of Comorbid Depression
  or Anxiety in Chronic Diseases: Insights from Patient Messages'
arxiv_id: '2503.11384'
source_url: https://arxiv.org/abs/2503.11384
tags:
- llms
- depression
- performance
- anxiety
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated large language models (LLMs) for detecting
  symptoms of depression or anxiety in patients with diabetes using secure patient
  messages. Five LLMs were tested using zero-shot and few-shot learning, with and
  without systemic persona, and using both binary and PHQ-4 classification approaches.
---

# Optimizing Large Language Models for Detecting Symptoms of Comorbid Depression or Anxiety in Chronic Diseases: Insights from Patient Messages

## Quick Facts
- arXiv ID: 2503.11384
- Source URL: https://arxiv.org/abs/2503.11384
- Reference count: 36
- Five LLMs tested for depression/anxiety symptom detection achieved >90% F-1 and accuracy in zero-shot learning

## Executive Summary
This study evaluated large language models for detecting symptoms of depression or anxiety in patients with diabetes using secure patient messages. Five LLMs were tested using zero-shot and few-shot learning, with and without systemic persona, and using both binary and PHQ-4 classification approaches. Llama 3.1 405B achieved the highest performance with 93% F-1 score and accuracy using zero-shot learning. Three out of five models demonstrated excellent performance (>90% F-1 and accuracy). Reasoning models showed near-perfect precision but lower recall. While LLMs performed well in binary classification and handling complex metrics like PHQ-4, inconsistencies in challenging cases warrant further real-world assessment. The findings suggest LLMs could assist in timely mental health screening and referrals for patients with chronic diseases.

## Method Summary
The study used 606 de-identified patient messages from diabetes patients at Stanford Health Care, labeled by psychiatrists for depression/anxiety symptoms. Five LLMs (Llama 3.1 8B/405B, Gemini Pro 1.5, OpenAI o1, DeepSeek R1) were evaluated using zero-shot and few-shot learning approaches, with and without systemic persona. Models were tested on binary classification (symptom present/absent) and PHQ-4 structured classification. Performance was measured using F-1 score, precision, recall, and accuracy with 95% confidence intervals via bootstrapping. Systemic persona included role prompting, directive commands, expertise emulation, and zero-shot chain-of-thought instructions.

## Key Results
- Llama 3.1 405B achieved 93% F-1 and accuracy using zero-shot learning without systemic persona
- Three models demonstrated excellent performance (>90% F-1 and accuracy)
- Reasoning models (OpenAI o1, DeepSeek R1) showed near-perfect precision but lower recall (68-84%)
- Binary classification outperformed PHQ-4 structured classification in knowledge models
- Systemic persona mostly enhanced few-shot learning but degraded zero-shot performance for some models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot learning enables effective symptom detection without task-specific training data when using sufficiently large knowledge models.
- Mechanism: Pre-trained linguistic representations encode semantic relationships between symptom descriptors and clinical presentations; the model maps patient language to psychiatric concepts through pattern matching against learned representations.
- Core assumption: The training corpus contained sufficient mental health discourse for the model to generalize symptom recognition to clinical messaging contexts.
- Evidence anchors:
  - [abstract] "Llama 3.1 405B achieving 93% in both F-1 and accuracy using a zero-shot approach"
  - [section] "the superlative performance of the zero-shot approach was particularly encouraging... those tailoring efforts could increase the cost of LLM use"
- Break condition: Performance degrades significantly when patient language diverges from training distribution (e.g., informal abbreviations, non-English text, domain-specific jargon not in pre-training).

### Mechanism 2
- Claim: Reasoning models optimize for precision at the cost of recall, producing near-perfect positive predictions but missing true cases.
- Mechanism: Reasoning models employ explicit chain-of-thought verification that raises confidence thresholds; they require stronger evidence before flagging symptoms, reducing false positives but increasing false negatives.
- Core assumption: The reasoning process implicitly applies conservative clinical judgment criteria similar to expert psychiatric assessment.
- Evidence anchors:
  - [abstract] "Reasoning models showed near-perfect precision but lower recall"
  - [section] "Two reasoning models showed nearly perfect precision (OpenAI o1: 1.00; DeepSeek R1: 0.99) yet presented much different recalls (OpenAI o1: 0.68; DeepSeek R1: 0.84)"
- Break condition: High-stakes screening scenarios where missing cases (low recall) is clinically unacceptable; binary classification tasks requiring balanced sensitivity and specificity.

### Mechanism 3
- Claim: Systemic persona improves few-shot learning but can degrade zero-shot performance depending on model architecture.
- Mechanism: Persona prompts establish role constraints that guide interpretation; in few-shot contexts, examples clarify how the persona should behave, while in zero-shot, persona may introduce ambiguity without demonstration.
- Core assumption: The persona formulation aligns with the model's learned representations of expert clinical reasoning.
- Evidence anchors:
  - [abstract] "Five LLMs were tested using zero-shot and few-shot learning, with and without systemic persona"
  - [section] "in few-shot settings, the systemic persona mostly enhanced F-1 and accuracy consistently across models up to 0.09... in zero-shot, F-1 and accuracy decreased with systemic persona compared to baseline in Llama 3.1 8B (F-1: 0.91→0.88)"
- Break condition: Persona instructions that conflict with model's training or introduce contradictory signals; overly complex persona definitions that exceed context window efficiency.

## Foundational Learning

- Concept: Zero-shot vs Few-shot Learning
  - Why needed here: The study demonstrates that zero-shot can outperform few-shot for certain tasks, which is counterintuitive for practitioners familiar with traditional ML where more training data typically helps.
  - Quick check question: Can you explain why adding labeled examples might decrease recall while increasing precision in this context?

- Concept: Precision-Recall Tradeoff in Clinical Screening
  - Why needed here: Understanding when to prioritize precision (avoid alert fatigue) vs recall (don't miss cases) is essential for model selection in deployment.
  - Quick check question: For a mental health triage system, would you prioritize precision or recall, and how would the clinical workflow change based on that choice?

- Concept: PHQ-4 as Structured Output
  - Why needed here: The study shows LLMs can perform multi-step classification (4 categories × 4 levels → threshold → binary output), demonstrating capability beyond simple classification.
  - Quick check question: What validation steps would you need before trusting an LLM's PHQ-4 scoring against clinician-administered assessments?

## Architecture Onboarding

- Component map:
  - Input layer: De-identified patient messages (PMAR clinical issues)
  - Prompt engineering: Persona definition + task instructions + few-shot examples (optional)
  - Model selection: Knowledge models (Llama 3.1 variants, Gemini Pro) vs Reasoning models (OpenAI o1, DeepSeek R1)
  - Classification head: Binary (symptom present/absent) or PHQ-4 structured output
  - Evaluation: Bootstrapped F-1, precision, recall, accuracy with 95% CIs

- Critical path:
  1. De-identify messages → 2. Apply prompt configuration → 3. Call LLM via secure API → 4. Parse classification output → 5. Compute metrics against psychiatrist-confirmed labels

- Design tradeoffs:
  - Larger model (405B) vs smaller model (8B): Performance vs inference cost/latency
  - Zero-shot vs few-shot: Simplicity vs potential precision gains
  - Binary vs PHQ-4: Simpler task vs explainable multi-dimensional output
  - Reasoning vs knowledge model: High precision (low false positives) vs balanced detection

- Failure signatures:
  - High precision/low recall: Likely using reasoning model; consider switching to knowledge model or lowering confidence threshold
  - Inconsistency on challenging cases: Messages with negative emotion keywords but no clinical significance; may need context-aware prompting or human-in-the-loop review
  - PHQ-4 scoring degradation: Knowledge models outperform reasoning models for this task; verify model selection

- First 3 experiments:
  1. Establish baseline: Replicate zero-shot binary classification with Llama 3.1 405B on your local data; verify F-1 ≥0.90 before proceeding.
  2. Precision optimization: If false positive burden is concern, test 4-shot learning with persona on Llama 3.1 8B; target 99% precision while monitoring recall.
  3. Challenging case audit: Curate 30-50 messages with emotion keywords but confirmed negative status; compare model performance against two-clinician consensus to assess deployment readiness.

## Open Questions the Paper Calls Out

- **Question**: What are the operational impacts of piloting LLM-based mental health screening in live clinical settings on detection rates, costs, and clinician acceptance?
  - **Basis in paper**: The authors state that "piloting text-based mental health screening in clinical settings is a suggested next step to evaluate the practical impact of this approach."
  - **Why unresolved**: This study was a retrospective model evaluation using historical benchmark data rather than a prospective clinical implementation.
  - **What evidence would resolve it**: Results from a prospective pilot study measuring symptom detection rates, operational costs, care efficiency, and user acceptance.

- **Question**: How can LLM reliability be improved for "challenging cases" where negative emotional language is present but clinical pathology is absent?
  - **Basis in paper**: The authors note that "inconsistencies in challenging cases warrant further real-life assessment" due to low precision in messages containing negative emotions but no pathology.
  - **Why unresolved**: Models exhibited low precision (0.43-0.58) in this subgroup, suggesting a tendency to over-flag non-clinical distress.
  - **What evidence would resolve it**: Evaluation of fine-tuned models on a held-out dataset of ambiguous cases showing improved discrimination between demoralization and clinical depression.

- **Question**: Does using binary classification for ground-truth labeling bias performance evaluations compared to PHQ-4 based standards?
  - **Basis in paper**: The paper states that "the use of binary classification in preparing the reference data may have influenced the outcomes, warranting further investigation in the future."
  - **Why unresolved**: Binary classification outperformed PHQ-4 classification, but it is unclear if this was due to model limitations or the reference standard methodology.
  - **What evidence would resolve it**: A comparative study where the ground truth is established via PHQ-4 scoring rather than binary annotation.

## Limitations

- Benchmark dataset limited to single healthcare system with diabetes patients, limiting generalizability to other chronic conditions
- Perfect precision scores (particularly OpenAI o1 at 1.00) appear unusually high and may indicate dataset peculiarities
- Performance sensitive to prompt engineering with systemic persona showing inconsistent effects across models
- Real-world deployment challenges with diverse patient populations and messages containing mixed emotional content

## Confidence

- **High Confidence**: Zero-shot learning effectiveness with large models; binary classification performance; precision-recall tradeoff in reasoning models
- **Medium Confidence**: PHQ-4 structured classification performance; systemic persona benefits in few-shot learning; model selection tradeoffs for clinical deployment
- **Low Confidence**: Perfect precision scores; generalizability to other chronic conditions; performance consistency across diverse populations

## Next Checks

1. **External Validation**: Test the zero-shot approach on de-identified patient messages from a different chronic disease cohort (e.g., heart failure or chronic kidney disease) to assess generalizability beyond diabetes. Compare performance against the original benchmark to identify domain-specific limitations.

2. **Challenging Case Analysis**: Conduct a focused evaluation on 50-100 messages containing negative emotional language or symptom keywords but clinically confirmed as non-pathological. Measure false positive rates and analyze failure modes to determine whether current models can reliably distinguish clinical significance from general distress.

3. **Real-World Workflow Integration**: Deploy a pilot implementation in a controlled clinical setting where LLM screening results are presented to care teams alongside traditional screening tools. Measure clinician acceptance, time savings, and any differences in referral patterns or patient outcomes compared to standard practice.