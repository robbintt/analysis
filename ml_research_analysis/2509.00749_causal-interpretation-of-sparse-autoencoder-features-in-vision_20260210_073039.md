---
ver: rpa2
title: Causal Interpretation of Sparse Autoencoder Features in Vision
arxiv_id: '2509.00749'
source_url: https://arxiv.org/abs/2509.00749
tags:
- features
- feature
- layer
- activation
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Causal Feature Explanation (CaFE) addresses the problem that sparse
  autoencoder (SAE) features in vision transformers are often misinterpreted when
  relying on activation locations, as self-attention mixes information across the
  image and activated patches may co-occur with but not cause feature firing. CaFE
  leverages Effective Receptive Field (ERF) by applying input attribution methods
  to identify the image patches that causally drive SAE feature activations, rather
  than just where activations are highest.
---

# Causal Interpretation of Sparse Autoencoder Features in Vision

## Quick Facts
- arXiv ID: 2509.00749
- Source URL: https://arxiv.org/abs/2509.00749
- Reference count: 12
- Primary result: CaFE identifies causal image patches driving SAE feature activations using ERF-based attribution, revealing non-local features in vision transformers

## Executive Summary
Causal Feature Explanation (CaFE) addresses the problem that sparse autoencoder (SAE) features in vision transformers are often misinterpreted when relying on activation locations, as self-attention mixes information across the image and activated patches may co-occur with but not cause feature firing. CaFE leverages Effective Receptive Field (ERF) by applying input attribution methods to identify the image patches that causally drive SAE feature activations, rather than just where activations are highest. Through patch insertion tests, CaFE with AttnLRP showed higher area under the insertion curve (AUC) for recovering feature activations compared to baseline activation-based methods. Analysis revealed that non-local SAE features, whose activations scatter across images, increase in frequency in higher layers (up to ~14% at layer 22) and encode complex, compositional concepts.

## Method Summary
CaFE computes ERF maps by backpropagating attribution from target SAE neurons through the SAE encoder and vision transformer using attention-aware LRP. The method identifies which image patches causally drive specific SAE feature activations rather than relying on activation locations. Patch insertion tests validate the approach by measuring activation recovery when inserting top-ranked patches. The framework is tested on CLIP-ViT-L/14 with Matryoshka SAEs trained on ImageNet-1K patches across transformer layers.

## Key Results
- CaFE with AttnLRP achieves higher insertion AUC than activation-based methods for recovering SAE feature activations
- Non-local SAE features increase in frequency in higher transformer layers, peaking at ~14% at layer 22
- Non-local features encode complex compositional concepts requiring context from multiple spatial regions

## Why This Works (Mechanism)

### Mechanism 1: Self-Attention Non-Local Mixing
Self-attention in vision transformers mixes information across entire images, causing activated patches to co-occur with but not causally drive feature firing. Attention layers aggregate global context through attention weights, so a token at spatial position A can activate based on visual content at distant position B. The token "carries" activation but the causal evidence exists elsewhere in the image. This distribution supports the intuition that self-attention progressively mixes global context, making later-layer activations increasingly difficult to interpret without ERF.

### Mechanism 2: Attribution-Based Effective Receptive Field
Input attribution methods (particularly AttnLRP) trace causal pathways through backpropagation to identify which image patches truly drive specific SAE feature activations. For each SAE feature activation z_k(I), the method backpropagates relevance scores from the target neuron through the SAE encoder and then through the vision transformer using attention-aware LRP. This produces attribution scores A(p|z_k,I) for each patch p, forming the ERF map. The attribution A is obtained by backpropagating relevance scores from the target SAE neuron through the SAE encoder and subsequently through the vision transformer.

### Mechanism 3: Hierarchical Feature Abstraction
Non-local SAE features emerge progressively in higher transformer layers (peaking at ~14% at layer 22), encoding compositional concepts that require context from multiple spatial regions. Early layers (<layer 9) maintain spatial locality as attention hasn't fully mixed global context. Deeper layers accumulate more cross-patch information flow, enabling features that fire only when multiple visual elements co-occur. Non-local features are scarce in early layers (<layer 9), with their frequency rising sharply in higher layers, peaking at layer 22 where ≈14% of features are non-local.

## Foundational Learning

- **Vision Transformer (ViT) Architecture and Patch Embeddings**
  - Why needed here: The entire CaFE framework operates on ViT patch embeddings. Understanding how images become patches, how tokens flow through attention layers, and what the CLS token represents is essential for interpreting ERF maps
  - Quick check question: Can you explain why a patch at position (3,4) in a ViT can encode information about content at position (1,1) after several attention layers?

- **Input Attribution Methods (LRP, Integrated Gradients)**
  - Why needed here: CaFE is attribution-method agnostic but relies on understanding what these methods compute. AttnLRP is the recommended method; understanding attention-aware relevance propagation helps debug when ERF maps look uninformative
  - Quick check question: If you run attribution and get uniform scores across all patches, what are two possible causes (method failure vs. genuine feature property)?

- **Sparse Autoencoder Decomposition**
  - Why needed here: You need to understand what SAE features represent (monosemantic concepts), how sparsity is enforced (L1 penalty), and why the encoder weights We serve as "feature detectors" to properly set up CaFE targets
  - Quick check question: Given an SAE with 10x expansion (m=10n features), why might feature k=3847 activate only for patches containing "wheel" while feature k=2091 activates for "stripes"?

## Architecture Onboarding

- **Component map:**
  Input Image → ViT Backbone → Hidden Representations h ∈ R^n → SAE Encoder → Sparse Features z ∈ R^m (m >> n) → Target Feature z_k selected for analysis → Attribution Backprop (AttnLRP/IG/Gradients) → ERF Map: {patch position → attribution score}

- **Critical path:** The attribution quality depends on: (1) properly hooking the SAE encoder into the ViT computation graph, (2) selecting the correct target neuron z_k (not the reconstructed activation), and (3) using an attribution method designed for transformers (AttnLRP preferred over vanilla Gradients)

- **Design tradeoffs:**
  - **AttnLRP vs. Integrated Gradients:** AttnLRP is more faithful for transformers (paper shows higher insertion AUC) but requires LRP implementation. IG is more widely available but may distribute attribution less precisely
  - **Layer selection for SAE:** Early layers have more localized features (easier to interpret), later layers have more semantically abstract features (potentially more interesting but require ERF)
  - **Single-image vs. aggregate ERF:** Computing ERF per image is precise but expensive; aggregating across top-activated images reveals consistent patterns but may miss image-specific context

- **Failure signatures:**
  - ERF maps that perfectly match activation maps: Likely implementation error (not actually computing attribution) or genuinely localized feature
  - Uniform/near-zero attribution across patches: Attribution method failing to propagate gradients (check if target is differentiable), or feature may be dead/never activates
  - High insertion AUC but semantically incoherent ERF: Attribution method capturing statistical correlation rather than causal mechanism (try different attribution method)
  - Non-local features in early layers (before layer 9): Should only occur for CLS token features; otherwise, check SAE training or layer indexing

- **First 3 experiments:**
  1. **Reproduce insertion test on held-out features:** Select 10 SAE features not used in paper's analysis, compute ERF with AttnLRP, run patch insertion test comparing ERF-ranked vs. activation-ranked patches. Target: insertion AUC improvement of 10-20% over baseline
  2. **Layer-wise non-local feature audit:** For each transformer layer, manually inspect top 20 features to classify as local/non-local based on ERF vs. activation divergence. Verify the reported ~14% peak at layer 22 and identify what semantic concepts non-local features capture
  3. **Cross-attribution method comparison on known non-local feature:** Identify one feature with clear non-local behavior (e.g., "Despair" feature from Figure 2), run ERF with AttnLRP, IG, Gradients, and KernelSHAP. Compare which method most sharply identifies the true causal region (e.g., spilled pills vs. face)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can scalable, automated criteria be developed to identify non-local SAE features without relying on subjective human annotation?
- Basis in paper: The authors state that "manual annotation of non-local features is inherently subjective; devising scalable, automated criteria remains future work."
- Why unresolved: The current analysis depends on labor-intensive manual inspection of feature activation maps versus ERF maps, which does not scale to thousands of features.
- What evidence would resolve it: A defined quantitative metric that correlates highly with human judgments of non-locality and can be applied automatically across entire layers.

### Open Question 2
- Question: Does the CaFE framework effectively transfer to text modalities and Large Language Models (LLMs)?
- Basis in paper: The authors note that "analogous patterns of context mixing arise in large-scale language models; extending our CaFE with ERF-based attribution to text modalities is also a compelling avenue."
- Why unresolved: The study is currently restricted to vision transformers (CLIP-ViT), and it is unverified if ERF-based attribution behaves similarly with discrete text tokens.
- What evidence would resolve it: Successful application of CaFE to SAE features in LLMs, identifying causal context tokens that differ from high-activation tokens.

### Open Question 3
- Question: How can the computational cost of ERF calculation be reduced for very deep or large-scale sparse autoencoders?
- Basis in paper: The paper lists as a limitation that "computing the ERF for each feature requires both forward and backward passes, which may be costly for very deep SAEs."
- Why unresolved: The backpropagation required for attribution methods like AttnLRP creates a bottleneck, limiting the efficiency of the interpretation pipeline.
- What evidence would resolve it: Development of approximation techniques or efficient attribution algorithms that significantly reduce inference time while maintaining the fidelity of the ERF maps.

## Limitations
- Attribution methods may capture correlation rather than true causation, and their faithfulness in transformers remains an active research area
- The characterization of non-local feature emergence depends on specific CLIP-ViT-L/14 architecture and Matryoshka SAE training regime
- Computing ERF for each feature requires both forward and backward passes, which may be costly for very deep SAEs

## Confidence
- **High confidence**: The insertion test methodology and AUC comparisons showing AttnLRP outperforms activation-based ranking are methodologically sound and directly supported by experimental results
- **Medium confidence**: The mechanism explaining why self-attention causes misinterpretation is theoretically sound but relies on attribution method assumptions that haven't been fully validated across different architectural variants
- **Low confidence**: Generalization claims about non-local feature behavior across different vision transformer architectures, patch sizes, or SAE training configurations are not explicitly validated

## Next Checks
1. **Cross-Architecture Attribution Validation**: Test CaFE on two additional vision transformer variants (e.g., Swin Transformer with local attention, DeiT with distillation) to verify whether AttnLRP maintains superior insertion AUC and whether non-local feature patterns persist. Target: confirm or refute the claim that self-attention mixing is architecture-independent.

2. **Attribution Method Ablation on Semantic Coherence**: Select 5 non-local features with clearest semantic meaning, compute ERF maps using AttnLRP, Integrated Gradients, Attention Rollout, and Path Attribution. Have 3 independent annotators rate semantic coherence of ERF-identified regions vs. activation regions on 7-point scales. Target: determine which method best aligns with human semantic judgment.

3. **Layer-wise Feature Property Audit**: For each transformer layer (1-24), randomly sample 50 activated SAE features, compute ERF vs. activation spatial correlation, and manually classify as local/non-local based on both quantitative metrics and visual inspection. Verify the reported ~14% peak at layer 22 and characterize what semantic concepts non-local features encode at each layer depth. Target: validate the hierarchical emergence pattern and identify any layer where the trend breaks.