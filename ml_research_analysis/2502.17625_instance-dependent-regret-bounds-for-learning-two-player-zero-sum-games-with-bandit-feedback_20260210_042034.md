---
ver: rpa2
title: Instance-Dependent Regret Bounds for Learning Two-Player Zero-Sum Games with
  Bandit Feedback
arxiv_id: '2502.17625'
source_url: https://arxiv.org/abs/2502.17625
tags:
- regret
- learning
- have
- bound
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of learning two-player zero-sum
  normal-form games under bandit feedback, where players only observe their own noisy
  payoffs without knowledge of their opponent's actions or strategies. The authors
  provide the first instance-dependent regret bounds for this setting, extending recent
  best-of-both-worlds techniques from stochastic multi-armed bandits to the game-theoretic
  context.
---

# Instance-Dependent Regret Bounds for Learning Two-Player Zero-Sum Games with Bandit Feedback

## Quick Facts
- **arXiv ID**: 2502.17625
- **Source URL**: https://arxiv.org/abs/2502.17625
- **Reference count**: 40
- **Primary result**: First instance-dependent regret bounds for learning two-player zero-sum games with bandit feedback

## Executive Summary
This paper studies the problem of learning two-player zero-sum normal-form games under bandit feedback, where players only observe their own noisy payoffs without knowledge of their opponent's actions or strategies. The authors provide the first instance-dependent regret bounds for this setting, extending recent best-of-both-worlds techniques from stochastic multi-armed bandits to the game-theoretic context. The core method applies the Tsallis-INF algorithm, a simple Follow-the-Regularized-Leader approach with Tsallis entropy regularization, to both players. The key insight is that this algorithm simultaneously achieves optimal worst-case regret and optimal instance-dependent regret bounds, adapting to the difficulty of the specific game instance being played.

## Method Summary
The paper applies the Tsallis-INF algorithm to both players in a two-player zero-sum game. This algorithm is a Follow-the-Regularized-Leader method with Tsallis entropy regularization, using learning rates Î·_t = 1/(2âˆšt). Each player independently maintains probability distributions over their actions, updates these distributions based on observed losses using importance-weighted estimators, and achieves regret bounds that depend on the specific game instance being played. The algorithm is parameter-free and uncoupled, requiring no knowledge of the opponent's strategy or the game structure.

## Key Results
- Instance-dependent regret bounds of the form O(câ‚ log T + âˆšcâ‚‚T) where câ‚ and câ‚‚ are game-dependent constants
- Effective O((Ï‰ + Ï‰') log T) regret when games have pure strategy Nash equilibria
- Last-iterate convergence to pure Nash equilibria with rate O(1/âˆšT)
- Near-optimal sample complexity for identifying pure Nash equilibria
- Tight instance-dependent lower bounds proving the regret bounds are optimal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The 1/2-Tsallis entropy regularizer in FTRL simultaneously achieves optimal worst-case and instance-dependent regret bounds.
- Mechanism: The regularizer Ïˆ(x) = -2âˆ‘âˆšx(i) creates an exploration schedule where the "local norm" induced by the regularizer naturally concentrates probability mass on empirically good actions. The learning rate Î·_t = 1/(2âˆšt) is tuned so that the algorithm's inherent exploration decreases at the rate where gap information can be reliably estimated from bandit feedback.
- Core assumption: The bandit feedback is unbiased and bounded; both players use the same algorithm independently.
- Evidence anchors:
  - [abstract]: "if both players apply the Tsallis-INF algorithm of Zimmert and Seldin (2018, arXiv:1807.07623), then their regret is at most O(câ‚ log T + âˆšcâ‚‚T)"
  - [section 2, Eq. (3)]: Shows the FTRL optimization with Ïˆ(x) = -2âˆ‘âˆšx(i) and Î·_t = 1/(2âˆšt)
- Break condition: If the regularizer is changed to negative Shannon entropy (standard Exp3), the best-of-both-worlds property is lost.

### Mechanism 2
- Claim: Instance-dependent regret emerges from a self-bounding technique where the regret bound is expressed as a function of itself, then solved via a quadratic inequality.
- Mechanism: The key inequality (Lemma 1 in paper: a â‰¤ b + âˆš(ac) â‡’ a â‰¤ 2b + c) allows the analysis to decompose regret into a "stochastic-like" log T term and a "worst-case" âˆšT term. The log T term's coefficient Ï‰ = âˆ‘_{iâˆ‰I} 1/Î”(i) is precisely the standard stochastic MAB gap complexity.
- Core assumption: The duality gap satisfies DGap(x, y) â‰¥ Î” Â· (x - x*)â‚Š for admissible gap parameters.
- Evidence anchors:
  - [section 3, Theorem 2]: Two bounds with coefficients |I|+|J|-2 (support size) and Î³ (boundary proximity)
- Break condition: If the game has no gap structure (all actions nearly equally good), then Î”(i) â†’ 0 and Ï‰ â†’ âˆ, causing the log T bound to vacuously exceed âˆšT.

### Mechanism 3
- Claim: Last-iterate convergence to PSNE is achieved by exploiting negative Bregman divergence terms in the FTRL regret decomposition.
- Mechanism: Standard FTRL analysis includes a -D(x*, x_{T+1})/Î·_{T+1} term. Since D(Â·,Â·) â‰¥ 0 and Î·_{T+1} â†’ 0, this term can only be negative if x_{T+1} â‰ˆ x*. The analysis shows E[D(x*, x_{T+1})] = O(Ï‰ logâº(mt/Ï‰Â²) + Ï‰' logâº(nt/Ï‰'Â²)), proving the iterate itselfâ€”not just the averageâ€”converges.
- Core assumption: A unique pure-strategy Nash equilibrium exists.
- Evidence anchors:
  - [section 4.2, Proposition 1]: "E[D(x*, x_t) + D(y*, y_t)] = O((1/âˆšt)(Ï‰ logâº(mt/Ï‰Â²) + Ï‰' logâº(nt/Ï‰'Â²)))"
- Break condition: If there is no PSNE (mixed equilibrium only), this specific last-iterate guarantee does not hold.

## Foundational Learning

- Concept: **Follow-the-Regularized-Leader (FTRL)**
  - Why needed here: The entire Tsallis-INF algorithm is an instantiation of FTRL; understanding how regularizers control exploration is essential.
  - Quick check question: Given loss estimates â„“Ì‚_1,...,â„“Ì‚_{t-1} and regularizer Ïˆ, write down the FTRL update equation for x_t.

- Concept: **Importance-Weighted Estimators for Bandit Feedback**
  - Why needed here: The paper constructs unbiased loss estimators â„“Ì‚_t(i) = ğŸ™[i_t = i](1 - r_t)/x_t(i) - 1; understanding why division by x_t(i) is necessary for unbiasedness is critical.
  - Quick check question: If x_t(i) = 0.01 and action i is never selected, what happens to the estimator variance for that action?

- Concept: **Duality Gap as Equilibrium Measure**
  - Why needed here: The paper bounds regret, which translates to duality gap bounds via DGap(Ä’[x_T], Ä’[y_T]) = (Reg_T + Reg'_T)/T.
  - Quick check question: For a 2Ã—2 game with matrix A = [[0, 1], [-1, 0]], compute the duality gap for x = (0.5, 0.5), y = (0.5, 0.5).

## Architecture Onboarding

- Component map:
  ```
  [Environment: Payoff Matrix A] 
        â†“ (players select i_t, j_t)
  [Nature: draws r_t ~ BerÂ±(A(i_t,j_t))]
        â†“
  [Tsallis-INF (Row Player)] â†â†’ [Tsallis-INF (Column Player)]
        (each sees only own r_t)
        â†“
  [IW Estimator â„“Ì‚_t, â„“Ì‚'_t] â†’ [FTRL Update x_{t+1}, y_{t+1}]
  ```

- Critical path:
  1. Implement FTRL with 1/2-Tsallis entropy (Eq. 3)â€”this requires solving a convex optimization per step
  2. Implement IW estimators with careful handling of division-by-zero (Eq. IW)
  3. Track cumulative loss estimates and maintain running strategy probabilities

- Design tradeoffs:
  - **Simple IW estimators vs. reduced-variance estimators**: Paper notes (footnote 2, Section 2) that more sophisticated estimators could improve constants but complicate analysis
  - **Known vs. unknown game structure**: Bounds depend on gap parameters Î”, Î”' which may not be known; algorithm itself is parameter-free
  - **Centralized vs. decentralized**: Midsearch algorithm (Maiti et al. 2024) achieves better sample complexity but requires coupled control; this algorithm is uncoupled and no-regret

- Failure signatures:
  - Regret scales as ~âˆšT instead of ~log T â†’ likely no PSNE exists or gap parameters are extremely small
  - Last-iterate oscillation without convergence â†’ check if unique PSNE assumption is violated
  - High variance in loss estimates â†’ IW estimator variance scales as 1/x_t(i); may need reduced-variance estimators

- First 3 experiments:
  1. **Validate regret scaling on game from Eq. (5)**: Set Îµ = T^{-1/3}, run Tsallis-INF vs. Exp3 and UCB1, plot log(Regret) vs. log(T). Expected slope: Tsallis-INF â‰ˆ 0.33, Exp3 â‰ˆ 0.5, UCB1 â‰ˆ 1.0.
  2. **PSNE identification accuracy**: Use the 4Ã—4 hard instance (Eq. 12), set n=m=256, vary Î”_min, compute frequency of correct PSNE identification vs. iterations. Compare against theoretical OPT = Î£1/Î”Â²_i.
  3. **Support size ablation**: Construct games with varying equilibrium support sizes (1, 2, 4, 8), measure how the âˆšT coefficient in regret scales with |I|+|J|. Should observe O(âˆš(|I|+|J|)T) scaling per Theorem 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can instance-dependent regret bounds be extended to general-sum multiplayer games under bandit feedback?
- Basis in paper: [explicit] "A natural step forward is generalizing our work to general-sum multiplayer games, which is not yet explored due to the added intricacy from the misaligned incentives of players."
- Why unresolved: Multiplayer general-sum games introduce misaligned incentives that break the zero-sum structure central to the current analysis.
- What evidence would resolve it: A regret bound analysis for multiplayer general-sum games with game-dependent constants, or a negative result showing fundamental barriers.

### Open Question 2
- Question: Can these instance-dependent bounds be extended to extensive-form or continuous games?
- Basis in paper: [explicit] "Equally important is extending our results to extensive-form games and continuous games, each of which introducing extra challenges with their structural complexity."
- Why unresolved: Extensive-form games have sequential structure; continuous games have infinite action spacesâ€”both require fundamentally different techniques beyond the normal-form analysis.
- What evidence would resolve it: Regret bounds for extensive-form or continuous games, or identification of specific structural barriers preventing extension.

### Open Question 3
- Question: Is the O(âˆšmax{m, n}) sample complexity gap for PSNE identification unavoidable in uncoupled learning dynamics?
- Basis in paper: [explicit] "Our algorithm leaves a O(âˆšmax{m, n}) gap in sample complexity for pure strategy Nash equilibrium identification, and whether this gap is unavoidable in uncoupled learning dynamics remains unknown."
- Why unresolved: Current uncoupled methods achieve sample complexity ~âˆšm times the optimal centralized bound; unknown if this is fundamental to decentralization.
- What evidence would resolve it: Either an uncoupled algorithm matching optimal sample complexity, or a lower bound proving the gap is inherent to uncoupled dynamics.

## Limitations
- The analysis assumes both players independently run Tsallis-INF without coordination, which may not achieve optimal convergence rates compared to centralized methods.
- The instance-dependent bounds rely on accurate estimation of gap parameters Î”, Î”' which may be unknown in practice.
- The last-iterate convergence result is only proven for pure strategy Nash equilibria and may not extend to mixed equilibria without additional assumptions.

## Confidence
- **High Confidence**: The worst-case O(âˆšT) regret bounds and the general regret framework using Tsallis-INF.
- **Medium Confidence**: The instance-dependent O(log T) bounds for games with pure Nash equilibria.
- **Medium Confidence**: The last-iterate convergence to pure Nash equilibria.

## Next Checks
1. **Gap Parameter Estimation**: Implement an online estimator for Î”_min and evaluate how estimation errors affect the practical performance of the algorithm on games with known equilibria.
2. **Mixed Equilibrium Extension**: Design and analyze a variant of Tsallis-INF that provably achieves last-iterate convergence for games with only mixed Nash equilibria, potentially using adaptive learning rates.
3. **Reduced-Variance Estimators**: Replace the simple importance-weighted estimators with more sophisticated estimators (e.g., mixture-based or follow-the-perturbed-leader variants) and measure the impact on empirical regret and convergence rates.