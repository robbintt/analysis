---
ver: rpa2
title: 'Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge'
arxiv_id: '2511.00505'
source_url: https://arxiv.org/abs/2511.00505
tags:
- knowledge
- corpus
- retrieval
- zero-rag
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Zero-RAG addresses the problem of knowledge redundancy between
  retrieval-augmented generation (RAG) corpora and large language models (LLMs). The
  core method uses a Mastery-Score metric to identify and prune redundant passages
  from the corpus, then employs a Query Router to bypass retrieval for questions the
  LLM can answer directly, and Noise-Tolerant Tuning to improve robustness against
  irrelevant documents.
---

# Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge

## Quick Facts
- **arXiv ID:** 2511.00505
- **Source URL:** https://arxiv.org/abs/2511.00505
- **Reference count:** 14
- **Key outcome:** Prunes 30% of Wikipedia corpus, accelerates retrieval by 22%, maintains RAG performance with <3-point EM drop, reduces overall processing time by >60%.

## Executive Summary
Zero-RAG addresses knowledge redundancy in retrieval-augmented generation (RAG) systems by pruning redundant passages from the corpus based on LLM mastery, routing queries intelligently to bypass retrieval when possible, and tuning for noise tolerance. The core method uses a Mastery-Score metric to identify and remove redundant knowledge, employs a Query Router to decide when to bypass retrieval, and applies Noise-Tolerant Tuning to improve robustness against irrelevant documents. Experiments on four QA datasets show that Zero-RAG can significantly accelerate retrieval while retaining RAG performance.

## Method Summary
Zero-RAG consists of three main components: a Mastery-Score-based pruning method that removes redundant knowledge from the RAG corpus, a Query Router that bypasses retrieval for questions the LLM can answer directly, and Noise-Tolerant Tuning to improve robustness against irrelevant documents. The Mastery-Score is computed by generating QA pairs for each sentence using GPT-4o-mini and measuring the LLM's Exact Match accuracy. A regression model is trained to predict these scores, enabling corpus-wide pruning. The Query Router is trained to classify queries as mastered or un-mastered, allowing direct LLM answering when possible. Noise-Tolerant Tuning uses LoRA fine-tuning with mixed retrieval samples to improve performance in the presence of noise.

## Key Results
- Prunes 30% of Wikipedia corpus while retaining RAG performance
- Accelerates retrieval by 22% with less than 3-point EM degradation
- Reduces overall processing time by over 60% in some cases

## Why This Works (Mechanism)
Zero-RAG works by addressing the fundamental inefficiency in traditional RAG systems where LLMs already possess much of the knowledge in external corpora. By identifying and removing redundant knowledge through the Mastery-Score metric, the system reduces the retrieval space and processing overhead. The Query Router component intelligently decides when to bypass retrieval entirely, leveraging the LLM's internal knowledge for straightforward questions. Noise-Tolerant Tuning improves the model's ability to handle irrelevant or noisy documents that inevitably appear in retrieval results, ensuring robust performance even with a pruned corpus.

## Foundational Learning
- **Mastery-Score metric**: Measures LLM's knowledge of individual sentences by generating QA pairs and computing EM accuracy; needed to identify redundancy, check by verifying high scores on factual sentences
- **Query routing**: Binary classification to decide between retrieval and direct LLM answering; needed to reduce latency, check by analyzing router accuracy on validation set
- **LoRA fine-tuning**: Parameter-efficient adaptation for noise tolerance; needed to handle irrelevant documents, check by comparing performance with and without noise documents
- **Corpus pruning strategy**: Percentile-based removal of high-Mastery-Score sentences; needed to balance performance and efficiency, check by plotting EM vs. pruning ratio
- **Retrieval latency measurement**: Timing retrieval operations; needed to quantify speedup, check by measuring latency before and after pruning
- **EM evaluation**: Exact Match accuracy metric; needed to assess answer quality, check by comparing EM scores across datasets

## Architecture Onboarding

**Component map:** Wikipedia corpus → Mastery-Score regressor → Pruned corpus → Query Router → Noise-Tolerant model → Answer generation

**Critical path:** Corpus → Mastery-Score computation → Regressor training → Pruning → Query Router training → Noise-Tolerant Tuning → Evaluation

**Design tradeoffs:** Aggressive pruning reduces latency but risks losing valuable information; routing decisions must balance accuracy against computational savings; noise tolerance requires additional training data and complexity

**Failure signatures:** >3-point EM drop indicates over-pruning or poor router decisions; high false negative rate in router suggests overly conservative routing; poor noise tolerance indicates insufficient fine-tuning

**3 first experiments:**
1. Train Mastery-Score regressor on small Wikipedia sample and plot EM vs. pruning ratio
2. Train Query Router and evaluate accuracy on validation set
3. Test Noise-Tolerant Tuning with mixed retrieval samples and measure EM and latency improvements

## Open Questions the Paper Calls Out
- **Domain transferability**: How does Zero-RAG perform on domain-specific corpora where LLM mastery differs significantly from Wikipedia? The paper acknowledges effectiveness on Wiki-based databases remains unexplored for specialized domains.
- **Noise sensitivity**: To what extent does noise or inconsistency in the source corpus increase the risk of accidentally pruning valuable information? The authors note performance depends on initial data quality.
- **Model portability**: Is the pruned corpus transferable, or does a corpus pruned for a specific large model degrade the performance of smaller or different architecture models? The method defines redundancy relative to the target LLM's knowledge without testing cross-model portability.

## Limitations
- Performance depends on initial data quality and may remove valuable information if original data contains high noise levels
- Effectiveness on domain-specific or multimodal datasets remains unexplored
- Pruned corpus may not be portable across different model architectures or sizes

## Confidence
- **High confidence** in the core idea of pruning redundant knowledge and using a Query Router to bypass retrieval
- **Medium confidence** in the Noise-Tolerant Tuning approach, as specifics on noise document selection are unclear
- **Low confidence** in exact replication of reported performance gains without full hyperparameter details

## Next Checks
1. Implement Mastery-Score regressor with small Wikipedia sample, generate QA pairs per sentence, and verify pruning threshold by plotting EM vs. pruning ratio
2. Train and test Query Router on held-out validation set to ensure correct identification of mastered vs. un-mastered queries
3. Evaluate Noise-Tolerant Tuning by running with mixed retrieval samples (relevant, noise, and no-RAG) and measure EM and latency improvements