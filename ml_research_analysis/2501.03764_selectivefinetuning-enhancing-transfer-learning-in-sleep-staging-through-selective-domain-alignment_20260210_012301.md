---
ver: rpa2
title: 'SelectiveFinetuning: Enhancing Transfer Learning in Sleep Staging through
  Selective Domain Alignment'
arxiv_id: '2501.03764'
source_url: https://arxiv.org/abs/2501.03764
tags:
- domain
- data
- sleep
- source
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of negative transfer in sleep
  stage classification due to domain shifts between source and target EEG data. The
  authors propose SelectiveFinetuning, a method that uses a pre-trained Multi-Resolution
  CNN (MRCNN) to extract EEG features and a domain alignment mechanism based on Earth
  Mover's Distance (EMD) to select source data most similar to the target domain.
---

# SelectiveFinetuning: Enhancing Transfer Learning in Sleep Staging through Selective Domain Alignment

## Quick Facts
- arXiv ID: 2501.03764
- Source URL: https://arxiv.org/abs/2501.03764
- Reference count: 40
- Primary result: Achieves 75.2% accuracy and 73.7 F1-score on SleepEDF; 76.7% accuracy and 74.9 F1-score on SHHS in cross-dataset transfer learning

## Executive Summary
This paper addresses negative transfer in sleep stage classification caused by domain shifts between source and target EEG datasets. The authors propose SelectiveFinetuning, which uses a pre-trained Multi-Resolution CNN to extract EEG features and Earth Mover's Distance to select source data most similar to the target domain. By selectively finetuning only the aligned source data, the method reduces domain mismatch and improves performance on the target domain. Experiments demonstrate state-of-the-art results in cross-dataset transfer learning tasks.

## Method Summary
The method involves pre-training a Multi-Resolution CNN (MRCNN) on source domain data, then using this model to extract features from both source and target batches. Earth Mover's Distance (EMD) is calculated between target features and each source batch to determine similarity, with rewards assigned inversely proportional to EMD values. Source batches with rewards above a threshold τ are selected for finetuning. The model is then fine-tuned on this selected subset, improving performance on the target domain while mitigating negative transfer from dissimilar source data.

## Key Results
- Achieves 75.2% accuracy and 73.7 F1-score on SleepEDF dataset in cross-dataset transfer
- Achieves 76.7% accuracy and 74.9 F1-score on SHHS dataset in cross-dataset transfer
- Outperforms baseline methods including SAR and MUDA in cross-dataset transfer learning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-resolution convolution captures frequency-specific sleep stage signatures necessary for cross-domain transfer.
- Mechanism: Two parallel convolutional branches process EEG signals at different scales—wide kernel (K=400) extracts low-frequency delta band features, narrow kernel (K=50) captures higher-frequency alpha and theta bands.
- Core assumption: Sleep stage discriminability depends on frequency-specific patterns that remain relatively consistent across subjects, even when overall signal distributions shift.
- Evidence anchors: [abstract] "capturing the distinctive characteristics of different sleep stages"; [section III.A] "wide kernel branch for lower frequencies like the delta (δ) band and a narrow kernel branch for higher frequencies such as the alpha (α) and theta (θ) bands"

### Mechanism 2
- Claim: Earth Mover's Distance provides a meaningful metric for quantifying distributional alignment between source and target EEG features.
- Mechanism: EMD computes optimal transport cost to transform source feature distribution into target feature distribution; reward R = 1/EMD prioritizes batches with lower transport cost.
- Core assumption: Feature-space distance correlates with transferability—source samples closer to target distribution in EMD space will yield better finetuning outcomes.
- Evidence anchors: [abstract] "employs Earth Mover's Distance to select source domain data most similar to the target domain"; [section III.C] "The policy function calculates the similarity between the domains using Earth Mover's Distance (EMD)"

### Mechanism 3
- Claim: Selective batch-level finetuning mitigates negative transfer by excluding source data with high distributional divergence from the target.
- Mechanism: Threshold τ filters out source samples with low reward values, preventing conflicting gradients during finetuning that would degrade target domain performance.
- Core assumption: Negative transfer primarily arises from source samples far from target distribution; removing them improves rather than harms knowledge transfer.
- Evidence anchors: [abstract] "By finetuning the model with selective source data, our SelectiveFinetuning enhances the model's performance on target domain that exhibits domain shifts"; [Figure 2] shows visual comparison of data distributions before and after alignment

## Foundational Learning

- **Concept: Domain Shift in EEG Signals**
  - Why needed here: The entire method is built on the premise that EEG signals vary across subjects and recording conditions. Understanding why this occurs (physiology, age, health status, electrode placement, sampling rates) is essential for diagnosing transfer failures.
  - Quick check question: Given two EEG recordings from different hospitals with different equipment, can you name at least three factors that would cause their signal distributions to differ?

- **Concept: Transfer Learning Paradigms (Inductive vs. Transductive vs. Unsupervised)**
  - Why needed here: This paper uses a transductive approach—no target labels are used during adaptation. Understanding where target labels are unavailable vs. available determines which transfer strategies are applicable.
  - Quick check question: In this paper's setup, what information from the target domain IS available during training, and what is NOT?

- **Concept: Earth Mover's Distance / Optimal Transport**
  - Why needed here: EMD is the core metric for domain alignment. You need to understand that EMD measures the minimum "work" to transform one distribution into another, and why this captures distributional similarity better than simpler metrics like mean difference.
  - Quick check question: If you have two Gaussian distributions with the same mean but different variances, would EMD between them be zero or non-zero, and why?

## Architecture Onboarding

- **Component map:**
  [Source Data Batch] → [MRCNN Feature Extractor] → [Feature Vectors Fs]
  [Target Data Batch] → [MRCNN Feature Extractor] → [Feature Vectors Ft]
                                      ↓
                              [EMD Calculator] → [Reward R]
                                      ↓
                              [Threshold Filter τ] → [Selective Finetuning]
                                      ↓
                              [Pre-trained MRCNN] → [Finetuned Model]

- **Critical path:**
  1. Pre-train MRCNN on large labeled source dataset (SleepEDF or SHHS)
  2. For each target batch, extract features using frozen MRCNN
  3. For each source batch, compute EMD against target batch features
  4. Assign reward R = 1/EMD; retain source batches where R > τ
  5. Finetune MRCNN on retained source batches
  6. Evaluate on target domain test set

- **Design tradeoffs:**
  - Batch-level vs. sample-level selection: Batch-level is more efficient but may include some dissimilar samples within retained batches
  - Frozen vs. updated feature extractor during selection: Paper uses frozen pre-trained features for EMD calculation, then updates during finetuning
  - Threshold τ setting: Not specified in paper—likely requires hyperparameter tuning per task

- **Failure signatures:**
  - Accuracy drops below "No Adapt" baseline → Negative transfer still occurring; threshold may be too permissive or EMD not capturing relevant differences
  - Very few source batches selected → Threshold too aggressive; relax τ
  - High variance in results across target subjects → Domain aligner not capturing subject-specific characteristics
  - No improvement over baselines (SAR, MUDA) → EMD may not be optimal metric for this feature space

- **First 3 experiments:**
  1. Reproduce baseline comparison: Implement "No Adapt" and at least one test-time adaptation method to verify MRCNN implementation produces comparable accuracy (~65% for No Adapt, ~73% for SAR on SHHS→EDF transfer)
  2. Ablation on threshold τ: Run SelectiveFinetuning with varying threshold values (e.g., 25th, 50th, 75th percentile of reward distribution) to characterize sensitivity
  3. Feature layer ablation: Test whether using features from different MRCNN layers (early vs. late convolutional layers) for EMD calculation affects alignment quality

## Open Questions the Paper Calls Out

- **Open Question 1:** How sensitive is the performance of SelectiveFinetuning to the selection threshold τ, and can this threshold be determined adaptively without manual tuning?
  - Basis in paper: The methodology defines a specific threshold τ to select source samples based on reward R, but the paper does not provide an ablation study or a mechanism for automatically determining the optimal value for this hyperparameter in new target domains.

- **Open Question 2:** What is the computational cost of calculating Earth Mover's Distance (EMD) during the domain alignment phase compared to the computational savings achieved by fine-tuning with fewer samples?
  - Basis in paper: The authors state that processing data in batches improves training efficiency, but EMD is computationally intensive (optimal transport). The paper does not quantify the time or resource overhead of the Domain Aligner against standard fine-tuning baselines.

- **Open Question 3:** Does the SelectiveFinetuning approach maintain its effectiveness when transferring knowledge to target domains with pathological sleep patterns significantly different from the healthy and apnea-focused cohorts used in this study?
  - Basis in paper: The introduction identifies "health status" as a major cause of domain shift, yet the experiments are limited to the SleepEDF (healthy/aged) and SHHS (apnea) datasets.

## Limitations
- EMD computational cost is not addressed; cubic complexity may make approach infeasible for large-scale EEG datasets
- Threshold τ selection is left unspecified, suggesting sensitivity to hyperparameter choice that could significantly impact performance
- Single-channel limitation restricts applicability to modern multi-modal sleep staging systems

## Confidence
- **High confidence**: Multi-resolution CNN architecture design (well-specified kernels and frequency bands)
- **Medium confidence**: EMD-based domain alignment mechanism (conceptually sound but computationally challenging)
- **Medium confidence**: Selective finetuning effectiveness (supported by results but threshold sensitivity unknown)

## Next Checks
1. **Computational feasibility test**: Profile EMD computation time on subsets of SleepEDF/SHHS data to determine practical batch sizes and whether approximation methods are needed
2. **Threshold sensitivity analysis**: Systematically vary τ across a range (e.g., 25th-75th percentile of rewards) to quantify impact on final performance and identify stable operating points
3. **Feature layer ablation study**: Compare EMD alignment and final performance using features from different MRCNN layers to identify optimal representation for domain alignment