---
ver: rpa2
title: Multivariate Conformal Prediction using Optimal Transport
arxiv_id: '2502.03609'
source_url: https://arxiv.org/abs/2502.03609
tags:
- conformal
- prediction
- transport
- optimal
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OT-CP, a method for multivariate conformal
  prediction that leverages optimal transport (OT) theory to construct uncertainty
  sets for multidimensional outputs. The key idea is to transform multivariate scores
  into a univariate space using an optimal transport map, enabling the application
  of standard conformal prediction techniques.
---

# Multivariate Conformal Prediction using Optimal Transport

## Quick Facts
- arXiv ID: 2502.03609
- Source URL: https://arxiv.org/abs/2502.03609
- Reference count: 40
- Primary result: Introduces OT-CP method that uses optimal transport maps to construct multivariate conformal prediction sets with distribution-free coverage

## Executive Summary
This paper introduces OT-CP, a method for multivariate conformal prediction that leverages optimal transport (OT) theory to construct uncertainty sets for multidimensional outputs. The key idea is to transform multivariate scores into a univariate space using an optimal transport map, enabling the application of standard conformal prediction techniques. The method uses entropic OT maps to approximate the Brenier map, which pushes the data distribution to a uniform distribution on a unit ball. This allows for principled ordering of multivariate scores and construction of conformal prediction sets that preserve distribution-free coverage guarantees. Experiments on a benchmark of multivariate regression tasks show that OT-CP outperforms existing methods (M-CP and Merge-CP) in terms of region size while maintaining coverage, particularly for datasets with lower dimensionality (d ≤ 6).

## Method Summary
OT-CP constructs conformal prediction sets for multivariate regression by first learning an entropic optimal transport map from the empirical distribution of residuals to a uniform distribution on a unit ball. This map is estimated using the Sinkhorn algorithm with entropic regularization. The conformity score for a candidate output is computed as the norm of the transported residual. Standard conformal prediction calibration is then applied to these scalar scores to determine prediction set radii, ensuring distribution-free coverage. The method is agnostic to the base predictor and works with any model that outputs point predictions.

## Key Results
- OT-CP outperforms existing methods (M-CP and Merge-CP) in terms of region size while maintaining coverage
- Performance gains are particularly notable for datasets with lower dimensionality (d ≤ 6)
- The method incurs higher computational costs due to OT map estimation but provides a practical solution for multivariate uncertainty quantification
- Coverage guarantees hold even when using an approximated transport map, as long as the calibration radius is computed from the same empirical distribution

## Why This Works (Mechanism)

### Mechanism 1
Optimal transport maps provide a principled way to convert vector-valued conformity scores into scalar ranks while preserving the geometric structure of multivariate data. The Brenier map T* transports the empirical distribution of residuals to a uniform ball. The rank of any point z is defined as ||T*(z)||—its distance from origin in the transported space. This creates a center-outward ordering where points near the distribution center receive lower ranks, naturally extending univariate quantile concepts to higher dimensions. Core assumption: The underlying score distribution has a density (required for Brenier's theorem uniqueness).

### Mechanism 2
Conformal prediction's coverage guarantee holds even when using an approximated transport map, as long as the calibration radius is computed from the same empirical distribution. Proposition 3.6 shows that for any approximation T̂, if we set the radius r̂_α to satisfy Û_n+1(B(0, r̂_α)) ≥ 1-α (where Û is the empirical pushforward), then coverage is preserved. The key insight is that CP's exchangeability argument applies to the transformed scores Z_i = ||T̂ ∘ S(X_i, Y_i)|| regardless of how well T̂ approximates the true map—the calibration step corrects for approximation error. Core assumption: Exchangeability of the sequence (Z_1, ..., Z_n, Z_n+1), which holds when the base predictor treats calibration data independently.

### Mechanism 3
Entropic regularization via Sinkhorn algorithm enables tractable OT map estimation while the conformal calibration absorbs any regularization bias. The entropic map T_ε(z) = Σ_j p_j(z) u_j uses soft assignments computed from Sinkhorn potentials. This avoids cubic complexity of linear assignment while remaining differentiable and out-of-sample applicable. The regularization parameter ε trades off computational speed (larger ε = faster Sinkhorn convergence) against approximation fidelity, but since CP calibrates the radius empirically, moderate regularization does not compromise validity. Core assumption: The number of target points m on the uniform sphere can be chosen independently of calibration sample size n.

## Foundational Learning

- **Exchangeability**: Why needed here: The entire coverage guarantee (Lemma 2.1, 3.5) rests on exchangeability of scores. Without understanding that CP requires only exchangeability—not i.i.d.—you cannot correctly reason about when coverage breaks. Quick check question: If you retrain the base model on calibration data, would OT-CP's coverage guarantee still hold?

- **Optimal Transport and Brenier's Theorem**: Why needed here: OT-CP is built on the mathematical fact that there exists a unique gradient of convex function (Brenier map) pushing one distribution to another. Understanding this is essential for grasping why ||T(z)|| defines a meaningful rank. Quick check question: In 1D, the Brenier map reduces to what familiar object? (Answer: the CDF)

- **Entropic Regularization and Sinkhorn Algorithm**: Why needed here: The practical implementation requires computing OT maps at scale. Sinkhorn iterations (matrix rescaling) provide a differentiable, GPU-friendly alternative to linear programming. Quick check question: What happens to the transport plan as ε → ∞? (Answer: it becomes uniform regardless of cost)

## Architecture Onboarding

- **Component map**: Base predictor -> Score computer -> OT map estimator -> Score transformer -> Calibration -> Prediction set

- **Critical path**: The calibration step (5) is where coverage guarantee is enforced—any error in OT estimation (3-4) is absorbed here. Do not skip or modify this quantile computation.

- **Design tradeoffs**:
  - m (target sphere points): Larger m (e.g., 32768) improves map quality but increases Sinkhorn runtime. Paper uses 2^12 to 2^15 depending on dimension.
  - ε (regularization): Start with ε=0.1. Lower ε requires more Sinkhorn iterations; higher ε may lose geometric detail.
  - Score function choice: Paper uses simple residual ||ŷ-y||_2. Mahalanobis distance is equivalent when data is Gaussian.

- **Failure signatures**:
  - Coverage significantly below 1-α: Check exchangeability violation (e.g., temporal dependence, covariate shift not corrected)
  - Extremely large prediction sets in high dimensions: Curse of dimensionality in OT estimation; try increasing m or using dimensionality reduction on outputs first
  - Runtime explosion: m too large; reduce to 8192 or increase ε

- **First 3 experiments**:
  1. **Sanity check on synthetic 2D data**: Generate bivariate Gaussian residuals, run OT-CP with m=8192, ε=0.1. Verify coverage ≈ 0.90 at α=0.1. Visualize prediction regions—should be non-elliptic if data is skewed.
  2. **Hyperparameter sweep**: On a single dataset, vary ε ∈ {0.001, 0.01, 0.1, 1.0} and m ∈ {4096, 8192, 16384, 32768}. Plot region size vs. coverage to find Pareto frontier. Expect ε=0.1, m=16384-32768 to be optimal.
  3. **Comparison to baselines**: Implement Merge-CP (L2 norm) and M-CP (max of per-dimension scores). Run on 3-5 datasets from the benchmark (e.g., taxi, air, atp1d). OT-CP should show smaller region sizes at equivalent coverage on low-dimensional datasets (d≤6).

## Open Questions the Paper Calls Out

### Open Question 1
Can the OT-CP framework be adapted to maintain efficiency in dimensions significantly higher than the d ≤ 16 regime tested? The paper notes that the "edge vanishing in the higher-dimensional regime" and explicitly cites the "curse of dimensionality" as a challenge for estimating OT maps. This remains unresolved as the entropic map estimator's statistical performance degrades as dimension increases. Demonstration of OT-CP achieving smaller region sizes than M-CP or Merge-CP on benchmark datasets where output dimension d > 20 or d > 50 would resolve this.

### Open Question 2
Is there a theoretically grounded, adaptive protocol for selecting the entropic regularization strength ε and the target grid size m? The authors state that the hyperparameters "describe a fundamental statistical and computational trade-off" and currently require tuning, noting that "both m and ε should be increased to track increase in dimension." The paper relies on empirical ablations to justify a default ε=0.1, lacking a formal rule for selecting these parameters across arbitrary datasets. A heuristic or theorem that defines optimal ε and m as functions of sample size n, dimension d, and desired coverage α would resolve this.

### Open Question 3
Can the method be extended to provide finite-sample conditional coverage guarantees without relying on asymptotic regularity assumptions? The paper guarantees finite-sample marginal coverage (Proposition 3.6) but notes that concurrent work achieves conditional coverage only under asymptotic regularity assumptions. Distribution-free conditional validity is notoriously difficult; the paper does not demonstrate if the OT-based score normalization captures local data geometry sufficiently to offer conditional validity. A modified OT-CP algorithm with a proof of finite-sample conditional coverage, or empirical results showing coverage stability across different strata of the input covariates, would resolve this.

## Limitations
- Performance degrades significantly in high dimensions (d ≥ 14-16) due to curse of dimensionality in OT map estimation
- Computational complexity is higher than baseline methods due to Sinkhorn iterations for OT map estimation
- Implementation details for MQF2 training and sphere sampling parameters are underspecified

## Confidence

- **High**: The theoretical framework (Proposition 3.6) providing distribution-free coverage for any approximation map T̂
- **Medium**: Empirical performance claims on benchmark datasets, given that hyperparameter choices (ε, m) were tuned
- **Low**: Generalizability to very high-dimensional problems (d > 14) and non-exchangeable data

## Next Checks

1. **Exchangeability robustness test**: Apply OT-CP to a dataset with temporal dependence (e.g., air quality with time series). Measure coverage degradation compared to i.i.d. assumption.

2. **High-dimensional scaling experiment**: Evaluate OT-CP on a synthetic dataset with d = 20, comparing performance against baselines and measuring runtime as m increases.

3. **Sphere sampling sensitivity**: Systematically vary n_S, n_R, n_o in the target sphere generation. Quantify impact on map quality (e.g., via OT cost) and prediction region size.