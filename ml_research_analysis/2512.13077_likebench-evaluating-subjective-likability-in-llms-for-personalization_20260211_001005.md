---
ver: rpa2
title: 'LikeBench: Evaluating Subjective Likability in LLMs for Personalization'
arxiv_id: '2512.13077'
source_url: https://arxiv.org/abs/2512.13077
tags:
- user
- conversation
- likability
- page
- humor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of measuring subjective likability\
  \ in personalized LLM interactions, which is currently under-measured by existing\
  \ benchmarks focused on memory recall and adherence. The core method, LikeBench,\
  \ introduces a multi-session, dynamic evaluation framework that assesses seven diagnostic\
  \ metrics of likability\u2014emotional adaptation, formality matching, knowledge\
  \ adaptation, reference understanding, conversation length fit, humor fit, and callback\u2014\
  using psychologically grounded user personas and simulated users."
---

# LikeBench: Evaluating Subjective Likability in LLMs for Personalization

## Quick Facts
- **arXiv ID**: 2512.13077
- **Source URL**: https://arxiv.org/abs/2512.13077
- **Reference count**: 40
- **Primary result**: DeepSeek R1 achieved 28% higher likability scores than Qwen3 despite having lower memory accuracy (86% vs 93%).

## Executive Summary
This paper addresses the gap between memory recall and subjective user satisfaction in personalized LLM interactions. The authors introduce LikeBench, a multi-session evaluation framework that decomposes "likability" into seven diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback. Using psychologically grounded personas and simulated users, the benchmark reveals that strong memory performance does not guarantee high likability—DeepSeek R1 outperformed Qwen3 by 28% on likability despite Qwen3's superior fact recall. The framework exposes a critical limitation: even state-of-the-art models like GPT-5 show limited robustness in longer, noisier interactions, with performance declining predictably after the initial "honeymoon" phase.

## Method Summary
LikeBench uses Claude 3.7 Sonnet as both simulated user and judge to evaluate LLMs across 50 synthetic profiles, each with 35 personality facets and 9 conversation style dimensions. Each profile undergoes 10 sessions of 5 turns each (2,500 total turns). The framework scores responses on 7 diagnostic metrics (1-5 scale per turn) and measures adaptability via Improvement Rate. The simulated user generates messages based on hidden personas and session priors, while target LLMs see only dialogue history. Memory accuracy is evaluated post-hoc by extracting and verifying recalled facts. The evaluation loop iterates through user generation, LLM response, metric scoring, and history updates.

## Key Results
- **Memory-likability decoupling**: DeepSeek R1 (86% memory accuracy, 17 facts/profile) outperformed Qwen3 (93% accuracy, 43 facts/profile) by 28% on likability scores
- **Performance degradation over time**: All models showed predictable decline in sessions 3-6 due to accumulated conversational noise and deeper emotional queries
- **GPT-5 strengths and weaknesses**: Scored highest on knowledge adaptation (80-100% utilization) but lower on conversation length fit, indicating a comprehensiveness-conciseness trade-off

## Why This Works (Mechanism)

### Mechanism 1: Multi-Dimensional Likability Decomposition
Decomposing subjective likability into 7 orthogonal diagnostic metrics allows precise failure isolation. Instead of optimizing for a single "helpfulness" score, models must succeed on specific axes like humor fit versus knowledge adaptation. This forces discriminative evaluation that aggregate scores obscure.

### Mechanism 2: Psychologically Grounded Persona Constraints
Using 35 personality facets (Big Five + Honesty-Humility + Humor Styles) with descriptive text rather than scalar ratings creates stricter, more human-like constraints. This prevents generic "people-pleasing" responses and forces models to navigate nuanced trait combinations.

### Mechanism 3: Decoupling of Memory Retrieval and Social Application
The framework separates fact storage (memory accuracy) from social utility (likability). Models may recall facts correctly but fail delivery through poor timing or emotional misalignment, revealing that utility depends on context-sensitive deployment, not just presence.

## Foundational Learning

- **Concept**: Facet-Based Personality Modeling
  - **Why needed here**: Essential for constructing the 35-facet personality vectors that drive the synthetic user profiles and constrain model behavior
  - **Quick check question**: Can you prompt a model to distinguish between "Gregariousness" (seeking crowds) and "Assertiveness" (taking charge), or would it collapse both into "Extroversion"?

- **Concept**: LLM-as-a-Judge Reliability
  - **Why needed here**: The entire scoring mechanism relies on Claude 3.7 Sonnet evaluating responses; understanding its biases is critical for interpreting results
  - **Quick check question**: If Model A evaluates itself, how would you detect if the scores are inflated compared to a cross-model evaluation?

- **Concept**: Context-Window vs. Stateful Tracking
  - **Why needed here**: The performance drop in longer sessions raises questions about whether this is context-window saturation or state-tracking failure
  - **Quick check question**: Does the "Honeymoon phase" drop (Session 3-6) imply the context window is full, or that accumulated history introduces noise that confuses persona adherence?

## Architecture Onboarding

- **Component map**: Persona Generator -> Prior Generator -> User Agent -> Evaluator -> Memory Evaluator
- **Critical path**: Prior Generation is most sensitive; without cross-session dependencies, the benchmark fails to test long-term callback and coordination
- **Design tradeoffs**:
  - Static vs. Dynamic Profiles: LikeBench uses static profiles, potentially penalizing models designed to "warm up" users
  - Information Asymmetry: Assistants see only dialogue history while User Agents see full profiles, creating a difficult inference problem
- **Failure signatures**:
  - "Honeymoon" Drop: High scores in Sessions 1-3 followed by predictable decline in 3-6 (R² > 0.7)
  - Aggressive Inference Penalties: Models that hallucinate implicit preferences are penalized heavily in likability
  - Verbosity Trade-off: High knowledge adaptation scores often conflict with conversation length fit
- **First 3 experiments**:
  1. **Metric Correlation Analysis**: Run baseline model and plot correlation matrix to verify metrics are distinct
  2. **Ablate the "Prior"**: Compare randomized vs standard priors to quantify multi-session difficulty
  3. **Dynamic Profile Test**: Implement evolving user profiles to measure if this closes later-session performance gaps

## Open Questions the Paper Calls Out

- **Open Question 1**: How can LLM architectures be optimized to prevent likability degradation during the "calibration" phase (Sessions 3-6) of extended interactions? The paper identifies fragility in longer contexts but doesn't propose specific mechanisms to sustain performance during complex middle phases.

- **Open Question 2**: What specific conversational strategies allow models with lower memory recall (e.g., DeepSeek R1) to achieve higher likability scores than models with superior memory accuracy? The paper establishes that rote fact retrieval is insufficient but doesn't isolate the compensatory mechanisms.

- **Open Question 3**: How can evaluation benchmarks capture dynamic, reciprocal adaptation where the LLM influences the user's preferences, rather than penalizing models for deviating from static profiles? Current benchmarks treat personas as fixed ground truth, failing to distinguish between hallucination and legitimate rapport building.

## Limitations

- **Judge reliability uncertainty**: The framework relies entirely on an LLM judge, introducing potential self-preference bias without human validation
- **Synthetic user generalizability**: Carefully constructed personas may not capture the complexity and contradictions of real human behavior
- **Static profile constraint**: Fixed user profiles may penalize natural human interaction strategies like "warming up" or influencing user preferences

## Confidence

- **High Confidence**: The core finding that memory accuracy and likability are decoupled is well-supported by comparative results
- **Medium Confidence**: The superiority of descriptive personas over coarse ratings is theoretically sound but lacks direct validation
- **Low Confidence**: The claim that 7 metrics capture the "majority of variance in user satisfaction" is speculative and may miss other important dimensions

## Next Checks

1. **Cross-Judge Validation**: Run the same benchmark using multiple judge models and compare score distributions to quantify scoring consistency
2. **Human Validation Subset**: Have human annotators score 10% of synthetic conversations to establish validity against ground truth
3. **Dynamic Profile Implementation**: Implement evolving user profiles to measure if this closes performance gaps in later sessions and validates the accumulated history noise hypothesis