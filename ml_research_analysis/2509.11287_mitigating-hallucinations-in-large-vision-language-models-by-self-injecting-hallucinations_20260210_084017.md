---
ver: rpa2
title: Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting
  Hallucinations
arxiv_id: '2509.11287'
source_url: https://arxiv.org/abs/2509.11287
tags:
- apasi
- preference
- hallucination
- llav
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Autonomous Preference Alignment via Self-Injection
  (APASI), a novel method to mitigate hallucinations in Large Vision-Language Models
  (LVLMs) without relying on external human annotations or auxiliary models. APASI
  autonomously constructs preference data by using the target LVLM to self-inject
  hallucinations into its own generated responses, creating valid preference pairs
  for training.
---

# Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations

## Quick Facts
- **arXiv ID:** 2509.11287
- **Source URL:** https://arxiv.org/abs/2509.11287
- **Reference count:** 40
- **Primary result:** Reduces hallucination ratios (4.5/1.8 decrease in hallucinated objects) without external human annotations or auxiliary models

## Executive Summary
This paper introduces APASI (Autonomous Preference Alignment via Self-Injection), a novel method to mitigate hallucinations in Large Vision-Language Models (LVLMs) without requiring external human annotations or auxiliary models. APASI autonomously constructs preference data by using the target LVLM to self-inject hallucinations into its own generated responses, creating valid preference pairs for training. The method employs an iterative alignment strategy with curriculum learning to progressively increase task difficulty and ensure stable model improvement. Experiments across six benchmarks demonstrate that APASI effectively reduces hallucination ratios while improving overall performance for multiple baseline LVLMs.

## Method Summary
APASI addresses LVLM hallucinations through autonomous preference alignment. The method generates preferred responses with the target LVLM, constructs a co-occurrence graph from parsed objects, and self-injects hallucinations using weighted sentence sampling, template filling, and a "blind" language-only LVLM to generate hallucinated sentences. This creates dis-preferred responses containing injected hallucinations. The approach uses iterative alignment with curriculum learning (T=3, decreasing ρ from 0.6 to 0.2) combined with DPO training. The entire pipeline operates without external human annotations or auxiliary models, relying solely on the target LVLM's capabilities for data generation and preference construction.

## Key Results
- Reduces hallucinated objects by 4.5 on Object-Hal and 1.8 on AMBER benchmarks
- Achieves results comparable or superior to methods requiring external dependencies
- Demonstrates effectiveness across multiple baseline LVLMs
- Shows stable performance improvement across iterative alignment cycles

## Why This Works (Mechanism)
APASI works by leveraging the target LVLM's own knowledge and generation capabilities to create realistic hallucination examples. By using co-occurrence graphs built from parsed objects and weighted sentence sampling, the method injects contextually plausible hallucinations that challenge the model's visual reasoning. The iterative alignment with curriculum learning progressively increases difficulty, allowing the model to first learn basic alignment before tackling more challenging hallucinations. The self-injection mechanism ensures that the hallucinated content is contextually appropriate and challenging for the target LVLM to distinguish from ground truth.

## Foundational Learning
- **Co-occurrence graphs:** Represent object relationships based on frequency of appearance together in training data. Needed to identify plausible hallucination candidates that maintain contextual coherence. Quick check: Verify graph density and whether injected objects have reasonable co-occurrence scores with existing objects.
- **Curriculum learning:** Gradually increases task difficulty during training. Needed to ensure stable learning progression and prevent catastrophic forgetting. Quick check: Monitor performance metrics across curriculum stages to confirm smooth improvement.
- **DPO training:** Direct Preference Optimization for learning from preference pairs. Needed to align model outputs with human-like preferences without explicit labels. Quick check: Validate that training loss decreases and preference accuracy improves on held-out pairs.
- **Blind LVLM:** Language-only model that generates hallucinated content without visual context. Needed to create diverse hallucination patterns. Quick check: Ensure generated hallucinations are plausible but incorrect given the image content.
- **Template-based injection:** Structured approach to inserting hallucinations into responses. Needed for controlled and reproducible hallucination generation. Quick check: Verify template diversity and appropriate usage frequency.
- **Object parsing with WordNet:** Semantic understanding of object relationships. Needed to identify synonym sets and build comprehensive co-occurrence relationships. Quick check: Validate synonym coverage and accuracy of object identification.

## Architecture Onboarding

### Component Map
Target LVLM -> Co-occurrence Graph Builder -> Template Engine -> Blind LVLM -> DPO Trainer -> Aligned LVLM

### Critical Path
1. Generate preferred responses with target LVLM
2. Parse objects and build co-occurrence graph
3. Self-inject hallucinations using weighted sampling and templates
4. Generate dis-preferred responses with blind LVLM
5. Train with DPO using preference pairs
6. Iterate with updated model and adjusted curriculum

### Design Tradeoffs
- **Self-injection vs external data:** Eliminates dependency on human annotations but relies on model's ability to generate challenging hallucinations
- **Template diversity vs control:** 21 templates provide variety but may limit hallucination types
- **Curriculum scheduling:** Fixed schedule (0.8-0.2) balances stability vs optimal learning rate
- **Graph-based vs semantic injection:** Co-occurrence graphs ensure plausibility but may miss rare but important hallucination types

### Failure Signatures
- Zero effective dataset size due to ρL rounding to 0
- Dis-preferred responses lacking hallucinations
- Training instability with fluctuating performance
- Limited hallucination diversity across iterations

### First 3 Experiments to Run
1. Generate preferred responses on SI-23k and validate object parsing accuracy
2. Create co-occurrence graph and test hallucination injection on sample responses
3. Run single iteration of DPO training and evaluate hallucination reduction on Object-Hal

## Open Questions the Paper Calls Out
- **Extending to factual hallucinations:** Can APASI be adapted to address hallucinations stemming from factual inaccuracies about real-world knowledge, rather than solely visual inconsistencies? The current method focuses on visual fidelity and doesn't explicitly model knowledge-based errors.
- **Scaling to larger models:** Does the effectiveness of self-injection scale efficiently to LVLMs with significantly larger parameter counts (70B+)? Current experiments were limited to 7B models due to computational constraints.
- **Targeting attribute and relationship hallucinations:** Can the self-injection mechanism be refined to specifically reduce attribute and relationship hallucinations, rather than focusing primarily on object existence? The current approach emphasizes object co-occurrence.

## Limitations
- Method primarily addresses visual fidelity hallucinations, not factual knowledge errors
- Effectiveness on larger LVLMs (70B+) remains untested
- Focus on object hallucinations may not adequately address attribute and relationship errors
- Performance sensitivity to template sampling and blind LVLM implementation details

## Confidence
- **High Confidence:** Core methodology and hallucination reduction metrics are technically sound and reproducible
- **Medium Confidence:** Relative performance improvements depend on implementation details not fully specified
- **Low Confidence:** Claims about effectiveness across diverse LVLM architectures lack direct experimental support

## Next Checks
1. **Template Sampling Validation:** Implement uniform random sampling for the 21 templates in Table 4 and verify hallucination quality and diversity across multiple runs.
2. **Blind LVLM Parameter Sweep:** Experiment with different temperature and top-p values for hallucination completion to determine sensitivity of hallucination quality and training stability.
3. **Curriculum Schedule Sensitivity:** Test alternative curriculum schedules to empirically determine if the specific fc(t)=0.8−0.2t schedule is critical for reported performance improvements.