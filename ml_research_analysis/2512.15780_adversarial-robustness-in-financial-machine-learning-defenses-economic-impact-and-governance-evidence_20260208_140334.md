---
ver: rpa2
title: 'Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact,
  and Governance Evidence'
arxiv_id: '2512.15780'
source_url: https://arxiv.org/abs/2512.15780
tags:
- adversarial
- robustness
- risk
- financial
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a comprehensive, dataset-agnostic pipeline\
  \ for evaluating adversarial robustness in tabular machine learning models used\
  \ in financial decision-making. The core method involves applying gradient-based\
  \ attacks (FGSM and PGD with \u03F5=0.05) to models trained on credit-scoring and\
  \ fraud detection data, then measuring impacts across discrimination (AUC, KS, Gini),\
  \ calibration (ECE, Brier), economic risk (Expected Loss, VaR95, ES95), distribution\
  \ drift, fairness, and explanation stability (SHAP, semantic)."
---

# Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence

## Quick Facts
- arXiv ID: 2512.15780
- Source URL: https://arxiv.org/abs/2512.15780
- Reference count: 3
- One-line primary result: Adversarial attacks degrade tabular model performance; adversarial training recovers some utility and enables governance-aligned reporting.

## Executive Summary
This paper introduces a comprehensive, dataset-agnostic pipeline for evaluating adversarial robustness in tabular machine learning models used in financial decision-making. The core method involves applying gradient-based attacks (FGSM and PGD with ε=0.05) to models trained on credit-scoring and fraud detection data, then measuring impacts across discrimination (AUC, KS, Gini), calibration (ECE, Brier), economic risk (Expected Loss, VaR95, ES95), distribution drift, fairness, and explanation stability (SHAP, semantic). The primary results show that adversarial perturbations cause notable performance degradation—AUC drops by roughly 10.6%, ECE rises significantly, and expected portfolio loss increases by ~5% with higher tail risk. Statistical significance is validated via bootstrap confidence intervals. Adversarial training partially recovers model utility, improving clean AUC and reducing expected loss with minor calibration trade-offs. The study highlights that explanation stability often declines before predictive performance, suggesting its use as an early-warning indicator. The framework supports governance-aligned reporting and is extensible to various tabular financial applications.

## Method Summary
The pipeline evaluates adversarial robustness using a gradient-based attack framework (FGSM and PGD, ε=0.05) on tabular models trained for credit scoring and fraud detection. Models are MLPs with two hidden layers (128/64 units, ReLU, dropout 0.3). Attacks generate adversarial examples constrained by domain projectors enforcing feature-wise bounds. Performance is assessed across discrimination (AUC, KS, Gini), calibration (ECE, Brier), economic risk (EL, VaR95, ES95), distribution drift, fairness, and explanation stability (SHAP, semantic). Adversarial training with PGD augments batches with perturbed examples to improve robustness. Bootstrap confidence intervals validate statistical significance. Outputs include JSON/CSV artifacts and LLM-based semantic explanations for governance reporting.

## Key Results
- Adversarial attacks reduce AUC by ~10.6% and increase expected portfolio loss by ~5% with higher tail risk (VaR95, ES95).
- Adversarial training partially recovers model utility, boosting clean AUC and reducing expected loss with minor calibration trade-offs.
- Explanation stability (SHAP) often declines before predictive performance, suggesting its use as an early-warning indicator for adversarial influence.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-based adversarial attacks degrade discrimination and increase economic risk in tabular financial models.
- Mechanism: FGSM and PGD compute gradients of the loss with respect to inputs and apply small, bounded perturbations (ε = 0.05) that shift instances across decision thresholds. This causes measurable drops in AUC, KS, and Gini while increasing false negatives and tail risk (VaR95, ES95).
- Core assumption: Models are differentiable (white-box access) and perturbations are constrained to plausible feature ranges via domain projectors.
- Evidence anchors:
  - [abstract] Results show AUC drops by ~10.6% and expected portfolio loss increases by ~5% under FGSM/PGD attacks with ε = 0.05.
  - [section] Section 6.1–6.2 formalizes FGSM and PGD; Table 1 quantifies AUROC decline; Table 3 shows elevated EL/VaR/ES.
  - [corpus] Related work (e.g., "Conditional Adversarial Fragility in Financial ML") notes adversarial vulnerability under stress, supporting boundary-shift dynamics.
- Break condition: Fails if models are non-differentiable (e.g., tree ensembles without surrogates), if gradients vanish, or if domain projectors over-constrain perturbations.

### Mechanism 2
- Claim: Adversarial training improves robustness by exposing the model to on-the-fly adversarial examples, partially recovering utility.
- Mechanism: PGD adversarial training augments each mini-batch with perturbed examples, forcing the model to learn smoother decision boundaries. This yields higher clean and adversarial AUC, reduced expected loss, and improved calibration under attack.
- Core assumption: Training-time adversarial examples are representative of deployment-time attacks; the perturbation budget remains consistent.
- Evidence anchors:
  - [abstract] Adversarial training partially recovers model utility, boosting clean AUC and minimizing expected loss.
  - [section] Section 5 describes the PGD-hardened model; Section 8.5 shows clean AUROC improving from 0.735 to 0.743 and post-attack ECE dropping to ~0.029–0.030.
  - [corpus] Corpus evidence on adversarial training in financial tabular settings is limited; most related work focuses on attacks, not defenses.
- Break condition: Breaks if test-time attacks exceed training ε, if over-regularization harms clean performance, or if distribution shift renders training adversarial examples unrepresentative.

### Mechanism 3
- Claim: Explanation stability degrades before predictive performance, enabling early-warning detection of adversarial influence.
- Mechanism: SHAP attribution vectors for clean vs. perturbed inputs are compared via cosine similarity, Spearman rank correlation, and ℓ2 distance. Attribution drift often precedes AUC decline, indicating shifts in model reasoning.
- Core assumption: SHAP values are faithful proxies for feature reliance and are sensitive to input perturbations.
- Evidence anchors:
  - [abstract] Explanation stability often declines before predictive performance, suggesting early-warning use.
  - [section] Section 7.5 defines SHAP stability metrics; Section 8.7 reports decreased cosine similarity and rank correlation under attack.
  - [corpus] Corpus does not provide explicit evidence on SHAP stability as an early-warning signal; this is a paper-specific finding.
- Break condition: Fails if SHAP attributions are inherently unstable, if perturbations affect reasoning without changing SHAP, or if feature interactions are highly non-linear.

## Foundational Learning

- Concept: **Gradient-based adversarial attacks (FGSM/PGD)**
  - Why needed here: The pipeline relies on these attacks to generate perturbations and measure robustness. Understanding how gradients manipulate inputs is essential for interpreting results and tuning ε.
  - Quick check question: Can you explain why PGD is generally stronger than FGSM for a fixed ε?

- Concept: **Calibration (ECE, Brier score)**
  - Why needed here: Adversarial perturbations distort predicted probabilities, affecting downstream decisions. Calibration metrics quantify this distortion beyond raw accuracy.
  - Quick check question: What does a high ECE indicate about model reliability?

- Concept: **Economic risk metrics (EL, VaR, ES)**
  - Why needed here: The pipeline translates prediction errors into portfolio-level financial impact. These metrics connect ML performance to business and regulatory concerns.
  - Quick check question: How does VaR95 differ from Expected Shortfall (ES95)?

## Architecture Onboarding

- Component map:
  Preprocessing (cleaning, encoding, scaling, train/val/test split, domain projector) -> Model (MLP: 128/64 units, ReLU, dropout 0.3) -> Attacks (FGSM/PGD, ε=0.05, domain projection) -> Defenses (PGD adversarial training, noise regularization) -> Evaluation (discrimination, calibration, economic risk, SHAP stability, drift, fairness, governance outputs)

- Critical path:
  1. Load and preprocess tabular data (schema-aware, domain constraints).
  2. Train baseline MLP and optionally PGD-hardened model.
  3. Generate adversarial examples (FGSM/PGD) with domain projection.
  4. Evaluate clean vs. adversarial performance across all metric categories.
  5. Compute bootstrap confidence intervals and generate governance reports.

- Design tradeoffs:
  - MLP vs. other models: MLP enables gradient-based attacks; tree ensembles may require surrogate models.
  - ε selection: Larger ε increases attack strength but may violate plausibility; ε=0.05 is a middle ground.
  - Adversarial training cost: PGD training is computationally expensive but yields better robustness.
  - SHAP stability: Sensitive to perturbation scale; may produce noise if ε is too small or features are sparse.

- Failure signatures:
  - AUC remains stable but ECE spikes → model is miscalibrated under attack.
  - EL/VaR/ES increase without AUC change → tail risk shifted, threshold-sensitive.
  - SHAP similarity drops before AUC → early warning triggered.
  - Bootstrap CIs overlap → degradation may not be statistically significant.

- First 3 experiments:
  1. **Baseline robustness check**: Train MLP on credit dataset, apply FGSM/PGD with ε=0.05, report AUC, ECE, EL, VaR95, ES95, and SHAP cosine similarity.
  2. **Adversarial training evaluation**: Train PGD-hardened MLP, repeat attacks, compare clean/adversarial AUC, ECE, and economic metrics to baseline.
  3. **Early-warning validation**: Vary ε from 0.01 to 0.10, track SHAP stability vs. AUC degradation to test whether explanation drift precedes performance drop.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed benefits of adversarial training transfer to non-neural architectures commonly used in finance, such as gradient-boosted trees?
- Basis in paper: [explicit] The authors state in Future Work that a broader battery of models is needed, specifically calling for the evaluation of "gradient-boosted trees and modern deep tabular architectures."
- Why unresolved: The study focuses exclusively on a Multilayer Perceptron (MLP) baseline; it is unclear if white-box gradient attacks and defenses apply similarly to tree-based ensembles which handle feature interactions differently.
- What evidence would resolve it: Empirical results from applying the identical pipeline (FGSM/PGD attacks and adversarial training) to XGBoost or LightGBM models on the same credit datasets.

### Open Question 2
- Question: How does the robustness evaluation change when perturbations are constrained by complex, joint business rules rather than simple feature-wise bounds?
- Basis in paper: [explicit] In the Limitations section, the authors note that current "plausibility constraints are primarily feature-wise and do not yet encode complex business rules or joint constraints (e.g., payment histories consistent over time)."
- Why unresolved: Current domain projectors enforce individual feature limits (e.g., non-negativity) but fail to invalidate unrealistic combinations of features, potentially overstating the attacker's real-world capabilities.
- What evidence would resolve it: A comparative study measuring model degradation under standard $\ell_\infty$ constraints versus constraints encoded by a business logic engine or causal graph.

### Open Question 3
- Question: To what extent does the choice of Large Language Model (LLM) introduce bias or instability into the Semantic Robustness Index (SRI)?
- Basis in paper: [explicit] The Limitations section highlights that the semantic robustness metrics "rely on an underlying language model whose own biases and robustness properties warrant careful study."
- Why unresolved: The LLM acts as a black-box assessor of explanation plausibility; if the LLM hallucinates or is sensitive to prompt formatting, the resulting SRI may not reliably reflect the financial model's stability.
- What evidence would resolve it: A sensitivity analysis showing SRI variance across different LLM providers or prompts for the same set of adversarial explanations.

## Limitations
- The study focuses exclusively on MLPs; benefits of adversarial training may not transfer to tree-based ensembles commonly used in finance.
- Domain projectors enforce only feature-wise bounds, not complex joint business rules, potentially overstating real-world attack capabilities.
- The LLM-based semantic robustness index relies on a black-box model whose biases and sensitivity are not fully characterized.

## Confidence
- **High confidence**: Gradient-based attacks (FGSM/PGD) cause measurable degradation in AUC, ECE, and economic risk (EL/VaR/ES) for differentiable tabular models.
- **Medium confidence**: Adversarial training partially recovers clean and adversarial performance, and explanation stability often declines before predictive performance.
- **Low confidence**: SHAP stability serves as a robust early-warning indicator, and the LLM-based semantic robustness index is meaningful.

## Next Checks
1. **Dataset and preprocessing verification**: Obtain or simulate a credit-scoring dataset matching the paper's schema, including feature bounds, immutable fields, and cost parameters. Verify preprocessing steps (imputation, encoding, scaling) match the paper.
2. **Economic metric validation**: Confirm that portfolio-level risk metrics (EL, VaR95, ES95) are computed correctly using the assumed PD, LGD, and EAD values, and that bootstrap confidence intervals are consistent with reported results.
3. **SHAP stability as early warning**: Systematically vary perturbation strength (ε from 0.01 to 0.10) and record when SHAP stability metrics decline relative to AUC. Compare to other potential early-warning signals like calibration drift or feature importance variance.