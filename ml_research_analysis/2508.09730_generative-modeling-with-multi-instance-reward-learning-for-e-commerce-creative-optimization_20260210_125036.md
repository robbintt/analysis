---
ver: rpa2
title: Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative
  Optimization
arxiv_id: '2508.09730'
source_url: https://arxiv.org/abs/2508.09730
tags:
- creative
- each
- candidate
- learning
- component
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the challenge of selecting the most effective\
  \ combination of creative elements\u2014such as titles, images, and highlights\u2014\
  for e-commerce advertising. This is complicated by an exponentially large search\
  \ space and sparse user feedback."
---

# Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization

## Quick Facts
- arXiv ID: 2508.09730
- Source URL: https://arxiv.org/abs/2508.09730
- Authors: Qiaolei Gu; Yu Li; DingYi Zeng; Lu Wang; Ming Pang; Changping Peng; Zhangang Lin; Ching Law; Jingping Shao
- Reference count: 8
- Primary result: 3.88% relative CTR lift and 3.26% RPM increase from GenCO framework

## Executive Summary
The paper addresses creative optimization for e-commerce advertising, where selecting optimal combinations of titles, images, and highlights from exponentially large candidate pools is critical for performance. The key challenge is that user feedback (clicks) arrives only at the combination level, while the goal is to learn which individual elements drive performance. GenCO solves this through a two-stage framework: first generating diverse candidate combinations via non-autoregressive generative modeling, then using multi-instance learning to attribute combination-level rewards to individual creative elements.

The framework was deployed on a leading e-commerce platform and achieved significant commercial impact: 3.88% relative lift in click-through rate and 3.26% increase in revenue per mille compared to existing baselines. The approach combines reinforcement learning for exploration, cross-component attention for modeling creative synergies, and multi-instance learning for credit assignment from sparse combination-level feedback to individual elements.

## Method Summary
GenCO is a two-stage framework for e-commerce creative optimization. The first stage uses a context-aware, non-autoregressive generative model with cross-component attention to efficiently produce diverse candidate combinations. The model generates preference scores for each candidate element in parallel, conditioned on shared context embeddings and attention vectors that capture inter-element dependencies. The second stage employs multi-instance learning to attribute combination-level rewards (clicks) to individual creative elements, enabling more accurate feedback signals. The generation process is optimized with reinforcement learning, enabling effective exploration of the combinatorial space and refinement of selections. The framework was trained on 275M users and 110M ads, with evaluation on 6.2M users and 7.2M ads.

## Key Results
- 3.88% relative lift in click-through rate compared to existing baselines
- 3.26% increase in revenue per mille (RPM)
- MIL component ablation shows -0.126% to -0.165% CTR decline across components
- RL component ablation shows -1.882% CTR decline for image component (largest single-component drop)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-autoregressive generative modeling with cross-component attention enables efficient exploration of exponentially large creative combination spaces while capturing inter-element dependencies.
- Mechanism: The generative model produces preference scores for each candidate element in parallel, conditioned on shared context embeddings and cross-component attention vectors that explicitly model interactions between different creative types (titles, images, highlights). Selection probabilities are computed via softmax normalization within each component, allowing parallel sampling rather than sequential chain generation.
- Core assumption: Inter-element dependencies (e.g., title-image synergy) can be captured through attention mechanisms over element embeddings, and these dependencies are predictive of user engagement.
- Evidence anchors:
  - [abstract]: "context-aware, non-autoregressive generative model to efficiently produce diverse candidate combinations"
  - [section]: Cross-component attention equation (13) shows explicit conditioning on all candidate elements across components
  - [corpus]: Weak direct evidence—corpus neighbors focus on generative frameworks generally but don't validate this specific attention mechanism
- Break condition: If cross-component attention weights remain uniform across different ads/contexts, or if generated combinations show no diversity improvement over random selection, the attention mechanism is not learning meaningful dependencies.

### Mechanism 2
- Claim: Multi-instance learning (MIL) enables more accurate reward signal propagation from sparse combination-level feedback to individual creative elements.
- Mechanism: Each creative combination is treated as a "bag" containing element "instances." The MIL framework jointly optimizes combination-level and element-level objectives through shared policy network parameters, allowing clicks observed at the bag level to provide supervision for individual instance representations via attention-weighted aggregation.
- Core assumption: Combination-level rewards can be meaningfully decomposed to constituent elements, and this decomposition improves generalization to unseen combinations.
- Evidence anchors:
  - [abstract]: "multi-instance learning model attributes combination-level rewards, such as clicks, to the individual creative elements"
  - [section]: Ablation study shows -0.126% to -0.165% CTR decline when MIL removed across most components
  - [corpus]: No direct corpus validation of MIL for creative optimization specifically
- Break condition: If element-level predictions show no improvement even when combination-level accuracy is high, or if attention weights for element attribution remain near-uniform, the credit assignment mechanism is not learning to disentangle element contributions.

### Mechanism 3
- Claim: Reinforcement learning with policy gradients addresses sparse supervision by enabling reward propagation to unexposed combinations and disentangling creative effectiveness from product appeal.
- Mechanism: The policy network generates categorical distributions over elements for each component. Policy gradient optimization uses observed/predicted rewards to update generation policy, allowing credit assignment to sampled combinations rather than requiring exposure of all combinations. This enables generalization to rarely-exposed elements.
- Core assumption: The reward signal r(Ai, ci, q) adequately captures creative effectiveness separate from product inherent appeal, and policy gradient sampling provides sufficient exploration.
- Evidence anchors:
  - [abstract]: "generative process is optimized with reinforcement learning, enabling the model to effectively explore and refine its selections"
  - [section]: Ablation shows -1.882% CTR decline for image component when RL removed—the largest single-component drop
  - [corpus]: "Rewarding Creativity" paper discusses RL for creative generation but in storytelling domain, not direct validation
- Break condition: If policy gradient training shows no convergence improvement over supervised-only training, or if generated combinations cluster around already-well-exposed elements without exploration, the RL mechanism is not providing effective exploration.

## Foundational Learning

- Concept: Multi-Instance Learning (MIL)
  - Why needed here: User feedback arrives only at the complete ad level (click/no-click), but optimization requires understanding which individual creative elements drove that feedback. MIL provides the mathematical framework for this bag-to-instance attribution problem.
  - Quick check question: Given a clicked ad with title A, image B, and highlight C, can you articulate why standard supervised learning on each element independently would produce weaker signals than MIL's joint optimization?

- Concept: Policy Gradient Methods in Discrete Action Spaces
  - Why needed here: Creative selection is fundamentally discrete (choose one element from each component pool). Standard gradient descent cannot directly optimize discrete sampling; policy gradients enable learning through stochastic sampling and reward-weighted updates.
  - Quick check question: Can you explain why REINFORCE-style policy gradient works for discrete creative selection while Q-learning would face practical challenges with the combinatorial action space?

- Concept: Cross-Attention Mechanisms
  - Why needed here: Creative elements don't operate in isolation—a title about "50% off" pairs better with product images than lifestyle images. Cross-attention allows element selection to condition on other candidates in the pool, modeling these synergies explicitly.
  - Quick check question: If you removed cross-component attention and only used element embeddings independently, what specific failure mode would you expect in the generated combinations?

## Architecture Onboarding

- Component map:
  - Context Encoder Network -> Cross-Component Attention -> Shared Policy Network (φ) -> Combination-Level Reward Model -> Selection

- Critical path:
  1. Input: (ad ID, user features, query, candidate sets for 6 creative components)
  2. Context encoding → 8-dim hi embedding
  3. For each candidate element: compute cross-component attention vector
  4. Policy network generates preference scores → softmax per component
  5. Sample K combinations in parallel
  6. Reward model scores each combination
  7. Select highest-scoring combination for serving
  8. Offline: collect click labels → update MIL loss + RL loss (λ=0.15)

- Design tradeoffs:
  - **Non-autoregressive vs. autoregressive generation**: Parallel sampling provides efficiency (critical for real-time serving) but may miss sequential dependencies. Paper shows this tradeoff is acceptable given attention-based context conditioning.
  - **Shared vs. separate networks for element/combination scoring**: Shared policy enables unified feature space but risks gradient interference. Ablation confirms this works better than separate models.
  - **Fixed padding (15 candidates) vs. dynamic sizing**: Simplifies batch processing but wastes computation for components with fewer candidates. Implementation uses fixed size with masking.

- Failure signatures:
  - Low attention weight variance across elements → cross-component attention not learning meaningful interactions
  - Sampled combinations cluster around same few elements despite softmax temperature → exploration insufficient, check reward scaling
  - Large discrepancy between offline sCTR and online CTR → overfitting to exposure bias in training data
  - RL loss diverging while MIL loss converges → reward signal too sparse or λ too high

- First 3 experiments:
  1. **Baseline comparison with disabled attention**: Set cross-component attention weights to uniform, measure CTR drop. If <1%, the attention mechanism isn't providing value.
  2. **Element-level ablation**: Remove MIL loss (keep only Lcomb), measure whether element-level predictions degrade more than combination-level predictions. This validates the credit assignment hypothesis.
  3. **Cold-start simulation**: Hold out 20% of creative elements from training (treat as zero-exposure), measure whether RL-enabled model outperforms supervised-only baseline on held-out elements. This tests the exploration/extrapolation claim.

## Open Questions the Paper Calls Out

- Question: Can the Multi-Instance Learning (MIL) framework fully disentangle creative effectiveness from inherent product appeal in scenarios with high popularity bias?
- Question: Does the performance lift of GenCO scale linearly or logarithmically as the size of candidate element pools increases significantly beyond the current industrial averages (e.g., >100 candidates)?
- Question: To what extent does the non-autoregressive generation strategy fail to capture sequential dependencies compared to autoregressive alternatives?

## Limitations

- The claimed commercial impact (3.88% CTR lift, 3.26% RPM increase) depends heavily on offline calibration quality and specific data collection under random exposure policy
- Cross-component attention mechanism's actual contribution to creative synergy discovery versus simple exposure bias memorization is unclear without direct ablation or attention weight analysis
- RL exploration claims are supported by ablation studies but lack direct comparison to alternative exploration strategies

## Confidence

- **High Confidence**: The two-stage framework architecture and MIL credit assignment mechanism are technically sound and well-specified. The core idea of using multi-instance learning for element-level attribution from combination-level rewards is novel and theoretically justified.
- **Medium Confidence**: The claimed commercial impact is specific and measurable but depends heavily on offline calibration quality. The RL exploration claims are supported by ablation studies but lack direct comparison to alternative exploration strategies.
- **Low Confidence**: The cross-component attention mechanism's actual contribution to creative synergy discovery versus simple exposure bias memorization is unclear without direct ablation or attention weight analysis.

## Next Checks

1. **Attention Ablation with Uniform Weights**: Replace learned cross-component attention with uniform weights and measure CTR drop. If <1%, attention isn't learning meaningful dependencies.
2. **Cold-Start Element Generalization**: Hold out 20% of creative elements entirely from training, then measure whether RL-enabled model outperforms supervised-only baseline on these unseen elements. This tests the exploration/extrapolation claim directly.
3. **MIL Credit Assignment Validation**: Remove MIL loss (keep only Lcomb), then compare element-level prediction quality versus combination-level quality. If element predictions degrade disproportionately, MIL is providing value; if both degrade similarly, MIL isn't effectively disentangling element contributions.