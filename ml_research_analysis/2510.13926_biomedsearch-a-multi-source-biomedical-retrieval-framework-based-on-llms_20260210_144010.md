---
ver: rpa2
title: 'BioMedSearch: A Multi-Source Biomedical Retrieval Framework Based on LLMs'
arxiv_id: '2510.13926'
source_url: https://arxiv.org/abs/2510.13926
tags:
- biomedical
- retrieval
- information
- search
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents BioMedSearch, a multi-source biomedical retrieval
  framework that integrates large language models (LLMs) with literature databases,
  protein knowledge bases, and web search to improve accuracy in biomedical question
  answering. The core idea is to decompose complex biomedical queries into sub-queries,
  extract keywords, construct task graphs, and retrieve information from multiple
  specialized sources.
---

# BioMedSearch: A Multi-Source Biomedical Retrieval Framework Based on LLMs

## Quick Facts
- arXiv ID: 2510.13926
- Source URL: https://arxiv.org/abs/2510.13926
- Reference count: 29
- Multi-source biomedical retrieval framework achieving 73.4% accuracy on complex temporal reasoning tasks

## Executive Summary
BioMedSearch addresses LLM hallucinations in biomedical question answering by integrating literature databases, protein knowledge bases, and web search with a structured retrieval pipeline. The framework decomposes complex queries into sub-queries, constructs task graphs, and retrieves information from multiple specialized sources to ground responses in authoritative evidence. Evaluation on a newly created dataset shows consistent accuracy improvements across three reasoning levels: 91.9% (Level 1), 81.0% (Level 2), and 73.4% (Level 3).

## Method Summary
BioMedSearch is an inference-only RAG framework that decomposes biomedical queries using an LLM planner, extracts keywords, constructs a directed acyclic graph (DAG) for retrieval routing, and queries three parallel modules: literature (PubMed/PMC/ScienceDirect), protein databases (UniProt/AlphaFold), and web search engines. Retrieved results undergo dual-stage filtering with 80% keyword coverage thresholds and PubMedBert semantic similarity ranking. The framework generates structured reports from sub-answers and constrains LLM response generation to retrieved context only, eliminating parametric knowledge reliance.

## Key Results
- Accuracy improved from 59.1% to 91.9% at Level 1 mechanistic identification
- Accuracy increased from 47.0% to 81.0% at Level 2 non-adjacent semantic integration
- Accuracy rose from 36.3% to 73.4% at Level 3 temporal causal reasoning
- Consistent improvements across multiple LLM architectures (ChatGPT-4.1, DeepSeek-R1, Qwen2.5)

## Why This Works (Mechanism)

### Mechanism 1: Query Decomposition and DAG Construction
Decomposing complex biomedical queries into structured sub-queries and keywords enables targeted retrieval across specialized sources. The LLM-based planner partitions user queries along biomedical dimensions, extracts fine-grained keywords, and organizes them into a DAG that prescribes retrieval paths and tool assignments. This structure routes each sub-query to the most appropriate source type with focused search terms. Break condition: If decomposition produces ambiguous or overly narrow sub-queries, the DAG misroutes retrieval and degrades relevance.

### Mechanism 2: Multi-Source Filtering with Evidence Grounding
The executor queries literature databases, protein databases, and web engines, then applies rigorous dual-stage filtering. Literature results undergo keyword coverage thresholds (≥80%) followed by PubMedBert-based semantic similarity ranking. Protein queries use UniProt ID resolution and AlphaFold for structural data. Web results are filtered by LLM-assessed relevance. Break condition: If filtering thresholds are too permissive, noisy web content enters synthesis; if too strict, relevant evidence is excluded.

### Mechanism 3: Context-Constrained Answer Generation
Retrieved evidence is synthesized into structured reports, and LLMs generate answers strictly from provided context without external access. For QA, top-k paragraphs are selected via PubMedBert similarity to the question. This forces reasoning over vetted evidence rather than parametric knowledge. Break condition: If selected paragraphs omit critical evidence chains, or if the model cannot infer implicit relationships across sub-answers, multi-hop reasoning fails.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: BioMedSearch is fundamentally a RAG architecture designed to mitigate LLM hallucinations in biomedical QA by grounding responses in retrieved evidence.
  - Quick check question: Can you explain how RAG differs from fine-tuning for domain adaptation?

- Concept: Task Decomposition and Query Planning
  - Why needed here: The planner decomposes queries into sub-queries and constructs a DAG, a core strategy for handling multi-hop biomedical questions.
  - Quick check question: What is the advantage of organizing sub-queries as a DAG rather than a linear list?

- Concept: Semantic Similarity and Dense Retrieval
  - Why needed here: PubMedBert embeddings and cosine similarity drive both literature filtering and paragraph selection, making dense retrieval central to the framework.
  - Quick check question: Why might a biomedical-domain embedding model like PubMedBert outperform a general-purpose model for this task?

## Architecture Onboarding

- Component map:
  - Biomedical Search Planner: Sub-query decomposition, keyword extraction, DAG construction
  - Biomedical Retrieval Executor: Literature Retrieval, Protein Information Retrieval, Web Search
  - Summary Report Generator: Sub-answer extraction, interlinking, structured report synthesis
  - QA Engine: Paragraph selection via PubMedBert, context-constrained LLM answering

- Critical path:
  1. Query → Planner: Decompose into sub-queries → Extract keywords → Build DAG
  2. DAG → Executor: Route each sub-query to appropriate source(s) → Retrieve and filter
  3. Filtered results → Report Generator: Extract sub-answers → Synthesize structured report
  4. Report + Question → QA Engine: Select top-k paragraphs → Generate constrained answer

- Design tradeoffs:
  - Precision vs. Coverage: Tight filtering improves precision but may miss evidence for niche topics
  - Latency vs. Comprehensiveness: Querying multiple databases increases latency but broadens evidence coverage
  - Domain Specificity vs. Generalization: PubMedBert optimization for biomedical accuracy limits general-domain applicability

- Failure signatures:
  - Empty or low-quality retrieval: Sub-queries too specific or keywords misextracted
  - Irrelevant web content: LLM filter misranks; may indicate threshold τ needs adjustment
  - Protein name disambiguation errors: UniProt ID lookup fails or returns wrong entry
  - Multi-hop reasoning gaps: Level 3 accuracy drops; indicates missing cross-paragraph linkages

- First 3 experiments:
  1. Ablation by source: Run with only literature, only web, only protein databases on BioMedMCQs
  2. Threshold sweep: Vary keyword coverage threshold and Top-k values; plot accuracy vs. precision/recall
  3. Decomposition quality analysis: Manually evaluate DAGs for query-sub-query alignment; correlate with QA performance

## Open Questions the Paper Calls Out

- Question: To what extent does the integration of domain-specific knowledge graphs and multimodal data (e.g., biomedical images and pathways) improve the framework's performance on complex reasoning tasks?
  - Basis: The conclusion states this as a future direction for improving performance on complex reasoning tasks.
  - Why unresolved: Current framework focuses exclusively on text-based retrieval without processing visual or graph data.
  - Evidence needed: Benchmark results on multimodal biomedical QA dataset showing improvements with added modalities.

- Question: Can task graph structures and retrieval paths be refined through structured knowledge integration to reduce reliance on LLM-driven decomposition heuristics?
  - Basis: The conclusion identifies refining task graph structures as a future direction.
  - Why unresolved: Current method relies entirely on LLM to decompose queries and construct DAGs, which may be suboptimal compared to ontological rule-guided structures.
  - Evidence needed: Comparative study showing knowledge-graph-augmented planner produces more efficient retrieval paths than current LLM-only planner.

- Question: Does high performance on BioMedMCQs multiple-choice benchmark correlate with factual accuracy and reduced hallucination in generation of free-text research reports?
  - Basis: The paper evaluates using MCQs but ultimate goal is structured research reports; MCQ validity as proxy is not established.
  - Why unresolved: High MCQ accuracy doesn't directly measure ability to synthesize retrieved facts into coherent, hallucination-free narrative.
  - Evidence needed: Human or automated evaluation of final generated reports assessing factual consistency alongside MCQ accuracy.

## Limitations

- Query Decomposition Quality: Framework's effectiveness depends on accurate sub-query generation and keyword extraction, which may fail for complex biomedical queries without systematic evaluation of decomposition accuracy.
- Filtering Threshold Sensitivity: Fixed 80% keyword coverage threshold and Top-k=10 values lack sensitivity analysis and may be overly restrictive for rare topics or permissive for noisy content.
- Source Contribution Analysis: Lack of ablation experiments to quantify individual contributions of literature, web, and protein sources to accuracy improvements.

## Confidence

- High Confidence: Multi-source retrieval architecture and filtering pipeline are technically sound with well-documented 30% accuracy improvement across all reasoning levels.
- Medium Confidence: Query decomposition driving accuracy gains assumes optimal planner performance without validating decomposition quality; framework robustness across LLMs plausible but not empirically distinguished.
- Low Confidence: Protein database integration contribution lacks ablation evidence; web search filtering via LLM relevance scoring implemented but not validated against alternatives.

## Next Checks

1. Decomposition Quality Audit: Manually evaluate 100 randomly sampled DAGs from BioMedMCQs test set, comparing planned sub-queries against gold-standard query decomposition and measuring biomedical entity extraction precision and recall.

2. Source Contribution Analysis: Implement ablation experiments removing each retrieval source individually and in combinations, then run full evaluation on BioMedMCQs to quantify each source's marginal contribution to accuracy at each reasoning level.

3. Threshold Sensitivity Study: Systematically vary keyword coverage threshold (50-95%) and Top-k values (5-20) on a validation subset of BioMedMCQs, plotting accuracy, precision, and recall curves to identify optimal filtering parameters and quantify robustness to threshold changes.