---
ver: rpa2
title: 'InfoSynth: Information-Guided Benchmark Synthesis for LLMs'
arxiv_id: '2601.00575'
source_url: https://arxiv.org/abs/2601.00575
tags:
- problems
- novelty
- diversity
- dataset
- mbpp-new
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfoSynth introduces an information-theoretic framework for generating
  high-quality, novel, and diverse coding benchmarks for LLM evaluation. The method
  uses KL-divergence and entropy to quantify benchmark novelty and diversity, then
  employs genetic algorithms with iterative code feedback to synthesize problems from
  seed datasets.
---

# InfoSynth: Information-Guided Benchmark Synthesis for LLMs

## Quick Facts
- arXiv ID: 2601.00575
- Source URL: https://arxiv.org/abs/2601.00575
- Reference count: 40
- Generates high-quality, novel, and diverse coding benchmarks for LLM evaluation using information-theoretic metrics

## Executive Summary
InfoSynth introduces an information-theoretic framework for generating high-quality coding benchmarks for LLM evaluation. The method uses KL-divergence and entropy to quantify novelty and diversity, then employs genetic algorithms with iterative code feedback to synthesize problems from seed datasets. The pipeline generates robust Python coding problems with 97% accuracy in test case and solution generation. Extensive experiments demonstrate InfoSynth's superiority over existing methods in benchmark generation while providing a scalable, self-verifying pipeline for LLM evaluation.

## Method Summary
InfoSynth uses an information-theoretic framework to synthesize coding benchmarks through three key components: KL-divergence and entropy metrics to quantify novelty and diversity without model evaluations, genetic algorithms (mutation and crossover) with k-farthest-neighbor filtering to expand coverage beyond seed distributions, and iterative code feedback with full history to improve solution correctness. The pipeline generates Python coding problems from seed datasets, achieving 97% accuracy in test case and solution generation while maintaining higher novelty and diversity compared to seed datasets.

## Key Results
- Achieves 97% accuracy in test case and solution generation for Python coding problems
- Generated benchmarks consistently exhibit higher novelty and diversity compared to seed datasets
- Allows control over difficulty and novelty-diversity tradeoffs, with k-farthest filtering increasing diversity but producing easier problems
- Outperforms existing methods in benchmark generation with a scalable, self-verifying pipeline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL-divergence and entropy provide computationally efficient proxies for benchmark novelty and diversity without requiring model evaluations.
- Mechanism: Novelty is estimated as D_KL(q||p) between the embedding distribution of generated problems (q) and seed problems (p) using k-NN distance ratios. Diversity is estimated as differential entropy via Kozachenko-Leonenko estimator. Both operate in a reduced embedding space (projected via UMAP to d=8-12).
- Core assumption: Embedding distances meaningfully capture semantic similarity between coding problems.
- Evidence anchors: Validation on LeetCode subsets shows topic-specific datasets have lower novelty relative to full dataset; related work uses model-based difficulty/novelty metrics.

### Mechanism 2
- Claim: Genetic operations (mutation + crossover) with k-farthest-neighbor filtering systematically expand coverage beyond seed distribution.
- Mechanism: Mutation creates difficulty variants (easy/medium/hard); crossover combines concepts from multiple problems. K-farthest filtering retains the k most dissimilar candidates from each generation, explicitly optimizing for embedding-space distance from the current dataset.
- Core assumption: The generator LLM can produce syntactically valid and conceptually coherent combinations of programming concepts.
- Evidence anchors: Topic coverage analysis shows InfoSynth increases problem counts across most LeetCode topic categories compared to seed MBPP; crossover-of-crossover problems filter out more often.

### Mechanism 3
- Claim: Iterative code feedback with full history induces chain-of-thought reasoning, improving solution correctness.
- Mechanism: Solutions and tests are executed in an isolated Python environment; the LLM receives execution output and modifies code. Passing the full feedback history allows the model to trace error causes across iterations.
- Core assumption: The generator can diagnose execution failures and produce correct fixes given error feedback.
- Evidence anchors: Iterative feedback acts as chain-of-thought reasoning, increasing solution-test pairs by 20% over 5 feedback iterations.

## Foundational Learning

- Concept: **KL-divergence as distributional distance**
  - Why needed here: Core to the novelty metric; understanding what D_KL(q||p) measures (asymmetric, non-negative, zero iff q=p) clarifies why it captures "how different" generated problems are from seeds.
  - Quick check question: If the generated dataset is a subset of the seed dataset, should novelty be high or low? (Answer: Low; the estimator may even go negative.)

- Concept: **k-NN density estimation**
  - Why needed here: Both KL-divergence and entropy estimators rely on k-nearest neighbor distances; understanding the bias-variance tradeoff (k/N ratio) is necessary for hyperparameter selection.
  - Quick check question: Why does larger k reduce variance but increase bias in density estimation? (Answer: Larger k smooths over more points, capturing global structure but missing local density variations.)

- Concept: **Genetic algorithms for data synthesis**
  - Why needed here: The pipeline uses mutation (modifying single problems) and crossover (combining multiple problems) as genetic operators; understanding selection pressure (via k-farthest filtering) explains how novelty is enforced.
  - Quick check question: What happens if crossover rate is too high relative to mutation? (Answer: The dataset may converge to increasingly complex problems combining existing concepts without introducing new ones.)

## Architecture Onboarding

- Component map:
  Seed Dataset → Mutation/Crossover via LLM → Candidates → K-Farthest Filter → Filtered Candidates → Solution + Test Generation → Code Packages → Iterative Code Feedback → Verified Problems → Deduplication → Final Dataset

- Critical path:
  1. Seed sampling for each colony (B_s problems per colony)
  2. LLM inference for mutation/crossover (primary cost driver)
  3. Code execution environment for iterative feedback (3 iterations recommended)
  4. K-farthest filtering requires embedding all current problems

- Design tradeoffs:
  - **Novelty vs. correctness**: Higher novelty correlates with lower pass rates (more novel problems are often out-of-distribution and harder for the model to solve)
  - **K-farthest filtering vs. difficulty**: Filtering increases diversity but produces easier problems (MBPP-Guided has higher pass rates than MBPP-New)
  - **Feedback iterations vs. cost**: >3 iterations yield marginal gains; diminishing returns
  - **Projection dimension**: d=8-12 recommended; lower dimensions lose discriminability, higher dimensions increase estimator variance

- Failure signatures:
  - Negative KL-divergence estimates: Occurs when comparing subset to superset; use relative comparisons only
  - High unparsable rate: Smaller models produce malformed outputs; increase parsing robustness or use stronger generator
  - Low diversity despite k-farthest filtering: Seed may be too narrow; use multiple seed sources
  - Crossover producing incoherent problems: Limit crossover depth; filter more aggressively

- First 3 experiments:
  1. Validate metrics on held-out data: Take a known dataset with topic labels, partition by topic, and verify that KL-divergence correctly ranks topic-specific subsets as less novel than the full dataset.
  2. Ablate k-farthest filtering: Run the pipeline with and without k-farthest selection on the same seed. Compare novelty, diversity, and pass rates.
  3. Characterize feedback iteration saturation: For a fixed problem set, plot pass rate vs. iteration count (0-5). Identify the iteration where marginal gain falls below 5%.

## Open Questions the Paper Calls Out
None

## Limitations
- Embedding space representativeness may not capture true semantic relationships if clustering occurs by surface features rather than conceptual content
- Generator LLM capabilities may be bounded, leading to increasingly convoluted problems without genuine novelty through repeated crossover operations
- Feedback loop assumptions may fail if errors stem from fundamental misunderstandings rather than implementation bugs

## Confidence
- **High Confidence**: Information-theoretic framework for quantifying novelty and diversity using KL-divergence and entropy is mathematically sound
- **Medium Confidence**: Genetic algorithm approach for systematic coverage expansion shows promising results but long-term novelty sustainability requires further validation
- **Medium Confidence**: Scalability claims are supported by experimental evidence, though computational cost warrants further investigation

## Next Checks
1. Conduct human evaluation study where annotators rate problem similarity and compare their judgments against KL-divergence and entropy estimates to validate embedding space
2. Run InfoSynth on the same seed dataset for multiple generations (5-10) and analyze whether novelty plateaus or declines over time
3. Test InfoSynth on non-coding domains (e.g., mathematical reasoning or natural language generation) to evaluate framework generalization beyond Python coding problems