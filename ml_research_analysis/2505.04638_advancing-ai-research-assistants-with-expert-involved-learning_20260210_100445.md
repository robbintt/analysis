---
ver: rpa2
title: Advancing AI Research Assistants with Expert-Involved Learning
arxiv_id: '2505.04638'
source_url: https://arxiv.org/abs/2505.04638
tags:
- human
- figure
- research
- outputs
- lmms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ARIEL evaluates AI performance on biomedical research tasks using
  expert-curated data. It benchmarks large language and multimodal models on summarizing
  long scientific articles and interpreting complex figures.
---

# Advancing AI Research Assistants with Expert-Involved Learning

## Quick Facts
- arXiv ID: 2505.04638
- Source URL: https://arxiv.org/abs/2505.04638
- Authors: Tianyu Liu; Simeng Han; Hanchen Wang; Xiao Luo; Pan Lu; Biqing Zhu; Yuge Wang; Keyi Li; Jiapeng Chen; Rihao Qu; Yufeng Liu; Xinyue Cui; Aviv Yaish; Yuhang Chen; Minsheng Hao; Chuhan Li; Kexing Li; Arman Cohan; Hua Xu; Mark Gerstein; James Zou; Hongyu Zhao
- Reference count: 40
- Primary result: ARIEL benchmarks LLMs and LMMs on biomedical research tasks, showing fine-tuning improves summarization while test-time scaling enhances figure interpretation, though models still lag human experts.

## Executive Summary
ARIEL introduces a comprehensive framework for evaluating AI assistants on complex biomedical research tasks, combining automated metrics with expert human judgment. The benchmark tests large language models on summarizing long scientific articles and multimodal models on interpreting complex figures, revealing current AI capabilities and limitations. Fine-tuning strategies improve text summarization quality, while test-time computation scaling enhances visual reasoning performance. The framework also explores AI-generated hypotheses from multimodal inputs, demonstrating both the potential and current constraints of AI in scientific reasoning.

## Method Summary
ARIEL evaluates AI models on two tasks: summarizing full-length biomedical manuscripts (2,571 papers, 70/30 train/test split) and interpreting scientific figures with 88 expert-designed questions across 10 curated images. The framework uses seven automated metrics for text (BLEU, ROUGE variants, BERTScore, F1-Radgraph, MEDCON) and accuracy-based scoring for figures, supplemented by blinded judgments from 12 PhD-level experts. Optimization strategies include LoRA fine-tuning for text, test-time computation scaling for images, and a verification-correction pipeline to elevate human analyst performance.

## Key Results
- LoRA fine-tuning substantially improves biomedical document summarization beyond prompt engineering alone
- Test-time computation scaling (o1 model) consistently enhances visual reasoning on complex scientific figures
- Human experts assess model outputs alongside automated metrics, revealing gaps in current AI capabilities
- Fine-tuned models outperform all closed-source LLMs on summarization tasks
- o1 achieves highest performance across all figure question categories, underscoring test-time computation utility

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific LoRA fine-tuning substantially improves long biomedical document summarization beyond prompt engineering alone. Lightweight fine-tuning adapts model weights to biomedical terminology and document structure, enabling better coverage of scientific content than prompting strategies that leave weights frozen. Core assumption: The biomedical training corpus sufficiently represents target document distributions. Evidence anchors: Fine-tuning substantially improved outcomes with adapted model outperforming all closed-source LLMs. Break condition: If target documents diverge significantly from COVID-19/healthcare themes dominating training corpus.

### Mechanism 2
Test-time computation scaling improves visual reasoning on complex scientific figures more reliably than standard inference. Models like o1 allocate additional inference-time compute to search through reasoning paths, enabling multi-step visual analysis that single-pass models cannot achieve. Core assumption: Observed gains stem from test-time scaling rather than model architecture differences. Evidence anchors: o1 consistently achieved highest performance across all question categories. Break condition: When figures require domain knowledge not present in model training, additional computation cannot compensate for knowledge gaps.

### Mechanism 3
Iterative verification-correction pipelines allow LMMs to elevate underperforming human analysts toward expert-level accuracy. LMM verifies human-written solutions against visual evidence, identifies specific errors, proposes corrections, and iterates until self-verification passes. Core assumption: LMM verification accurately identifies errors in human reasoning. Evidence anchors: Participants with lower initial analytical performance benefited notably from LMM assistance. Break condition: If LMM confidence calibration remains poor, the pipeline may accept incorrect corrections.

## Foundational Learning

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: Understanding why lightweight fine-tuning outperforms prompt engineering requires grasping how LoRA modifies only a small subset of weights
  - Quick check question: Can you explain why LoRA might improve biomedical summarization more than meta-prompting, given identical training data?

- Concept: **Test-Time Compute Scaling**
  - Why needed here: The o1 model's superior figure interpretation relies on inference-time search strategies rather than larger model size alone
  - Quick check question: How does allocating more compute at inference time differ from increasing model parameters, and what tradeoffs does this create for deployment?

- Concept: **Self-Verification/Correction Loops**
  - Why needed here: The verification-correction pipeline requires understanding when iterative self-checking converges versus when it amplifies errors
  - Quick check question: What failure modes emerge when a model's verification capability is weaker than its generation capability?

## Architecture Onboarding

- Component map:
  Datasets -> Baseline Models -> Optimization Strategies -> Human Evaluation -> Automated Metrics
  (PubMed corpus and scientific figures feed into open/closed-source models)
  (LoRA fine-tuning, test-time scaling, verification-correction applied)
  (12 PhD experts provide blinded judgments)
  (7 automated metrics for text, accuracy scoring for figures)

- Critical path:
  1. Curate task-specific datasets with expert-verified ground truth
  2. Run baseline evaluations on target models (open and closed-source)
  3. Apply appropriate optimization strategy (fine-tuning for text, test-time scaling for images)
  4. Validate improvements against both automated metrics and human expert judgment

- Design tradeoffs:
  Fine-tuning improves text quality but requires domain-specific training data and compute
  Test-time scaling improves accuracy but increases latency and API costs
  Majority voting (o1-5mv) improves reliability but 5× token consumption
  Human evaluation provides ground truth but limits scalability (n=7-8 experts per task)

- Failure signatures:
  Open-source models frequently failed to generate meaningful summaries—attributed to context length and data constraints
  LMMs struggle with color identification and insight discovery (high variance across models)
  Numerical inconsistencies in LLM-generated abstracts
  LMM confidence scores uncorrelated with accuracy for GPT-4V and Claude

- First 3 experiments:
  1. Baseline characterization: Run ChatGLM4 and GPT-4 on 20 held-out manuscripts using all seven metrics
  2. Fine-tuning ablation: Train ChatGLM4 with varying LoRA ranks (r=4, 8, 16, 32) on training split
  3. Verification-correction stress test: Apply pipeline to intentionally degraded human answers to measure correction accuracy by error category

## Open Questions the Paper Calls Out

### Open Question 1
Can Large Multimodal Models (LMMs) be calibrated to provide reliable self-assessed confidence scores for scientific figure interpretation? The authors note that neither GPT-4V nor Claude-Sonnet-3.5 produced meaningful confidence estimates (correlation p > 0.1), raising concerns about reliability. While o1 showed some correlation (p=0.006), it remained inconsistent. What evidence would resolve it: A fine-tuning or alignment method that results in strong, monotonic correlation between model-generated confidence scores and expert-evaluated accuracy.

### Open Question 2
Does a multi-agent LMM framework provide statistically significant advantages over single-model inference for generating verifiable scientific hypotheses? The paper reports no statistically significant differences between HypoGenAgent and single o1 model, leaving completeness-conciseness balance as open trade-off. What evidence would resolve it: External wet-lab validation or larger expert review showing HypoGenAgent produces significantly higher rate of experimentally testable hypotheses.

### Open Question 3
Can retrieval-augmented generation (RAG) or specialized numerical pre-training eliminate factual hallucinations in long-document summarization? Despite fine-tuning improvements, substantial numerical inconsistencies remain between LLM-generated abstracts and original texts. What evidence would resolve it: RAG-enhanced LLM evaluation demonstrating statistically significant reduction in numerical errors while maintaining high textual coverage.

## Limitations
- Biomedical fine-tuning corpus may not fully represent diversity beyond COVID-19 and healthcare topics
- Human expert evaluation limited to 12 PhD-level experts, potentially not scalable for broader application
- Figure interpretation task covers only 10 curated figures, potentially missing edge cases
- Verification-correction pipeline effectiveness depends heavily on LMM's confidence calibration

## Confidence
- **High confidence**: Test-time computation scaling reliably improves figure interpretation accuracy across question categories
- **Medium confidence**: LoRA fine-tuning improves biomedical summarization, though generalization beyond training domain remains uncertain
- **Low confidence**: Verification-correction pipeline consistently elevates human analysts to expert-level performance given inconsistent LMM confidence calibration

## Next Checks
1. Cross-domain generalization test: Evaluate fine-tuned models on non-COVID biomedical domains to assess performance degradation when document distributions diverge from training corpus
2. Confidence calibration audit: Systematically measure LMM confidence-accuracy correlation across diverse biomedical tasks to identify when verification-correction pipeline introduces systematic errors
3. Sample efficiency analysis: Determine minimum number of expert-annotated examples needed to achieve comparable performance gains from fine-tuning, addressing scalability concerns for specialized domains