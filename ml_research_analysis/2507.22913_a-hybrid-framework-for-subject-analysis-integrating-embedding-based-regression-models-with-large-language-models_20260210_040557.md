---
ver: rpa2
title: 'A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression
  Models with Large Language Models'
arxiv_id: '2507.22913'
source_url: https://arxiv.org/abs/2507.22913
tags:
- terms
- subject
- lcsh
- llms
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a hybrid framework integrating embedding-based
  ML models with LLMs to improve subject analysis for library cataloging. It addresses
  LLM limitations in determining optimal label counts and reducing hallucinations
  by using ML models to predict the number of subject headings and post-edit outputs.
---

# A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models

## Quick Facts
- **arXiv ID**: 2507.22913
- **Source URL**: https://arxiv.org/abs/2507.22913
- **Reference count**: 40
- **Primary result**: Hybrid framework improves recall from 43% to 63% and precision from 8% to 26% in library subject analysis

## Executive Summary
This paper presents a hybrid framework that addresses key limitations of large language models in library subject analysis by integrating embedding-based machine learning models. The framework predicts optimal subject heading counts and post-processes LLM outputs to reduce hallucinations, demonstrating significant improvements in both accuracy and standardization of generated subject headings. Tested on the Library of Congress Subject Headings using 78,260 book samples, the approach achieves substantial gains in recall (43% to 63%) and precision (8% to 26%) while maintaining computational efficiency.

## Method Summary
The framework combines embedding-based regression models with LLMs to automate subject analysis for library cataloging. It uses MPNet embeddings to predict optimal label counts, constrains LLM generation based on these predictions, and applies post-processing to map hallucinated terms to valid LCSH vocabulary. The approach includes optional fine-tuning via LoRA and multi-round Chain-of-Thought prompting to improve coverage. The system processes title and abstract fields to generate validated subject headings that comply with controlled vocabulary requirements.

## Key Results
- Recall improved from 43% to 63% using Chain-of-Thought with post-processing
- Precision improved from 8% to 26% when constraining LLM output based on predicted label counts
- Post-processing reduced hallucinations by replacing invalid terms with semantically similar LCSH terms
- Multi-round CoT approach increased recall from 0.43 (zero-shot) to 0.51 (CoT)

## Why This Works (Mechanism)

### Mechanism 1: Prediction-Guided Output Constraint
- **Claim**: Constraining LLM output quantity via pre-computed label counts improves precision without catastrophic recall loss
- **Mechanism**: Embedding models encode title+abstract into vectors; regression model predicts optimal LCSH term count N, which is injected into LLM prompt as explicit generation limit
- **Core assumption**: Optimal label count is predictable from metadata features and serves as reliable proxy for topical complexity
- **Evidence**: Table 5 shows limit N reduces average terms from 14.89 to 3.14, improves precision from 0.08 to 0.21, recall drops moderately from 0.43 to 0.29
- **Break condition**: High variance in label counts for similar metadata causes systematic under-generation

### Mechanism 2: Semantic Vocabulary Mapping (Post-Hoc Hallucination Repair)
- **Claim**: Mapping LLM outputs to controlled vocabulary via embedding similarity recovers recall lost to hallucinated terms
- **Mechanism**: Generated terms not found in LCSH are embedded, reduced via PCA (628→50 dims), then matched to nearest valid LCSH term using FAISS nearest neighbor search
- **Core assumption**: Hallucinated terms are semantically proximate to valid controlled vocabulary terms
- **Evidence**: Table 7 shows post-processing improves recall from 0.51→0.63 (CoT) and 0.43→0.52 (zero-shot)
- **Break condition**: LLM generates terms with no semantic overlap to LCSH, producing spurious matches

### Mechanism 3: Multi-Round Chain-of-Thought for Coverage Expansion
- **Claim**: Iterative prompting with cumulative context increases topical coverage and recall
- **Mechanism**: LLM prompted in 3 rounds, each receiving prior outputs and instructed to generate additional distinct terms covering unexplored aspects
- **Core assumption**: LLMs can reason about "gaps" in their own prior outputs when explicitly prompted
- **Evidence**: CoT-generated LCSH labels cover more aspects, recall increased from 0.43 (zero-shot) to 0.51 (CoT)
- **Break condition**: Too many rounds or insufficiently specific instructions produce redundant or off-topic terms

## Foundational Learning

- **Concept**: Multi-label classification with extreme label spaces
  - **Why needed**: Subject analysis is fundamentally MLC with ~318,500 unique LCSH labels; standard classification assumptions break down
  - **Quick check**: Can you explain why softmax over all labels is computationally infeasible here?

- **Concept**: Embedding-based semantic similarity (dense retrieval basics)
  - **Why needed**: Both count prediction and vocabulary mapping rely on vector representations of text
  - **Quick check**: What does PCA do to embedding vectors, and why might dimensionality reduction hurt recall?

- **Concept**: Controlled vocabulary and knowledge organization
  - **Why needed**: Entire framework designed to enforce LCSH compliance; understanding why "valid" terms matter is non-negotiable
  - **Quick check**: Why would a semantically correct term still be "invalid" in LCSH?

## Architecture Onboarding

- **Component map**: Input Layer (Title + Abstract) → Embedding Layer (MPNet/BERT/SciBERT) → Count Predictor (Regression) → LLM Generator (Llama-3.1-8B) → Post-Processor (FAISS nearest-neighbor search) → Output (Validated LCSH terms)

- **Critical path**: 1) Embed title+abstract → predict N 2) Construct prompt with N constraint → LLM generates N candidate terms 3) For each term not in LCSH vocabulary → retrieve nearest valid LCSH term 4) Return mapped term list

- **Design tradeoffs**: N vs. 2N vs. 3N constraint (higher N → higher recall, lower precision); LoRA vs. full SFT (LoRA 4x faster, 350x smaller storage); CoT rounds (more rounds increase recall but also noise)

- **Failure signatures**: Over-generation (avg terms >10, precision <0.15); Under-generation (avg terms <2, recall <0.20); Persistent hallucinations (post-processing shows <5% recall improvement)

- **First 3 experiments**: 1) Baseline zero-shot: Run LLM with standard prompt, no constraint, no post-processing; measure recall/precision/avg terms 2) Count constraint ablation: Apply predicted N as hard limit; compare N vs. 2N vs. 3N 3) Post-processing validation: Run vocabulary mapping on zero-shot outputs; measure recall gain and inspect failure cases

## Open Questions the Paper Calls Out

- **Question**: To what extent do improvements in automated metrics (recall/precision) align with human cataloger judgments regarding semantic relevance of generated subject headings?
- **Basis**: Authors state "Future work will incorporate human evaluation... generated LCSH terms may not be wrong just because they differ from those assigned by human catalogers"
- **Why unresolved**: Current study relies entirely on automatic string matching, cannot assess subjective quality or appropriateness
- **What evidence would resolve**: User study with professional catalogers evaluating relevance and accuracy against book content

- **Question**: How does Retrieval-Augmented Generation (RAG) compare to proposed post-processing vocabulary mapping in reducing hallucinations and ensuring valid LCSH output?
- **Basis**: Authors list "retrieval-augmented generation (RAG), which could use LCSH as external knowledge base," as method "not explored in this study"
- **Why unresolved**: Current framework corrects hallucinations after generation; unknown if grounding LLM with external knowledge before generation would yield higher accuracy
- **What evidence would resolve**: Comparative experiments evaluating hallucination rates and precision of RAG-based model versus proposed post-processing baseline

- **Question**: Can proposed hybrid framework's regression-based constraints and fine-tuning techniques improve performance of larger, proprietary models (e.g., GPT-4, DeepSeek-V3)?
- **Basis**: Authors note while models like DeepSeek-R1 achieved highest baseline recall, "we were unable to perform experiments applying our optimizations to these models due to constraints in feasibility of deploying them"
- **Why unresolved**: Unclear if improvements observed in Llama-3.1-8b transfer to models with different parameter scales and reasoning capabilities
- **What evidence would resolve**: Experimental results applying optimal number prediction constraints and fine-tuning to high-performing proprietary models

## Limitations

- Evaluation relies on single library dataset (UNT Library Catalog), raising questions about performance across different cataloging domains and languages
- Controlled vocabulary mapping assumes semantic proximity between hallucinated and valid terms, which may not hold for highly specialized or novel terminology
- Linear regression model for predicting label counts may struggle with complex documents where topical density doesn't correlate cleanly with metadata features

## Confidence

**High Confidence**: Core mechanism of using predicted label counts to constrain LLM output is well-validated through ablation studies showing consistent precision improvements (8% → 26%)

**Medium Confidence**: Chain-of-Thought approach shows recall improvements, but multi-round design lacks extensive ablation testing to determine optimal round counts

**Low Confidence**: Framework's performance on non-English cataloging data remains untested; PCA dimensionality reduction (628→50) preserving sufficient semantic information for accurate nearest-neighbor mapping hasn't been rigorously validated across different embedding models

## Next Checks

1. **Cross-Domain Validation**: Test framework on MARC records from different library systems (academic, public, specialized collections) to assess generalizability beyond UNT dataset

2. **Embedding Space Sensitivity**: Compare recall/precision using different embedding models (BERT, SciBERT, MPNet) and PCA dimensions (25, 50, 100) to quantify impact of semantic representation choices

3. **Constraint Parameter Optimization**: Systematically vary N constraint factor (N, 2N, 3N) across diverse document types to establish optimal trade-offs between precision and recall for different cataloging contexts