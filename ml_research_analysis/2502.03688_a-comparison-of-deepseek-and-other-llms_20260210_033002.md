---
ver: rpa2
title: A Comparison of DeepSeek and Other LLMs
arxiv_id: '2502.03688'
source_url: https://arxiv.org/abs/2502.03688
tags:
- classification
- llms
- error
- citation
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares DeepSeek-R1 with four other large language
  models (LLMs) on two classification tasks: authorship detection (human vs AI-generated
  text) and citation classification (academic citation types). Using both MADStat
  and a newly created CitaStat dataset, the study finds that Claude-3.5-sonnet consistently
  outperforms other models in classification accuracy, while DeepSeek-R1 shows competitive
  performance, outperforming Gemini, GPT, and Llama in most cases but falling short
  of Claude.'
---

# A Comparison of DeepSeek and Other LLMs

## Quick Facts
- arXiv ID: 2502.03688
- Source URL: https://arxiv.org/abs/2502.03688
- Authors: Tianchen Gao; Jiashun Jin; Zheng Tracy Ke; Gabriel Moryoussef
- Reference count: 7
- Primary result: Claude-3.5-sonnet outperforms other LLMs in classification tasks, with DeepSeek-R1 showing competitive but not leading performance

## Executive Summary
This paper presents a systematic comparison of DeepSeek-R1 against four other large language models (LLMs) - Claude-3.5-sonnet, GPT-4o, Llama-3.1-70B, and Gemini-1.5-pro - on two classification tasks. Using both MADStat and a newly created CitaStat dataset, the study evaluates model performance across authorship detection (human vs AI-generated text) and citation classification (academic citation types). The research finds Claude-3.5-sonnet to be the strongest performer overall, while DeepSeek-R1 demonstrates competitive capabilities, particularly in cost-effectiveness. The authors also introduce a hybrid approach combining traditional statistical methods (Higher Criticism) with LLMs, which significantly improves classification accuracy.

## Method Summary
The study evaluates five LLMs on two distinct classification tasks using carefully constructed datasets. For authorship detection, the MADStat dataset contains 1,200 text segments (600 human-written, 600 AI-generated) drawn from Nature journals, arXiv papers, and Stack Overflow posts. The CitaStat dataset comprises 1,600 citation sentences (400 each of background, uses, comparison, and future work categories) from AI/ML research papers. Each model processes 10-sentence segments with temperature set to 0.7, generating classification predictions that are compared against ground truth labels. The evaluation framework measures accuracy, F1 score, cost per task, and processing time. Additionally, the study tests a hybrid approach combining Higher Criticism statistical testing with LLM predictions to assess whether traditional statistical methods can enhance LLM performance.

## Key Results
- Claude-3.5-sonnet consistently achieves the highest classification accuracy across both datasets and tasks
- DeepSeek-R1 shows competitive performance, outperforming Gemini, GPT, and Llama in most cases but falling below Claude
- The HC+LLM hybrid approach significantly improves classification accuracy, with Claude-HC achieving the best overall results
- DeepSeek-R1 offers the lowest cost per task but requires more processing time compared to competitors

## Why This Works (Mechanism)
The superior performance of Claude-3.5-sonnet likely stems from its advanced training on diverse text corpora and fine-tuning for classification tasks. The effectiveness of the Higher Criticism + LLM approach demonstrates how traditional statistical methods can complement LLM strengths by providing additional signal detection capabilities. DeepSeek-R1's competitive performance despite being newer to the market suggests efficient training methodologies and optimization strategies. The cost-performance tradeoff observed across models reflects different architectural choices and training approaches - Claude prioritizes accuracy while DeepSeek emphasizes cost-efficiency.

## Foundational Learning

**Statistical Classification** - Understanding how models categorize data into discrete classes
*Why needed*: Essential for interpreting classification accuracy metrics and F1 scores
*Quick check*: Can distinguish between precision, recall, and F1 score calculations

**Higher Criticism Test** - A statistical method for detecting sparse signals in large datasets
*Why needed*: Critical for understanding the hybrid HC+LLM approach that enhances classification performance
*Quick check*: Can explain how HC identifies anomalous patterns in p-value distributions

**Language Model Architecture** - Understanding transformer-based model structures and training methodologies
*Why needed*: Provides context for interpreting performance differences between models
*Quick check*: Can describe the difference between decoder-only and encoder-decoder architectures

## Architecture Onboarding

**Component Map**: Text Input -> Tokenization -> Embedding Layer -> Transformer Blocks -> Classification Head -> Output Prediction

**Critical Path**: Input text → Tokenization → Embedding → Multi-head Attention → Feed-Forward Networks → Classification → Accuracy Score

**Design Tradeoffs**: Accuracy vs. cost (Claude prioritizes accuracy, DeepSeek emphasizes cost-efficiency), speed vs. precision (higher accuracy requires more processing time)

**Failure Signatures**: Inconsistent classifications across similar inputs, inability to distinguish subtle textual differences, performance degradation on domain-specific terminology

**First Experiments**:
1. Test classification accuracy on a small subset of both datasets with temperature variation (0.1, 0.5, 0.7, 0.9)
2. Compare processing time for 10-sentence vs. 5-sentence segments to identify optimal input length
3. Evaluate cost-performance ratio by running identical tasks across all five models

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on two specialized datasets that may not generalize to other classification tasks or domains
- Comparison framework focuses specifically on classification accuracy without examining other important LLM capabilities
- Performance metrics combine speed and cost considerations but don't account for real-world deployment factors such as API availability, rate limits, or integration complexity

## Confidence
**High confidence**: Claude-3.5-sonnet's superior classification performance on both datasets, DeepSeek-R1's competitive but not leading performance, the effectiveness of combining HC with LLMs
**Medium confidence**: Cost and speed comparisons between models, as these metrics can vary significantly based on implementation and usage patterns
**Low confidence**: Generalizability of results to non-classification tasks or different domains not represented in the two test datasets

## Next Checks
1. Test the same models and datasets with different temperature settings (0.1, 0.5, 0.7, 0.9) to assess robustness of classification performance
2. Evaluate model performance on a third, independently constructed dataset covering a different domain to test generalizability
3. Conduct a head-to-head comparison of the same models on multi-step reasoning tasks to determine if classification superiority translates to other cognitive capabilities