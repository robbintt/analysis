---
ver: rpa2
title: 'AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human
  Feedback'
arxiv_id: '2501.13333'
source_url: https://arxiv.org/abs/2501.13333
tags:
- agent
- sentence
- embeddings
- language
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selecting the most appropriate
  LLM agent from a pool of specialized agents for a given task described in natural
  language. The authors propose AgentRec, a method that extends the Sentence-BERT
  (SBERT) encoder model to recommend agents based on sentence embeddings aligned with
  human feedback.
---

# AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human Feedback

## Quick Facts
- **arXiv ID:** 2501.13333
- **Source URL:** https://arxiv.org/abs/2501.13333
- **Reference count:** 31
- **Primary result:** 92.2% top-1 accuracy for recommending one of 8 LLM agents from natural language prompts in <300ms

## Executive Summary
This paper introduces AgentRec, a method for selecting the most appropriate LLM agent from a pool of specialized agents for a given task described in natural language. The system extends the Sentence-BERT (SBERT) encoder model to recommend agents based on sentence embeddings aligned with human feedback. By encoding natural language prompts into sentence embeddings and minimizing the distance between embeddings of the same agent, the model can classify prompts based on their nearest neighbors using cosine similarity. The system is trained using a synthetic dataset generated from Llama-3.1-8B-Instruct and achieves a top-1 accuracy of 92.2% on test data, with each classification taking less than 300 milliseconds.

## Method Summary
AgentRec fine-tunes an SBERT encoder using BatchAllTripletLoss on a synthetic dataset of 10,000 single-sentence prompts (1,250 per agent) generated by Llama-3.1-8B-Instruct. The fine-tuning objective is to minimize intra-class embedding distance, creating separable clusters in embedding space. At inference, user prompts are standardized via a Rephrase and Respond (RaR) technique, encoded into 768-dimensional embeddings, and compared against cached corpora of agent-specific prompts using cosine similarity. A logarithmic generalized p-means score function with p=200 aggregates these similarities per agent to produce final recommendations. The system is further aligned to human preferences through reinforcement learning from human feedback (RLHF).

## Key Results
- **92.2% top-1 accuracy** on a held-out test set of 2,000 synthetic prompts
- **<300ms inference time** per classification (including RaR preprocessing)
- **Edge-case improvement:** RLHF corrects misclassifications such as routing "how do I eat well?" to the health agent instead of fitness

## Why This Works (Mechanism)

### Mechanism 1: Semantic Clustering via Triplet Loss Fine-Tuning
- **Claim:** Minimizing intra-class embedding distance creates separable agent clusters in embedding space.
- **Mechanism:** The SBERT encoder is fine-tuned using `BatchAllTripletLoss` to generate (anchor, positive, negative) triplets from single-sentence prompts. This forces embeddings of prompts belonging to the same agent to cluster together while pushing different agents' embeddings apart. Classification then reduces to nearest-neighbor search via cosine similarity.
- **Core assumption:** Agent-appropriate prompts share semantic structure that can be captured in 768-dimensional embedding space and remains separable across domains.
- **Evidence anchors:**
  - [section 2]: "The base encoder fine-tuning dataset was re-organized... through the sentence_transformers Python library's BatchAllTripletLoss function."
  - [section 3]: "The machine learning objective is to produce sentence embeddings where embeddings for a given agent generate a clean, separable cluster."
  - [corpus]: Weak direct evidence; neighbor papers discuss embedding semantics but not triplet-based agent clustering specifically.
- **Break condition:** If prompt semantic structure does not correlate with agent expertise (e.g., ambiguous or multi-domain prompts), clustering degrades. Figure 2 shows noisy overlap without fine-tuning.

### Mechanism 2: Generalized P-Means Score Function Amplifies High-Confidence Matches
- **Claim:** Using generalized p-means (p=200) as the aggregation function over cosine similarities significantly outperforms naive max or arithmetic mean.
- **Mechanism:** Rather than using the single highest cosine similarity (32.55% accuracy) or arithmetic mean (90.05%), the p-means with high p amplifies extreme similarity values while suppressing moderate ones. The logarithmic formulation (`Sq = ln[(1/n) Σ cos(θ)^p]^(1/p)`) numerically accentuates embeddings the model is confident about (±1 range extremes).
- **Core assumption:** Correct agent recommendations are associated with at least one high-confidence similarity match in the corpus, not distributed consensus.
- **Evidence anchors:**
  - [section 3.1]: "With a value of p=200—which accentuates the extreme cosine similarity scores to a high degree—we were able to produce a top-1 accuracy rating of 92.2%."
  - [section 3.1]: "Accurate agent recommendation relies heavily on cosine similarity scores which the recommendation system can confidently assess as similar (+1) or dissimilar (-1)."
  - [corpus]: No direct corroboration; p-means for retrieval scoring is not discussed in neighbor papers.
- **Break condition:** If the correct agent's corpus lacks any high-similarity embedding for a given prompt (e.g., out-of-distribution queries), the score function cannot recover via consensus.

### Mechanism 3: RLHF Aligns Embedding Space to Human Preference
- **Claim:** Supervised fine-tuning alone produces high accuracy but fails on nuanced edge cases; RLHF corrects this.
- **Mechanism:** After initial SFT (which achieved 93.55% but misclassified "how do I eat well?" as fitness instead of health), a reward model trained on human feedback adjusts the policy to align embeddings with human expectations about domain boundaries.
- **Core assumption:** Human annotators consistently agree on which agent should handle edge-case prompts, and this signal is learnable.
- **Evidence anchors:**
  - [section 3.2]: "even with traditional fine-tuning, it is irresponsible to claim that a model trained naively on the dataset... is aligned to human values."
  - [section 3.3]: "Using a learning rate of 1×10⁻⁴, a fine-tuned SBERT encoder had a top-1 test accuracy of 93.55%. However, it was unable to accurately recommend cases such as 'how do I eat well?' appropriately to the health agent."
  - [corpus]: Neighbor paper "MGFRec" discusses reinforcement-based recommendation with feedback, but not in embedding alignment context.
- **Break condition:** If reward model data is small or biased (N=2000 split), alignment may overfit to annotated edge cases without generalizing.

## Foundational Learning

- **Concept: Sentence-BERT (SBERT) Siamese Networks**
  - **Why needed here:** AgentRec extends SBERT; understanding how siamese networks produce comparable embeddings is prerequisite to grasping the triplet loss and cosine similarity pipeline.
  - **Quick check question:** Can you explain why SBERT uses a siamese architecture rather than a standard classifier head for sentence similarity?

- **Concept: Triplet Loss and Metric Learning**
  - **Why needed here:** The core training mechanism uses `BatchAllTripletLoss` to shape the embedding space. Without this, the clustering mechanism is opaque.
  - **Quick check question:** In a triplet (anchor, positive, negative), what is the optimization objective, and what happens if the negative is too similar to the anchor?

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - **Why needed here:** The paper claims RLHF is necessary for alignment beyond raw accuracy. Understanding reward modeling and policy optimization clarifies why SFT alone failed on edge cases.
  - **Quick check question:** What is the role of the reward model in RLHF, and how does it differ from the supervised loss?

## Architecture Onboarding

- **Component map:**
  RaR rephrasing -> SBERT encoding -> cosine similarity vs. all corpora -> p-means aggregation -> top-K selection

- **Critical path:**
  1. User prompt -> RaR rephrasing -> SBERT encoding -> cosine similarity vs. all corpora -> p-means aggregation -> top-K selection.
  2. Training path: Synthetic data generation -> triplet sampling -> SFT -> reward model training -> RLHF policy update.

- **Design tradeoffs:**
  - **Single-sentence limitation:** SBERT is designed for single sentences; long prompts lose information unless RaR succeeds. *Mitigation:* RaR module, but adds ~250ms latency.
  - **Cached corpora vs. dynamic updates:** Caching enables 50ms inference but requires re-computation if encoder weights change or new agents are added.
  - **High p-value in p-means:** Maximizes accuracy but is sensitive to outliers; may fail if no high-confidence match exists.
  - **Synthetic data dependency:** Real-world distribution may differ; deduplication may under-represent common edge cases.

- **Failure signatures:**
  - **Misclassified health/fitness overlap:** Prompts like "how do I eat well?" incorrectly routed to fitness agent (pre-RLHF).
  - **Context-sensitive word disambiguation failure:** Pre-fine-tuning, "calculus in teeth" routed to math agent instead of health.
  - **Out-of-domain prompts:** No mechanism for "none of the above"; system always returns top-K even if all scores are low.

- **First 3 experiments:**
  1. **Baseline comparison:** Run `all-mpnet-base-v2` without fine-tuning on the test split; measure top-1 accuracy and visualize embeddings (expect ~32% based on max-cosine baseline).
  2. **Ablate p-means:** Replace p=200 with arithmetic mean (p=1) and geometric mean; compare top-1 accuracy to isolate scoring contribution.
  3. **RLHF vs. SFT-only edge-case probe:** Create a held-out set of ambiguous prompts (health/fitness, therapy/health); compare accuracy with and without RLHF alignment to quantify edge-case improvement.

## Open Questions the Paper Calls Out
- How does the classification accuracy of AgentRec generalize to real-world, multi-sentence user prompts that differ statistically from the synthetic single-sentence dataset?
- How does the retrieval latency and classification accuracy scale as the number of available agents grows from the tested 8 to hundreds or thousands?
- Is the reliance on the "Rephrase and Respond" (RaR) technique for variable-length prompts a computational bottleneck for the proposed architecture?

## Limitations
- RLHF methodology lacks full detail (reward model architecture, human feedback sources, optimization hyperparameters)
- Synthetic data may not capture real-world prompt distributions, limiting generalization
- High p-value in p-means lacks theoretical justification and sensitivity analysis

## Confidence
- **High:** The core mechanism of using SBERT with triplet loss for semantic clustering (Mechanism 1) is well-supported by the architecture and results. The computational efficiency claims (300ms inference) are directly measurable.
- **Medium:** The effectiveness of the generalized p-means score function (Mechanism 2) is demonstrated empirically but lacks comparison to alternative scoring methods beyond the two baselines mentioned. The RLHF contribution to edge-case handling (Mechanism 3) is described but not fully validated with ablation studies.
- **Low:** The generalizability of the approach to agents beyond the 8 tested domains and to non-synthetic, real-world prompt distributions is not established.

## Next Checks
1. **Ablation study on RLHF:** Remove the RLHF step and test on the held-out edge-case prompt set (health/fitness, therapy/health overlaps) to quantify its specific contribution to accuracy.
2. **Synthetic vs. real data evaluation:** Generate a small set of real-world prompts for the 8 agents and compare performance against the synthetic test set to assess distribution mismatch.
3. **Sensitivity analysis of p-means parameter:** Systematically vary p from 1 to 500 and plot top-1 accuracy to identify the optimal range and understand the scoring function's robustness.