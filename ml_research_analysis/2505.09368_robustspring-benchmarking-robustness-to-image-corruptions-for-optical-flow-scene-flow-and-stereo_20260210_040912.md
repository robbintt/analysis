---
ver: rpa2
title: 'RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow,
  Scene Flow and Stereo'
arxiv_id: '2505.09368'
source_url: https://arxiv.org/abs/2505.09368
tags:
- robustness
- flow
- corruptions
- optical
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "RobustSpring introduces a comprehensive benchmark for evaluating\
  \ robustness to image corruptions across optical flow, scene flow, and stereo vision\
  \ tasks. The dataset applies 20 diverse corruptions\u2014including noise, blur,\
  \ weather, and compression artifacts\u2014in time-, stereo-, and depth-consistent\
  \ ways to the high-resolution Spring dataset, creating 20,000 challenging images."
---

# RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo

## Quick Facts
- **arXiv ID**: 2505.09368
- **Source URL**: https://arxiv.org/abs/2505.09368
- **Reference count**: 40
- **Primary result**: Introduces a comprehensive benchmark for evaluating robustness to image corruptions across optical flow, scene flow, and stereo vision tasks

## Executive Summary
RobustSpring introduces a comprehensive benchmark for evaluating robustness to image corruptions across optical flow, scene flow, and stereo vision tasks. The dataset applies 20 diverse corruptions—including noise, blur, weather, and compression artifacts—in time-, stereo-, and depth-consistent ways to the high-resolution Spring dataset, creating 20,000 challenging images. A novel corruption robustness metric based on Lipschitz continuity disentangles accuracy and robustness, enabling systematic comparison. Initial evaluations of 16 models reveal significant performance drops under corruptions, with weather and noise being most detrimental. Transformer-based architectures generally show better robustness, though no model excels across all corruption types. The benchmark highlights that accurate models are not necessarily robust, underscoring the importance of dedicated robustness evaluation. RobustSpring is available at spring-benchmark.org and serves as a critical tool for advancing robust dense matching models in real-world conditions.

## Method Summary
RobustSpring applies 20 diverse corruptions (noise, blur, weather, compression) to the Spring dataset in time-, stereo-, and depth-consistent ways. The benchmark introduces a novel corruption robustness metric based on Lipschitz continuity that disentangles accuracy from robustness. The evaluation includes 16 models across optical flow, scene flow, and stereo tasks, with results showing significant performance degradation under corruptions. Weather and noise corruptions are identified as most detrimental. Transformer-based architectures generally show better robustness than CNN-based models, though no single model excels across all corruption types.

## Key Results
- Transformer-based architectures generally show better robustness to corruptions than CNN-based models
- Weather and noise corruptions are most detrimental to model performance
- Accurate models are not necessarily robust—a clear trade-off exists between accuracy and robustness
- The Lipschitz-based robustness metric effectively disentangles accuracy from robustness
- Significant performance drops observed across all 16 evaluated models under various corruption types

## Why This Works (Mechanism)
The benchmark works by systematically introducing controlled corruptions that mimic real-world degradation while maintaining temporal, stereo, and depth consistency. The Lipschitz-based robustness metric captures the sensitivity of predictions to input perturbations, providing a principled way to evaluate robustness independently of accuracy. By using the high-quality Spring dataset with ground truth, the benchmark enables precise measurement of both accuracy and robustness across multiple dense matching tasks simultaneously.

## Foundational Learning

**Optical Flow**: Motion estimation between consecutive frames; needed for video analysis and autonomous driving; quick check: can estimate pixel-level motion vectors.

**Scene Flow**: 3D motion estimation in a scene; extends optical flow to 3D space; needed for 3D understanding and robotics; quick check: can estimate 3D point trajectories.

**Stereo Vision**: Depth estimation from two-view geometry; needed for 3D reconstruction; quick check: can triangulate 3D positions from disparity maps.

**Lipschitz Continuity**: Measures how sensitive a function is to input perturbations; needed for robustness quantification; quick check: small input changes should not cause large output changes.

**Image Corruptions**: Systematic degradation of image quality; needed to simulate real-world conditions; quick check: includes noise, blur, weather effects, and compression artifacts.

## Architecture Onboarding

**Component Map**: Spring dataset -> Corruption application -> Model inference -> Accuracy metrics + Robustness metrics

**Critical Path**: High-quality ground truth → Controlled corruption injection → Dense matching model → Performance evaluation → Robustness quantification

**Design Tradeoffs**: High-resolution data vs. computational cost; synthetic vs. real corruptions; comprehensive vs. focused evaluation; accuracy vs. robustness emphasis.

**Failure Signatures**: Performance drops under weather corruptions (fog, snow); sensitivity to high-frequency noise; degradation with compression artifacts; inconsistency across temporal/stereo views.

**First Experiments**:
1. Evaluate a CNN-based optical flow model (e.g., RAFT) on clean vs. corrupted data to establish baseline performance
2. Compare a transformer-based model (e.g., MaskFlownet) against CNN baselines under weather corruptions
3. Test stereo models on compressed images to measure sensitivity to JPEG artifacts

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to 16 models, potentially missing newer architectures
- Focus on synthetic corruptions may not fully represent real-world degradation patterns
- Quantitative metrics may not capture perceptual aspects of robustness
- Reliance on Spring dataset may limit generalizability to other domains

## Confidence

| Claim | Confidence |
|-------|------------|
| Systematic corruption application is reproducible | High |
| Performance trends (transformers > CNNs) are consistent | Medium |
| Accurate models are not necessarily robust | Medium |
| Lipschitz metric effectively disentangles accuracy/robustness | Medium |

## Next Checks
1. Evaluate top-performing models on additional real-world datasets to assess generalization beyond Spring
2. Conduct user studies to compare perceptual quality of results under corruptions
3. Test robustness against broader range of adversarial attacks to identify additional vulnerabilities