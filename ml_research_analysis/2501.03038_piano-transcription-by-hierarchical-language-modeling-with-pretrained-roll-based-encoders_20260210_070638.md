---
ver: rpa2
title: Piano Transcription by Hierarchical Language Modeling with Pretrained Roll-based
  Encoders
arxiv_id: '2501.03038'
source_url: https://arxiv.org/abs/2501.03038
tags:
- sequence
- language
- music
- note
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid method combining pre-trained roll-based
  encoders with an LM decoder for automatic music transcription. The proposed approach
  employs a hierarchical prediction strategy, first predicting onset and pitch, then
  velocity, and finally offset, to reduce computational costs by breaking down long
  sequences.
---

# Piano Transcription by Hierarchical Language Modeling with Pretrained Roll-based Encoders

## Quick Facts
- arXiv ID: 2501.03038
- Source URL: https://arxiv.org/abs/2501.03038
- Authors: Dichucheng Li; Yongyi Zang; Qiuqiang Kong
- Reference count: 31
- Primary result: Hybrid method combining pre-trained roll-based encoders with LM decoder outperforms traditional piano-roll outputs by 0.01 and 0.022 in onset-offset-velocity F1 score

## Executive Summary
This paper introduces a hierarchical language modeling approach for automatic piano transcription that combines pre-trained roll-based encoders with a transformer decoder. The method employs a three-stage prediction strategy (onset+pitch → velocity → offset) to reduce computational costs and improve scalability compared to flattened token sequences. Evaluated on two benchmark roll-based encoders (CRNN and HPPNet), the approach demonstrates that encoder quality significantly impacts performance more than LM size, achieving notable improvements over traditional piano-roll outputs.

## Method Summary
The method uses a two-stage training process: first pre-training a roll-based encoder (CRNN or HPPNet) on frame-level piano-roll objectives, then connecting it to a frozen LLaMA-style decoder. The hierarchical prediction strategy decomposes the transcription task into three separate models - onset-pitch, velocity, and offset - each with specialized query tokens. This approach reduces computational complexity from O((T+3N)²D) to O(3(T+N)²D) by breaking long sequences into shorter hierarchical components. The system uses absolute positional encodings for audio embeddings and rotary position embeddings for note tokens.

## Key Results
- Hierarchical approach reduces computational complexity by approximately threefold compared to flattened token sequences
- Encoder choice (CRNN vs HPPNet) has greater impact on performance than LM size
- Velocity prediction task shows different training dynamics, overfitting at ~100k steps while onset-pitch and offset plateau
- Performance degrades when scaling LM size beyond optimal point (181M outperforms 335M parameters)

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical token decomposition reduces computational complexity by approximately threefold compared to flattened token sequences. Instead of predicting all note attributes in a single flattened sequence of length T + 3N, the model uses three separate specialized models with sequence lengths of approximately T + N each. This changes complexity from O((T + 3N)²D) to O(3(T + N)²D), where T << N.

### Mechanism 2
Pre-trained roll-based encoders provide higher-quality audio representations than end-to-end encoder training for LM-based transcription. Roll-based encoders (CRNN, HPPNet) are pre-trained on frame-level piano-roll objectives, learning robust time-frequency representations that transfer effectively to note-level language modeling tasks.

### Mechanism 3
Specialized hierarchical models reduce task interference compared to single-model approaches, particularly for velocity prediction which shows different training dynamics. Three separate transformer models with identical architecture are trained for onset-pitch, velocity, and offset prediction respectively, preventing gradient interference between attributes with different learning rates.

## Foundational Learning

- **Piano-roll representation**: Understanding the baseline frame-level output format (Y ∈ {0, 1}^(T × K) with 88 pitches) that traditional AMT systems predict, which requires thresholding and post-processing. The hierarchical LM approach explicitly avoids this representation.
- **Autoregressive language modeling with cross-attention**: The LM decoder conditions on encoder representations (H = f_enc(X)) while autoregressively generating note tokens. Understanding how equation (4) combines encoder features with sequential generation is essential.
- **Positional encodings for multimodal sequences**: The system uses two distinct positional encoding schemes—absolute positional encodings for audio embeddings and RoPE for note tokens. This hybrid approach addresses the fundamentally different structure of continuous audio vs. discrete token sequences.

## Architecture Onboarding

- **Component map**: STFT/CQT → Encoder (CRNN or HPPNet, frozen) → Linear projection → LM decoder (LLaMA-style) → Task-specific predictions
- **Critical path**: Audio → STFT/CQT → Encoder → embeddings H ∈ ℝ^(T' × D) → projected embeddings + absolute PE → three separate LMs with query tokens → hierarchical predictions
- **Design tradeoffs**: Flattened vs. hierarchical (simpler vs. scalable), encoder freezing vs. fine-tuning (risk of forgetting vs. transfer), decoder-only vs. encoder-decoder (efficiency vs. handling long sequences)
- **Failure signatures**: Severe offset prediction failure in flattened approach (On Off F1 drops from ~0.82 to ~0.39), velocity overfitting at ~100k steps, large model degradation (335M underperforms 181M)
- **First 3 experiments**: 1) Encoder comparison baseline with CRNN on Maestro test set, 2) Ablation: flattened vs. hierarchical with same encoder and LM size, 3) Scaling test with "tiny" (4 layers) and "large" (12 layers) LM decoder variants

## Open Questions the Paper Calls Out

1. **Scalability of LM-based AMT systems**: How can the scalability be improved given that larger model sizes currently lead to overfitting rather than performance gains? The paper calls for further investigation after observing that larger models failed to improve metrics.

2. **Velocity prediction overfitting**: Why does the velocity prediction task exhibit significantly faster overfitting compared to onset-pitch and offset predictions in a hierarchical setup? The paper identifies early overfitting but does not provide analysis on the specific acoustic features causing this.

3. **Encoder-decoder vs decoder-only architecture**: Does an encoder-decoder architecture handle flattened token sequences more effectively than a decoder-only architecture for long-duration segments? The paper hypothesizes about benefits but does not experimentally validate this claim.

## Limitations

- Critical architectural details of CRNN and HPPNet encoders are omitted, creating significant barriers to faithful reproduction
- Hierarchical task decoupling assumes attributes can be predicted independently without joint optimization, lacking error correction mechanisms
- Limited scaling experiments show larger models (335M) underperform smaller ones (181M), suggesting data limitations or atypical scaling behavior

## Confidence

- **High Confidence**: Computational complexity reduction claim is mathematically sound and empirically validated by severe performance degradation in flattened approaches
- **Medium Confidence**: Encoder quality dominates LM size claim is supported but limited scaling range leaves uncertainty about true impact of decoder capacity
- **Low Confidence**: Separate models reduce task interference claim is primarily supported by observing different training dynamics but lacks direct comparison to joint training

## Next Checks

1. **Architecture fidelity verification**: Reconstruct exact CRNN and HPPNet architectures from available code repositories and reproduce encoder pre-training on frame-level piano-roll objectives, validating frozen embeddings transfer to simple downstream tasks.

2. **Ablation: Joint vs. hierarchical training**: Implement single transformer model predicting all three attributes jointly in flattened sequence format, then compare computational requirements and performance against hierarchical approach.

3. **Scaling law validation**: Systematically vary both encoder quality (CRNN vs. HPPNet vs. larger encoders) and LM size (4L, 6L, 12L variants) while holding other factors constant to clarify whether encoder dominance persists across full parameter space.