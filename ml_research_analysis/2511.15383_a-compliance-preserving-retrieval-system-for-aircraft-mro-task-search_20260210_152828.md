---
ver: rpa2
title: A Compliance-Preserving Retrieval System for Aircraft MRO Task Search
arxiv_id: '2511.15383'
source_url: https://arxiv.org/abs/2511.15383
tags:
- task
- retrieval
- system
- maintenance
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a compliance-preserving retrieval system
  for aircraft maintenance tasks that enables technicians to find procedures using
  natural-language queries without modifying certified systems. The method leverages
  stable ATA chapter metadata to build revision-robust embeddings, combined with vision-language
  parsing to structure certified manual content for previews, and LLM re-ranking to
  improve semantic accuracy.
---

# A Compliance-Preserving Retrieval System for Aircraft MRO Task Search

## Quick Facts
- arXiv ID: 2511.15383
- Source URL: https://arxiv.org/abs/2511.15383
- Reference count: 4
- Primary result: >90% retrieval accuracy with 95% reduction in lookup time while preserving compliance

## Executive Summary
This paper introduces a compliance-preserving retrieval system for aircraft maintenance tasks that enables technicians to find procedures using natural-language queries without modifying certified systems. The method leverages stable ATA chapter metadata to build revision-robust embeddings, combined with vision-language parsing to structure certified manual content for previews, and LLM re-ranking to improve semantic accuracy. A synthetic benchmark of 49k queries achieves >90% retrieval accuracy, while a bilingual study with 10 licensed AMTs shows 90.9% top-10 success rate and 95% reduction in lookup time—from 6-15 minutes to 18 seconds per task. These results demonstrate that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.

## Method Summary
The system extracts task metadata from certified aircraft manuals using vision-language parsing, then builds embeddings from stable ATA chapter hierarchies and task titles (excluding procedural text to maintain revision-robustness). Online retrieval uses multilingual embeddings to match natural-language queries against this metadata, followed by LLM re-ranking to improve semantic accuracy. The architecture preserves compliance by only providing previews and direct links to original certified content rather than modifying or synthesizing procedures. A synthetic benchmark of 49,643 queries (6 per task with typo variants) enables evaluation without human annotation costs.

## Key Results
- >90% retrieval accuracy achieved with synthetic benchmark of 49k queries
- 95% reduction in task lookup time (from 6-15 minutes to 18 seconds per task)
- Cross-lingual retrieval viable with 86% top-10 success rate for Korean queries vs 95.9% for English

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Building embeddings from stable ATA metadata rather than procedural text reduces re-indexing frequency across manual revisions while preserving retrieval accuracy.
- Mechanism: The system concatenates ATA chapter hierarchy titles with task titles (e.g., "Landing Gear → Brake System → Gear Brake → Removal") to form embeddings, deliberately excluding task procedural text which changes frequently across revisions.
- Core assumption: Task titles and hierarchy paths change far less frequently than full procedural text—a stability property that holds across OEM manual revision cycles.
- Evidence anchors:
  - [abstract] "constructs revision-robust embeddings from ATA chapter hierarchies"
  - [Section 3.1] "We exclude task procedural text (which changes frequently across revisions)"
  - [corpus] No direct corpus evidence on ATA stability; related MRO scheduling work (arxiv 2508.02640) addresses operational optimization but not metadata stability
- Break condition: If OEMs restructure ATA hierarchies or rename task titles frequently in new revisions, embedding staleness would require full re-indexing.

### Mechanism 2
- Claim: LLM re-ranking improves semantic matching over dense retrieval alone by reasoning across hierarchy paths and titles without exposing procedural content.
- Mechanism: The LLM receives a structured prompt with the query plus top-50 candidates (ATA IDs, hierarchy paths, titles) and outputs only a JSON array of re-ranked indices. This improves Hit@5 from 85.34% (dense retrieval) to 91.64% (Llama3.3-70B).
- Core assumption: The semantic gap between technician natural-language queries and formal task titles can be bridged by LLM reasoning over metadata alone, without procedural context.
- Evidence anchors:
  - [Section 3.2] "LLM receives a structured prompt containing... top-50 candidate tasks with their ATA IDs, hierarchy paths, and titles"
  - [Table 1] Dense retrieval 85.34% → LLM re-ranking 91.64% Hit@5 improvement
  - [corpus] arxiv 2512.23307 documents neural ranking vulnerabilities but doesn't address constrained re-ranking scenarios
- Break condition: If task titles share near-identical lexical forms but differ procedurally (e.g., "Brake Valve Removal" vs. "Brake Shuttle Valve Removal"), the LLM cannot disambiguate without procedural context.

### Mechanism 3
- Claim: Cross-lingual retrieval is viable using multilingual embeddings even when the knowledge base contains only English content.
- Mechanism: BGE-M3 embeddings map Korean queries to the same semantic space as English task embeddings, achieving 86.0% top-10 success rate for Korean queries vs. 95.9% for English.
- Core assumption: Multilingual embedding models sufficiently align aviation terminology across languages despite domain-specific terms lacking standardized translations.
- Evidence anchors:
  - [Section 4.2] "A multilingual embedding model (BGE-M3) enabled Korean queries to retrieve English task embeddings in the same semantic space"
  - [Table 2] English 95.9% vs. Korean 86.0% top-10 success rate
  - [corpus] No corpus evidence on cross-lingual MRO retrieval specifically
- Break condition: If aviation-specific English terms lack standardized equivalents in target languages (e.g., "thrust reverser," "brake shuttle valve"), embedding alignment degrades and retrieval fails.

## Foundational Learning

- Concept: ATA Chapter Hierarchies (Air Transport Association standard)
  - Why needed here: The entire embedding strategy relies on understanding how ATA structures organize maintenance documentation across 4-6 nested levels with specific chapter/subchapter/task identifiers.
  - Quick check question: Can you trace the path from Chapter 32 to a specific brake component removal task?

- Concept: Dense Retrieval vs. Re-ranking Architecture
  - Why needed here: The system separates first-stage retrieval (embedding similarity) from second-stage refinement (LLM re-ranking), and understanding this separation is essential for debugging and optimization.
  - Quick check question: Why might Hit@50 be high while Hit@5 remains low, and which stage addresses this?

- Concept: Compliance Constraints in Regulated Industries
  - Why needed here: Aviation regulations require technicians to read certified manual content directly—understanding why RAG synthesis is prohibited explains the "preview-then-link" design pattern.
  - Quick check question: Why can't the system display generated summaries of procedures, even if semantically accurate?

## Architecture Onboarding

- Component map:
  - **Offline Pipeline**: PDF manuals → VLM extraction (Qwen 2.5-VL-72B) → rule-based parsing → Task Knowledge DB with embeddings
  - **Online Pipeline**: Query → multilingual embedding (BGE-M3) → Top-50 retrieval → LLM re-ranking (Qwen3-8B/Llama3.3-70B) → Top-10 preview → link to certified viewer
  - **Fail-safe**: Invalid LLM output triggers fallback to baseline dense retrieval rankings

- Critical path: Query → embedding lookup → candidate retrieval → LLM re-ranking → JSON parsing → result display. The JSON parsing step is a single point of failure; malformed LLM output silently falls back to unrefined results.

- Design tradeoffs:
  - Smaller models (Qwen3-4B/8B) maintain >90% Hit@5 with lower latency and cost vs. 70B models—suitable for resource-constrained deployment
  - Embedding only metadata (not procedural text) trades disambiguation capability for revision-robustness
  - Top-10 presentation trades precision for recall, accepting that technicians review multiple candidates

- Failure signatures:
  - High fallback rate → LLM output format instability (check JSON parsing logs)
  - Korean query degradation >15% → embedding alignment issues with domain terminology
  - Low Hit@5 but high initial retrieval → re-ranking model not receiving proper candidate formatting

- First 3 experiments:
  1. Validate embedding stability: Compare retrieval accuracy across two manual revision versions using the same embedding index—expect <5% degradation if metadata stability holds.
  2. Measure re-ranking contribution: A/B test dense retrieval vs. LLM re-ranking on 100 held-out queries—quantify the 6.3pp gain in your deployment context.
  3. Stress-test fail-safe: Inject malformed LLM responses and verify fallback triggers correctly with logging—confirm no silent errors in production workflow.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating procedural context beyond task titles and hierarchy paths resolve ambiguities between near-identical task names that differ only by maintenance stage or sequence?
- Basis in paper: [explicit] The conclusion states: "Future work should incorporate procedural context to resolve ambiguities between similar task titles."
- Why unresolved: The current system deliberately excludes procedural text from embeddings to maintain revision-robustness, creating a trade-off between stability and disambiguation capability.
- What evidence would resolve it: A controlled experiment comparing retrieval accuracy on ambiguous task pairs when procedural context snippets are included versus metadata-only embeddings.

### Open Question 2
- Question: Can multimodal query capabilities (e.g., component photographs, fault indicator images) improve retrieval accuracy for technicians struggling to articulate textual queries?
- Basis in paper: [explicit] The conclusion states future work should "extend capabilities to multimodal queries... for a comprehensive MRO cognitive assistant."
- Why unresolved: The current system only handles text queries, while MRO workflows involve visual fault identification that technicians may struggle to verbalize precisely.
- What evidence would resolve it: A human study comparing text-only queries against text+image queries for common fault scenarios, measuring retrieval success rates and time-to-target.

### Open Question 3
- Question: Can domain-specific multilingual fine-tuning close the 9.9-point cross-lingual retrieval gap observed between English and Korean queries?
- Basis in paper: [inferred] The paper reports a 9.9-point gap (English 95.9% vs. Korean 86.0% top-10 success) and attributes failures partly to "translation ambiguity" where aviation-specific terms lack standardized Korean equivalents.
- Why unresolved: Generic multilingual embeddings (BGE-M3) may not adequately capture aviation terminology alignment across languages, but domain-specific fine-tuning data is scarce.
- What evidence would resolve it: A comparison study using aviation-domain parallel corpora (task titles, procedure descriptions) to fine-tune embeddings, evaluated on the same bilingual query set.

### Open Question 4
- Question: Does the system maintain >90% retrieval accuracy when deployed across airlines with different manual structures, aircraft types, and operational terminology conventions?
- Basis in paper: [inferred] The human study involved only 10 AMTs from a single Korean airline, with synthetic benchmarks limited to Boeing 737 manuals. Generalization to Airbus, regional jets, or other airline documentation practices remains untested.
- Why unresolved: ATA chapter hierarchies are standardized, but OEM-specific terminology, manual organization, and technician query patterns may vary significantly across airlines and aircraft families.
- What evidence would resolve it: A multi-site study across 2-3 airlines operating different aircraft types, measuring Hit@5/Hit@10 on standardized task sets.

## Limitations

- Embedding strategy relies on stable ATA metadata that may change across manual revisions, requiring full re-indexing
- Cross-lingual performance gap (9.9% lower for Korean vs English) suggests domain terminology alignment issues
- LLM re-ranking cannot resolve semantically ambiguous task titles without procedural context

## Confidence

- **Compliance-preserving design** (High): Well-justified by aviation regulations and explicit preview-then-link pattern
- **Embedding-based revision robustness** (Medium): Theoretical mechanism sound but corpus evidence lacking on ATA stability
- **LLM re-ranking effectiveness** (High): 6.3pp improvement statistically significant and reproducible
- **Cross-lingual retrieval viability** (Medium): 86% success rate shows feasibility but 10% gap indicates limited robustness

## Next Checks

1. **Revision stability test**: Obtain two different versions of the same aircraft manual (e.g., 2022 vs 2024) and measure retrieval accuracy degradation when using embeddings built from the older version—expect <5% degradation to confirm metadata stability.

2. **Ambiguity resolution analysis**: Manually examine 50 retrieval failures where top-1 is incorrect but correct answer appears in top-10; classify whether failures stem from lexical ambiguity, semantic mismatch, or metadata insufficiency.

3. **Cross-lingual terminology audit**: For 100 Korean queries, identify specific aviation terms that fail to retrieve correct English tasks; assess whether terminology gaps justify query translation preprocessing versus accepting the 10% performance cost.