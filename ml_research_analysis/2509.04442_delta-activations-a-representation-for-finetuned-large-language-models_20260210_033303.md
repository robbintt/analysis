---
ver: rpa2
title: 'Delta Activations: A Representation for Finetuned Large Language Models'
arxiv_id: '2509.04442'
source_url: https://arxiv.org/abs/2509.04442
tags:
- delta
- activations
- arxiv
- task
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Delta Activations, a method to represent
  finetuned language models as vector embeddings by measuring shifts in their internal
  activations relative to a base model. The approach addresses the challenge of navigating
  and understanding the growing ecosystem of post-trained models, which often lack
  standardized metadata and documentation.
---

# Delta Activations: A Representation for Finetuned Large Language Models

## Quick Facts
- arXiv ID: 2509.04442
- Source URL: https://arxiv.org/abs/2509.04442
- Authors: Zhiqiu Xu; Amish Sethi; Mayur Naik; Ser-Nam Lim
- Reference count: 40
- Key outcome: Introduces Delta Activations, achieving average silhouette score of 0.61 for clustering finetuned models by domain

## Executive Summary
This paper addresses the challenge of organizing and discovering finetuned language models in the rapidly growing ecosystem of post-trained models. Delta Activations represents each finetuned model as a vector embedding by measuring shifts in internal activations relative to a base model, using a small set of generic prompts. The method enables effective clustering of models by their domain specialization without requiring task-specific metadata or probing, outperforming baselines like flattened weights and output sentence embeddings across multiple model families.

## Method Summary
Delta Activations computes vector embeddings for finetuned models by passing a small probe dataset of 5 generic instruction prompts through both base and finetuned models, extracting the last-token hidden state from the final layer, and computing the difference between them. The per-prompt deltas are averaged to create a 4096-dimensional embedding that captures how the model's internal representations have shifted due to finetuning. For cross-architecture comparison, Delta Meaning uses inverse perplexity differences instead of activations. The method is evaluated on model pools derived from LLaMA-3.1-8B, Gemma-2-9B, and Qwen-2.5-7B, finetuned on five distinct domains using LoRA adapters.

## Key Results
- Achieves average silhouette score of 0.61 across three backbones, significantly outperforming flattened weights (-0.043) and output sentence embeddings (0.16-0.39)
- Demonstrates additive property where mixed finetuning datasets produce embeddings approximately equal to the vector sum of individual dataset embeddings
- Shows probe design matters: generic instruction templates outperform domain-specific prompts for clustering quality
- Intermediate layers (2/3 depth) provide slightly better embeddings than final layer for clustering domain specialization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Finetuning induces measurable shifts in hidden-state activations that encode domain/task specialization
- **Mechanism**: When a base LLM is finetuned on a domain corpus, gradient updates systematically alter how the model processes generic inputs. Computing the vector difference between finetuned and base model hidden states isolates the "behavioral delta" caused by finetuning, capturing how computation has shifted rather than what the model outputs.
- **Core assumption**: The activation shift on generic inputs reflects domain-specific adaptation in a way that generalizes across prompts. Assumption: Last token's final-layer representation encodes task-relevant structure even when prompt is domain-agnostic.
- **Evidence anchors**: [abstract] "represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model"; [Section 2.4] "∆f(x) quantifies how the model's internal representation of the input x diverges from the base model as a result of finetuning"; [corpus] Related work "Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences" independently validates that narrow finetuning creates interpretable biases in activations (FMR=0.55)

### Mechanism 2
- **Claim**: Generic instruction templates activate core computational pathways that reveal specialization without requiring task-specific probing
- **Mechanism**: The probe dataset uses paraphrased Alpaca-style instruction templates with dummy content designed to trigger general instruction-following circuits. Because finetuning redistributes activation patterns across the network, even generic inputs produce measurably different hidden states in specialized models.
- **Core assumption**: Generic prompts engage sufficiently broad model circuits that domain-specific shifts become detectable. Assumption: 5 prompts are sufficient to stabilize the embedding (Table 5a shows 1→5 helps, 5→20 adds nothing).
- **Evidence anchors**: [Section 2.3] "finetuned LLMs occasionally respond to such a generic instruction template with their specialization"; [Table 5c] Instruction templates (0.61) outperform domain-specific prompts (0.42) and Wikitext (0.44) for clustering; [corpus] Weak direct evidence; neighbor papers focus on finetuning effects but not probe design

### Mechanism 3
- **Claim**: Activation deltas exhibit an additive property that mirrors dataset composition
- **Mechanism**: When two datasets D1 and D2 are mixed during finetuning, the resulting activation delta approximately equals the vector sum of deltas from models trained on D1 and D2 separately. This suggests the embedding space has linear structure with respect to training-data composition.
- **Core assumption**: Finetuning on mixed data produces activation shifts that are approximately the linear combination of individual shifts. Assumption: LoRA adapters trained separately can be meaningfully compared to jointly trained adapters.
- **Evidence anchors**: [abstract] "exhibits an additive property when finetuning datasets are mixed"; [Table 4] Mixed-model similarity to sum (0.65, 0.73, 0.65) consistently exceeds similarity to either individual model; [Table 13] Full results across all 10 domain pairs confirm additive effect holds; [corpus] No direct corpus validation; this property is novel to this work

## Foundational Learning

- **Hidden States vs. Logits**:
  - Why needed here: Delta Activations operates on internal hidden states (typically final layer, last token), not output probabilities. Understanding that hidden states encode contextual meaning before projection to vocabulary space is essential.
  - Quick check question: In a decoder-only LLM, what does the final token's last-layer hidden state represent before the LM head projects it?

- **Silhouette Score**:
  - Why needed here: The paper uses silhouette score as the primary clustering quality metric. You need to understand that it measures both intra-cluster cohesion and inter-cluster separation, ranging from -1 to +1.
  - Quick check question: If a model has silhouette score 0.61, what does that indicate about its cluster assignment compared to a score of -0.04?

- **LoRA (Low-Rank Adaptation)**:
  - Why needed here: All main experiments use LoRA finetuning. Understanding that LoRA adds trainable low-rank matrices to weight matrices while freezing base weights explains why flattened-adapter baselines are compared.
  - Quick check question: Why would flattening LoRA adapter weights (~2×10⁷ dims) produce worse clustering than activation deltas (4096 dims)?

## Architecture Onboarding

- **Component map**: Probe Dataset Generator -> Forward Pass Engine -> Activation Extractor -> Delta Computer -> Embedding Store

- **Critical path**:
  1. Load base model and finetuned model (must share architecture)
  2. Generate or load probe dataset (5 generic templates)
  3. For each prompt: run forward pass on both models, extract final-layer last-token hidden state
  4. Compute per-prompt delta, average across prompts → final embedding
  5. For cross-architecture: use Delta Meaning (inverse perplexity differences) instead of activations

- **Design tradeoffs**:
  - **Layer depth**: Final layer (simplest) vs. 2/3 depth (slightly better, Table 6b)
  - **Token position**: Last token (default) vs. weighted average of all tokens (slightly better, Table 6a)
  - **Probe size**: N=5 is sufficient; N=1 is unstable; N>5 adds no gain (Table 5a)
  - **Representation type**: Activations (same-architecture only) vs. Delta Meaning (cross-architecture, but requires sampling continuations)

- **Failure signatures**:
  - Negative silhouette scores (e.g., flattened weights: -0.043) indicate embeddings don't cluster by domain
  - If output sentence embeddings work for one backbone but not others (Table 2), suggests probe isn't eliciting specialization
  - Cross-architecture clustering with Delta Activations will crash due to dimension mismatch

- **First 3 experiments**:
  1. Reproduce clustering on single backbone: Finetune LLaMA-3.1-8B on 3 domains (e.g., MATH, CODE, MEDICAL) with 3000 examples each, compute Delta Activations, verify silhouette >0.5
  2. Ablate probe design: Test N=1 vs. N=5 vs. N=20 prompts on same model pool; verify N=5 is sufficient
  3. Test additivity: Finetune on D1, D2, and D1∪D2; verify v(D1∪D2) ≈ v(D1) + v(D2) via cosine similarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Delta Activations be leveraged to mitigate negative interference when merging models, given that selecting the most similar models results in worse performance than random selection?
- Basis in paper: [explicit] The authors observe that selecting the 20 most similar models for merging yields 30.3% accuracy (lower than random) and suggest using embeddings to identify "maximally dispersed subsets" or training neural networks to optimize merging.
- Why unresolved: The paper identifies the counter-intuitive failure mode (similarity $\neq$ mergeability) but only provides conjectures regarding "model interference" without validating a specific algorithmic solution.
- What evidence would resolve it: A successful merging strategy that utilizes Delta Activations to enforce diversity (e.g., orthogonal embeddings) and outperforms the random selection baseline.

### Open Question 2
- Question: How does the method scale to model pools substantially larger (e.g., thousands of models) than the small pools (approx. 45 models) evaluated in the study?
- Basis in paper: [explicit] The authors state, "It is natural to ask how our method might perform on model pools substantially larger than those considered in our evaluation," noting such pools currently exist mostly in proprietary settings.
- Why unresolved: The experiments are limited to controlled pools of 15 models per backbone, leaving the computational efficiency and clustering validity in massive, uncurated ecosystems unproven.
- What evidence would resolve it: Evaluation of silhouette scores and retrieval latency on a large-scale repository containing thousands of diverse, publicly available adapters.

### Open Question 3
- Question: Why do intermediate layers (at 2/3 depth) provide more effective embeddings for clustering domain specialization than the final layer?
- Basis in paper: [inferred] Table 6b shows the 2/3 depth layer achieves the highest silhouette score (0.64) compared to the last layer (0.61). The authors note this parallels vision encoders but do not explain the mechanism in LLMs.
- Why unresolved: The paper empirically identifies the optimal layer but does not analyze why the final layer, which is closest to the output logits, suppresses the domain-specific signal.
- What evidence would resolve it: A layer-wise probing analysis determining if the 2/3 depth layer represents a "sweet spot" for conceptual knowledge before it is flattened into next-token prediction logits.

## Limitations
- Probe design may not generalize across all model families and finetuning paradigms, as the Alpaca-style templates might not engage task-relevant pathways in certain architectures
- Cross-architecture generalization via Delta Meaning is computationally expensive and requires sampling multiple continuations, with limited experimental validation
- Additivity property hasn't been validated across different finetuning methods beyond LoRA, leaving questions about its generality

## Confidence
- **High Confidence**: Core clustering results within single architectures (silhouette scores ~0.61) are well-supported with multiple backbones and domains; additive property demonstrations are robust within tested conditions
- **Medium Confidence**: Probe design advantages are demonstrated but could be influenced by dataset specifics; computational efficiency claims relative to full parameter comparison are reasonable but not extensively benchmarked
- **Low Confidence**: Cross-architecture generalization claims and generality of additive property across different finetuning methods remain weakly validated

## Next Checks
1. **Probe Ablation Across Domains**: Systematically test the effect of removing/adding specific probe prompts on clustering quality across all five domains to validate whether probe design is robust or if certain domains require domain-specific probes

2. **Cross-Architecture Stress Test**: Evaluate Delta Meaning across 3-5 different model pairs with varying architectural differences (decoder-only vs. encoder-decoder, different embedding sizes, etc.) to establish method's practical limits for model discovery

3. **Finetuning Method Generalization**: Repeat additivity experiments using full fine-tuning instead of LoRA, and with different adapter methods (IA3, Prefix Tuning) to determine whether linear structure in activation space is method-specific or general property of finetuning