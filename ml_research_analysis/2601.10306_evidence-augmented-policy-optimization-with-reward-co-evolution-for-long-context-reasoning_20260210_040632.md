---
ver: rpa2
title: Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context
  Reasoning
arxiv_id: '2601.10306'
source_url: https://arxiv.org/abs/2601.10306
tags:
- evidence
- reasoning
- answer
- reward
- eapo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of sparse reward signals in long-context
  reasoning by introducing Evidence-Augmented Policy Optimization (EAPO), which shifts
  focus from outcome-only supervision to dense process-based rewards. EAPO employs
  a Group-Relative Evidence Reward to provide fine-grained feedback on the quality
  of extracted evidence, ensuring the model is rewarded for high-fidelity retrieval.
---

# Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning

## Quick Facts
- arXiv ID: 2601.10306
- Source URL: https://arxiv.org/abs/2601.10306
- Reference count: 23
- Improves long-context reasoning accuracy by up to 7.5% over baselines

## Executive Summary
This paper tackles the challenge of sparse reward signals in long-context reasoning by introducing Evidence-Augmented Policy Optimization (EAPO), which shifts focus from outcome-only supervision to dense process-based rewards. EAPO employs a Group-Relative Evidence Reward to provide fine-grained feedback on the quality of extracted evidence, ensuring the model is rewarded for high-fidelity retrieval. To maintain supervision quality, it integrates an Adaptive Reward-Policy Co-Evolution mechanism that iteratively refines the reward model using outcome-consistent rollouts. Across eight benchmarks, EAPO achieves significant improvements over state-of-the-art baselines, with gains of up to 7.5% in accuracy, and notably outperforms much larger models, including proprietary ones.

## Method Summary
EAPO addresses long-context reasoning by decomposing the task into four stages: analysis, evidence extraction, reasoning, and answer generation. It introduces dense process supervision through a Group-Relative Evidence Reward, where a reward model evaluates evidence segments from a group of rollouts simultaneously, assigning comparative scores. An Adaptive Reward-Policy Co-Evolution mechanism iteratively refines the reward model using outcome-consistent rollouts, ensuring supervision quality improves with policy performance. The framework builds on Group Relative Policy Optimization (GRPO) with a multi-granular reward: 10% format adherence, 30% evidence quality, and 60% outcome correctness.

## Key Results
- Achieves up to 7.5% accuracy improvements over state-of-the-art baselines
- Outperforms much larger models, including proprietary ones
- Demonstrates that evidence extraction is the primary bottleneck in long-context reasoning
- Shows co-evolution mechanism sustains performance gains where static reward models plateau

## Why This Works (Mechanism)

### Mechanism 1: Dense Process Supervision via Group-Relative Evidence Reward
- **Claim:** Shifting from sparse outcome-only rewards to dense evidence-level feedback improves long-context reasoning by explicitly supervising the retrieval bottleneck.
- **Mechanism:** The reward model evaluates all evidence segments from a group of G rollouts simultaneously, assigning integer utility scores (1-5). These scores are normalized within-group (Re ∈ [0,1]), creating comparative pressure that rewards high-fidelity extraction relative to alternatives rather than against an absolute threshold.
- **Core assumption:** Evidence quality causally determines reasoning success, and models can learn to discriminate needle-in-a-haystack facts when given explicit comparative feedback.
- **Evidence anchors:**
  - [abstract]: "EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality."
  - [Section 3.2]: "This component serves as an intermediate process reward designed to assess the quality of the extracted evidence... enabling the policy to prioritize superior evidence through comparative feedback."
  - [corpus]: ProRAG (arXiv:2601.21912) similarly finds that "traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment."

### Mechanism 2: Adaptive Reward-Policy Co-Evolution
- **Claim:** Iteratively refining the reward model using outcome-consistent rollouts maintains supervision quality as the policy improves, preventing reward model obsolescence.
- **Mechanism:** The policy generates rollouts; the RM scores them. An Outcome Consistency filter retains only trajectories where RM judgments align with objective reality (high evidence scores → correct answers, low scores → incorrect). These high-confidence samples fine-tune the RM via supervised learning (Eq. 3), sharpening its discriminative boundary in sync with policy evolution.
- **Core assumption:** The reward model can be progressively calibrated using the policy's own outputs, and outcome-consistency is a reliable proxy for supervision quality.
- **Evidence anchors:**
  - [abstract]: "This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance."
  - [Section 5.1, Figure 5]: "While the static variant saturates after step 50, the full EAPO framework driven by Co-Evolution sustains its upward trajectory, achieving a higher performance ceiling."
  - [corpus]: Agentic Policy Optimization (arXiv:2512.01945) uses "Instruction-Policy Co-Evolution" for similar iterative alignment.

### Mechanism 3: Evidence-Augmented Reasoning Paradigm as Structural Constraint
- **Claim:** Forcing explicit evidence extraction before reasoning exposes the intermediate retrieval state for supervision and eliminates ungrounded "lucky guesses."
- **Mechanism:** The four-stage EAR workflow (<analysis>, <evidence>, <reasoning>, <answer>) with special tokens creates an intermediate artifact (the evidence block) that can be independently evaluated. Format Adherence Reward (Rf = 1 or 0) enforces this structure, terminating evaluation on invalid formats.
- **Core assumption:** Models can learn to follow this structured workflow, and the enforced decomposition does not impair reasoning flexibility.
- **Evidence anchors:**
  - [Section 2.1]: "This structural decomposition operationalized via special tokens not only focuses attention but, crucially, exposes the intermediate evidence state for direct supervision."
  - [Section 2.2, Figure 3]: Oracle (Evidence-Grounded) achieves 63.0% vs. 45.0% baseline, revealing "massive potential currently locked by retrieval failures."

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** EAPO builds on GRPO (Eq. 1) as its RL backbone. Understanding the advantage computation (Âi from standardized rewards) and the clipping mechanism (ϵlow, ϵhigh) is prerequisite to modifying the reward composition.
  - **Quick check question:** Can you explain why GRPO uses group-relative advantages rather than absolute reward values, and how this relates to variance reduction?

- **Concept: Process Reward Models (PRMs) vs. Outcome Reward Models (ORMs)**
  - **Why needed here:** The core thesis is that outcome-only supervision fails for long-context tasks. Understanding the distinction—and why PRMs provide denser credit assignment—is essential for motivating EAPO's design.
  - **Quick check question:** What is the credit assignment problem in RL for LLMs, and how does a PRM address it that an ORM cannot?

- **Concept: Reward Sparsity in Long-Context Settings**
  - **Why needed here:** The paper identifies sparse outcome rewards as the core failure mode. Understanding why longer contexts exacerbate sparsity (more steps, same single reward signal) grounds the motivation for dense supervision.
  - **Quick check question:** In a 100k-token context task, why does a single binary outcome reward provide insufficient learning signal for intermediate retrieval decisions?

## Architecture Onboarding

- **Component map:** Policy LLM → EAR format generation → Evidence extraction → RM scoring → Group normalization → Advantage computation → Policy update → Outcome consistency filtering → RM fine-tuning
- **Critical path:** Rollout generation → Evidence extraction → RM scoring → Group normalization → Advantage computation → Policy update → Outcome consistency filtering → RM fine-tuning (every 20 RL steps)
- **Design tradeoffs:**
  - β weight (evidence vs. outcome): Paper finds β=0.3 optimal; β=0.5 causes performance collapse (over-weighting intermediate steps)
  - RM update frequency: Every 20 steps chosen; more frequent may cause instability, less frequent may lose alignment
  - Group size G=6: Larger groups provide better relative comparisons but increase compute
- **Failure signatures:**
  - Static RM without co-evolution: Performance plateaus at ~step 50 (Figure 5)
  - Outcome-only GRPO: Negligible gains (+1.2% for 30B-Thinking), implicit evidence optimization is inefficient
  - Invalid EAR format: Rf=0 terminates evaluation; remaining rewards not computed
- **First 3 experiments:**
  1. **Validate evidence bottleneck hypothesis:** Replicate Tree-Structured Evidence Sampling (Section 2.2) on a held-out benchmark—confirm that Oracle (ground-truth evidence) lifts performance substantially
  2. **Ablate co-evolution:** Compare EAPO full vs. EAPO w/o RM Co-Evolution vs. GRPO baseline on MuSiQue; plot convergence curves similar to Figure 5
  3. **Sweep β weight:** Test β ∈ {0.0, 0.1, 0.3, 0.5} on a subset of LongBench-V2 tasks; verify that β=0.3 yields peak accuracy and β=0.5 causes degradation (confirming Figure 8 pattern)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in a dedicated section. The authors focus on presenting their solution and experimental validation rather than discussing unresolved issues or future research directions.

## Limitations
- Performance evaluation relies on proprietary reward models (gpt-4o-2024-11-20) for outcome consistency filtering
- Wikipedia-based Mixed QA dataset construction details are underspecified, creating reproducibility challenges
- Co-evolution mechanism's effectiveness depends heavily on the reliability of outcome-consistency filtering, which isn't independently validated

## Confidence
- **High confidence** in the core evidence bottleneck hypothesis and general framework design
- **Medium confidence** in the co-evolution mechanism's practical effectiveness
- **Low confidence** in exact reproducibility due to dataset construction ambiguity and reliance on GPT-4o for filtering

## Next Checks
1. **Validate evidence bottleneck hypothesis:** Replicate the Tree-Structured Evidence Sampling experiment on a held-out benchmark to confirm that Oracle (ground-truth evidence) substantially lifts performance compared to baseline.

2. **Ablate co-evolution mechanism:** Compare full EAPO against EAPO without RM Co-Evolution and GRPO baseline on MuSiQue, plotting convergence curves to verify the sustained improvement pattern shown in Figure 5.

3. **Test reward weight sensitivity:** Systematically sweep the β evidence reward weight across {0.0, 0.1, 0.3, 0.5} on LongBench-V2 tasks to confirm β=0.3 yields peak accuracy while β=0.5 causes degradation.