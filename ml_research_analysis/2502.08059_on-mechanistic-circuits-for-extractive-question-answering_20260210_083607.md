---
ver: rpa2
title: On Mechanistic Circuits for Extractive Question-Answering
arxiv_id: '2502.08059'
source_url: https://arxiv.org/abs/2502.08059
tags:
- context
- attention
- circuit
- language
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extracts mechanistic circuits for the real-world task
  of extractive question-answering (QA) in large language models, providing insights
  into how models use context versus parametric memory. The authors design probe datasets
  and use causal mediation analysis to identify context-faithfulness and memory-faithfulness
  circuits.
---

# On Mechanistic Circuits for Extractive Question-Answering

## Quick Facts
- arXiv ID: 2502.08059
- Source URL: https://arxiv.org/abs/2502.08059
- Reference count: 40
- One-line primary result: Extracted mechanistic circuits for extractive QA, enabling fast data attribution and improved QA accuracy via model steering.

## Executive Summary
This paper identifies mechanistic circuits in large language models that govern context- versus memory-based reasoning in extractive question-answering tasks. By using probe datasets and causal mediation analysis, the authors isolate context-faithful and memory-faithful circuits and demonstrate that a small set of attention heads reliably attributes data sources. They introduce ATTNATTRIB, a fast, single-head method that achieves state-of-the-art attribution accuracy, and show that using attributions as a prompting signal can improve QA accuracy by up to 9% by reducing hallucinations. The work bridges mechanistic interpretability and practical QA performance.

## Method Summary
The authors use causal mediation analysis on probe datasets to identify context-faithful and memory-faithful circuits in extractive QA models. They observe that a small subset of attention heads in the context circuit performs reliable data attribution by default. Building on this, they design ATTNATTRIB, a fast attribution method that leverages a single attention head for accurate data attribution. They further integrate attribution scores into prompting strategies to steer models toward context faithfulness, demonstrating measurable improvements in QA accuracy.

## Key Results
- Identified context-faithful and memory-faithful circuits via causal mediation analysis.
- ATTNATTRIB achieves state-of-the-art attribution accuracy using just one attention head in a single forward pass.
- Using attributions as an extra prompting signal improves extractive QA accuracy by up to 9%, reducing hallucinations.

## Why This Works (Mechanism)
The approach works because extractive QA models rely on distinct mechanistic circuits for processing context and leveraging parametric memory. The identified context-faithful attention heads perform reliable data attribution by default, allowing for efficient extraction of attribution signals. By steering models with these attributions during prompting, the method encourages context faithfulness, which directly translates to more accurate and less hallucinated answers.

## Foundational Learning
- **Causal mediation analysis**: Needed to disentangle the effects of context versus memory in model decisions; quick check: verify assumptions about mediator variables hold in probe datasets.
- **Attention head attribution**: Critical for identifying which model components drive context versus memory reliance; quick check: confirm attribution stability across similar inputs.
- **Mechanistic interpretability**: Provides the theoretical basis for mapping model behaviors to interpretable circuits; quick check: ensure circuit identification generalizes across datasets.
- **Model steering via prompting**: Enables practical application of mechanistic insights to improve task performance; quick check: test steering signals on out-of-distribution data.
- **Hallucination reduction**: Central to improving real-world QA reliability; quick check: measure hallucination rates before and after attribution-based prompting.
- **Probe dataset design**: Essential for isolating and testing specific model behaviors; quick check: validate probes cover a wide range of context-memory interactions.

## Architecture Onboarding
- **Component map**: Input text -> Attention layers -> Context-faithful heads (for attribution) -> Output layer -> Answer generation; Memory-faithful heads operate in parallel.
- **Critical path**: Input -> Context circuit (small set of heads) -> Attribution -> Prompted inference -> Answer.
- **Design tradeoffs**: Fast, single-head attribution (ATTRNATTRIB) versus multi-head or iterative methodsâ€”speed versus robustness.
- **Failure signatures**: Attribution errors or hallucinations when memory is over-relied upon; circuit misidentification under noisy or ambiguous inputs.
- **First experiments**:
  1. Verify context- versus memory-faithful head identification on a new QA dataset.
  2. Test ATTNATTRIB attribution accuracy on a held-out model family.
  3. Apply attribution-based prompting to a multi-hop reasoning dataset and measure hallucination reduction.

## Open Questions the Paper Calls Out
None provided.

## Limitations
- Generalization of ATTNATTRIB to models with different architectures or training paradigms is unclear.
- The 9% accuracy improvement may be sensitive to dataset specifics, prompting strategy, or model size.
- The analysis focuses on a limited set of attention heads and probe datasets, potentially missing broader failure modes.

## Confidence
- **High confidence**: Identification of context- and memory-faithful circuits via causal mediation analysis; default reliable data attribution by a subset of attention heads.
- **Medium confidence**: Generalization of ATTNATTRIB's attribution accuracy to other model families and tasks; magnitude and stability of the 9% prompting improvement.
- **Low confidence**: Robustness of mechanistic insights to changes in model architecture, training data, or task diversity; assumption of strict context versus memory separation.

## Next Checks
1. Test ATTNATTRIB on models with different transformer architectures (e.g., BERT, RoBERTa, GPT-style) and sizes to assess robustness and generalizability.
2. Conduct ablation studies removing or perturbing the identified context-faithful attention heads to measure impact on both attribution accuracy and downstream QA performance.
3. Evaluate the effectiveness of attribution-based prompting on a broader set of extractive QA datasets, including those with ambiguous or multi-hop reasoning, to determine the limits of the steering approach.