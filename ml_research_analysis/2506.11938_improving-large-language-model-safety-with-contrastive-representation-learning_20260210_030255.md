---
ver: rpa2
title: Improving Large Language Model Safety with Contrastive Representation Learning
arxiv_id: '2506.11938'
source_url: https://arxiv.org/abs/2506.11938
tags:
- triplet
- harmful
- attack
- loss
- circuit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a contrastive representation learning framework
  for improving the robustness of large language models (LLMs) against adversarial
  attacks. The proposed method, termed "triplet defense," formulates model defense
  as a contrastive learning problem by using a triplet-based loss combined with adversarial
  hard negative mining.
---

# Improving Large Language Model Safety with Contrastive Representation Learning

## Quick Facts
- arXiv ID: 2506.11938
- Source URL: https://arxiv.org/abs/2506.11938
- Reference count: 40
- Primary result: Triplet defense reduces LLM adversarial attack success rates from 29% to 5% (embedding attacks) and 14% to 0% (input attacks) without compromising capabilities

## Executive Summary
This paper introduces a contrastive representation learning framework for improving LLM robustness against adversarial attacks. The "triplet defense" formulates model defense as a contrastive learning problem using triplet-based loss combined with adversarial hard negative mining. The method encourages separation between benign and harmful representations in the model's embedding space, achieving significant reductions in attack success rates while preserving general capabilities. Experiments on Llama 3 8B demonstrate superior performance compared to existing representation engineering defenses like circuit breakers and RepBend.

## Method Summary
The triplet defense applies contrastive learning to internal transformer representations at layers 20-31. It uses a triplet loss formulation with mixed L2 and cosine distances to separate benign and harmful representations while preserving benign capabilities through KL regularization. An adversarial attack module generates hard negative examples during training, improving robustness against embedding-space attacks. The method generalizes existing defenses by satisfying four key properties: benign preservation, harmful disruption, representation separation, and harmful clustering. Training uses LoRA adapters on frozen reference models with carefully tuned hyperparameters.

## Key Results
- Embedding attack success rate reduced from 29% to 5% on StrongREJECT benchmark
- Input-space attack success rate reduced from 14% to 0% against REINFORCE-GCG
- GSM8K performance maintained (74.30% vs 75.36% baseline) while RepBend drops to 49.05%
- Improved out-of-distribution generalization with lower Mean Minimum Distance Ratio (MMDR)

## Why This Works (Mechanism)

### Mechanism 1: Triplet Loss for Representation Space Partitioning
The triplet loss creates explicit margins between benign and harmful representations in the embedding space. The loss optimizes four properties: new benign representations stay close to originals (preserving capability), new harmful representations move away from originals (disrupting harmful behavior), benign and harmful representations are pushed apart (enabling separation), and harmful representations cluster together (promoting uniform refusal). The margin-based formulation with mixed distance metrics (L2 + cosine) generalizes existing methods like circuit breakers and RepBend.

### Mechanism 2: Adversarial Hard Negative Mining for Robustness
Training on adversarially-generated harmful representations improves robustness against embedding-space attacks. An adversarial attack module (linear layer) is inserted at a random transformer layer and trained to produce successful harmful completions. These challenging negative examples (30% of harmful data) expose the defense to representations optimized to bypass safety mechanisms, rather than only "easy" harmful representations from standard prompts.

### Mechanism 3: Layer-Specific Representation Manipulation with KL Preservation
Applying triplet loss to representations from layers 20-31 while regularizing benign output distributions preserves general capabilities. The KL divergence term ensures benign logit distributions of the defended model match the original model, constraining representation modifications from degrading the model's ability to generate coherent, helpful responses. This targets deeper layers where semantic representations are more abstract.

## Foundational Learning

- Concept: **Triplet Loss in Contrastive Learning**
  - Why needed here: Core defense mechanism uses triplet loss to structure representation space
  - Quick check question: Given an anchor, positive, and negative sample, what happens to the gradient when d(a,n) - d(a,p) > margin?

- Concept: **Representation Engineering in Transformers**
  - Why needed here: Method manipulates internal activations rather than inputs/outputs
  - Quick check question: In a transformer with L layers, why might deeper layers (e.g., 20-31) be better targets for semantic manipulation than earlier layers?

- Concept: **Adversarial Training for Robustness**
  - Why needed here: Hard negative mining component trains internal attack module
  - Quick check question: In adversarial training, why might alternating between attack and defense updates cause instability, and how can this be mitigated?

## Architecture Onboarding

- Component map: Frozen reference model M provides original representations h_b,i and h_h,i for distance computation -> Trainable defense model M′ produces new representations h′_b,i and h′_h,i -> Attack module Attack_l generates adversarial harmful representations at random layer l -> Distance functions d_bp, d_bn, d_hp, d_hn compute mixed L2 + cosine distances -> Loss aggregator combines L_benign, L_harmful, and L_KL with weights α, β, γ

- Critical path: 1) Sample benign (x_b) and harmful (x_h) batches from D_b and D_h 2) Forward pass through both M and M′ to collect representations at target layers 3) Apply active Attack_l to harmful samples if using adversarial mining 4) Compute mean harmful representation (ĥ′) as positive/negative anchor 5) Compute triplet losses using specified distance functions and margins 6) Compute KL divergence on benign logits 7) Backpropagate combined loss through M′ only

- Design tradeoffs:
  - Margin values (m_b=500, m_h=1500): Larger margins increase separation but may cause instability
  - Adversarial mining frequency: More frequent retraining improves robustness but increases compute cost
  - Layer selection: Layers 20-31 for Llama 3 8B; different architectures may require re-tuning
  - Loss weights (α=0.5, β=0.4, γ=0.9): Higher γ preserves capability but may reduce safety gains

- Failure signatures:
  - Representation collapse: All h′_h,i converge to identical values; check if margin m_h is too large
  - Capability degradation: GSM8K/TruthfulQA scores drop significantly; increase γ or reduce training steps
  - High ASR on specific attack configs: Defense overfits to certain configurations; verify evaluation uses multiple configs
  - Divergent loss: Distance terms become unbounded; ensure margin-based formulation

- First 3 experiments:
  1. Baseline replication on Llama 3 8B: Train Triplet defense (A3 config) and verify ASR reduction matches paper
  2. Ablation of adversarial mining: Train A3 (no adversarial) vs. A4 (with adversarial) on same data split
  3. Layer sensitivity analysis: Vary target layer range and measure both ASR and capability benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
Can the triplet defense framework be effectively scaled to models with hundreds of billions of parameters without prohibitive computational costs?
Basis: Limitations section states training time "may limit the scalability of our approach to much larger models"
Unresolved: Experiments limited to 7B and 8B parameter models; quadratic or linear scaling remains unverified
Evidence needed: Successful application to >70B parameter model demonstrating sustained robustness and manageable training resource consumption

### Open Question 2
Do alternative contrastive losses, such as InfoNCE or N-Pair loss, provide better robustness or capability preservation than the triplet loss?
Basis: Appendix A.2 notes other losses "could enable training with multiple negatives"
Unresolved: Paper focuses exclusively on triplet-based formulation without comparing against other contrastive objectives
Evidence needed: Comparative ablation study showing ASR and benchmark performance for different contrastive losses

### Open Question 3
How does the choice of positive and negative samples (anchors) impact the effectiveness of representation separation?
Basis: Section 4.1 states "Future work should explore the use of other choices of $p_{h,i}$ and $n_{b,i}$"
Unresolved: Current method relies on mean of harmful representations as anchor; paper doesn't test individual examples or cluster centroids
Evidence needed: Experiments evaluating defense's generalization and robustness when using different sampling strategies for loss function

## Limitations

- Computational scalability constraints may limit application to models with hundreds of billions of parameters
- Performance depends on careful hyperparameter tuning, particularly mixing ratio in distance functions and margin values
- Adversarial mining component requires additional computational overhead and may not generalize to all attack patterns

## Confidence

**High Confidence**: Experimental demonstration that triplet defense generalizes across multiple attack configurations and improves upon circuit breakers and RepBend baselines, with well-documented ASR reductions.

**Medium Confidence**: Claim that triplet loss formulation satisfies all four desired properties is theoretically sound but depends on proper hyperparameter tuning, with real-world performance varying by dataset and architecture.

**Low Confidence**: Scalability claims to larger models and assertion that adversarial mining improves robustness against out-of-distribution attacks are based on limited experimental evidence.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary the mixing ratio in dmix distance (0.2/0.8, 0.5/0.5, 0.8/0.2 between L2 and cosine) and measure impact on ASR reduction and capability preservation across GSM8K, MMLU, and TruthfulQA.

2. **Cross-Architecture Transferability**: Apply triplet defense to a different model architecture (Phi-3 or Gemma) with layer range re-tuning and measure whether same hyperparameter values work or require architecture-specific tuning.

3. **Attack Module Ablation**: Train two variants - (a) fixed random linear attack module (no retraining) and (b) trainable attack module with different frequencies (every 10, 30, 60 steps), then compare embedding ASR and input-space ASR against StrongREJECT.