---
ver: rpa2
title: A Novel Hat-Shaped Device-Cloud Collaborative Inference Framework for Large
  Language Models
arxiv_id: '2503.18989'
source_url: https://arxiv.org/abs/2503.18989
tags:
- inference
- delay
- devices
- phase
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes HAT, a device-cloud collaborative inference
  framework for large language models (LLMs) that combines U-shaped inference for
  privacy and speculative decoding for low latency. HAT partitions the LLM into three
  submodels: input and output submodels on the device, and the middle submodel in
  the cloud, along with a lightweight adapter network for speculative decoding.'
---

# A Novel Hat-Shaped Device-Cloud Collaborative Inference Framework for Large Language Models

## Quick Facts
- arXiv ID: 2503.18989
- Source URL: https://arxiv.org/abs/2503.18989
- Reference count: 40
- Key outcome: HAT reduces TTFT by 41-54% and TBT by 41-77% compared to baselines through device-cloud collaboration with U-shaped partitioning and speculative decoding

## Executive Summary
This paper introduces HAT, a device-cloud collaborative inference framework for large language models that addresses the fundamental tension between privacy preservation and low-latency inference. The framework partitions the LLM into three submodels: shallow input layers and output head on the device, middle layers in the cloud, and a lightweight adapter network for speculative decoding. HAT incorporates prompt chunking to overlap communication and computation, and parallel drafting to utilize device idle time during cloud verification. Extensive experiments demonstrate significant latency reductions while maintaining data privacy by keeping raw inputs and final outputs on the device.

## Method Summary
HAT implements a three-part LLM partitioning strategy where shallow layers and the output head reside on the device while middle layers remain in the cloud, connected by a lightweight adapter network trained via knowledge distillation. The adapter enables speculative decoding by generating draft tokens locally, which are verified in a single cloud pass. For long prompts, HAT segments them into chunks, pipelining hidden state transmission with cloud computation to reduce TTFT. The framework also introduces parallel drafting, where the device generates candidate sequences for the next round during cloud verification wait time. The system dynamically adjusts chunk sizes based on real-time device and cloud state monitoring to optimize performance.

## Key Results
- HAT reduces Time-to-First-Token (TTFT) by 41% to 54% compared to baseline methods
- HAT reduces Time-Between-Tokens (TBT) by 41% to 77% compared to baseline methods
- The framework maintains data privacy by keeping raw inputs and final outputs on the device

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating U-shaped partitioning with speculative decoding reduces round-trip latency while preserving data privacy.
- **Mechanism:** The LLM is partitioned into three parts: shallow layers (input) and the head (output) reside on the device, while the middle layers reside in the cloud. An adapter network is trained via knowledge distillation to bridge the device-side layers into a functional Small Language Model (SLM). This SLM drafts multiple tokens locally, which are then verified in a single pass by the cloud-based middle layers.
- **Core assumption:** The lightweight adapter network can sufficiently approximate the semantic distribution of the cloud-based middle layers to maintain a high draft acceptance rate.
- **Evidence anchors:**
  - [abstract]: "HAT partitions the LLM into three submodels... stacked with a lightweight adapter network... to perform speculative decoding."
  - [section 3.4]: Describes the adapter network construction $\Lambda$ and the loss function used for distillation.
  - [corpus]: "Collaboration of Large Language Models and Small Recommendation Models" supports the general viability of device-cloud collaboration, though HAT's specific U-shaped adapter mechanism is distinct.
- **Break condition:** If the draft model's output distribution diverges significantly from the cloud model (low acceptance rate), the overhead of transmitting hidden states for rejected tokens negates the speedup.

### Mechanism 2
- **Claim:** Prompt chunking reduces Time-To-First-Token (TTFT) by pipelining communication and computation.
- **Mechanism:** Long prompts are segmented into smaller chunks. The transmission of a subsequent chunk's hidden states occurs in parallel with the cloud inference of the previous chunk. This overlaps the communication latency ($\frac{X_i \cdot A}{\beta_{up}}$) with computation latency ($g_t(\mu_t)$).
- **Core assumption:** The communication bandwidth for uploading hidden states is a bottleneck comparable to or greater than the per-chunk computation time.
- **Evidence anchors:**
  - [abstract]: "...segments long prompts into shorter chunks, enabling parallel transmission and processing."
  - [section 3.3]: Eq. (3) defines the optimal chunk size calculation to balance transmission and computation.
  - [corpus]: "Synera: Synergistic LLM Serving" discusses cloud-edge serving, but specific evidence for the "transmission-computation overlap" optimization in chunking is primarily derived from the HAT paper text.
- **Break condition:** If the chunk size is set too low, the total accumulated computation time across all chunks exceeds the original one-time transmission delay (fragmentation), increasing TTFT.

### Mechanism 3
- **Claim:** Parallel drafting reduces Time-Between-Tokens (TBT) by utilizing device idle time during cloud verification.
- **Mechanism:** While the device waits for the cloud to verify the current draft sequence and return results, the device preemptively generates candidate draft sequences for the *next* round using top-k candidate tokens from the current step.
- **Core assumption:** The round-trip time (RTT) plus cloud verification time is sufficient for the device to perform useful speculative drafting work.
- **Evidence anchors:**
  - [section 3.5]: "Parallel Drafting Module... enables devices to generate the draft sequence for the next round... during the verification stage."
  - [corpus]: "Token Level Routing Inference System" discusses routing for efficiency, but "parallel drafting during wait time" is a specific strategy detailed in the HAT text.
- **Break condition:** If the network latency is near-zero or the cloud is extremely fast, the device may not finish the parallel draft before the verification result returns, wasting compute resources.

## Foundational Learning

- **Concept:** U-Shaped Split Inference
  - **Why needed here:** This is the architectural baseline for privacy. You must understand that data is not just offloaded; the model itself is bisected to keep raw data and final outputs on the device.
  - **Quick check question:** Why does sending hidden states (intermediate activations) generally provide better privacy than sending raw tokens?

- **Concept:** Speculative Decoding (Draft-then-Verify)
  - **Why needed here:** This is the primary latency reduction engine. You need to distinguish between the "drafting" phase (autoregressive, fast, local) and the "verification" phase (parallel, slow, cloud).
  - **Quick check question:** Why does speculative decoding only work if the draft model's token acceptance rate is sufficiently high?

- **Concept:** Knowledge Distillation
  - **Why needed here:** Essential for understanding how the "Adapter Network" is created. The adapter isn't trained from scratch; it mimics the behavior of the cloud layers to ensure alignment.
  - **Quick check question:** In Eq. (4), why is a combination of Smooth L1 loss and Cross-Entropy loss used instead of just one?

## Architecture Onboarding

- **Component map:** Device (Input Layers + Output Head + Adapter) -> Cloud (Middle Layers) -> Device (Final Output)
- **Critical path:**
  1. **Prefill Phase:** Long Prompt -> [Chunking] -> (Upload Hidden States || Cloud Process Previous) -> First Token.
  2. **Decode Phase:** Input Token -> [Local Draft via Adapter] -> Upload Draft Hidden States -> [Cloud Verify] -> (Return Accepted Tokens || Parallel Draft Next Round).

- **Design tradeoffs:**
  - **Chunk Size:** Small chunks reduce per-burst delay but increase total computation steps (fragmentation). Large chunks saturate bandwidth. The system dynamically solves Eq. (3) to find the equilibrium.
  - **Adapter Capacity:** A larger adapter might mimic the cloud better (higher accuracy) but slows down local drafting (latency).

- **Failure signatures:**
  - **High TTFT with long prompts:** Likely indicates the chunk size is too large, forcing serial transmission-processing. Check the `State Monitoring` module's bandwidth estimates.
  - **Accuracy degradation:** Suggests the Adapter Network is not properly distilled or has drifted from the Cloud Middle Layers.
  - **Low Acceptance Rate:** The draft model is "hallucinating" relative to the verifier, causing the system to revert to standard inference speeds.

- **First 3 experiments:**
  1. **Baseline Latency Profiling:** Measure TTFT and TBT for standard Cloud-Only vs. U-Shaped vs. HAT to isolate the specific gains from the adapter and chunking.
  2. **Chunk Size Sensitivity Analysis:** Sweep chunk sizes (e.g., 32, 64, 128, 256 tokens) on a fixed bandwidth link to validate the theoretical optimal chunk size predicted by Eq. (3).
  3. **Acceptance Rate vs. Draft Length:** Vary the draft sequence length ($n$) and the probability threshold ($\eta$) to find the point where drafting overhead exceeds the verification speedup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the parallel drafting module degrade when the cloud server experiences significant queueing delays, violating the assumption of minimized in-cloud wait time?
- Basis in paper: [explicit] Section 3.5 states that to calculate the number of parallel drafting steps ($\lambda_i$), the system considers the case where "the in-cloud delay of the verification stage is minimized (i.e., no wait time in the cloud)."
- Why unresolved: The formula for parallel drafting (Eq. 6) optimizes resource utilization based on an ideal scenario. It does not account for the accumulation of requests in the cloud scheduler which would delay the verification result and potentially invalidate the predicted drafting window.
- What evidence would resolve it: Experimental results comparing theoretical vs. actual TBT reduction under high request generation rates where cloud queueing is statistically inevitable.

### Open Question 2
- Question: Can the proposed adapter network effectively approximate the middle submodel of LLMs with significantly different architectures, such as Mixture-of-Experts (MoE), without retraining overhead?
- Basis in paper: [inferred] Section 4.1 specifies the evaluation was limited to Vicuna-7B and Vicuna-13B (decoder-only dense transformers).
- Why unresolved: MoE models activate only a subset of weights per token, creating distinct computational and semantic patterns. The current "lightweight adapter network," trained via distillation on dense models, may fail to mimic these dynamics, potentially lowering speculative decoding acceptance rates.
- What evidence would resolve it: Experiments training the HAT adapter for an MoE model (e.g., Mixtral 8x7B) and measuring the accept length and speedup compared to the dense baselines.

### Open Question 3
- Question: How does the prompt chunking mechanism perform under highly volatile network conditions where bandwidth drops significantly during the prefill phase?
- Basis in paper: [inferred] Section 3.2 relies on moving averages (alpha=0.8) of bandwidth to determine the optimal chunk size $X_i$ for pipelining.
- Why unresolved: The optimization assumes relative stability during the calculation-to-execution window. Sudden bandwidth degradation could cause the transmission of "chunked hidden states" to take longer than the in-cloud computation, breaking the overlap and increasing TTFT rather than reducing it.
- What evidence would resolve it: Stress tests involving simulated mobile network fluctuations to measure the discrepancy between the predicted optimal chunk size and the actual required size during transmission.

## Limitations
- The adapter network architecture and training hyperparameters are not precisely specified, making faithful reproduction challenging
- Performance gains are evaluated only on Vicuna models (7B and 13B) with specific datasets, limiting generalizability
- The reported improvements depend heavily on the specific testbed configuration (30 Jetson devices + 8 A6000 GPUs)

## Confidence
- **High Confidence:** The fundamental concept of U-shaped model splitting for privacy preservation is well-established. The integration of speculative decoding with device-cloud collaboration follows established patterns in the literature, and the mathematical formulations for chunk sizing (Eq. 3) and parallel drafting (Eq. 6) are logically sound.
- **Medium Confidence:** The claimed latency improvements are plausible given the mechanisms described, but the exact magnitude depends heavily on the unspecified implementation details. The dynamic chunk sizing approach is theoretically valid but may be sensitive to the accuracy of state monitoring.
- **Low Confidence:** The specific performance numbers (exact TTFT/TBT reductions) cannot be independently verified without access to the complete implementation, training procedures, and the exact experimental setup.

## Next Checks
1. **Adapter Network Verification:** Train the adapter network using the specified distillation approach on ShareGPT data and measure its draft acceptance rate against the full Vicuna middle layers. Verify that the acceptance rate exceeds the break-even threshold where drafting overhead is justified.

2. **Chunk Size Sensitivity Validation:** Implement the dynamic chunk sizing algorithm and conduct controlled experiments varying network bandwidth and device compute capabilities. Validate that the system correctly identifies optimal chunk sizes according to Eq. (3) across different conditions.

3. **End-to-End Latency Profiling:** Measure TTFT and TBT for the complete HAT system versus baseline approaches (U-shaped only, Cloud-only) on a simplified testbed. Focus on isolating the specific contributions of the adapter network, prompt chunking, and parallel drafting to the overall performance gains.