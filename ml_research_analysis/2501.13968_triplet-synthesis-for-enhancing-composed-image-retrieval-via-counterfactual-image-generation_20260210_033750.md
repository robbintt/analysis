---
ver: rpa2
title: Triplet Synthesis For Enhancing Composed Image Retrieval via Counterfactual
  Image Generation
arxiv_id: '2501.13968'
source_url: https://arxiv.org/abs/2501.13968
tags:
- image
- triplets
- images
- retrieval
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel triplet synthesis method for Composed
  Image Retrieval (CIR) by leveraging counterfactual image generation. The method
  addresses the challenge of manually collecting high-quality training triplets, which
  is time-consuming and labor-intensive.
---

# Triplet Synthesis For Enhancing Composed Image Retrieval via Counterfactual Image Generation

## Quick Facts
- arXiv ID: 2501.13968
- Source URL: https://arxiv.org/abs/2501.13968
- Reference count: 0
- The paper proposes a novel triplet synthesis method using counterfactual image generation to enhance CIR model performance, especially in data-scarce scenarios.

## Executive Summary
This paper addresses the challenge of manually collecting high-quality training triplets for Composed Image Retrieval (CIR) by proposing a novel triplet synthesis method using counterfactual image generation. The approach leverages Language-guided Counterfactual Image (LANCE) models to automatically generate diverse training triplets by modifying reference captions and generating corresponding counterfactual images. Experimental results demonstrate significant improvements in CIR model performance, particularly when training data is limited, with up to 40.75% R@1 on CIRR compared to 39.66% without synthetic triplets.

## Method Summary
The method generates synthetic triplets by first using BLIP-2 to create reference captions from reference images, then fine-tuning LLaMA-7B with LoRA to perturb these captions by changing single attributes to create modification text and counterfactual captions. Null-text inversion is applied to obtain latent representations of reference images, which are then edited using Stable Diffusion with prompt-to-prompt editing to generate target images that reflect the caption modifications. These synthetic triplets are combined with original training data to train CIR models like fine-tuned BLIP or Combiner.

## Key Results
- Synthetic triplets significantly improve CIR performance in data-scarce scenarios (30% training data)
- Achieves 40.75% R@1 on CIRR compared to 39.66% without synthetic triplets
- Consistent improvements across both CIRR and FashionIQ datasets
- Ablation studies show synthetic data provides regularization benefits

## Why This Works (Mechanism)

### Mechanism 1: Semantic Isolation via Textual Perturbation
The method creates high-fidelity training signals by ensuring the visual delta between reference and target images corresponds strictly to the modification text. A fine-tuned LLM generates a counterfactual caption by perturbing a single attribute in the reference caption, forcing the downstream image generator to modify only the specific semantic concept defined by the text.

### Mechanism 2: Structural Preservation via Cross-Attention Control
Using Stable Diffusion with Prompt-to-Prompt editing and Null-text Inversion preserves the geometric structure of the reference image. Null-text Inversion maps the reference image to its latent trajectory without relying on the original prompt, while P2P injects cross-attention maps from the reference generation into the counterfactual generation, locking the spatial layout.

### Mechanism 3: Regularization through Synthetic Data Augmentation
Synthetic triplets act as a regularizer in data-scarce scenarios, forcing the CIR model to learn robust, fine-grained feature discrimination rather than memorizing dataset-specific shortcuts. By augmenting a reduced training set with diverse synthetic triplets, the model is exposed to variations it wouldn't otherwise see.

## Foundational Learning

- **Null-text Inversion**: Needed to accurately reconstruct real images for editing; optimizes unconditional embedding separately from conditional embedding to ensure perfect reconstruction as prerequisite for P2P editing.
  - Quick check: Can you explain why we need to optimize the unconditional embedding separately from the conditional embedding when inverting a real image?

- **Cross-Attention Maps in Diffusion**: Essential for understanding how P2P preserves background while changing subject; text tokens correspond to specific spatial regions in latent feature maps.
  - Quick check: If you swap the cross-attention map of the token "red" with "blue" at the generation step, what happens to the shape of the object associated with that color?

- **Composed Image Retrieval (CIR) Triplet Structure**: The entire goal is to generate ⟨I_ref, t, I_target⟩; understanding that the model learns f(I_ref, t) ≈ I_target is necessary to evaluate why "clean" synthetic triplets are valuable.
  - Quick check: In a triplet ⟨White Car, Change to Red, Red Car⟩, if the background also changes from "Road" to "Beach" in the target, how does this confuse the contrastive loss?

## Architecture Onboarding

- **Component map**: Input Image -> BLIP-2 Captioner -> LLaMA-7B Perturber -> Null-text Inverter -> Stable Diffusion Editor -> Target Image -> CIR Model
- **Critical path**: The Inverter → Editor link is critical; if inversion is poor, P2P attention injection will create a target image that looks nothing like the reference structure, breaking the triplet logic.
- **Design tradeoffs**: LLM size vs. perturbation speed; edit strength vs. identity preservation; must tune prompt-to-prompt edit strength to avoid either target looking like reference or ignoring attention map.
- **Failure signatures**: Semantic bleed (modification text changes one thing but generator changes others), subject identity loss (different object instance generated), texture collapse (unrealistic synthetic textures).
- **First 3 experiments**: 1) Inversion Quality Check - run pipeline on 100 images without text change, calculate LPIPS/PSNR between Reference and "Generated" Reference; 2) Triplet Consistency Verification - manually inspect 50 synthetic triplets for Object Identity Change, Background Change, Correct Local Change; 3) Data Scaling Ablation - replicate Figure 4 experiment with 10%, 30%, 50% real data to verify regularization effect.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but leaves several important areas unexplored, particularly regarding scalability limits, complex structural modifications, and bias analysis.

## Limitations
- The method relies heavily on the quality of LLM caption perturbation, which is not thoroughly validated
- Domain transfer from synthetic to real images may introduce artifacts the CIR model learns to exploit
- Limited testing on diverse visual domains beyond CIRR and FashionIQ datasets

## Confidence

**High Confidence**: Experimental methodology is sound with clear datasets, metrics, and baseline comparisons; ablation studies showing data-scarce performance improvements are rigorous.

**Medium Confidence**: Core mechanism claims about semantic isolation and structural preservation are supported by qualitative examples but lack quantitative validation of intermediate steps.

**Low Confidence**: Scalability and generalizability claims are based on limited datasets (5,000-3,000 synthetic triplets) and only tested on two datasets.

## Next Checks

1. **Intermediate Quality Validation**: Implement pipeline to evaluate each synthesis step independently - measure caption perturbation accuracy, image inversion quality (LPIPS between original and reconstructed), and edit fidelity.

2. **Domain Transfer Analysis**: Train CIR models on synthetic-only datasets and measure performance degradation on real test sets compared to real-only data to quantify domain gap.

3. **Cross-Model Generalization**: Replace Stable Diffusion components with alternative diffusion models (SDXL, Kandinsky) and retrain LLM perturbation model on different image-text datasets to measure performance maintenance.