---
ver: rpa2
title: 'Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion,
  and Metric Optimization'
arxiv_id: '2509.09321'
source_url: https://arxiv.org/abs/2509.09321
tags:
- medium
- hard
- easy
- task
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TAM-Bench addresses limitations in existing machine learning benchmarks
  by introducing an automated, web-agent-driven system that constructs a diverse,
  structured benchmark from real-world competition platforms like Kaggle, AIcrowd,
  and Biendata. The benchmark features automated task acquisition and standardization,
  leaderboard-based difficulty modeling, and multi-dimensional evaluation metrics
  that assess performance, constraint compliance, format validity, and task generalization.
---

# Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization

## Quick Facts
- arXiv ID: 2509.09321
- Source URL: https://arxiv.org/abs/2509.09321
- Reference count: 22
- Automated system constructs diverse ML benchmarks from real-world competition platforms

## Executive Summary
TAM-Bench introduces an automated web-agent-driven system for constructing adaptive machine learning benchmarks by harvesting tasks from real-world competition platforms like Kaggle, AIcrowd, and Biendata. The system addresses limitations in existing benchmarks by providing automated task acquisition and standardization, leaderboard-based difficulty modeling, and multi-dimensional evaluation metrics that assess performance, constraint compliance, format validity, and task generalization. Experiments using agents like AIDE and OpenHands demonstrate significant variability in submission success and format compliance, with GPT-4.1-based agents showing superior reliability while DeepSeek-V3 achieved competitive performance on select tasks, including first place on a challenging tabular task.

## Method Summary
TAM-Bench employs web agents to automatically construct diverse ML benchmarks from real-world competition platforms. The system standardizes tasks, implements leaderboard-based difficulty modeling, and introduces multi-dimensional evaluation metrics covering performance, constraint compliance, format validity, and generalization. The benchmark construction process automates task acquisition from platforms like Kaggle, AIcrowd, and Biendata, while the evaluation framework provides comprehensive assessment beyond simple accuracy metrics. The system was validated using 18 tasks from the Lite version with various agent-model combinations.

## Key Results
- GPT-4.1-based agents demonstrated superior reliability in submission success and format compliance
- OpenHands with DeepSeek-V3 achieved first place on a challenging tabular task
- High variability observed across different agent-model combinations in performance outcomes

## Why This Works (Mechanism)
The system leverages web agents to automate the entire benchmark lifecycle, from task discovery to evaluation. By harvesting tasks from active competition platforms, TAM-Bench captures real-world complexity and diversity that traditional benchmarks miss. The multi-dimensional evaluation framework provides nuanced assessment beyond single-metric approaches, while the automated standardization process ensures consistent task representation across heterogeneous sources.

## Foundational Learning
- Web-agent task automation: Enables scalable benchmark construction from dynamic online sources
  - Why needed: Manual benchmark creation is time-consuming and struggles to keep pace with evolving ML challenges
  - Quick check: Verify successful task extraction from multiple competition platforms
- Multi-dimensional evaluation metrics: Assesses performance across multiple axes (performance, constraints, format, generalization)
  - Why needed: Single metrics fail to capture the full spectrum of agent capabilities
  - Quick check: Validate metric coverage against real-world success criteria
- Automated task standardization: Transforms heterogeneous competition tasks into consistent benchmark format
  - Why needed: Competition platforms have varying data formats and evaluation procedures
  - Quick check: Confirm successful conversion of diverse task formats to standardized schema

## Architecture Onboarding
- Component map: Web scraper -> Task processor -> Difficulty modeler -> Evaluation engine -> Benchmark database
- Critical path: Task discovery → Standardization → Difficulty assessment → Agent submission → Multi-dimensional evaluation
- Design tradeoffs: Automation vs. quality control; breadth vs. depth of task coverage; complexity vs. interpretability of evaluation metrics
- Failure signatures: Incomplete task extraction, inconsistent standardization, difficulty misestimation, evaluation metric misalignment
- First experiments: 1) Validate task extraction from single platform, 2) Test standardization pipeline on heterogeneous tasks, 3) Run evaluation on simple agent submissions

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark construction relies on publicly available competition platforms, potentially missing specialized or emerging domains
- Evaluation framework may not capture all relevant aspects of agent capability, such as long-term learning adaptation
- Experimental scope limited to 18 tasks in Lite version, constraining statistical power

## Confidence
- Benchmark construction methodology: High
- Multi-dimensional evaluation framework: Medium
- Agent performance conclusions: Low to Medium

## Next Checks
1. Expand benchmark to include at least 50 diverse tasks across multiple domains to improve statistical robustness
2. Conduct longitudinal studies tracking agent performance over time to assess learning adaptation
3. Implement ablation studies to determine relative contribution of different evaluation dimensions