---
ver: rpa2
title: Zero-Shot 3D Visual Grounding from Vision-Language Models
arxiv_id: '2505.22429'
source_url: https://arxiv.org/abs/2505.22429
tags:
- visual
- spatial
- object
- arxiv
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SeeGround, a zero-shot 3D visual grounding
  framework that leverages 2D vision-language models (VLMs) without requiring 3D-specific
  training. The key innovation is reformulating 3D scenes into query-aligned rendered
  views combined with spatially enriched textual descriptions to bridge the modality
  gap.
---

# Zero-Shot 3D Visual Grounding from Vision-Language Models

## Quick Facts
- arXiv ID: 2505.22429
- Source URL: https://arxiv.org/abs/2505.22429
- Authors: Rong Li; Shijie Li; Lingdong Kong; Xulei Yang; Junwei Liang
- Reference count: 40
- Primary result: Achieves state-of-the-art zero-shot 3D visual grounding, improving over prior methods by 7.7% on ScanRefer and 7.1% on Nr3D benchmarks.

## Executive Summary
This paper introduces SeeGround, a zero-shot 3D visual grounding framework that leverages 2D vision-language models without requiring 3D-specific training. The key innovation is reformulating 3D scenes into query-aligned rendered views combined with spatially enriched textual descriptions to bridge the modality gap. By dynamically selecting optimal viewpoints and integrating visual and spatial signals, SeeGround achieves state-of-the-art zero-shot results while narrowing the gap with fully supervised alternatives.

## Method Summary
SeeGround reformulates 3D scenes for 2D VLMs by pre-computing an Object Lookup Table (OLT) with detected objects and their 3D bounding boxes, then dynamically rendering query-aligned 2D views with depth-aware visual prompting. The framework consists of three components: (1) OLT population from an open-vocabulary 3D detector, (2) Perspective Adaptation Module for query-driven viewpoint selection, and (3) Fusion Alignment Module for depth-aware visual prompting. During inference, the system parses queries to identify anchors, renders optimal views, projects 3D boxes to 2D with occlusion filtering, and prompts the VLM with rendered images, spatial text, and query to predict target objects.

## Key Results
- Achieves 46.1% overall accuracy on ScanRefer, a 7.7% improvement over prior zero-shot methods
- Outperforms previous approaches by 7.1% on Nr3D benchmark
- Narrows the gap with fully supervised methods (48.2% vs 57.6% on ScanRefer)
- Shows strong generalization on challenging queries with ambiguous or partial language inputs

## Why This Works (Mechanism)

### Mechanism 1: Query-Aligned Dynamic Rendering
The Perspective Adaptation Module (PAM) parses queries to identify anchor objects, then positions a virtual camera facing that anchor with backward and upward offsets. This captures both target detail and surrounding spatial context in a single rendered view. Evidence shows query-aligned strategy achieves 46.1% overall vs. 43.3% for Bird's Eye View, with +4.4% on Hard and +5.7% on View-Dependent queries.

### Mechanism 2: Depth-Aware Visual Prompting for Cross-Modal Alignment
The Fusion Alignment Module projects 3D bounding boxes onto rendered 2D images, compares projected depth against rendered depth maps to filter occluded objects, and places visual prompts at visible object centers. This creates direct binding between image regions and spatial descriptions. Adding FAM improves accuracy from 39.5% to 43.3%, a +3.8% gain.

### Mechanism 3: Hybrid Representation with Object Lookup Table
An Object Lookup Table (OLT) pre-computes and stores all detected objects with their 3D bounding boxes and semantic labels as natural language entries. During inference, the OLT provides spatial descriptions while the rendered image provides texture, color, and shape cues. Using coordinates alone yields 37.7%; adding layout yields 39.7%; adding texture yields 39.5%; full system achieves 46.1%.

## Foundational Learning

- **3D-to-2D Camera Projection**: Understanding how 3D world coordinates map to 2D image pixels via camera intrinsics/extrinsics is essential for interpreting the rendering and visual prompting pipeline. Quick check: Given a 3D point at (1.0, 2.0, 3.0) and a camera with focal length 500px positioned at origin looking down +Z, what are the approximate 2D pixel coordinates?

- **Vision-Language Model Prompting**: The entire framework relies on few-shot prompting to make VLMs parse queries, identify anchors, and predict target objects—prompt design directly affects performance. Quick check: What is the difference between a system prompt, few-shot examples, and a task query in VLM inference?

- **Open-Vocabulary 3D Detection**: The OLT foundation depends on Mask3D or similar detectors; understanding their outputs (bounding boxes, confidence scores, class labels) is necessary for debugging grounding failures. Quick check: How does an open-vocabulary detector differ from a closed-set detector in terms of class taxonomy and output structure?

## Architecture Onboarding

- **Component map**: Query → [LLM Parser] → Anchor + Candidates → [Perspective Adaptation] → Camera Pose → [Renderer] → 2D Image → [Depth-Aware Projection] → Visible Object IDs + Positions → [VLM] → Predicted Object ID → OLT Lookup → 3D Bounding Box Output

- **Critical path**: The OLT population (Step 1, run once per scene) must complete before any queries. Per-query inference flows: parse → select viewpoint → render → project → prompt VLM → lookup result. Latency bottleneck is typically VLM inference.

- **Design tradeoffs**: Rendering resolution (1000×1000 used) vs. VLM context window and inference speed; Mask3D detection quality vs. runtime (GT boxes yield 59.5%, Mask3D yields 44.1%); Static fallback viewpoints vs. query-aligned computation overhead.

- **Failure signatures**: Spatial relation errors (19% of failures) where model misinterprets "next to," "behind"; Egocentric reference failures like "when facing the door" requiring viewer perspective modeling; Detection propagation where missing OLT entries cause ungroundable targets.

- **First 3 experiments**: 1) Run on 10 scenes with Ground Truth OLT to confirm the 59.5% upper bound is reproducible; isolate VLM reasoning from detector errors. 2) Compare accuracy on "Unique" vs. "Multiple" splits and "Easy" vs. "Hard" (distractor count) to identify where visual prompting provides most value. 3) Replace Mask3D with OVIR-3D on the same scenes; reproduce the 30.7% vs. 44.1% comparison to understand which object categories drive the gap.

## Open Questions the Paper Calls Out

### Open Question 1
To what extent does high-fidelity rendering improve object discrimination in training-free 3D visual grounding compared to raw point cloud rendering? The authors note that limited rendering quality hampers object discrimination and suggest future work may incorporate high-fidelity rendering. A comparative study replacing raw point cloud rendering with high-fidelity neural rendering (e.g., NeRF or 3D Gaussian Splatting) on the same benchmarks would resolve this.

### Open Question 2
How can the Perspective Adaptation Module be enhanced to robustly handle complex egocentric references without 3D-specific training? The current viewpoint selection struggles with complex egocentric references (e.g., "when the window is on the left", "upon entering from the door"). Development of a prompt-engineering or geometric heuristic that successfully maps egocentric spatial language to global camera coordinates, validated on view-dependent subsets of Nr3D, would address this.

### Open Question 3
Can dedicated spatial reasoning modules be integrated into the Fusion Alignment Module to reduce the 19% spatial relation error rate observed in zero-shot settings? The error analysis reveals that spatial relation errors remain frequent (19%), suggesting limitations in fine-grained reasoning that could be addressed by dedicated spatial modules. An ablation study integrating a lightweight, training-free geometric reasoning module (e.g., explicit graph-based distance calculation) into the fusion process would provide evidence.

## Limitations

- **Detector Quality Ceiling**: Performance is fundamentally bottlenecked by the open-vocabulary detector quality, with a 15-point gap between Ground Truth boxes (59.5%) and Mask3D (44.1%) on ScanRefer.
- **Egocentric Reference Handling**: The system struggles with viewpoint-dependent queries like "the chair when the window is on the left" because the Perspective Adaptation Module relies on scene anchors rather than modeling viewer orientation.
- **Prompt Template Sensitivity**: Without exact few-shot prompt formats for anchor parsing and VLM querying, it's unclear how robust the method is to prompt variations.

## Confidence

- **High Confidence**: The core architectural innovation of query-aligned dynamic rendering is well-supported by ablation evidence (46.1% vs 43.3% for Bird's Eye View). The depth-aware visual prompting concept is validated by the +3.8% improvement when adding FAM. The hybrid representation approach is confirmed by progressive gains from coordinates → layout → texture → full system.
- **Medium Confidence**: The 7.7% and 7.1% zero-shot improvements over existing methods are credible given controlled comparisons, but depend on exact prompt templates and detector settings used. The claim of narrowing the gap with supervised methods (48.2% vs 57.6% on ScanRefer) is reasonable but assumes comparable detector quality.
- **Low Confidence**: Claims about robustness to missing anchors in Fig. 6 are supported by qualitative examples but lack systematic evaluation across failure modes. The performance on "View-Dependent" queries (33.2%) versus "View-Independent" (49.3%) suggests viewpoint selection helps, but the exact contribution of dynamic rendering versus other components isn't fully isolated.

## Next Checks

1. **Detector Quality Impact Study**: Run SeeGround on ScanRefer using both Ground Truth boxes and Mask3D across identical query sets to quantify the exact performance penalty from detection errors. This will establish the realistic ceiling for any future detector improvements.

2. **Prompt Template Sensitivity Analysis**: Systematically vary the few-shot prompt templates for anchor parsing and VLM querying while holding all else constant. Measure performance variance to establish robustness to prompt engineering, which is critical for practical deployment.

3. **Egocentric Reference Benchmark**: Create a curated test set of 50-100 queries requiring viewpoint-dependent reasoning (e.g., "the object to your right when facing the sofa"). Evaluate SeeGround's current limitations and identify whether extending PAM to model viewer orientation is feasible within the existing framework.