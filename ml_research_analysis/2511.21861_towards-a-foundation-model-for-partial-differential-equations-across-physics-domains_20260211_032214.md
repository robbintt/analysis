---
ver: rpa2
title: Towards a Foundation Model for Partial Differential Equations Across Physics
  Domains
arxiv_id: '2511.21861'
source_url: https://arxiv.org/abs/2511.21861
tags:
- pde-fm
- across
- datasets
- arxiv
- spectral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PDE-FM, a foundation model for physics-informed
  machine learning that unifies spatial, spectral, and temporal reasoning across heterogeneous
  partial differential equation (PDE) systems. The core method combines spatial-spectral
  tokenization, physics-aware conditioning, and a Mamba-based state-space backbone
  with an operator-theoretic decoder, enabling scalable and data-efficient modeling
  of complex physical dynamics.
---

# Towards a Foundation Model for Partial Differential Equations Across Physics Domains

## Quick Facts
- arXiv ID: 2511.21861
- Source URL: https://arxiv.org/abs/2511.21861
- Reference count: 6
- Key result: 46% reduction in mean VRMSE on 6+ datasets compared to prior operator-learning baselines

## Executive Summary
This paper introduces PDE-FM, a foundation model for physics-informed machine learning that unifies spatial, spectral, and temporal reasoning across heterogeneous partial differential equation systems. The core method combines spatial-spectral tokenization, physics-aware conditioning, and a Mamba-based state-space backbone with an operator-theoretic decoder, enabling scalable and data-efficient modeling of complex physical dynamics. Evaluated on twelve 2D and 3D datasets from The Well benchmark spanning hydrodynamic, radiative, elastic, and astrophysical phenomena, PDE-FM achieves state-of-the-art accuracy in six domains, reducing mean VRMSE by 46% relative to prior operator-learning baselines.

## Method Summary
PDE-FM combines a dual spatial-spectral tokenizer with a Mamba backbone and FNO decoder, trained on diverse PDE datasets with physics-aware FiLM conditioning. The model uses dataset-specific adapters to normalize heterogeneous inputs to a shared latent space, then processes them through spatial and spectral tokens fused via cross-attention. The Mamba backbone provides efficient long-range temporal modeling, while the FNO decoder biases reconstructions toward spectral smoothness. Training employs temperature-scaled curriculum sampling and dual spatial-spectral losses to prevent negative transfer and overfitting.

## Key Results
- Achieves state-of-the-art VRMSE on six datasets including Rayleigh-Benard convection and compressible turbulence
- Demonstrates 46% reduction in mean VRMSE compared to prior operator-learning baselines
- Shows robust cross-physics generalization, excelling in turbulent and radiative systems while maintaining strong performance in linear and steady-state regimes

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Spectral-State-Space Processing
The model encodes local spatial information via patch convolutions and global smoothness priors via a dedicated spectral token (from a truncated FFT). These are fused via cross-attention. The combined tokens are then processed by a Mamba backbone, which models long-range temporal dependencies in linear time. The final output is decoded using a shallow FNO layer, which biases the reconstruction toward spectral smoothness. This works because physical fields can be effectively represented by a combination of local spatial features and global spectral coefficients, and a recurrent model can capture their temporal evolution without the quadratic cost of attention.

### Mechanism 2: Physics-Aware Modular Conditioning
Physics metadata (boundary conditions, constitutive parameters, time grids) is injected into the model via FiLM layers on the spatial tokens. Dataset-specific "adapters" (small 1x1 convolutions) map heterogeneous input channels to a shared latent space, allowing the main backbone to be trained on a unified representation. This works because the core dynamics of disparate physical systems share a common latent representation that can be modulated by physics-specific metadata, and dataset-specific characteristics can be effectively normalized by lightweight adapters.

### Mechanism 3: Multi-Domain Curriculum Pretraining
The model is trained on 12 different datasets using a dual spatial-spectral loss (VRMSE + Spectral L2). Temperature scaling and difficulty-aware weighting prevent negative transfer and overfitting. This curriculum encourages the model to learn a shared representation of dynamics across different physical phenomena. This works because there are transferable physical invariants and structures (like conservation laws, incompressibility) that are shared across different PDE families, and a diverse training mix enables the model to learn them.

## Foundational Learning

- **Neural Operators (FNO)**: Understanding how FNOs learn mappings between function spaces using spectral convolutions is essential for grasping the model's output mechanism. Quick check: Can you explain how an FNO performs a convolution in the spectral domain and why this is efficient for learning solution operators to PDEs?

- **State Space Models (Mamba)**: The core backbone of PDE-FM is Mamba, a selective state-space model. Understanding its advantages over Transformers (linear time complexity, efficient recurrent processing) is key to appreciating the architecture's scalability. Quick check: How does Mamba achieve linear time complexity for sequence modeling compared to the quadratic complexity of standard self-attention in Transformers?

- **Physics-Informed Machine Learning (PIML) and PDEs**: The entire problem domain is solving PDEs. A basic grasp of different PDE types (elliptic, parabolic, hyperbolic), their solutions, and concepts like boundary conditions and conservation laws is non-negotiable. Quick check: What is a "stiff" PDE, and what kind of physical systems often exhibit stiffness?

## Architecture Onboarding

- **Component map**: Data -> Adapter (to latent space) -> Dual Tokenizer (spatial/spectral) -> Physics Encoder (FiLM + Cross-Attention) -> Mamba Backbone -> FNO Decoder -> Adapter (to output space) -> Loss Calculation

- **Critical path**: The model processes data through dataset-specific adapters to a shared latent space, creates dual spatial-spectral tokens, fuses them with physics metadata using FiLM and cross-attention, processes through the Mamba backbone for temporal modeling, and reconstructs using an FNO decoder.

- **Design tradeoffs**: The model trades full spectral resolution for computational efficiency by using a single spectral token. Mamba offers linear scaling for long sequences but may lack explicit global context modeling. The unified foundation approach may sacrifice peak performance on single domains compared to specialized models.

- **Failure signatures**: Poor performance on viscoelastic/long-term memory tasks (VRMSE >0.5 on viscoelastic_instability) indicates Mamba backbone limitations. Blurring or loss of fine-scale features suggests spectral token truncation or FNO decoder insufficiency.

- **First 3 experiments**: 1) Ablate the Tokenizer: Run a baseline with only spatial tokens to quantify spectral token contribution. 2) Backbone Swap: Replace Mamba with Transformer on Rayleigh-Benard convection to measure performance vs. efficiency tradeoff. 3) Cross-Domain Zero-Shot Transfer: Train on all datasets except shear_flow, then evaluate zero-shot performance to test foundation capability.

## Open Questions the Paper Calls Out

1. Can the integration of explicit physical inductive biases or latent memory mechanisms bridge the performance gap in elasticity-dominated systems? (Basis: Conclusion states viscoelastic systems require explicit physical inductive biases beyond current architecture)

2. To what extent does integrating conservation-based and energy-preserving loss regularization improve stability during long-term temporal rollouts? (Basis: Authors list this as primary avenue to improve stability across long rollouts)

3. Can adaptive spectral decoders that dynamically allocate resolution across spatial scales enhance efficiency without sacrificing accuracy? (Basis: Conclusion suggests developing adaptive spectral decoders as key future direction)

## Limitations
- Underperforms on viscoelastic systems requiring long-term memory, with VRMSE 0.52 vs 0.25 for convolutional baselines
- Spectral token truncation may limit high-frequency detail preservation in certain physical regimes
- Claims about true foundation model capability are overstated given explicit limitations on elastic/viscous memory problems

## Confidence
- **High confidence**: Hybrid spatial-spectral + Mamba+FNO architecture achieves reported VRMSE reductions on benchmark datasets; physics-aware conditioning through FiLM layers is well-established
- **Medium confidence**: Cross-physics generalization claims depend on specific dataset composition and sampling strategy
- **Low confidence**: Claims about serving as foundation model for all PDE systems are overstated given limitations on elastic systems

## Next Checks
1. Evaluate PDE-FM on viscoelastic instability dataset with extended temporal horizons to quantify Mamba backbone's memory limitations
2. Compute and compare spectral energy distribution between model predictions and ground truth across frequency bands to identify systematic high-frequency information loss
3. Train PDE-FM on all datasets except a new PDE system (e.g., magnetohydrodynamics), then measure performance degradation to assess true generalization beyond training corpus