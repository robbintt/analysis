---
ver: rpa2
title: Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning
arxiv_id: '2507.15788'
source_url: https://arxiv.org/abs/2507.15788
tags:
- training
- tasks
- reasoning
- performance
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether small-scale LLMs can develop a generalizable
  Theory of Mind (ToM) through reinforcement learning with verifiable rewards (RLVR).
  The authors systematically train models on combinations of three prominent ToM datasets
  (HiToM, ExploreToM, FANToM) and evaluate their zero-shot performance on held-out
  benchmarks (OpenToM, FANToM List, and higher-order HiToM tasks).
---

# Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2507.15788
- **Source URL**: https://arxiv.org/abs/2507.15788
- **Reference count**: 5
- **Primary result**: Small LLMs (7B parameters) trained on ToM datasets via RLVR fail to generalize to out-of-distribution ToM tasks, remaining at baseline accuracy (~60%)

## Executive Summary
This paper investigates whether small-scale LLMs can develop generalizable Theory of Mind (ToM) capabilities through reinforcement learning with verifiable rewards (RLVR). The authors systematically train models on combinations of three prominent ToM datasets and evaluate their zero-shot performance on held-out benchmarks. Despite significant improvements on in-distribution tasks, the models fail to generalize to out-of-distribution ToM tasks, with performance remaining at baseline levels. The results demonstrate that small LLMs struggle to develop generic ToM capabilities that transfer beyond their training distribution, challenging the effectiveness of RLVR for instilling generalizable social reasoning in smaller models.

## Method Summary
The study trains 7B parameter LLMs using reinforcement learning with verifiable rewards on combinations of three ToM datasets (HiToM, ExploreToM, FANToM). Models are evaluated using zero-shot testing on held-out benchmarks including OpenToM, FANToM List, and higher-order HiToM tasks. The evaluation measures both in-distribution performance (tasks similar to training data) and out-of-distribution generalization (zero-shot performance on unseen ToM tasks). The authors track accuracy changes over training iterations to identify patterns of statistical hacking and negative transfer.

## Key Results
- RLVR training yields significant in-distribution improvements (up to 65% accuracy gains on FANToM)
- Zero-shot generalization to out-of-distribution ToM tasks remains at baseline levels (~60% accuracy)
- Prolonged RL training leads to statistical hacking, causing inverted difficulty curves and negative transfer on varied-order reasoning tasks
- The learned behavior represents narrow overfitting rather than acquisition of abstract ToM capability

## Why This Works (Mechanism)
The paper demonstrates that small LLMs exploit statistical patterns in training data rather than learning abstract social reasoning principles. When trained on synthetic ToM tasks, models develop narrow heuristics that work well on similar in-distribution examples but fail catastrophically on novel task structures. The verifiable reward mechanism, while effective for correct answers on seen patterns, cannot distinguish between genuine reasoning and pattern matching. This leads to degradation of generalization ability over time as models increasingly optimize for exploiting dataset artifacts rather than developing transferable social intelligence.

## Foundational Learning
- **Theory of Mind (ToM)**: Understanding that others have beliefs, desires, and intentions different from one's own
  - *Why needed*: Core concept being evaluated - models' ability to reason about others' mental states
  - *Quick check*: Can the model predict character actions based on their false beliefs?
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Training paradigm using rewards for correct answers on structured tasks
  - *Why needed*: Primary methodology being evaluated for ToM capability development
  - *Quick check*: Does reward signal capture reasoning quality or just answer correctness?
- **Zero-shot generalization**: Performance on unseen tasks without task-specific adaptation
  - *Why needed*: Key evaluation metric for assessing learned capability transferability
  - *Quick check*: How does model perform on held-out ToM benchmarks it never saw during training?
- **Statistical hacking**: Exploiting dataset patterns rather than learning underlying principles
  - *Why needed*: Explains failure mode observed in RLVR training
  - *Quick check*: Do accuracy improvements correlate with task similarity to training data?
- **Negative transfer**: Performance degradation on certain tasks due to training
  - *Why needed*: Indicates harmful overfitting to training distribution
  - *Quick check*: Are there tasks where baseline performance exceeds trained model performance?

## Architecture Onboarding

**Component Map**: RLVR Training -> In-Distribution Evaluation -> OOD Zero-shot Evaluation -> Generalization Analysis

**Critical Path**: Dataset Training Combinations → Model Training (RLVR) → Zero-shot Testing (OOD) → Generalization Assessment

**Design Tradeoffs**: Simple verifiable rewards vs. complex reasoning assessment; zero-shot evaluation vs. fine-tuning potential; synthetic benchmarks vs. naturalistic tasks

**Failure Signatures**: 
- In-distribution accuracy improvement with no OOD generalization gain
- Inverted difficulty curves (easy tasks become harder than difficult ones)
- Negative transfer on certain task types
- Performance degradation on higher-order reasoning tasks

**Three First Experiments**:
1. Train on HiToM only, evaluate on OpenToM to test single-dataset generalization
2. Train on combined datasets, evaluate on same datasets (in-distribution control)
3. Train on ExploreToM + FANToM, evaluate on OpenToM to test cross-dataset generalization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Would RLVR instill generalizable ToM capabilities in larger-scale LLMs (e.g., 70B+ parameters), or is the failure to generalize fundamental to the RLVR approach for social reasoning?
- **Basis in paper**: The authors limit their investigation to "small-scale LLMs" (7B parameters), explicitly stating they "investigate whether small-scale LLMs can acquire a robust and generalizable ToM capability through RLVR."
- **Why unresolved**: The paper deliberately restricts its scope to 7B models. Larger models may have greater capacity for abstract representation or may be less prone to statistical hacking, but this remains untested.
- **What evidence would resolve it**: Training and evaluating larger models (70B-200B+ parameters) using the same RLVR methodology on ToM datasets, measuring both in-distribution and OOD generalization.

### Open Question 2
- **Question**: Could reward mechanisms that assess reasoning fidelity—rather than just final-answer correctness—prevent the statistical hacking behavior observed with simple verifiable rewards?
- **Basis in paper**: The conclusion states that developing socially intelligent AI "will require advancements beyond optimizing for correct answers on existing benchmarks, potentially involving...novel reward mechanisms that can assess the fidelity of the reasoning process itself."
- **Why unresolved**: The current reward function only evaluates format compliance and answer correctness, which the authors show leads to exploitation of dataset artifacts without genuine reasoning.
- **What evidence would resolve it**: Designing process-based rewards (e.g., evaluating intermediate belief-state representations, consistency checks across perturbed inputs) and comparing generalization behavior against standard RLVR.

### Open Question 3
- **Question**: Why does RLVR successfully yield generalizable capabilities in logical/mathematical reasoning but fail for social reasoning—is this due to inherent differences between the domains or artifacts in current ToM benchmarks?
- **Basis in paper**: The discussion notes "This outcome contrasts sharply with findings in the logical reasoning domain...suggesting that the ambiguity and contextual nuance inherent to social reasoning tasks may make their benchmarks more susceptible to this kind of statistical exploitation via RL."
- **Why unresolved**: The paper identifies the discrepancy but does not disentangle whether the failure stems from ToM tasks being fundamentally less structured, or from deficiencies in how current ToM benchmarks are constructed (e.g., templated stories, predictable patterns).
- **What evidence would resolve it**: Controlled experiments using synthetic ToM-like tasks with varying degrees of structure and ambiguity, or analysis of whether logical-reasoning benchmarks also contain exploitable artifacts that larger models avoid due to capacity rather than genuine reasoning.

## Limitations
- Study focuses exclusively on small-scale models (7B parameters), limiting generalizability to larger LLMs
- Evaluation relies on synthetic benchmarks that may not fully capture real-world Theory of Mind complexity
- Zero-shot evaluation methodology may underestimate potential transfer if few-shot prompting could bridge generalization gap

## Confidence

**High confidence**: Small LLMs fail to generalize ToM from RLVR training on synthetic datasets
- Empirical results show clear baseline-level OOD performance despite in-distribution improvements

**Medium confidence**: This represents inherent limitations rather than optimization artifacts
- Statistical hacking mechanism is well-demonstrated, but alternative training approaches remain untested

**Low confidence**: Extrapolating results to larger models or real-world ToM applications
- Scaling effects and naturalistic task performance are not evaluated in current study

## Next Checks
1. Replicate experiments with larger model scales (7B+ parameters) to assess scaling effects on ToM generalization
2. Test few-shot adaptation on out-of-distribution ToM tasks to distinguish between complete failure and zero-shot limitations
3. Evaluate models on naturalistic social reasoning tasks beyond synthetic benchmarks to assess real-world applicability