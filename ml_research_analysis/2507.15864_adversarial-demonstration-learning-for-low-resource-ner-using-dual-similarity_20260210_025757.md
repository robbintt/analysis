---
ver: rpa2
title: Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity
arxiv_id: '2507.15864'
source_url: https://arxiv.org/abs/2507.15864
tags:
- demonstration
- similarity
- input
- examples
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of named entity recognition
  (NER) in low-resource scenarios, where limited labeled data makes training effective
  models difficult. The authors identify two key issues in existing demonstration-based
  learning approaches: first, reliance on semantic similarity alone for selecting
  demonstration examples, which overlooks feature similarity; second, the tendency
  of models to ignore demonstration examples during training.'
---

# Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity

## Quick Facts
- **arXiv ID**: 2507.15864
- **Source URL**: https://arxiv.org/abs/2507.15864
- **Reference count**: 9
- **Primary result**: Achieves 41.92% F1 on German legal dataset (5-shot) vs 35.73% for baseline

## Executive Summary
This paper addresses the challenge of named entity recognition (NER) in low-resource scenarios by proposing Adversarial Demonstration Learning (ADL). The method combines dual similarity selection (semantic + feature similarity) with adversarial training to force models to attend to demonstration examples. Experiments across three datasets show consistent improvements over existing approaches, with particularly strong results in the most challenging 5-shot setting.

## Method Summary
The method uses StructShot as a base architecture and introduces two key innovations: dual similarity for demonstration selection and adversarial demonstration learning. Dual similarity combines semantic similarity (BERT embeddings) with feature similarity (trained cross-encoder predicting entity type overlap). The adversarial component perturbs demonstration labels during training to force the model to rely on demonstrations rather than prior knowledge. At inference, predictions are made via majority voting over multiple demonstrated inputs.

## Key Results
- A DELL achieves 41.92% F1 on German legal dataset (5-shot), outperforming DDSS baseline at 35.73%
- Consistent improvements across all three datasets and few-shot settings (5, 10, 20 examples per entity type)
- Ablation studies confirm both dual similarity and adversarial training contribute significantly to performance gains
- Ensemble prediction via majority voting reduces variance and improves accuracy

## Why This Works (Mechanism)

### Mechanism 1: Dual Similarity for Improved Demonstration Selection
Traditional methods select demonstrations based on semantic similarity alone, which can miss examples with similar entity types but different contexts. Dual similarity combines semantic similarity with feature similarity - the overlap in entity types between examples. This ensures demonstrations provide relevant "templates" for the types of entities present in the input, improving the model's ability to learn new input-label mappings.

### Mechanism 2: Adversarial Demonstration Learning (ADL)
Standard demonstration learning often fails because models ignore demonstrations and rely on pre-trained knowledge instead. ADL creates adversarial training samples by perturbing demonstration labels (e.g., swapping PER and LOC). This forces the model to attend to demonstrations since predicting correctly requires overriding its pre-trained priors and following the demonstration's "rule."

### Mechanism 3: Ensemble Prediction via Majority Voting
Random sampling of demonstrations introduces variance. By generating multiple demonstrations for each input and aggregating predictions through majority voting, the method reduces noise and improves final output accuracy, particularly when individual demonstration selections might be suboptimal.

## Foundational Learning

- **Named Entity Recognition (NER)**: Identifying and classifying named entities (people, organizations, locations) in text. Why needed: Core task being improved. Quick check: In "Apple Inc. is based in Cupertino," what are the entities and their types?

- **Demonstration Learning / In-Context Learning**: Providing labeled examples directly in the input to guide model behavior without updating weights. Why needed: Foundation for the proposed method. Quick check: How does this differ from traditional supervised fine-tuning?

- **Pre-trained Language Models (e.g., BERT)**: Models with extensive prior knowledge from pre-training. Why needed: Understanding why adversarial training is necessary - it forces models to override strong pre-trained priors. Quick check: What knowledge do models like BERT acquire during pre-training?

## Architecture Onboarding

- **Component map**: Demonstration Incorporator -> Model (BERT + classifier) -> Adversarial Training Module
- **Critical path**: The training loop with ADL is crucial - it involves creating perturbed demonstrations, running them through the model, and computing a loss that incentivizes following the perturbed rule.
- **Design tradeoffs**: 
  - Feature similarity predictor adds upfront cost vs simple semantic similarity
  - Larger ensemble size (k) improves accuracy but increases latency
  - More complex perturbation schemes could improve attention but may destabilize training
- **Failure signatures**: 
  - Model ignores demonstrations if predictions don't change with contradictory examples
  - Poor feature similarity scores indicate bad demonstration selection
  - Low performance on rare entities suggests demo pool limitations
- **First 3 experiments**: 
  1. Baseline comparison against DDSS on CoNLL03 and German legal datasets
  2. ADL ablation study - train without adversarial component and test demonstration attention
  3. Feature similarity predictor evaluation using Pearson correlation and accuracy metrics

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Missing key hyperparameters (γ, α, β values, BERT variant, ensemble size k) determined via grid search
- Underspecified feature similarity predictor training procedure
- Limited evaluation to NER tasks only, generalization to other domains unclear
- High variance in results (±10.48 F1 for 5-shot CoNLL) suggests potential instability

## Confidence
- **High Confidence**: Core mechanisms are logically coherent and ablation studies clearly demonstrate component contributions
- **Medium Confidence**: Empirical results may be sensitive to unspecified hyperparameters; variance suggests potential instability
- **Low Confidence**: Limited to three NER datasets, effectiveness on other tasks remains untested

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary γ, α, and β to identify optimal configurations and assess robustness
2. **Feature Similarity Predictor Quality**: Evaluate Pearson correlation on held-out test set to verify meaningful feature similarity learning
3. **Alternative Perturbation Schemes**: Test more complex adversarial perturbations beyond simple label swapping to assess attention improvements