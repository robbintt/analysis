---
ver: rpa2
title: Interactive Information Need Prediction with Intent and Context
arxiv_id: '2501.02635'
source_url: https://arxiv.org/abs/2501.02635
tags:
- context
- intent
- information
- source
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for interactive information
  need prediction, allowing users to select pre-search context (e.g., paragraph, sentence,
  or word) and optionally specify partial search intent (e.g., "how", "why", "applications").
  The authors study both explicit prediction through question generation and implicit
  prediction through answer retrieval.
---

# Interactive Information Need Prediction with Intent and Context

## Quick Facts
- **arXiv ID:** 2501.02635
- **Source URL:** https://arxiv.org/abs/2501.02635
- **Reference count:** 40
- **Primary result:** Interactive framework with user-provided context selection and partial search intent improves information need prediction performance

## Executive Summary
This paper introduces an interactive framework for predicting user information needs by allowing users to select pre-search context (e.g., paragraph, sentence, or word) and optionally specify partial search intent (e.g., "how", "why", "applications"). The authors evaluate both explicit prediction through question generation and implicit prediction through answer retrieval. Using adapted versions of the Inquisitive and MS MARCO datasets, they demonstrate that including partial search intent significantly improves prediction performance, especially when dealing with larger pre-search contexts. The study shows that this approach is promising for real-world applications and can help reduce user effort in information seeking.

## Method Summary
The framework allows users to select a text span from a source document (pre-search context) and optionally specify a partial search intent keyword. The system then predicts the user's information need through two approaches: explicit question generation using Flan-T5 models and implicit answer retrieval using BERT-based cross-encoders. The authors augment existing datasets with synthetic intent annotations using LLMs and evaluate various state-of-the-art generative and retrieval models. The interactive design aims to reduce the cognitive load associated with traditional query formulation while maintaining prediction accuracy.

## Key Results
- User-specified partial search intent significantly improves prediction performance, especially with larger pre-search contexts
- Retrieval models (cross-encoders) show more consistent performance than generation models when dealing with context noise
- The "Source + Intent" setting can approximate the performance of "Context" alone, validating the utility of partial intent
- Cross-encoders achieve the best performance but are noted as computationally costly compared to bi-encoders

## Why This Works (Mechanism)

### Mechanism 1
User-specified partial search intent acts as a disambiguating signal that mitigates noise in large pre-search contexts. When a user selects a large context (e.g., a full paragraph), the model encounters distracting information. By adding a partial intent keyword (e.g., "why"), the user effectively constrains the model's attention, filtering out irrelevant concepts within the paragraph and guiding the prediction toward the specific need. Core assumption: The user can articulate their need more easily through a keyword and selection than by formulating a full natural language query.

### Mechanism 2
Reducing pre-search context granularity (e.g., selecting a word vs. a paragraph) reduces prediction error by limiting exposure to distracting entities. Larger text spans increase the probability of containing "distracting" concepts unrelated to the user's specific trigger. By forcing a selection of a smaller span (context), the system mechanically reduces the noise-to-signal ratio, making the retrieval or generation task easier. Core assumption: The user's information need is localized to a specific span of text they can identify.

### Mechanism 3
Implicit prediction via retrieval is more robust to context noise than explicit prediction via question generation. Generative models (seq2seq) are sensitive to specific tokens in the input and may "hallucinate" or drift to irrelevant entities present in the noise. Retrieval models (bi/cross-encoders) score based on aggregate semantic similarity, which appears to be more forgiving of minor contextual distractions, resulting in more consistent performance. Core assumption: The target answer exists in the retrieval corpus and can be matched via semantic overlap even if the query formulation is imperfect.

## Foundational Learning

- **Concept: Pre-search Context vs. Query**
  - **Why needed here:** The paper operates on the premise that information needs arise *before* a query is typed. Understanding this distinction is crucial for framing the problem as "anticipatory" rather than "reactive."
  - **Quick check question:** How does the definition of "pre-search context" in this paper differ from the "query" in traditional IR?

- **Concept: Vocabulary Gap**
  - **Why needed here:** A primary motivation for this work is to help users who cannot articulate their need (the vocabulary gap) by allowing them to point (select context) rather than describe (type query).
  - **Quick check question:** Why does allowing a user to select a "context span" potentially reduce the cognitive load associated with the vocabulary gap?

- **Concept: Bi-Encoder vs. Cross-Encoder Retrieval**
  - **Why needed here:** The paper relies on these architectures for the implicit prediction path. Understanding their trade-offs (speed vs. accuracy) explains why Cross-Encoders performed best but are noted as costly.
  - **Quick check question:** Why might a Cross-Encoder be more robust to the "noise" of a full paragraph source than a Bi-Encoder?

## Architecture Onboarding

- **Component map:** User Interface -> Context/Span Selection -> Intent Input (Optional) -> Prediction Model (Generation/Retrieval) -> Output
- **Critical path:** The synergy of `Context + Intent`. Experiments show that `Source + Intent` can approximate the performance of `Context` alone. The intent keyword is the critical lever for rescuing performance when context is noisy.
- **Design tradeoffs:**
  - **Generation vs. Retrieval:** Generation offers interpretability (the user sees the predicted question) but is brittle and prone to drifting off-topic. Retrieval is robust and directly useful but implicit (the user must trust the system retrieved the right answer without seeing the "question").
  - **Context Scope:** Allow users to select large contexts to reduce their effort, or force small contexts to increase model accuracy? The paper suggests a middle ground where user effort (adding intent) compensates for large context selection.
- **Failure signatures:**
  - **Drifting:** The generation model produces a question about a secondary entity in the paragraph rather than the user's selected focus (e.g., asking about "Chechen crisis" when the user highlighted "Defense Minister")
  - **False Positive Retrieval:** The retriever finds a paragraph semantically similar to the *source* text but fails to address the specific *intent* (e.g., retrieving a definition when the user asked "why")
- **First 3 experiments:**
  1. **Replicate "Context + Intent" Baseline:** Fine-tune a Flan-T5-Base model on the MS MARCO dataset using the `Context` and `Intent` fields to predict the `Question`. Measure ROUGE-L to establish a benchmark for "minimal noise + maximal signal."
  2. **Test Noise Robustness (Source-only):** Train the same model using only the `Source` paragraph (no selected span) to predict the `Question`. Quantify the performance drop to verify the paper's claim that large contexts introduce noise.
  3. **Verify Intent Mitigation:** Train the model on `Source + Intent`. Compare the performance delta between Experiment 2 and Experiment 3. If `Source + Intent` recovers significant performance, validate the core claim that partial intent mitigates context noise.

## Open Questions the Paper Calls Out

- **Multi-turn interactions:** Can multi-turn interactions effectively resolve the trade-off between the user effort required to specify intent and the system's ability to formulate a precise resolution? The authors state this may be resolved through multi-turn interactions but leave it to future work.

- **User-centric approach:** How do user question preferences and interaction behaviors vary across different backgrounds, mediums, or goals in a real-world setting? The authors note that a more user-centric approach through user studies may reveal variations in question preference.

- **Consistency gap:** Can the consistency gap between generation and retrieval models be closed by generating multiple questions to maximize coverage? The authors suggest this might mitigate the observed performance differences.

## Limitations
- Intent extraction quality relies on LLMs generating "partial intent" keywords from ground-truth questions, which may not generalize to real user behavior
- Dataset bias from augmentation since both Inquisitive and MS MARCO datasets use synthetic intent and context annotations rather than human annotations
- Generalization to real-world usage assumes users can accurately select relevant context spans and articulate partial intent keywords

## Confidence
- **High confidence:** Core experimental findings showing that partial intent improves prediction performance (Tables 2 and 4)
- **Medium confidence:** Mechanism explanation that partial intent mitigates noise in large contexts
- **Low confidence:** Claim about retrieval being more robust than generation to context noise

## Next Checks
1. **Intent extraction ablation:** Test the system with manually annotated intent keywords on a subset of data to measure the impact of LLM-generated intent quality on the main findings
2. **Cross-domain evaluation:** Apply the trained models to a different domain (e.g., medical or technical documentation) to assess whether partial intent consistently mitigates context noise across domains
3. **User study validation:** Conduct a small-scale user study where participants select context spans and provide intent keywords, then measure if the predicted questions align with their actual information needs