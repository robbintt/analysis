---
ver: rpa2
title: 'A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical
  Survey on Single-Agent and Multi-Agent Safety'
arxiv_id: '2505.17342'
source_url: https://arxiv.org/abs/2505.17342
tags:
- safe
- safety
- constraints
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a rigorous overview of Safe Reinforcement
  Learning (SafeRL) based on Constrained Markov Decision Processes (CMDPs) and extends
  the discussion to Multi-Agent Safe RL (SafeMARL). It covers theoretical foundations
  including CMDP definitions, constrained optimization techniques, and key theorems.
---

# A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety

## Quick Facts
- arXiv ID: 2505.17342
- Source URL: https://arxiv.org/abs/2505.17342
- Authors: Ankita Kushwaha; Kiran Ravish; Preeti Lamba; Pawan Kumar
- Reference count: 15
- Primary result: Comprehensive technical survey of SafeRL and SafeMARL frameworks based on CMDPs

## Executive Summary
This survey provides a rigorous overview of Safe Reinforcement Learning (SafeRL) based on Constrained Markov Decision Processes (CMDPs) and extends the discussion to Multi-Agent Safe RL (SafeMARL). It covers theoretical foundations including CMDP definitions, constrained optimization techniques, and key theorems. The survey reviews state-of-the-art algorithms for single-agent SafeRL, including Lagrangian-based policy optimization, safety shields, and risk-sensitive methods. It also discusses recent advances in SafeMARL for cooperative and competitive settings, highlighting challenges like decentralized safety and non-stationary environments.

## Method Summary
The survey synthesizes research from multiple domains to provide a comprehensive technical guide. It organizes content around CMDP foundations, single-agent SafeRL algorithms, multi-agent extensions, and open research problems. The methodology involves systematic literature review of theoretical frameworks, algorithmic approaches, and practical implementations. The paper proposes five open research problems, three focusing on SafeMARL, to advance the field. Each problem is described with motivation, key challenges, and related prior work.

## Key Results
- Comprehensive coverage of CMDP-based SafeRL frameworks and their extensions to multi-agent settings
- Detailed analysis of state-of-the-art algorithms including Lagrangian optimization, safety shields, and risk-sensitive approaches
- Identification of five critical open research problems, with particular emphasis on SafeMARL challenges
- Technical synthesis bridging theoretical foundations with practical implementation considerations

## Why This Works (Mechanism)
The survey's effectiveness stems from its systematic organization of complex safety concepts into digestible frameworks. By grounding SafeRL in CMDP theory, it provides a unified mathematical foundation that connects diverse safety approaches. The extension to SafeMARL is particularly valuable as it addresses the unique challenges of decentralized safety in multi-agent environments. The inclusion of practical algorithm descriptions alongside theoretical foundations enables researchers to understand both the "why" and "how" of safe reinforcement learning approaches.

## Foundational Learning

**Constrained Markov Decision Processes (CMDPs)**
- Why needed: Provides formal framework for incorporating safety constraints into reinforcement learning
- Quick check: Verify understanding of the difference between standard MDPs and CMDPs

**Lagrangian-based Policy Optimization**
- Why needed: Enables handling of multiple constraints through dual formulation
- Quick check: Understand the relationship between primal and dual variables in constrained optimization

**Safety Shields**
- Why needed: Provides runtime enforcement of safety constraints during learning
- Quick check: Distinguish between corrective and preventive shield approaches

## Architecture Onboarding

**Component Map**
Constrained MDP Formulation -> Lagrangian Optimization -> Safety Shield Integration -> Risk-sensitive Learning

**Critical Path**
CMDP theoretical foundation → Single-agent SafeRL algorithms → Multi-agent SafeMARL extensions → Open problem formulation

**Design Tradeoffs**
The survey balances theoretical rigor with practical applicability. It prioritizes CMDP-based approaches while acknowledging alternative safety frameworks. The multi-agent coverage emphasizes cooperative settings but also addresses competitive scenarios. The open problems section provides actionable research directions while maintaining theoretical depth.

**Failure Signatures**
- Overemphasis on theoretical aspects may limit practical implementation guidance
- CMDP-centric approach might underrepresent non-constraint-based safety methods
- Multi-agent coverage may not fully capture large-scale decentralized system complexities

**3 First Experiments**
1. Implement a basic Lagrangian-based policy optimization algorithm on a CMDP benchmark
2. Compare safety shield effectiveness across different shield types in a multi-agent environment
3. Test risk-sensitive learning approaches on safety-critical control tasks

## Open Questions the Paper Calls Out
The paper proposes five open research problems:
1. How to design scalable SafeMARL algorithms for large-scale multi-agent systems?
2. What are effective approaches for decentralized safety constraint enforcement?
3. How to handle non-stationary environments in SafeMARL?
4. How to integrate safety learning with transfer learning across different domains?
5. What are optimal ways to balance exploration and safety in constrained settings?

## Limitations
- Focus on CMDP-based approaches may underrepresent alternative safety frameworks
- Treatment of multi-agent safety challenges may not fully capture large-scale decentralized system complexity
- Emphasis on theoretical foundations might limit practical applicability insights

## Confidence

**Theoretical foundations and CMDP framework**: High
**Single-agent SafeRL algorithms**: High
**Multi-agent safety challenges**: Medium
**Open problems formulation**: Medium

## Next Checks
1. Validate the survey's comprehensiveness by cross-referencing with recent conference proceedings (NeurIPS, ICML, ICLR) from the past two years
2. Test the proposed open problems' relevance by surveying active researchers in SafeRL and SafeMARL communities
3. Verify algorithmic descriptions by implementing key methods from the survey in a standardized benchmark environment