---
ver: rpa2
title: 'LJ-Spoof: A Generatively Varied Corpus for Audio Anti-Spoofing and Synthesis
  Source Tracing'
arxiv_id: '2601.07958'
source_url: https://arxiv.org/abs/2601.07958
tags:
- subsets
- speech
- anti-spoofing
- bona
- variants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LJ-Spoof is a large-scale, speaker-specific corpus designed to
  systematically vary TTS architectures, generative hyperparameters, vocoders, input
  sources, and neural post-processing to advance anti-spoofing and source-tracing
  research. It spans 30 TTS families, 500 generatively variant subsets, 10 neural
  post-processing variants, and over 3 million utterances derived from a single high-quality
  speaker.
---

# LJ-Spoof: A Generatively Varied Corpus for Audio Anti-Spoofing and Synthesis Source Tracing

## Quick Facts
- arXiv ID: 2601.07958
- Source URL: https://arxiv.org/abs/2601.07958
- Reference count: 0
- Primary result: Large-scale, speaker-specific corpus enabling systematic analysis of TTS variations on anti-spoofing and source-tracing

## Executive Summary
LJ-Spoof is a comprehensive corpus designed to advance audio anti-spoofing and synthesis source-tracing research through systematic variation of TTS architectures, generative hyperparameters, vocoders, and neural post-processing. Built on LJSpeech (13,100 utterances from a single narrator), it spans 30 TTS families, 500+ generatively variant subsets, and over 3 million utterances. The corpus uniquely enables analysis of how training regimes, input sources, and inference parameters influence spoof detectability while reducing artifact-based bias by treating codec-resynthesized bona fide as genuine.

## Method Summary
The corpus systematically varies TTS synthesis across 30 families (11 single-speaker, 2 fine-tuned, 20 zero-shot) using LJSpeech as the sole bona fide source. It generates 34 default synthetic subsets with official configurations, then creates 500+ variant subsets by sweeping temperature, solver steps, duration factors, emotion scaling, and stochastic seeds. Neural post-processing includes re-vocoding (5 vocoders) and re-codec (7 neural codecs). Metadata files map unique IDs to file paths, labels, and variant details, enabling reproducible training splits and fine-grained analysis.

## Key Results
- Enables fine-grained analysis of how inference parameters influence spoof detectability
- Reduces artifact-based bias by treating codec-resynthesized bona fide as genuine
- Supports multi-class source tracing across 30 TTS families and training regimes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Systematic variation across synthesis parameters enables detectors to generalize beyond default configurations rather than overfitting to single settings.
- **Mechanism:** By sweeping temperature (τ∈[0.1,1.0]), solver steps, duration factors, emotion scaling, and stochastic seeds across 500 variant subsets, the corpus exposes models to multiple realizations of the same synthesizer architecture under different inference configurations.
- **Core assumption:** Adversaries can adjust generative parameters to produce more natural-sounding spoofs, so detectors must be robust to this variation.
- **Evidence anchors:** [abstract] "uniquely enables fine-grained analysis of how... inference parameters (temperature, steps, duration) influence spoof detectability" [section 3.2.4] "These controlled variations yield families of synthetic subsets that isolate the effect of each parameter"
- **Break condition:** If parameter variation does not meaningfully alter acoustic fingerprints detectable by neural systems, the variant subsets provide diminishing returns.

### Mechanism 2
- **Claim:** Speaker-specific corpora enable isolation of synthesis artifacts from speaker-identity confounds.
- **Mechanism:** Using LJSpeech (13,100 utterances from single narrator) as the sole bona fide source allows SS-TTS training, FT-TTS fine-tuning, and ZS-TTS reference prompts all drawn from identical acoustic conditions. This holds recording quality, microphone characteristics, and vocal tract properties constant.
- **Core assumption:** Speaker-independent artifacts exist and can be learned when speaker variability is removed.
- **Evidence anchors:** [abstract] "speaker-specific corpus designed to systematically vary TTS architectures" [section 3.1] "We aim to isolate synthesizer and parameter effects under a single-speaker, studio-quality setting"
- **Break condition:** If synthesis artifacts are speaker-dependent (e.g., interact with vocal tract characteristics), single-speaker training may not transfer.

### Mechanism 3
- **Claim:** Labeling codec-resynthesized bona fide speech as genuine (rather than spoof) reduces bias toward resynthesis artifacts and encourages focus on content manipulation.
- **Mechanism:** By treating re-vocoded and codec-resynthesized bona fide audio as bona fide, while only labeling TTS-synthesized and manipulated content as spoof, detectors cannot rely on mere codec/vocoder footprints. They must detect anomalies arising from content generation.
- **Core assumption:** The core anti-spoofing problem concerns "contextually, acoustically, emotionally, or vocally manipulated" speech, not mere resynthesis.
- **Evidence anchors:** [abstract] "By treating codec-resynthesized bona fide as genuine... it reduces artifact-based bias and encourages detectors to focus on manipulation-induced anomalies" [section 4.1] "none of these attributes are changed by mere resynthesis of a bona fide signal"
- **Break condition:** If codec/vocoder artifacts correlate highly with synthesis artifacts in practice, this labeling may increase false negatives for certain attack types.

## Foundational Learning

- **Concept: TTS Architecture Paradigms (Autoregressive vs. Diffusion vs. LLM-based)**
  - Why needed here: LJ-Spoof spans 30 TTS families across paradigms; understanding architecture differences helps interpret which features transfer across systems.
  - Quick check question: Can you explain why diffusion-based TTS (e.g., Matcha-TTS) might produce different artifacts than autoregressive models (e.g., VITS)?

- **Concept: Vocoder and Neural Codec Functions**
  - Why needed here: Acoustic-vocoder variants and neural post-processing (re-vocoding, re-CODEC) are core variation axes; you must distinguish mel-spectrogram decoding from codec-based discrete token reconstruction.
  - Quick check question: What is the difference between HiFiGAN (neural vocoder) and EnCodec (neural codec) in terms of input representation?

- **Concept: Anti-Spoofing Evaluation Metrics (EER, t-DCF)**
  - Why needed here: The paper positions LJ-Spoof as a benchmark; you need to interpret detection performance correctly.
  - Quick check question: If a detector achieves 5% EER on default subsets but 15% EER on temperature-variant subsets, what does this indicate about generalization?

## Architecture Onboarding

- **Component map:**
  ```
  LJSpeech (13k bona fide)
       ↓
  ┌────┴────┐
  │ Default │ → 34 subsets (30 TTS families, published configs)
  │ Subsets │
  └────┬────┘
       ↓
  ┌────┴────────────────────────────────────────┐
  │ Variant Subsets (500+)                       │
  │ ├─ Training Category: SS-TTS / FT-TTS / ZS-TTS│
  │ ├─ Acoustic-Vocoder: Mel-config pairings     │
  │ ├─ Input: Prompt variants + Text variants    │
  │ ├─ Generative: Temp, steps, emotion, duration│
  │ └─ Neural Post: Re-vocoding + Re-CODEC       │
  └─────────────────────────────────────────────┘
       ↓
  Metadata (file index + variant index via UniqueID)
  ```

- **Critical path:**
  1. Load file index → map UniqueID to file path and top-level label (bona fide/spoof)
  2. Load variant index → map UniqueID to generative details (TTS family, variant category, parameters)
  3. Apply split protocol → balance bona fide/spoof, ensure coverage across variant families
  4. Train detector → binary anti-spoofing OR multi-class source tracing

- **Design tradeoffs:**
  - Single-speaker depth vs. multi-speaker breadth: LJ-Spoof trades speaker diversity for controlled analysis; future extensions planned for multi-speaker.
  - Bona fide labeling choice: Opposes CodecFake-Omni approach; may reduce false positives on legitimate compressed audio but could miss certain artifact patterns.
  - Scale vs. reproducibility: 3M+ utterances require significant storage/compute; metadata-driven subsetting is essential.

- **Failure signatures:**
  - Detector performs well on default subsets but fails on temperature-variant subsets → overfitting to default inference configs.
  - High false positive rate on re-vocoded bona fide → model learned vocoder artifacts as spoof cues (labeling strategy failing).
  - Source tracing fails to distinguish within TTS family across vocoder pairings → insufficient sensitivity to vocoder-specific signatures.

- **First 3 experiments:**
  1. **Baseline generalization test:** Train on default subsets, evaluate on temperature-variant subsets (τ=0.1, 0.5, 0.9) to quantify overfitting to inference parameters.
  2. **Training regime analysis:** Compare detection difficulty across SS-TTS, FT-TTS, and ZS-TTS spoofs; hypothesis: ZS-TTS may be harder due to speaker-adapted artifacts.
  3. **Labeling strategy ablation:** Train two detectors—one with codec-resynthesized bona fide labeled as spoof, one as bona fide—compare false positive rates on legitimate compressed audio (e.g., telephony codecs).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are current anti-spoofing systems robust to systematic variations in generative parameters (e.g., temperature, solver steps, duration/speed) within TTS pipelines?
- Basis in paper: [explicit] The authors explicitly ask: "Are current anti-spoofing systems robust to these generative variations?"
- Why unresolved: Prior datasets have not systematically swept inference controls, leaving the sensitivity of detectors to parameter-tuning attacks unknown.
- What evidence would resolve it: Benchmarking standard countermeasures on the 500 generatively variant subsets to measure performance degradation against non-default configurations.

### Open Question 2
- Question: Can detectors effectively distinguish semantically manipulated speech from neurally post-processed (e.g., codec-resynthesized) or untouched bona fide audio?
- Basis in paper: [explicit] The introduction explicitly queries if systems can "distinguish semantically/prosodically manipulated fakes from neurally post-processed or resynthesized bona fides, and from untouched bona fides."
- Why unresolved: The boundary between "spoof" and high-fidelity resynthesis is blurred; defining resynthesized bona fide as "genuine" is a hypothesis that requires validation.
- What evidence would resolve it: Evaluating false positive rates on the "neural post-processing" bona fide subset against true positive rates for TTS-generated spoofs.

### Open Question 3
- Question: Does the dataset's specific labeling strategy (treating codec-resynthesized bona fide as genuine) reduce artifact-based bias more effectively than labeling such samples as spoof?
- Basis in paper: [inferred] The authors argue their labeling choice "reduces artifact-based bias" compared to prior work like CodecFake-Omni, but this is a methodological assumption presented without validation results.
- Why unresolved: It is unclear if encouraging detectors to ignore codec artifacts (by labeling them as bona fide) improves generalization or if it creates a vulnerability to high-quality codec-based attacks.
- What evidence would resolve it: A comparative study of detector bias and robustness when trained on LJ-Spoof versus datasets that label codec-resynthesized audio as spoof.

### Open Question 4
- Question: To what extent do anti-spoofing models trained on this single-speaker, studio-quality corpus generalize to multi-speaker or in-the-wild scenarios?
- Basis in paper: [inferred] The conclusion lists "multi-speaker" and "in-the-wild speech" as future extensions, implying the current dataset is limited to single-speaker controlled conditions.
- Why unresolved: Models often overfit to speaker-specific traits or recording environments; performance on this corpus may not translate to real-world deepfake detection.
- What evidence would resolve it: Cross-dataset evaluation where models trained on LJ-Spoof are tested on multi-speaker or noisy datasets (e.g., ASVspoof 2021 LA).

## Limitations

- Single-speaker scope limits generalizability to real-world multi-speaker scenarios
- 3M+ utterance scale creates significant computational overhead despite metadata-driven subsetting
- Effectiveness of codec-resynthesized labeling strategy requires empirical validation against opposing approaches

## Confidence

- **High confidence**: The systematic parameter variation design and its potential to enable fine-grained analysis of detection difficulty across inference configurations; the speaker-specific isolation approach and its theoretical grounding in controlling for recording conditions.
- **Medium confidence**: The effectiveness of the codec-resynthesized labeling strategy in reducing false positives on legitimate compressed audio, pending empirical comparison with alternative labeling schemes; the corpus's utility for source tracing given the controlled variation across 30 TTS families.
- **Low confidence**: Whether single-speaker training regimes will transfer to real-world multi-speaker scenarios; whether the 3M+ utterance scale is necessary for robust detector training versus smaller, carefully constructed subsets.

## Next Checks

1. **Generalization test:** Train detector on default subsets, evaluate on temperature-variant subsets (τ=0.1, 0.5, 0.9) to quantify overfitting to inference parameters and assess whether systematic variation achieves intended robustness.
2. **Labeling strategy ablation:** Train two detectors—one with codec-resynthesized bona fide labeled as spoof, one as genuine—compare false positive rates on legitimate compressed audio (e.g., telephony codecs) to empirically validate the design choice.
3. **Speaker dependency analysis:** Apply speaker embedding analysis (e.g., ECAPA-TDNN) to assess whether synthesis artifacts transfer across different vocal tract characteristics when extending beyond the single-speaker LJSpeech base.