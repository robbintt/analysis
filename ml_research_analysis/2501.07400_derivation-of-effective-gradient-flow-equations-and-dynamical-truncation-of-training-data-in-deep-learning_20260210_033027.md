---
ver: rpa2
title: Derivation of effective gradient flow equations and dynamical truncation of
  training data in Deep Learning
arxiv_id: '2501.07400'
source_url: https://arxiv.org/abs/2501.07400
tags:
- gradient
- data
- where
- cost
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the interpretability of deep learning by
  deriving explicit gradient flow equations for the cumulative weights and biases
  in networks with ReLU activation. The author focuses on the Euclidean cost in the
  input layer, assuming weights are adapted to the coordinate system distinguished
  by activations.
---

# Derivation of effective gradient flow equations and dynamical truncation of training data in Deep Learning

## Quick Facts
- arXiv ID: 2501.07400
- Source URL: https://arxiv.org/abs/2501.07400
- Authors: Thomas Chen
- Reference count: 14
- Primary result: Derives explicit gradient flow equations showing deep ReLU networks perform dynamical truncation of training data clusters

## Executive Summary
This paper investigates the interpretability of deep learning by deriving explicit gradient flow equations for cumulative weights and biases in networks with ReLU activation. The author focuses on the Euclidean cost in the input layer, assuming weights are adapted to the coordinate system distinguished by activations. Key results include: 1) gradient flow corresponds to a dynamical process that progressively truncates data clusters in input space at an exponential rate, 2) explicit equations for the flow of cumulative biases and weights are derived, 3) several classes of solutions are analyzed, including equilibrium states and convergence to neural collapse, 4) the dynamics of orbits are interpreted geometrically, and 5) the gradient flow for the standard cost is analyzed for two special initial data configurations.

## Method Summary
The paper derives gradient flow equations by reparameterizing deep ReLU networks using cumulative weights and biases (β^(ℓ), R_ℓ). The analysis assumes weights are "adapted" to the activation function (diagonal in activation's coordinate system) and clusters are sufficiently separated. The truncation map τ^(ℓ)(x) = R_ℓ^T σ(R_ℓ(x + β^(ℓ))) - β^(ℓ) is the core functional unit. The effective gradient flow equations are expressed as ODEs involving empirical measures μ_ℓ over the input space, leading to explicit dynamics that show data points are progressively moved from the positive (active) to negative (truncated) sector of the ReLU activation.

## Key Results
- Gradient descent in deep ReLU networks corresponds to a dynamical "truncation" process where clusters of training data are progressively reduced in complexity in the input space
- Convergence rate of the cumulative bias increases exponentially as more training data points are truncated
- In the output layer, a conserved quantity creates a spectral gap that guarantees exponential convergence of the cost to zero for collapsed initial configurations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient descent in deep ReLU networks corresponds to a dynamical "truncation" process where clusters of training data are progressively reduced in complexity (contracted) in the input space.
- **Mechanism:** By re-parameterizing the network using cumulative weights and biases (β^(ℓ), R_ℓ), the gradient flow equations are transformed into explicit ODEs (Theorem 3.2). These equations describe the motion of affine maps (a^(ℓ)) relative to the ReLU activation sectors (R^Q_+ and R^Q_-). As training progresses, the flow moves probability mass (μ_ℓ) from the positive (active) sector to the negative (truncated) sector.
- **Core assumption:** Weights are "adapted" to the activation function (diagonal in the activation's coordinate system); cluster-separated truncations (layer ℓ affects only cluster ℓ).
- **Break condition:** If clusters are not sufficiently separated or weights are not aligned with the activation basis, the "cluster separated" solution form (Theorem 3.2) does not hold, requiring the complex "renormalized" flow of Theorem 5.1.

### Mechanism 2
- **Claim:** The convergence rate of the cumulative bias increases exponentially as more training data points are truncated.
- **Mechanism:** The gradient flow for the bias is governed by a diagonal matrix J^(ℓ)⊥_0 (Eq 4.2), which effectively measures the ratio of truncated data points (n/N_ℓ). The ODE (Eq 4.46) shows the decay rate scales with this ratio. As more points cross the boundary into the negative sector, the effective "mass" driving the gradient increases, accelerating convergence.
- **Core assumption:** The rotation matrix R_ℓ is fixed or nearly stationary during the bias flow (simplification for analysis).

### Mechanism 3
- **Claim:** In the output layer, a conserved quantity (matrix integral of motion) creates a spectral gap that guarantees exponential convergence of the cost to zero for collapsed initial configurations.
- **Mechanism:** The paper derives a conservation law I(s) = B(Q)(B(Q))^T - (W_(Q+1))^T W_(Q+1) (Proposition 8.3). If initial conditions allow this matrix to be positive or negative definite, a spectral gap λ_0 exists. This gap ensures that the cost gradient is strictly negative definite, preventing stagnation and forcing exponential decay.
- **Core assumption:** Initial data is fully collapsed (or suitable for the specific simplified output layer analysis in Section 8).
- **Break condition:** If the spectral gap condition (inf spec > λ_0) is not met (e.g., I(s) is degenerate), the guarantee of exponential convergence to zero loss is lost.

## Foundational Learning

- **Concept:** **Polar Decomposition & Weight Adaptation**
  - **Why needed here:** The paper relies on decomposing cumulative weights W = W* R and assuming W* is diagonal (adapted to ReLU). Without understanding this, the parameterization of the "truncation map" seems arbitrary.
  - **Quick check question:** Can you explain why assuming the weight matrix |W| is diagonal in the activation's coordinate system simplifies the truncation map to depend only on R and β?

- **Concept:** **Empirical Measures & Moments**
  - **Why needed here:** The effective gradient flow (Eq 3.19) is expressed as integrals over the empirical distribution μ_ℓ. Understanding how discrete training data maps to these continuous measures is essential for implementing the theoretical ODEs.
  - **Quick check question:** How does the matrix J^(ℓ)⊥_0 relate to the geometric position of the data support relative to the origin?

- **Concept:** **Lie Groups (Orthogonal Group O(Q))**
  - **Why needed here:** The weight dynamics live on the manifold O(Q). The gradient flow for R_ℓ (Eq 3.21) involves the Lie algebra o(Q) (anti-symmetric matrices).
  - **Quick check question:** Why must the gradient with respect to R_ℓ be projected onto the anti-symmetric subspace π_- to remain on the orthogonal manifold?

## Architecture Onboarding

- **Component map:** Input Layer (L_0) -> Affine Maps (a^(ℓ)) -> Truncation Maps (τ^(ℓ)) -> Cumulative Parameters (β^(ℓ), R_ℓ)
- **Critical path:** The derivation flows from defining the Euclidean cost in the input layer → expressing the cost via truncation maps → differentiating with respect to β and R → obtaining the ODEs for ∂_s β and ∂_s R
- **Design tradeoffs:**
  - **Interpretability vs. Generality:** The paper sacrifices generality (assuming adapted weights and cluster separation) to gain explicit, solvable ODEs that describe geometric data reduction
  - **Euclidean vs. Pullback Metric:** The paper argues for the Euclidean cost in input space (Eq 2.26) over the standard pullback cost (Eq 2.22) to avoid time-dependent metric tensors and degeneracies in the weight dynamics
- **Failure signatures:**
  - **Non-adapted Weights:** If |W| is not diagonal, the simplification τ(x) ≈ Rσ(R(x+β)) - β fails, and the derived ODEs are invalid
  - **Overlapping Clusters:** If supports of μ_ℓ intersect, "cluster separated truncations" (Def 3.1) cannot be applied, and the system moves to the complex renormalized case (Theorem 5.1)
  - **Spectral Gap Loss:** In the output layer analysis, if initial weights/biases do not yield a definite I(0), convergence may stall
- **First 3 experiments:**
  1. **Visualize Truncation Dynamics:** Implement the ODE ∂_s β^(ℓ) = -R^T J_0^⊥ R (β + ỹ) for a 2D dataset with two distinct clusters. Plot the movement of the clusters into the negative sector over "time" s
  2. **Verify Rate Acceleration:** Numerically integrate the 1D bias flow (Eq 4.42) with sorted data points x_i. Confirm that the decay rate (y-b)(s) increases stepwise as b(s) crosses each x_i
  3. **Test Conservation Law:** Simulate the output layer flow (Eq 8.6, 8.7) with random initial conditions. Calculate I(s) at each step to verify that B B^T - W^T W remains constant (Prop 8.3) and correlates with convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the gradient flow dynamics when layer dimensions are variable and cumulative weights are not aligned with the activation function (R̃_ℓ ≠ 1)?
- **Basis in paper:** "The analysis of more general situations including variable layer dimensions and general weights with R̃_ℓ ≠ 1 are left for future work." (Section 1)
- **Why unresolved:** The proof of Theorem 3.2 relies critically on the alignment assumption to cancel terms and obtain tractable ODEs. Removing this introduces additional rotational degrees of freedom.
- **What evidence would resolve it:** Derivation of modified gradient flow equations accounting for misaligned weights, with analysis of how truncation dynamics change.

### Open Question 2
- **Question:** What is the behavior of gradient flow when truncation maps act nontrivially on multiple clusters simultaneously (non-cluster-separated case)?
- **Basis in paper:** "An analysis of the dynamics is left for future work." for Theorem 5.1's general gradient flow equations (Section 5)
- **Why unresolved:** The general flow involves nested truncation maps with renormalized integration domains D^-(ℓ',Q) requiring intersection of multiple sector conditions—significantly more complex geometry than cluster-separated case.
- **What evidence would resolve it:** Explicit solution analysis or stability results for Theorem 5.1's equations; geometric characterization of convergence rates.

### Open Question 3
- **Question:** Do the exponential convergence results extend to overparametrized networks where K > N (more parameters than training points)?
- **Basis in paper:** "We remark that the situations considered in this work cover underparametrized DL networks... while overparametrized networks are customarily used in applications." (Section 8)
- **Why unresolved:** The analysis assumes invertible W^(ℓ) ∈ GL(Q) and cluster separation; overparametrized regimes typically have degenerate minima and different implicit regularization.
- **What evidence would resolve it:** Convergence rate analysis in overparametrized settings; comparison of truncation dynamics when hidden dimension exceeds input dimension.

## Limitations
- The analysis critically depends on the "adapted weights" assumption (W^(ℓ) diagonal in activation basis), which is restrictive and may not hold for practical networks
- The cluster-separated truncation assumption is strong, requiring sufficient spatial separation between classes
- The Euclidean cost formulation differs from standard cross-entropy losses used in practice
- The conservation law analysis assumes specific initial conditions (neural collapse) that may not be representative of general training dynamics

## Confidence
**High confidence:** The explicit gradient flow equations (Theorem 3.2) and their geometric interpretation as truncation dynamics are well-supported by the mathematical derivations. The exponential convergence rates for bias flow (Section 4.3) follow directly from the ODEs and are verifiable through numerical simulation.

**Medium confidence:** The connection between truncation dynamics and neural collapse (Section 4.4) is theoretically sound but relies on specific initial conditions. The conservation law analysis in Section 8 is mathematically rigorous for the simplified setup but may not generalize to arbitrary initializations.

**Low confidence:** The practical relevance of these findings to standard deep learning training remains unclear due to the restrictive assumptions about weight adaptation and cluster separation.

## Next Checks
1. **Numerical verification of truncation dynamics:** Implement the ODEs from Theorem 3.2 for a simple 2D dataset and verify that the empirical measures μ_ℓ indeed show decreasing support in the positive sector over training.

2. **Testing convergence rate acceleration:** For a 1D problem, numerically integrate the bias flow equation and confirm that the convergence rate increases as more data points cross the truncation boundary, matching the predicted scaling with n/N_ℓ.

3. **Conservation law validation:** Simulate the output layer dynamics with random initial conditions and verify that the matrix integral of motion B B^T - W^T W remains constant while correlating with convergence speed.