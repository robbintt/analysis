---
ver: rpa2
title: Evaluating improvements on using Large Language Models (LLMs) for property
  extraction in the Open Research Knowledge Graph (ORKG)
arxiv_id: '2502.10768'
source_url: https://arxiv.org/abs/2502.10768
tags:
- properties
- research
- orkg
- extraction
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of advanced prompt engineering
  techniques for property extraction in the Open Research Knowledge Graph (ORKG).
  Building on prior work, it applies techniques like few-shot learning, Chain of Thought
  (CoT) prompting, and persona descriptions to enhance Large Language Model (LLM)
  performance.
---

# Evaluating improvements on using Large Language Models (LLMs) for property extraction in the Open Research Knowledge Graph (ORKG)

## Quick Facts
- **arXiv ID:** 2502.10768
- **Source URL:** https://arxiv.org/abs/2502.10768
- **Reference count:** 40
- **Primary result:** Optimized prompt achieves 5.0 average properties per contribution vs 5.7 gold standard

## Executive Summary
This study evaluates advanced prompt engineering techniques for improving Large Language Model (LLM) performance in property extraction for the Open Research Knowledge Graph (ORKG). Building on prior work by Nechakhin et al., the research applies few-shot learning, Chain of Thought (CoT) prompting, and persona descriptions to enhance GPT-3.5-Turbo's ability to extract research dimensions from contribution descriptions. The optimized prompt significantly improves property extraction performance, achieving an average of 5.0 properties per contribution compared to 5.7 in the gold standard, while demonstrating better alignment with existing ORKG properties.

The study also introduces property matching techniques that map extracted properties to existing ORKG properties using literal and variant search methods. Results show that 75.33% of extracted properties match existing ORKG properties, demonstrating improved consistency and adherence to FAIR principles. The research confirms that advanced prompt engineering can substantially enhance LLM-based property extraction while addressing challenges like inconsistency in ORKG properties.

## Method Summary
The study evaluates property extraction for the Open Research Knowledge Graph (ORKG) by identifying research dimensions from research problem descriptions to characterize scholarly contributions. Using 150 ORKG contributions from a publicly available dataset, the research employs GPT-3.5-Turbo via OpenAI API with an optimized prompt combining few-shot examples, Chain of Thought prompting, persona description, and explicit output constraints. The evaluation uses an LLM-as-judge approach with GPT-3.5-Turbo to assess alignment and deviation scores on a 1-5 scale, along with property matching against existing ORKG properties via API. The code and prompts are available on GitHub for reproducibility.

## Key Results
- Optimized prompt achieves 5.0 average properties per contribution versus 5.7 gold standard
- Property matching shows 75.33% of extracted properties align with existing ORKG properties
- LLM-as-judge evaluation demonstrates improved alignment and reduced deviation compared to baseline

## Why This Works (Mechanism)
The optimized prompt engineering techniques work by providing structured guidance that helps the LLM understand the task context and expected output format. Few-shot examples demonstrate the desired property extraction pattern, Chain of Thought prompting encourages step-by-step reasoning, and persona descriptions establish the LLM's role as an ORKG expert. This combination reduces ambiguity and improves the model's ability to extract relevant properties that align with existing ORKG conventions.

## Foundational Learning
- **Few-shot learning**: Provides concrete examples of desired output format and content, reducing ambiguity in task understanding. Quick check: Verify few-shot examples cover diverse property types and common ORKG patterns.
- **Chain of Thought prompting**: Encourages systematic reasoning by breaking down the extraction process into sequential steps. Quick check: Confirm CoT steps are logically ordered and comprehensive.
- **Persona descriptions**: Establishes context and expertise expectations for the LLM, improving task alignment. Quick check: Ensure persona matches ORKG domain knowledge requirements.
- **Literal and variant search**: Enables matching extracted properties to existing ORKG properties despite minor textual differences. Quick check: Test search coverage with common property synonyms.
- **LLM-as-judge evaluation**: Provides automated assessment of property alignment and deviation using consistent criteria. Quick check: Validate judge prompts capture all relevant evaluation dimensions.

## Architecture Onboarding

**Component Map:**
Dataset -> Preprocessing -> GPT-3.5-Turbo (optimized prompt) -> Property Extraction -> LLM-as-judge Evaluation -> Property Matching API

**Critical Path:**
Research problem text → GPT-3.5-Turbo extraction → Property parsing → LLM-as-judge alignment/deviation scoring → ORKG API matching

**Design Tradeoffs:**
- Open-ended extraction allows flexibility but may produce inconsistent properties versus predefined property lists
- LLM-as-judge evaluation provides scalability but may inherit prompt biases from prior studies
- Few-shot examples improve performance but require careful selection to represent ORKG conventions

**Failure Signatures:**
- Invalid output format (non-Python list) indicates prompt structure issues
- Low mapping rates suggest poor few-shot example selection or property ontology inconsistencies
- High deviation scores indicate extracted properties diverge from ORKG standards

**First Experiments:**
1. Run extraction with controlled temperature parameters to establish reproducibility baseline
2. Perform ablation study removing each prompt component to quantify individual contributions
3. Conduct manual validation on 20-30 random contributions to verify LLM-as-judge scores

## Open Questions the Paper Calls Out

**Open Question 1:** Does a hybrid extraction process, where LLMs first evaluate a list of predefined properties before suggesting new ones, improve consistency compared to open-ended extraction alone? This conceptual "two-step process" was discussed but not empirically tested in the current study.

**Open Question 2:** To what extent does including full abstracts or methods sections as input context improve the accuracy of property extraction over using only the research problem? The current experiment was constrained to research problem titles, leaving the benefit of additional context unmeasured.

**Open Question 3:** How does the consolidation of the existing ORKG ontology to remove duplicate properties affect the success rate of automated property matching? The matching script struggled with duplicates, but actual database cleaning was outside the study's scope.

## Limitations
- Non-deterministic LLM behavior without specified temperature/seed parameters affects reproducibility
- Property matching relies on exact string matching which may miss semantically equivalent properties
- Evaluation methodology inherits potential biases from reused LLM-as-judge prompts

## Confidence
- **High confidence**: Core methodology and dataset availability are well-established with clear preprocessing steps
- **Medium confidence**: Performance metrics are plausible given methodology but non-deterministic outputs introduce uncertainty
- **Low confidence**: Evaluation methodology lacks full transparency in prompt formulation and procedure details

## Next Checks
1. Re-run property extraction pipeline with controlled temperature and seed parameters to establish baseline reproducibility
2. Conduct ablation studies systematically removing each prompt engineering component to quantify contributions
3. Perform manual validation on 20-30 randomly selected contributions to independently verify LLM-as-judge alignment/deviation scores