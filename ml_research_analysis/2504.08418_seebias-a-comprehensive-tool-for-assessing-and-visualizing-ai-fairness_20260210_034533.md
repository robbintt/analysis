---
ver: rpa2
title: 'seeBias: A Comprehensive Tool for Assessing and Visualizing AI Fairness'
arxiv_id: '2504.08418'
source_url: https://arxiv.org/abs/2504.08418
tags:
- fairness
- seebias
- performance
- white
- black
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: seeBias is an R package for comprehensive fairness evaluation of
  AI prediction models, addressing the limitation of existing tools that focus primarily
  on classification performance disparities while overlooking other critical aspects
  like calibration and ranking consistency. The package implements conventional group
  fairness metrics alongside visual assessments of calibration curves, prediction
  distributions, and rank-based fairness, with customizable visualizations for transparent
  reporting.
---

# seeBias: A Comprehensive Tool for Assessing and Visualizing AI Fairness

## Quick Facts
- arXiv ID: 2504.08418
- Source URL: https://arxiv.org/abs/2504.08418
- Reference count: 0
- seeBias is an R package for comprehensive fairness evaluation of AI prediction models

## Executive Summary
seeBias is an R package that addresses the limitation of existing fairness tools by implementing both conventional group fairness metrics and visual assessments of calibration curves, prediction distributions, and rank-based fairness. Using two case studies in criminal justice and healthcare, the tool revealed systematic biases not captured by conventional metrics, such as overestimation of risk for specific groups and disparities in positive predictive values across racial groups. The package requires minimal input parameters and is accessible to practitioners with varying programming experience.

## Method Summary
seeBias provides functions to evaluate binary prediction models using conventional group fairness metrics (TPR, FPR, BER, accuracy, PPV, NPV) alongside visual assessments of calibration curves, prediction distributions, and rank-based fairness. The tool accepts predictions as probabilities, scores, or binary outputs, along with sensitive variables and observed labels. It generates fairness tables with differences and ratios versus reference groups, and multi-panel visualizations including ROC curves, calibration plots, prediction distributions, and "number needed" metrics for practical significance interpretation.

## Key Results
- seeBias revealed systematic biases not captured by conventional metrics, such as overestimation of risk for specific groups and disparities in positive predictive values across racial groups
- The tool identified that Black individuals without recidivism received similar predicted risk to White individuals with recidivism, highlighting racial biases in model predictions
- Practical significance interpretations using "number needed" metrics showed that a PPV of 0.5 translates to 2 positive predictions needed to identify one true positive

## Why This Works (Mechanism)

### Mechanism 1
Integrating calibration-based and rank-based assessments with classification metrics reveals systematic biases that conventional fairness metrics alone may miss. Classification metrics compress calibration patterns and ranking consistency into single numbers, while visual calibration curves and prediction distribution plots preserve this granularity. Bias manifests across multiple performance dimensions, and some disparities require inspecting probability alignment rather than binary classification outcomes alone.

### Mechanism 2
Visual inspection of group-specific performance metrics with confidence intervals enables identification of disparities that scalar fairness metrics compress or obscure. Plotting actual performance metrics per group with 95% CIs reveals both the magnitude and uncertainty of disparities, while boxplots of prediction distributions show systematic over- or under-estimation patterns that fairness metric differences alone cannot diagnose.

### Mechanism 3
Translating PPV and NPV into "number needed" metrics (NNTP, NNTN) enables domain experts to assess practical significance of disparities. Converting PPV=0.5 to "2 positive predictions per true positive" grounds abstract ratios in concrete operational terms that can be compared across groups and evaluated against domain-specific thresholds for actionability.

## Foundational Learning

- **Group fairness metrics (Equal Opportunity, Equalized Odds, BER)**
  - Why needed: The tool reports these as baseline fairness assessments; understanding what TPR/FPR parity means is prerequisite to interpreting output
  - Quick check question: If Group A has TPR=0.8 and Group B has TPR=0.6 at the same threshold, does this violate equal opportunity?

- **Calibration concepts (calibration-in-the-large, calibration curves)**
  - Why needed: The tool implements calibration assessment as a core feature distinct from classification metrics
  - Quick check question: If predicted probability averages 0.3 for a group but the observed event rate is 0.5, is the model calibrated for that group?

- **Performance metric definitions (PPV, NPV, Accuracy)**
  - Why needed: The visualization displays these per group; understanding their relationship to prevalence is necessary for interpretation
  - Quick check question: If a model has PPV=0.4, what does that mean operationally, and how does class imbalance affect it?

## Architecture Onboarding

- **Component map:** Input functions -> Metric computation -> Output generators
- **Critical path:** 1) Format input with sensitive variables, labels, and predictions; 2) Call evaluate_prediction_*() function with threshold; 3) Apply summary() for fairness table; 4) Apply plot() for visualizations
- **Design tradeoffs:** Simplicity vs. mitigation coverage (no built-in mitigation), comprehensiveness vs. focus (extensive output requires selective reporting), group count limitation (optimized for ≤7 groups)
- **Failure signatures:** Missing reference level for sensitive variable, treating 0.8–1.25 range as definitive bias threshold, wide confidence intervals for underrepresented groups
- **First 3 experiments:** 1) Reproduce Case Study 1 with COMPAS data to validate installation, 2) Apply to local binary classification model comparing summary() and plot() outputs, 3) Extend to intersectional analysis with two sensitive variables

## Open Questions the Paper Calls Out

### Open Question 1
How can fairness visualization tools effectively scale to handle comparisons across a large number of demographic subgroups? Current visualization paradigms become cluttered and difficult to interpret beyond seven groups, limiting utility for fine-grained intersectional fairness analysis.

### Open Question 2
What are the underlying mechanisms governing trade-offs between fairness metrics and predictive performance? The theoretical relationship between different fairness constraints and model performance degradation patterns remains poorly characterized across diverse datasets and model architectures.

### Open Question 3
When should prediction models be adjusted to reflect observed demographic differences versus helping mitigate historical disparities? The decision involves complex ethical, clinical, and contextual considerations that vary by domain and lack consensus guidelines.

### Open Question 4
How can fairness metrics be translated into intuitively interpretable quantities for domain experts? While NNTP and NNTN are introduced as initial translations from PPV/NPV, comprehensive frameworks for communicating fairness remain underdeveloped.

## Limitations
- Difficulty handling comparisons across large numbers of groups, particularly for intersectional analyses
- No built-in bias mitigation functions, requiring use with external mitigation tools
- Access to clinical dataset for Case Study 2 requires ethical approval and data use agreement

## Confidence

### Major Uncertainties and Limitations
Confidence in seeBias's comprehensive fairness assessment capability is **High** for classification-based metrics, as these are well-established and implemented according to standard definitions. Confidence is **Medium** for calibration-based assessments, as the tool's visual calibration curves and logistic recalibration approach are conceptually sound, but lack direct validation studies. Confidence is **Low** for practical significance interpretations, as the "number needed" metric translation lacks empirical validation studies demonstrating improved decision-making outcomes.

## Next Checks
1. **Reproduce Case Study 1** using the included COMPAS dataset to verify installation and confirm that seeBias correctly identifies the reported racial and gender disparities in classification metrics and prediction distributions.

2. **Apply to an external model** with a known fairness issue (e.g., systematically miscalibrated predictions for minority groups) to test whether seeBias's calibration curves and prediction distribution visualizations reveal biases not captured by standard classification metrics.

3. **Conduct a user study** with domain experts comparing interpretations of seeBias's visual outputs versus traditional scalar fairness metrics to assess whether the tool improves practical understanding of fairness trade-offs.