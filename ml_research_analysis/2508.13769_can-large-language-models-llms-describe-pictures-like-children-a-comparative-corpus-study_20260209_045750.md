---
ver: rpa2
title: Can Large Language Models (LLMs) Describe Pictures Like Children? A Comparative
  Corpus Study
arxiv_id: '2508.13769'
source_url: https://arxiv.org/abs/2508.13769
tags:
- language
- corpus
- children
- word
- litkey
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether large language models (LLMs) can generate
  child-like text by comparing LLM-generated descriptions of picture stories to texts
  written by German primary school children (Litkey Corpus). Using multimodal prompts
  (text + images), two LLM-based corpora were created via zero-shot and few-shot prompting.
---

# Can Large Language Models (LLMs) Describe Pictures Like Children? A Comparative Corpus Study

## Quick Facts
- **arXiv ID**: 2508.13769
- **Source URL**: https://arxiv.org/abs/2508.13769
- **Reference count**: 40
- **Primary result**: LLM-generated texts differ significantly from children's texts in lexical richness, semantic alignment, and syntactic patterns despite few-shot prompting improvements.

## Executive Summary
This study evaluates whether large language models can generate child-like text by comparing LLM descriptions of picture stories to texts written by German primary school children. Using multimodal prompts (text + images), two LLM-based corpora were created via zero-shot and few-shot prompting. Analysis across word, syntax, and semantic levels revealed that LLM-generated texts were longer but less lexically rich, relied more on high-frequency words, and underrepresented nouns. Semantic vector space analysis showed low similarity between corpora. Few-shot prompting slightly improved word frequency and sentence length similarity but did not replicate lexical and semantic patterns. The findings suggest that current LLMs struggle to accurately model child language, raising concerns about their use in educational contexts.

## Method Summary
The study compared LLM-generated picture story descriptions to German primary school children's texts from the Litkey Corpus (1,922 texts from 251 children, ages ~9.6 years). GPT-4V (zero-shot) and GPT-4o (few-shot) were used via OpenAI API with Base64-encoded images and German prompts. Zero-shot prompts asked models to describe images as 9.6-year-old children; few-shot prompts added two random image-description pairs from different stories. Analysis included tokenization, log-TTR, word frequency correlation, POS tagging, and fastText embeddings for semantic similarity comparison.

## Key Results
- LLM-generated texts were longer but less lexically rich than children's texts (log-TTR: 0.65 vs 0.71)
- Few-shot prompting improved word frequency correlation (r=0.58 vs r=0.47) and reduced sentence length (median 11 vs 16 words)
- Semantic vector space analysis showed low similarity between corpora (cosine similarity correlation r=0.2 for few-shot, r=0.1 for zero-shot)
- LLMs overrepresented high-frequency words and underrepresented nouns compared to children's writing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Few-shot prompting with child-written examples partially shifts LLM output toward child-like text characteristics
- **Mechanism**: In-context learning allows the model to adapt its next-token predictions based on exemplar patterns in the prompt, adjusting word frequency distributions and sentence length toward the demonstrated style
- **Core assumption**: The model can extract and generalize stylistic patterns from the few-shot examples to new images
- **Evidence anchors**: [abstract] "Few-shot prompt increased similarities between children and LLM text to a minor extent, but still failed to replicate lexical and semantic patterns." [section] Table 1 shows word frequency correlation improved from r=0.47 (zero-shot) to r=0.58 (few-shot); sentence length median dropped from 16 to 11 words (closer to children's 10). [corpus] Neighbor paper "Evaluating LLMs on Generating Age-Appropriate Child-Like Conversations" similarly finds LLMs struggle with child-like dialogue, suggesting cross-study consistency
- **Break condition**: If few-shot examples are too few or too dissimilar to the target task, adaptation fails. The study used only 2 examples per prompt—insufficient for deep stylistic transfer

### Mechanism 2
- **Claim**: LLMs exhibit "regression-to-mean" in lexical selection, overproducing high-frequency words at the cost of lexical richness
- **Mechanism**: Next-token prediction optimizes for probable continuations based on training distribution; without explicit incentives for diversity, models converge on common vocabulary
- **Core assumption**: The training corpus contains more adult/professional text than child-authored text, skewing the learned probability distribution
- **Evidence anchors**: [abstract] "LLM-generated texts were longer but less lexically rich, relied more on high-frequency words." [section] Table 1: Litkey log-TTR = 0.71 vs. LLM-FS = 0.65 (lower = less diverse). LLM corpora have more tokens but fewer types. [section] Page 6: "This may result from a regression-to-the-mean phenomenon, reflecting LLMs' aim to generate broadly understandable text"
- **Break condition**: Higher temperature settings may increase lexical diversity (cited Schepens et al. [2]), but at cost of coherence. Without explicit diversity rewards, the mechanism persists

### Mechanism 3
- **Claim**: Multimodal (image + text) prompting enables grounded description but does not guarantee child-appropriate semantic representations
- **Mechanism**: Vision-language models encode visual features and generate text conditioned on both modalities, but semantic vector spaces remain misaligned with child language
- **Core assumption**: Visual grounding alone is insufficient—the model's learned semantic associations still reflect adult-language distributions
- **Evidence anchors**: [abstract] "Semantic vector space analysis showed low similarity between corpora." [section] Page 5: Cosine similarity correlation between Litkey and LLM-FS = r = 0.2; between Litkey and LLM-ZS = r = 0.1 (both low). [corpus] Weak corpus evidence—no neighbor papers directly validate this semantic-space finding for multimodal child-language generation
- **Break condition**: If models were fine-tuned on child-authored multimodal corpora, semantic alignment might improve; current work shows this is not achieved via prompting alone

## Foundational Learning

- **Concept: Type-Token Ratio (TTR) and Log-TTR**
  - **Why needed here**: Measures lexical richness—critical for assessing whether LLMs replicate children's vocabulary diversity or produce repetitive text
  - **Quick check question**: If a corpus has 10,000 tokens and 2,000 unique types, what is its TTR? (Answer: 0.2)

- **Concept: Word Embeddings and Cosine Similarity**
  - **Why needed here**: Enables quantitative comparison of semantic spaces between corpora; low cosine similarity indicates fundamentally different word-meaning associations
  - **Quick check question**: Two word vectors with cosine similarity of 0.9 are [close/distant] in semantic space? (Answer: close)

- **Concept: Zero-shot vs. Few-shot Prompting**
  - **Why needed here**: Understanding how exemplar-based conditioning affects output style is essential for designing prompts that better approximate target populations
  - **Quick check question**: Zero-shot prompting provides [no examples / multiple examples] in the prompt? (Answer: no examples)

## Architecture Onboarding

- **Component map**: Input images + text prompts -> Vision encoder -> Language decoder -> Evaluation pipeline
- **Critical path**: 1. Encode images to Base64; 2. Construct prompt (zero-shot: age only; few-shot: age + 2 random image-description pairs from different stories); 3. Call GPT API (temperature=0.7, max_tokens=2000-5000); 4. Tokenize outputs, compute psycholinguistic metrics; 5. Train corpus-specific fastText model, compute semantic similarity
- **Design tradeoffs**: Closed vs. open models: GPT-4V/4o provide best multimodal performance but lack transparency; open alternatives (DeepSeek) have weaker vision capabilities and more hallucinations; Temperature setting: higher values (e.g., 0.7) increase output diversity but may reduce coherence; Few-shot example count: more examples could improve style transfer but increase token costs and may reduce output variability
- **Failure signatures**: Low lexical richness (log-TTR < 0.65) despite prompting; Overuse of framing phrases ("Auf dem Bild sehe ich..."); Hallucinated objects/text in images (qualitative observation in paper); Semantic similarity r < 0.3 between generated and target corpus
- **First 3 experiments**: 1. **Temperature sweep**: Generate corpora at temperatures [0.5, 0.7, 0.9, 1.1] and measure lexical richness (log-TTR) and semantic similarity to identify optimal diversity-coherence balance; 2. **Few-shot scaling**: Test [0, 2, 5, 10] examples per prompt to determine if additional exemplars significantly improve child-language approximation or plateau early; 3. **Cross-lingual validation**: Replicate methodology on English child corpus (e.g., CHILDES) to assess whether findings generalize beyond German or are language-specific

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can fine-tuning or training LLMs on age-appropriate corpora replicate the semantic patterns of children's writing more effectively than prompting?
- **Basis in paper**: [explicit] The authors state that "Future research may call for training on age-appropriate corpora or fine-tuning with child texts and child language tasks."
- **Why unresolved**: The current study only evaluated prompting strategies (zero-shot and few-shot) on general-purpose models, which failed to replicate semantic patterns and lexical richness.
- **What evidence would resolve it**: A study fine-tuning a model on a child-language corpus (e.g., Litkey) and comparing the resulting semantic vector similarity and lexical diversity to the baseline prompting results.

### Open Question 2
- **Question**: To what extent do standard readability scores align with the psycholinguistic differences found between LLM-generated and child-produced texts?
- **Basis in paper**: [explicit] The authors list the "lack of readability assessments" as a key limitation and suggest "Future work should broaden evaluation metrics."
- **Why unresolved**: The study measured lexical richness and syntax but did not quantify readability, leaving the comparative complexity and accessibility of the texts for young readers unconfirmed.
- **What evidence would resolve it**: Calculating established readability metrics (e.g., Flesch-Kincaid or specific German adaptations) for both corpora and correlating them with the observed lexical and syntactic disparities.

### Open Question 3
- **Question**: Can developmentally plausible models (trained on smaller, multimodal datasets) better approximate child language than large-scale models?
- **Basis in paper**: [explicit] The authors suggest "adapt[ing] the size of training data of the models to the amount possibly available at a young age" (referencing the Baby LM challenge).
- **Why unresolved**: Current LLMs suffer from a regression-to-the-mean effect due to massive, adult-dominated training data, causing them to lack the specific vocabulary and semantic structures of children.
- **What evidence would resolve it**: Comparing the output of a "Baby LM" trained on a restricted, multimodal dataset against the Litkey Corpus using the same semantic and syntactic benchmarks.

### Open Question 4
- **Question**: Do open-source, multimodal models offer a viable alternative to proprietary models for child-language generation without increased hallucination?
- **Basis in paper**: [explicit] The authors note that "hallucinations remain an even stronger concern" with open-source models, yet they are necessary to resolve reproducibility issues inherent to closed models.
- **Why unresolved**: The study relied on deprecated or closed models (GPT-4V/GPT-4o), and the authors' initial tests of open-source alternatives revealed significant accuracy issues (e.g., misidentifying objects).
- **What evidence would resolve it**: A systematic benchmark of open-source vision-language models on the same picture-description task to quantify the trade-off between linguistic similarity and hallucination rates.

## Limitations

- Findings are limited to German language corpora and may not generalize to other languages
- Study used a single child corpus (Litkey) and specific set of eight picture stories, potentially introducing corpus-specific artifacts
- Closed nature of multimodal models (GPT-4V/4o) prevents detailed analysis of architectural differences between conditions

## Confidence

- **High confidence**: The finding that LLM-generated texts are longer but less lexically rich than children's texts (supported by log-TTR and frequency correlation metrics)
- **Medium confidence**: The claim that semantic vector spaces show low similarity between corpora (limited by weak cross-validation with other studies)
- **Medium confidence**: The conclusion that few-shot prompting only partially improves child-like text characteristics (based on modest improvements in frequency correlation and sentence length)

## Next Checks

1. **Temperature Sweep Validation**: Generate additional LLM corpora at temperatures [0.5, 0.7, 0.9, 1.1] and systematically measure lexical richness (log-TTR) and semantic similarity to identify optimal settings for balancing diversity and coherence

2. **Few-Shot Scaling Experiment**: Test [0, 2, 5, 10] example prompts per condition to determine whether additional exemplars produce diminishing returns or significant improvements in approximating child-like language patterns

3. **Cross-Lingual Replication**: Replicate the full methodology on an English child corpus (e.g., CHILDES) to assess whether the German-specific findings generalize across languages or represent language-specific phenomena