---
ver: rpa2
title: 'ReTAG: Retrieval-Enhanced, Topic-Augmented Graph-Based Global Sensemaking'
arxiv_id: '2509.25814'
source_url: https://arxiv.org/abs/2509.25814
tags:
- community
- topic
- graph
- global
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReTAG, a Retrieval-Enhanced, Topic-Augmented
  Graph framework for global sensemaking that improves response quality and reduces
  inference time compared to the baseline. ReTAG constructs topic-specific subgraphs
  and retrieves relevant community summaries to generate answers.
---

# ReTAG: Retrieval-Enhanced, Topic-Augmented Graph-Based Global Sensemaking

## Quick Facts
- arXiv ID: 2509.25814
- Source URL: https://arxiv.org/abs/2509.25814
- Reference count: 40
- Primary result: ReTAG achieves winning rates >50% and reduces inference time by up to 90.3% on News Articles dataset

## Executive Summary
ReTAG introduces a topic-augmented graph-based framework for global sensemaking that constructs topic-specific subgraphs and retrieves relevant community summaries to generate answers. The system improves response quality while dramatically reducing inference time compared to baseline methods. Experiments on Podcast and News Articles datasets demonstrate that topic augmentation improves content relevance and reduces computational complexity, while retrieval augmentation further enhances efficiency by selecting the most pertinent summaries.

## Method Summary
ReTAG builds hierarchical community summaries from entity-relation graphs, then mines topics from baseline summaries to construct topic-specific subgraphs. At inference, queries are classified into topics, keywords are expanded via LLM, and BM25 retrieves top-p summaries from the relevant topic subgraph. Sub-answers are generated sequentially and aggregated into a final response. The framework reduces computational complexity by constraining entity graphs to topic-relevant subsets while maintaining response quality through targeted retrieval and hierarchical community summarization.

## Key Results
- Winning rates exceed 50% across both Podcast and News Articles datasets
- Inference time reduced by up to 90.3% (News Articles) and 65.4% (Podcast) at higher community levels
- Topic augmentation doubles the proportion of relevant summaries per query compared to baseline
- Keyword expansion consistently improves recall@p across community levels

## Why This Works (Mechanism)

### Mechanism 1: Topic-Augmented Subgraphs Reduce Information Fragmentation
ReTAG constrains entity-relation graphs to topic-specific subsets, increasing summary-to-query relevance while reducing computational overhead. By mining topics from the corpus and building separate graphs per topic, each subgraph contains only entities/relations relevant to that topic, yielding smaller, denser subgraphs. The system falls back to general summaries when queries span multiple topics or topic classification fails.

### Mechanism 2: Retrieval-Augmented Summary Selection Reduces Inference Token Load
ReTAG selects only the top-p most relevant community summaries via keyword-expanded BM25 retrieval, maintaining response quality while dramatically reducing inference time. After topic classification, the system retrieves top-200 summaries using BM25 with keyword expansion, then generates responses only from retrieved summaries. This approach trades potential recall for substantial efficiency gains.

### Mechanism 3: Hierarchical Community Summarization Preserves Cross-Document Dependencies
ReTAG builds hierarchical communities via Leiden clustering and summarizes at multiple granularities, capturing both local entity relationships and global thematic patterns. Leaf communities summarize fine-grained entity relationships while higher-level communities aggregate leaf summaries, preserving inter-document relationships that flat retrieval would miss.

## Foundational Learning

- **Concept: Hierarchical Community Detection (Leiden Algorithm)**
  - Why needed here: ReTAG relies on partitioning the entity-relation graph into nested communities to understand why certain communities form and how level selection affects response granularity
  - Quick check question: Can you explain why a leaf community might not reside at the maximum level L, and how ReTAG handles this?

- **Concept: Context Window Management via Progressive Summarization**
  - Why needed here: The system must fit potentially massive community contexts into fixed LLM context windows (4K tokens in experiments)
  - Quick check question: When context exceeds the token limit, which edges are removed first, and why might this bias summaries toward high-degree entities?

- **Concept: Retrieval Evaluation (Recall@k)**
  - Why needed here: ReTAG's efficiency gains depend on retrieving sufficient relevant summaries
  - Quick check question: If recall@200 is 0.7 at level 4 but response quality matches baseline, what does this suggest about information redundancy across communities?

## Architecture Onboarding

- **Component map**: Graph Construction → Topic Mining → Topic-Augmented Graph Building → Inference Pipeline (Query → Topic Classification → Keyword Expansion → BM25 Retrieval → Sub-answer Generation → Final Aggregation)

- **Critical path**: Topic mining and topic-augmented graph construction happen once per corpus (offline). At inference, the bottleneck is LLM calls for topic classification, keyword expansion, sub-answer generation per retrieved summary chunk, and final answer synthesis.

- **Design tradeoffs**:
  - More topics: Better query targeting but higher indexing cost (linear in |T|)
  - Larger p: Higher recall but more LLM calls and longer inference
  - Higher community level: More global coverage but more summaries to process

- **Failure signatures**:
  - Topic classification returns null: Falls back to general summaries; check dataset description quality
  - Empty topic-augmented graph: Query terms may not align with mined topics
  - Sub-answers all have helpfulness_score = 0: Retrieved summaries may be irrelevant

- **First 3 experiments**:
  1. Reproduce baseline vs. ReTAG on a small corpus (50 documents): Measure inference time and manually inspect topic-specific summary quality
  2. Ablate keyword expansion: Run retrieval with and without LLM-generated keywords; measure recall@p
  3. Vary p and community level jointly: Plot inference time vs. winning rate for (p ∈ {50, 100, 200, 400}) × (level ∈ {1, 2, 3})

## Open Questions the Paper Calls Out

### Open Question 1
Can the preprocessing overhead for constructing and indexing topic-specific subgraphs be reduced to facilitate deployment in resource-constrained environments? The authors identify the "cost associated with indexing topic-augmented community summaries" and dependence on "high-performing LLMs" as primary limitations.

### Open Question 2
How can ReTAG be adapted to handle dynamic corpora where documents are frequently added or updated without requiring full graph re-indexing? The methodology assumes a static dataset where topics are mined and graphs are constructed in a single offline phase.

### Open Question 3
To what extent does the multi-step synthesis process in ReTAG affect factual consistency and hallucination rates compared to baseline methods? The evaluation relies on LLM-based "winning rates" but does not report metrics for factual accuracy or faithfulness.

### Open Question 4
How does the performance of ReTAG change when replacing the BM25-based retrieval with dense embedding-based retrieval methods? The retrieval augmentation component specifically employs BM25 with keyword expansion.

## Limitations

- Evaluation relies entirely on LLM-based judgment, creating circular validation where improvements may be over-recognized by the LLM judge
- Performance gains measured primarily on two specific corpora (Podcast and News Articles) with similar structure, limiting generalizability
- Topic mining process depends on quality of LLM-generated baseline summaries, potentially amplifying extraction errors

## Confidence

- **High Confidence**: Retrieval augmentation reduces inference time (supported by Table 5's consistent timing improvements)
- **Medium Confidence**: Topic augmentation improves response quality (winning rates >50% are compelling but dependent on LLM judge calibration)
- **Low Confidence**: Hierarchical community summarization is necessary for global sensemaking (no ablation study isolating community hierarchy)

## Next Checks

1. **Human Validation Study**: Have human annotators rate a stratified sample of ReTAG vs. baseline responses on the same queries used in the LLM evaluation, measuring agreement with LLM judgments.

2. **Cross-Domain Transfer Test**: Apply ReTAG to a fundamentally different corpus type (e.g., scientific papers, legal documents, or social media threads) and measure whether topic augmentation and retrieval benefits persist or degrade.

3. **Ablation of Topic Mining Dependency**: Run the topic-augmented graph construction pipeline using human-curated topics instead of LLM-mined topics on the same corpus, measuring impact on both response quality and inference efficiency.