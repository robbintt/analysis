---
ver: rpa2
title: 'VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language
  Models'
arxiv_id: '2505.22654'
source_url: https://arxiv.org/abs/2505.22654
tags:
- visual
- tokens
- token
- vscan
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of large vision-language
  models (LVLMs) when processing high-resolution or multi-image/video inputs, which
  generate long visual token sequences and incur significant computational costs due
  to quadratic attention complexity. The authors conduct a comprehensive empirical
  analysis of how visual tokens are processed throughout both visual encoding and
  language decoding stages.
---

# VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models

## Quick Facts
- arXiv ID: 2505.22654
- Source URL: https://arxiv.org/abs/2505.22654
- Reference count: 40
- Primary result: 2.91× speedup with 95.4% accuracy retention on LLaVA-NeXT-7B

## Executive Summary
This paper addresses the computational inefficiency of large vision-language models (LVLMs) when processing high-resolution or multi-image/video inputs, which generate long visual token sequences and incur significant computational costs due to quadratic attention complexity. The authors propose VScan, a training-free visual token reduction framework that achieves 2.91× speedup and 10× FLOPs reduction while retaining 95.4% of original performance on LLaVA-NeXT-7B. The method employs complementary global-local scanning during visual encoding to capture semantically important and spatially diverse tokens, followed by middle-layer pruning during language model decoding to remove textually irrelevant tokens while preserving cross-modal interactions.

## Method Summary
VScan is a two-stage, training-free visual token reduction framework for LVLMs. Stage 1 (Visual Encoding) uses complementary global-local scanning: Global Scan selects tokens with highest [CLS] attention from deep ViT layers to capture semantically important regions, while Local Scan partitions images into windows and selects top tokens within each window using shallow-layer attention to preserve fine-grained details. Unselected tokens are merged with their most similar selected counterparts via cosine similarity averaging. Stage 2 (LLM Decoding) prunes visual tokens at middle layers (e.g., layer 16 of 32) based on their attention relevance to the last instruction token, removing textually irrelevant tokens after cross-modal interactions have matured while preserving cross-modal reasoning.

## Key Results
- 2.91× prefilling speedup and 10× FLOPs reduction on LLaVA-NeXT-7B while retaining 95.4% of original performance
- Outperforms state-of-the-art methods across four LVLMs (LLaVA-1.5, LLaVA-NeXT, Qwen-2.5-VL, Video-LLaVA) and sixteen benchmarks
- Achieves superior performance-efficiency trade-offs with 11.1% average visual token retention (50/50 global/local split, R2=33.3%)
- Optimal pruning occurs at middle LLM layers due to reduced position bias and stabilized predictions

## Why This Works (Mechanism)

### Mechanism 1
A complementary global-local scanning strategy during visual encoding captures comprehensive visual information. Global Scan uses deep-layer [CLS] attention to select semantically significant tokens, while Local Scan partitions images into windows and uses shallow-layer attention to select top tokens per window, preserving fine-grained details. The union of both scans captures both high-level context and local diversity that single-strategy approaches miss.

### Mechanism 2
Token merging based on cosine similarity preserves information from pruned tokens. Each unselected token is assigned to its most similar selected token in feature space, and the final representation is the average of itself and all assigned unselected tokens. This effectively aggregates features of local regions into representative tokens, preventing abrupt information loss.

### Mechanism 3
Middle-layer pruning in the LLM decoder optimizes the trade-off between preserving cross-modal reasoning and maximizing computational savings. Early layers exhibit strong positional bias vulnerable to pruning disruption, while late-layer pruning offers minimal compute reduction. Middle layers (e.g., layer 16) have matured cross-modal interactions and stabilized predictions, making them optimal for removing textually irrelevant tokens.

## Foundational Learning

- **Self-Attention and Cross-Attention Mechanisms in Transformers**: Understanding attention scores is essential since VScan's logic is based on analyzing them. Quick check: What does a high attention score between token A and token B signify about their relationship?

- **The Vision-Language Model (VLM) Pipeline (ViT -> Projector -> LLM)**: VScan intervenes at two specific points in this pipeline. Quick check: At what point do the visual and textual modalities first interact in a standard VLM?

- **Training-Free Inference Optimization**: VScan is explicitly "training-free," distinguishing it from techniques requiring model fine-tuning. Quick check: What is the primary advantage and key limitation of training-free optimization compared to fine-tuning?

## Architecture Onboarding

- **Component map**: Input Processing -> Visual Encoder (Global Scan -> Local Scan -> Merging Module -> Projector) -> LLM Decoder (Early Layers -> Middle Layer Pruning -> Remaining Layers) -> Output

- **Critical path**: The Middle-Layer Pruning decision is most critical for both correctness and efficiency. Flawed selection logic will discard necessary context, directly harming the model's final response. The Local Scan is also critical for tasks requiring fine-grained visual grounding.

- **Design tradeoffs**: R1 (Visual Encoder Retention) vs. R2 (LLM Retention) - higher R1 preserves more visual information but increases early compute; lower R2 increases later-stage speed but risks removing needed visual information. Global/Local Ratio - 50/50 is default but tasks requiring dense detail may benefit from higher local proportion. Choice of Layer k - optimal layer is architecture-dependent; pruning too early causes positional bias issues, too late reduces potential savings.

- **Failure signatures**: Incorrect reasoning or hallucination indicates aggressive middle-layer pruning cut off critical visual evidence. Degraded performance on dense vision tasks (TextVQA, RefCOCO) indicates Local Scan isn't preserving fine-grained details effectively. Lower-than-expected speedup may occur if pruning logic implementation introduces overhead.

- **First 3 experiments**: 1) Layer-wise Ablation Study: Vary LLM pruning layer k and plot accuracy vs. layer index to validate "middle-layer is optimal" hypothesis. 2) Global vs. Local Scan Ablation: Compare Global-Scan-only, Local-Scan-only, and combined approach on benchmarks with different visual characteristics. 3) End-to-End Efficiency-Accuracy Trade-off: Run complete VScan system with several retention rate configurations and report accuracy, latency, and TFLOPs compared to baseline and SOTA.

## Open Questions the Paper Calls Out

### Open Question 1
How can the trade-off between efficiency and accuracy be optimized for tasks requiring fine-grained visual understanding when aggressive token pruning is applied? The paper acknowledges this inherent trade-off but doesn't propose mechanisms to mitigate accuracy loss on fine-grained tasks under aggressive pruning.

### Open Question 2
Is there a principled, architecture-agnostic method for determining the optimal middle layer for pruning, rather than using heuristic layer selection? The paper uses fixed middle layers based on empirical validation but doesn't derive a generalizable rule for selecting optimal layers across different model architectures.

### Open Question 3
What is the optimal balance between global and local token selection, and should this balance adapt dynamically based on input characteristics or task requirements? The paper shows 50/50 split performs best but doesn't explore whether different images or tasks benefit from adaptive ratios.

### Open Question 4
How does VScan's performance generalize to visual encoders with fundamentally different architectures (e.g., ConvNeXt, Swin Transformer) or to LVLMs with alternative fusion mechanisms? The paper evaluates models all using ViT-based encoders and attention-based fusion, but the method may not directly transfer to other architectures.

## Limitations

- **Architectural Dependency**: VScan's effectiveness is tightly coupled to specific LVLM architectures, requiring careful hyperparameter tuning (optimal pruning layer, retention rates) for each new model.
- **Input Distribution Vulnerability**: The method's reliance on attention scores and cosine similarity assumes certain visual feature space structure, which may fail on highly complex scenes or distributed critical information.
- **Implementation Complexity**: While "training-free," implementing the two-stage scanning and merging process adds pipeline complexity and potential computational overhead not fully characterized.

## Confidence

**High Confidence (80-100%)**
- Middle-layer pruning superiority over early-layer pruning is robust and well-supported by ablation studies
- Complementary global-local scans effectively capture different aspects of visual information
- General framework of using attention scores for token selection is valid and grounded in learned importance weights

**Medium Confidence (50-80%)**
- Specific numerical results (95.4% accuracy retention, 10× FLOPs reduction) are highly dependent on exact implementation details and hyperparameter choices
- Superiority over state-of-the-art methods may vary depending on task distribution and evaluation protocol

**Low Confidence (0-50%)**
- Long-term robustness across diverse LVLM architectures and input distributions is not established
- Interaction between visual token reduction and LLM autoregressive generation for very long sequences is not fully explored

## Next Checks

1. **Architecture Transferability Test**: Apply VScan to a third, distinct LVLM architecture not in original evaluation (e.g., IDEFICS or PaLI). Systematically sweep pruning layer and retention rates to find optimal configuration, measuring accuracy and FLOPs reduction to assess model-specific tuning dependency.

2. **Task-Specific Ablation Study**: Conduct focused ablation on fine-grained visual tasks (TextVQA, POPE). Vary local scan proportion (25%, 50%, 75%) while keeping global scan fixed to quantify detail preservation vs. efficiency trade-off.

3. **Error Analysis and Attribution**: For failure cases where VScan degrades performance, perform detailed analysis using attention visualization to trace pruned tokens and determine if they were critical for correct answers. Compare pruned token distributions in correct vs. incorrect responses to identify systematic failure modes.