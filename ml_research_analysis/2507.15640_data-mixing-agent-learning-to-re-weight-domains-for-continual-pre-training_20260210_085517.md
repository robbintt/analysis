---
ver: rpa2
title: 'Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training'
arxiv_id: '2507.15640'
source_url: https://arxiv.org/abs/2507.15640
tags:
- data
- target
- domain
- mixing
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Data Mixing Agent, a model-based framework
  that learns to re-weight domains for continual pre-training of large language models.
  The method addresses catastrophic forgetting by using reinforcement learning to
  parameterize data mixing heuristics through trajectory sampling and evaluation feedback.
---

# Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training

## Quick Facts
- arXiv ID: 2507.15640
- Source URL: https://arxiv.org/abs/2507.15640
- Reference count: 9
- Primary result: Reinforcement learning agent learns to re-weight domains during continual pre-training, achieving 3.02% average improvement across 12 benchmarks while mitigating catastrophic forgetting.

## Executive Summary
This paper introduces Data Mixing Agent, a model-based framework that learns to dynamically re-weight domain distributions during continual pre-training of large language models. The approach addresses catastrophic forgetting by parameterizing data mixing as a Markov Decision Process and optimizing via offline reinforcement learning. The agent learns generalizable heuristics for navigating domain distributions through trajectory sampling and proxy-based feedback, achieving significant performance gains over strong baselines while maintaining source domain capabilities.

## Method Summary
The framework treats data mixing as a sequential decision-making problem where an agent learns to navigate domain distributions over time. It generates training data by sampling domain probability sequences, training small proxy models on these trajectories, and evaluating them on lightweight benchmarks to collect reward signals. The agent (a 2-layer Transformer decoder) is first trained via supervised fine-tuning on high-quality trajectories, then optimized using Conservative Q-Learning to predict optimal domain distributions. During target model training, the agent runs inference to dynamically adjust data loaders based on training history and feedback.

## Key Results
- Achieves 3.02% average improvement across 8 general and 4 math reasoning benchmarks compared to strong baselines
- Outperforms fixed data mixing ratios and regression-based approaches in both math and code domains
- Demonstrates generalization to unseen source fields, target models, and domain spaces without retraining
- Successfully mitigates catastrophic forgetting while improving target domain performance

## Why This Works (Mechanism)

### Mechanism 1: Sequential Decision Making for Data Mixing
Data mixing is parameterized as a Markov Decision Process where the agent learns a policy to navigate domain distributions over time rather than finding static ratios. The agent observes history of data distributions and model feedback to predict next domain probability distribution, allowing dynamic adaptation like warming up on source data before shifting to target data.

### Mechanism 2: Off-Policy Optimization via Conservative Q-Learning (CQL)
CQL stabilizes training by preventing overestimation of rewards for unseen data mixtures. Unlike standard Q-learning which can hallucinate high rewards for out-of-distribution actions, CQL adds regularization penalty forcing the agent to stay close to the distribution of sampled trajectories while optimizing for balanced performance.

### Mechanism 3: Proxy-Based Environment Feedback
Lightweight proxy models (50M parameters) trained on sampled trajectories provide accurate feedback signals for agent training. This decouples the cost of exploration from target model training by creating a dataset of (State, Action, Reward) tuples through evaluating small models on benchmark subsets.

## Foundational Learning

- **Markov Decision Processes (MDPs)**: Understanding states (history), actions (ratios), and rewards (performance) is required to grasp why an agent is needed instead of regression model. Quick check: Can you explain why "state" includes history of previous distributions rather than just current distribution?
- **Offline Reinforcement Learning (Offline RL)**: Agent cannot interact with live target model during training and must learn from static dataset of past experiments. This constraint dictates use of CQL. Quick check: Why is standard Q-learning dangerous in offline setting?
- **Catastrophic Forgetting**: Core problem the "balanced performance" reward tries to solve. Agent must learn that maximizing target score at expense of source score is negative outcome. Quick check: If reward function only included MATH benchmark score, how would agent's behavior likely differ?

## Architecture Onboarding

- **Component map**: Trajectory Sampler -> Proxy Environment -> Agent (Actor) -> Critic
- **Critical path**: Data Generation → Warm-up → RL Optimization → Inference
- **Design tradeoffs**: Proxy Fidelity vs. Cost (50M proxy is fast but may not reflect 7B+ model capabilities), Exploration vs. Conservatism (inductive biases restrict search space)
- **Failure signatures**: Oscillation (wild alternation between domains), Mode Collapse (converges to 100% target data), Cold Start Failure (outputs random distributions initially)
- **First 3 experiments**: 1) Proxy Validation (verify 50M proxy ranking correlates with 1B+ model), 2) Ablation on RL (compare DataAgentRL vs DataAgentSFT), 3) Generalization Test (apply math-trained agent to code domain)

## Open Questions the Paper Calls Out

- **Generalization to distant domains**: Can agent generalize to target fields with distinct semantic structures (e.g., medicine or law) without retraining? Only math and code tested, which share logical structures unlike domains like biomedicine.
- **Scaling to larger models**: Does re-weighting policy learned from 50M proxy models scale effectively to significantly larger target models (e.g., 70B+ parameters)? Scaling laws are non-linear and heuristics may not hold.
- **Reward signal overfitting**: Is agent susceptible to overfitting to specific datasets used in evaluation environment? Maximizing scores on validation sets might not correlate perfectly with general reasoning improvements.

## Limitations
- Proxy model fidelity assumption remains partially validated - small model dynamics may not reflect large model behaviors across all domain mixtures
- Trajectory coverage analysis not performed - unknown if sampled dataset adequately covers high-reward regions of action space
- Order-independence not tested - MDP formulation advantage over static regression not demonstrated in controlled setting where mixing is order-independent

## Confidence

- **High Confidence**: Core mechanism of using RL to parameterize data mixing trajectories is well-supported by mathematical framework and experimental results
- **Medium Confidence**: Proxy-based feedback system shows strong results but relies on untested assumption about small model correlation with large models
- **Low Confidence**: CQL-based offline optimization has no precedent in this specific application and paper doesn't demonstrate behavior with suboptimal offline datasets

## Next Checks

1. **Proxy Fidelity Ablation**: Systematically vary proxy model sizes and measure correlation decay between proxy ranking and target model performance to quantify minimum viable proxy size and identify failure modes.

2. **Trajectory Coverage Analysis**: Visualize distribution of sampled trajectories in (state, action) space, identify low-density regions, then train agent on progressively worse coverage datasets to measure performance degradation.

3. **Order-Independence Test**: Design experiment where domain mixing effectiveness is explicitly order-independent and measure whether MDP formulation provides any advantage over static regression approaches.