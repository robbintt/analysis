---
ver: rpa2
title: 'Reading Between the Lines: The One-Sided Conversation Problem'
arxiv_id: '2511.03056'
source_url: https://arxiv.org/abs/2511.03056
tags:
- turn
- participant
- reasoning
- know
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the one-sided conversation problem, where
  only one side of a dialogue is available due to privacy or legal constraints. It
  introduces two tasks: reconstructing the missing speaker''s turns and generating
  summaries from one-sided transcripts.'
---

# Reading Between the Lines: The One-Sided Conversation Problem

## Quick Facts
- **arXiv ID:** 2511.03056
- **Source URL:** https://arxiv.org/abs/2511.03056
- **Reference count:** 40
- **Primary result:** One-sided conversation reconstruction benefits from future turn context and placeholder prompting; reconstruction-free summarization works best for open dialogue.

## Executive Summary
This paper tackles the challenge of reconstructing or summarizing conversations where only one side is available, such as in legal or privacy-sensitive recordings. The authors introduce two tasks: reconstructing the missing speaker's turns and generating summaries directly from one-sided transcripts. They evaluate prompting and finetuned models across three dialogue datasets, finding that access to one future turn and turn lengths improves reconstruction accuracy. Large language models like Claude perform well with prompting, while smaller models require finetuning. The study also shows that for open-domain conversations, generating summaries directly from one-sided input is more effective than first reconstructing missing turns.

## Method Summary
The authors introduce a one-sided conversation (1SC) task where one speaker's turns are masked, requiring reconstruction or summarization from the remaining dialogue. They evaluate both prompting approaches using large language models (Claude-4-Sonnet, Llama-3.2-1B-Instruct) and finetuning a Llama-3.2-1B model on synthetic data. The reconstruction task predicts masked turns using local context (turns N-1, N+1) with optional future turn and length metadata. Summarization is performed either reconstruction-heavy (with reconstructed turns) or reconstruction-free (directly from masked input). Evaluation uses GPT-4O rubric scoring across five dimensions plus LLM-extracted precision/recall metrics.

## Key Results
- Access to one future turn (Turn N+1) and turn length metadata improves reconstruction accuracy across all datasets.
- Placeholder prompting with "xxxx" for unknown specifics significantly reduces hallucination in reconstructions.
- Reconstruction-free summarization outperforms reconstruction-heavy approaches for open-domain dialogues (DailyDialog).
- Large models (Claude) achieve high performance with prompting, while smaller models (Llama-1B) require finetuning but still underperform.
- All-at-once multi-turn prediction performs poorly compared to turn-by-turn generation.

## Why This Works (Mechanism)

### Mechanism 1: Future Turn Context Improves Reconstruction
The user's subsequent response provides discriminative signal about what information the masked speaker must have conveyed, enabling abductive reasoning about the missing content. This works because conversations maintain local coherence where adjacent turns constrain each other's content. However, this breaks when Turn N+1 contains entirely new topics not grounded in Turn N, or when conversations have abrupt topic shifts.

### Mechanism 2: Placeholder Prompting Reduces Hallucination
Explicitly instructing models to output "XXXXXXX" for unknown specific details reduces fabrication of concrete facts by providing a sanctioned output channel for uncertainty. This works when models can reliably identify which details are unavailable from context versus which they should infer. It breaks when models cannot distinguish between context-available and context-unavailable details, or when placeholders proliferate making output unusable.

### Mechanism 3: Reconstruction-Free Summarization Outperforms Reconstruction-Heavy in Open Dialogue
For less task-oriented conversations, summarizing directly from one-sided input produces better summaries than first reconstructing missing turns because reconstruction errors cascade into summaries. This works when summarization models can infer high-level intent without low-level turn accuracy. It breaks in highly task-oriented dialogues where specific details are essential and reconstruction provides missing slots.

## Foundational Learning

- **Text Infilling / Masked Language Modeling**
  - Why needed here: 1SC reconstruction is fundamentally a structured infilling task—predicting complete turns rather than short spans.
  - Quick check question: Can you explain why BERT-style span prediction differs from generating full conversational turns?

- **Dialogue State Tracking Concepts**
  - Why needed here: Understanding user intent and slot-filling helps interpret what information the masked speaker must provide.
  - Quick check question: What is the difference between tracking dialogue state and generating dialogue content?

- **LLM-as-Judge Evaluation**
  - Why needed here: The paper uses GPT-4O to score reconstructions across multiple rubric dimensions; understanding this methodology is critical for interpreting results.
  - Quick check question: What are the failure modes of LLM-based evaluators when judging their own outputs or outputs from similar models?

## Architecture Onboarding

- **Component map:** Input processor -> Reconstruction engine -> Anti-hallucination module -> Summarization engine -> Evaluator

- **Critical path:**
  1. Single-turn prediction with local context (Turn N-1, N, N+1) is the default; full-context adds marginal gains
  2. Placeholder prompting must be enforced at prompt level—no post-hoc correction
  3. Summarization should default to reconstruction-free unless task orientation signals high slot-dependency

- **Design tradeoffs:**
  - Large models (Claude) + prompting vs. small models (Llama-1B) + finetuning: Paper shows small models fail to match large model performance even with finetuning
  - Include Turn N+1 (better accuracy) vs. exclude (truly real-time inference): N+1 requires one-turn delay
  - Reconstruction-heavy vs. reconstruction-free summarization: Trade accuracy for robustness depending on domain

- **Failure signatures:**
  - Abrupt topic shifts (Candor) → low semantic similarity scores (~1.1-1.4)
  - Over-confident specific generation without placeholders → low anti-hallucination scores
  - Reconstruction cascade errors in summaries → wrong entities/events in final output

- **First 3 experiments:**
  1. **Baseline reproduction:** Replicate the local-context (three-turn) reconstruction task with placeholder prompting on MultiWOZ; validate anti-hallucination improvement.
  2. **Ablation on context windows:** Compare full-context, N+1-only, turn-lengths-only, and local-context to quantify each signal's contribution.
  3. **Domain transfer test:** Train on MultiWOZ (task-oriented), test on DailyDialog (open-domain) to measure generalization and identify where reconstruction-free summarization becomes preferable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-turn prediction methods be developed that outperform turn-by-turn approaches for 1SC reconstruction?
- Basis in paper: The paper states "our work...opens future directions, including multi-turn prediction" and notes that "experiments showed that prompting a LM to fill multiple turns was ineffective."
- Why unresolved: The paper only evaluates single-turn prediction; Table 4 shows turn-by-turn significantly outperforms all-at-once generation.
- What evidence would resolve it: Novel architectures or training objectives that leverage inter-turn dependencies across multiple masked turns, evaluated on the same datasets.

### Open Question 2
- Question: What training methods or architectural improvements would enable smaller models (<1B parameters) to achieve competitive 1SC reconstruction without relying on large-scale prompting?
- Basis in paper: The limitations section states "there remains a performance gap between large and smaller models on the 1SC task, despite fine-tuning the smaller models; further investigation is needed to close this gap."
- Why unresolved: Even with finetuning on 3.6M examples, Llama-1B achieves rubric scores far below prompted Claude on DailyDialog.
- What evidence would resolve it: Systematic ablation of model size, training data, or specialized architectures achieving comparable metrics.

### Open Question 3
- Question: How can 1SC systems better handle free-flowing conversations with frequent, abrupt topic shifts?
- Basis in paper: The limitations note that "free-flowing conversations, such as those in the Candor corpus, feature frequent and abrupt topic shifts, which reduce performance."
- Why unresolved: Candor results show lower semantic similarity compared to MultiWOZ/DailyDialog, and topic modeling for one-sided input remains unexplored.
- What evidence would resolve it: Integration of discourse segmentation or topic tracking into 1SC pipelines, with improved performance on Candor-style datasets.

### Open Question 4
- Question: Does incorporating previously reconstructed turns as context improve subsequent reconstruction accuracy in online 1SC settings?
- Basis in paper: The conclusion lists "leveraging past predictions in online settings" as an open future direction.
- Why unresolved: The current evaluation treats each turn independently; error propagation from earlier incorrect reconstructions was not studied.
- What evidence would resolve it: Experiments comparing independent turn prediction vs. cascaded prediction using prior model outputs, measuring cumulative accuracy effects.

## Limitations

- The study relies heavily on LLM-based evaluation (GPT-4O as judge), which may not perfectly align with human judgment and could be biased toward outputs from similar model architectures.
- Finetuning hyperparameters for the Llama-1B model are not fully specified beyond referencing Donahue et al. (2020), making exact reproduction challenging.
- The synthetic SODA dataset's contribution to finetuning effectiveness is not independently validated, and placeholder "xxxx" strategy lacks ablation studies to determine optimal placement or frequency.

## Confidence

- **High Confidence:** The core finding that access to one future turn (Turn N+1) and turn length information improves reconstruction accuracy is well-supported by consistent performance improvements across all three datasets.
- **Medium Confidence:** The claim that reconstruction-free summarization outperforms reconstruction-heavy approaches in open dialogue settings is supported by A/B testing but may be domain-dependent and requires further validation across additional conversational domains.
- **Medium Confidence:** The effectiveness of placeholder prompting for reducing hallucination is demonstrated through rubric scores, but the long-term reliability and potential over-conservatism of this approach need more extensive testing.

## Next Checks

1. **Independent Human Evaluation:** Conduct blind human annotation studies comparing LLM-generated reconstructions and summaries against ground truth to validate GPT-4O rubric scores and assess potential evaluator bias.

2. **Cross-Domain Generalization Test:** Train on MultiWOZ and test on DailyDialog (and vice versa) to quantify how well the reconstruction and summarization approaches transfer across task-oriented and open-domain conversation styles.

3. **Ablation on Placeholder Strategy:** Systematically vary the frequency and placement of "xxxx" placeholders in prompts to find the optimal balance between hallucination prevention and output usability, measuring both anti-hallucination scores and end-user comprehension.