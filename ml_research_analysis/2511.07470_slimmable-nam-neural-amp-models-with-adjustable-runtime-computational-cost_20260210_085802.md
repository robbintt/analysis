---
ver: rpa2
title: 'Slimmable NAM: Neural Amp Models with adjustable runtime computational cost'
arxiv_id: '2511.07470'
source_url: https://arxiv.org/abs/2511.07470
tags:
- neural
- slimmable
- network
- computational
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Slimmable Neural Amp Models (Slimmable NAMs),
  which allow musicians to adjust the computational cost of neural amp models in real-time
  without additional training. The core method involves training a single neural network
  that can dynamically adjust its width (number of channels) during inference, enabling
  users to trade off between accuracy and computational efficiency via a simple GUI
  slider in an audio plugin.
---

# Slimmable NAM: Neural Amp Models with adjustable runtime computational cost

## Quick Facts
- **arXiv ID**: 2511.07470
- **Source URL**: https://arxiv.org/abs/2511.07470
- **Reference count**: 12
- **Primary result**: Neural amp models with real-time adjustable computational cost via width-multiplier-based slimmability, enabling trade-offs between accuracy and efficiency

## Executive Summary
This paper introduces Slimmable Neural Amp Models (Slimmable NAMs), a method that enables real-time adjustment of computational cost for neural amp models through a single trained network with dynamic width adjustment. The approach uses a width-multiplier technique to create models that can operate at different computational budgets while maintaining acceptable audio quality. Demonstrated using a stacked WaveNet architecture trained on various guitar amplifier tones, the system allows musicians to control the trade-off between accuracy and computational efficiency via a simple GUI slider in an audio plugin. The method provides a Pareto-optimal trade-off between real-time factor and error-signal ratio compared to fixed models.

## Method Summary
The Slimmable NAM approach trains a single neural network that can dynamically adjust its width (number of channels) during inference, allowing users to trade off between accuracy and computational efficiency. The core mechanism involves training with a width-multiplier-based slimmability technique, where the network can operate at different widths (e.g., 0.5×, 1.0×, 1.5× the full width) without additional training. This is implemented using a stacked WaveNet architecture trained on recordings of various guitar amplifiers across different tones (clean, crunch, rhythm, and lead). The trained model can be deployed in audio plugins where users can adjust the computational cost in real-time through a GUI slider, enabling practical use on systems with varying computational constraints while maintaining high accuracy when full resources are available.

## Key Results
- Slimmable NAMs achieve a Pareto-optimal trade-off between real-time factor (higher is better) and error-signal ratio (lower is better) compared to fixed-width models
- The system enables real-time use of neural amp models on computationally constrained systems while maintaining high accuracy when full resources are available
- Width adjustment can be performed via a simple GUI slider with negligible computational overhead for the switching mechanism

## Why This Works (Mechanism)
The approach leverages established width-multiplier techniques from the broader ML literature, where neural networks can be trained to operate at different widths by adjusting the number of channels in convolutional layers. During training, the network is exposed to different width configurations, learning to maintain acceptable performance across the full range of possible widths. This creates a single model that can dynamically adjust its computational requirements at inference time. The stacked WaveNet architecture is particularly suitable for this approach because its depth-wise separable convolutions and residual connections can be easily scaled by adjusting channel dimensions without requiring architectural changes.

## Foundational Learning
- **Width-multiplier technique**: Method for scaling neural network width by adjusting channel dimensions; needed to enable dynamic computational cost adjustment during inference; quick check: verify that channel scaling preserves relative feature importance across widths
- **Stacked WaveNet architecture**: Deep convolutional network with residual connections and dilated convolutions; needed for its ability to model complex temporal dependencies in audio signals; quick check: confirm that depth and dilation parameters capture appropriate frequency content for amp modeling
- **Pareto optimality**: State where no objective can be improved without worsening another; needed to evaluate the trade-off between accuracy and computational efficiency; quick check: plot all model configurations to verify no point dominates another in both metrics
- **Error-signal ratio metric**: Quantitative measure of reconstruction accuracy in audio applications; needed to objectively compare model performance across different widths; quick check: validate that lower error-signal ratio corresponds to perceptually better audio quality
- **Real-time factor**: Measure of computational efficiency in audio processing; needed to quantify the practical usability of models on different hardware; quick check: benchmark on target hardware to ensure real-time performance at all width settings
- **Dynamic width adjustment**: Ability to change network width during inference without retraining; needed to provide runtime flexibility for users; quick check: verify smooth transitions between width settings without audio artifacts

## Architecture Onboarding

**Component Map**: Input audio -> Stacked WaveNet layers (scalable width) -> Output audio reconstruction -> Width controller (GUI slider) -> Dynamic channel adjustment

**Critical Path**: Audio input → WaveNet processing layers → Width scaling mechanism → Output reconstruction → Audio output

**Design Tradeoffs**: The primary tradeoff involves choosing between higher accuracy (wider networks) and lower computational cost (narrower networks). The stacked WaveNet architecture was selected for its effectiveness in audio modeling, but alternative architectures could potentially offer different efficiency-accuracy curves. The width adjustment mechanism adds minimal overhead but requires careful implementation to avoid introducing latency during real-time switching.

**Failure Signatures**: Performance degradation occurs when width is reduced too far, resulting in increased error-signal ratio and potentially audible artifacts. The model may also struggle with extreme dynamic range content if trained primarily on moderate-level signals. Width switching during transients or complex playing techniques might introduce brief artifacts if not properly synchronized with the audio buffer processing.

**3 First Experiments**:
1. Benchmark the model across different width settings (0.5×, 1.0×, 1.5×, 2.0×) to characterize the accuracy-efficiency trade-off curve
2. Test real-time switching performance by rapidly adjusting the width slider during sustained notes and chord transitions
3. Compare subjective audio quality at different width settings through listening tests with guitarists across various playing styles and amplifier tones

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on a specific stacked WaveNet architecture and training dataset, limiting generalizability to other model architectures or amplifier types
- Lacks comprehensive ablation studies on how different architectural choices affect slimmability trade-offs
- Does not quantify the computational overhead of the width-switching mechanism under various real-time conditions

## Confidence

**Computational Efficiency Claims (High confidence)**: The width-multiplier-based slimmability mechanism is well-established in ML literature, and the demonstrated real-time adjustable computational cost is technically sound with Pareto-optimal trade-off evidence.

**Audio Quality Claims (Medium confidence)**: While quantitative metrics support the claims, the paper lacks extensive subjective listening tests to validate that quality differences at various width settings are perceptually meaningful in real-world playing scenarios.

**Practical Implementation Claims (Low confidence)**: The assertion that the approach can be "easily integrated into existing workflows" lacks specific technical details about implementation challenges, plugin compatibility issues, or potential edge cases during real-time switching.

## Next Checks
1. Conduct comprehensive listening tests with professional guitarists across different amplifier types and playing styles to assess perceptual differences between width settings
2. Test the approach with alternative neural network architectures (e.g., U-Net, CNN variants) to evaluate architectural generalizability and identify optimal configurations for amp modeling
3. Measure the actual computational overhead and latency introduced by the width-switching mechanism under various real-time conditions and hardware constraints