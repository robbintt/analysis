---
ver: rpa2
title: On the expressivity of sparse maxout networks
arxiv_id: '2510.14068'
source_url: https://arxiv.org/abs/2510.14068
tags:
- networks
- functions
- linear
- neural
- virtual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes the expressivity of sparse maxout neural networks,
  where each neuron has a fixed indegree constraint and uses maxout activation. The
  authors establish a duality between functions computable by such networks and a
  class of virtual polytopes, using this connection to bound the dimension of these
  polytopes.
---

# On the expressivity of sparse maxout networks

## Quick Facts
- arXiv ID: 2510.14068
- Source URL: https://arxiv.org/abs/2510.14068
- Reference count: 9
- Key outcome: Sparse maxout networks with fixed indegree constraints cannot represent all continuous piecewise linear functions, proving that sparsity is a real constraint on expressivity that cannot be compensated by width alone.

## Executive Summary
This work analyzes the expressivity of sparse maxout neural networks, where each neuron has a fixed indegree constraint and uses maxout activation. The authors establish a duality between functions computable by such networks and a class of virtual polytopes, using this connection to bound the dimension of these polytopes. They show that sparse networks, regardless of depth, cannot represent all continuous piecewise linear functions, proving that sparsity is a real constraint on expressivity that cannot be compensated by width alone.

## Method Summary
The paper establishes a theoretical framework connecting sparse maxout networks to virtual polytopes. The authors use combinatorial arguments about polytope arrangements to prove expressivity bounds. For the specific case of rank-2 maxout networks with indegree-2, they fully characterize the class of computable functions through a sharp separation analysis for each depth up to ⌈log₂(n+1)⌉.

## Key Results
- Sparse maxout networks cannot represent all continuous piecewise linear functions, regardless of depth or width
- For any fixed depth, indegree constraint, and rank, there exists functions computable by fully connected networks with two hidden layers that cannot be represented by the sparse network
- Complete characterization of rank-2 indegree-2 networks up to depth ⌈log₂(n+1)⌉

## Why This Works (Mechanism)
The mechanism relies on the combinatorial structure of polytope arrangements in the input space. Maxout networks partition the input space into regions where different linear functions are active, and the sparsity constraint limits how these regions can be arranged. The duality with virtual polytopes provides a mathematical framework to characterize which arrangements are possible under sparsity constraints.

## Foundational Learning
- **Virtual polytopes**: Mathematical objects representing the arrangement of linear regions in maxout networks; needed to characterize computable functions under sparsity constraints
- **Continuous piecewise linear functions**: Class of functions representable by neural networks; quick check: functions that are linear within convex regions and continuous across boundaries
- **Indegree constraint**: Maximum number of inputs each neuron can receive; quick check: structural limitation on network connectivity
- **Maxout activation**: Activation function taking maximum over a set of linear functions; quick check: creates piecewise linear decision boundaries
- **Expressivity bounds**: Limits on what functions a network architecture can compute; quick check: theoretical limits on representational capacity

## Architecture Onboarding
**Component Map**: Input -> Sparse Maxout Layer(s) -> Output
**Critical Path**: Input features → constrained connectivity → maxout activation → output computation
**Design Tradeoffs**: Sparsity vs expressivity, depth vs width, fixed constraints vs adaptive connectivity
**Failure Signatures**: Inability to represent certain continuous piecewise linear functions, sharp separation between sparse and dense network capabilities
**First Experiments**: 1) Compare sparse vs dense maxout on simple piecewise linear functions 2) Test expressivity bounds for different indegree constraints 3) Validate polytope characterization through empirical partitioning

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical bounds may not directly translate to empirical performance differences
- Exact rank and indegree constraints may not reflect practical scenarios with approximate or adaptive sparsity
- Extension to higher ranks and different indegree constraints requires additional theoretical development

## Confidence
- Expressivity bounds and impossibility results: High
- Virtual polytope characterization: High
- Practical implications: Low
- Extension to general cases: Medium

## Next Checks
1. Empirical validation comparing sparse vs dense maxout networks on tasks where expressivity differences would be most pronounced
2. Investigation of whether approximate sparsity or adaptive indegree constraints can mitigate the theoretical limitations
3. Analysis of how the theoretical bounds relate to actual performance metrics like accuracy and generalization in practical settings