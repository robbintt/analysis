---
ver: rpa2
title: Toward Subtrait-Level Model Explainability in Automated Writing Evaluation
arxiv_id: '2509.08345'
source_url: https://arxiv.org/abs/2509.08345
tags:
- subtrait
- explainability
- scores
- score
- writing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of generative language models for
  subtrait-level scoring and explainability in automated writing evaluation. The authors
  develop a writing skills tree that decomposes high-level writing traits into fine-grained
  subtraits and collect human scores and span evidence for 225 responses across 45
  middle school prompts.
---

# Toward Subtrait-Level Model Explainability in Automated Writing Evaluation

## Quick Facts
- **arXiv ID**: 2509.08345
- **Source URL**: https://arxiv.org/abs/2509.08345
- **Reference count**: 40
- **Key outcome**: LLMs can provide granular, explainable writing feedback through subtrait scoring, though performance falls short of high-stakes assessment standards

## Executive Summary
This paper explores using large language models for fine-grained writing evaluation by decomposing high-level writing traits into specific subtraits. The authors develop a writing skills tree and evaluate zero-shot prompting with GPT-4o mini for automated subtrait scoring across 225 middle school responses. While models showed consistency across runs and provided interpretable evidence through span extraction, they achieved only fair agreement with human scores (QWK 0.2-0.4), below the threshold for high-stakes assessment use. The work demonstrates potential for granular, explainable feedback but highlights significant reliability challenges that must be addressed before operational deployment.

## Method Summary
The authors created a writing skills tree that breaks down high-level writing traits into fine-grained subtraits, then collected human scores and span evidence for 225 responses across 45 middle school prompts. They evaluated zero-shot prompting with GPT-4o mini for automated subtrait scoring, measuring agreement between model and human scores using quadratic weighted kappa (QWK). The study also assessed model consistency across multiple scoring runs using Krippendorff's alpha and evaluated span extraction capabilities for providing interpretable evidence. Human-human inter-rater reliability was measured to establish baseline subjectivity in subtrait scoring.

## Key Results
- Zero-shot prompting achieved fair agreement between model and human scores (QWK 0.2-0.4)
- Models showed consistency across multiple scoring runs (Krippendorff's alpha 0.6-0.7)
- Human-human inter-rater reliability was moderate for most subtraits (QWK 0.4-0.6)
- Span extraction provided interpretable evidence, though model selections often included more text than human selections
- Models struggled with recall at score boundaries

## Why This Works (Mechanism)
The approach works by decomposing complex writing evaluation into specific, measurable subtraits that can be individually assessed by language models. By providing structured prompts that guide the model through each subtrait systematically, the system can generate both scores and supporting evidence. The span extraction capability allows for interpretable feedback by identifying specific text segments that justify each score, making the evaluation process transparent and actionable for writers.

## Foundational Learning

**Writing Skills Tree**: Hierarchical decomposition of writing traits into fine-grained subtraits
- Why needed: Enables granular, targeted evaluation of specific writing competencies
- Quick check: Can the tree capture all relevant aspects of writing quality while remaining manageable?

**Quadratic Weighted Kappa (QWK)**: Statistical measure of agreement between raters that accounts for ordinal scales
- Why needed: Provides appropriate metric for evaluating ordinal score agreement in writing assessment
- Quick check: Does QWK appropriately capture the degree of disagreement between scores?

**Zero-shot Prompting**: Prompting approach that requires no training examples or fine-tuning
- Why needed: Enables rapid deployment and evaluation without extensive dataset preparation
- Quick check: Can the model generalize from natural language instructions to accurate scoring?

## Architecture Onboarding

**Component Map**: Prompt template -> LLM inference -> Score calculation -> Span extraction -> Agreement measurement

**Critical Path**: Prompt construction → Model inference → Score aggregation → Evidence extraction → Agreement calculation

**Design Tradeoffs**: Zero-shot prompting prioritizes flexibility and ease of deployment over potentially higher performance from fine-tuning; span extraction adds interpretability but may reduce precision

**Failure Signatures**: Poor boundary score recall, inconsistent subtrait scoring, evidence spans that don't align with human selections

**First Experiments**: 1) Test few-shot learning approach to improve score agreement; 2) Evaluate chain-of-thought prompting for better boundary handling; 3) Compare different model sizes for performance trade-offs

## Open Questions the Paper Calls Out
None

## Limitations
- Zero-shot prompting achieved only fair agreement (QWK 0.2-0.4), below high-stakes assessment standards
- Limited scope to middle school prompts with 225 responses, raising generalizability concerns
- Inherent subjectivity in subtrait scoring complicates validation despite moderate human inter-rater reliability
- Systematic struggles with recall at score boundaries suggest persistent model limitations

## Confidence

- **High confidence**: Models show consistency across multiple scoring runs and can generate interpretable span evidence
- **Medium confidence**: The writing skills tree effectively decomposes high-level traits into fine-grained subtraits
- **Low confidence**: Current zero-shot prompting achieves sufficient reliability for practical automated writing evaluation use

## Next Checks
1. Conduct cross-validation using prompts and responses from different educational levels and writing genres
2. Implement and evaluate alternative prompting strategies (few-shot learning, chain-of-thought prompting)
3. Design systematic error analysis focusing on boundary cases and low-performing subtraits