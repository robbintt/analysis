---
ver: rpa2
title: On Benchmarking Human-Like Intelligence in Machines
arxiv_id: '2502.20502'
source_url: https://arxiv.org/abs/2502.20502
tags:
- human
- human-like
- cognitive
- tasks
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that current AI benchmarks are insufficient for
  assessing human-like intelligence because they often lack human-validated labels,
  fail to capture human response variability, and rely on oversimplified tasks. The
  authors conducted a human evaluation study on ten existing AI benchmarks, revealing
  significant biases and flaws in task and label designs.
---

# On Benchmarking Human-Like Intelligence in Machines

## Quick Facts
- arXiv ID: 2502.20502
- Source URL: https://arxiv.org/abs/2502.20502
- Reference count: 40
- Key outcome: Current AI benchmarks fail to assess human-like intelligence due to lack of human-validated labels, inadequate representation of human variability, and oversimplified tasks; authors propose five recommendations for more rigorous evaluation.

## Executive Summary
This paper presents a comprehensive critique of current AI benchmarks for assessing human-like intelligence, identifying fundamental flaws in how these benchmarks are designed and evaluated. Through a human evaluation study on ten existing benchmarks, the authors demonstrate that ground truth labels often disagree with actual human judgments, with only 63.51% average agreement across 300 stimuli tested. The paper argues that benchmarks should evaluate models against population-level distributions of human judgments rather than simplified labels, and proposes five concrete recommendations for designing more cognitively valid assessment tools that better capture the complexity of human cognition.

## Method Summary
The authors conducted a human evaluation study on 300 stimuli sampled from ten AI benchmarks, recruiting 240 participants through Prolific to provide judgments on randomly assigned datasets. Each participant completed 30 trials using slider scales (1-100) to collect soft labels instead of traditional multiple-choice responses. The study converted these soft labels to categorical choices using specified thresholds and calculated agreement rates with benchmark ground truth labels. This methodology allowed the researchers to quantify how well existing benchmark labels align with actual human judgments and to demonstrate the inadequacy of current evaluation approaches for capturing human-like intelligence.

## Key Results
- Human agreement with benchmark ground truth labels averaged only 63.51% across all stimuli, with 26.67% of stimuli showing <50% agreement
- Only 2 of 10 benchmarks had human agreement rates >70%, indicating widespread label validity issues
- 57.69% of participant ratings fell between 20-80 on slider scales, demonstrating that human judgments are often graded rather than binary
- The study reveals that current benchmarks fail to capture human response variability and uncertainty, with simplified task designs that lack ecological validity

## Why This Works (Mechanism)

### Mechanism 1: Human-Validated Ground Truth Substitution
- Claim: Replacing theoretically-derived labels with actual human response data improves benchmark validity for assessing human-like AI.
- Mechanism: Human participants provide judgments on benchmark stimuli; their aggregated responses become the evaluation standard rather than expert-assigned "correct" answers. This captures subjective reasoning domains where no objective ground truth exists.
- Core assumption: Human-like AI should match actual human behavior patterns, not idealized or expert-specified outcomes.
- Evidence anchors:
  - [abstract] "lack of human-validated labels" identified as key shortcoming
  - [Section 4.1] Human evaluation found only 63.51% agreement with ground truth labels; 26.67% of stimuli had <50% agreement
  - [corpus] CogToM benchmark explicitly expands beyond narrow paradigms to capture fuller cognitive spectrum, aligning with this critique

### Mechanism 2: Population Distribution Preservation
- Claim: Evaluating AI against full distributions of human judgments (rather than majority-voted single labels) reveals whether models capture human variability.
- Mechanism: Collect soft labels from multiple annotators; compare model output distributions to human distributions using metrics like KL divergence or Wasserstein distance. This preserves information about disagreement patterns that majority voting discards.
- Core assumption: Human cognitive diversity is meaningful and should be replicated; subpopulation differences matter for alignment.
- Evidence anchors:
  - [abstract] "inadequate representation of human response variability and uncertainty" cited as key flaw
  - [Section 4.2] Recommends reporting distributional metrics and explaining structure within answer distributions (e.g., distinct modes)
  - [corpus] HugAgent explicitly critiques population-level consensus tuning for erasing individual reasoning styles—directly supports this mechanism

### Mechanism 3: Graded Judgment Elicitation via Soft Labels
- Claim: Continuous-scale responses (soft labels) capture graded beliefs and uncertainty that binary multiple-choice formats erase.
- Mechanism: Replace discrete choice questions with slider-based ratings (e.g., 1-100 scale). This preserves individual uncertainty (one person's 60 vs. 90) distinguishable from population-level disagreement.
- Core assumption: Human cognition involves graded beliefs, not just categorical decisions; uncertainty is structurally meaningful.
- Evidence anchors:
  - [abstract] References "inadequate representation of... uncertainty"
  - [Section 4.3] 57.69% of participant ratings fell between 20-80, reflecting graded judgments binary labels would discard
  - [corpus] Weak direct evidence—neighboring papers focus on task design rather than label granularity

## Foundational Learning

- **Inter-annotator Agreement Metrics**
  - Why needed here: The paper's central empirical finding relies on measuring how often humans agree with benchmark labels. Understanding agreement rate calculation and its limitations is prerequisite to interpreting the 63.51% figure meaningfully.
  - Quick check question: If 10 annotators label a stimulus and 6 choose option A while 4 choose option B, what is the agreement rate with a ground truth of A?

- **Soft vs. Hard Labels in Machine Learning**
  - Why needed here: The paper advocates for soft labels (probability distributions over classes) rather than hard labels (single class). This distinction underpins Recommendations 2 and 3.
  - Quick check question: How does training on soft labels differ from training on hard labels in terms of loss function and model behavior?

- **Ecological Validity in Experimental Design**
  - Why needed here: Recommendation 5 critiques simplified, ecologically-invalid tasks. Understanding this concept from cognitive psychology explains why current benchmarks may fail to predict real-world AI behavior.
  - Quick check question: Why might a Theory of Mind benchmark using the Sally-Anne test fail to predict an AI's ability to navigate actual social interactions?

## Architecture Onboarding

- **Component map**:
  - Human data collection pipeline (recruitment, task interface, attention checks)
  - Soft label aggregation module (per-stimulus distribution storage)
  - Distribution comparison metrics (KL divergence, Wasserstein distance)
  - Subpopulation analysis layer (conditional distributions by annotator attributes)
  - Task design validator (checks for cognitive theory grounding, ecological validity)

- **Critical path**:
  1. Define target cognitive construct → 2. Survey cognitive psychology meta-literature → 3. Design ecologically-valid stimuli → 4. Collect soft labels from diverse annotators (n≥20 per stimulus recommended) → 5. Store full distributions → 6. Evaluate models against distributional metrics

- **Design tradeoffs**:
  - Sample size vs. cost: More annotators improve distribution estimates but increase expense
  - Task complexity vs. interpretability: Richer tasks may yield harder-to-analyze responses
  - Soft labels vs. downstream compatibility: Some existing ML pipelines expect hard labels
  - Cultural specificity vs. generalization: Diverse populations capture variability but complicate "human-like" definition

- **Failure signatures**:
  - High variance with no interpretable structure (suggests poorly designed stimuli)
  - Bimodal distributions uncorrelated with demographic factors (suggests ambiguous stimuli)
  - Model matches mean but not variance (suggests overconfident model)
  - Agreement rates near random baseline (suggests fundamentally flawed labels)

- **First 3 experiments**:
  1. Re-annotate 50 stimuli from an existing benchmark you use; calculate human agreement rate with published labels. If <70%, investigate label validity.
  2. Convert a binary classification task to slider-based collection; compare model performance when trained on hard vs. soft labels.
  3. For a benchmark claiming to test a cognitive capacity (e.g., Theory of Mind), map its tasks against a cognitive psychology meta-review (e.g., Beaudoin et al.'s 220 ToM tasks). Identify coverage gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions should AI systems be designed to replicate human cognitive biases versus achieving objective, "bias-free" rationality?
- Basis in paper: [explicit] The authors ask, "should AI systems replicate these human cognitive limitations?" and state, "There is no clear answer here... the extent to which AI should replicate human cognitive biases must be evaluated on a case-by-case basis."
- Why unresolved: While some biases (like racial prejudice) are clearly harmful, others (like loss aversion) may be essential for accurate human behavioral modeling or seamless collaboration, creating a conflict between accuracy and safety.
- What evidence would resolve it: A taxonomy categorizing specific cognitive biases by their utility in different application domains (e.g., simulation vs. decision support) and empirical results showing how replicating "useful" biases improves human-AI team performance.

### Open Question 2
- Question: How can the collection of human evaluation data, particularly for interactive tasks, be scaled efficiently without compromising data quality?
- Basis in paper: [explicit] The authors identify scalability as a challenge, noting, "we urge substantial additional research into ways that we can make evaluation with humans more scalable especially as we consider human-likeness not just in a single decision... but in interactions."
- Why unresolved: Current crowdsourcing methods are resource-intensive, and moving from single-instance labeling to capturing complex, multi-turn interaction traces significantly increases the time and cost required for data collection.
- What evidence would resolve it: The development and validation of novel methodologies or automated tools that reduce the marginal cost of collecting interaction data while maintaining the rigor of human validation.

### Open Question 3
- Question: What specific evaluation metrics can effectively quantify the alignment between AI and population-level distributions of human judgment?
- Basis in paper: [inferred] While the authors recommend using metrics like KL divergence or Wasserstein distances, they note the need to "be clear and seek explicitly to measure" these distributions without prescribing a single standard for the field.
- Why unresolved: There is currently no standardized metric for "human-likeness" that balances the need to capture distributional diversity (variance) with the need to assess correctness or intent, making comparisons across benchmarks difficult.
- What evidence would resolve it: A comparative analysis of distributional metrics on a standardized task set, identifying which metrics most strongly correlate with human subjective assessments of "human-like" behavior.

## Limitations
- The empirical validation is based on a single human evaluation study with 240 participants on 300 stimuli, which may not capture the full diversity of human cognition across cultures and demographics
- The study demonstrates flaws in existing benchmarks but does not validate whether the proposed recommendations actually produce better predictors of human-like intelligence
- The soft label aggregation methodology (converting slider ratings to categorical choices) may introduce artifacts that affect agreement calculations

## Confidence
- **High confidence**: The identification of specific flaws in existing benchmarks (63.51% average human agreement with ground truth, 26.67% of stimuli <50% agreement) and the proposed methodological recommendations are well-supported by empirical evidence from the human evaluation study
- **Medium confidence**: The claim that current benchmarks fail to capture human response variability is supported, but the extent to which distributional evaluation would meaningfully improve benchmark validity requires further validation with actual model evaluations
- **Low confidence**: The assertion that soft labels with slider scales better capture graded human judgments is theoretically sound but lacks direct empirical validation showing improved model alignment compared to binary alternatives

## Next Checks
1. **Benchmark Coverage Analysis**: Map the 10 evaluated benchmarks against established cognitive psychology taxonomies (e.g., Beaudoin et al.'s meta-review of 220 Theory of Mind tasks) to quantify coverage gaps and identify which cognitive capacities remain unbenchmarked
2. **Distributional Evaluation Implementation**: Take an existing model evaluated on one of these benchmarks and implement distributional comparison metrics (KL divergence or Wasserstein distance) against the human response distributions. Compare model rankings when evaluated against hard labels versus distributional metrics
3. **Ecological Validity Assessment**: Design a follow-up experiment where AI systems that perform well on traditional benchmark metrics are tested on ecologically-valid versions of the same tasks (e.g., moving from simplified social reasoning scenarios to naturalistic conversation contexts) to measure correlation decay