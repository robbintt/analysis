---
ver: rpa2
title: 'MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training
  Recipe'
arxiv_id: '2509.18154'
source_url: https://arxiv.org/abs/2509.18154
tags:
- wang
- zhang
- reasoning
- data
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MiniCPM-V 4.5 introduces a unified 3D-Resampler model architecture\
  \ for efficient image and video encoding, achieving up to 96\xD7 compression rate\
  \ compared to existing models. This allows the 8B parameter model to process high-resolution\
  \ images and long videos with significantly reduced GPU memory and inference time."
---

# MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe

## Quick Facts
- arXiv ID: 2509.18154
- Source URL: https://arxiv.org/abs/2509.18154
- Authors: Tianyu Yu; Zefan Wang; Chongyi Wang; Fuwei Huang; Wenshuo Ma; Zhihui He; Tianchi Cai; Weize Chen; Yuxiang Huang; Yuanqian Zhao; Bokai Xu; Junbo Cui; Yingjing Xu; Liqing Ruan; Luoyuan Zhang; Hanyu Liu; Jingkun Tang; Hongyuan Liu; Qining Guo; Wenhao Hu; Bingxiang He; Jie Zhou; Jie Cai; Ji Qi; Zonghao Guo; Chi Chen; Guoyang Zeng; Yuxuan Li; Ganqu Cui; Ning Ding; Xu Han; Yuan Yao; Zhiyuan Liu; Maosong Sun
- Reference count: 40
- Primary result: Achieves state-of-the-art performance among models under 30B size, outperforming GPT-4o-latest and Qwen2.5-VL 72B while using only 46.7% GPU memory and 8.7% inference time of Qwen2.5-VL 7B on VideoMME

## Executive Summary
MiniCPM-V 4.5 introduces a unified 3D-Resampler model architecture for efficient image and video encoding, achieving up to 96× compression rate compared to existing models. This allows the 8B parameter model to process high-resolution images and long videos with significantly reduced GPU memory and inference time. A unified learning paradigm for document knowledge and OCR eliminates reliance on fragile external parsers by dynamically corrupting text regions and asking the model to reconstruct them, enabling robust text recognition and contextual inference within the same training objective. A hybrid reinforcement learning strategy enables both short and long reasoning modes, with flexible control and mutual performance enhancement.

## Method Summary
MiniCPM-V 4.5 employs a unified 3D-Resampler architecture that extends 2D spatial compression to temporal dimensions via learnable query tokens with positional embeddings. The training pipeline includes staged pre-training (image-caption, OCR, interleaved), general SFT followed by Long-CoT + 3D-Resampler fine-tuning, and GRPO-based RL with hybrid short/long reasoning rollouts (50/50 split). The unified document learning uses dynamic visual corruption at three levels to simultaneously train OCR and knowledge reasoning without external parsers. Efficiency gains come from extreme compression (6,448×448 video frames → 64 tokens) and hybrid RL reducing training costs while maintaining performance.

## Key Results
- Achieves 77.1 OpenCompass score, outperforming GPT-4o-latest and Qwen2.5-VL 72B
- Processes 6-second, 2-fps, 448×448 videos using only 128 tokens (96× compression)
- Uses 46.7% GPU memory and 8.7% inference time of Qwen2.5-VL 7B on VideoMME
- Requires only 33.3% long reasoning samples to match peak long reasoning performance

## Why This Works (Mechanism)

### Mechanism 1: Unified 3D-Resampler for Spatial-Temporal Compression
- **Claim:** Joint spatial-temporal compression via 3D-Resampler achieves higher compression rates without proportional performance degradation compared to frame-by-frame encoding.
- **Mechanism:** The 3D-Resampler extends 2D spatial compression by adding temporal positional embeddings to learnable query tokens. Video frames are grouped into packages along the temporal dimension; cross-attention then compresses each package into a fixed-length sequence, exploiting redundancy between adjacent frames. This reduces a 6-second, 2-fps, 448×448 video from 1,536–3,072 tokens (comparable models) to 128 tokens—a 12–24× reduction.
- **Core assumption:** Adjacent video frames contain highly redundant visual information that can be jointly modeled and compressed without losing task-critical information.
- **Evidence anchors:** [abstract]: "using just 46.7% GPU memory cost and 8.7% inference time of Qwen2.5-VL 7B on VideoMME"; [section 2.1.1]: "96× compression rate for video tokens, where 6,448×448 video frames can be jointly compressed into 64 video tokens"; [section 3.4, Table 5]: Ablation shows 3D-Resampler achieves 67.3 (w/ subtitles) vs 2D baseline 65.5, while using only 21.3 tokens/frame vs 64.0; [corpus]: Weak direct support; corpus neighbors focus on cooking applications rather than MLLM compression mechanisms
- **Break condition:** If video content has high per-frame uniqueness (e.g., rapid scene cuts, high-motion sports), temporal redundancy assumption weakens and compression may lose critical temporal cues.

### Mechanism 2: Unified Document Learning via Dynamic Visual Corruption
- **Claim:** Training on dynamically corrupted document images unifies OCR capability and knowledge acquisition without requiring external parsers.
- **Mechanism:** Text regions in document images are stochastically corrupted at three levels: (1) low corruption preserves readability for OCR learning; (2) moderate corruption forces integrated inference from visual cues + context; (3) high corruption (masking) forces pure contextual inference from layout, tables, images. The model learns to adaptively switch strategies based on corruption level.
- **Core assumption:** The visibility gradient of text in documents spans a spectrum from "fully legible" to "context-only inferable," and a single model can learn this adaptive switching.
- **Evidence anchors:** [abstract]: "unified learning paradigm for document knowledge and text recognition without heavy data engineering"; [section 2.2.3]: "By dynamically corrupting text regions with varying corruption levels, the model learns to adaptively and properly switch between accurate text recognition and multimodal context-based knowledge reasoning"; [section 3.4, Table 4]: Unified Learning achieves 51.4 MMMU / 76.5 AI2D / 617 OCRBench vs External Parser baseline 49.0 / 74.9 / 576; [corpus]: No direct corpus evidence for this specific corruption-based unified learning approach
- **Break condition:** If corruption levels are mismatched to task difficulty (e.g., always high corruption for OCR-heavy documents), the model may learn to hallucinate rather than recognize; if always low corruption, contextual knowledge acquisition may fail.

### Mechanism 3: Hybrid Reinforcement Learning for Mode Cross-Generalization
- **Claim:** Jointly training short and long reasoning modes via randomized rollouts improves both modes while reducing total training cost.
- **Mechanism:** During RL training, 50% of rollouts use short reasoning mode, 50% use long reasoning mode (step-by-step traces). GRPO optimizes both jointly without KL penalty or entropy loss. The paper hypothesizes that analytical depth from long reasoning bolsters short reasoning, while efficiency from short reasoning refines long reasoning.
- **Core assumption:** Foundational perceptual and cognitive skills are shared across reasoning modes, enabling positive transfer between them.
- **Evidence anchors:** [abstract]: "hybrid strategy requires only 33.3% long reasoning samples to match the peak long reasoning performance"; [section 2.4.3]: "This training schedule not only preserves the efficiency of short responses while retaining complex reasoning capabilities, but also fosters cross-generalization"; [section 3.4, Table 3]: Hybrid achieves 77.1 OpenCompass with 3.1B tokens vs Long-only 77.0 with 4.4B tokens (70.5% training cost); [corpus]: No direct corpus evidence; corpus neighbors do not address hybrid RL strategies for reasoning modes
- **Break condition:** If short and long reasoning modes require fundamentally incompatible reward landscapes, cross-generalization may fail; Table 3 shows hybrid outperforms single-mode, suggesting this is not the case for this implementation.

## Foundational Learning

- **Concept: Vision-Language Models (VLLMs/MLLMs)**
  - **Why needed here:** MiniCPM-V 4.5 is an MLLM that must process both visual inputs (images, videos) and text through a unified architecture. Understanding how visual tokens interface with LLM decoders is prerequisite to grasping the 3D-Resampler's role.
  - **Quick check question:** Can you explain how a visual encoder's output (e.g., SigLIP features) is converted into tokens that an LLM can process?

- **Concept: Cross-Attention and Query-Based Resampling**
  - **Why needed here:** The core compression mechanism relies on learnable query tokens attending to visual features via cross-attention. Without this foundation, the 2D→3D extension and compression rate claims will be opaque.
  - **Quick check question:** Given a 448×448 image producing 1024 visual feature tokens, how would 64 learnable query tokens compress this via cross-attention?

- **Concept: Reinforcement Learning from Verifiable Rewards (RLVR) and GRPO**
  - **Why needed here:** The hybrid RL strategy uses GRPO (Group Relative Policy Optimization) with rule-based and probability-based rewards. Understanding reward shaping, rollouts, and policy optimization is necessary to interpret the training recipe.
  - **Quick check question:** Why would removing KL penalty and entropy loss improve training stability in this hybrid setting?

## Architecture Onboarding

- **Component map:** Input: Image/Video → Visual Encoder (e.g., SigLIP) → produces visual features → Image Partitioning (LLaVA-UHD) / Video Packing (temporal packages) → Unified 3D-Resampler → compresses to fixed-length tokens → LLM Decoder (8B params) → generates text response → Output: Short reasoning mode OR Long reasoning mode (controllable via prompt)

- **Critical path:** The 3D-Resampler is the efficiency bottleneck. If it under-compresses, LLM inference cost explodes; if it over-compresses, visual information is lost. The unified design (same weights for images/videos) enables efficient 2D→3D extension via lightweight SFT.

- **Design tradeoffs:**
  - **Compression rate vs. performance:** Table 5 shows 3D-Resampler uses 21.3 tokens/frame vs 64.0 (2D), with slightly improved performance. The tradeoff space is: higher compression → faster/cheaper inference but potential information loss.
  - **Short vs. long reasoning mode:** Short mode is efficient; long mode enables complex reasoning. Hybrid training reduces cost but requires careful reward shaping.
  - **Data quality vs. quantity:** Section 2.3.2 emphasizes filtering for challenging prompts over volume for Long-CoT data.

- **Failure signatures:**
  - **3D-Resampler failure:** Video understanding benchmarks (VideoMME, MotionBench) drop disproportionately vs. image benchmarks; temporal reasoning questions fail.
  - **Document learning failure:** OCRBench scores degrade (indicating over-corruption), or knowledge benchmarks drop (indicating under-corruption masking didn't force context learning).
  - **Hybrid RL failure:** Long reasoning becomes verbose without quality gain; short reasoning degrades on complex tasks. Check entropy/response length curves in Figure 3.

- **First 3 experiments:**
  1. **Token compression sweep:** Vary query token count (32, 64, 128) in 3D-Resampler and measure VideoMME score vs. inference time. Identify the Pareto frontier.
  2. **Corruption level calibration:** Train on document data with different corruption probability distributions (e.g., 70% low/20% moderate/10% high vs. 33% each). Evaluate on OCRBench + MMMU to find the balance point.
  3. **Hybrid ratio ablation:** Train with 25% / 50% / 75% long reasoning mode ratio. Plot OpenCompass score vs. training token cost to validate the 50% choice and understand sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the underlying mechanism enabling cross-generalization between short and long reasoning modes in hybrid RL training?
- **Basis in paper:** [explicit] "We hypothesize that this is because both modes share foundational perceptual and cognitive skills. The analytical depth cultivated by long reasoning appears to bolster the short reasoning, while the efficiency and directness learned from the short reasoning refine the long reasoning process."
- **Why unresolved:** The paper observes mutual performance enhancement but does not conduct controlled experiments to isolate which specific skills transfer or how the sharing occurs mechanistically.
- **What evidence would resolve it:** Ablation studies tracking individual skill metrics (e.g., perception accuracy vs. reasoning depth) across modes, plus probing experiments that measure representation overlap between short and long reasoning activations.

### Open Question 2
- **Question:** How does the compression rate of the 3D-Resampler affect retention of fine-grained temporal and spatial details across different video durations and content types?
- **Basis in paper:** [inferred] The paper reports 96× compression (64 tokens for 6,448 frames) but does not analyze performance degradation or information loss at this extreme rate compared to intermediate compression levels.
- **Why unresolved:** While efficiency gains are demonstrated, no systematic study investigates the trade-off curve between compression rate and task performance on fine-grained video understanding tasks.
- **What evidence would resolve it:** Benchmark evaluations across a range of compression rates (e.g., 16×, 32×, 64×, 96×) on tasks specifically requiring fine spatial or temporal discrimination, such as action recognition in cluttered scenes or subtle motion detection.

### Open Question 3
- **Question:** What is the optimal distribution and scheduling of corruption levels in the unified document learning paradigm for balancing OCR robustness and knowledge acquisition?
- **Basis in paper:** [inferred] The paper describes three corruption levels (low, moderate, high) with stochastic application but does not quantify the relative proportions or explore whether dynamic scheduling strategies could improve learning efficiency.
- **Why unresolved:** The dynamic corruption strategy is presented as a unified approach, yet the paper does not ablate different mixing ratios or curriculum strategies for corruption level introduction.
- **What evidence would resolve it:** Controlled experiments varying the probability distribution over corruption levels during training, measuring downstream performance on pure OCR tasks vs. knowledge-intensive reasoning tasks separately.

### Open Question 4
- **Question:** How does the probability-based reward (RLPR) complement rule-based verification across different answer complexity levels, and where does it fail?
- **Basis in paper:** [inferred] The paper reports that combining rule-based and probability-based rewards "consistently and substantially outperforms" rule-only, but provides limited analysis of failure cases or the boundary conditions where probability-based rewards introduce noise.
- **Why unresolved:** The ablation shows aggregate improvement but does not characterize specific prompt types or response characteristics where RLPR provides maximal benefit versus where it may degrade performance.
- **What evidence would resolve it:** Per-category analysis on held-out test sets, stratified by answer length, domain (math vs. general reasoning), and verifiability, with detailed error analysis comparing VR-only vs. VR+PR conditions.

## Limitations

- 3D-Resampler implementation details (package size scheduling, cross-attention dimensions) remain underspecified
- Data composition claims lack specific volume and mixing ratio details
- Hybrid RL generalizability across different MLLM backbones is uncertain
- Unified document learning robustness to diverse document types beyond reported benchmarks is unclear

## Confidence

- **High Confidence**: Architecture claims (3D-Resampler design, unified document learning mechanism) - well-supported by ablation studies and performance gains across benchmarks
- **Medium Confidence**: Efficiency claims (46.7% GPU memory, 8.7% inference time) - based on comparison with Qwen2.5-VL 7B, but exact baseline configurations are unclear
- **Medium Confidence**: Hybrid RL claims (33.3% long reasoning samples matching peak performance) - supported by training curves but sensitivity to ratio and reward design is not fully explored
- **Low Confidence**: Data composition claims - specific dataset volumes, mixing ratios, and quality filtering criteria are not detailed

## Next Checks

1. **Compression efficiency validation**: Conduct a controlled ablation study varying 3D-Resampler query token count (32, 64, 128) and package sizes on VideoMME, measuring the Pareto frontier between compression rate and performance to verify the claimed 96× compression maintains task performance.

2. **Document learning robustness check**: Train with different corruption probability distributions (e.g., 70% low/20% moderate/10% high vs. 33% each) on a held-out document dataset and evaluate on OCRBench + MMMU to identify optimal corruption balance and test generalization to unseen document layouts.

3. **Hybrid RL ratio sensitivity**: Systematically vary the long reasoning mode ratio in RL training (25%, 50%, 75%) on OpenCompass, plotting performance vs. training token cost to validate the 50% choice and identify whether the claimed cross-generalization is robust to ratio changes.