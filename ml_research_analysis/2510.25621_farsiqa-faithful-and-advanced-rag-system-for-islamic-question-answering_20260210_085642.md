---
ver: rpa2
title: 'FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering'
arxiv_id: '2510.25621'
source_url: https://arxiv.org/abs/2510.25621
tags:
- answer
- evidence
- question
- query
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FARSIQA, a novel retrieval-augmented generation
  (RAG) system designed for high-stakes Persian Islamic question answering. It addresses
  the challenges of hallucination and unfaithfulness in LLMs when applied to sensitive
  domains.
---

# FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering

## Quick Facts
- arXiv ID: 2510.25621
- Source URL: https://arxiv.org/abs/2510.25621
- Reference count: 29
- Introduces FAIR-RAG, an iterative, adaptive framework for high-stakes Islamic QA with over 1M documents

## Executive Summary
FARSIQA is a retrieval-augmented generation (RAG) system specifically designed for Persian Islamic question answering, addressing critical issues of hallucination and unfaithfulness in large language models when applied to sensitive domains. The system introduces the FAIR-RAG architecture, which iteratively decomposes complex queries, assesses evidence sufficiency, and refines retrieval until a comprehensive context is built from over one million authoritative Islamic documents. This approach is motivated by the need for reliable, faithful answers in high-stakes religious contexts where accuracy is paramount.

## Method Summary
FARSIQA implements an iterative, adaptive RAG framework called FAIR-RAG that systematically addresses the challenges of faithfulness and accuracy in Islamic question answering. The core innovation lies in its multi-stage process: complex questions are first decomposed into manageable sub-questions, followed by iterative retrieval cycles where evidence sufficiency is continuously assessed. When insufficient evidence is found, the system refines its retrieval strategy and repeats the process until a comprehensive, faithful context is assembled. This approach contrasts with traditional RAG systems by incorporating adaptive mechanisms that ensure completeness and accuracy before generating final answers.

## Key Results
- Achieves 97.0% Negative Rejection rate, representing a 40-point improvement over baseline systems
- Attains 74.3% Answer Correctness on the IslamicPCQA benchmark
- Demonstrates state-of-the-art performance for high-stakes Islamic question answering tasks

## Why This Works (Mechanism)
The FAIR-RAG architecture works by addressing the fundamental limitations of standard RAG systems through iterative refinement and evidence sufficiency assessment. By decomposing complex questions into simpler components, the system can retrieve more targeted and comprehensive evidence for each aspect. The iterative nature allows for continuous evaluation of whether the retrieved context is sufficient to answer the question faithfully, reducing the risk of hallucination or incomplete answers. This adaptive approach ensures that the final context used for generation is both comprehensive and authoritative, which is particularly critical in sensitive domains like Islamic jurisprudence where incomplete or inaccurate information could have significant consequences.

## Foundational Learning
- **Iterative Retrieval Refinement**: Systematically improves retrieval quality through multiple cycles - needed to ensure comprehensive context coverage, quick check: measure improvement in evidence completeness across iterations
- **Evidence Sufficiency Assessment**: Evaluates whether retrieved context adequately supports answer generation - needed to prevent hallucination and incomplete answers, quick check: validate sufficiency metrics against human judgment
- **Query Decomposition**: Breaks complex questions into simpler sub-questions - needed to enable targeted and effective retrieval, quick check: test decomposition accuracy on multi-faceted questions
- **Faithfulness Metrics**: Measures how accurately generated answers reflect source evidence - needed for high-stakes domains where accuracy is critical, quick check: compare faithfulness scores across different evaluation datasets

## Architecture Onboarding

Component Map:
User Query -> Query Decomposition -> Iterative Retrieval -> Evidence Sufficiency Assessment -> Context Assembly -> Answer Generation

Critical Path:
The most critical path involves Query Decomposition → Iterative Retrieval → Evidence Sufficiency Assessment, as these three components work together to ensure that the final context is both comprehensive and faithful before answer generation occurs.

Design Tradeoffs:
The system trades computational efficiency for faithfulness by implementing multiple retrieval iterations and evidence assessments. This design choice prioritizes accuracy over speed, which is appropriate for high-stakes domains but may limit real-time applicability.

Failure Signatures:
Potential failures include getting stuck in infinite retrieval loops if evidence sufficiency thresholds are too high, or producing incomplete answers if decomposition strategies fail to capture all aspects of complex questions. The system may also struggle with questions requiring cross-document synthesis if the knowledge base has gaps.

First Experiments:
1. Test query decomposition accuracy on a diverse set of complex Islamic questions
2. Measure evidence sufficiency assessment reliability against human expert evaluation
3. Evaluate iterative retrieval improvement by comparing context completeness across retrieval cycles

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance heavily dependent on the quality and representativeness of the IslamicPCQA benchmark, which is not publicly available for independent verification
- Limited implementation details provided for key components like query decomposition strategy and evidence sufficiency assessment algorithms
- No ablation studies conducted to quantify the relative contribution of each component to overall performance

## Confidence

Performance claims: High confidence (supported by specific metrics of 97.0% Negative Rejection and 74.3% Answer Correctness, though dependent on benchmark quality)
Architectural innovation: Medium confidence (conceptually sound iterative approach, but lacks detailed implementation specifics)
Domain applicability: Medium confidence (effective for Islamic QA but claims not yet validated across multiple sensitive domains)

## Next Checks
1. Conduct independent evaluation of FARSIQA on a publicly available, diverse QA benchmark to verify generalization beyond IslamicPCQA
2. Perform ablation study to measure the contribution of iterative decomposition and evidence sufficiency assessment to overall performance
3. Analyze the knowledge base for potential biases, gaps, or contradictions that could affect faithfulness and reliability