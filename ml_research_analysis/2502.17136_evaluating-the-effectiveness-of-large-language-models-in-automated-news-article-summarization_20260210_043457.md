---
ver: rpa2
title: Evaluating the Effectiveness of Large Language Models in Automated News Article
  Summarization
arxiv_id: '2502.17136'
source_url: https://arxiv.org/abs/2502.17136
tags:
- few-shot
- news
- risk
- zero-shot
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models (LLMs) for automated
  news summarization in supply chain risk analysis. Multiple LLMs were tested using
  zero-shot, few-shot, and fine-tuning approaches on a dataset of 1,535 news articles.
---

# Evaluating the Effectiveness of Large Language Models in Automated News Article Summarization

## Quick Facts
- **arXiv ID**: 2502.17136
- **Source URL**: https://arxiv.org/abs/2502.17136
- **Reference count**: 23
- **Primary result**: Large language models significantly enhance news summarization for supply chain risk analysis, with Few-Shot GPT-4o mini achieving the highest overall performance while maintaining cost-efficiency.

## Executive Summary
This study evaluates large language models (LLMs) for automated news summarization in supply chain risk analysis. The researchers tested multiple LLMs using zero-shot, few-shot, and fine-tuning approaches on a dataset of 1,535 news articles. Performance was assessed through automated metrics (ROUGE, BLEU, BERTScore) and human evaluation. The Few-Shot GPT-4o mini model demonstrated superior performance in summary quality, coherence, and risk identification while maintaining cost-efficiency. Open-source models like LLaMA and Mistral also showed strong capabilities. The results confirm that LLMs significantly enhance news summarization for risk analysis applications, though combining automated and human evaluation is essential for reliable outcomes.

## Method Summary
The study employed a comprehensive evaluation framework testing multiple LLMs across three prompting strategies (zero-shot, few-shot, fine-tuning) on 1,535 news articles related to supply chain risk. The researchers used a diverse set of evaluation metrics including ROUGE for content overlap, BLEU for n-gram precision, BERTScore for semantic similarity, and human evaluation for qualitative assessment. Models tested included GPT-4o mini, GPT-4o, LLaMA variants, Mistral, and Gemma. The evaluation combined automated scoring with human judgment to assess summary quality, coherence, and risk identification capabilities. The study also considered computational costs and efficiency across different model configurations.

## Key Results
- Few-Shot GPT-4o mini achieved the highest overall performance across summary quality, coherence, and risk identification metrics
- Open-source models (LLaMA, Mistral) demonstrated competitive performance, offering viable alternatives for resource-constrained applications
- Combined automated and human evaluation proved essential for reliable assessment of summarization quality

## Why This Works (Mechanism)
The effectiveness of LLMs in news summarization stems from their ability to process and synthesize large volumes of textual information while maintaining contextual understanding. The few-shot prompting approach allows models to learn summarization patterns from minimal examples, balancing performance with computational efficiency. The models' transformer architectures enable them to capture semantic relationships and maintain coherence across generated summaries. By leveraging pre-trained knowledge and adapting to specific domains through targeted prompts, LLMs can effectively identify and extract key risk-related information from news articles while preserving essential context and relationships.

## Foundational Learning
- **Zero-shot vs Few-shot vs Fine-tuning**: Different prompting strategies offer tradeoffs between performance and resource requirements - zero-shot requires no examples but performs worse, few-shot balances efficiency and quality, while fine-tuning provides highest performance at highest cost
- **ROUGE, BLEU, BERTScore metrics**: Multiple evaluation metrics capture different aspects of summary quality - ROUGE measures content overlap, BLEU assesses n-gram precision, and BERTScore evaluates semantic similarity
- **Transformer architecture fundamentals**: Understanding self-attention mechanisms and positional encoding is crucial for comprehending how LLMs process and generate sequential text
- **Domain adaptation techniques**: Methods for adapting pre-trained models to specific domains like supply chain risk analysis without extensive retraining
- **Human-in-the-loop evaluation**: The importance of combining automated metrics with human judgment to capture nuanced quality aspects that metrics alone miss

## Architecture Onboarding
**Component map**: News articles -> Preprocessing pipeline -> LLM (zero-shot/few-shot/fine-tuned) -> Summary generation -> Automated metrics (ROUGE/BLEU/BERTScore) -> Human evaluation -> Performance assessment

**Critical path**: Raw news articles undergo preprocessing and are fed to the LLM using selected prompting strategy, generating summaries that are evaluated through both automated metrics and human judgment to determine overall effectiveness

**Design tradeoffs**: Zero-shot offers lowest cost but reduced quality; few-shot balances performance and efficiency; fine-tuning maximizes quality but requires significant resources. Open-source models provide cost advantages but may sacrifice some performance compared to proprietary models like GPT-4o mini.

**Failure signatures**: Over-reliance on automated metrics without human evaluation can miss nuanced quality issues; domain-specific terminology may be poorly handled without proper fine-tuning; computational costs may become prohibitive with large-scale fine-tuning approaches.

**First experiments**:
1. Compare zero-shot, few-shot, and fine-tuned performance on a small subset of articles to establish baseline tradeoffs
2. Evaluate summary coherence and risk identification accuracy using human judges on sample outputs
3. Test different model configurations (temperature, max tokens) to optimize summary quality and length

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses on single domain (supply chain risk), limiting generalizability to other news summarization contexts
- Evaluation metrics may not fully capture nuanced quality requirements for risk analysis applications
- Comparison between models complicated by varying computational costs and potential biases in human evaluation

## Confidence
- **High confidence**: LLMs significantly improve news summarization efficiency compared to manual methods; Few-Shot GPT-4o mini demonstrated superior performance metrics
- **Medium confidence**: Open-source models (LLaMA, Mistral) can achieve competitive performance for resource-constrained applications; combined automated and human evaluation is essential for reliable outcomes
- **Medium confidence**: The methodology can be adapted for other specialized news summarization tasks beyond supply chain risk

## Next Checks
1. Replicate the evaluation across multiple domains (e.g., financial news, political reporting, scientific discoveries) to test generalizability
2. Implement longitudinal testing to assess model performance stability over extended periods with evolving news content
3. Conduct blind comparative studies where human evaluators assess summaries without knowing which model generated them to eliminate potential evaluation bias