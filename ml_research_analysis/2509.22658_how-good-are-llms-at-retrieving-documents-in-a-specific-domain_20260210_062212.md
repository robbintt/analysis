---
ver: rpa2
title: How good are LLMs at Retrieving Documents in a Specific Domain?
arxiv_id: '2509.22658'
source_url: https://arxiv.org/abs/2509.22658
tags:
- search
- knowledge
- queries
- base
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of large language models
  (LLMs) for document retrieval in the environmental and earth science domain. Traditional
  keyword-based search systems like Elasticsearch often fail to capture multi-intent
  queries, limiting their precision and recall.
---

# How good are LLMs at Retrieving Documents in a Specific Domain?
## Quick Facts
- arXiv ID: 2509.22658
- Source URL: https://arxiv.org/abs/2509.22658
- Reference count: 33
- LLMs significantly outperform keyword-based search for multi-intent queries in environmental/earth science domain

## Executive Summary
This study evaluates large language models for document retrieval in the environmental and earth science domain. Traditional keyword-based systems like Elasticsearch struggle with multi-intent queries, leading to poor precision and recall. The authors developed EnvKB, a Retrieval-Augmented Generation framework using a fine-tuned LLM to improve semantic understanding and retrieval accuracy. Through comprehensive evaluation using 1,500 queries, they demonstrate that LLM-based systems significantly outperform traditional keyword search, particularly for complex multi-intent queries.

## Method Summary
The research developed a Retrieval-Augmented Generation (RAG) framework called EnvKB to address limitations of traditional keyword-based search systems. The framework leverages a fine-tuned large language model trained on environmental and earth science domain data to improve semantic understanding and retrieval accuracy. The evaluation used a curated dataset of 1,500 queries (1,000 single-intent, 500 multi-intent) to compare EnvKB against Elasticsearch and other LLMs using metrics like Hits@10 and BERTScore. The study specifically examined performance differences between single-intent and multi-intent queries to understand LLM capabilities in handling complex information needs.

## Key Results
- LLM-based systems achieved Hits@10 of 0.96 vs 0.37 for Elasticsearch on mixed-intent queries
- BERTScore improved from 0.45 (Elasticsearch) to 0.92 (LLM-based systems) for mixed-intent queries
- Fine-tuned models showed improved performance, validating domain-specific training benefits

## Why This Works (Mechanism)
LLMs capture semantic relationships and contextual understanding that keyword-based systems miss, particularly for multi-intent queries that require understanding of complex, interconnected concepts.

## Foundational Learning
1. Semantic search vs keyword search - Why needed: Understanding different retrieval paradigms; Quick check: Can explain when each approach is appropriate
2. Multi-intent query processing - Why needed: Core challenge being addressed; Quick check: Can identify multi-intent patterns in sample queries
3. RAG framework architecture - Why needed: Understanding the retrieval-generation pipeline; Quick check: Can trace data flow from query to response
4. Domain-specific fine-tuning - Why needed: Critical for achieving high performance; Quick check: Can explain how domain data improves model performance
5. Evaluation metrics (Hits@10, BERTScore) - Why needed: Measuring retrieval effectiveness; Quick check: Can compute and interpret these metrics
6. Environmental/earth science terminology - Why needed: Domain context for understanding results; Quick check: Can identify key domain concepts in queries

## Architecture Onboarding
Component map: Query -> LLM Processor -> Document Retriever -> Response Generator -> Results
Critical path: User query enters system → LLM interprets semantic intent → Retriever searches document index → Generator synthesizes response → Results returned to user
Design tradeoffs: Fine-tuning vs zero-shot performance, computational cost vs accuracy gains, domain specificity vs generalizability
Failure signatures: Poor multi-intent handling, irrelevant document retrieval, semantic misunderstanding of domain-specific terminology
First experiments:
1. Test single-intent query performance to establish baseline
2. Evaluate multi-intent query handling with complex environmental queries
3. Compare fine-tuned vs non-fine-tuned model performance on domain-specific queries

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Results limited to environmental/earth science domain, may not generalize to other fields
- Evaluation dataset may not reflect all real-world search patterns
- Only compared against Elasticsearch, not other modern semantic search systems

## Confidence
- High confidence: LLM superiority over keyword-based search for multi-intent queries
- Medium confidence: Fine-tuning benefits for domain-specific retrieval
- Medium confidence: EnvKB's ability to handle complex queries

## Next Checks
1. Test EnvKB's performance on queries from different scientific domains to assess domain transfer capabilities
2. Evaluate retrieval accuracy using alternative semantic search baselines beyond Elasticsearch
3. Conduct a user study measuring actual retrieval effectiveness from domain experts' perspectives, including time-to-insight and result satisfaction metrics