---
ver: rpa2
title: Graph-based Integrated Gradients for Explaining Graph Neural Networks
arxiv_id: '2509.07648'
source_url: https://arxiv.org/abs/2509.07648
tags:
- graph
- gb-ig
- nodes
- integrated
- gradients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces graph-based integrated gradients (GB-IG),
  an extension of the integrated gradients (IG) method for explaining graph neural
  networks. GB-IG addresses the limitation of IG, which assumes continuous data, by
  incorporating graph structure through paths between nodes.
---

# Graph-based Integrated Gradients for Explaining Graph Neural Networks

## Quick Facts
- arXiv ID: 2509.07648
- Source URL: https://arxiv.org/abs/2509.07648
- Authors: Lachlan Simpson; Kyle Millar; Adriel Cheng; Cheng-Chew Lim; Hong Gunn Chew
- Reference count: 32
- Primary result: GB-IG achieves higher fidelity and Jaccard scores than standard IG variants on graph-structured data

## Executive Summary
This paper introduces Graph-based Integrated Gradients (GB-IG), an extension of the integrated gradients method for explaining Graph Neural Networks. GB-IG addresses the limitation of standard IG, which assumes continuous data, by incorporating graph structure through shortest-path integration. The method accumulates attributions over paths in the graph, accounting for both node features and structural connectivity, and demonstrates superior performance compared to IG variants on both synthetic and real-world datasets.

## Method Summary
GB-IG extends integrated gradients to graph-structured data by integrating gradients along shortest paths between a baseline node and the target node. The method computes attributions by accumulating the product of feature differences and gradients along each path, then averaging contributions across all shortest paths. A key innovation is the base-point selection heuristic, which chooses a node at maximal distance from the target and maximizes path entropy to select among candidates. The approach satisfies a relaxed "path-based completeness" axiom specific to graph data.

## Key Results
- GB-IG outperforms standard IG variants on four synthetic datasets with ground-truth explanations, achieving up to 0.0557 higher fidelity and 0.2291 higher Jaccard index scores
- On three real-world datasets (Pubmed, Cora, CiteSeer), GB-IG demonstrates higher fidelity scores compared to IG variants, though with slightly lower sparsity
- The entropy-maximizing base-point selection heuristic provides more effective counterfactual explanations than alternative baseline choices
- GB-IG successfully identifies important nodes and substructures in node classification tasks where standard IG methods fail

## Why This Works (Mechanism)

### Mechanism 1
Replacing straight-line interpolation with shortest-path integration improves explanation fidelity for graph-structured data. Standard IG assumes a straight line (Euclidean geodesic) between baseline and input, but GB-IG utilizes graph geometry by integrating gradients along shortest paths connecting base-point to target node. This accounts for the discrete structure and message-passing nature of GNNs where information flows along edges. Core assumption: critical information for prediction is transmitted along shortest topological paths. Break condition: if GNN utilizes long-range dependencies or global pooling mechanisms where specific edge-paths are less relevant than global structural roles.

### Mechanism 2
Summing attributions over multiple shortest paths accounts for non-unique connectivity inherent to graphs. Unlike Euclidean space where shortest path is unique, graphs can have multiple equally short routes between nodes. GB-IG identifies all shortest paths and averages attribution contributions across them, preventing explanations from depending arbitrarily on single path choice. Core assumption: all shortest structural routes contribute equally to explanation. Break condition: in dense graphs, number of shortest paths may explode combinatorially, leading to high computational cost and potential signal dilution.

### Mechanism 3
An entropy-maximizing base-point selection heuristic grounds explanation in structurally distinct regions of graph. Choice of baseline fundamentally alters IG explanations. GB-IG proposes selecting base-point at maximal distance from target node, and when multiple exist, choosing node where connecting paths have maximum entropy (highest connectivity/degree product). Core assumption: valid counterfactual for node is structurally most distant node connected via high-information paths. Break condition: in graphs with uniform degree distributions or disjoint components, entropy measure may fail to differentiate candidate base-points.

## Foundational Learning

- **Concept:** Integrated Gradients (IG) & Baselines
  - Why needed: GB-IG is modification of fundamental IG axiom; cannot understand "Graph-based" extension without understanding standard IG integrates gradient output w.r.t input along straight line from baseline to input
  - Quick check: Can you explain why "zero vector" baseline is problematic for graphs but common in image processing?

- **Concept:** Geodesics vs. Euclidean Lines
  - Why needed: Core theoretical contribution relies on concept from Geometric Deep Learning that "shortest path" on manifold (or graph) is generalization of "straight line"
  - Quick check: In graph with unweighted edges, what algorithm computes the geodesic?

- **Concept:** Fidelity vs. Sparsity in XAI
  - Why needed: Paper evaluates success using these metrics; Fidelity measures if explanation identifies important features, Sparsity measures simplicity
  - Quick check: If explanation highlights 99% of nodes as "important," would it have high Fidelity? (Yes). Would it be useful? (Likely Noâ€”check Sparsity).

## Architecture Onboarding

- **Component map:** Path Finder -> Entropy Calculator -> Gradient Accumulator -> Aggregator
- **Critical path:** Base-point selection and path enumeration are pre-processing steps; gradient calculation depends entirely on specific paths identified; path-finding step is likely bottleneck for large, dense graphs
- **Design tradeoffs:** Fidelity vs. Sparsity (GB-IG has slightly lower sparsity but higher fidelity than IG-Gaussian); Completeness (GB-IG satisfies "path-based completeness" which is relaxation of standard IG completeness)
- **Failure signatures:** Negative Fidelity (explanation identifies "important" nodes whose removal increases model confidence); Computational Freeze (path finder not bounded on dense graphs)
- **First 3 experiments:**
  1. Generate "House" motif synthetic graph (ShapeGGen) and verify if GB-IG highlights motif nodes with higher Jaccard index compared to IG-Zero
  2. On Cora dataset, compare "Max Entropy" base-point selection against "Random Max Distance" selection to isolate value of information-theoretic heuristic
  3. Occlude top-k nodes identified by GB-IG and measure drop in classification probability vs. IG-Gaussian to confirm fidelity improvements shown in Table 2

## Open Questions the Paper Calls Out

### Open Question 1
How can the set of shortest paths in GB-IG be effectively pruned or limited to improve computational efficiency and explanation sparsity without significantly degrading fidelity? Basis: Conclusion states "For future work, we will investigate limiting number of paths in GB-IG. Reducing number of paths will reduce computational complexity and increase sparsity." Unresolved because current implementation accumulates attributions over all shortest paths resulting in lower sparsity and higher computational cost. Evidence: Ablation study demonstrating method for selecting subset of high-information paths that maintains high Fidelity and Jaccard scores while significantly improving Sparsity.

### Open Question 2
What is the optimal threshold for binarizing attributions to define "important" nodes in GB-IG, and how does this choice impact robustness of explanation? Basis: Section 5.1 notes "Following [1] the threshold is set to 0.8. Further investigation of appropriate threshold is left to future work." Unresolved because paper normalizes attributions to [0,1] but relies on arbitrary threshold (0.8) to convert continuous attributions into binary masks required for calculating Fidelity and Sparsity metrics. Evidence: Sensitivity analysis showing fluctuation of Fidelity and Jaccard index scores across range of threshold values (0.1 to 0.9) on synthetic datasets.

### Open Question 3
Can GB-IG methodology be generalized to weighted or directed graphs where definition of "shortest path" is structurally different from unweighted, undirected graphs? Basis: Methodology section explicitly restricts definition: "Given an unweighted graph G, a path..." and related work mentions extension applies to "undirected and unweighted graphs." Unresolved because algorithm relies on discrete hops between nodes; weighted graphs require integrating over varying edge costs, and directed graphs introduce reachability constraints that may violate "maximum distance" base-point assumption. Evidence: Formal extension of GB-IG equations for weighted adjacency matrices and empirical validation on standard weighted graph benchmarks.

## Limitations

- Computational complexity of enumerating all shortest paths in dense graphs is not explicitly addressed, limiting scalability
- Entropy-based base-point selection may not generalize well to graphs with uniform degree distributions or disjoint components
- Evaluation focuses on node classification tasks; applicability to other GNN tasks (e.g., graph classification) is not explored

## Confidence

- **High Confidence:** Theoretical foundation of GB-IG (path-based completeness) and superiority over standard IG on synthetic datasets with ground-truth explanations
- **Medium Confidence:** Performance gains on real-world datasets (Cora, CiteSeer, Pubmed), as ground-truth explanations are not available for direct comparison
- **Low Confidence:** Generalizability of entropy-based base-point selection heuristic across diverse graph topologies

## Next Checks

1. **Scalability Test:** Evaluate GB-IG on large, dense synthetic graphs to assess computational feasibility and path enumeration limits
2. **Base-point Ablation:** Compare entropy-based base-point selection against random or centrality-based heuristics on Cora/CiteSeer to isolate its contribution
3. **Task Generalization:** Apply GB-IG to graph classification benchmark (e.g., MUTAG) to test applicability beyond node classification