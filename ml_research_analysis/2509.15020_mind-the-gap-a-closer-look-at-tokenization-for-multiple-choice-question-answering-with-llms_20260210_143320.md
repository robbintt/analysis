---
ver: rpa2
title: 'Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering
  with LLMs'
arxiv_id: '2509.15020'
source_url: https://arxiv.org/abs/2509.15020
tags:
- space
- prompt
- option
- answer
- letter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The choice of whether to tokenize the space before the answer letter
  in multiple-choice question answering prompts significantly affects LLM accuracy
  and calibration. We find that tokenizing the space together with the letter consistently
  improves accuracy and reduces expected calibration error across diverse models and
  datasets, with gains up to 11%.
---

# Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs

## Quick Facts
- arXiv ID: 2509.15020
- Source URL: https://arxiv.org/abs/2509.15020
- Reference count: 26
- Key outcome: Tokenizing the space before answer letters improves LLM MCQA accuracy by up to 11% and reduces calibration error

## Executive Summary
This paper reveals that a seemingly trivial formatting choice—whether to tokenize the space before answer letters in multiple-choice question answering prompts—significantly impacts LLM performance. Through systematic evaluation across 14 datasets and models ranging from 1.8B to 72B parameters, the authors demonstrate that tokenizing the space together with the answer letter consistently improves both accuracy and calibration. This effect is robust across different prompt formats and languages, with gains up to 11% on certain benchmarks. The findings underscore the need for standardized evaluation protocols in LLM research, as this formatting choice can alter model rankings and create unfair comparisons.

## Method Summary
The authors evaluate two tokenization strategies for MCQA prompts ending with "Answer:": (1) "Letter token" where the space and letter are separate tokens, and (2) "Space-Letter token" where the space and letter form a single token. For each question, they extract next-token probabilities for answer options (A, B, C, D) under both conditions, using greedy decoding (temperature=0). They measure accuracy and Expected Calibration Error (ECE) with 10 bins, testing statistical significance via McNemar's test (accuracy) and paired bootstrap resampling (ECE), p<0.05. The evaluation covers open-weight models only, as proprietary API-based models don't expose next-token logits.

## Key Results
- Tokenizing space with answer letters improves accuracy by up to 11% across diverse models and datasets
- Expected Calibration Error (ECE) is consistently reduced with space-letter tokenization
- The effect persists across prompt variations, including parentheses, numeric labels, and different languages
- Model rankings can change based on tokenization choice, highlighting the need for standardized evaluation

## Why This Works (Mechanism)

### Mechanism 1: Default Tokenization Alignment
Tokenizing the space with the answer letter ("␣X") matches the tokenizer's natural output, producing probability signals that better align with the model's learned distribution. During pre-training, space-prefixed tokens commonly appear at answer boundaries, so using this token for prediction leverages patterns the model internalized. This effect could weaken if models were primarily fine-tuned on MCQA formats that explicitly separate space from answer letters.

### Mechanism 2: Probability Mass Concentration
Space-prefixed tokens yield more concentrated probability distributions, improving both accuracy and calibration. The logits for "␣X" tokens may have higher effective signal-to-noise because they function as complete semantic units in the vocabulary, reducing competition from spurious token alternatives during softmax. For extremely large models with highly diverse training, vocabulary token frequency effects may saturate.

### Mechanism 3: Token Identity Mismatch
Evaluating on "X" tokens when the prompt context primes "␣X" creates a token identity mismatch that degrades performance. The model's internal representations of "A" vs "␣A" differ semantically—standalone "A" may activate multiple meanings, while "␣A" in the MCQA suffix position more strongly binds to the "option label" concept. If models learn equivalent representations for both token types through explicit alignment training, effects should converge.

## Foundational Learning

### Concept: Byte-Pair Encoding (BPE) Tokenization
Understanding that tokenizers merge frequent character sequences means " A" (space+A) and "A" are distinct vocabulary entries with different embeddings and frequencies. Quick check: Given the prompt "Answer:", will the next character naturally form " A" or "A" as a token?

### Concept: Expected Calibration Error (ECE)
The paper measures reliability via ECE—how well a model's confidence matches its actual accuracy. Lower ECE means when the model says "80% confident," it's correct ~80% of the time. Quick check: If a model assigns 95% probability to its predicted answer but only gets 70% of such predictions correct, is it well-calibrated?

### Concept: Next-Token Probability Extraction for MCQA
The evaluation extracts the answer by comparing P("A"|prompt), P("B"|prompt), etc., then selecting the argmax. This assumes the relevant tokens exist in vocabulary and have meaningful probability mass. Quick check: Why might P(" A"|prompt) differ systematically from P("A"|prompt) even though they represent the same answer choice?

## Architecture Onboarding

### Component Map:
MCQA Prompt → [Tokenizer] → Token Ids → [LLM Forward Pass] → Logits → [Extract specific token logits] → [Softmax over answer tokens] → [Argmax selection] → Predicted Answer → [Compare to ground truth] → [Compute Accuracy + ECE] → Metrics

### Critical Path:
1. Prompt construction: End with "Answer:"—space handling decision happens here
2. Tokenization: Critical branch—tokenize as [..., ":", " ", "X"] vs [..., ":", " X"]
3. Logit extraction: Must query the SAME token type used in tokenization
4. Probability comparison: Ensure valid probability distribution over answer tokens

### Design Tradeoffs:
| Strategy | Pros | Cons |
|----------|------|------|
| "X" (Letter token) | Matches how options appear in prompt list | Mismatches tokenizer's default continuation; lower accuracy |
| "␣X" (Space-Letter token) | Matches default tokenization; higher accuracy & calibration | Tokens don't match option-list presentation format |

Paper recommendation: Use "␣X" tokenization consistently for evaluation and comparison.

### Failure Signatures:
- >5% accuracy swing from tokenization change → Tokenization convention not standardized in your pipeline
- Model rankings reorder across runs → Check for implicit tokenization differences in evaluation frameworks
- High ECE despite reasonable accuracy → Tokenization may be extracting from suboptimal token representation
- Inconsistent results across prompt templates → Tokenization effect may be interacting with other prompt variations

### First 3 Experiments:
1. Baseline replication: Run your model on MMLU with both "X" and "␣X" tokenization to quantify the effect magnitude specific to your setup.
2. Prompt template sensitivity: Test whether the effect holds across your production prompt formats (few-shot, CoT, different label styles).
3. Calibration audit: Compare ECE across tokenization strategies to determine if confidence estimates are reliable enough for downstream use cases (e.g., abstention thresholds).

## Open Questions the Paper Calls Out

### Open Question 1
Does the space tokenization effect persist in models larger than 72B parameters, or does it diminish with increased model scale? The Limitations section states: "Testing our findings with large-scale LLMs remains for future work." The authors were computationally constrained to models up to 72B; even their largest model (Qwen 2.5 72B) showed an 11% accuracy gain on HellaSwag, suggesting the effect may not diminish with scale.

### Open Question 2
What is the underlying mechanism causing tokenizing the space with the letter to improve accuracy and calibration? The paper demonstrates the effect robustly but does not provide a causal explanation for why this tokenization choice improves performance, despite offering two rationales in Appendix A. The authors present token similarity and default tokenization arguments but do not experimentally validate which mechanism (if either) explains the observed improvements.

### Open Question 3
How can the tokenization effect be assessed for closed-source API-based models that do not expose next-token logits? The Limitations section notes: "Our evaluation focuses on open-weight models, as we require access to all next-token logits, which are not provided for proprietary, API-based models." Many widely-used LLMs (e.g., GPT-4, Claude) only provide generated text outputs, making the tokenization comparison impossible with standard APIs.

## Limitations

- The findings are limited to English-language MCQA with standard 4-option formats; impact on non-English or open-ended multiple-choice tasks remains untested.
- The mechanism explanations are largely inferential, showing correlation between tokenization choice and performance but not definitively proving BPE-level tokenization artifacts are the primary driver.
- The calibration improvements, though statistically significant, may have diminishing practical returns depending on specific use cases and acceptable error rates.

## Confidence

**High Confidence**: The empirical finding that tokenizing the space together with the answer letter consistently improves accuracy and calibration across diverse models and datasets. The statistical significance tests are appropriate and results are robust.

**Medium Confidence**: The mechanism explanations linking the effect to BPE tokenization patterns and probability mass concentration. While the evidence supports these mechanisms, alternative explanations cannot be ruled out without additional controlled experiments.

**Medium Confidence**: The practical recommendation to standardize tokenization for fair LLM evaluation. This follows logically from the empirical findings, but the broader implications for the evaluation ecosystem require further validation.

## Next Checks

1. **Cross-linguistic validation**: Test the tokenization effect on multiple-choice question answering datasets in languages with different tokenization characteristics (e.g., Chinese, Arabic, Finnish) to validate whether the effect generalizes beyond English BPE patterns.

2. **Mechanism isolation experiment**: Design a controlled experiment where models are explicitly fine-tuned with both tokenization variants to determine if learned representations can overcome the tokenization effect, helping distinguish between pre-training artifacts versus fundamental representational differences.

3. **Downstream task impact assessment**: Evaluate whether the calibration improvements translate to better decision-making in practical applications that use MCQA outputs as inputs (e.g., automated grading, knowledge assessment) to validate practical significance beyond the benchmark setting.