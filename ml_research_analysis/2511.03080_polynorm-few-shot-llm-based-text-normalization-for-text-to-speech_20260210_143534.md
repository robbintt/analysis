---
ver: rpa2
title: 'PolyNorm: Few-Shot LLM-Based Text Normalization for Text-to-Speech'
arxiv_id: '2511.03080'
source_url: https://arxiv.org/abs/2511.03080
tags:
- normalization
- text
- languages
- language
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PolyNorm, a few-shot LLM-based text normalization
  system for text-to-speech. It uses prompt engineering and in-context learning with
  high-quality multilingual examples to replace rule-based systems.
---

# PolyNorm: Few-Shot LLM-Based Text Normalization for Text-to-Speech

## Quick Facts
- arXiv ID: 2511.03080
- Source URL: https://arxiv.org/abs/2511.03080
- Authors: Michel Wong; Ali Alshehri; Sophia Kao; Haotian He
- Reference count: 8
- Primary result: GPT-4o achieves WER as low as 4.17% in German and 7.88% in Japanese across 8 languages

## Executive Summary
PolyNorm introduces a prompt-based approach to text normalization for text-to-speech using Large Language Models. The system leverages in-context learning with few-shot prompting and high-quality multilingual examples to replace traditional rule-based systems. Experiments across eight languages show significant improvements over production baselines, with up to 65% WER reduction. The approach reduces manual engineering effort while maintaining cross-lingual consistency through a unified prompt design.

## Method Summary
PolyNorm uses few-shot prompting with GPT-4o/GPT-4o-mini to perform text normalization without fine-tuning. The method employs a 3-component prompt structure: English instruction prompt, localized ICL examples (80-100 per language), and target input. Development follows a hillclimbing iteration process where error analysis identifies underperforming categories, triggering targeted ICL example additions. The system is language-agnostic, using the same prompt format across eight languages, with only Japanese requiring supplementary katakana handling.

## Key Results
- GPT-4o achieves WER as low as 4.17% in German and 7.88% in Japanese
- Up to 65% WER reduction compared to production baseline across 8 languages
- Introduces PolyNorm-Benchmark, a multilingual dataset of 27 normalization categories with 540 examples per language

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In-context learning with few-shot prompting enables LLMs to perform text normalization without fine-tuning.
- **Mechanism:** Curated examples across 27 normalization categories are provided in the prompt, allowing the model to pattern-match from explicit demonstrations.
- **Core assumption:** The LLM's pre-trained multilingual knowledge can be reliably activated through exemplar-guided prompting rather than weight updates.
- **Evidence anchors:**
  - [abstract]: "We propose PolyNorm, a prompt-based approach to TN using Large Language Models (LLMs)... minimal human intervention"
  - [section 3]: "We adopt a few-shot prompting strategy using ICL examples to guide the LLM in performing text normalization across all languages"
  - [corpus]: Related work (Zhang et al., 2024) achieved ~40% lower error rates with GPT-based TN
- **Break condition:** If ICL examples are unrepresentative of edge cases or if the model lacks sufficient pre-training in a target language, performance degrades.

### Mechanism 2
- **Claim:** Language-agnostic prompt design with localized examples reduces engineering effort while maintaining cross-lingual consistency.
- **Mechanism:** A unified English instruction prompt is paired with language-specific ICL examples (80-100 per language), allowing deployment across 8 languages without re-architecting the system.
- **Core assumption:** Normalization categories transfer across languages despite differing surface realizations.
- **Evidence anchors:**
  - [abstract]: "language-agnostic pipeline for automatic data curation and evaluation"
  - [section 3.1]: "We use a unified prompt format across eight languages, varying only the localized examples"
  - [corpus]: Dialect Normalization paper (2025) similarly combines LLMs with morphological rules for low-resource varieties
- **Break condition:** Languages with significant morphological complexity or non-whitespace tokenization require additional handling beyond the current pipeline.

### Mechanism 3
- **Claim:** Iterative error analysis with ICL example refinement systematically improves normalization accuracy.
- **Mechanism:** Development set outputs are compared against expert-verified references; underperforming categories trigger targeted ICL example additions. This "hillclimbing" process reduced GPT-4o WER from iteration 2 (e.g., Japanese 12.32%) to iteration 3 (7.88%).
- **Core assumption:** Errors cluster in identifiable patterns that can be addressed through additional exemplars rather than architectural changes.
- **Evidence anchors:**
  - [section 3.2]: "We analyzed discrepancies... identified categories or patterns where the model underperformed, then revised or added ICL examples focused on these error types"
  - [tables 5-6]: Iteration 2 vs. 3 comparison shows WER improvements across most language-category pairs
- **Break condition:** If errors are sparse and non-systematic, or if adding examples causes prompt length issues, iteration yields diminishing returns.

## Foundational Learning

- **Concept: Text Normalization (TN) in TTS**
  - **Why needed here:** TN converts written forms (e.g., "17°", "$5.00", "NASA") into spoken equivalents ("seventeenth", "five dollars", "NASA"). Understanding this preprocessing step is essential for evaluating PolyNorm's contributions.
  - **Quick check question:** Given "Dr. Smith lives at 123 Main St.", what normalized form should a TTS system receive?

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** PolyNorm relies entirely on ICL—providing examples in the prompt—rather than fine-tuning. This is the core technical lever for reducing manual engineering.
  - **Quick check question:** How does ICL differ from traditional supervised fine-tuning in terms of what gets updated?

- **Concept: WER vs. BLEU Metrics**
  - **Why needed here:** The paper reports both WER (word error rate, lower is better) and BLEU (surface similarity, higher is better). WER captures normalization errors more directly; BLEU reflects overall similarity. Chinese/Japanese use Character Error Rate instead of WER.
  - **Quick check question:** If a system outputs "twenty twenty three" instead of "two thousand twenty three" for "2023", is this a semantic error or a stylistic variant?

## Architecture Onboarding

- **Component map:** Instruction Prompt -> ICL Example Bank -> Target Input Handler -> LLM Backend -> Evaluation Pipeline -> Iteration Loop
- **Critical path:** ICL example quality -> Prompt construction -> LLM inference -> Expert review -> Error clustering -> ICL augmentation
- **Design tradeoffs:**
  - Prompt length vs. coverage: More ICL examples improve accuracy but increase latency and cost
  - Expert curation vs. synthetic data: PolyNorm uses expert-verified examples for ICL; synthetic (DeepSeek) for benchmark
  - Unified vs. language-specific prompts: Only Japanese required special handling; assumption is this generalizes, but low-resource languages may need more
- **Failure signatures:**
  - Context ambiguity: "3-2" normalized as "three minus two" instead of "three to two" in sports scores
  - Tokenization errors in CJK: Japanese/Chinese character segmentation can conflate normalization with tokenization issues
  - Category drift: Unseen normalization types may fallback to suboptimal heuristics
  - Bias propagation: Gender ambiguities in languages like Arabic may resolve incorrectly without explicit handling
- **First 3 experiments:**
  1. **Baseline replication:** Run PolyNorm prompt with GPT-4o on PolyNorm-Benchmark for a single language (e.g., German) to confirm WER ≈4.17%
  2. **ICL ablation:** Reduce ICL examples from 80-100 to 20 per language and measure WER degradation
  3. **Cross-lingual transfer test:** Use English ICL examples for Spanish normalization (no localized examples) to assess language-agnostic claim boundaries

## Open Questions the Paper Calls Out
- How can suprasegmental features like pitch accent and tone prediction be integrated into the normalization pipeline without disrupting naturalness?
- Can targeted prompting effectively restore diacritics in languages like Arabic and Hebrew where markers determine grammatical and phonetic meaning?
- How well does the reduction in Word Error Rate (WER) correlate with actual improvements in downstream Text-to-Speech naturalness and intelligibility?

## Limitations
- ICL Example Dependency and Reproducibility: The core technical contribution relies on 80-100 expert-curated ICL examples per language, which are not publicly released, creating significant reproducibility barriers.
- Language Resource Sensitivity: Performance varies notably across languages (German: 4.17% WER vs. Lithuanian: 6.99% WER), suggesting the approach may be less effective for low-resource or morphologically complex languages.
- Category Coverage Gaps: While 27 normalization categories are claimed, the paper doesn't exhaustively validate coverage of edge cases like ambiguous numerical contexts or novel URL formats.

## Confidence
- **High Confidence Claims:**
  - Few-shot prompting with GPT-4o significantly outperforms the production baseline (65% WER reduction)
  - PolyNorm-Benchmark is a valid multilingual evaluation dataset with appropriate category coverage
  - The prompt engineering approach reduces manual engineering effort compared to rule-based systems
- **Medium Confidence Claims:**
  - The approach generalizes across 8 diverse languages with consistent methodology
  - Iterative ICL refinement systematically improves performance
  - Language-agnostic prompt design is sufficient for most languages
- **Low Confidence Claims:**
  - "Minimal human intervention" truly applies across all deployment scenarios
  - The approach scales effectively to truly low-resource languages not in the current 8
  - Hillclimbing iteration will continue to yield significant improvements beyond iteration 3

## Next Checks
1. **ICL Example Ablation Study:** Systematically reduce ICL examples from 80-100 to 20-30 per language and measure WER degradation to quantify dependency on example quantity.
2. **Cross-Lingual Transfer Test:** Use English ICL examples to normalize Spanish text (no localized examples) and measure performance degradation to validate language-agnostic claim boundaries.
3. **Low-Resource Language Extension:** Apply the PolyNorm approach to a genuinely low-resource language using only 20 ICL examples and compare against a simple rule-based baseline.