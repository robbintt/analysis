---
ver: rpa2
title: Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation
  Steering
arxiv_id: '2511.00617'
source_url: https://arxiv.org/abs/2511.00617
tags:
- gid00001
- gid00068
- gid00083
- gid00064
- steering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops a unifying Bayesian model of how large language
  models (LLMs) can be controlled at inference time using both in-context learning
  (ICL) and activation steering. The key insight is that both methods operate by updating
  the model''s belief in latent concepts: ICL accumulates evidence through likelihood
  functions, while activation steering directly alters concept priors.'
---

# Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering

## Quick Facts
- arXiv ID: 2511.00617
- Source URL: https://arxiv.org/abs/2511.00617
- Reference count: 40
- Primary result: Unifies in-context learning and activation steering as Bayesian belief updates with predictive accuracy r=0.98

## Executive Summary
This paper develops a unifying Bayesian model explaining how large language models can be controlled through both in-context learning (ICL) and activation steering. The key insight is that both methods update the model's belief in latent concepts: ICL accumulates evidence through likelihood functions while steering directly alters concept priors. This framework yields a closed-form model predicting LLM behavior across both intervention types, successfully explaining sigmoidal learning curves, predictable steering shifts, and the additivity of both interventions in log-belief space.

## Method Summary
The method computes Context-Aligned Activation (CAA) steering vectors by contrasting activations on concept-matching versus non-matching prompts, then applies these vectors at identified optimal layers during inference. ICL behavior is modeled as Bayesian evidence accumulation with power-law discounting (N^(1-α)), while steering is modeled as linear shifts in log-prior odds. The 4-parameter model (α, γ, a, b) maps steering magnitude m and shot count N to persona-matching probability via a sigmoid transformation. Validation uses 10-fold cross-validation on held-out magnitude values.

## Key Results
- Successfully unifies ICL and activation steering as Bayesian belief updates
- Predicts sigmoidal learning curves (vs N^(1-α)) and steering response curves with r=0.98 correlation
- Demonstrates additivity of interventions in log-belief space creating phase boundaries
- Achieves high predictive accuracy across multiple LLM architectures and datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In-context learning updates model beliefs via sub-linear evidence accumulation, producing sigmoidal learning curves.
- **Mechanism:** Each in-context example contributes evidence for concept c, but the Bayes factor scales as γN^(1−α) rather than linearly, reflecting power-law discounting of evidence. This yields posterior probability p(c|x) = σ(−log o(c|x)) with sigmoidal dynamics when plotted against N^(1−α).
- **Core assumption:** LLMs learn latent concepts during pretraining that are evoked at inference; evidence accumulation follows a power-law discount factor τ(N) = N^−α derived from scaling laws observed in ICL.
- **Evidence anchors:**
  - [abstract] "ICL as Bayesian inference where evidence accumulates sub-linearly"
  - [section 4.1] "we follow Wurgaft et al. (2025) and model evidence accumulation as sub-linear by multiplying the log-likelihood by a discount factor τ(N)"
  - [corpus] Neighbor work "The Shape of Beliefs" studies belief geometry and dynamics in LLM posteriors, supporting Bayesian framing.
- **Break condition:** If the concept c is not represented in the model's latent space (e.g., novel concepts absent from pretraining), no amount of ICL can accumulate meaningful evidence.

### Mechanism 2
- **Claim:** Activation steering linearly shifts log-prior odds in concept space, producing predictable sigmoidal behavior changes.
- **Mechanism:** Under the Linear Representation Hypothesis, hidden states v decompose as weighted sums of orthogonal concept vectors. Steering by magnitude m along direction d_i adds a·m to the log posterior odds, which the model interprets as a prior shift from p(c)/p(c') to p'(c)/p'(c').
- **Core assumption:** Concepts are linearly accessible, steerable, and additively represented; LRH holds within a bounded magnitude range.
- **Evidence anchors:**
  - [abstract] "steering operates by changing concept priors"
  - [section 4.2] "steering yields a constant shift in the model log-posterior odds... We therefore argue the effects of steering are best described as alteration of a model's prior beliefs"
  - [corpus] "Manipulating Transformer-Based Models" surveys controllability across prompts, activations, and weights, supporting intervention mechanisms.
- **Break condition:** At extreme steering magnitudes, LRH breaks down—behavior converges to chance (p=0.5) rather than continuing to shift monotonically; representations move outside the support where P(c|v) is defined.

### Mechanism 3
- **Claim:** ICL and steering combine additively in log-belief space, creating phase boundaries with sharp behavioral transitions.
- **Mechanism:** Since log o(c|x) = a·m + b + γN^(1−α), interventions are additive in log-odds. The crossover point N*(m) where belief flips from c' to c follows a predictable function: N*(m) = (−am + b)/γ)^(1/(1−α)). Small parameter changes near this boundary induce sudden behavioral shifts.
- **Core assumption:** Independence of intervention effects in log-space; single optimal steering layer per concept.
- **Evidence anchors:**
  - [abstract] "additivity of both interventions in log-belief space, which results in distinct phases such that sudden and dramatic behavioral shifts can be induced"
  - [section 4.3] "The boundary between phases—the cross-over point N* when belief in concept c surpasses belief in c'—can be predicted"
  - [corpus] Corpus lacks direct replication of phase boundary predictions; this appears novel.
- **Break condition:** If steering magnitude exceeds the linear range or if concept representations are non-linear, additivity fails and phase prediction degrades.

## Foundational Learning

- **Concept:** **Bayesian posterior odds and Bayes factors**
  - Why needed here: The entire framework formalizes belief as posterior odds o(c|x) = p(c|x)/p(c'|x), decomposed into priors and likelihood ratios. Understanding log-odds transformations is essential for grasping why sigmoidal curves emerge.
  - Quick check question: If p(c|x) = 0.8, what is log o(c|x)?

- **Concept:** **Linear Representation Hypothesis (LRH)**
  - Why needed here: Justifies why steering vectors exist and why they produce linear effects in log-space. LRH posits that concepts correspond to directions in representation space that can be read/written linearly.
  - Quick check question: If a representation v = 2d₁ + 3d₂ where d₁,d₂ are orthogonal concept vectors, what happens if we add 0.5d₁?

- **Concept:** **Sigmoidal response functions**
  - Why needed here: Both ICL curves (vs N^(1−α)) and steering response (vs m) exhibit sigmoidal shapes because they operate on log-odds transformed through σ. This explains the "sudden learning" phenomenon near transition points.
  - Quick check question: At what log-odds value does the sigmoid σ(log-odds) equal 0.5?

## Architecture Onboarding

- **Component map:** Compute CAA steering vectors from contrasting datasets → Select optimal steering layer via layer sweep → Apply steering vectors during inference → Collect behavioral data across m and N → Fit 4-parameter Bayesian model → Validate via cross-validation

- **Critical path:**
  1. Identify target concept and construct contrasting datasets
  2. Scan layers to find optimal steering layer ℓ* (peak effect at m=±1)
  3. Extract steering vectors at ℓ* using CAA
  4. Collect behavioral data across m ∈ [−10, +10] and N ∈ {0, 1, ..., 128}
  5. Fit model parameters using L-BFGS with cross-validation
  6. Validate by predicting held-out phase boundaries N*(m)

- **Design tradeoffs:**
  - **Layer selection:** Single-layer steering vs multi-layer; paper finds one layer dominates effect but this may not generalize to all concepts
  - **Magnitude range:** Must stay within linear regime; too large m causes breakdown to chance behavior
  - **Fitting strategy:** Model fits population averages (not individual samples), trading granularity for predictability

- **Failure signatures:**
  - Steering has no effect → Concept may not be linearly represented (some models like phi-4-mini showed this)
  - Behavior plateaus at p=0.5 at high m → LRH breakdown; moved outside valid representation region
  - Poor cross-validation fit → Concept lacks sufficient signal in likelihood p(x|c); try different datasets

- **First 3 experiments:**
  1. **Layer sweep:** For each even layer, compute steering vector on target concept, apply at m=±1 with N=1 shot, measure persona-matching probability. Select layer with largest |p(m=+1) − p(m=−1)|.
  2. **ICL-only baseline:** With m=0, vary N from 0 to 128 shots. Fit sigmoid to p(c|x) vs N^(1−α) to estimate α and verify sub-linear scaling.
  3. **Phase boundary prediction:** Using 10-fold cross-validation (holding out 3 adjacent m values per fold), fit full model and predict N* for held-out magnitudes. Compute correlation r between predicted and empirical crossover points.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the belief dynamics model generalize to non-binary concept spaces where belief can vary along multiple directions simultaneously?
  - Basis in paper: [explicit] "One limitation of this work is that we only consider binary concepts... Future work may explore how our theory and model generalize to non-binary concept spaces, where there may be more than one direction for belief to vary across."
  - Why unresolved: The current model assumes a single latent concept c and its complement c′, but real-world concepts may have multiple competing or overlapping dimensions.
  - What evidence would resolve it: Extending the framework to multi-dimensional concept spaces and testing whether additivity still holds when steering along multiple concept directions.

- **Open Question 2:** Why do normalized steering vectors computed over multiple ICL shots exhibit weaker effects than single-shot vectors?
  - Basis in paper: [explicit] "we found that, counter-intuitively, after vectors were normalized to equal magnitude, steering vectors computed over multiple shots had an even weaker effect than steering with vectors computed over a single query. Further work is required to explain this effect."
  - Why unresolved: This suggests that belief representations may shift as context accumulates, potentially representing the same concept differently across context lengths.
  - What evidence would resolve it: Analyzing how steering vector directions rotate or change in semantic space as shot count increases, and testing whether alternative vector extraction methods (e.g., SAE-based) show similar behavior.

- **Open Question 3:** Why does steering fail to affect behavior in some models (e.g., phi-4-mini-instruct), and does this indicate non-linear belief representation or methodological limitations?
  - Basis in paper: [explicit] "we found cases with some LLMs (e.g. phi-4-mini-instruct) where steering had no clear effect on behavior—this may suggest that these models represent belief in a non-linear way, or it may indicate a limitation of our particular method for constructing steering vectors."
  - Why unresolved: It remains unclear whether the linear representation hypothesis simply fails for certain architectures, or whether CAA is insufficient to extract valid steering directions in these cases.
  - What evidence would resolve it: Testing alternative steering methods (SAE-based, supervised probing) on non-responsive models and measuring whether concept information is linearly accessible via probes even when steering fails.

- **Open Question 4:** Are concept likelihood functions implemented non-linearly in specific layers, and what is their spatial relationship to linear belief representations?
  - Basis in paper: [explicit] "are concept likelihood functions implemented in a non-linear way, and are they implemented in earlier or later layers relative to this linear belief representation?"
  - Why unresolved: The model assumes likelihood accumulation via context, but the mechanistic implementation within transformer layers—particularly where Bayesian inference occurs—remains unknown.
  - What evidence would resolve it: Layer-wise causal interventions on attention heads or MLP components during ICL to identify which circuit elements contribute to likelihood vs. prior updates.

## Limitations

- The framework assumes pretraining-derived concepts; novel concepts may not be steerable via CAA vectors
- Linear Representation Hypothesis may break down at extreme steering magnitudes, causing behavior to converge to chance
- The assumption of single dominant steering layer may not generalize to all concepts or model architectures

## Confidence

- **High Confidence:** The Bayesian framing of ICL as evidence accumulation with power-law discounting is well-supported by empirical sigmoidal curves and extensive prior work on scaling laws. The predictive accuracy (r=0.98) across multiple architectures provides strong validation.
- **Medium Confidence:** The Linear Representation Hypothesis explanation of activation steering is theoretically sound but may not hold for all concepts or at extreme steering magnitudes. The phi-4-mini counterexample suggests architectural dependencies.
- **Medium Confidence:** The additivity of ICL and steering in log-belief space is demonstrated but relies on single-layer steering and may not generalize to multi-concept scenarios or when both interventions are applied simultaneously at non-optimal layers.

## Next Checks

1. **Architectural Generalization Test:** Apply the unified framework to a diverse set of model architectures (including smaller models like phi-4-mini) and systematically document which concepts show linear representations vs. non-linear behavior, establishing architectural boundaries of the LRH assumption.

2. **Multi-Concept Steering Experiment:** Design experiments where steering vectors target multiple concepts simultaneously (e.g., combining Dark Triad traits) to test whether additivity holds in higher-dimensional belief space and whether steering vectors remain orthogonal or interfere.

3. **Novel Concept Transferability:** Create synthetic concepts absent from pretraining and attempt both ICL and steering interventions. Measure whether ICL can accumulate evidence for truly novel concepts or whether the framework's assumption of pretraining-derived concepts is fundamental to its predictive power.