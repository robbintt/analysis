---
ver: rpa2
title: Imitation from Observations with Trajectory-Level Generative Embeddings
arxiv_id: '2601.00452'
source_url: https://arxiv.org/abs/2601.00452
tags:
- expert
- learning
- offline
- reward
- suboptimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses offline imitation learning from observations
  (LfO), where expert demonstrations are scarce and only state observations are available,
  while the offline dataset consists of suboptimal trajectories. The key challenge
  is extracting a reliable learning signal when expert and suboptimal data have disjoint
  support.
---

# Imitation from Observations with Trajectory-Level Generative Embeddings

## Quick Facts
- **arXiv ID**: 2601.00452
- **Source URL**: https://arxiv.org/abs/2601.00452
- **Reference count**: 40
- **Key outcome**: TGE matches or exceeds state-of-the-art offline LfO baselines on D4RL benchmarks, especially with limited expert data.

## Executive Summary
This paper addresses the challenge of offline imitation learning from observations (LfO) where expert demonstrations are scarce and only state observations are available, while the offline dataset consists of suboptimal trajectories. The core difficulty is extracting a reliable learning signal when expert and suboptimal data have disjoint support. The authors propose Trajectory-level Generative Embeddings (TGE), which uses a temporal diffusion model to learn a latent trajectory embedding space from the suboptimal data. A kernel-based surrogate reward is then constructed by measuring the distance of each trajectory to expert-like neighbors in this embedding space, enabling robust policy learning even under severe support mismatch. Empirically, TGE consistently matches or outperforms state-of-the-art offline LfO baselines across D4RL locomotion and manipulation benchmarks, especially in settings with limited expert data.

## Method Summary
TGE learns a latent trajectory embedding space using a temporal diffusion model trained on suboptimal trajectories from an offline dataset. The diffusion encoder maps trajectory segments to 64-dimensional embeddings, which are normalized to a unit hypersphere. For each state transition, rewards are computed as the negative log-distance to the m=10 nearest expert-like embeddings using a heavy-tailed logarithmic kernel. This surrogate reward approximates behavioral similarity and enables offline RL algorithms (IQL or ReBRAC) to learn policies that mimic expert behavior. The method is particularly effective when expert data is scarce and expert/suboptimal distributions have limited overlap.

## Key Results
- TGE achieves state-of-the-art performance on D4RL locomotion and manipulation tasks with limited expert data.
- The method significantly outperforms density-ratio approaches (DILO, SMiRL) in few-expert settings.
- Ablation studies confirm the importance of temporal context (H=32 vs H=1) and the logarithmic kernel for reward separability.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A trajectory diffusion model's encoder produces latent embeddings that naturally separate expert-like from suboptimal trajectories without discriminative supervision.
- **Mechanism**: The denoising score-matching objective forces the model to learn temporally coherent features robust to noise. Because the model must reconstruct clean trajectories across noise levels, the latent space organizes by behavioral similarity rather than superficial single-step features.
- **Core assumption**: The suboptimal dataset contains enough variation that expert-like behaviors form distinguishable clusters in the learned embedding space.
- **Evidence anchors**: T-SNE visualization shows expert and suboptimal samples form distinct clusters with clear boundaries.

### Mechanism 2
- **Claim**: Particle-based entropy estimation using k-nearest neighbor distances provides a theoretically grounded surrogate reward equivalent to maximizing expert likelihood.
- **Mechanism**: Cross-entropy between occupancy measures can be approximated as the log-distance between behavioral samples and their expert neighbors. Maximizing cumulative reward using the kernel formulation is equivalent to minimizing this cross-entropy.
- **Core assumption**: The embedding space distances meaningfully reflect behavioral similarity.
- **Evidence anchors**: Derivation shows H(ρπ, ρE) ≈ E[r(zi)], linking reward maximization to cross-entropy minimization.

### Mechanism 3
- **Claim**: The logarithmic kernel with heavy-tailed decay provides dense, graded signals even under severe support mismatch between expert and offline distributions.
- **Mechanism**: Unlike Gaussian kernels that produce sharply concentrated rewards, the log kernel f(d) = -log(1+d) decays slowly, preserving relative ordering of trajectories. This ensures suboptimal states far from expert support still receive distinguishable, non-zero rewards rather than collapsed near-zero values.
- **Core assumption**: Graded feedback is preferable to binary expert/non-expert signals for guiding policy improvement.
- **Evidence anchors**: Analysis contrasts density-ratio methods with TGE's distance-shaped reward, showing better separability.

## Foundational Learning

- **Denoising Diffusion Probabilistic Models (DDPMs)**
  - *Why needed here*: TGE repurposes the diffusion encoder as a representation learner; understanding the forward/reverse process and denoising objective is essential.
  - *Quick check question*: Can you explain why minimizing the noise prediction loss in Equation 4 induces semantically meaningful representations?

- **Offline RL algorithms (IQL, ReBRAC)**
  - *Why needed here*: TGE produces rewards but relies on standard offline RL backbones for policy optimization; understanding their in-sample constraints is critical.
  - *Quick check question*: Why does IQL use expectile regression for value learning, and how does this relate to offline RL's out-of-distribution action problem?

- **k-Nearest Neighbor Entropy Estimation**
  - *Why needed here*: The reward formulation derives from non-parametric entropy estimation; understanding the volume-based estimator clarifies why log-distance works.
  - *Quick check question*: Why does averaging over m nearest neighbors (rather than using just the nearest) improve estimator stability?

## Architecture Onboarding

- **Component map**: Temporal Diffusion Encoder (U-Net ϕθ) -> Embedding Normalizer -> k-NN Reward Module -> Offline RL Backbone

- **Critical path**: Train diffusion model on Dμ (~4-6 hours on L40S) → Encode all trajectories, normalize to sphere → Compute log-kernel distances to expert embeddings → Train offline RL policy on reward-augmented dataset

- **Design tradeoffs**:
  - *Horizon H*: Longer H captures more temporal context but increases compute; H=32 is default, H=1 severely degrades performance
  - *Kernel choice*: Log kernel provides better separability than Gaussian; avoid Gaussian despite its familiarity
  - *Backbone selection*: IQL is more stable; ReBRAC with adaptive BC weighting may perform better but requires tuning βactor per domain

- **Failure signatures**:
  - *Collapsed rewards*: If expert and suboptimal embeddings overlap significantly, reward variance drops → check embedding visualization
  - *Poor performance with short horizons*: H<8 shows degradation → ensure horizon is sufficient
  - *Instability on human demonstrations*: If cloned data works but human data fails, may need different backbone or BC coefficient

- **First 3 experiments**:
  1. **Sanity check**: Train diffusion on a simple D4RL dataset (e.g., halfcheetah-medium), visualize embeddings with t-SNE colored by ground-truth trajectory quality. Confirm clustering before proceeding.
  2. **Ablation horizon**: Run H ∈ {1, 4, 16, 32} on walker2d-medium with IQL backbone. Verify performance scales with temporal context as in Figure 3.
  3. **Kernel comparison**: Compare log vs Gaussian kernel reward distributions on hopper-random. Confirm log kernel produces heavier-tailed, more separable rewards.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can TGE's trajectory-level latent reward framework be effectively extended to online fine-tuning settings where agents can interact with the environment after offline pre-training?
- **Basis in paper**: [explicit] Conclusion states: "Future work could explore extending this trajectory-level latent reward framework to online fine-tuning settings."
- **Why unresolved**: The current method is purely offline and does not address how to maintain or adapt the generative embedding space when new experience becomes available during online interaction.
- **What evidence would resolve it**: Empirical results showing TGE-initialized policies can successfully fine-tune online, with analysis of whether the embedding space requires recalibration during online learning.

### Open Question 2
- **Question**: Can larger-scale video diffusion models enable TGE to perform imitation from cross-embodiment observations (e.g., learning from human videos to control robots)?
- **Basis in paper**: [explicit] Conclusion states: "Future work could explore... leveraging larger-scale video diffusion models to enable imitation from cross-embodiment observations."
- **Why unresolved**: The current experiments use state-based observations from the same embodiment; the generalization of trajectory-level embeddings across different observation modalities and embodiments remains untested.
- **What evidence would resolve it**: Demonstrations of successful policy transfer from video demonstrations of different embodiments using video diffusion encoders with the TGE reward formulation.

### Open Question 3
- **Question**: Under what conditions do density-ratio-based methods (like DILO) outperform TGE, particularly for datasets generated by consistent behavioral policies?
- **Basis in paper**: [inferred] Table 1 shows TGE underperforms DILO on cloned+expert Adroit tasks, with the paper suggesting "density-ratio-based methods such as DILO may be better able to exploit the resulting structural overlap."
- **Why unresolved**: The paper does not characterize the dataset properties (policy consistency, distribution structure) that determine when each approach is preferable.
- **What evidence would resolve it**: Systematic experiments varying behavioral policy consistency and distribution overlap, identifying regimes where TGE vs. density-ratio methods excel.

## Limitations

- The method requires significant computational overhead for training the trajectory diffusion model (4-6 hours on a single GPU), which may limit practical applicability in resource-constrained settings.
- The approach assumes the offline dataset contains sufficient variation to form distinguishable expert-like clusters in the embedding space; if the suboptimal data is too uniformly poor, clustering may fail.
- TGE underperforms density-ratio methods (DILO) on certain tasks with cloned expert data, suggesting the method may not be universally superior across all data regimes.

## Confidence

- **High**: Empirical performance claims on D4RL benchmarks; ablation showing importance of temporal context and kernel choice.
- **Medium**: Theoretical justification for reward formulation via entropy estimation; mechanism claims about diffusion encoder's representation learning.
- **Low**: Generalizability to domains with severe support mismatch or highly constrained action spaces; robustness to different offline RL backbones beyond IQL/ReBRAC.

## Next Checks

1. **Stress test support mismatch**: Evaluate TGE on a synthetic dataset where expert and suboptimal trajectories have minimal overlap (e.g., different gaits in locomotion tasks). Measure performance degradation and analyze embedding space separability.

2. **Cross-domain transfer**: Apply TGE-trained embeddings from one locomotion task (e.g., halfcheetah) to a different task (e.g., walker2d) with no shared dynamics. Assess whether the learned representation generalizes or overfits to the source task's behavior manifold.

3. **Real-world noisy observations**: Test TGE on datasets with partial observability or sensor noise (e.g., occlusion in images or delayed states). Verify that the diffusion encoder's denoising capability translates to robustness under realistic observation perturbations.