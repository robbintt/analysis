---
ver: rpa2
title: Combinatorial Bandit Bayesian Optimization for Tensor Outputs
arxiv_id: '2602.00640'
source_url: https://arxiv.org/abs/2602.00640
tags:
- tensor
- kernel
- regret
- where
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses Bayesian optimization (BO) for tensor-valued\
  \ black-box functions, introducing TOBO (Tensor-Output BO) and TOCBBO (Combinatorial\
  \ Tensor-Output BO). TOBO employs a novel Tensor-Output Gaussian Process (TOGP)\
  \ with two kernel classes\u2014non-separable and separable\u2014that capture input-dependent\
  \ and mode-wise dependencies, respectively."
---

# Combinatorial Bandit Bayesian Optimization for Tensor Outputs

## Quick Facts
- arXiv ID: 2602.00640
- Source URL: https://arxiv.org/abs/2602.00640
- Authors: Jingru Huang; Haijie Xu; Jie Guo; Manrui Jiang; Chen Zhang
- Reference count: 40
- Primary result: TOBO and TOCBBO achieve sublinear regret bounds and outperform baselines in synthetic and real-world experiments across chemistry, materials science, 3D printing, and semiconductor manufacturing

## Executive Summary
This paper introduces TOBO (Tensor-Output BO) and TOCBBO (Combinatorial Tensor-Output BO) for Bayesian optimization of tensor-valued black-box functions. TOBO employs a novel Tensor-Output Gaussian Process (TOGP) with two kernel classes—non-separable and separable—that capture input-dependent and mode-wise dependencies, respectively. TOCBBO extends TOGP to handle partially observed outputs via a partially observed TOGP (PTOGP) and uses a combinatorial multi-arm bandit-UCB2 (CMAB-UCB2) criterion for joint input and output subset selection. Both methods achieve sublinear regret bounds and demonstrate strong empirical performance across diverse domains.

## Method Summary
TOBO addresses Bayesian optimization for tensor-valued functions by introducing TOGP, which models tensor outputs using two kernel classes. Non-separable kernels capture complex input-output dependencies through additive structures, while separable kernels model mode-wise dependencies using Kronecker products. TOCBBO extends this framework to partially observed tensor outputs by incorporating a PTOGP and employing CMAB-UCB2 for combinatorial acquisition. The methods achieve sublinear regret bounds and are validated through synthetic and real-world experiments across multiple domains.

## Key Results
- TOBO consistently achieves the lowest instantaneous regret compared to baselines
- TOCBBO excels in scenarios with limited observable outputs
- Both methods outperform sMTGP, MLGP, and MVGP in diverse applications
- Strong empirical performance demonstrated in chemistry, materials science, 3D printing, and semiconductor manufacturing

## Why This Works (Mechanism)
TOBO and TOCBBO work by modeling tensor-valued functions through TOGP, which captures dependencies between inputs and tensor outputs using two kernel classes. Non-separable kernels handle complex input-output interactions through additive structures, while separable kernels efficiently model mode-wise dependencies via Kronecker products. TOCBBO extends this to partially observed outputs by incorporating PTOGP and using CMAB-UCB2 for joint input-output selection. The sublinear regret bounds arise from the careful balance between exploration and exploitation in the combinatorial acquisition strategy.

## Foundational Learning
- **Gaussian Process Regression**: Needed to model uncertainty in tensor outputs; quick check: verify GP posterior mean and variance formulas
- **Kronecker Product Kernels**: Enables efficient computation for separable tensor dependencies; quick check: confirm Kronecker product properties for matrix operations
- **Multi-Armed Bandit Theory**: Underpins the exploration-exploitation trade-off in BO; quick check: validate UCB2 regret bounds
- **Combinatorial Optimization**: Required for joint input-output subset selection; quick check: test CMAB-UCB2 on small combinatorial instances
- **Tensor Algebra**: Fundamental for representing and manipulating multi-dimensional outputs; quick check: verify tensor contraction operations
- **Maximum Likelihood Estimation**: Used for hyperparameter tuning in TOGP; quick check: confirm convergence of optimization algorithm

## Architecture Onboarding
**Component Map**: Input -> TOGP (Non-separable/Separable kernels) -> Acquisition (UCB2/CMAB-UCB2) -> Output Selection

**Critical Path**: Data collection → Kernel parameter estimation → GP posterior computation → Acquisition function optimization → Output subset selection

**Design Tradeoffs**: Non-separable kernels offer flexibility but scale poorly; separable kernels are efficient but assume independence; CMAB-UCB2 balances exploration-exploitation but adds computational overhead

**Failure Signatures**: Poor performance with high input dimensionality; breakdown under extreme output sparsity; overfitting with insufficient data

**First Experiments**: 1) Test TOGP on synthetic tensor functions with varying dependencies 2) Benchmark CMAB-UCB2 on small combinatorial instances 3) Validate regret bounds on simple 2D/3D functions

## Open Questions the Paper Calls Out
None

## Limitations
- Regret bounds rely on specific kernel assumptions that may not hold universally
- Separability assumption may fail for functions with complex input-output interactions
- Computational complexity scales poorly with tensor order and dimensions
- Performance in extremely sparse output scenarios remains unverified

## Confidence
- TOBO regret bounds: High
- TOCBBO combinatorial formulation: Medium
- Empirical performance claims: Medium
- Scalability assertions: Low

## Next Checks
1. Test TOBO/TOCBBO on synthetic tensor functions with varying degrees of input-output dependence to validate kernel assumptions
2. Benchmark scalability by applying TOBO to tensors with order >4 and dimensions >100
3. Evaluate TOCBBO under extreme output sparsity (≤5% observability) to assess robustness