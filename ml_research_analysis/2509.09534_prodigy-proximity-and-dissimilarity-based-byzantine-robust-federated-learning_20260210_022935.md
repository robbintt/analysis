---
ver: rpa2
title: 'ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning'
arxiv_id: '2509.09534'
source_url: https://arxiv.org/abs/2509.09534
tags:
- learning
- clients
- data
- aggregation
- byzantine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Byzantine-robust federated learning under
  heterogeneous data distributions, where malicious clients can collude to bias the
  global model. Existing robust aggregation methods, which rely on gradient similarity,
  often fail when honest gradients are highly diverse due to non-IID data, allowing
  attackers to craft misleading updates that appear trustworthy.
---

# ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning

## Quick Facts
- arXiv ID: 2509.09534
- Source URL: https://arxiv.org/abs/2509.09534
- Reference count: 36
- One-line primary result: ProDiGy outperforms state-of-the-art defenses in Byzantine-robust federated learning under non-IID data by combining proximity and dissimilarity scoring.

## Executive Summary
This paper addresses the challenge of Byzantine-robust federated learning when data is non-IID and malicious clients collude. Existing gradient-similarity-based defenses often fail under heterogeneous data because honest gradients naturally diverge, creating a gap that attackers can exploit. The authors propose ProDiGy, which uses a dual scoring system that evaluates gradients based on both their proximity to other gradients and their dissimilarity within local neighborhoods. This approach penalizes suspiciously uniform gradients while rewarding natural diversity among honest updates.

## Method Summary
ProDiGy computes a trust score for each client by combining two components: proximity and dissimilarity. The proximity score measures how close a client's gradient is to others, excluding the closest $f-1$ and farthest $f$ neighbors to avoid bias from collusion. The dissimilarity score measures the coefficient of variance within a client's $f$-nearest neighborhood, with higher variance indicating more trustworthy gradients. The final trust score is the product of these two components, and the aggregation uses a weighted average where weights are these trust scores, filtering out the lowest $f$ scores.

## Key Results
- ProDiGy achieves the highest worst-case accuracy across all tested attack scenarios and non-IID conditions.
- The method maintains robustness even under strong attack scenarios where other defenses fail entirely.
- ProDiGy demonstrates consistent performance improvements over state-of-the-art defenses like NNM combined with Median, Trimmed Mean, GeoMed, Krum, and CClip.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A dual scoring system combining proximity and dissimilarity improves robustness compared to proximity-only methods.
- Mechanism: ProDiGy computes a trust score $s(k) = s_p(k) \cdot s_d(k)$ for each client $k$. $s_p(k)$ (proximity) is the inverse of the sum of squared Euclidean distances from client $k$ to other clients (excluding the $f-1$ nearest and $f$ farthest). $s_d(k)$ (dissimilarity) is the coefficient of variance of gradients in $k$'s $f$-nearest neighborhood. The aggregation is a weighted average of gradients where weights are trust scores.
- Core assumption: Honest gradients will have a natural level of similarity to each other while malicious gradients, particularly colluding ones, may appear "too similar" to each other or artificially clustered.
- Evidence anchors:
  - [abstract] "...evaluating the client gradients using a joint dual scoring system based on the gradients' proximity and dissimilarity."
  - [section] "The main innovation is a composite trust score s(k), which consists of two complementary components s_p(k) and s_d(k)."
  - [corpus] Corpus signals confirm this is a novel combination, with neighbors like FLTG using angle-based defense and FedSV using Shapley values, neither explicitly using a combined proximity-dissimilarity metric for trust scoring.
- Break condition: This mechanism relies on the assumption that honest gradients are *more* diverse than malicious, colluding gradients. If an attack crafts gradients that are both diverse *and* harmful, the dissimilarity score may not effectively penalize them.

### Mechanism 2
- Claim: Excluding the $f-1$ nearest and $f$ farthest neighbors from the proximity score computation mitigates bias from both colluding attackers and extreme outliers.
- Mechanism: The proximity score calculation $s_p(k) = \frac{1}{\sum_{i=f}^{N-f-1} \delta_{k,i}}$ explicitly ignores the distances of the closest $f-1$ and farthest $f$ neighbors. This prevents a cluster of malicious clients from artificially boosting each other's proximity scores and reduces the influence of Byzantine gradients sent far from honest ones.
- Core assumption: The number of Byzantine clients $f$ is known or a good upper bound is available. Malicious clients collude or send outlying updates.
- Evidence anchors:
  - [section] "...PR ODIGY excludes the f-1 nearest and f farthest neighbors in the computation of the proximity score... to prevent colluding clients from being assigned disproportionately high proximity scores."
  - [corpus] Neighbors like NNM (Nearest Neighbor Mixing) and Bucketing are pre-aggregation techniques that also aim to reduce the influence of outliers, supporting the principle of excluding extreme gradients.
- Break condition: If the estimate for $f$ is significantly lower than the actual number of colluding attackers, the exclusion window may be too small, allowing malicious gradients to influence the proximity score.

### Mechanism 3
- Claim: A high dissimilarity score indicates a more trustworthy neighborhood, assuming attackers create suspiciously uniform gradients.
- Mechanism: The dissimilarity score $s_d(k) = \frac{\sigma(N_f(k))}{\|\mu(N_f(k))\|}$ measures the coefficient of variance within a client's $f$-nearest neighborhood. A higher coefficient of variance is considered more benign because it reflects natural diversity among honest gradients, while a low variance could indicate a cluster of colluding, highly similar malicious updates.
- Core assumption: Colluding adversaries produce gradients that are more similar to each other than honest gradients are to one another, especially under non-IID data. The paper states "Colluding Byzantine clients... can artificially create highly similar malicious local updates."
- Evidence anchors:
  - [section] "The dissimilarity score further penalizes suspiciously similar gradients by integrating the coefficient of variance among the neighboring clients into the trust score."
  - [corpus] Related work in Sybil attack detection (referenced in the paper's Related Work section) assumes high similarity can indicate malicious collusion, which is a conceptual anchor for this mechanism.
- Break condition: The Sign-Flip attack, where a malicious client sends $-g_k$, can create gradients that are not overly similar to each other but are adversarial. The paper notes Sign-Flip is the most challenging attack for ProDiGy, and under severe heterogeneity, honest gradients themselves can be highly diverse, which may confuse the dissimilarity score.

## Foundational Learning

- Concept: **Federated Learning (FL) and the Byzantine Threat Model.**
  - Why needed here: This is the core problem setting. ProDiGy is designed for a central-server FL architecture where a subset of clients ($f < N/2$) may be malicious, colluding, and omniscient about the system.
  - Quick check question: Can you explain why the "Byzantine" problem is harder in FL than a simple data poisoning attack in centralized learning?

- Concept: **Non-IID Data and the Gradient Similarity Gap.**
  - Why needed here: The key challenge ProDiGy addresses is the failure of similarity-based defenses when data is non-IID. In this setting, honest gradients diverge, creating a "gap" that attackers can exploit to make their malicious updates look like honest outliers.
  - Quick check question: If clients' data distributions are highly different (non-IID), would you expect their gradients to be very similar or very different? How might an attacker exploit this?

- Concept: **Robust Aggregation and Trust Scoring.**
  - Why needed here: ProDiGy is a robust aggregation rule. It doesn't filter clients out entirely but assigns a trust score used as a weight in the weighted average aggregation step. This is a common pattern in Byzantine-robust FL.
  - Quick check question: How does a trust-score weighted average provide more robustness than a simple median or trimmed mean, especially when $f$ is relatively large?

## Architecture Onboarding

- Component map: Distance Calculation -> Dual Scoring Engine -> Aggregation Module
- Critical path: The critical path for robustness is the **Dual Scoring Engine**. Errors here, such as an incorrect $f$ parameter or a misinterpretation of distance/diversity, will propagate directly into the trust scores and corrupt the aggregated model.
- Design tradeoffs:
  - **Assumption of $f$:** The method assumes an upper bound on the number of malicious clients is known. Overestimating $f$ reduces model utility by potentially filtering honest clients, while underestimating it allows malicious clients to influence the aggregation.
  - **Complexity vs. Robustness:** The paper notes the algorithm has $O(N^2 d)$ complexity due to pairwise distance calculation. This is comparable to other distance-based defenses like Krum but more expensive than coordinate-wise methods like Trimmed Mean. The tradeoff is accepted for higher robustness under non-IID data.
  - **Attack Sensitivity:** ProDiGy shows strong defense against collusion-based attacks (ALIE, FOE) but is less robust against attacks like Sign-Flip, especially without local momentum. This is a key tradeoff in its design philosophy.
- Failure signatures:
  - **Convergence Failure:** Other defenses fail (often dropping to near-random-guessing accuracy) on non-IID data under attacks like ALIE and FOE. ProDiGy maintains higher worst-case accuracy in these scenarios.
  - **Sign-Flip Attack:** This is the most challenging attack for ProDiGy, as reflected in the experimental tables. A drop in accuracy specifically under this attack may indicate the system is relying too heavily on the dissimilarity score.
  - **Fluctuating Learning:** The paper notes that without local momentum, learning can be unstable under the Sign-Flip attack for all defenses.
- First 3 experiments:
  1. **Baseline Test (IID, No Attack):** Run FL on an IID dataset (e.g., CIFAR-10 with $\alpha \to \infty$) with no malicious clients. This establishes the model's baseline accuracy and confirms the ProDiGy implementation doesn't harm performance in the benign setting.
  2. **Robustness Test (non-IID + Collusion Attack):** Run FL on a non-IID dataset (e.g., CIFAR-10 with Dirichlet $\alpha=0.1$) with $f=0.3N$ malicious clients launching the ALIE attack. This is the primary use case from the paper and should show ProDiGy outperforming a baseline like NNM+Median.
  3. **Ablation Test (Varying $f$):** Run a sweep of experiments on the same non-IID/attack setup but vary the fraction of malicious clients ($f/N$) from 0.1 to 0.3. This tests the sensitivity of the system to the core assumption about the number of adversaries.

## Open Questions the Paper Calls Out

- **Question:** What are the optimal attack strategies for colluding adversaries to evade ProDiGy's dual scoring system?
  - Basis: [explicit] The conclusion states: "The exploration of optimal attack strategies by colluding adversaries... is the subject of future research."
  - Why unresolved: While ProDiGy defends against standard attacks (ALIE, FOE), its vulnerability to adaptive attacks specifically designed to bypass the dissimilarity scoring mechanism remains unexplored.
  - Evidence: Formulation of an adaptive attack that manipulates gradient variance to exploit the trade-off between the proximity and dissimilarity scores.

- **Question:** Can the aggregation rule perform robustly without prior knowledge of the number of Byzantine clients ($f$)?
  - Basis: [explicit] The authors note that future work may focus on "integrating an estimation strategy for $f$ into ProDiGy."
  - Why unresolved: The current implementation requires $f$ as a static input parameter to determine neighborhood sizes and filtering thresholds.
  - Evidence: An extension of the algorithm that dynamically estimates $f$ in real-time while maintaining the empirical accuracy levels shown in the paper.

- **Question:** Does ProDiGy offer formal theoretical guarantees for convergence and robustness?
  - Basis: [explicit] The authors list "investigating theoretical guarantees for the robustness of ProDiGy" as a direction for future work.
  - Why unresolved: The paper relies entirely on empirical validation using CIFAR-10 and FEMNIST, providing no mathematical proof of convergence or error bounds.
  - Evidence: Derivation of convergence rates for the dual scoring mechanism, particularly under non-IID data distributions.

## Limitations

- **Parameter Dependency:** ProDiGy requires prior knowledge of the number of Byzantine clients ($f$), and incorrect estimates can significantly impact performance.
- **Sign-Flip Vulnerability:** The method shows particular weakness against Sign-Flip attacks, where malicious gradients are not overly similar but still adversarial.
- **Computational Complexity:** The pairwise distance calculation introduces $O(N^2 d)$ complexity, which may be prohibitive for very large client populations.

## Confidence

- **High**: The mathematical formulation of proximity and dissimilarity scores, and the experimental methodology (CIFAR-10, FEMNIST, attack implementation)
- **Medium**: The claim that the dual scoring system is universally superior across all Byzantine attack types
- **Low**: Generalization to other federated learning architectures (e.g., cross-silo settings with different communication patterns)

## Next Checks

1. **Attack Diversity Test**: Evaluate ProDiGy against a broader range of Byzantine attacks including model replacement and data poisoning to assess robustness beyond the current suite.
2. **Parameter Sensitivity Analysis**: Systematically vary the assumed $f$ parameter (both overestimation and underestimation) to quantify the impact on accuracy and identify break points.
3. **Real-World Deployment Simulation**: Implement ProDiGy in a more realistic federated learning scenario with communication delays, client dropouts, and varying participation rates to test practical robustness.