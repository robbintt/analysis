---
ver: rpa2
title: 'REAct: Rational Exponential Activation for Better Learning and Generalization
  in PINNs'
arxiv_id: '2503.02267'
source_url: https://arxiv.org/abs/2503.02267
tags:
- activation
- functions
- react
- neural
- pinns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REAct, a novel activation function for Physics-Informed
  Neural Networks (PINNs) that addresses the challenge of optimization and generalization
  across diverse physical systems. The key innovation is a generalized form of tanh
  with four learnable shape parameters, allowing greater flexibility and control over
  key shape characteristics like zero crossings, frequency, and saturation regions.
---

# REAct: Rational Exponential Activation for Better Learning and Generalization in PINNs

## Quick Facts
- arXiv ID: 2503.02267
- Source URL: https://arxiv.org/abs/2503.02267
- Authors: Sourav Mishra; Shreya Hallikeri; Suresh Sundaram
- Reference count: 27
- Key outcome: Introduces REAct activation function for PINNs with four learnable parameters, achieving up to three orders of magnitude lower MSE on heat problems compared to standard activations

## Executive Summary
This paper presents REAct, a novel activation function designed specifically for Physics-Informed Neural Networks (PINNs) that addresses the critical challenge of optimization and generalization across diverse physical systems. REAct extends the traditional tanh function by incorporating four learnable shape parameters that provide unprecedented control over key characteristics including zero crossings, frequency, and saturation regions. The activation demonstrates superior performance across five forward simulation problems and shows particular promise in improving noise rejection capabilities for inverse problems.

## Method Summary
REAct is a generalized form of the hyperbolic tangent function that introduces four learnable parameters: $\alpha$ (frequency control), $\beta$ (zero-crossing adjustment), $\gamma$ (saturation region control), and $\delta$ (asymmetry parameter). These parameters are learned during training alongside the network weights, allowing the activation function to adapt its shape dynamically to the specific physical system being modeled. The function maintains computational efficiency while providing significantly greater flexibility than standard activations like ReLU, tanh, or Swish. The learnable parameters enable REAct to optimize the trade-off between approximation accuracy and generalization across different physical domains.

## Key Results
- Achieves up to three orders of magnitude lower Mean Squared Error on heat conduction problems compared to standard activations
- Demonstrates superior function approximation capabilities with better capture of complex function variations
- Shows improved noise rejection in inverse problems, providing more accurate parameter estimates across varying noise levels
- Excels across five different forward simulation problems including heat, wave, and diffusion equations

## Why This Works (Mechanism)
REAct's success stems from its ability to dynamically adapt the activation function shape during training rather than using fixed, predetermined functions. The four learnable parameters allow the network to optimize the balance between local sensitivity and global stability, which is crucial for capturing the diverse behaviors present in different physical systems. The flexibility in zero crossings and saturation regions enables better handling of boundary conditions and physical constraints inherent in PINNs. This adaptability directly addresses the limitation of standard activations that are optimized for general-purpose deep learning rather than physics-specific problems.

## Foundational Learning
- **Physics-Informed Neural Networks (PINNs)**: Neural networks that incorporate physical laws as constraints during training, essential for solving forward and inverse problems in computational physics
  - Why needed: Provides the foundation for understanding the specific challenges in PINN optimization and generalization
  - Quick check: Verify understanding of how PINNs differ from standard supervised learning approaches

- **Activation function optimization**: The process of selecting or designing activation functions that enhance neural network performance for specific tasks
  - Why needed: Explains the motivation behind developing specialized activations for PINNs rather than using off-the-shelf functions
  - Quick check: Compare standard activation functions and their limitations in physics applications

- **Learnable parameters in activation functions**: Activation functions that include parameters that can be optimized during training alongside network weights
  - Why needed: Crucial for understanding how REAct achieves its adaptive capabilities
  - Quick check: Analyze how additional learnable parameters affect training dynamics and convergence

## Architecture Onboarding
- **Component map**: Input -> REAct activation (with 4 learnable parameters: α, β, γ, δ) -> Network layers -> Output
- **Critical path**: Data input → Activation function transformation → Layer computations → Loss calculation with physics constraints → Parameter updates (including activation parameters)
- **Design tradeoffs**: Increased flexibility and performance vs. additional computational overhead and potential optimization complexity
- **Failure signatures**: Convergence issues if activation parameters become extreme values; poor generalization if parameters overfit to specific problems
- **First experiments**: 1) Test REAct on simple function approximation tasks to verify basic functionality, 2) Compare convergence speed with standard activations on heat equation, 3) Evaluate noise sensitivity on synthetic data with controlled noise levels

## Open Questions the Paper Calls Out
None

## Limitations
- Limited validation across physical domains - all experiments focus on forward simulation and specific inverse problems without testing on entirely different physical systems
- No detailed computational complexity analysis or runtime comparisons provided to quantify the overhead of four additional learnable parameters per activation
- Noise rejection improvements demonstrated only on Burgers equation with artificially added noise, not validated on real measurement noise or other inverse problems

## Confidence
- **High confidence**: Performance improvements on tested forward simulation problems (heat, wave, diffusion, etc.)
- **Medium confidence**: Noise rejection capabilities in inverse problems (limited to Burgers equation)
- **Low confidence**: Claims about generalization across diverse physical systems (based on limited domain testing)

## Next Checks
1. Test REAct on real-world experimental datasets from multiple physical domains to verify noise rejection claims with actual measurement noise
2. Conduct systematic ablation studies to quantify the impact of each learnable parameter on convergence speed and final accuracy
3. Evaluate scalability by testing REAct on larger network architectures and higher-dimensional problems to assess computational overhead and optimization stability