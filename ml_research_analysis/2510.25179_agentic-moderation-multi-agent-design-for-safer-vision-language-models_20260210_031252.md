---
ver: rpa2
title: 'Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models'
arxiv_id: '2510.25179'
source_url: https://arxiv.org/abs/2510.25179
tags:
- safety
- shield
- moderation
- arxiv
- reflection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Agentic Moderation, a multi-agent framework\
  \ for defending vision-language models (LVLMs) against cross-modal adversarial attacks.\
  \ The system uses three specialized agents\u2014Shield, Responder, and Reflector\u2014\
  that collaborate to dynamically screen, guide, and refine model outputs based on\
  \ safety policies."
---

# Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models

## Quick Facts
- arXiv ID: 2510.25179
- Source URL: https://arxiv.org/abs/2510.25179
- Reference count: 31
- One-line primary result: Multi-agent framework reduces multimodal adversarial attack success by 7–19% while maintaining low non-following rate

## Executive Summary
Agentic Moderation introduces a multi-agent framework for defending vision-language models (LVLMs) against cross-modal adversarial attacks. The system employs four specialized agents—Shield, Responder, Evaluator, and Reflector—that collaborate to dynamically screen, guide, and refine model outputs based on safety policies. Evaluated across five datasets and four LVLMs, the approach achieves significant reductions in Attack Success Rate (7–19%) while maintaining low Non-Following Rate and improving Refusal Rate by 4–20%.

The modular design supports flexible, interpretable, and scalable safety alignment, demonstrating that agentic architectures can serve as adaptive guardrails for multimodal AI systems. The framework balances safety performance with inference efficiency through configurable agent activation patterns, enabling deployment configurations ranging from lightweight Shield-only to comprehensive Shield-plus-Reflection setups.

## Method Summary
The framework implements a multi-agent moderation loop where inputs are first classified by the Shield Agent into 45 policy categories, then processed by the Responder Agent with contextual guidance, evaluated by the Evaluator Agent, and potentially refined through the Reflector Agent's feedback loop. The system operates as a LangChain-based orchestration with DSPy components, using a Coordinator to manage agent interactions and early-exit conditions. The approach combines pre-generation filtering with iterative post-hoc evaluation, allowing for dynamic safety enforcement that adapts to context rather than relying on static rules.

## Key Results
- Reduces Attack Success Rate by 7–19% across five datasets and four LVLMs
- Maintains low Non-Following Rate while improving Refusal Rate by 4–20%
- Achieves more balanced safety performance than static or binary filters
- Shield module adds only ~0.015s overhead; Reflection adds ~1.5s average latency

## Why This Works (Mechanism)

### Mechanism 1: Policy-Guided Pre-Generation Filtering
The Shield Agent classifies inputs into 45 predefined policy categories and assigns one of three actions (block, reframe, forward), appending "should do" and "should not do" guidance to the prompt. This transforms safety from a post-hoc filter into a generation-time constraint by conditioning the Responder Agent's behavior.

### Mechanism 2: Iterative Post-Hoc Evaluation and Regeneration
After generation, the Evaluator Agent classifies responses and assigns harmfulness scores. If violations are detected, the Reflector Agent produces structured "Issue—Fix" feedback, triggering up to two regeneration iterations. The paper reports that "a single reflection iteration is sufficient to reach safety convergence in most cases."

### Mechanism 3: Modular Agent Coordination with Early Exit
A central Coordinator orchestrates agents with early-exit conditions: Shield can block immediately, and safe outputs bypass the Reflector entirely. This enables deployment configurations ranging from Shield-only (low latency) to full Shield+Reflection (higher safety assurance).

## Foundational Learning

- **Cross-modal adversarial attacks**: Why needed—the paper assumes familiarity with how adversarial inputs exploit vision-language modality gaps. Quick check: Can you explain why a benign-looking image paired with innocuous text might produce a harmful output in an LVLM?

- **Safety-utility trade-off in moderation**: Why needed—the paper optimizes for both low Attack Success Rate and low Non-Following Rate. Quick check: Why might a moderator that refuses all requests be "safe" but practically useless?

- **Agentic orchestration patterns**: Why needed—the framework uses a Coordinator to manage specialized agents in an iterative loop. Quick check: What is the difference between a pipeline (fixed sequence) and an agentic loop (dynamic, conditional iteration)?

## Architecture Onboarding

- **Component map**: Coordinator -> Shield (input classification) -> Responder (VLM generation) -> Evaluator (output assessment) -> Reflector (feedback for unsafe outputs) -> Responder (regeneration)

- **Critical path**: Input → Shield classification → (if block: refusal) → (if reframe/forward: Shield guidance → Responder generates) → Evaluator assesses → (if safe: return) → (if unsafe: Reflector feedback → Responder regenerates up to 2 iterations)

- **Design tradeoffs**: Shield-only: ~0.015s overhead, lower safety coverage for context-dependent harms vs. Shield+Reflection: ~1.5s additional latency, higher safety assurance but increased compute

- **Failure signatures**: Over-refusal (high Refusal Rate on benign inputs), High Non-Following Rate (guidance conflicts with task), Persistent ASR (reflection loop converges on unsafe outputs)

- **First 3 experiments**: 1) Replicate Table I on single LVLM to validate relative contribution of each agent, 2) Latency profiling across all four agent configurations, 3) Ablation on policy categories to identify critical policy coverage

## Open Questions the Paper Calls Out

### Open Question 1
How can adaptive agent scheduling and cost-aware coordination strategies be implemented to balance safety guarantees with system responsiveness? The current framework uses a fixed iteration loop (up to two reflections), which introduces latency (approx. 1.5s) that may be unnecessary for low-risk inputs. A dynamic scheduling mechanism that selectively activates the Reflector agent based on real-time risk assessment would demonstrate reduced average latency without increasing Attack Success Rate.

### Open Question 2
Do the reported safety improvements generalize to the full distributions of adversarial benchmarks, beyond the 100-instance samples used for evaluation? Small samples may not capture the full variance of attack types or the "long tail" of implicit cross-modal interactions. Experimental results run on the complete AdvBench, FigStep, and MM-Safety datasets showing consistent ASR reductions would resolve this uncertainty.

### Open Question 3
Is the agentic framework robust against adaptive attacks specifically designed to exploit the fixed policy categories or the reasoning loop of the agents? It is unclear if an adversary could craft inputs that specifically bypass the Shield's "45 predefined policy categories" or manipulate the Reflector into validating unsafe outputs. Evaluation against white-box attacks where the adversary has access to the moderation agents' prompts and taxonomies would resolve this.

## Limitations

- Lack of direct specification of the 45 policy categories and precise LLM configurations for Evaluator/Reflector agents blocks exact replication
- Study focus on four LVLM models and five curated datasets limits generalizability to other multimodal architectures
- Safety claims contingent on Evaluator reliability and Reflector feedback effectiveness, neither externally validated

## Confidence

- **High confidence**: Modular agent coordination design and general safety-performance trade-off trends (ASR ↓, NF ↓, RR ↑) well-supported by ablation results
- **Medium confidence**: Efficacy of Shield's 45-category policy classification supported by internal ablation but not externally validated
- **Low confidence**: Evaluator reliability in detecting subtle harms and Reflector feedback effectiveness in all cases assumed rather than empirically proven

## Next Checks

1. **Taxonomy validation**: Publish or externally benchmark the 45 Shield policy categories against a held-out set of adversarial inputs to assess coverage gaps and false-positive/negative rates

2. **Evaluator reliability study**: Test the Evaluator on a balanced mix of safe and adversarial outputs from multiple models to quantify classification accuracy and calibration

3. **Cross-model generalization**: Deploy the full agentic framework on at least two additional LVLM architectures (e.g., Gemini, Claude) and two new adversarial datasets to verify robustness beyond the reported model-dataset pairs