---
ver: rpa2
title: 'TextCAM: Explaining Class Activation Map with Text'
arxiv_id: '2510.01004'
source_url: https://arxiv.org/abs/2510.01004
tags:
- textcam
- visual
- saliency
- text
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TextCAM, a method that extends Class Activation
  Mapping (CAM) with natural language explanations. While CAM highlights spatial regions
  influencing model predictions, it lacks semantic insight into the visual attributes
  driving those decisions.
---

# TextCAM: Explaining Class Activation Map with Text

## Quick Facts
- arXiv ID: 2510.01004
- Source URL: https://arxiv.org/abs/2510.01004
- Reference count: 22
- Method combines CAM with CLIP embeddings and LDA for textual explanations of visual saliency

## Executive Summary
This paper introduces TextCAM, a method that extends Class Activation Mapping (CAM) with natural language explanations. While CAM highlights spatial regions influencing model predictions, it lacks semantic insight into the visual attributes driving those decisions. TextCAM bridges this gap by combining CAM's spatial localization with vision-language model (VLM) semantic alignment. The approach computes channel-level semantic representations using CLIP embeddings and linear discriminant analysis, then aggregates them with CAM weights to produce textual descriptions of salient visual evidence.

## Method Summary
TextCAM extends CAM by generating natural language explanations through semantic alignment between visual features and textual descriptions. The method computes channel-level semantic representations using CLIP embeddings and linear discriminant analysis, then aggregates these with CAM weights to produce textual descriptions of salient visual evidence. The approach is extended to generate feature channel groups for fine-grained visual-textual explanations. Experiments demonstrate the method's ability to produce interpretable rationales that improve human understanding and detect spurious correlations across ImageNet, CLEVR, and CUB-200 datasets.

## Key Results
- Achieves 100% accuracy in retrieving correct concepts on CLEVR dataset
- Produces faithful, interpretable rationales that improve human understanding
- Successfully detects spurious correlations while preserving model fidelity
- Demonstrates consistent performance across diverse datasets and architectures

## Why This Works (Mechanism)
TextCAM works by bridging the semantic gap between spatial localization (CAM) and visual attributes (text). The mechanism leverages CLIP's strong cross-modal representations to align visual features with language, while LDA helps identify discriminative semantic patterns within feature channels. By aggregating these semantic representations with CAM weights, the method can generate coherent textual explanations that directly correspond to the model's decision-making process. The channel grouping extension allows for fine-grained explanations by organizing related visual features into coherent semantic clusters.

## Foundational Learning
- Class Activation Mapping (CAM): A technique for generating visual explanations by highlighting spatial regions important for classification decisions. Why needed: Provides the spatial localization foundation that TextCAM builds upon.
- Vision-Language Models (VLMs) like CLIP: Models trained on paired image-text data that learn joint visual-linguistic representations. Why needed: Enables semantic alignment between visual features and natural language.
- Linear Discriminant Analysis (LDA): A dimensionality reduction technique that maximizes class separability. Why needed: Helps compute discriminative semantic representations from feature channels.
- Semantic Alignment: The process of mapping visual features to their corresponding textual descriptions. Why needed: Bridges the gap between spatial saliency and interpretable explanations.
- Feature Channel Grouping: Organizing related feature channels into coherent semantic groups. Why needed: Enables fine-grained, multi-concept explanations of model decisions.

## Architecture Onboarding
Component map: Input Image -> CNN Backbone -> CAM Generation -> CLIP Embeddings + LDA -> Semantic Aggregation -> Text Generation
Critical path: The semantic aggregation stage is critical, as it combines CAM weights with semantically aligned representations to generate meaningful explanations.
Design tradeoffs: The method trades computational overhead (CLIP embeddings + LDA) for improved interpretability and semantic richness in explanations.
Failure signatures: Poor semantic alignment when CLIP's training distribution differs significantly from target data; failure to capture complex visual relationships; over-reliance on dominant visual features.
First experiments:
1. Validate semantic alignment quality by comparing generated explanations with ground truth descriptions on CUB-200.
2. Test spurious correlation detection by introducing known biases in training data and evaluating explanation accuracy.
3. Assess robustness by evaluating on out-of-distribution samples and adversarial examples.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on CLIP embeddings which may introduce domain-specific biases
- LDA effectiveness for channel-level semantic representations in complex visual domains is uncertain
- Evaluation focuses on standard benchmarks with limited real-world deployment analysis

## Confidence
High confidence: Core methodology combining CAM with CLIP embeddings and LDA
Medium confidence: Claims about improved human understanding and spurious correlation detection
Medium confidence: 100% accuracy on CLEVR may not generalize to real-world scenarios

## Next Checks
1. Conduct a human study with domain experts to validate whether generated textual explanations improve understanding and decision-making compared to CAM alone.
2. Test the method on out-of-distribution data and adversarial examples to assess robustness and identify potential failure modes.
3. Evaluate the method's performance on datasets with known spurious correlations to verify its ability to detect and explain these patterns in real-world scenarios.