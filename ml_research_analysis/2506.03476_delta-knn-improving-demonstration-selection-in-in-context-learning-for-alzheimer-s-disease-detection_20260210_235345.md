---
ver: rpa2
title: 'Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer''s
  Disease Detection'
arxiv_id: '2506.03476'
source_url: https://arxiv.org/abs/2506.03476
tags:
- prompt
- learning
- language
- delta-knn
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Alzheimer's Disease detection
  using in-context learning with large language models on picture description tasks.
  The proposed Delta-KNN method improves demonstration selection by constructing a
  delta matrix to quantify performance gains and using a KNN-based retriever to dynamically
  select optimal examples.
---

# Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection

## Quick Facts
- arXiv ID: 2506.03476
- Source URL: https://arxiv.org/abs/2506.03476
- Authors: Chuyuan Li; Raymond Li; Thalia S. Field; Giuseppe Carenini
- Reference count: 32
- One-line primary result: Delta-KNN achieves 78.5% accuracy on Canary dataset, surpassing supervised classifiers and existing ICL baselines for Alzheimer's Disease detection.

## Executive Summary
Delta-KNN improves demonstration selection in in-context learning for Alzheimer's Disease detection by quantifying the performance gains of each training example through a delta matrix. The method uses KNN-based retrieval to dynamically select optimal demonstrations based on aggregated performance deltas rather than semantic similarity alone. Experiments on ADReSS and Canary datasets show Delta-KNN consistently outperforms existing ICL baselines and achieves state-of-the-art accuracy, particularly when using Llama-3.1.

## Method Summary
Delta-KNN constructs a delta matrix where each cell δ(doci, docj) represents the performance gain when doci serves as a demonstration for docj. For inference, test inputs are embedded and k-nearest neighbors are retrieved from the training set. Demonstrations are ranked by their average delta scores across these neighbors, and the top candidates are selected for the prompt. The method uses 4-shot ICL with balanced positive and negative examples, and includes task-specific prompt engineering with guided chain-of-thought reasoning.

## Key Results
- Achieves 78.5% accuracy on Canary dataset, surpassing supervised classifiers
- Outperforms existing ICL baselines including MarginSel, Learning to Select, and Promptagator
- Task-specific prompt engineering with G.-CoT significantly improves performance (11-15 points gain)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performance-gain quantification improves demonstration selection over semantic similarity alone.
- Mechanism: The delta score δ(doci, docj) = P1(ŷ|doci, docj; θ) - P0(ŷ|docj; θ) captures the marginal benefit of using doci as a demonstration for docj.
- Core assumption: The delta scores computed on held-out examples transfer to test-time selection decisions.
- Evidence anchors:
  - [abstract] "Our method leverages a delta score to assess the relative gains of each training example"
  - [section 3] "Each cell in ∆ contains a delta score (δ) to represent the improvement or degradation contributed by a specific demonstration doci to a target docx"
- Break condition: If delta scores show high variance across runs or fail to generalize across distribution shifts, the matrix becomes unreliable for selection.

### Mechanism 2
- Claim: Aggregating delta scores over k-nearest neighbors smooths selection and handles test-set diversity.
- Mechanism: Instead of selecting based on a single similar document, Delta-KNN averages δ(doci, ·) over k neighbors: δ̄(doci, ·) = (1/k) Σ δ(doci, docjk').
- Core assumption: Documents similar in embedding space share similar optimal demonstration preferences.
- Evidence anchors:
  - [abstract] "coupled with a KNN-based retriever that dynamically selects optimal 'representatives' for a given input"
  - [section 3] "We hypothesize that the average delta score δ̄ derived from guiding the documents most similar to the target document is more informative and effective"
- Break condition: If embedding similarity does not correlate with demonstration effectiveness, KNN aggregation provides no benefit.

### Mechanism 3
- Claim: Task-specific prompt engineering with guided CoT amplifies ICL effectiveness for clinical domains.
- Mechanism: The Role+Context+Linguistic+G.-CoT prompt structure provides domain grounding and explicit reasoning steps, helping the model attend to clinically relevant linguistic features.
- Core assumption: The LLM has sufficient latent medical/linguistic knowledge that can be activated through structured prompting.
- Evidence anchors:
  - [section 5.2, Table 2] "without prompt engineering (Prompt 1), Delta-KNN achieves 69% accuracy... 11 and 15 points lower than the best-performing design (Prompt 7)"
- Break condition: If the base model lacks relevant medical knowledge, no amount of prompt engineering will elicit correct reasoning.

## Foundational Learning

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The entire method builds on ICL—the ability of LLMs to perform tasks from examples in the prompt without weight updates.
  - Quick check question: Can you explain why zero-shot AD detection shows severe class imbalance (predicting mostly "Patient") while few-shot ICL balances predictions?

- Concept: **Performance Gain Quantification**
  - Why needed here: Delta-KNN's core innovation is measuring how much each demonstration helps, not just how similar it is.
  - Quick check question: Why might a semantically similar demonstration harm performance on a complex task like AD detection?

- Concept: **Embedding-Based Retrieval**
  - Why needed here: The KNN component relies on vector similarity search using OpenAI text-embedding-3-large.
  - Quick check question: The paper found LLM internal embeddings underperformed OpenAI embeddings—what factors might explain this?

## Architecture Onboarding

- Component map:
  ```
  Training Phase:
  [Training Set D] → [LLM zero-shot & one-shot inference] → [Delta Matrix ∆ (d×d)]
                                                              ↓
  [Embedding Model] → [Vector Store] ← [Training Set D]
  
  Inference Phase:
  [Test Input docx] → [Embedding Model] → [KNN Search] → [k neighbors from D]
                                                              ↓
                        [Delta Matrix ∆] → [Aggregate δ̄ for each candidate demo]
                                                ↓
                                      [Select top-n demos by max δ̄]
                                                ↓
                                      [Construct prompt + LLM inference]
  ```

- Critical path:
  1. Delta Matrix construction (O(d²) LLM calls)—must complete before deployment
  2. Embedding precomputation for all training documents
  3. k-value tuning via cross-validation (paper found k=13 optimal)
  4. Prompt template finalization

- Design tradeoffs:
  - **Embedding choice**: OpenAI embeddings vs. LLM hidden states—external embeddings won but add API dependency
  - **k value**: Low k risks noisy selection; high k dilutes signal—requires empirical tuning
  - **Delta Matrix computation cost**: O(d²) forward passes; for d=100 training examples, this is ~10,000 LLM calls
  - **Shot count**: Paper found 4-shot optimal; more examples didn't help and sometimes hurt

- Failure signatures:
  - High variance across runs (>5% accuracy swing) suggests delta scores are unstable
  - If specificity remains very low while sensitivity is high, demonstrations may not correct model bias
  - If performance degrades with more demonstrations, the selection mechanism is choosing unhelpful examples

- First 3 experiments:
  1. **Reproduce delta matrix construction**: Take 20 training examples, compute δ(doci, docj) for all pairs using a small LLM. Verify matrix contains both positive and negative values (some demos hurt performance).
  2. **Ablate k value**: On a held-out validation set, test k ∈ {1, 5, 10, 15, 20} and plot accuracy. Confirm non-monotonic relationship (paper shows peak around 13).
  3. **Compare retrieval embeddings**: Run Delta-KNN with OpenAI embeddings vs. LLM hidden states (try layers L8, L16, L24, L32). Replicate finding that mid-layer embeddings outperform final layer.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the k hyperparameter (number of nearest neighbors for delta aggregation) be optimized adaptively without requiring empirical search on a held-out set?
  - Basis in paper: [explicit] "determining the optimal number of target 'representatives' in the Delta Matrix is non-trivial, as it possibly depends on multiple factors... In future work, we aim to develop more advanced methods for optimizing this hyperparameter."
  - Why unresolved: The authors empirically tune k (finding k=13 optimal) via cross-validation, noting it depends on prompt, LLM, similarity metric, and embedding model in complex ways.
  - What evidence would resolve it: A self-adaptive k selection mechanism that achieves comparable performance without requiring a separate validation loop.

- **Open Question 2**: Does combining hidden state representations from multiple LLM layers improve Delta-KNN retrieval compared to single-layer embeddings?
  - Basis in paper: [explicit] "Future work could explore combining representations from multiple layers to enhance text encoding (Li et al., 2025b)."
  - Why unresolved: The authors found LLM-derived embeddings underperformed external embeddings (OpenAI), with performance varying significantly by layer; mid-layers (L16, L24) outperformed the final layer.
  - What evidence would resolve it: Experiments comparing multi-layer pooling strategies against single-layer and external embeddings on both ADReSS and Canary datasets.

- **Open Question 3**: How does Delta-KNN scale to larger datasets beyond a few hundred examples, given the O(n²) complexity of Delta Matrix construction?
  - Basis in paper: [inferred] The Limitations section notes O(n²) complexity but only briefly suggests clustering as a potential solution without investigation. The method was tested on small datasets (156 and 130 samples).
  - Why unresolved: The quadratic pairwise computation becomes prohibitive for larger clinical datasets; no experiments or analysis address this scaling challenge.
  - What evidence would resolve it: Analysis of computational costs on synthetic larger datasets, plus experiments with clustering-based or sampling-based approximations to Delta Matrix construction.

## Limitations

- The method requires O(d²) LLM inference calls to construct the delta matrix, making it computationally expensive for large training sets
- Performance claims depend on access to private datasets (Canary) that cannot be independently verified
- The optimal k=13 value appears dataset-specific and may not generalize across domains
- The method creates API dependency on external embedding models (OpenAI text-embedding-3-large)

## Confidence

- **High Confidence**: The core mechanism of delta score computation and KNN-based selection is clearly specified and internally consistent. The paper's ablation studies (prompt engineering impact, k-value tuning) are well-documented.
- **Medium Confidence**: The superiority over baselines is demonstrated but depends on private data access. The 78.5% accuracy on Canary is impressive but cannot be independently verified without dataset access.
- **Low Confidence**: Generalization claims beyond AD detection are unsupported. The paper doesn't address computational costs or scaling limitations for larger datasets.

## Next Checks

1. **Dataset Access and Preprocessing Verification**: Obtain both ADReSS and Canary datasets, implement exact preprocessing (CHAT token removal, WhisperX transcription for Canary), and verify the train/test splits used in the paper.

2. **Delta Matrix Construction Validation**: Replicate the delta matrix computation on a small subset (20 examples) and verify that (a) the matrix contains both positive and negative values, and (b) the marginal performance gains are statistically significant.

3. **k-Value Sensitivity Analysis**: Systematically test k ∈ {1, 5, 10, 15, 20, 25} on a held-out validation set to confirm the non-monotonic relationship and identify the optimal value, comparing against the paper's k=13 finding.