---
ver: rpa2
title: 'Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for
  Enhanced Interaction'
arxiv_id: '2601.20162'
source_url: https://arxiv.org/abs/2601.20162
tags:
- user
- mobile
- preference
- music
- experiences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Me-Agent addresses personalization challenges in mobile agents
  by introducing a two-level habit learning approach: User Preference Learning for
  modeling user preferences without training, and Hierarchical Preference Memory for
  storing and retrieving application-specific knowledge. The system employs a Vision-Language
  Model-based reward mechanism to evaluate task completion and extracts actionable
  experiences from successful interactions.'
---

# Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for Enhanced Interaction

## Quick Facts
- arXiv ID: 2601.20162
- Source URL: https://arxiv.org/abs/2601.20162
- Reference count: 16
- Primary result: 1.000 App Selection Accuracy and 89.3% task completion rate on ambiguous instructions

## Executive Summary
Me-Agent introduces a training-free personalized mobile agent that handles ambiguous instructions through two-level user habit learning. The system employs User Preference Learning (UPL) to model user preferences via trajectory comparison without parameter updates, combined with Hierarchical Preference Memory (HPM) for scalable retrieval of app-specific workflows and content preferences. A Vision-Language Model-based reward mechanism evaluates task completion, while the architecture enables robust performance across different backbone models. Experiments on the User FingerTip benchmark demonstrate significant improvements over baseline mobile agents in both app selection accuracy and task completion rates.

## Method Summary
Me-Agent implements a two-level personalization approach: User Preference Learning optimizes preferences in context space through parallel trajectory rollouts evaluated by a VLM-based reward model, extracting natural language experiences that are stored and injected into future prompts. Hierarchical Preference Memory organizes knowledge into intent categories (L1) and app-specific workflows/preferences (L2), enabling efficient retrieval without context overflow. The system handles ambiguous instructions through two-stage resolution—first resolving target applications based on learned experiences, then retrieving relevant content via semantic similarity. Implementation uses group_size=2, temperature=0.3, and a perception pipeline combining DBNet OCR, GroundingDINO for icon detection, and GPT-4o for captions.

## Key Results
- Achieved 1.000 App Selection Accuracy on both Type I and Type II ambiguous instructions
- Reached 0.534 BERTScore on ambiguous instructions, significantly outperforming baselines
- Attained 89.3% task completion rate on E dataset, a 39.7% improvement over baselines
- Maintained 0.861 action accuracy and 0.978 reflection precision during execution

## Why This Works (Mechanism)

### Mechanism 1: User Preference Learning via Trajectory Comparison
Shifting preference optimization from parameter space to context space enables personalization without model fine-tuning. For each instruction, the system performs G parallel rollouts, evaluates each trajectory with a VLM-based reward model (scoring goal achievement, step validity, result visibility, error detection), then uses an LLM to compare successful vs failed trajectories and extract natural language "experiences" (e.g., "User prefers QQ Music for music tasks"). These experiences are stored and injected into future prompts. The core assumption is that user preferences are stable enough that natural language summaries captured from trajectory comparisons generalize to future instructions.

### Mechanism 2: Hierarchical Preference Memory for Scalable Retrieval
Organizing user memory by intent category (L1) and app-specific workflows/preferences (L2) enables efficient retrieval while preventing prompt overflow. L1 memory maps intent categories (e.g., "Music") to app sets. L2 memory stores app-specific workflows (UI element positions, action sequences) and content preferences (user's favorite songs). During inference, only relevant app-specific memory is retrieved based on the resolved target app, rather than injecting all accumulated knowledge. The core assumption is that app-specific knowledge has high locality—most retrieved information is only relevant when interacting with that specific app.

### Mechanism 3: Two-Stage Ambiguity Resolution via Experience-Guided Inference
Decomposing ambiguous instructions into application resolution and content retrieval stages enables handling of both Type I (app-ambiguous) and Type II (content-ambiguous) instructions. When an instruction lacks an explicit app, the agent infers P(app | experience_pool) and selects the highest-scoring app. For implicit content references ("my favorite"), it retrieves top-k candidates from the target app's memory via semantic similarity, then uses LLM reasoning to select the final content. The core assumption is that user behavior history contains sufficient signal to infer both app and content preferences with high accuracy.

## Foundational Learning

- **Training-free RL / Context-space optimization**: Me-Agent uses GRPO-style rollout comparison without backpropagation. You need to understand how reward signals can guide behavior through prompt engineering rather than weight updates. Quick check: Can you explain how comparing multiple trajectories and selecting the best differs from traditional reinforcement learning with gradient updates?

- **Hierarchical memory organization**: The two-level memory structure (intent → apps → workflows/preferences) is core to the architecture. Understanding when to store at L1 vs L2 is critical for implementation. Quick check: Given a user who switches between QQ Music and NetEase Music depending on mood, at which level would you store this preference pattern?

- **VLM-based reward modeling**: The reward model evaluates trajectory quality from screenshot sequences. You need to understand how vision-language models can provide scalar rewards for GUI actions. Quick check: What four dimensions does the paper's VLM reward model evaluate, and why might screenshot-based evaluation be preferred over action-based evaluation?

## Architecture Onboarding

- **Component map**:
```
User Instruction
       ↓
[Experience Pool E] ←── User Preference Learning ──← Trajectory Rollouts (G×)
       ↓                       ↑                           ↓
Application Resolution    Personal Reward Model      Screenshot Sequences
       ↓                       ↓                           ↓
[Hierarchical Preference Memory] ──→ Content Retrieval
    L1: Intent→Apps                  L2: App→Workflows+Prefs
       ↓
Prompt Injection (resolved app + content + experiences)
       ↓
Mobile Agent Execution
```

- **Critical path**:
  1. **Training phase**: Execute G rollouts per instruction → VLM scores trajectories → LLM extracts experiences → Update experience pool and HPM with ADD/UPDATE/DELETE operations
  2. **Inference phase**: Receive ambiguous instruction → Resolve app from experience pool → Retrieve content from L2 memory → Construct personalized prompt → Execute

- **Design tradeoffs**:
  - Parameter-free vs fine-tuning: Enables API deployment but limits preference encoding capacity to natural language expressiveness
  - Hierarchical vs flat memory: Reduces prompt length but requires accurate app resolution before content retrieval can proceed
  - Rollout count G: Higher G improves experience quality but increases latency and cost during training

- **Failure signatures**:
  - UI changes invalidate stored positions: "learned experiences may become invalid when the application is updated" [Limitations]
  - Context-independent recommendations: System may suggest inappropriate content when user's situation differs from historical patterns [Limitations]
  - Experience pool drift: Without proper deduplication, redundant or conflicting experiences accumulate

- **First 3 experiments**:
  1. **Validate UPL in isolation**: Run the reward model on 20 manually annotated trajectories across 3 apps. Verify that VLM scores correlate with human judgments of task success (target: Spearman ρ > 0.7).
  2. **Test hierarchical retrieval precision**: For a user with preferences across 5 apps, inject ambiguous instructions and measure whether the correct app is retrieved from L1 and correct content from L2 (target: ASA > 0.9 on held-out instructions).
  3. **Stress test experience staleness**: Intentionally modify UI element positions in a test app after training, measure degradation in task completion rate, and quantify recovery speed after re-extracting experiences.

## Open Questions the Paper Calls Out

### Open Question 1
How can Me-Agent maintain high personalization accuracy when target applications undergo interface updates that invalidate stored UI positional data? The "Limitations" section states that "learned experiences may become invalid when the application is updated or unexpected pop-ups appear," causing execution failures because stored positions no longer match the interface. This remains unresolved because the current Hierarchical Preference Memory relies on extracting specific UI positions which lack semantic robustness against layout changes. Evidence that would resolve this includes experiments demonstrating personalization performance retention when tested against simulated app updates or version-shifted UI layouts.

### Open Question 2
Can the user modeling approach be extended to incorporate dynamic contextual factors such as real-time location or emotional state without requiring parameter updates? The "Limitations" section notes that the current approach "does not consider dynamic factors such as the user's current location or emotional state," potentially leading to poor recommendations when user behavior deviates from historical patterns. This remains unresolved because the current User Preference Learning module relies on aggregating historical interaction trajectories but lacks a mechanism to ingest or weigh real-time environmental or physiological signals. Evidence that would resolve this includes an extension of the User FingerTip benchmark that includes context-dependent instructions requiring integration of dynamic sensor data.

### Open Question 3
Does the experience optimization strategy (ADD/DELETE/UPDATE) effectively manage memory saturation and redundancy in real-world, long-term usage scenarios? The dataset statistics show only 15 training instructions per user, whereas the paper claims to store "long-term memory," leaving the efficiency of the experience management mechanism untested over extended interaction periods. This remains unresolved because the optimization module prevents unbounded prompt growth, but it is unclear if the logic for merging or deleting experiences preserves critical but infrequent user preferences over thousands of interactions. Evidence that would resolve this includes a longitudinal study tracking memory size and retrieval accuracy over hundreds of instructions per user, analyzing the retention rate of distinct but rare preferences.

## Limitations
- Learned experiences may become invalid when applications update or unexpected pop-ups appear
- The personalization approach does not consider dynamic factors such as user's current location or emotional state
- Only evaluated on simulated environment with 15 training instructions per user

## Confidence
- **High confidence**: Task completion metrics on User FingerTip benchmark, hierarchical memory organization principles, and general two-stage ambiguity resolution framework
- **Medium confidence**: Specific VLM reward model formulation, experience extraction quality from trajectory comparisons, and scalability of memory system to diverse user populations
- **Low confidence**: Cross-app preference transfer capabilities, robustness to rapid UI changes, and performance on real-world ambiguous instructions beyond benchmark distribution

## Next Checks
1. **Generalization stress test**: Evaluate Me-Agent on held-out subset of apps not seen during training, measuring degradation in task completion and preference accuracy to assess model generalization
2. **UI change resilience**: Simulate app UI updates by modifying element positions in test environment, then measure recovery time and accuracy degradation after retraining with updated experiences
3. **Memory scalability analysis**: Gradually increase number of stored experiences across multiple apps and measure inference latency, token overflow occurrences, and retrieval precision to identify scaling bottlenecks