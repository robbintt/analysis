---
ver: rpa2
title: Can Large Language Models Identify Implicit Suicidal Ideation? An Empirical
  Evaluation
arxiv_id: '2502.17899'
source_url: https://arxiv.org/abs/2502.17899
tags:
- suicidal
- ideation
- implicit
- suicide
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models' (LLMs) ability to identify
  and respond to implicit suicidal ideation. Using a novel dataset, DeepSuiMind, grounded
  in psychological theory, the authors assess 8 widely used LLMs across two prompting
  conditions.
---

# Can Large Language Models Identify Implicit Suicidal Ideation? An Empirical Evaluation

## Quick Facts
- arXiv ID: 2502.17899
- Source URL: https://arxiv.org/abs/2502.17899
- Reference count: 40
- Primary result: LLMs struggle to detect subtle suicide cues and often fail to provide appropriate support, with most scoring below 80 in response quality and under 50% in appropriate response rate

## Executive Summary
This study evaluates eight widely used large language models on their ability to identify and respond to implicit suicidal ideation using a novel dataset, DeepSuiMind, grounded in psychological theory. Results show that while models can handle explicit suicidal statements reasonably well, they struggle significantly with implicit cues and often fail to provide clinically appropriate support. Distress-aware prompting improves performance but gaps remain, especially in reducing hopelessness and offering hope. The findings highlight the need for more clinically grounded evaluation frameworks to ensure safe LLM deployment in sensitive mental health contexts.

## Method Summary
The study constructs DeepSuiMind dataset using theoretical frameworks (D/S-IAT, Automatic Negative Thoughts, and psychosocial stressors) to generate implicit suicidal ideation scenarios. Eight LLMs are evaluated across two prompting conditions (Standard vs. Distress-Aware) using GPT-4 as automated evaluator. Responses are scored on five clinical dimensions (Empathy, Connection, Practical Support, Reducing Hopelessness, Offering Hope) plus binary applicability judgment. The dataset includes 1,308 test cases plus supplementary explicit ideation data from Reddit posts.

## Key Results
- Models scored below 80 in response quality across most dimensions
- Appropriate Response Success Rate (ARSR) remained under 50% for implicit cases
- Distress-aware prompting improved sensitivity but detection gaps persisted, especially for reducing hopelessness
- "Death-Me" associations yielded lowest scores across all models despite being most clinically predictive
- LLaMA-3-8B showed high refusal rates on explicit content but fewer on implicit cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distress-aware prompting improves LLM sensitivity to implicit suicidal ideation by priming attention to emotional risk cues.
- Mechanism: Subtle contextual framing shifts model behavior from conversational default to risk-aware response mode without triggering explicit safety refusals.
- Core assumption: Models possess latent capability to recognize implicit risk but fail to activate this capability without appropriate contextual signals.
- Evidence anchors: Gemini-1.5's ARSR rises from 77.67% to 91.83% under distress-aware prompting; related work confirms implicit signals are harder to detect without contextual framing.

### Mechanism 2
- Claim: The implicit-explicit performance gap stems from recognition failure rather than response formulation failure.
- Mechanism: Models can generate clinically appropriate responses when suicidal intent is explicit, but fail to detect subtle D/S-IAT-based associations that require inference across metaphor, emotional flatness, and cognitive distortions.
- Core assumption: Appropriate crisis response knowledge exists in model weights but is not triggered by ambiguous input features.
- Evidence anchors: Death-Me associations yield lowest scores despite being most clinically predictive; models become more sensitive once alerted to potential distress.

### Mechanism 3
- Claim: Psychology-grounded dataset construction captures implicit ideation patterns that passive data collection misses.
- Mechanism: Structured synthesis combining implicit association structures, cognitive distortion patterns, and contextual stressors produces linguistically diverse samples that reflect how suicidal ideation manifests in private dialogue.
- Core assumption: Synthetic data constructed with theoretical grounding approximates real implicit suicidal expression sufficiently for evaluation purposes.
- Evidence anchors: Dataset validated against real Reddit posts with small effect sizes; synthetic data enables controlled psychological grounding but may miss real-world nuance.

## Foundational Learning

- **Death/Suicide Implicit Association Test (D/S-IAT)**: Core theoretical framework for the dataset; measures unconscious self-death vs. self-life associations that predict suicidal behavior independent of self-report. *Quick check: Can you explain why "Death-Me" associations are harder for LLMs to detect than "Death-Not-Me" patterns?*

- **Automatic Negative Thoughts (ANT)**: Ten cognitive distortions (e.g., all-or-nothing thinking, overgeneralization) that structure implicit suicidal language in the dataset. *Quick check: Which ANT type would produce language like "One rejection means I'll never be loved"?*

- **Crisis Intervention Principles**: Five evaluation dimensions are grounded in Rogers, Beck, and Snyder's therapeutic frameworks. *Quick check: Why does "Reducing Hopelessness" score lower than "Empathy" across all models?*

## Architecture Onboarding

- **Component map**: Dataset Construction -> Evaluation Pipeline -> Experiment Conditions (Standard Prompting vs. Distress-Aware Prompting)
- **Critical path**: Generate implicit suicidal text using theoretically grounded prompts → Collect LLM responses under two prompting conditions → Score responses on five clinical dimensions plus applicability criteria → Compare implicit vs. explicit detection performance
- **Design tradeoffs**: Synthetic data enables controlled psychological grounding but may miss real-world nuance; GPT-4 as evaluator shows 0.706-0.798 Cohen's Kappa with human raters—acceptable but imperfect
- **Failure signatures**: High Empathy scores with low Reducing Hopelessness = surface-level validation without risk recognition; LLaMA-3-8B frequent refusals on explicit content but fewer on implicit = safety triggers mismatched to actual risk
- **First 3 experiments**: 1) Replicate distress-aware prompting effect on held-out implicit cases to confirm mechanism generalizes; 2) Ablate each psychological component to identify which contributes most to detection difficulty; 3) Test whether few-shot examples of implicit→appropriate response pairs improve ARSR without explicit prompting

## Open Questions the Paper Calls Out

- **Open Question 1**: Which specific model architectures and fine-tuning strategies respond most effectively to distress-aware prompting for implicit ideation? The current study evaluates models as black boxes, identifying performance gaps without isolating the architectural mechanisms that drive sensitivity to subtle cues.

- **Open Question 2**: To what extent do these implicit ideation detection capabilities generalize across diverse linguistic and cultural contexts? The DeepSuiMind dataset and evaluation framework are currently monolingual and potentially Western-centric in their psychological framing.

- **Open Question 3**: Does high performance on the synthetic DeepSuiMind dataset reliably predict clinical efficacy in real-world therapeutic settings? The dataset relies on simulated scenarios grounded in theory, which may lack the "cultural, contextual, and emotional nuances" of authentic patient interactions.

## Limitations
- The dataset relies on synthetic generation rather than authentic implicit suicidal expressions, raising questions about ecological validity despite small effect size validation against real Reddit posts
- GPT-4 serves as both synthesis engine and evaluation metric, creating potential circularity in assessment quality
- The distress-aware prompting mechanism may not generalize to all implicit patterns or real-world deployment scenarios

## Confidence
- **High confidence**: LLMs show measurable but limited performance on implicit ideation detection; explicit cases are handled better than implicit ones
- **Medium confidence**: Distress-aware prompting improves implicit case detection through psychological cue priming; the specific mechanisms and generalizability require further validation
- **Low confidence**: The five-dimensional clinical scoring framework fully captures appropriate crisis response quality; automated scoring may miss nuanced aspects of human support

## Next Checks
1. Test distress-aware prompting across diverse LLM architectures and datasets to confirm the psychological cue mechanism generalizes beyond this specific implementation
2. Validate DeepSuiMind synthetic data against a held-out set of clinically verified implicit suicidal expressions from therapy transcripts or crisis hotline interactions
3. Implement human-in-the-loop validation for a stratified sample of model responses to quantify where automated scoring diverges from clinical expertise