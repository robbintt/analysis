---
ver: rpa2
title: 2D Representation for Unguided Single-View 3D Super-Resolution in Real-Time
arxiv_id: '2511.08224'
source_url: https://arxiv.org/abs/2511.08224
tags:
- depth
- super-resolution
- pncc
- image
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 2Dto3D-SR introduces a framework for real-time single-view 3D super-resolution
  that eliminates the need for high-resolution RGB guidance. The core idea is to encode
  3D geometry from a single viewpoint into a structured 2D representation using Projected
  Normalized Coordinate Code (PNCC), enabling direct application of existing 2D image
  super-resolution architectures.
---

# 2D Representation for Unguided Single-View 3D Super-Resolution in Real-Time

## Quick Facts
- arXiv ID: 2511.08224
- Source URL: https://arxiv.org/abs/2511.08224
- Reference count: 0
- Primary result: Real-time single-view 3D super-resolution without RGB guidance using 2D PNCC representation

## Executive Summary
2Dto3D-SR introduces a framework for real-time single-view 3D super-resolution that eliminates the need for high-resolution RGB guidance. The core idea is to encode 3D geometry from a single viewpoint into a structured 2D representation using Projected Normalized Coordinate Code (PNCC), enabling direct application of existing 2D image super-resolution architectures. This approach avoids the complexities of 3D point-based or RGB-guided methods. The framework is implemented with two architectures: Swin Transformers for high accuracy and Vision Mamba for high efficiency. Experiments show the Swin Transformer model achieves state-of-the-art accuracy on standard benchmarks, while the Vision Mamba model delivers competitive results at real-time speeds.

## Method Summary
The method converts low-resolution depth maps to PNCC format using camera intrinsics, then applies 2D super-resolution architectures (Swin Transformer or Vision Mamba) to produce high-resolution PNCC outputs. The predicted PNCC is converted back to depth maps or point clouds. Two output strategies are explored: predicting full XYZ coordinates or just depth (Z). The framework trains from scratch using pixel-wise Charbonnier loss, avoiding the need for RGB pretraining.

## Key Results
- SwinT-PNCC achieves state-of-the-art RMSE on standard 3D super-resolution benchmarks
- VM-PNCC delivers competitive accuracy (within 1-4 cm of SwinT) at real-time speeds (0.027-0.074s across scales)
- Z-only prediction slightly outperforms XYZ prediction, enforcing geometric consistency through camera ray constraints
- The unguided approach matches or exceeds performance of RGB-guided methods without requiring high-resolution RGB data

## Why This Works (Mechanism)

### Mechanism 1
Encoding 3D geometry into structured 2D representations enables direct reuse of mature 2D image super-resolution architectures for 3D tasks. The PNCC maps normalized 3D coordinates to RGB channels, preserving single-view geometric information in an image-compatible format while remaining reversible. The 2D spatial structure preserves sufficient geometric relationships for 2D SR models to learn meaningful upsampling patterns. Break condition: If camera intrinsics are unknown or significantly distorted, the PNCC encoding becomes unreliable and the entire pipeline degrades.

### Mechanism 2
Vision Mamba architectures achieve competitive accuracy with substantially lower latency than transformer-based alternatives for this task. VM-PNCC leverages state-space model architecture with linear complexity O(N) versus quadratic O(N²) of transformer attention, enabling real-time inference while maintaining competitive RMSE within ~1-4 cm of SwinT-PNCC. Break condition: If geometric detail requires truly global context (e.g., large surface discontinuities), the limited receptive field of early Mamba layers may miss long-range dependencies before deep feature fusion.

### Mechanism 3
Predicting only the Z channel (depth) rather than full XYZ yields marginally better geometric consistency. When predicting Z alone and recovering XY via known camera intrinsics, the model is constrained to output points along valid camera rays. This enforces geometric plausibility and reduces the solution space, leading to slightly lower RMSE. Break condition: If intrinsics change between capture and processing (e.g., cropped images, zoom), the Z→XYZ reconstruction will produce incorrect point clouds.

## Foundational Learning

- **Projected Normalized Coordinate Code (PNCC)**
  - Why needed here: This is the core representation that enables the entire framework. Without understanding how 3D coordinates map to RGB values and back, you cannot debug data preparation or output interpretation.
  - Quick check question: Given a depth value of 2.5m at pixel (320, 240) with fx=fy=500, cx=320, cy=240, and scale s=5, what are the PNCC RGB values?

- **Camera Intrinsics and Back-Projection**
  - Why needed here: The pipeline requires intrinsics for both PNCC encoding and Z→XYZ reconstruction. Misaligned intrinsics cause systematic geometric errors.
  - Quick check question: If you crop a 640×480 image to 320×240 starting at (160, 120), how must you adjust cx and cy?

- **Charbonnier Loss**
  - Why needed here: Both models are trained with pixel-wise Charbonnier loss, a robust L1 variant. Understanding its behavior with outliers helps explain training dynamics.
  - Quick check question: How does Charbonnier loss differ from L1 loss when |x| >> ε, and why might this matter for depth outliers?

## Architecture Onboarding

- **Component map:** Depth map + intrinsics → PNCC encoding → Invalid pixel masking → Model (SwinT/VM) → Upsampling CNN fusion → PNCC output → Un-normalize → Depth map/point cloud

- **Critical path:**
  1. Data preprocessing: Ensure depth maps have correctly associated intrinsics; invalid pixels must be tracked through masking.
  2. PNCC encoding: Apply Eq. 1 with consistent scale factor s; verify RGB values are in [0,1] after normalization.
  3. Model inference: Forward pass through chosen architecture; invalid regions are excluded from loss but filled for stability.
  4. Geometric reconstruction: Un-normalize predicted PNCC; if Z-only, compute XY from intrinsics.

- **Design tradeoffs:**
  - **SwinT-PNCC vs VM-PNCC:** SwinT offers ~1-2 cm better RMSE but is 3-6× slower. Choose based on latency budget.
  - **Z vs XYZ prediction:** Z-only enforces ray consistency but requires accurate intrinsics at inference. XYZ is more robust to intrinsics mismatch but slightly worse accuracy.
  - **Training from scratch:** Authors train directly on PNCC rather than fine-tuning from RGB pretrained weights—likely necessary due to domain shift.

- **Failure signatures:**
  - Blurred depth edges: Model may be undertrained or bicubic baseline—check training loss convergence.
  - Scattered outlier points in point cloud: May indicate invalid pixel handling issues or model overfitting to training distribution.
  - Systematic depth offset: Check scale factor s consistency between encoding and decoding.
  - Performance drop on real data: Domain gap from synthetic training—consider augmentation or fine-tuning on target domain.

- **First 3 experiments:**
  1. Baseline validation: Implement bicubic PNCC upsampling → un-normalize → measure RMSE against ground truth depth. Verify your PNCC encoding/decoding is correct by checking that identity (no SR) reconstructs original depth exactly.
  2. Architecture comparison: Train both SwinT-PNCC and VM-PNCC on a subset of NYUv2 (e.g., 10K samples) with Z-only output. Compare RMSE and inference time to confirm expected tradeoffs before full training.
  3. Generalization test: Train on NYUv2, evaluate on Middlebury without any domain adaptation. If RMSE exceeds 20 cm at ×4, investigate intrinsics handling or add basic augmentation (scale jitter, noise injection).

## Open Questions the Paper Calls Out
- How can the 2Dto3D-SR framework be extended to handle multi-view 3D inputs to overcome the limitations of single-view occlusion?
- Can this 2D representation pipeline be effectively applied to other 3D data enhancement tasks, such as geometry denoising or hole completion?
- Why does predicting only the Z-channel (depth) result in equal or better performance than predicting the full XYZ PNCC, despite the latter containing more geometric information?
- How robust is the framework to errors in camera intrinsic parameters during the PNCC generation process?

## Limitations
- The method relies on accurate camera intrinsics at both encoding and decoding stages, with no quantification of sensitivity to calibration errors
- Single-view design cannot recover occluded geometry, limiting completeness of 3D reconstructions
- The custom NYUv2 alignment procedure is not detailed, preventing exact replication of reported results
- Scale factor s in PNCC computation is unspecified, making exact reproduction impossible

## Confidence
- **High confidence:** Swin Transformer architecture performance and general methodology are well-supported by extensive literature and ablation studies
- **Medium confidence:** Vision Mamba efficiency claims are plausible given architecture's linear complexity, but no direct corpus evidence exists for 3D tasks specifically
- **Medium confidence:** The Z-only prediction advantage is demonstrated empirically but relies on the strong assumption of known intrinsics at inference

## Next Checks
1. Verify PNCC encoding/decoding by implementing the full pipeline with synthetic depth data and camera parameters, checking that identity mapping (no SR) reconstructs the original depth exactly
2. Reproduce the Z vs XYZ ablation on a small NYUv2 subset to confirm the reported ~0.5cm accuracy difference before full training
3. Test model generalization by training on synthetic data and evaluating on real-world RGB-D-D dataset, measuring domain adaptation requirements