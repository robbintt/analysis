---
ver: rpa2
title: An Efficient Open World Environment for Multi-Agent Social Learning
arxiv_id: '2508.15679'
source_url: https://arxiv.org/abs/2508.15679
tags:
- agents
- environment
- learning
- agent
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-Agent Craftax (MAC), an efficient JAX-based
  multi-agent environment built on Craftax, designed to study social learning, collaborative
  tool use, and cooperation vs. competition in open-ended settings.
---

# An Efficient Open World Environment for Multi-Agent Social Learning

## Quick Facts
- arXiv ID: 2508.15679
- Source URL: https://arxiv.org/abs/2508.15679
- Authors: Eric Ye; Ren Tao; Natasha Jaques
- Reference count: 17
- Primary result: MAC shows emergent collaborative tool use (90% using others' tables) but no benefit from social learning methods.

## Executive Summary
This paper introduces Multi-Agent Craftax (MAC), an efficient JAX-based multi-agent environment built on Craftax, designed to study social learning, collaborative tool use, and cooperation vs. competition in open-ended settings. MAC features multiple self-interested agents pursuing independent goals in a large, partially-observable, sparse reward environment with shared tools and common enemies. Experiments compared state-of-the-art MARL algorithms with and without social learning methods, finding no significant performance benefit from observing expert agents (cultural transmission scores near zero). However, agents shared tools 90% of the time, using others' crafting tables more often than their own, demonstrating emergent collaborative behavior. Competitive and cooperative settings reduced performance compared to baseline independent training, with cooperation increasing agent proximity.

## Method Summary
The MAC environment extends Craftax-Classic to multi-agent by adding 4 players sharing world state, per-agent rewards (+1 per achievement, +0.1×health change), action conflict resolution, and mob logic targeting the nearest player. The training uses Independent PPO (IPPO) with LSTM policies (512→256→LSTM256→actor/critic), collecting rollouts from all 4 agents and applying independent updates per agent. Three experiment families were run: social learning (pre-train expert, train learner with expert visible/hidden/half-removed, compute CT score), tool sharing (train all from scratch, log own vs others' table usage), and cooperation/competition (shared reward vs attack-enabled vs proximity bonus, compare achievements and proximity curves).

## Key Results
- Cultural transmission scores near zero (-0.056), indicating no retention of expert knowledge
- 90% of tool usage came from others' crafting tables rather than own
- Cooperation settings increased agent proximity but reduced achievement completion
- Competitive settings with attacks also reduced performance vs baseline
- No MARL algorithm outperformed IPPO in this environment

## Why This Works (Mechanism)

### Mechanism 1: Environmental Stigmergy via Persistent Tool Affordances
- **Claim**: Agents benefit from shared tools not through active coordination, but because environmental modifications persist and alter the action space for all agents.
- **Core assumption**: The cost of locating and using another agent's tool is lower than crafting one's own, and tools persist indefinitely in the environment.
- **Evidence**: Agents shared tools 90% of the time, using others' crafting tables more often than their own.

### Mechanism 2: Attention Competition from Common Enemies
- **Claim**: Shared hostile mobs create survival pressure that partially aligns individual incentives with proximity to other agents.
- **Core assumption**: Mobs always target nearest player; agents don't compete for kill rewards.
- **Evidence**: Mob logic uses nearest player; agents may be implicitly incentivized to cooperate to defeat common enemies.

### Mechanism 3: Achievement Structure with Shared Prerequisites
- **Claim**: The tech tree design creates overlapping subtask dependencies that allow one agent's progress to bootstrap another's.
- **Core assumption**: Achievement rewards are independent per agent; resources are not severely constrained.
- **Evidence**: Goals share underlying subtasks, such as tool crafting, with a directed acyclic graph structure.

## Foundational Learning

### Concept: Partially Observable Markov Games (POMGs)
- **Why needed**: MAC is formally defined as a POMG where agents receive local 7×9 observation grids, not global state. This explains why recurrent policies (LSTM) outperform MLP—agents must integrate observations over time to infer world state.
- **Quick check**: Why would an MLP policy get "stuck" when surrounded by objects while an LSTM could recover?

### Concept: Cultural Transmission Score
- **Why needed**: This metric quantifies social learning: CT = ½(A_full - A_solo)/E + ½(A_half - A_solo)/E. Negative scores (as found: -0.056) indicate agents don't retain expert knowledge after the expert departs.
- **Quick check**: If A_full > A_solo but A_half ≈ A_solo, what does this suggest about when learning happens?

### Concept: Independent PPO (IPPO)
- **Why needed**: The baseline algorithm trains each agent with PPO using only local observations and rewards, no centralized critic or parameter sharing. This design choice explains why agents don't spontaneously learn to read others' behaviors—no training signal encourages it.
- **Quick check**: Why would adding a social auxiliary loss fail if agents rarely stay in proximity to observe each other?

## Architecture Onboarding

### Component Map
```
┌─────────────────────────────────────────────┐
│           MAC Environment (JAX)             │
├─────────────────────────────────────────────┤
│ • World state (shared terrain, mobs, items) │
│ • Per-agent observations (7×9 symbolic grid)│
│ • Achievement tracker (22 per agent)        │
│ • Tool ownership logger (placed_by, used_by)│
└─────────────────────────────────────────────┘
              ↓ observations, rewards
┌─────────────────────────────────────────────┐
│         Training Loop (IPPO + variants)     │
├─────────────────────────────────────────────┤
│ • 400 parallel environments on GPU          │
│ • LSTM policy (512→256→LSTM256→actor/critic)│
│ • Optional: social auxiliary loss head      │
│ • Metrics: CT score, proximity, tool usage  │
└─────────────────────────────────────────────┘
```

### Critical Path
1. **Environment setup**: Install JAX with GPU support, load MAC from provided codebase
2. **Baseline run**: Train 4 agents with IPPO + LSTM for 100M steps (~1 hour on L40)
3. **Expert extraction**: Select highest-performing checkpoint as expert policy
4. **Social learning experiment**: Train new agent alongside frozen expert, compute CT score
5. **Tool sharing analysis**: Parse logs for table placement vs. usage statistics

### Design Tradeoffs
- **Symbolic vs. pixel observations**: Symbolic trains faster; pixel is more general but needs larger networks
- **LSTM vs. MLP**: LSTM handles partial observability (agents don't get stuck); MLP is simpler but limited
- **Fixed vs. variable episode length**: Fixed enables fair comparison; variable is more natural but risks training divergence

### Failure Signatures
- **CT score ≈ 0**: Social learning not working—check agent proximity over time
- **Achievement plateau at 15-16/22**: Exploration limit; may need 1B steps or curriculum
- **Proximity bonus reduces reward**: Agents learn to follow rather than explore
- **Training curves diverge**: Early deaths create uneven training—use fixed timesteps for comparison runs

### First 3 Experiments
1. **Baseline reproduction**: Run IPPO + LSTM with 4 agents for 100M steps. Verify mean achievements ≈15.4 ± 1.3 per episode.
2. **Tool sharing validation**: Enable tool tracking, confirm 90%+ usage of others' tables. Analyze whether this correlates with faster achievement of higher-tier goals.
3. **Proximity-ablated social learning**: Add proximity bonus (0.01-0.05) + social auxiliary loss. Test whether forced proximity enables the auxiliary loss to improve CT score above zero.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can agents in MAC achieve significant reductions in sample complexity by successfully acquiring skills from other agents via social learning?
- **Basis**: The authors state in Section 7 (Future Work) that "if agents could build on skills acquired from other agents via social learning, sample complexity could be significantly reduced."
- **Why unresolved**: Current experiments showed cultural transmission scores near zero, indicating agents failed to learn from experts.
- **What evidence would resolve it**: An algorithm demonstrating higher performance or faster convergence when trained with experts compared to a solo baseline.

### Open Question 2
- **Question**: Can social learning algorithms be developed that do not rely on artificial proximity bonuses, which were found to reduce exploration and reward?
- **Basis**: Section 6.3 hypothesizes that social learning failed because agents naturally drift apart, and forcing proximity via bonuses lowers total reward.
- **Why unresolved**: The tension between staying close enough to observe experts and exploring the environment for independent resources remains unsolved.
- **What evidence would resolve it**: A method achieving positive cultural transmission without explicit proximity rewards or performance degradation.

### Open Question 3
- **Question**: Can competitive reward structures be designed to drive the emergence of complex behavior in MAC, rather than just reducing performance?
- **Basis**: Section 6.3 notes that unlike prior work (Suarez et al., 2019), enabling attacks in MAC reduced performance, raising the question of how to utilize competition effectively.
- **Why unresolved**: The specific implementation of competition (attacking) hindered achievement rather than stimulating intelligent adversarial behavior.
- **What evidence would resolve it**: A competitive setting where agents achieve higher average achievements or unlock later tech-tree nodes compared to the independent baseline.

## Limitations

- Exact PPO hyperparameters beyond batch structure are unspecified, making exact reproduction challenging
- Expert pre-training duration and selection criteria are not detailed
- The analysis of why social learning fails is speculative—agents rarely observe each other, yet the paper does not conclusively rule out alternative explanations
- Lack of ablation on observation radius or reward shaping leaves the door open for alternative explanations of the CT score near zero

## Confidence

- **High confidence**: The environmental mechanisms enabling tool sharing (persistent tool affordances, achievement overlap) are well-supported by the quantitative data
- **Medium confidence**: The claim that current social learning methods are ineffective is supported by CT scores, but the underlying reasons (low proximity, sparse rewards) are inferred rather than directly tested
- **Low confidence**: The assertion that cooperation/competition settings reduce performance is based on relative comparisons, but without ablation on episode length or agent mortality, it's unclear if the observed effects are robust

## Next Checks

1. **Ablation on agent proximity**: Modify observation radius or add a proximity reward to force agents to stay within visual range. Measure whether this increases CT score above zero and whether it correlates with improved achievement completion.
2. **Tool degradation experiment**: Introduce tool decay (e.g., crafting tables break after N uses). Track whether tool sharing drops from 90% and if achievement completion slows, directly testing the stigmergy hypothesis.
3. **Episode length control**: Run experiments with fixed episode length (as in single vs multi comparison) to eliminate training imbalance from differential agent mortality. Verify whether cooperation/competition performance differences persist under controlled timesteps.