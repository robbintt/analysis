---
ver: rpa2
title: Budget Allocation Policies for Real-Time Multi-Agent Path Finding
arxiv_id: '2507.16874'
source_url: https://arxiv.org/abs/2507.16874
tags:
- agents
- budget
- planning
- fixed
- mapf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Real-Time Multi-Agent Path Finding (RT-MAPF),
  where agents must find collision-free paths within strict computational time budgets,
  committing to fixed action sequences before planning completes. The authors identify
  that standard MAPF algorithms fail to allocate computational resources effectively
  in over-constrained scenarios, leading to poor partial solutions.
---

# Budget Allocation Policies for Real-Time Multi-Agent Path Finding

## Quick Facts
- arXiv ID: 2507.16874
- Source URL: https://arxiv.org/abs/2507.16874
- Reference count: 7
- Primary result: ConflictProportion budget allocation significantly improves makespan in over-constrained RT-MAPF scenarios compared to shared budget approaches

## Executive Summary
This paper addresses Real-Time Multi-Agent Path Finding (RT-MAPF), where agents must find collision-free paths within strict computational time budgets, committing to fixed action sequences before planning completes. The authors identify that standard MAPF algorithms fail to allocate computational resources effectively in over-constrained scenarios, leading to poor partial solutions. The core contribution is a set of budget allocation policies that distribute planning resources across agents and neighborhoods. Experimental results on standard MAPF benchmarks show significant improvements, with ConflictProportion allocation solving more problems with smaller makespans compared to baseline shared budget allocation.

## Method Summary
The paper proposes two budget allocation policies for RT-MAPF: Fixed allocation for Prioritized Planning (PrP) where each agent receives an equal computational budget, and ConflictProportion for MAPF-LNS2, which allocates budget to neighborhoods based on their proportional involvement in conflicts with a lower bound. The system uses the Rolling Horizon Collision Resolution (RHCR) framework with Morag et al.'s fail policy (IStay). Planning budget B = 15 × num_agents, with execution window w = 5 (Exp.1) or 2-8 (Exp.2). The hybrid approach runs PIBT and LNS2 in parallel, returning the better partial solution.

## Key Results
- Fixed budget allocation for PrP prevents starvation where early agents exhaust the entire shared budget
- ConflictProportion allocation for LNS2 solves more problems with smaller makespans compared to shared budget allocation
- Hybrid PIBT+LNS2 with ConflictProportion yielded the best overall performance
- The method effectively addresses finding useful partial solutions under real-time constraints

## Why This Works (Mechanism)

### Mechanism 1: Conflict-Proportionate Neighborhood Budgeting
Allocating planning budget to agent neighborhoods based on their proportional involvement in conflicts improves solution quality in over-constrained scenarios. The ConflictProportion policy calculates budget as B(N) = B_total × (Σ conflicts in N)/(Σ all conflicts), with a lower bound to ensure minimum viable search effort. This routes resources toward bottlenecks while preserving minimum search effort.

### Mechanism 2: Fixed Agent-Level Budgeting (PrP)
Distributing a fixed budget per agent prevents "starvation" where early agents in a priority queue exhaust the entire shared budget. The total budget B is divided by the number of agents k, with each agent receiving B/k search nodes. Unused budget is redistributed to remaining agents, isolating the cost of difficult agents.

### Mechanism 3: Hybrid PIBT and LNS2 Execution
Combining a fast, rule-based policy (PIBT) with a search-based optimizer (LNS2) yields lower makespan than either alone by covering the blind spots of the other. The system runs PIBT and LNS2 in parallel, comparing partial solutions and committing to the better one.

## Foundational Learning

- **Concept: Large Neighborhood Search (LNS) in MAPF**
  - Why needed: The paper modifies MAPF-LNS2, which works by iteratively selecting subsets of agents (neighborhoods) and replanning their paths while keeping others fixed
  - Quick check: How does the "neighborhood" definition in LNS differ from the "priority" definition in Prioritized Planning?

- **Concept: Rolling Horizon Collision Resolution (RHCR)**
  - Why needed: This is the execution framework used, decoupling the planning horizon from the execution window
  - Quick check: If the planning horizon is shorter than the execution window, what risk is introduced?

- **Concept: Solution Makespan vs. Sum of Costs**
  - Why needed: The paper evaluates success primarily via makespan (time for the last agent to arrive)
  - Quick check: In a scenario with 100 agents, why might increasing the path length of 1 agent be acceptable if it reduces the maximum time taken by any single agent?

## Architecture Onboarding

- **Component map**: Input (Graph, Agent starts/goals, Global Budget B, Execution Window w) -> Budget Manager (implements Fixed/ConflictProportion) -> Planners (PrP Engine, LNS2 Engine, PIBT Engine) -> Fail Policy (IStay) -> Executor (commits first w steps)
- **Critical path**: Observe current state -> Budget Manager computes limits -> Run LNS2 (and PIBT if hybrid) -> If timeout/invalid trigger Fail Policy -> Select best valid prefix -> Execute w steps
- **Design tradeoffs**: Shared vs. Fixed Budget (simpler vs. robust to starvation); Neighborhood Size vs. Budget (larger neighborhoods require higher lower bounds)
- **Failure signatures**: Budget Exhaustion (Nodes Expanded == Budget Limit frequently); Starvation (early agents consume 100% budget); IStay Loops (high makespan with low throughput)
- **First 3 experiments**: 1) Starvation Reproduction: Run PrP with Shared vs. Fixed budget on bottleneck map; 2) Policy Sensitivity: Run LNS2 with ConflictProportion vs. Fixed(50) on Random-20 maps; 3) Hybrid Validation: Run LNS2 alone vs. PIBT vs. Hybrid on Room-4

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can online learning mechanisms dynamically adjust budget allocation during execution to outperform static policies like ConflictProportion?
- Basis: Future work may consider incorporating online learning mechanisms to adjust the budget allocated to different agents during execution
- Why unresolved: The paper only evaluates static allocation policies; adaptive, learning-based approaches remain unexplored

### Open Question 2
- Question: Does ConflictProportion transfer effectively to lifelong MAPF where agents receive new targets continuously?
- Basis: Future work may consider applying ConflictProportion in lifelong MAPF settings
- Why unresolved: All experiments focus on RT-MAPF with fixed targets, not lifelong scenarios with streaming task allocation

### Open Question 3
- Question: Is there a theoretically grounded lower bound formula for neighborhood budget allocation that outperforms the current heuristic BL(N)?
- Basis: The lower bound was chosen empirically without theoretical justification
- Why unresolved: The lower bound is ad-hoc; no analysis proves it is optimal or near-optimal

## Limitations
- Experimental validation relies heavily on synthetic benchmarks without real-world deployment testing
- The ConflictProportion lower bound calculation assumes a quadratic relationship between neighborhood size and required budget that may not hold for complex map topologies
- The hybrid PIBT+LNS2 approach assumes PIBT's computational cost is negligible, but this is not empirically validated across different problem densities

## Confidence
- High confidence: The Fixed budget allocation mechanism for PrP effectively prevents starvation
- Medium confidence: ConflictProportion allocation improves makespan in over-constrained scenarios
- Low confidence: The assumed negligible cost of PIBT in hybrid execution

## Next Checks
1. Measure actual PIBT computational cost across varying agent densities to validate the "near-zero" assumption
2. Perform ablation studies on ConflictProportion's lower bound parameter to determine sensitivity to neighborhood size variations
3. Test budget allocation policies on real-world robotics scenarios with dynamic obstacles to assess robustness beyond synthetic benchmarks