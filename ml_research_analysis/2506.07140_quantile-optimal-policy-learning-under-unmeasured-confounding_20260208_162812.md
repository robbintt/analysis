---
ver: rpa2
title: Quantile-Optimal Policy Learning under Unmeasured Confounding
arxiv_id: '2506.07140'
source_url: https://arxiv.org/abs/2506.07140
tags:
- function
- policy
- assumption
- proof
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies quantile-optimal policy learning under unmeasured\
  \ confounding, where the goal is to find a policy maximizing the \u03B1-quantile\
  \ of the reward distribution. The problem is challenging due to nonlinearity of\
  \ the quantile objective, unobserved confounders, and insufficient coverage of the\
  \ offline dataset."
---

# Quantile-Optimal Policy Learning under Unmeasured Confounding

## Quick Facts
- arXiv ID: 2506.07140
- Source URL: https://arxiv.org/abs/2506.07140
- Reference count: 40
- Primary result: First sample-efficient algorithm for estimating quantile-optimal policy with unmeasured confounding, achieving O~(n^{-1/2}) quantile-optimality.

## Executive Summary
This paper addresses the challenging problem of learning policies that maximize a specific quantile (e.g., median or worst-case) of the reward distribution in the presence of unmeasured confounding. The authors propose a causal-assisted approach that leverages instrumental variables and negative controls to identify the structural quantile function through nonlinear functional integral equations. By combining minimax estimation with a pessimism principle, the method constructs conservative policy estimates that are robust to insufficient data coverage in offline settings. The resulting algorithm achieves sample-efficient learning with provable regret bounds.

## Method Summary
The method solves a nonlinear conditional moment restriction to identify the structural quantile function using instrumental variables or negative controls. It employs a minimax optimization framework with nonparametric models, where the empirical loss is constructed via Fenchel duality to represent conditional moment errors. The algorithm incorporates pessimism to address insufficient coverage by maximizing a lower bound of the policy value rather than the estimated value itself. A regularized policy learning approach is introduced for computational tractability, converting constrained optimization into a penalty term that can be solved via gradient descent.

## Key Results
- Proposes the first sample-efficient algorithm for quantile-optimal policy learning with unmeasured confounding
- Achieves O~(n^{-1/2}) quantile-optimality under mild coverage assumptions
- Introduces pessimism principle to handle insufficient data coverage in offline settings
- Develops computationally tractable regularized policy learning method

## Why This Works (Mechanism)

### Mechanism 1: Identification via Instrumental Variables (IV) and Negative Controls (NC)
The paper identifies the structural quantile function using auxiliary variables (IV or NC) to remove bias from unmeasured confounders. By observing an Instrumental Variable Z that affects action A but is conditionally independent of reward Y, the method establishes a nonlinear conditional moment restriction: E[W(D; h*α) | X, Z] = 0. Solving this integral equation recovers the causal effect of actions on the reward's quantile, bypassing naive regression bias. The IV must satisfy exclusion restriction and be relevant.

### Mechanism 2: Minimax Estimation via Fenchel Duality
The paper solves the ill-posed nonlinear inverse problem using a minimax optimization framework. Instead of directly solving the conditional expectation, the method minimizes an empirical loss function L_n(h) constructed via Fenchel duality to represent squared conditional moment error. This transforms the problem into finding the saddle point between a hypothesis function h and a test function θ. The test function space must be rich enough to approximate the moment function.

### Mechanism 3: Pessimism Principle for Robust Policy Learning
The paper ensures robustness against insufficient data coverage by maximizing a pessimistic lower bound of the policy value. It constructs a solution set S_ε or adds a regularization term to penalize estimators with high variance in sparse data regions. This prevents the learned policy from exploiting spurious correlations in under-explored regions. The offline data must cover the distribution induced by the optimal policy, though not necessarily all suboptimal policies.

## Foundational Learning

- **Concept: Quantile Regression & Structural Quantile Function**
  - Why needed here: Unlike standard RL optimizing for expected reward, this paper optimizes for a specific quantile of the reward distribution, which is a nonlinear functional.
  - Quick check question: Can you explain why optimizing for the median reward (L1 loss logic) is different from optimizing for the mean reward (L2 loss logic) when data has outliers?

- **Concept: Nonparametric Instrumental Variables (NPIV)**
  - Why needed here: The core identification strategy relies on IVs to solve the endogeneity problem without assuming a specific parametric form for the reward function.
  - Quick check question: What is the "exclusion restriction" in the context of an instrumental variable?

- **Concept: Offline Reinforcement Learning (Offline RL)**
  - Why needed here: The problem setting assumes a fixed dataset generated by a behavior policy, requiring specific techniques (like pessimism) to handle the lack of online exploration.
  - Quick check question: Why is "distribution shift" a critical failure mode in offline RL but less so in online RL?

## Architecture Onboarding

- **Component map:** Input Module -> Causal Estimator (Minimax Solver) -> Uncertainty Quantifier -> Policy Optimizer
- **Critical path:** The Minimax Solver is the bottleneck. Accurately estimating the structural quantile function h*α via the nonlinear conditional moment restriction is the hardest computational step.
- **Design tradeoffs:** Algorithm 1 (Solution Set) is theoretically tighter (Regret O~(n^{-1/2})) but involves constrained optimization which is computationally intractable. Algorithm 2 (Regularized) converts constraints to a penalty term, making it tractable via gradient descent but with a slightly looser regret bound (O~(n^{-1/2(1-εR)})).
- **Failure signatures:** Weak Instrument (if Z has low correlation with A, leading to high variance in hα), Coverage Failure (if offline data lacks examples of high-reward actions, causing pessimistic policy to default to low-reward actions)
- **First 3 experiments:** 1) Synthetic Validation (Linear Case) - Replicate Section 5.1 setup with linear function classes, 2) Pessimism Ablation - Compare "Greedy" vs "Pessimistic" approaches with varying structured proportion p, 3) Quantile Level Sensitivity - Sweep α values (0.15, 0.5, 0.85) to test downside vs upside optimization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework be adapted to handle discrete outcomes without relying on smoothing techniques?
- Basis in paper: [explicit] Appendix A states, "While our paper does not address discrete outcomes... we believe a similar algorithm could be applied... We leave this investigation to future research."
- Why unresolved: The theoretical analysis relies on Assumption 4.7, which requires the conditional density of the outcome to be continuous to ensure the pathwise derivative of the operator T exists for local expansion.
- What evidence would resolve it: A theoretical proof establishing regret bounds for discrete outcomes, potentially by modifying the pseudo-metric definition or local expansion techniques to handle non-continuous distributions.

### Open Question 2
- Question: Is the proposed algorithm provably effective for general causal structures beyond Instrumental Variables (IV) and Negative Controls (NC)?
- Basis in paper: [explicit] Section 6 concludes, "We posit that the proposed algorithm is equally effective when the ODCP possesses different underlying causal structures, as long as the quantile treatment effect can be identified by conditional moment restrictions. We leave this investigation to future research."
- Why unresolved: The paper focuses its theoretical guarantees specifically on IV and NC setups, and while the method is general, the proofs for consistency and regret bounds are tailored to the specific operators T_IV and T_NC.
- What evidence would resolve it: A unified theoretical analysis showing that the O~(n^{-1/2}) regret bound holds for any causal structure satisfying a general conditional moment restriction identification condition.

### Open Question 3
- Question: What are the verifiable sufficient conditions for the global identification (Assumption 4.1) of the structural quantile function in this setting?
- Basis in paper: [inferred] Appendix A discusses Assumption 4.1, noting that verifying global identification is difficult and relies on strong regularity conditions from Wong (2022), stating "The only study we are aware of that carefully outlines conditions for global identification is Wong (2022)."
- Why unresolved: The assumption is necessary for the uniqueness of the solution to the conditional moment restriction, but standard conditions are often restrictive or difficult to check in nonparametric models.
- What evidence would resolve it: Derivation of necessary and sufficient conditions for global identification that are weaker or more practically verifiable than those currently cited in the literature.

## Limitations

- **Identification Assumptions:** Method relies heavily on validity of instrumental variables or negative controls. If these assumptions are violated, the estimated quantile function will be biased, undermining all downstream policy learning.
- **Coverage Assumptions:** Pessimism principle assumes partial coverage of optimal policy's induced distribution. If offline data has zero overlap with this optimal policy distribution, the learned policy will likely be overly conservative.
- **Computational Tractability:** While Algorithm 2 introduces regularization for computational tractability, solving the minimax optimization problem for the structural quantile function remains challenging, especially with nonparametric function classes.

## Confidence

- **High Confidence:** Core identification mechanism using IVs or NCs to remove confounding bias, and pessimism principle for addressing coverage issues are well-established concepts in causal inference and offline RL, respectively. Sample efficiency result (O~(n^{-1/2}) quantile-optimality) under stated assumptions is theoretically sound.
- **Medium Confidence:** Specific minimax estimation framework using Fenchel duality for solving nonlinear conditional moment restriction is valid, but practical performance depends heavily on choice of function classes and optimization algorithm. Regularization approach in Algorithm 2 for computational tractability is reasonable but may introduce approximation errors.
- **Low Confidence:** Empirical validation is limited to a single synthetic dataset. Method's performance on real-world data with complex confounding structures and high-dimensional contexts is unknown. Randomized search heuristic for policy optimization may not scale well to more complex policy classes.

## Next Checks

1. **Synthetic Data Stress Test:** Systematically vary the strength of the instrument Z (correlation with A) and the degree of confounding (correlation between A and Y via unmeasured U). Confirm that the method's performance degrades gracefully as these assumptions are weakened.

2. **Real-World Dataset Application:** Apply the method to a semi-synthetic dataset (e.g., IHDP) where unmeasured confounding is simulated, or a real dataset with known IV structure (e.g., Mendelian randomization). Compare against baseline methods that ignore confounding.

3. **Function Class Sensitivity Analysis:** Test the method with different hypothesis spaces (e.g., neural networks, splines) and test function spaces. Quantify the impact of approximation error on the final policy regret.