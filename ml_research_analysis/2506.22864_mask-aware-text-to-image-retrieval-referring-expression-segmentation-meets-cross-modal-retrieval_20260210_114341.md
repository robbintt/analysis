---
ver: rpa2
title: 'Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets
  Cross-modal Retrieval'
arxiv_id: '2506.22864'
source_url: https://arxiv.org/abs/2506.22864
tags:
- retrieval
- image
- object
- segmentation
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Mask-aware Text-to-Image Retrieval (MaTIR),
  a novel task unifying text-to-image retrieval with referring expression segmentation.
  The authors propose a two-stage framework: first, SAM 2 generates object masks and
  Alpha-CLIP extracts region-level embeddings for segmentation-aware retrieval; second,
  a multimodal large language model (MLLM) re-ranks results and localizes objects
  with bounding boxes matched to segmentation masks.'
---

# Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval

## Quick Facts
- arXiv ID: 2506.22864
- Source URL: https://arxiv.org/abs/2506.22864
- Authors: Li-Cheng Shen; Jih-Kang Hsieh; Wei-Hua Li; Chu-Song Chen
- Reference count: 40
- Primary result: 92.97% mAP@50 for image-level retrieval on COCO (vs. 78.07% for Cluster-CLIP)

## Executive Summary
Mask-aware Text-to-Image Retrieval (MaTIR) introduces a novel task that unifies text-to-image retrieval with referring expression segmentation. The approach leverages pre-trained models (SAM 2, Alpha-CLIP, and MLLM) in a two-stage pipeline to achieve state-of-the-art performance without requiring any training. The method demonstrates significant improvements in both retrieval accuracy and segmentation quality, offering a scalable solution for object-level visual search across diverse image datasets.

## Method Summary
The MaTIR framework operates in two stages: First, SAM 2 generates object masks from images while Alpha-CLIP extracts region-level embeddings for segmentation-aware retrieval. Second, a multimodal large language model re-ranks retrieval results and localizes objects with bounding boxes matched to segmentation masks. This training-free approach achieves superior performance by combining the strengths of segmentation models for precise object localization with cross-modal retrieval capabilities, enabling both image-level and object-level retrieval with semantic segmentation outputs.

## Key Results
- Achieves 92.97% mAP@50 for image-level retrieval on COCO (vs. 78.07% for Cluster-CLIP)
- Achieves 71.64 mAP@50@50 for object-level retrieval and segmentation (vs. 55.48 for SAM4MLLM)
- Demonstrates zero-training requirement while outperforming state-of-the-art methods

## Why This Works (Mechanism)
The approach works by leveraging pre-trained models that have already learned rich visual and language representations. SAM 2 provides accurate object segmentation masks that enable region-level feature extraction, while Alpha-CLIP's cross-modal embeddings capture semantic relationships between text queries and visual regions. The MLLM re-ranking stage benefits from contextual understanding to refine results and generate accurate bounding boxes matched to segmentation masks, creating a synergistic pipeline that combines precise localization with semantic understanding.

## Foundational Learning
- **Cross-modal embeddings (Alpha-CLIP):** Learned representations that map text and image features into a shared semantic space; needed to establish semantic correspondence between queries and visual regions; quick check: verify embedding space alignment through nearest neighbor search
- **Object segmentation (SAM 2):** Instance segmentation that identifies and delineates object boundaries; needed to provide precise region masks for feature extraction; quick check: measure IoU scores on COCO validation set
- **MLLM re-ranking:** Large language models that understand multimodal context and can refine retrieval results; needed to leverage contextual understanding for final result selection; quick check: compare re-ranking performance with and without MLLM
- **Region-level feature extraction:** Process of extracting features from specific image regions rather than entire images; needed to enable object-level retrieval and segmentation; quick check: verify feature dimensionality matches retrieval requirements
- **Zero-shot learning:** Ability to perform tasks without task-specific training; needed to demonstrate scalability and avoid training data requirements; quick check: test on out-of-distribution datasets

## Architecture Onboarding

Component map: SAM 2 -> Alpha-CLIP -> MLLM re-ranking

Critical path: Input image → SAM 2 segmentation → Alpha-CLIP region embeddings → Retrieval stage → MLLM re-ranking → Final output (ranked images + bounding boxes)

Design tradeoffs: The two-stage pipeline trades computational efficiency for accuracy by running multiple pre-trained models sequentially. While this approach achieves superior performance, it introduces potential error propagation and higher inference costs compared to single-stage methods.

Failure signatures: The system may struggle with complex relational queries involving multiple objects, ambiguous descriptions, or unusual object poses that challenge segmentation models. Error propagation from poor segmentation masks can cascade through the pipeline, and MLLM re-ranking may fail when initial retrieval results are semantically distant from the query.

First experiments: (1) Run SAM 2 on COCO validation images to verify segmentation quality with IoU metrics, (2) Test Alpha-CLIP embeddings on sample text-image pairs to confirm semantic alignment, (3) Perform retrieval with Alpha-CLIP alone to establish baseline performance before adding MLLM re-ranking.

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on pre-trained models limits generalizability to domains with different visual characteristics
- Two-stage pipeline introduces error propagation and computational costs that may hinder real-time applications
- Evaluation focuses on standard metrics without extensive analysis of failure cases for complex relational queries

## Confidence

**High Confidence:** Retrieval performance improvements (92.97% vs 78.07% mAP@50) and zero-training requirement are well-supported by metrics and ablation studies.

**Medium Confidence:** Scalability claims are reasonable given training-free nature, but require validation on industrial-scale datasets beyond COCO and D3.

**Medium Confidence:** MLLM re-ranking contribution is supported by metrics, but specific component contributions could be better isolated through additional ablations.

## Next Checks
1. Evaluate MaTIR performance on specialized datasets like LVIS or OpenImages to assess robustness to diverse object categories and long-tail distributions
2. Systematically analyze failure cases involving compositional queries to identify limitations in handling spatial relationships and attribute combinations
3. Measure end-to-end inference time and memory requirements to quantify practical deployment constraints and identify optimization opportunities