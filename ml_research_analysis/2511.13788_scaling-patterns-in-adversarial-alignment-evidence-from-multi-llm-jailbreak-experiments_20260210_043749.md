---
ver: rpa2
title: 'Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak
  Experiments'
arxiv_id: '2511.13788'
source_url: https://arxiv.org/abs/2511.13788
tags:
- harm
- adversarial
- attacker
- alignment
- refusal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically explores how adversarial jailbreaks\
  \ scale across large language models (LLMs) by simulating 6000 multi-turn attacker-target\
  \ exchanges across a range of model sizes (0.6B\u2013120B parameters). Using standardized\
  \ adversarial prompts from JailbreakBench, the researchers measured harm scores\
  \ and refusal behavior to assess the impact of relative model size on adversarial\
  \ potency."
---

# Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments

## Quick Facts
- arXiv ID: 2511.13788
- Source URL: https://arxiv.org/abs/2511.13788
- Authors: Samuel Nathanson; Rebecca Williams; Cynthia Matuszek
- Reference count: 40
- Primary result: Larger attacker models elicit higher harm scores from smaller targets, with harm scaling logarithmically with attacker-to-target size ratio

## Executive Summary
This study systematically explores how adversarial jailbreaks scale across large language models by simulating 6000 multi-turn attacker-target exchanges across model sizes ranging from 0.6B to 120B parameters. Using standardized adversarial prompts from JailbreakBench, the researchers measured harm scores and refusal behavior to assess the impact of relative model size on adversarial potency. Results show that larger attacker models more effectively elicit harmful responses from smaller targets, with harm scaling logarithmically with the attacker-to-target parameter ratio. The study reveals that attacker-side alignment and refusal behavior serve as key protective mechanisms, with attacker refusal frequency showing strong negative correlation with harm outcomes.

## Method Summary
The researchers conducted 6000 attacker-target exchanges using 11 attacker models (0.6B-120B parameters) and 3 target models across 30 adversarial prompts from JailbreakBench. Each interaction proceeded through up to 5 turns, with attackers generating structured JSON messages and targets responding to adversarial prompts. Three independent LLM judges evaluated harm scores (1-5 scale) and refusal flags, with early stopping triggered by attacker refusal detection. The study measured mean harm scores, attack success rates, and analyzed variance decomposition to understand how model size, family, and alignment affect adversarial outcomes.

## Key Results
- Strong positive correlation between mean harm and log(attacker-to-target size ratio): Pearson r = 0.51, p < 0.001
- Harm variance substantially higher across attackers (0.180) than targets (0.097)
- Attacker refusal frequency strongly negatively correlated with harm: ρ = -0.93, p < 0.001
- Attacker family and size ratio explain more variance in harm outcomes than target family or harm domain

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Capability Scaling
Larger attacker models produce higher harm scores against smaller targets because greater expressive and persuasive capacity enables more effective framing strategies (roleplay, hypotheticals, educational framing) that bypass alignment safeguards. The attacker's superior linguistic capability allows adaptive multi-turn persuasion, with harm scaling logarithmically with the attacker-to-target parameter ratio. This effect assumes parameter count serves as a proxy for persuasive capability, though it may be confounded by alignment quality variations.

### Mechanism 2: Attacker-Side Refusal as Primary Safety Control
Attacker refusal frequency functions as the dominant protective mechanism in multi-LLM adversarial settings, with strong negative correlation (ρ = -0.93) indicating that refusal suppresses harm across model families. When an attacker model's alignment triggers refusal, the adversarial exchange terminates early, preventing harmful target outputs. This protective mechanism assumes refusal behavior reflects stable alignment rather than prompt-specific fragility.

### Mechanism 3: Attacker-Dominated Harm Variance
Harm variance is substantially higher across attackers (0.180) than targets (0.097) because attacker family and size ratio explain more variance than target family or harm domain. Different attacker architectures produce heterogeneous persuasive strategies even with identical prompts, indicating attacker-side characteristics drive adversarial outcomes more than target susceptibility. This assumes target models have relatively uniform defensive postures within families while attackers vary more in their capacity to generate effective attacks.

## Foundational Learning

- **Logarithmic scaling relationships**: The harm-size ratio correlation is log-scaled (log(attacker/target)), meaning a 10x size increase produces constant harm increment. This prevents misinterpreting linear parameter differences as directly predictive. Quick check: If an attacker is 100x larger than target, how does predicted harm compare to 10x ratio?

- **Correlation vs. causation in observational LLM studies**: The study flags that parameter count is confounded with training data, alignment procedures, and architecture. Understanding this distinction prevents overclaiming mechanistic certainty. Quick check: What three factors are confounded with model scale in this study?

- **Multi-turn adversarial dialogue structure**: Attacks proceed through T_max turns with attacker adaptation (plan → message → critique). Understanding this loop is prerequisite to interpreting why refusal terminates the attack chain. Quick check: What triggers early stopping in the adversarial exchange?

## Architecture Onboarding

- **Component map**: Attacker System Prompt → M_A generates message → M_T responds → Judges score harm AND refusal detector checks attacker output → If refusal=1, terminate; else continue to T_max → Aggregate scores across judges

- **Critical path**: The attack proceeds through structured JSON generation (PLAN/MESSAGE/CRITIQUE), target response, judge evaluation of harm (1-5 scale), and binary refusal detection that triggers early stopping. The loop continues up to 5 turns unless refusal terminates the exchange.

- **Design tradeoffs**: The study uses continuous harm scoring (1-5) for nuanced partial-success detection rather than binary attack success rate, employs single-judge refusal detection for efficiency, and relies on LLM judges for scale rather than human evaluation.

- **Failure signatures**: High harm with low refusal indicates attacker model lacks alignment; low harm with high refusal suggests over-alignment blocking benign use cases; high judge variance indicates scoring prompt ambiguity; unexpected correlation breaks suggest confounds between size and alignment regime.

- **First 3 experiments**: 1) Baseline replication with 30 prompts across 3 attacker-target pairs to verify correlation; 2) Ablation on refusal detection to measure protective effect size; 3) Cross-family robustness using leave-one-family-out protocol to confirm scaling pattern persistence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do adversarial scaling patterns persist in persistent, multi-agent settings with longer interaction horizons and adaptive planning?
- Basis in paper: [explicit] Future work should extend analyses to "persistent, multi-agent settings" to test generalization beyond "short-turn interactions and static model pairings."
- Why unresolved: The study intentionally simplified interactions to a bounded, few-turn dialogue structure without persistent memory or recursive escalation loops.
- What evidence would resolve it: Testing the same scaling hypotheses in unconstrained multi-agent environments with longer time horizons.

### Open Question 2
- Question: Does model parameter count causally drive adversarial success when training data and alignment procedures are held constant?
- Basis in paper: [explicit] The Limitations section notes that scale is confounded with data quantity and alignment, necessitating "controlled ablation experiments or counterfactual fine-tuning studies."
- Why unresolved: Parameter count currently serves as a proxy for a broader capability bundle, making causal inference impossible from the observed correlations.
- What evidence would resolve it: Ablation studies that systematically vary model size while using identical training sets and alignment techniques.

### Open Question 3
- Question: How much does the diversity of alignment regimes (e.g., RLHF vs. constitutional tuning) contribute to harm variance compared to model architecture?
- Basis in paper: [explicit] The authors note that cross-family alignment procedures differ, requiring "standardized fine-tuning benchmarks or jointly aligned model suites" to isolate the source of variance.
- Why unresolved: Public models used in the study employ opaque, heterogeneous alignment reinforcement strategies.
- What evidence would resolve it: Comparative experiments using models trained with standardized alignment protocols across different architectures.

## Limitations

- **Architectural confounding**: The observed scaling relationships may conflate model size with architectural differences, training data composition, and alignment procedures, making it difficult to isolate pure capability scaling effects.
- **Judge reliability and circularity**: Harm scoring relies entirely on LLM judges, creating potential circularity where judges share vulnerabilities with the models they evaluate, and the single-judge refusal detection lacks redundancy.
- **Static prompt set**: The 30 JailbreakBench prompts may not represent the full space of adversarial strategies, potentially limiting generalization to novel attack vectors.

## Confidence

- **High confidence**: The empirical observation that harm variance is substantially higher across attackers than targets (0.180 vs 0.097) is directly measurable and robust across different model families. The negative correlation between attacker refusal and harm (ρ = -0.93) is also highly significant and mechanistically clear.
- **Medium confidence**: The logarithmic scaling relationship between harm and size ratio (Pearson r = 0.51) is statistically significant, but the mechanistic interpretation (that larger models are inherently more persuasive) is confounded by training differences. The claim about attacker-side dominance is supported but could reflect alignment quality rather than capability.
- **Low confidence**: Absolute harm score calibration across judges and domains is uncertain due to the absence of human evaluation. The specific numerical values (mean harms of 1.2-4.5) may shift significantly with different judge populations or scoring protocols.

## Next Checks

1. **Human validation of harm scoring**: Have human annotators score 100 randomly selected target responses using the same 1-5 harm rubric to compare human vs. LLM judge agreement and calibrate absolute scale.

2. **Cross-architecture scaling experiment**: Select models of identical architecture but different sizes (e.g., LLaMA variants from 7B to 70B) to isolate size effects from architectural confounding and test whether logarithmic scaling pattern persists.

3. **Novel prompt generalization test**: Generate 20 new adversarial prompts not in JailbreakBench that target different attack strategies to validate whether the attacker size ratio correlation holds across an expanded prompt space.