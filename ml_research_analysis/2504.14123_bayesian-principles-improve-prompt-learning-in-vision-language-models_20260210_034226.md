---
ver: rpa2
title: Bayesian Principles Improve Prompt Learning In Vision-Language Models
arxiv_id: '2504.14123'
source_url: https://arxiv.org/abs/2504.14123
tags:
- softmax
- function
- learning
- prompt
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses overfitting in prompt learning for vision-language\
  \ models (VLMs) by proposing a Bayesian framework that integrates P\xF3lya-Gamma\
  \ augmentation with a one-vs-each softmax approximation. This approach balances\
  \ task-specific adaptation with generalization by introducing a prior over the logits,\
  \ parameterized by the pre-trained model, and a posterior corresponding to the fine-tuned\
  \ model."
---

# Bayesian Principles Improve Prompt Learning In Vision-Language Models

## Quick Facts
- arXiv ID: 2504.14123
- Source URL: https://arxiv.org/abs/2504.14123
- Authors: Mingyu Kim; Jongwoo Ko; Mijung Park
- Reference count: 40
- Key outcome: OVE-PG improves generalization to unseen classes in prompt learning, achieving 75.04% average accuracy on unseen classes with gains up to 3.99% over baselines.

## Executive Summary
This paper addresses overfitting in prompt learning for vision-language models by introducing a Bayesian framework that combines Pólya-Gamma augmentation with a one-vs-each softmax approximation. The method introduces a Gaussian prior over the logits, parameterized by the pre-trained model, and a posterior corresponding to the fine-tuned model. By avoiding the "winner-takes-all" pressure of standard softmax and maintaining uncertainty through Bayesian principles, the approach achieves superior generalization to unseen classes across multiple datasets and prompt learning baselines.

## Method Summary
The authors propose OVE-PG (One-vs-Each with Pólya-Gamma augmentation) as a Bayesian alternative to standard softmax in prompt learning. The method establishes a Gaussian prior over the logits where the mean is the output of the frozen pre-trained model, and a posterior corresponding to the fine-tuned model. The OVE approximation decomposes the multi-class problem into pairwise binary comparisons, while Pólya-Gamma augmentation transforms the intractable sigmoid likelihood into a Gaussian form, enabling closed-form Bayesian updates. A KL-divergence term regularizes the fine-tuned posterior to stay close to the pre-trained prior, retaining global knowledge while allowing task-specific adaptation.

## Key Results
- OVE-PG consistently improves performance on unseen classes across various prompt learning baselines (CoOp, CoCoOp, MaPLe, APEX)
- Achieved an average accuracy of 75.04% on unseen classes, with gains of up to 3.99% over baselines
- Shows robustness across different hyperparameter settings and is computationally efficient
- Outperforms standard softmax and other regularization methods on base-to-new generalization tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing standard softmax with One-vs-Each (OVE) approximation reduces overfitting by decomposing the multi-class problem into pairwise binary comparisons
- **Mechanism:** OVE approximation lower-bounds softmax likelihood using product of sigmoid functions over pairwise logit differences, avoiding "winner-takes-all" pressure
- **Core assumption:** Standard softmax is prone to overfitting one-hot target information; composite likelihood of sigmoids provides robust learning signal
- **Evidence anchors:** Abstract mentions adopting OVE to avoid overfitting issues; Page 2 discusses how OVE introduces advantageous noises
- **Break condition:** May fail on datasets requiring extremely sharp decision boundaries between highly similar classes

### Mechanism 2
- **Claim:** Pólya-Gamma augmentation transforms intractable sigmoid likelihood into Gaussian form for closed-form Bayesian updates
- **Mechanism:** Introduces auxiliary Pólya-Gamma variables that, conditional on these variables, make the likelihood Gaussian, enabling Gaussian posterior over logits
- **Core assumption:** Training benefits from treating logit function as random variable with Gaussian uncertainty rather than deterministic point estimate
- **Evidence anchors:** Page 3, Eq. 2 shows Gaussian likelihood conditioned on ω; Page 3, Section 2.3 details PG augmentation
- **Break condition:** Cannot be implemented if computational budget doesn't allow for Monte Carlo sampling of auxiliary variables

### Mechanism 3
- **Claim:** Regularizing fine-tuned posterior to stay close to pre-trained prior retains global knowledge (Knowledge Distillation)
- **Mechanism:** Establishes Gaussian prior where mean is frozen pre-trained model output; includes KL-divergence term penalizing drift from pre-trained predictions
- **Core assumption:** Pre-trained model's logits on downstream task represent useful "mean" function containing universal knowledge worth preserving
- **Evidence anchors:** Page 4, Eq. 17 approximates KL divergence as squared difference between fine-tuned and pre-trained logit means; Page 1, Abstract mentions deriving prior with pre-trained model parameterization
- **Break condition:** Will hinder adaptation if downstream task is fundamentally out-of-distribution from pre-training data

## Foundational Learning

- **Concept: Contrastive Language-Image Pre-training (CLIP)**
  - **Why needed here:** Method relies on CLIP architecture where text embeddings act as linear classifiers (logits); must understand $f_c(x) = I(x)^\top T(p_c)$ represents logit for image-text pair
  - **Quick check question:** In CLIP, what mathematical operation defines the logit value for a specific image-text pair?

- **Concept: Conjugate Priors (Gaussian-Gaussian)**
  - **Why needed here:** Core theoretical leap uses Pólya-Gamma variables to force conjugacy between likelihood and Gaussian prior; understanding Gaussian prior updating to Gaussian posterior is required
  - **Quick check question:** If a prior is Gaussian and the likelihood is Gaussian, what is the form of the posterior distribution?

- **Concept: Prompt Learning (CoOp/CoCoOp)**
  - **Why needed here:** Paper modifies existing prompt learning baselines; must distinguish between frozen model weights ($I, T$) and learnable prompt parameters ($\theta$)
  - **Quick check question:** In CoOp, are the weights of the image encoder updated during training, or are the context vectors updated?

## Architecture Onboarding

- **Component map:** Frozen Encoders ($I, T$) -> Learnable Prompts ($\theta$) -> Logit Functions ($\mu, \mu_\theta$) -> OVE Matrix ($A$) -> PG Sampler -> Posterior Sampler
- **Critical path:**
  1. Compute Prior Logits ($\mu$) using frozen CLIP prompts
  2. Compute Posterior Logits ($\mu_\theta$) using learnable prompts
  3. Sample Pólya-Gamma variables ($\omega$) using Prior Logits
  4. Sample Posterior Logit Samples ($\psi_\theta$) using $\omega$ and label information ($\kappa$)
  5. Compute Loss (Negative Log Likelihood + KL Divergence) and backpropagate to update $\theta$
- **Design tradeoffs:**
  - Stability vs. Speed: Method requires sampling $M$ times per batch; higher $M$ improves approximation but linearly increases training time
  - Generalization vs. Accuracy: High regularization weight ($\beta$) ensures strong generalization to unseen classes but may sacrifice accuracy on training classes
- **Failure signatures:**
  - Training Collapse: If $\alpha$ is set too high without balancing $\beta$, gradients may vanish as model is forced to stay identical to prior
  - No Gain over Softmax: If dataset is extremely large or pre-trained model has zero relevant knowledge, overhead may not yield significant improvements
- **First 3 experiments:**
  1. Base-to-New Generalization: Train on subset of classes (e.g., 6 classes) and test on hold-out classes to verify improved generalizability
  2. Cross-Dataset Transfer: Train on ImageNet and test on distinct datasets (e.g., Pets, Cars) to verify robustness against domain shift
  3. Ablation on $\beta$: Sweep regularization weight (e.g., 0.1 to 0.7) on validation set to find "sweet spot" between retaining prior knowledge and adapting to new data

## Open Questions the Paper Calls Out
- The paper concludes with hope that the framework can be expanded to a wide range of other domains in a modal-agnostic manner, though all experimental validation is confined to vision-language models using ViT backbones on image classification datasets.

## Limitations
- Approximation Quality: OVE approximation's closeness to true softmax likelihood across different data regimes is not empirically validated
- Computational Overhead: While described as "computationally efficient," wall-clock training time against baselines is not benchmarked
- Prior Dependence: Effectiveness hinges on pre-trained model's logits being useful prior; scenarios with zero relevant knowledge are not tested
- Hyperparameter Sensitivity: Regularization weight β is tuned per-dataset and shows sensitivity, raising questions about robustness in real-world deployment

## Confidence

- **High Confidence**: Empirical observation that OVE-PG outperforms standard softmax and other regularization methods on base-to-new generalization tasks
- **Medium Confidence**: Theoretical justification for why OVE approximation reduces overfitting; mathematical argument provided but "noise introduction" claim not empirically validated
- **Medium Confidence**: Claim of computational efficiency; likely more efficient than full fine-tuning but direct time-to-convergence comparison absent
- **Low Confidence**: Generality to out-of-distribution tasks where pre-trained model's knowledge is irrelevant; this scenario not tested

## Next Checks

1. **Empirical Approximation Quality Test**: For synthetic dataset with known class separations, compute exact softmax likelihood and compare to OVE approximation across varying difficulty levels; quantify approximation error and correlation with downstream performance.

2. **Pre-trained Model Knowledge Transfer Test**: Design experiment where pre-trained model is explicitly trained on disjoint set of classes from downstream task (e.g., train CLIP on animals, test on vehicles); measure whether OVE-PG still outperforms standard softmax or actively harms performance by over-regularizing.

3. **Training Time Efficiency Benchmark**: Implement controlled experiment where OVE-PG and strong baseline (e.g., CoOp with standard softmax and label smoothing) are trained for fixed number of epochs; measure final validation accuracy and total wall-clock time to assess practical trade-off between accuracy and efficiency.