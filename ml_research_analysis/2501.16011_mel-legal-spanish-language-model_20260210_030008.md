---
ver: rpa2
title: 'MEL: Legal Spanish Language Model'
arxiv_id: '2501.16011'
source_url: https://arxiv.org/abs/2501.16011
tags:
- legal
- spanish
- language
- texts
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors developed MEL, a Spanish legal language model based
  on XLM-RoBERTa-large, pre-trained on Spanish legal documents including BOE and congressional
  records. MEL outperforms existing multilingual and domain-specific models on Spanish
  legal text classification tasks.
---

# MEL: Legal Spanish Language Model

## Quick Facts
- arXiv ID: 2501.16011
- Source URL: https://arxiv.org/abs/2501.16011
- Reference count: 23
- MEL outperforms existing models on Spanish legal text classification with F1 scores of 0.8025 on Eurlex and 0.9260 on private corpus

## Executive Summary
MEL is a Spanish legal language model built on XLM-RoBERTa-large, pre-trained on Spanish legal documents including BOE and congressional records. The model demonstrates superior performance on Spanish legal text classification tasks compared to existing multilingual and domain-specific models. MEL shows particular strength in handling limited training data scenarios while maintaining high accuracy on both public and private legal datasets.

## Method Summary
The authors developed MEL by pre-training XLM-RoBERTa-large on Spanish legal corpora, specifically incorporating documents from the BOE (Official Gazette of the Spanish Government) and congressional records. This domain-specific approach targeted Spanish legal language patterns and terminology. The model was then evaluated on both a public multilingual dataset (Eurlex-50k) and a private Spanish legal corpus using standard classification metrics, with particular attention to performance with limited training data.

## Key Results
- On Eurlex-50k dataset, MEL achieved F1 score of 0.8025, outperforming Legal-XLM-RoBERTa-Large (0.7933), XLM-RoBERTa-Large (0.7962), and RoBERTalex (0.7890)
- On private multiclass legal corpus, MEL achieved F1 score of 0.9260, surpassing all baseline models
- With limited training data, MEL achieved F1 of 0.8812 versus 0.7803 for Legal-XLM-RoBERTa-Large, demonstrating superior data efficiency

## Why This Works (Mechanism)
Domain-specific pre-training on Spanish legal documents enables MEL to capture specialized legal terminology, syntax, and contextual patterns unique to Spanish legal texts. By incorporating BOE and congressional records during pre-training, the model develops deeper understanding of legal document structures and formal language conventions specific to Spanish jurisdiction. This specialized training allows MEL to outperform general multilingual models and even other legal domain models that may not be optimized for Spanish legal language nuances.

## Foundational Learning
1. **Spanish Legal Terminology** - Understanding specialized legal vocabulary and phrases used in Spanish jurisdiction
   - Why needed: Enables accurate interpretation of legal concepts and document types
   - Quick check: Model correctly identifies legal document categories and extracts key legal concepts

2. **Domain Adaptation Techniques** - Methods for fine-tuning general language models on specialized corpora
   - Why needed: Transfers general language understanding to specific legal domain
   - Quick check: Performance improvement on legal tasks compared to base model

3. **Multilingual Pre-training** - XLM-RoBERTa's ability to handle multiple languages through shared representations
   - Why needed: Provides foundation for Spanish legal understanding while maintaining cross-lingual capabilities
   - Quick check: Model maintains performance on non-Spanish legal texts while excelling in Spanish

## Architecture Onboarding

**Component Map**: XLM-RoBERTa-large -> Spanish Legal Pre-training -> Task-Specific Fine-tuning -> Classification Output

**Critical Path**: Pre-training corpus preparation -> XLM-RoBERTa base model loading -> Legal domain pre-training -> Task-specific fine-tuning -> Evaluation

**Design Tradeoffs**: Domain-specific pre-training improves legal task performance but may reduce general language versatility; Spanish focus enables superior performance on Spanish legal texts but limits cross-jurisdictional applicability

**Failure Signatures**: Poor performance on non-Spanish legal texts; potential overfitting to specific document types present in training corpus; reduced effectiveness on legal subdomains not represented in pre-training data

**First Experiments**:
1. Evaluate MEL on a different Spanish legal dataset to verify performance consistency
2. Test MEL's performance on legal texts from other Spanish-speaking countries to assess cross-jurisdictional capabilities
3. Compare MEL with and without congressional records component to isolate contribution of different data sources

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to one public dataset (Eurlex-50k) and one private Spanish legal corpus, restricting generalizability across different legal subdomains
- Private corpus composition, size, and distribution details are not transparent, raising concerns about potential evaluation bias
- Limited training data performance results lack specific details about sample sizes and conditions, making practical implications unclear

## Confidence
- **High confidence**: MEL's superior performance on publicly available Eurlex-50k dataset compared to general multilingual models
- **Medium confidence**: MEL's performance advantages on private legal corpus and with limited training data, pending additional details about data composition
- **Medium confidence**: Domain-specific pre-training benefits, though improvement extent may vary across different legal subdomains

## Next Checks
1. Conduct evaluation on additional publicly available Spanish legal datasets to verify performance consistency across different legal domains
2. Perform ablation studies comparing MEL with and without congressional records component to isolate contribution of different data sources
3. Test MEL's robustness on out-of-domain legal texts from different Spanish-speaking countries to assess cross-jurisdictional applicability