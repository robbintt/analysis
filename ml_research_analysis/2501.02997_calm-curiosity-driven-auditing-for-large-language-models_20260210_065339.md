---
ver: rpa2
title: 'CALM: Curiosity-Driven Auditing for Large Language Models'
arxiv_id: '2501.02997'
source_url: https://arxiv.org/abs/2501.02997
tags:
- auditing
- audit
- target
- intrinsic
- calm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of auditing black-box large language
  models (LLMs) without access to their internal parameters, focusing on uncovering
  harmful or biased input-output pairs. To tackle this, the authors propose CALM (Curiosity-Driven
  Auditing for Large Language Models), which uses intrinsically motivated reinforcement
  learning to fine-tune an LLM as an auditor agent.
---

# CALM: Curiosity-Driven Auditing for Large Language Models

## Quick Facts
- arXiv ID: 2501.02997
- Source URL: https://arxiv.org/abs/2501.02997
- Reference count: 8
- Primary result: Curiosity-driven RL fine-tuning of GPT-2 discovers harmful input-output pairs in black-box LLMs with >80% accuracy using ~15,000 queries

## Executive Summary
CALM addresses the challenge of auditing black-box large language models for harmful behaviors without internal access. The method uses intrinsically motivated reinforcement learning to fine-tune an audit LLM (GPT-2) that generates prompts to probe target LLMs. The key innovation is a token-level intrinsic bonus based on policy cover theory that drives exploration of novel regions in the token embedding space before any extrinsic reward is received. Experiments on inverse suffix generation and toxic completion tasks demonstrate CALM's effectiveness in uncovering problematic behaviors with consistent improvements over baseline methods.

## Method Summary
CALM employs intrinsically motivated reinforcement learning to fine-tune GPT-2 as an auditor agent. The audit LLM generates candidate prompts autoregressively, receiving both extrinsic rewards (task success like triggering toxic output) and intrinsic bonuses (novelty in token embedding space). The intrinsic bonus uses prediction error against fixed random networks as a proxy for state visitation frequency, encouraging exploration of under-visited regions. Proximal Policy Optimization (PPO) optimizes a composite objective balancing auditing performance, exploration, and stability through KL divergence regularization. The method operates without target LLM gradients, making it suitable for black-box auditing scenarios.

## Key Results
- Achieved >80% auditing objective accuracy on Llama-3-8B with approximately 15,000 queries
- Consistently outperformed baseline methods including sentence-level exploration approaches
- Demonstrated effective exploration of novel prompt regions before receiving extrinsic rewards
- Showed ablation results confirming token-level intrinsic bonus effectiveness over sentence-level alternatives

## Why This Works (Mechanism)

### Mechanism 1: Intrinsic Motivation Enables Exploration Before Exploitation
The token-level intrinsic bonus drives the audit LLM to explore novel regions of the prompt space before receiving any extrinsic reward, preventing premature convergence to repetitive, meaningless prompts. At each generation step, CALM computes a novelty score for each token based on its sparsity in the embedding space. Tokens in under-explored regions receive higher intrinsic rewards, which encourages the audit LLM to generate diverse prompts that probe unseen behaviors of the target LLM. This works because harmful input-output pairs are rare and scattered across the prompt space; random or greedy sampling will miss them without directed exploration.

### Mechanism 2: Policy Cover-Based Token-Level Intrinsic Bonus Approximates State Visit Frequency
CALM approximates the inverse of the policy cover—how often token embeddings have been visited—using prediction error against fixed random networks, yielding a tractable intrinsic bonus. For token embedding h, the intrinsic bonus is R̂I(s) = ||ψ₁(h) - g₁(h)|| · ||ψ₂(h) - g₂(h)||, where ψ₁, ψ₂ are trained encoders and g₁, g₂ are fixed random networks. Prediction error is high for novel embeddings, yielding high bonus. ψ₂ is reinitialized each step to prevent overfitting. This works because prediction error on random networks is a valid proxy for state visitation counts in high-dimensional embedding spaces.

### Mechanism 3: Regularized Objective Balances Auditing, Exploration, and Stability
The composite objective—extrinsic auditing reward + λI · intrinsic reward - λKL · KL divergence—enables effective discovery while preventing the audit LLM from drifting too far from its pretrained capabilities. The extrinsic reward r(s,o) captures task success (e.g., toxic output triggered). The intrinsic reward encourages exploration. The KL penalty anchors the policy to the reference model. PPO optimizes this tradeoff. This works because the KL coefficient λKL can be tuned to allow meaningful adaptation without catastrophic forgetting or mode collapse.

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: CALM uses PPO as its RL backbone to fine-tune the audit LLM. Understanding clipping, advantage estimation, and policy ratio constraints is essential to debug training instability.
  - Quick check question: Can you explain why PPO uses a clipped objective and how the KL penalty in CALM differs from PPO's standard trust-region behavior?

- Concept: Intrinsic Motivation / Curiosity in RL
  - Why needed here: The core innovation is a curiosity-driven bonus. You must understand why exploration bonuses help in sparse-reward settings and how prediction error proxies novelty.
  - Quick check question: If you observe the intrinsic bonus collapsing to near-zero after 5000 steps, what might be happening and how would you diagnose it?

- Concept: Black-Box Optimization for Discrete Prompt Spaces
  - Why needed here: CALM operates without target LLM gradients. Understanding the challenges of discrete, high-dimensional search spaces clarifies why intrinsic exploration is critical.
  - Quick check question: Why is zero-order gradient estimation impractical for LLM-scale prompt spaces, and how does RL-based prompt generation sidestep this?

## Architecture Onboarding

- Component map: Audit LLM (GPT-2) -> Target LLM (black-box API) -> Extrinsic Reward Function -> Intrinsic Bonus Module -> PPO Optimizer -> KL Divergence Monitor

- Critical path:
  1. Sample initial prompt z for the audit task
  2. Audit LLM generates token sequence s_T autoregressively; at each step, compute intrinsic bonus for sampled token
  3. Submit s_T to target LLM; collect output o
  4. Compute extrinsic reward r(s,o) and aggregate intrinsic bonus across tokens
  5. Compute advantage via GAE; update policy and value function via PPO
  6. Reinitialize ψ₂ before next iteration; log KL divergence

- Design tradeoffs:
  - Audit LLM size: GPT-2 is lightweight but may have limited prompt diversity; larger auditors (e.g., Llama-7B) could improve coverage at higher compute cost
  - Token-level vs. sentence-level intrinsic bonus: Token-level enables finer-grained exploration but requires per-step computation; sentence-level (as in CRT baseline) is cheaper but less effective per ablation
  - Intrinsic coefficient λI: High values increase exploration but may delay convergence to high-reward regions; ablation shows λI=100 yields gradual improvement

- Failure signatures:
  - Repetitive prompts with near-zero intrinsic bonus: Encoder overfits to random network; consider reinitializing both ψ₁ and ψ₂ or increasing encoder capacity
  - High variance in auditing objective across runs: Intrinsic exploration insufficient; increase λI or reduce KL penalty
  - Incoherent audit prompts: KL penalty too low; increase λKL or reduce learning rate

- First 3 experiments:
  1. Replicate inverse suffix generation on Llama-3-8B with λI=10: Verify convergence curve and L0 coverage match Figure 1/2; log intrinsic bonus over time
  2. Ablate intrinsic bonus (λI=0 vs. λI=10 vs. λI=100): Compare auditing objective growth, variance, and prompt diversity; confirm token-level bonus outperforms sentence-level baseline
  3. Stress-test KL penalty: Run with λKL ∈ {0.01, 0.1, 0.5}; identify the point where prompts remain coherent yet exploration is not stifled; correlate with KL divergence logs

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of auditor backbone architecture (beyond GPT-2) affect CALM's ability to uncover harmful behaviors in target LLMs? The paper states they adopted lightweight GPT-2 as the audit LLM backbone and believe a more powerful auditor backbone will enhance CALM's performance, but only tested GPT-2 with frozen early layers and fine-tuned last two transformer blocks.

### Open Question 2
How does CALM compare to non-RL black-box optimization methods such as evolutionary algorithms for LLM auditing? The paper explicitly states in Appendix C that a comprehensive comparison between RL and evolutionary algorithms will be addressed in future work, having only compared to RL and CRT baselines.

### Open Question 3
Can CALM effectively audit for more complex undesirable behaviors beyond toxic completion and inverse suffix generation? The paper only validates two specific auditing objectives and claims generality but lacks empirical demonstration across diverse harm categories like stereotypes, discrimination, privacy leakage, or hallucination.

### Open Question 4
How robust is CALM's performance across different intrinsic bonus coefficient (λ) settings? The paper only tested two values (λ=10, λ=100) and shows λ affects exploration-exploitation balance but provides no systematic hyperparameter sweep or theoretical guidance for λ selection across different target models or tasks.

## Limitations
- Empirical evaluation relies on synthetic auditing tasks rather than real-world harmful content discovery
- Intrinsic bonus mechanism's reliance on random network prediction error lacks external validation
- Audit LLM's limited capacity (GPT-2, last two layers) may constrain prompt diversity and quality
- Performance gap between controlled benchmarks and real-world auditing scenarios remains unclear

## Confidence
**High Confidence**: The mechanism of using intrinsic motivation to drive exploration in sparse-reward settings is well-established in RL literature. The technical implementation of PPO-based fine-tuning with KL regularization follows standard practices. The correlation between intrinsic coefficient λI and auditing performance in ablation studies is clearly demonstrated.

**Medium Confidence**: The policy cover theory justification for the intrinsic bonus design is internally consistent but lacks external validation. The effectiveness of token-level exploration over sentence-level methods is supported by ablation but could benefit from broader comparison. The claim that 15,000 queries achieve >80% accuracy on Llama-3-8B is specific but based on limited task diversity.

**Low Confidence**: Generalization to real-world harmful content discovery scenarios remains untested. The sensitivity to hyperparameter choices (particularly λI and λKL) across different target LLMs and task domains is not thoroughly explored. The computational overhead of the intrinsic bonus module relative to its benefit in practical auditing scenarios is not quantified.

## Next Checks
1. **Cross-Domain Transferability Test**: Apply CALM to a third auditing task (e.g., jailbreak detection or privacy leak discovery) using the same audit LLM and compare performance to the two tasks presented. This would validate whether the method generalizes beyond the specific prompt structures used in the paper.

2. **Intrinsic Bonus Ablation on Random Networks**: Replace the random network prediction error with an alternative novelty metric (e.g., cosine distance in embedding space or token frequency counting) and measure the impact on auditing performance. This would test whether the specific proxy choice is critical or if simpler alternatives suffice.

3. **Target LLM Scale Sensitivity Analysis**: Evaluate CALM's performance on a smaller model (e.g., GPT-2) and a larger model (e.g., Llama-2-13B or Claude) using identical hyperparameters. This would reveal whether the 15,000-query efficiency claim holds across model scales or requires recalibration.