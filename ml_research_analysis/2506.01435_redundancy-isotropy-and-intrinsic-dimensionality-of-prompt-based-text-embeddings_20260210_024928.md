---
ver: rpa2
title: Redundancy, Isotropy, and Intrinsic Dimensionality of Prompt-based Text Embeddings
arxiv_id: '2506.01435'
source_url: https://arxiv.org/abs/2506.01435
tags:
- embeddings
- text
- tasks
- retrieval
- dimensionality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the redundancy of prompt-based text embeddings
  by applying post-hoc dimensionality reduction. The authors evaluate how reducing
  the number of dimensions affects performance across classification, clustering,
  retrieval, and semantic textual similarity (STS) tasks.
---

# Redundancy, Isotropy, and Intrinsic Dimensionality of Prompt-based Text Embeddings

## Quick Facts
- **arXiv ID**: 2506.01435
- **Source URL**: https://arxiv.org/abs/2506.01435
- **Reference count**: 40
- **Primary result**: Prompt-based text embeddings are highly redundant—reducing dimensions to <0.5% of original preserves most performance for classification and clustering tasks.

## Executive Summary
This paper investigates the redundancy of prompt-based text embeddings by applying post-hoc dimensionality reduction across classification, clustering, retrieval, and semantic textual similarity tasks. The authors find that even naive dimensionality reduction—keeping only the first 25% of dimensions—causes minimal performance degradation, indicating high redundancy in these embeddings. Remarkably, for classification and clustering tasks, reducing dimensions to less than 0.5% of the original still preserves most performance. The study also shows that reduced embeddings can outperform same-dimensional embeddings from smaller models. To understand this phenomenon, the authors analyze intrinsic dimensionality and isotropy of the embeddings, finding that embeddings for classification and clustering tasks have lower intrinsic dimensionality and less isotropic distributions, correlating with higher redundancy and robustness to reduction.

## Method Summary
The authors evaluate redundancy by applying post-hoc dimensionality reduction to embeddings from large prompt-based models (7.6B parameters, 3584 dimensions) and comparing performance to smaller models (33M-560M parameters). They test across 21 tasks from the Massive Text Embedding Benchmark (5 classification, 3 clustering, 6 retrieval, 7 STS). For each model, they reduce embeddings by taking the first d dimensions without normalization, evaluating at specific dimensionalities from 2 to full size. Classification uses logistic regression with default sklearn parameters. Intrinsic dimensionality is measured using TwoNN algorithm on 10,000 randomly sampled Wikipedia texts, and isotropy is assessed using IsoScore. The analysis examines how these properties correlate with redundancy across different task types.

## Key Results
- Taking the first 25% of dimensions causes minimal performance degradation across most tasks, indicating high redundancy in prompt-based embeddings
- For classification and clustering tasks, reducing dimensions to <0.5% of original preserves most performance
- Reduced embeddings from large models can outperform same-dimensional embeddings from smaller models
- Embeddings for classification and clustering tasks have lower intrinsic dimensionality and less isotropic distributions, correlating with higher redundancy

## Why This Works (Mechanism)
The high redundancy observed in prompt-based text embeddings stems from their generation process and geometric properties. When models process text with prompts, they may encode information in a distributed manner where different dimensions capture overlapping or redundant information. The lower intrinsic dimensionality for classification and clustering tasks suggests these tasks can be solved in lower-dimensional subspaces, making them more amenable to aggressive reduction. The less isotropic distributions (IsoScore values further from 0) indicate that the embedding space is structured in ways that preserve task-relevant information even when most dimensions are removed. This combination of distributed encoding and structured geometry allows prompt-based embeddings to maintain performance despite severe dimensionality reduction.

## Foundational Learning
- **Dimensionality reduction**: The process of reducing the number of random variables under consideration, by obtaining a set of principal variables. Needed to test redundancy; quick check: verify reduction method preserves task-relevant information.
- **Intrinsic dimensionality**: The minimum number of parameters needed to represent a dataset without information loss. Needed to understand why some embeddings are more redundant; quick check: TwoNN algorithm implementation.
- **Isotropy**: Property where data points are uniformly distributed in all directions. Needed to characterize embedding geometry; quick check: IsoScore values near 0 indicate high isotropy.
- **Prompt-based embeddings**: Text embeddings generated by providing instructions/prompts to language models. Needed as the target of analysis; quick check: ensure correct prompt format for each task.
- **TwoNN algorithm**: A method for estimating intrinsic dimensionality by analyzing nearest neighbor distances. Needed for ID computation; quick check: sample size > embedding dimension for stability.

## Architecture Onboarding

**Component Map**
gte-Qwen2/SFR-2/E5-mistral -> Dimensionality Reduction -> Task Evaluation -> ID/IsoScore Analysis

**Critical Path**
1. Load large prompt-based model and generate full-dimensional embeddings
2. Apply dimensionality reduction (take first d dimensions)
3. Evaluate on task-specific metrics
4. Analyze intrinsic dimensionality and isotropy
5. Correlate geometric properties with redundancy patterns

**Design Tradeoffs**
- Naive vs. sophisticated dimensionality reduction: Authors chose simple truncation for interpretability
- Task-specific vs. generic prompts: Used task-specific prompts to maximize performance
- Sample size for ID analysis: 10,000 Wikipedia texts balances computational cost and stability

**Failure Signatures**
- Sharp performance drops at low dimensions suggest task requires full embedding space
- Unstable ID/IsoScore estimates indicate insufficient sampling or high dimensionality
- OOM errors with 7B models suggest need for quantization or smaller batches

**3 First Experiments**
1. Reduce gte-Qwen2 embeddings to 64 dimensions and evaluate on classification tasks
2. Compute TwoNN intrinsic dimensionality on 1,000 Wikipedia samples to verify algorithm implementation
3. Calculate IsoScore for retrieval task embeddings to check isotropy patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Random sampling of Wikipedia texts lacks specified seed, affecting ID/IsoScore estimates
- Complete prompt set for instruction-based models not fully enumerated, potentially affecting consistency
- Analysis focuses exclusively on English Wikipedia, limiting generalizability to other languages

## Confidence

**High Confidence Claims:**
- Taking first 25% of dimensions causes minimal performance degradation
- Classification and clustering tasks show higher redundancy to dimensionality reduction

**Medium Confidence Claims:**
- Correlation between lower intrinsic dimensionality/less isotropic distributions and higher redundancy
- Reduced embeddings outperforming same-dimensional smaller models depends on exact prompt implementation

## Next Checks

1. Verify prompt completeness by cross-referencing Table 3 prompts with cited "previous studies" for instruction-based models

2. Run ID and IsoScore analysis with multiple random seeds for Wikipedia sampling to assess stability of intrinsic dimensionality estimates

3. Test whether taking first d dimensions before or after prompt-based inference affects observed redundancy patterns for instruction-tuned models