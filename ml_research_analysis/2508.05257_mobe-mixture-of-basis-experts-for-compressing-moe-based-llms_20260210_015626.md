---
ver: rpa2
title: 'MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs'
arxiv_id: '2508.05257'
source_url: https://arxiv.org/abs/2508.05257
tags:
- mobe
- matrices
- rank
- expert
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs
  Large Mixture-of-Experts (MoE) models like DeepSeek-V3 and Kimi-K2 suffer from prohibitive
  memory requirements during deployment. While existing MoE compression methods achieve
  parameter reduction, they incur significant accuracy drops (7-14% relative) even
  at modest compression rates.'
---

# MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs

## Quick Facts
- arXiv ID: 2508.05257
- Source URL: https://arxiv.org/abs/2508.05257
- Reference count: 40
- Key outcome: MoBE achieves 24-30% parameter reduction with only 1-2% absolute accuracy drop on MoE LLMs

## Executive Summary
MoBE introduces a novel compression method for Mixture-of-Experts (MoE) models that factorizes expert weight matrices using shared basis matrices across all experts within each layer. Unlike existing methods that achieve parameter reduction at the cost of significant accuracy drops (7-14%), MoBE maintains 98% of original performance while reducing parameters by 24-30%. The method learns a factorization W = AB where B is re-parameterized as a linear combination of basis matrices shared across experts, achieving consistently lower reconstruction error compared to prior methods. Experiments on diverse MoE models including DeepSeek-V3 and Kimi-K2 demonstrate the effectiveness of this approach for enabling more efficient deployment of trillion-parameter models.

## Method Summary
MoBE compresses MoE models by factorizing each expert's up/gate matrix W_i = A_i B_i, where A_i is expert-specific and B_i is a linear combination of m shared basis matrices across all n experts in a layer (m ≪ n). The method uses Z-score normalization on original weights, then optimizes the factorization with Adam (lr=0.07) to minimize reconstruction MSE. Down matrices are preserved unchanged based on prior work showing they store critical knowledge. The approach applies SiLU or Tanh activation to the basis combination for enhanced representational capacity. For very large models like Kimi-K2 with 384 experts, experts are grouped (e.g., 2 groups of 64) to manage basis matrix counts. The optimization is data-free, requiring only the original pretrained weights.

## Key Results
- MoBE achieves 24-30% parameter reduction across diverse MoE models (Ling-Lite-Chat, DeepSeek-V2-Lite-Chat, Qwen3-30B, DeepSeek-V3-0324, Qwen3-235B, Kimi-K2-Instruct)
- Reconstruction error is consistently 50% lower than MoLAE and D²-MoE baselines across all tested models
- Performance retention reaches ~98% of original (only 1-2% absolute accuracy drop, ~2% relative)
- Particularly effective for large-scale models, enabling deployment of trillion-parameter MoE architectures
- MoBE† variant reduces activated experts (8→6) to compensate for activation parameter increase, maintaining similar compression rates

## Why This Works (Mechanism)

### Mechanism 1: Shared Basis Factorization with Expert-Specific Transformations
Decomposing expert weight matrices into shared basis matrices plus expert-specific transformations achieves lower reconstruction error than pure SVD-based approaches. Each up/gate matrix W_i is factorized as W_i = A_i B_i, where B_i = Σ_j α_i,j B_j. The basis matrices {B_j} capture common information across all n experts in a layer (with m ≪ n basis matrices), while transformation matrices A_i encode expert-specific information. This exploits redundancy across experts without forcing a single shared representation.

### Mechanism 2: Gradient-Based Reconstruction Error Minimization
Learning the factorization through gradient descent on reconstruction error preserves more information than analytical decomposition methods like SVD. The optimization problem min_{A,B,α} Σ_i ||W_i - Â_i||² is solved using Adam optimizer. This allows joint optimization of all components (basis matrices, transformation matrices, and combination coefficients) rather than sequential decomposition that propagates errors.

### Mechanism 3: Non-Linear Activation on Basis Combinations
Applying bipolar activation functions (SiLU, Tanh) to the linear combination of basis matrices enhances representational capacity compared to purely linear combinations. The reconstruction becomes Â_i = A_i · f(Σ_j α_i,j B_j) where f is SiLU or Tanh. This non-linearity allows the basis combination to capture more complex transformations. ReLU is unsuitable because it induces sparsity in B_i that the smaller A_i cannot compensate for.

## Foundational Learning

- **Concept: Rank Decomposition / Low-Rank Factorization**
  - Why needed here: MoBE's core operation is W = AB factorization. Understanding how rank constrains representational capacity is essential for setting the intermediate dimension r and interpreting compression-quality tradeoffs.
  - Quick check question: If W ∈ R^(p×d) with p=4096, d=1536, and you set rank r=1024, what are the dimensions of A and B? What compression ratio does this yield for a single matrix?

- **Concept: Mixture-of-Experts Architecture (SwiGLU variant)**
  - Why needed here: MoBE specifically targets up/gate projections in SwiGLU experts while preserving down projections. Understanding why down matrices are "less amenable to compression" (per prior work) informs where to apply factorization.
  - Quick check question: In E_i(x) = W_down · (W_up x ⊙ SiLU(W_gate x)), which matrices does MoBE factorize and which does it preserve? Why might down matrices store "critical knowledge"?

- **Concept: SVD and Effective Rank**
  - Why needed here: Baseline methods (MoLAE, D²-MoE) use SVD. Appendix B shows effective rank analysis explaining why pure SVD compression fails—expert matrices have higher effective rank than compression thresholds allow.
  - Quick check question: If a matrix has effective rank r_e = 1800 (for 95% variance) but compression requires retaining only k=1000 singular values, what happens to reconstruction quality? How does MoBE potentially avoid this limitation?

## Architecture Onboarding

- **Component map:** Original MoE layer: Router + n experts → MoBE layer: Router (unchanged) + n experts with A_i^gate, A_i^up (expert-specific), {B_j^gate}, {B_j^up} (m shared basis per layer), α_i,j (combination coefficients), W_down (copied from original)

- **Critical path:** 1) Load pretrained MoE model 2) Extract {W_gate^i, W_up^i} for all n experts per layer 3) Apply Z-score normalization: W_i^Z = (W_i - μ_W) / σ_W 4) Initialize {A_i}, {B_j}, {α_i,j} 5) Optimize Eq(5) with Adam (lr=0.07) to minimize reconstruction MSE 6) Fold σ_W into A_i; μ_W is negligible 7) Assemble MoBE layer with factorized up/gate + original down matrices

- **Design tradeoffs:** m (number of basis matrices): Lower m → more compression but higher MSE. Paper uses m=4 for small models, up to m=128 for Kimi-K2 (with expert grouping). r (intermediate rank): Paper sets r=p for simplicity. Lower r → more compression but potential accuracy drop. k' (activated experts in MoBE†): Reducing from 8→6 compensates for ~33% activation parameter increase. Down matrix compression: Deliberately avoided based on prior work showing these store critical knowledge.

- **Failure signatures:** High reconstruction MSE: Check Z-score normalization. Optimization instability: If using Alternating Optimization, switch to Adam. Large accuracy drops on specific tasks: Check if down matrices were accidentally compressed. Inference slower than expected: MoBE requires multiple fused-MoE kernel calls; custom mega-kernel needed for production.

- **First 3 experiments:** 1) Validation on small model: Apply MoBE to Ling-Lite-Chat with m=4 basis matrices. Compare per-layer MSE against MoLAE (should see ~50% reduction). Verify <2% accuracy drop on 2-3 benchmarks. 2) Ablation on activation function: Using Qwen3-30B-A3B gate matrices, compare {none, tanh, silu, gelu, sigmoid, relu}. Confirm SiLU/Tanh achieve lowest MSE. 3) Scaling to target model: Apply validated configuration to your target model. Start with m = n/8 to n/4. Monitor both MSE and downstream accuracy.

## Open Questions the Paper Calls Out

- Can knowledge distillation between the original MoE model and the MoBE-compressed model recover the 1–2% accuracy gap without reintroducing significant memory overhead?
- Can a specialized mega-kernel implementation for MoBE's factorized operations match or exceed the inference efficiency of existing fused-MoE kernels?
- What is the optimal relationship between the intermediate rank r (in W=AB) and the compression-accuracy trade-off, and can adaptive per-layer rank selection further improve MoBE?
- Does the expert activation balance during pre-training correlate with the compressibility and reconstruction error of MoE models under MoBE?

## Limitations

- The custom fused-MoE kernel requirement for inference efficiency represents a significant deployment barrier not addressed in the paper
- Expert grouping strategy for very large models (Kimi-K2) is vaguely described without explaining optimal partitioning methods
- No ablation studies on the rank parameter r, leaving uncertainty about potential additional compression gains
- Optimization convergence criteria and iteration counts are not specified, making training stability assessment difficult

## Confidence

- **High Confidence:** MoBE achieves lower reconstruction MSE than MoLAE and D²-MoE (Figures 2-4 show consistent ~50% reduction); parameter reduction of 24-30% is well-documented (Table 1)
- **Medium Confidence:** The 98% performance retention claim (1-2% absolute accuracy drop) is supported by experiments but relies on specific hyperparameter choices (m values, r=p) that may not generalize optimally
- **Low Confidence:** The claim that MoBE is "particularly effective for large-scale MoE models" lacks direct ablation studies comparing effectiveness across model scales, and the Kimi-K2 expert grouping strategy remains underspecified

## Next Checks

1. **Convergence analysis:** Run optimization for 50, 100, and 200 epochs on DeepSeek-V2-Lite-Chat and plot MSE convergence curves to identify optimal stopping points and training stability

2. **Rank sensitivity study:** For Qwen3-30B-A3B, test r ∈ {512, 1024, 2048} while varying m to map the MSE-accuracy trade-off surface and identify optimal compression configurations

3. **Downstream task sensitivity:** Evaluate MoBE compression on reasoning-intensive tasks (GSM8k, HumanEval) separately from knowledge tasks to identify if certain capability domains are more vulnerable to compression artifacts