---
ver: rpa2
title: Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response
  Reuse
arxiv_id: '2509.25808'
source_url: https://arxiv.org/abs/2509.25808
tags:
- responses
- response
- arxiv
- training
- ar3po
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vanishing advantage problem in Group Relative
  Policy Optimization (GRPO), a reinforcement learning from verifiable rewards (RLVR)
  algorithm for large language models. The issue arises when all responses in a group
  are either correct or incorrect, causing advantages to collapse to zero and halting
  training.
---

# Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse

## Quick Facts
- **arXiv ID**: 2509.25808
- **Source URL**: https://arxiv.org/abs/2509.25808
- **Reference count**: 7
- **Primary result**: AR3PO reduces rollout cost by up to 4.2× while maintaining or improving performance vs GRPO and DAPO on mathematical reasoning benchmarks

## Executive Summary
This paper addresses the vanishing advantage problem in GRPO, where identical rewards across response groups collapse training signals to zero. AR3PO introduces adaptive rollout (dynamically allocating more responses to difficult prompts) and response reuse (leveraging previously correct responses) to maintain training signals while reducing computation. Experiments on 7B, 8B, and 32B models show AR3PO achieves comparable or better performance than DAPO while using 4.2× fewer responses on average.

## Method Summary
AR3PO combines adaptive rollout with response reuse to address vanishing advantages in GRPO. Adaptive rollout uses S=2 stages with k=4 responses each, removing prompts that receive correct responses after each stage. Prompts without correct responses receive additional sampling. A replay buffer stores correct responses from all prior steps. For prompts still without correct responses, AR3PO samples from the buffer and stops gradients on reused responses (Option II), using them only for advantage computation while updating the policy using on-policy samples. The method reduces the zero-reward prompt ratio from ~30% to <20% while cutting average responses per prompt from 8 to ~5.7.

## Key Results
- AR3PO reduces average responses per prompt from 8 (GRPO) to 5.7 while maintaining comparable accuracy
- On 32B models, AR3PO matches DAPO's accuracy (53.2% vs 52.8% on Olympiad Bench) with 4.2× fewer responses
- AR3PO reduces prompts without any correct response from ~30% to <20% through response reuse
- Across 7B, 8B, and 32B models, AR3PO consistently outperforms GRPO and matches DAPO on multiple mathematical benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Allocating more rollouts to difficult prompts while reducing computation on easy prompts improves sampling efficiency without sacrificing training signal quality.
- Mechanism: Multi-stage adaptive rollout partitions response generation into S stages. At each stage, prompts with ≥1 correct response exit the pool; remaining prompts proceed to additional sampling. This implicitly weights difficult prompts higher in optimization since they receive more training samples.
- Core assumption: Prompt difficulty is correlated with the number of rollouts needed to obtain at least one correct response, and this dynamic allocation better matches computational investment to learning value.
- Evidence anchors:
  - [abstract] "adaptive rollout, which dynamically allocates more responses to difficult prompts while saving computation on easier ones"
  - [section 4.4, Figure 2d] "The most difficult prompts with a success rate of 0.0-0.2 receive the largest allocation, averaging 6.95 responses, whereas the easiest prompts... use only about 4 responses per step"
  - [corpus] Related work "Adaptive Rollout Allocation for Online RLVR" (arXiv:2602.01601) similarly addresses uniform allocation inefficiency, suggesting this is a recognized problem class
- Break condition: If prompts are mislabeled by difficulty or if success rate is a poor proxy for learning value, allocation may worsen efficiency. Also breaks if curriculum pacing requires easy prompts for stability.

### Mechanism 2
- Claim: Reusing previously generated correct responses provides training signals for prompts where current sampling yields no correct responses, reducing the "zero-reward prompt" ratio.
- Mechanism: Maintain a replay buffer B of correct responses. For prompts with all-incorrect groups after adaptive rollout, replace one incorrect response with a sampled correct response from B. Recompute advantages using Eq. 1 with the mixed group.
- Core assumption: The advantage signal from mixing an off-policy correct response with on-policy incorrect responses provides useful gradient direction (negative advantages on incorrect responses discourage wrong exploration).
- Evidence anchors:
  - [abstract] "response reuse, which leverages previously generated correct responses to provide useful training signals"
  - [section 4.4, Figure 2c] "AR3PO reduces the ratio [of prompts without any correct response] from about 0.3 to below 0.2"
  - [corpus] Concurrent works (Sun et al., 2025; Zhang et al., 2025a) apply rollout replay; this paper distinguishes itself via importance ratio handling
- Break condition: If the policy has shifted substantially, reused correct responses may not reflect reachable states, providing misleading advantages. Also breaks if the replay buffer is poisoned by verification errors.

### Mechanism 3
- Claim: Stopping gradients on reused responses while using them for advantage computation reduces variance from stale importance ratios.
- Mechanism: For reused responses, either (Option I) recompute token probabilities under current policy πθ to reduce importance ratio variance (trading bias for variance), or (Option II) stop gradients entirely on reused responses and update only using on-policy samples—interpreting this as negative sample training.
- Core assumption: Controlling variance in policy gradient estimates is more critical than reducing bias (citing Sutton et al., 1998), and the current model "should have already acquired the knowledge" to solve previously-solved prompts.
- Evidence anchors:
  - [section 3.2] "Stop the gradient on oc and update the model only using the on-policy samples... discouraging the policy from exploring wrong directions"
  - [section 4.3, Table 2] "AR3PO w/ option II" achieves 43.1 average accuracy vs. 41.8 for direct rollout replay
  - [corpus] Weak corpus signal; concurrent works don't address importance ratio drift as explicitly
- Break condition: If the model has catastrophically forgotten previously-learned solutions, gradient stopping prevents recovery. Also breaks if negative advantages on incorrect responses systematically push the policy toward incorrect behaviors.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO) and advantage normalization**
  - Why needed here: AR3PO is explicitly designed to fix GRPO's vanishing advantage problem; understanding how GRPO computes advantages via within-group normalization is prerequisite.
  - Quick check question: Given a group of 4 responses with rewards [1, 0, 0, 0], what are the normalized advantages?

- Concept: **Importance sampling ratios in policy gradient methods**
  - Why needed here: Response reuse introduces off-policy samples with potentially stale behavior policies, requiring understanding of how importance ratios affect gradient variance.
  - Quick check question: Why does a large discrepancy between behavior policy and current policy destabilize PPO-style training?

- Concept: **Verifiable rewards vs. learned reward models**
  - Why needed here: AR3PO operates in RLVR setting where rewards are binary verification outcomes; the vanishing advantage problem is specific to discrete/binary reward structures.
  - Quick check question: In mathematical reasoning with binary correctness rewards, why is it problematic when all responses in a group are correct?

## Architecture Onboarding

- Component map:
  - **Prompt pool U**: Dynamic set of prompts awaiting correct responses during adaptive rollout
  - **Replay buffer B**: Stores (prompt, correct_response) pairs from all prior steps
  - **Verifier**: Binary reward function (Math-Verify in experiments)
  - **Multi-stage sampler**: Generates k responses per prompt per stage, removes successful prompts
  - **Advantage computer**: Normalizes rewards within response groups (Eq. 1)
  - **Policy updater**: GRPO objective with optional gradient stopping on reused responses

- Critical path: Sample batch → Adaptive rollout (S stages) → Check for zero-reward prompts → Fetch from buffer B if needed → Recompute advantages → Apply gradient stopping (Option II) or recompute probabilities (Option I) → Policy update → Update buffer B

- Design tradeoffs:
  - S=2, k=4 (max 8 responses) vs. GRPO's fixed 8: Adaptive uses fewer responses on average (5.7 in experiments) but requires multi-stage orchestration overhead
  - Option I (recompute probabilities) vs. Option II (gradient stopping): Option I is more computationally expensive; Option II is the recommended default
  - Buffer size: Paper does not specify limits; unbounded growth may cause memory issues

- Failure signatures:
  - **Zero advantages persist**: If adaptive rollout + buffer fail to provide mixed-reward groups, training stalls
  - **Importance ratio explosion**: If Option I is used and policy drifts significantly, recomputed ratios may still exhibit high variance
  - **Verification errors poison buffer**: Incorrect "correct" responses in B provide misleading advantages
  - **Easy prompts over-represented early**: First epoch has no buffer, so early training may have higher zero-reward rates

- First 3 experiments:
  1. **Reproduction baseline**: Implement GRPO on Qwen2.5-7B with Math500, verify vanishing advantage frequency matches paper (~30% zero-reward prompts without reuse)
  2. **Ablation on adaptive rollout alone**: Run AR3PO without response reuse (B=∅) to isolate adaptive rollout's contribution to efficiency
  3. **Buffer size sensitivity**: Test with bounded replay buffer (e.g., 1000, 5000, 10000 entries) to identify memory/performance tradeoffs not reported in paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AR3PO be extended to LLM agent settings where reasoning involves multi-turn tool use and environmental interaction?
- Basis in paper: [explicit] The conclusion states: "In the future, we plan to extend AR3PO to LLM agent settings and improve the efficiency of trajectory sampling."
- Why unresolved: The current method assumes single-turn response generation with verifiable rewards, whereas agent settings require handling multi-step trajectories, tool outputs, and potentially delayed rewards.
- What evidence would resolve it: A modified AR3PO implementation applied to agent benchmarks (e.g., web navigation, code execution) showing maintained efficiency gains while handling multi-turn dynamics.

### Open Question 2
- Question: How does AR3PO's performance scale with longer maximum response lengths, particularly matching DAPO's 20,480-token limit?
- Basis in paper: [inferred] Section 4.5 notes: "DAPO uses a maximum response length of 20480, whereas we limit it to 4096, suggesting that our method still has room for improvement with longer responses."
- Why unresolved: The paper does not ablate response length, so it remains unclear whether AR3PO's efficiency gains persist or whether longer responses introduce new challenges for adaptive rollout and response reuse.
- What evidence would resolve it: Controlled experiments varying maximum response length (e.g., 4K, 8K, 16K, 20K tokens) on the 32B model, reporting both accuracy and rollout cost.

### Open Question 3
- Question: Does AR3PO generalize effectively to non-mathematical reasoning domains such as code generation or scientific problem-solving?
- Basis in paper: [inferred] All experiments use mathematical reasoning benchmarks (Math500, Minerva Math, Olympiad Bench, AIME 2024) with a binary math verifier. No results are provided for other verifiable domains.
- Why unresolved: The adaptive rollout mechanism depends on difficulty estimation through correct response rates, which may behave differently in domains where correctness is less granular or verification is noisier.
- What evidence would resolve it: Application of AR3PO to code benchmarks (e.g., HumanEval, MBPP) or scientific reasoning tasks, comparing against GRPO and DAPO baselines.

### Open Question 4
- Question: What is the theoretical impact of the bias introduced by recomputing token probabilities on convergence guarantees?
- Basis in paper: [inferred] Section 3.2 acknowledges that recomputing token probabilities "introduces bias into the optimization objective" but argues variance reduction is more critical, without formal analysis.
- Why unresolved: The paper provides empirical support but no theoretical characterization of how this bias affects policy optimization convergence or final performance bounds.
- What evidence would resolve it: A theoretical analysis of the biased objective's convergence properties, or empirical studies measuring the bias-variance tradeoff across different policy divergence thresholds.

## Limitations

- The paper doesn't provide clear ablations separating adaptive rollout benefits from response reuse, making it difficult to quantify each component's contribution
- Replay buffer implementation details (size limits, eviction policies) are unspecified, which could affect reproducibility and performance
- The method's effectiveness on non-mathematical domains is untested, limiting generalizability claims
- Importance ratio clipping thresholds are left unspecified, leaving a critical hyperparameter undetermined

## Confidence

- **High confidence**: The core empirical results showing AR3PO outperforms GRPO on sampling efficiency and matches DAPO's performance across multiple model sizes and benchmarks
- **Medium confidence**: The claim that adaptive rollout alone provides significant efficiency gains, as the paper doesn't provide a clear ablation separating adaptive rollout benefits from response reuse
- **Low confidence**: The assertion that "difficult prompts receive more training samples" translates to better learning, as the paper doesn't analyze whether prompts identified as difficult by success rate are genuinely harder to learn

## Next Checks

1. **Difficulty correlation validation**: Run AR3PO on a held-out subset of prompts with known difficulty gradients (e.g., curriculum-ordered problems) to verify that adaptive rollout's difficulty assessment aligns with actual learning progress rather than random reward variance

2. **Buffer size sensitivity analysis**: Systematically vary the replay buffer size (e.g., 1000, 5000, 10000, unlimited) and measure (a) training stability, (b) final performance, and (c) zero-reward prompt reduction to identify optimal buffer management strategies not reported in the paper

3. **Importance ratio variance analysis**: Implement both Option I and Option II, then measure the variance of importance ratios and policy gradient estimates over training. Compare these metrics against the reported final accuracies to determine whether variance reduction (Option I) or bias tolerance (Option II) drives the observed performance differences