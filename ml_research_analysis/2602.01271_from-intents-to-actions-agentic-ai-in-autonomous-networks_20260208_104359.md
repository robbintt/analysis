---
ver: rpa2
title: 'From Intents to Actions: Agentic AI in Autonomous Networks'
arxiv_id: '2602.01271'
source_url: https://arxiv.org/abs/2602.01271
tags:
- preference
- mean
- each
- throughput
- service
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents an agentic AI system for intent-driven autonomous
  networks, structured around three specialized agents: an interpreter, optimizer,
  and controller. The interpreter converts high-level network intents into structured
  optimization templates using a dual-SLM architecture; the optimizer employs preference-aligned
  Bayesian optimization to dynamically adapt controller policies; and the controller
  uses distributed envelope Q-learning for multi-objective reinforcement learning.'
---

# From Intents to Actions: Agentic AI in Autonomous Networks

## Quick Facts
- arXiv ID: 2602.01271
- Source URL: https://arxiv.org/abs/2602.01271
- Reference count: 40
- Key outcome: Agentic AI system achieves superior throughput and reliability trade-offs in 5G link adaptation compared to traditional RL and state-of-the-art baselines.

## Executive Summary
This paper presents an agentic AI system for intent-driven autonomous networks, structured around three specialized agents: an interpreter, optimizer, and controller. The interpreter converts high-level network intents into structured optimization templates using a dual-SLM architecture; the optimizer employs preference-aligned Bayesian optimization to dynamically adapt controller policies; and the controller uses distributed envelope Q-learning for multi-objective reinforcement learning. Experiments in a 5G-compliant simulator show the system outperforms traditional RL and state-of-the-art link adaptation baselines, achieving superior throughput and reliability trade-offs across diverse service intents. The approach enables scalable, intent-aware radio resource management that adapts policies in real-time to evolving network conditions and heterogeneous service requirements.

## Method Summary
The system maps natural language intents to network actions through a triadic workflow: (1) an interpreter agent uses dual small language models to convert intents into structured optimization templates, (2) an optimizer agent employs preference-aligned Bayesian optimization to adapt controller policies, and (3) a controller agent uses distributed envelope Q-learning to execute actions. The architecture separates timescales - deliberative reasoning occurs on seconds-to-minutes while reactive control operates at sub-millisecond latency - enabling stable cognitive planning alongside real-time adaptation.

## Key Results
- Dual-SLM interpreter achieves 98% OTM accuracy on held-out intent dataset
- Preference-aligned Bayesian optimization converges 30% faster than standard methods while maintaining KPI stability
- Distributed envelope Q-learning improves hypervolume coverage by 22% compared to state-of-the-art MORL baselines
- System achieves superior throughput-reliability trade-offs across eMBB and full buffer service types in 5G simulations

## Why This Works (Mechanism)

### Mechanism 1
Timescale separation stabilizes cognitive planning against real-time control. The system decouples deliberative reasoning (interpreter/optimizer on seconds-to-minutes) from reactive control (controller on sub-millisecond). This prevents high-latency reasoning from disrupting low-latency loops and lets each agent specialize. Core assumption: Network dynamics are sufficiently slow that mid-term adaptation by the optimizer remains valid over controller horizons. Break condition: If network conditions change faster than optimizer update cadence, preference vectors may lag and degrade intent satisfaction.

### Mechanism 2
Preference-conditioned MORL enables a single policy to serve heterogeneous, conflicting intents. The controller learns a preference-conditioned Q-function that generalizes across the preference simplex via envelope backups. The optimizer steers preferences using Bayesian optimization to align with intents without retraining the policy. Core assumption: Linear scalarization is sufficient to represent the intent trade-offs; the Pareto front is approximately convex. Break condition: If intents require non-linear utility over objectives, linear scalarization may not capture user preferences.

### Mechanism 3
Dual-SLM interpreter reduces cognitive overhead while preserving domain grounding. A fine-tuned SLM performs deterministic intent-to-OTM translation; a separate ICL-based SLM handles adaptive reasoning over KPI statistics. Structured OTMs enforce schema validity; guardrails and cooldowns stabilize threshold updates. Core assumption: The intent space and OTM schema are sufficiently bounded that a 7B-parameter SLM can generalize after domain fine-tuning. Break condition: If intents contain novel vocabulary or complex constraints outside the fine-tuning distribution, translation accuracy may drop.

## Foundational Learning

- **Multi-objective reinforcement learning (MORL) and envelope Q-learning**: Needed because the controller must trade off competing objectives (throughput vs. reliability, resource usage vs. payload) under varying preferences. Quick check: Can you explain how envelope backups differ from standard Bellman updates in their treatment of vector-valued Q-functions?

- **Bayesian optimization with acquisition functions**: Needed because the optimizer must adapt preference vectors efficiently when objective evaluations are expensive (simulated network performance). Quick check: How does a Gaussian process surrogate model balance exploration vs. exploitation in sequential query selection?

- **Instruction fine-tuning of language models for structured output**: Needed because the interpreter must produce schema-compliant OTM JSON from natural language intents with high reliability. Quick check: What are the trade-offs between full fine-tuning and LoRA-based adaptation for domain-specific structured generation tasks?

## Architecture Onboarding

- **Component map**: Intent → Dual-SLM interpreter → OTM → PAX-BO optimizer → Preference ω → D-EQL controller → Actions (MCS index) → Network KPI telemetry → OTM adaptation

- **Critical path**: 1. Intent receipt → Translator SLM produces initial OTM. 2. Optimizer reads OTM, initializes ω, runs PAX-BO loop. 3. Controller executes policy with current ω, collects experiences. 4. Telemetry feeds back to optimizer and interpreter. 5. Interpreter may refine OTM thresholds under guardrails.

- **Design tradeoffs**: Compute vs. generality: Dual-SLM reduces memory/compute but limits cognitive complexity compared to a single large LLM. Exploration vs. stability: Trust regions in PAX-BO prevent erratic ω changes but may slow adaptation to sudden network shifts. Centralized vs. distributed: D-EQL's learner–actor decoupling improves sample efficiency but adds synchronization overhead.

- **Failure signatures**: Constraint oscillation: Frequent OTM threshold updates indicate over-reactive monitoring; check hysteresis settings. Preference drift: PAX-BO stuck at Lmin without improvement; inspect acquisition landscape and reset criteria. Policy degeneracy: Controller BLER rises without throughput gain; may indicate ω far from optimal region or training undercoverage.

- **First 3 experiments**: 1. Validate translator accuracy on held-out intents (schema validity, constraint correctness) per Table 5. 2. Run PAX-BO on a single-service constrained problem (e.g., throughput maximization under BLER ≤ 0.1) and confirm convergence vs. trust region ablation. 3. Train D-EQL on a simplified MORL benchmark (e.g., Fruit Tree Navigation) and compare hypervolume/CRF1 against Yang et al. (2019) and Basaklar et al. (2023) baselines.

## Open Questions the Paper Calls Out

### Open Question 1
Can the triadic workflow be effectively scaled across hierarchical RAN layers (e.g., cell to cluster) while maintaining intent consistency? Basis in paper: Section 8 identifies scaling the workflow across hierarchical layers as a key challenge for ensuring intent consistency and agent interoperability. Why unresolved: The current architecture is demonstrated on a single control loop (link adaptation), whereas cross-layer coordination introduces conflicting timescales and objectives. What evidence would resolve it: A demonstration of the system coordinating multiple controller agents across different RAN stack layers simultaneously.

### Open Question 2
How robust is the fine-tuned Translator SLM to ambiguous or out-of-distribution intents not present in the synthetic training corpus? Basis in paper: Section 11.1 relies on a curated synthetic dataset of 90k samples, while Section 9.6 notes "prompt sensitivity" and "distribution shift" as failure modes. Why unresolved: Synthetic paraphrases may not capture the full variability or slang used by human operators in production environments. What evidence would resolve it: Evaluation of the translator's schema accuracy on a held-out set of noisy, naturalistic, or adversarial operator intents.

### Open Question 3
Does the D-EQL controller maintain stability under real-time hardware latency constraints absent in the simulation environment? Basis in paper: All experiments are conducted in a 5G-compliant event-driven simulator, lacking hardware-in-the-loop validation. Why unresolved: Simulators abstract away hardware-induced jitter and compute delays, which are critical for the sub-millisecond latency budgets of link adaptation. What evidence would resolve it: Empirical results from an over-the-air testbed or hardware-in-the-loop setup confirming KPI targets are met under physical constraints.

## Limitations
- System performance depends on three proprietary or unspecified components: high-fidelity 5G simulator, exact state representation vector, and OTM schema grounding in real network constraints
- MORL envelope backups assume linear scalarization is sufficient for intent representation, which may not hold for non-convex Pareto frontiers or complex multi-service scenarios
- Current architecture demonstrated on single control loop (link adaptation), not validated for cross-layer coordination or hardware-in-the-loop scenarios

## Confidence

- **High confidence**: Timescale separation mechanism - well-established hierarchical control pattern with clear causal pathway
- **Medium confidence**: Dual-SLM interpreter - empirical results show high accuracy, but lacks comparison to single large model baselines and has limited generalization testing
- **Medium confidence**: Preference-conditioned MORL - builds on established MORL theory, but assumes linear utility and convex Pareto fronts without validation under alternative utility structures

## Next Checks
1. Validate translator accuracy on held-out intents (schema validity, constraint correctness) per Table 5
2. Run PAX-BO on a single-service constrained problem (e.g., throughput maximization under BLER ≤ 0.1) and confirm convergence vs. trust region ablation
3. Train D-EQL on a simplified MORL benchmark (e.g., Fruit Tree Navigation) and compare hypervolume/CRF1 against Yang et al. (2019) and Basaklar et al. (2023) baselines