---
ver: rpa2
title: Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval
  Augmented Generation Large Language Model Systems
arxiv_id: '2511.16654'
source_url: https://arxiv.org/abs/2511.16654
tags:
- retrieval
- multimodal
- text
- visual
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two multimodal retrieval approaches for RAG
  systems: text-based chunk retrieval (where images are summarized into text before
  embedding) and direct multimodal embedding retrieval (where images are stored natively
  in vector space). The study evaluates these approaches across 6 LLM models and 2
  embedding models on a newly created financial earnings call benchmark with 40 question-answer
  pairs.'
---

# Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems

## Quick Facts
- arXiv ID: 2511.16654
- Source URL: https://arxiv.org/abs/2511.16654
- Reference count: 2
- This paper compares text-based chunk retrieval with direct multimodal embedding retrieval for RAG systems.

## Executive Summary
This paper evaluates two approaches for multimodal retrieval in RAG systems: text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in vector space). The study compares these approaches across 6 LLM models and 2 embedding models on a newly created financial earnings call benchmark with 40 question-answer pairs. Direct multimodal embedding retrieval significantly outperforms text-based approaches, achieving 13% absolute improvement in mean average precision (mAP@5) and 11% improvement in normalized discounted cumulative gain (nDCG@5). These correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5. The multimodal approach also produces more accurate and factually consistent answers, particularly for larger models with stronger multimodal reasoning capabilities.

## Method Summary
The study constructs a financial earnings benchmark with 40 manually created QA pairs requiring both text (transcripts) and visual (slides) information integration. Two retrieval pipelines are built: text-only approach using GPT-5 to summarize images before embedding with text-embedding-ada-002, and multimodal approach embedding native images and text directly with Jina v4. Both retrieve top-5 documents from Azure AI Search. Six LLMs (GPT-4o, GPT-4o-mini, GPT-4.1, GPT-4.1-mini, GPT-5, GPT-5-mini) generate answers, with LLM-as-a-judge (GPT-5) evaluating quality across 6 criteria including correctness, numerical fidelity, and hallucination detection.

## Key Results
- Direct multimodal embedding retrieval achieves 13% absolute improvement in mAP@5 (32% relative improvement) over text-based approaches
- nDCG@5 improves by 11% absolute (20% relative improvement) with multimodal retrieval
- Larger models (GPT-5) show stronger preference for multimodal approach with 82% average score vs 18% for text-based
- Multimodal retrieval reduces hallucinations by 80 percentage points for larger models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct multimodal embeddings preserve visual-spatial information that text summarization loses during preprocessing.
- Mechanism: Unified embedding models map images and text into a shared semantic vector space via contrastive learning, retaining layout, scale, numerical precision, and spatial relationships that are degraded when images are converted to LLM-generated text descriptions.
- Core assumption: Visual features captured by multimodal encoders align with query semantics in ways that textual descriptions cannot fully replicate.
- Evidence anchors: [abstract] "LLM-based summarization... causes loss of contextual information and visual details critical for downstream retrieval." [section 4.4] "LLM summarization introduces not only information loss but also fabricated details that propagate to downstream generation."

### Mechanism 2
- Claim: Cross-modal retrieval in a unified vector space enables more precise semantic matching between text queries and visual content.
- Mechanism: Text queries are embedded into the same multimodal space as native images, allowing direct similarity computation without intermediate modality conversion, improving ranking quality by placing semantically relevant visual documents higher in retrieval results.
- Core assumption: The embedding model's cross-modal alignment is sufficiently strong for domain-specific content.
- Evidence anchors: [section 3.2.2] "At query time, text queries are embedded into the same multimodal space, enabling semantic retrieval of both textual passages and visual content based on shared meaning." [table 1] mAP@5 improves from 0.3963 to 0.5234; nDCG@5 improves from 0.5448 to 0.6543.

### Mechanism 3
- Claim: Larger vision-language models leverage native visual context more effectively than smaller models, amplifying retrieval benefits.
- Mechanism: Higher-capacity models possess stronger cross-modal reasoning capabilities, enabling them to extract and synthesize information from retrieved images. Smaller "mini" models have limited capacity to process visual context, showing diminished returns.
- Core assumption: Model scale correlates with multimodal reasoning capability; the bottleneck shifts from retrieval quality to model utilization capacity.
- Evidence anchors: [section 4.3] "GPT-5 shows the strongest preference for the multimodal approach with an average score of 0.82 compared to 0.18... smaller mini models exhibit more balanced scores." [section 4.4] "The diminishing returns observed for mini models indicate that effective utilization of multimodal context requires sufficient model capacity."

## Foundational Learning

- Concept: Dense Retrieval & Vector Similarity
  - Why needed here: Understanding how queries and documents are embedded and compared via cosine/dot-product similarity is foundational to RAG retrieval.
  - Quick check question: Can you explain why mAP@5 measures ranking quality, not just retrieval accuracy?

- Concept: Multimodal Embedding Alignment
  - Why needed here: CLIP-style contrastive learning creates shared text-image representations; this paper leverages Jina v4 for unified embeddings.
  - Quick check question: What does it mean for text and images to occupy the "same vector space"?

- Concept: LLM-as-a-Judge Evaluation
  - Why needed here: Pairwise comparisons using GPT-5 assess answer quality dimensions (correctness, hallucination, numerical fidelity) beyond exact match.
  - Quick check question: Why might LLM-as-a-judge be preferred over human evaluation for iterative experiments?

## Architecture Onboarding

- Component map: Document preprocessing (Transcript chunking + slide-to-image extraction) -> Embedding layer (Jina v4 or text-embedding-ada-002) -> Vector database (Azure AI Search) -> Retrieval (Top-K similarity search K=5) -> Generation (Vision-language models GPT-4o, GPT-4o-mini, GPT-4.1, GPT-4.1-mini, GPT-5, GPT-5-mini)

- Critical path:
  1. Extract images from documents (slides, charts, tables)
  2. Embed images natively using multimodal encoder
  3. Index in unified vector database alongside text chunks
  4. At query time, embed text query into multimodal space
  5. Retrieve top-K mixed-modality results
  6. Pass native images directly to VLM for generation

- Design tradeoffs:
  - Preprocessing complexity: Multimodal pipelines require image detection/extraction vs. simpler LLM-summarization approach
  - Infrastructure compatibility: Text-only RAG systems can reuse existing pipelines; multimodal requires VLM-capable inference
  - Model selection: Larger models maximize retrieval benefits but increase inference cost

- Failure signatures:
  - High retrieval scores but poor answer quality → Model capacity insufficient for cross-modal reasoning
  - Low nDCG with acceptable precision → Ranking degradation from modality mismatch
  - Persistent hallucinations despite native retrieval → Summarization artifacts in text chunks or insufficient visual grounding

- First 3 experiments:
  1. Baseline comparison: Text-only retrieval vs. direct multimodal retrieval on domain-specific benchmark; measure mAP@5, nDCG@5.
  2. Model tier analysis: Evaluate retrieval-to-generation pipeline across model sizes; identify capacity thresholds where multimodal benefits emerge.
  3. Hallucination audit: Use LLM-as-a-judge to compare "No Unsupported Additions" rates between text-summarized and native-image retrieval conditions.

## Open Questions the Paper Calls Out

- Question: Does the performance advantage of direct multimodal embedding retrieval generalize to specialized domains such as medical, legal, or scientific documents where visual content serves different semantic roles?
  - Basis in paper: [explicit] The Conclusion states: "Future work should extend evaluation to diverse domains including medical, legal, and scientific documents where visual content serves different purposes."
  - Why unresolved: The current study evaluates only a financial earnings benchmark (charts, tables), leaving the efficacy of native image embeddings in domains with vastly different visual conventions (e.g., medical imaging, legal diagrams) untested.
  - What evidence would resolve it: Comparative benchmark results (mAP@5, nDCG@5) from medical, legal, and scientific datasets showing similar or distinct performance trends between text-based and direct multimodal retrieval strategies.

- Question: Can automated document parsing pipelines be developed to reliably segment and classify visual elements (tables vs. images) across diverse document formats to minimize preprocessing overhead?
  - Basis in paper: [explicit] The Limitations section notes that "distinguishing between tables and images remains challenging" and calls for future work to "develop robust document parsing pipelines that automatically segment and classify visual elements."
  - Why unresolved: The study identifies preprocessing complexity as a key barrier, noting that current tools struggle to automatically handle different visual types and formats (e.g., PDF vs. PowerPoint) without manual intervention.
  - What evidence would resolve it: The creation and validation of an automated parsing tool that accurately segments complex documents into retrievable units (text chunks, distinct images, tables) with high precision across multiple file formats.

- Question: Does the observed reduction in hallucinations scale effectively with smaller, lower-capacity models, or is it strictly correlated with the model's multimodal reasoning capabilities?
  - Basis in paper: [inferred] The Results section notes that "diminishing returns observed for mini models indicate that effective utilization of multimodal context requires sufficient model capacity," while also highlighting that larger models saw an 80 percentage point reduction in hallucinations.
  - Why unresolved: While the paper shows larger models (GPT-5) benefit significantly from reduced hallucinations, the 50/50 split for GPT-5-mini suggests the retrieval method's ability to ground smaller models against hallucinations is not fully understood.
  - What evidence would resolve it: A fine-grained ablation study specifically measuring hallucination rates (Unsupported Additions) across a wider spectrum of model sizes to determine the minimum capacity required to leverage native visual grounding.

## Limitations

- The study evaluates only a financial earnings call domain, limiting generalizability to other multimodal contexts like medical or legal documents.
- The study identifies preprocessing complexity as a key barrier, noting that current tools struggle to automatically handle different visual types and formats (e.g., PDF vs. PowerPoint) without manual intervention.
- GPT-5 and GPT-5-mini references suggest forward-looking capabilities not publicly available, creating potential evaluation gaps for real-world replication.

## Confidence

- **High Confidence**: Retrieval performance improvements (mAP@5: 32% relative gain, nDCG@5: 20% relative gain) are directly measured and statistically clear from the reported metrics.
- **Medium Confidence**: Claim that larger models benefit more from native visual context is supported but limited by the small sample of model tiers tested (mini vs. full versions).
- **Low Confidence**: The mechanism explanation for why text summarization causes information loss could benefit from more ablation studies isolating specific types of lost information (e.g., numerical precision vs. spatial relationships).

## Next Checks

1. **Cross-Domain Replication**: Test the multimodal vs. text-summarization approach on a non-financial benchmark (e.g., medical reports with imaging) to validate generalizability of the retrieval advantage.

2. **Ablation on Image Resolution**: Compare retrieval performance using native images at different resolutions to quantify the impact of visual detail preservation on embedding quality.

3. **Human Evaluation Correlation**: Conduct a small-scale human evaluation study to measure correlation between LLM-as-a-judge scores and human judgment quality, particularly for the "no unsupported additions" criterion where LLM judgment may be overly strict.