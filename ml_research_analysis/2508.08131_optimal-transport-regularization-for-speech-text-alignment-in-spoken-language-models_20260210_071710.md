---
ver: rpa2
title: Optimal Transport Regularization for Speech Text Alignment in Spoken Language
  Models
arxiv_id: '2508.08131'
source_url: https://arxiv.org/abs/2508.08131
tags:
- speech
- embeddings
- transport
- slms
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Optimal Transport Regularization (OTReg)
  to address the modality gap between speech and text representations in Spoken Language
  Models (SLMs). The key idea is to formulate speech-text alignment as an optimal
  transport problem, using the optimal transport plan to derive a regularization loss
  that aligns speech embeddings with transcript embeddings.
---

# Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models

## Quick Facts
- arXiv ID: 2508.08131
- Source URL: https://arxiv.org/abs/2508.08131
- Reference count: 26
- Primary result: Optimal Transport Regularization (OTReg) improves cross-domain generalization in Spoken Language Models, reducing WER on CoVoST-2 and FLEURS datasets

## Executive Summary
This paper introduces Optimal Transport Regularization (OTReg) to address the modality gap between speech and text representations in Spoken Language Models (SLMs). The key idea is to formulate speech-text alignment as an optimal transport problem, using the optimal transport plan to derive a regularization loss that aligns speech embeddings with transcript embeddings. The method is lightweight, requiring no additional labels or learnable parameters, and integrates seamlessly into existing SLM training procedures. Experimental results on multilingual ASR tasks demonstrate that OTReg enhances speech-text alignment and reduces the modality gap, achieving lower Word Error Rates (WER) on test sets CoVoST-2 and FLEURS.

## Method Summary
OTReg formulates speech-text alignment as an optimal transport problem, where the goal is to find the optimal transport plan that maps speech embeddings to text embeddings. This plan is then used to derive a regularization loss that encourages the model to align speech and text representations in a semantically meaningful way. The method is applied during training by computing the optimal transport cost between speech and text embeddings, and using this cost as a regularization term in the loss function. The approach is designed to be lightweight and easy to integrate, requiring no additional labels or learnable parameters beyond the base SLM architecture.

## Key Results
- OTReg reduces Word Error Rates (WER) on CoVoST-2 and FLEURS test sets compared to base models and CTC-based approaches
- The method achieves WERs of 13.12 and 6.90 on CoVoST-2 and FLEURS, respectively
- OTReg enhances cross-domain generalization by aligning speech and text representations

## Why This Works (Mechanism)
The paper claims that optimal transport regularization works by explicitly modeling the alignment between speech and text modalities as a transportation problem, where the optimal transport plan provides a principled way to measure and minimize the modality gap. By regularizing the model to follow this optimal alignment, the method encourages the SLM to learn more consistent representations across speech and text inputs.

## Foundational Learning
- **Optimal Transport**: Mathematical framework for finding the most efficient way to transform one probability distribution into another; needed to formulate speech-text alignment as a transportation problem; quick check: verify that the transport plan converges and produces meaningful alignments
- **Modality Gap**: The discrepancy between representations of the same content in different modalities (speech vs text); needed to understand the problem OTReg addresses; quick check: measure representation similarity before and after applying OTReg
- **Speech Embeddings**: Vector representations of speech audio; needed as input to the optimal transport computation; quick check: ensure embeddings capture phonetic and semantic information
- **Text Embeddings**: Vector representations of text transcripts; needed as the target for speech alignment; quick check: verify embeddings preserve word-level information
- **Regularization Loss**: Additional loss term that encourages desired model behavior; needed to incorporate optimal transport into training; quick check: monitor loss contribution and convergence behavior

## Architecture Onboarding

### Component Map
Speech Encoder -> Speech Embeddings -> Optimal Transport Cost Computation -> Regularization Loss -> Text Encoder

### Critical Path
The critical path involves computing speech embeddings, calculating the optimal transport cost between speech and text embeddings, and incorporating this cost as a regularization term in the training loss. The text encoder processes transcripts in parallel to generate text embeddings for comparison.

### Design Tradeoffs
The main tradeoff is between computational cost and alignment quality. While optimal transport provides a principled alignment mechanism, computing transport plans can be expensive, especially for long sequences. The method trades some computational efficiency for improved cross-modal alignment and generalization.

### Failure Signatures
- High optimal transport costs indicating poor alignment between speech and text embeddings
- Degraded ASR performance if regularization is too strong
- Computational bottlenecks during training due to transport plan computation
- Convergence issues if transport costs dominate the primary task loss

### First Experiments
1. Measure optimal transport costs before and after applying OTReg to verify alignment improvement
2. Compare WER on held-out validation sets to ensure regularization doesn't harm primary task performance
3. Analyze computational overhead by measuring training time per batch with and without OTReg

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns with larger models and longer sequences due to optimal transport computation cost
- Limited evaluation to ASR tasks without testing on speech translation or dialog systems
- Lack of analysis on how regularization affects learned representations beyond WER metrics
- No ablation studies on different transport metrics or comparison with alternative alignment methods

## Confidence
- WER improvements on tested datasets: High
- Claims about cross-domain generalization: Medium  
- Claims about seamless integration and lightweight nature: Medium
- Claims about robustness across different model sizes and tasks: Low

## Next Checks
1. Benchmark OTReg on larger models (10B+ parameters) to assess scalability and computational overhead during training
2. Test the method on non-ASR tasks like speech translation and spoken dialog systems to evaluate generalization beyond transcription
3. Conduct ablation studies comparing squared Euclidean transport to other metrics (e.g., cosine distance, Earth Mover's Distance) and analyze the impact on different types of speech-text modality gaps