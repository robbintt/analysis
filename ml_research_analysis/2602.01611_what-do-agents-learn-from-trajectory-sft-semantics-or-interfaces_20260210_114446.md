---
ver: rpa2
title: 'What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?'
arxiv_id: '2602.01611'
source_url: https://arxiv.org/abs/2602.01611
tags:
- interface
- agents
- agent
- action
- perturb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Trajectory-based supervised fine-tuning (SFT) improves agent benchmark
  scores but conflates semantic tool learning with memorization of training-time interface
  patterns, making standard evaluations unreliable for assessing true capability.
  To disentangle these effects, we introduce PIPE, a protocol-level evaluation framework
  that minimally perturbs environment interfaces while preserving task semantics and
  execution behavior.
---

# What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?

## Quick Facts
- arXiv ID: 2602.01611
- Source URL: https://arxiv.org/abs/2602.01611
- Reference count: 40
- Agents improve on benchmarks through semantic learning and interface memorization; standard evaluations cannot distinguish them.

## Executive Summary
Trajectory-based supervised fine-tuning (SFT) boosts agent benchmark scores but conflates semantic tool learning with memorization of training-time interface patterns, making standard evaluations unreliable for assessing true capability. To disentangle these effects, we introduce PIPE, a protocol-level evaluation framework that minimally perturbs environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym, and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric quantifying preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics invisible under standard evaluation.

## Method Summary
The paper introduces PIPE, a perturbation-based evaluation framework that minimally rewrites environment interface specifications (e.g., action names to synonyms or symbols) while preserving task semantics and environment dynamics. This allows isolation of interface reliance from semantic understanding. Interface Reliance (IR) is introduced as a metric that quantifies an agent's preference for training-time interface surface forms using a counterbalanced dual-interface evaluation. The paper evaluates agents across 16 environments from AgentBench and AgentGym, comparing trajectory-SFT trained models against untrained baselines using both PIPE and IR.

## Key Results
- Trajectory-SFT agents show substantially larger performance drops under minimal interface rewrites (PIPE) than untrained agents, revealing interface shortcutting.
- Interface Reliance (IR) is higher for trajectory-SFT agents, indicating strong preference for training-time action names.
- Interface shortcutting exhibits non-monotonic training dynamics and is environment-dependent, invisible under standard evaluation.

## Why This Works (Mechanism)

### Mechanism 1
Trajectory-SFT improves benchmark scores through two conflated effects—semantic tool understanding (STU) and interface shortcutting (ISS)—which standard evaluation cannot distinguish. During SFT, the model is exposed to action names, formats, and interaction patterns that appear repeatedly across training trajectories. This allows the model to improve success rates either by internalizing tool semantics (generalizable across interfaces) or by memorizing surface-level patterns tied to the specific training-time interface (non-generalizable). Core assumption: Training-time and evaluation-time interfaces share identical surface forms (action names, formats), creating an inductive bias toward exploiting interface-specific regularities rather than solely learning tool semantics.

### Mechanism 2
Minimal interface perturbation (PIPE) isolates interface reliance by preserving task semantics and environment dynamics while breaking surface-form correspondences between training and evaluation. PIPE rewrites only the interface specification (action names to synonyms or symbols) while keeping task descriptions, environment backend, and observation structures unchanged. This creates a controlled mismatch between training-seen and evaluation-seen interface surface forms without increasing intrinsic task difficulty. Core assumption: Semantic understanding of tools should generalize across synonymous or semantically opaque action names if the model has learned functional meaning rather than surface patterns.

### Mechanism 3
Interface Reliance (IR) quantifies an agent's preference for training-time interface surface forms using a counterbalanced dual-interface evaluation. IR exposes the agent to both original and perturbed (synonym) interfaces simultaneously in the same environment, counterbalancing presentation order. It computes the geometric mean of per-task preference ratios (original usage / synonym usage), which robustly aggregates interface choice without being dominated by outlier tasks. Core assumption: An agent with strong interface reliance will preferentially invoke original action names even when synonym names are equally available and functionally identical; this preference reflects memorization of training-time surface forms.

## Foundational Learning

- **Trajectory-based Supervised Fine-Tuning (trajectory-SFT)**: The paper's central claim depends on understanding that trajectory-SFT trains agents by imitating expert interaction sequences, which inherently exposes the model to both semantic content and interface surface patterns.
  - Why needed here: Explains how agents can learn both tool semantics and interface-specific patterns during training.
  - Quick check question: Can you explain how trajectory-SFT differs from standard instruction tuning in terms of what patterns the model might memorize?

- **Agent Evaluation Protocol (environment, task, interface specification)**: The paper formalizes agent evaluation as interaction with an environment via a specific interface specification (action names, descriptions, formats). Understanding this protocol is necessary to see how perturbations isolate interface reliance.
  - Why needed here: Clarifies the relationship between environments, tasks, and interface specifications in agent evaluation.
  - Quick check question: Given an environment with action set A and interface specification I, what happens to task difficulty if you rewrite I but keep A unchanged?

- **Semantic Invariance vs. Surface-Form Sensitivity**: The paper's diagnostic relies on the distinction between capabilities that are invariant to interface surface forms (semantic understanding) and those that are sensitive to them (interface shortcuts).
  - Why needed here: Provides the theoretical foundation for why interface perturbations can reveal interface reliance.
  - Quick check question: If an agent successfully uses tools after action names are replaced with synonyms, does this indicate semantic learning, interface shortcutting, or both?

## Architecture Onboarding

- **Component map**: Perturbation Layer -> PIPE Evaluation Runner -> IR Computation Module -> Legacy Action Counter
- **Critical path**: 
  1. Identify target environment and its interface specification (action names, descriptions).
  2. Design perturbation mappings (synonym and symbol) that preserve functional descriptions.
  3. Wrap environment with perturbation layer; verify backend semantics unchanged.
  4. Run evaluation under all three interface conditions (original, perturb1, perturb2).
  5. Compute Δ scores per environment; identify large negative Δ as interface shortcutting.
  6. For IR, set up dual-interface prompts with counterbalanced ordering; compute preference ratios.
- **Design tradeoffs**:
  - **Perturbation strictness**: Disallowing original interface names (strict mode) forces agents to adapt but may exaggerate degradation; allowing both may underestimate reliance.
  - **Synonym selection**: Human-curated synonyms may inadvertently introduce semantic shifts; symbol perturbations are cleaner but more artificial.
  - **IR smoothing parameter α**: Controls sensitivity to edge cases; higher α reduces outlier dominance but may dilute signal.
- **Failure signatures**:
  - **Legacy action loops**: Agent repeatedly invokes deprecated action names despite invalid-action feedback (see Table 5, Appendix A).
  - **Schema hallucination**: Agent fabricates environment interactions or reply formats not grounded in observations (see Table 6, Appendix B).
  - **Non-monotonic training dynamics**: IR may increase then decrease across epochs, indicating shortcut exploitation followed by partial semantic recovery (see Figure 4).
- **First 3 experiments**:
  1. **Baseline PIPE evaluation**: Apply PIPE (synonym perturbation only) to an existing trajectory-SFT agent on 2-3 AgentBench environments; compute Δ and compare to untrained baseline.
  2. **IR sanity check**: Run IR evaluation on an untrained model; verify IR values cluster near 1 (no preference) across environments.
  3. **Training checkpoint analysis**: Fine-tune a model for 3-5 epochs; at each checkpoint, evaluate under original and perturbed interfaces; plot Δ and IR vs. epoch to observe shortcut dynamics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating interface diversity during trajectory-SFT (e.g., randomizing action names per trajectory) reduce interface shortcutting while preserving semantic learning?
- Basis in paper: The paper shows interface shortcuts emerge when training and evaluation share the same interface, and notes "agents are more likely to learn tool semantics when the training regime is 'loose'" (Appendix D).
- Why unresolved: The paper diagnoses the problem but does not experimentally test training-time interventions beyond noting AgentFLAN's correlational robustness.
- What evidence would resolve it: Train agents with interface-augmented trajectories (multiple action-name aliases per tool) and evaluate using PIPE to measure if IR decreases without degrading original-interface scores.

### Open Question 2
- Question: What is the causal mechanism behind the non-monotonic training dynamics where early epochs increase interface reliance while later epochs partially recover semantic learning (as observed in DATABASE)?
- Basis in paper: Figure 4 shows IR increases then decreases during training, with the paper noting "early training increases Origin performance while decreasing perturbed performance and increasing IR" followed by partial recovery.
- Why unresolved: The paper documents this phenomenon but does not explain whether it reflects statistical learning dynamics, regularization effects, or data distribution properties.
- What evidence would resolve it: Controlled ablations varying trajectory ordering, tool frequency, and epoch count with fine-grained IR tracking across training.

### Open Question 3
- Question: Can reinforcement learning or alternative training objectives beyond behavioral cloning mitigate interface shortcutting while maintaining or improving task success?
- Basis in paper: The paper focuses on trajectory-SFT via behavioral cloning and notes shortcuts may stem from "exploiting interface-level regularities," but does not test whether reward-based training encourages semantic grounding.
- Why unresolved: RL-based agent training may incentivize environment-agnostic tool understanding but could introduce other failure modes.
- What evidence would resolve it: Compare agents trained via SFT vs. RL on the same trajectory data, evaluating both with PIPE/IR and original benchmarks.

## Limitations

- The central confound addressed is well-supported empirically but rests on the assumption that PIPE perturbations truly preserve task semantics while only breaking surface forms.
- The paper does not extensively explore whether interface reliance reflects genuine memorization versus emergent positional or format preferences.
- The claim that IR values near 1 for untrained agents definitively indicate "no interface reliance" may be oversimplified.

## Confidence

- **High Confidence**: The empirical observation that trajectory-SFT agents show larger performance drops under PIPE perturbations than untrained agents (Δ degradation). The IR metric and its implementation are clearly defined and technically sound.
- **Medium Confidence**: The interpretation that Δ degradation directly measures "interface shortcutting" rather than, e.g., model brittleness to distributional shift or prompt formatting changes. The non-monotonic training dynamics are intriguing but require deeper analysis to rule out alternative explanations.
- **Low Confidence**: The claim that IR values near 1 for untrained agents definitively indicate "no interface reliance." Untrained agents may have baseline preferences due to prompt formatting or positional biases unrelated to training-time exposure.

## Next Checks

1. **Perturbation Semantic Validation**: Conduct ablation studies where perturbed interfaces are validated by human raters for semantic equivalence; compare model performance drops across perturbation types to isolate surface-form effects from semantic shifts.

2. **Cross-Domain Interface Generalization**: Test whether agents trained on one interface variant (e.g., synonym-perturbed) show degraded performance on another (e.g., symbol-perturbed), which would strengthen evidence that degradation reflects interface memorization rather than general robustness.

3. **IR Bias Isolation**: Evaluate IR on agents with controlled positional or formatting biases (e.g., always preferring first-listed actions) to quantify how much IR reflects interface reliance versus artifactual preferences.