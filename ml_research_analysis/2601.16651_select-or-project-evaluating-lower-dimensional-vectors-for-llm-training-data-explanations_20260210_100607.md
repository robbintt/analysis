---
ver: rpa2
title: Select or Project? Evaluating Lower-dimensional Vectors for LLM Training Data
  Explanations
arxiv_id: '2601.16651'
source_url: https://arxiv.org/abs/2601.16651
tags:
- gradient
- components
- full
- subset
- component
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to efficiently compute training data
  influence for large language models by reducing the dimensionality of model gradients.
  The authors propose a retrieval-based benchmark to evaluate whether selecting a
  small, architecturally-informed subset of model components or projecting the full
  gradient into a lower-dimensional space better captures training data influence.
---

# Select or Project? Evaluating Lower-dimensional Vectors for LLM Training Data Explanations

## Quick Facts
- **arXiv ID**: 2601.16651
- **Source URL**: https://arxiv.org/abs/2601.16651
- **Reference count**: 28
- **Primary result**: A greedy component selection approach outperforms both random projection and full gradients for efficient training data influence retrieval in LLMs.

## Executive Summary
This paper addresses the computational challenge of identifying training data influence for large language models by evaluating dimensionality reduction strategies for model gradients. The authors propose a retrieval-based benchmark where the goal is to match paraphrased or model-generated queries to their original training examples using gradient-based similarity. They compare three approaches: using the full gradient, projecting gradients into lower dimensions via random projection, and selecting a small subset of the most informative model components. The key finding is that a greedily selected subset of model components consistently outperforms both the full gradient and random projection baselines in retrieval accuracy while being significantly more computationally efficient.

## Method Summary
The method computes per-sample gradients of training examples against all 113 component tensors (16 layers × 7 components + embedding) in a 1.2B parameter LLM. To enable efficient component selection, the approach pre-computes pairwise dot products between all gradient vectors, allowing cosine similarity for any subset to be reconstructed from scalar sums. A greedy forward selection algorithm then iteratively adds components that maximize retrieval accuracy on a benchmark task. The method is evaluated against random projection baselines and full gradient comparisons on two query datasets: paraphrased versions of training examples and model-generated completions.

## Key Results
- Greedy selection achieves 0.998 accuracy with <5% of parameters in paraphrased setting, exceeding full gradient (0.993)
- MLP Gate and Up projections consistently outperform attention K/V projections, with MLP Gate-Proj achieving 0.256 mean accuracy vs. Attn Value-Proj at 0.198
- Full gradient accuracy drops to 0.218 (near random chance at 0.20) in model-generated setting, while greedy selection maintains performance
- Component-wise selection is 300-900× more efficient than random projection at equivalent dimensionality

## Why This Works (Mechanism)

### Mechanism 1: Component-wise Gradient Information Asymmetry
A small, architecturally-informed subset of model components captures task-relevant influence information more effectively than the full gradient. Different layer components encode instance-specific information unequally, with MLP blocks carrying stronger discriminative signals for training data attribution than attention K/V projections. The greedy selection exploits this asymmetry by iteratively adding components that maximize retrieval accuracy, effectively identifying which architectural elements encode the most task-relevant gradient information.

### Mechanism 2: Gradient Signal Dilution in Aggregated Representations
The full gradient contains noisy or contradictory signals from less-informative components that dilute the influence signal, making targeted selection preferable to dense projection. When all components are aggregated (full gradient) or projected (random projection), the discriminative signal from informative components is mixed with noise from less-informative ones. Greedy selection filters out low-signal components, improving the signal-to-noise ratio for the retrieval task.

### Mechanism 3: Efficient Subset Evaluation via Dot Product Linearity
Cosine similarity for any component subset can be reconstructed from pre-computed component-wise dot products, enabling efficient greedy search without storing high-dimensional gradients. The dot product of concatenated vectors equals the sum of constituent dot products. By pre-computing scalar values δ^(l,k)_{i,j} = v_{W^(l,k)}(s_i)^T v_{W^(l,k)}(s_j) for all component-sample pairs, any subset's similarity can be reconstructed by summation.

## Foundational Learning

- **Gradient-based influence estimation**: Why needed: The entire method relies on comparing gradients of training and test samples to infer training data influence. Without understanding that gradients indicate parameter-space directions for loss reduction, the retrieval rationale is opaque. Quick check: Given a model with parameters θ, what does ∇_θ L(s_i, θ_T) represent for a training sample s_i evaluated at final parameters θ_T?

- **Cosine similarity vs. dot product for gradient comparison**: Why needed: The paper uses cosine similarity rather than raw dot products to normalize for gradient magnitude variation across samples, ensuring directional alignment drives retrieval rather than magnitude. Quick check: Why would gradient magnitude vary across samples, and how does cosine similarity address this?

- **Random projection (Johnson-Lindenstrauss lemma)**: Why needed: The paper compares against random projection as a baseline. Understanding that RP approximately preserves pairwise distances in lower dimensions clarifies why it captures global geometry but not task-specific informativeness. Quick check: What geometric property does random projection preserve, and why might this be insufficient for identifying the most informative dimensions?

## Architecture Onboarding

- **Component map**: Input → Gradient computation (113 component tensors) → Pre-computation (component-wise dot products) → Selection engine (greedy forward algorithm) → Retrieval evaluation (BM25 + cosine similarity) → Output (component list + accuracy scores)

- **Critical path**: 1) Compute gradients for all samples against all 113 components; 2) Pre-compute and store all pairwise component-wise dot products; 3) Construct candidate sets via BM25; 4) Run greedy selection; 5) Evaluate on paraphrased and model-generated query sets

- **Design tradeoffs**: Accuracy vs. parameter budget (non-monotonic relationship); pre-computation cost vs. selection speed (4 hours upfront, minutes for selection); candidate set size (b=5 default for tractability); static vs. dynamic attribution (final checkpoint only)

- **Failure signatures**: Paraphrasing instruction execution (translates instead of paraphrases, ~1% of cases); model-generated setting collapse (full gradient accuracy drops to 0.218); middle layer underperformance (accuracy dip around layers 6-12); attention K/V projection failure (Value projection performs below random chance at 0.198)

- **First 3 experiments**: 1) Reproduce single-component performance table for all 113 components on both D_p and D_m; 2) Run greedy selection with varying parameter budgets and plot accuracy vs. cumulative parameter fraction; 3) Compare against random projection baseline at matched dimensionality for each parameter budget tested

## Open Questions the Paper Calls Out

1. **Architecture generalization**: Do the findings that MLP components outperform attention components for retrieval hold across different model architectures and scales beyond the 1.2B parameter model tested? The authors suggest exploring generalization across several axes, including different model architectures, scales, and data domains.

2. **Dynamic attribution adaptation**: Can the greedy component selection strategy be effectively adapted to dynamic attribution methods that track influence throughout the training process? The authors note that future work should extend analysis to dynamic attribution methods that track influence at multiple checkpoints.

3. **Downstream task utility**: Does targeted component selection provide superior performance for downstream tasks like data cleaning or pruning compared to using the full gradient? The authors suggest the approach could support applications that rely on similar loss-gradient comparisons, such as data cleaning- or pruning methods.

## Limitations
- The method's effectiveness depends on architectural choices and retrieval task, may not generalize to different model families or influence tasks
- Computational scalability is limited by O(|W| × N²) storage requirement for pre-computed dot products, becoming prohibitive for larger models
- The retrieval task proxy assumes semantic similarity indicates training data influence, which may not capture all forms of influence

## Confidence
- **High Confidence**: Greedy selection consistently outperforms random projection and full gradients; dot product linearity for efficient subset evaluation is mathematically sound
- **Medium Confidence**: Signal dilution explanation is plausible but not directly measured; claims about MLP Gate/Up projections being most informative may not generalize to all model families
- **Medium Confidence**: Reported accuracies are high but depend on specific paraphrasing quality and model-generated outputs

## Next Checks
1. Apply the greedy selection method to a different LLM architecture (e.g., Llama or Mistral) and verify whether the same component prioritization pattern holds for the same retrieval task

2. Evaluate the method on a non-retrieval influence task, such as identifying the most influential training examples for a specific test prediction or detecting data poisoning, to verify the approach generalizes beyond paraphrasing

3. Systematically test whether the greedy selection truly captures complementary information by comparing its performance against random projection when restricted to the same top-k components identified by the greedy method