---
ver: rpa2
title: 'One Size Does Not Fit All: Architecture-Aware Adaptive Batch Scheduling with
  DEBA'
arxiv_id: '2511.03809'
source_url: https://arxiv.org/abs/2511.03809
tags:
- batch
- gradient
- size
- adaptive
- deba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEBA challenges the assumption that adaptive batch size methods
  generalize across neural network architectures. By monitoring gradient variance,
  gradient norm variation, and loss variation, DEBA dynamically adjusts batch sizes
  during training.
---

# One Size Does Not Fit All: Architecture-Aware Adaptive Batch Scheduling with DEBA

## Quick Facts
- arXiv ID: 2511.03809
- Source URL: https://arxiv.org/abs/2511.03809
- Authors: FranÃ§ois Belias; Naser Ezzati-Jivan; Foutse Khomh
- Reference count: 7
- Primary result: Architecture-aware adaptive batch scheduling improves training efficiency by 45-62% for lightweight models while avoiding instability in deeper architectures

## Executive Summary
DEBA challenges the assumption that adaptive batch size methods generalize across neural network architectures. The method monitors gradient variance, gradient norm variation, and loss variation to dynamically adjust batch sizes during training. Systematic evaluation across six architectures (ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V3, ViT-B16) on CIFAR-10/100 reveals architecture-dependent adaptation efficacy: lightweight and medium-depth models achieve 45-62% speedup with 1-7% accuracy gains, while deeper architectures show unstable performance. A baseline stability profiling framework predicts adaptive compatibility through lightweight fixed-batch runs, enabling architecture-aware deployment.

## Method Summary
DEBA implements architecture-aware adaptive batch scheduling by monitoring three key signals: gradient variance, gradient norm variation, and loss variation. The method employs a sliding window for statistics computation and enforces a 5+ epoch cooldown between batch size adaptations. Architecture-specific threshold tuning is required for optimal performance. The baseline stability profiling framework predicts adaptive compatibility by running lightweight fixed-batch experiments before deployment. The approach challenges universal application of adaptive batch scheduling by demonstrating that different architectures respond differently to batch size variations.

## Key Results
- Lightweight models (MobileNet-V3, DenseNet-121, EfficientNet-B0) achieve 45-62% training speedup with 1-7% accuracy gains
- Deeper architectures show unstable performance when adaptive batch scheduling is applied
- Baseline stability profiling framework successfully predicts architecture compatibility with adaptive methods
- Critical implementation choices include sliding window statistics, 5+ epoch cooldowns, and architecture-specific threshold tuning

## Why This Works (Mechanism)
DEBA works by recognizing that neural network architectures have fundamentally different sensitivities to batch size variations during training. The method monitors three key signals - gradient variance, gradient norm variation, and loss variation - that indicate when batch size changes will be beneficial or detrimental. Lightweight and medium-depth models benefit from larger batches that improve computational efficiency, while deeper architectures experience training instability when batch sizes change dynamically. The architecture-aware approach matches the adaptation strategy to each model's characteristics rather than applying universal rules.

## Foundational Learning
- Adaptive batch scheduling: Dynamic adjustment of batch sizes during training based on optimization signals
  - Why needed: Fixed batch sizes often represent a tradeoff between computational efficiency and convergence stability
  - Quick check: Monitor training curves for batch size effects on convergence speed and final accuracy

- Architecture-dependent optimization: Recognition that different neural network architectures respond differently to the same training modifications
  - Why needed: Universal optimization techniques often fail to account for architectural variations in gradient flow and parameter sensitivity
  - Quick check: Compare performance of optimization techniques across diverse architecture families

- Stability profiling: Pre-training evaluation to predict compatibility with adaptive optimization techniques
  - Why needed: Prevents wasting computational resources on incompatible architecture-optimization combinations
  - Quick check: Run short fixed-batch experiments to establish baseline stability characteristics

## Architecture Onboarding

Component Map:
- Training loop -> Signal monitoring (gradient variance, gradient norm variation, loss variation) -> Batch size adaptation decision -> Weight update

Critical Path:
1. Signal monitoring during training iterations
2. Statistical analysis using sliding window
3. Adaptation decision with cooldown enforcement
4. Batch size update and continued training

Design Tradeoffs:
- Adaptation frequency vs. training stability: More frequent adaptations provide better optimization but risk instability
- Signal sensitivity vs. noise resistance: Thresholds must be tuned to avoid overreacting to transient fluctuations
- Computational overhead vs. efficiency gains: Monitoring adds overhead but can be offset by improved training efficiency

Failure Signatures:
- Unstable training curves with oscillating loss
- Degraded final accuracy compared to fixed batch baselines
- Excessive adaptation frequency leading to convergence issues

First Experiments:
1. Run baseline stability profiling on target architecture with fixed batch sizes
2. Apply DEBA with conservative thresholds on architecture predicted to be compatible
3. Compare training efficiency and final accuracy against fixed batch baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to image classification on CIFAR-10/100 benchmarks
- Theoretical justification for monitoring signal selection remains incomplete
- 5+ epoch cooldown requirement may introduce significant overhead on large-scale training

## Confidence

| Claim | Confidence |
|-------|------------|
| Architecture-dependent efficacy | High |
| Monitoring signal effectiveness | Medium |
| Baseline profiling predictive power | Medium |
| Speedup and accuracy improvement ranges | High |

## Next Checks
1. Evaluate DEBA's performance on larger-scale datasets (ImageNet, domain-specific data) to test scalability and confirm architecture-aware benefits persist beyond CIFAR benchmarks
2. Conduct ablation studies isolating each monitoring signal's individual contribution to identify whether all three are necessary or if simpler variants maintain efficacy
3. Test DEBA's compatibility with other adaptive optimization methods (Adam, LAMB) to determine whether architecture-aware batch scheduling principles extend beyond SGD momentum baseline