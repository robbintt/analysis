---
ver: rpa2
title: Knowledge Graph Augmented Large Language Models for Disease Prediction
arxiv_id: '2512.01210'
source_url: https://arxiv.org/abs/2512.01210
tags:
- reasoning
- disease
- paths
- prediction
- kg-guided
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating clinically interpretable
  and grounded reasoning for disease prediction using electronic health records. The
  core method involves a knowledge graph-guided chain-of-thought (CoT) framework that
  maps ICD-9 codes to a biomedical knowledge graph (PrimeKG), extracts disease-relevant
  reasoning paths, and uses these as scaffolds to generate temporally consistent CoT
  explanations.
---

# Knowledge Graph Augmented Large Language Models for Disease Prediction

## Quick Facts
- arXiv ID: 2512.01210
- Source URL: https://arxiv.org/abs/2512.01210
- Reference count: 33
- Primary result: KG-guided models achieve AUROC 0.66–0.70 and macro-AUPR 0.40–0.47 on 10 diseases; transfer zero-shot to CRADLE, improving accuracy from ~0.40–0.51 to 0.72–0.77.

## Executive Summary
This paper addresses the challenge of generating clinically interpretable and grounded reasoning for disease prediction using electronic health records. The core method involves a knowledge graph-guided chain-of-thought (CoT) framework that maps ICD-9 codes to a biomedical knowledge graph (PrimeKG), extracts disease-relevant reasoning paths, and uses these as scaffolds to generate temporally consistent CoT explanations. Lightweight LLMs are then fine-tuned on this KG-anchored supervision corpus. Across ten diseases and small training cohorts (400–1000 cases), the KG-guided models outperform classical baselines, achieving AUROC of 0.66–0.70 and macro-AUPR of 0.40–0.47. They also transfer effectively zero-shot to an external cohort, improving accuracy from ~0.40–0.51 to 0.72–0.77, with blinded clinician evaluations showing strong preference for the KG-guided CoT explanations in clarity, relevance, and correctness.

## Method Summary
The framework generates clinically interpretable disease predictions by first mapping ICD-9 codes to a biomedical knowledge graph (PrimeKG) using a three-stage process (exact match, similarity threshold τ=0.85, GPT-4o validation). For each target disease, GPT-4o selects K_node=8 relevant features from the mapped nodes, then shortest paths (≤5 hops) are extracted and pruned to K_path=5 reasoning paths. A KG-guided CoT prompt is used to generate reasoning traces conditioned on visit features and ground-truth labels. Only samples whose conclusions match observed outcomes are retained (label-consistency filtering). The resulting corpus fine-tunes LLaMA-3.1-8B-Instruct or Gemma-7B-Instruct using AdamW (lr=1e-5, cosine, max grad norm 1.0), batch size 8, sequence length 4,096, 10 epochs, and DeepSpeed ZeRO-3.

## Key Results
- KG-guided models achieve AUROC 0.66–0.70 and macro-AUPR 0.40–0.47 on MIMIC-III test sets for 10 diseases.
- Zero-shot transfer to CRADLE cohort improves accuracy from ~0.40–0.51 to 0.72–0.77 without additional fine-tuning.
- Blinded clinician evaluations show strong preference for KG-guided CoT explanations over baselines in clarity, relevance, and correctness.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KG paths constrain LLM reasoning to medically grounded trajectories, reducing hallucination.
- Mechanism: By extracting shortest paths (≤5 hops) from PrimeKG between disease-relevant nodes and target diseases, then injecting these as scaffolds into the CoT prompt, the model receives explicit biomedical relationships that structure its reasoning.
- Core assumption: PrimeKG relations accurately reflect clinically valid biomedical causality; shortest paths capture the most relevant reasoning chains.
- Evidence anchors: Abstract mentions mining disease-specific relevant nodes and reasoning paths as scaffolds; section notes shortest paths limit "overthinking" and preserve immediate biomedical relations; KAT-GNN (arXiv:2511.01249) shows broader utility of KG constraints in clinical tasks.
- Break condition: If PrimeKG has incomplete or biased coverage for your target diseases, path extraction may yield irrelevant or missing connections.

### Mechanism 2
- Claim: Label-consistency filtering removes CoT traces whose conclusions contradict ground truth, yielding higher-quality supervision.
- Mechanism: Each generated CoT must end with an explicit binary conclusion ("Yes" or "No"). Only samples where the CoT-implied label matches the observed outcome y_t+1 are retained.
- Core assumption: Correct conclusions correlate with higher-quality intermediate reasoning steps; incorrect conclusions indicate flawed or hallucinated chains.
- Evidence anchors: Abstract states retaining only samples whose conclusions match observed outcomes; section specifies C_t,d is kept if CoT-implied label matches ground-truth outcome; limited direct corpus evidence on label-consistency filtering for CoT.
- Break condition: If filtering removes too many samples (>80%), training data becomes insufficient; may indicate prompt misalignment or label noise.

### Mechanism 3
- Claim: Lightweight LLMs fine-tuned on KG-anchored CoT generalize zero-shot to out-of-distribution cohorts.
- Mechanism: The KG-anchored supervision encodes disease-specific reasoning patterns (risk factors, temporal cues, diagnostic evidence) rather than cohort-specific correlations. At inference, the model conditions on the same KG evidence structure, enabling transfer without retraining.
- Core assumption: Reasoning patterns learned from MIMIC-III apply to CRADLE; disease mechanisms are cohort-invariant.
- Evidence anchors: Abstract reports models transfer zero-shot to CRADLE, improving accuracy from roughly 0.40–0.51 to 0.72–0.77; section notes MIMIC-trained models transfer effectively to CRADLE without additional fine-tuning; related work on temporal reasoning (arXiv:2501.18724) explores LLM generalization but lacks cross-cohort transfer metrics.
- Break condition: If target cohort has substantially different feature spaces, entity mapping may fail without re-mapping.

## Foundational Learning

- Concept: **Knowledge Graph Structure**
  - Why needed here: Understanding nodes, edges, and multi-hop paths is essential for grasping how PrimeKG provides reasoning scaffolds.
  - Quick check question: Given a KG with nodes {Diabetes, Retinopathy, VEGF} and edges {Diabetes—causes→Retinopathy, Retinopathy—treated_by→VEGF}, what is the 2-hop path from Diabetes to VEGF?

- Concept: **Chain-of-Thought Prompting**
  - Why needed here: The framework generates explicit reasoning traces before conclusions; understanding CoT structure is prerequisite to modifying prompts or debugging outputs.
  - Quick check question: In a CoT prompt for disease prediction, should the conclusion appear before or after the reasoning steps?

- Concept: **Fine-Tuning Hyperparameters**
  - Why needed here: The paper reports specific settings (AdamW, lr=1e-5, batch size 8, 10 epochs); replicating or adapting requires understanding why these matter.
  - Quick check question: What is the risk of using too high a learning rate when fine-tuning a pretrained LLM on small clinical datasets?

## Architecture Onboarding

- Component map: Entity Mapper -> Relevance Miner -> Path Extractor -> CoT Generator -> Label Filter -> Fine-Tuner
- Critical path: Entity Mapper → Relevance Miner → Path Extractor → CoT Generator → Label Filter → Fine-Tuner. Breaks at Entity Mapper if ICD-to-KG mapping fails; breaks at Label Filter if retention rate is too low.
- Design tradeoffs:
  - K_node (8) vs coverage: Fewer nodes → more focused reasoning but may miss relevant comorbidities
  - Path length L (5) vs specificity: Longer paths capture indirect relations but risk weak clinical plausibility
  - GPT-4o for data generation (~$100 cost) vs quality: Higher-quality CoT but introduces dependency on proprietary model
  - Small models (8B/7B) vs interpretability: Easier deployment and lower inference cost, but may underperform larger models on complex cases
- Failure signatures:
  - Low retention rate after filtering (<30%) → prompt may not guide model to correct conclusions; inspect rejected samples
  - High accuracy but low AUROC (<0.55) on transfer → model learned cohort-specific heuristics, not generalizable reasoning
  - Clinician feedback cites "repetitive differentials" or "illogical reasoning" → path extraction may include spurious edges; re-run path pruning
- First 3 experiments:
  1. Ablate label-consistency filtering: Train on unfiltered CoT corpus; compare AUROC and macro-AUPR to filtered version on held-out test set.
  2. Vary K_node (4 vs 8 vs 12): Measure impact on clinician preference scores and prediction metrics; identify saturation point.
  3. Test transfer to a third cohort: Apply MIMIC-trained model to a publicly available EHR dataset (e.g., eICU) with re-mapped ICD codes; assess whether gains persist.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance and explanation fidelity change when using knowledge graphs with varying levels of incompleteness or bias? The authors note the approach depends on the coverage and quality of PrimeKG, so gaps or biases can propagate. This remains unresolved as the study relies solely on PrimeKG without evaluating sensitivity to KG sparsity or noise.
- **Open Question 2**: Does utilizing richer graph reasoning algorithms beyond shortest paths improve the clinical validity of the generated reasoning? The authors suggest moving beyond simple shortest paths to richer graph reasoning may improve explanation quality. This is unresolved as shortest paths were chosen to limit "overthinking," potentially pruning valid complex etiological chains.
- **Open Question 3**: Can the framework effectively integrate unstructured clinical notes and lab results to support more nuanced multimodal reasoning? The authors note that extending the framework to richer EHR views (notes, medications, labs) could support more nuanced multimodal reasoning. This remains unresolved as the current implementation is restricted to structured ICD-9 binary feature vectors.

## Limitations

- The framework's performance depends heavily on the completeness and quality of the underlying biomedical knowledge graph (PrimeKG), with gaps or biases propagating to generated explanations.
- The zero-shot transfer results, while impressive, are demonstrated only between MIMIC-III (ICU) and CRADLE (outpatient T2D); broader validation across diverse clinical settings and code systems is needed.
- Clinician preference evaluations were based on a relatively small sample (3 clinicians, 30 samples per disease), which may not capture the full spectrum of clinical scenarios or rare conditions.

## Confidence

- **High confidence**: The core mechanism of KG-guided CoT generation and label-consistency filtering is well-specified and technically sound. The reported AUROC and macro-AUPR improvements over baselines are statistically significant and reproducible given access to the same data.
- **Medium confidence**: The claim of superior clinician-preferred explanations is based on a small blinded evaluation (3 clinicians, 30 samples per disease). While promising, this sample size may not capture the full spectrum of clinical scenarios.
- **Medium confidence**: The zero-shot transfer results are impressive but depend on the specific pairing of MIMIC-III (ICU) and CRADLE (outpatient T2D). The mechanism suggests broader applicability, but this requires validation.

## Next Checks

1. **Ablation of label-consistency filtering**: Train on unfiltered CoT corpus and compare prediction metrics and clinician preference scores to the filtered version.
2. **Cross-code-system transfer**: Apply the MIMIC-trained model to an ICD-10 or SNOMED-CT dataset (e.g., eICU) with re-mapped clinical concepts to test robustness beyond the reported ICD-9 to PrimeKG mapping.
3. **Clinician evaluation expansion**: Scale the blinded clinician evaluation to 50+ samples per disease and include diverse clinical conditions (acute vs chronic, common vs rare) to assess consistency of preference across clinical scenarios.