---
ver: rpa2
title: 'ZeroGUI: Automating Online GUI Learning at Zero Human Cost'
arxiv_id: '2505.23762'
source_url: https://arxiv.org/abs/2505.23762
tags:
- task
- arxiv
- tasks
- event
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ZeroGUI addresses the challenge of training GUI agents by introducing
  an online learning framework that eliminates the need for human-labeled training
  data. The method leverages VLMs to automatically generate diverse training tasks
  and estimate task success rewards through voting-based evaluation of agent trajectories.
---

# ZeroGUI: Automating Online GUI Learning at Zero Human Cost

## Quick Facts
- arXiv ID: 2505.23762
- Source URL: https://arxiv.org/abs/2505.23762
- Reference count: 40
- Online learning framework eliminates human-labeled data for GUI agents

## Executive Summary
ZeroGUI introduces an online learning framework that enables GUI agents to train autonomously without human-labeled data. The method leverages VLMs to automatically generate diverse training tasks and estimate task success rewards through voting-based evaluation of agent trajectories. This approach addresses the scalability challenge in GUI agent training while maintaining adaptability to dynamic environments. When applied to UI-TARS and Aguvis agents across desktop and mobile platforms, ZeroGUI achieved significant performance improvements, with UI-TARS-7B-DPO showing 14% relative gains and Aguvis-7B demonstrating 63% relative improvements in task success rates.

## Method Summary
ZeroGUI employs a two-stage reinforcement learning approach with modified GRPO optimization. In Stage 1, agents train on VLM-generated tasks sampled from a pool of automatically created scenarios, with rewards estimated through unanimous voting across trajectory screenshots. Stage 2 applies test-time training on the actual test set. The method uses GPT-4o for task generation with example-guided prompting and Qwen2.5-VL-32B for reward estimation, explicitly excluding agent responses from reward consideration. Dynamic sampling (DAPO) filters tasks based on performance accuracy, while a two-epoch training schedule balances generated task training with test-time adaptation.

## Key Results
- UI-TARS-7B-DPO achieved 14% relative performance improvement in success rates
- Aguvis-7B demonstrated 63% relative improvement across test environments
- Two-stage RL strategy showed superior performance compared to single-stage approaches

## Why This Works (Mechanism)
ZeroGUI succeeds by creating a closed-loop training system where VLMs generate tasks and evaluate agent performance without human intervention. The two-stage approach first broadens capability coverage through diverse generated tasks, then refines behavior for specific test scenarios. The voting-based reward estimation provides more reliable feedback than single VLM predictions, while excluding agent responses prevents reward leakage. The method's effectiveness stems from addressing the data bottleneck in GUI agent training while maintaining the adaptability needed for dynamic interfaces.

## Foundational Learning

**VLM-based Reward Estimation**
- Why needed: Traditional reward engineering requires manual specification or human annotation, which is costly and non-scalable
- Quick check: Verify unanimous voting across 4 Qwen2.5-VL-32B queries produces consistent rewards

**Dynamic Task Sampling (DAPO)**
- Why needed: Prevents overfitting to easy tasks while maintaining training stability
- Quick check: Monitor task selection distribution; should exclude 0% and 100% accuracy tasks

**Two-stage RL Training**
- Why needed: Separates capability expansion from task-specific optimization
- Quick check: Compare performance when skipping either training stage

## Architecture Onboarding

**Component Map**
GPT-4o (Task Generation) -> Qwen2.5-VL-32B (Reward Estimation) -> GRPO (RL Training) -> GUI Agent

**Critical Path**
Task generation → Reward estimation → RL optimization → Agent deployment

**Design Tradeoffs**
- Voting vs. single VLM: Higher accuracy but increased computational cost
- Two-stage vs. single-stage: Better performance but more training time
- Generated vs. real tasks: Broader coverage but potential distribution mismatch

**Failure Signatures**
- KL gradient explosion: Max(log πθ − log πref) > 20 indicates need for k2-KL loss
- High false positives: Unanimous voting fails, requiring reward function adjustment
- Poor infeasible task detection: Success rate drops on full test set vs. feasible subset

**3 First Experiments**
1. Verify task generation pipeline produces diverse, feasible tasks
2. Test reward estimation consistency across multiple VLM queries
3. Run single-stage GRPO to establish baseline performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on single-task success rates, not multi-task coordination or error recovery
- Method relies heavily on VLM quality, potentially brittle for complex multi-step tasks
- AndroidLab validation appears less thorough with fewer generated tasks and limited performance breakdown

## Confidence

**High confidence:** Two-stage training methodology and basic implementation are well-specified and reproducible

**Medium confidence:** Reported performance improvements depend heavily on VLM quality and prompt engineering

**Medium confidence:** Zero-cost claim is technically accurate but may overstate practical applicability given VLM API costs

## Next Checks

1. Reproduce core results on OSWorld with UI-TARS-7B-DPO, comparing success rates before and after ZeroGUI training using provided GRPO parameters

2. Implement AndroidLab task generation pipeline independently to verify generated tasks maintain quality and diversity comparable to OSWorld

3. Conduct ablation studies removing either the generated task training stage or test-time training to quantify their individual contributions to performance gains