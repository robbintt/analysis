---
ver: rpa2
title: Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems
arxiv_id: '2512.04476'
source_url: https://arxiv.org/abs/2512.04476
tags:
- expert
- experts
- arxiv
- activation
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently running Mixture-of-Experts
  (MoE) models on GPU-NDP systems when model weights exceed GPU memory capacity. The
  authors propose a context-aware approach that leverages prefill-stage routing statistics
  to guide expert placement and quantization during decoding.
---

# Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems

## Quick Facts
- arXiv ID: 2512.04476
- Source URL: https://arxiv.org/abs/2512.04476
- Reference count: 40
- Primary result: 8.7× decoding throughput improvement over MoNDE system with 0.13% accuracy drop

## Executive Summary
This paper presents a context-aware inference framework for Mixture-of-Experts (MoE) models on GPU-NDP systems that addresses memory capacity constraints when model weights exceed GPU memory. The approach leverages prefill-stage routing statistics to guide expert placement and quantization during decoding, pinning hot experts in GPU HBM while executing cold experts in-place on CXL-NDP devices using mixed-precision quantization (1-4 bit). The method achieves significant throughput improvements while maintaining minimal accuracy degradation, making it particularly effective for large-scale MoE models like Mixtral-8x7B and Mixtral-8x22B.

## Method Summary
The authors propose a two-phase approach that collects routing statistics during the prefill stage and uses these statistics to inform expert placement and quantization during decoding. The system performs a single expert migration after prefill based on the correlation between prefill and decoding activation patterns. Cold experts are dynamically quantized to 1-4 bits and executed in-place on CXL-NDP devices, while hot experts remain in full precision in GPU HBM. The placement strategy minimizes expensive data migrations between CPU memory and GPU by leveraging CXL's low-latency, high-bandwidth interconnect. Expert importance is determined through context-aware analysis, allowing for adaptive bitwidth assignment that balances accuracy and performance.

## Key Results
- 8.7× decoding throughput improvement over state-of-the-art MoNDE system
- Only 0.13% average accuracy drop across evaluated benchmarks
- 67% reduction in expert migrations compared to existing approaches
- Effective handling of large MoE models including Mixtral-8x7B and Mixtral-8x22B

## Why This Works (Mechanism)
The approach exploits the strong correlation between prefill and decoding routing patterns to make informed placement decisions before the decoding phase begins. By leveraging CXL-NDP's low-latency interconnect, the system can execute quantized expert computations directly on the accelerator while maintaining hot experts in high-bandwidth GPU memory. The mixed-precision strategy allocates higher precision to critical experts while aggressively quantizing less important ones, optimizing the accuracy-performance tradeoff. The single-migration policy after prefill minimizes overhead while ensuring optimal placement for the majority of tokens.

## Foundational Learning
- **Mixture-of-Experts (MoE) Architecture**: Why needed - Understanding how MoE partitions computation across multiple specialized networks; Quick check - Verify top-k gating mechanism and expert activation patterns
- **CXL-NDP (Compute Express Link - Near Data Processing)**: Why needed - Understanding how accelerators can execute computations close to memory; Quick check - Confirm CXL bandwidth and latency characteristics
- **Expert Importance Scoring**: Why needed - Determining which experts are critical for maintaining model accuracy; Quick check - Validate correlation between expert activation frequency and downstream performance impact
- **Mixed-Precision Quantization**: Why needed - Balancing computational efficiency with numerical accuracy; Quick check - Verify calibration methodology for determining optimal bitwidth per expert
- **Routing Correlation Analysis**: Why needed - Establishing statistical relationships between prefill and decoding phases; Quick check - Measure cosine similarity across different model architectures
- **Single-Migration Policy**: Why needed - Minimizing overhead from expert placement changes; Quick check - Evaluate impact of multiple migration strategies on overall performance

## Architecture Onboarding

Component Map: Input Sequence -> Prefill Phase (Routing Stats Collection) -> Expert Placement Decision -> Decoding Phase (Execution with Mixed-Precision)

Critical Path: Token generation → Prefill execution → Routing statistics analysis → Expert migration (if needed) → Decoding execution with quantized cold experts and full-precision hot experts

Design Tradeoffs: The system trades a single expensive migration at prefill completion against potentially multiple smaller migrations during decoding. Mixed-precision quantization reduces memory bandwidth requirements but introduces computational overhead for dequantization. The context-aware approach assumes routing stability across phases, which may not hold for all workloads.

Failure Signatures: Degraded performance if routing correlation breaks down; Accuracy loss if quantization bitwidth selection is suboptimal; Memory thrashing if expert access patterns are highly volatile; CXL bandwidth saturation under high expert migration rates.

First Experiments:
1. Measure routing correlation (cosine similarity) between prefill and decoding phases for target model architecture
2. Benchmark CXL-NDP device latency and bandwidth under realistic expert access patterns
3. Evaluate accuracy degradation across different quantization bitwidths for representative experts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the prefill-to-decoding routing correlation (0.89 cosine similarity observed for Mixtral-8×7B) generalize across diverse MoE architectures with different router designs (e.g., DeepSeek, Qwen-MoE, OLMoE)?
- Basis in paper: [inferred] The empirical analysis in Section 3.2 and Figure 4 validates the prefill-decoding similarity only for Mixtral-8×7B on TruthfulQA. No results are provided for other MoE architectures that may employ different gating mechanisms or top-k selection strategies.
- Why unresolved: The entire placement strategy hinges on this correlation. If it weakens for other architectures, the single-migration-after-prefill policy may lead to suboptimal expert placement.
- What evidence would resolve it: Cosine similarity measurements between prefill and decoding activation distributions across multiple MoE model families and diverse downstream tasks.

### Open Question 2
- Question: How does the context-aware placement and quantization strategy perform under batched inference with multiple concurrent sequences?
- Basis in paper: [inferred] All evaluations (Figures 5–6, Table 3) report single-sequence latency and throughput. The system design (Algorithm 1) processes one sequence at a time, collecting per-sequence prefill statistics.
- Why unresolved: Production serving systems typically batch requests. Concurrent sequences may have conflicting expert importance rankings, complicating GPU memory allocation and bitwidth assignment.
- What evidence would resolve it: End-to-end latency and throughput measurements under multi-sequence batched inference with varying batch sizes and sequence length distributions.

### Open Question 3
- Question: How robust is the Expert Bitwidth Selector to the choice and size of the calibration dataset used for constructing the per-expert loss table?
- Basis in paper: [explicit] Section 5.3 states: "we use 1024 samples from the C4 dataset as the calibration set for constructing the loss table." No ablation or sensitivity analysis is provided regarding calibration data composition or scale.
- Why unresolved: If the loss table poorly estimates expert sensitivity on out-of-distribution inputs, the mixed-precision allocation may introduce larger accuracy degradation than the reported 0.13%–3.4%.
- What evidence would resolve it: Ablation studies varying calibration dataset size (e.g., 256, 512, 2048 samples) and source (e.g., domain-specific corpora) while measuring downstream task accuracy.

## Limitations
- Performance comparisons lack detailed baseline configurations and system parameters
- Accuracy degradation claims insufficiently validated across diverse datasets and model configurations
- Context-aware routing assumes routing pattern stability that may not generalize to all workloads
- Mixed-precision quantization strategy lacks comprehensive quality-of-service analysis
- Evaluation focuses on synthetic workloads without real-world deployment scenarios

## Confidence

**Performance claims (8.7× improvement): Medium** - Results are promising but limited evaluation scope and lack of ablation studies reduce confidence
**Accuracy preservation (0.13% drop): Medium** - Claim appears reasonable but insufficient validation across diverse scenarios
**Context-aware routing effectiveness: Medium** - Methodology seems sound but real-world applicability remains uncertain

## Next Checks

1. Conduct ablation studies comparing different routing statistics collection intervals and their impact on accuracy and performance across varying sequence lengths
2. Evaluate system behavior under realistic production workloads with non-uniform expert access patterns and varying batch sizes
3. Measure end-to-end quality-of-service metrics including latency percentiles and tail latency behavior under stress conditions