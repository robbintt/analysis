---
ver: rpa2
title: 'Dr.LLM: Dynamic Layer Routing in LLMs'
arxiv_id: '2510.12773'
source_url: https://arxiv.org/abs/2510.12773
tags:
- accuracy
- layers
- routing
- routers
- router
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dr.LLM introduces lightweight per-layer routers that decide to
  skip, execute, or repeat each transformer block in a frozen pretrained LLM. The
  routers are trained on explicit supervision derived from Monte Carlo Tree Search
  (MCTS), which finds accuracy-preserving or improving layer configurations under
  a compute budget.
---

# Dr.LLM: Dynamic LayerRouting in LLMs

## Quick Facts
- arXiv ID: 2510.12773
- Source URL: https://arxiv.org/abs/2510.12773
- Reference count: 37
- Key result: Improves reasoning accuracy by up to 4.0 percentage points while saving up to 11 layers per example on ARC and DART benchmarks

## Executive Summary
Dr.LLM introduces lightweight per-layer routers that dynamically decide whether to skip, execute, or repeat each transformer block in a frozen pretrained LLM during inference. The routers are trained using explicit supervision derived from Monte Carlo Tree Search (MCTS), which finds accuracy-preserving or improving layer configurations under a compute budget. This approach achieves significant efficiency gains while maintaining or improving accuracy on reasoning tasks, with the ability to generalize to out-of-domain benchmarks while retaining most of the efficiency benefits.

## Method Summary
The method employs per-layer lightweight MLPs as routers that process mean-pooled hidden states from each transformer block to make execution decisions. These routers are trained using MCTS-generated supervision that identifies optimal layer execution patterns under a given compute budget. The training uses focal loss with class balancing to handle severe class imbalance between skip, execute, and repeat decisions. The approach is model-agnostic and works by freezing the base LLM weights while only training the routing components, enabling efficient inference without modifying the original model architecture.

## Key Results
- Improves accuracy by up to 4.0 percentage points while saving up to 11 layers per example on ARC and DART reasoning tasks
- Generalizes to out-of-domain benchmarks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% average accuracy drop while retaining efficiency
- Outperforms prior routing methods by up to 7.7 percentage points on reasoning tasks
- Shows structured routing patterns: early layers consistently executed, middle layers frequently skipped, late layers often repeated on complex reasoning tasks

## Why This Works (Mechanism)
The method works by learning optimal layer execution patterns through supervised routing. MCTS exploration finds accuracy-preserving layer configurations under compute constraints, providing explicit supervision for router training. The mean-pooling over hidden states captures global context for routing decisions, while focal loss handles the severe class imbalance between skip, execute, and repeat actions. The per-layer routers make independent decisions that collectively optimize the accuracy-efficiency trade-off for each input example.

## Foundational Learning
- **Monte Carlo Tree Search (MCTS)**: A search algorithm that balances exploration and exploitation to find optimal decision paths. Needed to generate accurate routing supervision without requiring manual labeling. Quick check: Verify MCTS exploration-exploitation balance through search tree depth and node visit statistics.
- **Focal Loss**: A modified cross-entropy loss that down-weights well-classified examples and focuses on hard negatives. Needed to handle severe class imbalance where skip decisions vastly outnumber execute/repeat. Quick check: Monitor class-specific loss contributions during training.
- **Mean Pooling over Hidden States**: Aggregation technique that captures global context across sequence positions. Needed to provide routers with comprehensive input representation for routing decisions. Quick check: Compare routing accuracy using different pooling strategies (mean, max, attention).
- **Per-layer Routing**: Independent decision-making at each transformer block. Needed to enable fine-grained control over execution patterns. Quick check: Analyze routing consistency across layers for similar inputs.
- **Class Balancing**: Technique to handle imbalanced training data by adjusting class weights. Needed to prevent routers from defaulting to majority skip decisions. Quick check: Verify balanced class representation in training loss.
- **Frozen Base Model**: Approach where pretrained model weights remain unchanged during routing training. Needed to preserve base model capabilities while adding efficiency. Quick check: Compare base model performance before and after routing training.

## Architecture Onboarding

**Component Map**
Base LLM -> Per-layer Routers -> Layer Execution Controller -> Output

**Critical Path**
Input hidden state → Mean pooling → Router MLP → Execution decision (skip/execute/repeat) → Next layer

**Design Tradeoffs**
- Router complexity vs. decision quality: Two-layer MLPs provide sufficient accuracy while minimizing overhead
- MCTS search depth vs. supervision quality: Deeper searches yield better routing patterns but increase training time
- Class imbalance handling vs. model simplicity: Focal loss addresses imbalance without complex sampling strategies

**Failure Signatures**
- Routers consistently choosing skip may indicate insufficient exploration during MCTS training
- Poor generalization to out-of-domain tasks suggests routing patterns are too task-specific
- High variance in layer savings across examples indicates unstable routing decisions

**3 First Experiments**
1. Ablation study removing focal loss to measure impact of class imbalance handling
2. Comparison of different pooling strategies (mean, max, attention) on routing accuracy
3. Evaluation of router performance with varying MCTS search budgets

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on computationally intensive MCTS supervision generation that may not scale efficiently to larger models
- Assumes fixed per-example budget that may not be optimal for heterogeneous workloads
- Evaluation focused on specific model family and reasoning tasks, with limited evidence on generation-heavy workloads

## Confidence
- High confidence in core methodology and empirical results on evaluated benchmarks
- Medium confidence in generalizability to other model sizes and task types
- Low confidence in scalability of MCTS supervision generation to industrial-scale deployments

## Next Checks
1. Evaluate Dr.LLM on a broader range of model sizes (e.g., 7B, 13B parameters) and architectures to assess scalability
2. Test the routers on generation-heavy tasks (such as long-form QA or summarization) to determine if routing patterns extend to open-ended generation
3. Measure the wall-clock time and memory overhead of the MCTS supervision generation process across different dataset sizes to quantify practical feasibility