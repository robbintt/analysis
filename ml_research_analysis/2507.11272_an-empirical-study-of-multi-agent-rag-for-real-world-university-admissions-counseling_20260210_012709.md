---
ver: rpa2
title: An Empirical Study of Multi-Agent RAG for Real-World University Admissions
  Counseling
arxiv_id: '2507.11272'
source_url: https://arxiv.org/abs/2507.11272
tags:
- university
- retrieval
- maraus
- systems
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents MARAUS, a multi-agent RAG system for university
  admissions counseling in Vietnam. It addresses the limitations of LLM-only chatbots
  by integrating hybrid retrieval (BM25 + semantic), multi-agent orchestration, and
  LLM-based re-ranking to handle complex, domain-specific queries.
---

# An Empirical Study of Multi-Agent RAG for Real-World University Admissions Counseling

## Quick Facts
- arXiv ID: 2507.11272
- Source URL: https://arxiv.org/abs/2507.11272
- Authors: Anh Nguyen-Duc; Chien Vu Manh; Bao Anh Tran; Viet Phuong Ngo; Luan Le Chi; Anh Quang Nguyen
- Reference count: 20
- Primary result: MARAUS achieved 92% accuracy, reduced hallucination rates from 15% to 1.45%, and processed 6,000+ queries with <4s response time in real-world university admissions counseling

## Executive Summary
MARAUS is a multi-agent RAG system designed to handle complex university admissions counseling queries in Vietnam. The system addresses limitations of standard LLM chatbots by combining hybrid retrieval (BM25 + semantic), multi-agent orchestration, and LLM-based re-ranking to manage domain-specific, high-stakes queries. Deployed in a real-world setting, it processed over 6,000 user interactions across six query types with high accuracy and low hallucination rates. The system demonstrates that domain-specialized multi-agent RAG can effectively handle complex educational queries at low cost and with practical deployment considerations.

## Method Summary
MARAUS implements a multi-agent coordinator that classifies incoming queries and routes them to specialized agents. The system uses hybrid retrieval combining FAISS (dense) and ElasticSearch BM25 (sparse) to retrieve relevant passages, followed by GPT-4o mini cross-encoder re-ranking to filter top candidates. A citation validation post-processor enforces grounding by requiring passage citations in generated responses. The knowledge corpus consists of 1,376 FAQ pairs and 98 document chunks processed through Vietnamese-specific preprocessing (VnCoreNLP tokenization, chunking at 500 tokens with 100-token stride), embedded with all-mpnet-base-v2.

## Key Results
- Achieved 92% accuracy across six query types in real-world deployment
- Reduced hallucination rates from 15% to 1.45% through citation enforcement
- Maintained response times under 4 seconds while processing over 6,000 interactions
- Cost-effective deployment at $11.58 USD for two weeks of operation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating sparse (BM25) and dense (semantic) retrieval, followed by an LLM-based cross-encoder re-ranker, improves context precision compared to single-method retrieval.
- **Mechanism:** BM25 ensures high recall for specific acronyms or exact policy terms (e.g., "KV1 area"), while semantic search captures conceptual matches. The LLM-based re-ranker then scores these candidates jointly with the query to filter out topically related but contextually irrelevant passages before generation.
- **Core assumption:** The re-ranking LLM possesses sufficient domain understanding to distinguish between superficially similar but operationally distinct policy chunks.
- **Evidence anchors:**
  - [Section 4.3]: "The union of BM25 and FAISS outputs is passed to a hybrid re-ranking stage... GPT-4o mini assigns a normalized relevance score... reduces false positives by 38%."
  - [Section 4.5, Table 2]: Hybrid RAG achieves 0.985 precision, outperforming RAG+Re-rank (0.90) and LLM-only (0.70).
  - [Corpus]: *URAG* (Neighbor 13646) confirms the efficacy of unified hybrid RAG in similar university admission contexts.
- **Break condition:** If a user query uses vocabulary completely absent from the corpus (OOV) and lacks semantic overlap, both BM25 and dense retrieval may fail to retrieve relevant candidates, causing the re-ranker to score garbage inputs.

### Mechanism 2
- **Claim:** Multi-agent orchestration with specialized routing improves handling of composite tasks (e.g., score calculation) that require logic rather than just text retrieval.
- **Mechanism:** A central coordinator classifies query intent. Calculation queries are routed to a "Score Calculation Agent" which extracts structured attributes to execute arithmetic logic (e.g., transcript points + priority bonuses), whereas informational queries route to standard RAG pipelines.
- **Core assumption:** The intent classifier can accurately disambiguate between a request for a definition ("What is a KV1 area?") and a calculation ("I am in KV1, what is my score?").
- **Evidence anchors:**
  - [Section 4.1]: "The core of MARAUS is a multi-agent coordinator that classifies incoming queries... Score calculation agent: Handles numeric computation."
  - [Section 5.1]: Identifies "Logical Reasoning and Calculation" as a distinct query type handled by the system.
  - [Corpus]: *KIMAs* (Neighbor 15611) supports the use of configurable multi-agent systems for knowledge-intensive conversations.
- **Break condition:** If a query is ambiguous (e.g., mentioning a score but asking for policy implications), misrouting to the Information Agent may yield a generic explanation rather than the specific calculation the user intended.

### Mechanism 3
- **Claim:** Enforcing explicit citation of retrieved passage IDs in the final output significantly reduces hallucination rates.
- **Mechanism:** A post-processor validates the presence of passage citations in the generated text. If citations are missing—indicating the model may be relying on parametric knowledge rather than context—the system discards the output and regenerates it with penalized decoding parameters.
- **Core assumption:** The presence of a citation ID correlates strongly with the response being grounded in the provided context, and the LLM can be forced to comply via prompt/penalty.
- **Evidence anchors:**
  - [Section 4.4]: "If generated answers lack at least one passage citation, they are discarded... reduces hallucination rates from 15% (LLM-only) to 1.45%."
  - [Abstract]: Mentions hallucination rates reduced from 15% to 1.45%.
  - [Corpus]: Direct corpus evidence for this specific *citation enforcement loop* is weak in the provided neighbor abstracts (most focus on general RAG or multi-agent structures).
- **Break condition:** If the retrieval stage returns a "distractor" chunk that is semantically similar but factually incorrect, the LLM may cite it confidently, resulting in a "grounded hallucination."

## Foundational Learning

- **Concept:** Hybrid Search (BM25 + Dense Retrieval)
  - **Why needed here:** University admissions contain strict terminology (codes, proper nouns) where semantic similarity fails, but also complex phrasing where keywords fail.
  - **Quick check question:** Can you explain why a query about "Region 1 priority" might fail in a purely vector-based search?

- **Concept:** LLM Cross-Encoding / Re-ranking
  - **Why needed here:** Vector similarity is often too coarse for high-stakes Q&A; re-ranking uses the LLM's attention mechanism to deeply compare the query against the specific retrieved chunk.
  - **Quick check question:** Why is re-ranking treated as a separate step from the initial retrieval, rather than just increasing `k` in the vector search?

- **Concept:** Agentic Routing (Query Classification)
  - **Why needed here:** A single RAG pipeline cannot efficiently handle both "What is the tuition?" (retrieval) and "Calculate my total score" (computation/logic).
  - **Quick check question:** What is the failure mode if a calculation query is sent to a standard RAG generator without tool use or structured logic?

## Architecture Onboarding

- **Component map:** Ingestion (Text cleaning → VnCoreNLP tokenization → Chunking) → Indexing (FAISS + ElasticSearch) → Runtime (Multi-Agent Coordinator → Specific Agent → Hybrid Retriever → LLM Re-ranker → Generator → Citation Validator)
- **Critical path:** The **Re-ranking and Citation Validation** loop is the primary defense against misinformation. If the re-ranker passes irrelevant context, or the validator fails to enforce grounding, the system outputs hallucinations.
- **Design tradeoffs:**
  - **Latency vs. Accuracy:** The paper reports <4s response time, but this involves sequential steps (Retrieval → Re-ranking → Generation). The Re-ranking step specifically adds LLM inference overhead.
  - **Chunk Size:** 500 tokens were chosen to balance context retention with LLM window constraints (Section 4.2). Smaller chunks might lose policy context; larger chunks might introduce noise.
- **Failure signatures:**
  - **"I don't know" loops:** If the Citation Validator is too strict and retrieval is weak, the system may repeatedly refuse to answer.
  - **Citation Hallucinations:** The model invents a citation ID that looks real but isn't in the database (less likely with explicit prompting, but possible).
- **First 3 experiments:**
  1. **Retrieval Ablation:** Disable BM25 or Semantic search individually on a test set of 100 queries to measure the marginal precision gain of the hybrid approach.
  2. **Re-ranking Threshold Tuning:** Vary the relevance score cutoff (currently normalized 0-1) for the LLM re-ranker to find the optimal balance between recall and processing time.
  3. **Intent Classifier Stress Test:** Feed ambiguous queries (mixing calculation and definition) to the Multi-Agent Coordinator to measure routing accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- System effectiveness relies heavily on domain-specific knowledge curation from a single institution, limiting generalizability
- Multi-agent routing mechanism lacks detailed specification of the intent classification model and thresholds
- Citation validation approach may not scale well to domains where passage IDs are less stable

## Confidence
- **High Confidence:** Hybrid retrieval mechanism and its measured impact on precision (0.985 vs 0.70 for LLM-only) is well-supported by ablation testing
- **Medium Confidence:** Multi-agent orchestration claims are supported by system architecture but lack detailed validation of routing accuracy
- **Low Confidence:** Hallucination reduction mechanism's effectiveness depends on undocumented implementation details of the citation validator

## Next Checks
1. **Retrieval Ablation Study:** Disable individual retrieval components (BM25, semantic, re-ranking) on 100 held-out queries to measure marginal precision gains
2. **Multi-Agent Routing Stress Test:** Evaluate the intent classifier on ambiguous queries that mix calculation and informational elements
3. **Citation Validation Scalability Test:** Assess the citation validator's performance on a larger, more diverse corpus