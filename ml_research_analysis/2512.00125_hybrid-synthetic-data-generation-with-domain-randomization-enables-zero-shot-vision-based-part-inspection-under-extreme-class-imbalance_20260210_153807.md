---
ver: rpa2
title: Hybrid Synthetic Data Generation with Domain Randomization Enables Zero-Shot
  Vision-Based Part Inspection Under Extreme Class Imbalance
arxiv_id: '2512.00125'
source_url: https://arxiv.org/abs/2512.00125
tags:
- data
- manufacturing
- images
- fail
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid synthetic data generation (SDG) framework
  that integrates simulation-based rendering, domain randomization, and real background
  compositing to enable zero-shot learning for industrial part inspection. The approach
  addresses the challenge of training machine learning models in manufacturing environments
  where defective samples are intrinsically rare, leading to severe class imbalance.
---

# Hybrid Synthetic Data Generation with Domain Randomization Enables Zero-Shot Vision-Based Part Inspection Under Extreme Class Imbalance

## Quick Facts
- arXiv ID: 2512.00125
- Source URL: https://arxiv.org/abs/2512.00125
- Authors: Ruo-Syuan Mei; Sixian Jia; Guangze Li; Soo Yeon Lee; Brian Musser; William Keller; Sreten Zakula; Jorge Arinez; Chenhui Shao
- Reference count: 40
- Primary result: SDG-based approach achieves 90-91% balanced accuracy under severe class imbalance while baselines reach 50% accuracy

## Executive Summary
This paper presents a hybrid synthetic data generation (SDG) framework that combines simulation-based rendering with domain randomization and real background compositing to enable zero-shot learning for industrial part inspection. The approach addresses the challenge of training machine learning models in manufacturing environments where defective samples are intrinsically rare. By generating 12,960 labeled images in one hour through systematic variation of part geometry, lighting, and surface properties, then compositing synthetic parts onto real backgrounds, the framework achieves high performance without requiring real defect data during training.

The two-stage architecture uses YOLOv8n for object detection and MobileNetV3-small for quality classification, trained exclusively on synthetic data. Evaluation on 300 real industrial parts shows mAP@0.5 of 0.995 for detection, 96% classification accuracy, and 90.1% balanced accuracy. Comparative evaluation against few-shot real-data baseline approaches demonstrates significant improvement, with the SDG-based approach maintaining 90-91% balanced accuracy under severe class imbalance while baselines reach only 50% accuracy.

## Method Summary
The hybrid SDG framework integrates simulation-based rendering with domain randomization and real background compositing. Synthetic parts are rendered in Blender 4.3.2 using EEVEE, with domain randomization applied to bending angle (8 configurations), light power (3 levels), and surface roughness (3 levels). These 72 unique part configurations per class are then composited onto real backgrounds extracted from inspection station images using a vision-language model. Classical augmentations (rotation, blur, brightness) are applied during compositing. The two-stage architecture consists of YOLOv8n for object detection and MobileNetV3-small for binary quality classification, both trained on synthetic data only and evaluated on real industrial parts.

## Key Results
- mAP@0.5 of 0.995 for object detection on real parts
- 96% classification accuracy and 90.1% balanced accuracy for quality assessment
- SDG-based approach maintains 90-91% balanced accuracy under severe class imbalance while few-shot baselines reach only 50% accuracy
- 12,960 synthetic images generated in one hour with 1:1 pass/fail ratio

## Why This Works (Mechanism)

### Mechanism 1: Domain Randomization Over Environmental Factors
Systematic variation across bending angle, light power, and surface roughness creates a distribution of training conditions that forces the network to learn robust features rather than simulation-specific cues. The 4×3×3 factorial design yields 72 unique part configurations per class, enabling models to learn invariance to these factors rather than overfitting to simulation artifacts.

### Mechanism 2: Real Background Compositing for Sim-to-Real Transfer
Vision-language models extract backgrounds from real inspection station images, and synthetic parts are composited onto these real backgrounds rather than synthetic environments. This approach preserves authentic environmental textures, shadows, and clutter while maintaining geometric control, substantially reducing the domain gap between simulation and reality.

### Mechanism 3: Balanced Synthetic Dataset Generation
The pipeline generates equal pass and fail images (1:1 ratio) regardless of real-world rarity, countering class imbalance by creating balanced training data. This forces the classifier to allocate equal representational capacity to both classes during training, enabling effective learning of discriminative features for the minority class.

## Foundational Learning

- **Domain Randomization**: This core technique enables zero-shot transfer by creating synthetic training distributions that span real-world variability. If real conditions fall outside randomization ranges, performance degrades.
  - Quick check: If your real environment has lighting from angles 0–45°, should your randomization cover 0–45° or a wider range? Why?

- **Two-Stage Detection-Classification Pipeline**: Separating localization (YOLOv8n) from quality assessment (MobileNetV3-small) enables specialized optimization for each task. This decoupling matters because detection requires geometric precision while classification needs discriminative feature learning.
  - Quick check: Why train the classifier on ground-truth cropped regions rather than detections from the first stage during training?

- **Balanced Accuracy vs. Raw Accuracy Under Class Imbalance**: The paper emphasizes balanced accuracy (90.1%) over raw accuracy (96%) because the 11:1 pass/fail ratio makes raw accuracy misleading. A model that predicts "pass" for everything achieves 91.7% raw accuracy but 50% balanced accuracy.
  - Quick check: In a 20:1 pass/fail dataset, a model achieves 95% raw accuracy. Is this model useful? What additional metric do you need?

## Architecture Onboarding

- **Component map**: CAD import → mesh modification (defect simulation) → domain randomization parameter setup → rendering → background extraction → compositing → annotation → YOLOv8n training → MobileNetV3 training → real-world evaluation

- **Critical path**: The pipeline follows a linear flow from CAD model through rendering and compositing to the two-stage classification system, with each component building on the previous one.

- **Design tradeoffs**:
  - EEVEE vs. Cycles: EEVEE chosen for 60× speed advantage; tradeoff is lower photorealism
  - YOLOv8n vs. larger models: Chosen for edge deployment (7.6ms inference); tradeoff is potential accuracy ceiling
  - Binary vs. multi-class: Binary simplifies problem but loses defect severity information

- **Failure signatures**:
  - Type I errors (false alarms): Parts near decision boundary (bending angles approaching specification limits) → indicates randomization may not fully cover marginal cases
  - Type II errors (missed defects): Extreme specular reflection → indicates lighting randomization ranges insufficient for real conditions
  - Detection failure: COCO-pretrained baseline achieved only 0.060 precision → indicates domain gap without fine-tuning

- **First 3 experiments**:
  1. Validate rendering realism: Render known parts and compare side-by-side with real images. Check if lighting, shadows, and surface textures match production conditions. Adjust randomization ranges if systematic differences appear.
  2. Detection-only baseline: Train YOLOv8n on synthetic data and evaluate on a small held-out real set (n=30–50) before building the classifier. If mAP@0.5 < 0.90, debug the compositing pipeline before proceeding.
  3. Class imbalance stress test: Evaluate classifier on artificially imbalanced validation sets (5:1, 10:1, 20:1 pass/fail ratios) to verify balanced accuracy remains stable. If performance degrades, inspect whether synthetic defect variety matches real defect modes.

## Open Questions the Paper Calls Out

### Open Question 1: Generalization to Diverse Parts
The authors identify validating the SDG pipeline on diverse parts and manufacturing processes as a key area for future work, noting the current validation is limited to a single automotive bracket. The experimental scope was restricted to one specific part geometry, leaving performance on other manufacturing objects unknown. What evidence would resolve it: Benchmark results from applying the pipeline to multiple distinct manufacturing parts with varying material properties and imaging hardware.

### Open Question 2: Comparison with Domain Adaptation Techniques
The authors propose comprehensive evaluation against other domain adaptation techniques as a limitation, as the study primarily compared the method against few-shot baselines. The methodological comparison omitted other standard sim-to-real transfer strategies, making it difficult to situate the method's efficiency relative to the broader field. What evidence would resolve it: Comparative experimental results pitting the SDG model against domain-adversarial or self-supervised baselines using the same evaluation datasets.

### Open Question 3: Active Learning Integration
The authors propose investigating active learning approaches where misclassified real samples inform updates to the domain randomization and adaptation strategy. The current pipeline relies on fixed randomization ranges that fail to capture the high-intensity specular reflections identified in the error analysis as a source of Type II misclassifications. What evidence would resolve it: A demonstrated reduction in Type II errors when the simulation parameters are dynamically updated based on real-world failure cases.

## Limitations

- Absence of detailed training hyperparameters (epochs, learning rates, batch sizes, weight decay) for both YOLOv8n and MobileNetV3-small, making exact reproduction challenging
- Domain randomization strategy shows vulnerabilities when real inspection conditions fall outside the randomized parameter ranges, particularly with extreme specular reflections
- The synthetic defects may not capture all real defect modes, and the paper doesn't analyze whether synthetic defect distributions match production defect distributions

## Confidence

- **High Confidence**: The core finding that synthetic data generation with domain randomization enables zero-shot transfer for industrial inspection under class imbalance (90.1% balanced accuracy vs. 50% for few-shot baselines)
- **Medium Confidence**: The claim that real background compositing significantly reduces domain gap (lacks quantitative contribution analysis)
- **Medium Confidence**: The assertion that balanced synthetic datasets fully compensate for real-world class imbalance (synthetic defects may not match real defect distributions)

## Next Checks

1. **Parameter Coverage Validation**: Systematically test the classifier on real parts exhibiting lighting conditions outside the 5–15W randomization range. Document whether performance degrades predictably and whether expanding randomization ranges improves results.

2. **Component Ablation Study**: Reproduce the pipeline with synthetic backgrounds instead of real backgrounds while keeping all other parameters constant. Measure the change in mAP@0.5 and balanced accuracy to quantify the contribution of real background compositing.

3. **Defect Distribution Analysis**: Compare synthetic defect characteristics (surface roughness, bending angles) against actual production defects using statistical measures. If synthetic defects systematically differ from real defects, adjust the randomization parameters to better match production defect distributions.