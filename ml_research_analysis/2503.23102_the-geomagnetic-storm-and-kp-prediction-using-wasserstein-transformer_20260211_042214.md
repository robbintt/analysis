---
ver: rpa2
title: The geomagnetic storm and Kp prediction using Wasserstein transformer
arxiv_id: '2503.23102'
source_url: https://arxiv.org/abs/2503.23102
tags:
- magnetic
- data
- field
- geomagnetic
- solar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accurately forecasting geomagnetic
  activity, specifically the 3-day and 5-day planetary Kp index, which is crucial
  for monitoring space weather and mitigating its impacts on technological systems.
  The proposed method is a novel multimodal Transformer-based framework that integrates
  heterogeneous data sources, including satellite measurements, solar images, and
  Kp time series.
---

# The geomagnetic storm and Kp prediction using Wasserstein transformer

## Quick Facts
- arXiv ID: 2503.23102
- Source URL: https://arxiv.org/abs/2503.23102
- Reference count: 0
- One-line primary result: Novel multimodal Transformer framework with Wasserstein alignment achieves performance comparable to NOAA's operational Kp forecast, accurately capturing both quiet and storm phases.

## Executive Summary
This paper introduces a multimodal Transformer-based framework for predicting the planetary Kp geomagnetic index 3 and 5 days ahead. The model integrates solar images, satellite plasma/magnetic parameters, and historical Kp data, using Wasserstein distance to align probability distributions across modalities. An auxiliary high-Kp branch improves storm detection, and online fine-tuning ensures responsiveness to temporal dynamics. Comparative experiments show performance comparable to NOAA's operational model, accurately capturing both quiet and storm phases.

## Method Summary
The model processes SDO/AIA 193Å solar images through a Swin Transformer, satellite measurements through a normalized Transformer encoder, and Kp time series through another encoder. Image features are row-normalized then PCA-reduced to 512 dimensions. A 40-timestep input window (5 days) predicts 24 output steps (3 days) at 3-hour resolution. Three modality-specific Transformer encoders produce probability distributions over fixed bins, with Wasserstein distance regularization aligning these distributions. The fused features pass through local and global branches before four classification heads (28-class, 10-class, 3-class Kp, and binary high-Kp ≥7). The combined loss includes cross-entropy and Wasserstein terms. Daily online fine-tuning uses sliding windows of only past data to prevent lookahead contamination.

## Key Results
- Achieves performance comparable to NOAA's operational Kp forecast model
- Successfully captures both quiet and storm phases of geomagnetic activity
- Accurately predicts Kp levels during multiple storm events (May 8, 11-15, 2024)
- Binary high-Kp branch improves detection of extreme storm conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wasserstein distance aligns probability distributions across heterogeneous modalities, improving forecast consistency.
- Mechanism: Each modality-specific Transformer encoder projects to a shared probability space over fixed bins. The 1D discrete Wasserstein distance is computed between cumulative distribution functions of paired modalities (image-satellite, satellite-Kp) and added as a regularization term to the loss.
- Core assumption: Modalities observing the same physical phenomenon should produce aligned predictive distributions when properly fused.
- Evidence anchors:
  - [abstract] "incorporation of the Wasserstein distance into the transformer and the loss function to align the probability distributions across modalities"
  - [section 2.4] Defines W₁(p,q) = Σ|Fₚ(k) − F_q(k)| and describes projection via softmax over bins
  - [corpus] Limited direct precedent; no neighbor papers explicitly apply Wasserstein alignment to multimodal space weather forecasting
- Break condition: If modalities carry fundamentally different temporal horizons (e.g., solar images represent conditions hours before satellite wind arrivals), distribution alignment may force spurious consistency and degrade accuracy.

### Mechanism 2
- Claim: Multimodal Transformer fusion captures complementary physical signals for extended-horizon Kp prediction.
- Mechanism: Three parallel Transformer encoders process image features (Swin-extracted, PCA-reduced to 512D), normalized satellite plasma/magnetic parameters, and raw Kp time series. Outputs are concatenated, then passed through local (Conv1D) and global (attention + pooling) branches before multi-head classification.
- Core assumption: Solar imagery provides early storm signatures, satellite data captures near-Earth conditions, and Kp history encodes recent geomagnetic state—all jointly informative.
- Evidence anchors:
  - [abstract] "integrating heterogeneous data sources, including satellite measurements, solar images, and Kp time series"
  - [section 2.4] Full architecture diagram showing three input branches, projection layers, Wasserstein loss, and fusion outputs
  - [corpus] Related work (Wang et al. 2023, Alobaid et al. 2022) combines images and solar wind for geomagnetic/CME prediction, supporting multimodal benefit
- Break condition: If any modality has systematic missingness or lag (e.g., image gaps during key storm onset), the fusion may amplify noise unless imputation or masking is explicitly handled.

### Mechanism 3
- Claim: Auxiliary high-Kp branch plus online fine-tuning improves detection of extreme storm events and adapts to temporal drift.
- Mechanism: A binary sigmoid output is trained with binary cross-entropy to flag Kp ≥ 7. Separately, the model is fine-tuned daily using a sliding window of only past data, ensuring no lookahead contamination before multi-day forecasts.
- Core assumption: Storm events are rare; explicit supervision and continual adaptation counteract class imbalance and concept drift.
- Evidence anchors:
  - [abstract] "auxiliary high Kp branch for improved detection of storm conditions" and "continuously fine-tuned using recent historical data"
  - [section 2.6] Details online fine-tuning with past-only windows and multi-day prediction cropping strategy
  - [corpus] Early Prediction of Geomagnetic Storms paper (2024) emphasizes class imbalance challenges; no neighbor explicitly validates daily fine-tuning for Kp
- Break condition: If fine-tuning windows are too short or hyperparameters (epochs, learning rate) are unstable, the model may overfit to recent noise and lose generalization.

## Foundational Learning

- Concept: Wasserstein Distance (Earth Mover's Distance)
  - Why needed here: Quantifies distribution alignment between modalities; central to the proposed loss.
  - Quick check question: Given two probability vectors p = [0.7, 0.3] and q = [0.4, 0.6], compute their 1D discrete Wasserstein distance using cumulative sums.

- Concept: Transformer Encoder (Multi-Head Attention, Positional Handling)
  - Why needed here: Encodes temporal dependencies across modalities before fusion.
  - Quick check question: Explain why self-attention may struggle with very long input windows (e.g., 40 timesteps × multiple modalities) and name one common mitigation.

- Concept: Class Imbalance and Oversampling in Time Series
  - Why needed here: Kp distribution is skewed toward quiet periods; the paper applies data expansion.
  - Quick check question: Why does naive random oversampling risk information leakage in sliding-window time series, and how does the paper's method avoid it?

## Architecture Onboarding

- Component map:
  SDO/AIA 193Å images → Swin Transformer → PCA(512D) → Transformer encoder
  Satellite parameters → normalization → Transformer encoder
  Kp index → normalization → Transformer encoder
  Three encoder outputs → projection layers → Wasserstein alignment → concatenation
  Concatenated features → local branch (Conv1D + residual) + global branch (attention + pooling)
  Global features → four classification heads (28-class, 10-class, 3-class, binary)

- Critical path: Image quality check → Swin feature extraction → PCA → Transformer encoder → probability projection → Wasserstein alignment loss → fusion → multi-head outputs → combined loss (CE + λ·Wasserstein). Any failure in quality check or projection destabilizes alignment and final predictions.

- Design tradeoffs:
  - Wasserstein alignment vs. simplicity: Adds regularization strength but introduces hyperparameter λ and bin-design sensitivity.
  - Multiple classification heads vs. single regression: Provides interpretability (storm categories) but increases optimization complexity.
  - Daily fine-tuning vs. static model: Improves responsiveness to drift but raises deployment overhead and overfitting risk.

- Failure signatures:
  - Persistent underprediction of high-Kp events → binary branch may be underweighted or positive samples insufficient.
  - High variance across prediction horizons → fine-tuning window too short or learning rate too aggressive.
  - Wasserstein loss not decreasing → bin granularity or projection dimension may be misconfigured.

- First 3 experiments:
  1. Ablate Wasserstein alignment (set λ = 0) and compare 3-day/5-day Kp MAE and storm detection recall against full model.
  2. Vary fine-tuning window length (e.g., 7, 14, 30 days) and monitor stability of high-Kp recall and overall error distribution.
  3. Swap Swin Transformer for a simpler CNN encoder on images; assess whether image feature quality drives marginal gains over satellite + Kp-only baseline.

## Open Questions the Paper Calls Out

- Future research will focus on exploring additional techniques and alternative formulations of the alignment loss to further improve model generalization and forecasting precision.
- The model performed well on May 8, May 11, May 12, May 13, May 14, and May 15, but not on May 9 and May 10.

## Limitations
- Daily fine-tuning strategy and window length are underspecified, risking overfitting to recent noise.
- Transformer encoder hyperparameters (layers, heads, hidden size) and training schedule (LR, batch size, epochs) are not reported, limiting exact reproduction.
- No ablation study quantifies the marginal value added by each modality (solar images, satellite measurements, Kp time series).

## Confidence

- **High confidence**: Multimodal integration improves forecasting (supported by related work and ablation studies).
- **Medium confidence**: Wasserstein alignment improves consistency across modalities (mechanism is well-defined but lacks direct validation in literature).
- **Low confidence**: Daily fine-tuning reliably adapts to temporal drift without overfitting (no neighbor papers validate this approach for Kp prediction).

## Next Checks

1. Ablate Wasserstein alignment (λ = 0) and compare 3-day/5-day Kp MAE and storm detection recall against full model.
2. Vary fine-tuning window length (7, 14, 30 days) and monitor stability of high-Kp recall and overall error distribution.
3. Swap Swin Transformer for a simpler CNN encoder on images; assess whether image features drive marginal gains over satellite + Kp-only baseline.