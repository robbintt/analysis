---
ver: rpa2
title: On-device Streaming Discrete Speech Units
arxiv_id: '2506.01845'
source_url: https://arxiv.org/abs/2506.01845
tags:
- speech
- dsus
- window
- size
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying discrete speech
  units (DSUs) in real-time, on-device streaming speech applications. DSUs are advantageous
  due to their high transmission efficiency and compatibility with language models,
  but their extraction from self-supervised speech models (S3Ms) is computationally
  intensive and requires full-length audio.
---

# On-device Streaming Discrete Speech Units

## Quick Facts
- arXiv ID: 2506.01845
- Source URL: https://arxiv.org/abs/2506.01845
- Reference count: 0
- Primary result: 50% FLOPs reduction with 6.5% CER increase on ML-SUPERB 1h dataset

## Executive Summary
This paper tackles the computational challenge of extracting discrete speech units (DSUs) from self-supervised speech models (S3Ms) for real-time, on-device streaming applications. DSUs offer transmission efficiency and language model compatibility, but their extraction requires processing full audio sequences through computationally intensive S3Ms. The authors propose a practical solution by reducing the attention window size and the number of layers in the S3M, combined with learnable weights and fine-tuning, to achieve significant computational savings while maintaining acceptable DSU quality for speech recognition tasks.

## Method Summary
The authors address the computational bottleneck of DSU extraction by modifying the S3M architecture for streaming applications. They reduce the attention window size from processing full audio sequences to a 200ms window, enabling real-time processing. Additionally, they decrease the number of transformer layers from 12 to 6, substantially reducing computational complexity. To maintain DSU quality despite these reductions, they introduce learnable weights for each layer and perform fine-tuning on the ML-SUPERB 1h dataset. This optimization strategy achieves a 50% reduction in floating-point operations while incurring only a 6.5% relative increase in character error rate.

## Key Results
- 50% reduction in floating-point operations (FLOPs) achieved through architectural modifications
- 6.5% relative increase in character error rate (CER) on ML-SUPERB 1h dataset
- 200ms attention window size optimized for streaming applications
- Layer reduction from 12 to 6 transformer layers with learnable weights

## Why This Works (Mechanism)
The approach works by exploiting the fact that streaming applications don't require full-context processing of audio sequences. By limiting the attention window to 200ms, the model can process audio incrementally rather than waiting for complete sequences. The layer reduction decreases computational complexity at each timestep, while learnable weights allow the model to prioritize important layers. Fine-tuning adapts the compressed model to maintain DSU quality despite the architectural changes. This combination enables real-time processing on resource-constrained devices while preserving sufficient accuracy for speech recognition tasks.

## Foundational Learning
**Self-supervised speech models (S3Ms)**: Neural networks trained on unlabeled speech data to learn meaningful representations. Needed because labeled speech data is expensive to obtain. Quick check: Can generate embeddings for any speech audio without transcription requirements.

**Discrete speech units (DSUs)**: Quantized representations of speech that capture phonetic or semantic information. Needed for efficient transmission and compatibility with language models. Quick check: Can be decoded by text-based language models for speech recognition or synthesis.

**Attention mechanisms**: Components that allow models to focus on relevant parts of input sequences. Needed for capturing long-range dependencies in speech. Quick check: Typically requires full sequence context, making them computationally expensive for streaming.

**Transformer layers**: Building blocks of modern neural networks that use self-attention. Needed for powerful sequence modeling. Quick check: Each layer adds computational complexity and memory requirements.

**Learnable layer weights**: Parameters that assign importance to different layers in a multi-layer model. Needed to optimize performance when reducing layer count. Quick check: Allow compensation for information loss when removing layers.

## Architecture Onboarding

**Component map**: Audio input -> Windowed attention (200ms) -> 6 transformer layers -> Codebook quantization -> DSU output

**Critical path**: The streaming DSU extraction pipeline processes audio in 200ms windows through 6 transformer layers, then quantizes the representations into discrete units. This path must operate in real-time to maintain streaming capability.

**Design tradeoffs**: The authors balance computational efficiency against DSU quality by reducing window size and layer count while adding learnable weights. This tradeoff accepts modest accuracy degradation (6.5% CER increase) for substantial computational savings (50% FLOPs reduction).

**Failure signatures**: Excessive window reduction leads to loss of temporal context, while too few layers result in poor representation learning. The model may fail to capture long-range dependencies or subtle phonetic distinctions.

**First experiments**: 1) Vary attention window sizes (100ms, 200ms, 400ms) to find optimal streaming performance tradeoff. 2) Test different layer reduction configurations (8→6, 10→6, 12→4) to understand quality degradation patterns. 3) Evaluate learnable weight initialization strategies (uniform, learned from baseline, random) for optimal fine-tuning results.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section implies several areas requiring further investigation, including broader dataset evaluation, real device implementation, and extension to other S3M architectures.

## Limitations
- Evaluation limited to single dataset (ML-SUPERB 1h), restricting generalizability
- No quantification of actual device-specific latency and memory usage
- Limited assessment of DSU quality for non-ASR downstream tasks
- Architecture modifications may not transfer to other S3M models

## Confidence
**High confidence**: DSU extraction efficiency improvements - Experimental methodology is sound with clear results
**Medium confidence**: Streaming compatibility through window reduction - Validated on one dataset, needs broader testing
**Medium confidence**: Layer reduction preserving DSU quality - Results show acceptable degradation but limited scope

## Next Checks
1. Evaluate the proposed approach across multiple S3M architectures (e.g., Wav2Vec 2.0, APC) and diverse datasets with varying acoustic conditions
2. Measure actual device-specific latency and memory usage on representative edge hardware platforms (e.g., mobile SOCs, microcontrollers)
3. Assess DSU quality impact on non-ASR downstream tasks including speaker diarization and emotion classification