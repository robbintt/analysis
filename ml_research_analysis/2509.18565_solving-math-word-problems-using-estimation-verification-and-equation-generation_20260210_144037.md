---
ver: rpa2
title: Solving Math Word Problems Using Estimation Verification and Equation Generation
arxiv_id: '2509.18565'
source_url: https://arxiv.org/abs/2509.18565
tags:
- answer
- estimation
- verification
- dataset
- solving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the difficulty LLMs face in solving Math Word
  Problems (MWPs) due to complex reasoning and mathematical requirements. The proposed
  method, EV oSS, combines LLM-based decomposition and equation generation with symbolic
  equation solving, followed by estimation-based verification to ensure answer accuracy.
---

# Solving Math Word Problems Using Estimation Verification and Equation Generation

## Quick Facts
- arXiv ID: 2509.18565
- Source URL: https://arxiv.org/abs/2509.18565
- Reference count: 33
- Key outcome: EVoSS achieves state-of-the-art results on multiple datasets including Algebra (92.8%), SV AMP (89.4%), and SV AMPClean (90.5%), improving previous best results by nearly two percent on average.

## Executive Summary
This paper addresses the challenge of solving Math Word Problems (MWPs) using Large Language Models (LLMs), which often struggle with complex reasoning and mathematical requirements. The proposed EV oSS method combines LLM-based decomposition and equation generation with symbolic equation solving, followed by estimation-based verification to ensure answer accuracy. The approach introduces a novel estimation verification step inspired by elementary math teaching practices, where the LLM estimates the answer and compares it to the symbolic solver's output. If verification fails, an iterative rectification process is employed. The method achieves state-of-the-art results on multiple datasets including Algebra (92.8%), SV AMP (89.4%), and SV AMPClean (90.5%), improving previous best results by nearly two percent on average.

## Method Summary
EV oSS is a pipeline that first decomposes a math word problem into simplified statements using a 2-shot LLM prompt, then generates formal algebraic equations from those statements using a 3-shot prompt. These equations are passed to SymPy for exact symbolic solving, isolating the LLM from arithmetic errors. The LLM also generates a rough estimate of the answer using rounding and simplified logic. The estimation is compared to the symbolic answer within a tolerance threshold α (40-50% depending on dataset). If verification fails, the system enters an iterative rectification loop where the estimation is used as a hint to guide the LLM to regenerate equations. The method requires 2+ LLM calls per problem plus potential rectification loops.

## Key Results
- EV oSS achieves 92.8% accuracy on Algebra dataset, improving previous best by 2.1%
- EV oSS achieves 89.4% accuracy on SV AMP dataset, improving previous best by 1.9%
- EV oSS achieves 90.5% accuracy on SV AMPClean dataset, improving previous best by 2.0%
- On GSM8K, EV oSS achieves 82.2% accuracy, improving previous best by 0.9%
- The method outperforms Self-Consistency (SCoSS) baselines by 8-13% on the new Trig300 dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating semantic reasoning from arithmetic execution reduces calculation errors common in LLMs.
- **Mechanism:** The LLM is prompted to decompose word problems into declarative algebraic equations rather than solving them directly. These formal equations are passed to an external symbolic solver (SymPy), which guarantees arithmetic precision, effectively isolating the LLM's weakness in calculation from its strength in language understanding.
- **Core assumption:** The LLM can reliably translate natural language semantics into valid symbolic syntax (equations) without arithmetic hallucination.
- **Evidence anchors:**
  - [abstract] "...prompts an LLM to create equations from a decomposition... followed by using an external symbolic equation solver..."
  - [section II.D] "...mitigates arithmetic errors using an external symbolic solver that solves the equations without error."
  - [corpus] Related work (PoT, PAL) supports the efficacy of offloading computation to interpreters, though specific corpus papers on estimation are sparse.
- **Break condition:** If the LLM generates syntactically invalid equations or maps semantic relationships incorrectly (e.g., `a = b + c` instead of `a = b - c`), the symbolic solver will either fail or execute the wrong logic perfectly.

### Mechanism 2
- **Claim:** Estimation acts as a distinct reasoning path to verify the plausibility of generated equations.
- **Mechanism:** Inspired by elementary pedagogy, the system asks the LLM to generate a "rough estimate" using rounding and simplified logic. This estimate is compared against the precise symbolic answer. If the symbolic answer falls outside a tolerance threshold (e.g., $\pm$40-50%), it signals a likely error in the initial equation formulation rather than a minor arithmetic slip.
- **Core assumption:** An LLM is capable of generating a "good enough" estimate independently of its formal equation generation path (i.e., the estimation is not derived from the potentially flawed equations).
- **Evidence anchors:**
  - [abstract] "...the LLM is instructed to solve the MWP a second time, but this time with the objective of estimating... The estimation is then compared to the generated answer to verify."
  - [section IV.D] "The comparison involves checking whether the estimation is within $\alpha$% of the generated answer."
  - [corpus] Weak direct evidence in provided corpus; mechanism is primarily anchored in the paper's internal pedagogical justification.
- **Break condition:** If the problem requires high precision or the estimation logic is cognitively as complex as the solving logic, the LLM may estimate incorrectly, causing a "false negative" verification failure.

### Mechanism 3
- **Claim:** Iterative rectification with hint injection forces convergence toward a valid solution.
- **Mechanism:** When verification fails (answer $\neq$ estimate), the system does not merely retry. It enters a rectification loop where the "estimation" is fed back to the LLM as a hint. This constrains the search space for the LLM, guiding it to regenerate equations that align with the approximate magnitude of the correct answer.
- **Core assumption:** The estimation generated in Mechanism 2 is closer to the truth than the formal equation output.
- **Evidence anchors:**
  - [section IV.E] "If verification fails... the estimation will be given as a hint to guide the LLM... The same answer must be generated at least 2 out of 3 times to be accepted."
  - [table vi] Shows EVoSS outperforming Self-Consistency (SCoSS), suggesting guided rectification is more efficient than simple re-sampling.
  - [corpus] Not explicitly covered in provided corpus neighbors.
- **Break condition:** If the estimation is wildly inaccurate, using it as a hint will "hallucinate" the LLM into a confident but wrong final answer.

## Foundational Learning

- **Concept: Symbolic vs. Sub-Symbolic Reasoning**
  - **Why needed here:** The architecture relies on the boundary between neural reasoning (LLM handling text/estimates) and symbolic reasoning (SymPy handling algebra). You must understand that LLMs are probabilistic token predictors, not calculators.
  - **Quick check question:** Why does `2 + 2` sometimes equal `5` in an LLM's output but never in SymPy?

- **Concept: Prompt Engineering (Few-Shot & Decomposition)**
  - **Why needed here:** The system uses "two-shot" and "three-shot" prompting to enforce specific output formats (e.g., `[[var a]]`, `[[eq ...]]`). Understanding how to structure these examples is critical for the parser to work.
  - **Quick check question:** How does providing examples of "decomposition" in the prompt change the LLM's output distribution compared to zero-shot?

- **Concept: Heuristic Verification & Thresholds ($\alpha$)**
  - **Why needed here:** The system relies on a fuzziness factor ($\alpha$). Understanding precision vs. recall trade-offs in verification is necessary to tune the system for different problem types (e.g., Algebra vs. GSM8K).
  - **Quick check question:** If $\alpha$ is set too low (e.g., 5%), what happens to the rectification loop on valid answers that involve rounding?

## Architecture Onboarding

- **Component map:** Input Pre-processor -> Decomposition Engine (LLM) -> Equation Generator (LLM) -> Symbolic Solver (SymPy) -> Estimator (LLM) -> Verifier (Logic) -> Rectifier (LLM)
- **Critical path:** The chain is `Decomposition` -> `Equation Gen` -> `SymPy`. If SymPy returns an error (e.g., over-determined system), the system loops back to Equation Gen with a specific error hint.
- **Design tradeoffs:**
  - **Latency vs. Accuracy:** The method requires 2+ LLM calls per problem (Solve + Estimate) plus potential rectification loops. This triples latency compared to standard CoT.
  - **Rigidity vs. Flexibility:** The strict equation format (`[[eq ...]]`) reduces parsing errors but may struggle with ambiguous natural language that doesn't fit the template.
- **Failure signatures:**
  - **Regex Extraction Failure:** LLM outputs correct math but wrong format (e.g., missing `[[eq` tags), causing SymPy to receive null input.
  - **Infinite Rectification:** The estimation and symbolic answer never converge, causing the system to hit the max attempt limit (default 3).
  - **Over-determined Systems:** The LLM generates more equations than variables, causing SymPy to crash if not handled by the error-checking logic.
- **First 3 experiments:**
  1. **Unit Test the Pipeline:** Manually input an Algebra problem and verify the Regex successfully extracts equations from the LLM's specific output string format.
  2. **Threshold Calibration:** Run a batch of 50 SVAMP problems and sweep α from 20% to 60% to find the optimal verification tolerance (paper suggests 40-50%).
  3. **Ablation Run:** Disable the "Estimation Verification" step and compare accuracy on the Trig300 dataset to measure the specific performance delta contributed by the verifier (expect ~8-13% drop based on paper).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the EV oSS estimation verification method perform when deployed on modern LLMs with stronger baseline mathematical reasoning capabilities?
- **Basis in paper:** [explicit] The authors state that "As LLMs continue to evolve and improve, they may naturally become better at solving MWPs without the help of outside methods," citing GPT-4's 35% improvement over GPT-3.5 on GSM8K as evidence that augmented approaches may become less impactful.
- **Why unresolved:** The study relies exclusively on gpt-3.5-turbo, leaving open whether the 8-13% gains from estimation verification persist when applied to more capable base models.
- **What evidence would resolve it:** Replicate EV oSS experiments using GPT-4, Claude, or other frontier models on the same datasets; compare percentage improvements over baselines to determine if the method's value diminishes with stronger base models.

### Open Question 2
- **Question:** Can estimation verification be extended effectively to advanced mathematical domains beyond basic arithmetic and trigonometry, such as calculus, linear algebra, or statistics?
- **Basis in paper:** [inferred] The paper demonstrates EV oSS on arithmetic (SVAMP, GSM8K), algebra, and a new trigonometry dataset, but the Trig300 results (65.5% vs 4-17% for baselines) suggest domain-specific behavior. The method assumes LLMs can produce reasonable estimates, which may not hold for more abstract mathematics where intuition is harder to apply.
- **Why unresolved:** The estimation verification step requires the LLM to produce a meaningful rough estimate—a process that becomes increasingly difficult as mathematical concepts grow more abstract and less amenable to intuitive approximation.
- **What evidence would resolve it:** Apply EV oSS to datasets covering calculus word problems, probability questions, or multi-step linear algebra scenarios; analyze whether estimation quality degrades and whether verification thresholds (α) require domain-specific tuning.

### Open Question 3
- **Question:** What is the principled basis for automatically selecting the optimal verification threshold α across different problem types and datasets?
- **Basis in paper:** [explicit] The ablation study (Table 4) shows that optimal α varies: 40 for Algebra, 50 for GSM8K and SVAMP. The authors note that α "varies between different datasets ranging from 40 to 50" but provide no automated method for determining this a priori.
- **Why unresolved:** Currently, α must be empirically tuned per dataset, limiting the method's applicability to new problem distributions without costly hyperparameter search.
- **What evidence would resolve it:** Develop a meta-learning or adaptive approach that predicts optimal α based on problem features (e.g., number of equations, numerical magnitude, operation type) and validate that predicted α values achieve performance comparable to empirically-tuned ones.

## Limitations

- The estimation verification mechanism lacks strong empirical validation of the independence assumption between estimation and formal equation generation paths.
- The threshold calibration (α) appears dataset-specific without theoretical guidance on optimal tolerance selection.
- The method requires 2+ LLM calls per problem plus potential rectification loops, tripling latency compared to standard CoT approaches.

## Confidence

- **High Confidence:** The core decomposition + symbolic solving architecture (Mechanisms 1) is well-established in prior work and directly verifiable through SymPy's correctness guarantees.
- **Medium Confidence:** The estimation verification mechanism (Mechanism 2) shows strong empirical results but relies on assumptions about LLM estimation behavior that are not independently validated.
- **Medium Confidence:** The iterative rectification approach (Mechanism 3) demonstrates improved performance over self-consistency baselines, but the specific contribution of the estimation hint versus simple re-sampling remains unclear.

## Next Checks

1. **Estimation Path Independence Test:** Conduct ablation studies where the LLM's estimation is explicitly derived from the generated equations versus computed independently, measuring performance differences to validate the independence assumption.
2. **Threshold Sensitivity Analysis:** Systematically vary α across the full range (10-60%) on all datasets to map the precision-recall tradeoff curve and identify optimal thresholds with statistical significance.
3. **Estimation Quality Evaluation:** Manually annotate a subset of problems to assess whether LLM-generated estimations are reasonable approximations of correct answers, and correlate estimation accuracy with successful verification rates.