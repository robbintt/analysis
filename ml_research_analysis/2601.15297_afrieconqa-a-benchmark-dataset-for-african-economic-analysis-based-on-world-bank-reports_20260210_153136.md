---
ver: rpa2
title: 'AfriEconQA: A Benchmark Dataset for African Economic Analysis based on World
  Bank Reports'
arxiv_id: '2601.15297'
source_url: https://arxiv.org/abs/2601.15297
tags:
- economic
- retrieval
- question
- dataset
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AfriEconQA, a specialized benchmark dataset
  for African economic analysis using World Bank reports. The dataset consists of
  8,937 curated QA pairs requiring high-precision numerical reasoning and temporal
  disambiguation from technical institutional documents.
---

# AfriEconQA: A Benchmark Dataset for African Economic Analysis based on World Bank Reports

## Quick Facts
- **arXiv ID**: 2601.15297
- **Source URL**: https://arxiv.org/abs/2601.15297
- **Reference count**: 35
- **Key outcome**: Introduces AfriEconQA, a benchmark dataset of 8,937 QA pairs from World Bank reports requiring high-precision numerical reasoning and temporal disambiguation for African economic analysis.

## Executive Summary
This paper presents AfriEconQA, a specialized benchmark dataset for African economic analysis using World Bank reports. The dataset consists of 8,937 curated QA pairs requiring high-precision numerical reasoning and temporal disambiguation from technical institutional documents. The study evaluates retrieval-augmented generation systems on this challenging domain-specific data, revealing that current models possess virtually no parametric knowledge of African economic indicators. Hybrid retrieval strategies combining dense and sparse methods show promise, with open-weight models like Qwen 32B demonstrating strong extractive precision for numerical data. The benchmark achieves this through careful filtering of synthetic questions and strict quality controls, ensuring high-fidelity evidence-answer alignment.

## Method Summary
The AfriEconQA dataset was created by processing 236 World Bank PDF reports into 64,892 text chunks (1,000 characters with 200 overlap). From an initial 10,018 synthetic questions, 8,937 were curated through a four-stage filtering pipeline ensuring numerical verbatim accuracy, entity decontextualization, meta-language removal, and substantive quality. The benchmark uses dual indexing with BM25 for sparse retrieval and BGE-m3/Google text-embedding-004 for dense retrieval, combined via Reciprocal Rank Fusion. Evaluation employs multiple metrics including Exact Match, F1, BLEU, ROUGE-L, and LLM-Judge (GPT-4o-mini as semantic judge with ≥0.7 threshold). Five retrieval strategies and three generators (GPT-4o, Qwen 2.5 32B, GPT-5 Mini zero-shot) were tested on a stratified sample of 300 questions.

## Key Results
- Zero-shot models fail to answer over 90% of queries (EM: 0.053, LLM-Judge: 0.081), demonstrating severe parametric knowledge gaps for African economic indicators.
- Hybrid retrieval (RRF fusion of dense + sparse) achieves the highest LLM-Judge score (0.512) despite slightly degraded MRR (0.722 vs 0.763 for pure dense).
- Open-weight models like Qwen 32B achieve higher Exact Match scores than GPT-4o (0.270 vs 0.223 on Hybrid Google) due to stronger extractive precision for numerical data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmentation is non-optional for African economic analysis because LLMs lack parametric knowledge of this domain.
- Mechanism: World Bank reports on African economies are largely absent from pretraining corpora, creating a "parametric vacuum." Zero-shot models default to hallucination or refusal when queried about specific macroeconomic indicators.
- Core assumption: The absence from pretraining corpora is genuine domain coverage failure, not merely insufficient model scale.
- Evidence anchors: Abstract shows zero-shot models fail >90% of queries; Section 6.1 confirms World Bank indicators are virtually absent from LLM pretraining.

### Mechanism 2
- Claim: Hybrid retrieval (Sparse + Dense via Reciprocal Rank Fusion) improves answer quality despite slightly degraded ranking metrics.
- Mechanism: Dense retrieval captures semantic meaning but struggles with precise numerical matching. BM25 provides exact lexical matching for numbers and acronyms. RRF fuses both, diversifying retrieved contexts.
- Core assumption: The generator can synthesize diverse or partially redundant contexts without confusion.
- Evidence anchors: Section 4.2 describes the hybrid strategy; Section 6.2 shows Hybrid Google + GPT-4o achieved highest LLM-Judge score (0.512) despite MRR drop from 0.763 to 0.722.

### Mechanism 3
- Claim: Open-weight models (Qwen 32B) achieve higher extractive precision for numerical data than proprietary APIs.
- Mechanism: Qwen exhibits "extractive discipline"—faithfully copying numerical figures and verbatim phrases from evidence. GPT-4o tends toward semantic paraphrasing, which improves readability but penalizes exact match metrics.
- Core assumption: The evaluation's verbatim numerical constraint reflects real institutional requirements.
- Evidence anchors: Abstract notes Qwen 32B's strong extractive precision; Section 6.3 shows Qwen 32B consistently achieved higher EM scores than GPT-4o (0.270 vs 0.223).

## Foundational Learning

- Concept: **Reciprocal Rank Fusion (RRF)**
  - Why needed here: Combines sparse and dense retrieval results into a single ranked list
  - Quick check question: Given two ranked lists where Document A ranks #1 in sparse and #5 in dense, and Document B ranks #3 in both, which has higher RRF score (η=60)?

- Concept: **Temporal Disambiguation**
  - Why needed here: Economic queries often require distinguishing historical actuals from projections
  - Quick check question: If a 2022 report says "growth this year will be 5.2%" and a 2024 report says "growth this year was 3.1%," which should answer "What was Rwanda's GDP growth in 2022?"

- Concept: **Parametric vs. Retrieval-Augmented Knowledge**
  - Why needed here: Central finding is that parametric knowledge fails catastrophically for this domain
  - Quick check question: Why might a model correctly answer "What is the capital of Nigeria?" without retrieval, but fail on "What was Nigeria's merchandise trade deficit as a percentage of GDP in Q3 2023?"

## Architecture Onboarding

- Component map: Corpus (236 reports → 64,892 chunks) -> Dual Index (BM25 + embeddings) -> Retriever (k=3 + optional RRF) -> Generator (GPT-4o/Qwen 32B) -> Evaluator (EM, F1, BLEU, ROUGE, LLM-Judge)

- Critical path: 1) Document ingestion → chunking with overlap 2) Dual indexing (BM25 + embeddings) 3) Query → retrieve top-k from each index 4) (If hybrid) RRF fusion of result sets 5) Generator synthesizes answer from retrieved chunks 6) Evaluate against ground-truth with metadata provenance

- Design tradeoffs:
  - Chunk size (1,000 chars): Balances retrieval granularity against context completeness
  - k=3 retrieval depth: Authors note List and Comparison queries suffer; multi-hop retrieval may help but adds latency
  - Verbatum numerical constraint: Ensures fidelity but may penalize systems that correctly paraphrase

- Failure signatures:
  - Zero-shot models: EM ~0.05, LLM-Judge ~0.08 (parametric vacuum)
  - BM25-only: Low recall (R@3 = 0.437), struggles with semantic queries
  - Dense-only: Higher MRR but lower numerical precision
  - List/Comparison queries: ~20-23% accuracy across all systems (suggests architectural ceiling)

- First 3 experiments:
  1. Establish baseline: Run GPT-4o zero-shot on 50 questions; confirm parametric failure (expect <10% accuracy)
  2. Compare retrieval modes: Test BM25-only, Dense-only (BGE), and Hybrid (RRF) on same 50 questions; measure P@3, MRR, and EM
  3. Generator comparison: Using best retriever config, compare GPT-4o vs. Qwen 32B on numerical factoid queries; expect Qwen higher EM, GPT-4o higher semantic judge score

## Open Questions the Paper Calls Out

- Can agentic or multi-hop retrieval strategies significantly improve performance on "List" and "Comparison" queries compared to single-pass architectures?
  - Basis: Section 6.4 notes single-pass architectures struggle with these categories (achieving ~20-23% accuracy), suggesting "potential benefits from agentic or multi-hop retrieval strategies in future work."
  - Why unresolved: Current study limited to single-pass retrieval with fixed depth (k=3), leaving multi-step reasoning architectures untested.

- Do the observed trade-offs between dense and hybrid retrieval persist when evaluating the full 8,937-question dataset?
  - Basis: Section 7 states study evaluated only a stratified sample of 300 questions due to computational constraints, and "future work should validate these findings on the complete dataset at scale."
  - Why unresolved: While the sample is stratified, it remains a subset; it's unconfirmed if findings hold universally across nearly 9,000 instances.

- To what extent does the mandatory "verbatim numerical constraint" penalize models that produce semantically correct abstractive answers?
  - Basis: Section 7 notes the verbatim constraint "may limit the evaluation of systems on abstractive reasoning tasks," while Section 6.3 shows GPT-4o suffers in Exact Match due to paraphrasing despite high semantic scores.
  - Why unresolved: The benchmark enforces strict character-for-character match for numbers, making it difficult to distinguish between hallucination and valid semantic paraphrase.

## Limitations

- Dataset accessibility: The dataset will be "made publicly available upon publication," creating uncertainty about reproducibility until repository release.
- Temporal generalization: Findings about parametric knowledge gaps may not generalize to other institutional data sources or regions beyond World Bank African economic reports.
- Evaluation artifact concerns: Strict verbatim numerical constraint for EM scoring may not reflect real-world requirements and may penalize semantically correct abstractive answers.

## Confidence

**High Confidence**:
- Parametric knowledge gap for African economic indicators (95%+ failure rate in zero-shot)
- Hybrid retrieval provides superior answer quality despite MRR degradation
- Qwen 32B demonstrates higher extractive precision than GPT-4o for numerical data

**Medium Confidence**:
- RRF parameters (η constant) are optimal for this domain
- List and Comparison query performance ceiling (~20-23%) represents fundamental architectural limitation
- 1,000 character chunk size optimally balances context vs precision

**Low Confidence**:
- Synthetic QA generation fully preserves real-world complexity
- Findings generalize beyond World Bank reports to other institutional document types
- Current performance represents ceiling rather than floor for future systems

## Next Checks

1. Reproduce zero-shot failure: Run GPT-4o zero-shot on 50 AfriEconQA questions; confirm parametric knowledge gap with <10% accuracy.

2. Hybrid retrieval ablation: Test BM25-only, Dense-only, and Hybrid RRF configurations on same question set; measure if MRR degradation (0.763→0.722) is offset by LLM-Judge improvement (0.347→0.512).

3. Generator precision comparison: Using best retrieval setup, compare GPT-4o vs Qwen 32B on numerical factoid questions; verify Qwen's higher EM (0.270 vs 0.223) at cost of semantic paraphrasing penalty.