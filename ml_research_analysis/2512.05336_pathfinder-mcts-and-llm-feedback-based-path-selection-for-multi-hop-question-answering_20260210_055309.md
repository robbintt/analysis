---
ver: rpa2
title: 'PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question
  Answering'
arxiv_id: '2512.05336'
source_url: https://arxiv.org/abs/2512.05336
tags:
- answer
- question
- pathfinder
- reasoning
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PATHFINDER addresses multi-hop question answering by improving
  training data quality for iterative reasoning with retrieval. It uses Monte Carlo
  Tree Search (MCTS) to generate diverse Chain-of-Thought (CoT) traces, then filters
  them using sub-answer recall and LLM-as-a-judge verification.
---

# PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question Answering

## Quick Facts
- **arXiv ID**: 2512.05336
- **Source URL**: https://arxiv.org/abs/2512.05336
- **Reference count**: 0
- **Primary result**: PATHFINDER achieves 75.00 EM on 2WikiMultiHopQA and 39.70 EM on HotpotQA using Qwen-2.5-7B-Instruct

## Executive Summary
PATHFINDER addresses multi-hop question answering by improving training data quality for iterative reasoning with retrieval. It uses Monte Carlo Tree Search (MCTS) to generate diverse Chain-of-Thought (CoT) traces, then filters them using sub-answer recall and LLM-as-a-judge verification. The method also reformulates sub-queries when retrieval fails. PATHFINDER outperforms comparable baselines on public datasets, achieving 75.00 EM and 78.38 F1 on in-distribution 2WikiMultiHopQA, and 39.70 EM and 51.88 F1 on out-of-distribution HotpotQA using Qwen-2.5-7B-Instruct. It significantly improves over DeepRAG, especially when retrieval-only, due to better trace filtering and sub-question reformulation that avoids hallucinations.

## Method Summary
PATHFINDER generates training data for multi-hop QA by first using MCTS with Qwen-72B-Instruct to create diverse reasoning traces. Each question undergoes 12 MCTS rollouts with max depth 12, branching at each node using UCT selection. Two actions are defined: One-Step Thought (A1) for generating reasoning steps and Sub-question Generation/Answering (A2) for decomposing questions. Generated traces are filtered through two stages: Stage 1 discards traces where golden sub-answer recall < 1, ensuring factual grounding. Stage 2 applies LLM-as-Judge using Qwen-72B-Instruct across four criteria (incorrectness, redundancy, irrelevance, faithfulness) with weighted error scoring. The minimum error score trace is selected for training, with ties broken by shortest length. The final model (Qwen-2.5-7B-Instruct or Gemma-2-9B-Instruct) is fine-tuned on filtered traces using masked context loss.

## Key Results
- PATHFINDER achieves 75.00 EM and 78.38 F1 on in-distribution 2WikiMultiHopQA
- PATHFINDER achieves 39.70 EM and 51.88 F1 on out-of-distribution HotpotQA
- Ablation shows LLM-as-Judge provides largest gain: SP-only (71.40 EM) → SP+AV (71.90 EM) → SP+AV+LJ (75.00 EM)

## Why This Works (Mechanism)

### Mechanism 1: MCTS-Based Diverse Trace Generation
Monte Carlo Tree Search produces diverse, correct reasoning paths that serve as higher-quality training data than single-path generation methods. MCTS explores multiple reasoning trajectories using UCT to balance exploitation of promising paths with exploration of alternatives. Reward propagates backward from terminal nodes based on final answer correctness (1 if correct, 0 otherwise). Sampling temperature variation at each node creates branching—2 samples for thought generation (A1), 3 for sub-question generation (A2).

### Mechanism 2: Two-Stage Trace Filtering Pipeline
Sequential filtering by sub-answer recall then LLM-as-a-judge verification removes erroneous traces and selects optimal training examples. Stage 1 discards traces where golden sub-answer recall < 1 (ensures factual grounding). Stage 2 applies LLM judge across 4 dimensions—incorrectness, redundancy, irrelevance, faithfulness—assigning weighted error scores. Final selection picks minimum error score trace; ties broken by shortest length.

### Mechanism 3: Query Reformulation on Retrieval Failure
Training models to reformulate sub-queries when retrieved context lacks evidence reduces hallucinated answers. When sub-answer extraction fails (context insufficient or ambiguous), model generates reformulated query with additional specificity rather than forcing unsupported answer.

## Foundational Learning

- **Monte Carlo Tree Search (MCTS)**
  - Why needed here: Core data generation mechanism; understanding UCT, rollout policies, and backpropagation is essential for debugging trace quality.
  - Quick check question: Given a node with Q=0.6, N=10 visits, parent with N=50 visits, and exploration weight w=1.0, calculate its UCT score. (Answer: 0.6/10 + √(ln(50)/10) ≈ 0.06 + 0.41 = 0.47)

- **Chain-of-Thought (CoT) Faithfulness**
  - Why needed here: Filtering criteria center on faithfulness—knowing what makes a trace faithful vs hallucinated is prerequisite for evaluating LLM judge outputs.
  - Quick check question: A sub-answer states "Sergio Corbucci is Italian" but retrieved context only mentions "Italian movies" without naming Corbucci's nationality. Is this faithful? (Answer: No—lacks supporting evidence in context; this is exactly the faithfulness error type from section 3.2)

- **Retrieval-Augmented Generation (RAG) for Multi-Hop QA**
  - Why needed here: PATHFINDER operates within iterative RAG framework; distinguishing parametric knowledge from retrieved knowledge is central to the design.
  - Quick check question: Why does the paper argue retrieval-only mode outperforms parametric-knowledge modes when using a strong retriever? (Answer: LLM parametric knowledge can introduce hallucinations; strong retriever (ColBERTv2) provides better grounding than weak retriever (BM25) used in prior work)

## Architecture Onboarding

- Component map:
Question → MCTS Engine: Qwen-72B → Trace Pool → Stage 1 Filter ← Golden Sub-Answers → Stage 2 Filter ← LLM Judge: Qwen-72B → Training Data → Fine-tune Target: Qwen-7B or Gemma-9B → Inference: Iterative RAG ↑ Retriever: ColBERTv2 + Wikipedia

- Critical path:
1. **Data generation phase**: Question → MCTS with retrieval → multiple candidate traces → filtering → one optimal trace per question
2. **Training phase**: Optimal traces → causal LM fine-tuning (context masked, loss on thoughts/sub-queries/answers)
3. **Inference phase**: Fine-tuned model generates sub-queries iteratively, retrieves context, reformulates on failure

- Design tradeoffs:
| Decision | Choice | Alternative | Rationale |
|----------|--------|-------------|-----------|
| MCTS depth | 12 | Higher | Balances coverage vs computation; most multi-hop questions resolve within 6-8 hops |
| Sampling temps | 0.6 (thought), 1.0 (sub-query), 0.2 (answer) | Uniform | High temp for query diversity, low temp for answer extraction accuracy |
| Training epochs | 5 (Qwen), 2 (Gemma) | More | Early stopping based on validation; Gemma converges faster |
| Retriever top-k | 3 | Higher | Reduces noise; 3 passages sufficient for most sub-questions |

- Failure signatures:
1. **Empty trace pool after filtering** → Question too complex or MCTS rollouts insufficient; increase rollouts or add manual exemplars
2. **High LLM judge rejection rate** → Systematic reasoning errors in MCTS generator; inspect rejected traces for patterns
3. **Reformulation loops at inference** → Model not adequately trained on "not found" cases; add more failure-case traces to training
4. **OOD performance collapse** → Overfitting to in-distribution reasoning patterns; increase training diversity or add OOD examples

- First 3 experiments:
1. **Validate ablation claims**: Reproduce Table 2 with SP-only, SP+AV, SP+AV+LJ configurations on held-out 2WikiMultiHopQA subset. Confirm LLM judge provides largest gain. Expected: ~3.5 EM improvement from LJ stage.
2. **Isolate reformulation mechanism**: Create adversarial test set where initial retrieval intentionally fails (swap entity names). Compare PATHFINDER vs DeepRAG on reformulation success rate. Expected: PATHFINDER shows higher recovery rate.
3. **Cross-dataset transfer**: Train on 2WikiMultiHopQA only, evaluate zero-shot on HotpotQA. Measure degradation vs mixed training. This validates whether MCTS traces generalize or require domain-specific generation.

## Open Questions the Paper Calls Out
- Can preference learning from retrieval success/failure pairs further improve sub-question generation alignment with the retriever?
- How robust is PATHFINDER to retriever quality degradation compared to baseline methods?
- Does LLM-as-a-Judge introduce systematic biases when using the same model family for both trace generation and validation?
- What is the computational cost tradeoff between MCTS-based data generation and simpler trace synthesis methods?

## Limitations
- Performance significantly drops on out-of-distribution HotpotQA (39.70 EM vs 75.00 EM in-distribution), suggesting limited generalization
- Method requires strong retriever (ColBERTv2); performance with weaker retrievers not evaluated
- 8,000→6,852 sample reduction rate (14.35%) seems high and may limit training data diversity

## Confidence
- **High Confidence**: Ablation results showing progressive improvement from SP → SP+AV → SP+AV+LJ on 2WikiMultiHopQA (75.00 EM final)
- **Medium Confidence**: Retrieval-only mode outperforming parametric-knowledge modes based on ColBERTv2 vs BM25 comparison
- **Low Confidence**: Generalization to HotpotQA (39.70 EM) due to significant performance gap and lack of analysis on failure modes

## Next Checks
1. **Prompt Template Validation**: Implement the A1/A2 action prompts using the provided sampling temperatures and verify that MCTS generates diverse traces rather than converging to single-path reasoning. Check if different prompt formulations affect trace diversity.
2. **Filtering Threshold Sensitivity**: Experiment with relaxing sub-answer recall threshold from 1.0 to 0.9 and measure impact on final EM/F1 scores. This tests whether the strict filtering improves reasoning quality or simply reduces training data.
3. **Zero-Shot Transfer Analysis**: Train PATHFINDER on 2WikiMultiHopQA only, evaluate on HotpotQA, then analyze failed questions to identify whether failures stem from reasoning pattern mismatches or knowledge gaps. This distinguishes between architectural limitations and data distribution effects.