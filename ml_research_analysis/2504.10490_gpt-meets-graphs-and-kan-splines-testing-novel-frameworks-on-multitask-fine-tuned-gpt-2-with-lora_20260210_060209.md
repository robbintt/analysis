---
ver: rpa2
title: 'GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask Fine-Tuned
  GPT-2 with LoRA'
arxiv_id: '2504.10490'
source_url: https://arxiv.org/abs/2504.10490
tags:
- lora
- graph
- attention
- figure
- learnable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates integrating Kolmogorov-Arnold Networks (KAN)
  and Graph Attention Networks (GAT) with Low-Rank Adaptation (LoRA) into a pre-trained
  GPT-2 model for multi-task learning. The goal was to assess whether these interpretable,
  structured architectures could improve performance on sentiment analysis, paraphrase
  detection, and sonnet generation tasks.
---

# GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask Fine-Tuned GPT-2 with LoRA
## Quick Facts
- arXiv ID: 2504.10490
- Source URL: https://arxiv.org/abs/2504.10490
- Reference count: 0
- Primary result: LoRA outperforms Hybrid KAN-LoRA and Graph-LoRA in multitask fine-tuning.

## Executive Summary
This work evaluates integrating Kolmogorov-Arnold Networks (KAN) and Graph Attention Networks (GAT) with Low-Rank Adaptation (LoRA) into a pre-trained GPT-2 model for multi-task learning. The goal was to assess whether interpretable, structured architectures could improve performance on sentiment analysis, paraphrase detection, and sonnet generation. Empirical evaluations revealed that LoRA-only fine-tuning remains the most effective approach, outperforming the more complex interpretable architectures. The results suggest that efficient parameter adaptation via LoRA is currently the best strategy for this multi-task setting.

## Method Summary
The study fine-tunes GPT-2 (Large, 1280-dim) using LoRA (r=32, α=64) with AdamW optimizer (LR=1e-5, weight decay=0.2). Three variants are tested: standard LoRA, Hybrid KAN-LoRA (replacing MLP layers with KAN), and Graph-LoRA (adding document-level GAT). Tasks include sentiment analysis (SST, CFIMDB), paraphrase detection (Quora), and sonnet generation. Training runs for 10-15 epochs on sentiment/sonnet tasks and 3-5 on paraphrase. Model performance is evaluated using classification accuracy and CHRF scores.

## Key Results
- LoRA achieves 55.249% accuracy on SST, 99.18% on CFIMDB dev, 89.9% on paraphrase test, and 42.097 CHRF on sonnet generation.
- Neither Hybrid KAN-LoRA nor Graph-LoRA outperforms optimized LoRA in any task.
- KAN-LoRA shows weaker memorization and convergence stalls in early epochs.
- Graph-LoRA incurs significant computational overhead (18+ hours on A100) without performance gains.

## Why This Works (Mechanism)
### Mechanism 1: Low-Rank Adaptation Stability
LoRA constrains updates to a low-dimensional subspace, acting as a regularizer against overfitting in multi-task settings. It freezes pre-trained weights $W_0$ and learns incremental updates $\Delta W = BA$, applying weight decay and dropout to these low-rank matrices.

### Mechanism 2: Hybrid KAN-LoRA Functional Decomposition
KAN attempts to improve function approximation by learning activation functions on edges (splines) rather than fixed activations on nodes. The architecture splits computation into a learnable base path and a spline path: $h = (W_{base} + \frac{\alpha}{r} BA)f_{base}(x) + W_{spline}\sigma_{KAN}(x)$.

### Mechanism 3: Graph Attention Context Propagation
Representing text as a graph allows attention mechanisms to propagate contextual information between non-adjacent tokens based on semantic relationships. A multi-head GAT updates node representations by aggregating features from neighbors using attention coefficients.

## Foundational Learning
- **Concept: Low-Rank Matrix Factorization (SVD)**
  - Why needed: LoRA relies on the assumption that weight updates are low-rank.
  - Quick check: If a weight matrix is $1024 \times 1024$ and rank $r=8$, how many trainable parameters does LoRA introduce compared to full fine-tuning?

- **Concept: B-Splines**
  - Why needed: KAN architectures replace weights with learnable spline functions.
  - Quick check: How does the number of spline knots affect the capacity and smoothness of the learned function in a KAN layer?

- **Concept: Graph Inductive Biases**
  - Why needed: Graph-LoRA assumes text benefits from graph structures.
  - Quick check: In a document-level text graph, what defines an edge between two token nodes?

## Architecture Onboarding
- **Component map:** Tokenization -> Transformer blocks (with LoRA) -> Classification/Generation head
- **Critical path:** Input Tokenization -> Graph Construction (Graph-LoRA only) -> Forward Pass through Transformer -> LoRA Injection -> Head
- **Design tradeoffs:** LoRA trades maximal adaptability for storage efficiency and reduced overfitting; KAN offers interpretability but introduces training instability; Graph attention captures non-local syntax but requires expensive preprocessing.
- **Failure signatures:** KAN convergence stall (loss plateaus early), Graph OOM/timeout (18+ hours on A100), performance collapse (accuracy dropping significantly).
- **First 3 experiments:** 1) Baseline LoRA Tuning with r ∈ {4, 16, 32} and α ∈ {16, 32, 64}; 2) KAN Layer Sanity Check by replacing a single MLP layer; 3) Graph Construction Profiling to benchmark data loading pipeline.

## Open Questions the Paper Calls Out
1. Can Hybrid KAN-LoRA or Graph-LoRA close the performance gap with standard LoRA if trained on larger model sizes (e.g., GPT-2 Large) rather than being restricted to the 768-parameter variant?
2. Do KAN-based transformers provide measurable benefits in specific NLP domains involving symbolic reasoning (e.g., mathematical language processing) where they outperform standard MLPs?
3. Can the computational overhead of dynamic document-level graph construction be reduced to make Graph-LoRA viable for standard training pipelines?

## Limitations
- Computational cost prevented fair comparison of architectural efficacy independent of model capacity.
- Limited evaluation to standard NLP tasks (sentiment, paraphrase, sonnet generation) that may not showcase KAN strengths.
- Graph construction overhead made Graph-LoRA impractical for standard training pipelines.

## Confidence
- High: LoRA outperforms more complex interpretable architectures in multi-task fine-tuning.
- Medium: KANs underperform on standard NLP tasks but may excel in symbolic reasoning domains.
- Low: Graph-LoRA's poor performance may be due to unsuitable inductive bias or inefficient implementation.

## Next Checks
1. Validate LoRA baseline by tuning r and α parameters on SST task.
2. Profile graph construction time for Graph-LoRA to confirm it exceeds 50% of total step time.
3. Compare epoch-wise loss curves between KAN-LoRA and LoRA to confirm slower convergence.