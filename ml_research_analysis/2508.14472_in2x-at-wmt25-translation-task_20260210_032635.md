---
ver: rpa2
title: In2x at WMT25 Translation Task
arxiv_id: '2508.14472'
source_url: https://arxiv.org/abs/2508.14472
tags:
- arxiv
- language
- preprint
- tasks
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the open-system submission by the In2x research
  team for the WMT25 General Machine Translation Shared Task, focusing on Japanese-related
  translation tasks. The work addresses the challenge of extending large language
  models (LLMs) to less commonly spoken languages, particularly Japanese, where expressive
  and culturally appropriate translation remains difficult.
---

# In2x at WMT25 Translation Task
## Quick Facts
- arXiv ID: 2508.14472
- Source URL: https://arxiv.org/abs/2508.14472
- Reference count: 10
- Primary result: First place in unrestricted track, second overall for Japanese-related translation at WMT 2025

## Executive Summary
This paper presents the In2x research team's open-system submission for the WMT25 General Machine Translation Shared Task, focusing on Japanese-related translation tasks. The work addresses the challenge of extending large language models to less commonly spoken languages by proposing a generalizable paradigm for transferring English LLM strengths to Japanese through curriculum design, cross-lingual alignment, and preference signals rewarding naturalness. The approach involves three post-training phases: balancing STEM and humanities capabilities with diverse corpora, refining long-text processing, and fast annealing with high-quality data. A detailed alignment pipeline constructs 2 million post-training samples, including 1.5 million for supervised fine-tuning and 500k for reinforcement learning.

## Method Summary
The In2x approach employs a systematic knowledge transfer framework from English LLMs to Japanese, utilizing curriculum design, cross-lingual alignment, and preference signals that reward naturalness. The methodology involves three post-training phases: (1) balancing STEM and humanities capabilities through diverse corpora, (2) refining long-text processing capabilities, and (3) fast annealing with high-quality data. The alignment pipeline constructs 2 million post-training samples, with 1.5 million for supervised fine-tuning and 500k for reinforcement learning. The reinforcement learning stage employs a trajectory-corrected GRPO algorithm with dual-clip mechanisms, soft-length penalties, and entropy regularization. According to preliminary WMT 2025 results, the In2x model achieved first place in the unrestricted track and second place overall for Japanese-related translation tasks without task-specific fine-tuning.

## Key Results
- Achieved first place in the unrestricted track and second place overall for Japanese-related translation tasks at WMT 2025
- Demonstrated superior performance compared to large-scale proprietary models like Gemini-2.5-Pro, GPT-4.1, Claude-4, and DeepSeek-V3
- Validated effectiveness of systematic knowledge transfer from English LLMs to Japanese without task-specific fine-tuning

## Why This Works (Mechanism)
The approach works by systematically transferring English LLM capabilities to Japanese through structured curriculum design, cross-lingual alignment, and preference signals that reward naturalness. The three-phase post-training process addresses the specific challenges of Japanese translation, including cultural appropriateness and expressive quality. The trajectory-corrected GRPO algorithm with dual-clip mechanisms and soft-length penalties helps optimize translation quality while maintaining linguistic naturalness.

## Foundational Learning
- **Curriculum design**: Why needed - To systematically build language capabilities from basic to advanced levels; Quick check - Verify progression through training phases matches intended capability development
- **Cross-lingual alignment**: Why needed - To bridge structural and cultural differences between English and Japanese; Quick check - Assess alignment quality through translation consistency metrics
- **Preference signals**: Why needed - To reward naturalness and cultural appropriateness in translations; Quick check - Validate preference model accuracy on human-annotated samples
- **Trajectory correction**: Why needed - To stabilize RL optimization in complex translation spaces; Quick check - Monitor reward stability during training
- **Dual-clip mechanisms**: Why needed - To prevent extreme policy updates while maintaining learning progress; Quick check - Track policy variance during training
- **Entropy regularization**: Why needed - To maintain exploration and prevent premature convergence; Quick check - Measure entropy decay patterns

## Architecture Onboarding
**Component map**: English LLM base model -> Curriculum training phases (STEM/humanities balance, long-text refinement, fast annealing) -> Cross-lingual alignment module -> Preference signal integration -> Trajectory-corrected GRPO optimization

**Critical path**: Data preparation and corpus construction -> Supervised fine-tuning (1.5M samples) -> Reinforcement learning with trajectory-corrected GRPO (500k samples) -> Model evaluation and ranking

**Design tradeoffs**: The approach prioritizes systematic knowledge transfer over task-specific optimization, trading potential fine-tuning benefits for generalizability. The dual-clip mechanisms in GRPO balance stability with learning speed, while the preference signal system emphasizes naturalness over literal accuracy.

**Failure signatures**: Potential failures include misalignment between English and Japanese linguistic structures, preference signal bias toward specific translation styles, and reinforcement learning instability due to trajectory correction complexity.

**First experiments**: 1) Validate corpus quality and diversity metrics for the 2M post-training samples; 2) Test trajectory-corrected GRPO convergence compared to standard GRPO implementations; 3) Benchmark translation quality across multiple Japanese-English language pairs.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unclear including the detailed technical specifications of the trajectory-corrected GRPO algorithm, corpus construction methodology, and generalizability beyond Japanese-related tasks.

## Limitations
- Lack of detailed technical validation for the proposed trajectory-corrected GRPO algorithm
- No ablation studies or comparative analysis with standard GRPO implementations
- Corpus construction process not fully specified, raising questions about sample quality and potential biases

## Confidence
- High confidence in the general framework of LLM knowledge transfer through curriculum design, cross-lingual alignment, and preference signals
- Medium confidence in the specific implementation details of the three post-training phases and the alignment pipeline
- Low confidence in the superiority claims over proprietary models and the final WMT 2025 ranking results due to reliance on preliminary data

## Next Checks
1. Conduct ablation studies comparing the trajectory-corrected GRPO with standard GRPO implementations to isolate the impact of the dual-clip mechanisms, soft-length penalties, and entropy regularization
2. Provide detailed corpus statistics and quality control measures for the 2 million post-training samples, including diversity metrics and potential bias analysis
3. Perform standardized benchmarking across multiple language pairs and tasks, with statistical significance testing, to validate the generalizability and robustness of the proposed approach beyond Japanese-related translation