---
ver: rpa2
title: 'StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in
  Streaming Videos'
arxiv_id: '2512.01707'
source_url: https://arxiv.org/abs/2512.01707
tags:
- gaze
- object
- video
- fixation
- streaming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STREAMGAZE is the first benchmark for gaze-guided streaming video
  understanding, addressing the gap in evaluating how multimodal large language models
  (MLLMs) use human gaze signals for temporal reasoning and proactive prediction.
  It introduces a semi-automatic pipeline that aligns egocentric videos with raw gaze
  trajectories via fixation extraction, region-specific visual prompting, and scanpath
  construction to generate spatio-temporally grounded QA pairs.
---

# StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos

## Quick Facts
- arXiv ID: 2512.01707
- Source URL: https://arxiv.org/abs/2512.01707
- Reference count: 40
- Key outcome: First benchmark for gaze-guided streaming video understanding, revealing substantial MLLM performance gaps on temporal reasoning and proactive prediction tasks.

## Executive Summary
STREAMGAZE introduces the first benchmark for evaluating how multimodal large language models (MLLMs) use human gaze signals for temporal reasoning and proactive prediction in streaming videos. The benchmark addresses a critical gap in streaming video understanding by requiring models to interpret real-time gaze, track attention shifts, and infer intentions from only past and currently observed frames. Through a semi-automatic pipeline that aligns egocentric videos with raw gaze trajectories, STREAMGAZE generates 8,521 QA pairs across 10 tasks covering past, present, and proactive reasoning. Experiments show substantial performance gaps between state-of-the-art MLLMs and human performance, revealing limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction.

## Method Summary
STREAMGAZE's method involves a semi-automatic pipeline that converts raw gaze trajectories into spatio-temporally grounded QA pairs. The process begins with gaze projection using camera intrinsics, followed by fixation extraction using spatial stability (dispersion threshold), temporal stability (minimum duration), and scene consistency (hue-saturation histogram correlation). Region-specific visual prompting marks attended vs. peripheral regions using FOV definition (15° angular radius), enabling object extraction via MLLMs. Scanpaths are constructed to encode temporal reasoning constraints, and QA pairs are generated through LLM prompting with human verification (~83% correctness). The benchmark evaluates models on 10 tasks requiring reasoning from only past/current frames with gaze signals, using accuracy metrics and multi-trigger accuracy for proactive tasks.

## Key Results
- All tested MLLMs (GPT-4o, InternVL-3.5, ViSpeak) show substantial performance gaps compared to human performance (0.827) across all 10 tasks
- Streaming models (VideoLLM-Online) fail completely on past tasks (0.0 accuracy) and produce generic responses instead of task-specific answers
- None of the gaze prompting strategies (textual, visual, salience map) consistently outperform the gaze-free baseline, suggesting architectural limitations in current MLLMs
- Proactive tasks show high false positive rates (InternVL3.5: 83.6% FP rate on OAA), indicating poor intention modeling

## Why This Works (Mechanism)

### Mechanism 1: Fixation Extraction
Raw gaze points are clustered into fixation intervals using point-wise spatial stability (dispersion threshold), temporal stability (minimum duration), and scene consistency (hue-saturation histogram correlation across frames). This converts continuous jittery signals into discrete attention events. Core assumption: Stable gaze clusters correspond to intentional visual attention rather than noise or saccades. Break condition: Rapid ego-motion causing continuous scene changes may fragment fixations or create spurious attention clusters.

### Mechanism 2: Region-Specific Visual Prompting
FOV regions (circular patches centered on gaze) are cropped with red fixation markers; out-of-FOV regions mask the center and prompt MLLMs to extract only peripheral objects. This spatial decomposition enables controllable task difficulty. Core assumption: The 15° perifoveal radius accurately captures human perceptual focus across varying video resolutions. Break condition: Small or partially visible objects at FOV boundaries may be ambiguously classified.

### Mechanism 3: Scanpath Sequences
Fixation-level object sets are ordered chronologically to form scanpaths S = {(O_fov, O_out)}^N. QA tasks query transitions, sequences, and predictions that require models to integrate gaze history rather than process frames independently. Core assumption: Gaze transition patterns reveal user intention and can predict future actions. Break condition: Non-linear attention patterns (revisiting objects, parallel monitoring) may not fit sequential transition assumptions.

## Foundational Learning

- **Eye-tracking fundamentals (fixations, saccades, scanpaths)**: The entire pipeline depends on converting raw gaze into fixations; misunderstanding what constitutes stable attention will break data quality. Quick check: Can you explain why minimum duration thresholds matter for distinguishing fixations from saccades?

- **Egocentric video characteristics (ego-motion, FOV constraints)**: Camera shake and head movement constantly shift what's visible; preprocessing must account for this when projecting gaze coordinates. Quick check: How would continuous head rotation affect the assumption that gaze clusters indicate stable object attention?

- **Streaming vs. offline video understanding**: Tasks explicitly require reasoning from only past/current frames; models must not access future context during evaluation. Quick check: What's the difference between the temporal windows for "past" tasks [0, t_q] vs. "present" tasks [t_q-60, t_q]?

## Architecture Onboarding

- **Component map**: Raw gaze + egocentric video -> [Preprocessing] Gaze projection, coordinate alignment -> [Fixation Extraction] Spatial/temporal/scene filtering -> [Region-specific Object Extraction] FOV/out-of-FOV MLLM prompting -> [Scanpath Construction] Temporal sequence assembly -> [QA Generation + Human Verification] Task-specific question creation

- **Critical path**: Fixation extraction quality directly determines all downstream tasks. If fixations are too fragmented (high false positives) or too sparse (missed attention), QA pairs will be misaligned with actual user attention.

- **Design tradeoffs**: Larger FOV radius → easier object identification but less precise attention grounding; stricter fixation thresholds → fewer but more reliable QA pairs; automated vs. human verification → scalability vs. quality (paper achieves ~83% correctness rate with semi-automatic approach)

- **Failure signatures**: Models over-relying on frame-local visual cues while ignoring gaze signals (evidenced by gaze-prompting strategies not consistently outperforming gaze-free baseline); high false positive rates on proactive tasks (InternVL3.5-8B: 83.6% false positives on OAA); complete failure on temporal reasoning tasks (VideoLLM-online: 0.0 accuracy on all past tasks)

- **First 3 experiments**: 1) Reproduce fixation extraction on a small video subset with varying τ_scene thresholds (0.85, 0.90, 0.95) to visualize how scene consistency affects detected fixation count and duration distribution; 2) Validate region-specific visual prompting by comparing MLLM object extraction accuracy on manually annotated FOV/out-of-FOV regions for 20 sample fixations; 3) Ablate gaze input strategies (textual coordinates, visual markers, salience maps) on a single task type to confirm paper's finding that salience maps help present/proactive tasks but no strategy universally improves performance.

## Open Questions the Paper Calls Out

### Open Question 1: Gaze-Guided Proactive Reasoning
How can streaming MLLMs be architecturally enhanced to perform robust gaze-guided proactive reasoning, rather than relying solely on visual prompting overlays? Current architectures treat gaze as an auxiliary visual input rather than as a fundamental reasoning signal. Evidence: None of the prompting strategies consistently outperform the gaze-free baseline, suggesting architectural limitations.

### Open Question 2: Heterogeneous Task Requirements
What task-aware or dynamically integrated approaches can effectively handle the heterogeneous requirements across the ten STREAMGAZE tasks? Different tasks respond unevenly to the same input gaze or reasoning strategies. Evidence: A single, uniform prompting or reasoning strategy is insufficient as shown by varying task-wise performance.

### Open Question 3: Curriculum Learning for Task Balance
Can curriculum learning or task-balanced training strategies improve MLLM performance and uniformity across gaze-guided streaming tasks? STREAMGAZE spans ten heterogeneous tasks with inherently imbalanced sample sizes. Evidence: Performance does not increase uniformly across tasks even with combined training data.

### Open Question 4: Situational Perception and Temporal Memory
How can MLLMs develop the broader situational perception and richer temporal memory needed to close the human-model gap on Scene Recall and Object Transition Prediction tasks? Current models over-focus on foreground gaze objects while neglecting surrounding scene context. Evidence: Large disparity between Scene Recall (SR) and Object Transition Prediction (OTP) suggests models struggle with background object representation and gaze transition modeling.

## Limitations

- **Threshold Specification Gaps**: Critical parameters like rthresh (spatial dispersion threshold) and τdur (minimum fixation duration) lack explicit numerical values, requiring manual tuning during reproduction
- **Dataset Accessibility**: Exact gaze trajectory quality, temporal sampling rates, and annotation completeness remain unspecified across the three egocentric datasets
- **Generalization Boundaries**: Results focus on short-term (<60s) gaze-driven reasoning and may not extend to longer temporal horizons or scenarios with rapid attention shifts

## Confidence

- **High Confidence**: The core claim that current MLLMs struggle with gaze-guided temporal reasoning (evidenced by performance gaps across all 10 tasks, with several models scoring 0.0 on past tasks)
- **Medium Confidence**: The effectiveness of the semi-automatic pipeline for generating spatio-temporally grounded QA pairs, given the reported 83% human-verified correctness rate but lack of detailed error analysis
- **Low Confidence**: The generalizability of salience map prompting benefits across different MLLM architectures, as results show task-specific rather than universal improvements

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary fixation extraction parameters (rthresh, τdur, τscene) on a validation subset to determine their impact on QA pair quality and downstream model performance

2. **Cross-Dataset Robustness Test**: Apply the benchmark to an independent egocentric dataset with gaze data to verify whether performance patterns hold across different video content and gaze signal characteristics

3. **Temporal Window Ablation**: Evaluate model performance on reduced temporal windows (e.g., ω=30s, ω=90s) to identify whether the 60s window is optimal or introduces unnecessary complexity for certain task types