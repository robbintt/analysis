---
ver: rpa2
title: Superposition in Graph Neural Networks
arxiv_id: '2509.00928'
source_url: https://arxiv.org/abs/2509.00928
tags:
- graph
- features
- node
- pooling
- superposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a representation-centric framework to study\
  \ superposition\u2014the phenomenon where multiple features share the same directions\u2014\
  in graph neural networks. Using controlled datasets with unambiguous graph concepts,\
  \ the authors extract features as class-conditional centroids (task-aligned) and\
  \ linear-probe directions (model-decodable), then analyze their geometry with basis-invariant\
  \ metrics: Effective Rank, Superposition Index (SI), and Welch-Normalized Overlap\
  \ (WNO)."
---

# Superposition in Graph Neural Networks

## Quick Facts
- arXiv ID: 2509.00928
- Source URL: https://arxiv.org/abs/2509.00928
- Reference count: 40
- Primary result: Introduces a representation-centric framework to study feature superposition in GNNs using controlled datasets and geometric metrics

## Executive Summary
This work presents a systematic framework to analyze superposition—the sharing of representational directions by multiple features—in Graph Neural Networks. The authors develop a methodology using controlled synthetic datasets with unambiguous graph concepts, extracting features as class-conditional centroids (task-aligned) and linear-probe directions (model-decodable). They analyze representational geometry using basis-invariant metrics including Effective Rank, Superposition Index, and Welch-Normalized Overlap. The study reveals that increasing width follows a three-phase pattern in overlap, topology imprints overlap onto node-level features that pooling partially remixes into task-aligned axes, and that sharper pooling increases axis alignment while reducing channel sharing.

## Method Summary
The authors introduce a representation-centric framework to study superposition in GNNs using controlled synthetic datasets with unambiguous graph concepts. They extract features as class-conditional centroids (task-aligned) and linear-probe directions (model-decodable), then analyze their geometry using basis-invariant metrics: Effective Rank, Superposition Index (SI), and Welch-Normalized Overlap (WNO). The framework allows systematic examination of how representational geometry evolves with architectural choices like width and pooling mechanisms. Experiments compare node-level and pooled representations across different pooling strategies, revealing how topological information propagates through the network and how pooling can mitigate node-level entanglement while preserving accuracy.

## Key Results
- Increasing model width follows a three-phase pattern in overlap: initial increase, plateau, then decrease
- Topology imprints overlap onto node-level features that pooling partially remixes into task-aligned axes
- Sharper pooling increases axis alignment and reduces channel sharing
- Shallow models can settle into metastable low-rank embeddings

## Why This Works (Mechanism)
The framework works by explicitly disentangling task-aligned features (class centroids) from model-decodable features (linear-probe directions), then quantifying their geometric relationships using basis-invariant metrics. This separation allows researchers to observe how representational geometry evolves during training and how architectural choices affect feature sharing. The use of synthetic datasets with unambiguous graph concepts provides ground truth for validating that extracted features truly capture the intended concepts. The combination of node-level and pooled analysis reveals how local node representations aggregate into global task representations through pooling operations.

## Foundational Learning

**Graph Neural Networks (GNNs)**
Why needed: Understanding the basic architecture of message passing between nodes is essential for grasping how information flows and features are learned
Quick check: Can explain how node features are updated through aggregation of neighbor information

**Superposition in Neural Networks**
Why needed: The concept of multiple features sharing representational directions is central to understanding representational geometry
Quick check: Can define superposition and explain why it matters for model interpretability

**Representation Geometry**
Why needed: The framework relies on geometric analysis of feature spaces using metrics like Effective Rank and overlap
Quick check: Can explain what Effective Rank measures and why it's basis-invariant

**Pooling in GNNs**
Why needed: Pooling operations are shown to significantly affect feature alignment and superposition
Quick check: Can describe different pooling strategies (mean, max, attention) and their effects on representations

## Architecture Onboarding

**Component Map**
Data Generation -> Feature Extraction -> Geometric Analysis -> Result Interpretation

**Critical Path**
The most critical sequence is controlled data generation with unambiguous concepts → feature extraction as class centroids and probe directions → geometric analysis using SI and WNO metrics → interpretation of overlap patterns

**Design Tradeoffs**
- Synthetic vs real data: Synthetic provides ground truth but may not generalize
- Task-aligned vs model-decodable: Different perspectives on what constitutes a "feature"
- Node-level vs pooled analysis: Local vs global representational properties

**Failure Signatures**
- If metrics don't converge during training, may indicate unstable feature learning
- If overlap patterns don't follow expected three-phase behavior, may suggest dataset issues
- If pooling doesn't reduce node-level entanglement, may indicate pooling mechanism problems

**First Experiments**
1. Verify three-phase width-overlap relationship on a simple synthetic dataset
2. Compare node-level vs pooled representations for a basic pooling strategy
3. Test effect of pooling sharpness on axis alignment using attention-based pooling

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on synthetic datasets with unambiguous graph concepts that may not capture real-world complexity
- Focus on task-aligned and model-decodable features may overlook other important aspects of graph representation learning
- Limited exploration of deeper GNN architectures and their superposition characteristics

## Confidence

**High confidence**: Three-phase pattern in overlap as width increases, connection between pooling sharpness and axis alignment
**Medium confidence**: Topology imprinting overlap onto node-level features, shallow models settling into metastable embeddings
**Low confidence**: Pooling fully mitigating node-level entanglement while preserving accuracy

## Next Checks

1. Validate findings on real-world graph datasets (social networks, biological graphs) to assess generalizability
2. Test superposition and pooling effects across broader range of GNN architectures including deeper models
3. Investigate role of dynamic or context-dependent features in superposition scenarios with changing node attributes or graph structures