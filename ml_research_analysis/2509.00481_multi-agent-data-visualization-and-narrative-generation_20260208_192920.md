---
ver: rpa2
title: Multi-Agent Data Visualization and Narrative Generation
arxiv_id: '2509.00481'
source_url: https://arxiv.org/abs/2509.00481
tags:
- data
- visualization
- agents
- generation
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a lightweight multi-agent system for automated
  data analysis and narrative generation. The system employs specialized agents to
  analyze tabular datasets, generate visualizations, and create coherent narratives
  for insight communication.
---

# Multi-Agent Data Visualization and Narrative Generation

## Quick Facts
- arXiv ID: 2509.00481
- Source URL: https://arxiv.org/abs/2509.00481
- Reference count: 19
- Key outcome: Lightweight multi-agent system for automated data analysis, visualization generation, and narrative creation with 75% story retention and 6-11 seconds runtime per story

## Executive Summary
This paper presents a modular multi-agent system for transforming tabular datasets into web-based data visualization reports with coherent narratives. The architecture employs specialized agents (Data Analysis, Story Generation, Visualization Generation, Code Generation, Visualization Execution, Visualization Critique, Story Execution, Report Generation, Report Execution, and Monitoring) that operate in parallel branches and aggregate outputs through structured handoffs. The system uses a hybrid approach combining LLM reasoning for high-level tasks with deterministic components for code execution, validation, and templating, enabling transparent and reliable outputs. Evaluated across four diverse datasets ranging from 418 to 4,111 rows, the system demonstrates strong generalizability with 75% story retention rates and minimal dependencies, supporting sustainable human-AI collaboration in data communication.

## Method Summary
The system uses role-based multi-agent orchestration where agents process data through specialized pipelines. It starts with a Data Analysis Agent converting CSV data to JSON metadata schema, followed by parallel Story and Visualization Generation agents that create narrative ideas and chart concepts. Code Generation produces Plotly visualization code, which Visualization Execution agents execute with retry mechanisms, and Visualization Critique agents refine outputs. Story Execution agents curate narratives using visualizations, Report Generation agents order content, and Report Execution agents render HTML reports using Jinja2 templates. The hybrid architecture externalizes deterministic logic (code execution, validation via Pydantic, templating) from LLM-driven reasoning, using a single LLM dependency for all reasoning tasks. The system employs Python multiprocessing for parallel agent execution and includes monitoring for metrics collection.

## Key Results
- 75% story retention rate across four diverse datasets (418-4,111 rows)
- 6-11 seconds runtime per story generation
- Strong generalizability across datasets with varying sizes and characteristics
- Minimal dependencies with lightweight single-LLM architecture

## Why This Works (Mechanism)

### Mechanism 1
Role-based agent specialization improves output quality through focused expertise at each pipeline stage. The system decomposes data-to-communication into discrete roles (Data Analysis → Story Agents → Visualization Agents → Report Agents), where each agent receives structured inputs and produces typed outputs (CSV→JSON schema→story ideas→Plotly code→HTML). This enables focused processing without context dilution. Core assumption: Task decomposition into specialized roles produces higher-quality outputs than monolithic single-agent approaches. Evidence: System uses specialized agents for distinct workflow stages. Break condition: When inter-agent dependencies create bottlenecks or datasets require deeply cross-cutting reasoning.

### Mechanism 2
Hybrid architecture externalizing critical logic from LLMs to deterministic components improves transparency and reliability. LLMs handle high-level reasoning and narrative generation; deterministic Python modules handle code execution, error handling, templating (Jinja2), and validation (Pydantic). Retry mechanisms and metrics collection are implemented deterministically. Core assumption: Deterministic components are more reliable for execution, validation, and state management than LLM-generated logic. Evidence: System combines hybrid multi-agent architecture with deterministic components. Break condition: When domain-specific logic cannot be easily externalized or deterministic rules become brittle.

### Mechanism 3
Independent parallel agent processing with aggregation supports generalizability and computational efficiency. Visualization Generation, Code Generation, and Story Generation agents operate in parallel branches; outputs are aggregated by downstream agents (Story Execution, Report Generation). This reduces sequential dependencies and enables 6-11 seconds per story runtime. Core assumption: Parallelization of generative tasks is feasible without strict ordering constraints. Evidence: System achieves 6-11 seconds runtime with parallel processing. Break condition: When parallel outputs conflict, requiring expensive reconciliation or regeneration.

## Foundational Learning

- Concept: Role-based multi-agent orchestration
  - Why needed here: System's effectiveness depends on understanding how agents coordinate, pass structured data (JSON schemas, Pydantic models), and manage state across the pipeline
  - Quick check question: Can you diagram the handoff flow from Data Analysis Agent to Report Execution Agent, identifying where multiplexing and aggregation occur?

- Concept: Hybrid LLM-deterministic system design
  - Why needed here: Critical logic is externalized from LLMs; engineers must decide what to delegate to LLMs vs. deterministic code
  - Quick check question: Given a new agent task, what criteria would you use to decide whether to implement it as LLM-driven or deterministic?

- Concept: Pydantic models for structured agent I/O
  - Why needed here: Agents exchange typed, validated data; Pydantic ensures schema compliance and reduces downstream errors
  - Quick check question: How would you define a Pydantic model for Story Execution Agent output that captures ranked stories with visualization references?

## Architecture Onboarding

- Component map: Data Analysis Agent → (parallel: Story Generation + Visualization Generation) → Code Generation → Visualization Execution → Visualization Critique → Story Execution → Report Generation → Report Execution. Monitoring runs orthogonally.

- Critical path: Data Analysis→(parallel: Story Generation + Visualization Generation)→Code Generation→Visualization Execution→Visualization Critique→Story Execution→Report Generation→Report Execution. Monitoring runs orthogonally.

- Design tradeoffs:
  - Lightweight single-LLM dependency vs. richer multi-model ensembles (chose simplicity, portability)
  - Deterministic externalization vs. end-to-end LLM flexibility (chose transparency, reliability)
  - Parallel generation vs. sequential consistency (chose efficiency, accepts reconciliation overhead)

- Failure signatures:
  - LLM code generation syntax errors triggering retries (mitigated by retry mechanisms)
  - Missing semantic column descriptions degrading story and visualization quality
  - Low story retention rates indicating filtering over-aggression or poor story-visualization alignment

- First 3 experiments:
  1. Run system on new tabular dataset (500-1000 rows) with and without column descriptions; measure story retention and visualization error rates
  2. Introduce deterministic validation step before Code Generation; compare retry rates and runtime to baseline
  3. Replace single LLM with smaller model for Story Generation only; evaluate narrative coherence and story retention

## Open Questions the Paper Calls Out

### Open Question 1
How can human-in-the-loop mechanisms be effectively integrated to validate intermediate steps for complex, domain-specific datasets? The paper states human-in-the-loop is required to validate and verify each intermediate step, especially for complex domain-specific datasets. This validation layer is conceptual rather than implemented. Evidence: User study measuring reduction of semantic errors when domain experts intervene at specific pipeline checkpoints.

### Open Question 2
How can the system be extended to support interactive capabilities for reader-driven discovery beyond static narratives? The discussion notes reports could benefit from interactive capabilities to enable reader-driven discovery through exploration. Current outputs are static HTML reports without dynamic user query support. Evidence: Implementation of interactive query layer and comparative evaluation of insight depth against static reports.

### Open Question 3
How can the system maintain narrative quality when semantic descriptions of dataset columns are missing or sparse? The paper highlights that missing semantic descriptions can significantly affect generated story ideas and visualization interpretation. The system's reliance on metadata suggests potential fragility with raw or poorly documented datasets. Evidence: Ablation studies removing column descriptions and measuring resulting degradation in story coherence and visualization relevance.

## Limitations
- Missing specific LLM prompts and model identity, critical for reproducibility and code generation reliability
- Story retention metric definition unspecified, making performance benchmarking difficult
- Limited corpus evidence for hybrid externalization pattern compared to orchestration-focused approaches
- Claims about generalizability across diverse datasets weakly supported by only 4 datasets

## Confidence

- **High confidence**: Modular multi-agent architecture and parallelization approach are well-specified and technically sound
- **Medium confidence**: 75% story retention rate and 6-11 second runtime are plausible but cannot be independently verified without prompt details
- **Low confidence**: Claims about generalizability across diverse datasets are weakly supported by limited validation

## Next Checks

1. Test system with systematically varied prompts (with/without few-shot examples, different instruction styles) to measure impact on story retention and code execution success rates

2. Replace single LLM with different model sizes/capabilities across all agent roles to quantify contribution of model choice to narrative quality and runtime

3. Compare system performance on datasets with detailed vs. cryptic column descriptions to measure effect of semantic metadata on visualization relevance and story coherence