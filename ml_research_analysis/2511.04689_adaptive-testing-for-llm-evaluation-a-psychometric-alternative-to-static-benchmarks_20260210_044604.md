---
ver: rpa2
title: 'Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static
  Benchmarks'
arxiv_id: '2511.04689'
source_url: https://arxiv.org/abs/2511.04689
tags:
- items
- atlasse
- ability
- item
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ATLAS introduces an adaptive testing framework for LLM evaluation
  based on Item Response Theory that dynamically selects items using Fisher information
  to achieve measurement precision. The framework reduces the number of required benchmark
  items by up to 90% while maintaining accuracy - for example, achieving 0.157 MAE
  on HellaSwag with only 41 items versus 5,600 in the full benchmark.
---

# Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks

## Quick Facts
- **arXiv ID**: 2511.04689
- **Source URL**: https://arxiv.org/abs/2511.04689
- **Reference count**: 39
- **Primary result**: Reduces required benchmark items by up to 90% while maintaining accuracy through adaptive item selection

## Executive Summary
This paper introduces ATLAS, an adaptive testing framework for LLM evaluation based on Item Response Theory (IRT). Unlike static benchmarks that treat all items equally, ATLAS dynamically selects questions using Fisher information to achieve measurement precision with fewer items. The framework demonstrates that it can achieve similar accuracy to full benchmarks using only 10% of the items - for example, 0.157 MAE on HellaSwag with just 41 items versus 5,600 in the complete benchmark. ATLAS also provides ability estimates that better distinguish between models with identical accuracy scores by accounting for item difficulty and discrimination properties.

## Method Summary
ATLAS employs Item Response Theory to model the relationship between LLM ability and item characteristics through item characteristic curves (ICCs). The framework selects items adaptively by maximizing Fisher information at each step, which identifies questions that provide the most information about the model's ability given its current estimated performance. This approach reduces the total number of items needed while maintaining measurement precision. The system estimates model ability rather than simply counting correct answers, allowing it to distinguish between models that achieve the same accuracy but differ in their ability to handle items of varying difficulty. The framework was validated across five major benchmarks including HellaSwag, MMLU, and TruthfulQA.

## Key Results
- Achieves 0.157 MAE on HellaSwag with only 41 items versus 5,600 items in full benchmark
- Reduces required items by up to 90% while maintaining evaluation accuracy
- 23-31% of models shift by more than 10 rank positions when evaluated using ability estimates versus raw accuracy

## Why This Works (Mechanism)
ATLAS works by treating LLM evaluation as a psychometric measurement problem rather than a simple accuracy-counting exercise. Item Response Theory models the probability of correct responses based on both model ability and item characteristics (difficulty and discrimination). By selecting items that maximize Fisher information - questions where the model's estimated ability creates maximum uncertainty about its true ability - ATLAS efficiently pinpoints model capabilities. This adaptive approach focuses on questions that provide the most information about ability differences rather than asking all questions equally.

## Foundational Learning
- **Item Response Theory (IRT)**: A psychometric framework modeling the relationship between latent ability and item responses - needed to understand how model performance varies with item difficulty
- **Fisher Information**: A measure of how much information an observable random variable carries about an unknown parameter - needed to identify which items provide the most information about model ability
- **Item Characteristic Curves (ICCs)**: Mathematical functions describing the probability of correct response as a function of ability and item parameters - needed to predict model performance on different items
- **Ability Estimation**: The process of inferring latent model capability from observed responses - needed to move beyond simple accuracy counting
- **Adaptive Testing**: Dynamic item selection based on previous responses - needed to minimize the number of questions while maximizing information gained
- **Discrimination Parameter**: A measure of how well an item differentiates between models of slightly different abilities - needed to select items that effectively distinguish models

## Architecture Onboarding
**Component map**: Benchmarks (with ICCs) -> IRT Model -> Fisher Information Calculator -> Item Selector -> Model Ability Estimator -> Evaluation Results
**Critical path**: Item selection based on current ability estimate → model response → ability update → next item selection
**Design tradeoffs**: Information efficiency vs. coverage of failure modes - adaptive testing optimizes precision but may miss rare edge cases
**Failure signatures**: Poor ICC calibration leads to suboptimal item selection; incorrect ability estimates cause information-maximizing choices to be suboptimal
**First experiments**:
1. Validate ICC construction process on a small subset of items before full adaptive testing
2. Compare ability estimates from ATLAS against traditional accuracy scores on benchmark subsets
3. Test adaptive item selection on models with known ability differences to verify ranking accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Requires reliable item characteristic curves for each benchmark item, which may not be available for many datasets
- The 90% reduction in items comes at potential cost of missing rare failure modes that appear only in discarded items
- Assumes ICCs are already constructed and validated, creating significant upfront requirements

## Confidence
- **High confidence**: 23-31% of models shifting by more than 10 rank positions; 0.157 MAE on HellaSwag with 41 items
- **Medium confidence**: Generalizability of numerical results to other benchmarks; preservation of global performance structure while enabling finer discrimination

## Next Checks
1. Test ATLAS on emerging or specialized benchmarks where ICCs must be constructed from scratch to evaluate end-to-end viability
2. Compare ATLAS's coverage of failure modes against static benchmarks by analyzing which specific items are discarded
3. Validate the framework's performance when applied to non-NLP domains or multimodal evaluation tasks