---
ver: rpa2
title: Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov
  Functions
arxiv_id: '2505.10947'
source_url: https://arxiv.org/abs/2505.10947
tags:
- lyapunov
- stability
- function
- generalized
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of certifying stability for reinforcement
  learning (RL) policies using generalized Lyapunov functions. The key insight is
  that the classical Lyapunov decrease condition can be relaxed to a multi-step, weighted
  decrease condition, making it easier to construct stability certificates for learned
  policies.
---

# Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions

## Quick Facts
- arXiv ID: 2505.10947
- Source URL: https://arxiv.org/abs/2505.10947
- Reference count: 40
- Primary result: Generalized Lyapunov functions with multi-step decrease conditions certify stability of RL policies where classical methods fail, with 76.7±1.3 volume certified for inverted pendulum (M=2) versus 42.9±1.2 for classical method.

## Executive Summary
This paper addresses the challenge of certifying stability for reinforcement learning policies using Lyapunov functions. The key insight is relaxing the classical single-step decrease requirement to a weighted multi-step average decrease, allowing certification of policies with transient instability. The authors propose augmenting RL value functions with neural network residual terms and jointly learning state-dependent step weights to satisfy this generalized decrease condition. The method is evaluated on Gymnasium and DeepMind Control benchmarks, successfully certifying stability of RL policies where classical Lyapunov methods fail.

## Method Summary
The approach constructs Lyapunov candidates as V(x) = J^π_γ(x) + φ(x;θ₁) + β‖x‖², where J^π_γ is the pre-trained RL value function, φ is a learned residual network, and β‖x‖² ensures positive definiteness. Stability is certified via a generalized decrease condition requiring V to decrease on average over M steps with state-dependent weights σ. The residual and weight networks are trained to minimize violations of this condition using trajectories from the policy. Formal verification using α,β-CROWN bounds confirms stability over continuous state domains. For joint synthesis, the method optimizes both controller and certificate to maximize certified region volume while enforcing the decrease condition.

## Key Results
- The generalized Lyapunov approach certifies stability of RL policies on inverted pendulum and cartpole tasks where classical one-step methods fail
- Joint synthesis with generalized Lyapunov loss achieves 76.7±1.3 volume certified ROA for inverted pendulum (M=2) versus 42.9±1.2 for classical method
- Multi-step certification enables larger ROAs by accommodating transient instability during swing-up phases
- Verification complexity grows significantly with horizon M (Table 3b shows >90× increase from M=1 to M=3 for Quadrotor)

## Why This Works (Mechanism)

### Mechanism 1: Multi-Step Average Decrease Relaxation
The strict single-step Lyapunov decrease requirement (V(x_{t+1}) < V(x_t)) is relaxed to a weighted multi-step average decrease, allowing certification of policies with temporary increases in V during transient phases. This is formalized as requiring (1/M)Σσ_i(x)V(x_{t+i}) < V(x_t), which permits non-monotonic behavior as long as the weighted future values decrease sufficiently.

### Mechanism 2: Value Function Augmentation
The standard RL value function J^π_γ fails to be a Lyapunov function due to the discount factor γ, but becomes valid when augmented with a learned neural residual φ. This correction adjusts the geometry of the value function landscape to satisfy the generalized decrease condition, with the residual network compensating for the discounting effect.

### Mechanism 3: Joint Synthesis via Counter-Example Training
Jointly training the controller and certificate using multi-step loss expands the certified ROA beyond post-hoc certification. The optimization uses Projected Gradient Descent to find states where the decrease condition fails (falsification) and adds them to training, iteratively expanding the verified domain.

## Foundational Learning

- **Lyapunov Stability Theory**: Understanding that V(x) represents "energy" or "distance to equilibrium" and must strictly decrease over time for stability. Quick check: If V(x) increases at step k but decreases by step k+5, is it a valid classical Lyapunov function? (No, but it might be generalized).

- **Value Functions in RL (Discounting)**: The discount factor γ < 1 causes value functions to decay independently of system dynamics, breaking the strict Lyapunov decrease property. Quick check: Why does γ < 1 prevent J^* from being a Lyapunov function for stable systems?

- **Linear Matrix Inequalities (LMIs)**: LMIs provide computationally tractable sufficient conditions for stability in the linear (LQR) case. Quick check: In Theorem 4.4, what does LMI feasibility imply about stability relative to discount factor γ?

## Architecture Onboarding

- **Component map**: Policy Network (π) -> Value Network (J^π_γ) -> Residual Network (φ) -> Step-Weight Network (σ) -> Verifier (α,β-CROWN)

- **Critical path**: Sample initial states → Roll out policy for M steps → Compute Lyapunov violation loss → Optimize φ and σ parameters → Verify with α,β-CROWN

- **Design tradeoffs**: Larger M allows certification of more complex transient behaviors but increases verification complexity and computation time significantly.

- **Failure signatures**: High loss indicates condition cannot be satisfied; concentration of weights to step 1 degenerates to classical method; verification timeout occurs with large M.

- **First 3 experiments**: 1) Replicate LQR stability with varying discount factors, 2) Train certificate for fixed SAC pendulum policy and visualize non-monotonic V behavior, 3) Compare joint synthesis with classical vs generalized Lyapunov loss on ROA volumes.

## Open Questions the Paper Calls Out

### Open Question 1
How can the minimal viable certification horizon M be systematically determined for a given system and policy? The paper empirically selects M but provides no principled method to choose M a priori or bound its necessity.

### Open Question 2
Can the generalized Lyapunov framework scale to high-dimensional systems such as humanoids or dexterous manipulators? Current experiments are limited to low-dimensional systems (2–6 state dimensions).

### Open Question 3
Under what formal conditions do multi-step LMI formulations guarantee weaker feasibility than classical one-step LMIs? The paper empirically shows multi-step conditions certify more cases but lacks theoretical characterization.

### Open Question 4
How can formal verification be extended to jointly certify neural controllers, Lyapunov functions, and learned state-dependent step weights? Current verification uses fixed weights; state-dependent weight networks complicate bound propagation.

## Limitations
- Verification scalability remains a core bottleneck with CROWN-based bounds growing exponentially with horizon M
- The effectiveness of state-dependent weighting networks for formal verification is questionable due to additional nonlinearities
- Reliance on pre-trained value functions assumes reasonable cost estimates from RL training, which may not hold for different reward structures

## Confidence
- **High confidence**: Multi-step relaxation mechanism is theoretically sound and well-supported by LQR analysis
- **Medium confidence**: Value function augmentation works empirically but theoretical guarantees for nonlinear systems are limited
- **Low confidence**: Joint synthesis shows promising results but ablation studies are limited and falsification may get stuck

## Next Checks
1. Systematically measure CROWN verification time and certified ROA volume as functions of M for a fixed policy
2. Analyze learned step-weight distributions σ_i to verify they are not degenerate and correlate with transient behavior
3. Evaluate certificate performance when trained with perturbed or poorly estimated value functions to assess robustness