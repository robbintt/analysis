---
ver: rpa2
title: 'Tracing How Annotators Think: Augmenting Preference Judgments with Reading
  Processes'
arxiv_id: '2511.21912'
source_url: https://arxiv.org/abs/2511.21912
tags:
- reading
- annotators
- annotation
- annotator
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for capturing annotator reading
  processes during preference annotation tasks by using mouse tracking to record fine-grained
  reading behaviors. The authors created a dataset, PreferRead, containing 1,000 preference
  annotation items each annotated by three participants, capturing both preference
  judgments and detailed mouse-tracked reading behaviors including re-reading patterns,
  reading paths, and word-level gaze durations.
---

# Tracing How Annotators Think: Augmenting Preference Judgments with Reading Processes

## Quick Facts
- arXiv ID: 2511.21912
- Source URL: https://arxiv.org/abs/2511.21912
- Authors: Karin de Langis; William Walker; Khanh Chi Le; Dongyeop Kang
- Reference count: 0
- This paper introduces a novel method for capturing annotator reading processes during preference annotation tasks by using mouse tracking to record fine-grained reading behaviors.

## Executive Summary
This paper introduces a novel method for capturing annotator reading processes during preference annotation tasks by using mouse tracking to record fine-grained reading behaviors. The authors created a dataset, PreferRead, containing 1,000 preference annotation items each annotated by three participants, capturing both preference judgments and detailed mouse-tracked reading behaviors including re-reading patterns, reading paths, and word-level gaze durations. The analysis reveals that annotators re-read their chosen responses most frequently, rarely revisit prompts, and often skip the latter portions of rejected responses, suggesting cognitive economy in decision-making. Importantly, re-reading is associated with higher inter-annotator agreement, while longer reading paths and times correlate with lower agreement, indicating that reading behaviors provide valuable signals for understanding annotator reliability and decision-making in subjective NLP tasks.

## Method Summary
The study builds on the Mouse Tracking for Reading (MoTR) paradigm to capture reading behaviors during preference annotation. Using a JSPsych-based web interface, participants read blurred prompts and responses with text revealed incrementally by cursor movement. Mouse entry/exit events were recorded at character-level granularity, then consolidated to word-level fixations with durations filtered to 160-4000ms range. The PreferRead dataset contains 1,000 instances from the Helpful-Harmless dataset, each annotated by three participants (300 total) meeting criteria of native English speakers with ≥95% approval ratings. Metrics extracted include re-reading frequencies, reading path lengths, word coverage, and duration patterns, with analyses examining associations between these behaviors and inter-annotator agreement.

## Key Results
- Annotators re-read their chosen responses most frequently (38.4% of trials) compared to rejected responses (25.4%), with re-reading associated with higher inter-annotator agreement
- Annotators rarely revisit prompts and frequently skip latter portions of rejected responses, suggesting cognitive economy in decision-making
- Longer reading paths and times correlate with lower agreement, while re-reading behavior correlates with higher agreement, providing behavioral signals for annotator reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mouse tracking provides a scalable proxy for eye-tracking in capturing cognitive attention during annotation tasks.
- Mechanism: Users reveal blurred text incrementally by moving their cursor; hover durations on character spans approximate gaze fixations, which reflect cognitive processing. The method bypasses hardware requirements of eye-tracking while preserving behavioral signal.
- Core assumption: Mouse movements correlate sufficiently with visual attention to capture meaningful reading behavior patterns.
- Evidence anchors:
  - [abstract]: "fine-grained annotator reading behaviors obtained from mouse tracking"
  - [section 2]: "Mouse Tracking for Reading (MoTR) paradigm... shows comparable sensitivity to eye-tracking in detecting psycholinguistic effects"
  - [corpus]: Weak direct corpus support; related papers focus on annotation quality incentives rather than behavioral capture methods.
- Break condition: Tasks requiring extremely rapid reading (speed reading) or mobile/touch interfaces without precise cursor control.

### Mechanism 2
- Claim: Re-reading frequency signals annotator confidence and correlates with higher inter-annotator agreement.
- Mechanism: When annotators re-read a section after leaving it, they perform a confirmation check. Re-reading chosen responses (38.4% of trials vs. 25.4% for rejected) reflects validation of the decision rather than exploration.
- Core assumption: Re-reading behavior reflects metacognitive verification rather than simple confusion or distraction.
- Evidence anchors:
  - [abstract]: "annotators re-read their chosen response more often than rejected ones, with re-reading associated with higher inter-annotator agreement"
  - [section 4.3.2]: "re-reading is significantly more common among annotators who agree (χ²(1) = 11.25, p = 0.001)"
  - [corpus]: No direct corpus corroboration; related annotation quality work focuses on incentives, not behavioral signals.
- Break condition: When texts are so short that re-reading provides no new information, or when annotators have domain expertise enabling single-pass decisions.

### Mechanism 3
- Claim: Longer reading paths and times indicate annotator uncertainty and predict lower agreement.
- Mechanism: Path length (number of section transitions) and looping (back-and-forth between responses) reflect indecision. Annotators who cannot quickly identify a preference traverse more sections before deciding.
- Core assumption: Path complexity reflects genuine decision uncertainty rather than thoroughness or careful deliberation.
- Evidence anchors:
  - [abstract]: "Longer reading paths and times correlate with lower agreement"
  - [section 4.3.3]: "Annotators who disagree show slightly longer path lengths (M = 2.56 vs. M = 2.42; t = -3.3, p = 0.001)"
  - [corpus]: Weak support; corpus papers address disagreement sources but not behavioral path metrics.
- Break condition: Complex documents requiring legitimate multi-section navigation (legal, medical texts) where long paths reflect task demands rather than uncertainty.

## Foundational Learning

- Concept: **Fixation duration filtering (160ms–4000ms window)**
  - Why needed here: Raw mouse hover data includes noise from accidental movements and distracted pauses; filtering isolates cognitively meaningful reading events.
  - Quick check question: If a user hovers over a word for 50ms while quickly scrolling, should this be counted as a fixation? (Answer: No, below 160ms threshold.)

- Concept: **Krippendorff's alpha for inter-annotator agreement**
  - Why needed here: Measuring whether reading behaviors systematically relate to annotation quality requires a robust agreement metric for categorical judgments.
  - Quick check question: The paper reports α = 0.25—is this considered high or low agreement? (Answer: Fair agreement, typical for subjective tasks.)

- Concept: **Word coverage vs. reading completeness**
  - Why needed here: Interpreting skipped text requires distinguishing between cognitive economy (efficient decision-making) and carelessness; 100% coverage is not expected due to function word skipping.
  - Quick check question: If an annotator reads only 55% of response words, does this indicate poor quality? (Answer: Not necessarily—word coverage did not significantly differ between agreeing and disagreeing annotators.)

## Architecture Onboarding

- Component map:
  - JSPsych web app with blurred text revealed by cursor
  - Mouse event logging per character span
  - Data processing pipeline: character events → word-level aggregation → fixation filtering
  - Metrics extraction: re-read flags, path length, loop detection, word coverage, duration vectors
  - Output: PreferRead dataset with behavioral traces

- Critical path:
  1. Participant instruction and mouse-tracking practice (acclimatization)
  2. Trial presentation with randomized response positions
  3. Real-time mouse event logging
  4. Post-selection rationale capture
  5. Post-hoc fixation filtering and metric computation

- Design tradeoffs:
  - Naturalism vs. scalability: Mouse tracking is less natural than unobtrusive reading but enables remote, large-scale collection without eye-tracking hardware
  - Coverage vs. efficiency: Requiring full text reading might improve completeness but increase task time and cost; current design allows cognitive economy
  - Duration thresholds: 160ms–4000ms filter removes noise but may exclude valid very short or very long fixations

- Failure signatures:
  - Low word coverage (<10%): Indicates participant skipped trial or experienced interface issues—excluded from analysis
  - Excessive path length without re-reading: May indicate interface confusion rather than genuine indecision
  - Uniform duration across all words: Suggests cursor following without actual reading

- First 3 experiments:
  1. Baseline replication: Apply the mouse-tracking interface to a new domain (e.g., sentiment annotation) to test whether re-reading/agreement patterns generalize beyond preference tasks.
  2. Threshold sensitivity analysis: Vary fixation duration bounds (e.g., 100ms–3000ms) to measure impact on metric stability and agreement correlations.
  3. Intervention study: Force full-response reading before selection (e.g., disable submit button until 80% coverage) to test whether cognitive economy patterns change and whether agreement improves.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on mouse tracking as proxy for eye-tracking may not capture all aspects of reading behavior, particularly for fast readers or on touch-based devices
- Fixation duration filtering (160ms–4000ms) is an arbitrary threshold that may exclude valid cognitive events or include noise
- Dataset size (1,000 items × 3 annotators) provides statistical power but may be insufficient to detect subtle effects in reading behavior patterns

## Confidence

- **High Confidence**: The finding that re-reading chosen responses occurs more frequently than re-reading rejected ones (38.4% vs 25.4%). This pattern is consistently observed across agreement levels and has a clear cognitive interpretation as decision validation.
- **Medium Confidence**: The association between re-reading and higher inter-annotator agreement (χ²(1) = 11.25, p = 0.001). While statistically significant, this relationship could be influenced by unmeasured factors like task difficulty or participant expertise.
- **Medium Confidence**: The correlation between longer reading paths/times and lower agreement. The effect sizes are modest, and path length could reflect legitimate task complexity rather than uncertainty.

## Next Checks

1. Cross-domain replication: Apply the mouse-tracking interface to a different annotation task (e.g., sentiment classification or summarization quality) to test whether re-reading patterns and agreement correlations generalize beyond preference judgments.

2. Threshold sensitivity analysis: Systematically vary the fixation duration bounds (e.g., test 100ms–3000ms and 200ms–5000ms windows) to determine how robust the metric correlations are to different filtering parameters and whether results are sensitive to this choice.

3. Intervention validation: Design an experiment where annotators are required to achieve minimum word coverage before submitting their choice, then compare reading behaviors and agreement rates to the original protocol to test whether cognitive economy patterns are task artifacts or genuine behavioral signatures.