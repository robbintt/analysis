---
ver: rpa2
title: 'CFunModel: A "Funny" Language Model Capable of Chinese Humor Generation and
  Processing'
arxiv_id: '2503.20417'
source_url: https://arxiv.org/abs/2503.20417
tags:
- humor
- crosstalk
- joke
- chinese
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CFunModel, a large language model specifically
  designed for Chinese humor generation and processing. The authors constructed CFunSet,
  a comprehensive Chinese humor dataset comprising over 160,000 samples across multiple
  tasks including humor recognition, crosstalk response selection, joke generation,
  and joke explanation.
---

# CFunModel: A "Funny" Language Model Capable of Chinese Humor Generation and Processing

## Quick Facts
- arXiv ID: 2503.20417
- Source URL: https://arxiv.org/abs/2503.20417
- Reference count: 22
- Primary result: CFunModel achieves 91.70% accuracy in crosstalk response selection and 85.98% in humor recognition, outperforming GPT-4o and other models

## Executive Summary
CFunModel is a specialized Chinese language model developed for humor generation and processing. The researchers created CFunSet, a comprehensive dataset with over 160,000 Chinese humor samples across four tasks: humor recognition, crosstalk response selection, joke generation, and joke explanation. By performing supervised fine-tuning on Qwen2.5-7B-Instruct using this dataset, CFunModel demonstrates superior performance compared to popular LLMs like GPT-4o and DeepSeek-V3 on Chinese humor-related tasks.

## Method Summary
The authors constructed CFunSet, a large-scale Chinese humor dataset containing 160,000+ samples across multiple humor tasks. They then performed supervised fine-tuning on Qwen2.5-7B-Instruct using this curated dataset to develop CFunModel. The model was evaluated across various humor-related tasks including crosstalk response selection, humor recognition, joke generation, and joke explanation, with comparisons against baseline models including GPT-4o and DeepSeek-V3.

## Key Results
- CFunModel achieves 91.70% accuracy for Dougen and 88.99% for Penggen responses in crosstalk response selection
- The model demonstrates 85.98% accuracy in humor recognition tasks
- Case studies show CFunModel produces more coherent and humorous content compared to baseline models in joke and crosstalk generation

## Why This Works (Mechanism)
CFunModel works by leveraging supervised fine-tuning on a specialized Chinese humor dataset, allowing the model to learn domain-specific patterns and cultural nuances of Chinese humor. The approach enables the model to capture context-specific humor elements that general-purpose language models may miss.

## Foundational Learning
- **Chinese humor patterns**: Understanding cultural and linguistic nuances specific to Chinese humor (needed because humor is culturally contextual; quick check: accuracy on culturally-specific humor tasks)
- **Crosstalk dialogue structure**: Learning the specific format and timing of Chinese crosstalk performances (needed because crosstalk has unique structural requirements; quick check: response selection accuracy)
- **Humor recognition patterns**: Identifying what makes content humorous in Chinese context (needed because humor detection requires cultural understanding; quick check: accuracy on humor classification tasks)

## Architecture Onboarding
- **Component map**: Qwen2.5-7B-Instruct -> Supervised Fine-tuning -> CFunModel
- **Critical path**: Dataset preparation -> Supervised fine-tuning -> Evaluation across humor tasks
- **Design tradeoffs**: Specialized vs. general-purpose capabilities; Chinese-only vs. multilingual support
- **Failure signatures**: May struggle with non-Chinese humor; potential overfitting to CFunSet patterns
- **First experiments**: 1) Test on non-Chinese humor datasets, 2) Evaluate long-term deployment performance, 3) Conduct ablation studies on dataset components

## Open Questions the Paper Calls Out
None

## Limitations
- Exclusively focuses on Chinese language humor, limiting cross-cultural applicability
- Evaluation methodology may not fully capture nuanced humor perception across different backgrounds
- Potential overfitting to specific humor patterns present in the CFunSet dataset

## Confidence
- Outperforms GPT-4o and other models on Chinese humor tasks: Medium confidence
- Generalizability to real-world applications: Medium confidence
- Ability to generate novel humor beyond training distribution: Medium confidence

## Next Checks
1. Conduct cross-cultural transferability tests by evaluating CFunModel on non-Chinese humor datasets to assess generalization capabilities
2. Implement long-term deployment studies to monitor the model's humor generation quality across diverse real-world contexts over extended periods
3. Perform ablation studies to quantify the contribution of different dataset components to overall model performance