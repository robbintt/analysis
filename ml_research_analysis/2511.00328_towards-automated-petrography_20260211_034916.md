---
ver: rpa2
title: Towards Automated Petrography
arxiv_id: '2511.00328'
source_url: https://arxiv.org/abs/2511.00328
tags:
- mineral
- lithos
- should
- dataset
- section
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the LITHOS Benchmark, the largest and most
  diverse publicly available framework for automated petrographic analysis. LITHOS
  includes 211,604 high-resolution image patches and 105,802 expert-annotated mineral
  grains across 25 mineral categories, with annotations capturing mineral class, spatial
  coordinates, and geometric properties.
---

# Towards Automated Petrography

## Quick Facts
- **arXiv ID:** 2511.00328
- **Source URL:** https://arxiv.org/abs/2511.00328
- **Reference count:** 40
- **Primary result:** Dual-encoder transformer leveraging PPL and XPL polarization achieves higher mineral classification accuracy than single-modality baselines.

## Executive Summary
This paper introduces LITHOS, the largest and most diverse publicly available benchmark for automated petrographic analysis, featuring 211,604 high-resolution image patches and 105,802 expert-annotated mineral grains across 25 mineral categories. The authors propose LITHOS Baseline, a dual-encoder transformer architecture that processes paired plane-polarized (PPL) and cross-polarized (XPL) images to exploit complementary polarization information. The model consistently outperforms single-polarization baselines on both binary (quartz vs. non-quartz) and multi-class mineral classification tasks, demonstrating the value of polarization synergy. The dataset, code, and pretrained models are publicly released to support reproducibility and further research in automated petrography.

## Method Summary
The LITHOS Baseline employs a dual-encoder transformer architecture where two frozen ViT encoders process PPL and XPL images separately, preserving modality-specific features. These encoders feed into dual-decoder layers with cross-attention, allowing the model to learn relationships between polarization views before fusing them via a weighted sum. The system is trained first on a binary task (quartz vs. non-quartz) then fine-tuned for multi-class classification. Training uses frozen encoders to reduce computational load while the decoders learn to integrate cross-polarization features effectively.

## Key Results
- Dual-encoder architecture consistently outperforms single-modality baselines on both binary and multi-class mineral classification tasks.
- Cross-attention between decoders enables richer representations by conditioning features from one polarization on structural context from the other.
- LITHOS Benchmark provides 211,604 paired image patches and 105,802 expert annotations across 25 mineral categories, establishing a new standard for automated petrography research.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Processing paired polarization modalities (PPL and XPL) through separate, specialized encoders yields higher classification accuracy than single-modality inputs.
- **Mechanism:** Plane-polarized light (PPL) reveals color and relief, while cross-polarized light (XPL) reveals birefringence and extinction angles. By freezing pre-trained encoders for each modality, the model preserves these distinct feature sets before fusion, forcing the system to learn complementary rather than redundant representations.
- **Core assumption:** The visual features in PPL and XPL are sufficiently distinct that treating them as separate modalities is superior to processing them as stacked channels or a single stream.
- **Evidence anchors:**
  - [abstract] "...processes paired plane-polarized (PPL) and cross-polarized (XPL) images to leverage complementary polarization information."
  - [Section 4] "This design encourages each encoder to focus on distinct, complementary representations."
  - [corpus] Corpus evidence is weak; neighbor "LithOS" refers to an operating system, not this dataset.
- **Break condition:** If a mineral class has no distinguishing features in one modality (e.g., appears isotropic in XPL), the dual-encoder relies entirely on the other branch, reducing the "synergy" to mere ensemble averaging.

### Mechanism 2
- **Claim:** Cross-attention between decoder layers is the effective driver of "polarization synergy," outperforming simple feature summation.
- **Mechanism:** The dual-decoder applies self-attention within a modality and cross-attention to the opposite modality. This allows the model to condition features from the PPL branch based on structural context from the XPL branch (and vice versa) before fusion.
- **Core assumption:** The relationships between PPL and XPL features are spatially complex and cannot be captured by simply concatenating feature maps or averaging probabilities.
- **Evidence anchors:**
  - [Section 4] "...dual-decoder architecture... applies self-attention within their assigned feature set and cross-attention to features from the other polarization."
  - [Section 5] "This design facilitates richer representations and leads to more accurate classifications..."
- **Break condition:** If the spatial alignment between PPL and XPL image patches is imperfect (pixel-offset), cross-attention may attend to noise rather than correlated mineral features.

### Mechanism 3
- **Claim:** "Weak" supervision via major/minor axis intersection is sufficient for centering classification patches without full segmentation masks.
- **Mechanism:** Instead of pixel-perfect masks, the dataset provides vector paths (major/minor axes). The model uses the intersection of these paths as the centroid for the region of interest, reducing annotation cost while maintaining spatial focus.
- **Core assumption:** The geometric center of the intersecting axes correlates strongly with the most discriminative visual features of the grain.
- **Evidence anchors:**
  - [Section 3.1] "...offer a meaningful compromise between coarse annotations and full segmentation, enabling models to leverage spatial and structural representations..."
  - [Section 3.1] "Each patch was centered at the intersection of the major and minor axes..."
- **Break condition:** For highly irregular or non-elliptical grains, the axis intersection may fall on grain boundaries or empty space, misdirecting the model's attention.

## Foundational Learning

- **Concept: Optical Polarization Microscopy (PPL vs XPL)**
  - **Why needed here:** The model is designed specifically to fuse these two distinct physical views. Without understanding that PPL shows texture/color while XPL shows internal crystal structure, the dual-encoder design seems redundant.
  - **Quick check question:** Does the model process PPL and XPL as separate input channels (RGB+RGB) or as separate input streams? (Answer: Separate streams via dual encoders).

- **Concept: Vision Transformers (ViT) & Self-Attention**
  - **Why needed here:** The baseline architecture relies on ViT encoders and attention mechanisms. Understanding that ViT splits images into patches (tokens) is necessary to comprehend how the model handles "long-range dependencies" in mineral textures.
  - **Quick check question:** Why are the ViT encoders frozen during the fusion training phase?

- **Concept: Long-tailed Distribution in Geology**
  - **Why needed here:** The dataset is heavily imbalanced (Quartz dominates). Metrics like Macro-F1 are prioritized over Accuracy to ensure the model isn't just memorizing the majority class.
  - **Quick check question:** Why might high accuracy still indicate a failing model in this specific geological context?

## Architecture Onboarding

- **Component map:** PPL image → Frozen ViT encoder → Self-attention + Cross-attention → Weighted fusion → Classification head
  XPL image → Frozen ViT encoder → Self-attention + Cross-attention → Weighted fusion → Classification head

- **Critical path:** The flow moves from **Frozen Pretraining** (training single ViTs) → **Feature Extraction** → **Cross-Attention Decoding** → **Weighted Fusion**.

- **Design tradeoffs:**
  - **Frozen vs. Fine-tuned Encoders:** The authors freeze 90% of the 674M parameters to reduce computational load. This trades potential adaptation performance for training efficiency.
  - **Weighted Sum vs. Concatenation:** The authors chose a learnable weighted sum over projection layers for simplicity and fewer parameters, claiming similar performance.

- **Failure signatures:**
  - **The "Feldspar Confusion":** Misclassifying Sanidine, Microcline, and generic Feldspar due to similar optical properties (low relief, weak birefringence).
  - **Long-tail collapse:** High performance on Quartz/Monocrystalline but F1 < 0.2 on rare classes like "Heavy mineral" or "Opaque."

- **First 3 experiments:**
  1. **Reproduce Binary Baseline:** Train the single-modality ViTs on PPL-only and XPL-only to quantify the performance gap before implementing the dual-encoder.
  2. **Fusion Ablation:** Replace the Cross-Attention Decoder with a simple concatenation layer to verify if the "attention mechanism" is strictly necessary or if basic fusion suffices.
  3. **Patch Centroid Validation:** Visualize the input patches overlaid with the axis annotations to ensure the "intersection" logic correctly centers the grain for irregularly shaped minerals.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can incorporating the full geometric extent of the major and minor axes as weak supervision improve classification performance over point-based supervision?
  - **Basis in paper:** [explicit] Section 6 states the supervision strategy could be improved by using "not only the intersection... but also the full extent of those paths."
  - **Why unresolved:** The current baseline uses only the intersection point (single-grain supervision), leaving the potential of the full vector paths unexplored.
  - **What evidence would resolve it:** Comparative experiments training models with path-based annotations versus point-based labels on the LITHOS test set.

- **Open Question 2:** To what extent can specialized techniques (e.g., re-sampling, cost-sensitive losses) mitigate the low F1-scores observed in the dataset's long-tailed mineral classes?
  - **Basis in paper:** [explicit] Section 6 notes that "further techniques could be applied to mitigate this [class imbalance] and potentially improve performance metrics."
  - **Why unresolved:** The authors preserved the natural imbalance to reflect realistic conditions, resulting in F1-scores below 0.2 for rare classes like Opaque.
  - **What evidence would resolve it:** Benchmarking long-tail recognition methods on the LITHOS Benchmark to measure performance gains on underrepresented categories.

- **Open Question 3:** How robust are models trained on LITHOS when applied to thin sections from geological regions outside of Colombia?
  - **Basis in paper:** [explicit] Section 6 acknowledges "geographical limitations" and notes the lack of data from other global regions limits broader applications.
  - **Why unresolved:** The dataset is exclusively sourced from Colombian soils, potentially introducing a regional bias that limits global generalization.
  - **What evidence would resolve it:** Evaluation of the LITHOS Baseline on external petrographic datasets from diverse global locations.

## Limitations

- **Limited geographical diversity:** The dataset exclusively contains samples from Colombian soils, potentially introducing regional bias that limits global generalization.
- **Severe class imbalance:** Rare mineral classes have very few samples (F1-scores below 0.2), and the long-tail distribution remains a fundamental challenge for multi-class performance.
- **Simplified geometric supervision:** The axis intersection approach may misalign patches for highly irregular or non-elliptical mineral grains, potentially degrading classification accuracy.

## Confidence

- **Mechanism 1 (Polarization Synergy):** High - Supported by controlled ablation experiments showing consistent performance gains over single-modality baselines.
- **Mechanism 2 (Cross-Attention Fusion):** Medium to High - Theoretical justification is sound, but more complex fusion strategies were not explored.
- **Mechanism 3 (Weak Supervision):** Medium - Pragmatic approach but untested for highly irregular grain morphologies.

## Next Checks

1. **Verify data alignment:** Visualize a random batch to ensure PPL/XPL patches show the same physical location and maintain strict pair alignment throughout training.
2. **Monitor class-specific metrics:** Track per-class Recall and Macro-F1 scores to detect class imbalance failure modes, as accuracy alone is misleading in this dataset.
3. **Test frozen encoder strategy:** Confirm that encoders are truly frozen (not accidentally fine-tuned) by checking parameter update patterns during training to validate the computational efficiency claim.