---
ver: rpa2
title: An overview of neural architectures for self-supervised audio representation
  learning from masked spectrograms
arxiv_id: '2509.18691'
source_url: https://arxiv.org/abs/2509.18691
tags:
- audio
- learning
- mamba
- speech
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive overview of self-supervised
  audio representation learning through masked spectrogram modeling, comparing Transformer,
  Mamba, and xLSTM architectures. The authors evaluate these approaches on ten diverse
  audio classification tasks, demonstrating that Mamba-based models outperform standard
  Transformers by approximately 30% in overall score, while xLSTM-based models achieve
  competitive results.
---

# An overview of neural architectures for self-supervised audio representation learning from masked spectrograms

## Quick Facts
- **arXiv ID:** 2509.18691
- **Source URL:** https://arxiv.org/abs/2509.18691
- **Authors:** Sarthak Yadav; Sergios Theodoridis; Zheng-Hua Tan
- **Reference count:** 40
- **Primary result:** Mamba and xLSTM outperform standard Transformers by ~30% in overall score for self-supervised audio representation learning from masked spectrograms.

## Executive Summary
This paper provides a comprehensive comparison of self-supervised audio representation learning using three neural architectures: Transformer, Mamba, and xLSTM. The authors evaluate these approaches on ten diverse audio classification tasks, demonstrating that Mamba-based models outperform standard Transformers by approximately 30% in overall score, while xLSTM-based models achieve competitive results. The study establishes Mamba and xLSTM as viable alternatives to Transformers for learning general-purpose audio representations, particularly excelling in handling longer sequences and different time-frequency resolutions.

## Method Summary
The study employs a Masked Spectrogram Modeling (MSM) framework where log-mel spectrograms are extracted from audio, divided into patches, and 50% of patches are randomly masked during pretraining. The framework uses an SSAST-style approach where masked tokens remain in the sequence rather than being dropped. Three architectures are compared: standard Transformer blocks, Mamba blocks, and xLSTM blocks, all trained on AudioSet for 100 epochs using MSE loss on masked patches only. Downstream evaluation uses a frozen encoder with a linear probe across ten HEAR benchmark tasks.

## Key Results
- Mamba-based models outperform standard Transformers by approximately 30% in overall score
- xLSTM-based models achieve competitive results, particularly for longer sequences
- All architectures benefit from SSAST-style masking (keeping tokens in sequence) rather than MAE-style dropping
- Mamba and xLSTM show advantages in handling different time-frequency resolutions and longer sequences

## Why This Works (Mechanism)
Masked spectrogram modeling forces models to learn contextual relationships between visible and masked patches, capturing both local spectral patterns and global temporal structures. The SSAST-style approach (keeping masked tokens in sequence) is particularly important for recurrent architectures like Mamba and xLSTM, which require sequential inputs to maintain temporal coherence and causality.

## Foundational Learning
- **Log-mel spectrograms:** Convert audio waveforms into 2D representations capturing frequency content over time. *Why needed:* Provides structured input for patch-based modeling. *Quick check:* Verify 80 mel-bins at 16kHz sampling rate.
- **Patch extraction:** Divide spectrograms into non-overlapping patches (4×16 time×frequency). *Why needed:* Enables token-based processing similar to vision transformers. *Quick check:* Ensure patches cover full spectrogram without overlap.
- **SSAST masking:** Keep masked tokens in sequence rather than dropping them. *Why needed:* Essential for recurrent architectures to maintain sequence continuity. *Quick check:* Masked patches replaced with learnable mask token, not removed.
- **Expansion factor (E_f=3):** Scale depth of Mamba/xLSTM blocks to match Transformer capacity. *Why needed:* Ensures fair architectural comparison. *Quick check:* Verify block count adjusted by E_f in recurrent models.

## Architecture Onboarding

**Component map:** Audio waveform → Log-mel spectrogram → Patches → Masked patches → Encoder (Transformer/Mamba/xLSTM) → Embeddings → Downstream classifier

**Critical path:** Masking → Encoder forward pass → MSE loss computation on masked patches only → Parameter update via AdamW

**Design tradeoffs:** SSAST-style masking enables recurrent architectures but increases memory usage compared to MAE-style dropping; Mamba/xLSTM handle longer sequences better but may have higher computational overhead per step.

**Failure signatures:** Using MAE-style dropping with Mamba/xLSTM causes OOM or shape errors due to broken sequence continuity; memory issues with xLSTM on sequences longer than 5 seconds.

**First experiments:** 1) Train baseline Transformer with SSAST masking on 2-second AudioSet crops; 2) Replace with Mamba blocks, verify sequence handling; 3) Test xLSTM with 5-second crops to assess memory limits.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section identifies areas for future investigation regarding different masking strategies, time-frequency resolutions, and task-specific fine-tuning approaches.

## Limitations
- Evaluation focuses on fixed masking strategy (50% unstructured) and spectrogram resolution (80 mel-bands)
- Does not explore task-specific fine-tuning versus frozen encoders
- Constrained to 2-second crops during pretraining, potentially underutilizing sequence modeling strengths of Mamba/xLSTM
- Unknown peak learning rate value limits exact reproduction

## Confidence
- **High confidence:** Mamba and xLSTM can be trained using the SSAST framework with MSE loss on masked spectrogram patches, achieving competitive downstream performance to Transformers
- **Medium confidence:** The claimed ~30% overall score improvement for Mamba over Transformer is accurate within tested experimental conditions
- **Medium confidence:** xLSTM-based models achieve competitive results, but memory constraints during pretraining limit exploration of longer sequences

## Next Checks
1. Test Mamba and xLSTM with varying masking ratios (30%, 70%) to assess robustness across different audio classification tasks
2. Evaluate task-specific fine-tuning versus frozen encoder approaches to determine optimal transfer learning strategy
3. Conduct scaling studies with different time-frequency resolutions and patch sizes to establish guidelines for optimal input representations