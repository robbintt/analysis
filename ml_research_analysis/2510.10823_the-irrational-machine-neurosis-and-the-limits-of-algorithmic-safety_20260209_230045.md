---
ver: rpa2
title: 'The Irrational Machine: Neurosis and the Limits of Algorithmic Safety'
arxiv_id: '2510.10823'
source_url: https://arxiv.org/abs/2510.10823
tags:
- when
- energy
- agent
- cost
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work catalogues neurosis-like behaviours in embodied AI\u2014\
  such as flip-flop, plan churn, perseveration loops, paralysis, hypervigilance, futile\
  \ search, belief incoherence, tie-break thrashing, corridor thrashing, optimality\
  \ compulsion, metric mismatch, policy oscillation, myopic ping-pong, and exploration\
  \ paralysis\u2014and pairs each with lightweight detectors and deterministic escape\
  \ policies (short commitments, margins to switch, temporal smoothing, and principled\
  \ arbitration). It shows that phobic avoidance can persist under full visibility\
  \ when learned aversive costs dominate local choice, producing long detours despite\
  \ globally safe routes."
---

# The Irrational Machine: Neurosis and the Limits of Algorithmic Safety

## Quick Facts
- arXiv ID: 2510.10823
- Source URL: https://arxiv.org/abs/2510.10823
- Authors: Daniel Howard
- Reference count: 28
- Primary result: Catalogues neurosis-like AI failure modes and proposes deterministic escape policies plus genetic-programming-based adversarial testing to expose architectural limits.

## Executive Summary
This paper identifies a spectrum of looping and oscillatory failure modes in embodied AI—termed "neurosis-like" behaviours—such as flip-flop, plan churn, perseveration loops, and paralysis. For each, lightweight detectors and deterministic escape policies (short commitments, margins to switch, temporal smoothing, principled arbitration) are proposed. The work demonstrates that learned aversive costs can cause phobic avoidance even under full observability, producing globally suboptimal paths. To systematically surface such failures, it introduces a genetic-programming-driven destructive testing framework that evolves adversarial environments and perturbations to maximise safety, compliance, and efficiency pressures.

## Method Summary
The paper catalogues 12 specific failure modes observed in reinforcement learning agents, each paired with a lightweight detection rule and a deterministic escape policy. Detection often relies on simple temporal or state-space metrics (e.g., repeated state visitation, policy oscillation frequency). Escape policies include short-term commitment thresholds, margin-based action switching, and temporal smoothing of value estimates. To expose deeper architectural flaws, the paper proposes using genetic programming to evolve worlds and perturbations that maximise pressure on safety, compliance, and efficiency—generating adversarial curricula and counterfactual traces for analysis.

## Key Results
- Enumerated 12 neurosis-like behaviours (flip-flop, plan churn, perseveration loops, paralysis, hypervigilance, futile search, belief incoherence, tie-break thrashing, corridor thrashing, optimality compulsion, metric mismatch, policy oscillation, myopic ping-pong, exploration paralysis).
- Demonstrated that phobic avoidance can persist under full visibility when learned aversive costs dominate local choice, causing long detours despite globally safe routes.
- Proposed genetic-programming-driven destructive testing to evolve adversarial worlds that expose architectural limits beyond symptom-level patches.

## Why This Works (Mechanism)
The proposed approach works by treating pathological looping and oscillation as detectable patterns in state visitation or policy choice, then applying deterministic, short-term overrides to break cycles. This sidesteps the need for complex retraining or hyperparameter tuning during episodes. The genetic-programming testing framework amplifies rare but critical failure modes by evolving environments that maximise stress on safety and efficiency constraints, thereby surfacing latent architectural weaknesses.

## Foundational Learning
- **Neurosis-like failure modes** (why needed: provides shared vocabulary for recurrent AI pathologies; quick check: can you map each mode to a concrete RL failure case?)
- **Lightweight detection rules** (why needed: enables runtime intervention without heavy computation; quick check: does the detector trigger only on true failure signatures?)
- **Deterministic escape policies** (why needed: offers immediate, interpretable fixes vs. retraining; quick check: do escape actions reduce loop recurrence in tests?)
- **Genetic programming for adversarial testing** (why needed: automates discovery of rare but critical failure scenarios; quick check: does evolved curriculum expose failures missed by random exploration?)
- **Aversive cost dominance** (why needed: explains how learned penalties can override optimal paths even with full observability; quick check: can you isolate cost magnitude vs. reward structure effects?)
- **Adversarial curricula** (why needed: enables systematic safety validation beyond random perturbations; quick check: do generated worlds produce novel, non-random failure modes?)

## Architecture Onboarding
- **Component map:** Sensor inputs → RL policy → Action selector → Environment → State monitor → Detector → Escape policy arbiter → Modified action → Sensor inputs
- **Critical path:** Perception → Policy → Action → Environment → State monitor → Loop detection → Escape policy override → Modified action
- **Design tradeoffs:** Lightweight detection vs. false positives; deterministic escape vs. potential policy disruption; genetic programming complexity vs. coverage of rare failures.
- **Failure signatures:** Repeated state visitation, rapid policy oscillation, unchanging action sequences despite changing states, persistent detours despite shorter paths.
- **First experiments:**
  1. Implement detectors and escape policies in grid-world RL (PPO) and measure loop recurrence reduction.
  2. Vary learned cost magnitudes vs. reward structures to isolate conditions causing phobic avoidance under full observability.
  3. Prototype genetic-programming destructive testing on grid navigation to evaluate generation of safety-compromising adversarial scenarios.

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation is limited; systematic testing across diverse agents and environments is absent.
- Claim that phobic avoidance persists "under full visibility" conflates learned cost structure with epistemic uncertainty—no ablation isolates cost-learning pathologies from perception failures.
- Destructive-testing framework is theoretically promising but lacks reported results; scalability to high-dimensional continuous state spaces remains unproven.

## Confidence
- **High confidence:** Identification of specific looping/oscillatory failure modes; utility of deterministic escape policies is well-established in control literature.
- **Medium confidence:** Framing as "neurotic" is conceptually useful but not rigorously validated; genetic-programming testing exposing architectural limits is plausible but untested.
- **Low confidence:** Specific mechanisms of "phobic avoidance" under full observability; scalability and practical effectiveness of the adversarial curriculum framework.

## Next Checks
1. Implement and test the proposed escape policies across multiple RL algorithms (PPO, SAC, DQN) in grid-world and continuous control benchmarks to measure recurrence reduction rates.
2. Conduct ablation studies varying learned cost magnitudes vs. reward structures to isolate conditions causing "phobic" detours under full observability.
3. Prototype the genetic-programming destructive testing pipeline on a simplified domain (e.g., grid navigation) to evaluate its ability to generate safety-compromising adversarial scenarios beyond random exploration.