---
ver: rpa2
title: 'Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided
  LLMs for Ridesourcing Mode Choice Modeling'
arxiv_id: '2509.18181'
source_url: https://arxiv.org/abs/2509.18181
tags:
- ridesourcing
- data
- sapa
- latent
- travel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of accurately predicting ridesourcing
  mode choices in the presence of severe class imbalance and missing psychological
  factors. The authors propose the SAPA framework, which uses LLMs to synthesize theory-grounded
  latent attitudes from travel survey data and integrates these with demographic and
  trip features.
---

# Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling

## Quick Facts
- **arXiv ID**: 2509.18181
- **Source URL**: https://arxiv.org/abs/2509.18181
- **Reference count**: 40
- **Primary result**: SAPA improves ridesourcing mode choice prediction PR-AUC by up to 75.9% over baselines

## Executive Summary
This paper introduces SAPA, a two-stage hierarchical framework that leverages Large Language Models (LLMs) to synthesize theory-grounded latent attitudes from travel survey data and integrate them with demographic and trip features to predict ridesourcing mode choices. The approach addresses the challenges of severe class imbalance and missing psychological factors in transportation modeling. Experiments on a large-scale multi-year travel survey show SAPA significantly outperforms state-of-the-art baselines, with particular improvements on rare-event prediction metrics.

## Method Summary
SAPA employs a two-stage hierarchical approach: Stage 1 uses an LLM (Llama-3.1-8B-Instruct) to generate traveler personas from demographics and spatial embeddings, then trains an XGBoost classifier with SMOTE to predict individual propensity to use ridesourcing. Stage 2 uses the same LLM to score seven theory-grounded latent variables (e.g., Time Sensitivity, Tech Affinity) from personas, creates interaction terms with trip attributes, and feeds these along with the propensity score into a final LightGBM classifier for trip-level prediction. The framework uses persona-level stratified train/test splits to prevent data leakage and handles class imbalance through scale_pos_weight adjustments.

## Key Results
- SAPA achieves PR-AUC of 0.2479, outperforming state-of-the-art baselines by up to 75.9%
- The two-stage hierarchical structure improves minority-class prediction, resolving baseline overfitting (0.1463 PR-AUC, 0 true positives) to 0.3887 with meaningful precision/recall
- Interaction terms between attitudes and trip context provide additional predictive power, contributing 30.2% of feature importance
- Ablation studies show both propensity scores and latent variable features are critical for performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-synthesized latent attitudes provide predictive signal for ridesourcing choice modeling.
- Mechanism: LLMs map observable demographic and trip features to theory-grounded psychological constructs using in-context prompts tied to TPB and TAM. These synthetic features recover part of the variance normally explained by missing psychometric survey data.
- Core assumption: The LLM's internal representations encode correlations between observable traveler profiles and latent attitudes that approximate population-level patterns in the training data.
- Evidence anchors: [abstract] "uses Large Language Models (LLMs) to synthesize theory-grounded latent attitudes to predict ridesourcing choices"; [section: Stage 1 results] LLM-derived features resolved baseline overfitting, improving test PR-AUC from 0.1463 to 0.3887.

### Mechanism 2
- Claim: A two-stage hierarchical structure improves generalization by separating stable individual disposition from situational trip-level choice.
- Mechanism: Stage 1 estimates an individual-level propensity score (probability of ever using ridesourcing), capturing long-term attitudes and demographic tendencies. Stage 2 conditions trip-level prediction on this propensity, reducing the learning burden on situational features alone.
- Core assumption: Individual disposition toward ridesourcing is relatively stable and partially explains trip-level variance.
- Evidence anchors: [abstract] "a hierarchical approach... first modeling individual propensity to use ridesourcing, then predicting trip-level choices"; [section: Ablation Study] Adding propensity score alone lifts PR-AUC from 0.1346 to 0.2060.

### Mechanism 3
- Claim: Interaction terms between synthesized attitudes and trip attributes capture context-dependent behavioral activation.
- Mechanism: Multiplying latent scores by trip variables (e.g., Time Sensitivity × travel_time) models how a trait is "activated" by situational context.
- Core assumption: Latent attitudes have stronger effects when relevant trip dimensions (time, cost, comfort) are salient.
- Evidence anchors: [abstract] "integrates... latent-variable scores (with their interaction terms), and observable trip attributes"; [section: Ablation Study (Table 5)] Full SAPA with all interactions achieves PR-AUC 0.2479 vs 0.2167 without interactions.

## Foundational Learning

- **Concept**: Class imbalance and PR-AUC
  - Why needed here: Ridesourcing trips are ~1% of all trips. Standard metrics can mislead; PR-AUC focuses on positive-class performance and is appropriate for rare-event evaluation.
  - Quick check question: If a model predicts "no ridesourcing" for all trips, what would its PR-AUC be?

- **Concept**: Latent variables in behavioral modeling
  - Why needed here: Psychological constructs drive choices but are rarely measured in large travel surveys. SAPA synthesizes them to proxy unobserved heterogeneity.
  - Quick check question: Why can't we directly observe "Time Sensitivity" from a standard trip diary?

- **Concept**: Feature interactions in tree-based models
  - Why needed here: SAPA explicitly creates interaction terms rather than relying on models to learn them implicitly.
  - Quick check question: In LightGBM, are feature interactions learned automatically? When would explicit interaction features still help?

## Architecture Onboarding

- **Component map**: LLM Persona Generation -> Propensity Model -> Latent Variable Scoring -> Trip-Level Classifier
- **Critical path**: 
  1. Ensure persona-level stratified split to avoid leakage
  2. Validate Stage 1 propensity model separately; if it fails (e.g., zero true positives on test), stop and debug LLM features or SMOTE configuration
  3. For Stage 2, ablate: observables only → + latent → + propensity → + interactions. Monitor PR-AUC and F1 delta at each step

- **Design tradeoffs**:
  - **LLM choice**: Larger models may produce more nuanced personas but increase latency and cost
  - **Number of latent variables**: Seven variables grounded in TPB/TAM. Adding more may overfit or dilute signal
  - **Interaction engineering**: Six theory-driven interactions. Exhaustive pairwise interactions risk dimensionality explosion and multicollinearity

- **Failure signatures**:
  - **Stage 1 overfitting**: Baseline observables-only model achieves non-zero PR-AUC but zero true positives → indicates propensity model is not learning minority class
  - **Latent variable drift**: If cross-sectional validation shows ridesourcing users and non-users have identical latent score distributions, synthesis is not capturing discriminative signal
  - **Interaction saturation**: If full model with interactions performs worse than without, check for overfitting or data leakage in interaction computation

- **First 3 experiments**:
  1. Replicate baseline vs +propensity ablation on a 20% holdout. Confirm PR-AUC lift and that true positives exist
  2. Run cross-sectional validation: compare mean latent scores for users vs non-users. Check alignment with theory (e.g., users higher in Tech Affinity, lower in Pro-Car Attitude)
  3. Ablate interaction terms: train Stage 2 with and without interactions across two classifiers (e.g., LightGBM and XGBoost). Report PR-AUC, F1, and top feature importances

## Open Questions the Paper Calls Out

- **Open Question 1**: Do LLM-synthesized latent attitudes correlate with ground-truth psychometric measurements?
  - Basis in paper: The authors note that "LLM-generated latent variable scores are synthetic approximations, not ground-truth psychological measurements."
  - Why unresolved: The study validates predictive accuracy but does not verify if the synthesized attitudes align with actual survey-based psychometrics.
  - What evidence would resolve it: A study comparing SAPA-generated scores against a specialized dataset containing explicit attitudinal survey responses.

- **Open Question 2**: Does the SAPA framework generalize to urban contexts with different mobility cultures?
  - Basis in paper: The authors identify testing transferability to "other urban contexts" and "different geographic and cultural settings" as a critical next step.
  - Why unresolved: The framework is currently validated only on a multi-year survey from the Puget Sound region.
  - What evidence would resolve it: Replicating the framework's performance gains on datasets from cities with distinct transportation infrastructures.

- **Open Question 3**: How do inherent LLM biases affect the fairness and accuracy of synthesized traveler personas?
  - Basis in paper: The authors state LLMs "inherit and amplify societal biases" and call for "formal bias auditing" in future work.
  - Why unresolved: The paper focuses on predictive power, leaving the potential for stereotypical or discriminatory persona generation unquantified.
  - What evidence would resolve it: An audit measuring the correlation between demographic variables and persona attributes to detect spurious or unfair bias.

## Limitations
- Latent variable synthesis relies entirely on LLM output, with no ground truth validation of the psychological constructs
- Cross-sectional persona stratification avoids leakage but limits assessment of temporal stability in synthesized attitudes
- Only one domain (ridesourcing mode choice) and one dataset (PSRC survey) tested; generalizability to other travel modes remains uncertain

## Confidence
- **High**: The two-stage hierarchical design reduces class imbalance impact and improves minority-class prediction (PR-AUC gains 75.9%)
- **Medium**: LLM-generated latent attitudes improve predictive performance over observables-only baselines, but ground truth validation is indirect
- **Low**: Interaction terms consistently improve model performance; effect may be dataset-specific and warrants further validation

## Next Checks
1. **Cross-sectional validation**: Compare mean latent scores between ridesourcing users and non-users; verify alignment with behavioral theory
2. **Temporal stability test**: Train on 2017–2019 data, validate on 2020–2023; assess whether synthesized attitudes remain predictive across changing travel contexts
3. **Alternative LLM ablations**: Replace Llama-3.1-8B-Instruct with GPT-4o or Claude-3.5; measure PR-AUC and feature importance shifts to isolate LLM-specific effects