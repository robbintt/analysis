---
ver: rpa2
title: 'JustRL: Scaling a 1.5B LLM with a Simple RL Recipe'
arxiv_id: '2512.16649'
source_url: https://arxiv.org/abs/2512.16649
tags:
- training
- simple
- performance
- recipe
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of whether complex reinforcement
  learning techniques are necessary for training small language models (SLMs) in mathematical
  reasoning. The authors present JustRL, a minimal single-stage training approach
  with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B
  reasoning models.
---

# JustRL: Scaling a 1.5B LLM with a Simple RL Recipe

## Quick Facts
- arXiv ID: 2512.16649
- Source URL: https://arxiv.org/abs/2512.16649
- Authors: Bingxiang He; Zekai Qu; Zeyuan Liu; Yinghao Chen; Yuxin Zuo; Cheng Qian; Kaiyan Zhang; Weize Chen; Chaojun Xiao; Ganqu Cui; Ning Ding; Zhiyuan Liu
- Reference count: 5
- Primary result: Simple RL recipe achieves 54.9% and 64.3% accuracy on math benchmarks with 1.5B models using 2× less compute

## Executive Summary
This paper challenges the assumption that complex reinforcement learning techniques are necessary for training small language models in mathematical reasoning. The authors introduce JustRL, a minimal single-stage training approach with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B parameter reasoning models. JustRL demonstrates smooth, monotonic improvement over 4,000+ training steps without the collapses or plateaus that typically motivate sophisticated RL interventions, achieving 54.9% and 64.3% average accuracy across nine mathematical benchmarks while using 2× less compute than existing approaches.

The key insight is that added complexity may be solving problems that disappear with a stable, scaled-up baseline. Ablation studies show that common "tricks" like explicit length penalties and robust verifiers can actually degrade performance by collapsing exploration. The authors conclude that the field may be over-engineering solutions to RLHF challenges, and they release their models and code to establish a simple, validated baseline for the community.

## Method Summary
JustRL is a single-stage RL training approach designed for small language models (1.5B parameters) focused on mathematical reasoning. The method uses fixed hyperparameters throughout training without the complex interventions typically employed in RLHF, such as explicit length penalties, KL divergence constraints, or robust verification systems. The approach relies on a stable training pipeline that maintains smooth, monotonic improvement over thousands of steps without requiring the sophisticated monitoring and adjustment mechanisms that characterize more complex RL approaches.

## Key Results
- Achieved 54.9% and 64.3% average accuracy across nine mathematical reasoning benchmarks
- Demonstrated 2× compute efficiency compared to sophisticated RL approaches
- Showed smooth, monotonic training improvement over 4,000+ steps without collapses or plateaus
- Ablation studies revealed that standard tricks like length penalties and verifiers may degrade performance by reducing exploration

## Why This Works (Mechanism)
JustRL works by eliminating the instabilities that typically require complex interventions in RLHF training. The single-stage approach with fixed hyperparameters creates a stable optimization landscape where the model can explore mathematical reasoning strategies without being constrained by artificial penalties or overly conservative constraints. This stability allows for consistent learning progress without the need for dynamic adjustments, length normalization, or verification mechanisms that can inadvertently restrict the model's ability to discover effective reasoning patterns.

## Foundational Learning
- **Reinforcement Learning from Human Feedback (RLHF)**: The framework for aligning language models with human preferences through reward-based training
  - *Why needed*: Standard supervised training doesn't capture the nuanced quality judgments required for mathematical reasoning
  - *Quick check*: Verify that reward signals are properly scaled and temporally consistent across training steps

- **Single-stage RL training**: A unified training process without intermediate phases or curriculum learning
  - *Why needed*: Reduces training complexity and eliminates potential misalignments between training stages
  - *Quick check*: Monitor for consistent reward trends without sudden drops or plateaus

- **Mathematical reasoning benchmarks**: Standardized evaluation tasks specifically designed to test logical and quantitative reasoning capabilities
  - *Why needed*: Provides objective, measurable criteria for assessing model performance on reasoning tasks
  - *Quick check*: Ensure benchmark diversity covers multiple mathematical domains and difficulty levels

## Architecture Onboarding

**Component Map:**
Data Pipeline -> Model (1.5B LLM) -> Reward Function -> RL Optimizer -> Updated Model

**Critical Path:**
Input mathematical problems → Model generates solutions → Reward function evaluates correctness → RL optimizer updates model parameters → New model generates improved solutions

**Design Tradeoffs:**
- Simplicity vs. potential for fine-tuned optimization
- Single-stage training vs. staged curriculum approaches
- Fixed hyperparameters vs. adaptive optimization schedules
- Computational efficiency vs. potential ceiling on performance

**Failure Signatures:**
- Sudden drops in training reward indicating instability
- Plateauing performance suggesting exploration collapse
- Degraded performance on held-out benchmarks indicating overfitting
- Inconsistent reasoning patterns suggesting reward hacking

**First 3 Experiments:**
1. Run baseline JustRL training for 1,000 steps and verify smooth reward progression
2. Compare JustRL performance against complex baselines on identical hardware
3. Test ablation of length penalties and verifiers to confirm their negative impact on exploration

## Open Questions the Paper Calls Out
None explicitly identified in the provided content.

## Limitations
- Results are limited to 1.5B parameter models and may not generalize to larger or smaller models
- Evaluation focuses exclusively on mathematical reasoning tasks, leaving domain generalizability uncertain
- Uses single-prompt evaluation rather than comprehensive sampling methods, potentially underestimating performance variability

## Confidence
- Core claim that simple RL works well for small reasoning models: **Medium**
- Claim that complex RL techniques may be unnecessary: **Medium-Low**
- Claim of 2× compute efficiency: **Medium** (requires fair comparison validation)

## Next Checks
1. Test JustRL across model scales from 500M to 7B parameters to determine scalability limits
2. Evaluate performance on non-mathematical reasoning tasks (common sense, logical inference, multi-hop reasoning)
3. Conduct controlled experiments comparing JustRL against complex baselines on identical hardware to validate the 2× compute efficiency claim