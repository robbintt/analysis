---
ver: rpa2
title: Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk
  Detection
arxiv_id: '2507.00693'
source_url: https://arxiv.org/abs/2507.00693
tags:
- suicide
- risk
- speech
- features
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of detecting suicide risk in
  adolescents using spontaneous speech. The authors propose a multimodal framework
  that integrates acoustic embeddings, text embeddings, and interpretable features
  extracted by a large language model (LLM).
---

# Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection

## Quick Facts
- arXiv ID: 2507.00693
- Source URL: https://arxiv.org/abs/2507.00693
- Authors: Yifan Gao; Jiao Fu; Long Guo; Hong Liu
- Reference count: 0
- Primary result: 74% accuracy and 0.740 F1 score on SW1 challenge test set

## Executive Summary
This study addresses adolescent suicide risk detection using spontaneous speech through a multimodal framework integrating acoustic, text, and LLM-extracted interpretable features. The authors propose using DeepSeek-R1 to automatically identify five clinically relevant suicide risk indicators (self-harm, pressure, social support, unhealthy outlets, exercise) from speech transcripts. The method combines frozen pretrained audio and text embeddings with Random Forest classification on LLM features, achieving first place in the SpeechWellness Challenge (SW1). The approach demonstrates that LLM-extracted interpretable features, when combined with traditional acoustic and semantic features, significantly improve detection accuracy while maintaining clinical transparency.

## Method Summary
The framework preprocesses spontaneous speech audio with noise reduction, segments it into 30-second chunks with 10% overlap, and extracts features from three modalities. Acoustic features are obtained from frozen pretrained models (HuBERT, Wav2Vec2, Whisper), text features from BERT and RoBERTa models applied to Whisper transcripts, and interpretable features from DeepSeek-R1 using structured prompts to identify five binary suicide risk indicators. Individual models are trained separately (MLPs for acoustic/text, Random Forest for LLM features), then combined through weighted voting ensemble where DeepSeek-R1 features receive higher weight. The system is evaluated on the SW1 dataset with 400 training participants, achieving 74% accuracy and 0.740 F1 score on the test set.

## Key Results
- Achieved 74% accuracy and 0.740 F1 score on SW1 test set, ranking first in the challenge
- Multimodal ensemble combining acoustic, text, and LLM features outperformed single-modality approaches (74% vs. 51-61% for unimodal models)
- DeepSeek-R1 extracted interpretable features achieved highest single-model performance (65.2% accuracy, 67.9% F1) among all tested approaches
- Weighted voting ensemble with DeepSeek-R1 upweighted showed significant improvement over unweighted combinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-extracted interpretable features improve detection accuracy and clinical utility over raw embeddings alone.
- Mechanism: DeepSeek-R1 uses structured prompts to extract binary indicators (self-harm behavior, pressure, social support, unhealthy outlets, exercise) from transcripts, converting unstructured speech into clinically meaningful features that Random Forest can classify.
- Core assumption: The five LLM-identified indicators are causally relevant to suicide risk and are reliably detectable from spontaneous speech transcripts.
- Evidence anchors:
  - [abstract] "DeepSeek-R1 automatically identifies key suicide risk indicators such as self-harm behavior, pressure, social support, unhealthy coping, and exercise"
  - [Page 3, Table 2] DeepSeek-R1 achieves highest single-model performance (Acc: 0.652, F1: 0.679) on ER task
  - [corpus] Neighbor paper "Evidence-Driven Marker Extraction" (FMR: 0.46) similarly uses LLM-extracted markers for suicide detection, suggesting convergent validity of this approach
- Break condition: If LLM prompt design fails to capture culturally or linguistically specific expressions of distress, feature quality degrades. The paper notes transcripts are Chinese; prompts were translated for publication—original prompt quality is assumed but not validated.

### Mechanism 2
- Claim: Acoustic and textual modalities capture complementary risk signals.
- Mechanism: Audio models (HuBERT, Wav2Vec2, Whisper) capture prosody, intonation, and energy variations that text loses during transcription; text models (BERT, RoBERTa) capture semantic content and metaphorical emotional cues that audio models miss.
- Core assumption: Suicide risk manifests in both paralinguistic (acoustic) and linguistic (semantic) channels simultaneously.
- Evidence anchors:
  - [Page 3, Results] "the three pre-trained audio models... outperform the text-based models... One main reason is that audio models directly capture emotional cues embedded in speech—such as prosody, intonation, and variations in energy"
  - [Page 4, Table 3] Audio-only voting (rows 1-6, 8-9) achieves 0.540 test accuracy; adding RoBERTa + DeepSeek-R1 improves to 0.740
  - [corpus] Neighbor paper "Suicide Risk Assessment Using Multimodal Speech Features" (FMR: 0.56) confirms similar multimodal integration on same SW1 dataset
- Break condition: If ASR quality is poor (noisy audio, accented speech), text-derived features become unreliable. The paper uses noise reduction but does not report ASR word error rate.

### Mechanism 3
- Claim: Weighted voting ensemble with emphasis on interpretable features generalizes better than any single modality.
- Mechanism: Multiple model outputs are combined via voting; DeepSeek-R1 features receive higher weight, prioritizing clinically interpretable signals over raw embeddings.
- Core assumption: The specific model combination and weighting scheme transfers to unseen adolescent populations.
- Evidence anchors:
  - [Page 4, Table 3] Best ensemble (1, 2, 5, 6, 9, 13, 16) achieves 0.740 accuracy/F1 vs. baseline 0.510-0.610
  - [Page 4] "we increased the voting weight of DeepSeek-R1 to emphasize its importance in capturing critical signals"
  - [corpus] Corpus evidence on ensemble weighting is weak; neighbor papers do not report comparable voting schemes
- Break condition: If test population differs significantly from training (different age ranges, cultural contexts, or assessment instruments), ensemble weights may not transfer.

## Foundational Learning

- Concept: **Self-supervised speech representations (Wav2Vec2, HuBERT, Whisper)**
  - Why needed here: These models provide frozen feature extractors trained on 50K-680K hours of speech, eliminating need for labeled audio data. Understanding their pretraining objectives helps debug why Whisper excels on Expression Description (emotional cues) vs. HuBERT on other tasks.
  - Quick check question: Can you explain why a model pretrained on LibriSpeech (read speech) might transfer differently to spontaneous adolescent speech than one pretrained on diverse internet audio?

- Concept: **Prompt engineering for structured LLM extraction**
  - Why needed here: DeepSeek-R1 must output consistent binary labels + supporting quotes. The prompt (Figure 2) uses precise formatting constraints to enforce structured outputs. Without this understanding, you cannot modify indicators or adapt to new languages.
  - Quick check question: What failure mode occurs if the LLM outputs commentary outside the bracketed quotes when the prompt forbids it?

- Concept: **Voting ensembles vs. learned fusion**
  - Why needed here: The paper uses voting rather than concatenating features into a single classifier. This preserves modality independence and allows per-modality debugging, but sacrifices potential cross-modal interaction learning.
  - Quick check question: What information is lost when using late fusion (voting on predictions) versus early fusion (concatenating features before classification)?

## Architecture Onboarding

- Component map:
  Raw Audio -> [Koala denoising] -> 30s segments (10% overlap)
  | |
  v v
  [ASR: Whisper] -> Transcript -> [BERT/RoBERTa] -> Text embeddings -> MLP
  | | |
  v v v
  [DeepSeek-R1 + Prompt] -> 5 binary features -> Random Forest
  |
  v
  [HuBERT/Wav2Vec2/Whisper encoder] -> Acoustic embeddings -> MLP
  [MLP outputs + RF output] -> Weighted voting -> Final prediction

- Critical path:
  1. Audio preprocessing quality (noise reduction, segmentation) affects both ASR accuracy and acoustic feature quality
  2. DeepSeek-R1 prompt design determines interpretable feature validity
  3. Voting weight allocation determines ensemble balance

- Design tradeoffs:
  - **Frozen vs. fine-tuned encoders**: Paper uses frozen pretrained models; fine-tuning on SW1 data could improve performance but risks overfitting with only 400 training samples
  - **5 LLM indicators vs. more**: Limited to five features for interpretability; expanding indicators could capture more risk factors but increases prompt complexity and LLM cost
  - **Voting vs. stacking**: Voting is simpler and preserves interpretability; stacking with a meta-learner could capture cross-modal patterns but adds complexity

- Failure signatures:
  - ASR errors propagate to both text embeddings and LLM features; check transcript quality first if performance drops
  - DeepSeek-R1 extraction failures (wrong format, missing quotes) indicate prompt drift or API changes
  - Large train-dev gap suggests acoustic model overfitting; increase dropout or reduce model capacity

- First 3 experiments:
  1. **Ablation by modality**: Remove each modality (audio, text, LLM features) from voting to quantify contribution; paper shows partial results but not full ablation
  2. **Prompt robustness test**: Vary prompt phrasing for DeepSeek-R1 to measure sensitivity; add adversarial transcripts (e.g., metaphorical language) to test generalization
  3. **Cross-task validation**: Train on ER task only, test on PR/ED to measure whether learned patterns transfer across speech tasks; this tests whether risk indicators are task-independent

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed multimodal framework effectively predict longitudinal suicide risk or future behaviors, given that the current model is trained only on cross-sectional data reflecting immediate mental states?
- Basis in paper: [explicit] The authors state that the MINI-KID scale classifies current suicide risk based on "immediate responses at the time of assessment and not serving as a prediction of future suicidal behavior."
- Why unresolved: The dataset (SW1) provides a binary classification of current risk rather than time-series data or follow-up outcomes, making it impossible to validate the model's predictive power for future events.
- What evidence would resolve it: A longitudinal study where baseline speech features are used to predict suicidal behaviors occurring weeks, months, or years after the initial recording.

### Open Question 2
- Question: Does the reliance on the MINI-KID scale as ground truth introduce significant label noise due to patient underreporting, and how can the model be validated against more objective clinical measures?
- Basis in paper: [explicit] The authors acknowledge that the MINI-KID's "reliance on self-reported data can lead to underreporting or misinterpretation of symptoms."
- Why unresolved: If the "gold standard" labels are compromised by psychological barriers to disclosure, the model may be learning to detect superficial compliance rather than deep-seated risk, or conversely, it might detect risk in "no-risk" labeled subjects who are concealing their state.
- What evidence would resolve it: Correlating model predictions with external clinical consensus or biological markers, rather than relying solely on self-reported questionnaire scores.

### Open Question 3
- Question: How robust is the DeepSeek-R1 feature extraction pipeline to the variability of spontaneous speech, and would an open-ended extraction approach outperform the current method of querying five pre-defined indicators?
- Basis in paper: [inferred] The authors engineer the prompt to check for five specific indicators (Self-harm, Pressure, Social Support, Unhealthy Outlets, Exercise) based on prior literature, but do not test if the LLM could identify novel or uncategorized risk factors.
- Why unresolved: It is unclear if constraining the LLM to a fixed set of interpretable features limits the model's ability to capture complex or idiosyncratic linguistic markers of suicide risk that fall outside these five categories.
- What evidence would resolve it: An ablation study comparing the performance of the fixed-prompt features against features extracted via unsupervised or open-ended prompting strategies.

### Open Question 4
- Question: To what extent does the proposed framework generalize to non-adolescent populations or diverse linguistic backgrounds, given the specific demographic (Chinese adolescents) and language models (Chinese-BERT) used?
- Basis in paper: [inferred] The study is evaluated exclusively on the SW1 dataset (participants aged ~13-14) and utilizes Chinese-specific text models, but frames the problem as general "suicide risk detection."
- Why unresolved: Adolescents exhibit distinct linguistic patterns and emotional regulation behaviors compared to adults, and the acoustic features learned may be age-specific rather than universal markers of suicide risk.
- What evidence would resolve it: Cross-dataset evaluation on adult speech corpora or multilingual datasets to determine if the acoustic and semantic weights transfer effectively.

## Limitations
- Dataset access limitations prevent independent replication since SW1 is not publicly available
- Ground truth label quality concerns due to MINI-KID scale's reliance on self-reported data and potential underreporting
- Underspecified ensemble weighting scheme with only general statement that DeepSeek-R1 was "upweighted" without specific values

## Confidence
- **High confidence**: The core methodology of using frozen pretrained models for feature extraction is standard practice and well-supported
- **Medium confidence**: The specific combination of models, DeepSeek-R1 prompt design, and weighted voting scheme that produced the top SW1 results is credible but not fully reproducible due to underspecification
- **Low confidence**: The clinical interpretability and generalizability of the five LLM-extracted indicators across diverse populations remains uncertain without external validation

## Next Checks
1. **External validation**: Test the trained model on an independent adolescent suicide risk dataset to assess generalizability beyond the SW1 population
2. **ASR quality analysis**: Measure transcription accuracy on spontaneous adolescent speech, particularly for emotional and metaphorical content, to quantify its impact on downstream features
3. **Feature importance analysis**: Conduct ablation studies removing each modality and interpretable feature to quantify their individual contributions and validate the claimed weighting priorities