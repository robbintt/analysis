---
ver: rpa2
title: 'VLM-Guard: Safeguarding Vision-Language Models via Fulfilling Safety Alignment
  Gap'
arxiv_id: '2502.10486'
source_url: https://arxiv.org/abs/2502.10486
tags:
- safety
- harmful
- alignment
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of safeguarding vision-language
  models (VLMs) from harmful queries, a problem exacerbated by the "modality gap"
  that undermines the safety alignment inherited from their language components. The
  proposed solution, VLM-Guard, is an inference-time intervention that projects the
  VLM's hidden representations onto a subspace orthogonal to a "safety steering direction"
  derived from a safety-aligned language model.
---

# VLM-Guard: Safeguarding Vision-Language Models via Fulfilling Safety Alignment Gap

## Quick Facts
- arXiv ID: 2502.10486
- Source URL: https://arxiv.org/abs/2502.10486
- Authors: Qin Liu; Fei Wang; Chaowei Xiao; Muhao Chen
- Reference count: 14
- One-line primary result: VLM-Guard improves VLM safety, reducing attack success rates by 66-75% while maintaining response quality.

## Executive Summary
This paper addresses the safety alignment gap in vision-language models (VLMs), where the addition of visual inputs disrupts the safety alignment inherited from their language components. The proposed solution, VLM-Guard, is an inference-time intervention that projects the VLM's hidden representations onto a subspace orthogonal to a "safety steering direction" derived from a safety-aligned language model. This approach steers harmful queries away from harmful responses while maintaining response quality. Experiments on three malicious instruction settings demonstrate significant improvements in safety, with ASR reductions of 66-75% compared to baseline methods.

## Method Summary
VLM-Guard extracts a Safety Steering Direction (SSD) from the LLM component of a VLM by computing the difference in activation hidden states between pairs of harmful and harmless queries. This SSD is then used to project the VLM's hidden states onto a subspace orthogonal to the harmful direction during inference. The method leverages the LLM component as supervision for safety alignment, addressing the "modality gap" that causes harmful queries to cluster closer to harmless ones when visual inputs are introduced.

## Key Results
- VLM-Guard significantly improves safety, reducing attack success rates (ASR) by 66-75% compared to baseline methods.
- The approach maintains response quality as measured by perplexity, indicating that the projection does not degrade the model's reasoning capabilities.
- Experiments conducted on three malicious instruction settings (MaliciousInstruct, Jailbreak dataset, and MM-Harmful Bench) demonstrate the effectiveness of VLM-Guard in various scenarios.

## Why This Works (Mechanism)

### Mechanism 1: Modality-Induced Safety Decay
The introduction of visual inputs disrupts the safety alignment of the underlying LLM by blurring the representation boundary between harmful and harmless queries. The "modality gap" causes a distribution shift in the shared embedding space, leading to harmful queries clustering closer to harmless ones.

### Mechanism 2: Safety Steering Direction (SSD) Extraction
The inherent safety awareness of the LLM component can be mathematically extracted as a directional vector to supervise the VLM. By computing the difference in activation hidden states between pairs of harmful and harmless queries, the method isolates the principal components that correlate with "refusal" behavior.

### Mechanism 3: Orthogonal Subspace Projection
Removing the component of the VLM's hidden state that aligns with the harmful direction neutralizes the unsafe intent while preserving response fluency. During inference, the model projects the VLM's hidden states onto a subspace orthogonal to the SSD, effectively stripping away the features that trigger unsafe generation.

## Foundational Learning

- **Concept:** Singular Value Decomposition (SVD)
  - **Why needed here:** The method uses SVD to reduce the dimensionality of activation differences and find the primary "direction" of safety in the model's hidden states.
  - **Quick check question:** Can you explain why we take the right singular vectors (V) from the SVD of the activation difference matrix to find the direction of variance?

- **Concept:** Inference-Time Intervention (Representation Engineering)
  - **Why needed here:** Unlike training (SFT/RLHF), this method modifies the model's forward pass dynamically at test time without updating weights.
  - **Quick check question:** How does modifying the hidden state h_l at layer l affect the subsequent layers and the final logit output?

- **Concept:** The Modality Gap
  - **Why needed here:** This is the core theoretical motivation. You must understand that different data modalities (image vs. text) occupy distinct regions in the shared embedding space.
  - **Quick check question:** Why does the separation of image and text embeddings in a shared space weaken the text-only safety classifier?

## Architecture Onboarding

- **Component map:**
  1. Anchor Dataset: 100 harmful + 100 harmless "How to" text queries.
  2. LLM Backbone: The language model inside the VLM used to extract the SSD.
  3. SVD Calculator: Computes the steering matrix V_{m,l} offline.
  4. VLM Engine: The target model where intervention occurs.
  5. Intervention Hook: A module at layer l that applies the projection to the hidden states.

- **Critical path:**
  1. Offline Phase: Run anchor data through the LLM backbone → Calculate Activation Difference A → SVD(A) → Save steering vectors V_{m,l}.
  2. Online Phase: User sends multimodal query → VLM processes up to layer l → Hook applies projection h' = h - h V^T V → VLM completes forward pass.

- **Design tradeoffs:**
  - Layers (L_G): Intervening on too many layers may destroy semantic coherence; too few may fail to block the harmful intent.
  - Subspace Dimension (m): Using more singular vectors (m) captures more of the "safety" concept but risks over-constraining the representation.
  - Gate Threshold: The binary gate (h_l(q)V_{1,l} > 0) determines if the query is harmful. Setting this threshold balances false refusal vs. attack success.

- **Failure signatures:**
  - Semantic Collapse: The model outputs "I cannot fulfill..." for benign queries (over-refusal) or generates fluent but irrelevant text due to over-projection.
  - High Perplexity: If the projection distorts the hidden state too far from the distribution the model expects, perplexity (PPL) will spike, indicating unnatural language.

- **First 3 experiments:**
  1. Blank Image Test: Replicate the "Figure 1" experiment. Verify that the unmodified VLM attacks successfully with a blank image, then confirm VLM-Guard blocks it.
  2. Layer Sensitivity Analysis: Ablate the intervention layers (L_G). Try intervening only at the middle layers vs. final layers to find the "sweet spot" for safety vs. fluency.
  3. Hyperparameter (m) Scan: Vary the number of SVD components (m) used in the projection. Plot Attack Success Rate (ASR) vs. Perplexity to find the Pareto frontier.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the modality gap negatively impact non-safety capabilities like reasoning and general understanding?
- Basis in paper: The authors state in the Limitations section that "the influence of this gap on other capabilities such as reasoning and understanding remains to be investigated in future research."
- Why unresolved: The experimental scope was restricted to safety metrics (Attack Success Rate) and general fluency (Perplexity), without evaluating complex task performance.
- What evidence would resolve it: Evaluating VLM-Guard on standard multimodal reasoning benchmarks (e.g., ScienceQA) to verify that the intervention does not degrade cognitive capabilities.

### Open Question 2
- Question: Can the subspace projection strategy be adapted to address safety misalignment during the training phase?
- Basis in paper: The authors acknowledge their approach "primarily intervenes at the inference stage and does not address the issue comprehensively during the training phase."
- Why unresolved: VLM-Guard currently functions as a post-hoc activation manipulation; it does not permanently correct the model's weights or the vision-language connector.
- What evidence would resolve it: Integrating the orthogonality constraint into the training loss function of a VLM and comparing the resulting safety alignment against inference-time intervention.

### Open Question 3
- Question: Is the Safety Steering Direction (SSD) extracted from the LLM component robust across different VLM architectures?
- Basis in paper: The experiments are conducted exclusively on the LLaVA-1.5-7b model, leaving the generalizability to other architectures untested.
- Why unresolved: It is unclear if the "safety steering direction" is universal or dependent on the specific embedding space of the LLaVA architecture.
- What evidence would resolve it: Applying VLM-Guard to VLMs with different projection mechanisms (e.g., InstructBLIP) to see if the same LLM-derived vector effectively prevents attacks.

## Limitations
- The effectiveness of VLM-Guard relies heavily on the linearity assumption of the safety alignment space. If harmful query representations are distributed non-linearly, the SVD-derived steering direction may be insufficient.
- The method requires a pre-existing LLM safety alignment to bootstrap from. For VLMs trained without strong safety alignment or from scratch, this approach would be ineffective.
- The method introduces a computational overhead at inference time, though exact latency impact is not quantified. This could limit real-world deployment in latency-sensitive applications.

## Confidence

- **High Confidence:** The geometric mechanism of projecting hidden states orthogonal to the SSD is mathematically sound and well-specified. The empirical improvement in ASR reduction (66-75%) is directly measurable.
- **Medium Confidence:** The claim that the "modality gap" is the primary cause of safety decay is plausible but not definitively proven. The paper provides correlational evidence (PCA visualizations) but not causal ablation studies.
- **Low Confidence:** The assertion that response quality (measured by perplexity) is "maintained" is weak. Perplexity is an imperfect proxy for semantic coherence and relevance, and the paper does not report user studies or task-completion metrics.

## Next Checks

1. **Ablation on Anchor Dataset Quality:** Systematically vary the number and diversity of anchor pairs used to extract the SSD. Show how ASR reduction degrades with fewer anchors or anchors from a different distribution (e.g., "safety" vs. "harmful").
2. **False Positive Rate Quantification:** Beyond perplexity, measure the model's refusal rate on a curated set of benign but ambiguous queries (e.g., "How to defend yourself" vs. "How to attack someone"). This tests the false positive rate of the binary gate.
3. **Non-Linear Safety Boundary Test:** Apply a non-linear dimensionality reduction technique (e.g., UMAP) to the anchor dataset activations. If harmful and harmless queries are not linearly separable in this space, the SVD approach has a fundamental limitation.