---
ver: rpa2
title: 'MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs and
  Deep Learning'
arxiv_id: '2502.18371'
source_url: https://arxiv.org/abs/2502.18371
tags:
- memorability
- video
- advertisement
- mindmem
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MindMem, a multimodal deep learning model
  that predicts advertisement memorability by integrating visual, auditory, and textual
  data. The model uses pre-trained embeddings from LongVA, Qwen2 Audio, and Qwen2
  Text, combined with self-attention pooling and cross-attention mechanisms to capture
  intra- and inter-modal dependencies.
---

# MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs and Deep Learning

## Quick Facts
- **arXiv ID:** 2502.18371
- **Source URL:** https://arxiv.org/abs/2502.18371
- **Reference count:** 40
- **Primary result:** Achieves Spearman's correlation of 0.631 on LAMBDA dataset, outperforming single- and dual-modal models and state-of-the-art methods.

## Executive Summary
This paper introduces MindMem, a multimodal deep learning model that predicts advertisement memorability by integrating visual, auditory, and textual data. The model uses pre-trained embeddings from LongVA, Qwen2 Audio, and Qwen2 Text, combined with self-attention pooling and cross-attention mechanisms to capture intra- and inter-modal dependencies. On the LAMBDA dataset, MindMem achieves a Spearman's correlation coefficient of 0.631, outperforming single- and dual-modal models and state-of-the-art methods like Henry and ViT-Mem. The model also demonstrates strong performance on the Memento10K dataset (ρ=0.731). Analysis reveals that video pacing, scene complexity, and emotional diversity positively influence memorability. Additionally, the MindMem-ReAd system, which uses LLM-based simulations, improves advertisement memorability by up to 74.12% on a test set of 50 videos.

## Method Summary
MindMem integrates multimodal data (video, audio, text) through frozen pre-trained encoder LLMs (LongVA, Qwen2-Audio, Qwen2-Text) that extract modality-specific embeddings. These embeddings are projected to a shared 1,024-dimensional latent space, then processed through self-attention pooling to capture intra-modal dependencies and cross-attention mechanisms to capture inter-modal interactions. The pooled representations are fused through a lightweight neural network to predict memorability scores between 0 and 1. The MindMem-ReAd system extends this by using a fine-tuned LLaMA 3.1 LLM to generate scene descriptions based on titles/key messages, improving predicted memorability.

## Key Results
- Achieves Spearman's correlation coefficient of 0.631 on LAMBDA dataset, outperforming single- and dual-modal models (ρ=0.564-0.589) and state-of-the-art methods like Henry and ViT-Mem
- Demonstrates strong generalization with ρ=0.731 on Memento10K dataset for natural videos
- Shows video pacing, scene complexity, and emotional diversity positively influence memorability
- MindMem-ReAd system improves advertisement memorability by up to 74.12% on test set of 50 videos

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multimodal integration of visual, auditory, and textual data improves memorability prediction over single- or dual-modal approaches.
- **Mechanism:** Pre-trained encoder LLMs extract modality-specific embeddings, projected to shared 1,024-dimensional space, leveraging complementary information across modalities.
- **Core assumption:** Human memory encoding is inherently multimodal, and combining modalities captures cognitive processes that single modalities miss.
- **Evidence:** Table 1 shows MindMem (ρ=0.631) outperforms Video only (0.564), Text only (0.589), and Audio only (0.336).

### Mechanism 2
- **Claim:** Self-attention pooling captures intra-modal dependencies more effectively than simple pooling methods.
- **Mechanism:** Variable-length embeddings pass through self-attention layers that compute weighted attention scores, emphasizing contextually relevant features before aggregation into fixed-length vectors.
- **Core assumption:** Not all frames/segments within a modality contribute equally to memorability; selective attention improves signal-to-noise ratio.
- **Evidence:** Table 2 ablation shows self-attention pooling (ρ=0.614) significantly outperforms average pooling (ρ=0.424) and max pooling (ρ=0.462).

### Mechanism 3
- **Claim:** Cross-attention mechanisms capture inter-modal dependencies, enabling the model to simulate human-like sensory integration.
- **Mechanism:** Each modality's pooled representation attends to the other two modalities, producing cross-attended embeddings that combine modality-specific features with contextual information from other modalities.
- **Core assumption:** Memorability emerges from interactions between modalities (e.g., emotional audio + visual scene), not just their sum.
- **Evidence:** Table 2 shows self + cross-attention combination (ρ=0.631) outperforms self-attention alone (ρ=0.614).

## Foundational Learning

- **Attention mechanisms (self-attention and cross-attention)**
  - *Why needed here:* Core architectural component for both intra-modal feature selection and inter-modal alignment. Without understanding attention, you cannot debug why certain features are weighted more heavily.
  - *Quick check:* Given a sequence of 10 video frame embeddings, can you compute a basic single-head self-attention output manually?

- **Transfer learning with frozen pre-trained encoders**
  - *Why needed here:* MindMem uses frozen LongVA, Qwen2-Audio, and Qwen2-Text as embedding extractors. Understanding what these models encode is critical for debugging representation issues.
  - *Quick check:* What information might LongVA capture that Qwen2-Text cannot, and vice versa?

- **Spearman's rank correlation coefficient**
  - *Why needed here:* Primary evaluation metric (ρ=0.631). Unlike MSE, Spearman measures monotonic relationship, which matters when ranking ads by memorability is more important than exact score prediction.
  - *Quick check:* If predicted scores are [0.2, 0.5, 0.8] and ground truth is [0.1, 0.6, 0.9], what is the approximate Spearman correlation?

## Architecture Onboarding

- **Component map:** Raw inputs (video, audio, text) → Frozen encoders (LongVA, Qwen2-Audio, Qwen2-Text) → Projection layers (Linear → LayerNorm → Dropout) → 1,024-dim shared space → Self-attention pooling (per modality) → Cross-attention (each modality attends to other two) → Fusion network (Concat → FC → ReLU → Dropout → Linear → Sigmoid) → Predicted memorability score (0-1)

- **Critical path:** Frozen encoder outputs → projection alignment → self-attention pooling. If embeddings are not properly normalized/aligned at projection stage, downstream attention mechanisms receive noisy inputs.

- **Design tradeoffs:**
  - Frozen vs. fine-tuned encoders: Freezing reduces compute and prevents overfitting but limits domain adaptation to advertisements
  - 1,024-dim shared space: Higher dimensions capture more information but increase memory and compute
  - Text-only model as judge for MindMem-ReAd: Pragmatic for evaluation but may miss audio/visual factors that full multimodal model would capture

- **Failure signatures:**
  - Audio-only model performs poorly (ρ=0.336) → audio embeddings may lack sufficient signal for short clips
  - Large gap between training and validation performance → check for overfitting in fusion network
  - Cross-attention provides minimal improvement over self-attention → modalities may already be highly correlated

- **First 3 experiments:**
  1. **Baseline sanity check:** Run single-modal models (video-only, text-only, audio-only) on your data. If text-only doesn't achieve ρ≥0.4, verify text extraction pipeline.
  2. **Ablation on attention:** Compare average pooling → self-attention → self+cross-attention on held-out validation set. Replicate finding that self-attention provides largest gain.
  3. **Modality drop analysis:** For ads where MindMem predicts incorrectly, identify which modality contributes most to error by zeroing out each modality's embedding and measuring impact.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the improvement in predicted memorability scores for regenerated advertisements (MindMem-ReAd) translate to actual increased recall in human viewers?
- **Basis:** The study reports a "74.12% improvement" in memorability for regenerated ads, but this metric is derived from a "text-only trained model" acting as judge, rather than empirical human feedback.
- **Why unresolved:** There is disconnect between model's prediction (based on text descriptions) and real-world outcome (human memory), creating risk of optimization bias.
- **What evidence would resolve it:** A follow-up user study where human participants view original and regenerated advertisements, with brand/scene recall measured after 24-hour delay.

### Open Question 2
- **Question:** To what extent do cultural context and individual viewer biases influence generalizability of MindMem model across global markets?
- **Basis:** Authors state models "rely on specific multimodal inputs, which might not capture other relevant factors like cultural context or individual-specific biases."
- **Why unresolved:** LAMBDA dataset may not fully represent diverse cultural responses to stimuli, limiting model's applicability to international advertising campaigns.
- **What evidence would resolve it:** Testing model on diverse, geographically specific datasets and analyzing performance discrepancies across different demographic or cultural groups.

### Open Question 3
- **Question:** Is audio modality inherently less predictive for short-form video content, or is its lower performance an artifact of insufficient audio context in 3-second clips?
- **Basis:** Results on Memento10K show Spearman's ρ of only 0.291 for audio-only prediction compared to 0.709 for video. Authors "suspect that the 3-second audio clips provide insufficient information."
- **Why unresolved:** Unclear if audio features are universally weaker predictors for general video memorability compared to ads, or if specific duration of Memento10K clips crippled audio encoder.
- **What evidence would resolve it:** Ablation study on LAMBDA dataset using truncated audio segments of varying lengths (e.g., 3s, 10s, 30s) to isolate effect of duration on audio modality's predictive weight.

## Limitations
- LAMBDA dataset is proprietary, limiting direct replication of primary results (ρ=0.631)
- Audio performance was notably weak (ρ=0.336) even with cross-modal integration
- Paper omits critical training hyperparameters (learning rate, batch size, epochs, dropout rates)
- MindMem-ReAd system validates against small test set (n=50), limiting confidence in 74.12% improvement claim

## Confidence
- **High confidence:** Multimodal integration improves over single/dual-modal approaches
- **Medium confidence:** Self-attention pooling provides significant gains over simple pooling methods
- **Medium confidence:** Cross-attention captures meaningful inter-modal dependencies
- **Low confidence:** MindMem-ReAd's 74.12% improvement claim (based on small test set, LLM simulation rather than human evaluation)

## Next Checks
1. **Modality ablation study on held-out validation set:** Systematically disable each modality and measure impact on ρ to confirm which modalities drive predictions and whether cross-modal gains are robust.
2. **Temporal alignment validation:** Verify audio, visual, and text features are properly synchronized within each clip by visualizing attention weights to detect temporal misalignments.
3. **Cross-dataset generalization test:** Train MindMem on Memento10K and evaluate on different advertisement memorability dataset to assess whether learned multimodal interactions generalize beyond training distribution.