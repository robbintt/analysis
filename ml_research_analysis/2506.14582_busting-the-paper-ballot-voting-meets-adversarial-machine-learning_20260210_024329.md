---
ver: rpa2
title: 'Busting the Paper Ballot: Voting Meets Adversarial Machine Learning'
arxiv_id: '2506.14582'
source_url: https://arxiv.org/abs/2506.14582
tags:
- adversarial
- ballot
- attacks
- examples
- voting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that machine learning classifiers used in
  US election tabulators are vulnerable to adversarial examples. The authors train
  six models (SVM, SimpleCNN, VGG-16, ResNet-20, CaiT, and Twins) on four new ballot
  datasets and show that standard white-box attacks fail due to gradient masking caused
  by numerical instability.
---

# Busting the Paper Ballot: Voting Meets Adversarial Machine Learning

## Quick Facts
- **arXiv ID**: 2506.14582
- **Source URL**: https://arxiv.org/abs/2506.14582
- **Reference count**: 40
- **Primary result**: Machine learning classifiers in US election tabulators are vulnerable to adversarial examples, with physical attacks achieving high misclassification rates using commodity printers and scanners

## Executive Summary
This work demonstrates that machine learning classifiers used in US election tabulators are vulnerable to adversarial examples. The authors train six models (SVM, SimpleCNN, VGG-16, ResNet-20, CaiT, and Twins) on four new ballot datasets and show that standard white-box attacks fail due to gradient masking caused by numerical instability. They overcome this by modifying the difference of logits ratio loss function. Physical attacks using commodity printers and scanners succeed in causing misclassification rates high enough to flip close elections. For example, ResNet-20 shows 0.962 attack success rate for ðœ– = 16/255 in physical attacks. The study highlights risks of deploying ML in voting systems and recommends transparency, standardized datasets, and improved model design.

## Method Summary
The researchers trained six different ML classifiers on four newly created ballot datasets to evaluate vulnerability to adversarial attacks. They discovered that standard white-box attacks failed due to gradient masking from numerical instability in the classifiers. To overcome this, they modified the difference of logits ratio loss function. Physical attacks were then conducted using commodity printers and scanners to test real-world feasibility. The study examined both digital and physical attack scenarios across multiple model architectures to assess the robustness of ML-based voting systems.

## Key Results
- Standard white-box attacks fail on election tabulators due to gradient masking from numerical instability
- Modified difference of logits ratio loss function overcomes gradient masking and enables successful attacks
- Physical attacks using commodity printers and scanners achieve misclassification rates high enough to flip close elections
- ResNet-20 achieves 0.962 attack success rate for ðœ– = 16/255 in physical attacks

## Why This Works (Mechanism)
The vulnerability stems from the use of machine learning classifiers in voting systems, which inherit the fundamental weakness to adversarial examples that affects all ML models. The numerical instability in the classifiers creates gradient masking, preventing standard white-box attacks from working effectively. By modifying the loss function to address this masking, the researchers can craft adversarial examples that successfully fool the classifiers. Physical attacks work because the printed adversarial examples maintain their deceptive properties when scanned back into the system, demonstrating that the vulnerability persists beyond the digital domain.

## Foundational Learning
- **Machine learning classification in voting systems**: Why needed - ML is increasingly used for ballot tabulation efficiency; Quick check - Understand the basic workflow of how ML classifies votes from ballot images
- **Adversarial examples**: Why needed - These are carefully crafted inputs that cause ML models to make mistakes; Quick check - Can you explain how small perturbations can change classification decisions?
- **Gradient masking**: Why needed - This is when models appear robust to attacks but are actually hiding their gradients; Quick check - Can you identify signs that a model might be gradient masking rather than truly robust?
- **Difference of logits ratio loss**: Why needed - This specific loss function is used to craft adversarial examples; Quick check - Can you explain how this loss function differs from standard cross-entropy?

## Architecture Onboarding
- **Component map**: Datasets -> Model Training -> Attack Generation -> Physical Printing -> Scanning -> Classification
- **Critical path**: Training data preparation â†’ Model selection and training â†’ Adversarial attack generation â†’ Physical attack implementation â†’ Result evaluation
- **Design tradeoffs**: Using more complex models (like ResNet) provides better baseline accuracy but may increase vulnerability to attacks; simpler models (like SVM) may be less accurate but potentially more robust
- **Failure signatures**: High classification accuracy on clean data but dramatic drops when adversarial examples are introduced; failure to converge on standard attack methods indicating gradient masking
- **First experiments**: 1) Test standard white-box attacks on trained models to confirm gradient masking, 2) Implement modified loss function and verify it overcomes masking, 3) Conduct physical attacks with printed adversarial examples and measure classification rates

## Open Questions the Paper Calls Out
None

## Limitations
- Attack success rates were achieved under controlled laboratory conditions using commodity printers and scanners, which may not fully represent real-world election environments
- Research focuses exclusively on US election tabulators, limiting generalizability to other voting systems globally
- Study does not address potential countermeasures or practical feasibility of executing such attacks at scale in actual election scenarios

## Confidence
- **High confidence**: The fundamental vulnerability of ML classifiers to adversarial examples, the existence of gradient masking in voting system classifiers, and the successful demonstration of physical adversarial attacks
- **Medium confidence**: The generalizability of attack success rates to real-world election environments and the scalability of such attacks
- **Medium confidence**: The long-term effectiveness of the modified loss function across different model architectures and threat models

## Next Checks
1. Test adversarial examples across multiple scanning passes and real-world ballot handling conditions to assess attack robustness
2. Evaluate the modified loss function's effectiveness against a broader range of model architectures beyond those tested
3. Investigate potential countermeasures and their effectiveness in mitigating the demonstrated vulnerabilities