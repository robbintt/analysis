---
ver: rpa2
title: An Expert-grounded benchmark of General Purpose LLMs in LCA
arxiv_id: '2510.19886'
source_url: https://arxiv.org/abs/2510.19886
tags:
- llms
- were
- reviewers
- tasks
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides the first expert-grounded benchmark of general-purpose
  large language models (LLMs) for life cycle assessment (LCA). Seventeen experienced
  LCA practitioners reviewed 168 AI-generated answers from eleven LLMs across 22 LCA-related
  tasks, evaluating scientific accuracy, explanation quality, robustness, verifiability,
  and adherence to instructions.
---

# An Expert-grounded benchmark of General Purpose LLMs in LCA

## Quick Facts
- arXiv ID: 2510.19886
- Source URL: https://arxiv.org/abs/2510.19886
- Authors: Artur Donaldson; Bharathan Balaji; Cajetan Oriekezie; Manish Kumar; Laure Patouillard
- Reference count: 5
- One-line primary result: Expert review of 11 general-purpose LLMs on 22 LCA tasks revealed 37% of responses contain inaccurate or misleading information, with hallucination rates up to 40% for citations.

## Executive Summary
This study provides the first expert-grounded benchmark of general-purpose large language models (LLMs) for life cycle assessment (LCA). Seventeen experienced LCA practitioners reviewed 168 AI-generated answers from eleven LLMs across 22 LCA-related tasks, evaluating scientific accuracy, explanation quality, robustness, verifiability, and adherence to instructions. Results show that 37% of responses contained inaccurate or misleading information, with hallucination rates up to 40% for citations. While some models performed well on accuracy and explanation, no model excelled across all criteria. The study highlights significant reliability gaps in using LLMs naÃ¯vely for LCA and underscores the need for larger, diverse benchmarks and grounding mechanisms to improve trustworthiness and practical utility.

## Method Summary
The study evaluated 11 general-purpose LLMs (mix of commercial and open-weight models) using 22 manually-created LCA prompts requiring short-form answers. Responses were generated via single-shot generation without retrieval tools or RAG systems. Expert reviewers with 5+ years LCA experience evaluated outputs through a multi-stage workflow on the Zooniverse platform, assessing scientific accuracy (1-4 Likert), explanation quality (1-4 Likert), robustness (binary: inaccurate/misleading), verifiability (citation hallucination rate), and instruction following (binary format adherence). The evaluation focused on five task types: Goal & Scope definition, Life Cycle Inventory analysis, Impact Assessment, Interpretation, and Citation tasks.

## Key Results
- 37% of all responses contained inaccurate or misleading information, with 40% hallucination rate for citations
- No model excelled across all evaluation criteria, though Gemini 2.0 Flash showed highest accuracy (90%) but lowest robustness (83%)
- Citation hallucination rates varied significantly, with some models producing non-existent citations at rates up to 40%
- Open-weight models (Llama 4 Scout, Llama 4 Maverick) performed comparably to commercial models in certain criteria

## Why This Works (Mechanism)

### Mechanism 1: Expert-Grounded Multi-Criteria Validation
- Claim: Substituting automated ground-truth metrics with structured human expert review enables evaluation in domains where methodological variance prevents a single "correct" answer.
- Mechanism: The system decomposes the review process into discrete stages (Accuracy, Explanation, Verifiability) using Likert scales and binary checks. This forces the reviewer to distinguish between a factually correct but poorly explained answer and a well-argued but misleading one, reducing the ambiguity inherent in open-ended LCA tasks.
- Core assumption: Expert practitioners possess a shared, albeit implicit, standard of "good practice" that can be consistently applied even without standardized protocols.
- Evidence anchors:
  - [abstract]: "...addressing the absence of standardized evaluation frameworks in a field where no clear ground truth or consensus protocols exist."
  - [section]: Section 2.1.5 & 4.1.1 notes that for open-ended tasks, Likert scales assess answer quality where no golden standard exists.
- Break condition: The mechanism degrades if the reviewer pool is too small or biased, leading to high standard error (noted in Section 3.7) or if inter-rater reliability is low.

### Mechanism 2: Latent Knowledge Stress-Testing via Citation
- Claim: Prohibiting access to external tools and requiring citations acts as a stress test for the model's internal knowledge representation, revealing hallucination rates as a proxy for reliability.
- Mechanism: By prompting for specific references without retrieval access, the model must rely on compressed weights. The resulting "hallucinated" or non-existent citations serve as an objective, binary failure signal that correlates with the model's ability to handle domain-specific facts.
- Core assumption: The ability to correctly recall a citation correlates strongly with the ability to correctly apply the underlying LCA methodology.
- Evidence anchors:
  - [abstract]: "Hallucination rates varied significantly, with some models producing hallucinated citations at rates of up to 40%."
  - [section]: Section 2.1.4 defines citation ability as a "proxy measure of hallucination rate because it is easily and objectively verifiable."
- Break condition: The test is invalid if the model's training data was contaminated with the specific benchmark questions (Section 4.1.5), though the authors mitigated this by creating novel prompts.

### Mechanism 3: Grounding as a Mitigation for Oracle Behavior
- Claim: Transitioning from "free-form oracle" usage to Retrieval-Augmented Generation (RAG) constrains the output space, reducing errors by anchoring generation in vetted external data.
- Mechanism: Instead of relying on the model to generate facts from memory (which causes the 37% error rate observed), the proposed architecture uses the LLM as a reasoning engine over provided context (e.g., LCI databases). This ensures that claims are supported by inspectable evidence rather than probabilistic guessing.
- Core assumption: The retrieved external context is itself accurate, complete, and relevant to the specific LCA query.
- Evidence anchors:
  - [abstract]: "...underscores the need... for grounding mechanisms to improve trustworthiness..."
  - [section]: Section 19/Conclusion suggests integrating "retrieval-augmented generation over vetted LCI and EF corpora... so that generated text is consistently grounded."
- Break condition: The mechanism fails if the retrieval step returns irrelevant or low-quality documents, leading to "garbage in, garbage out" or if the model ignores the provided context.

## Foundational Learning

- Concept: **LCA Methodological Variance**
  - Why needed here: LCA outcomes often vary not due to calculation errors but due to subjective choices (e.g., system boundaries). Understanding this explains why a simple "correctness" benchmark is insufficient and why expert review is required.
  - Quick check question: Why can't we just compare an LLM's LCA results against a single database value to determine accuracy?

- Concept: **Model Hallucination (in specialized domains)**
  - Why needed here: The paper identifies hallucination (specifically fake citations) as a critical failure mode. Learners must understand that LLMs prioritize fluent generation over factual accuracy, especially in niche fields with less training data.
  - Quick check question: If an LLM generates a confident citation to a non-existent paper, is this a bug or a feature of its architecture?

- Concept: **Open-Weight vs. Commercial Models**
  - Why needed here: The study challenges the assumption that commercial models are superior, showing open-weight models (e.g., Llama 4 Scout) competing on par. This informs procurement and deployment choices.
  - Quick check question: Does paying for a commercial API guarantee better scientific accuracy for niche tasks according to this benchmark?

## Architecture Onboarding

- Component map:
  Input Layer (22 Task-Specific Prompts) -> Inference Layer (11 General-Purpose LLMs) -> Evaluation Layer (Zooniverse Platform -> 17 Expert Reviewers -> 5 Criteria Rubric)

- Critical path:
  1. Define task type (e.g., Goal & Scope vs. Inventory Analysis)
  2. Select model based on specific criteria (e.g., Explanation quality vs. Cost)
  3. **Implement Grounding**: Wrap the model in a RAG system to reduce the 37% error baseline
  4. **Validate**: Run outputs through a human-in-the-loop check focused on the "Verifiability" criterion

- Design tradeoffs:
  - **Accuracy vs. Variance**: Top models (Gemini 2.0 Flash) showed high accuracy but also high variance/instability (Section 3.7). Stable models (GPT-4.1) might be preferred for enterprise consistency.
  - **Reasoning vs. Clarity**: Chain-of-thought models (DeepSeek-r1) confused reviewers due to formatting (Section 3.8), trading off raw reasoning power for user clarity.

- Failure signatures:
  - **Hallucination**: Citations to non-existent DOIs or authors
  - **Instruction Drift**: Ignoring word limits or omitting requested sections (e.g., Goal & Scope)
  - **CoT Bloat**: Releasing raw chain-of-thought logs that reviewers find confusing or discontinuous

- First 3 experiments:
  1. **Baseline Test**: Run the provided benchmark prompts (from GitHub repo) against your chosen internal model to replicate the 37% error rate and establish a "naive" baseline
  2. **RAG Ablation**: Select 5 high-hallucination prompts from the benchmark and implement a basic RAG connector to a standard LCA database; measure the reduction in "Misleading Information" flags
  3. **Instruction Robustness**: Test if explicit formatting instructions (e.g., "Return JSON only") improve the "Format Adherence" score (currently ~65-94%) across different temperature settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do retrieval-augmented generation (RAG) or agentic pipelines perform on LCA tasks compared to the general-purpose "free-form oracle" models evaluated in this benchmark?
- Basis in paper: [explicit] The conclusion states that "the next wave of studies should stress-test agentic and RAG-based pipelines on real practitioner tasks" to verify if grounding mechanisms improve trustworthiness.
- Why unresolved: This study intentionally isolated general-purpose models using baseline prompting without external tools or retrieval to establish a risk baseline.
- What evidence would resolve it: A comparative benchmark using the same LCA tasks where models are equipped with access to vetted Life Cycle Inventory (LCI) and emission factor (EF) corpora.

### Open Question 2
- Question: What is the quantifiable trade-off between the labor hours saved by using LLMs and the time required for experts to review and correct the outputs?
- Basis in paper: [explicit] The conclusion identifies the need to "quantify reviewer effort saved versus corrections required" to determine practical utility.
- Why unresolved: The study measured accuracy and hallucination rates but did not collect time-motion data on the human review process required to fix the identified errors.
- What evidence would resolve it: A user study tracking the time practitioners take to complete specific LCA tasks with versus without AI assistance, including the time spent correcting specific model errors.

### Open Question 3
- Question: What is the statistical inter-rater reliability among LCA experts when evaluating the same LLM-generated responses?
- Basis in paper: [inferred] The authors note in Section 3.7 that "measuring inter-rater reliability scores was not possible due to only a handful of LLM responses receiving more than one expert review."
- Why unresolved: The limited number of expert reviews per response prevented the calculation of agreement metrics (e.g., Krippendorf's Alpha), leaving the consistency of expert judgment unknown.
- What evidence would resolve it: A larger-scale evaluation where multiple distinct experts review the exact same set of model responses to allow for the calculation of statistical agreement coefficients.

## Limitations
- The benchmark's reliance on single or dual expert reviews per response introduces significant statistical uncertainty, with high standard error potentially masking true performance differences between models.
- The absence of standardized LCA protocols creates an inherently subjective evaluation framework where "scientific accuracy" depends on individual expert interpretation rather than objective ground truth.
- The exclusion of retrieval-augmented generation in the baseline evaluation means reported error rates (37% overall, up to 40% citation hallucination) may not reflect real-world performance when models are properly grounded.

## Confidence
- **High Confidence**: The finding that no model excels across all evaluation criteria is robust, supported by consistent patterns across multiple expert reviewers and task types. The identification of hallucination as a critical failure mode (37% overall rate) is objectively verifiable through citation analysis.
- **Medium Confidence**: Performance rankings between specific models (e.g., Gemini 2.0 Flash vs. GPT-4.1) carry moderate uncertainty due to limited review counts per response and the subjective nature of explanation quality assessments.
- **Low Confidence**: The assumption that citation hallucination rates directly correlate with overall LCA knowledge reliability remains speculative, as the benchmark does not validate whether models with lower hallucination rates produce more accurate methodological applications.

## Next Checks
1. **Inter-rater Reliability Validation**: Replicate the benchmark with expanded reviewer pools (minimum 5 experts per response) and compute Cohen's kappa to establish statistical significance of performance differences between models.
2. **RAG Implementation Benchmark**: Implement retrieval-augmented generation using standard LCA databases (e.g., Ecoinvent, ELCD) for the 5 highest-hallucination prompts and measure reduction in misleading information flags compared to naive generation.
3. **Task-Type Sensitivity Analysis**: Conduct ablation testing by isolating task categories (e.g., only Goal & Scope definition vs. only Impact Assessment) to determine if certain LCA subtasks are inherently more reliable for LLM applications than others.