---
ver: rpa2
title: 'Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal
  Large Language Models'
arxiv_id: '2505.24164'
source_url: https://arxiv.org/abs/2505.24164
tags:
- reward
- arxiv
- data
- zhang
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Mixed-R1, a unified reinforcement learning
  framework that improves multimodal large language models (MLLMs) across diverse
  reasoning tasks. The method combines a mixed reward function design with a carefully
  curated post-training dataset (Mixed-45K) containing 45,000 high-quality examples
  spanning five task categories: yes/no questions, multiple-choice questions, chart/document
  analysis, visual grounding, and open-ended responses.'
---

# Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2505.24164
- Source URL: https://arxiv.org/abs/2505.24164
- Reference count: 40
- Primary result: Unified reinforcement learning framework improves MLLMs 2-5% across reasoning benchmarks

## Executive Summary
Mixed-R1 introduces a unified reinforcement learning framework that enhances multimodal large language models across diverse reasoning tasks. The method combines task-specific reward functions with a carefully curated dataset (Mixed-45K) containing 45,000 high-quality examples spanning five task categories. A novel tokenizer-based Bidirectional Max-Average Similarity (BMAS) reward enables effective open-ended response evaluation without external language models. Experiments demonstrate consistent improvements across benchmarks including MathVista, MathVision, MMMU, MMStar, and AI2D.

## Method Summary
Mixed-R1 employs GRPO with four specialized reward functions: matching reward for binary/multiple-choice tasks, chart reward for numerical reasoning, IoU reward for grounding tasks, and BMAS reward for open-ended responses. The method filters training data by sampling 8 responses per question from a baseline model and removing examples where all responses receive identical rewards. The unified reward combines individual task rewards with a format reward for structure compliance. Training uses batch_size=64, temperature=1.0, KL_coef=0.04, and lr=3e-6 on Qwen2.5-VL-7B-Instruct.

## Key Results
- 2-5% improvement across MathVista, MathVision, MMMU, MMStar, MMBench, and AI2D benchmarks
- BMAS outperforms external LLM judges (ModernBERT, Sentence-BERT, Qwen2.5-0.5B) with 44.7 average vs. 43.3-43.9
- 45K dataset outperforms both smaller (20K) and larger (90K) variants, demonstrating quality over quantity
- Incremental gains of +3% average improvement from adding MCQ reward over YORN-only baseline

## Why This Works (Mechanism)

### Mechanism 1
Task-specific reward functions enable stable multi-task RL training across heterogeneous MLLM tasks. Each data type receives a specialized reward that provides meaningful gradient signals within GRPO's advantage computation. The unified reward system combines these with format rewards to maintain response structure.

### Mechanism 2
Bidirectional Max-Average Similarity (BMAS) provides effective reward signals for open-ended responses without external models. BMAS computes bidirectional max-average cosine similarity between token embeddings from generated and ground-truth responses, handling synonyms and paraphrasing through maximum similarity per direction.

### Mechanism 3
Data filtering via baseline model sampling removes examples where GRPO advantage would collapse. Before training, Qwen2.5-VL-7B generates 8 responses per question at temperature 1.0, removing samples where all responses receive identical rewards to prevent zero-variance advantage computation.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Why needed: Core RL algorithm that eliminates need for separate critic model by computing advantages relative to sampled responses. Quick check: Given rewards [0,0,1,0,1,0,0,1], what is advantage of third response? (A₃ = (1-0.375)/0.484 ≈ 1.29)

- **Tokenizer Embeddings vs. Hidden States**: Why needed: BMAS uses input embeddings before transformer layers; understanding this distinction is critical for implementation. Quick check: Why prefer input embeddings over hidden states for reward computation? (Avoids backprop through embedding extraction, more stable, computationally cheaper)

- **Reward Signal Scale and Normalization**: Why needed: GRPO's advantage computation relies on group-wise normalization; mixed reward types with different scales require careful handling. Quick check: If IoU rewards cluster near 0.8-0.9 while matching rewards are 0/1, how might this affect learning? (High-IoU samples may appear artificially "better," potentially biasing policy toward IoU tasks)

## Architecture Onboarding

- **Component map**: Mixed-45K Dataset → Data Filter (baseline sampling) → Filtered Training Data → Reward Function Router → GRPO Training Loop → Policy Model

- **Critical path**: Data filtering → Reward routing → BMAS embedding extraction → Advantage computation → Policy update. BMAS is the most fragile component.

- **Design tradeoffs**:
  - BMAS vs. External Judge: BMAS saves compute but may miss semantic nuances; external judges add cost and potential embedding space mismatch
  - Dataset size: 45K chosen over 90K (quality beats quantity for GRPO)
  - Temperature 1.0 for filtering: Encourages diversity but may reject valid edge cases

- **Failure signatures**:
  - Advantage collapse: All responses receive identical rewards → std = 0 → division error or zero gradient
  - BMAS saturation: Reward stays near 1.0 throughout training → no learning signal
  - Format overfitting: Model produces correct structure but incorrect answers → λ too high
  - Cross-task interference: Adding IoU degrades MCQ performance → reward scales misaligned

- **First 3 experiments**:
  1. Single-reward baseline: Train with only matching reward on MCQ data to establish GRPO convergence
  2. BMAS ablation on held-out captions: Compare BMAS vs. human-annotated similarity scores on 100 pairs
  3. Reward scale sensitivity: Run mini-sweeps with λ ∈ {0.1, 0.5, 1.0} and IoU scaling factors

## Open Questions the Paper Calls Out

- **Video and multi-image extension**: Can unified reward design be effectively extended to video understanding and multiple-image reasoning tasks? The authors explicitly state future work will address this limitation.

- **BMAS superiority conditions**: Why does BMAS outperform bipartite graph matching and external LLM judges for open-ended reward calculation, and under what conditions might this advantage diminish?

- **Optimal data scaling**: What is the optimal data scale and mixture ratio for Mixed-R1 training, and does scaling beyond 45K samples yield diminishing returns on certain reasoning domains?

## Limitations

- Dataset curation using 7B baseline may introduce bias toward "medium" difficulty problems, with uncertain generalization to out-of-distribution cases
- BMAS validation relies on comparative ablation rather than direct human judgment correlation
- Reward scale compatibility across binary, continuous, and embedding-based rewards lacks comprehensive empirical validation

## Confidence

- **High confidence**: GRPO implementation effectiveness for multi-task RL (supported by incremental gains from adding reward types)
- **Medium confidence**: 2-5% benchmark improvements (statistically demonstrated but modest)
- **Low confidence**: BMAS superiority over external LLM judges (indirect comparison, no direct validation)

## Next Checks

1. Manually evaluate 100 open-ended responses where BMAS assigns high rewards but external judges disagree to quantify semantic understanding capability

2. Systematically vary reward scaling factors (particularly for IoU and chart rewards) and measure impact on cross-task performance

3. Evaluate models on held-out dataset of explicitly filtered problems (too easy or too hard) to assess generalization tradeoffs