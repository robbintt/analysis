---
ver: rpa2
title: 'COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability'
arxiv_id: '2510.04196'
source_url: https://arxiv.org/abs/2510.04196
tags:
- safety
- content
- reasoning
- multimodal
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents COSMO-RL, a reinforcement learning framework
  for training Large Multimodal Reasoning Models (LMRMs) that jointly optimizes safety,
  value, and general reasoning capability. The method addresses the challenge of safety
  alignment in multimodal settings by integrating staged training, policy stabilization
  via Clipped Policy Gradient with Policy Drift (CPGD), and multimodal jailbreak data
  augmentation into a unified pipeline.
---

# COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability

## Quick Facts
- arXiv ID: 2510.04196
- Source URL: https://arxiv.org/abs/2510.04196
- Reference count: 25
- Key outcome: Safety (+20.5% MM-SafetyBench), value (+26.2% FLAMES), and reasoning (+7.7% average) co-evolved via staged RL with CPGD and multimodal jailbreak augmentation.

## Executive Summary
This paper introduces COSMO-RL, a two-stage reinforcement learning framework that jointly optimizes reasoning, safety, and helpfulness in Large Multimodal Reasoning Models (LMRMs). By integrating Clipped Policy Gradient with Policy Drift (CPGD) stabilization, multimodal jailbreak data augmentation, and Oracle Reward Models (ORMs), the framework achieves strong improvements across safety, value alignment, and general reasoning benchmarks. COSMO-R1 demonstrates that safety and capability can co-evolve effectively without compromising robustness to jailbreak attacks.

## Method Summary
COSMO-RL employs a staged RL pipeline: (1) SFT with structured Chain-of-Thought (CoT) data distilled from teacher models; (2) Stage 1 RL optimizing general reasoning; (3) Stage 2 RL jointly optimizing safety, value, and general capability using a unified multiobjective reward. The framework uses CPGD for policy stabilization, multimodal jailbreak augmentation (textual: paraphrasing/synonym substitution; visual: GPT-4o key-element extraction), and three ORMs (Safety, Value, Knowledge). The reward combines Visual-Focus, Helpful, Format, and Task-Aware components, applied to Qwen2.5-VL-72B, InternVL3-78B, Qwen2.5-VL-7B, and DeepSeek-R1-Distill-Llama-70B backbones.

## Key Results
- +20.5% on MM-SafetyBench and +21.8% average across safety benchmarks (MM-SafetyBench, MSSBench, SIUO, XSTest).
- +26.2% on FLAMES and +13.7% on M3oralBench for value alignment.
- +7.7% average across MMMU, MathVista, Olympiad, GPQA Diamond, and GAOKAO-MM for general reasoning.
- Robust to multimodal jailbreak attacks while maintaining over-safety mitigation.

## Why This Works (Mechanism)
The framework's effectiveness stems from staged training that first establishes strong reasoning capability, then jointly refines safety and helpfulness via a stabilized RL policy. CPGD mitigates policy drift during joint optimization, while multimodal jailbreak augmentation ensures robustness to adversarial inputs. The unified reward function balances multiple objectives without degrading any single dimension, enabling co-evolution of safety and capability.

## Foundational Learning
- **Reinforcement Learning with Oracle Reward Models**: Why needed—to align model behavior with complex human values (safety, helpfulness) that are hard to encode manually. Quick check—verify ORM labels are consistent and cover edge cases.
- **Policy Gradient Stabilization (CPGD)**: Why needed—to prevent catastrophic policy drift during multiobjective RL. Quick check—monitor KL divergence between old and new policies during training.
- **Multimodal Jailbreak Data Augmentation**: Why needed—to train robustness against adversarial inputs that exploit modality-specific weaknesses. Quick check—test augmented samples on baseline models to confirm effectiveness.

## Architecture Onboarding

**Component Map**: SFT Data → Stage 1 RL (General) → Stage 2 RL (Joint) → ORMs (Safety/Value/Knowledge) → Unified Reward → CPGD Policy Updates

**Critical Path**: SFT → Stage 1 RL → Stage 2 RL → Evaluation. Stage 1 must converge before Stage 2 to avoid safety degradation.

**Design Tradeoffs**: Fixed reward weights vs. adaptive scheduling; staged vs. simultaneous training; depth of CoT vs. training efficiency.

**Failure Signatures**: Over-refusal (low safe acceptance on XSTest-Safe), safety degradation post-Stage 2 (MM-SafetyBench drop), jailbreak vulnerability (high success rate on augmented attacks).

**First Experiments**: (1) Train Stage 1 RL on general reasoning data; (2) Evaluate safety metrics pre/post-Stage 2; (3) Test jailbreak robustness on augmented samples.

## Open Questions the Paper Calls Out
- Can reward weights be dynamically adjusted during training to improve safety-capability balance?
- How dependent are gains on specific ORM architectures and training procedures?
- Does two-stage training generalize to multi-stage curricula for complex multimodal reasoning?
- Can joint multimodal training more effectively reduce over-safety than adding a helpfulness reward alone?

## Limitations
- Reliance on proxy ORM metrics rather than direct human evaluation introduces uncertainty about real-world robustness.
- Multimodal jailbreak augmentation details are abstracted, limiting reproducibility of robustness claims.
- Staged training may introduce instability if Stage 1 is not sufficiently converged before Stage 2.

## Confidence
- **High Confidence**: Staged RL framework design and CPGD stabilization are well-justified; benchmark improvements are specific and measurable.
- **Medium Confidence**: Effectiveness of multimodal jailbreak augmentation and unified reward function lacks full transparency; ablation findings may not generalize across backbones.
- **Low Confidence**: Long-term safety in dynamic environments is unaddressed; potential failure modes beyond reported benchmarks are not discussed.

## Next Checks
1. Conduct human evaluation of Safety ORM judgments on diverse multimodal queries, including edge cases and adversarial examples.
2. Test framework robustness against a broader range of multimodal jailbreak attacks and adversarial inputs.
3. Evaluate performance on multimodal tasks outside specified benchmarks (e.g., healthcare, autonomous driving) to assess generalizability.