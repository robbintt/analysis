---
ver: rpa2
title: Factor Analysis with Correlated Topic Model for Multi-Modal Data
arxiv_id: '2504.18914'
source_url: https://arxiv.org/abs/2504.18914
tags:
- data
- topic
- factor
- factm
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FACTM is a novel Bayesian model for integrating simple and structured
  multi-view data by combining factor analysis with correlated topic modeling. It
  handles structured views (e.g., text, single-cell data) where each sample contains
  multiple data points with clustering structure, and links them to simple views through
  sample-specific modifications of population-level cluster proportions.
---

# Factor Analysis with Correlated Topic Model for Multi-Modal Data

## Quick Facts
- **arXiv ID:** 2504.18914
- **Source URL:** https://arxiv.org/abs/2504.18914
- **Reference count:** 40
- **Primary result:** FACTM integrates simple and structured multi-view data by linking factor analysis to correlated topic modeling through sample-specific topic proportion adjustments, outperforming existing methods in cluster identification and integration accuracy.

## Executive Summary
FACTM is a novel Bayesian model for integrating simple and structured multi-view data by combining factor analysis with correlated topic modeling. It handles structured views (e.g., text, single-cell data) where each sample contains multiple data points with clustering structure, and links them to simple views through sample-specific modifications of population-level cluster proportions. The model is optimized using variational inference and includes a supervised rotation method to enhance interpretability of latent factors with respect to binary features. On text/video benchmarks and real-world music/COVID-19 datasets, FACTM outperforms existing methods in identifying clusters in structured data and integrating them with simple modalities, demonstrating superior accuracy in parameter estimation and improved predictive power for label classification. The rotation method enhances factor interpretability by creating stronger associations between consecutive factors and binary features.

## Method Summary
FACTM combines factor analysis for simple views with correlated topic modeling for structured views, using a shared latent factor matrix Z to link both. The structured view includes a sample-specific link variable μn that modifies population-level topic proportions based on the shared factors, creating a bridge between modalities. The model uses mean-field variational inference with coordinate ascent updates for most parameters, employing L-BFGS-B optimization for the structured-view parameters due to softmax non-conjugacy. A post-hoc supervised rotation method using the Kabsch-Umeyama algorithm aligns factors with binary features to enhance interpretability without affecting model likelihood.

## Key Results
- FACTM achieves superior Spearman correlation (>0.8) in recovering true factors from synthetic data compared to baseline methods
- FACTM demonstrates improved classification AUC (≥0.75) on benchmark datasets compared to FA-only and two-stage FA+CTM approaches
- Supervised rotation creates statistically significant (p<0.05) improvements in diagonal associations between factors and binary features

## Why This Works (Mechanism)

### Mechanism 1: Cross-Structure Linkage via Sample-Specific Topic Modulation
- **Claim:** FACTM enables information flow between simple and structured views through a dedicated linking variable that modifies population-level topic proportions per sample.
- **Mechanism:** The model introduces μn as a sample-specific adjustment to the population mean μ^(0) in the structured view. This variable depends on the shared latent factors Z and structured-view-specific loadings W, creating a bridge: the same factors that explain variance in simple views also influence topic distributions in structured views. During inference, μn receives gradients from both the factor analysis likelihood (via the normal prior on μn) and the topic model likelihood (via its role in the softmax transformation of ηn).
- **Core assumption:** The relationship between latent factors and topic proportions is approximately linear through the loadings W, and the prior precision t captures the strength of this coupling.
- **Evidence anchors:**
  - [abstract] "links them to simple views through sample-specific modifications of population-level cluster proportions"
  - [section 3.2] "This sample-specific term constitutes the link between the structured view and the remaining views in the model and depends on the factors Z and view-specific loadings W"
  - [corpus] Weak direct corpus support—neighbor papers address multi-modal fusion but not specifically this linked FA-CTM architecture; no comparable mechanism found
- **Break condition:** If the linear coupling assumption fails (e.g., topic-factor relationships are highly nonlinear), the linkage may underperform; the paper notes plans for nonlinear extensions in Section 5.

### Mechanism 2: Variational Inference with Conjugate Updates
- **Claim:** FACTM achieves tractable inference through mean-field variational approximation with closed-form coordinate ascent updates for most parameters.
- **Mechanism:** The model exploits conjugacy relationships in the Bayesian formulation. For μn, the normal prior and normal likelihood combine to yield a normal posterior with analytically computable parameters (Section 3.3). For topic model parameters, the multinomial-Dirichlet conjugacy enables closed-form updates for β. The ηn updates require L-BFGS-B optimization due to the softmax non-conjugacy, using a Taylor-based lower bound with auxiliary parameter ζn. The spike-and-slab prior on simple-view loadings enables automatic sparsity determination.
- **Core assumption:** Mean-field factorization is approximately valid—latent variables are conditionally independent given observed data, which may not hold if strong posterior correlations exist.
- **Evidence anchors:**
  - [section 3.3] Provides explicit update equations showing coordinate ascent structure
  - [appendix A.1] Full derivations for ηn optimization including gradient formulas
  - [corpus] No direct corpus evidence for this specific inference scheme in related multi-modal work
- **Break condition:** If posterior correlations are strong (e.g., factors and topics highly coupled), mean-field may underestimate uncertainty; diagnostic: check ELBO convergence behavior and compare with MCMC if feasible.

### Mechanism 3: Supervised Rotation via Correlation Alignment
- **Claim:** Post-hoc rotation of latent factors can enhance interpretability by aligning factors with known binary features without changing model likelihood.
- **Mechanism:** Factor analysis has rotational invariance—rotating factors Z and loadings W by orthogonal matrix R preserves ZW'. The method computes cross-correlation matrix H between factors and binary features using point-biserial correlation, performs SVD (H = USV'), and constructs rotation R = UV'. This is a post-processing step applied after model fitting, not during training.
- **Core assumption:** The binary features provide meaningful supervision signal, and the optimal rotation exists within the space of orthogonal transformations (not all possible linear transformations).
- **Evidence anchors:**
  - [section 3.3.1] Describes Kabsch-Umeyama algorithm application with point-biserial correlation
  - [section 4.3] Figure 6 shows improved diagonal associations after rotation on Mirex dataset
  - [corpus] No corpus evidence for this specific rotation technique in multi-modal contexts
- **Break condition:** If factors don't have inherent structure aligned with the binary features, rotation may create spurious interpretability; check: do multiple rotation initializations converge to similar solutions?

## Foundational Learning

- **Concept: Factor Analysis and Probabilistic PCA**
  - **Why needed here:** FACTM builds on FA for simple views; understanding the decomposition Y = ZW' + ε, rotational invariance, and sparsity-inducing priors is essential.
  - **Quick check question:** Given data matrix Y, can you explain why rotating factors and loadings preserves the likelihood?

- **Concept: Correlated Topic Model (CTM) vs LDA**
  - **Why needed here:** The structured view uses CTM, which replaces LDA's Dirichlet prior with a logistic normal, enabling topic covariance inference.
  - **Quick check question:** What additional capability does CTM provide over LDA, and what parameter captures this?

- **Concept: Variational Inference and ELBO**
  - **Why needed here:** The entire inference framework uses coordinate ascent on the ELBO with mean-field approximation.
  - **Quick check question:** Why is the ELBO a lower bound on the marginal likelihood, and what does the mean-field assumption simplify?

## Architecture Onboarding

- **Component map:** Simple views (M modalities) → Y^m matrices with spike-and-slab loadings W^m → Shared factors Z → Structured view with topic-word distributions β and topic proportions η (logistic normal) → Link variable μn → Supervised rotation (optional)

- **Critical path:**
  1. Initialize variational parameters (factors, loadings, topic parameters)
  2. Iterate: update simple-view parameters → update structured-view parameters → update link variable μn → update shared factors Z
  3. Monitor ELBO convergence
  4. (Optional) Apply supervised rotation post-training

- **Design tradeoffs:**
  - More factors K: Better reconstruction but higher complexity and potential overfitting
  - More topics L: Finer granularity in structured data but slower inference (ηn dimension increases)
  - Link precision t: Higher values enforce stronger factor-topic coupling; defaults to t=1
  - Spike-and-slab vs pure ARD: Spike-and-slab adds feature-level sparsity but more hyperparameters

- **Failure signatures:**
  - ELBO oscillation or non-convergence: Check learning rate, reduce complexity, or increase iterations
  - All topics collapse to similar distributions: May indicate too few topics or poor initialization; try different seeds
  - Factors have near-zero variance across samples: Check data normalization; factors may be capturing noise
  - Covariance Σ^(0) estimation poor: CTM alone may fail; ensure link variable is active (λ > 0 in simulations showed importance)

- **First 3 experiments:**
  1. **Synthetic data validation:** Generate data with known ground truth (as in Section 4.1), run FACTM with varying λ (link strength), measure Spearman correlation of recovered vs true factors. Target: >0.8 correlation at λ=1.
  2. **Ablation on structured view:** Compare FACTM vs FA-only (ignore structured data) vs FA+CTM (two-stage) on benchmark like CMU-MOSEI. Measure classification AUC using latent factors as features. Target: FACTM ≥ FA+CTM.
  3. **Rotation interpretability check:** On a dataset with known binary labels (e.g., Mirex emotion classes), fit FACTM, apply rotation, and verify diagonal associations improve. Use Wilcoxon tests as in Figure 6. Target: More significant associations on diagonal post-rotation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can incorporating automatic prior learning techniques, such as prior predictive matching, significantly improve the model's performance compared to the non-informative priors currently utilized?
- **Basis in paper:** [explicit] Section 5 states the intention to "enhance our hyperparameter selection process by incorporating automatic learning techniques" in future work.
- **Why unresolved:** The current study fixes hyperparameters to be non-informative or consistent across methods, leaving the potential benefits of learned priors unexplored.
- **What evidence would resolve it:** A comparative study showing improved parameter estimation accuracy or predictive power when using automatic prior learning versus the default settings.

### Open Question 2
- **Question:** How can FACTM be extended to model nonlinear dependencies between views and latent factors without sacrificing the computational efficiency of the current variational inference scheme?
- **Basis in paper:** [explicit] Section 5 notes that the model currently assumes linear dependencies due to the conjugacy of parametric distributions, limiting expressiveness.
- **Why unresolved:** The analytical solutions used for optimization rely on this conjugacy, which does not hold for nonlinear relationships.
- **What evidence would resolve it:** A modified model architecture (e.g., using neural networks) that captures nonlinear structures in benchmark data while maintaining tractable inference.

### Open Question 3
- **Question:** How robust is the supervised rotation method when the number of binary supervision features differs from the number of latent factors, or when the correlation signal is weak?
- **Basis in paper:** [inferred] The rotation method constructs a correlation matrix H and uses SVD, implying a dependency on the alignment between factors and features which may not always exist.
- **Why unresolved:** The experiments show success on specific datasets, but the sensitivity of the Kabsch-Umeyama algorithm to rank mismatch or noise in this specific context is not analyzed.
- **What evidence would resolve it:** Ablation studies evaluating the interpretability of rotated factors under conditions of sparse or noisy binary feature inputs.

## Limitations

- The linear coupling assumption between factors and topic proportions may not capture complex nonlinear relationships in some datasets
- Mean-field variational inference may underestimate posterior uncertainty when factors and topics are strongly correlated
- The supervised rotation method could create spurious interpretability if factors lack meaningful structure aligned with binary features

## Confidence

- **High confidence:** The FA-CTM architecture and basic inference framework are well-specified and theoretically sound
- **Medium confidence:** The cross-structure linkage mechanism works as described, though its effectiveness depends on data characteristics
- **Medium confidence:** The supervised rotation improves interpretability, but its value is dataset-dependent

## Next Checks

1. Test FACTM on synthetic data with known ground truth and measure Spearman correlation recovery across different link strength parameters
2. Run ablation studies comparing FACTM against FA-only and FA+CTM two-stage approaches on benchmark datasets
3. Verify supervised rotation effectiveness by checking diagonal association improvements on datasets with known binary labels using statistical tests