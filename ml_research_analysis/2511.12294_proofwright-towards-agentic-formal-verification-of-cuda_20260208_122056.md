---
ver: rpa2
title: 'ProofWright: Towards Agentic Formal Verification of CUDA'
arxiv_id: '2511.12294'
source_url: https://arxiv.org/abs/2511.12294
tags:
- verification
- agent
- vercors
- cuda
- kernels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ProofWright introduces an agentic framework that combines formal
  verification with LLM-generated CUDA kernels to ensure memory safety, thread safety,
  and semantic correctness. The system uses two main components: a VerCors Agent that
  automatically generates annotations for verifying memory and thread safety, and
  a Semantic Equivalence Framework that translates PyTorch specifications into Rocq
  formal proofs to validate functional correctness.'
---

# ProofWright: Towards Agentic Formal Verification of CUDA

## Quick Facts
- arXiv ID: 2511.12294
- Source URL: https://arxiv.org/abs/2511.12294
- Reference count: 40
- Primary result: Combines formal verification with LLM-generated CUDA kernels, achieving 74% safety verification and 14% semantic equivalence on KernelBench L1

## Executive Summary
ProofWright introduces an agentic framework that automatically verifies memory safety, thread safety, and semantic correctness of LLM-generated CUDA kernels. The system uses two main components: a VerCors Agent that generates annotations for verifying memory and thread safety, and a Semantic Equivalence Framework that translates PyTorch specifications into Rocq formal proofs to validate functional correctness. On KernelBench L1, ProofWright successfully verified safety properties for 74% of generated kernels and established semantic equivalence for 14% of them, uncovering subtle bugs missed by conventional testing. The approach adds only a modest 3-minute overhead per kernel, demonstrating scalable, automated formal verification of GPU code.

## Method Summary
ProofWright combines formal verification tools (VerCors for safety, Rocq for semantic correctness) with LLM-based agents that generate and refine verification annotations. The VerCors Agent uses in-context learning from a knowledge base and dynamic annotation guide to iteratively generate memory and thread safety contracts. The Semantic Equivalence Framework translates PyTorch specifications into Rocq theorems using a static analyzer and MLRocq library, then proves functional equivalence. Both agents interact with their respective formal tools, which serve as ungameable ground truth to validate correctness.

## Key Results
- Successfully verified memory safety and thread safety for 74% of generated CUDA kernels
- Established semantic equivalence between CUDA kernels and PyTorch specifications for 14% of kernels
- Added only ~3 minutes of verification overhead per kernel
- Uncovered subtle bugs missed by conventional testing approaches

## Why This Works (Mechanism)

### Mechanism 1: VerCors Agent's Iterative Annotation Generation via Learned Guides
The agent uses in-context learning from a knowledge base and dynamically updated annotation guide to iteratively generate and refine VerCors annotations for memory/thread safety. When verification fails, it uses error feedback to repair annotations. This generalizes verification strategies from past successes to new kernels.

### Mechanism 2: Semantic Equivalence via Formal PyTorch-to-Rocq Translation
A static analyzer converts PyTorch models into operation graphs, which are mapped to formal Rocq specifications using the MLRocq library. The Rocq Agent synthesizes alternative mathematical representations and constructs proofs to validate equivalence between CUDA kernels and PyTorch specs.

### Mechanism 3: Ensuring Reliability via Verifier as Ground Truth
Both VerCors and Rocq agents interact with formal tools that reject incorrect annotations or proofs. This closed-loop system catches errors immediately, preventing hallucinated annotations from being accepted as correct.

## Foundational Learning

Concept: **Hoare Logic and Program Contracts (Pre/Postconditions).**
Why needed: The VerCors approach is built on specifying program behavior via contracts. Understanding preconditions, postconditions, and invariants is essential.
Quick check: Given a simple kernel that adds two arrays, what would be a valid precondition for thread `i`'s write access to `C[i]`?

Concept: **Concurrent Separation Logic (CSL) and Permissions.**
Why needed: CUDA is parallel, requiring reasoning about memory partitioning and access permissions. CSL provides the formal framework for proving race-freedom.
Quick check: In CSL, why can't two threads hold a write permission (permission value 1) to the same memory location at the same time?

Concept: **Interactive Theorem Proving (e.g., in Rocq/Coq).**
Why needed: Proving semantic equivalence involves constructing formal mathematical proofs, requiring understanding of tactics and formalization.
Quick check: What is the role of a "tactic" in an interactive theorem prover like Rocq?

## Architecture Onboarding

Component map: PyTorch Spec -> Static Analyzer -> MLRocq Library -> PyTorch-to-Rocq Translator -> Rocq Agent -> Lowered VerCors Annotations -> VerCors Agent -> VerCors Verifier

Critical path: PyTorch Spec -> Static Analyzer -> Rocq Spec -> (Rocq Agent proves equivalence) -> Lowered VerCors Annotations -> (VerCors Agent adds safety annotations) -> VerCors Verifier

Design tradeoffs:
- Automation vs. Coverage: 74% safety, 14% semantic equivalence due to tool and agent limitations
- Trusted vs. Untrusted: Front-end is non-LLM based for trustworthiness, but final lowering step is LLM-based
- Overhead vs. Assurance: ~3 minutes per kernel is acceptable for high-assurance scenarios

Failure signatures:
- Agent Failure (9%): Caused by indirect addressing, complex shared memory patterns, or VerCors syntax limitations
- Verifier Instability (17%): SMT solver issues with non-linear arithmetic or quantifiers
- Semantic Proof Failure: Kernels with reductions, pooling, or complex cross-thread interactions

First 3 experiments:
1. Run VerCors Agent on 10-kernel subset used to create initial Annotation Guide to confirm reproducibility
2. Manually inspect MLRocq definitions for simple operation (e.g., ReLU) and trace Rocq theorem creation
3. Take kernel from "Verifier Instability" category and experiment with manual frame statement to observe resolution

## Open Questions the Paper Calls Out

Open Question 1: How can LLM-based verification agents be effectively integrated with dynamic analysis tools that do not expose a simple text interface for expert guidance? (Basis: Section 7 notes LLMs struggle with non-textual instrumentation APIs)

Open Question 2: Can the lowering of verified Rocq specifications into VerCors functional annotations be fully automated without manual verification? (Basis: Section 5.2 notes current lowering is LLM-based and not fully trusted)

Open Question 3: How can verification agents robustly handle SMT solver instability caused by non-linear arithmetic? (Basis: Section 6.1 identifies SMT solver instability as significant problem requiring manual debugging)

## Limitations

- Limited coverage with 74% safety verification and only 14% semantic equivalence success rates
- Final Rocq-to-VerCors lowering step is performed by an LLM and is explicitly not fully trusted
- Verifier instability from SMT solver issues causes failures on correct code with complex indexing

## Confidence

High Confidence: Core mechanism of using formal verification tools as ungameable ground truth is sound and well-justified
Medium Confidence: Iterative annotation generation approach is plausible but generalization capability from 10-kernel guide is not fully demonstrated
Low Confidence: Semantic equivalence framework's ability to handle complex kernels beyond simple mappings is questionable given low 14% success rate

## Next Checks

1. Validate the Annotation Guide's Generalization: Run VerCors Agent on the 10 manually annotated kernels used to create the initial guide to confirm it can reproduce the original successful verifications

2. Trace a Simple Semantic Proof: Manually inspect the MLRocq definitions for a basic operation (e.g., ReLU from Listing 5) and trace how PyTorch Static Analyzer output is translated into a Rocq theorem

3. Diagnose Verifier Instability: Select a kernel from the "Verifier Instability" category (e.g., Listing 8) and experiment with adding the manual frame statement (Listing 9) to observe if it resolves the instability