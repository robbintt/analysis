---
ver: rpa2
title: 'Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding'
arxiv_id: '2601.01089'
source_url: https://arxiv.org/abs/2601.01089
tags:
- attention
- protein
- gene
- embeddings
- enhancer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Central Dogma Transformer (CDT) addresses the challenge of integrating
  DNA, RNA, and protein foundation models for mechanism-oriented cellular understanding.
  The architecture employs directional cross-attention layers that mirror biological
  information flow: DNA-to-RNA attention models transcriptional regulation while RNA-to-Protein
  attention models translational relationships, producing a unified Virtual Cell Embedding.'
---

# Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding

## Quick Facts
- **arXiv ID:** 2601.01089
- **Source URL:** https://arxiv.org/abs/2601.01089
- **Reference count:** 0
- **Primary result:** CDT achieved Pearson correlation of 0.503 for predicting enhancer effects from sequence using CRISPRi data, representing 63% of theoretical ceiling

## Executive Summary
Central Dogma Transformer (CDT) introduces a mechanism-oriented AI architecture that integrates DNA, RNA, and protein foundation models through directional cross-attention layers that mirror biological information flow. The architecture employs DNA-to-RNA attention to model transcriptional regulation and RNA-to-Protein attention to model translational relationships, producing unified Virtual Cell Embeddings. Using fixed RNA and protein embeddings with CRISPRi enhancer perturbation data from K562 cells, CDT achieved predictive performance of r = 0.503, reaching 63% of the theoretical ceiling set by cross-experiment variability (r = 0.797). The study demonstrates that CDT's biologically aligned architecture enables meaningful interpretability through attention and gradient analyses, with gradient-based methods successfully identifying a CTCF binding site showing physical contact with both enhancer and target gene in Hi-C data.

## Method Summary
CDT employs directional cross-attention layers that mirror biological information flow: DNA-to-RNA attention models transcriptional regulation while RNA-to-Protein attention models translational relationships. The architecture produces a unified Virtual Cell Embedding by integrating foundation models for DNA, RNA, and protein. For evaluation, the model used fixed RNA and protein embeddings while training on CRISPRi enhancer perturbation data from K562 cells to predict enhancer effects from sequence. The directional attention mechanism ensures that information flows in the same direction as biological processes, with DNA features influencing RNA representations and RNA features influencing protein representations. The fixed embedding approach was chosen to leverage existing high-quality foundation models while focusing the CDT training on learning the regulatory relationships between molecular layers.

## Key Results
- Achieved Pearson correlation of 0.503 for predicting enhancer effects from sequence using CRISPRi data
- Reached 63% of theoretical ceiling (r = 0.797) set by cross-experiment variability
- Gradient analysis identified a CTCF binding site whose Hi-C data showed physical contact with both enhancer and target gene

## Why This Works (Mechanism)
CDT's success stems from its architectural alignment with biological reality. By structuring attention layers to mirror the directional flow of information in cells - DNA to RNA for transcription, RNA to Protein for translation - the model learns representations that reflect actual cellular mechanisms rather than arbitrary statistical correlations. This biological grounding enables the model to capture meaningful regulatory relationships between molecular layers. The fixed embedding approach leverages existing high-quality foundation models while allowing CDT to focus on learning the complex interactions between molecular layers, though this constraint may limit adaptability to context-dependent relationships.

## Foundational Learning
- **Directional cross-attention**: Attention mechanisms that flow in specific biological directions (DNA→RNA, RNA→Protein) rather than bidirectional; needed to model the asymmetric nature of biological information flow; quick check: verify attention weights show stronger influence in intended direction
- **Foundation model integration**: Combining pre-trained models for different molecular types into a unified framework; needed to leverage existing biological knowledge while learning new regulatory relationships; quick check: ensure embeddings maintain biological meaning after integration
- **Virtual Cell Embedding**: Unified representation space that captures multi-omics relationships; needed to enable holistic cellular understanding rather than siloed molecular analyses; quick check: validate embedding captures known biological relationships
- **Fixed embedding training**: Strategy of keeping some model components frozen during training; needed to leverage pre-trained biological knowledge while focusing learning on regulatory relationships; quick check: verify frozen components maintain expected properties
- **Mechanistic interpretability**: Using model internals (attention, gradients) to generate biological hypotheses; needed to move beyond black-box predictions to understanding underlying mechanisms; quick check: validate interpretations with orthogonal experimental data

## Architecture Onboarding

**Component Map:** DNA Foundation Model -> DNA-to-RNA Cross-Attention -> RNA Foundation Model -> RNA-to-Protein Cross-Attention -> Protein Foundation Model -> Virtual Cell Embedding

**Critical Path:** The directional flow from DNA through RNA to Protein represents the critical path, with each cross-attention layer learning regulatory relationships between adjacent molecular layers. The fixed embedding constraint means RNA and Protein components don't update during training, focusing learning on the cross-attention mechanisms.

**Design Tradeoffs:** Fixed embeddings leverage high-quality pre-trained models but limit adaptability to context-specific relationships. Directional attention ensures biological realism but may miss bidirectional regulatory mechanisms. The architecture prioritizes interpretability and biological alignment over maximum predictive performance.

**Failure Signatures:** Uniform attention patterns across samples suggest the model isn't learning meaningful regulatory relationships. Poor correlation between predicted and observed effects indicates the cross-attention mechanisms aren't capturing relevant biological signals. Gradient-based interpretations that don't align with known biology suggest the model has learned spurious correlations.

**First Experiments:**
1. Validate directional attention by testing whether reversing attention directions degrades performance
2. Test sensitivity to fixed embedding assumption by comparing with joint fine-tuning
3. Evaluate generalizability by applying CDT to different cell types and perturbation types

## Open Questions the Paper Calls Out
None

## Limitations
- Fixed embedding assumption prevents adaptation of RNA and protein representations to context-dependent relationships
- Single cell type (K562) and perturbation type (CRISPRi) limit generalizability across cellular contexts
- Interpretability analysis relies on single case study, making it difficult to assess robustness across genomic scenarios

## Confidence

**High Confidence:** Architectural design principles and directional cross-attention mechanism are well-founded and technically sound

**Medium Confidence:** Pearson correlation achievement (r = 0.503) is valid for specific experimental setup, but fixed embedding constraint limits broader applicability

**Medium Confidence:** Interpretability findings from single case study are suggestive but require validation across multiple genomic contexts

## Next Checks

1. Test CDT's performance on multiple cell types and perturbation modalities (e.g., CRISPR knockout, chemical perturbations) to assess generalizability beyond K562 CRISPRi data

2. Implement joint fine-tuning of all three embeddings (DNA, RNA, protein) rather than fixed embeddings to evaluate whether adaptive representations improve predictive performance and biological interpretability

3. Conduct systematic interpretability analysis across multiple genomic regions and cell types to validate whether attention patterns and gradient signals consistently reveal mechanistically relevant biological relationships, including validation through orthogonal experimental assays