---
ver: rpa2
title: 'Generating Spatial Synthetic Populations Using Wasserstein Generative Adversarial
  Network: A Case Study with EU-SILC Data for Helsinki and Thessaloniki'
arxiv_id: '2501.16080'
source_url: https://arxiv.org/abs/2501.16080
tags:
- data
- synthetic
- population
- populations
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that Wasserstein Generative Adversarial
  Networks (WGANs) can effectively generate high-dimensional synthetic populations
  from EU-SILC census data for urban applications. The approach successfully creates
  synthetic populations for Helsinki and Thessaloniki by training WGANs on national
  microdata and balancing using either EU-SILC weights or aggregated statistics.
---

# Generating Spatial Synthetic Populations Using Wasserstein Generative Adversarial Network: A Case Study with EU-SILC Data for Helsinki and Thessaloniki

## Quick Facts
- arXiv ID: 2501.16080
- Source URL: https://arxiv.org/abs/2501.16080
- Reference count: 25
- Primary result: WGANs effectively generate high-dimensional synthetic populations but under-represent fringe profiles, risking discrimination in agent-based simulations.

## Executive Summary
This study demonstrates that Wasserstein Generative Adversarial Networks (WGANs) can effectively generate high-dimensional synthetic populations from EU-SILC census data for urban applications. The approach successfully creates synthetic populations for Helsinki and Thessaloniki by training WGANs on national microdata and balancing using either EU-SILC weights or aggregated statistics. The synthetic populations achieved excellent statistical validity, with strong Pearson's correlation coefficients, R-squared values, and low SRMSE scores. Bland-Altman analysis confirmed good overall fit between synthetic and original data. However, the study identified a critical challenge: WGANs tend to under-represent fringe profiles, particularly evident in the variable self-perceived health. This under-representation poses discrimination risks for vulnerable groups in agent-based simulations. The findings suggest that while WGANs offer a computationally tractable solution for high-dimensional synthetic population generation, careful validation and potential mitigation strategies are needed to ensure fair representation of all population segments.

## Method Summary
The method employs WGAN-GP to generate synthetic populations from EU-SILC microdata. The approach uses 57 variables one-hot encoded into binary vectors (294 dims for Finland, 404 for Greece). Missing values are imputed via IterativeImputer/KNN. Two balancing strategies are tested: (1) WGAN-impute generates 300k candidates and extracts to match demographic keys, (2) duplicate-impute uses EU-SILC weights. The WGAN architecture includes a Critic (input_dim→100→150→1 with InstanceNorm1d, LeakyReLU 0.2) and Generator (latent_dim→150→100→feature_dim with BatchNorm1d, LeakyReLU 0.2, Sigmoid output). Validation uses SRMSE, Pearson correlation, R-squared, and Bland-Altman plots.

## Key Results
- WGANs successfully generate high-dimensional synthetic populations with excellent statistical validity (SRMSE, correlation, R-squared)
- Spatial fitting via WGAN-impute produces better demographic alignment than simple weight-duplication
- Critical under-representation of fringe profiles, particularly in self-perceived health variables, poses discrimination risks
- Validation against WGAN-imputed training data creates a "double distortion" blind spot that masks fringe profile erosion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial training allows for the tractable generation of high-dimensional synthetic populations where traditional combinatorial methods fail.
- Mechanism: The system employs a Wasserstein Generative Adversarial Network (WGAN) where a Generator creates fake records and a Critic evaluates them against real microdata. By optimizing the Wasserstein distance (rather than Jensen-Shannon divergence), the model avoids gradient vanishing problems common in high-dimensional spaces. This allows the model to learn complex joint distributions of 57 variables (294-dimension binary vector) without the computational explosion associated with methods like Iterative Proportional Fitting (IPF).
- Core assumption: The Wasserstein distance provides stable gradients sufficient to map a latent space to the complex manifold of demographic data.
- Evidence anchors: [Section 1] notes computational challenges in current methods; [Section 2.2] describes critic-generator competition; corpus confirms WGAN utility in learning from microsamples.
- Break condition: Mode collapse or failure to converge indicated by oscillating loss without improvement.

### Mechanism 2
- Claim: Spatial fitting via WGAN-imputation produces better demographic alignment than simple weight-duplication, but at the cost of internal validity for fringe variables.
- Mechanism: To localize national data to a specific city, the system first trains a WGAN on national data to generate a large pool of candidates, then extracts records matching the city's specific aggregated statistics. This "WGAN-impute" approach creates a training set that fits the city's profile better than merely duplicating records based on EU-SILC weights.
- Core assumption: The synthetic records generated for imputation are statistically equivalent to real records for all variables, not just the targeted demographic keys.
- Evidence anchors: [Section 2.1] details the WGAN-impute approach; [Section 4] concedes challenges for representing fringe values; corpus supports target-specific synthesis.
- Break condition: If the initial national model fails to generate sufficient candidates for rare demographic combinations in the target city, the spatial fit will be incomplete or biased.

### Mechanism 3
- Claim: Validating synthetic populations against WGAN-imputed training data creates a "double distortion" blind spot that masks the under-representation of vulnerable groups.
- Mechanism: If the training data is itself synthetic, validating the final synthetic population against this training data hides errors. The paper shows that while general metrics look excellent, specific fringe variables like "self-perceived health" are distorted. The neural network prioritizes frequent profiles, eroding rare values. Validating against the original weight-imputed data exposes this erosion; validating against the WGAN-imputed data hides it.
- Core assumption: Neural networks exhibit a frequency bias, inherently "averaging" outputs toward common profiles and under-representing tails.
- Evidence anchors: [Section 3.3] shows fit seems good when compared to WGAN-imputed originals but reveals under-representation when compared to weight-imputed originals; [Section 4] explicitly warns of the "double effect"; corpus indicates related work in diversity focus.
- Break condition: The mechanism fails to detect discrimination if the validation set is chosen poorly (validating synthetic vs. synthetic-imputed).

## Foundational Learning

- Concept: **Wasserstein Distance (Earth Mover's Distance)**
  - Why needed here: This is the core loss function replacing standard GAN loss. It provides a smoother gradient space when the generator's distribution is disjoint from the real data, which is critical for converging on complex tabular data.
  - Quick check question: How does the gradient penalty function in the code prevent the critic from outputting unbounded values that destabilize training?

- Concept: **Internal vs. External Validity in Synthetic Data**
  - Why needed here: The paper distinguishes between general statistical fit (external/demographic) and the preservation of specific complex relationships (internal). High R-squared values do not guarantee that specific sub-populations are viable for simulation.
  - Quick check question: Why would a model with high Pearson correlation coefficients still fail to produce a fair agent-based simulation?

- Concept: **One-Hot Encoding & Binary Vectors**
  - Why needed here: The architecture requires transforming categorical census data into a binary vector (size 294). Understanding how to map variables like "self-perceived health" into this space and back is required for data preparation and interpretation.
  - Quick check question: In the provided code, how does the feature_dimension relate to the one-hot encoded size of the input data?

## Architecture Onboarding

- Component map: Input (EU-SILC Microdata) -> Pre-processing (Imputation/One-Hot Encoding) -> Balancing Layer (Duplicate vs. WGAN-impute) -> Core Engine (PyTorch WGAN with GP) -> Evaluation (SRMSE/R-squared, Bland-Altman plots)

- Critical path:
  1. Impute missing values in raw EU-SILC data (Iterative/KNN imputer)
  2. Transform to binary vector (294 dims)
  3. Apply Balancing Strategy (Critical decision point: Duplicate vs. WGAN-impute)
  4. Train WGAN to convergence (Monitor Critic/Generator loss divergence)
  5. Generate synthetic pool
  6. Validate using Bland-Altman against original distributions to check for fringe profile erosion

- Design tradeoffs:
  - Weight-impute (Approach 2): Better preserves "Self-Perceived Health" distributions (less discrimination risk) but fits external demographic keys poorly
  - WGAN-impute (Approach 1): Fits external demographic keys perfectly but introduces a "double distortion" that skews health variables and risks under-representing vulnerable groups

- Failure signatures:
  - The "Double Distortion" Trap: Validation metrics look excellent (>0.9), but Bland-Altman plots show outliers in specific variables. Comparing synthetic data only to WGAN-imputed training data will hide this failure
  - Computational Ceiling: Section 3.2 notes a laptop could not generate populations above 700,000 records, necessitating regional extracts rather than full national synthesis

- First 3 experiments:
  1. Baseline Reconstruction: Train the WGAN on raw Finland EU-SILC data and validate reconstruction of single variables using SRMSE
  2. Sensitivity Analysis on Balancing: Train two models for Helsinki—one on Weight-imputed data and one on WGAN-imputed data. Compare "Self-Perceived Health" distributions against original data
  3. Bland-Altman Diagnostic: Generate a Bland-Altman plot for the best-performing model to identify which specific variable-value combinations fall outside confidence intervals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can deep generative models be adapted to ensure the fair representation of fringe profiles (small, vulnerable groups) in synthetic populations?
- Basis in paper: [explicit] The discussion explicitly states the urgent need for further investigation in representing fringe group profiles, noting that current methods lead to discrimination in agent-based simulations.
- Why unresolved: The paper demonstrates that WGANs inherently under-represent fringe profiles to prioritize frequent examples, and the study did not test mitigation strategies for this bias.
- What evidence would resolve it: A modified WGAN architecture or loss function that enforces statistical parity for minority classes without compromising global statistical validity.

### Open Question 2
- Question: Can neural manifold clustering or similar advanced validation techniques effectively preserve and verify deeper statistical structures in high-dimensional synthetic data?
- Basis in paper: [explicit] The conclusion suggests exploring Bland-Altman and other methods like neural manifold clustering techniques to preserve deeper statistical structures.
- Why unresolved: Current standard validation metrics are "shallow" and fail to detect simultaneous over- and under-representations that threaten internal validity.
- What evidence would resolve it: A comparative study showing that neural manifold clustering can identify structural misrepresentations that correlate with errors in agent-based simulation outcomes.

### Open Question 3
- Question: Is it possible to develop a balancing strategy that matches external aggregated statistics without introducing the distortion of fringe variables observed in the WGAN-imputation approach?
- Basis in paper: [inferred] The results show a trade-off where WGAN-impute fits demographics better but distorts health variables, while duplicate-impute preserves health profiles better but fits demographics poorly.
- Why unresolved: The paper identifies this distortion as a "particular challenge" but only tests two distinct methods, leaving the search for a unified solution open.
- What evidence would resolve it: A new balancing algorithm achieving statistically significant improvement in fringe variable representation while maintaining the same level of fit to aggregated demographic statistics.

## Limitations

- WGANs under-represent fringe profiles, particularly in self-perceived health variables, posing discrimination risks in agent-based simulations
- Validation against WGAN-imputed training data creates a "double distortion" blind spot that masks bias against vulnerable groups
- Computational limitations prevent generating full national populations on standard hardware, requiring regional extracts

## Confidence

- **High Confidence**: Statistical validity metrics (SRMSE, Pearson correlation, R-squared) and general feasibility of WGANs for high-dimensional synthetic population generation
- **Medium Confidence**: Discrimination risk from fringe profile under-representation and the "double distortion" validation blind spot
- **Low Confidence**: Exact threshold at which fringe profile under-representation becomes operationally problematic in agent-based simulations

## Next Checks

1. **Fringe Profile Sensitivity Analysis**: Systematically vary the frequency of rare demographic profiles in training data to quantify the relationship between training distribution and synthetic output representation

2. **Cross-City Validation**: Apply the same WGAN-impute approach to multiple cities within the same country to test whether fringe profile erosion is consistent or varies with local demographic characteristics

3. **Alternative Validation Frameworks**: Develop and test validation protocols that specifically target fringe profile representation, such as stratified sampling validation or outlier detection methods robust to "double distortion" effects