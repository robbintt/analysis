---
ver: rpa2
title: Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?
arxiv_id: '2507.10174'
source_url: https://arxiv.org/abs/2507.10174
tags:
- transformer
- learning
- offline
- decision
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the perceived advantages of Decision Transformer
  (DT) for offline reinforcement learning in sparse-reward environments. The authors
  propose Filtered Behavior Cloning (FBC), a simple method that filters low-performing
  trajectories before applying behavior cloning.
---

# Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?

## Quick Facts
- arXiv ID: 2507.10174
- Source URL: https://arxiv.org/abs/2507.10174
- Reference count: 2
- One-line primary result: Filtered Behavior Cloning outperforms Decision Transformer on sparse-reward offline RL tasks using fewer parameters and less training time

## Executive Summary
This paper challenges the perceived advantages of Decision Transformer (DT) for offline reinforcement learning in sparse-reward environments. The authors propose Filtered Behavior Cloning (FBC), a simple method that filters low-performing trajectories before applying behavior cloning. Through extensive experiments on Robomimic and D4RL benchmarks, FBC achieves competitive or superior performance to DT, using fewer parameters, less training data, and shorter wall-clock time. On D4RL sparsified datasets, FBC outperforms DT in 7 out of 9 tasks with a 4% aggregate improvement. On Robomimic, FBC achieves a 3.5% aggregate improvement. The results suggest DT offers no advantage over FBC in sparse-reward settings, and based on prior work, likely not in dense-reward environments either.

## Method Summary
The authors compare Decision Transformer (DT) against Filtered Behavior Cloning (FBC) on offline RL benchmarks. FBC filters trajectories by return—keeping only successful trajectories for natively sparse Robomimic datasets, and top 10% by final return for sparsified D4RL datasets—then applies vanilla behavior cloning with a 2-layer MLP. DT uses a 3-layer transformer with return-to-go conditioning. The experiments include both natively sparse tasks (Robomimic MG datasets with Lift and PickPlaceCan) and sparsified versions of D4RL locomotion tasks (walker2d, hopper, halfcheetah with medium/medium-replay/medium-expert variants). Training uses 5 seeds with 50 evaluation rollouts per checkpoint.

## Key Results
- FBC achieves 4% aggregate improvement over DT on D4RL sparsified datasets (7/9 tasks)
- FBC achieves 3.5% aggregate improvement over DT on Robomimic (2/2 tasks)
- FBC uses ~2x fewer parameters and trains ~3x faster than DT
- FBC matches or exceeds DT performance despite its simplicity

## Why This Works (Mechanism)

### Mechanism 1
Filtering low-performing trajectories before training can match or exceed the performance of return-conditioned sequence modeling in sparse-reward settings. FBC removes trajectories with low terminal returns, then applies standard behavior cloning. In sparse-reward settings, return-to-go (RTG) in DT primarily serves to distinguish successful from unsuccessful trajectories—filtering achieves this directly without requiring the model to learn the distinction. This works when state representation is approximately Markovian or can be made so by including recent state-action history.

### Mechanism 2
DT's transformer architecture provides no measurable advantage when the target return provides only binary success/failure information. In sparse-reward settings, RTG is effectively binary (1 for success, 0 for failure). The attention mechanism cannot extract richer temporal credit assignment from this signal, so DT collapses to approximately learning a weighted average of successful trajectories—similar to filtered BC but with unnecessary complexity.

### Mechanism 3
DT may learn from both high-quality and low-quality trajectories, but this provides no advantage over simply excluding low-quality data. The authors tested Filtered Decision Transformer (FDT)—DT trained only on filtered trajectories. FDT performed worse than DT on D4RL and similarly on Robomimic, suggesting DT's performance comes from implicit upweighting of successful trajectories during training, not from learning compensatory patterns from failures.

## Foundational Learning

- **Offline Reinforcement Learning**: Learning from fixed datasets without environment interaction. Critical for understanding why distributional shift between behavior and learned policies is a core challenge here but not in online RL.
  - Quick check: Can you explain why distributional shift between the behavior policy and learned policy is a core challenge in offline RL but not in online RL?

- **Behavior Cloning (BC)**: Supervised learning to mimic demonstrated actions. Essential for understanding FBC's foundation and the assumption that training distribution covers optimal behavior.
  - Quick check: If your filtered dataset contains only successful trajectories but they all solve the task in different ways, what multimodality problem might BC encounter?

- **Return-to-Go (RTG) Conditioning**: Conditioning action prediction on target returns. Key to understanding DT's primary innovation and how RTG interacts with sparse vs. dense rewards.
  - Quick check: In a sparse-reward task where 30% of trajectories succeed, what happens if you set RTG=1 at inference but the model has seen mostly RTG=0 during training?

## Architecture Onboarding

- **Component map**: Dataset → Trajectory Filter (top 10% by return or success-only) → MLP (2 layers, 512 hidden units) → Behavior Cloning Loss
  DT Pipeline: Dataset → Transformer Encoder (3 layers, 1 head, 128 dim) → RTG-conditioned action prediction via cross-attention over (RTG, state, action) sequence
  FDT Pipeline: Same as DT but with filtered dataset

- **Critical path**: 
  1. Compute trajectory returns for your dataset
  2. Apply filtering rule (success-only for sparse, top-10% for sparsified)
  3. Verify filtered dataset size is sufficient (the paper's Robomimic MG datasets had 244-716 trajectories post-filtering)
  4. Train MLP with BC loss; evaluate with target return matching your filtering threshold

- **Design tradeoffs**:
  - Filtering threshold (x%): Lower values increase quality but reduce data; the paper uses 10% for sparsified rewards without ablation
  - Context length for DT: Paper uses K=1 for Robomimic, K=20 for D4RL—longer context didn't help sparse tasks
  - MLP vs Transformer backbone: MLP is ~2x fewer parameters and ~3x faster training but cannot model long-range dependencies

- **Failure signatures**:
  - FBC underperforms when the filtered dataset is too small or lacks diversity
  - FBC cannot stitch together partial solutions from different trajectories
  - DT underperforms relative to FBC when RTG provides no richer signal than binary success/failure
  - Both methods fail if the state representation is insufficiently Markovian

- **First 3 experiments**:
  1. Reproduce the filtering baseline on a single D4RL environment (e.g., Walker2d-Medium) with x=10%, comparing FBC vs. BC vs. DT to validate the paper's claim
  2. Ablate the filtering threshold (x ∈ {5%, 10%, 20%, 50%}) to characterize the data-quality vs. data-quantity tradeoff for your specific task
  3. Test FBC on a dense-reward variant of the same task to identify where the sparse-reward assumption breaks and transformer-based methods may become preferable

## Open Questions the Paper Calls Out

- **Is Decision Transformer ever preferable for raw-state robotic tasks in offline reinforcement learning?**
  - The paper shows DT is not preferable for sparse-reward settings, and prior work suggests DT is also not preferable for dense-reward settings. The possibility of some unexplored domain where DT excels remains open.

- **Why does Filtered Behavior Cloning outperform Decision Transformer despite DT's architectural sophistication?**
  - Section 6 directly asks this question and offers hypotheses including overfitting and poor credit assignment without empirical validation. The discussion provides a speculative thought experiment—that RTG conditioning in sparse settings merely indicates trajectory quality—but the exact mechanisms remain unverified.

- **What architectural modifications are necessary for vanilla Decision Transformer to become competitive with simpler offline RL methods?**
  - The conclusion states the work "calls for ongoing and continuing algorithmic and architectural improvements on top of the vanilla DT to fully unleash the power of the transformer architecture." Variants like Q-value Regularized DT and Graph Decision Transformer show promise, but which specific modifications are essential remains unclear.

## Limitations
- Conclusions are primarily limited to sparse-reward robotic tasks in Robomimic and D4RL benchmarks
- Filtering threshold (10%) was not systematically ablated
- Results may not extend to dense-reward tasks, high-dimensional visual observations, or different domains like games or control tasks

## Confidence
- **High confidence**: FBC matches or exceeds DT performance on sparse-reward tasks with fewer parameters and faster training
- **Medium confidence**: DT provides no advantage over FBC for raw-state robotic tasks in offline RL
- **Medium confidence**: The transformer architecture's attention mechanism doesn't extract meaningful temporal credit assignment from binary RTG signals

## Next Checks
1. Test FBC vs DT on dense-reward variants of the same tasks to identify where transformer methods may become preferable
2. Systematically vary the filtering threshold (x% ∈ {5%, 10%, 20%, 50%}) to characterize the data-quality vs. data-quantity tradeoff
3. Evaluate FBC on image-based robotic tasks (e.g., Robomimic's visual variants) to test whether conclusions extend beyond raw-state observations