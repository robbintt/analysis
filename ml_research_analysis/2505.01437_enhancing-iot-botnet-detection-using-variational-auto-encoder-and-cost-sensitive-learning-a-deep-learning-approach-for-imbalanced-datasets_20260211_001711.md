---
ver: rpa2
title: 'Enhancing IoT-Botnet Detection using Variational Auto-encoder and Cost-Sensitive
  Learning: A Deep Learning Approach for Imbalanced Datasets'
arxiv_id: '2505.01437'
source_url: https://arxiv.org/abs/2505.01437
tags:
- class
- instances
- learning
- dataset
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed lightweight, effective deep learning models
  for detecting IoT botnet attacks using Variational Auto-encoder (VAE) and cost-sensitive
  learning. The approach addressed high dimensionality and class imbalance in IoT
  attack datasets by generating additional minority class instances in a controlled
  manner and assigning higher costs for misclassifying minority class instances.
---

# Enhancing IoT-Botnet Detection using Variational Auto-encoder and Cost-Sensitive Learning: A Deep Learning Approach for Imbalanced Datasets

## Quick Facts
- arXiv ID: 2505.01437
- Source URL: https://arxiv.org/abs/2505.01437
- Reference count: 28
- Primary result: Lightweight deep learning models improved minority class detection (Normal, Theft) F1-scores to 0.97 and 0.96 respectively on Bot-IoT dataset using VAE augmentation and cost-sensitive learning

## Executive Summary
This study presents a deep learning approach for detecting IoT botnet attacks in highly imbalanced datasets by combining Variational Auto-encoder (VAE) based controlled data augmentation, classic Auto-encoder dimension reduction, and cost-sensitive learning. The method addresses the challenge of detecting minority attack classes in IoT traffic by generating additional minority instances in a controlled manner and assigning higher misclassification costs for rare classes. Evaluated on Bot-IoT and CIC-IoT datasets, the approach significantly improved minority class recognition without degrading majority class detection, achieving F1-scores up to 0.97 for previously underrepresented classes.

## Method Summary
The approach consists of three phases: (1) VAE-based controlled augmentation of minority class instances, where separate VAEs are trained for each minority class to generate additional samples without exceeding original instance counts; (2) classic Auto-encoder dimension reduction that projects high-dimensional feature vectors to lower-dimensional latent spaces (8 dimensions for Bot-IoT, 15 for CIC-IoT); and (3) cost-sensitive learning with DNN or BLSTM classifiers, where class weights inversely proportional to class frequencies penalize minority class misclassifications more heavily. The models were trained using Adam optimizer, categorical cross-entropy loss, and dropout regularization.

## Key Results
- BLSTM model achieved F1-scores of 0.97 for "Normal" and 0.96 for "Theft" classes on Bot-IoT dataset
- Controlled VAE augmentation (65% Normal, 80% Theft) improved minority class detection without overwhelming the model with synthetic instances
- Cost-sensitive learning with class weights (Theft:800, Normal:367, DDoS:1) compensated for remaining class imbalance
- Approach demonstrated effective enhancement of minority class recognition without impacting majority class detection performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Controlled VAE-based augmentation improves minority class detection without overwhelming the model with synthetic instances
- Mechanism: VAE learns latent distribution of minority classes and generates additional samples constrained to not outnumber original observations, preventing the model from learning solely from synthetic patterns
- Core assumption: VAE trained on limited minority samples can capture sufficient high-level characteristic patterns despite reconstruction noise
- Evidence anchors: Abstract mentions "controlled manner" of generation; Section III-C explains reconstruction error accumulation and need for generation control
- Break condition: If minority class has <50 samples, VAE reconstruction error may exceed meaningful pattern capture

### Mechanism 2
- Claim: Cost-sensitive learning compensates for remaining class imbalance after augmentation
- Mechanism: Class weight matrix modifies loss function so misclassifying weighted minority instances contributes more to total loss, forcing discriminative feature learning for rare classes
- Core assumption: Iterative weight tuning converges to values that improve minority recall without causing majority class degradation
- Evidence anchors: Abstract mentions "assigning higher costs"; Section III-D describes cost-sensitive learning principle
- Break condition: If weights too high, model overfits to minority classes (precision drops); if too low, recall gains negligible

### Mechanism 3
- Claim: Auto-encoder dimension reduction preserves discriminative information while producing lightweight representations
- Mechanism: Classic AE trained on full dataset projects high-dimensional features (28-84 dimensions) to low-dimensional latent vectors (8-15 dimensions) for classifier training
- Core assumption: Compressed latent space retains features necessary for distinguishing all traffic categories including minority classes
- Evidence anchors: Abstract mentions addressing high dimensionality; Section III-E describes AE dimension reduction to 8 and 15 dimensions
- Break condition: If latent dimension too small, inter-class separation collapses; if too large, lightweight deployment benefit lost

## Foundational Learning

- Concept: Variational Auto-encoder (VAE) - encoder maps input to latent distribution parameters, decoder samples from this distribution to reconstruct input. Why needed: VAE is the data augmentation engine; understanding ELBO optimization is essential for tuning generation quality. Quick check: Can you explain why a VAE's latent space is more suitable for generation than a standard Auto-encoder's latent space?

- Concept: Cost-sensitive learning - modifies loss function by applying class-specific weights so minority class errors contribute more to optimization objective. Why needed: Data augmentation alone does not eliminate imbalance; cost-sensitive learning provides learning bias toward minority classes. Quick check: If you set minority class weight to 100 and majority to 1, what failure mode would you expect?

- Concept: Class imbalance evaluation metrics - accuracy is misleading; precision, recall, and F1-score per class reveal true minority class performance. Why needed: Paper shows 98%+ accuracy while minority F1-scores were initially near zero; proper metric selection is critical. Quick check: Why does high accuracy on an imbalanced dataset not imply good minority class detection?

## Architecture Onboarding

- Component map: VAE (Conv1D encoder-decoder) → generates synthetic minority instances → Classic AE (encoder only) → projects all data to latent space → Classifier (DNN/BLSTM) + cost-sensitive weights
- Critical path: Minority class sampling → VAE training per minority class → generate controlled synthetic instances → merge with original data → AE dimension reduction → apply class weights → train classifier → evaluate per-class metrics
- Design tradeoffs: Synthetic ratio (more samples increase diversity but raise reconstruction error risk); latent dimension (lower dimensions reduce model size but may lose discriminative features); class weights (higher weights improve minority recall but risk precision loss)
- Failure signatures: Minority precision near 0 with high recall (model over-predicts minority class); minority recall near 0 with high precision (model rarely predicts minority class); accuracy high but minority F1 near 0 (classic imbalance artifact)
- First 3 experiments: 1) Baseline: Train DNN on original imbalanced data without augmentation or cost weights; 2) Augmentation-only: Add VAE-generated minority instances but no cost weights; 3) Full approach: Combine VAE augmentation + AE dimension reduction + tuned class weights

## Open Questions the Paper Calls Out

- Open Question 1: How does the approach perform in terms of latency and memory footprint when deployed on actual resource-constrained IoT edge hardware? The paper claims "lightweight" models but provides no empirical evidence regarding computational overhead on target hardware.

- Open Question 2: Can the iterative manual tuning of class weights and synthetic instance ratios be replaced by automated, theoretically grounded optimization? The current methodology relies on subjective manual adjustments rather than algorithmic determination of cost-sensitivity parameters.

- Open Question 3: To what extent does reconstruction error associated with training VAEs on extremely scarce data impact fidelity of generated synthetic instances? While authors propose "controlled manner" generation, they do not quantify statistical similarity between real and synthetic minority instances.

## Limitations
- DNN and BLSTM layer dimensions (neurons per layer) are not specified, making exact architecture reproduction impossible
- VAE and AE architectural details (encoder/decoder depth, latent distribution parameters) are unspecified
- Training hyperparameters (learning rate, batch size, epochs, early stopping) are not provided
- Class weight tuning methodology lacks detailed thresholds for convergence
- Only two datasets evaluated; generalizability to other IoT traffic remains unknown

## Confidence

- High confidence: VAE-based controlled augmentation mechanism (well-established approach with clear mathematical foundation)
- Medium confidence: AE dimension reduction effectiveness (core concept supported, but specific dimension choices not validated)
- Medium confidence: Cost-sensitive learning contribution (mechanism understood but tuning process insufficiently detailed)
- Low confidence: Exact architectural specifications and hyperparameter values needed for faithful reproduction

## Next Checks

1. Reproduce baseline results: Train DNN on original imbalanced Bot-IoT data without augmentation or cost weights to establish initial minority class failure mode (expect near-zero F1 for Normal and Theft)

2. Validate augmentation impact: Implement VAE generation at specified ratios (65% Normal, 80% Theft) and measure per-class F1-score improvements

3. Test weight tuning stability: Systematically vary class weights around specified values and measure precision-recall tradeoff to identify optimal operating point