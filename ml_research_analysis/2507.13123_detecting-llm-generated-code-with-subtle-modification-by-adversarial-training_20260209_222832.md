---
ver: rpa2
title: Detecting LLM-generated Code with Subtle Modification by Adversarial Training
arxiv_id: '2507.13123'
source_url: https://arxiv.org/abs/2507.13123
tags:
- adversarial
- code
- codegptsensor
- samples
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CodeGPTSensor+, an enhanced LLM-generated code
  detection model that uses adversarial training to improve robustness against minor
  code modifications. It introduces a Multi-objective Identifier and Structure Transformation
  (MIST) module that generates high-quality adversarial samples by combining identifier
  replacement and code structure transformation strategies, while balancing attack
  success rate, semantic consistency, and perturbation magnitude through a multi-objective
  optimization framework.
---

# Detecting LLM-generated Code with Subtle Modification by Adversarial Training

## Quick Facts
- **arXiv ID:** 2507.13123
- **Source URL:** https://arxiv.org/abs/2507.13123
- **Reference count:** 40
- **One-line primary result:** CodeGPTSensor+ achieves 0.970/0.992 accuracy on Java/Python tests and 190.8%/272.8% improvement on adversarial sets.

## Executive Summary
CodeGPTSensor+ is an enhanced LLM-generated code detection model that addresses robustness against minor code modifications through adversarial training. The model integrates a Multi-objective Identifier and Structure Transformation (MIST) module that generates high-quality adversarial samples by combining identifier replacement and code structure transformation strategies. By balancing attack success rate, semantic consistency, and perturbation magnitude through a multi-objective optimization framework, the model achieves state-of-the-art detection accuracy while significantly improving resilience to adversarial attacks.

## Method Summary
The method combines a base RoBERTa-based CodeGPTSensor classifier with an adversarial training framework. MIST uses Tree-sitter for parsing and a genetic algorithm (NSGA-II) to optimize three objectives: attack success rate, semantic distance, and edit distance. The system generates adversarial samples by replacing identifiers and transforming code structures, then fine-tunes the base model on a 70/30 mix of original and adversarial samples. The approach is validated on the HMCorp dataset with Java and Python code samples.

## Key Results
- Detection accuracy of 0.970 on Java test sets and 0.992 on Python test sets
- Adversarial accuracy improvements of 190.8% on Java and 272.8% on Python test sets
- MIST outperforms baseline adversarial attack algorithms in both attack success rate and sample quality
- CodeGPTSensor+ achieves 0.968/0.982 adversarial accuracy on Java/Python test sets

## Why This Works (Mechanism)

### Mechanism 1
Adversarial training improves robustness by forcing the model to rely on invariant features rather than superficial syntactic patterns. The MIST module generates adversarial samples that preserve semantics but alter syntax (e.g., renaming variables, swapping loop structures). By mixing these "challenging" samples (30%) with original samples (70%) during fine-tuning, the model learns to ignore brittle features like specific variable names and focuses on deeper code logic, effectively "inoculating" the detector against obfuscation.

### Mechanism 2
Multi-objective optimization ensures adversarial samples are effective for training by balancing attack efficacy with semantic fidelity. MIST uses NSGA-II to optimize three conflicting objectives: minimizing Adversarial Loss (fooling the model), minimizing Semantic Distance (keeping meaning intact), and minimizing Edit Distance (keeping changes subtle). This ensures the generated samples are "hard" for the model to classify but still valid code, preventing the training process from collapsing due to nonsensical inputs.

### Mechanism 3
Style-aware probabilistic transformation enhances the "naturalness" of adversarial samples, making them better proxies for human-edited code. Instead of random transformations, MIST calculates the frequency of code structures in human-written vs. LLM-generated datasets. It then applies transformations probabilistically to align the LLM-generated code's style closer to human patterns, effectively teaching the detector to recognize LLM logic even when "dressed" in human-like syntax.

## Foundational Learning

**Adversarial Training in Discrete Domains**: Unlike images where pixels can be nudged continuously, code is discrete. You must understand how to generate valid, compilable code variants that preserve logic to train the model effectively. Quick check: Can you explain why adding random characters to code fails as an adversarial strategy compared to renaming a variable to a context-aware synonym?

**NSGA-II (Non-dominated Sorting Genetic Algorithm)**: The MIST module relies on this to solve the multi-objective problem (Attack vs. Semantic vs. Edit distance). Understanding "dominance" is key to tuning the generator. Quick check: If Sample A has higher attack success than Sample B, but Sample B has better semantic consistency, which one does NSGA-II prefer?

**Code Structure vs. Semantic Equivalence**: The core premise relies on transforming code structure (syntax) without altering function (semantics). Quick check: Does converting a standard `for` loop to a `while` loop change the program's output? Why is this safe for adversarial training?

## Architecture Onboarding

**Component map:** HMCorp Dataset -> MIST Module -> Augmented Training Set -> CodeGPTSensor+ (RoBERTa-based Detector)

**Critical path:**
1. **Parsing:** Raw code is parsed to identify identifiers and structures using Tree-sitter
2. **Optimization:** MIST applies mutations (renaming/structure swap) guided by the multi-objective fitness function using NSGA-II
3. **Selection:** Best candidates (non-dominated solutions) are selected to form the adversarial batch
4. **Fine-Tuning:** The base detector is retrained on the mix of original and adversarial samples

**Design tradeoffs:**
- **ASR vs. Sample Quality:** Prioritizing high attack success rates might lower semantic consistency (making bad training data)
- **Training Ratio:** The 70/30 split (original/adversarial) is a stability tradeoff; more adversarial data increases robustness but risks "catastrophic forgetting" of original patterns
- **Query Efficiency:** MIST uses genetic algorithms which are query-intensive; this is feasible for offline training but costly for real-time generation

**Failure signatures:**
- **Syntactic Collapse:** Generated adversarial samples fail to compile/parse (MIST validation failure)
- **Overfitting to MIST:** The detector learns to detect MIST-specific artifacts rather than generalizing to LLM features
- **Semantic Drift:** The optimizer prioritizes structure changes that accidentally alter logic (e.g., off-by-one errors in loop conversion)

**First 3 experiments:**
1. **Unit Test the MIST Generator:** Input a simple Java function and run MIST. Verify that the output compiles and passes unit tests (validating semantic consistency)
2. **Ablation on Objectives:** Retrain the model using only Attack Success Rate (ASR) as the objective (ignoring semantic/edit distance). Measure the drop in performance on the original test set to justify the multi-objective approach
3. **Cross-Attack Generalization:** Train CodeGPTSensor+ using MIST samples, then test it against samples generated by a baseline method (e.g., ALERT or MOAA) to verify if robustness transfers

## Open Questions the Paper Calls Out

**Generalization to Newer LLMs:** Can CodeGPTSensor+ maintain detection robustness against code generated by newer Large Language Models (e.g., GPT-4, Claude) that possess different generation capabilities and stylistic signatures than the gpt-3.5-turbo model used for training? The model's adversarial training is grounded in the artifacts and stylistic tendencies of gpt-3.5-turbo, and future versions may exhibit different code generation capabilities and styles.

**Language Support Beyond Java/Python:** How effective is the MIST module and CodeGPTSensor+ when applied to programming languages with syntax and structural paradigms significantly different from Java and Python? The analysis is limited to these two languages, excluding other widely used programming languages.

**Complex Code Modifications:** To what extent does CodeGPTSensor+ withstand complex code modifications such as function splitting or the introduction of third-party libraries, which lie outside the current perturbation strategies? The MIST module currently combines identifier substitution and structural transformation but may not handle architectural refactoring or dependency changes.

## Limitations

- **Dataset Dependency:** The effectiveness of MIST is tightly coupled to the quality and representativeness of the HMCorp dataset, which may not reflect real-world diversity
- **Generalization Gap:** The paper validates against a held-out adversarial set generated by MIST but does not extensively test cross-algorithm robustness, leaving open the possibility of overfitting to MIST-specific artifacts
- **Semantic Consistency Verification:** While semantic consistency is claimed, the actual semantic distance metrics and how they prevent logic-breaking mutations are not fully detailed

## Confidence

- **High Confidence:** The core architectural claims (RoBERTa-based detector + Tree-sitter + NSGA-II) are well-specified and reproducible. The reported accuracy improvements on HMCorp test sets (0.970/0.992) are directly measurable
- **Medium Confidence:** The adversarial training framework is theoretically sound, but its robustness depends on the fidelity of MIST-generated samples. Without cross-attack validation, we cannot be certain the model generalizes beyond MIST-specific patterns
- **Low Confidence:** The long-term efficacy of this approach against adaptive attackers who may learn to evade MIST-style transformations remains unproven

## Next Checks

1. **Cross-Attack Generalization Test:** Retrain CodeGPTSensor+ using MIST samples, then evaluate it against adversarial samples generated by a baseline method (e.g., ALERT or MOAA). This will verify if the robustness transfers beyond MIST-specific patterns
2. **Semantic Drift Analysis:** Instrument MIST to log semantic distance metrics for each generated sample. Analyze whether high-attack-success samples maintain semantic equivalence by running unit tests on transformed code
3. **Dataset Ablation Study:** Retrain the model using MIST on a subset of HMCorp with artificially reduced stylistic diversity (e.g., only one coding style). Measure the drop in adversarial accuracy to quantify the importance of style-aware transformations