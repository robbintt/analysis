---
ver: rpa2
title: Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic
arxiv_id: '2601.21972'
source_url: https://arxiv.org/abs/2601.21972
tags:
- arxiv
- agents
- multi-agent
- learning
- collaboration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops Multi-Agent Actor-Critic (MAAC) methods for
  optimizing decentralized LLM collaboration. The authors propose two approaches:
  CoLLM-CC with a centralized critic and CoLLM-DC with decentralized critics.'
---

# Learning Decentralized LLM Collaboration with Multi-Agent Actor-Critic

## Quick Facts
- arXiv ID: 2601.21972
- Source URL: https://arxiv.org/abs/2601.21972
- Authors: Shuo Liu; Tianle Chen; Ryan Amiri; Christopher Amato
- Reference count: 40
- Primary result: Multi-Agent Actor-Critic with centralized critic (CoLLM-CC) outperforms Monte Carlo and decentralized critic (CoLLM-DC) approaches in long-horizon and sparse-reward settings

## Executive Summary
This paper introduces Multi-Agent Actor-Critic (MAAC) methods for optimizing decentralized Large Language Model (LLM) collaboration. The authors propose two approaches: CoLLM-CC with a centralized critic and CoLLM-DC with decentralized critics. Their experiments across writing, coding, and game-playing domains demonstrate that while Monte Carlo methods and CoLLM-DC achieve comparable performance to CoLLM-CC in short-horizon and dense-reward settings, they underperform CoLLM-CC on long-horizon or sparse-reward tasks. The centralized critic approach shows superior sample efficiency and lower variance in gradient estimates, particularly in challenging scenarios with sparse rewards.

## Method Summary
The authors extend Multi-Agent Actor-Critic (MAAC) frameworks to decentralized LLM collaboration by developing two variants: CoLLM-CC (Centralized Critic) and CoLLM-DC (Decentralized Critics). The centralized critic approach uses a single critic to evaluate all agent actions, while the decentralized version employs separate critics for each agent. The framework trains agents to collaborate effectively by optimizing their policies based on critic feedback, with the centralized approach showing advantages in sample efficiency and variance reduction compared to both decentralized critics and Monte Carlo methods.

## Key Results
- CoLLM-CC outperforms both Monte Carlo methods and CoLLM-DC in long-horizon and sparse-reward tasks
- Monte Carlo methods require substantially more samples to train effectively than CoLLM-CC
- CoLLM-DC struggles to converge in sparse-reward settings despite performing comparably to CoLLM-CC in short-horizon, dense-reward scenarios
- The centralized critic approach demonstrates higher sample efficiency and lower variance in gradient estimates

## Why This Works (Mechanism)
The centralized critic provides global state information and coordinated feedback that enables more efficient learning across agents. By having access to all agent actions and states, the centralized critic can provide more informative value estimates and reduce the variance in policy gradient updates. This is particularly beneficial in sparse-reward and long-horizon settings where local critics struggle to provide meaningful credit assignment. The centralized approach also enables better coordination signals between agents, leading to more effective collaboration strategies.

## Foundational Learning
- Multi-Agent Actor-Critic (MAAC): Framework for training multiple agents simultaneously using actor-critic methods; needed for decentralized LLM collaboration as it enables coordinated policy optimization
- Centralized vs. Decentralized Critics: Architectural choice affecting information flow and credit assignment; critical for understanding performance differences between CoLLM-CC and CoLLM-DC
- Sparse vs. Dense Rewards: Reward structure affecting learning difficulty; essential for interpreting why centralized critics outperform in sparse-reward settings
- Sample Efficiency: Measure of learning speed; key metric for comparing different training approaches
- Variance in Gradient Estimates: Statistical property affecting training stability; important for understanding why centralized critics provide more stable learning

## Architecture Onboarding

Component Map:
LLM Agents -> Policy Networks -> Centralized/Decentralized Critics -> Value Estimates -> Policy Updates

Critical Path:
Observation -> Agent Policy -> Action -> Environment Response -> Critic Evaluation -> Policy Gradient -> Updated Policy

Design Tradeoffs:
Centralized critics provide better coordination and sample efficiency but create potential bottlenecks and single points of failure. Decentralized critics offer better scalability and fault tolerance but struggle with credit assignment in sparse-reward settings. The choice between them depends on task characteristics and system scale requirements.

Failure Signatures:
- CoLLM-DC convergence issues manifest as unstable learning curves and poor performance in sparse-reward tasks
- Monte Carlo methods show slow learning progress and require excessive samples
- Centralized critic bottlenecks appear as increased latency in larger agent populations

Three First Experiments:
1. Compare learning curves of CoLLM-CC vs CoLLM-DC in a simple multi-agent coordination task with sparse rewards
2. Measure sample efficiency by tracking performance improvement per training episode across all three methods
3. Test scalability by gradually increasing agent count and measuring performance degradation in CoLLM-CC

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Centralized critic approach may face scalability issues in larger multi-agent systems due to computational bottlenecks
- Results are limited to specific domains (writing, coding, game-playing) and may not generalize to other multi-agent scenarios
- Convergence analysis for CoLLM-DC lacks rigor compared to CoLLM-CC
- Implementation details for Monte Carlo comparison are not fully specified

## Confidence
- High confidence in the relative performance ordering (CoLLM-CC > CoLLM-DC â‰ˆ Monte Carlo in long-horizon/sparse settings)
- Medium confidence in sample efficiency advantages due to missing implementation details
- Medium confidence in technical correctness of MAAC framework extension

## Next Checks
1. Scale experiments to larger agent populations (10+ agents) to test centralized critic bottlenecks and persistent CoLLM-DC convergence issues
2. Implement ablation studies removing specific components of CoLLM-CC to isolate performance drivers
3. Test across diverse multi-agent domains beyond writing/coding/games, particularly with heterogeneous agent capabilities