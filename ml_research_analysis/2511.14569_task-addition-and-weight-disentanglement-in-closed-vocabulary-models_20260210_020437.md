---
ver: rpa2
title: Task Addition and Weight Disentanglement in Closed-Vocabulary Models
arxiv_id: '2511.14569'
source_url: https://arxiv.org/abs/2511.14569
tags:
- task
- addition
- weight
- disentanglement
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the applicability of task arithmetic, a
  technique for editing pre-trained models, to closed-vocabulary models. Unlike open-vocabulary
  models like CLIP, closed-vocabulary models lack language supervision and require
  task-specific classification heads.
---

# Task Addition and Weight Disentanglement in Closed-Vocabulary Models

## Quick Facts
- arXiv ID: 2511.14569
- Source URL: https://arxiv.org/abs/2511.14569
- Reference count: 40
- Primary result: Task arithmetic can be extended to closed-vocabulary models with weight disentanglement emerging across various pre-training schemes

## Executive Summary
This paper investigates the applicability of task arithmetic, a technique for editing pre-trained models, to closed-vocabulary models. Unlike open-vocabulary models like CLIP, closed-vocabulary models lack language supervision and require task-specific classification heads. The authors introduce a two-stage fine-tuning process: linear probing followed by encoder fine-tuning, and apply task arithmetic to various pre-training schemes including supervised, self-supervised (MAE, DINO), and contrastive (CLIP) methods. They find that weight disentanglement, crucial for task arithmetic, is a general property of pre-training, appearing across different closed-vocabulary models. Task addition performance is high across most pre-training schemes, with CLIP models achieving the best results. However, linear probing alone achieves competitive performance to task addition, making it a cost-effective alternative.

## Method Summary
The authors adapt task arithmetic for closed-vocabulary models by introducing a two-stage fine-tuning process. First, they perform linear probing where only the classification head is trained while the encoder remains frozen. Then they conduct encoder fine-tuning where both components are trained together. This approach addresses the lack of language supervision in closed-vocabulary models. The method is tested across multiple pre-training schemes: supervised learning, self-supervised methods (MAE, DINO), and contrastive learning (CLIP). Task arithmetic is performed by computing differences between fine-tuned models and applying these differences to the original pre-trained models to add new tasks. The study evaluates performance on standard image classification datasets including CIFAR-10, CIFAR-100, and Tiny ImageNet.

## Key Results
- Weight disentanglement emerges consistently across supervised, self-supervised (MAE, DINO), and contrastive (CLIP) pre-training methods
- CLIP-based models achieve the highest task addition performance, followed by supervised models
- Linear probing alone achieves competitive performance to the more expensive task addition approach
- MAE and DINO models show task addition capabilities, though with slightly lower performance than CLIP and supervised models

## Why This Works (Mechanism)
The success of task arithmetic in closed-vocabulary models stems from the emergence of weight disentanglement during pre-training. This property allows task-specific parameters to be added or subtracted without interfering with the model's general knowledge. The two-stage fine-tuning process is critical because it first establishes task-specific representations through linear probing, then refines these representations during encoder fine-tuning. This process creates well-defined parameter subspaces for each task that can be manipulated through arithmetic operations. The closed-vocabulary setting requires this more complex approach compared to open-vocabulary models like CLIP, which can leverage shared language representations across tasks.

## Foundational Learning

1. **Task Arithmetic**: A technique for editing pre-trained models by adding or removing task-specific parameters through arithmetic operations. Needed to enable efficient multi-task learning without catastrophic forgetting. Quick check: Can task parameters be reliably computed as differences between fine-tuned models?

2. **Weight Disentanglement**: The property where task-specific parameters become independent from general knowledge during pre-training. Critical for task arithmetic to work without interference. Quick check: Do parameter differences remain stable when models are fine-tuned on similar tasks?

3. **Linear Probing**: A training approach where only the classification head is trained while the encoder remains frozen. Provides a computationally efficient baseline for task learning. Quick check: Does linear probing performance correlate with downstream task addition success?

4. **Closed-vocabulary vs Open-vocabulary Models**: Closed-vocabulary models require task-specific classification heads, while open-vocabulary models like CLIP use shared language representations. Important distinction that affects how task arithmetic is implemented. Quick check: Can the same task arithmetic approach be applied to both model types?

5. **Two-stage Fine-tuning**: The process of first linear probing then encoder fine-tuning. Creates well-defined parameter subspaces for each task. Quick check: Does the order of fine-tuning stages affect final performance?

6. **Pre-training Scheme Impact**: Different pre-training methods (supervised, MAE, DINO, CLIP) affect the quality and disentanglement of learned representations. Determines the effectiveness of subsequent task addition. Quick check: Which pre-training scheme provides the most disentangled representations?

## Architecture Onboarding

Component Map: Pre-trained Encoder -> Classification Head -> Fine-tuning Stage 1 (Linear Probing) -> Fine-tuning Stage 2 (Encoder Fine-tuning) -> Task Arithmetic Operation

Critical Path: The fine-tuning stages are critical as they establish the parameter subspaces that enable task arithmetic. The quality of weight disentanglement during these stages directly determines the success of task addition.

Design Tradeoffs: Linear probing offers computational efficiency but may limit performance compared to full fine-tuning. The two-stage approach increases computational cost but provides better task addition capabilities. CLIP's language supervision provides superior performance but limits applicability to closed-vocabulary settings.

Failure Signatures: Poor weight disentanglement manifests as degraded performance when adding multiple tasks or when tasks interfere with each other. Inconsistent task addition results across different pre-training schemes indicate issues with the fine-tuning process.

Three First Experiments:
1. Test linear probing performance across different pre-training schemes to establish baseline task learning capabilities
2. Evaluate task addition for single tasks across all pre-training methods to compare effectiveness
3. Assess multi-task addition performance to test scalability and interference between tasks

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Results primarily based on ResNet-50 architectures, limiting generalization to other backbone architectures
- Evaluation focuses on relatively simple classification tasks, with unexplored performance on more complex downstream tasks
- Two-stage fine-tuning process increases computational costs compared to linear probing alone
- Long-term stability and forgetting effects from multiple consecutive task additions remain unexplored

## Confidence

**Major Claims and Confidence Levels:**

1. **Weight disentanglement is a general property across different pre-training schemes** (High confidence): The consistent emergence of weight disentanglement across supervised, self-supervised (MAE, DINO), and contrastive (CLIP) pre-training methods is well-supported by experimental results.

2. **Task arithmetic performance varies significantly by pre-training method** (Medium confidence): While CLIP models show superior task addition performance, the relative performance of other pre-training schemes shows variability that warrants further investigation with larger sample sizes and diverse tasks.

3. **Linear probing is a cost-effective alternative to task addition** (High confidence): The comparable performance of linear probing to task addition is clearly demonstrated, though the study does not explore scenarios where task addition might still be preferable.

## Next Checks

1. Test task arithmetic scalability and effectiveness on larger backbone architectures (e.g., Vision Transformers) and more diverse downstream tasks beyond simple classification.

2. Conduct ablation studies to quantify the contribution of each stage in the two-stage fine-tuning process and identify opportunities for optimization or simplification.

3. Evaluate the long-term stability and potential forgetting effects when performing multiple consecutive task additions, particularly for tasks added early in the sequence.