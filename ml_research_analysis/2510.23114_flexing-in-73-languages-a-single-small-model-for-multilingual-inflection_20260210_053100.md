---
ver: rpa2
title: 'Flexing in 73 Languages: A Single Small Model for Multilingual Inflection'
arxiv_id: '2510.23114'
source_url: https://arxiv.org/abs/2510.23114
tags:
- https
- inflection
- computational
- sigmorphon
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the absence of an open-source, lightweight,
  multilingual morphological inflection system for unseen words across diverse languages.
  The authors propose a single Transformer-based encoder-decoder model with 5.69M
  parameters, trained jointly on 73 languages using character tokenization and a language
  ID token.
---

# Flexing in 73 Languages: A Single Small Model for Multilingual Inflection

## Quick Facts
- arXiv ID: 2510.23114
- Source URL: https://arxiv.org/abs/2510.23114
- Authors: Tomáš Sourada; Jana Straková
- Reference count: 40
- Key outcome: A 5.69M-parameter multilingual model achieves 81.36% average accuracy across 73 languages, outperforming monolingual baselines and ranking 1st/3rd on SIGMORPHON benchmarks.

## Executive Summary
This paper presents a lightweight, open-source Transformer-based system for multilingual morphological inflection that works across 73 languages. The model uses character-level tokenization and a language ID token to disambiguate lemmas that appear in multiple languages. Trained jointly from scratch on UD treebanks, it achieves strong generalization to unseen lemmas and outperforms monolingual baselines in most languages. The work fills a gap by providing a single, small model suitable for low-resource settings.

## Method Summary
The authors train a 5.69M-parameter encoder-decoder Transformer from scratch using character tokenization and a language ID token. The model is trained jointly on 73 languages using frequency-weighted, lemma-disjoint train-dev-test splits from Universal Dependencies treebanks. To prevent high-resource languages from dominating training, they apply temperature-based corpus upsampling (τ=0.5). The input format includes a language ID token followed by morphological tags and the lemma, with the output being the inflected form. The model achieves competitive results on SIGMORPHON 2022/2023 benchmarks and outperforms monolingual baselines on average across all 73 languages.

## Key Results
- Achieves 81.36% average accuracy across 73 languages on UD treebanks
- Outperforms monolingual baselines by 4.69% on macro-average accuracy
- Ranks 1st globally on SIGMORPHON 2023 and 3rd on SIGMORPHON 2022 benchmarks
- Single 5.69M-parameter model vs. 73 separate monolingual models

## Why This Works (Mechanism)

### Mechanism 1: Cross-lingual morphological transfer
Joint training on 73 languages enables positive transfer that improves inflection accuracy over monolingual models. Languages share morphological patterns across families; the shared encoder-decoder learns generalizable transformation rules that transfer to lower-resource languages in the mix. Core assumption: Morphological operations contain learnable commonalities across typologically diverse languages.

### Mechanism 2: Language ID conditioning for lemma disambiguation
Prepending a language ID token enables the model to disambiguate identical lemma strings that have different inflection behaviors across languages. The language ID becomes part of the input representation, allowing attention mechanisms to route information through language-specific subspaces within shared parameters. Core assumption: Identical character sequences can require different transformations depending on language context.

### Mechanism 3: Temperature-scaled corpus upsampling
Upsampling with temperature smoothing (τ=0.5) prevents high-resource languages from dominating training while maintaining data efficiency. Temperature controls the effective distribution over corpora; τ=0.5 partially flattens the natural size imbalance, giving lower-resource languages more representation per epoch than naive pooling would. Core assumption: A balanced training signal across languages improves average multilingual performance.

## Foundational Learning

- Concept: Encoder-decoder Transformer architectures
  - Why needed here: The model is a sequence-to-sequence Transformer; understanding cross-attention, positional encodings, and autoregressive decoding is essential.
  - Quick check question: Can you explain how the decoder attends to encoder outputs during inference?

- Concept: Morphological inflection as a structured transduction task
  - Why needed here: The task maps (lemma, tags) → inflected form; this differs from classification and requires understanding of string-to-string generation.
  - Quick check question: Given lemma "run" and tags `V;PST`, what should the output be?

- Concept: Lemma-disjoint evaluation
  - Why needed here: All test lemmas are unseen during training; this measures generalization, not memorization.
  - Quick check question: Why would evaluating on lemmas seen during training inflate reported performance?

## Architecture Onboarding

- Component map: Language ID token -> morphological tags + lemma chars -> 4-layer Transformer encoder -> decoder with cross-attention -> character-level autoregressive generation

- Critical path:
  1. Data preprocessing extracts lemma-tag-form triples from UD treebanks
  2. Frequency-weighted, lemma-disjoint split creates train/dev/test
  3. Upsampling with τ=0.5 balances corpora during batch construction
  4. Model trains from scratch (no pretrained weights)
  5. Checkpoint selection uses dev set (macro-averaged across languages for multi)

- Design tradeoffs:
  - Model capacity: 5.69M parameters keeps deployment lightweight but may underfit very high-resource languages
  - Character-level tokenization: Handles OOV robustly but increases sequence length
  - Single multi model vs. 73 monolingual models: Simplifies deployment; sacrifices per-language optimization

- Failure signatures:
  - Low accuracy on languages with complex non-concatenative morphology (e.g., Semitic root-pattern systems) without sufficient training data
  - Copy-baseline-like behavior: model outputs lemma unchanged → may indicate insufficient training or tag encoding issues
  - High variance across seeds for low-resource languages → suggests data scarcity sensitivity

- First 3 experiments:
  1. Reproduce SIGMORPHON 2023 benchmark results (Table 2) to validate training pipeline
  2. Ablate language ID token: remove it and compare accuracy on a subset of languages with overlapping lemma strings
  3. Test temperature values {0.0, 0.3, 0.5, 0.7, 1.0} on a held-out language pair to observe high/low-resource tradeoffs

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on concatenative morphological systems, lacking systematic coverage of non-concatenative morphologies like templatic systems
- Key design decisions (temperature parameter, model capacity) lack empirical ablation studies to justify their specific values
- Frequency-weighted, lemma-disjoint resampling procedure is described conceptually but lacks precise algorithmic specification for exact reproduction

## Confidence
- High Confidence: Core architectural contribution and training methodology are well-specified and reproducible; SIGMORPHON benchmark results are externally verifiable
- Medium Confidence: Claim that multilingual training outperforms monolingual baselines is supported but lacks statistical significance testing across languages
- Low Confidence: Assertion of comprehensive solution for "diverse languages" is overstated given morphological typology coverage gaps

## Next Checks
1. Evaluate the model on languages representing different morphological typologies (concatenative, templatic, isolating, polysynthetic) to identify systematic failure patterns
2. Systematically vary the temperature parameter τ across {0.0, 0.3, 0.5, 0.7, 1.0} on a subset of languages to quantify impact on high-resource vs. low-resource performance
3. Create controlled experiments using language pairs with overlapping lemma strings but different inflection patterns to measure the specific contribution of language ID conditioning to disambiguation accuracy