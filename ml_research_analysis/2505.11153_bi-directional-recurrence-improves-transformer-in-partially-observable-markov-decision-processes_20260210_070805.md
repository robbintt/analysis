---
ver: rpa2
title: Bi-directional Recurrence Improves Transformer in Partially Observable Markov
  Decision Processes
arxiv_id: '2505.11153'
source_url: https://arxiv.org/abs/2505.11153
tags:
- learning
- reinforcement
- memory
- environments
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel bi-recurrent model architecture for
  reinforcement learning in partially observable environments, replacing multiple
  feed-forward layers with a single bi-directional GRU layer. Experiments on 23 POMDP
  environments show that the proposed DBGFQN model outperforms existing transformer-based,
  attention-based, and recurrence-based methods by margins ranging from 87.39% to
  482.04% on average, while reducing overall parameter count by 25%.
---

# Bi-directional Recurrence Improves Transformer in Partially Observable Markov Decision Processes

## Quick Facts
- arXiv ID: 2505.11153
- Source URL: https://arxiv.org/abs/2505.11153
- Reference count: 40
- Primary result: DBGFQN outperforms transformer and recurrence baselines by 87.39%–482.04% on 23 POMDP environments while reducing parameters by 25%

## Executive Summary
This paper introduces DBGFQN, a novel transformer architecture for reinforcement learning in partially observable environments that replaces multiple feed-forward layers with a single bi-directional GRU layer. The model combines multi-head self-attention with bidirectional recurrence to better capture sequential dependencies and contextual information in POMDPs. Experiments across 23 environments show DBGFQN outperforms existing transformer-based, attention-based, and recurrence-based methods by substantial margins while maintaining parameter efficiency. The bidirectional GRU improves handling of partial observability and increases sample efficiency, enabling effective learning from fewer interactions.

## Method Summary
DBGFQN uses an encoder-decoder architecture where the encoder consists of two layers of multi-head self-attention followed by a single bi-directional GRU layer, replacing the multiple feed-forward networks typical in transformer architectures. The model processes observation sequences with positional encoding, applies attention mechanisms to capture long-range dependencies, then refines representations with bidirectional temporal processing. Trained via DQN-style updates with experience replay (500K buffer, batch size 32, learning rate 0.0003) for 2M timesteps. The bidirectional GRU processes sequences in both forward and backward directions, enabling each timestep to incorporate future context during training.

## Key Results
- DBGFQN achieves 87.39%–482.04% performance improvement over baselines across 23 POMDP environments
- Reduces parameter count by 25% compared to transformer-based methods
- Outperforms pure attention and pure recurrence baselines in environments with strong structural patterns
- Bi-directional recurrence shows clear advantage over uni-directional in memory-intensive tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bi-directional recurrence captures temporal dependencies that feed-forward expansion misses in structured POMDPs
- Mechanism: BiGRU processes sequences in both directions, allowing each timestep to incorporate future context during training, disambiguating aliased states
- Core assumption: Environments have conditional dependencies where future observations clarify past hidden states
- Evidence anchors: Abstract states bi-directional recurrence better captures sequential dependencies; Section 5.3 shows BiGRU outperforms uni-directional in Gridverse Memory Four Rooms 7x7

### Mechanism 2
- Claim: Self-attention provides long-range dependency modeling that complements recurrent sequential processing
- Mechanism: Multi-head self-attention identifies relevant past timesteps across the entire history, while BiGRU refines these with explicit temporal ordering
- Core assumption: Optimal decisions require integrating information from arbitrary past timesteps
- Evidence anchors: Section 1 describes MHSA capturing long-range dependencies while BiGRU processes temporal information; Equations 4-10 detail the attention mechanism

### Mechanism 3
- Claim: Hallucinated structural cues improve learning even when they don't affect transitions
- Mechanism: Artificial room boundaries provide structural landmarks that serve as reference points for building internal representations
- Core assumption: Models learn better with exploitable regularities, even if perceptual rather than functional
- Evidence anchors: Section 5.4 shows artificially introducing structure improves performance; Figure 8 demonstrates improvement in DTQN and DGFQN

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The entire architecture is designed for scenarios where agents receive incomplete observations and must maintain belief states over hidden environment states
  - Quick check question: Can you explain why a standard DQN fails when the agent cannot observe the full state, and what component must be added?

- Concept: Gated Recurrent Units (GRUs) and Bi-directional RNNs
  - Why needed here: The core architectural innovation replaces FFN with BiGRU; understanding reset/update gates and bidirectional processing is essential to debug and extend the model
  - Quick check question: What is the difference between a uni-directional GRU and a BiGRU in terms of what information each timestep can access?

- Concept: Q-Learning with Function Approximation
  - Why needed here: DBGFQN outputs Q-values and is trained via Bellman error; understanding temporal difference learning is required to implement the loss function correctly
  - Quick check question: How is the target Q-value computed during training, and why does off-policy learning require a replay buffer?

## Architecture Onboarding

- Component map: Observation sequence OT → Embedding ϕ + Positional Encoding PE → LayerNorm → Multi-Head Self-Attention → Residual Add → LayerNorm → BiGRU → Residual Add → LayerNorm → Linear Q-head
- Critical path: Observation embedding → MHSA (Eq. 4-10) → BiGRU forward/backward passes (Eq. 11-20) → Q-value output → Action selection via argmax
- Design tradeoffs:
  - Bi-directional vs. uni-directional: BiGRU provides better context but requires full sequence during training; cannot be used for true online inference without modifications
  - Context length K=50: Longer context improves memory but increases compute; paper used 50 timesteps
  - Embedding dimension 64/128: Smaller reduces parameters but may limit expressivity in complex environments
- Failure signatures:
  - Uni-directional variants underperforming DTQN suggests missing backward context in structured environments
  - Hallway/HeavenHell showing DRQN at 0.0 indicates pure recurrence without attention fails on tasks requiring long-term memory integration
  - Large grid environments (16 Rooms 21x21) showing near-zero success across all models indicates context length K=50 may be insufficient
- First 3 experiments:
  1. Ablate recurrence direction: Run DGFQN (uni-directional GRU) vs. DBGFQN on Gridverse Memory 7x7 to confirm bidirectional advantage (expected gap per Table 1: 0.1934 vs. 0.6677 success rate)
  2. Vary context length: Test K=[25, 50, 100] on GV Memory 4 Rooms 13x13 to identify where performance saturates or degrades
  3. Hallucination stress test: Add artificial structure to GV Memory 13x13 baseline and compare DTQN vs. DBGFQN improvement magnitude to validate the structural cue hypothesis from Figure 8

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DBGFQN architecture maintain its parameter efficiency and performance advantages when applied to continuous control tasks or high-dimensional environments outside of grid-based POMDPs?
- Basis in paper: Section 6 states that tested environments are "primarily grid-based and may not fully capture the complexity of real-world continuous control tasks"
- Why unresolved: Current evaluation is limited to discrete grid-worlds, leaving efficacy in continuous or high-dimensional state spaces unknown
- What evidence would resolve it: Empirical results on continuous control benchmarks (e.g., MuJoCo) or high-dimensional visual input tasks

### Open Question 2
- Question: How can the computational overhead introduced by bi-directional recurrence be mitigated during training, particularly for agents requiring very long context windows?
- Basis in paper: Section 6 identifies a limitation where "the introduction of bidirectional recurrence adds computational overhead during training, particularly for long sequences"
- Why unresolved: While the model reduces parameter count, training cost associated with sequential processing in BiGRUs may limit scalability
- What evidence would resolve it: Comparative analysis of training time and memory usage, or optimization proposal that neutralizes this overhead

### Open Question 3
- Question: Can the relationship between environment structure (conditional dependencies) and the efficacy of bi-recurrence be formalized theoretically rather than relying solely on empirical observation?
- Basis in paper: Section 6 notes that "the analysis of causality remains empirical and may not generalize to all POMDP settings"
- Why unresolved: Paper empirically demonstrates structure helps but lacks theoretical framework to predict when recurrence complexity is justified
- What evidence would resolve it: Formal study or theoretical proof characterizing specific class of POMDPs where bi-directional recurrence provides strict performance guarantee

## Limitations

- The paper relies entirely on synthetic and controlled POMDP benchmarks without validation on real-world robotics or autonomous driving scenarios
- The claim about recurrence-based approaches performing better in structured POMDPs lacks formal classification of these environment types
- The BiGRU's requirement for full context during training creates a deployment limitation for true online inference
- The 25% parameter reduction claim depends on comparing against DTQN with multiple FFN layers, but specific DTQN architecture size isn't provided

## Confidence

- **High confidence**: The empirical results showing DBGFQN outperforming DTQN and other baselines on the 23 tested environments. The architectural design is clearly specified and reproducible.
- **Medium confidence**: The mechanism claims about bidirectional recurrence improving conditional dependency handling. While supported by ablation results, the paper doesn't provide analysis of which specific environments benefit most from backward context.
- **Low confidence**: The generalization claim that recurrence-based approaches are broadly superior for structured POMDPs. The evidence comes from a curated set of grid-based environments without exploration of diverse POMDP classes or real-world applications.

## Next Checks

1. **Ablation study on context length**: Systematically test K=[25, 50, 100] on GV Memory 4 Rooms 13x13 to identify performance saturation points and verify the 50-timestep choice is optimal rather than arbitrary.

2. **Online inference capability test**: Modify the BiGRU to operate in a causal manner (no backward dependency) and evaluate performance degradation to quantify the online deployment limitation.

3. **Real-world POMDP validation**: Implement DBGFQN on a robotics navigation task with partial observability (e.g., obstacle avoidance with limited sensor range) to test claims beyond synthetic environments.