---
ver: rpa2
title: 'Beyond Plain Demos: A Demo-centric Anchoring Paradigm for In-Context Learning
  in Alzheimer''s Disease Detection'
arxiv_id: '2511.06826'
source_url: https://arxiv.org/abs/2511.06826
tags:
- task
- demo
- test
- da4icl
- injection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles Alzheimer's disease detection from speech transcripts,
  an out-of-distribution and low-resource task where conventional in-context learning
  fails due to highly homogeneous demos and insufficient fine-grained class cues.
  The proposed solution, DA4ICL, jointly broadens demo diversity via diverse and contrastive
  retrieval and deepens demo representation through projection-based vector anchoring
  at every Transformer layer, shifting from test-centric to demo-centric task signal
  integration.
---

# Beyond Plain Demos: A Demo-centric Anchoring Paradigm for In-Context Learning in Alzheimer's Disease Detection

## Quick Facts
- **arXiv ID:** 2511.06826
- **Source URL:** https://arxiv.org/abs/2511.06826
- **Reference count:** 40
- **Primary result:** F1 scores of 86.11% on Test, 86.12% on Lu, and 80.23% on Pitt datasets for Alzheimer's disease detection from speech transcripts.

## Executive Summary
This paper addresses Alzheimer's disease detection from speech transcripts, a low-resource task where standard in-context learning (ICL) fails due to semantically homogeneous demonstrations. The proposed DA4ICL framework overcomes this by (1) broadening demo diversity through Diverse and Contrastive Retrieval (DCR) and (2) deepening demo representation via Projected Vector Anchoring (PVA) at every Transformer layer. Unlike standard task-vector methods that inject signals at the test token, DA4ICL anchors task vectors at demo tokens across all layers, shifting from test-centric to demo-centric reasoning. Across three benchmark datasets, DA4ICL achieves F1 scores of 86.11% on Test, 86.12% on Lu, and 80.23% on Pitt, significantly outperforming both standard ICL and recent task-vector methods.

## Method Summary
DA4ICL jointly broadens demo diversity via Diverse and Contrastive Retrieval (DCR) and deepens demo representation through Projected Vector Anchoring (PVA) at every Transformer layer. The method retrieves demonstrations using four criteria: semantic similarity/dissimilarity and length similarity/dissimilarity, each yielding one AD and one HC pair. For each main-demo, it retrieves eight sub-demos using the same criteria. Task vectors are extracted from sub-demo sequences at all layers and projected onto main-demo hidden states using a layer-specific scaling factor (γ). This demo-centric approach preserves the original semantic direction while modulating magnitude, preventing the semantic distortion common in addition/replacement-based methods.

## Key Results
- DA4ICL achieves F1 scores of 86.11% on Test, 86.12% on Lu, and 80.23% on Pitt datasets
- PVA outperforms standard task-vector methods (addition/replacement) by 7-15% F1 points
- DCR is essential for PVA success; without DCR, standard TV methods fail to improve beyond ICL baseline
- DA4ICL shows significantly better cross-dataset generalization compared to standard ICL

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieving demonstrations based on contrastive criteria (diverse and contrastive retrieval, or DCR) expands "context width," mitigating the performance saturation caused by semantically homogeneous datasets.
- **Mechanism:** Standard semantic retrieval clusters similar examples, which fails when the task input (e.g., picture descriptions) is inherently uniform. DCR forces the model to process "dissimilar" and "structurally distinct" examples, thereby surfacing fine-grained discriminative cues that standard retrieval misses.
- **Core assumption:** The model possesses latent reasoning capabilities that can be better activated by maximizing the coverage of the demo space rather than optimizing for narrow similarity to the test sample.
- **Evidence anchors:**
  - [abstract]: "proposed solution, DA4ICL, jointly broadens demo diversity via diverse and contrastive retrieval"
  - [methodology]: "DCR module mitigates the insufficient context width caused by conventional retrieval"
  - [corpus]: General support found in "Exploring Explanations Improves the Robustness of In-Context Learning," but specific contrastive retrieval for homogeneity is distinct here.
- **Break condition:** If the training corpus is too small to provide meaningful "dissimilar" examples without becoming irrelevant, the retrieved noise may degrade the signal-to-noise ratio.

### Mechanism 2
- **Claim:** Projecting task vectors onto hidden states (Projected Vector Anchoring, or PVA) preserves the semantic direction of the original token while modulating its magnitude, preventing the semantic distortion common in addition/replacement-based TV methods.
- **Mechanism:** Unlike "Add TV" (vector addition) or "Replace TV" (vector replacement), which alter the geometric direction of the hidden state, PVA computes a scalar projection. This keeps the vector on its original semantic "manifold" while amplifying the task-specific signal.
- **Core assumption:** The semantic content of a token is encoded primarily in the *direction* of its hidden state vector, while task relevance is encoded in the *magnitude* or specific sub-spaces.
- **Evidence anchors:**
  - [methodology]: "preserves the original semantic direction... modulating only its magnitude"
  - [results]: "PVA achieves the best performance, outperforming addition, replacement"
  - [corpus]: Minimal direct support in provided corpus; appears to be a novel injection geometry distinct from standard latent steering.
- **Break condition:** If the extracted Task Vector is orthogonal to the demo's hidden state, the projection yields near-zero signal, effectively nullifying the anchor.

### Mechanism 3
- **Claim:** Anchoring signals at every Transformer layer at the *demo* tokens (rather than a single injection at the test token) distributes the task signal more robustly through the attention mechanism.
- **Mechanism:** Standard TV methods inject a single vector at the test token, risking "pseudo-sample overfitting." DA4ICL injects vectors at the demo separators (→) across all layers. This allows the model's self-attention to aggregate these reinforced cues naturally during inference, shifting from "test-centric" to "demo-centric" reasoning.
- **Core assumption:** The Transformer's attention mechanism effectively aggregates distributed signals from multiple reinforced demo tokens to the final prediction position.
- **Evidence anchors:**
  - [abstract]: "shifts from test-centric to demo-centric task signal integration"
  - [results]: Fig. 4 shows that while DCR helps ICL, it fails to help standard TV methods, implying the injection architecture (PVA) is the necessary catalyst.
  - [corpus]: "Where to show Demos in Your Prompt: A Positional Bias" supports the sensitivity of ICL to demo positioning/structure.
- **Break condition:** Excessive compute overhead; the paper notes DA4ICL takes ~521s vs ~15s for Vanilla ICL (Appendix D.4), which may be prohibitive for real-time applications.

## Foundational Learning

- **Concept: Residual Stream Formulation in Transformers**
  - **Why needed here:** The PVA mechanism relies on modifying hidden states ($h^{(\ell)}$) within the residual stream. Understanding Equation (1) ($h = h + mha + mlp$) is critical to grasping why projection is safer than addition—it prevents disrupting the recursive information flow.
  - **Quick check question:** Why does adding a large vector directly to a hidden state risk "washing out" the original semantic information in subsequent layers?

- **Concept: Task Vectors (TV) in ICL**
  - **Why needed here:** The paper positions itself as an evolution of TV methods. You must understand the standard "Extraction -> Injection" pipeline (extracting the last token's HS from a prompt with a pseudo-sample) to see why DA4ICL's demo-centric approach is a paradigm shift.
  - **Quick check question:** In standard TV methods, what is the role of the "pseudo-sample," and why does the paper argue this leads to overfitting?

- **Concept: Out-of-Distribution (OOD) & Low-Resource Adaptation**
  - **Why needed here:** The core motivation is AD detection, a domain with "extremely scarce" data and "pervasive semantic homogeneity." The architecture is specifically tuned to extract signal from data that looks identical on the surface.
  - **Quick check question:** Why does semantic similarity retrieval fail when all input samples describe the exact same scene (e.g., the "Cookie Theft" picture)?

## Architecture Onboarding

- **Component map:** DCR Retriever -> Sub-Demo Aggregator -> TV Extractor -> PVA Injector -> Frozen LLM
- **Critical path:** The system does not simply retrieve demos and prompt. It **retrieves -> extracts vectors -> projects vectors -> injects at all layers**. The heavy lifting happens *before* and *during* the forward pass of the inference prompt. If PVA is skipped, performance degrades to standard ICL levels (ablation in Table 6).
- **Design tradeoffs:**
  - **Compute vs. Stability:** DA4ICL significantly increases inference time (Appendix D.4: ~520s vs ~15s for ICL) because it requires multiple forward passes (one for each sub-demo set extraction) and modified layer-wise computation. The tradeoff is massive stability and F1 gains (80%+ vs 70%).
  - **Diversity vs. Noise:** DCR explicitly selects *dissimilar* examples. If the dataset is dirty, this imports noise.
- **Failure signatures:**
  - **Semantic Misalignment:** If $\gamma$ (scaling factor) is too high in middle layers (8-23), the projection may distort the hidden state manifold, causing the model to output hallucinated clinical labels.
  - **Retrieval Collapse:** On very small datasets (N < 50), "dissimilar" retrieval becomes effectively random, potentially neutralizing the DCR advantage.
- **First 3 experiments:**
  1. **Granularity Ablation:** Run DA4ICL using "Add TV" instead of "Projected TV" (PVA) to quantify the specific contribution of the projection geometry (compare with Table 2).
  2. **Layer Sensitivity:** Vary the injection scaling factor $\gamma$. Set $\gamma=0$ for layers 0-7 and observe the drop in performance to verify the paper's claim that "shallow-layer signals... dissipate" without anchoring.
  3. **Retrieval Stress Test:** Replace DCR with purely random retrieval but keep PVA. This isolates whether the vector anchoring or the demo selection drives the majority of the 86% F1 score.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance and relative gain of DA4ICL scale across different Large Language Model parameter sizes (e.g., 3B, 14B, 70B)?
- **Basis in paper:** [explicit] The authors state in Appendix B.1: "A full multi-scale analysis (e.g., 3B/8B/14B/70B) is left as future work with both absolute and relative gains reported."
- **Why unresolved:** The current study restricts its evaluation to a single backbone model (Llama-3.1-8B), leaving the interaction between model capacity and the proposed anchoring paradigm undefined.
- **What evidence would resolve it:** Benchmarks of DA4ICL on smaller (3B) and larger (70B) models to determine if the "demo-centric" benefits scale linearly or suffer from diminishing returns as pre-training exposure increases.

### Open Question 2
- **Question:** Can the Diverse & Contrastive Retrieval (DCR) and Projected Vector Anchoring (PVA) mechanisms be effectively adapted for multi-modal AD detection (e.g., incorporating acoustic features)?
- **Basis in paper:** [explicit] Appendix D.3 notes: "Integrating paralinguistics and multi-modal descriptors into the DCR&PVA framework and expand cross-corpus validation to additional clinical cohorts... is promising in future works."
- **Why unresolved:** The proposed framework is currently text-only, yet the AD detection task inherently involves speech/audio data which contains rich paralinguistic markers (pauses, prosody) not captured in transcripts.
- **What evidence would resolve it:** An extension of the PVA mechanism to inject vectors derived from acoustic embeddings into the LLM's hidden states, evaluated on multi-modal AD datasets.

### Open Question 3
- **Question:** Does the DA4ICL framework generalize to other low-resource, fine-grained classification tasks that do not exhibit the specific "semantic homogeneity" found in AD narrative transcripts?
- **Basis in paper:** [inferred] The paper attributes the failure of standard ICL to the specific nature of the AD task (homogeneous contexts describing the same scene). It is unclear if the computational overhead of PVA is necessary for tasks with naturally higher inter-class variance.
- **What evidence would resolve it:** Application of the DA4ICL framework to other fine-grained domains (e.g., sentiment nuance detection, specific legal clause classification) to test if the "demo-centric" anchoring provides universal benefits or is specialized for homogeneous inputs.

### Open Question 4
- **Question:** Can the computational overhead of layer-wise Projected Vector Anchoring be reduced through selective layer injection without compromising performance?
- **Basis in paper:** [inferred] Tables 9 and 10 reveal DA4ICL incurs a significant efficiency penalty (e.g., ~283s elapsed time vs ~13s for Vanilla ICL on an 80GB GPU) due to processing all 32 layers.
- **Why unresolved:** While the authors analyze the layer-wise scaling factors ($\gamma$), they do not explore whether anchoring can be restricted to specific critical layers (e.g., only the first and last 8 layers) to improve latency for real-time clinical use.
- **What evidence would resolve it:** Ablation studies varying the number of anchor layers (e.g., anchoring every 2nd layer or only deep layers) to identify a performance-efficiency trade-off threshold.

## Limitations
- **Fragile assumption on diversity:** DCR assumes sufficient semantic diversity exists in the training corpus to support "dissimilar" sampling—a fragile assumption for extremely small datasets (N < 50).
- **Orthogonality sensitivity:** The projection mechanism's sensitivity to Task Vector orthogonality is not quantified; if the demo and test hidden states are nearly orthogonal, PVA injects negligible signal.
- **Compute overhead:** The method significantly increases inference time (521s vs 15s for Vanilla ICL), which may be prohibitive for real-time clinical applications.

## Confidence
- **High Confidence:** The empirical superiority of DA4ICL over standard ICL and TV methods (F1 86.11% vs ~70-75%) is well-supported by the results tables. The ablation showing PVA's importance (vs Add/Replace TV) is methodologically sound.
- **Medium Confidence:** The claim that DCR is necessary for PVA's success (Fig. 4 shows DCR helps ICL but not standard TV) is plausible but could reflect interaction effects rather than pure causality.
- **Low Confidence:** The assertion that semantic homogeneity is the primary failure mode for standard ICL in AD detection is reasonable but not rigorously proven. Alternative explanations (e.g., domain shift, prompt formatting) are not fully explored.

## Next Checks
1. **Retrieval Stress Test:** Replace DCR with purely random retrieval while keeping PVA intact. If F1 drops to standard ICL levels (~70-75%), it confirms demo diversity is the primary driver. If F1 remains high (~85%), it suggests PVA alone provides most of the benefit.
2. **Layer-Wise Sensitivity Analysis:** Systematically vary the scaling factor γ across layers (e.g., test γ ∈ {0.0, 0.1, 0.5, 1.0} for each layer group). This would reveal whether the paper's fixed schedule is optimal or if certain layers are over/under-weighted.
3. **Task Vector Orthogonality Experiment:** For each demo, compute the cosine similarity between its Task Vector and the test sample's hidden state. If median similarity is <0.3, PVA may be injecting near-zero signal for most demos, suggesting the method relies heavily on a few highly aligned examples rather than robust distributed reasoning.