---
ver: rpa2
title: 'The Self-Improvement Paradox: Can Language Models Bootstrap Reasoning Capabilities
  without External Scaffolding?'
arxiv_id: '2502.13441'
source_url: https://arxiv.org/abs/2502.13441
tags:
- crescent
- data
- shot
- questions
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CRESCENT, a framework for self-improving
  large language models (LLMs) without external supervision. CRESCENT generates high-quality
  domain-specific question-answer pairs through bait prompting, rejection sampling-based
  diversification, and majority voting-based consensus enhancement.
---

# The Self-Improvement Paradox: Can Language Models Bootstrap Reasoning Capabilities without External Scaffolding?

## Quick Facts
- arXiv ID: 2502.13441
- Source URL: https://arxiv.org/abs/2502.13441
- Reference count: 11
- Primary result: Llama3-8B-Instruct improves from 34.5% to 63.3% on GSM8K 0-shot using self-generated data

## Executive Summary
This paper introduces CRESCENT, a framework for self-improving large language models without external supervision. The method generates high-quality domain-specific question-answer pairs through bait prompting, rejection sampling-based diversification, and majority voting-based consensus enhancement. Experiments show CRESCENT-generated data significantly improves mathematical reasoning capabilities while preserving general performance, outperforming existing approaches like Magpie.

## Method Summary
CRESCENT uses a four-step pipeline: (1) bait prompting generates raw domain-specific questions, (2) diversification removes redundant questions via embedding similarity and deduplication prompts, (3) consensus enhancement selects answers via majority voting from multiple CoT samples, and (4) SFT fine-tunes the model on the resulting QA pairs. The framework operates without external supervision, seed data, or third-party models, using only the LLM's own capabilities.

## Key Results
- Llama3-8B-Instruct improves from 34.5% to 63.3% on GSM8K 0-shot using CRESCENT-generated data
- Performance improves consistently from 25k to 75k training samples, then plateaus
- CRESCENT outperforms Magpie in generating domain-specific math data
- No catastrophic forgetting observed on general benchmarks (ARC-C, MMLU, IFEval, HellaSwag, GPQA)

## Why This Works (Mechanism)

### Mechanism 1: Diversification via Rejection Sampling
Rejection sampling-based deduplication reduces redundancy and increases coverage of the problem space. Each question is embedded and compared against prior questions via L2 distance. If below threshold θ=0.25, a deduplication prompt forces modification. Evidence shows removing diversification drops accuracy 77.6% → 71.1% (-6.5%), and T-SNE visualization shows clustered, lower-difficulty questions without it.

### Mechanism 2: Consensus Enhancement via Majority Voting
Consensus enhancement filters incorrect reasoning paths by selecting the most frequent answer. For each question, sample m=5 independent CoT answers at T=0.95. Select the most frequent final answer, assuming correct paths converge while incorrect ones diverge. Ablation shows removing consensus enhancement drops accuracy 77.6% → 73.0% (-4.6%).

### Mechanism 3: Bait Prompting for Domain Activation
Bait prompting constrains generation to a target domain without requiring seed data. A simple prompt ("Generate a diverse math word problem requiring multi-step reasoning") activates the model's latent domain knowledge. Evidence shows CRESCENT generates actual problems while Magpie produces math-related chat, demonstrating bait prompting is more targeted.

## Foundational Learning

### Concept: Rejection Sampling
Why needed here: CRESCENT filters similar questions via threshold-based accept/reject decisions. Understanding how θ shapes the final distribution is essential for tuning.
Quick check question: Given θ=0.25 and L2 distance 0.15 between a new and existing question, should the new question be accepted or flagged for deduplication?

### Concept: Majority Voting / Self-Consistency
Why needed here: Core answer-selection mechanism. Understanding when consensus correlates with correctness helps diagnose voting failures.
Quick check question: If 5 sampled answers are [3, 3, 5, 3, 7], what is the majority-voted answer?

### Concept: SFT Dynamics with Synthetic Data
Why needed here: Generated QA pairs train the model via SFT. Understanding data volume/quality tradeoffs explains results like the 75k ceiling.
Quick check question: Why might performance plateau beyond 75k samples, as observed in Figure 7?

## Architecture Onboarding

### Component map:
Bait Prompting Module → Raw questions → Embedding + similarity check → (if d<θ: dedup prompt) → Deduplicated questions → Multi-sample answering → Majority vote → Final QA pairs → SFT

### Critical path:
Bait prompt → Raw questions → Embed + similarity check → (if d<θ: dedup prompt) → Deduplicated questions → Multi-sample answering → Majority vote → Final QA pairs → SFT

### Design tradeoffs:
- θ: Lower = more diversity but more rejections; paper uses 0.25
- m: More samples = better consensus but higher cost; paper uses 5
- Temperature: 0.95 encourages diversity but may lower sample quality
- Data volume: Diminishing returns beyond 75k

### Failure signatures:
- Low diversity: T-SNE shows tight clustering; accuracy drops without diversification
- Consensus failure: Majority consistently selects wrong answers → model error rate too high
- Domain drift: Questions are off-topic/trivial → bait prompt insufficient or model lacks domain knowledge
- Catastrophic forgetting: General benchmark performance drops (paper shows this does not occur, but monitor)

### First 3 experiments:
1. Reproduce diversification ablation (w/ vs. w/o deduplication) on GSM8K 5-shot to confirm ~6.5% gap.
2. Sweep θ ∈ {0.15, 0.25, 0.35}; measure embedding variance and downstream accuracy to validate optimal threshold.
3. Apply CRESCENT to a new domain (e.g., logical puzzles) with appropriate bait prompt to test generalization beyond math.

## Open Questions the Paper Calls Out
1. Can the CRESCENT framework be applied to base models that lack instruction tuning? The authors state they did not investigate whether the same approach can be used to generate high-quality, domain-specific data for base models without instruction tuning.
2. Does CRESCENT generalize to reasoning domains beyond mathematical word problems? The authors note experiments are confined to math reasoning capabilities, with further extensions subject to future work.
3. What causes the performance saturation observed when scaling synthetic data volume beyond 75k samples? Section 4.4 notes performance stabilizes between 75k and 150k, suggesting an upper limit, but the cause is unclear.

## Limitations
- Method validated only on mathematical word problems; generalization to other domains unknown
- Consensus enhancement requires deterministic answers (numbers), limiting applicability to open-ended reasoning
- Answer normalization for majority voting not fully specified, creating reproducibility challenges

## Confidence
- High: Self-improvement is possible without external scaffolding; diversification and consensus enhancement both contribute measurable gains; no catastrophic forgetting on tested general benchmarks
- Medium: Bait prompting can elicit valid domain-specific questions without demonstrations; the 75k data point represents a true performance ceiling; gains are primarily due to reasoning improvement, not formatting fixes
- Low: Not applicable (no low-confidence claims identified)

## Next Checks
1. Reproduce the consensus enhancement ablation (77.6% → 73.0% drop) to verify that majority voting is a meaningful contributor and not sensitive to random seed.
2. Sweep the similarity threshold θ (e.g., 0.15, 0.25, 0.35) and measure embedding distance distributions and downstream accuracy to confirm the chosen value is optimal, not arbitrary.
3. Apply CRESCENT to a non-mathematical domain (e.g., logical puzzles or commonsense reasoning) with an appropriately designed bait prompt to test domain generalization and identify any prompt engineering requirements.