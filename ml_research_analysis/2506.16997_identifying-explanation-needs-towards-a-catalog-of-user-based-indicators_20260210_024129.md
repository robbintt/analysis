---
ver: rpa2
title: 'Identifying Explanation Needs: Towards a Catalog of User-based Indicators'
arxiv_id: '2506.16997'
source_url: https://arxiv.org/abs/2506.16997
tags:
- indicators
- explanation
- need
- system
- needs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of identifying when users need explanations
  in software systems. It addresses the challenge of eliciting explanation needs without
  introducing biases such as hypothetical or confirmation bias.
---

# Identifying Explanation Needs: Towards a Catalog of User-based Indicators

## Quick Facts
- arXiv ID: 2506.16997
- Source URL: https://arxiv.org/abs/2506.16997
- Reference count: 40
- One-line primary result: A catalog of 39 user-based indicators (17 behavior, 8 system events, 14 emotional/physical) that signal explanation needs in software systems

## Executive Summary
This paper addresses the challenge of identifying when users need explanations in software systems by developing a catalog of user-based indicators. The research team conducted an online study with 66 participants who reported their explanation needs, behaviors, and emotional states while using three software systems. Through a rigorous four-round qualitative coding process, they established taxonomies for different types of explanation needs and the indicators that can signal them. The resulting catalog provides a foundation for both research and practical applications in explainability, enabling systems to detect when users struggle and require assistance.

## Method Summary
The researchers employed a mixed-method approach combining qualitative coding with quantitative validation. Participants completed a survey about recently used software systems, answering questions about their explanation needs and the behaviors, system events, and emotional states that accompanied those needs. The free-text responses were then coded through four iterative rounds: Round 1 adapted an existing taxonomy for needs; Rounds 2-4 established and refined taxonomies for behavior-based, system event-based, and emotional/physical indicators. The coding process used dual independent coding with conflict resolution and calculated interrater agreement using Brennan & Prediger's kappa coefficient. The final catalog was validated through multiple coding rounds and statistical analysis of relationships between indicators and need types.

## Key Results
- Identified 17 behavior-based indicators, 8 system event-based indicators, and 14 emotional/physical reaction indicators
- Found that interaction-related explanation needs were most frequently reported and could be identified through behavioral patterns
- Established that back-and-forth navigation and canceled actions were particularly strong behavioral indicators
- Achieved substantial to almost perfect interrater agreement (κ=0.59-0.91) across coding categories

## Why This Works (Mechanism)
The approach works by systematically collecting self-reported data about real usage experiences rather than hypothetical scenarios, which reduces common biases in user research. By focusing on actual behaviors, system events, and emotional reactions that users experienced and remembered, the study captures authentic signals of explanation need. The multi-round coding process with independent validation ensures reliability of the identified indicators, while the diversity of software systems used by participants provides some generalizability across contexts.

## Foundational Learning
- **Interrater Agreement (Brennan & Prediger κ)**: Needed to ensure coding reliability when multiple researchers classify qualitative data; quick check: calculate κ values for all coding categories
- **Taxonomy Development Process**: Required to systematically organize complex qualitative data into usable categories; quick check: verify each round of coding produces convergent results
- **Qualitative Coding with MAXQDA**: Essential tool for managing and analyzing large volumes of free-text survey responses; quick check: ensure all responses are coded and categorized
- **Self-Reported vs Observed Behavior**: Understanding the limitations and advantages of retrospective data collection; quick check: compare reported behaviors with actual usage logs where available
- **Explanation Need Types**: Different categories of user confusion requiring different types of explanations; quick check: map each indicator to specific need types in the catalog

## Architecture Onboarding
- **Component Map**: User Experience -> Survey Responses -> Qualitative Coding -> Indicator Taxonomy -> Explanation System Integration
- **Critical Path**: Survey Design → Data Collection → Coding Rounds → Indicator Validation → Catalog Publication
- **Design Tradeoffs**: Self-reported data (authentic but retrospective) vs. observational data (objective but potentially intrusive); broad indicator catalog vs. domain-specific precision
- **Failure Signatures**: Low interrater agreement indicates ambiguous indicator definitions; inconsistent mapping between indicators and needs suggests incomplete taxonomy
- **First Experiments**: 1) Test indicator detection accuracy on screen recordings of actual usage; 2) Validate catalog across different software domains; 3) Implement real-time detection system and measure user satisfaction

## Open Questions the Paper Calls Out
**Open Question 1**: What is the predictive precision and recall of the identified behavior-based indicators when applied to real-time usage data? The authors plan to test indicators using various criteria including precision, recall, and implementation effort, but this requires observational validation rather than retrospective survey data.

**Open Question 2**: Which unconscious user behaviors indicate a need for explanation that are not captured by self-reporting methods? The survey methodology could only capture behaviors participants were aware of, potentially missing implicit indicators that require observational studies.

**Open Question 3**: Which specific facial expressions are most strongly correlated with the onset of an explanation need? While facial expressions were reported as a general category, the data lacked granularity to distinguish specific physical manifestations requiring experimental facial coding systems.

## Limitations
- The study relies entirely on self-reported data without observational validation, limiting confidence in behavioral indicators interpreted from text descriptions
- The taxonomy development process involved iterative refinement, but exact decision rules for boundary cases are not fully specified
- The sample is limited to participants who could articulate their needs in writing, potentially excluding users with lower literacy or non-native speakers

## Confidence
- **High confidence**: The classification of emotional/physical indicators (14 items) due to their explicit nature and strong interrater agreement (κ=0.91)
- **Medium confidence**: The mapping between behavior-based indicators and explanation needs, as this requires inference from free-text responses
- **Low confidence**: The generalizability of the catalog across different software domains, as the study focused on three unspecified systems

## Next Checks
1. Conduct observational studies to validate that identified behavioral patterns (e.g., back-and-forth navigation, canceled actions) actually correlate with explanation needs in real-time usage
2. Test the indicator catalog across diverse software types (productivity tools, entertainment platforms, educational systems) to assess domain transferability
3. Implement an automated detection system using the 39 indicators and evaluate its precision/recall against ground truth explanation needs in live software environments