---
ver: rpa2
title: A Survey of Large Language Model Agents for Question Answering
arxiv_id: '2503.19213'
source_url: https://arxiv.org/abs/2503.19213
tags:
- arxiv
- llms
- question
- reasoning
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews LLM-based agents for question
  answering, organizing the field across planning, question understanding, information
  retrieval, answer generation, and follow-up interaction. It highlights how these
  agents overcome limitations of traditional pipelines and naive LLMs by leveraging
  external tools, iterative reasoning, and dynamic environment interaction.
---

# A Survey of Large Language Model Agents for Question Answering

## Quick Facts
- arXiv ID: 2503.19213
- Source URL: https://arxiv.org/abs/2503.19213
- Reference count: 40
- Primary result: LLM-based agents outperform naive LLMs by leveraging external tools, iterative reasoning, and dynamic environment interaction

## Executive Summary
This survey provides a systematic review of LLM-based agents for question answering, organizing the field across planning, question understanding, information retrieval, answer generation, and follow-up interaction. It highlights how these agents overcome limitations of traditional pipelines and naive LLMs by leveraging external tools, iterative reasoning, and dynamic environment interaction. Key techniques include prompting-based and tuning-based planning, query reformulation, hybrid retrieval-ranking approaches, tool-augmented generation, and follow-up error resolution. The survey also identifies critical challenges: lack of fine-grained evaluation, hallucination, reasoning gaps, and autonomous tool selection. It calls for more robust benchmarking, improved calibration, causal reasoning integration, and LLM-driven document indexing.

## Method Summary
The survey presents a conceptual framework for LLM-based QA agents without providing a single implementable method. It describes an architecture with memory M, planning module π_p, thinking module π_t, and a formal interaction loop (Algorithm 1) where the agent executes actions, receives observations, and updates memory. The work reviews various techniques (ReAct, CoT, PoT, Self-RAG, HyQE) but lacks implementation details, training configurations, or unified benchmarks. The focus is on conceptual organization rather than reproducible methodology.

## Key Results
- LLM agents achieve superior QA results compared to traditional pipelines and naive LLM QA systems through external environment interaction
- Hybrid retrieval with LLM-based reranking improves relevance over sparse-only methods, particularly for semantic mismatches
- Dynamic planning via LLM-as-controller helps with multi-step or ambiguous questions based on reported benchmark results

## Why This Works (Mechanism)

### Mechanism 1: External Environment Interaction
LLM-based agents outperform naive LLMs by querying external knowledge sources during inference. The agent executes external actions (A_t), receives observations (O_t = E(A_t)), and updates memory via concatenation. This allows fetching current information the LLM lacks parametrically. Core assumption: external sources (databases, APIs, tools) are more reliable for domain-specific or time-sensitive knowledge than the LLM's frozen weights.

### Mechanism 2: Dynamic Planning via LLM-as-Controller
Decomposing QA into planned action sequences helps with multi-step or ambiguous questions. A planning module π_p(M) selects the next action by conditioning on current memory. This replaces static pipelines with context-dependent strategies. Core assumption: the LLM can infer appropriate decomposition strategies from prompts or fine-tuning data, and these transfer to unseen questions.

### Mechanism 3: Retrieval-Reranking-Compression Pipeline
Hybrid retrieval with LLM-based reranking may improve relevance over sparse-only methods, particularly for semantic mismatches. Dense retrieval fetches candidates; an LLM or tuned model assigns relevance scores r(q, d_i); compression reduces token cost while preserving key content. Core assumption: LLMs better capture semantic relevance than TF-IDF, and compressed context retains sufficient signal.

## Foundational Learning

- **Concept: Agent State as Accumulating Memory**
  - Why needed here: Understanding M = M ∥ (A_t, O_t) is essential for debugging why an agent repeats actions or loses context.
  - Quick check question: After two retrieval actions and one thinking step, what does memory contain?

- **Concept: Dense vs. Sparse Retrieval**
  - Why needed here: The IR module combines both; diagnosing retrieval failures requires knowing when each works or fails.
  - Quick check question: Why might BM25 fail to retrieve a document that uses synonyms for query terms?

- **Concept: Tool-Augmented Generation**
  - Why needed here: Many QA agents delegate computation (e.g., PoT for math) to external interpreters; understanding this delegation prevents misattribution of errors.
  - Quick check question: In Program-of-Thoughts, does the LLM compute the final answer or the code interpreter?

## Architecture Onboarding

- **Component map:**
  Memory (M) → Planning Module (π_p) → (Retrieve / Think / Call Tool) → Update Memory → Repeat until termination → Generate Answer

- **Critical path:**
  Question → Init Memory → Plan Action → (Retrieve / Think / Call Tool) → Update Memory → Repeat until termination → Generate Answer

- **Design tradeoffs:**
  - Prompting-based planning is flexible but prompt-sensitive; tuning-based planning generalizes better but requires trajectory data and risks overfitting
  - More retrieved documents improve coverage but increase noise and token cost
  - Deeper tool integration expands capability but complicates error handling and latency

- **Failure signatures:**
  - Agent loops on same action without progress → planning prompt may lack termination criteria
  - Hallucination despite retrieved context → ranker failed or agent ignored documents
  - Tool call errors → API mismatch or malformed LLM output
  - Memory exceeds context window → compression module not triggered or too weak

- **First 3 experiments:**
  1. Compare naive LLM QA vs. retrieval-augmented agent on HotpotQA (multi-hop) and SQuAD (single-hop) to isolate retrieval benefit
  2. Ablate the reranking module on open-domain questions; measure precision@k and final answer accuracy
  3. Test ReAct-style prompting vs. a tuning-based planner (e.g., FireAct) on action efficiency and success rate across ambiguous (ASQA) and factual (StrategyQA) datasets

## Open Questions the Paper Calls Out

### Open Question 1
Can specific neural circuits or intrinsic representations corresponding to uncertainty and overconfidence be identified within LLMs to enable self-aware hallucination detection? Current calibration methods have not significantly outperformed basic logit-based or ensemble strategies. Discovery of specific attention heads or neurons that correlate with model hallucination would resolve this.

### Open Question 2
How can LLM agents be designed to extract and apply foundational knowledge from long-term memory to new scenarios, rather than treating every interaction as an independent session? Current memory mechanisms rely on appending interaction history, which is insufficient for true adaptation. An agent architecture that demonstrates improved reasoning performance on a novel task solely through utilization of a compressed, persistent memory module would resolve this.

### Open Question 3
How can LLM agents be enabled to autonomously plan and create new tools (e.g., scripts, functions) to solve recurring problems, rather than relying solely on pre-defined tools? This requires higher-level meta-cognition and planning than current approaches provide. A system where an agent automatically generates, tests, and stores a reusable Python function to address a recurring class of data retrieval queries would resolve this.

## Limitations
- No implementation details, prompt templates, or training configurations provided
- Lacks unified benchmark or evaluation protocol
- Mechanisms are theoretically sound but require significant engineering and experimental validation

## Confidence
- **High**: The formal interaction loop and memory accumulation mechanism (M = M ∥ (A_t, O_t)) are well-specified and theoretically grounded
- **Medium**: Benefits of external environment interaction and retrieval-augmented generation are supported by benchmark results but lack causal mechanism proof
- **Low**: Tuning-based planning approaches are described but without training data specifications or validation of transfer to out-of-distribution queries

## Next Checks
1. Implement the baseline ReAct-style agent using Algorithm 1 and test on HotpotQA vs. naive LLM QA to isolate retrieval benefits
2. Conduct an ablation study removing the LLM reranker from the retrieval pipeline to measure its impact on precision@k and final answer accuracy
3. Compare prompting-based (ReAct) vs. tuning-based (FireAct) planning on action efficiency and success rate across ambiguous (ASQA) and factual (StrategyQA) datasets