---
ver: rpa2
title: 'Epidemiology of Large Language Models: A Benchmark for Observational Distribution
  Knowledge'
arxiv_id: '2511.03070'
source_url: https://arxiv.org/abs/2511.03070
tags:
- race
- education
- knowledge
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first benchmark designed to evaluate
  whether large language models (LLMs) can access and accurately represent real-world
  observational distributions across diverse domains such as health, economics, and
  social behavior. Using datasets from U.S.
---

# Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge

## Quick Facts
- arXiv ID: 2511.03070
- Source URL: https://arxiv.org/abs/2511.03070
- Reference count: 40
- Primary result: Even the best-performing LLMs score significantly below optimal (22/100 for low-dimensional tasks, 17/100 for high-dimensional) in internalizing real-world observational distributions.

## Executive Summary
This paper introduces the first benchmark to evaluate whether large language models can accurately represent real-world observational distributions across domains like health, economics, and social behavior. Using datasets from U.S. national surveys, the authors create tasks requiring LLMs to estimate conditional probabilities over population-level statistics. Results show that current models, including state-of-the-art open and closed-weight models, score well below optimal (22/100 for low-dimensional, 17/100 for high-dimensional tasks), indicating limited ability to internalize real-world statistics. The findings suggest that LLMs lack robust Layer 1 (observational) knowledge in Pearl's Causal Hierarchy, casting doubt on their reliability for higher-level causal or counterfactual reasoning.

## Method Summary
The benchmark uses 10 U.S. population datasets (ACS, NHANES, BRFSS, MEPS, NSDUH, SCF, GSS, IPEDS, BLS, FBI UCR) to create 169 tasks requiring estimation of conditional distributions P(V_Y|V_X). Tasks are split into 75 low-dimensional (single conditioning variable) and 94 high-dimensional (2-5 conditioning variables) tasks. Models are evaluated using two prompting strategies: QA prompting (multiple choice with permutation averaging) and likelihood prompting (direct probability bucket elicitation). A modular evaluation framework supports both open- and closed-weight models. Performance is measured using an L1-based scoring system normalized against random guessing and constant baselines, with bootstrap sampling establishing statistical significance thresholds.

## Key Results
- Best-performing models scored 22/100 on low-dimensional tasks and 17/100 on high-dimensional tasks
- No substantial performance improvement from fine-tuning or instruction-tuning
- Likelihood prompting (41/100) outperformed QA prompting (22/100)
- RAG with GPT-4.1 showed no improvement on zero-score tasks
- Performance correlates weakly with model parameter count and training data

## Why This Works (Mechanism)

### Mechanism 1
L1 distance scoring normalized against baselines quantifies distributional misalignment while accounting for sampling uncertainty. The scoring computes D(P̃||P) using L1-norm over conditional distributions, then normalizes against uniform (random guessing) and 0/1 baselines. Bootstrap sampling establishes the S=100 threshold at the 5% significance level where models become statistically indistinguishable from ground truth.

### Mechanism 2
Poor Layer 1 (observational) knowledge implies unreliable Layer 2 (interventional) and Layer 3 (counterfactual) inference per the Causal Hierarchy Theorem. The CHT establishes that observational distributions P(V) underdetermine interventional P(V|do(X)) and counterfactual distributions without additional causal assumptions. If LLMs' internal P̃(V) ≠ P(V), no guarantees hold for higher-layer inferences even with correct causal structure.

### Mechanism 3
Likelihood prompting outperforms QA prompting because it elicits explicit probability estimates rather than relying on next-token probabilities over answer labels. QA prompting extracts P̃ from normalized next-token probabilities over answer tokens (A, B, C...), which confounds answer probability with token position bias. Likelihood prompting asks models to directly output probability ranges, reducing this confound.

## Foundational Learning

- **Pearl's Causal Hierarchy (PCH)**
  - Why needed here: The entire theoretical framing depends on distinguishing observational (Layer 1), interventional (Layer 2), and counterfactual (Layer 3) knowledge.
  - Quick check question: If you observe that people who carry umbrellas are more likely to get wet, can you infer that carrying an umbrella causes wetness? Which PCH layer does this confound?

- **Curse of Dimensionality in Distribution Learning**
  - Why needed here: The paper invokes Stone (1982) to explain why high-dimensional conditional distributions P(V_Y|V_X) with |V_X| ≥ 2 are fundamentally harder to learn.
  - Quick check question: Why does estimating P(Y|X_1, X_2, X_3, X_4, X_5) require far more data than P(Y|X_1), even if each variable is binary?

- **Bootstrap Uncertainty Quantification**
  - Why needed here: The scoring system uses bootstrap resampling to establish what counts as "statistically indistinguishable" from ground truth.
  - Quick check question: If your dataset has 1,000 samples and you draw 100 bootstrap samples of size 1,000, what source of variability are you quantifying?

## Architecture Onboarding

- **Component map**:
  Dataset Layer -> Task Generator -> Elicitation Module -> Scoring Engine -> Model Interface

- **Critical path**:
  1. Load dataset → extract P(V_Y|V_X) via bin-counting (low-dim) or LightGBM (high-dim)
  2. Generate prompts from templates with all VX combinations
  3. Query model → extract P̃(V_Y|V_X) via chosen prompting strategy
  4. Compute L1 distance → normalize against uniform/0/1 baselines
  5. Bootstrap ground truth → set S=100 threshold → interpolate final score

- **Design tradeoffs**:
  - L1 vs KL divergence: L1 avoids sensitivity to zero-probability events but may underpenalize tail errors
  - Permutation averaging in QA: Mitigates ordering bias but increases compute O(n! or 120 samples)
  - Closed model evaluation: Limited to likelihood prompting due to no token probability access
  - High-dimensional ground truth: Uses LightGBM rather than empirical counts to handle sparse conditioning sets

- **Failure signatures**:
  - Scores clustering near zero across all models → suggests task design flaw or overly stringent baseline
  - Large variance across prompt permutations → indicates ordering bias not fully corrected
  - Fine-tuned models scoring zero on all high-dimensional tasks → suggests overfitting or catastrophic forgetting
  - RAG showing no improvement on zero-score tasks → retrieval not finding relevant statistics

- **First 3 experiments**:
  1. Implement the mean-baseline P_mean(V_Y=1|V_X) = E_P[V_Y] for all vX and verify it scores ~46/100
  2. For one low-dimensional task, run all |dom(V_Y)|! permutations with a single model and measure variance
  3. Evaluate one model family across all dimensions d ∈ {1,2,3,4,5} on BRFSS diabetes tasks

## Open Questions the Paper Calls Out

- How does the observed deficiency in Layer 1 knowledge quantitatively degrade performance on Layer 2 and Layer 3 reasoning tasks? (Basis: Future work should "evaluate capabilities of AI models for inference in Layers 2 & 3 of the PCH")

- To what extent do LLMs internalize observational distributions for populations outside the United States? (Basis: "all datasets considered describe US populations" and calls for "wider geographic coverage")

- Why does retrieval-augmented generation (RAG) fail to improve benchmark scores on tasks where models initially scored zero? (Basis: GPT-4.1 with web-based RAG still scored zero on select tasks, "warrants further investigation")

## Limitations

- Prompt Engineering Sensitivity: The paper demonstrates likelihood prompting outperforms QA prompting but the underlying cause remains unclear, and exact prompt templates are not fully specified.
- Closed-Model Evaluation Constraints: Evaluation of closed models (GPT-4.1, o4-mini) is significantly limited due to API access constraints and only using likelihood prompting.
- Ground Truth Construction for High-Dimensional Tasks: The exact binning thresholds and feature engineering choices for high-dimensional tasks are not fully specified, requiring trust in machine learning approximations.

## Confidence

**Primary Claim: LLMs Lack Robust Layer 1 Knowledge** (Confidence: High)
**Secondary Claim: Limited Layer 2/3 Inference Capability** (Confidence: Medium)
**Tertiary Claim: No Fine-tuning Benefits** (Confidence: Medium)

## Next Checks

1. **Permutation Averaging Validation**: For a representative sample of low-dimensional tasks, systematically test the impact of permutation averaging in QA prompting by comparing scores with and without this correction.

2. **Fine-tuning Data Analysis**: Analyze the fine-tuning datasets used for the models tested to determine whether they actually contain the types of observational distribution knowledge being evaluated.

3. **Open vs Closed Model Comparison**: Design a controlled experiment comparing open and closed models on identical subsets of tasks using identical prompting strategies.