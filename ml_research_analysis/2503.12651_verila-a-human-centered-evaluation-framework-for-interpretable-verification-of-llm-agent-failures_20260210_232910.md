---
ver: rpa2
title: 'VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification
  of LLM Agent Failures'
arxiv_id: '2503.12651'
source_url: https://arxiv.org/abs/2503.12651
tags:
- agent
- eggs
- task
- agents
- criteria
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces VeriLA, a human-centered framework to verify
  failures in LLM-based compound AI systems. VeriLA combines human-defined agent criteria,
  uncertainty estimates, and plan structure to create agent-specific verifiers that
  predict execution failures.
---

# VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification of LLM Agent Failures

## Quick Facts
- arXiv ID: 2503.12651
- Source URL: https://arxiv.org/abs/2503.12651
- Reference count: 40
- Primary result: 88% accuracy in detecting LLM agent execution failures

## Executive Summary
VeriLA introduces a human-centered framework to verify failures in LLM-based compound AI systems. The framework combines human-defined agent criteria, uncertainty estimates, and plan structure to create agent-specific verifiers that predict execution failures. By enabling granular detection of faulty agents and providing interpretable failure reasons, VeriLA reduces manual review effort while maintaining high accuracy. Evaluation on mathematical reasoning tasks demonstrates that verifiers achieve 88% accuracy in detecting agent failures, with aggregation metrics effectively prioritizing likely task failures.

## Method Summary
VeriLA operates through a three-stage pipeline: (1) a planning agent decomposes tasks into DAG plans using a human-curated agent registry, (2) execution agents perform subtasks with context propagation, and (3) Random Forest verifiers predict failures using 26 features including verbalized confidence, logit-based confidence, self-consistency metrics, external LLM judge scores on human-defined criteria, and plan structure features. The framework is trained on human-annotated ground truth labels and aggregates individual agent predictions to predict task-level failure.

## Key Results
- Verifier achieves 88% accuracy in detecting agent execution failures
- Agent criteria features significantly enhance verifier performance compared to uncertainty-only approaches
- Aggregation metrics effectively prioritize likely task failures, with mean and outdegree showing stable performance across datasets

## Why This Works (Mechanism)

### Mechanism 1: Human-Criteria Grounding Improves Verifier Alignment
Incorporating human-defined agent criteria into verifier training improves detection accuracy and interpretability. External LLM judges score each agent output against predefined, task-specific criteria (e.g., accuracy, format adherence, context sufficiency), grounding predictions in human expectations rather than model internals alone. This approach assumes human criteria capture meaningful dimensions of execution success that correlate with actual failures.

### Mechanism 2: Multi-Signal Uncertainty Improves Failure Detection
Combining multiple uncertainty estimation techniques provides complementary signals that improve verifier accuracy. The verifier aggregates verbalized confidence, logit-based confidence, and self-consistency metrics, each capturing different aspects of model uncertainty. Lower uncertainty correlates with higher execution correctness, and different methods capture non-redundant information.

### Mechanism 3: Plan Structure Enables Error Propagation Awareness
Encoding DAG structure allows the verifier to weight failures by their downstream impact. Features include agent dependencies, node position, and connectivity metrics. Agents closer to source/sink nodes or with higher connectivity have disproportionate impact on task success, enabling effective aggregation for task-level failure prediction.

## Foundational Learning

- **Concept: Compound AI Systems and Agent Decomposition**
  - Why needed here: VeriLA assumes tasks are decomposed into subtasks executed by specialized agents with defined roles and I/O contracts
  - Quick check: Can you explain how a planning agent generates a DAG where nodes represent agents and edges represent data dependencies?

- **Concept: LLM Uncertainty Estimation Methods**
  - Why needed here: The verifier relies on verbalized confidence, logit-based confidence, and self-consistency as input features
  - Quick check: What is the difference between verbalized confidence (LLM self-reports) and logit-based confidence (token probabilities), and when might they disagree?

- **Concept: Human-Centered Evaluation Criteria Design**
  - Why needed here: Verifier performance depends on well-defined, agent-specific criteria
  - Quick check: For a "Filter" agent that filters a list based on a condition, what three criteria would you define, and how would you score them objectively?

- **Concept: Supervised Classifier Training for Verification**
  - Why needed here: The verifier is a Random Forest classifier trained on human-annotated ground truth labels
  - Quick check: Why might inter-rater reliability (Fleiss' kappa: 0.41) affect verifier generalization, and how does the paper address this?

## Architecture Onboarding

- **Component map:**
  1. Planning Agent (GPT-4o, temp=0.1): Decomposes task into DAG using agent registry
  2. Agent Registry: Human-curated roles, inputs, outputs, and criteria per agent type
  3. Execution Agents (GPT-4o, temp=0.1): Perform assigned subtasks with context from predecessors
  4. Feature Extractor: Collects 26 features per execution (uncertainty, criteria scores, structure)
  5. Agent Verifiers (Random Forest, 100 estimators): Binary classifiers trained per-dataset to predict failure
  6. Aggregation Module: Combines verifier scores using min/mean/distance/degree metrics
  7. Human Annotation Pipeline: MTurk or author labeling for ground truth (unanimity required)

- **Critical path:**
  1. Define agent registry with roles, I/O formats, and evaluation criteria
  2. Planning agent generates DAG plan and per-agent prompts
  3. Validate plan structure (manual filtering in paper)
  4. Execute agents sequentially with context propagation
  5. Extract features: LLM judge scores (criteria), uncertainty metrics, structure encoding
  6. Train verifier on human-annotated labels
  7. Apply aggregation metrics to predict task-level failure

- **Design tradeoffs:**
  - Verifier autonomy vs. human alignment: External LLM judges add cost but improve alignment
  - Annotation quality vs. quantity: Paper discards non-unanimous labels, reducing training data but improving reliability
  - Aggregator generality vs. specificity: No single aggregator dominates; mean and outdegree are "stable defaults" but may underperform on specific domains

- **Failure signatures:**
  - Low inter-rater reliability (kappa <0.5): Indicates ambiguous criteria or complex agent roles
  - Verifier accuracy drops when criteria removed: Confirms criteria are primary signal
  - Aggregator performance varies across datasets: Suggests structural weighting assumptions are domain-dependent
  - High verifier accuracy but low task-level prediction: Indicates aggregation is weak link

- **First 3 experiments:**
  1. Reproduce ablation study: Train verifier with all features, then remove each feature group individually
  2. Test aggregator sensitivity: Apply all six aggregation metrics to a held-out test set
  3. Pilot annotation quality check: Label 50 agent outputs with 3 annotators each and compute Fleiss' kappa

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does VeriLA improve human-AI interaction and reduce cognitive load in a live user study compared to baseline inspection methods?
- Basis in paper: The Conclusion states plans for a "crowdsourced user study that evaluates VeriLAâ€™s subtask and task-level verification performance and usability, comparing outcomes with and without VeriLA."
- Why unresolved: The current paper relies on a technical case study measuring accuracy (88%) on datasets, but does not validate the claimed benefits of "reducing human cognitive load" or "efficiency" via actual human-subject experiments.
- What evidence would resolve it: Results from the proposed user study measuring task completion times, error detection rates, and subjective usability scores between control groups and groups using VeriLA.

### Open Question 2
- Question: Can the framework maintain high verification accuracy in open-ended domains like fact-checking or open-domain QA where "human standards" are more subjective?
- Basis in paper: The Conclusion identifies the need to "expand our framework to broader applications, such as open-domain question answering and fact-checking."
- Why unresolved: The current evaluation is limited to mathematical reasoning (GSM8K, BBH) where answers are objective; it is unclear if the current features are sufficient for verifying semantic correctness in open-ended tasks.
- What evidence would resolve it: Benchmark results showing VeriLA's verifier performance on open-domain datasets like TruthfulQA or HotpotQA compared to the mathematical reasoning baseline.

### Open Question 3
- Question: How robust is the verifier when trained on data with low inter-annotator agreement, rather than the filtered unanimous samples used in the study?
- Basis in paper: The methodology notes that crowdsourced annotations for GSM8K had low inter-rater reliability (Fleiss' kappa 0.41), and the authors discarded non-unanimous samples, reducing the training set significantly.
- Why unresolved: In real-world applications, human expectations often vary, and relying solely on unanimous agreement may limit the verifier's ability to handle edge cases or diverse human perspectives.
- What evidence would resolve it: An ablation study comparing the verifier's performance when trained on the full noisy dataset versus the filtered high-agreement dataset.

### Open Question 4
- Question: Can the aggregation metrics be improved by dynamically incorporating free-form user feedback during the execution process?
- Basis in paper: The Conclusion proposes developing an "advanced aggregator that directly leverages user-centric aspects like free-form user feedback and a real-time incentive system."
- Why unresolved: Current aggregation metrics rely on static structural features and execution scores; they do not adapt based on user intervention or qualitative feedback provided during the task flow.
- What evidence would resolve it: Implementation of a feedback-aware aggregator that updates failure probabilities in real-time, demonstrating higher prioritization accuracy in scenarios involving iterative human-agent collaboration.

## Limitations
- Human annotation quality remains a critical bottleneck with low inter-rater reliability (Fleiss' kappa: 0.41)
- Generalizability of structural weighting assumptions across domains is uncertain, as aggregation metrics perform variably
- Unknown hyperparameters for the Random Forest verifier and potential API limitations for extracting token-level logits

## Confidence
- **High confidence**: Verifier architecture combining human criteria, uncertainty estimates, and plan structure as complementary signals
- **Medium confidence**: Specific performance metrics (88% accuracy) and aggregation metric effectiveness, given dataset-specific variations
- **Low confidence**: Long-term robustness across diverse compound AI system architectures and criteria definitions

## Next Checks
1. Replicate the ablation study by training verifiers with all features, then systematically removing each feature group to quantify individual contribution impacts
2. Test aggregation metric sensitivity by applying all six methods to a held-out test set and comparing failure detection curves across datasets
3. Pilot annotation quality assessment by having three annotators label 50 agent outputs to compute Fleiss' kappa and identify criteria refinement needs before full-scale annotation