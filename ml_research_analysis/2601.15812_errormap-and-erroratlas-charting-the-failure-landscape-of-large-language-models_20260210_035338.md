---
ver: rpa2
title: 'ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language
  Models'
arxiv_id: '2601.15812'
source_url: https://arxiv.org/abs/2601.15812
tags:
- error
- errors
- failure
- language
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ErrorMap, a method to systematically chart
  the sources of failure in large language models (LLMs) by analyzing incorrect predictions
  to extract structured taxonomies of model errors. Unlike existing evaluations that
  measure success rates, ErrorMap focuses on understanding why models fail, transforming
  raw errors into interpretable categories.
---

# ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models

## Quick Facts
- **arXiv ID:** 2601.15812
- **Source URL:** https://arxiv.org/abs/2601.15812
- **Reference count:** 40
- **Primary result:** Systematic method to chart LLM failure modes via structured error taxonomy, applied to 35 datasets and 83 models to generate ErrorAtlas.

## Executive Summary
This paper introduces ErrorMap, a novel method to systematically chart the sources of failure in large language models (LLMs) by analyzing incorrect predictions to extract structured taxonomies of model errors. Unlike existing evaluations that measure success rates, ErrorMap focuses on understanding why models fail, transforming raw errors into interpretable categories. The method uses an LLM judge to analyze each incorrect output, extracting the first major error and assigning a concise label. These labels are then grouped recursively into a multi-level taxonomy. Applying ErrorMap to 35 datasets and 83 models, the authors construct ErrorAtlas, a comprehensive taxonomy revealing 17 high-level error categories such as "Missing Required Element" and "Specification Misinterpretation," which are prevalent but understudied. ErrorAtlas uncovers distinct failure patterns across models and tasks, offering deeper insights than standard benchmarks. The approach is validated for accuracy, coverage, and robustness, and demonstrates practical utility in model debugging, benchmark analysis, and model selection. Code and taxonomy are publicly released for ongoing updates.

## Method Summary
ErrorMap is a two-stage LLM-based method for generating hierarchical taxonomies of model errors. In Stage 1, an LLM judge analyzes each incorrect prediction from a benchmark, using the input, reference answer, and optional Informative Correct Predictions (ICPs) from other models to construct required criteria, evaluate the prediction, and output a structured JSON containing the first major error and a free-form error label. In Stage 2, these labels are aggregated and organized into a multi-level taxonomy through an iterative LLM clustering process: an initial batch is used to generate categories, new batches refine or merge them, and a final review ensures coherence. The resulting ErrorAtlas taxonomy (17 high-level categories) can be applied to new models for comparative diagnostic analysis, revealing failure patterns beyond aggregate task-level metrics.

## Key Results
- Generated ErrorAtlas, a comprehensive taxonomy of 17 high-level error categories, validated for coverage (95%), accuracy (92%), and usefulness.
- Applied ErrorMap to 35 datasets and 83 models, revealing distinct failure patterns and previously overlooked error types.
- Demonstrated practical utility in model debugging, benchmark analysis, and model selection with interpretable, comparative evaluation.

## Why This Works (Mechanism)

### Mechanism 1: Per-Instance Error Analysis via LLM-as-Judge
- Claim: Structured, criteria-based analysis of incorrect predictions extracts interpretable "first major error" labels that capture underlying failure causes.
- Mechanism: An analyst LLM is prompted with the original input, reference answer, model prediction, and optional Informative Correct Predictions (ICPs). It is instructed to: (1) identify the required criteria or reasoning steps to solve the problem correctly; (2) evaluate the incorrect prediction against each criterion (quality: correct/partially correct/incorrect); (3) identify the first major error that caused the failure; and (4) output a structured JSON with an `error_summary` and a free-form `error_title` (label). This transforms raw failures into structured data.
- Core assumption: An LLM can accurately identify and summarize the root cause of another model's error when given the correct context and a structured prompt.
- Evidence anchors:
  - [abstract] "It extracts a model's unique 'failure signature', clarifies what benchmarks measure..."
  - [section] Page 3, Stage 1: "The judge is asked to construct a structured solution to the instance... identify the first major error that caused the prediction to fail and create both a summary... and an informative label that highlights the failed skill."
  - [corpus] Weak/No direct corpus evidence for this specific LLM-as-error-judge mechanism. Related work on LLM-as-a-judge exists but is not cited as a direct precedent for error taxonomy extraction.
- Break condition: If the analyst LLM cannot reliably distinguish between its own errors and the target model's errors, or if it fails to follow the structured output format, the labels become noisy and downstream taxonomy quality degrades.

### Mechanism 2: Hierarchical Taxonomy Construction via Iterative Clustering
- Claim: Free-form error labels can be aggregated and organized into a coherent, multi-level taxonomy using an iterative LLM-based clustering and refinement process.
- Mechanism: The method follows a data mining approach (adapted from Wan et al., 2024, per page 4):
  1. **Category Generation:** A first batch of error labels (with frequencies) is passed to an LLM, which proposes an initial set of categories with descriptions.
  2. **Iterative Refinement:** New batches of labels are incrementally presented with existing categories, allowing the LLM to update, merge, or add categories.
  3. **Final Review:** A concluding pass ensures coherence and absence of overlap/ambiguity.
  This is followed by **Error Assignment (Stage 2.b)**, where another LLM call classifies each instance-level error into the finalized categories. The process can recurse to build deeper hierarchy levels.
- Core assumption: The semantic similarity between error labels, as understood by an LLM, maps well to meaningful clusters of underlying model failures.
- Evidence anchors:
  - [abstract] "ErrorMap introduces a novel LLM-based method... by extracting structured taxonomies of model errors."
  - [section] Page 3-4, Stage 2: "This stage organizes the errors... into a multi-level taxonomy... constructed in a top-down manner through iterative cycles..."
  - [corpus] Weak. The corpus mentions other benchmarking taxonomies but not this specific iterative LLM-based clustering mechanism for error analysis.
- Break condition: If the error label distribution is extremely sparse or highly domain-specific, the LLM may create overly generic or overly granular categories, reducing the taxonomy's utility.

### Mechanism 3: Comparative Evaluation Using Static ErrorAtlas Taxonomy
- Claim: A pre-constructed, comprehensive taxonomy (ErrorAtlas) can be applied to new model/dataset combinations to provide fine-grained, comparative diagnostic insights beyond single aggregate scores.
- Mechanism: ErrorAtlas, generated once from 35 datasets and 83 models (17 high-level categories), serves as a static label set. For new analysis, only Stage 1 (instance analysis) and Stage 2.b (assignment) are needed. The resulting distribution of errors per model/dataset allows for direct comparison (e.g., Model A has more computation errors, Model B has more reasoning errors). This enables "behavioral model-diffing."
- Core assumption: The error patterns captured in ErrorAtlas are sufficiently general to cover most common LLM failures across diverse tasks.
- Evidence anchors:
  - [abstract] "...it generates ErrorAtlasâ€”a static taxonomy revealing 17 high-level error categories... ErrorMap enables interpretable, comparative evaluation of model weaknesses beyond task-level metrics..."
  - [section] Page 6, 5.1: "...ErrorMap provides a structured way to assess behavioral changes between iterations... Figure 3 presents the differences in number of errors [between Gemini 1.5 flash and pro]."
  - [corpus] Weak. No corpus papers validate this specific mechanism.
- Break condition: If ErrorAtlas is not periodically updated, it may fail to capture novel failure modes in new model architectures or task types, leading to poor coverage.

## Foundational Learning
- Concept: **LLM-as-a-Judge / Evaluation**
  - Why needed here: ErrorMap fundamentally relies on an LLM (the analyst/judge) to perform the core cognitive task of diagnosing errors in another model's output. Understanding the strengths, weaknesses, and biases of LLM evaluators is critical.
  - Quick check question: Can you explain two known failure modes of LLM-as-a-judge systems (e.g., length bias, self-preference)?

- Concept: **Prompt Engineering for Structured Output**
  - Why needed here: The method requires the analyst LLM to output complex, structured JSON objects containing specific fields like `required_criteria`, `quality`, and `error_summary`. Mastery of prompting for reliable structured generation is essential.
  - Quick check question: How would you modify a prompt to force an LLM to output valid JSON, and what are the common pitfalls?

- Concept: **Taxonomy / Clustering in NLP**
  - Why needed here: The core output is a taxonomy of errors. Understanding how semantic similarity, hierarchical clustering, and category description clarity contribute to a useful taxonomy is necessary to evaluate ErrorAtlas's quality.
  - Quick check question: What are the trade-offs between a flat and a hierarchical taxonomy for error analysis? What metrics can be used to evaluate taxonomy quality?

## Architecture Onboarding
- Component map:
  1. **Input Data:** Benchmark datasets (input text, references, correct/incorrect model predictions).
  2. **Failure Filter:** A threshold-based component (or binary metric) to identify incorrect predictions.
  3. **Stage 1: Per-Instance Error Analyzer (LLM Judge):** A large, instruction-tuned LLM (e.g., gpt-oss-120b) with a detailed, structured prompt. Outputs a JSON with error label and analysis.
  4. **Error Label Aggregator:** Collects unique error labels and their frequencies.
  5. **Stage 2.a: Taxonomy Generator (LLM Clusterer):** An LLM with prompts for category generation, iterative refinement, and final review. Uses parameters like `batch_size`, `max_num_clusters`.
  6. **Stage 2.b: Error Classifier (LLM Classifier):** An LLM that assigns each error instance to a category from the generated taxonomy.
  7. **Taxonomy Store / ErrorAtlas:** The final, hierarchical JSON structure of error categories.
  8. **Downstream Analysis:** Visualization tools (e.g., donut charts of error distributions) and comparative scripts.

- Critical path:
  1. **Data Preparation & Filtering:** Load benchmark data, run models, filter for incorrect predictions using thresholds.
  2. **Stage 1 Execution (Parallelizable):** For each failed instance, call the analyst LLM. This is the most computationally expensive step.
  3. **Aggregation:** Collect all `error_title` strings.
  4. **Taxonomy Construction (One-time for ErrorAtlas):** Run the iterative LLM clustering pipeline (2.a) to generate categories, then (2.b) to assign all errors.
  5. **Application (for new models):** For a new model, run Stage 1, then only Stage 2.b using the static ErrorAtlas categories.

- Design tradeoffs:
  - **Custom vs. Static Taxonomy:** ErrorMap allows for dynamic, dataset-specific taxonomies, which may be more precise. ErrorAtlas provides a static, general taxonomy for easier comparison and replicability but may miss niche errors.
  - **Sample Size vs. Compute Cost:** The paper uses ~10% sampling to manage cost (Page 4). This introduces potential bias if the error distribution is highly skewed.
  - **Analyst LLM Choice:** A more capable LLM (e.g., gpt-oss-120b) gives better analysis but costs more. A smaller model is cheaper but may produce noisier labels.

- Failure signatures:
  - **Judge Hallucination:** The analyst LLM invents a criterion or misinterprets the error, leading to a misleading label.
  - **Taxonomy Drift:** Over iterations, category names or descriptions shift, leading to ambiguity or overlap.
  - **Low Coverage:** The final taxonomy leaves too many errors in an "Other" or "Hard to Analyze" bucket, indicating missed patterns.
  - **Circular Evaluation:** Since LLMs judge LLMs, systemic biases (e.g., over-penalizing certain phrasing styles) could skew the taxonomy.

- First 3 experiments:
  1. **Taxonomy Construction & Validation:** Re-run the ErrorAtlas construction pipeline on a *different* random 10% sample of the same data. Compare the resulting high-level categories for overlap and consistency to measure robustness.
  2. **Stage 1 Accuracy Audit:** Manually review a sample (e.g., 50-100) of the analyst LLM's `error_summary` and `error_title` outputs for a few models. Check if the identified error makes sense and is correctly attributed.
  3. **Model Diff on a Single Benchmark:** Pick two model versions (e.g., Llama 3 vs. Llama 2) and a specific benchmark (e.g., a subset of MMLU-Pro). Apply the full ErrorMap pipeline to generate a dedicated taxonomy and compare their error distributions to validate the "behavioral model-diff" use case.

## Open Questions the Paper Calls Out
- **Question:** To what extent does the reliance on an LLM as the "analyst" introduce systematic biases or blind spots in the resulting error taxonomy compared to human expert analysis?
  - Basis in paper: [explicit] Section F acknowledges the "somewhat circular" nature of using LLMs to analyze LLMs and notes that the assumption of validity "may not always hold in practice."
  - Why unresolved: While the authors validate accuracy using a meta-judge (another LLM), they do not provide a large-scale comparison against a ground-truth taxonomy generated entirely by human experts.
  - What evidence would resolve it: A comparative study evaluating the overlap and divergence between ErrorMap-generated categories and those created by human annotators on the same set of model failures.

- **Question:** How can the framework be adapted to robustly handle "soft" error categories where a single instance reflects multiple overlapping failure modes?
  - Basis in paper: [explicit] Section F states that "error categories are inherently soft" and admits that "a single mistake may reflect multiple underlying issues" (e.g., factual vs. conceptual errors).
  - Why unresolved: The current method focuses on identifying the "first major error" to create a clean hierarchy, potentially masking complex, multi-causal failures in the final taxonomy.
  - What evidence would resolve it: Developing a multi-label classification mechanism for the "Error Assignment" stage and measuring the diagnostic utility of a non-exclusive taxonomy versus the current exclusive one.

- **Question:** Is a static, general taxonomy like ErrorAtlas sufficient for diagnosing failures in highly specialized domains, or are bespoke taxonomies strictly necessary for accuracy?
  - Basis in paper: [explicit] Section F mentions that despite efforts to create a diverse taxonomy, "there may be cases where [ErrorAtlas] does not represent certain specific domains well."
  - Why unresolved: The paper validates ErrorAtlas on a broad mix of benchmarks but does not test the coverage depth or granularity of the taxonomy in narrow, high-stakes fields (e.g., specific sub-fields of law or medicine).
  - What evidence would resolve it: Applying ErrorAtlas to a specialized domain dataset and comparing its coverage rate (percentage of non-"Other" classifications) against a custom, domain-specific taxonomy generated by ErrorMap.

## Limitations
- **Dependence on LLM Judge:** Relies on a strong LLM (e.g., `gpt-oss-120b`) which may not be replicable with publicly available models, affecting accuracy and cost.
- **Circular Evaluation Risk:** Using LLMs to judge LLMs introduces potential systemic biases that could skew the taxonomy.
- **Static Taxonomy Coverage:** ErrorAtlas may not cover highly specialized domains, requiring bespoke taxonomies for niche applications.

## Confidence
- **High:** ErrorMap's core methodology (LLM-as-judge with structured prompts) is technically sound and reproducible with appropriate models.
- **Medium:** ErrorAtlas taxonomy quality and coverage claims, given dependence on proprietary models and sampling strategy.
- **Medium:** Practical utility claims for model debugging and selection, pending broader validation across task types.

## Next Checks
1. **Robustness Audit:** Re-run ErrorAtlas construction on a different random 10% sample of the same data and compare high-level category overlap to measure taxonomy stability.
2. **Evaluator Substitution Test:** Replace `gpt-oss-120b` with a strong open model (e.g., GPT-4o or Claude 3.5 Sonnet) and measure changes in coverage, accuracy, and taxonomy coherence.
3. **Novel Task Coverage Test:** Apply ErrorAtlas to a dataset outside the original 35 (e.g., a new coding or multimodal benchmark) and quantify the proportion of errors that fall into "Other" or "Hard to Analyze" categories.