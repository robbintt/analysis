---
ver: rpa2
title: Unsupervised Elicitation of Language Models
arxiv_id: '2506.10139'
source_url: https://arxiv.org/abs/2506.10139
tags:
- labels
- arxiv
- human
- unsupervised
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Internal Coherence Maximization (ICM), a
  novel unsupervised method to elicit specific capabilities from pretrained language
  models without relying on external supervision. The core idea is to fine-tune models
  on their own generated labels, optimizing for mutual predictability (how well each
  label can be inferred from others) and logical consistency (avoiding degenerate
  solutions).
---

# Unsupervised Elicitation of Language Models

## Quick Facts
- arXiv ID: 2506.10139
- Source URL: https://arxiv.org/abs/2506.10139
- Reference count: 39
- One-line primary result: ICM matches or exceeds golden-labeled models on standard NLP tasks using only self-generated labels.

## Executive Summary
This paper introduces Internal Coherence Maximization (ICM), a novel unsupervised method to elicit specific capabilities from pretrained language models without relying on external supervision. The core idea is to fine-tune models on their own generated labels, optimizing for mutual predictability and logical consistency. ICM uses a simulated annealing-inspired search algorithm to find high-quality label sets. On standard NLP tasks like GSM8K-verification, TruthfulQA, and Alpaca reward modeling, ICM matches or exceeds performance of models trained with golden labels and significantly outperforms those trained with crowdsourced human labels.

## Method Summary
ICM maximizes a utility function combining mutual predictability (how well each label can be inferred from others) and logical consistency (avoiding degenerate solutions). The algorithm initializes with random labels, then iteratively proposes new labels via in-context learning, fixing inconsistencies and accepting changes based on simulated annealing. For large-scale applications, a scalable variant replaces exact in-context learning with fine-tuning. The method was evaluated on GSM8K-verification, TruthfulQA, and Alpaca tasks, demonstrating competitive performance against supervised baselines.

## Key Results
- ICM matches golden-labeled models on GSM8K-verification and TruthfulQA benchmarks
- ICM outperforms crowdsourced human labels on Alpaca preference ranking
- ICM-derived reward models enable training a Claude 4 Sonnet assistant matching human-supervised counterparts
- ICM succeeds on superhuman tasks like author gender prediction where LMs exceed human performance

## Why This Works (Mechanism)

### Mechanism 1: Mutual Predictability as Concept Coherence
Labels maximizing conditional predictability among examples reflect coherent concepts already latent in the pretrained model. This works by computing P_θ(y_i | x_i, D \ {(x_i, y_i)}) via in-context learning and summing log-probabilities across all examples. The core assumption is that pretrained LMs encode rich representations of target concepts recoverable through conditional probability structure.

### Mechanism 2: Logical Consistency Prevents Degenerate Solutions
Axiomatic constraints block trivial solutions by penalizing label pairs violating logical rules (e.g., "A > B and B > A cannot both be True"). The consistency function c(x_i, y_i, x_j, y_j) ∈ {0,1} is subtracted from the predictability score. This is critical because mutual predictability alone allows degenerate solutions like assigning the same label to all data points.

### Mechanism 3: Annealed Search Enables Approximate Global Optimization
Simulated annealing with iterative label correction approximates the intractable integer program of maximizing U(D). The algorithm initializes K random labels (K≈8 balances demonstration vs. noise) and iteratively samples examples, proposes labels, fixes inconsistencies, and accepts changes based on temperature. This enables exploration of the discrete label assignment space while converging to high-scoring configurations.

## Foundational Learning

- **In-Context Learning with Conditional Probability Estimation**: Computing P_θ(y_i | x_i, other labeled examples) via few-shot prompting is critical for mutual predictability. Quick check: Given a pretrained LM and (input, label) pairs, can you compute the log-probability of a candidate label for a new input?

- **Combinatorial Optimization via Simulated Annealing**: The metaheuristic search over discrete label assignments requires understanding temperature schedules and acceptance criteria. Quick check: Explain why simulated annealing accepts worse solutions with probability exp(Δ/T) and how cooling affects exploration vs. exploitation.

- **Logical Consistency Constraints in Classification**: Defining c(x_i, y_i, x_j, y_j) correctly is task-specific and critical for preventing degenerate solutions. Quick check: For a pairwise preference task (A > B), what consistency constraint should hold between labels for pairs (A, B) and (B, A)?

## Architecture Onboarding

- **Component map**: Data Preparation -> Initialization Module -> Scoring Engine -> Search Loop -> Output
- **Critical path**: Initialization → Search loop → Convergence check. The ConsistencyFix subroutine (Alg. 2) runs after every label proposal and is O(M · |consistency_groups|).
- **Design tradeoffs**: K (initial labels): larger K adds noise but provides more demonstrations; α (predictability weight): higher α prioritizes mutual predictability but may restrict search; Temperature schedule: aggressive cooling speeds convergence but risks local optima.
- **Failure signatures**: Uniform label assignments indicate weak consistency weight; slow convergence suggests α too high or sampling biased; poor zero-shot mutual predictability indicates concept not salient in pretrained model.
- **First 3 experiments**: 
  1. Sanity check on synthetic data with known golden labels (N=50, accuracy >85%)
  2. Ablation on consistency by running ICM with and without logical consistency term
  3. Scaling test comparing ICM performance on TruthfulQA using different model sizes

## Open Questions the Paper Calls Out
- Can a diagnostic metric be developed to predict whether a target concept is sufficiently "salient" in a pretrained model for ICM to successfully elicit it?
- To what extent does ICM's strong performance on standard benchmarks rely on data contamination (memorization) from the pretraining phase rather than genuine capability elicitation?
- Why does ICM-derived supervision underperform human supervision on "crisp" reasoning tasks (math/code) while outperforming it on "fuzzy" tasks (safety/chat)?

## Limitations
- ICM requires target concepts to be already encoded in the pretrained model's representations - it cannot elicit arbitrary capabilities
- Performance appears sensitive to consistency rule specification and search hyperparameters
- The method may not generalize to all NLP tasks, with some domains showing average parity but higher variance in real-world deployment

## Confidence
**High Confidence**: ICM's core mechanism is well-supported by empirical results and ablation studies
**Medium Confidence**: Claims of matching or exceeding supervised models are supported on tested benchmarks but may not generalize universally
**Low Confidence**: The assertion about superhuman task performance lacks strong empirical support in the paper

## Next Checks
1. Apply ICM to a diverse set of 5-10 additional NLP tasks spanning different domains to assess broad applicability
2. Replicate production assistant experiments with different base models to validate scalable variant trade-offs
3. Systematically test ICM on tasks where the target concept is progressively less salient in the pretrained model to characterize elicitation limits