---
ver: rpa2
title: Surrogate Ensemble in Expensive Multi-Objective Optimization via Deep Q-Learning
arxiv_id: '2602.00540'
source_url: https://arxiv.org/abs/2602.00540
tags:
- uni00000013
- optimization
- uni00000011
- uni00000014
- e-01
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SEEMOO, a reinforcement learning-assisted surrogate
  ensemble framework for expensive multi-objective optimization (EMOO). The key idea
  is to use a deep Q-network to dynamically schedule different surrogate models (GP,
  MLPs, KNN, RBFN) during the optimization process based on the current optimization
  state.
---

# Surrogate Ensemble in Expensive Multi-Objective Optimization via Deep Q-Learning

## Quick Facts
- arXiv ID: 2602.00540
- Source URL: https://arxiv.org/abs/2602.00540
- Reference count: 40
- This paper proposes SEEMOO, a reinforcement learning-assisted surrogate ensemble framework for expensive multi-objective optimization (EMOO).

## Executive Summary
This paper introduces SEEMOO, a reinforcement learning-assisted surrogate ensemble framework for expensive multi-objective optimization. The framework uses a deep Q-network to dynamically select among different surrogate models (GP, MLPs, KNN, RBFN) during optimization based on the current optimization state. By enabling adaptive model selection, SEEMOO addresses the limitation of static surrogate selection in existing surrogate-assisted evolutionary algorithms (SAEAs). The approach was evaluated on 24 benchmark problems and demonstrated superior performance compared to single-surrogate baselines.

## Method Summary
SEEMOO implements a bi-level meta-black-box optimization framework where a DQN meta-controller selects surrogate models for a base NSGA-II optimizer. The surrogate pool consists of five models: Gaussian Process with composite kernel, two MLPs, KNN, and RBFN. The DQN uses an 8-dimensional state vector per model capturing usage history, IGD improvement, convergence metrics, and surrogate error variation. The policy is trained with epsilon-greedy exploration, experience replay, and IGD-based rewards. The framework was tested on 24 benchmark problems with 50-dimensional initial populations, 100 optimization steps, and 550 maximum function evaluations.

## Key Results
- SEEMOO achieved an average rank of 2.67 across 24 benchmark problems, outperforming ablation variants (ranks 4.50-4.56)
- IGD convergence curves showed consistent improvement across ZDT, DTLZ, WFG, and other problem families
- Ablation studies confirmed each surrogate model's contribution, with performance degrading when any model was removed
- Dense IGD-based rewards outperformed sparse binary and HV-based alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A learned DQN policy can adaptively select surrogate models based on real-time optimization state, outperforming static selection.
- Mechanism: An 8-dimensional state vector per candidate model captures usage history, IGD improvement, convergence dynamics, and search progress. The DQN maps this state to Q-values, selecting the surrogate with highest expected value. The policy learns via experience replay to maximize cumulative IGD improvement.
- Core assumption: Surrogate effectiveness varies across optimization phases and problem landscapes in a learnable pattern.
- Evidence anchors:
  - [abstract]: "a deep Q-network serves as dynamic surrogate selector: Given the optimization state, it selects desired surrogate model for current-step evaluation"
  - [section 3.3.1]: State features include cumulative usage, recent frequency, model-specific improvement, global step-wise improvement, decision/objective space convergence, surrogate error variation, and step ratio
  - [corpus]: Related work "BOFormer" applies RL to multi-objective Bayesian optimization, but corpus lacks direct comparative evidence for DQN-based surrogate scheduling

### Mechanism 2
- Claim: A heterogeneous surrogate pool provides complementary modeling capabilities; removing any model degrades performance.
- Mechanism: GP provides uncertainty quantification and local precision; MLPs capture global feature patterns; KNN handles irregular local structures; RBFN offers smooth interpolation. The DQN learns when each bias is advantageous.
- Core assumption: No single surrogate architecture is optimal across all optimization phases and problem types.
- Evidence anchors:
  - [abstract]: "A pre-collected model pool that maintains different surrogate models"
  - [section 4.3.1 Table 2]: Ablation shows w/o GP drops to rank 4.50, w/o MLPs to 3.83, w/o KNN to 3.61, w/o RBFN to 4.00—each removal degrades performance
  - [corpus]: "Large Language Model-Driven Surrogate-Assisted Evolutionary Algorithm" uses single LLM-based surrogate; corpus suggests ensemble approaches remain underexplored

### Mechanism 3
- Claim: Dense IGD-based reward signals outperform sparse binary or HV-based alternatives for policy learning.
- Mechanism: Reward r_t = (IGD_{t-1} - IGD_t) / (IGD_max - IGD_min) provides normalized, continuous feedback. Positive reward when archive improves; negative when it degrades. This creates stable gradients for DQN training.
- Core assumption: IGD improvement correlates with effective surrogate selection and generalizes across problem scales.
- Evidence anchors:
  - [section 3.3.3 Eq. 9]: Explicit reward formulation with normalization
  - [section 4.3.2 Table 2]: Binary reward averages rank 4.44, HV reward 4.56, IGD reward 2.67—demonstrating dense IGD signal superiority
  - [corpus]: "In-Context Multi-Objective Optimization" discusses multi-objective acquisition functions but corpus lacks comparative reward design studies

## Foundational Learning

- Concept: Multi-objective optimization fundamentals (Pareto fronts, IGD, hypervolume)
  - Why needed here: IGD serves as both evaluation metric and reward signal; understanding convergence-diversity trade-offs is essential.
  - Quick check question: Why does IGD require a reference Pareto front, and what happens if the reference is poor quality?

- Concept: Deep Q-Learning (Q-functions, experience replay, epsilon-greedy)
  - Why needed here: The meta-policy is a DQN; debugging requires understanding Q-value interpretation and training dynamics.
  - Quick check question: What does a Q-value of 0.3 for selecting GP versus 0.1 for MLP imply about the learned policy?

- Concept: Surrogate-assisted evolutionary algorithms (model training, infill criteria, evaluation budgets)
  - Why needed here: SEEMOO wraps a surrogate-assisted NSGA-II; understanding the base optimizer clarifies what the meta-policy controls.
  - Quick check question: Why must true evaluations still occur even with accurate surrogates?

## Architecture Onboarding

- Component map: Problem → LHS Initialization → True Eval → Database T → State Extractor → DQN Agent → Action (select surrogate M_i) → Surrogate M_i trained on T → NSGA-II evolution → Infill selection → True eval batch → Update T + Archive → Reward computation → Experience buffer → DQN update

- Critical path:
  1. State extraction quality (8 features per model) determines policy input
  2. Surrogate pool diversity determines action space utility
  3. Reward normalization stability determines cross-problem learning
  4. Epsilon decay schedule (0.8 → 0.01) balances exploration-exploitation

- Design tradeoffs:
  - Pool size (5 models) vs. decision complexity: More models increase flexibility but dilute per-model experience
  - State dimension (8 × m) vs. generalization: Richer features may overfit training distribution
  - Batch size (b=5) vs. budget consumption: Larger batches provide more data per step but exhaust budget faster
  - Training epochs (50) vs. compute cost: More epochs improve policy but require substantial optimization runs

- Failure signatures:
  - Policy collapses to single model: Check reward distribution skew, epsilon schedule too aggressive
  - IGD stagnates early: Verify surrogate training data quality, check archive update logic
  - Poor test generalization: State features may encode problem-specific patterns; increase training diversity
  - Q-values diverge: Monitor gradient magnitudes, reduce learning rate, increase buffer size

- First 3 experiments:
  1. **Baseline sanity check**: Run GP-only, MLP-only, KNN-only, RBFN-only on 3 ZDT problems to verify single-surrogate performance bounds and NSGA-II integration correctness.
  2. **State feature ablation**: Train SEEMOO variants with subsets of state features (e.g., only usage history vs. only convergence metrics) to identify which features drive policy decisions.
  3. **Minimal pool validation**: Start with 2-model pool (GP + RBFN), measure IGD, then incrementally add models to confirm each provides marginal benefit without introducing instability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SEEMOO perform on real-world expensive multi-objective optimization problems compared to synthetic benchmarks?
- Basis in paper: Future work states: "validating the framework's practical utility and generalization capability on real-world engineering optimization tasks."
- Why unresolved: Experiments are limited to 24 synthetic benchmarks (ZDT, DTLZ, WFG, etc.) with known Pareto fronts; real-world problems have unknown landscape characteristics and may challenge the learned policy.
- What evidence would resolve it: Evaluation on engineering optimization tasks (e.g., aerodynamic shape optimization, neural architecture search) with comparisons to baseline SAEAs.

### Open Question 2
- Question: Can SEEMOO's surrogate scheduling mechanism generalize to many-objective problems with more than 5 objectives?
- Basis in paper: Tested problems have max 5 objectives, but the attention-based state extractor claims to "support universal optimization state representation of problems with varied objective numbers."
- Why unresolved: The compositional landscape complexity and decision-space sparsity increase substantially with more objectives, potentially degrading the DQN policy's effectiveness.
- What evidence would resolve it: Benchmarking on MaF or other many-objective test suites (10+ objectives) with IGD/HV comparisons.

### Open Question 3
- Question: What is the computational overhead of the DQN-based scheduling relative to surrogate model training?
- Basis in paper: The paper claims efficiency but does not report wall-clock times for the meta-level RL agent versus surrogate evaluations.
- Why unresolved: The attention mechanism and DQN inference add computational cost; whether this overhead is negligible compared to expensive function evaluations is unclear.
- What evidence would resolve it: Detailed timing analysis comparing SEEMOO's total runtime to single-surrogate baselines across problem scales.

### Open Question 4
- Question: Does SEEMOO transfer effectively to other MOEA architectures beyond NSGA-II?
- Basis in paper: Future work notes: "integrating SEEMOO with various MOEA architectures."
- Why unresolved: The current design assumes NSGA-II's selection and variation operators; decomposition-based (MOEA/D) or indicator-based (IBEA) algorithms have different state dynamics.
- What evidence would resolve it: Cross-MOEA experiments showing SEEMOO improves GP, MOEA/D-EGO, or other SAEA variants.

## Limitations
- The framework's performance on real-world problems with unknown Pareto fronts and different landscape characteristics remains untested
- Computational overhead from the DQN-based scheduling and attention mechanism is not quantified relative to surrogate evaluation costs
- The 5-model ensemble may be excessive for problems with uniform landscape characteristics favoring a single model

## Confidence
- Primary claim (SEEMOO improves EMOO performance): Medium confidence
- Surrogate pool diversity necessity: Medium confidence
- IGD-based dense reward superiority: Medium confidence
- DQN policy learns generalizable surrogate selection patterns: Medium confidence

## Next Checks
1. **Cross-problem generalization test**: Evaluate SEEMOO on a held-out set of real-world expensive optimization problems (e.g., aerodynamic design, materials discovery) to verify that learned policies transfer beyond synthetic benchmarks.

2. **Surrogate pool sensitivity analysis**: Systematically vary the number and composition of surrogates (2-8 models) to identify the minimum effective ensemble size and test whether domain-specific surrogate selection improves performance on particular problem families.

3. **Robustness to IGD instability**: Introduce controlled noise into reference Pareto front estimation and test whether the DQN policy degrades gracefully, comparing IGD-based rewards against alternative dense signals like hypervolume improvement rate.