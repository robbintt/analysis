---
ver: rpa2
title: Mathematical analysis of the gradients in deep learning
arxiv_id: '2501.15646'
source_url: https://arxiv.org/abs/2501.15646
tags:
- holds
- function
- lemma
- item
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a rigorous mathematical framework for describing
  the generalized gradients used in the training of deep fully-connected feedforward
  artificial neural networks (ANNs), particularly when non-differentiable activation
  functions like the ReLU function are employed. The authors establish that these
  generalized gradients are uniquely defined through an appropriate approximation
  procedure and prove that they coincide with the standard gradient of the cost functional
  on every open set where the cost functional is continuously differentiable.
---

# Mathematical analysis of the gradients in deep learning

## Quick Facts
- arXiv ID: 2501.15646
- Source URL: https://arxiv.org/abs/2501.15646
- Authors: Steffen Dereich; Thang Do; Arnulf Jentzen; Frederic Weber
- Reference count: 40
- Primary result: Establishes rigorous mathematical framework for generalized gradients in deep learning with non-differentiable activation functions

## Executive Summary
This paper provides a rigorous mathematical framework for describing the generalized gradients used in training deep fully-connected feedforward artificial neural networks, particularly when non-differentiable activation functions like ReLU are employed. The authors establish that these generalized gradients are uniquely defined through an appropriate approximation procedure and prove they coincide with standard gradients on regions where the cost functional is continuously differentiable. The work bridges the practical implementation of gradient-based training methods in deep learning libraries with their mathematical foundations.

## Method Summary
The authors develop a mathematical framework based on generalized gradients and Fréchet subgradients to characterize the gradients used in deep learning optimization. They employ approximation procedures to establish uniqueness of generalized gradients and demonstrate their relationship to standard gradients on continuously differentiable regions of the cost functional. The analysis specifically addresses ReLU activation functions and their generalizations, providing theoretical justification for their use in stochastic gradient descent optimization methods.

## Key Results
- Generalized gradients used in deep learning are uniquely defined through appropriate approximation procedures
- These generalized gradients coincide with standard gradients of the cost functional on every open set where the cost functional is continuously differentiable
- The generalized gradients are limiting Fréchet subgradients of the cost functional, providing theoretical justification for their use in optimization

## Why This Works (Mechanism)
The paper's approach works by establishing a rigorous mathematical foundation for the gradients used in deep learning optimization. By defining generalized gradients through approximation procedures and proving their relationship to Fréchet subgradients, the authors provide a theoretical framework that justifies the practical implementation of gradient-based training methods in deep learning libraries.

## Foundational Learning

1. **Generalized Gradients**: Why needed - to handle non-differentiable activation functions in deep learning; Quick check - verify that the generalized gradient reduces to the standard gradient when the function is differentiable.

2. **Fréchet Subgradients**: Why needed - to provide a rigorous mathematical framework for optimization; Quick check - confirm that limiting Fréchet subgradients capture the behavior of practical optimization methods.

3. **Approximation Procedures**: Why needed - to establish uniqueness of generalized gradients; Quick check - verify that the approximation procedure converges to a unique limit.

4. **Cost Functionals**: Why needed - to characterize the optimization objective; Quick check - ensure the cost functional satisfies necessary differentiability conditions on open sets.

## Architecture Onboarding

Component Map:
Input Layer -> Hidden Layers (with ReLU activation) -> Output Layer -> Cost Functional

Critical Path:
The critical path for gradient computation flows from the output layer through backpropagation, computing gradients at each layer using the generalized gradient framework. This path is essential for parameter updates during training.

Design Tradeoffs:
The framework assumes fully-connected feedforward architectures, which provides mathematical tractability but limits applicability to modern architectures like convolutional networks or transformers. The focus on ReLU activation functions enables rigorous analysis but may not generalize to all non-differentiable activations.

Failure Signatures:
- Non-uniqueness of generalized gradients indicates problems with the approximation procedure
- Failure of cost functional to be continuously differentiable on open sets suggests limitations in the theoretical framework
- Discontinuities in the gradient computation may indicate numerical instability

First Experiments:
1. Verify the uniqueness of generalized gradients through numerical experiments with different approximation procedures
2. Compare generalized gradients with subgradients across various loss functions and network configurations
3. Test the stability and convergence of optimization using generalized gradients in practical deep learning scenarios

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- The framework is built on fully-connected feedforward architectures, limiting applicability to modern deep learning structures
- Analysis focuses specifically on ReLU activation functions and their generalizations, leaving questions about other non-differentiable activations
- The assumption of continuously differentiable cost functionals may not always hold in practice

## Confidence

High:
- Mathematical proofs establishing relationship between generalized gradients and Fréchet subgradients
- Claims about uniqueness of generalized gradients through approximation procedures

Medium:
- Claims about practical implications of generalized gradients in real-world optimization
- Applicability of results to different loss functions and network configurations

## Next Checks
1. Test the framework's applicability to modern network architectures beyond fully-connected feedforward networks, particularly convolutional and transformer architectures.
2. Validate the theoretical predictions through extensive numerical experiments comparing generalized gradients with subgradients across different loss functions and network configurations.
3. Investigate the stability and convergence properties of optimization using these generalized gradients in practical deep learning scenarios, particularly in cases where the cost functional may not be continuously differentiable.