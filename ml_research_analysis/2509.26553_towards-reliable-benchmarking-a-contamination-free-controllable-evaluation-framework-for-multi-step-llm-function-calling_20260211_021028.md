---
ver: rpa2
title: 'Towards Reliable Benchmarking: A Contamination Free, Controllable Evaluation
  Framework for Multi-step LLM Function Calling'
arxiv_id: '2509.26553'
source_url: https://arxiv.org/abs/2509.26553
tags:
- func
- function
- nodes
- core
- irrelevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FuncBenchGen, a contamination-free framework
  for evaluating multi-step function calling in large language models. It generates
  synthetic tasks represented as DAGs of function dependencies, allowing precise control
  over task difficulty including dependency depth and presence of irrelevant functions.
---

# Towards Reliable Benchmarking: A Contamination Free, Controllable Evaluation Framework for Multi-step LLM Function Calling

## Quick Facts
- arXiv ID: 2509.26553
- Source URL: https://arxiv.org/abs/2509.26553
- Authors: Seiji Maekawa; Jackson Hassell; Pouya Pezeshkpour; Tom Mitchell; Estevam Hruschka
- Reference count: 25
- Key outcome: Introduces FuncBenchGen, a contamination-free framework for evaluating multi-step function calling in LLMs using synthetic DAG-based tasks

## Executive Summary
This paper introduces FuncBenchGen, a framework for evaluating multi-step function calling in large language models using synthetically generated tasks. Unlike static benchmarks that risk data contamination, FuncBenchGen generates function schemas with randomized names and type annotations on-the-fly, ensuring evaluation integrity. The framework represents tasks as directed acyclic graphs (DAGs) where nodes are function calls and edges represent data dependencies, allowing precise control over task difficulty through parameters like core function count, dependency depth, and irrelevant function presence.

The authors evaluate seven leading LLMs across various configurations and identify key failure modes, with "Value Not Yet Known" errors accounting for 66-81% of failures. They demonstrate that reasoning-optimized models like GPT-5 significantly outperform general-purpose models but struggle with longer function call sequences, especially when irrelevant functions share type-compatible variables. A simple mitigation strategy that restates all known variable values with each function output improves GPT-5's success rate from 62.5% to 81.3%, though not all models benefit equally from this approach.

## Method Summary
FuncBenchGen generates synthetic multi-step tool-use tasks represented as directed acyclic graphs (DAGs) of function dependencies. The framework creates function schemas with randomized names and type annotations on-the-fly, avoiding data contamination from pretraining corpora. Tasks are instantiated from configurable DAG structures with parameters controlling core nodes, dependency depth, connected irrelevant nodes (CINs), and disconnected irrelevant nodes (DINs). The execution environment validates function calls against schemas, executes logic, and returns outputs while tracking variable state. Models are evaluated on their ability to correctly sequence function calls to compute target variable values, with success measured by correct output and efficiency measured by average function calls used.

## Key Results
- GPT-5 (reasoning-optimized) achieves 81.3% success rate on difficult tasks with variable restatement, outperforming general-purpose models significantly
- "Value Not Yet Known" errors dominate failures (66-81% across all models), indicating state-tracking challenges
- Connected irrelevant functions (CINs) prove more challenging than disconnected ones, exploiting type-compatible variable sharing
- Variable restatement improves GPT-5 from 62.5% to 81.3% success, but shows mixed results for other models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic task generation at evaluation time prevents data contamination.
- Mechanism: FuncBenchGen generates function schemas with randomized names and type annotations on-the-fly, ensuring no overlap with pretraining corpora. Tasks are instantiated from configurable DAG structures rather than retrieved from static datasets.
- Core assumption: Models have not been trained on synthetic function schemas that are generated with random identifiers; contamination primarily arises from memorized question–answer pairs rather than general reasoning patterns.
- Evidence anchors:
  - [abstract] "FuncBenchGen...generates synthetic multi-step tool-use tasks...while avoiding data leakage."
  - [section 3.2] "This on-the-fly generation ensures contamination-free evaluation."
  - [corpus] Neighbor paper KBE-DME (arXiv:2510.21182) addresses contamination in static benchmarks but does not provide direct validation of FuncBenchGen's specific approach.
- Break condition: If model pretraining included procedurally generated code with similar DAG-constrained call patterns, synthetic generation may not guarantee contamination-freeness.

### Mechanism 2
- Claim: Explicit variable restatement mitigates brittle state tracking in multi-turn tool use.
- Mechanism: By returning all known variable values with each function output, the framework reduces working-memory load and helps models avoid "Value Not Yet Known" errors—the most common failure mode (66–81% of errors across models).
- Core assumption: The primary bottleneck is attention/state drift across turns, not fundamental inability to reason about dependencies; providing redundant context does not introduce confusion.
- Evidence anchors:
  - [abstract] "...simple mitigation strategy that explicitly restates prior variable values...yields substantial gains...improvement from 62.5% to 81.3% for GPT-5."
  - [section 4.6] "Value Not Yet Known: The model tries to use a variable whose value has not been established...accounting for over 66% of failures in every model."
  - [corpus] No direct corpus evidence on state-tracking mitigation in function calling; neighbor papers focus on contamination and routing rather than state management.
- Break condition: If variable restatement expands context length beyond model's effective attention window for very long sequences, benefits may reverse.

### Mechanism 3
- Claim: Connected irrelevant functions (CINs) degrade performance more than disconnected irrelevant functions (DINs) by exploiting type-compatible variable sharing.
- Mechanism: CINs share type signatures with core nodes, forcing models to distinguish semantic relevance from mere type compatibility. This increases decision complexity at each branching point in the call sequence.
- Core assumption: Models use type signatures as a primary signal for function selection; when type-compatible distractors exist, models cannot reliably filter by semantic task relevance.
- Evidence anchors:
  - [abstract] "...connected distractors—irrelevant functions sharing type-compatible variables with relevant functions—prove especially difficult to handle."
  - [section 4.3] "CINs have the most negative impact on model performance compared to other types across most tested LLMs."
  - [corpus] Weak external validation; corpus papers address contamination and routing but not the CIN vs. DIN distinction.
- Break condition: If models were fine-tuned with explicit task-relevance classifiers or tool-documentation embeddings, the CIN penalty might diminish.

## Foundational Learning

- Concept: **Directed Acyclic Graphs (DAGs) for task decomposition**
  - Why needed here: Multi-step function calling is formalized as DAG traversal, where nodes are function calls and edges represent data dependencies.
  - Quick check question: Given three functions A→B→C, what is the minimum number of sequential calls needed if B depends on A's output and C depends on B's?

- Concept: **Type and subtype signatures in function linking**
  - Why needed here: FuncBenchGen links functions via type/subtype matching rather than variable names, simulating real-world semantic compatibility.
  - Quick check question: If function F1 outputs type `User` with subtype `Admin`, which input signature should F2 accept to consume F1's output?

- Concept: **State tracking in multi-turn agent loops**
  - Why needed here: The dominant failure mode is using unknown or stale variable values across turns; understanding state propagation is essential for debugging and mitigation.
  - Quick check question: After calling `get_user(id=5)` returns `{name: "Alice", role: "admin"}`, what state must be available before calling `delete_user(user=?)`?

## Architecture Onboarding

- Component map:
  - Graph Generator -> Function Schema Creator -> Execution Environment -> Failure Classifier
  - Models (GPT-5, GPT-4.1, Gemini-2.5, Qwen3) -> Execution Environment -> Task Success/Failure

- Critical path:
  1. Configure difficulty parameters (core nodes, depth, CINs, DINs)
  2. Generate DAG and function schemas
  3. Present initial variables and function list to model
  4. Model iteratively calls functions; environment validates and returns outputs
  5. Terminate when model outputs target variable value or exhausts call budget

- Design tradeoffs:
  - Type-based linking vs. name-based: Type-based increases realism but introduces CIN confounds; name-based simplifies tasks
  - Variable restatement: Improves accuracy but increases token usage; critical for long sequences, optional for short
  - Random function names: Ensures contamination-freeness but reduces ecological validity compared to realistic API names

- Failure signatures:
  - Value Not Yet Known (66–81%): Model calls function with variable not yet produced—indicates planning or state-tracking gap
  - Incorrect Value (17–33%): Model uses wrong value for known variable—indicates retrieval error
  - Function Not Found / Wrong Input Count (<1%): Rare; models generally respect schema constraints

- First 3 experiments:
  1. Baseline sweep: Run all models with n_core ∈ {5, 10, 20}, depth ∈ {1, n_core−1}, no irrelevant nodes; establish per-model depth-scaling curves
  2. CIN vs. DIN ablation: For fixed n_core=10, vary n_conn ∈ {0, 5, 10} and n_dis ∈ {0, 5, 10} in factorial design; quantify CIN penalty
  3. Mitigation validation: Enable variable restatement; re-run best model (GPT-5) on highest-difficulty configuration; compare success rates and call efficiency against baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs effectively handle multi-step function calling with complex control flows such as conditional logic and iteration?
- Basis in paper: [explicit] The conclusion states: "Incorporating more complex control flows such as conditional logic and iteration would be our interesting future direction."
- Why unresolved: The current framework only represents function dependencies as static DAGs, which cannot express branching paths or looping behavior common in real-world workflows.
- What evidence would resolve it: Extending FuncBenchGen to generate tasks with conditional branches and iterative calls, then evaluating model performance on these structures.

### Open Question 2
- Question: Why do some models (e.g., Gemini-2.5-Flash, Qwen3) fail to benefit from the variable restatement mitigation strategy?
- Basis in paper: [inferred] Section 4.6.1 notes these models show "a small improvement or a slight performance drop, suggesting that these models may struggle to effectively utilize the additional context provided."
- Why unresolved: The paper does not investigate whether the issue stems from context length limitations, attention mechanisms, or differences in training objectives across model families.
- What evidence would resolve it: Ablation studies varying the format and verbosity of restated values, combined with attention analysis on how different models process the additional context.

### Open Question 3
- Question: What architectural or algorithmic improvements would enable reliable function calling over hundreds of available functions, as required by real-world MCP servers?
- Basis in paper: [inferred] Section 4.5 notes that GPT-5's performance degrades below 10% with 40 core nodes even in the disconnected setting, and remarks that "even the best models are not ready to be used over a large function sets which is happening with existing MCP servers."
- Why unresolved: The paper evaluates scaling only up to 40 functions and does not propose or test methods for improving scalability.
- What evidence would resolve it: Testing hierarchical function retrieval, function clustering, or iterative planning approaches on generated benchmarks with 100+ functions.

## Limitations

- Contamination-free claim relies on unverified assumption that random synthetic schemas are unseen during pretraining
- Type-based linking introduces artificial confounds via connected irrelevant nodes that may not generalize to real-world API usage
- Variable restatement mitigation effectiveness at scale for very long call sequences remains untested
- Framework uses 3-digit random integers as variable values rather than semantically meaningful data, limiting ecological validity

## Confidence

**High confidence**: The contamination-free generation mechanism, the dominance of "Value Not Yet Known" errors as a failure mode, and the effectiveness of variable restatement for GPT-5 (62.5% → 81.3% improvement) are well-supported by direct experimental evidence.

**Medium confidence**: The claim that reasoning-optimized models outperform general-purpose models in function calling, and that CINs are more challenging than DINs, is supported by the experimental results but could benefit from additional validation across more model families and task domains.

**Low confidence**: The generalizability of the CIN/DIN distinction to real-world function calling scenarios and the long-term effectiveness of variable restatement for maintaining state in very long call sequences are speculative without additional testing.

## Next Checks

1. **Pretraining Corpus Analysis**: Analyze whether any LLM pretraining data contains procedurally generated code with DAG-constrained call patterns similar to FuncBenchGen's synthetic schemas to verify the contamination-free assumption.

2. **Real-World API Mapping**: Map a set of real-world API endpoints to FuncBenchGen's type-based linking mechanism to assess whether the CIN/DIN distinction and associated performance penalties reflect actual developer challenges.

3. **Scaling Study**: Evaluate the variable restatement mitigation strategy on sequences exceeding 20 function calls to determine if context length limitations or diminishing returns emerge at scale.