---
ver: rpa2
title: Multi-Domain Explainability of Preferences
arxiv_id: '2505.20088'
source_url: https://arxiv.org/abs/2505.20088
tags:
- concepts
- response
- user
- concept
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work proposes a fully automated method for generating interpretable,
  multi-domain concept-based explanations of preference mechanisms, such as human
  preferences, LLM-as-a-Judge (LaaJ), and reward models. The method involves four
  stages: (1) using an LLM to discover concepts that distinguish chosen from rejected
  responses per domain; (2) representing each query-response triplet as a concept
  vector; (3) training a white-box Hierarchical Multi-Domain Regression (HMDR) model
  to predict preferences; and (4) analyzing model weights to derive local and global
  explanations.'
---

# Multi-Domain Explainability of Preferences

## Quick Facts
- **arXiv ID**: 2505.20088
- **Source URL**: https://arxiv.org/abs/2505.20088
- **Reference count**: 40
- **Key outcome**: Proposes fully automated method for interpretable, multi-domain concept-based explanations of preference mechanisms with strong prediction accuracy and validation through human preference experiments

## Executive Summary
This paper introduces a novel method for generating interpretable explanations of preference mechanisms across multiple domains. The approach automatically discovers domain-specific concepts using LLMs, represents query-response pairs as concept vectors, and trains a white-box Hierarchical Multi-Domain Regression (HMDR) model to predict preferences while maintaining interpretability. The method is validated across eight diverse domains and twelve preference mechanisms, demonstrating strong predictive performance comparable to black-box alternatives while enabling interpretable explanations.

## Method Summary
The method consists of four stages: (1) LLM-based concept discovery to identify distinguishing features between chosen and rejected responses in each domain, (2) representation of query-response triplets as concept vectors capturing these discovered concepts, (3) training of a white-box HMDR model to predict preferences while explicitly optimizing for out-of-domain generalization, and (4) analysis of model weights to derive both local and global explanations. The HMDR architecture captures shared and domain-specific effects, enabling interpretable insights into preference mechanisms while maintaining competitive predictive accuracy.

## Key Results
- Achieved ~66% prediction accuracy for human preferences and ~80% average for LLM-as-a-Judge and reward models
- HMDR model performance comparable to black-box alternatives while maintaining interpretability
- Application-driven evaluations (Judge Hack and Tie Break) showed explanation-guided responses were preferred by judges and improved alignment with human preferences

## Why This Works (Mechanism)
The method succeeds by combining LLM-discovered domain concepts with a structured white-box model that can learn both shared and domain-specific patterns in preference data. The four-stage pipeline ensures that concept discovery is automated yet interpretable, representation captures relevant features, and the HMDR model maintains transparency while achieving competitive performance. The explicit optimization for out-of-domain generalization addresses a key challenge in preference modeling.

## Foundational Learning
- **LLM concept discovery**: Using LLMs to automatically identify distinguishing concepts between chosen and rejected responses - needed for scalable, domain-specific feature extraction; quick check: validate discovered concepts against human-annotated labels
- **Concept vector representation**: Encoding query-response pairs as vectors of discovered concepts - needed for structured model input; quick check: ensure vector dimensionality matches number of discovered concepts
- **Hierarchical multi-domain modeling**: Architecture that captures both shared and domain-specific effects - needed for generalization across domains; quick check: compare performance of shared vs. domain-specific parameters
- **White-box optimization**: Designing models that maintain interpretability while achieving competitive accuracy - needed for trust and validation; quick check: verify model weights align with intuitive concept importance
- **Out-of-domain generalization**: Explicit optimization for performance on unseen domains - needed for practical deployment; quick check: measure performance drop when testing on truly new domains
- **Explanation validation**: Using human preference experiments to verify explanation quality - needed for establishing method credibility; quick check: conduct blinded preference tests comparing explained vs. unexplained responses

## Architecture Onboarding

**Component Map**: LLM Concept Discovery -> Concept Vector Representation -> HMDR Model Training -> Explanation Analysis

**Critical Path**: The LLM discovers concepts → Triplets are represented as concept vectors → HMDR model learns preference patterns → Weights are analyzed for explanations

**Design Tradeoffs**: White-box HMDR sacrifices some predictive power for interpretability compared to black-box alternatives, but explicit out-of-domain optimization partially compensates for this limitation

**Failure Signatures**: 
- LLM bias in concept discovery affecting downstream explanations
- Poor concept representation leading to low prediction accuracy
- HMDR model unable to capture complex preference patterns
- Explanations not aligning with human intuition despite good predictions

**3 First Experiments**:
1. Validate concept discovery by comparing LLM-discovered concepts against human-annotated labels in a small sample
2. Test concept vector representation quality by measuring prediction accuracy with varying numbers of concepts
3. Evaluate HMDR model performance on in-domain vs. out-of-domain data to verify generalization claims

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Reliance on LLM-generated concept discovery introduces potential biases from the language model itself
- Evaluation focuses primarily on domains where LLMs can effectively represent concepts
- Small-scale human evaluations (80 pairs for Judge Hack, 5 tasks for Tie Break) may not generalize broadly

## Confidence
- **High confidence**: The core methodology for multi-domain concept discovery and HMDR model architecture is well-defined and reproducible
- **Medium confidence**: Predictive performance claims, given the limited human evaluation sample sizes
- **Low confidence**: Generalizability of explanation quality across domains not thoroughly tested

## Next Checks
1. Conduct larger-scale human preference experiments to validate explanation quality across more domains and preference mechanisms
2. Test the method on domains where LLM concept discovery may be particularly challenging (e.g., highly technical or specialized domains)
3. Compare explanation quality and prediction accuracy against other interpretable preference modeling approaches in head-to-head evaluations