---
ver: rpa2
title: Psychological Counseling Ability of Large Language Models
arxiv_id: '2503.07627'
source_url: https://arxiv.org/abs/2503.07627
tags:
- questions
- llms
- psychological
- correctness
- counseling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically assessed the psychological counseling
  ability of mainstream Large Language Models (LLMs) using 1096 questions from the
  Chinese National Counselor Level 3 Examination. The results showed that LLMs achieved
  relatively low accuracy rates, with an average of 43.26% for Chinese questions and
  36.10% for English questions.
---

# Psychological Counseling Ability of Large Language Models

## Quick Facts
- arXiv ID: 2503.07627
- Source URL: https://arxiv.org/abs/2503.07627
- Reference count: 8
- Primary result: LLMs achieved low accuracy rates (43.26% Chinese, 36.10% English) on Chinese National Counselor Level 3 Exam, with RAG boosting one model's performance by 13.8%.

## Executive Summary
This study systematically evaluates the psychological counseling ability of mainstream Large Language Models (LLMs) using a comprehensive dataset of 1096 questions from the Chinese National Counselor Level 3 Examination. The research reveals that LLMs demonstrate relatively low accuracy rates on professional counseling assessments, with significant variations based on question type, language, and the implementation of retrieval-augmented generation techniques. The findings suggest that while LLMs show potential in analytical reasoning, they face substantial challenges in domain-specific knowledge recall and cross-linguistic counseling applications.

## Method Summary
The study employed a rigorous evaluation framework using 1096 multiple-choice questions from China's National Counselor Level 3 Exam, categorized into Knowledge-based, Analytical-based, and Application-based types. Five mainstream LLMs (GPT-3.5, GPT-4, GLM-3, Gemini, and ERNIE-3.5) were tested using specific role-play prompts to output strictly in "Number, Option" format. The evaluation was conducted in both Chinese and English, with a subset of tests incorporating Retrieval-Augmented Generation (RAG) using a Psychological Counselor's Guidebook to enhance ERNIE-3.5's performance.

## Key Results
- LLMs achieved an average accuracy of 43.26% on Chinese questions and 36.10% on English questions
- RAG implementation with a Psychological Counselor's Guidebook improved ERNIE-3.5's correctness rate from 45.8% to 59.6%
- Models demonstrated higher accuracy on Analytical-based questions (44.8% Chinese, 41.2% English) compared to Knowledge-based questions (40.8% Chinese, 31.6% English)

## Why This Works (Mechanism)

### Mechanism 1
Supplementing a general-purpose LLM with external, domain-specific knowledge significantly improves its performance on specialized professional assessments. Retrieval-Augmented Generation (RAG) provides the model with non-parametric knowledge at inference time, bypassing limitations in recalling precise, niche information from pre-trained parametric memory. This directly addresses "Knowledge-based errors" by providing access to specialized counseling theory information.

### Mechanism 2
An LLM's counseling competence is heavily influenced by the language and cultural context of its training data. Models optimized for Chinese data showed higher accuracy on Chinese questions, suggesting psychological counseling concepts are culturally and linguistically embedded. Translation can introduce bias or information loss, making the models' performance a function of their proficiency with specific language and associated cultural context.

### Mechanism 3
LLMs possess a gradient of competence across different cognitive tasks, showing greater aptitude for analysis of case information than for recall of precise theoretical knowledge. Trained on vast, general text corpora, these models develop strong pattern recognition and linguistic analysis skills for "Analytical-based" questions, but lack the structured, textbook-like knowledge required to answer "Knowledge-based" questions without hallucinating.

## Foundational Learning

### Concept: Parametric vs. Non-Parametric Knowledge
- Why needed here: This distinction is the core reason for the study's key result. Parametric knowledge (in the model's weights) is insufficient, while non-parametric knowledge (in the guidebook) provided via RAG corrects this.
- Quick check question: Does providing a textbook to a student test their long-term memory (parametric) or their ability to find and apply information (non-parametric)?

### Concept: Evaluation Benchmark Validity
- Why needed here: The study's conclusions rest entirely on the 1,096 questions being a valid proxy for "psychological counseling ability."
- Quick check question: Does a high score on a multiple-choice professional exam guarantee competence in a dynamic, interpersonal real-world task?

### Concept: Retrieval-Augmented Generation (RAG)
- Why needed here: This is the intervention shown to be effective. Understanding that RAG is a technique to inject external, up-to-date, or specialized information into an LLM is essential.
- Quick check question: Instead of retraining a model, what is a faster way to give it access to a new, specialized textbook?

## Architecture Onboarding

### Component map:
Psychological Counseling Exam Questions -> LLMs (GPT-3.5, GPT-4, GLM-3, Gemini, ERNIE-3.5) -> Evaluation Engine -> Accuracy Metrics
Enhanced with: Psychological Counselor's Guidebook + RAG Pipeline -> ERNIE-3.5 -> Improved Performance

### Critical path:
The study flows from defining the 3 question types -> testing all 5 models on both language versions -> identifying low baseline accuracy -> implementing RAG on one model (ERNIE-3.5) -> observing a significant performance boost.

### Design tradeoffs:
**Quantitative vs. Qualitative:** The design prioritizes objective, scalable measurement (multiple-choice accuracy) over nuanced qualitative assessment of a counseling session. This provides hard numbers but may miss critical aspects of therapeutic alliance.
**Language Duality:** Testing in two languages provides broader insight but introduces the confounder of translation quality. The design choice was to include both to reveal language bias.
**Static Knowledge:** The RAG component uses a static guidebook. The tradeoff is that it doesn't test the model's ability to learn or adapt in real-time to novel situations outside the text.

### Failure signatures:
**Low Baseline Accuracy:** Any accuracy in the 30-46% range on a professional exam is a clear failure signal for using these models without domain-specific enhancements.
**Knowledge-Type Disparity:** A significant drop in performance on "Knowledge-based" vs. "Analytical-based" questions is a signature of a model lacking specialized theoretical training.
**Language-Based Performance Gap:** A statistically significant difference between Chinese and English versions indicates the model's competence is not robust across linguistic contexts.

### First 3 experiments:
1. **Implement a RAG pipeline** for a general-purpose LLM and test it on a sample of professional exam questions from a different domain (e.g., law, medicine) to see if the performance boost generalizes.
2. **Perform a failure analysis** on the "Application-based" questions where the model was provided with the guidebook but still failed. This would reveal if the failure is due to poor retrieval or flawed reasoning.
3. **Run a small-scale qualitative evaluation** where human experts rate the empathetic quality and professional appropriateness of responses from the baseline vs. the RAG-enhanced model on an open-ended prompt.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Does high performance on standardized counseling exams translate to efficacy in dynamic, real-world counseling interactions?
**Basis in paper:** [explicit] The authors acknowledge that "Knowledge of counseling theory does not necessarily reflect the model's ability to practice in actual counseling," and the static case analysis used differs from actual conversation.
**Why unresolved:** The study utilized multiple-choice questions based on fixed cases rather than open-ended, interactive dialogue, meaning the models' ability to manage a live therapeutic relationship remains untested.
**What evidence would resolve it:** A study comparing LLM exam scores with human expert ratings of the same models conducting simulated therapy sessions.

### Open Question 2
**Question:** How do specialized mental health LLMs (e.g., MentaLLaMA, SoulChat) compare to generalist models on professional qualification benchmarks?
**Basis in paper:** [explicit] The authors list this as a primary limitation, stating, "we only examined some of the mainstream LLMs... many other LLMs exist, particularly the Mental Health Large Model."
**Why unresolved:** The study restricted its evaluation to general-purpose models (GPT-4, ERNIE-3.5, etc.), excluding models specifically fine-tuned for psychological tasks.
**What evidence would resolve it:** Benchmarking the specific "Mental Health Large Models" mentioned in the paper against the National Counselor Level 3 dataset used here.

### Open Question 3
**Question:** To what extent can advanced prompt engineering or specific fine-tuning strategies improve LLM counseling accuracy beyond the gains seen with simple RAG?
**Basis in paper:** [explicit] The authors note that "prompts in this study used only simple direct questions and answers" and suggest future research should explore "insights for future enhancement."
**Why unresolved:** The study established a baseline with simple prompts and one RAG instance, but did not test complex prompt strategies (e.g., chain-of-thought) or domain-specific fine-tuning.
**What evidence would resolve it:** Comparative results from the same dataset using varied prompting techniques (e.g., few-shot, CoT) and specialized fine-tuning.

## Limitations
- **Generalizability:** Results based on a specific Chinese professional exam may not translate to real-world counseling competence involving dynamic, open-ended interactions
- **Cultural and Linguistic Specificity:** Performance differences between Chinese and English may reflect translation and cultural context rather than universal LLM limitations
- **Ecological Validity:** The "no explanation" constraint removes therapeutic dialogue, potentially not accurately reflecting true counseling abilities

## Confidence
- **High Confidence:** LLMs perform significantly below human expert levels on professional counseling exams. RAG with domain-specific knowledge can improve performance.
- **Medium Confidence:** LLMs show a cognitive gradient, performing better on analytical tasks than on knowledge recall. The performance gap between Chinese and English is primarily due to language and cultural factors.
- **Low Confidence:** A high score on this multiple-choice exam is a strong predictor of real-world counseling competence.

## Next Checks
1. **Test the RAG hypothesis on a different domain:** Implement a similar RAG pipeline for a general-purpose LLM on a set of professional exam questions from a different field (e.g., medical licensing exam). If the performance boost is replicated, it strengthens the claim that domain-specific knowledge is the limiting factor.
2. **Conduct a qualitative expert review:** Have certified counselors evaluate and rate the responses from both the baseline and RAG-enhanced models on a set of open-ended counseling scenarios. This would assess aspects like empathy and therapeutic appropriateness that the multiple-choice format cannot capture.
3. **Analyze RAG retrieval failures:** For the questions where the RAG-enhanced model still failed, examine the retrieval and generation process to determine if the failure was due to poor retrieval of relevant chunks or an inability to reason with the provided information. This would pinpoint the next bottleneck in the system.