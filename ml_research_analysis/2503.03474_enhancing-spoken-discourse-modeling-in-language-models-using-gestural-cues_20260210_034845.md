---
ver: rpa2
title: Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues
arxiv_id: '2503.03474'
source_url: https://arxiv.org/abs/2503.03474
tags:
- gesture
- language
- discourse
- gestures
- spoken
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes incorporating gestural cues into language models
  to improve spoken discourse modeling. The method encodes 3D human motion sequences
  into discrete gesture tokens using VQ-VAE, aligns these with text embeddings through
  feature alignment, and fine-tunes the model on text infilling tasks for discourse
  connectives, quantifiers, and stance markers.
---

# Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues

## Quick Facts
- arXiv ID: 2503.03474
- Source URL: https://arxiv.org/abs/2503.03474
- Authors: Varsha Suresh; M. Hamza Mughal; Christian Theobalt; Vera Demberg
- Reference count: 33
- Primary result: Gesture-enhanced language model improves discourse marker prediction F1 by 4.8% average across three tasks

## Executive Summary
This paper proposes incorporating gestural cues into language models to improve spoken discourse modeling. The method encodes 3D human motion sequences into discrete gesture tokens using VQ-VAE, aligns these with text embeddings through feature alignment, and fine-tunes the model on text infilling tasks for discourse connectives, quantifiers, and stance markers. Results show that incorporating gestures enhances marker prediction accuracy, with an average F1 score improvement of 4.8% across all three tasks compared to text-only baseline. Error analysis reveals that gestures are particularly helpful for disambiguating underrepresented markers.

## Method Summary
The approach involves three stages: (1) Gesture tokenization using VQ-VAE to convert 3D motion sequences into discrete tokens, (2) Feature alignment where gesture embeddings are projected into the text embedding space through an MLP projector and jointly trained with masked prediction objectives, and (3) Fine-tuning with LoRA adapters for each discourse marker task. The model processes BEAT2 dataset speech transcripts with corresponding 3D upper-body motion capture, predicting masked discourse markers through text infilling. The VQ-VAE uses a codebook of size 512, and the alignment stage employs 30% masking with joint masked language modeling and gesture prediction objectives.

## Key Results
- Gesture-enhanced model achieves 4.8% average F1 improvement across discourse connectives, quantifiers, and stance markers
- Gestures particularly benefit underrepresented markers, reducing confusion with high-frequency distractors
- Ablation studies show both gesture tokens and feature alignment contribute significantly to performance gains
- Error analysis indicates gestures help disambiguate temporal connectives ("after," "while") and contrastive markers ("but") from frequent "and"

## Why This Works (Mechanism)

### Mechanism 1: Discrete Gesture Tokenization via VQ-VAE
- Claim: Encoding 3D motion sequences into discrete tokens preserves fine-grained gestural information that can be processed alongside text.
- Mechanism: A VQ-VAE encoder chunks motion sequences, quantizes latent representations against a learned codebook (K=512), and the reconstruction objective forces the codebook embeddings to capture meaningful motion patterns. Unlike coarse grid-based approaches, this retains coordinated wrist-shoulder dynamics.
- Core assumption: Gestures contain language-relevant patterns that can be compressed into discrete units without losing discourse-relevant semantics.
- Evidence anchors:
  - [abstract]: "encode 3D human motion sequences into discrete gesture tokens using a VQ-VAE"
  - [Section 3.1]: "gesture representations capture fine-grained motion details as they are trained to reconstruct 3D human motion"
  - [corpus]: Related work (Xu et al., 2024) uses VQ-VAE for gesture synthesis, supporting viability—but corpus lacks direct evidence for discourse tasks.
- Break condition: If gesture tokens become too abstract (large K) or too coarse (small K), the codebook may not capture discourse-relevant distinctions (e.g., finger movements differentiating "two" vs. "three").

### Mechanism 2: Cross-Modal Feature Alignment
- Claim: Projecting gesture embeddings into the language model's input space enables joint reasoning across modalities.
- Mechanism: An MLP projector maps gesture token embeddings (256-dim) to the text embedding space (768-dim). Joint training with Masked Gesture Prediction and Masked Language Modeling forces the projector to align semantically related gesture-text pairs. Positional embeddings tie gesture tokens temporally to their co-occurring words.
- Core assumption: The alignment objective creates meaningful correspondences between gesture patterns and linguistic discourse functions rather than surface correlations.
- Evidence anchors:
  - [abstract]: "gesture token embeddings are then aligned with text embeddings through feature alignment, mapping them into the text embedding space"
  - [Section 5.3.2]: Ablation shows performance drop without feature alignment, "highlighting the importance of this stage in effectively fusing the two modalities"
  - [corpus]: Limited corpus support; prior gesture-language work (Xu & Cheng 2023) uses simpler grid tokens without explicit alignment objectives.
- Break condition: If gesture-text temporal alignment is imprecise (Whisper timestamps are word-level, not token-level), positional embeddings may misalign gesture tokens with their intended words.

### Mechanism 3: Gestures Disambiguate Underrepresented Discourse Markers
- Claim: Gestures provide complementary signals that preferentially improve prediction for rare discourse markers.
- Mechanism: Linguistic research documents gesture-marker co-occurrences (e.g., raised index finger for "however," palm-up for high certainty). The model leverages these patterns to disambiguate rare markers from frequent distractors (e.g., distinguishing "after" from "and").
- Core assumption: The BEAT2 dataset contains sufficient gesture-marker co-occurrences for the model to learn these associations.
- Evidence anchors:
  - [abstract]: "improves marker prediction accuracy, particularly for underrepresented markers"
  - [Section 5.4]: Error analysis shows GestureLM improves on temporal connectives ("after," "while") and contrastive markers ("but") that Text-only confuses with high-frequency "and"
  - [corpus]: Linguistic studies cited (Kendon, 1995; Casasanto & Jasmin, 2012) support gesture-temporal marker links, but corpus lacks computational validation beyond this paper.
- Break condition: If a marker class has too few training examples (<30, as filtered), gesture patterns may not be learnable regardless of alignment quality.

## Foundational Learning

- **Concept: VQ-VAE (Vector Quantized Variational Autoencoder)**
  - Why needed here: The gesture tokenizer uses VQ-VAE to discretize continuous motion data. Without understanding quantization, codebook learning, and the commitment loss term, you cannot debug tokenization quality or adjust codebook size.
  - Quick check question: If reconstruction loss is low but downstream task performance is poor, which codebook parameter should you investigate first?

- **Concept: Cross-Modal Feature Alignment**
  - Why needed here: The MLP projection stage is the critical bridge between gesture and text spaces. Understanding contrastive and masked prediction objectives helps diagnose whether alignment is failing or the downstream task is underfitting.
  - Quick check question: What does it signal if Masked Gesture Prediction loss plateaus early while Masked Language Modeling loss continues decreasing?

- **Concept: Discourse Markers and Gesture Co-Occurrence**
  - Why needed here: The evaluation targets three marker types with documented gesture links. Without this grounding, you cannot interpret error patterns or design better annotation.
  - Quick check question: Why might quantifiers like "some" be confused with numerals like "two" based on gesture features?

## Architecture Onboarding

- **Component map:**
  - Gesture Tokenizer (VQ-VAE encoder + codebook + decoder) → produces discrete gesture tokens from 3D joint rotations
  - MLP Projector (2 dense layers + GeLU) → maps 256-dim gesture embeddings to 768-dim text space
  - Pre-trained RoBERTa-base → frozen backbone receiving concatenated [text + gesture] embeddings
  - LoRA Adapters (r=128, α=256) → task-specific fine-tuning layers for each marker category
  - Gesture/Text Tokenizers → Whisper for text timestamps; 6D rotation representation for joints

- **Critical path:**
  1. Verify VQ-VAE reconstruction quality before alignment (bad tokens → bad alignment → bad everything)
  2. Monitor joint masked prediction losses during feature alignment; both should decrease
  3. Position embedding correctness: gesture tokens must inherit correct text token positions
  4. LoRA fine-tuning: overfitting is likely with small marker classes; use validation F1 for early stopping

- **Design tradeoffs:**
  - Codebook size K=512: Larger K captures more gesture variety but risks fragmentation; smaller K merges distinct gestures
  - Masking percentage (30% chosen): Lower masking reduces alignment pressure; higher masking destroys context
  - Upper-body only (13 joints, no fingers): Simpler tracking but cannot distinguish finger-based quantifiers (e.g., "two" vs. "three")
  - Encoder-only model (RoBERTa): Good for classification; limits applicability to generative decoder-based LMs

- **Failure signatures:**
  - Random gesture embeddings perform similarly to learned embeddings → alignment failed or gesture signal is noise
  - High-frequency markers (e.g., "and") dominate predictions → class imbalance not addressed; gestures not helping
  - Position-only embeddings outperform learned embeddings → temporal duration, not gesture semantics, drives predictions
  - Large variance across seeds (see Table 1 grid-based token ablations) → training instability or insufficient data

- **First 3 experiments:**
  1. **Gesture tokenizer sanity check**: Reconstruct held-out motion sequences; visualize decoded vs. original joint trajectories. Quantify reconstruction error across codebook utilization (are all 512 codes used?).
  2. **Adversarial alignment test**: Replace learned gesture embeddings with random normal vectors and zero vectors (as in Table 2). Confirm that pre-trained embeddings outperform both baselines; if not, alignment is not learning meaningful representations.
  3. **Per-class marker analysis**: For each discourse marker, compute confusion reduction between GestureLM and Text-only (as in Figure 3). Identify which markers benefit most; cross-reference with training frequency to validate the long-tail hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can gestural cues contribute to modeling implicit coherence relations where meaning is not explicitly conveyed through language but is expressed solely through gesture?
- Basis in paper: [explicit] Conclusion states: "Future research should investigate whether gestures can also contribute in cases where meaning is not explicitly conveyed through language but is instead expressed solely through gesture (e.g., implicit coherence relations), which would require annotated data for evaluation."
- Why unresolved: The current study focuses only on explicit discourse markers (connectives, quantifiers, stance markers) with clear lexical correspondences, leaving unexplored whether gestures can reveal discourse structure absent lexical cues.
- What evidence would resolve it: Evaluation on corpora annotated for implicit coherence relations (e.g., PDTB implicit relations), comparing gesture-enhanced vs. text-only models on predicting relation types without explicit connectives.

### Open Question 2
- Question: How can gesture-enhanced models be effectively adapted for decoder-based language models trained for next-token prediction rather than masked language modeling?
- Basis in paper: [explicit] Limitations section states: "our approach is currently designed for encoder-only MLM-based models, as they are generally used for token classification. In future work, we aim to explore how gesture-enhanced models can be helpful in decoder-based models."
- Why unresolved: The current feature alignment approach uses masked prediction objectives suited to encoder architectures; decoder models require causal attention and different training paradigms that may not integrate gesture tokens equivalently.
- What evidence would resolve it: Comparative experiments adapting the gesture tokenization and alignment pipeline to decoder-only models (e.g., GPT-style), evaluating on generative spoken discourse tasks such as dialogue continuation or marker insertion.

### Open Question 3
- Question: How much does incorporating finger joint movements improve disambiguation between fine-grained gesture categories, such as distinguishing numerical quantifiers?
- Basis in paper: [explicit] Limitations section notes: "our model captures only the upper-body joints up to the wrist, excluding finger joint movements... gestures relying on finger joints, such as distinguishing between two and three cannot be disambiguated. Future work will incorporate these finer-grained features."
- Why unresolved: Error analysis shows GestureLM confuses "some" with "two" or "one," suggesting wrist movements are insufficient to distinguish precise numerical gestures that rely on finger configurations.
- What evidence would resolve it: Ablation experiments comparing full hand-tracking (including finger joints) against wrist-only models, with per-class accuracy analysis on numerical vs. non-numerical quantifiers.

## Limitations
- Relies on single dataset (BEAT2) and specific marker categories, limiting generalizability
- Upper-body-only gesture capture excludes finger movements critical for distinguishing numerical quantifiers
- Discrete tokenization may lose nuanced temporal patterns despite preserving motion details
- Temporal alignment between gestures and text tokens is approximate, using word-level timestamps

## Confidence

- **High confidence**: The core methodology (VQ-VAE tokenization, feature alignment, LoRA fine-tuning) is technically sound and well-documented. The improvement over text-only baseline (4.8% F1) is consistently observed across multiple seeds and marker types. The ablation studies convincingly demonstrate that both the gesture tokens and alignment stage contribute meaningfully.

- **Medium confidence**: The claim that gestures particularly help underrepresented markers is supported by error analysis but could benefit from more systematic per-class breakdowns and statistical significance testing. The interpretation of why certain markers benefit more than others relies partly on linguistic intuition that isn't fully validated through controlled experiments.

- **Low confidence**: Claims about the specific gesture patterns learned by the model (e.g., which gesture features correspond to which markers) are not directly verified. The paper doesn't provide interpretability analyses showing what the model actually uses from gesture embeddings versus position or timing information alone.

## Next Checks

1. **Adversarial Alignment Validation**: Replace learned gesture embeddings with random normal vectors and zero vectors during feature alignment, then compare performance against the full model. This would definitively test whether the alignment stage learns meaningful gesture-text correspondences rather than exploiting temporal or positional cues alone.

2. **Cross-Dataset Generalization Test**: Apply the trained GestureLM to a held-out subset of BEAT2 or an external dataset with different speakers, topics, or speaking styles to assess whether gesture benefits transfer beyond the training distribution. Include speakers not seen during training to evaluate robustness.

3. **Finger Gesture Ablation Study**: Train a version of the model with full-body motion (including fingers) on a subset of markers where finger configurations are known to be important (numerals, quantifiers). Compare performance against the upper-body-only version to quantify the cost of the current simplification and identify which marker types suffer most from this limitation.