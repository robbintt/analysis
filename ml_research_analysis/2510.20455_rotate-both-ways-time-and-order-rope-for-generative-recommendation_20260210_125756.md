---
ver: rpa2
title: 'Rotate Both Ways: Time-and-Order RoPE for Generative Recommendation'
arxiv_id: '2510.20455'
source_url: https://arxiv.org/abs/2510.20455
tags:
- time
- rope
- index
- attention
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of modeling both sequential and
  temporal information in generative recommendation systems. While transformers excel
  at sequence modeling, standard rotary position embeddings (RoPE) only encode token
  order, missing critical temporal patterns like bursts, long-term rhythms, and calendar
  effects in user behavior.
---

# Rotate Both Ways: Time-and-Order RoPE for Generative Recommendation

## Quick Facts
- arXiv ID: 2510.20455
- Source URL: https://arxiv.org/abs/2510.20455
- Reference count: 35
- Key outcome: TO-RoPE variants outperform standard RoPE, absolute embeddings, and relative bias encodings on MovieLens-20M and proprietary gaming data, with split-by-head and split-by-dimension achieving highest HR@K and NDCG@K scores.

## Executive Summary
This paper addresses the challenge of modeling both sequential and temporal information in generative recommendation systems. While transformers excel at sequence modeling, standard rotary position embeddings (RoPE) only encode token order, missing critical temporal patterns like bursts, long-term rhythms, and calendar effects in user behavior. To solve this, the authors propose Time-and-Order RoPE (TO-RoPE), a unified framework that incorporates both index and time as angle sources in rotary embeddings. They present three instantiations: early fusion (angles combined within each plane), split-by-dimension (dedicating some planes to index, others to time), and split-by-head (specializing heads to either modality). Experiments on MovieLens-20M and a large proprietary gaming dataset show that TO-RoPE variants consistently outperform existing methods like absolute embeddings, relative bias encodings, and single-source RoPE. The split-by-head and split-by-dimension approaches achieve the highest Hit@K and NDCG@K scores, demonstrating the effectiveness of integrating time and order information in rotary embeddings for generative recommendation.

## Method Summary
The authors propose TO-RoPE to jointly encode discrete sequence order and continuous wall-clock time in rotary embeddings for generative recommendation. The framework treats index and time as angle sources shaping the query-key geometry directly, with three instantiations: early fusion (angles combined within each plane), split-by-dimension (dedicating some planes to index, others to time), and split-by-head (specializing heads to either modality). Experiments use GPT-2 style decoder-only transformers with 12 layers (~85M parameters) on MovieLens-20M (5-core filtered, max sequence length 50) and proprietary gaming dataset (max length 1024, tens of millions of users, 1 year timespan). Models are evaluated using Hit Rate@K and NDCG@K for K ∈ {10, 50, 200/300} with leave-one-out split for MovieLens and temporal split (last day test, second-last validation) for proprietary data.

## Key Results
- TO-RoPE variants consistently outperform existing methods like absolute embeddings, relative bias encodings, and single-source RoPE on both MovieLens-20M and proprietary gaming datasets.
- Split-by-head and split-by-dimension approaches achieve the highest Hit@K and NDCG@K scores, demonstrating the effectiveness of integrating time and order information.
- Early fusion sometimes fails to outperform time-only RoPE due to destructive interference, while split variants consistently win.
- Split ratio of 0.3-0.5 for time allocation in split-by-dimension performs best.

## Why This Works (Mechanism)

### Mechanism 1
Jointly encoding discrete sequence order and continuous wall-clock time improves retrieval of relevant history items compared to index-only encoding. Standard RoPE rotates query and key vectors by an angle based on the discrete token index. TO-RoPE introduces a second angle source based on the normalized timestamp, forcing the attention score to depend on both the relative index gap (capturing session continuity) and the relative time gap (capturing staleness or seasonal drift). User intent is a function of both sequence position and temporal context, and these require distinct geometric encodings.

### Mechanism 2
Isolating time and index information into separate rotary planes or heads prevents destructive interference found in simple angle addition. In early fusion where angles are added, the trigonometric interaction creates a destructive interference term that can push attention scores toward zero. Split-by-dimension allocates specific dimensions purely to time or index, removing this cross-term and allowing the model to sum clean cosine kernels. The interaction between index and time is better modeled by the network learning separate representations rather than algebraically combining them in the attention score directly.

### Mechanism 3
Mapping continuous time to rotation angles enables the learning of multi-scale temporal patterns without hand-crafted buckets. By using a log-spaced bank of frequencies, RoPE acts as a band-pass filter. High frequencies capture short-term bursts; low frequencies capture long-range rhythms. When time drives the angle, the attention mechanism naturally aligns interactions that share similar phase offsets, enabling periodic behavior detection.

## Foundational Learning

### Concept: Rotary Position Embedding (RoPE)
- Why needed here: Unlike learned absolute embeddings, RoPE rotates vectors, making attention a function of relative distance critical for extrapolating to new sequence lengths and time gaps.
- Quick check question: If you double the time gap between two interactions, does a standard absolute embedding change the dot product? (Answer: No, unless the embedding table changes, which it doesn't for fixed gaps).

### Concept: Attention as a Band-Pass Filter
- Why needed here: The paper frames the attention score not just as "relevance" but as a sum of cosine waves. Understanding this explains why "split-by-dim" works: it ensures clean frequencies for time vs. index.
- Quick check question: Why does RoPE use a log-spaced frequency ladder instead of a single frequency? (Answer: To capture both short-range and long-range dependencies simultaneously).

### Concept: Staleness vs. Order
- Why needed here: This is the core semantic distinction in the paper. "Order" is 1, 2, 3...; "Staleness" is 1 hour ago vs. 1 year ago. Models usually conflate them; TO-RoPE separates them.
- Quick check question: A user watched Ep1, waited 1 year, then watched Ep2. An index-only model thinks they are "close." What does a time-only model think?

## Architecture Onboarding

### Component map:
Input: Item IDs + Raw Timestamps → Preprocessing: Normalize timestamps to scalar τ → Frequency Bank: Fixed or learnable ω_time and ω_index → Rotation Layer (split-by-dim or split-by-head) → Attention: Standard scaled dot-product.

### Critical path:
The timestamp normalization (τᵢ = (uᵢ - u_ref)/s) is the most brittle component. If the scaling factor s is too small, time angles spiral rapidly, destroying signal; if too large, angles are too flat to distinguish recent events.

### Design tradeoffs:
Split-by-Head vs. Split-by-Dim: Split-by-head preserves full frequency resolution per modality but risks modal blindness (a head sees only time). Split-by-dim mixes modes in every head but reduces frequency resolution per mode. Split-by-head is often easier to debug (ablate time heads vs. index heads).

### Failure signatures:
- Sudden performance drop at inference: Likely a timestamp normalization error (e.g., inference timestamps are out-of-range relative to u_ref).
- No improvement over vanilla RoPE: Likely destructive interference in Early Fusion; switch to Split variants.
- Loss spikes: Time frequency ω_time is too high, causing gradient instability.

### First 3 experiments:
1. Sanity Check (Index-only vs. Time-only): Run vanilla RoPE with index, then with normalized time only. Time-only should likely outperform index-only if temporal patterns dominate your data.
2. Interference Test (Early Fusion vs. Split): Compare adding angles vs. splitting dimensions. If Split wins, it confirms destructive interference is a problem in your data.
3. Ratio Sweep: For Split-by-dim, sweep the allocation ratio (e.g., 10% time, 50% time, 90% time) to find the dominant modality for your domain.

## Open Questions the Paper Calls Out
The paper explicitly lists "split-by-layer which ties gates by depth" and "split-by-group" as "Other possible instantiations" but excludes them from the experimental results, leaving the efficacy of depth-based or GQA-based specialization unconfirmed. It is unclear if the "wavelength-targeted" approach for time frequencies, which allows explicit period selection, offers a significant advantage over the generic "base-form" log-spacing. The paper states a requirement to normalize raw timestamps by a factor s to keep them in a "reasonable range" similar to the index range, but does not analyze how variations in this normalization factor impact the rotary geometry or gradient flow.

## Limitations
- Destructive interference in early fusion variant can cause underperformance on proprietary dataset.
- Improper time normalization can cause frequency mismatch and instability.
- Suboptimal split ratio allocation can reduce performance gains.

## Confidence
High: Method specification, experimental setup, and results are clearly described and reproducible with standard implementations.
Medium: Some architectural details like exact model dimensions and training duration are unspecified but don't fundamentally impact the core contribution.
Low: Proprietary dataset access limits complete external validation of the strongest results.

## Next Checks
1. Verify timestamp normalization parameters (reference u_ref and scale factor s) are appropriate for your data distribution.
2. Compare early fusion performance against split variants to confirm destructive interference is present in your domain.
3. Test different split ratios (0.1, 0.3, 0.5, 0.9) to identify optimal time allocation for your specific dataset.