---
ver: rpa2
title: Video Text Preservation with Synthetic Text-Rich Videos
arxiv_id: '2511.05573'
source_url: https://arxiv.org/abs/2511.05573
tags:
- text
- diffusion
- generation
- video
- text-to-video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work tackles the challenge of generating legible text within
  videos produced by text-to-video (T2V) diffusion models, which often struggle with
  coherence and suffer from artifacts. The proposed lightweight method leverages synthetic
  supervision: first generating text-rich images with a text-to-image (T2I) diffusion
  model, then animating them into videos using a text-agnostic image-to-video (I2v)
  model.'
---

# Video Text Preservation with Synthetic Text-Rich Videos

## Quick Facts
- arXiv ID: 2511.05573
- Source URL: https://arxiv.org/abs/2511.05573
- Authors: Ziyang Liu; Kevin Valencia; Justin Cui
- Reference count: 25
- Primary result: Lightweight fine-tuning on synthetic text-video pairs improves short-text legibility and temporal consistency in T2V models without architectural changes

## Executive Summary
This work addresses the challenge of generating legible text within videos produced by text-to-video (T2V) diffusion models, which often struggle with coherence and suffer from artifacts. The proposed lightweight method leverages synthetic supervision: first generating text-rich images with a text-to-image (T2I) diffusion model, then animating them into videos using a text-agnostic image-to-video (I2V) model. These synthetic video-prompt pairs are used to fine-tune the pre-trained Wan2.1 T2V model without architectural changes. The approach improves short-text legibility and temporal consistency, while also revealing emerging structural priors for longer text. The results demonstrate that curated synthetic data and weak supervision can effectively enhance textual fidelity in video generation, offering a practical alternative to costly retraining.

## Method Summary
The method generates synthetic training data by first creating text-rich images using a T2I diffusion model with explicit text prompts, then animating these images into short videos using a text-agnostic I2V model that avoids text-related keywords to prevent hallucination. This synthetic video-prompt pairs dataset is used to fine-tune the pre-trained Wan2.1-T2V-1.3B model without modifying its architecture or loss functions. The approach leverages the architectural strengths of Wan2.1's identity-preservation mechanisms, which transfer to text fidelity through shared requirements for high-frequency detail preservation and temporal coherence.

## Key Results
- Short text (2-4 words) shows significant improvement in legibility and temporal consistency after fine-tuning
- Long text prompts produce "text-like" patterns with structural priors but lack complete character decoding
- The lightweight approach achieves meaningful gains without architectural modifications or full retraining

## Why This Works (Mechanism)

### Mechanism 1
A synthetic T2I→I2V cascade produces higher-quality text-video training pairs than native T2V generation. T2I models render legible text in single frames; a text-agnostic I2V model animates these frames while preserving spatial high-frequency features like letterforms. This creates weakly supervised pairs where the "ground truth" video contains cleaner text than T2V models natively produce. The core assumption is that I2V models will retain first-frame text fidelity when not explicitly prompted about text; T2I text quality transfers through animation. Break condition: If I2V models degrade high-frequency text details regardless of prompt design, the synthetic data quality collapses.

### Mechanism 2
Omitting text keywords from I2V prompts prevents hallucinated or degraded text artifacts. When I2V models receive text-related prompts, they may override input image content with hallucinated glyphs. By using text-free prompts (e.g., describing motion/scene only), the model focuses on temporal smoothness without injecting new textual artifacts. The core assumption is that the I2V model's text-generation capability is the source of degradation; suppressing it via prompt design preserves input text. Break condition: If text-free prompts still trigger hallucination, or if motion causes unavoidable glyph distortion, the premise fails.

### Mechanism 3
Identity-preservation architectures (Wan2.1) transfer to text fidelity due to shared high-frequency and temporal coherence requirements. Wan2.1's attention-based consistency and high-frequency-aware processing were designed for face/subject identity but also support glyph-level accuracy across frames. Fine-tuning on synthetic text videos activates these capabilities for textual content. The core assumption is that text preservation and identity preservation impose similar demands on spatial detail and temporal stability. Break condition: If text requires character-level semantic understanding beyond high-frequency preservation, identity architectures may be insufficient.

## Foundational Learning

- **Diffusion model fine-tuning vs. training from scratch**: Why needed here: The method avoids costly full retraining by fine-tuning Wan2.1 on a small synthetic dataset; understanding this distinction is critical for resource planning. Quick check question: Can you explain why fine-tuning a pre-trained T2V model is orders of magnitude cheaper than pre-training, and what constraints it imposes on what the model can learn?

- **Weak supervision and synthetic data pipelines**: Why needed here: The approach uses synthetically generated videos as supervision rather than human-annotated ground truth; understanding the trade-offs (scale vs. quality vs. distribution shift) is essential. Quick check question: What failure modes can arise when a model is trained on synthetic data generated by another model?

- **Temporal consistency in video diffusion**: Why needed here: Text must remain legible across frames; temporal coherence mechanisms (attention, feature propagation) determine whether glyphs stay stable or morph/degrade. Quick check question: In a video diffusion model, what architectural components enforce consistency across frames, and how might they interact with high-frequency text details?

## Architecture Onboarding

- **Component map**: T2I generator (e.g., Stable Diffusion) -> T2I→I2V synthetic data pipeline -> Wan2.1-T2V-1.3B fine-tuning

- **Critical path**: 1. Generate text-rich images via T2I with explicit text prompts. 2. Animate images to video via I2V with text-free prompts (motion/scene only). 3. Fine-tune Wan2.1 on (video, prompt) pairs without architecture or loss changes. 4. Evaluate on short-text legibility and temporal consistency.

- **Design tradeoffs**: Synthetic vs. real data (controllable vs. distribution shift), text-agnostic I2V (reduces hallucination vs. limits motion control), no architecture changes (preserves capacity vs. caps potential improvements)

- **Failure signatures**: Short text legible but longer text degrades into "text-like" patterns without valid characters, text remains static when motion is expected, synthetic data distribution diverges from real use cases

- **First 3 experiments**: 1. Baseline comparison: Generate videos with vanilla Wan2.1 vs. fine-tuned version on identical prompts containing short text (2-4 words); measure OCR accuracy and frame-wise consistency. 2. Prompt ablation for I2V stage: Compare text-free vs. text-containing I2V prompts on synthetic data quality; quantify hallucination rates. 3. Long-text stress test: Evaluate fine-tuned model on prompts with 8+ word sequences; characterize where structural priors emerge vs. where full decoding fails.

## Open Questions the Paper Calls Out

### Open Question 1
How can standardized benchmarks be developed to accurately evaluate text fidelity in video generation? The authors state, "Given the absence of standardized metrics for this task, we advocate for future work to develop text-focused video benchmarks, possibly combining OCR reliability, temporal consistency, and human readability scoring." Current OCR tools are unreliable for video due to frame-wise inconsistency and distortion, forcing reliance on qualitative human observation. A dataset and metric suite that robustly correlates automated scores with human perception of text legibility over time would resolve this.

### Open Question 2
Can the "emerging structural priors" observed in the model be refined to ensure exact character decoding for longer, more complex text? The paper notes that while short text improves, "for longer sentences, our model does not consistently preserve full word integrity," and suggests future work should explore "more diverse text scenarios." The model currently mimics typographic layout (structural priors) but frequently fails to render correct characters (exact decoding) when prompted with long sentences. Successful generation of videos containing legible paragraphs where OCR accuracy matches the input prompt would resolve this.

### Open Question 3
To what extent does performance on synthetic supervision transfer to real-world text generation scenarios? The authors conclude that "Future work may explore... real-world data" to build upon the findings derived from curated synthetic data. The method relies on a specific T2I-to-I2V synthetic loop; it is unverified if this weak supervision generalizes to "in-the-wild" video distributions. Evaluation results on a dataset of real-world videos showing that the fine-tuned model generalizes beyond the synthetic domain would resolve this.

### Open Question 4
Would incorporating explicit architectural inductive biases (e.g., glyph-level losses) enhance performance without compromising the lightweight nature of the method? The paper contrasts its approach (fine-tuning only) with T2I methods that use "specific text encoders" or "glyph-level OCR loss," suggesting a potential unexplored hybrid approach. It is unclear if "weak supervision" has reached a performance ceiling that necessitates structural model changes. Ablation studies comparing the current fine-tuning approach against a variant equipped with text-specific architectural modules would resolve this.

## Limitations

- Synthetic data distribution may not capture real-world text diversity, degradation modes, or complex layouts, creating domain shift during deployment
- Long-text performance remains fundamentally limited; the method only produces "text-like" patterns without full character decoding for sequences beyond short phrases
- No standardized automated metrics; reliance on qualitative human evaluation introduces reproducibility and scalability concerns

## Confidence

- **High confidence**: Synthetic T2I→I2V cascade produces cleaner text-video pairs than native T2V generation for short text
- **Medium confidence**: Text-free I2V prompts effectively suppress hallucination without compromising temporal consistency
- **Medium confidence**: Wan2.1 identity-preservation architecture transfers benefits to text fidelity through shared high-frequency and temporal coherence requirements
- **Low confidence**: Synthetic supervision scales to complex real-world text rendering beyond controlled short-phrase scenarios

## Next Checks

1. Generate a large-scale test set with diverse real-world text (street signs, billboards, subtitles) and evaluate the fine-tuned model's performance on each category, measuring both OCR accuracy and visual quality

2. Compare synthetic data quality across different I2V model choices (Stable Video Diffusion, ModelScope, etc.) and prompt strategies to quantify the impact of animator selection on final text preservation

3. Implement and evaluate OCR-based automated metrics specifically designed for video text detection and tracking, then validate their correlation with human judgments across the model's operational range