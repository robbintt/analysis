---
ver: rpa2
title: Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical
  Vector Spaces
arxiv_id: '2505.07831'
source_url: https://arxiv.org/abs/2505.07831
tags:
- categorical
- neuron
- arxiv
- activation
- sub-dimensions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether the activation level of a token
  in a synthetic neuron is related to its coordinates within a vector space formed
  by the categorical sub-dimensions of that neuron. The researchers propose that neurons
  can be interpreted as categorical vector spaces whose basis consists of categorical
  sub-dimensions extracted from preceding neurons.
---

# Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces

## Quick Facts
- **arXiv ID**: 2505.07831
- **Source URL**: https://arxiv.org/abs/2505.07831
- **Reference count**: 0
- **Primary result**: Higher activation levels in synthetic neurons correlate with greater dimensional proximity to multiple categorical sub-dimensions, suggesting activation acts as an intra-neuronal attention mechanism.

## Executive Summary
This study investigates whether token activation levels in synthetic neurons relate to their coordinates within categorical vector spaces formed by sub-dimensions extracted from preceding neurons. Using data from GPT2-XL's second MLP layer, researchers found that highly activated tokens tend to belong to intersections of multiple categorical sub-dimensions, leading to categorical convergence and reduced polysemy. The results support a model where neuronal polysemy can be understood as a vector space structured by activation, with activation serving as an intra-neuronal attention mechanism that guides concept formation.

## Method Summary
The study analyzed GPT-2 XL's MLP Layers 0 and 1 to identify top-100 "core-tokens" per neuron based on average activation. For each Layer 1 neuron, researchers identified top-10 precursor neurons in Layer 0 using MLP weights, then defined "taken-clusters" as intersections between core-tokens of target and precursor neurons. Dimensional proximity was calculated as mean cosine similarity between tokens and taken-clusters using static input embeddings. The analysis tested whether high-activation tokens showed greater proximity to categorical sub-dimensions using Kruskal-Wallis tests and Kendall's τ correlation.

## Key Results
- 78.01% of neurons (n=614) showed positive correlations between activation levels and dimensional proximity to categorical sub-dimensions
- Kendall's τ correlation values ranged from 0.78-0.85 (p < .0001) across neuron groups
- High-activation tokens exhibited categorical convergence (belonging to multiple sub-dimension intersections) while low-activation tokens showed categorical divergence (single sub-dimensions)

## Why This Works (Mechanism)

### Mechanism 1: Categorical Sub-Dimension Extraction via Clipping
- Claim: Neurons extract categorical sub-dimensions from strongly-connected precursor neurons, forming a non-orthogonal basis
- Mechanism: The aggregation function (∑wᵢⱼxᵢⱼ + a) projects tokens from precursors based on categorical priming, inter-neuronal attention, and categorical phasing
- Evidence: Abstract states "composed of categorical sub-dimensions extracted from preceding neurons"; Section 1.5 describes "categorical clipping" process
- Break condition: Fails if precursor neurons lack strong weight connections

### Mechanism 2: Activation-Dimension Proximity Relationship
- Claim: Token activation magnitude positively correlates with proximity to multiple categorical sub-dimensions
- Mechanism: High-activation tokens belong to intersections of multiple sub-dimensions (categorical convergence), reducing degrees of freedom
- Evidence: 78% of neurons show positive δ between high/low activation proximity; Kendall's τ = 0.78-0.85 (p < .0001)
- Break condition: Weakens if sub-dimensions are orthogonal rather than non-orthogonal

### Mechanism 3: Intra-Neuronal Attention as Semantic Segmentation
- Claim: Activation levels serve as attentional vector isolating less-polysemous concepts from polysemous token sets
- Mechanism: Higher activation segments show greater categorical homogeneity (activational differentiation)
- Evidence: Section 1.5 discusses "relative activational differentiation"; Section 5.1 describes activation space influencing sub-dimension generation
- Break condition: Fails if activation distributions are uniform across categorical clusters

## Foundational Learning

- **Concept: Superposition hypothesis (Elhage et al.)**
  - Why needed here: Paper positions itself as alternative/complement to superposition-based explanations
  - Quick check question: Can you explain why superposition suggests neurons encode more features than available dimensions?

- **Concept: Non-orthogonal vector bases**
  - Why needed here: Core claim is that categorical sub-dimensions form a non-orthogonal basis
  - Quick check question: In a non-orthogonal basis, can two basis vectors have dot product > 0?

- **Concept: Categorical clipping vs. distributed representations**
  - Why needed here: Paper reverses standard framing—polysemy emerges from intra-neuronal structure
  - Quick check question: How does the paper's "intra-neuronal" polysemy differ from "inter-neuronal" superposition?

## Architecture Onboarding

- **Component map**: Layer n-1 neurons (precursors) -> Weight connections (wᵢⱼ) -> Aggregation function (∑wᵢⱼxᵢⱼ + a) -> Layer n neuron activation space -> Categorical vector space (sub-dimensions as basis) -> Intra-neuronal attention (activation-sorted token analysis) -> Critical concept-in-action (high-activation zone)

- **Critical path**: Identifying taken-clusters (tokens shared between precursor and target neuron core-tokens) -> Computing dimensional proximity (mean cosine similarity) -> Correlating with activation levels

- **Design tradeoffs**: Analysis limited to layers 0-1 of GPT2-XL; core-tokens defined as top-100 activations; cosine similarity in embedding space is sole categorical proximity metric

- **Failure signatures**: Neurons with <3 taken-clusters or <40 total tokens excluded; χ² test shows equiprobable distribution (P > 0.05) fails hypothesis; PCA Factor 1 shows negative correlations between sub-dimensions

- **First 3 experiments**:
  1. Replicate on later layers (n=10-15) to test mechanism persistence beyond early layers
  2. Control for embedding dimension by replacing GPT2-XL embeddings with independently-trained embeddings
  3. Intervention study: Artificially boost weights for specific precursor-target connections and measure taken-cluster shifts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed intra-neuronal categorical vector space structure persist across deeper layers and diverse model architectures?
- Basis in paper: Methodology restricts analysis to second MLP layer (Layer 1), leaving remaining 46 layers unexplored
- Why unresolved: Unclear if "categorical clipping" and "categorical phasing" mechanisms function similarly in deeper layers
- What evidence would resolve it: Replicating PCA and correlation analyses on final layers of GPT2-XL and distinct architectures (Llama 3, Mistral)

### Open Question 2
- Question: Can the "critical categorical zone" be causally manipulated to control semantic output, or is it merely a statistical correlation?
- Basis in paper: Authors propose "intra-neuronal attention" and "conceptualization" based on geometric observations, but results rely on passive statistical correlations
- Why unresolved: Correlation does not prove this zone drives model's semantic decisions; could be epiphenomenon
- What evidence would resolve it: Causal intervention experiments clamping/ablating activations within identified "critical categorical zone"

### Open Question 3
- Question: How can the intra-neuronal vector space model be mathematically unified with existing inter-neuronal superposition theories?
- Basis in paper: Paper describes framework as "alternative, but not contradictory" approach to superposition
- Why unresolved: Superposition theory explains polysemy as features distributed across many neurons; this paper locates it inside single neuron
- What evidence would resolve it: Formal theoretical framework demonstrating how intra-neuronal non-orthogonal bases interact with distributed representations

### Open Question 4
- Question: Can this framework operationalize "cognitive entanglement" between neural networks and symbolic systems?
- Basis in paper: Conclusion suggests analyzing neurons as categorical vector spaces could serve methodological approach for "cognitive entanglement"
- Why unresolved: Paper theorizes mapping between "concepts-in-action" and symbolic knowledge but doesn't demonstrate implementation
- What evidence would resolve it: Development of neuro-symbolic interface extracting "critical concept-in-action" and mapping to formal symbolic ontology

## Limitations
- Restricted scope to first two MLP layers of GPT2-XL leaves uncertain whether model generalizes to deeper layers
- Methodology relies on static input embeddings which may not capture context-dependent semantic relationships
- Exclusion criteria (neurons with fewer than 3 taken-clusters) may introduce selection bias

## Confidence
- **High confidence**: 78.01% of neurons demonstrate positive correlations between activation levels and dimensional proximity (supported by robust statistical measures)
- **Medium confidence**: Mechanism linking categorical clipping to intra-neuronal attention (theoretically consistent but depends on untested assumptions)
- **Low confidence**: Broader claims about activation serving as attentional vector for concept formation (extends beyond empirical results into speculative territory)

## Next Checks
1. Replicate analysis across all MLP layers (n=0-23) in GPT2-XL to determine whether activation-proximity relationship strengthens, weakens, or transforms as representations become more abstract
2. Conduct controlled intervention study by artificially boosting weights for specific precursor-target connections and measuring whether taken-clusters shift toward predicted sub-dimensions
3. Compare findings using contextualized embeddings (hidden states) versus static input embeddings to determine whether categorical proximity reflects pre-existing semantic relationships or emerges through contextual processing