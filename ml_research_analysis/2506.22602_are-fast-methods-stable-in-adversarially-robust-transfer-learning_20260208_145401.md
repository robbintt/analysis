---
ver: rpa2
title: Are Fast Methods Stable in Adversarially Robust Transfer Learning?
arxiv_id: '2506.22602'
source_url: https://arxiv.org/abs/2506.22602
tags:
- fgsm
- fine-tuning
- adversarial
- robustness
- racc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the stability of fast adversarial training\
  \ methods, specifically FGSM, in robust transfer learning compared to training from\
  \ scratch. While FGSM is known to suffer from catastrophic overfitting when training\
  \ models from scratch, the authors surprisingly find that FGSM is much more stable\
  \ during adversarial fine-tuning, maintaining robustness at standard perturbation\
  \ budgets of \u03B5 = 4 and \u03B5 = 8 without catastrophic overfitting."
---

# Are Fast Methods Stable in Adversarially Robust Transfer Learning?

## Quick Facts
- arXiv ID: 2506.22602
- Source URL: https://arxiv.org/abs/2506.22602
- Authors: Joshua C. Zhao; Saurabh Bagchi
- Reference count: 40
- Primary result: FGSM avoids catastrophic overfitting during adversarial fine-tuning at standard ε values, maintaining robustness with 4× less training time than PGD.

## Executive Summary
This paper investigates whether fast adversarial training methods, specifically FGSM, remain stable during robust transfer learning compared to training from scratch. The authors find that FGSM fine-tuning does not suffer from catastrophic overfitting at standard perturbation budgets (ε = 4, ε = 8), unlike when training from scratch. This stability is further enhanced with parameter-efficient fine-tuning methods, where FGSM remains stable even up to ε = 32 for linear probing. The findings demonstrate that FGSM can serve as an efficient alternative to PGD in robust transfer learning, using 4× less training time while only losing 0.39% and 1.39% test robustness for ε = 4 and ε = 8 respectively.

## Method Summary
The study compares FGSM and PGD for adversarial fine-tuning in transfer learning across five datasets (Caltech256, Stanford Dogs, Oxford Flowers102, CIFAR-10, CIFAR-100). Models use robust pre-trained weights from ARES 2.0 (Swin-B or ViT-B/16) and are fine-tuned with various methods: full fine-tuning, linear probing, BitFit, and Adapter. FGSM uses single-step attacks (α = ε), while PGD uses 7 steps with step size ε/4. Training employs SGD with lr=0.05, momentum=0.9, weight decay=5e-4, and batch size 128 for 20-40 epochs depending on dataset size. Robustness is evaluated using PGD-10 and AutoAttack.

## Key Results
- FGSM fine-tuning maintains stability at standard ε values (4, 8) without catastrophic overfitting, unlike training from scratch
- FGSM achieves 4× speedup compared to PGD-7 while only losing 0.39% and 1.39% test robustness for ε = 4 and ε = 8 respectively
- Parameter-efficient fine-tuning methods (especially linear probing) enhance FGSM stability, maintaining robustness up to ε = 32
- Additional regularization methods required for FGSM when training from scratch are unnecessary for adversarial fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** FGSM avoids catastrophic overfitting during adversarial fine-tuning at standard perturbation budgets (ε = 4, ε = 8), unlike when training from scratch.
- **Mechanism:** Robust pre-trained weights provide an initialization that prevents convergence to the "degenerate minimum" associated with catastrophic overfitting. The pre-trained model already encodes useful representations that constrain the optimization landscape.
- **Core assumption:** The stability comes from initialization quality rather than FGSM's inherent properties.
- **Evidence anchors:**
  - [abstract] "FGSM fine-tuning does not suffer from any issues with catastrophic overfitting at standard perturbation budgets of ε = 4 or ε = 8"
  - [Section 3.1] "When training with FGSM from scratch, prior work has found that even ε = 7 can cause models to have problems with catastrophic overfitting"
  - [corpus] Related work "Catastrophic Overfitting, Entropy Gap and Participation Ratio" confirms CO as a core FGSM problem in training-from-scratch scenarios
- **Break condition:** Catastrophic overfitting re-emerges at larger ε values (≥12 for full fine-tuning, ≥16 for full fine-tuning with significant performance degradation).

### Mechanism 2
- **Claim:** Parameter-efficient fine-tuning (PEFT) methods enhance FGSM stability by constraining feature updates.
- **Mechanism:** PEFT methods restrict the number of tunable parameters, which limits how much model features can change. This constraint functions as implicit regularization, preventing the model from reaching degenerate solutions that cause catastrophic overfitting.
- **Core assumption:** Feature restriction is the causal factor for enhanced stability (stated as hypothesis by authors, not proven).
- **Evidence anchors:**
  - [abstract] "This stability is further enhanced with parameter-efficient fine-tuning methods, where FGSM remains stable even up to ε = 32 for linear probing"
  - [Section 3.1] "We hypothesize that this comes from the feature-restricting nature of PEFTs... By constraining the number of fine-tuned parameters, this also restricts how much the model features can be altered"
  - [corpus] Corpus lacks direct evidence on PEFT + FGSM interactions; this appears novel
- **Break condition:** Different PEFT methods have different stability thresholds: Adapter stable to ε = 12, BitFit to ε = 16, linear probing to ε = 32.

### Mechanism 3
- **Claim:** The performance gap between FGSM and PGD at large ε stems from attack strength differences, not instability.
- **Mechanism:** At larger ε values, FGSM's single-step perturbation becomes increasingly dissimilar to PGD's multi-step approximation (measured via cosine similarity). This is an estimation quality issue rather than training instability.
- **Core assumption:** Cosine similarity between perturbations correlates with training effectiveness.
- **Evidence anchors:**
  - [Section 3.3] "As ε becomes larger, the cosine similarity between the perturbations of δ_FGSM and δ_PGD decreases"
  - [Section 3.3] At ε = 4, FGSM loses only 0.39% robustness; at ε = 8, loses 1.39% — gap increases with ε
  - [corpus] Corpus contains FGSM attack detection literature but limited direct comparison with PGD in transfer contexts
- **Break condition:** At standard ε values (4, 8), similarity is sufficient for comparable performance; gap becomes problematic only at ε ≥ 16.

## Foundational Learning

- **Concept:** Adversarial Training (AT)
  - **Why needed here:** The entire paper operates within the AT framework (Equation 2: min-max optimization). Understanding the inner maximization (attack generation) vs. outer minimization (model training) is essential.
  - **Quick check question:** Can you explain why generating adversarial examples is considered the "inner maximization" in the AT objective?

- **Concept:** Catastrophic Overfitting (CO)
  - **Why needed here:** The paper's central finding is that CO is avoided in fine-tuning. You must understand what CO looks like (sudden robustness collapse to strong attacks) to appreciate why its absence matters.
  - **Quick check question:** What is the signature behavior of catastrophic overfitting during training curves?

- **Concept:** FGSM vs. PGD Attack Generation
  - **Why needed here:** The speed-accuracy trade-off between these methods drives the paper's efficiency claims. FGSM uses one large step (α = ε); PGD uses multiple smaller steps with projection.
  - **Quick check question:** Why does PGD-7 require approximately 4× more time than FGSM?

## Architecture Onboarding

- **Component map:** Pre-trained backbone (Swin-B/ViT-B/16) -> Fine-tuning head (full, linear, BitFit, Adapter) -> Attack module (FGSM/PGD) -> Evaluation (PGD-10/AutoAttack)

- **Critical path:**
  1. Load robustly pre-trained model weights
  2. Configure fine-tuning method (choose: full, linear, BitFit, or Adapter)
  3. Generate adversarial examples using FGSM (not PGD for efficiency)
  4. Train on adversarial examples for 20-40 epochs with SGD (lr=0.05, momentum=0.9)
  5. Evaluate with PGD-10 or AutoAttack

- **Design tradeoffs:**
  - **Full fine-tuning:** Highest capacity, stable to ε = 8, longest training time
  - **Linear probing:** Maximum stability (to ε = 32), fastest training, may underfit on dissimilar downstream tasks
  - **BitFit/Adapter:** Balance between stability (ε = 12-16) and adaptation capacity
  - **FGSM vs. PGD:** 4× speedup with 0.39-1.39% robustness loss at standard ε

- **Failure signatures:**
  - Catastrophic overfitting: Robustness curve drops sharply mid-training while natural accuracy continues rising (see Figure 5 at ε ≥ 12 for full fine-tuning)
  - Under-convergence: Both natural and robust accuracy plateau low (possible with linear probing on highly dissimilar datasets)
  - Attack strength mismatch: Large robustness gap between FGSM and PGD trained models at ε ≥ 16

- **First 3 experiments:**
  1. **Stability verification:** Fine-tune with FGSM at ε = 8 on Caltech256 using full fine-tuning; plot natural accuracy and PGD-10 robustness over 40 epochs to confirm no catastrophic overfitting occurs
  2. **PEFT comparison:** Compare FGSM stability with linear probing vs. full fine-tuning at ε = 16 to validate enhanced stability claim
  3. **Efficiency benchmark:** Measure wall-clock time for FGSM vs. PGD-7 fine-tuning on identical hardware; verify ~4× speedup with comparable robustness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do variations in pre-training settings—such as dataset similarity, image dimensions, or pre-training $\epsilon$—impact the estimation difference between FGSM and PGD adversarial examples?
- **Basis in paper:** [explicit] The Discussion section explicitly identifies these factors as influential and calls for future work to explore how different transfer learning scenarios affect the estimation gap.
- **Why unresolved:** The current study uses a fixed set of ARES 2.0 pre-trained weights and standard datasets, without ablation on the pre-training configurations themselves.
- **What evidence would resolve it:** Empirical studies measuring the cosine similarity between FGSM and PGD perturbations across models pre-trained with varying dataset classes and perturbation budgets.

### Open Question 2
- **Question:** What is the theoretical mechanism that allows FGSM to avoid converging to the "degenerate minimum" associated with catastrophic overfitting during fine-tuning?
- **Basis in paper:** [inferred] The authors hypothesize that parameter-efficient fine-tuning (PEFT) restricts feature alteration, acting as a regularizer, but they do not provide a formal theoretical proof.
- **Why unresolved:** The paper presents an empirical observation of stability but lacks a theoretical derivation of why the optimization landscape differs from training from scratch.
- **What evidence would resolve it:** Theoretical analysis or visualization of the loss landscape geometry during FGSM fine-tuning compared to training from scratch.

### Open Question 3
- **Question:** Can the performance gap between FGSM and PGD at large perturbation budgets ($\epsilon > 16$) be closed by shifting regularization focus from stability to estimation strength?
- **Basis in paper:** [inferred] The Discussion notes that since stability is inherent, future regularization could target improving FGSM estimation to make the performance gap with PGD smaller.
- **Why unresolved:** The paper identifies the gap at large $\epsilon$ as a result of attack strength but does not experiment with methods designed to specifically enhance single-step estimation quality.
- **What evidence would resolve it:** Applying estimation-focused regularizers to FGSM fine-tuning and measuring the resulting robustness at high epsilon values.

## Limitations

- Focus on relatively small-scale datasets and standard ε values limits generalizability to larger, more complex tasks
- Lack of theoretical grounding for why pre-trained weights specifically prevent catastrophic overfitting
- Study does not explore diverse pre-training architectures or robustness levels, leaving open questions about generalizability

## Confidence

- **High confidence:** FGSM stability at standard ε values (4, 8) in fine-tuning; 4× training time reduction; no need for additional regularization in fine-tuning scenarios
- **Medium confidence:** PEFT methods enhance FGSM stability due to feature restriction; robustness gaps at higher ε stem from attack strength differences rather than instability
- **Low confidence:** Stability mechanisms are fully understood; findings generalize to all pre-training scenarios and dataset scales

## Next Checks

1. Test FGSM fine-tuning stability on larger-scale datasets (e.g., ImageNet-1K subsets) to verify scalability of the observed stability
2. Experiment with pre-trained models of varying robustness levels (e.g., ε=2 vs ε=8 pre-training) to assess initialization sensitivity
3. Conduct ablation studies on PEFT methods to isolate whether feature restriction or parameter count reduction drives enhanced stability