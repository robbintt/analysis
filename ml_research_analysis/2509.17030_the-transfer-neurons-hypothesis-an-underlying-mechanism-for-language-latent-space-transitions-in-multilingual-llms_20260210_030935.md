---
ver: rpa2
title: 'The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent
  Space Transitions in Multilingual LLMs'
arxiv_id: '2509.17030'
source_url: https://arxiv.org/abs/2509.17030
tags:
- layer
- neurons
- principal
- component
- semantics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the lack of understanding of how multilingual
  large language models (LLMs) perform semantic processing across languages. It proposes
  the Transfer Neurons Hypothesis, which posits that specific neurons in the MLP module
  are responsible for transferring representations between language-specific latent
  spaces and a shared semantic latent space.
---

# The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs

## Quick Facts
- **arXiv ID**: 2509.17030
- **Source URL**: https://arxiv.org/abs/2509.17030
- **Authors**: Hinata Tezuka; Naoya Inoue
- **Reference count**: 40
- **Primary result**: Specific MLP neurons (transfer neurons) enable semantic processing across languages in multilingual LLMs by transitioning between language-specific and shared semantic latent spaces

## Executive Summary
This study addresses the fundamental gap in understanding how multilingual large language models perform semantic processing across languages. The authors propose the Transfer Neurons Hypothesis, which identifies specific neurons in the MLP module that facilitate movement between language-specific latent spaces and a shared semantic latent space. Through empirical validation using neuron deactivation experiments on a 1.3B parameter BLOOM model trained on 46 languages, the study demonstrates that these transfer neurons are critical for multilingual reasoning. Deactivating these neurons causes significant performance degradation on multilingual knowledge QA tasks, with F1 scores dropping by up to 0.62, while having minimal effect on randomly selected neurons. The research also reveals distinct categories of transfer neurons with different roles in semantic space transitions.

## Method Summary
The authors developed a methodology to identify transfer neurons through correlation analysis of residual stream changes during input/output transitions across languages. They extracted test cases where input and output languages differed, then computed correlation coefficients between residual stream changes and neuron activation values. Neurons with high correlation values were classified as transfer neurons. The study then performed deactivation experiments where identified transfer neurons were set to zero activation, measuring the impact on multilingual knowledge QA performance. They analyzed two types of transfer neurons: Type-1 (moving to semantic space) and Type-2 (moving to language-specific spaces). The analysis included attention pattern visualization and comparison of language-specificity between neuron types.

## Key Results
- Deactivating transfer neurons causes significant performance degradation on multilingual knowledge QA tasks, with F1 scores dropping by up to 0.62 in some cases
- Type-2 transfer neurons (moving to language-specific spaces) show higher language-specificity than Type-1 neurons (moving to semantic space)
- The MLP module contains 8,448 neurons, of which 3,230 are classified as transfer neurons
- Attention patterns differ significantly between deactivated and normal neurons, suggesting distinct functional roles

## Why This Works (Mechanism)
The Transfer Neurons Hypothesis posits that multilingual LLMs maintain separate language-specific latent spaces while also possessing a shared semantic latent space for cross-lingual reasoning. Transfer neurons in the MLP module act as bridges between these spaces, enabling the model to transition representations from language-specific contexts to semantic understanding and back to language-specific outputs. This mechanism allows the model to leverage semantic knowledge learned in one language when processing another language, explaining the observed cross-lingual transfer capabilities.

## Foundational Learning
- **Language-specific latent spaces**: Separate representational spaces maintained for each language, necessary for understanding the model's internal organization and why cross-lingual transfer is challenging
- **Shared semantic latent space**: A common representational space where semantic meaning is processed independently of language, crucial for understanding how models achieve cross-lingual reasoning
- **MLP module neuron activation patterns**: How individual neurons contribute to processing, essential for identifying which neurons are responsible for specific functions like language transfer
- **Residual stream correlation analysis**: Statistical method for identifying functionally relevant neurons, needed to systematically discover transfer neurons without manual inspection
- **Cross-lingual transfer capability**: The ability to apply knowledge from one language to another, fundamental to understanding multilingual model performance

## Architecture Onboarding
- **Component map**: Input sequence -> Embedding layer -> Transformer blocks (Self-Attention + MLP) -> Output layer -> Language-specific vs Shared semantic latent spaces (mediated by Transfer neurons in MLP)
- **Critical path**: Input language encoding -> Transfer neuron activation in MLP -> Movement to shared semantic space -> Cross-lingual reasoning -> Movement to target language space -> Output generation
- **Design tradeoffs**: Separate language spaces provide language-specific optimization but require transfer mechanisms; shared semantic space enables cross-lingual reasoning but may lose language-specific nuances
- **Failure signatures**: Significant performance degradation on multilingual QA when transfer neurons are deactivated; reduced cross-lingual generalization; language-specific errors increase
- **3 first experiments**:
  1. Correlation analysis between residual stream changes and neuron activations across language pairs
  2. Deactivation of top-ranked transfer neurons and measurement of performance impact
  3. Comparison of attention patterns between deactivated and normal neurons

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The transfer neuron identification relies on a specific heuristic based on residual stream correlations, which may include false positives or miss relevant neurons
- The study focuses on 8 languages, potentially limiting generalizability to other language families or script types
- Deactivation experiments show correlation with performance degradation but cannot definitively prove causal mechanisms
- The analysis provides inferential rather than definitive causal links between attention patterns and transfer functionality

## Confidence
- **High Confidence**: The existence of neurons showing language-specific activation patterns that influence cross-lingual performance
- **Medium Confidence**: The categorization into Type-1 and Type-2 transfer neurons with distinct semantic space transition roles
- **Medium Confidence**: The claim that deactivating transfer neurons causes significant performance degradation on multilingual QA tasks

## Next Checks
1. Conduct ablation studies across additional language families (e.g., Semitic, Dravidian, or Southeast Asian languages) to test generalizability of transfer neuron identification
2. Implement causal intervention techniques (e.g., causal mediation analysis) to establish stronger causal links between identified neurons and cross-lingual transfer capabilities
3. Perform fine-tuning experiments where transfer neurons are selectively amplified rather than deactivated to test whether their enhancement improves cross-lingual reasoning capabilities