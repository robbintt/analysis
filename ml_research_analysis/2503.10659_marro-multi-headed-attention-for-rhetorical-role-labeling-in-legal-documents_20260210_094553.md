---
ver: rpa2
title: 'MARRO: Multi-headed Attention for Rhetorical Role Labeling in Legal Documents'
arxiv_id: '2503.10659'
source_url: https://arxiv.org/abs/2503.10659
tags:
- legal
- rhetorical
- documents
- embeddings
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MARRO, a family of models for rhetorical
  role labeling in legal documents that combines transformer-inspired multi-headed
  attention with BiLSTM-CRF architecture. The authors address the challenge of labeling
  sentences in lengthy legal documents by using multi-headed attention over sentence
  embeddings to capture contextual relationships.
---

# MARRO: Multi-headed Attention for Rhetorical Role Labeling in Legal Documents

## Quick Facts
- **arXiv ID**: 2503.10659
- **Source URL**: https://arxiv.org/abs/2503.10659
- **Reference count**: 40
- **Primary result**: MARRO models achieve F1 scores of 0.724 on Indian Supreme Court documents and 0.617 on UK Supreme Court documents

## Executive Summary
This paper introduces MARRO (Multi-headed Attention for Rhetorical Role Labeling), a family of models designed to identify rhetorical roles of sentences in lengthy legal documents. The models combine transformer-inspired multi-headed attention with BiLSTM-CRF architecture to capture contextual relationships between sentences. The authors propose four variants: MARRO base using sent2vec embeddings, TF-MARRO using LEGAL-BERT embeddings, and two multi-task learning versions that incorporate label shift prediction. The models are evaluated on two datasets from Indian and UK Supreme Courts, achieving state-of-the-art results. The paper also contributes a new dataset of 150 annotated Indian Supreme Court documents with 30,729 sentences.

## Method Summary
The MARRO family addresses the challenge of labeling sentences in lengthy legal documents by using multi-headed attention over sentence embeddings to capture contextual relationships. The base architecture combines BiLSTM layers with multi-headed attention and CRF for sequence labeling. Four variants are proposed: MARRO base using sent2vec embeddings, TF-MARRO using LEGAL-BERT embeddings, and two multi-task learning versions (MTL-MARRO and MTL-TF-MARRO) that incorporate label shift prediction as an auxiliary task. The multi-headed attention mechanism allows the model to attend to different parts of the document simultaneously, capturing complex contextual relationships that are crucial for identifying rhetorical roles in legal texts.

## Key Results
- MARRO base achieves F1 score of 0.724 on the Indian Supreme Court dataset
- TF-MARRO variant achieves F1 score of 0.617 on the UK Supreme Court dataset
- Multi-task learning versions show consistent improvements across both datasets
- The models outperform existing state-of-the-art approaches for rhetorical role labeling in legal documents

## Why This Works (Mechanism)
The multi-headed attention mechanism allows MARRO to capture different types of contextual relationships between sentences in legal documents simultaneously. Unlike single attention heads that might focus on one type of relationship, multiple heads can learn to attend to different aspects - such as temporal relationships, argumentative structures, or hierarchical dependencies. This is particularly effective for legal documents where sentences often have complex interdependencies. The BiLSTM-CRF architecture provides strong sequential modeling capabilities while the CRF layer ensures globally coherent label sequences. When combined with transformer-based embeddings (LEGAL-BERT), the model can leverage pre-trained legal domain knowledge, further improving performance on the task.

## Foundational Learning

**Sentence Embeddings**: Why needed: Legal documents require understanding of sentence-level meaning for role classification. Quick check: Compare sent2vec vs LEGAL-BERT embeddings on downstream task performance.

**Multi-headed Attention**: Why needed: Legal sentences have complex interdependencies that single attention heads cannot capture. Quick check: Compare single vs multi-headed attention performance on F1 scores.

**BiLSTM-CRF Architecture**: Why needed: Sequential dependencies and label coherence are critical in legal text annotation. Quick check: Evaluate impact of removing CRF layer on label consistency.

**Multi-task Learning**: Why needed: Auxiliary task of label shift prediction provides additional supervision signal. Quick check: Compare single-task vs multi-task variants on same datasets.

## Architecture Onboarding

**Component Map**: Sentence Embeddings -> Multi-headed Attention -> BiLSTM -> CRF -> Rhetorical Role Labels

**Critical Path**: Sentence embeddings are first generated, then passed through multi-headed attention to capture contextual relationships, followed by BiLSTM for sequential processing, and finally CRF for sequence labeling.

**Design Tradeoffs**: The choice between sent2vec and LEGAL-BERT embeddings represents a tradeoff between computational efficiency and domain-specific knowledge. Multi-task learning adds complexity but provides performance gains through auxiliary supervision.

**Failure Signatures**: Poor performance on lengthy documents, inconsistent label predictions across similar sentence patterns, failure to capture long-range dependencies, and sensitivity to embedding quality.

**First Experiments**:
1. Evaluate sentence embedding quality by computing similarity between semantically related legal sentences
2. Test attention mechanism by visualizing attention weights between sentence pairs
3. Assess label coherence by measuring consistency of rhetorical role predictions across document sections

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Datasets are relatively small with only 200 UK Supreme Court and 150 Indian Supreme Court annotated documents
- Models struggle with lengthy legal documents, showing suboptimal performance on UK dataset (F1=0.617)
- Evaluation focuses primarily on sentence-level classification without extensive error analysis
- Limited discussion of how labeling errors propagate through downstream legal reasoning tasks

## Confidence
- **High confidence**: Multi-headed attention improves sentence-level rhetorical role classification
- **Medium confidence**: MARRO achieves state-of-the-art performance (limited comparison set)
- **Medium confidence**: Multi-task learning with label shift prediction provides consistent benefits

## Next Checks
1. Evaluate MARRO models on additional legal domains and jurisdictions with larger annotated datasets
2. Conduct ablation studies to quantify individual contributions of multi-headed attention, BiLSTM-CRF, and transformer embeddings
3. Perform error analysis on misclassified sentences to identify systematic weaknesses and validate attention mechanism effectiveness