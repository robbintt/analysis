---
ver: rpa2
title: 'Towards Safer Chatbots: Automated Policy Compliance Evaluation of Custom GPTs'
arxiv_id: '2502.01436'
source_url: https://arxiv.org/abs/2502.01436
tags:
- gpts
- policy
- compliance
- custom
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a fully automated method for evaluating the
  policy compliance of user-configured chatbots (Custom GPTs) in the GPT Store. The
  approach combines large-scale GPT discovery, policy-driven red-teaming prompts,
  and LLM-as-a-judge compliance assessment.
---

# Towards Safer Chatbots: Automated Policy Compliance Evaluation of Custom GPTs

## Quick Facts
- **arXiv ID**: 2502.01436
- **Source URL**: https://arxiv.org/abs/2502.01436
- **Reference count**: 40
- **Primary result**: 58.7% of Custom GPTs exhibit at least one policy violation

## Executive Summary
This study presents a fully automated method for evaluating the policy compliance of user-configured chatbots (Custom GPTs) in the GPT Store. The approach combines large-scale GPT discovery, policy-driven red-teaming prompts, and LLM-as-a-judge compliance assessment. Validated against human annotations, the method achieves an F1 score of 0.975 for detecting policy violations. Applied to 782 Custom GPTs, it reveals that 58.7% exhibit at least one policy violation, with most violations stemming from base model behaviors rather than customization.

## Method Summary
The authors developed an automated pipeline for discovering and evaluating Custom GPTs. The method employs three main components: (1) automated GPT discovery using the GPT Store's web interface, (2) generation of policy-driven red-teaming prompts based on OpenAI's content policy, and (3) LLM-as-a-judge evaluation using GPT-4 to assess compliance. The system crawls the GPT Store, extracts configuration details, generates targeted prompts designed to elicit policy violations, and uses a fine-tuned LLM to classify responses. Human annotators validated the system's accuracy, achieving strong agreement with the automated assessments.

## Key Results
- 58.7% of evaluated Custom GPTs exhibited at least one policy violation
- Most violations stemmed from base model behaviors rather than customization
- Automated detection method achieved F1 score of 0.975 when validated against human annotations

## Why This Works (Mechanism)
The approach leverages the strengths of large language models for both generating adversarial test cases and evaluating responses. By using policy-driven red-teaming prompts, the system systematically explores edge cases that might trigger violations. The LLM-as-a-judge methodology provides consistent, scalable evaluation that can be automated across large numbers of chatbots. The discovery mechanism exploits the publicly accessible GPT Store interface to build a comprehensive dataset for analysis.

## Foundational Learning
- **Policy-driven red-teaming**: Generating prompts specifically designed to elicit policy violations; needed to systematically test compliance boundaries; quick check: compare prompt diversity and coverage
- **LLM-as-a-judge evaluation**: Using language models to assess policy compliance; needed for scalable, consistent evaluation; quick check: validate against human judgments
- **Automated discovery mechanisms**: Crawling interfaces to build comprehensive datasets; needed to sample large ecosystems effectively; quick check: assess discovery completeness
- **Configuration analysis**: Examining prompt engineering and instruction tuning; needed to attribute violations to specific sources; quick check: compare customized vs. default behaviors
- **Base model behavior attribution**: Distinguishing between customization effects and inherent model tendencies; needed for actionable insights; quick check: control experiments with identical prompts

## Architecture Onboarding

**Component Map**: GPT Discovery -> Red-Teaming Prompt Generation -> LLM-as-Judge Evaluation -> Human Validation

**Critical Path**: The pipeline follows a linear flow from discovery through evaluation, with human validation serving as the gold standard for accuracy measurement.

**Design Tradeoffs**: The system prioritizes automation and scalability over nuanced contextual understanding, trading some accuracy for the ability to evaluate thousands of chatbots. The use of publicly accessible interfaces enables broad discovery but may miss private or unlisted GPTs.

**Failure Signatures**: False positives may occur when legitimate uses of sensitive topics are flagged as violations. False negatives may result from prompts that fail to trigger certain behavioral patterns. The system may miss context-dependent violations that only emerge during extended interactions.

**First Experiments**: 1) Validate discovery completeness by comparing found GPTs against known samples; 2) Test red-teaming prompt effectiveness by measuring violation elicitation rates; 3) Evaluate human-LLM agreement across diverse policy violation types.

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Analysis captures only static prompt-based behaviors rather than dynamic interaction patterns
- Sample of 782 GPTs represents a snapshot that may not reflect current GPT Store composition
- Attribution of violations to base model behavior requires further investigation as prompt engineering effects may be more nuanced
- Automated LLM-as-a-judge methodology may miss context-dependent violations that emerge during actual use

## Confidence
- **High confidence**: Automated detection methodology and validation against human annotations; finding that majority of GPTs contain violations
- **Medium confidence**: Attribution of violations to base model vs. customization; representativeness of the GPT sample
- **Medium confidence**: Completeness of policy coverage in the evaluation framework

## Next Checks
1. Conduct longitudinal analysis tracking policy compliance rates over time as GPTs are updated and OpenAI's base models evolve
2. Perform live interaction testing with a subset of flagged GPTs to identify violations that only manifest through dynamic user engagement
3. Implement A/B testing comparing different prompt engineering strategies to quantify their effectiveness in reducing policy violations beyond base model improvements