---
ver: rpa2
title: Direct Semantic Communication Between Large Language Models via Vector Translation
arxiv_id: '2511.03945'
source_url: https://arxiv.org/abs/2511.03945
tags:
- semantic
- translation
- injection
- vector
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of semantic communication between
  large language models (LLMs) by proposing a vector translation approach that bypasses
  text-based token exchange. The authors develop a dual-encoder translator to learn
  bidirectional mappings between the latent representation spaces of two distinct
  LLMs (LLaMA-2-7B and Mistral-7B-Instruct), achieving an average cosine alignment
  of 0.538.
---

# Direct Semantic Communication Between Large Language Models via Vector Translation

## Quick Facts
- arXiv ID: 2511.03945
- Source URL: https://arxiv.org/abs/2511.03945
- Reference count: 9
- Primary result: Achieved average cosine alignment of 0.538 between Llama-2-7B and Mistral-7B-Instruct latent spaces

## Executive Summary
This paper introduces a novel approach for direct semantic communication between large language models by translating between their latent representation spaces, bypassing traditional text-based token exchange. The authors develop a dual-encoder translator that learns bidirectional mappings between the 4096-dimensional hidden states of two distinct LLMs, enabling one model to generate text that semantically matches another model's output when given only abbreviated prompts. The approach demonstrates that cross-model latent communication is feasible while preserving computational stability through a conservative vector injection mechanism.

## Method Summary
The authors propose a vector translation framework where a dual-encoder translator learns to map between the latent spaces of two LLMs (Llama-2-7B and Mistral-7B-Instruct). The translator consists of a 4096D→512D bottleneck with 8-head attention, trained on five domain-specific prompt pairs using a composite loss function. During inference, the target model's hidden states are replaced with a 30% blended version of the translated vector at the final three layers and token positions, steering generation without destabilizing logits. The approach achieves bidirectional communication with a notable 2.01:1 transfer asymmetry favoring general-purpose models over instruction-tuned variants.

## Key Results
- Achieved average cosine alignment of 0.538 between latent spaces
- Demonstrated successful bidirectional translation with stable injection at 30% blending strength
- Observed 2.01:1 transfer asymmetry (Llama→Mistral) indicating general-purpose models yield more transferable representations than instruction-tuned variants
- Preserved computational stability with no significant perplexity increase during injection

## Why This Works (Mechanism)
The approach works by learning a shared semantic space between two distinct LLMs through vector translation, rather than relying on text-based communication. The dual-encoder architecture with a bottleneck forces the translator to learn compressed, meaningful representations that capture semantic equivalence across models. The conservative injection strategy (30% blending) ensures that the translated vectors influence the target model's generation without overwhelming its internal dynamics, maintaining stability while achieving semantic alignment.

## Foundational Learning

**Vector Space Alignment**: Understanding how to map between different high-dimensional representation spaces of neural networks. *Why needed*: Core to enabling cross-model communication. *Quick check*: Verify that translated vectors from different models cluster around similar semantic meanings.

**Contrastive Learning**: Using InfoNCE loss to align similar representations while pushing dissimilar ones apart. *Why needed*: Ensures semantic consistency in the shared space. *Quick check*: Measure if cosine similarity increases between semantically equivalent vectors after training.

**Latent Space Injection**: Modifying internal model representations during inference to influence generation. *Why needed*: Enables steering model outputs without fine-tuning. *Quick check*: Confirm that injected vectors produce coherent outputs when blended at appropriate strengths.

## Architecture Onboarding

**Component Map**: Training Data -> Dual-Encoder Translator -> Vector Injection Hook -> Target Model

**Critical Path**: Extract hidden states → Train translator → Inject translated vectors → Generate steered output

**Design Tradeoffs**: The 30% blending strength represents a conservative approach that prioritizes stability over maximum semantic transfer, trading some alignment quality for computational reliability.

**Failure Signatures**: Logit destabilization occurs when α > 0.3 or injection targets early layers, resulting in incoherent generated text and increased perplexity.

**First Experiments**:
1. Train translator on single domain and measure cosine similarity between translated and reference vectors
2. Vary blending coefficient α from 0.1 to 0.7 and measure output coherence and perplexity
3. Test bidirectional translation asymmetry by comparing Llama→Mistral vs Mistral→Llama performance

## Open Questions the Paper Calls Out
None

## Limitations
- Mathematical specification gaps exist for distribution preservation loss and cycle-consistency loss formulations
- Training data specificity is limited, with exact prompt content and hidden state extraction layers not fully specified
- Injection mechanism precision is empirically derived rather than theoretically justified

## Confidence
**High Confidence**: Core architectural approach and training parameters are clearly specified and reproducible
**Medium Confidence**: Vector injection mechanism and stability properties are well-described, though empirically derived parameters introduce some uncertainty
**Low Confidence**: Exact mathematical formulations for key loss components remain unclear, potentially affecting learned vector space quality

## Next Checks
1. Implement and validate the distribution matching loss by measuring mean and standard deviation convergence of source and target vector distributions during training
2. Systematically vary the blending coefficient α (0.1, 0.3, 0.5, 0.7) and measure target model perplexity and output coherence to confirm 30% as maximum stable value
3. Measure and compare cosine similarity achieved in both translation directions across multiple runs to verify the 2.01:1 asymmetry ratio and investigate correlation with model pretraining objectives