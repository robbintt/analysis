---
ver: rpa2
title: 'MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making'
arxiv_id: '2511.06592'
source_url: https://arxiv.org/abs/2511.06592
tags:
- audio
- bias
- clinical
- gender
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic evaluation of demographic
  bias in audio LLMs for clinical decision-making, focusing on surgical recommendations.
  The authors constructed MedVoiceBias, a dataset of 170 clinical cases synthesized
  into speech using 36 distinct voice profiles varying in age, gender, and emotion.
---

# MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making

## Quick Facts
- arXiv ID: 2511.06592
- Source URL: https://arxiv.org/abs/2511.06592
- Reference count: 25
- This paper presents the first systematic evaluation of demographic bias in audio LLMs for clinical decision-making, focusing on surgical recommendations.

## Executive Summary
This paper presents the first systematic evaluation of demographic bias in audio LLMs for clinical decision-making, focusing on surgical recommendations. The authors constructed MedVoiceBias, a dataset of 170 clinical cases synthesized into speech using 36 distinct voice profiles varying in age, gender, and emotion. Across six state-of-the-art audio LLMs, they found severe modality-dependent bias, with surgery recommendation rates varying by up to 34.9 percentage points between text and audio inputs. Age disparities of up to 12% persisted in most models even with chain-of-thought prompting, while gender bias was eliminated through explicit reasoning. Emotion effects were not detected due to poor recognition performance. These findings demonstrate that audio LLMs are susceptible to making clinical decisions based on voice characteristics rather than medical evidence, highlighting the urgent need for bias-aware architectures before clinical deployment.

## Method Summary
The authors constructed MedVoiceBias, a dataset of 170 clinical cases from DDXPlus synthesized into speech using 36 distinct voice profiles spanning age (young 20–29, old ≥60), gender (M/F), and 6 emotions. They evaluated six state-of-the-art audio LLMs (DeSTA2.5-Audio 8B, Qwen2.5-Omni 3B/7B, Gemini Flash 2.0/2.5, GPT-4o-mini-audio) under two conditions: Direct Answer and diagnose-then-decide chain-of-thought prompting. Surgery recommendation rates were computed for each demographic subgroup, with Fisher's exact test comparing audio cohorts against text-only baseline. The study also included an ASR condition using Whisper transcripts without paralinguistic features to isolate modality effects.

## Key Results
- Audio LLMs showed surgery recommendation rate differences of up to 34.9 percentage points between text and audio inputs
- Age disparities of up to 12% persisted in most models even with chain-of-thought prompting
- Gender bias was eliminated through explicit reasoning (CoT prompting), while emotion effects were not detected due to poor recognition performance

## Why This Works (Mechanism)
Audio LLMs process both linguistic content and paralinguistic features simultaneously, making them susceptible to bias from voice characteristics. The study demonstrates that these models can make clinical decisions based on voice attributes rather than medical evidence, particularly when processing audio inputs that contain demographic information in speech patterns.

## Foundational Learning
- **Fisher's exact test**: Why needed - statistical validation of demographic bias differences; Quick check - p<0.05 threshold with ≥2% absolute difference
- **Chain-of-thought prompting**: Why needed - evaluate whether explicit reasoning mitigates demographic bias; Quick check - compare DA vs CoT performance across demographic groups
- **Modality-dependent bias**: Why needed - understand how different input formats (text vs audio) affect model behavior; Quick check - compare recommendation rates across text, ASR, and full audio conditions

## Architecture Onboarding

**Component Map**: Clinical cases -> Voice synthesis (36 profiles) -> Audio LLMs (6 models) -> Surgery recommendation rate calculation -> Statistical bias analysis

**Critical Path**: The critical path involves synthesizing clinical cases into speech, processing through audio LLMs, and statistically analyzing recommendation rates across demographic subgroups while controlling for text baseline.

**Design Tradeoffs**: The study uses synthetic voices for experimental control, sacrificing ecological validity for systematic variation of demographic attributes. This enables controlled bias measurement but may not capture all real-world voice variation.

**Failure Signatures**: Failure to detect emotion-based bias despite synthetic emotion variation suggests poor emotion recognition performance across models. Persistent age bias even with CoT prompting indicates deep-seated modality-dependent processing issues.

**First Experiments**: 
1. Validate Fisher's exact test results with alternative statistical thresholds (p<0.01, p<0.10)
2. Test whether CoT prompt modifications affect gender bias elimination
3. Compare ASR-only results with full audio to quantify paralinguistic feature contribution

## Open Questions the Paper Calls Out
None

## Limitations
- The study uses synthetic voices rather than real clinical recordings, limiting ecological validity
- Emotion bias analysis is fundamentally limited by poor emotion recognition performance across models
- Exact prompt templates and inference parameters are not fully specified, complicating reproduction

## Confidence

**High Confidence**: The core finding that audio LLMs exhibit demographic bias in clinical recommendations (particularly age-related disparities) is well-supported by the experimental design and statistical analysis.

**Medium Confidence**: The claim that explicit reasoning (CoT prompting) eliminates gender bias but not age bias is supported, though the exact mechanism requires the specific prompt text for full validation.

**Low Confidence**: Conclusions about emotion-based bias are unreliable due to the acknowledged poor emotion recognition performance (<17% accuracy).

## Next Checks

1. **Prompt Template Validation**: Obtain and test the exact DA and CoT prompt templates used in the study to verify that the observed gender bias elimination through reasoning is reproducible.

2. **ASR Pipeline Verification**: Replicate the Whisper-based ASR condition using the same segmentation and processing approach to confirm that the observed bias transfer from audio to text representations is consistent.

3. **Statistical Sensitivity Analysis**: Re-run the Fisher's exact tests with varying p-value thresholds (0.01, 0.05, 0.10) and effect size thresholds (1%, 2%, 5%) to assess the robustness of the claimed bias effects.