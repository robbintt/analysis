---
ver: rpa2
title: High-Layer Attention Pruning with Rescaling
arxiv_id: '2507.01900'
source_url: https://arxiv.org/abs/2507.01900
tags:
- pruning
- attention
- layer
- learning
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HARP (High-layer Attention Rescaled Pruning),
  a training-free structured pruning method that selectively removes attention heads
  from the higher layers of large language models. The approach leverages the observation
  that attention heads in deeper layers are less important due to increased token
  representation similarity.
---

# High-Layer Attention Pruning with Rescaling

## Quick Facts
- **arXiv ID**: 2507.01900
- **Source URL**: https://arxiv.org/abs/2507.01900
- **Reference count**: 35
- **Primary result**: Training-free pruning method achieves 16.7% inference speedup on 65k-token sequences while maintaining performance across 27 datasets

## Executive Summary
This paper introduces HARP (High-layer Attention Rescaled Pruning), a training-free structured pruning method that selectively removes attention heads from the higher layers of large language models. The approach leverages the observation that attention heads in deeper layers are less important due to increased token representation similarity. HARP prunes query and key parameters in these high layers, allowing the attention mechanism to be skipped entirely, and introduces an adaptive rescaling parameter to maintain representation scale after pruning. Comprehensive experiments on LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B demonstrate HARP's superiority over existing methods across 27 datasets, particularly in generation tasks.

## Method Summary
HARP identifies high-layer attention heads as redundant due to over-smoothing, where token representations become increasingly similar with depth, causing attention to degenerate into simple averaging. The method removes query and key weight matrices from the top P layers, bypassing the full attention computation. To compensate for magnitude changes from skipping the aggregation step, HARP introduces an adaptive rescaling parameter α in the residual connection. This parameter is optimized through a layer-wise greedy search using perplexity as the evaluation metric, starting from the top layer and proceeding downward. The pruning and rescaling are applied without retraining, making the method efficient and practical for deployment.

## Key Results
- Achieves up to 16.7% inference speedup for 65k-token sequences while maintaining or improving task performance
- Outperforms existing methods (FLAP, Wanda-sp) across 27 datasets including TriviaQA, GSM8K, and MMLU
- Shows significant advantages for generation tasks compared to discriminative tasks
- Maintains performance with only ~3.3% parameter reduction by targeting the O(N²) attention bottleneck

## Why This Works (Mechanism)

### Mechanism 1: Over-smoothing Induced High-Layer Redundancy
- Claim: Pruning attention heads in higher layers causes less performance degradation than pruning lower layers.
- Mechanism: As network depth increases, token representations become increasingly aligned (high pairwise cosine similarity). This causes the attention operation in higher layers to degenerate into simple averaging, which provides little discriminative value.
- Core assumption: The similarity-based degeneration of attention is a primary driver of redundancy in high layers (Theorem 1).
- Evidence anchors:
  - [abstract] "token representations become increasingly similar (over-smoothed) as depth increases, making attention operations degenerate into simple averaging."
  - [section 2.2] Theorem 1: "When Sim(H(ℓ)) → 1... The head thus degenerates into averaging value vectors and cannot distinguish tokens."
  - [corpus] Corpus signals indicate related work on "Garbage Attention" and "BOS Sink Heads" supports the idea of systematic redundancy in specific attention components.
- Break condition: If lower layers also exhibit extremely high token similarity (Sim(H) ≈ 1) or if a specific task relies heavily on the averaging behavior of high-layer attention.

### Mechanism 2: Representation Magnitude Rescaling
- Claim: An adaptive rescaling parameter (α) in the residual block compensates for representation scale changes caused by bypassing the attention aggregation step.
- Mechanism: The normal attention aggregation (a row-normalized weighted average) alters the Frobenius norm of the hidden state matrix (||H'||_F ≠ ||H||_F). Bypassing it changes the magnitude, which is corrected by a learned α (H_new = H + α * H * W_v * W_o).
- Core assumption: Maintaining the pre-pruning magnitude of token representations is critical for stable downstream layer performance.
- Evidence anchors:
  - [abstract] "To compensate for the resulting change in representation scale due to bypassing the aggregation step, the authors introduce an adaptive rescaling parameter..."
  - [section 2.3] Proposition 1: "...the aggregation step changes the magnitude of token representations."
  - [corpus] Corpus evidence for this specific α-based correction is weak in related pruning papers; this appears to be a novel contribution.
- Break condition: If a specific LLM's residual connections or LayerNorm are robust enough to handle magnitude shifts without the α parameter.

### Mechanism 3: Perplexity-Guided Greedy Search for Alpha
- Claim: A layer-wise, top-down greedy search using perplexity effectively identifies optimal α values without expensive retraining.
- Mechanism: Instead of making α a differentiable trainable parameter, it is treated as a discrete hyperparameter. The search starts from the top layer, finds the α (from a grid {0, 0.1...1.0}) that minimizes perplexity, fixes it, and moves to the next lower layer.
- Core assumption: Perplexity on a held-out corpus (e.g., WikiText) is a reliable proxy for overall model performance post-pruning, and the top-down greedy approach yields a near-optimal solution.
- Evidence anchors:
  - [abstract] "This parameter is optimized through a layer-wise greedy search using perplexity as the evaluation metric."
  - [section 2.3] Algorithm 1 outlines the "Top-down α search for HARP."
  - [corpus] Corpus evidence for this specific greedy method is not highlighted; the approach is presented as an efficient alternative to training-based methods.
- Break condition: If the optimal α values are strongly interdependent across layers (violating the greedy assumption) or if perplexity does not correlate with task-specific performance.

## Foundational Learning

- Concept: **Self-Attention Complexity and Bottlenecks**
  - Why needed here: Understanding why pruning attention is crucial for long-context tasks. The O(N²) complexity means attention becomes the dominant cost as sequence length N grows.
  - Quick check question: For a sequence length N=16,000 and hidden dimension d=4096, which operation is likely the primary computational bottleneck: the FFN (O(N·d²)) or the self-attention (O(N²·d))?

- Concept: **Over-smoothing in Deep Networks**
  - Why needed here: Grasping the core theoretical justification for why high-layer attention is redundant. Over-smoothing explains why token representations become indistinguishable.
  - Quick check question: If the average pairwise cosine similarity (Sim(H)) of token representations at a given layer is 0.99, what does this imply about the ability of an attention head to distinguish between tokens?

- Concept: **Residual Connections and Representation Scale**
  - Why needed here: Understanding the need for the rescaling (α) parameter. Bypassing attention alters the signal magnitude passed through the residual connection.
  - Quick check question: In a standard transformer block with a residual connection (Output = x + Attention(x)), what happens to the output's Frobenius norm if the Attention(x) term is removed entirely?

## Architecture Onboarding

- Component map: Pruning target (query and key parameters in top P layers) -> Bypass mechanism (skip attention computation) -> Adaptive rescaling (α parameter in residual) -> Greedy search (top-down perplexity optimization)

- Critical path:
    1.  **Identify Pruning Depth (P):** Determine how many of the top layers to prune based on the desired attention head pruning ratio (e.g., 8 layers for LLaMA3.1-8B at a 1/4 ratio)
    2.  **Top-Down Alpha Search:** For each layer ℓ from L-1 down to L-P, perform a grid search for α (0.0 to 1.0 in 0.1 steps) to find the value that yields the lowest perplexity on a calibration dataset (e.g., WikiText)
    3.  **Apply Pruning & Rescaling:** Physically remove the W_Q and W_K parameters from the model. Modify the forward pass of these layers to implement the simplified, α-scaled computation
    4.  **Inference:** The pruned model now runs the modified forward pass for the top P layers, providing speedups especially for long sequences

- Design tradeoffs:
    - **Search Cost vs. Retraining Cost:** The greedy search requires multiple forward passes (e.g., 80 passes for 8 layers with 10 alpha candidates), which is more expensive than one-shot metrics but far cheaper than full retraining
    - **Generation vs. Discriminative Performance:** The paper notes pruning affects generation tasks (longer responses) more than discriminative tasks (short selections). The method optimizes for perplexity, a proxy more aligned with generation quality
    - **Parameter Reduction vs. Speedup:** HARP yields a modest parameter reduction (~3.3%) but a significant speedup (16.7% for 65k tokens) because it targets the O(N²) attention bottleneck, which is critical for long contexts

- Failure signatures:
    - **Catastrophic Generation Collapse:** If α is not tuned or is set to an inappropriate default (e.g., 1.0 for all layers), performance on generation tasks like GSM8K can drop to near zero (Table 6)
    - **No Perplexity Improvement:** If the searched α values are unstable across different calibration datasets, the method's robustness may be compromised. The paper shows α values derived from WikiText and Pile10K are similar, but this may not hold for all datasets
    - **Performance Drop on Short Contexts:** While designed for long-context gains, the method may not provide benefits and could even slightly degrade performance on tasks dominated by short sequences where attention is not the bottleneck

- First 3 experiments:
    1.  **Validate the core claim:** Prune the top 8 layers of LLaMA3.1-8B using HARP and compare generation task performance (e.g., TriviaQA, GSM8K) against baseline methods (e.g., FLAP, Wanda-sp) under the same attention head pruning ratio
    2.  **Ablate the rescaling parameter:** Run HARP with α=1.0 (no search) vs. HARP with searched α values. Compare perplexity and generation performance to quantify the contribution of the rescaling
    3.  **Measure efficiency gains:** Benchmark inference time for both the original and HARP-pruned models across a range of sequence lengths (e.g., 1k to 65k tokens) to confirm the theoretical speedup in long-context scenarios

## Open Questions the Paper Calls Out

- **Can Bayesian optimization or differentiable relaxations improve the efficiency and effectiveness of the rescaling parameter (α) search compared to the current greedy grid search?**
  - Basis: [explicit] The authors state: "Designing more advanced search strategies (e.g., Bayesian optimization or differentiable relaxations) is an interesting open direction for future work."
  - Why unresolved: The current greedy grid search over {0, 0.1, ..., 1.0} is simple but potentially suboptimal and requires ~80 forward passes for 8 layers
  - Evidence: Comparative experiments showing perplexity, downstream task performance, and search cost using advanced optimization methods versus grid search

- **How does HARP perform when applied to larger models (e.g., >70B parameters) and multimodal language models where attention mechanisms may serve different functions?**
  - Basis: [explicit] The conclusion states: "Future work includes extending HARP to larger and multimodal models."
  - Why unresolved: The over-smoothing hypothesis and attention redundancy may differ fundamentally in multimodal settings where attention crosses modalities; the paper only tested 7-9B models
  - Evidence: Experiments applying HARP to vision-language models with analysis of cross-modal attention patterns in higher layers

- **Can HARP be effectively combined with quantization and KV-cache compression, and do these methods interact synergistically or create conflicts?**
  - Basis: [explicit] The conclusion lists "combining it with techniques such as quantization or KV-cache compression" as future work
  - Why unresolved: Different compression techniques may interfere; the rescaling after attention pruning may conflict with quantization-aware scaling schemes
  - Evidence: Ablation studies measuring combined speedup and accuracy retention when applying HARP alongside quantization and KV-cache compression

- **Can task-specific pruning criteria improve performance for specialized downstream tasks compared to the current perplexity-based alpha selection?**
  - Basis: [explicit] The authors call for "developing more task-aware pruning criteria to further improve robustness and efficiency"
  - Why unresolved: Perplexity is a generic metric; certain tasks (e.g., mathematical reasoning, code generation) may benefit from preserving specific attention patterns that perplexity doesn't capture
  - Evidence: Comparing perplexity-based α selection against task-aware metrics on specialized benchmarks like GSM8K or HumanEval

## Limitations

- The pruning mechanism relies heavily on the over-smoothing hypothesis, which assumes token representations in deep layers converge toward identical values, though empirical validation across different architectures would strengthen this foundation
- The adaptive rescaling parameter introduces a novel correction that lacks extensive comparison with alternative normalization strategies, leaving questions about whether simpler techniques could achieve similar results
- The greedy perplexity-guided search for α values is presented as efficient but untested against alternative search strategies, and the assumption of near-optimal solutions lacks theoretical guarantees

## Confidence

- **High Confidence**: The efficiency gains from pruning high-layer attention are well-established. The O(N²) complexity reduction is mathematically sound, and the experimental results showing 16.7% speedup for 65k-token sequences are directly measurable and reproducible
- **Medium Confidence**: The superiority over existing methods across 27 datasets is supported by comprehensive experiments, but the comparison framework has limitations. The paper doesn't provide ablation studies isolating the contribution of high-layer pruning from the rescaling mechanism
- **Low Confidence**: The greedy perplexity-guided search for α values is presented as efficient but untested against alternative search strategies. The assumption that top-down greedy optimization yields near-optimal solutions lacks theoretical guarantees, and the sensitivity to different calibration datasets remains unclear

## Next Checks

1. **Ablation Study on Alpha Parameter**: Implement HARP without the rescaling parameter (α=1.0 fixed) and compare performance across all 27 datasets to isolate the contribution of the adaptive rescaling mechanism

2. **Cross-Dataset Alpha Stability**: Run the top-down α search using different calibration datasets (WikiText, Pile10K, and two additional datasets) and measure the variance in selected α values and downstream performance to test the robustness of the greedy search approach

3. **Token Similarity Validation**: Compute and visualize the average pairwise cosine similarity of token representations at each layer for multiple models (LLaMA3.1-8B, Mistral-7B, Qwen2-7B) across different tasks to empirically validate the over-smoothing hypothesis