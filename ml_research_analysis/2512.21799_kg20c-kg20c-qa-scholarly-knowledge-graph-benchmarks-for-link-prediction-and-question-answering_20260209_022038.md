---
ver: rpa2
title: 'KG20C & KG20C-QA: Scholarly Knowledge Graph Benchmarks for Link Prediction
  and Question Answering'
arxiv_id: '2512.21799'
source_url: https://arxiv.org/abs/2512.21799
tags:
- scholarly
- graph
- kg20c
- knowledge
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KG20C and KG20C-QA, two curated datasets
  for question answering on scholarly knowledge graphs. KG20C is a high-quality scholarly
  KG constructed from Microsoft Academic Graph, covering 20 top computer science conferences
  with standardized train/validation/test splits.
---

# KG20C & KG20C-QA: Scholarly Knowledge Graph Benchmarks for Link Prediction and Question Answering

## Quick Facts
- arXiv ID: 2512.21799
- Source URL: https://arxiv.org/abs/2512.21799
- Authors: Hung-Nghiep Tran; Atsuhiro Takasu
- Reference count: 30
- Primary result: MEI achieves MRR 0.230, Hit@1 0.157 on KG20C link prediction

## Executive Summary
This paper introduces KG20C and KG20C-QA, two curated datasets for question answering on scholarly knowledge graphs. KG20C is a high-quality scholarly KG constructed from Microsoft Academic Graph, covering 20 top computer science conferences with standardized train/validation/test splits. KG20C-QA extends KG20C by providing question-answer pairs in both natural language and entity-relation forms, enabling evaluation of both text-based and graph-based QA models. Baseline experiments show that multi-relational embedding models (CP h, MEI) outperform single-relational baselines, with MEI achieving best performance (MRR 0.230, Hit@1 0.157), though results indicate the benchmarks remain challenging and not saturated.

## Method Summary
KG20C is constructed by selecting 20 top computer science conferences via CORE 2020 A* ranking, applying citation threshold filtering (≥20 citations), and ensuring entity/relation coverage guarantees with careful split design to prevent inverse-relation leakage. KG20C-QA provides question-answer pairs in both natural language and entity-relation forms, covering 96K train, 7K valid, and 7K test QA pairs. Baseline models include Random, Word2vec skipgram, CP-h (canonical polyadic), and MEI (matrix-enhanced interactions), trained with Adam optimizer using full softmax cross-entropy with negative sampling. Hyperparameters are tuned via random search, with early stopping on validation MRR and results reported as median of 3 random seeds.

## Key Results
- Multi-relational embedding models (CP-h, MEI) outperform single-relational baselines on KG20C link prediction
- MEI achieves best performance with MRR 0.230 and Hit@1 0.157
- QA performance varies substantially across relation types, with domain-related questions remaining particularly challenging
- Type-filtered evaluation reveals performance differences when restricting candidates to correct semantic types

## Why This Works (Mechanism)

### Mechanism 1
Multi-relational embedding models capture scholarly graph structure more effectively than single-relational baselines because they model relation-specific interactions. CP and MEI factorize the knowledge graph tensor into entity and relation embeddings, enabling learned representations to encode how different relation types (e.g., "author_write_paper" vs. "paper_cite_paper") modulate entity interactions.

### Mechanism 2
QA performance varies substantially across relation types due to differences in structural predictability and answer cardinality. Relations with constrained answer sets (e.g., "paper_in_venue" has few valid venues per paper) achieve higher Hits@k because the ranking task is inherently easier, while relations with many valid answers (e.g., "paper_in_domain" with broad topic coverage) create larger candidate pools and lower precision.

### Mechanism 3
Curated benchmark construction with quality filtering and leakage prevention enables reproducible KG evaluation that noisy large-scale graphs cannot provide. KG20C applies venue selection, citation threshold filtering, entity/relation coverage guarantees, and avoids inverse-relation leakage through careful split design.

## Foundational Learning

- **Knowledge Graph Embeddings (KGE)**: Understanding how CP and MEI represent entities and relations as vectors is essential to interpret the baseline results. *Quick check*: Can you explain why a tensor factorization model like CP would capture relation-specific semantics better than a skipgram model trained on random walks?

- **Filtered Evaluation Protocol**: The paper uses "filtered" MRR and Hits@k, which removes known true triples from candidate rankings during evaluation. *Quick check*: If evaluating the triple (Paper_A, cites, Paper_B), why must we filter out Paper_C if (Paper_A, cites, Paper_C) is also in the training set?

- **Type-Filtered Metrics**: KG20C-QA introduces type-filtered evaluation where candidates are restricted to semantically valid types (e.g., only Venue entities for publication queries). *Quick check*: How would type-filtered evaluation change the effective difficulty of a "Who wrote this paper?" query compared to unconstrained ranking?

## Architecture Onboarding

- **Component map**: TSV files (train.txt, valid.txt, test.txt) -> Entity/relation metadata -> Embedding layer (100 effective dims) -> Scoring layer (model-specific interactions) -> Training loop (Adam, early stopping) -> Evaluation layer (filtered protocol, type-filtered variant)

- **Critical path**: 1) Load and validate KG20C splits (ensure train/valid/test disjointness, entity coverage) 2) Initialize embeddings with consistent dimensionality (100 effective dims across models) 3) Train with early stopping on validation MRR 4) Evaluate on test set using filtered protocol, optionally with type constraints

- **Design tradeoffs**: Dataset scale vs. quality (smaller but cleaner vs. massive but noisy), one-hop vs. multi-hop QA (single-relation only vs. planned multi-hop), graph vs. text evaluation (entity-relation form benchmarked vs. natural language deferred)

- **Failure signatures**: Random-baseline-level performance (~0 MRR) indicates embedding initialization failure, data loading errors, or incorrect loss function; training loss decreases but validation MRR flat suggests overfitting to training relations; large gap between type-filtered and unconstrained metrics indicates model relying on type distribution priors

- **First 3 experiments**: 1) Reproduce MEI baseline (target MRR ≈ 0.230) 2) Ablate relation types (train separate models on subsets to isolate performance drivers) 3) Type-filtered vs. unconstrained comparison (quantify type prior effects)

## Open Questions the Paper Calls Out

- **Multi-hop reasoning implementation**: How can multi-hop reasoning be effectively implemented on KG20C-QA, and what performance gains are achievable compared to one-hop baselines? The current release only emphasizes one-hop queries, establishing foundation for multi-hop extension.

- **LLM vs. graph-based methods**: How do large language models perform on KG20C-QA when answering questions in natural language form compared to graph-based embedding methods on entity-relation form? Baseline experiments only evaluated graph-based methods on entity-relation form.

- **Modeling challenging relations**: What modeling innovations can improve embedding performance on challenging relation types such as paper-domain associations and citation prediction? Current embeddings struggle with relations involving many-to-many mappings and sparse connectivity.

## Limitations

- KG20C covers only 20 top CS conferences from 1990-2010, limiting generalizability to other domains and time periods
- Only one-hop QA templates have been constructed, with multi-hop reasoning left for future work
- No LLM evaluation has been conducted on the natural language form of KG20C-QA questions

## Confidence

- Multi-relational models outperform single-relational baselines: **High confidence** (clear performance gap MRR 0.230 vs. 0.068)
- Relation types exhibit varying difficulty: **Medium confidence** (supported by type-filtered results but no external validation)
- Curated benchmarks enable reproducible evaluation: **High confidence** (rigorous filtering protocol described)

## Next Checks

1. Implement MEI with exact hyperparameter tuning protocol (random search over learning rate, weight decay, dropout) to reproduce MRR ≈ 0.230
2. Conduct ablation study comparing type-filtered vs. unconstrained evaluation to quantify type prior effects
3. Verify KG20C split integrity by checking for inverse-relation leakage and entity coverage across train/valid/test sets