---
ver: rpa2
title: Pardon? Evaluating Conversational Repair in Large Audio-Language Models
arxiv_id: '2601.12973'
source_url: https://arxiv.org/abs/2601.12973
tags:
- audio
- repair
- semantic
- conversational
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the gap in evaluation of large audio-language
  models (LALMs), which focus on answer accuracy and robustness to acoustic perturbations
  but ignore conversational repair behavior when inputs become semantically unanswerable.
  To fill this gap, the authors introduce a repair-aware evaluation setting that explicitly
  distinguishes answerable from unanswerable spoken inputs using a semantic-acoustic
  masking protocol, where answer-critical semantic content is selectively removed.
---

# Pardon? Evaluating Conversational Repair in Large Audio-Language Models

## Quick Facts
- **arXiv ID:** 2601.12973
- **Source URL:** https://arxiv.org/abs/2601.12973
- **Reference count:** 17
- **Key outcome:** Introduces EAR score to evaluate conversational repair in LALMs, revealing that most models fail to recognize semantic unanswerability and do not initiate appropriate repair despite high accuracy on answerable inputs.

## Executive Summary
This work addresses a critical gap in large audio-language model (LALM) evaluation by introducing conversational repair as a distinct capability beyond accuracy and robustness. The authors propose the Evaluability Awareness and Repair (EAR) score, a non-compensatory metric that jointly evaluates task competence on answerable inputs and repair behavior on unanswerable inputs. Through semantic-acoustic masking protocols applied to two spoken QA benchmarks, they demonstrate that while many models achieve high accuracy on answerable queries, most fail to recognize semantic unanswerability and instead hallucinate responses. The EAR score exposes a significant gap between accuracy and conversational reliability, showing that current evaluation practices substantially overestimate real-world performance. Models exhibiting repair behavior, such as DeSTA2.5-Audio and Audio Flamingo 3, achieve substantially higher EAR scores, validating that repair awareness captures a distinct behavioral capability.

## Method Summary
The authors introduce a repair-aware evaluation framework for LALMs that explicitly distinguishes answerable from unanswerable spoken inputs using semantic-acoustic masking. They create paired input variants: answerable conditions with semantic-invariant masking (function words masked) and unanswerable conditions with semantic-degrading masking (answer-critical segments masked). Task competence (C) is measured as accuracy on answerable inputs, while repair behavior (R) is classified by an LLM-as-judge (GPT-4o) into explicit repair (R=1), generic refusal (R=0.5), or hallucination (R=0). The EAR score is computed as the harmonic mean of C and R, creating a non-compensatory metric that penalizes models excelling at one capability while failing at the other. Evaluation is conducted on two spoken QA benchmarks (WDYL and SLUE-SQA-5) using seven LALMs under zero-shot settings.

## Key Results
- Most LALMs achieve high accuracy (C) on answerable inputs but fail to recognize semantic unanswerability, resulting in low EAR scores due to hallucination behavior
- DeSTA2.5-Audio and Audio Flamingo 3 exhibit explicit repair behavior, achieving substantially higher EAR scores than accuracy-focused models like Baichuan-Omni
- EAR score exposes a significant gap between accuracy-centric evaluation and conversational reliability, revealing that current practices overestimate real-world reliability
- Models show heterogeneous sensitivity to different masking types, suggesting they rely on distinct acoustic cues to detect semantic unanswerability

## Why This Works (Mechanism)

### Mechanism 1: Semantic-Acoustic Masking Creates Controlled Answerability Conditions
Selectively masking answer-critical semantic content creates paired answerable/unanswerable inputs that isolate evaluability as an evaluation variable. The protocol uses text-to-audio alignment to identify ground-truth answer spans, then replaces those acoustic segments with controlled signals. This controls semantic completeness while preserving temporal structure, ensuring human listeners cannot answer correctly when answer-critical information is absent.

### Mechanism 2: Non-Compensatory EAR Score Penalizes One-Dimensional Competence
The harmonic mean formulation of EAR prevents high task accuracy from masking poor repair behavior. EAR = 2·C·R/(C+R) where C is task competence and R is repair behavior. Harmonic mean heavily penalizes imbalance—a model with C=90% but R=10% scores EAR≈18%, not ~50% as arithmetic mean would give.

### Mechanism 3: LLM-as-Judge Enables Scalable Repair Classification
GPT-4o can categorize model responses into explicit repair, generic refusal, or neither with sufficient consistency for benchmarking. The judge receives (query, masked context, response) and outputs categorical decisions based on a rubric distinguishing slot-aware repair, generic refusal, and hallucination/unaware answering. Categorical output reduces subjectivity in repair classification.

## Foundational Learning

- **Concept: Large Audio-Language Model (LALM) Architecture**
  - **Why needed here:** The paper evaluates modular LALMs that combine audio encoders with LLM backbones; understanding this separation clarifies where repair failures may originate.
  - **Quick check question:** Can you explain why masking acoustic segments might affect an LALM differently than masking text tokens in a text-only LLM?

- **Concept: Conversational Repair in Human Dialogue**
  - **Why needed here:** The evaluation targets a human-like capability—recognizing when information is missing and requesting clarification—rather than simple refusal or silence.
  - **Quick check question:** What distinguishes "explicit repair" (R=1) from "generic refusal" (R=0.5) in human conversation terms?

- **Concept: Non-Compensatory Metrics**
  - **Why needed here:** EAR's harmonic mean design is central to the paper's contribution; understanding why arithmetic mean would fail helps interpret results.
  - **Quick check question:** If Model A scores C=100%, R=25% and Model B scores C=75%, R=75%, which has higher EAR, and why does this matter for deployment decisions?

## Architecture Onboarding

- **Component map:**
Input Audio → Semantic-Acoustic Masking Module → Answerable/Unanswerable Variants → LALM Under Test (inference) → Response Output → LLM-as-Judge (GPT-4o) → Repair Classification → EAR Score Computation (harmonic mean of C and R)

- **Critical path:**
1. Ground-truth answer alignment to audio timestamps (most fragile step—alignment errors propagate)
2. Masking application ensuring answer-critical content removal
3. Judge prompt consistency across all responses
4. EAR aggregation at dataset level

- **Design tradeoffs:**
- **Masking type:** White noise vs. silence vs. music—paper shows relative rankings are stable but absolute scores vary; choose based on target deployment noise profile
- **Judge model selection:** Paper uses GPT-4o; smaller models may reduce cost but require validation
- **Single-turn vs. multi-turn:** Current framework evaluates single-turn repair; extending to dialogue requires additional context modeling

- **Failure signatures:**
- High C + Low R + Low EAR → Model hallucinates when uncertain (Baichuan-Omni pattern)
- Low C + High R → Over-cautious model, may refuse valid queries
- High variance across masking types → Model's repair behavior is acoustic-cue dependent, not semantically grounded

- **First 3 experiments:**
1. **Reproduce EAR baseline on WDYL subset:** Select 100 instances, apply semantic-degrading masking, run inference on 2-3 open-source models, verify judge classification consistency with paper's patterns (expect C≈70-80%, R<15% for most models)
2. **Ablate masking severity:** Compare minimally-aligned masking vs. expanded-window masking on same instances; confirm that increased masking severity raises R scores as paper predicts
3. **Test prompt enhancement:** Apply transcribe-then-answer prompting to a model with low baseline R; document whether R improves or degrades—this validates whether repair failures stem from prompt design or intrinsic capability gaps

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the EAR score be adapted as a differentiable training objective, and would optimizing for it improve conversational reliability in LALMs?
- **Basis in paper:** The authors state in the Limitations section that "EAR is not intended as a direct training objective" and motivate "development and evaluation of models that are not only accurate but also appropriately conversationally aware."
- **Why unresolved:** The current work only uses EAR for evaluation; no training experiments were conducted to determine whether repair awareness can be learned or improved through direct optimization.
- **What evidence would resolve it:** Training experiments comparing models fine-tuned with EAR-based objectives against baseline models on both answerable and unanswerable inputs, showing improved R scores without degrading C.

### Open Question 2
- **Question:** How does conversational repair behavior emerge and unfold in multi-turn dialogue settings where repair may depend on accumulated conversational context?
- **Basis in paper:** The Limitations section states: "our evaluation instantiates this framework only in single-turn interactions and does not explicitly model multi-turn conversational dynamics or long-term dialogue state tracking. In more realistic dialogue settings, repair may unfold over multiple turns."
- **Why unresolved:** The current framework evaluates repair only in isolated single-turn QA pairs, leaving unexplored how models track repair across extended interactions.
- **What evidence would resolve it:** Extension of the EAR framework to multi-turn dialogues, measuring whether models maintain repair awareness across conversation turns and successfully resolve ambiguities iteratively.

### Open Question 3
- **Question:** What specific acoustic or linguistic cues do different LALMs use to detect semantic unanswerability, and why do models show heterogeneous sensitivity to different masking types?
- **Basis in paper:** The sensitivity analysis finds "model-specific sensitivity to different masking realizations" and notes "this heterogeneity suggests that different models rely on distinct acoustic cues to detect semantic unanswerability, resulting in diverse responses under various forms of audio-level semantic degradation."
- **Why unresolved:** The paper observes the phenomenon but does not investigate the underlying mechanisms or representations that determine which cues each model uses.
- **What evidence would resolve it:** Ablation studies analyzing intermediate representations across masking types, or probing experiments identifying which acoustic features correlate with repair behavior in each model.

## Limitations
- Masking alignment accuracy is critical but not fully specified, creating potential reproducibility concerns
- LLM-as-judge consistency introduces model-dependent subjectivity without inter-annotator agreement metrics
- Single-turn evaluation scope may underestimate real-world repair capabilities in conversational settings

## Confidence
- **High Confidence:** The harmonic mean EAR formulation demonstrates robust discriminative power between models with similar accuracy but divergent repair behavior
- **Medium Confidence:** The semantic-acoustic masking protocol effectively isolates answerability conditions, though specific alignment methodology uncertainties remain
- **Medium Confidence:** The LLM-as-Judge paradigm provides scalable repair classification, but confidence is tempered by lack of inter-annotator reliability data

## Next Checks
1. **Reproduce EAR baseline on WDYL subset:** Process 100 instances through the complete pipeline—semantic-acoustic masking, LALM inference, GPT-4o repair classification, EAR calculation. Verify that model ranking patterns match paper's findings (e.g., DeSTA2.5-Audio achieving higher EAR than accuracy-focused models like Baichuan-Omni).

2. **Validate masking effectiveness through human evaluation:** Select 20 unanswerable instances and have human annotators independently classify responses into repair categories (R=1, 0.5, 0). Compare inter-annotator agreement with GPT-4o classifications to assess judge reliability.

3. **Test masking severity ablation:** Systematically vary masking window sizes (minimal vs expanded) for identical answer-critical segments. Confirm that increased masking severity correlates with improved R scores as predicted by the framework, validating that repair behavior responds to semantic completeness rather than acoustic artifacts.