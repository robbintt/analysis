---
ver: rpa2
title: 'LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods'
arxiv_id: '2505.16129'
source_url: https://arxiv.org/abs/2505.16129
tags:
- evaluation
- translation
- language
- zhang
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper challenges the direct scoring approach for using large
  language models (LLMs) in machine translation quality estimation (MTQE). Instead
  of prompting LLMs to assign numeric scores, the authors propose a generation-based
  method: using decoder-only LLMs to generate high-quality reference translations,
  then measuring semantic similarity between these references and machine translations
  using sentence embeddings.'
---

# LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods

## Quick Facts
- **arXiv ID:** 2505.16129
- **Source URL:** https://arxiv.org/abs/2505.16129
- **Reference count:** 14
- **Primary result:** Generation-based evaluation outperforms direct LLM scoring and reference-free MTME metrics on 8 language pairs.

## Executive Summary
This paper challenges the use of large language models (LLMs) for direct scoring in machine translation quality estimation (MTQE). Instead, it proposes a hybrid generation-based approach where decoder-only LLMs generate high-quality reference translations, and sentence embeddings measure semantic similarity between these references and machine translations. Evaluated across 8 language pairs, this method consistently outperforms both direct LLM scoring baselines and existing reference-free MTME metrics, particularly for low-resource languages. The approach leverages the strengths of both generative models (fluency) and embedding models (semantic comparison), offering a more stable, interpretable, and flexible evaluation paradigm.

## Method Summary
The method uses decoder-only LLMs to generate reference translations from source sentences, then computes semantic similarity between these references and machine translation outputs using sentence embeddings (all-mpnet-base-v2). This three-step process—generation, embedding, similarity scoring—replaces direct LLM regression to numeric scores. The approach aligns with the "Generative AI Paradox," exploiting LLMs' training objective (next-token prediction) rather than forcing them into misaligned regression tasks. It was tested on WMT22 data across 8 language pairs using 8 different LLMs, measuring correlation with human Direct Assessment scores via Spearman's ρ and Pearson's r.

## Key Results
- Generation-based evaluation consistently outperforms direct LLM scoring baselines across all 8 language pairs tested.
- The method shows superior stability and interpretability compared to direct scoring approaches.
- Particularly strong performance on low-resource language pairs where traditional reference-free metrics struggle.
- Supports hybrid evaluation architectures combining fluent generation with semantic assessment.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing evaluation into generation (producing references) and scoring (measuring similarity) yields higher segment-level correlation with human judgments than direct LLM scoring.
- **Mechanism:** A decoder-only LLM generates a reference translation from the source sentence. Then, a sentence embedding model (all-mpnet-base-v2) computes cosine similarity between this generated reference and the machine translation output. This similarity score serves as the quality estimate.
- **Core assumption:** The semantic gap between an LLM-generated high-quality reference and the MT output correlates with translation quality as perceived by humans.
- **Evidence anchors:**
  - [abstract] "generation-based evaluation paradigm that leverages decoder-only LLMs to produce high-quality references, followed by semantic similarity scoring using sentence embeddings"
  - [section 3.1] Three-step process explicitly defined: (1) generate references, (2) compute semantic similarity, (3) evaluate correlation with human DA scores
  - [corpus] No directly comparable corpus evidence; related papers address LLM scoring in different domains
- **Break condition:** If LLM-generated references are systematically lower quality than MT outputs for certain domains or language pairs, similarity scores will not discriminate quality effectively.

### Mechanism 2
- **Claim:** LLMs produce more stable and valid outputs when tasked with generation (their training objective) rather than regression to a numeric score.
- **Mechanism:** Decoder-only LLMs are trained for next-token prediction via causal language modeling. Direct scoring requires mapping input to a single numeric value—a regression task misaligned with training. Generation leverages the model's native capability, reducing failures and invalid outputs.
- **Core assumption:** The "Generative AI Paradox" explains performance gaps: models trained to generate fluently may not reliably evaluate semantic fidelity.
- **Evidence anchors:**
  - [section 2] "decoder-only models are trained for next-token prediction rather than regression"
  - [section 3.2] "prompting them to generate translations aligns with their training objective, resulting in fewer failures and a more robust evaluation pipeline"
  - [corpus] Weak evidence; corpus neighbors discuss LLM scorers in unrelated tasks
- **Break condition:** If generation quality degrades sharply for low-resource languages or specialized domains, the method's advantage over direct scoring diminishes.

### Mechanism 3
- **Claim:** Hybrid architectures combining decoder-only LLMs (for fluent generation) with encoder-based embedding models (for semantic comparison) outperform monolithic approaches.
- **Mechanism:** Each component operates in its strength regime—LLMs generate contextually appropriate, fluent references; encoder models (Sentence-BERT) produce embeddings optimized for semantic similarity measurement.
- **Core assumption:** Reference quality is the bottleneck in MT evaluation; generating high-quality synthetic references improves evaluation regardless of the downstream similarity metric.
- **Evidence anchors:**
  - [abstract] Method "outperforms both intra-LLM direct scoring baselines and external non-LLM reference-free metrics"
  - [section 3.2] WMT 2024 QE report "encouraging hybrid approaches"; method "aligns with this vision"
  - [corpus] No direct corpus validation; one neighbor paper (RetroTrust) uses ensemble scorers in retrosynthesis, suggesting broader applicability of hybrid scoring
- **Break condition:** If the embedding model fails to capture nuanced semantic equivalence (idioms, cultural references, paraphrase), similarity scores diverge from human judgment regardless of reference quality.

## Foundational Learning

- **Concept: Machine Translation Quality Estimation (MTQE)**
  - Why needed: The core problem addressed—automatically approximating human judgment of translation quality without gold references.
  - Quick check: Can you distinguish reference-based metrics (require human translation) from reference-free metrics (use source only)?

- **Concept: Sentence Embeddings & Cosine Similarity**
  - Why needed: The scoring component relies on embedding both the generated reference and MT output, then computing their similarity.
  - Quick check: If two sentences have cosine similarity of 0.95, what does that imply about their semantic relationship?

- **Concept: Decoder-only vs Encoder-only Architectures**
  - Why needed: The hybrid design depends on understanding why LLMs excel at generation while encoders excel at semantic comparison.
  - Quick check: Why might a model trained for next-token prediction struggle to output a calibrated numeric score?

## Architecture Onboarding

- **Component map:** Source sentence -> LLM (decoder-only) -> Generated reference -> Sentence-BERT embeddings -> Cosine similarity -> Quality score
- **Critical path:**
  1. Prompt engineering for reference generation (see Figure 2 prompt template)
  2. Clean generation extraction (single sentence, no alternatives or explanations)
  3. Embedding computation via Sentence-BERT
  4. Similarity scoring and correlation analysis
- **Design tradeoffs:**
  - Model scale: LLaMA-2-13B showed instability (461/867 failures on RO-EN); LLaMA-3-8B was more stable—larger ≠ better
  - Target language limitation: All experiments use English as target; generalization to other targets is untested
  - Reference control: Prompts can specify tone/style, but this adds complexity and may introduce inconsistency
- **Failure signatures:**
  - Invalid outputs: LLM returns explanations, multiple translations, or empty responses
  - Near-zero or negative correlation: Indicates method is not capturing quality signals for that language pair
  - Embedding degradation: Low-resource languages may have weaker sentence representations in all-mpnet-base-v2
- **First 3 experiments:**
  1. Replicate Experiment 1 baseline comparison: Test generation-based method vs. Qian et al. (2024) direct scoring on 3-5 language pairs using 2-3 open-source LLMs.
  2. Benchmark against MTME reference-free metrics: Compare to COMETKiwi, UniTE-src, COMET-QE on WMT22 data using Kendall's τ alongside ρ and r.
  3. Ablation on embedding models: Substitute all-mpnet-base-v2 with alternative sentence encoders (e.g., LaBSE, multilingual E5) to measure sensitivity to the semantic similarity component.

## Open Questions the Paper Calls Out
None

## Limitations
- Target language limitation: Evaluation only covers English as target language; effectiveness for non-English targets remains untested.
- Generation quality bottleneck: Entire method's reliability depends on LLM's ability to produce high-quality references, which may degrade for low-resource languages or specialized domains.
- Hyperparameter sensitivity: Critical decoding parameters (temperature, max_tokens, top_p) are unspecified, making exact replication difficult.

## Confidence
- **High confidence:** Hybrid architecture design (decoder+encoder) provides clear theoretical advantages and outperforms direct scoring baselines in reported experiments
- **Medium confidence:** Generation-based approach is more stable than direct scoring, though exact failure rates depend on decoding hyperparameters
- **Low confidence:** Claims about performance on low-resource pairs (NE-EN, UK-EN) require external validation due to limited evidence and potential LLM quality degradation

## Next Checks
1. **Generalization testing:** Evaluate the generation-based method on non-English target languages (e.g., FR→DE, ES→IT) to verify cross-lingual applicability beyond English-centric pairs.
2. **Component ablation:** Systematically replace the all-mpnet-base-v2 sentence encoder with alternatives (LaBSE, multilingual E5) to quantify sensitivity to the semantic similarity component and test robustness.
3. **Decoding hyperparameter sweep:** Conduct controlled experiments varying temperature, top_p, and max_tokens to identify optimal settings that maximize correlation stability across all language pairs, particularly low-resource ones.