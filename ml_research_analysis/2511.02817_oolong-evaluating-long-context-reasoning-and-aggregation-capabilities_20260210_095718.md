---
ver: rpa2
title: 'Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities'
arxiv_id: '2511.02817'
source_url: https://arxiv.org/abs/2511.02817
tags:
- context
- episode
- answer
- label
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Oolong, a benchmark designed to evaluate
  long-context reasoning and aggregation capabilities in language models. The key
  innovation is posing tasks that require analyzing individual segments of text and
  aggregating these analyses to answer distributional questions, going beyond simple
  retrieval or needle-in-a-haystack tasks.
---

# Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities

## Quick Facts
- arXiv ID: 2511.02817
- Source URL: https://arxiv.org/abs/2511.02817
- Reference count: 13
- Primary result: Frontier models achieve less than 50% accuracy on distributional aggregation tasks over 128K context

## Executive Summary
Oolong introduces a benchmark for evaluating long-context reasoning and aggregation capabilities in language models. Unlike existing benchmarks that focus on retrieval or simple question-answering, Oolong requires models to analyze individual segments of text and aggregate these analyses to answer distributional questions. The benchmark includes both synthetic tasks (Oolong-synth) constructed from in-context learning datasets and real-world conversational data from Dungeons & Dragons transcripts (Oolong-real). Results show that even state-of-the-art models like GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro struggle with these aggregation tasks, achieving less than 50% accuracy at 128K context length.

## Method Summary
The benchmark consists of two task sets: Oolong-synth uses synthetic tasks constructed from in-context learning datasets, while Oolong-real uses real-world conversational data from Dungeons & Dragons transcripts. Tasks require models to identify relevant context, perform classification on individual segments, and aggregate results to answer distributional questions. The evaluation tests multi-step reasoning over long inputs by requiring models to process multiple segments and combine their analyses, going beyond simple retrieval or needle-in-a-haystack tasks.

## Key Results
- State-of-the-art models achieve less than 50% accuracy on both Oolong-synth and Oolong-real tasks at 128K context length
- Models struggle with aggregation tasks even when they can identify relevant context
- The gap between retrieval capabilities and aggregation reasoning highlights a fundamental limitation in current long-context models

## Why This Works (Mechanism)
The benchmark works by isolating the aggregation capability from other long-context skills through controlled task design. By requiring models to first identify relevant context, then perform per-segment classification, and finally aggregate these classifications, Oolong creates a clear multi-step pipeline that exposes weaknesses in reasoning over distributed information.

## Foundational Learning
- **Context identification**: Models must locate relevant segments within long inputs - needed to ensure aggregation tasks test reasoning rather than retrieval
- **Per-segment classification**: Individual analysis of each relevant segment - needed to establish the building blocks for aggregation
- **Distributional reasoning**: Combining multiple classification results into aggregate answers - needed to test the core aggregation capability
- **Multi-step reasoning**: Following the pipeline from context identification through aggregation - needed to evaluate complete reasoning chains
- **Synthetic task construction**: Creating controlled evaluation scenarios - needed to isolate specific capabilities
- **Real-world conversation analysis**: Processing dialogue transcripts - needed to validate synthetic findings in practical settings

## Architecture Onboarding

### Component Map
Context Identification -> Per-Segment Classification -> Distributional Aggregation -> Final Answer

### Critical Path
The critical path involves sequential processing: first identifying relevant context segments, then performing classification on each segment, and finally aggregating these classifications into a final answer. Performance bottlenecks can occur at any stage, but aggregation is identified as the primary weakness.

### Design Tradeoffs
The benchmark prioritizes controlled evaluation over real-world complexity. Synthetic tasks offer clean evaluation but may oversimplify real scenarios, while real-world tasks add authenticity but introduce uncontrolled variables. The 128K context limit balances computational feasibility with meaningful long-context evaluation.

### Failure Signatures
Models consistently fail at the aggregation stage even when they can identify relevant context and perform individual classifications. This manifests as correct per-segment analysis but incorrect final answers, indicating the aggregation step as the primary failure point.

### First Experiments
1. Test models on tasks where only one segment is relevant to isolate context identification from aggregation
2. Evaluate aggregation performance on pre-classified segments to isolate the aggregation capability
3. Compare performance on synthetic vs. real tasks to identify where complexity impacts aggregation ability

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses narrowly on distributional questions requiring aggregation, potentially missing other aspects of long-context reasoning
- Synthetic tasks may not generalize to more complex real-world scenarios
- Evaluation assumes reliable context identification, not accounting for ambiguous relevance scenarios
- Only tests up to 128K context length, potentially missing performance characteristics at larger windows

## Confidence
- High: Core claims about current model limitations are well-supported by consistent sub-50% accuracy across multiple state-of-the-art models
- Medium: Benchmark's ability to isolate aggregation capabilities is demonstrated but may oversimplify real-world complexity
- Medium: Claim that existing benchmarks inadequately test aggregation is supported but lacks comprehensive comparative analysis

## Next Checks
1. Test model performance on Oolong tasks with progressively longer context lengths (256K, 512K) to establish scaling patterns and identify context length thresholds where performance degrades
2. Conduct ablation studies where the context identification step is performed by a different system (e.g., retrieval-based) to isolate the impact of aggregation reasoning from context selection capabilities
3. Evaluate models on modified Oolong tasks where the relevant context spans multiple non-contiguous segments to test robustness of aggregation across more complex context structures