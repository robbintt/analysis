---
ver: rpa2
title: 'QuizRank: Picking Images by Quizzing VLMs'
arxiv_id: '2509.15059'
source_url: https://arxiv.org/abs/2509.15059
tags:
- image
- images
- questions
- answer
- wikipedia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QuizRank is a novel image ranking system for Wikipedia that uses
  vision-language models (VLMs) to evaluate images as educational aids. The method
  generates multiple-choice questions from article text about visual properties of
  the subject, then quizzes the VLM on images.
---

# QuizRank: Picking Images by Quizzing VLMs

## Quick Facts
- arXiv ID: 2509.15059
- Source URL: https://arxiv.org/abs/2509.15059
- Authors: Tenghao Ji; Eytan Adar
- Reference count: 10
- Primary result: VLMs rank images as educational aids with Pearson r=0.61 correlation to human judgments

## Executive Summary
QuizRank is a novel image ranking system for Wikipedia that uses vision-language models (VLMs) to evaluate images as educational aids. The method generates multiple-choice questions from article text about visual properties of the subject, then quizzes the VLM on images. Better images help the VLM answer more questions correctly. Contrastive question generation improves discrimination between similar concepts by focusing on distinguishing features. In evaluation, VLMs showed strong correlation with human performance (Pearson r=0.61), successfully distinguished in-class from distractor images, and achieved clear separation with contrastive questions (4/4 vs 0/4 correct).

## Method Summary
The QuizRank framework converts Wikipedia article text into multiple-choice questions about visual properties of concepts. VLMs take these quizzes while viewing candidate images, with images ranked by the percentage of questions answered correctly. The system uses base question generation from article text, then applies contrastive refinement when necessary to improve discrimination between similar concepts. A key innovation is the use of chain-of-thought reasoning and explicit instruction to prevent VLMs from using prior knowledge instead of visual evidence.

## Key Results
- VLMs showed strong correlation with human quiz-takers (Pearson r=0.61)
- System successfully distinguished in-class from distractor images
- Contrastive questions achieved clear separation (4/4 vs 0/4 correct in case study)
- ~6 questions provide stable rankings (Spearman ρ≥0.9) comparable to 10 questions

## Why This Works (Mechanism)

### Mechanism 1: Proxy Evaluation via VLM Competence
The system treats VLMs as proxies for human learners, operationalizing image quality as functional utility. If a VLM can answer more questions correctly when viewing a specific image, that image is likely a more effective illustrative aid. The core assumption is that VLM's ability to extract information from images correlates strongly with human ability to do the same.

### Mechanism 2: Contrastive Feature Isolation
Generating questions that explicitly target differences between a target concept and a distractor concept improves the ranking signal for visually similar items. This forces the VLM to look for discriminatory evidence rather than just category matching, resulting in better separation of in-class images from distractors.

### Mechanism 3: Context-Sanitized Reasoning
The system uses specific prompt engineering to prevent VLMs from "cheating" by using parametric memory. By forcing the model to justify its answer based on visual evidence rather than training data, the system ensures the score reflects the image's content, not the model's training corpus.

## Foundational Learning

- **Vision-Language Models (VLMs) as Evaluators**: VLMs are multimodal reasoning engines that can "see" and describe images. QuizRank repurposes them from "generators" to "graders." Quick check: Can you explain why a standard text-only LLM cannot perform the ranking task in QuizRank?

- **Prompt Engineering & Chain-of-Thought (CoT)**: The system relies heavily on prompt design to force reasoning. Quick check: Why does the prompt forbid mentioning the specific object name (e.g., "Donatello's David") in the question stem?

- **Information Retrieval vs. Visual Grounding**: A core challenge is distinguishing between the model "knowing" the answer (training data) and "seeing" the answer (visual grounding). Quick check: If a VLM answers "Bronze" for a statue material without looking at the image, is this a failure of the image or the evaluation mechanism?

## Architecture Onboarding

- **Component map**: Source Ingestion (Wikipedia Text + Image Candidates) -> Question Generator (LLM) -> Contrastive Module -> VLM Evaluator -> Ranker

- **Critical path**: The most sensitive component is the Contrastive Module. If the base questions do not distinguish between a high-quality image and a distractor (score difference < 2), the system triggers contrastive generation.

- **Design tradeoffs**: Base vs. Contrastive generation balances cost vs. precision. Quiz size optimization shows ~6 questions offer stability comparable to 10 questions. Model selection involves tradeoffs between correlation quality and computational cost.

- **Failure signatures**: "Generic" trap (all images score 100%), "Memory" leak (irrelevant images score 100%), Zero-score (high-quality images score 0%).

- **First 3 experiments**: 1) Reproduce Gujia vs. Chandrakala case study; 2) Memorization audit with post-training images; 3) Stress test on "new" images to confirm ranking performance holds for unindexed visual content.

## Open Questions the Paper Calls Out

### Open Question 1
Can the QuizRank framework be generalized to multilingual Wikipedia editions with distinct cultural visual references? The current evaluation is restricted to English Wikipedia, and it is unclear if generated questions transfer effectively across languages or if VLMs interpret culturally specific images consistently.

### Open Question 2
Does weighting questions by difficulty or feature importance improve the accuracy of image rankings compared to simple accuracy counts? The current implementation treats all questions equally, potentially overvaluing trivial visual features while undervaluing distinguishing ones.

### Open Question 3
Can the question generation pipeline be adapted to evaluate images for abstract, biographical, or historical concepts that lack distinct visual properties? The current prompt engineering focuses on observable physical attributes, which may fail for concepts where visual representation is symbolic or unavailable.

## Limitations
- Human correlation (r=0.61) suggests substantial unexplained variance between VLM and human judgment
- Distractor selection methodology remains underspecified, creating reproducibility barriers
- Performance on visually abstract or concept-rich images versus concrete object images is not systematically explored

## Confidence
- **High Confidence**: The general framework of using VLMs to evaluate images as educational aids through question-answering is well-supported
- **Medium Confidence**: The contrastive question generation method shows clear promise but needs further validation across diverse concept pairs
- **Low Confidence**: The assumption that VLM instruction-following is consistently reliable across all concepts and image types

## Next Checks
1. **Distractor Selection Robustness Test**: Implement and test multiple distractor selection strategies across 20 diverse concepts to determine which yields the most consistent contrastive performance.

2. **Visual Modality Impact Analysis**: Systematically evaluate QuizRank performance across different image types (photographs, illustrations, paintings, architectural diagrams) for 10 concepts per modality.

3. **VLM Memorization Audit Expansion**: Design targeted experiments where VLMs are tested on concepts with highly distinctive visual features that would be difficult to guess from text alone.