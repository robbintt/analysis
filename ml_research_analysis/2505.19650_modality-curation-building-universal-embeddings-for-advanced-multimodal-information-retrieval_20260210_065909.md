---
ver: rpa2
title: 'Modality Curation: Building Universal Embeddings for Advanced Multimodal Information
  Retrieval'
arxiv_id: '2505.19650'
source_url: https://arxiv.org/abs/2505.19650
tags:
- retrieval
- arxiv
- multimodal
- data
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UNITE, a universal multimodal embedding framework
  that addresses the challenge of heterogeneous data sources and cross-modal alignment
  in multimodal information retrieval. The core method involves strategic data curation
  across text, image, and video modalities, combined with Modal-Aware Masked Contrastive
  Learning (MAMCL) to mitigate competitive relationships among different modality
  instances.
---

# Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval

## Quick Facts
- arXiv ID: 2505.19650
- Source URL: https://arxiv.org/abs/2505.19650
- Reference count: 40
- Primary result: UNITE achieves 70.3% overall average score on MMEB benchmark, outperforming specialized models across 40+ retrieval tasks

## Executive Summary
This paper introduces UNITE, a universal multimodal embedding framework that addresses the challenge of heterogeneous data sources and cross-modal alignment in multimodal information retrieval. The framework strategically curates data across text, image, and video modalities, then employs Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate competitive relationships among different modality instances. UNITE demonstrates state-of-the-art performance on 40+ retrieval tasks, including fine-grained and instruction-based scenarios, surpassing existing specialized domain-specific models.

The work reveals several counterintuitive findings, including that training with video-text pairs outperforms image-text pairs on image-text retrieval tasks, and that fine-grained video-text alignment improves performance with text-text data but degrades it with video-text data. These discoveries challenge established assumptions in multimodal learning and suggest that strategic modality curation and tailored training protocols are pivotal for robust cross-modal representation learning.

## Method Summary
UNITE addresses multimodal information retrieval through strategic data curation and a novel training approach. The framework first curates high-quality data across text, image, and video modalities, then applies Modal-Aware Masked Contrastive Learning (MAMCL) to learn unified embeddings. MAMCL specifically addresses the competitive relationship between different modality instances during contrastive learning by masking and weighting different modalities appropriately. The approach enables the model to learn cross-modal representations that generalize across diverse retrieval tasks while maintaining strong performance on both spatial and temporal retrieval challenges.

## Key Results
- Achieves 70.3% overall average score on MMEB benchmark, surpassing larger models including mmE5 11B and IDMR 26B
- Attains 72.5% R@1 on WebVid-CoVR, demonstrating strong cross-modal retrieval capabilities
- Shows 86.5% R@1 on spatial retrieval tasks compared to 52.4% on temporal tasks, revealing modality-specific performance gaps
- Discovers that video-text training pairs outperform image-text pairs for image-text retrieval, contradicting conventional assumptions

## Why This Works (Mechanism)
UNITE works by addressing the fundamental challenge of learning unified representations across heterogeneous modalities through strategic data curation and tailored training objectives. The framework recognizes that different modalities have distinct characteristics - text provides semantic richness, images capture spatial relationships, and videos encode both spatial and temporal dynamics. By carefully curating data that maximizes cross-modal alignment while minimizing noise, UNITE creates an optimal training distribution. MAMCL then learns to map these diverse inputs into a shared embedding space while mitigating the competitive relationships that arise when contrasting instances from different modalities. This approach allows the model to develop robust cross-modal representations that transfer effectively across diverse retrieval tasks.

## Foundational Learning
- Multimodal contrastive learning: Why needed - to learn aligned representations across different modalities; Quick check - ensure contrastive loss properly aligns embeddings from different modalities
- Cross-modal alignment: Why needed - to enable meaningful similarity comparisons across text, image, and video; Quick check - verify retrieval performance on cross-modal tasks
- Data curation strategies: Why needed - to ensure high-quality, diverse training data that captures real-world distribution; Quick check - measure data quality metrics and downstream task performance
- Temporal understanding in video: Why needed - to capture both spatial and temporal relationships in video content; Quick check - evaluate performance on temporal retrieval tasks
- Negative sampling in contrastive learning: Why needed - to provide informative contrastive signals for learning discriminative representations; Quick check - analyze impact of negative sample size on retrieval accuracy

## Architecture Onboarding

**Component map:** Text/Image/Video Data Curation -> Modal-Aware Masked Contrastive Learning -> Unified Embedding Space -> Retrieval Tasks

**Critical path:** Data curation (ensuring quality and diversity) → MAMCL training (learning unified representations) → Retrieval adaptation (fine-tuning for specific tasks)

**Design tradeoffs:** The framework prioritizes universal generalization over modality-specific optimization, trading some modality-specific performance for broader applicability. The choice of MAMCL over standard contrastive learning addresses modality competition but adds complexity to the training pipeline.

**Failure signatures:** Poor cross-modal retrieval indicates issues with alignment during MAMCL training; strong text-only but weak cross-modal performance suggests data curation imbalance; temporal retrieval underperformance points to insufficient temporal understanding in the video modality.

**First experiments to run:**
1. Ablation study removing MAMCL to measure impact of modality-aware contrastive learning
2. Retrieval performance comparison using only image-text vs video-text training pairs
3. Cross-dataset generalization test on completely unseen retrieval benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does training with video-text pairs outperform image-text pairs on image-text retrieval tasks, contradicting conventional assumptions?
- Basis in paper: The paper states this finding "contradicts the established findings in conventional image-text research, challenging existing assumptions about the optimal data sources for such retrieval tasks."
- Why unresolved: The authors present this as a novel discovery but do not provide theoretical or empirical analysis explaining the underlying mechanism.
- What evidence would resolve it: A systematic study isolating factors like temporal dynamics, data diversity, or representational richness that transfer from video to image tasks.

### Open Question 2
- Question: Why does fine-grained video-text alignment improve performance when combined with text-text data but degrade performance when combined with video-text data?
- Basis in paper: Table 10 shows fine-grained alignment helps with TT data (1→2) but hurts with TV data (3→4, 5→6). The paper asks "How can we optimally utilize fine-grained video-text pairs?"
- Why unresolved: The interaction between alignment stages and retrieval adaptation data types remains poorly understood.
- What evidence would resolve it: Analysis of representation shifts during fine-grained alignment and how they interact with different retrieval adaptation objectives.

### Open Question 3
- Question: How can temporal video-text retrieval performance be substantially improved to match spatial retrieval performance levels?
- Basis in paper: The paper states "compared with the level that has been achieved in spatial tasks, there is still a great deal of room for improving temporal tasks."
- Why unresolved: Even with the 7B model, temporal retrieval (52.4% R@1) substantially lags behind spatial retrieval (86.5% R@1).
- What evidence would resolve it: Development of specialized temporal understanding modules or temporal-focused training data strategies.

### Open Question 4
- Question: How can audio modality be effectively integrated into the UNITE framework while maintaining balanced performance across all modalities?
- Basis in paper: The Limitations section states "audio emerges as another potential modality... Our exploration reveals that balancing multiple modalities remains challenging, suggesting the need for further investigation into modality expansion."
- Why unresolved: The paper acknowledges attempts but found balancing multiple modalities challenging; no solution is proposed.
- What evidence would resolve it: Successful extension of MAMCL to audio and demonstration of unified benchmarks across text, image, video, and audio.

## Limitations
- Heavy reliance on proprietary datasets and commercial APIs for data curation raises reproducibility concerns
- Evaluation focuses primarily on retrieval benchmarks without extensive testing of real-world deployment scenarios
- The 7B parameter version requires significant computational resources, limiting accessibility for smaller research groups

## Confidence

**State-of-the-art performance claims**: High confidence - supported by extensive benchmark comparisons across 40+ tasks with clear quantitative metrics.

**Universal embedding capabilities**: Medium confidence - while performance is strong across modalities, the paper lacks comprehensive ablation studies on truly cross-domain generalization beyond curated test sets.

**Curatorial approach efficacy**: Medium confidence - the data curation strategy shows promise but relies heavily on proprietary datasets, making independent verification challenging.

## Next Checks

1. Conduct ablation studies isolating the impact of each curatorial component (data filtering, modality alignment, negative sampling) to quantify their individual contributions to performance gains.

2. Test model performance on out-of-distribution datasets and tasks not included in the original curation pipeline to assess true generalization capabilities.

3. Perform bias and fairness audits across different demographic groups and content types to evaluate the ethical implications of the curation strategies.