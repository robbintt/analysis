---
ver: rpa2
title: 'Entangled in Representations: Mechanistic Investigation of Cultural Biases
  in Large Language Models'
arxiv_id: '2508.08879'
source_url: https://arxiv.org/abs/2508.08879
tags:
- cultural
- knowledge
- llms
- culture
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates cultural biases in large language models
  (LLMs) by introducing Culturescope, a mechanistic interpretability method that probes
  internal cultural knowledge representations. The authors propose a cultural flattening
  score to quantify how distinctive cultural knowledge is misrepresented through dominant
  cultures.
---

# Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models

## Quick Facts
- arXiv ID: 2508.08879
- Source URL: https://arxiv.org/abs/2508.08879
- Reference count: 40
- This paper introduces Culturescope, a mechanistic interpretability method to probe cultural knowledge representations in LLMs, finding Western-dominance bias and cultural flattening effects.

## Executive Summary
This study investigates cultural biases in large language models through a mechanistic interpretability lens, introducing Culturescope as a method to probe internal cultural knowledge representations. The authors develop a cultural flattening score to quantify how distinctive cultural knowledge gets misrepresented through dominant cultures. Using activation patching and attention analysis on multiple-choice questions with hard negative options, they reveal that LLMs exhibit systematic Western-dominance bias and cultural flattening in their internal representations. The research provides empirical evidence across three models (Llama-3.1, Aya-Expanse, and Qwen2.5) using the BLEnD and CAMeL-2 datasets, demonstrating asymmetric knowledge transfer patterns and validating Culturescope's effectiveness in eliciting relevant cultural knowledge.

## Method Summary
The paper introduces Culturescope, a mechanistic interpretability framework that probes cultural knowledge representations in LLMs through activation patching and attention analysis. The method focuses on multiple-choice question formats with carefully designed hard negative options to elicit cultural knowledge. A key innovation is the cultural flattening score, which quantifies how distinctively cultural information becomes homogenized through dominant cultural representations. The approach analyzes transformer-based models at the activation level, tracking how cultural knowledge flows through attention mechanisms and comparing responses across different cultural contexts. This mechanistic investigation reveals how cultural biases manifest internally within LLMs rather than just at the output level.

## Key Results
- Culturescope successfully quantifies Western-dominance bias and cultural flattening in LLMs through activation-level analysis
- Low-resource cultures show weaker susceptibility to cultural biases, attributed to limited parametric knowledge in training data
- The cultural flattening score effectively measures how distinctive cultural knowledge gets misrepresented through dominant cultural representations

## Why This Works (Mechanism)
The methodology works by examining how cultural knowledge is represented and processed at the activation level within transformer architectures. By using activation patching techniques, the researchers can isolate and measure how specific cultural knowledge units contribute to model outputs. The hard negative multiple-choice format forces models to access deeper cultural representations rather than surface-level associations. Attention analysis reveals which cultural representations dominate during reasoning, while the cultural flattening score provides a quantitative metric for measuring how distinctively cultural information becomes homogenized through dominant cultural representations.

## Foundational Learning
- **Cultural bias quantification**: Understanding how to measure cultural bias in AI systems is crucial for identifying systematic representational issues. Quick check: Verify that the cultural flattening score captures meaningful distinctions between cultural representations.
- **Mechanistic interpretability techniques**: Activation patching and attention analysis are essential tools for probing internal model representations. Quick check: Confirm that activation differences correspond to meaningful semantic changes in outputs.
- **Transformer architecture fundamentals**: Understanding how information flows through attention mechanisms is necessary to interpret cultural knowledge processing. Quick check: Map attention patterns to specific cultural knowledge pathways.
- **Cultural representation theory**: Knowledge of how cultures encode distinctive information helps design appropriate evaluation methods. Quick check: Validate that test questions capture genuinely distinctive cultural knowledge.
- **Dataset bias awareness**: Understanding how training data composition affects model representations is critical for interpretation. Quick check: Analyze training data distribution across cultural contexts.

## Architecture Onboarding

**Component map**: Input tokens → Embedding layer → Multi-head attention → Feed-forward networks → Output layer

**Critical path**: Cultural knowledge retrieval occurs primarily through attention mechanisms during the decoding process, where relevant cultural representations are activated and processed through feed-forward networks before generating output.

**Design tradeoffs**: The study prioritizes mechanistic interpretability over generation quality, focusing on controlled multiple-choice scenarios rather than free-form generation. This allows for precise measurement of cultural knowledge activation but may not capture all bias manifestations.

**Failure signatures**: Cultural flattening manifests as similar activation patterns across distinct cultural contexts, while Western-dominance bias shows as disproportionate attention to Western cultural representations during reasoning tasks.

**First experiments**:
1. Test Culturescope methodology on open-ended generation tasks to validate cross-format consistency
2. Apply cultural flattening score to additional cultural contexts beyond the initial test set
3. Conduct ablation studies removing specific cultural representations to measure impact on model outputs

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses primarily on multiple-choice question formats, potentially missing how cultural biases manifest in open-ended generation tasks
- The sample size of cultures examined may not represent the full diversity of cultural knowledge patterns across all regions
- Reliance on specific datasets (BLEnD and CAMeL-2) could introduce dataset-specific biases that limit generalizability

## Confidence
- Cultural flattening score effectively quantifies dominant culture bias: Medium confidence
- Western-dominance bias is prevalent in LLMs: Medium confidence
- Low-resource cultures show weaker cultural bias susceptibility: Low confidence

## Next Checks
1. Test Culturescope methodology on open-ended generation tasks rather than just multiple-choice questions to validate cross-format consistency
2. Expand cultural coverage by testing on additional cultures and datasets to verify whether findings hold beyond the current scope
3. Conduct ablation studies on training data to distinguish between genuine cultural independence and simply lack of representation for low-resource cultures