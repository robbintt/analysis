---
ver: rpa2
title: 'MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical
  Subject Classification'
arxiv_id: '2512.18256'
source_url: https://arxiv.org/abs/2512.18256
tags:
- problems
- mathematical
- performance
- reasoning
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MSC-180, a benchmark dataset for automated
  theorem proving that spans 60 mathematical disciplines and includes 180 problems
  ranging from undergraduate to graduate difficulty. Problems are formalized in Lean
  4 and curated from authoritative textbooks, with rigorous expert verification.
---

# MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical Subject Classification

## Quick Facts
- **arXiv ID:** 2512.18256
- **Source URL:** https://arxiv.org/abs/2512.18256
- **Reference count:** 40
- **Primary result:** 180 formalized problems across 60 mathematical domains reveal significant domain bias and limited cross-domain reasoning in leading 7B-parameter theorem provers

## Executive Summary
MSC-180 introduces a systematic benchmark for automated theorem proving spanning 60 mathematical disciplines from the Mathematics Subject Classification (MSC2020), featuring 180 problems ranging from undergraduate to graduate difficulty. Problems are formalized in Lean 4 and curated from authoritative textbooks with expert verification. The benchmark reveals significant performance gaps across domains, with the best 7B-parameter model achieving only 18.89% overall pass rate under pass@32 evaluation. Results show models struggle with core abstract mathematics while performing better on interdisciplinary applied problems, suggesting reliance on pattern matching rather than systematic reasoning.

## Method Summary
The benchmark evaluates three 7B-parameter theorem provers (DeepSeek-Prover-V2, Kimina-Prover, BFS-Prover) on 180 Lean 4 formalized problems across 60 MSC domains. Problems are curated from textbooks, formalized, and verified through multiple stages including automated checking and expert review. Models are evaluated using pass@k metrics (specifically pass@32), domain@k coverage ratios, and coefficient of variation (CV@k) to measure performance dispersion across domains. The evaluation uses single NVIDIA A100 80GB GPUs and different generation strategies per model paradigm.

## Key Results
- Overall pass@32 performance: 18.89% (best model DeepSeek-Prover-V2)
- Significant domain bias: CV@k values of 1.27-1.72 (4-6× above high-variability threshold)
- Graduate problems show 2× lower pass rates than undergraduate problems
- Core mathematics (algebra, topology) performs worse than interdisciplinary applied fields

## Why This Works (Mechanism)

### Mechanism 1: Domain Distribution Mismatch Detection via CV@k
- Claim: High coefficient of variation across mathematical domains indicates reliance on training distribution patterns rather than transferable reasoning.
- Mechanism: The paper introduces CV@k = σ/μ of per-domain Pass@k rates. Observed CV values (1.27–1.72) are 4–6× above statistical high-variability thresholds, suggesting performance concentrates in familiar domains rather than distributing evenly.
- Core assumption: Uniform cross-domain performance would indicate systematic reasoning capability; high variance indicates pattern-matching from training corpora.
- Evidence anchors:
  - [abstract] "The observed CV values are 4–6 times higher than the statistical high-variability threshold, indicating that the models still rely on pattern matching from training corpora rather than possessing transferable reasoning mechanisms."
  - [Section 4.3] DeepSeek-Prover-V2 shows lower CV (1.27) vs. BFS-Prover (1.72), correlating with template-based reasoning capability.
  - [corpus] Related work (DeepTheorem) notes similar formal-informal alignment challenges, but no direct CV@k comparisons exist.
- Break condition: If models trained with explicit cross-domain regularization achieve lower CV@k without improving absolute Pass@k, variance alone is insufficient as a reasoning quality signal.

### Mechanism 2: Hierarchical Decomposition Improves Sampling Efficiency
- Claim: Hierarchical/lemma-based proof generation (DeepSeek-Prover-V2) achieves better sampling efficiency on core mathematics than end-to-end or search-based approaches.
- Mechanism: Subgoal decomposition maps complex theorems to intermediate lemmas, reducing proof search depth per step. Increasing sampling budget from 8 to 32 improves core math performance (11.1% → 15.9%) more than interdisciplinary (24.1% → 25.9%).
- Core assumption: Core mathematical proofs have more diverse equally-valid paths; hierarchical methods better explore this space.
- Evidence anchors:
  - [Section 4.5] "Increasing the sampling budget leads to a much higher performance improvement in core mathematical fields... suggesting that core mathematical problems often have more diverse yet equally difficult proof paths."
  - [Section 4.1.1] DeepSeek-Prover-V2 "generates proofs by incorporating intermediate lemmas and hierarchical reasoning steps, mimicking the human approach of decomposing complex problems."
  - [corpus] Seed-Prover (related work) uses similar lemma-based approaches for IMO problems, but direct comparison data is unavailable.
- Break condition: If BFS-Prover with substantially larger search budgets matches hierarchical performance, decomposition advantage may be computational rather than architectural.

### Mechanism 3: Abstract Reasoning Gap in Core Mathematics
- Claim: Models perform better on interdisciplinary applied problems than core abstract mathematics due to structural similarity to procedural/code patterns in training data.
- Mechanism: Core mathematics (algebra, topology, number theory) requires abstract axiom manipulation. Applied fields (mathematical physics, CS-related math) involve computational/algorithmic structures resembling code. Models succeed more on the latter (e.g., DeepSeek on "relativistic scattering energy ratio").
- Core assumption: Pre-training corpora contain more procedural/algorithmic content than abstract mathematical reasoning traces.
- Evidence anchors:
  - [Section 4.5.1] "In fields such as Algebra (MSC 13), Number Theory (MSC 11), and Topology (MSC 54), proof construction heavily relies on the rigorous manipulation of abstract axioms and definitions. Models often require dozens of generation attempts yet still struggle."
  - [Section 4.5.1] "Success [in applied fields] may be attributed to the richness of such 'procedural' content in its pre-training data."
  - [corpus] IndiMathBench and related benchmarks focus on similar formalization challenges but lack MSC-domain granularity for comparison.
- Break condition: If models fine-tuned on abstract mathematics corpora show improved core math performance without architecture changes, the gap is data-distribution rather than reasoning-capability fundamental.

## Foundational Learning

- Concept: Lean 4 Tactics and Proof States
  - Why needed here: The benchmark formalizes all problems in Lean 4; understanding tactic generation (ext, simp, constructor) is essential to interpret model outputs and failure modes (e.g., BFS-Prover's circular simp reference in Appendix B).
  - Quick check question: Given a Lean goal `∀ x ∈ S, P x`, what tactic would you use to begin the proof?

- Concept: Pass@k Metric and Sampling
  - Why needed here: All results use Pass@k with different k interpretations per paradigm (independent samples vs. search iterations). Misunderstanding leads to invalid cross-model comparison.
  - Quick check question: If a model solves 3/10 problems with k=5 attempts each, what is its Pass@5?

- Concept: MSC2020 Classification Hierarchy
  - Why needed here: The benchmark's 60 domains map to MSC codes; understanding disciplinary groupings (core vs. interdisciplinary) is necessary to interpret domain bias analysis.
  - Quick check question: Which MSC codes would you expect to overlap with computer science? (Answer: 68, potentially 94)

## Architecture Onboarding

- Component map: Textbook OCR -> DeepSeek extraction -> Kimina-Autoformalizer (20 candidates/problem) -> Lean 4 compilation -> DeepSeek semantic verification -> Expert manual reconstruction
- Critical path: Start with kimina-lean-server setup -> validate on 3-5 problems manually -> run Pass@32 baseline on your model -> compute CV@k by domain -> analyze failure patterns by MSC category
- Design tradeoffs:
  - 3 problems/domain × 60 domains = broad coverage but shallow per-domain evaluation; increasing depth requires 3× expert hours per domain
  - Graduate-dominant difficulty ensures discriminative power but may underestimate undergraduate-level capability
  - Lean 4 formalization limits comparability with Isabelle/HOL-based benchmarks
- Failure signatures:
  - High CV@k with moderate Pass@k -> pattern-matching specialization (common in all three models)
  - Low Domain@k with high per-domain Pass@k in few domains -> narrow specialist
  - Circular reasoning in tactic generation (BFS-Prover example, Appendix B) -> search without semantic understanding
  - Incomplete proofs on graduate problems -> broken reasoning chains, missing implicit premises
- First 3 experiments:
  1. Establish baseline Pass@32 and CV@32 for your model; compare against DeepSeek-Prover-V2's 18.89% / 1.27 benchmark
  2. Stratify by difficulty: compute undergraduate vs. graduate pass rates; expect 2× gap if pattern-matching hypothesis holds
  3. Domain ablation: identify top-3 and bottom-3 performing MSC codes for your model; check training corpus overlap using domain keywords

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What training or architectural modifications could enable models to develop transferable reasoning mechanisms rather than relying on pattern matching from training corpora?
- Basis in paper: [explicit] The authors state that "the observed CV values are 4–6 times higher than the statistical high-variability threshold, indicating that the models still rely on pattern matching from training corpora rather than possessing transferable reasoning mechanisms and systematic generalization capabilities."
- Why unresolved: The paper identifies this as the core limitation but does not propose or test interventions to address it.
- What evidence would resolve it: Training models with domain-balanced curricula or meta-learning objectives, then measuring CV@k reduction on MSC-180.

### Open Question 2
- Question: What factors drive the consistent performance gap between core mathematics and interdisciplinary fields (e.g., 15.9% vs. 25.9% for DeepSeek-Prover at pass@32)?
- Basis in paper: [explicit] The paper reports that "performance of all models in core mathematical fields is significantly weaker than in cross-disciplinary applied fields" and attributes this to computational patterns versus abstract conceptual proofs.
- Why unresolved: The hypothesis is stated but not systematically tested against alternatives such as training data distribution bias or Mathlib library coverage differences.
- What evidence would resolve it: Ablation studies controlling for proof length, quantifier complexity, and Mathlib dependency depth across both problem categories.

### Open Question 3
- Question: How would larger-scale models (e.g., 70B+ parameters) perform on MSC-180, and would scaling reduce the observed CV@k dispersion?
- Basis in paper: [inferred] The evaluation was restricted to "three leading 7B-parameter theorem provers," leaving scaling effects unexplored despite acknowledged model capacity limitations.
- Why unresolved: The paper does not test whether scaling improves cross-domain generalization or merely amplifies pattern-matching capabilities.
- What evidence would resolve it: Benchmarking 70B and larger models on MSC-180 with CV@k analysis.

## Limitations

- Dataset scope limited to 3 problems per domain, potentially missing domain-specific difficulty variations
- Graduate-level bias may overestimate real-world undergraduate theorem proving capabilities
- Lean 4 formalization limits cross-benchmark comparability with Isabelle/HOL-based datasets

## Confidence

- High Confidence: Pass@32 results showing 18.89% overall performance, CV@k values indicating domain variance, graduate vs undergraduate performance gap
- Medium Confidence: Domain bias interpretation, hierarchical decomposition advantage hypothesis, training data bias explanation
- Low Confidence: Generalizability to larger models, long-term CV@k metric stability, real-world application extrapolation

## Next Checks

1. **Cross-Domain Regularization Test**: Train a model with explicit domain-balancing regularization and measure CV@k changes. If CV@k decreases without Pass@k improvement, variance alone may not indicate reasoning quality.

2. **Undergraduate Difficulty Subsample**: Extract and evaluate only undergraduate-level problems from MSC-180. If Pass@32 doubles while maintaining similar CV@k, the graduate-level difficulty may be masking broader capability patterns.

3. **Multi-Attempt Dependency Analysis**: For hierarchical models like DeepSeek-Prover-V2, track whether later attempts build on successful elements from earlier ones. If attempts show strong dependence, the Pass@k assumption of independence is violated and metrics should be adjusted.