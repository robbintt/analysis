---
ver: rpa2
title: Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs
arxiv_id: '2506.08633'
source_url: https://arxiv.org/abs/2506.08633
tags:
- speech
- dialogue
- training
- connector
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents a fully open-source approach to spoken dialogue
  state tracking (DST) by connecting pre-trained speech encoders and large language
  models (LLMs) through a trainable connector module. The system maps spoken user
  input and dialogue history to JSON-formatted dialogue states, training in two stages:
  first aligning the speech encoder and LLM via ASR pretraining, then jointly fine-tuning
  for ASR and DST with LoRA adapters.'
---

# Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs

## Quick Facts
- **arXiv ID**: 2506.08633
- **Source URL**: https://arxiv.org/abs/2506.08633
- **Reference count**: 0
- **Primary result**: WavLM-large + connector + OLMo-1B achieves 34.66% JGA on SpokenWOZ, with Gemma-2-9B-Instruct improving to 42.17% JGA

## Executive Summary
This work presents a fully open-source approach to spoken dialogue state tracking (DST) by connecting pre-trained speech encoders and large language models (LLMs) through a trainable connector module. The system maps spoken user input and dialogue history to JSON-formatted dialogue states, training in two stages: first aligning the speech encoder and LLM via ASR pretraining, then jointly fine-tuning for ASR and DST with LoRA adapters. Experiments on SpokenWOZ and Speech-Aware MultiWOZ datasets show that WavLM-large + connector + OLMo-1B achieves state-of-the-art 34.66% joint goal accuracy on SpokenWOZ, with Gemma-2-9B-Instruct further improving performance to 42.17% JGA. The approach demonstrates strong generalization and benefits significantly from fuzzy matching-based post-processing for named entity resolution.

## Method Summary
The approach connects WavLM-large speech encoder to LLMs (OLMo-1B or Gemma-2-9B-Instruct) through a 2-layer transformer connector preceded by a subsampling layer. The connector maps downsampled speech embeddings (6× reduction) into the LLM's embedding space, where they function as soft prompts. Training occurs in two stages: (1) ASR pretraining with frozen LLM, and (2) joint ASR-DST fine-tuning with LoRA adapters. The model outputs structured JSON dialogue states token-by-token, conditioning on both current user speech and accumulated dialogue history. Post-processing with fuzzy matching against entity databases improves named entity resolution.

## Key Results
- WavLM-large + connector + OLMo-1B achieves 34.66% JGA on SpokenWOZ
- Adding agent history improves JGA from 27.27% to 31.04%
- Fuzzy matching post-processing provides ~3% absolute JGA improvement
- Gemma-2-9B-Instruct with connector achieves 42.17% JGA on SpokenWOZ
- Two-stage training (ASR pretraining) improves JGA by ~5.5% absolute (22.43→27.89)

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Training for Cross-Modal Alignment
The two-stage training approach (ASR pretraining → joint ASR-DST fine-tuning) enables robust cross-modal alignment before task-specific adaptation. Stage 1 freezes the LLM and trains only the encoder + connector on large-scale ASR data, forcing the connector to learn a meaningful mapping from speech representations to the LLM's text embedding space. Stage 2 then freezes the encoder, adds LoRA adapters to the LLM, and fine-tunes connector + LoRA jointly for DST while maintaining ASR capability. Evidence shows A3 (lora16 with ASR init) achieves 27.89 JGA vs A1 (no ASR init, lora16) at 22.43 JGA, demonstrating ~5.5% absolute improvement from ASR pretraining.

### Mechanism 2: Connector with Subsampling for Sequence Matching
The connector module with subsampling acts as a learnable adapter that compresses speech sequences to match LLM token granularity while preserving semantic content. WavLM produces frame-level speech embeddings at ~50Hz. The subsampling layer stacks 6 neighboring embeddings and projects them (6× downsampling), reducing sequence length to be more comparable with text token rates. The 2-layer transformer connector then refines these representations into the LLM's embedding dimension, where they function as "soft prompts" conditioning the LLM's generation. The subsampling preserves sufficient acoustic detail for DST slot values while making sequence lengths tractable for the LLM.

### Mechanism 3: Joint ASR-DST Training for Inference Consistency
Joint ASR-DST training is necessary because the model must generate its own dialogue history transcriptions at inference time. During training, the model learns to output both transcription and dialogue state in a single JSON string. This forces the ASR and DST components to share representations and ensures DST predictions can condition on model-generated (not oracle) transcriptions during the history accumulation at inference. Using Whisper context instead of decoded utterances improves JGA from 31.91 to 32.89, indicating ASR quality limits DST performance.

## Foundational Learning

- **Speech-text representation alignment**: Why needed - The core technical challenge is mapping speech encoder outputs (trained on acoustic objectives) into an LLM's semantic space (trained on text). Without alignment, the LLM cannot interpret speech embeddings meaningfully. Quick check: Can you explain why simply concatenating speech embeddings with text tokens would fail without a trained connector?

- **LoRA (Low-Rank Adaptation)**: Why needed - Full fine-tuning of multi-billion parameter LLMs is computationally expensive and risks catastrophic forgetting. LoRA adds trainable low-rank matrices to attention weights, enabling efficient adaptation while keeping base weights frozen. Quick check: If LoRA rank r=16 is used on an attention projection of size 4096×4096, how many trainable parameters does LoRA add per projection?

- **Autoregressive JSON generation**: Why needed - The model outputs structured dialogue states as JSON strings token-by-token. This requires the LLM to learn both the semantic content and syntactic format simultaneously. Quick check: What post-processing step does the paper use to handle cases where the model outputs an invalid slot value not in the database?

## Architecture Onboarding

- **Component map**: Speech Encoder (WavLM-large) -> Subsampling Layer (6×) -> Connector (2-layer transformer) -> LLM (OLMo-1B/Gemma-2-9B-Instruct with LoRA) -> Post-processor (Fuzzy matching)

- **Critical path**: 1) ASR pretraining (Stage 1): Train encoder + connector on Fisher/Switchboard/LibriSpeech/How2 with frozen LLM; 2) DST fine-tuning (Stage 2): Freeze encoder, train connector + LoRA on SpokenWOZ + MultiWOZ; 3) Optional: Single additional epoch on target dataset only (FT-sw or FT-mw); 4) Inference: Turn-by-turn JSON generation with fuzzy matching post-processing

- **Design tradeoffs**: Full fine-tuning vs. LoRA - Full FT yields better results but becomes impractical at scale; LoRA enables scalable training with modest performance gap. User-only vs. user+agent history - Including agent turns consistently improves JGA (e.g., A8→A9: 27.27→31.04) but increases context length. Open vs. closed models - OLMo-1B chosen for transparency/reproducibility; Gemma-2-9B achieves higher performance but with less training data transparency.

- **Failure signatures**: Overfitting on small datasets - Training only on SpokenWOZ leads to overfitting; augmenting with MultiWOZ helps. Domain mismatch - Models trained on conversational ASR data underperform on MultiWOZ human-verbatim test sets. Named entity errors - Without fuzzy matching, ~3% absolute JGA is lost; slot values not in database cause failures.

- **First 3 experiments**: 1) Connector ablation - Compare (a) random connector + LoRA (no ASR init), (b) connector-only training, (c) full 2-stage training. Expect replication of A1 (22.43), A2 (20.02), A3 (27.89) pattern on SpokenWOZ. 2) History ablation - With best configuration from #1, compare user-only vs. user+agent dialogue history. Expect ~3% JGA improvement from adding agent turns. 3) Post-processing impact - Measure JGA with and without fuzzy matching on predicted slot values. Expect ~3% absolute improvement, especially on named entity slots.

## Open Questions the Paper Calls Out

- **Scaling anomaly with OLMo-7B**: The paper notes that OLMo-7B did not outperform OLMo-1B despite success with Gemma-2-9B, but provides no analysis for this scaling anomaly. This leaves unclear whether the issue stems from OLMo architecture specifically, connector capacity, or hyperparameter settings for larger models.

- **Generalization to unseen domains**: The authors note that Gemma models "do not achieve nearly as stellar performance on the MWOZ dev and human-verbatim test sets," and explicitly state there is "room for improvement in terms of generalization to previously unseen named entities." The performance gap between SpokenWOZ (human speech) and MultiWOZ (TTS speech) suggests the model learns dataset-specific acoustic features rather than robust, domain-invariant speech-text alignments.

- **Native named entity generation**: The abstract and results highlight that "fuzzy matching-based output post-processing... greatly improves performance," providing a consistent 3-4% absolute increase in Joint Goal Accuracy. This reliance on database lookup implies the end-to-end model struggles to map acoustic signals to precise lexical entities, often outputting phonetically valid but incorrect strings.

## Limitations

- **Reproducibility barriers**: While claiming full open-sourcing, the core WavLM-large speech encoder is not open-source, and LoRA training details (optimizer hyperparameters, exact early stopping) are underspecified.

- **Dataset scale and overfitting**: SpokenWOZ is relatively small for LLM training, leading to overfitting risks that require augmentation with Speech-Aware MultiWOZ, which introduces domain mismatch problems.

- **Heavy reliance on post-processing**: The system's performance heavily depends on fuzzy matching post-processing for named entities, contributing ~3% absolute JGA and suggesting struggles with precise slot value generation.

## Confidence

- **High Confidence**: The two-stage training framework (ASR pretraining → joint ASR-DST fine-tuning) demonstrably improves performance over direct DST training, with clear quantitative evidence (A1→A3: 22.43→27.89 JGA).

- **Medium Confidence**: The connector architecture and subsampling mechanism are reasonable solutions to the speech-LLM alignment problem, but the exact subsampling factor (6×) and transformer configuration were likely determined empirically.

- **Low Confidence**: Claims about generalization to unseen domains are not directly tested. The SA-MultiWOZ augmentation shows some transfer capability, but the paper doesn't evaluate zero-shot or few-shot performance on truly novel dialogue domains.

## Next Checks

1. **Connector Architecture Ablation**: Systematically vary the subsampling factor (3×, 6×, 9×) and connector depth (1-3 layers) to determine if the reported architecture is optimal or one of many viable solutions.

2. **ASR Error Propagation Analysis**: Measure JGA degradation across dialogue turns when using model-generated vs. ground-truth transcriptions for dialogue history, quantifying the compounding error effect that the paper only briefly mentions.

3. **Named Entity Robustness Test**: Evaluate performance with and without fuzzy matching on a held-out subset of SpokenWOZ where slot values are intentionally perturbed (typos, paraphrases) to assess the model's intrinsic slot value generation accuracy.