---
ver: rpa2
title: 'Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment
  and Mapping of Urban Scenes'
arxiv_id: '2504.16538'
source_url: https://arxiv.org/abs/2504.16538
tags:
- urban
- street
- sagai
- task
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAGAI (Streetscape Analysis with Generative
  Artificial Intelligence), a four-step workflow that automates urban streetscape
  scoring using open-access geospatial data and vision-language models. The method
  integrates OpenStreetMap geometries, Google Street View imagery, and a lightweight
  LLaVA model to generate structured spatial indicators from images via customizable
  natural language prompts.
---

# Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes

## Quick Facts
- arXiv ID: 2504.16538
- Source URL: https://arxiv.org/abs/2504.16538
- Authors: Joan Perez; Giovanni Fusco
- Reference count: 40
- Key outcome: SAGAI automates urban streetscape scoring using OpenStreetMap geometries, Google Street View imagery, and LLaVA models to generate structured spatial indicators from images via customizable natural language prompts, achieving 91.67% accuracy for binary urban-rural classification, 64.17% for commercial feature detection, and 54.05% for sidewalk width estimation.

## Executive Summary
SAGAI introduces a four-step workflow that automates urban streetscape scoring using open-access geospatial data and vision-language models. The method integrates OpenStreetMap geometries, Google Street View imagery, and a lightweight LLaVA model to generate structured spatial indicators from images via customizable natural language prompts. Two case studies in Nice and Vienna demonstrate SAGAI's capacity to produce geospatial outputs from vision-language inference without task-specific training or proprietary software dependencies.

## Method Summary
The SAGAI workflow integrates OpenStreetMap street geometries with Google Street View imagery, processing each point from four cardinal directions through a zero-shot vision-language model (LLaVA v1.6 Mistral-7B) to extract structured numeric indicators via natural language prompts. The system aggregates point-level scores to street segments, enabling cartographic interpretation. Three tasks demonstrate the approach: binary urban-rural classification (91.67% accuracy), commercial feature detection (64.17% accuracy), and sidewalk width estimation (54.05% accuracy).

## Key Results
- SAGAI achieves 91.67% overall accuracy for binary urban-rural classification using zero-shot VLM inference
- Commercial feature detection shows 64.17% accuracy with documented tendency to hallucinate storefronts from non-commercial objects
- Sidewalk width estimation achieves 54.05% accuracy with systematic underestimation and strict upper bound of 2 meters
- The pipeline successfully generates geospatial outputs from vision-language inference without task-specific training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured natural language prompts can condition a vision-language model (VLM) to extract specific numeric indicators from unstructured street-level imagery without task-specific fine-tuning.
- **Mechanism:** The LLaVA v1.6 model encodes images via a CLIP-compatible vision encoder. A user-defined prompt acts as a conditional instruction, steering the LLM's next-token prediction to generate a constrained scalar output rather than open-ended text.
- **Core assumption:** The model's pre-trained visual and linguistic knowledge base contains sufficient conceptual understanding of urban features to map visual pixels to semantic definitions provided in the prompt.
- **Evidence anchors:** Abstract mentions generating "structured spatial indicators from images via customizable natural language prompts." Section 3.3 describes the custom scoring function embedding prompts with image tokens and using low-temperature sampling to extract concise scalar outputs.
- **Break condition:** Tasks requiring precise measurement or contextual reasoning cause the mechanism to fail or degrade, resulting in hallucinations or systematic underestimation.

### Mechanism 2
- **Claim:** Aggregating multi-directional point-inferences onto a topological network reduces directional noise and creates coherent street-level indicators.
- **Mechanism:** The workflow samples points along OpenStreetMap polylines, captures four orthogonal images per point, and aggregates scores (via mean or sum) to the street segment. This spatial smoothing filters out single-view anomalies and aligns visual data with the underlying street network topology.
- **Core assumption:** The features of interest are spatially persistent enough that a 40-meter sampling interval captures the signal without excessive redundancy or omission.
- **Evidence anchors:** Abstract states the pipeline includes "automated mapping that aggregates visual scores at both the point and street levels." Section 3.4 details the aggregation logic where point-level metrics are linked to unique street segment identifiers.
- **Break condition:** In areas with sparse imagery or complex urban form, gaps in Street View coverage break the spatial continuity, resulting in "grey" zones where the mechanism cannot produce data.

### Mechanism 3
- **Claim:** 4-bit quantization enables heavyweight transformer models to run on resource-constrained (free-tier) hardware with acceptable precision loss for coarse classification tasks.
- **Mechanism:** By loading the LLaVA Mistral-7B model in 4-bit quantized format, the system fits into the memory limits of standard Google Colab runtimes. This compression allows the inference mechanism to execute without crashing, trading floating-point precision for accessibility.
- **Core assumption:** The precision degradation introduced by quantization does not fundamentally alter the binary or low-cardinality classification capabilities required for the target tasks.
- **Evidence anchors:** Section 3.3 explicitly states the model is "loaded in 4-bit quantized format to support efficient inference in memory-constrained environments." Section 6 discusses the deliberate choice of the "lightest possible configuration" to ensure broad usability.
- **Break condition:** Tasks requiring fine-grained visual discrimination may degrade significantly, evidenced by the lower accuracy (54.05%) in the measuring task compared to categorization (91.67%).

## Foundational Learning

- **Concept: Vision-Language Models (VLMs) & Zero-Shot Inference**
  - **Why needed here:** The core engine of SAGAI is not a traditional classifier but a generative model. Understanding that LLaVA aligns image patches to text tokens is essential to grasp why "prompting" replaces "training."
  - **Quick check question:** How does a VLM differ from a standard object detector like YOLO when asked to find "gentrification" in an image?

- **Concept: Spatial Interpolation & Modifiable Areal Unit Problem (MAUP)**
  - **Why needed here:** SAGAI aggregates point data to street segments. The choice of spacing (40m) and aggregation method (mean vs. sum) fundamentally alters the output map.
  - **Quick check question:** If you halve the sampling distance to 20m, how would the "Total Storefront" sum for a street segment change, and is that a reflection of reality or methodology?

- **Concept: Model Quantization (PTQ)**
  - **Why needed here:** To replicate or adapt this pipeline, one must understand that running a 7B parameter model typically requires >14GB VRAM, which is reduced via 4-bit quantization to fit in free-tier Colabs.
  - **Quick check question:** What is the primary trade-off introduced when loading a Large Language Model in 4-bit precision versus 16-bit?

## Architecture Onboarding

- **Component map:** BBox coordinates -> OSM Generator -> GSV Downloader -> LLaVA Engine -> Aggregator
- **Critical path:** The dependency chain is strictly linear: Module 1 (Spatial) → Module 2 (Imagery) → Module 3 (Inference) → Module 4 (Mapping). A failure in the GSV API key (Module 2) or OSM connectivity (Module 1) halts the entire pipeline.
- **Design tradeoffs:**
  - **Accessibility vs. Accuracy:** The system uses LLaVA-Mistral-7B (quantized) instead of GPT-4V to ensure free, local execution. This results in lower precision for complex measurements (e.g., sidewalk width <2m limit).
  - **Sampling Density vs. Cost:** 40m spacing balances coverage against API quotas and inference time. Finer sampling increases cost linearly.
- **Failure signatures:**
  - **Hallucinated Storefronts:** The model frequently interprets utility boxes, waste containers, or parked trucks as "shopfronts" (Task 2 failure).
  - **The "Grass Strip" Error:** The model systematically misclassifies grass verges as sidewalks (Task 3 failure).
  - **Coverage Gaps:** "Grey" streets in the output map caused by strict filtering of invalid GSV images (detected by single-pixel-value dominance).
- **First 3 experiments:**
  1. **Smoke Test (Nice BBox):** Run Modules 1 & 2 on a 1km² area in Nice to verify the OSM/GSV connection and inspect the "no imagery" filters.
  2. **Prompt Sensitivity Analysis:** In Module 3, modify the Task 1 prompt to include "highway" vs. "residential" distinctions and compare the output CSV distribution against the default binary "urban/rural" prompt.
  3. **Resolution/Quantization Stress Test:** Attempt to load the model in 8-bit (if hardware permits) or run the "Measuring" task on a set of images with known sidewalk widths to verify the "never returns > 2 meters" limit reported in the paper.

## Open Questions the Paper Calls Out
None

## Limitations

- SAGAI is limited to urban environments with Google Street View coverage, creating blind spots in non-Western cities and rural areas
- The system cannot handle occluded or ambiguous imagery where urban features are partially hidden
- 40m sampling interval may miss important micro-features in dense urban environments
- No task-specific fine-tuning means the model relies entirely on pre-trained knowledge, limiting precision for specialized urban features

## Confidence

**High Confidence Claims (Supported by quantitative results and methodology):**
- SAGAI can successfully generate urban-rural classification maps using zero-shot VLM inference
- The pipeline architecture (OSM → GSV → LLaVA → aggregation) is technically feasible and reproducible
- Google Street View coverage gaps are a real constraint affecting data completeness
- The 4-bit quantization approach enables execution in free-tier environments

**Medium Confidence Claims (Plausible but require more validation):**
- LLaVA-Mistral-7B provides sufficient accuracy for urban research applications despite performance limitations
- The 40m sampling interval represents an optimal balance between coverage and resource efficiency
- Natural language prompts can effectively guide VLMs to extract numeric urban indicators
- The aggregation method (mean/sum per street segment) produces meaningful spatial patterns

**Low Confidence Claims (Theoretical or weakly supported):**
- SAGAI represents a generalizable framework that can be easily adapted to other urban research themes
- The accuracy levels achieved are sufficient for policy-relevant urban analysis
- The computational efficiency gains from 4-bit quantization do not significantly compromise analytical validity

## Next Checks

1. **Geographic Generalization Test:** Deploy SAGAI in three diverse cities (North American, Asian, African) to validate cross-cultural urban feature recognition and assess whether hallucination patterns persist across different architectural styles and urban forms.

2. **Resolution vs. Performance Trade-off Analysis:** Systematically vary the Street View image resolution (640x640 vs 512x512 vs 256x256) while measuring accuracy degradation for each task to quantify the precision loss from using lower-resolution imagery for computational efficiency.

3. **Temporal Stability Assessment:** Apply SAGAI to the same geographic areas using Street View imagery from different years (e.g., 2018, 2021, 2024) to evaluate whether the model's performance remains consistent as urban features change over time, particularly for commercial detection and sidewalk assessment.