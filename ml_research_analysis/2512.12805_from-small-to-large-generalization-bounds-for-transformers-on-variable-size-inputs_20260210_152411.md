---
ver: rpa2
title: 'From Small to Large: Generalization Bounds for Transformers on Variable-Size
  Inputs'
arxiv_id: '2512.12805'
source_url: https://arxiv.org/abs/2512.12805
tags:
- logit
- have
- bound
- error
- continuous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides theoretical and empirical evidence that Transformers
  exhibit size generalization, extending from smaller to larger inputs by analyzing
  their outputs on sampled tokensets compared to continuous counterparts. The key
  result is a generalization bound that decays as $eO(n^{-1/(D{\chi}+2)} + n^{-\rho})$
  with the number of tokens $n$, where $D{\chi}$ captures the intrinsic dimensionality
  of the data and $\rho$ measures the stability of positional encodings.
---

# From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs

## Quick Facts
- arXiv ID: 2512.12805
- Source URL: https://arxiv.org/abs/2512.12805
- Authors: Anastasiia Alokhina; Pan Li
- Reference count: 40
- Primary result: Provides theoretical and empirical evidence that Transformers exhibit size generalization, extending from smaller to larger inputs with error bounds decaying as $O(n^{-1/(D_{\chi}+2)} + n^{-\rho})$.

## Executive Summary
This paper establishes theoretical generalization bounds for Transformers operating on variable-size inputs, demonstrating that outputs on discrete sampled tokensets converge to continuous-domain equivalents as token count increases. The key result shows error bounds that decay with the number of tokens, governed by data intrinsic dimensionality and positional encoding stability. Experiments on point clouds and graphs confirm the predicted convergence rates, while comparison of stable versus unstable positional encodings demonstrates that stable encodings significantly improve generalization. The analysis overcomes challenges from the non-Lipschitz nature of attention and extends prior work on graph neural networks to the Transformer architecture.

## Method Summary
The method validates theoretical generalization bounds for Transformers on variable-size inputs through two main experiments. First, it measures worst-case output error between discrete and continuous tokensets by training a single attention layer with spectral normalization to maximize the difference between outputs on high-resolution and low-resolution tokensets. Second, it compares stable versus unstable positional encodings for generalization by training on small graphs and testing on larger graphs. The implementation uses point clouds from ModelNet40 mesh with 10,000 high-resolution points and graphs generated from graphons with nodes sampled uniformly. The architecture consists of a single attention layer with mean pooling followed by a 2-layer MLP, with spectral normalization enforced on all weights. Training uses gradient ascent to maximize the output difference, with 100 runs per configuration to ensure statistical significance.

## Key Results
- Generalization error bounds decay as $O(n^{-1/(D_{\chi}+2)} + n^{-\rho})$ where $D_{\chi}$ captures data intrinsic dimensionality and $\rho$ measures positional encoding stability
- Stable positional encodings (random-walk transition matrices) enable size generalization, while unstable encodings (shortest-path distances) fail to generalize
- Convergence rates match theoretical predictions: $O(n^{-1/4})$ for 2D point clouds and $O(n^{-1/3})$ for 1D graphs
- Lipschitz constants grow polynomially with network depth, enabling error propagation analysis despite softmax's non-Lipschitz nature

## Why This Works (Mechanism)

### Mechanism 1: Discretization-Convergence via Regular Measure
- Claim: Transformer outputs on discrete sampled tokensets converge to continuous-domain equivalents as token count increases.
- Mechanism: The empirical measure μ_X of sampled tokens approximates the underlying measure μ when μ satisfies regularity (no "holes" in support). Each ball of radius r carries at least O(r^{D_χ}) probability, enabling uniform concentration over the hypothesis class despite learnable parameters.
- Core assumption: (L_f, C_χ, D_χ)-admissible tokensets where the probability measure is regular.
- Evidence anchors:
  - [abstract] "bound is determined by the sampling density and the intrinsic dimensionality of the data manifold"
  - [Theorem 1] Error bound scales as n^{-1/(D_χ+2)}
  - [corpus] Limited direct corpus support for this specific discretization mechanism; related work on graphon convergence exists but for MPNNs, not Transformers.

### Mechanism 2: Positional Encoding Stability Governs Generalization
- Claim: Stable relative positional encodings enable size generalization; unstable PEs cause generalization gaps.
- Mechanism: Stable RPEs satisfy concentration: Pr[|p(X_i, X_j; T) - p(X_i, X_j; T)| ≥ Rn^{-ρ}τ] ≤ Cn²(e^{-τ²} + e^{-τ}). This ensures discrete RPEs converge to continuous counterparts. Random-walk transition probabilities are stable (ρ = min(α/2, -1/2+α)); shortest-path distances are not.
- Core assumption: RPE function p is (ρ, R)-stable as defined in Definition 5.
- Evidence anchors:
  - [Proposition 1] k-step random-walk transition matrix is stable for graphs from graphons
  - [Figure 4] Transition-matrix RPE generalizes better than shortest-path RPE empirically
  - [Section 4.1] Explicit counterexample: shortest-path distance on constant graphon never converges

### Mechanism 3: Local Lipschitzness Preservation Through Layers
- Claim: Hidden states remain locally Lipschitz across layers despite softmax's non-Lipschitz nature, enabling error propagation analysis.
- Mechanism: Each layer's output error is bounded by a combination of discretization error and RPE perturbation. The Lipschitz constant grows as O(4^K · L_logit^K · L_v^{K(K+1)/2}), but local Lipschitzness is preserved when (||h||_{Lip(r)} + ||p||_{Lip})r ≤ 1/(2||logit||_{Lip}).
- Core assumption: Hypothesis class H restricts to Lipschitz logit, value, and pooling functions with stable RPEs (Definition 6).
- Evidence anchors:
  - [Lemma C.6] Explicit bound on local Lipschitzness of next hidden state
  - [Theorem 3] Constants H_1, H_2, H_3 grow polynomially with Lipschitz parameters
  - [corpus] No direct corpus corroboration; this is a novel theoretical contribution for Transformers.

## Foundational Learning

- **Graphons as Continuous Graph Limits**
  - Why needed here: The paper models graphs as samples from graphons (W: [0,1]² → [0,1]), enabling analysis of size-varying graphs as discretizations of a common continuous structure.
  - Quick check question: Can you explain why a graphon provides a size-invariant representation of a graph family?

- **Ahlfors Q-Regularity of Measures**
  - Why needed here: This regularity assumption (balls of radius r have measure O(r^D)) ensures no "holes" in the token space, which is essential for the discretization error bound.
  - Quick check question: Why does standard covering-number analysis fail for Transformer attention (see Section B counterexample)?

- **Spectral Normalization for Lipschitz Control**
  - Why needed here: The hypothesis class assumes Lipschitz logit/value functions; spectral normalization on weight matrices (||W||_op ≤ 1) enforces this constraint.
  - Quick check question: How does spectral normalization differ from weight decay in enforcing Lipschitz bounds?

## Architecture Onboarding

- **Component map**: Input tokenset -> RPE Module (stable vs unstable) -> Attention Layer -> Pooling -> Output
- **Critical path**: RPE stability → discretization error bound (Theorem 1) → generalization bound (Theorem 2). If RPE is unstable, the n^{-ρ} term dominates and generalization fails.
- **Design tradeoffs**:
  - Random-walk RPEs: Stable but limited expressive power for k steps (k ≤ √n in Proposition 1)
  - Shortest-path RPEs: More expressive but unstable — breaks size generalization
  - Deeper networks: Error compounds with O(16^{K+1}·L_pool·K·L_v^{K(K+1)/2}·L_logit^K)
- **Failure signatures**:
  - Training on small graphs, testing on large graphs: Check if RPE values shift systematically
  - Exponentially large Lipschitz constants: Bound becomes vacuous
  - Sparse measure regions: Sampling error dominates
- **First 3 experiments**:
  1. **RPE Stability Ablation**: Compare random-walk vs. shortest-path RPE on graph classification with train/test size mismatch (replicate Figure 4 setup)
  2. **Convergence Rate Validation**: Plot output error ||Θ(T*) - Θ(T_n)|| vs. n on log-log scale; verify slope matches n^{-1/(D_χ+2)} (replicate Figure 3)
  3. **Depth Scaling Test**: Measure how generalization gap scales with number of layers K; verify O(16^{K+1}) growth pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can similar generalization bounds be established for data modalities with causal or sequential dependencies, such as natural language or images, where the i.i.d. token sampling assumption does not hold?
- Basis in paper: [explicit] "An important direction for future research is to investigate whether similar bounds can be established for more complex data modalities with causal relationships, such as natural language or images."
- Why unresolved: The theoretical framework assumes tokens are sampled i.i.d. from an underlying continuous domain, which fundamentally differs from the autoregressive structure and sequential dependencies in language and vision tasks.
- What evidence would resolve it: Deriving analogous bounds under non-i.i.d. data-generating processes, or demonstrating that certain causal structures admit similar convergence guarantees.

### Open Question 2
- Question: Which positional encoding schemes beyond random-walk transition matrices satisfy the $(\rho, R)$-stability condition under size-varying inputs?
- Basis in paper: [explicit] "A full characterization of which RPEs achieve stability in the size-varying setting is left for future work."
- Why unresolved: The paper proves stability only for $k$-step random-walk transition probabilities on graphs and shows shortest-path distances are unstable, but many other PE methods remain unanalyzed.
- What evidence would resolve it: Theoretical proofs establishing stability parameters $\rho$ for specific PE families, or counterexamples demonstrating instability.

### Open Question 3
- Question: Do stable and expressive positional encodings (e.g., resistance distance-based encodings) satisfy stability guarantees in size-variable scenarios?
- Basis in paper: [explicit] "An important direction for future research would be establishing stability guarantees in size-variable scenarios for other positional encoding schemes used in practice, e.g., stable and expressive PEs and resistance distance-based ones."
- Why unresolved: These methods are empirically effective but their theoretical stability properties under discretization from continuous sources have not been formally analyzed.
- What evidence would resolve it: Proofs establishing concentration bounds analogous to Definition 5 for these encoding schemes.

## Limitations

- The theoretical framework relies on strong assumptions about measure regularity (Ahlfors regularity) that may not characterize real-world data distributions
- The hypothesis class assumes spectral normalization enforces Lipschitz constraints, but post-training Lipschitz verification is challenging in practice
- The analysis assumes a single attention layer with mean pooling, limiting applicability to deeper architectures
- The generalization bound's dependence on O(16^{K+1}) factors with depth K suggests practical limits on how deep the theory extends

## Confidence

**High Confidence**: The discretization-convergence mechanism (Mechanism 1) is well-grounded theoretically, with the n^{-1/(D_χ+2)} rate following from standard concentration arguments for regular measures. The empirical validation on synthetic datasets provides direct support.

**Medium Confidence**: The positional encoding stability mechanism (Mechanism 2) has strong theoretical backing through the stability definition and Proposition 1, but relies on specific assumptions about graphon structure. The empirical comparison in Figure 4 is compelling but uses synthetic data.

**Low Confidence**: The local Lipschitzness preservation mechanism (Mechanism 3) is the most speculative, as it requires maintaining bounded Lipschitz constants across layers despite the non-Lipschitz nature of softmax attention. The polynomial growth in constants with depth K is concerning for practical applicability.

## Next Checks

1. **Distributional Robustness Test**: Validate the generalization bounds on real-world variable-size datasets (e.g., molecular graphs of varying sizes, text sequences of different lengths) to assess whether the Ahlfors regularity assumption holds empirically. Measure the gap between predicted and observed generalization error.

2. **Depth Scaling Verification**: Systematically measure how the Lipschitz constants and generalization bounds scale with depth K in practice. Compare the theoretical O(16^{K+1}) growth against empirical measurements of the generalization gap as depth increases beyond the single-layer case analyzed theoretically.

3. **Sparse Measure Stress Test**: Construct pathological datasets with regions of exponentially small measure probability (violating the C_χ/2 requirement) and measure how the generalization bounds degrade. This would validate whether the "no holes" assumption is critical for the theory to hold.