---
ver: rpa2
title: 'Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal
  Inference Possible?'
arxiv_id: '2601.15254'
source_url: https://arxiv.org/abs/2601.15254
tags:
- setting
- assumption
- sparse
- instruments
- high-dimensional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of estimating causal effects in
  settings where data are unpaired across environments (i.e., X and Y cannot be measured
  jointly), under hidden confounding. The authors cast the problem as a high-dimensional
  instrumental variable (IV) regression, with environments serving as instruments.
---

# Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?

## Quick Facts
- arXiv ID: 2601.15254
- Source URL: https://arxiv.org/abs/2601.15254
- Reference count: 40
- One-line primary result: SplitUP provides consistent causal effect estimation in high-dimensional IV settings with unpaired data by using cross-fold sample splitting to eliminate asymptotic bias

## Executive Summary
This paper addresses the challenge of estimating causal effects when data from different experimental environments are unpaired—meaning the treatment (X) and outcome (Y) cannot be measured jointly. The authors frame this as an instrumental variable (IV) problem where the environment indicator serves as the instrument. Standard two-sample IV estimators fail in high-dimensional settings where the number of instruments grows while observations per instrument remain fixed, suffering from persistent asymptotic bias. The proposed SplitUP method overcomes this limitation through a cross-fold sample splitting strategy that removes the bias while maintaining consistency.

The method extends naturally to sparse causal effects through ℓ1-regularized estimation and includes post-selection refitting for improved efficiency. Theoretical results establish consistency and asymptotic normality as the number of environments grows, contrasting with the asymptotic bias of naive approaches. The authors also provide practical finite-sample variance reduction techniques, making the method implementable in real-world settings.

## Method Summary
The SplitUP method treats unpaired data as a high-dimensional instrumental variable regression problem, with environments serving as instruments. The key innovation is a cross-fold sample splitting strategy that constructs the denominator of the IV estimator using cross-products of covariance estimates from different folds, eliminating the asymptotic bias that plagues standard two-sample IV estimators in high-dimensional settings. For sparse effects, the method employs ℓ1-penalized generalized method of moments (GMM) estimation followed by post-selection refitting on the identified support. The approach maintains consistency as the number of environments grows while keeping the sample size per environment fixed, a regime where traditional methods fail.

## Key Results
- SplitUP removes asymptotic bias present in standard two-sample IV estimators in the high-dimensional instrument regime
- Causal effects are identifiable even when the number of covariates exceeds the number of instruments, provided the effect vector is sparse
- The method provides consistency and asymptotic normality as the number of environments grows while sample size per environment remains constant
- Finite-sample variance reduction techniques are provided for practical implementation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-fold sample splitting removes the asymptotic bias inherent in standard two-sample IV estimators when the number of instruments (environments) grows.
- **Mechanism:** In high-dimensional instrument regimes ($m \to \infty$), the plug-in denominator $\hat{Cov}(\tilde{I}, \tilde{X})^\top \hat{Cov}(\tilde{I}, \tilde{X})$ suffers from a persistent measurement-error term because the estimation error does not vanish relative to the signal. The **SplitUP** method splits the instrument-covariate sample into $K$ folds and constructs the denominator using cross-products of covariance estimates from *different* folds ($h \neq k$). Since the estimation errors in independent folds are uncorrelated ($E^\top_A E_B \to 0$), the measurement-error bias cancels out asymptotically.
- **Core assumption:** The estimation errors in the instrument-covariate covariance are independent across the randomly split folds.
- **Evidence anchors:**
  - [Abstract]: "SplitUP... removes asymptotic bias present in standard two-sample IV estimators in the high-dimensional instrument regime."
  - [Section 4.2]: "A simple solution to the measurement-error problem... is to use sample splitting... because $E_A^\top E_B \to_p 0$ recovering consistency."
  - [Corpus]: Neighbors discuss unpaired data handling (e.g., "Bridged Clustering") but do not verify the specific bias-removal mechanism for IV regression.
- **Break condition:** If the number of observations per environment is large ($n/m \to \infty$), the standard TS-IV bias vanishes, and the added variance from splitting might make SplitUP less efficient than simpler estimators.

### Mechanism 2
- **Claim:** Causal effects are identifiable even when the number of covariates $d$ exceeds the number of instruments $m$, provided the effect vector is sparse.
- **Mechanism:** Standard IV requires $\text{rank}(\text{Cov}(I, X)) = d$, which fails if $m < d$. By imposing an $\ell_1$-penalty, the method relies on a "restricted nullspace" condition rather than full rank. This ensures that no two distinct sparse vectors can produce the same covariance signatures, allowing the true support to be recovered.
- **Core assumption:** The true causal effect vector $\beta^*$ has at most $s^*$ non-zero components (sparsity).
- **Evidence anchors:**
  - [Section 3.1]: Theorem 3.2 states identification for sparse $\beta^*$ is possible if $\text{ker}(\text{Cov}(I, X)) \cap \Sigma_{2s^*} = \{0\}$.
  - [Section 4.1]: "When the full-rank condition... fails... We instead impose sparsity... via $\ell_1$-penalized GMM."
  - [Corpus]: General references to sparse bridging in unpaired data exist, but not this specific rank-nullspace condition.
- **Break condition:** If the effect is dense (many non-zero coefficients) and $m < d$, the causal effect is not identifiable by this or any linear IV method.

### Mechanism 3
- **Claim:** Unpaired data from different experimental environments can be treated as a unified Instrumental Variable (IV) problem.
- **Mechanism:** The environment indicator $I$ serves as the instrument. It satisfies the exclusion restriction (affects outcome $Y$ only via treatment $X$) and relevance (correlates with $X$). Because $I$ is observed in both samples, one can compute $\text{Cov}(I, Y)$ and $\text{Cov}(I, X)$ separately and solve for $\beta^*$ without ever observing $X$ and $Y$ jointly.
- **Core assumption:** Exclusion restriction: The environment does not directly affect the outcome ($E[\epsilon|I] = 0$), and the covariance structure of $I, X$ is consistent across samples.
- **Evidence anchors:**
  - [Abstract]: "...cast as an instrumental variable (IV) regression with the environment acting as a... instrument."
  - [Section 2.1]: Assumption 1 explicitly requires $E[\epsilon|I] = 0$.
  - [Corpus]: "Paired by the Teacher" discusses synthesizing pairs, whereas this method avoids synthesis by leveraging covariance structure.
- **Break condition:** If the environment directly modifies $Y$ (e.g., a drug changes health outcomes via a pathway other than the biomarker $X$), the exclusion restriction is violated, and the estimate is biased.

## Foundational Learning

- **Concept: Two-Sample Instrumental Variables (TS-IV)**
  - **Why needed here:** This is the baseline method SplitUP improves upon. You must understand why regressing means works in classical settings to see why it fails in high-dimensional settings.
  - **Quick check question:** In a standard TS-IV setting with $m=1$ and large $n$, why does splitting the data not significantly bias the estimate, whereas in high dimensions ($m \approx n$) it does?

- **Concept: Restricted Eigenvalue (RE) Condition**
  - **Why needed here:** This mathematical condition is required to guarantee that the Lasso ($\ell_1$ penalty) recovers the correct sparse support.
  - **Quick check question:** Why does the RE condition allow for estimation when the standard full-rank matrix inversion condition fails?

- **Concept: GMM (Generalized Method of Moments)**
  - **Why needed here:** SplitUP is framed as a GMM estimator. Understanding how weighting matrices $\mathbf{W}$ affect efficiency is key to optimizing the implementation.
  - **Quick check question:** How does the choice of the weight matrix $\mathbf{W}$ in the objective function $\hat{g}(\beta)^\top \mathbf{W} \hat{g}(\beta)$ affect the variance of the resulting estimator?

## Architecture Onboarding

- **Component map:** Input data $\{(I_i, Y_i)\}_{i=1}^n$ and $\{(\tilde{I}_j, \tilde{X}_j)\}_{j=1}^{\tilde{n}}$ -> Cross-Covariance Engine (computes $C_{XY}$ and split denominator $C_{XX}$) -> Solver (GMM optimizer or Lasso solver) -> Refitter (if sparse, second pass refitting on selected support)

- **Critical path:** The construction of the denominator $C_{XX}$ using **CrossFoldDenom** (Algorithm 6). If you naively compute $\hat{Cov}^\top \hat{Cov}$ on the full sample, the system will suffer from asymptotic bias in the high-dimensional regime.

- **Design tradeoffs:**
  - **Analytic vs. Monte Carlo Split:** The paper provides a closed-form solution for "infinite splits" (`SplitUP analytic`). This is faster and has lower variance than averaging over $H$ random splits (`SplitUP`), but the analytic form requires careful implementation of the diagonal correction term.
  - **L1 Regularization:** Required for $d > m$ or weak instruments. Adds a hyperparameter $\lambda$ (regularization strength) that typically scales as $\approx 1/\sqrt{m}$.

- **Failure signatures:**
  - **TS-IV Peaking:** In sparse settings (Setting 3), standard TS-IV exhibits a "peaking" phenomenon where error spikes at intermediate sample sizes due to inverting a near-singular noise matrix.
  - **Corpus Mismatch:** If $\text{Cov}(I, X) \neq \text{Cov}(\tilde{I}, \tilde{X})$ (Assumption 1(i) fails), the cross-sample consistency breaks down entirely.

- **First 3 experiments:**
  1. **Dimensionality Stress Test:** Run Setting 2 (High-Dim, Dense). Plot MAE vs. $m$ for both TS-IV and SplitUP. Confirm TS-IV plateaus (biased) while SplitUP converges to 0.
  2. **Sparsity Recovery:** Run Setting 3 (High-Dim, Sparse). Verify that `UP-GMM` (no split) is biased while `SplitUP (l1)` recovers the true support $S^*$.
  3. **Infinite Split Validation:** Compare `SplitUP (H=10)` vs. `SplitUP (analytic)`. Confirm they produce numerically indistinguishable results, validating the analytic implementation.

## Open Questions the Paper Calls Out

None identified in the provided content.

## Limitations

- The exclusion restriction assumption (environment doesn't directly affect outcome) is critical and may be violated in many practical settings
- The method requires consistent covariance structure across unpaired datasets, which may not hold due to measurement differences or population shifts
- The sparsity assumption, while reasonable in many scientific contexts, may fail in settings with dense causal effects

## Confidence

**High Confidence Claims:**
- SplitUP removes asymptotic bias present in standard two-sample IV estimators when the number of instruments grows while sample size per instrument remains fixed
- The cross-fold splitting strategy successfully eliminates the measurement-error bias in the denominator estimation
- Sparse causal effects are identifiable under the restricted nullspace condition when full rank fails

**Medium Confidence Claims:**
- The analytic "infinite split" variant performs equivalently to finite random splits in practice
- The ℓ1-regularization approach successfully recovers sparse causal effects in high-dimensional settings
- The method is practically implementable with finite-sample variance reduction techniques

**Low Confidence Claims:**
- The exclusion restriction holds exactly in real-world experimental settings
- Covariance structure consistency across unpaired datasets is maintained in practice
- The restricted eigenvalue condition is satisfied for realistic sparse causal graphs

## Next Checks

1. **Robustness to Assumption Violations:** Implement synthetic experiments where the exclusion restriction is violated (e.g., environments directly affect outcomes through unmeasured pathways). Quantify bias in SplitUP versus standard TS-IV to understand the method's sensitivity to this critical assumption.

2. **Covariance Structure Stability:** Design experiments with paired data where you systematically corrupt one sample's measurement process (e.g., add noise, change units, apply transformations). Measure how violations of Assumption 1(i) affect SplitUP's performance compared to standard methods.

3. **Finite-Sample Behavior in Dense Settings:** Run simulations with increasing sparsity levels from highly sparse (s* << m) to dense (s* approaching d). Characterize the transition point where ℓ1-regularization no longer provides identification and measure the relative efficiency of SplitUP versus alternative dense estimation methods.