---
ver: rpa2
title: 'Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic
  Data'
arxiv_id: '2506.23174'
source_url: https://arxiv.org/abs/2506.23174
tags:
- data
- synthetic
- performance
- quality
- affinity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the quality limitations of wireless synthetic
  data generated by generative models. The authors identify that current wireless
  synthetic data suffers from prevalent limited affinity, leading to mislabeled data
  and degraded task performance.
---

# Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data

## Quick Facts
- arXiv ID: 2506.23174
- Source URL: https://arxiv.org/abs/2506.23174
- Reference count: 40
- This paper addresses quality limitations of wireless synthetic data by proposing a semi-supervised filtering framework that improves task accuracy by 4.3% over baseline.

## Executive Summary
This paper tackles the prevalent limited affinity issue in wireless synthetic data generated by GANs and diffusion models, which leads to degraded task performance. The authors propose SynCheck, a quality-guided utilization scheme that treats synthetic data as unlabeled and filters low-affinity samples while assigning pseudo-labels to the rest during iterative training. By combining real labeled data with filtered synthetic data in a semi-supervised framework, SynCheck consistently outperforms quality-oblivious utilization, even reversing baseline degradation from 13.4% to 4.3% improvement.

## Method Summary
The paper proposes SynCheck, a two-phase semi-supervised learning framework for quality-guided synthetic data utilization. In Phase 1 (warm-up), a ResNet34 backbone is trained on labeled real data using task loss and one-vs-all loss, while unlabeled synthetic data contributes through consistency and entropy regularization. In Phase 2 (iteration), synthetic samples are filtered based on classifier prediction plus per-class inlier detection, then assigned pseudo-labels and used for further training with domain-specific augmentations. The method calibrates data quality using margin distributions and JS divergence, treating synthetic data as unlabeled to avoid propagating errors from low-affinity samples.

## Key Results
- SynCheck achieves 4.3% performance improvement over baseline synthetic utilization, reversing baseline degradation from 13.4%
- Synthetic data filtered by SynCheck shows enhanced affinity while maintaining comparable diversity to unfiltered raw data
- The framework consistently outperforms quality-oblivious utilization across different wireless sensing tasks and datasets

## Why This Works (Mechanism)
SynCheck works by recognizing that synthetic data quality varies significantly due to generative models' lack of awareness of untrained conditions and domain-specific processing. By treating synthetic data as unlabeled and using semi-supervised learning, the framework can identify and filter out low-affinity samples while leveraging the remaining high-quality synthetic data. The margin calibration procedure quantifies affinity by comparing train and calibrated test margin distributions using JS divergence, enabling systematic quality assessment. The two-phase approach first establishes a reliable baseline on real data, then progressively incorporates high-quality synthetic samples with pseudo-labels.

## Foundational Learning
- Semi-supervised learning with unlabeled data: Why needed - enables utilization of abundant synthetic data without requiring manual labels; Quick check - verify pseudo-labels match classifier predictions on filtered inliers
- Margin distribution analysis for affinity quantification: Why needed - provides systematic way to measure synthetic data quality beyond simple accuracy; Quick check - ensure JS divergence between train/test margin distributions stays below 6% variation
- One-vs-all inlier-outlier detection: Why needed - identifies synthetic samples that are genuine representatives of real data distribution; Quick check - verify train accuracy >90% on real data for detector reliability
- Domain-specific augmentations for RF data: Why needed - accounts for wireless signal variations and processing effects; Quick check - compare performance with and without augmentations
- Two-phase training strategy: Why needed - establishes reliable baseline before incorporating synthetic data; Quick check - verify warm-up phase achieves reasonable accuracy before iteration
- JS divergence for quality calibration: Why needed - quantifies distributional similarity between synthetic and real data; Quick check - ensure calibration reduces margin gap variance

## Architecture Onboarding

**Component Map:** Real data (labeled) -> ResNet34 backbone -> Task classifier + Inlier detectors -> Pseudo-labels -> Synthetic data (unlabeled) -> Filtered synthetic data (pseudo-labeled) -> Augmented training

**Critical Path:** Warm-up phase (real data training) → Margin calibration (quality assessment) → Iteration phase (filtering + pseudo-labeling) → Final model training

**Design Tradeoffs:** The framework trades computational complexity for improved data utilization by adding filtering and pseudo-labeling steps. This adds overhead but significantly improves performance when synthetic data quality is limited.

**Failure Signatures:** 
- Synthetic filtering too aggressive (near 0% inlier rate) or too permissive (near 100% inlier rate)
- Performance degradation persists despite synthetic data inclusion
- Margin calibration yields inconsistent quality estimates (>6% variation)

**First Experiments:**
1. Train detector-only baseline to verify inlier detection accuracy exceeds 90% on held-out real data
2. Run ablation comparing performance with and without pseudo-labeling step
3. Measure synthetic data diversity change by comparing average pairwise JS divergence before/after filtering

## Open Questions the Paper Calls Out
None

## Limitations
- The framework depends on availability of functional generative models for synthetic data generation
- Several technical details remain underspecified including exact detector architecture and filtering thresholds
- Performance relies on correctly reproducing the margin calibration and filtering procedures

## Confidence
- Claim: SynCheck improves task accuracy over baseline synthetic utilization — Medium
- Claim: Filtered synthetic data shows higher affinity and comparable diversity — Medium
- Claim: Two-phase semi-supervised training is effective — High

## Next Checks
1. Verify detector threshold calibration by testing on a held-out real subset and measuring inlier detection accuracy; target >90% correct classification
2. Compare synthetic data diversity before/after filtering by computing average pairwise JS divergence between synthetic and real margin distributions; ensure <10% increase post-filtering
3. Run ablation: train with and without pseudo-labeling step to confirm its contribution to the 4.3% gain