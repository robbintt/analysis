---
ver: rpa2
title: 'CelloAI: Leveraging Large Language Models for HPC Software Development in
  High Energy Physics'
arxiv_id: '2508.16713'
source_url: https://arxiv.org/abs/2508.16713
tags:
- code
- generation
- scientific
- celloai
- kernels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CelloAI is a locally hosted coding assistant that leverages LLMs
  with retrieval-augmented generation to address HPC software development challenges
  in High Energy Physics. It employs separate collections for code and text, syntax-aware
  chunking using Tree-sitter, and callgraph-enhanced prompts to improve context assembly
  and code generation accuracy.
---

# CelloAI: Leveraging Large Language Models for HPC Software Development in High Energy Physics

## Quick Facts
- arXiv ID: 2508.16713
- Source URL: https://arxiv.org/abs/2508.16713
- Authors: Mohammad Atif; Kriti Chopra; Ozgur Kilic; Tianle Wang; Zhihua Dong; Charles Leggett; Meifeng Lin; Paolo Calafiura; Salman Habib
- Reference count: 0
- CelloAI achieves 79% code retrieval completeness versus 17-54% for baselines in High Energy Physics software development.

## Executive Summary
CelloAI addresses the challenge of HPC software development in High Energy Physics by providing a locally-hosted coding assistant that leverages large language models with retrieval-augmented generation. The system decouples code and text corpora, uses syntax-aware chunking via Tree-sitter to preserve function boundaries, and enhances prompts with callgraph lineage to improve context assembly and code generation accuracy. Evaluations on ATLAS, CMS, and DUNE applications demonstrate significant improvements in code retrieval completeness and successful GPU kernel porting across programming models while ensuring data privacy and transparency.

## Method Summary
CelloAI is built around a retrieval-augmented generation pipeline for HPC software in High Energy Physics. It employs separate ChromaDB collections for code and text corpora to prevent retrieval imbalance, uses Tree-sitter for syntax-aware chunking that preserves complete function boundaries, and generates callgraphs via Doxygen to inject dependency awareness into prompts. The CelloRetriever combines dual encoding queries with pattern-matching reranking to improve deterministic recall. The system supports local LLM inference with open-weight models and targets two primary tasks: code documentation generation and GPU kernel porting across programming models (CUDA, OpenMP, Kokkos, HIP, SYCL).

## Key Results
- Achieves 79% code retrieval completeness versus 17-54% for baseline systems on HEP testbeds
- Successfully ports GPU kernels across five programming models with open-weight models
- Prevents context window overflow and retrieval fragmentation through syntax-aware chunking and separate collections
- Ensures data privacy and transparency through local inference with open-weight models

## Why This Works (Mechanism)

### Mechanism 1
Decoupling storage of source code and scientific text prevents retrieval imbalance and allows configurable context assembly. CelloAI uses separate ChromaDB collections for code and text, preventing a large code corpus from numerically dominating retrieval results over a smaller text corpus. This ensures the LLM receives both algorithmic ("how") and conceptual ("why") context. The core assumption is that relevant context requires a balanced mix of implementation details and scientific rationale, which single-collection embedding similarity scores fail to prioritize naturally. Evidence shows Patatrack retrieving 100% code (50/50 chunks) in a unified collection, starving the model of text context.

### Mechanism 2
Syntax-aware chunking via Abstract Syntax Trees (AST) preserves functional boundaries, reducing retrieval fragmentation. Instead of fixed-size windows, CelloAI uses Tree-sitter to parse code into concrete syntax trees, chunking at logical boundaries like complete function definitions and classes. This ensures retrieved snippets contain full signatures and scope. The core assumption is that incomplete code fragments degrade LLM reasoning and generation quality more than varying chunk sizes do. Standard splitters bisect template lists or orphan blocks, whereas Tree-sitter emits self-contained units, reducing "partial routines" in the context window.

### Mechanism 3
Injecting callgraph lineage into prompts improves code translation correctness by exposing dependencies. During ingestion, CelloAI generates callgraphs using Doxygen, and when a function is retrieved, its two-hop lineage is summarized and appended to the prompt. This explicitly shows the LLM how the function fits into the broader execution flow. The core assumption is that LLMs suffer from "dependency blindness" in large codebases, and explicitly stating upstream/downstream connections reduces bugs in generated code. Evidence shows models using CelloAI generated CPU wrappers for GPU kernels, with Qwen2.5-Coder-32B writing multi-clause mapping directives.

## Foundational Learning

- **Tree-sitter & Abstract Syntax Trees (AST)**: Standard text splitters destroy code structure. You must understand ASTs to configure the chunker to respect scopes (namespaces, classes) rather than character counts. Quick check: Does a `RecursiveCharacterTextSplitter` guarantee that a closing brace `}` is retrieved with its corresponding function signature?

- **Vector Embeddings & Cosine Similarity**: CelloRetriever relies on embedding models to find "semantically similar" code. You need to distinguish between lexical matching (finding variable names) and semantic retrieval (finding "parallel reduction logic"). Quick check: Why would a query for "GPU memory allocation" fail to retrieve a function named `allocate_buffers()` if the embedding model was trained only on natural language?

- **HPC Programming Models (CUDA, OpenMP, Kokkos)**: The evaluation measures "porting" between these models. You must understand the difference between a kernel (`__global__`), a target region (`#pragma omp target`), and a parallel loop to interpret the "Hard" kernel failures. Quick check: In the context of the paper, why is writing `#pragma omp target teams distribute parallel for` incorrect for a kernel that updates a single value?

## Architecture Onboarding

- **Component map**: Code Ingestion -> AST Parsing (Tree-sitter) -> Embedding -> User Query -> Dual Retrieval (Code + Text) -> Pattern Reranking -> Callgraph Injection -> Local LLM Inference
- **Critical path**: Code Ingestion -> AST Parsing (Tree-sitter) -> Embedding -> User Query -> Dual Retrieval (Code + Text) -> Pattern Reranking -> Callgraph Injection -> Local LLM Inference
- **Design tradeoffs**: Local vs. Cloud prioritizes data privacy and cost stability (no token fees) at the expense of requiring local GPU hardware and engineering maintenance. Determinism vs. Recall: CelloRetriever uses pattern matching to boost deterministic recall, trading off pure semantic exploration for guaranteed keyword presence.
- **Failure signatures**: Easy Kernel Hallucination (model uses complex loop directives for single-value updates), Memory Mapping Disconnect (model generates valid kernel syntax but fails to link `is device ptr` with `omp target alloc`), Context Imbalance (unified collection retrieves only code, ignoring documentation).
- **First 3 experiments**: (1) Chunking A/B Test: Run CelloRetriever with fixed-size chunking vs. Tree-sitter on a sample repository, counting "partial routines" in top 40 results. (2) Retrieval Completeness: Execute kernel enumeration prompt against a known codebase with and without Pattern Matching layer to verify recall boost. (3) Translation Sanity Check: Port "Hello World" CUDA kernel to OpenMP, verifying correct `#pragma omp target` without unnecessary loop directives.

## Open Questions the Paper Calls Out

### Open Question 1
Can fine-tuning effectively resolve data-movement reasoning and directive selection bottlenecks to successfully port "hard" GPU kernels? Current models fail on "hard" kernels primarily due to incorrect or missing memory map clauses, a limitation not solved by context augmentation alone. Evidence would be a fine-tuned model correctly implementing host-to-device memory mappings for computationally intensive kernels where base models failed.

### Open Question 2
What standardized, end-to-end frameworks are required to benchmark LLM-generated HPC code for correctness and performance? Current evaluations rely on manual compilation checks and human scoring rather than automated, reproducible metrics. Evidence would be an automated validation pipeline that reliably executes generated code and verifies results against reference datasets.

### Open Question 3
How can decoding hyperparameters be optimized to balance generation reproducibility with the need to explore diverse code solutions? Lowering temperature enhances reproducibility but risks suboptimal code, while higher temperature yields variability; the optimal balance for HPC is unknown. Evidence would be a systematic study correlating decoding settings with rates of valid kernel generation and performance efficiency.

## Limitations
- Evaluation methodology lacks statistical significance testing for the 25 percentage point improvement in retrieval completeness
- Narrow scope focusing primarily on CUDA-to-OpenMP translation task limits generalizability to full range of HPC scenarios
- Multiple specialized components (separate collections, Tree-sitter, callgraphs, pattern matching) may create maintenance overhead and failure points
- LLM decoding hyperparameters significantly impact results but optimal configurations are not provided

## Confidence

**High Confidence**: Separate collections for code and text to prevent retrieval imbalance is well-supported by direct evidence from Section V.A.1 showing Patatrack's unified collection retrieved 100% code chunks with zero text context.

**Medium Confidence**: Syntax-aware chunking using Tree-sitter shows promise based on qualitative descriptions of avoiding "partial routines," but lacks quantitative comparisons between AST-based and fixed-window chunking approaches.

**Low Confidence**: Callgraph injection mechanism's impact on code generation correctness is supported by limited examples but lacks systematic evaluation across different dependency scenarios.

## Next Checks

1. **A/B Test for Chunking Strategy**: Implement both Tree-sitter AST-based chunking and standard fixed-size chunking on the same testbed, manually auditing the percentage of "complete" versus "partial" routines retrieved to provide quantitative validation.

2. **Statistical Significance Testing for Retrieval Completeness**: Re-run kernel enumeration task on FastCaloSim using both CelloAI and best baseline with multiple random query seeds, performing paired t-test to determine whether the 25 percentage point difference is statistically significant.

3. **Porting Task Generalization Study**: Extend porting evaluation beyond FastCaloSim by testing all five programming model pairs across at least two different testbeds, measuring compilation success rates and functional correctness for each combination.