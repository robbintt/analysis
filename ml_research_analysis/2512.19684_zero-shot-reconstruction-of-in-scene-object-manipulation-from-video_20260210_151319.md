---
ver: rpa2
title: Zero-shot Reconstruction of In-Scene Object Manipulation from Video
arxiv_id: '2512.19684'
source_url: https://arxiv.org/abs/2512.19684
tags:
- object
- hand
- motion
- pose
- reconstruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first system to reconstruct in-scene
  object manipulation from monocular RGB video. The key innovation is a two-stage
  optimization framework that recovers complete hand-object motion in scene coordinates.
---

# Zero-shot Reconstruction of In-Scene Object Manipulation from Video

## Quick Facts
- **arXiv ID**: 2512.19684
- **Source URL**: https://arxiv.org/abs/2512.19684
- **Reference count**: 40
- **Primary result**: First system to reconstruct in-scene object manipulation from monocular RGB video with 7.67mm MPJPE and 45.71mm MRPE on DexYCB.

## Executive Summary
This paper introduces a novel two-stage optimization framework for reconstructing complete hand-object manipulation sequences from monocular RGB video. The key innovation is using the object as a stable anchor to resolve the inherent depth ambiguity in hand tracking, combined with physical constraints and a human motion prior to generate physically plausible interactions. The method achieves state-of-the-art performance on benchmark datasets while demonstrating robustness on in-the-wild videos.

## Method Summary
The approach uses foundation models to initialize core components (object mesh, poses, scene point cloud, hand poses) and then performs a two-stage optimization. First, it optimizes hand-object interaction using contact point matching and physical constraints (penetration avoidance, motion smoothness). Second, it completes the grasping motion using a human motion prior. This sequential approach addresses the depth ambiguity problem by treating the object as the primary anchor for scene coordinates, significantly simplifying the reconstruction problem compared to direct hand tracking.

## Key Results
- Achieves 7.67mm MPJPE and 45.71mm MRPE on DexYCB dataset
- Significantly outperforms baseline methods in both pose accuracy and physical plausibility metrics
- Demonstrates robust performance on both benchmark datasets and in-the-wild videos
- Successfully handles the three main challenges: ill-posed scene reconstruction, ambiguous hand-object depth, and need for physically plausible interactions

## Why This Works (Mechanism)

### Mechanism 1: Object-Centric Motion Anchoring
The system resolves monocular hand tracking depth ambiguity by treating the object as the primary anchor for scene coordinates. Objects are rigid bodies with stable textures, allowing for robust 6-DoF pose tracking via feature matching. The system first estimates the object's trajectory and then "hangs" the hand trajectory onto this stable anchor, correcting the hand's metric depth based on the object's known position.

### Mechanism 2: 2D-to-3D Contact Lifting via Normals
Physically plausible grasps are achieved by lifting 2D mask intersections to 3D contact points, disambiguating "front" vs. "back" contact using surface normals. The system projects hand vertices to 2D, checks if they fall inside the object mask, then casts rays to find corresponding 3D points on the object mesh, using vertex normals to resolve depth ambiguity.

### Mechanism 3: Sequential Feedback Loop for Motion Completion
A two-stage optimization allows the physically constrained "interaction" phase to correct the depth ambiguity of the "approach" phase. The system first optimizes the interaction stage using strict physical contact constraints, then propagates this accurate, optimized end-state backward to initialize the grasping stage, using a human motion prior to fill gaps where the hand was out of view.

## Foundational Learning

**Concept: Signed Distance Fields (SDF)**
- Why needed here: The system uses an SDF loss to prevent hand mesh penetration of the object mesh
- Quick check question: If a hand vertex is inside an object mesh, is its signed distance value positive or negative?

**Concept: MANO Parametric Model**
- Why needed here: Optimization works on pose parameters θ (rotation) and translation τ, not raw vertices
- Quick check question: What does the β parameter represent in the MANO model, and is it optimized in the Interaction Stage?

**Concept: 6-DoF Pose Tracking**
- Why needed here: The system relies on the object's 6-DoF pose as the "anchor" for the hand
- Quick check question: Why is 6-DoF pose estimation generally more robust for rigid objects than for articulated hands in this context?

## Architecture Onboarding

**Component map:**
Input: RGB Video + SAM2 Masks -> Initializers (SpatialTrackerV2, HaPTIC, Hi3DGen/Amodal3R, FoundationPose) -> Stage 1 (Interaction Opt) -> Stage 2 (Grasping Opt)

**Critical path:** The Object Pose Reconstruction is the linchpin. If FoundationPose or the generated Mesh is incorrect, the contact constraints in the Interaction Stage will force the hand into an incorrect pose to satisfy the "phantom" geometry.

**Design tradeoffs:**
- Sequential vs. Joint Optimization: The paper splits the timeline to handle conflicting goals (smooth motion vs. tight contact), sacrificing global temporal consistency for local physical plausibility
- Zero-shot vs. Training: Relying on foundation models allows zero-shot generalization but creates a fragile dependency chain

**Failure signatures:**
- "Flying Hands": If contact matching fails, the hand hovers near the object. Check L_con weights
- "Sliding Objects": If the object pose estimator drifts, the hand will appear to slide relative to the object surface
- Mesh Intersection: If L_sdf weight is too low, fingers will pass through the object

**First 3 experiments:**
1. Module Validation: Run on DexYCB with Ground Truth object poses to isolate hand-optimization performance vs. object-tracking noise
2. Loss Ablation: Disable L_con and observe the MPJPE/MRPE delta (paper shows this increases MRPE significantly)
3. Threshold Sensitivity: Vary the grasping frame detection threshold to see if the split point causes discontinuities

## Open Questions the Paper Calls Out

1. Can object reconstruction quality be improved by utilizing observations from the entire video sequence rather than relying solely on the first frame?

2. How can the system's dependency on upstream foundation models be mitigated to handle failure cases caused by low-light conditions or severe motion blur?

3. Can the framework be extended to handle articulated objects, or is it fundamentally limited by its rigid body assumption?

4. How can the trade-off between enforcing physical plausibility and maintaining pose accuracy be optimized when ground-truth annotations are themselves physically implausible?

## Limitations

- Foundation model dependency chain creates potential failure propagation
- Physical plausibility metrics only validated on synthetic benchmark datasets
- Temporal coherence not guaranteed due to sequential optimization approach
- Limited to rigid objects, cannot handle articulated objects like scissors or knives

## Confidence

- Foundation Model Dependency Chain: Medium
- Physical Plausibility Metrics: Medium
- Temporal Coherence: Low

## Next Checks

1. Module Isolation Test: Run the pipeline on DexYCB using ground truth object poses instead of FoundationPose to quantify the impact of object tracking accuracy on final hand pose performance.

2. Physical Plausibility Stress Test: Evaluate the optimized results on real in-the-wild videos using both automated physical metrics and human perceptual studies to assess physical realism.

3. Temporal Continuity Analysis: Measure and visualize the temporal derivatives (velocity, acceleration) across the interaction boundary to verify that the stage-splitting does not introduce unnatural motion discontinuities.