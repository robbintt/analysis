---
ver: rpa2
title: Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering
arxiv_id: '2508.06345'
source_url: https://arxiv.org/abs/2508.06345
tags:
- graph
- trfs
- node
- tasks
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DynamicTRF, a novel framework for zero-shot
  graph question answering that dynamically selects optimal topology representation
  forms (TRFs) for large multimodal models. The method constructs a TRF Preference
  dataset to identify task-specific TRF preferences, then trains a router to adaptively
  assign the best TRF from a curated set of eight diverse representations (five visual
  layouts and three textual formats).
---

# Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering

## Quick Facts
- arXiv ID: 2508.06345
- Source URL: https://arxiv.org/abs/2508.06345
- Reference count: 27
- Key outcome: DynamicTRF achieves up to 27.8 GRE score vs 5.6-6.6 for fixed TRF approaches while maintaining >96% accuracy on most tasks

## Executive Summary
DynamicTRF introduces a novel framework for zero-shot graph question answering that dynamically selects optimal topology representation forms (TRFs) for large multimodal models. The method constructs a TRF Preference dataset to identify task-specific TRF preferences, then trains a router to adaptively assign the best TRF from a curated set of eight diverse representations. Experiments across seven in-domain graph algorithmic tasks and two out-of-domain applications show DynamicTRF significantly outperforms baseline methods while maintaining strong cross-model transferability.

## Method Summary
DynamicTRF uses a TRF Router (DeBERTaV3-base) to classify questions and select from 8 topology representation forms (5 visual layouts from Graphviz, 3 textual formats). The router is trained on a TRF Preference dataset where each question is labeled with optimal TRFs based on a Graph Response Efficiency metric that balances accuracy against token usage. During inference, the selected TRF is passed to a frozen LMM Reasoner (e.g., GPT-4o) to generate the answer. The framework is evaluated on 7 in-domain algorithmic tasks and 2 out-of-domain graph applications.

## Key Results
- Achieves up to 27.8 GRE score compared to 5.6-6.6 for fixed TRF approaches
- Maintains accuracy above 96% on most in-domain tasks
- Demonstrates strong cross-model transferability between GPT-4o and Gemini-2.5 Pro
- No single TRF dominates all tasks, validating dynamic routing approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Routing graph questions to specific Topology Representation Forms (TRFs) based on task type improves zero-shot performance over using a single, static representation.
- **Mechanism:** The DynamicTRF framework leverages a router to select from visual and textual TRFs. Evidence suggests correlation between task category and optimal TRF: "Perceptual-Intensive" tasks favor Visual TRFs while "Edge-Weighted" tasks favor Textual TRFs.
- **Core assumption:** The router effectively captures the underlying "question intent" or structural complexity and maps it to the representational format that minimizes cognitive load for the LMM.
- **Evidence anchors:**
  - [Section 4.4]: Table 2 explicitly categorizes task preferences, showing visual TRFs dominate Connectivity while textual TRFs dominate Maximum Flow.
  - [Section 5.3]: Table 5 demonstrates that no single TRF dominates all tasks, validating the necessity of dynamic routing.
- **Break condition:** The mechanism likely fails if the input graph exceeds visual resolution limits for Visual TRFs or if textual context windows are exhausted for large Adjacency Matrices.

### Mechanism 2
- **Claim:** Optimizing for Graph Response Efficiency (GRE) allows the system to balance performance against computational cost (token usage).
- **Mechanism:** The router is trained on a preference dataset where labels are determined by maximizing the GRE metric ($GRE = Accuracy / (Tokens)^\alpha$).
- **Core assumption:** Token count acts as a reliable proxy for "effort" or latency in this context, and the exponent $\alpha$ correctly weights user preference for brevity versus correctness.
- **Evidence anchors:**
  - [Section 4.3]: Defines the GRE metric formally to trade off accuracy and token consumption.
  - [Section 5.3]: Table 6 shows that tuning $\alpha$ directly shifts the balance.
- **Break condition:** The mechanism degrades if the LMM hallucinates concise but incorrect reasoning chains that artificially inflate GRE during training data generation.

### Mechanism 3
- **Claim:** Routing preferences learned from one LMM can transfer to another, suggesting the "optimal representation" is partially task-intrinsic rather than purely model-specific.
- **Mechanism:** The TRF Router is trained on preferences derived from one model and applied to another. The paper observes consistent improvements over baselines even with transferred routers.
- **Core assumption:** The "cognitive bias" regarding how graphs are best presented is consistent enough across different SOTA LMM architectures to permit cross-model transfer.
- **Evidence anchors:**
  - [Section 5.4]: Table 7 quantifies transferability, showing that transferring GPT-4o's router to Gemini still yields positive gains over baselines.
- **Break condition:** Transfer learning likely fails if the target LMM has a vastly different context window or visual processing architecture that renders specific TRFs ineffective.

## Foundational Learning

- **Concept:** **Topology Representation Forms (TRFs)**
  - **Why needed here:** This paper is fundamentally about selecting the correct *input format* (Visual vs. Textual vs. Matrix) for a graph problem.
  - **Quick check question:** Can you distinguish why an Adjacency Matrix might be superior for dense graphs but an Edge Set is better for sparse graphs?

- **Concept:** **Pareto Optimality**
  - **Why needed here:** The paper claims the router achieves Pareto optimality in the accuracy-efficiency trade-off.
  - **Quick check question:** If you improve Accuracy but Token count stays the same, are you Pareto-dominant over the previous state?

- **Concept:** **Zero-Shot Generalization**
  - **Why needed here:** The system must route questions without task-specific fine-tuning of the main LMM.
  - **Quick check question:** Does the DynamicTRF framework require re-training the LMM Reasoner, or just the TRF Router? (Answer: Just the Router).

## Architecture Onboarding

- **Component map:** TRF Generator -> TRF Router (DeBERTa) -> LMM Reasoner
- **Critical path:** The generation of the **TRF Preference (TRFP) Dataset**. If this dataset is noisy or biased toward cheap-but-wrong answers, the router will fail.
- **Design tradeoffs:**
  - **Alpha ($\alpha$) in GRE:** Setting $\alpha \approx 0$ prioritizes accuracy (high token cost); $\alpha > 1$ prioritizes brevity (risking lower accuracy).
  - **Router Complexity:** The paper uses DeBERTa-base. A larger router might capture nuance better but adds latency.
- **Failure signatures:**
  - **Router Collapse:** The router maps all questions to a single TRF (e.g., always Tset) regardless of the input.
  - **Efficiency Trap:** High GRE scores driven by extremely short, incorrect answers (hallucinations) that slip through validity checks.
- **First 3 experiments:**
  1. **Validation of $F_{ZS}$:** Run a single TRF (e.g., Tset) vs. DynamicTRF on the 7 in-domain tasks to confirm that "one-size-fits-all" underperforms.
  2. **Sensitivity Analysis:** Vary the hyperparameter $\alpha$ (e.g., 0.0, 0.5, 1.0) and plot the Accuracy vs. Token curve to find the operational sweet spot.
  3. **Router Generalization:** Train the router on "easy" algorithmic tasks (Connectivity) and test on "hard" tasks (Max Flow) to see if it learns structural syntax or just task-specific heuristics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a single, universal TRF router be trained to generalize effectively across diverse LMM architectures without suffering from the specific model biases observed in cross-model transfer experiments?
- **Basis in paper:** [explicit] Section 5.4 notes that transferring the router between GPT-4o and Gemini-2.5 Pro results in trade-offs, indicating distinct model-specific preferences that challenge universal transferability.
- **Why unresolved:** While the paper demonstrates that transfer is possible and often better than baselines, it does not propose a method to neutralize the specific cognitive biases of different LMMs to create a truly architecture-agnostic router.
- **What evidence would resolve it:** Experiments showing that a router trained on a multi-model ensemble achieves statistically similar performance to model-specific routers when tested on held-out architectures.

### Open Question 2
- **Question:** What advanced features or router architectures are required to close the remaining performance gap between the trained TRF Router and the theoretical "Ideal Routing" upper bound?
- **Basis in paper:** [explicit] Section 5.3 (Ablation Study) states that "there is still a gap between the TRF Router and 'Ideal Routing', highlighting the preserved promising potential of TRF routing."
- **Why unresolved:** The current approach uses a standard DeBERTaV3-base classifier which fails to capture all nuances of the TRF preference space, leaving significant efficiency and accuracy gains on the table.
- **What evidence would resolve it:** A study demonstrating that a router with enriched input features or a more sophisticated architecture significantly reduces the relative error between the trained router's GRE score and the Ideal Routing GRE score.

### Open Question 3
- **Question:** How does DynamicTRF performance degrade or adapt when applied to graphs significantly larger than the 30-node limit used in the TRF Preference dataset construction?
- **Basis in paper:** [inferred] Section 4.4 explicitly constrains the TRFP dataset generation to random graphs with node count $N \in [3, 30]$; however, real-world applications often involve much larger topologies.
- **Why unresolved:** The paper does not evaluate the scaling laws of the router or the TRFs, nor does it verify if the preferences learned on small graphs hold true for larger, more complex structures.
- **What evidence would resolve it:** Evaluation results of the router's accuracy and GRE scores on synthetic and real-world graphs ranging from 50 to 500+ nodes.

## Limitations
- Computational cost of constructing the TRF Preference dataset requires 10 runs per TRF-question pair
- Focus on undirected graphs and specific algorithmic tasks limits generalizability
- GRE metric assumes token count is a reliable proxy for computational effort across different LMM architectures
- Reliance on Erdős–Rényi random graphs may bias the router against real-world graph topologies

## Confidence
- **High Confidence:** Dynamic TRF selection mechanism is well-supported by empirical results across multiple tasks and LMMs
- **Medium Confidence:** GRE metric effectiveness and cross-model transferability show promise but need more extensive validation
- **Low Confidence:** Claims about task-intrinsic vs. model-specific preferences require more extensive cross-model validation

## Next Checks
1. **Efficiency-Accuracy Pareto Frontier:** Systematically vary α from 0.1 to 2.0 and plot the resulting accuracy-token trade-off curve to identify the optimal operational point.
2. **Cross-Domain Robustness:** Test DynamicTRF on graph datasets with different characteristics (directed graphs, community structures) to evaluate whether routing preferences transfer beyond the generated graphs.
3. **Router Failure Mode Analysis:** Intentionally corrupt the TRF Preference dataset and measure the degradation in router performance to quantify sensitivity to dataset quality.