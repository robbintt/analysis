---
ver: rpa2
title: 'Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment
  Framework'
arxiv_id: '2601.20689'
source_url: https://arxiv.org/abs/2601.20689
tags:
- quality
- image
- uni00000013
- assessment
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEAF proposes a label-efficient IQA framework that decouples perceptual
  quality learning from MOS calibration by distilling MLLM priors into a lightweight
  student. Instead of fine-tuning large MLLMs with extensive MOS labels, LEAF uses
  teacher-provided point-wise judgments and confidence-weighted pair-wise preferences
  for joint distillation, followed by calibration with a small MOS subset.
---

# Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework

## Quick Facts
- arXiv ID: 2601.20689
- Source URL: https://arxiv.org/abs/2601.20689
- Authors: Xinyue Li; Zhichao Zhang; Zhiming Xu; Shubo Xu; Xiongkuo Min; Yitong Chen; Guangtao Zhai
- Reference count: 20
- Primary result: Achieves strong IQA performance (SRCC/PLCC of 0.777/0.801 on KonIQ-10k) without MOS labels, and matches state-of-the-art with only 10-30% MOS supervision

## Executive Summary
LEAF proposes a two-stage label-efficient IQA framework that decouples perceptual quality learning from MOS calibration by distilling MLLM priors into a lightweight student. Instead of fine-tuning large MLLMs with extensive MOS labels, LEAF uses teacher-provided point-wise judgments and confidence-weighted pair-wise preferences for joint distillation, followed by calibration with a small MOS subset. Experiments on UGC and AIGC benchmarks show strong MOS-aligned performance, achieving competitive results without MOS and matching or exceeding state-of-the-art with minimal supervision.

## Method Summary
LEAF uses a frozen MLLM (InternVL-3.5-8B) as teacher to generate quality judgments for unlabeled images. In Stage-1 Teacher-Guided Distillation (TGD), the teacher provides point-wise scores (computed from token probability expectations) and pair-wise preferences (with confidence weighting via entropy). A lightweight ConvNeXt-Base student is trained on this joint supervision without any MOS labels. In Stage-2 Calibration Fine-Tuning (CFT), the student is fine-tuned on a small MOS-labeled subset (10-30% of training data) using a combined MSE and PLCC loss to align the quality scale with human ratings.

## Key Results
- Label-free performance: 0.777/0.801 (SRCC/PLCC) on KonIQ-10k, 0.861/0.867 on SPAQ, 0.749/0.811 on AGIQA-3K
- With 10% MOS: 0.841/0.899 (SRCC/PLCC) on AGIQA-3K, matching or exceeding state-of-the-art
- With 30% MOS: 0.850/0.912 (SRCC/PLCC) on AGIQA-3K, outperforming all baselines
- Joint distillation (point-wise + pair-wise + confidence) outperforms individual components

## Why This Works (Mechanism)

### Mechanism 1: Perception-Calibration Decoupling
The core bottleneck in MLLM-based IQA is not perceptual understanding but MOS scale alignment. By separating perceptual knowledge transfer from scale calibration, LEAF avoids costly MLLM fine-tuning while achieving strong human alignment with minimal labels.

### Mechanism 2: Joint Point-wise and Pair-wise Distillation
Combining absolute quality judgments with relative preferences provides complementary supervision that outperforms either signal alone. Point-wise judgments provide scalar quality estimates while pair-wise preferences encode ranking relationships with confidence weighting.

### Mechanism 3: Confidence-Weighted Pair-wise Filtering
Entropy-based confidence estimation allows selective use of teacher decisions, improving distillation quality. Low-confidence pairs are discarded and remaining pairs are weighted by confidence in the ranking loss.

## Foundational Learning

- **Knowledge Distillation**: Transfers knowledge from frozen MLLM teacher to lightweight student via soft labels and preferences. Quick check: Why do soft labels (probability distributions) often outperform hard labels for distillation?

- **Mean Opinion Score (MOS) and Correlation Metrics**: IQA evaluation uses SRCC (rank correlation) and PLCC (linear correlation) against human MOS. Quick check: Why might a model achieve high SRCC but low PLCC, and what does this imply about calibration?

- **Ordinal Regression from Token Probabilities**: The teacher converts discrete quality tokens to continuous scores via expected value over ordinal mappings. Quick check: How does mapping {Excellent, Good, Fair, Poor, Bad} to {5,4,3,2,1} and computing expectation differ from taking the argmax token?

## Architecture Onboarding

- **Component map**: InternVL-3.5-8B (frozen teacher) -> ConvNeXt-Base (trainable student) -> MOS calibration subset
- **Critical path**: 1) Generate teacher point-wise scores for all training images, 2) Construct pair-wise supervision with confidence, 3) Train student on L_dis until convergence, 4) Fine-tune student on D_MOS using L_cal, 5) Evaluate on held-out test split
- **Design tradeoffs**: Teacher size vs. supervision quality (38B vs 8B), student capacity (ConvNeXt-Base optimal vs. ConvNeXt-Large degradation), MOS ratio (10-30% with diminishing returns beyond)
- **Failure signatures**: High SRCC but low PLCC after CFT (insufficient calibration), student underperforms teacher ranking (distillation not converging), poor cross-dataset transfer (domain-incompatible priors)
- **First 3 experiments**: 1) CFT-only baseline with 10% MOS on AGIQA-3K to confirm distillation benefit, 2) Vary pair count and measure impact on label-free performance, 3) Swap InternVL-3.5-8B for Qwen3-VL-8B to validate teacher-agnostic design

## Open Questions the Paper Calls Out

### Open Question 1
Can perceptual priors distilled from a generic MLLM teacher transfer effectively across fundamentally different domains (e.g., from natural UGC to medical imaging) using only calibration, or is domain-specific distillation required? The framework's generalizability relies on MLLM perception universality, but experiments don't test cross-domain transfer.

### Open Question 2
Does employing an active learning strategy to select the small MOS subset for calibration outperform the random sampling strategy used in the current study? Random sampling may include redundant or low-information samples; uncertainty-based or diversity-based sampling might achieve similar performance with fewer labels.

### Open Question 3
How does the student model's performance degrade when the MLLM teacher hallucinates descriptions or provides confident but incorrect judgments on novel distortion types? While the paper shows improved performance, it doesn't analyze failure cases where the teacher's perception is confidently wrong, potentially teaching the student incorrect quality priors.

## Limitations
- Framework effectiveness depends entirely on MLLM teacher's ability to provide accurate quality judgments
- Still requires 10-30% MOS supervision for final calibration, limiting true label-efficiency
- Student architecture fixed to ConvNeXt-Base with degradation observed in larger models

## Confidence
- **High**: MLLMs possess strong perceptual quality understanding that transfers via distillation; decoupling perception and calibration is sound
- **Medium**: Joint point-wise and pair-wise distillation with confidence weighting provides superior supervision (individual component contributions not fully isolated)
- **Low**: Framework's generalizability across diverse IQA domains (medical imaging, remote sensing) not tested

## Next Checks
1. Replace InternVL-3.5-8B with Qwen3-VL-8B or Q-Probe and evaluate whether same student architecture achieves comparable performance on AGIQA-3K
2. Plot relationship between teacher confidence scores and actual prediction accuracy on held-out validation set to reveal systematic overconfidence patterns
3. Implement active learning or uncertainty sampling for calibration set and compare performance against random sampling to determine if 10-30% requirement can be reduced further