---
ver: rpa2
title: 'CLARity: Reasoning Consistency Alone Can Teach Reinforced Experts'
arxiv_id: '2510.09278'
source_url: https://arxiv.org/abs/2510.09278
tags:
- reasoning
- uni00000013
- training
- reward
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLARITY improves reasoning quality in reinforcement learning for
  multiple-choice question domains by introducing a consistency-aware reward mechanism
  that monitors logical coherence between reasoning and final answers. The framework
  employs a two-stage refine-then-monitor training pipeline and dynamic data reformulation
  to maximize limited data utility.
---

# CLARity: Reasoning Consistency Alone Can Teach Reinforced Experts

## Quick Facts
- arXiv ID: 2510.09278
- Source URL: https://arxiv.org/abs/2510.09278
- Reference count: 40
- Improves reasoning consistency and accuracy in RL-based MCQ answering through consistency-aware rewards

## Executive Summary
CLARity introduces a novel approach to improving reasoning quality in reinforcement learning for multiple-choice question domains. The framework uses a consistency-aware reward mechanism that monitors logical coherence between reasoning trajectories and final answers, eliminating the need for domain-expert reward models. By employing a two-stage refine-then-monitor training pipeline and dynamic data reformulation, CLARity achieves significant improvements in both reasoning consistency (16.5%) and accuracy (7.5%) over standard baselines.

## Method Summary
CLARity is a consistency-aware learning framework for reinforcement learning that improves reasoning quality in multiple-choice question domains. The method uses a two-stage training pipeline: Stage-1 enforces explicit option-wise reasoning through structure rewards, while Stage-2 applies consistency rewards alongside accuracy rewards to allow flexible generalization. A key innovation is the use of a general-purpose LLM as a consistency reward model that parses "believed-correct" options from reasoning text and compares them to final answers. The framework also includes dynamic data reformulation that transforms easy MCQs into harder combinatorial questions to maximize limited data utility.

## Key Results
- Achieves 16.5% improvement in response consistency compared to standard RL baselines
- Delivers 7.5% improvement in accuracy across multiple-choice question domains
- Human evaluations confirm enhanced professionalism and readability of responses
- Demonstrates that smaller, general-purpose LLMs can effectively guide expert model training by monitoring consistency alone

## Why This Works (Mechanism)

### Mechanism 1: Consistency-Aware Reward Signal
- Claim: Penalizing mismatches between reasoning trajectories and final answers improves reasoning quality without requiring domain-expert reward models.
- Mechanism: A small general-purpose LLM parses the "believed-correct" options from the reasoning text and compares them to the final answer. Penalties apply when (a) no clear judgments can be extracted, or (b) extracted judgments don't match the final selection.
- Core assumption: General-purpose LLMs possess sufficient semantic understanding to identify basic correctness judgments (e.g., "Option A is correct") without domain expertise.

### Mechanism 2: Two-Stage Refine-Then-Monitor Pipeline
- Claim: Structured stage-based training prevents reward hacking while preserving reasoning flexibility.
- Mechanism: **Stage-1** enforces explicit option-wise reasoning via regex-based structure rewards, making outputs parseable. **Stage-2** removes format constraints, applies consistency rewards + accuracy rewards, allowing the model to generalize beyond rigid formats.

### Mechanism 3: Dynamic Data Reformulation
- Claim: Transforming easy MCQs into harder combinatorial questions maximizes limited data utility and improves generalization.
- Mechanism: (1) Deconstruct each MCQ into atomic proposition statements (query + option), (2) LLM-polish for fluency and diversity, (3) Dynamically reformulate the easiest α% of instances by randomly grouping propositions into new multi-select questions with template prompts.

## Foundational Learning

- **Concept: Reward Hacking in RL**
  - Why needed here: Small reward models can be gamed—the policy learns to produce outputs the weak checker rates highly, not genuinely good reasoning.
  - Quick check question: If your reward model only checks format, what reasoning behavior might the policy learn instead of genuine reasoning?

- **Concept: Logical Consistency vs. Factual Correctness**
  - Why needed here: The paper distinguishes between getting the right answer (accuracy) and having reasoning that supports that answer (consistency). Standard RL optimizes only the former.
  - Quick check question: Can a response have high consistency but low accuracy? What about high accuracy but low consistency?

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: The training algorithm used; understanding advantage estimation from grouped rollouts helps debug reward signal issues.
  - Quick check question: How does GRPO differ from PPO in how it estimates advantages?

## Architecture Onboarding

- **Component map**: Policy Model (Qwen-2.5-3B/7B-Instruct) -> Consistency Reward Model (Qwen-2.5-7B-Instruct) -> Data Reformulation Module (LLM-based proposition splitter + random grouper) -> Training Framework (veRL with GRPO)

- **Critical path**:
  1. Pre-compute initial pass rates → identify easy subset (α%)
  2. Deconstruct + reformulate easy instances into shuffled propositions
  3. Stage-1: 500 samples, 2 epochs, structure + format rewards only
  4. Stage-2: Remaining data, 8 epochs, consistency + answer + format rewards; switch to reformulated data at step 100

- **Design tradeoffs**:
  - Larger reward model (7B) → better consistency detection but higher training overhead (8 hours → 1+ day)
  - Smaller α (conservative reformulation) → more stability but less data efficiency
  - Stricter Stage-1 enforcement → less reward hacking but potential overfitting to rigid format

- **Failure signatures**:
  - High accuracy but low consistency: Standard RL without consistency reward
  - Short, superficial responses: Reward model too small or Stage-1 ablated
  - Good validation but poor out-of-domain transfer: Missing data reformulation

- **First 3 experiments**:
  1. **Baseline sanity check**: Run standard RL on JEC-QA; confirm accuracy rises but consistency drops (replicate Figure 2).
  2. **Ablation on reward model size**: Compare 7B vs. 1.5B consistency checker on detection rate (expect 94% vs. 40%).
  3. **Pipeline ablation**: Remove Stage-1, train with consistency reward from start; measure response length and "As-a-Whole" frequency (expect degradation per Figure 5).

## Open Questions the Paper Calls Out

- **Question**: Can lightweight encoders like BERT or RoBERTa replace the 7B consistency reward model while maintaining effectiveness?
  - Basis in paper: "A more efficient alternative would be to replace the LLM reward model with a lightweight encoder such as BERT or RoBERTa. However, this would require large-scale, high-quality labeled data... We therefore leave this as a promising direction for future research."
  - Why unresolved: Training such encoders for reasoning comprehension and option judgment identification requires costly labeled data that is currently unavailable.

- **Question**: Does dynamically varying the number of candidate options per question improve generalization in the data reformulation strategy?
  - Basis in paper: "In this paper, we do not fully exploit its potential flexibility—for example, dynamically varying the number of candidate options per question... We believe these directions hold promise for further improving generalization."
  - Why unresolved: The current implementation uses fixed-size groupings during reformulation; variable option counts were not tested.

## Limitations
- **Reward model dependency**: The approach's effectiveness critically depends on using a sufficiently large (7B) general-purpose LLM as the consistency checker.
- **Data reformulation assumptions**: The mechanism assumes that easy questions provide weak learning signals, but this premise lacks direct empirical validation.
- **Generalizability to other domains**: While results show improvement on legal and medical MCQs, effectiveness on other reasoning-intensive domains remains untested.

## Confidence
- **High confidence**: The core claim that consistency-aware rewards improve reasoning quality when paired with appropriate reward model size.
- **Medium confidence**: The effectiveness of the two-stage training pipeline in preventing reward hacking.
- **Medium confidence**: The dynamic data reformulation mechanism's contribution to generalization.

## Next Checks
1. **Reward model size sensitivity analysis**: Systematically test consistency detection rates and training outcomes across multiple reward model sizes (1.5B, 3B, 7B, 13B) on the same task to quantify the detection-rate vs. training-efficiency tradeoff.

2. **Stage contribution isolation**: Run a factorial experiment ablating each training stage independently (Stage-1 only, Stage-2 only, both stages) while keeping all other components constant to precisely measure each stage's contribution to final performance.

3. **Domain transfer validation**: Apply the CLARity framework to a fundamentally different reasoning domain (e.g., mathematical word problems or scientific reasoning) to test whether the consistency-monitoring approach generalizes beyond legal/medical MCQs.